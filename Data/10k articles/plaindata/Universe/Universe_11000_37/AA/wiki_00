{"id": "24443029", "url": "https://en.wikipedia.org/wiki?curid=24443029", "title": "2009 Australian dust storm", "text": "2009 Australian dust storm\n\nThe 2009 Australian dust storm, also known as the Eastern Australian dust storm, was a dust storm that swept across the Australian states of New South Wales and Queensland from 22 to 24 September. The capital, Canberra, experienced the dust storm on 22 September, and on 23 September the storm reached Sydney and Brisbane. Some of the thousands of tons of dirt and soil lifted in the dust storm were dumped in Sydney Harbour and the Tasman Sea.\n\nOn 23 September, the dust plume measured more than in width and in length and covered dozens of towns and cities in two states. By 24 September, analysis using MODIS at NASA measured the distance from the northern edge at Cape York to the southern edge of the plume to be 3,450 km. While the cloud was visible from space, on the ground the intense red-orange colour and drop in temperature drew comparisons with nuclear winter, Armageddon, and the planet Mars.\n\nThe dust storm was described by the Bureau of Meteorology as a \"pretty incredible event\" that was the worst in the state of New South Wales in nearly 70 years. The phenomenon was reported around the world. The Weather Channel's Richard Whitaker said: \"This is unprecedented. We are seeing earth, wind and fire together\".\n\nAir particle concentration levels reached 15,400 micrograms per cubic metre of air. Normal days register up to 20 micrograms and bushfires generate 500 micrograms. This concentration of dust broke records in many towns and cities. The CSIRO estimated that the storm carried some 16 million tonnes of dust from the deserts of Central Australia, and during the peak of the storm, the Australian continent was estimated to be losing 75,000 tonnes of dust per hour off the NSW coast north of Sydney. The dust storm coincided with other extreme weather conditions which affected the cities of Adelaide and Melbourne.\n\nThe dust is believed to have originated from far-western New South Wales and north-east South Australia. This includes an area known as the 'Corner Country', a dry, remote area of far-western New South Wales. In South Australia the dust may also have come from Lake Eyre Basin or the Woomera area, the latter raising concerns that it was radioactive and dangerous since the area contains the Olympic Dam uranium mine.\n\nAccording to the New South Wales regional director of the Bureau of Meteorology, Barry Hanstrum, the cause was an \"intense north low-pressure area\" which \"picked up a lot of dust from the very dry interior of the continent\". Senior forecaster Ewan Mitchel said winds from a cold front picked up dust from north-east South Australia on 22 September. That night the winds strengthened to 100 km per hour and collected more dust from areas in New South Wales that were drought affected.\n\nThe first city to be affected was Broken Hill, which was 'blacked out' at about 3:30 pm on 22 September 2009. At least one mine was shut down. It was also witnessed in Cowra. The storm blew across Canberra and the surrounding region by midday on 23 September 2009, before being washed away by overnight rain, the heaviest rainfall over Canberra in months.\n\nIt was reported that the dust set off smoke alarms across the state and prompted increased demand for emergency services. Asthma sufferers were hospitalised. Rain was also reported to have resulted, with cricket ball-sized hailstones falling.\n\nThe dust storm also reached the north coast of NSW on the morning of 23 September 2009. Coffs Harbour was affected by 7 am. At Coffs Harbour Airport visibility was down to 500 metres by 9 am and the airport remained closed until 10:30am. Grafton and the Clarence Valley were affected by 8:30 am.\nIt caused flight delays at Ballina airport and flight cancellations for most of the day at Lismore airport with visibility at 700 metres. A local school rugby union carnival was also called off.\n\nThe storm caused severe disruption to international flights—several early morning Air New Zealand flights from Auckland, Christchurch and Wellington had to return to New Zealand after finding themselves unable to land at Sydney Airport. These flights were listed as cancelled and many others were rescheduled to a later time. 18 international flights were diverted to Melbourne Airport or Brisbane Airport, while six others were cancelled altogether. There were delays of six hours reported for overseas flights, whilst domestic flights experienced disruption of as much as three hours. \n\nRoads were disrupted, including the main tunnel of the M5 East Motorway which was shut down. Building sites were closed. Ferry services were cancelled. Canterbury Park Racecourse's scheduled day of horse racing was abandoned.\n\nSchools were disrupted as those children who attended were distracted by the dust storm, while many parents kept their children home. School trips and sports activities were cancelled for the day, and children were directed to stay inside during breaks in some schools. Face masks experienced surging sales in Sydney as concerned residents rushed to protect themselves against the dust, with at least one retailer indicating she had sold more than during the swine flu pandemic.\n\nResidents of Windorah in South West Queensland reported low visibility on the morning of 22 September. By 23 September visibility in Toowoomba and Ipswich in South East Queensland was reduced to .\n\nBrisbane was affected by the dust storm, although low visibility was less of a problem at Brisbane Airport than it was at Sydney.\n\nThe Gold Coast was also affected by the dust storm by 11.30 am, reducing visibility to . Work stopped at construction sites due to health concerns, powerlines were down in some areas, the Q-deck was closed and traffic was slow with motorists using headlights. False fire alarms resulted in the evacuation of the Southport Magistrates Court. Flights were able to depart but incoming flights were diverted.\nThe beaches remained open with added 'no swimming' flags in unpatrolled areas. Two fishermen off the coast of South Stradbroke Island were lost and a helicopter was required to locate them.\n\nThe dust storm reached Central and North Queensland by the evening of 23 September 2009. However the effect was less serious, with visibility between 50 and 7,000 metres. Commercial flights were not disrupted. Affected areas include Townsville, Blackwater, Rockhampton, Mackay, Cairns and the Gulf of Carpentaria (Normanton and Kowanyama).\n\nRed dust from the storm reached New Zealand on the morning of 25 September 2009, behind a weather front that brought cold temperatures to the North Island. It was observed by satellite, atmospheric monitoring equipment (a beta attenuation monitor) at Auckland International Airport and by dust settling on the ground. Dust settled across Auckland as well as in the Northland, Waikato, Bay of Plenty and Taranaki districts of the North Island and it also reached the South Island's West Coast.\n\nA second dust storm, originating in the same area but believed to be smaller, reached Broken Hill and Cobar by 10 pm on 25 September 2009. This storm arrived in Sydney between 4 and 5 am on 26 September 2009, it pushed the EPA Air Quality Index into the 'Poor to Hazardous' range. However this was not as intense and had cleared by mid morning. The storm reached Brisbane on the evening of 26 September 2009, with the haze expected to clear by 28 September 2009. On 29 November 2009, another minor dust storm occurred, which decreased visibility to 5 km over Sydney.\n\n\n \n"}
{"id": "43156556", "url": "https://en.wikipedia.org/wiki?curid=43156556", "title": "2014 GAIL pipeline explosion", "text": "2014 GAIL pipeline explosion\n\nOn June 27, 2014 a massive fire broke out following a blast in Gas Authority of India Limited (GAIL) 18\" size underground gas Pipeline at Nagaram, East Godavari districtin East Godavari district of Andhra Pradesh, India. The accident took place near Tatipaka refinery of Oil and Natural Gas Corporation (ONGC), about 180 km from state capital Vijayawada.\n\nAbout 18 people were reportedly killed and over 40 injured in the accident. The injured were shifted to hospitals at Amalapuram and Kakinada towns.\n\nState Finance Minister Yanamala Rama Krishnudu told reporters that 22 people were killed and many others were injured when the fire broke out around 5.30 a.m. in Nagaram village in Mamidikuduru mandal of the coastal district.\nThe minister said: “The fire caused massive losses. Coconut trees, other crops, cattle and wild birds in over 10 acres were reduced to ashes.”\n\n"}
{"id": "75047", "url": "https://en.wikipedia.org/wiki?curid=75047", "title": "Aeroelasticity", "text": "Aeroelasticity\n\nAeroelasticity is the branch of physics and engineering that studies the interactions between the inertial, elastic, and aerodynamic forces that occur when an elastic body is exposed to a fluid flow. Although historical studies have been focused on aeronautical applications, recent research has found applications in fields such as energy harvesting and understanding snoring. The study of aeroelasticity may be broadly classified into two fields: static aeroelasticity, which deals with the static or steady response of an elastic body to a fluid flow; and dynamic aeroelasticity, which deals with the body's dynamic (typically vibrational) response. Aeroelasticity draws on the study of fluid mechanics, solid mechanics, structural dynamics and dynamical systems.\nThe synthesis of aeroelasticity with thermodynamics is known as aerothermoelasticity, and its synthesis with control theory is known as aeroservoelasticity.\n\nThe 2nd failure of Samuel Langley's prototype plane on the Potomac has been attributed to aeroelastic effects (specifically, torsional divergence). Problems with torsional divergence plagued aircraft in the First World War, and were solved largely by trial-and-error and ad-hoc stiffening of the wing. In 1926, Hans Reissner published a theory of wing divergence, leading to much further theoretical research on the subject.\n\nIn the development of aeronautical engineering at Caltech, Theodore von Kármán started a course \"Elasticity applied to Aeronautics\". After teaching the course for one term, Kármán passed it over to Ernest Edwin Sechler, who developed aeroelasticity in that course, and in publication of textbooks on the subject.\n\nIn 1947, Arthur Roderick Collar defined aeroelasticity as \"the study of the mutual interaction that takes place within the triangle of the inertial, elastic, and aerodynamic forces acting on structural members exposed to an airstream, and the influence of this study on design.\"\n\nIn an aeroplane, two significant static aeroelastic effects may occur. \"Divergence\" is a phenomenon in which the elastic twist of the wing suddenly becomes theoretically infinite, typically causing the wing to fail spectacularly. \"Control reversal\" is a phenomenon occurring only in wings with ailerons or other control surfaces, in which these control surfaces reverse their usual functionality (e.g., the rolling direction associated with a given aileron moment is reversed).\n\nDivergence occurs when a lifting surface deflects under aerodynamic load so as to increase the applied load, or move the load so that the twisting effect on the structure is increased. The increased load deflects the structure further, which eventually brings the structure to the point of divergence. Divergence can be understood as a simple property of the differential equation(s) governing the wing deflection. For example, modelling the airplane wing as an isotropic Euler–Bernoulli beam, the uncoupled torsional equation of motion is:\n\nWhere \"y\" is the spanwise dimension, \"θ\" is the elastic twist of the beam, \"GJ\" is the torsional stiffness of the beam, \"L\" is the beam length, and \"M’\" is the aerodynamic moment per unit length. Under a simple lift forcing theory the aerodynamic moment is of the form:\n\nWhere \"C\" is a coefficient, \"U\" is the free-stream fluid velocity, and α is the initial angle of attack. This yields an ordinary differential equation of the form:\n\nWhere:\n\nThe boundary conditions for a clamped-free beam (i.e., a cantilever wing) are:\n\nWhich yields the solution:\n\nAs can be seen, for \"λL = π/2 + nπ\", with arbitrary integer number \"n\", \"tan(λL)\" is infinite. \"n = 0\" corresponds to the point of torsional divergence. For given structural parameters, this will correspond to a single value of free-stream velocity U. This is the torsional divergence speed. Note that for some special boundary conditions that may be implemented in a wind tunnel test of an airfoil (e.g., a torsional restraint positioned forward of the aerodynamic center) it is possible to eliminate the phenomenon of divergence altogether.\n\nControl surface reversal is the loss (or reversal) of the expected response of a control surface, due to deformation of the main lifting surface. For simple models (e.g. single aileron on an Euler-Bernoulli beam), control reversal speeds can be derived analytically as for torsional divergence. Control reversal can be used to aerodynamic advantage, and forms part of the Kaman servo-flap rotor design.\n\nDynamic aeroelasticity studies the interactions among aerodynamic, elastic, and inertial forces. Examples of dynamic aeroelastic phenomena are:\n\nFlutter is a dynamic instability of an elastic structure in a fluid flow, caused by positive feedback between the body's deflection and the force exerted by the fluid flow. In a linear system, 'flutter point' is the point at which the structure is undergoing simple harmonic motion—zero net damping—and so any further decrease in net damping will result in a self-oscillation and eventual failure. 'Net damping' can be understood as the sum of the structure's natural positive damping, and the negative damping of the aerodynamic force. Flutter can be classified into two types: \"hard flutter\", in which the net damping decreases very suddenly, very close to the flutter point; and \"soft flutter\", in which the net damping decreases gradually. Methods of predicting flutter in linear structures include the p-method, the k-method and the p-k method. In water the mass ratio of the pitch inertia of the foil vs that of the circumscribing cylinder of fluid is generally too low for binary flutter to occur, as shown by explicit solution of the simplest pitch and heave flutter stability determinant.\n\nFor nonlinear systems, flutter is usually interpreted as a limit cycle oscillation (LCO), and methods from the study of dynamical systems can be used to determine the speed at which flutter will occur.\nStructures exposed to aerodynamic forces—including wings and aerofoils, but also chimneys and bridges—are designed carefully within known parameters to avoid flutter. In complex structures where both the aerodynamics and the mechanical properties of the structure are not fully understood, flutter can be discounted only through detailed testing. Even changing the mass distribution of an aircraft or the stiffness of one component can induce flutter in an apparently unrelated aerodynamic component. At its mildest this can appear as a \"buzz\" in the aircraft structure, but at its most violent it can develop uncontrollably with great speed and cause serious damage to or lead to the destruction of the aircraft, as in Braniff Flight 542. Famously the original Tacoma Narrows Bridge was destroyed as a result of aeroelastic fluttering.\n\nIn some cases, automatic control systems have been demonstrated to help prevent or limit flutter-related structural vibration.\nFlutter as a controlled aerodynamic instability phenomenon is used intentionally and positively in windmills for generating electricity and in other works like making musical tones on ground-mounted devices, as well as on musical kites. Flutter is not always a destructive force; recent progress has been made in windmills for underserved communities in developing countries, designed specifically to take advantage of this effect. The oscillating motion allows variable-stroke waterpumping to match the variable power in the wind. Semirotary binary flutter can also have an upper critical airspeed at which it stops, affording automatic high wind protection The resulting Wing'd Pump has been designed to mount on the well it pumps or float on the pond it draws from. At its large scale the flutter is coupled by static gravity imbalance as well as dynamic imbalance. Further a gravity pendulum achieves large amplitude elasticity most practically. The same annual output can be achieved with wing length equal to a multiblade rotary windpump's diameter, in half the windspeed regime. P. Sharp and J. Hare showed a toy linear generator run by two flutter wings.\n\nThe word \"flutter\" is typically linked to the form of aerodynamic instability discussed above. Though, a connection between dry friction and flutter instability in a simple mechanical system has been discovered, watch the movie for more details.\n\nBuffeting is a high-frequency instability, caused by airflow separation or shock wave oscillations from one object striking another. It is caused by a sudden impulse of load increasing. It is a random forced vibration. Generally it affects the tail unit of the aircraft structure due to air flow downstream of the wing.\n\nThe methods for buffet detection are:\n\nFlow is highly non-linear in the transonic regime, dominated by moving shock waves. It is mission-critical for aircraft that fly through transonic Mach numbers. The role of shock waves was first analyzed by Holt Ashley. A phenomenon that impacts stability of aircraft known as 'transonic dip', in which the flutter speed can get close to flight speed, was reported in May 1976 by Farmer and Hanson of the Langley Research Center.\n\nAeroelasticity involves not just the external aerodynamic loads and the way they change but also the structural, damping and mass characteristics of the aircraft. Prediction involves making a mathematical model of the aircraft as a series of masses connected by springs and dampers which are tuned to represent the dynamic characteristics of the aircraft structure. The model also includes details of applied aerodynamic forces and how they vary.\n\nThe model can be used to predict the flutter margin and, if necessary, test fixes to potential problems. Small carefully chosen changes to mass distribution and local structural stiffness can be very effective in solving aeroelastic problems.\n\nThese videos detail the Active Aeroelastic Wing two-phase NASA-Air Force flight research program to investigate the potential of aerodynamically twisting flexible wings to improve maneuverability of high-performance aircraft at transonic and supersonic speeds, with traditional control surfaces such as ailerons and leading-edge flaps used to induce the twist.\n\n\n"}
{"id": "435420", "url": "https://en.wikipedia.org/wiki?curid=435420", "title": "Anaerobic respiration", "text": "Anaerobic respiration\n\nAnaerobic respiration is respiration using electron acceptors other than molecular oxygen (O). Although oxygen is not the final electron acceptor, the process still uses a respiratory electron transport chain.\n\nIn aerobic organisms undergoing respiration, electrons are shuttled to an electron transport chain, and the final electron acceptor is oxygen. Molecular oxygen is a highly oxidizing agent and, therefore, is an excellent electron acceptor. In anaerobes, other less-oxidizing substances such as sulfate (SO), nitrate (NO), sulphur (S), or fumarate are used. These terminal electron acceptors have smaller reduction potentials than O, meaning that less energy is released per oxidized molecule. Therefore, generally speaking, anaerobic respiration is less efficient than aerobic.\n\nThere are two important microbial methane formation pathways, through carbonate reduction (respiration), and acetate fermentation.\n\nCellular respiration (both aerobic and anaerobic) utilizes highly reduced chemical compounds such as NADH and FADH2 (for example produced during glycolysis and the citric acid cycle) to establish an electrochemical gradient (often a proton gradient) across a membrane, resulting in an electrical potential or ion concentration difference across the membrane. The reduced chemical compounds are oxidized by a series of respiratory integral membrane proteins with sequentially increasing reduction potentials with the final electron acceptor being oxygen (in aerobic respiration) or another chemical substance (in anaerobic respiration). A proton motive force drives protons down the gradient (across the membrane) through the proton channel of ATP synthase. The resulting current drives ATP synthesis from ADP and inorganic phosphate.\n\nFermentation, in contrast, does not utilize an electrochemical gradient. Fermentation instead only uses substrate-level phosphorylation to produce ATP. The electron acceptor NAD+ is regenerated from NADH formed in oxidative steps of the fermentation pathway by the reduction of oxidized compounds. These oxidized compounds are often formed during the fermentation pathway itself, but may also be external. For example, in homofermentative lactic acid bacteria, NADH formed during the oxidation of glyceraldehyde-3-phosphate is oxidized back to NAD+ by the reduction of pyruvate to lactic acid at a later stage in the pathway. In yeast, acetaldehyde is reduced to ethanol to regenerate NAD+. The two processes thus generate ATP in very different ways, and the terms should not be treated as synonyms.\n\nAnaerobic respiration is a critical component of the global nitrogen, iron, sulfur, and carbon cycles through the reduction of the oxyanions of nitrogen, sulfur, and carbon to more-reduced compounds. The biogeochemical cycling of these compounds, which depends upon anaerobic respiration, significantly impacts the carbon cycle and global warming. Anaerobic respiration occurs in many environments, including freshwater and marine sediments, soil, subsurface aquifers, deep subsurface environments, and biofilms. Even environments, such as soil, that contain oxygen also have micro-environments that lack oxygen due to the slow diffusion characteristics of oxygen gas.\n\nAn example of the ecological importance of anaerobic respiration is the use of nitrate as a terminal electron acceptor, or dissimilatory denitrification, which is the main route by which fixed nitrogen is returned to the atmosphere as molecular nitrogen gas. Another example is methanogenesis, a form of carbonate respiration, that is used to produce methane gas by anaerobic digestion. Biogenic methane is used as a sustainable alternative to fossil fuels. On the negative side, uncontrolled methanogenesis in landfill sites releases large volumes of methane into the atmosphere, where it acts as a powerful greenhouse gas. Sulfate respiration produces hydrogen sulfide, which is responsible for the characteristic 'rotten egg' smell of coast wetlands and has the capacity to precipitate heavy metal ions from solution, leading to the deposition of sulfidic metal ores.\n\nDissimilatory denitrification is widely used in the removal of nitrate and nitrite from municipal wastewater. An excess of nitrate can lead to eutrophication of waterways into which treated water is released. Elevated nitrite levels in drinking water can lead to problems due to its toxicity. Denitrification converts both compounds into harmless nitrogen gas.\n\nSpecific types of anaerobic respiration are also critical in bioremediation, which uses microorganisms to convert toxic chemicals into less-harmful molecules to clean up contaminated beaches, aquifers, lakes, and oceans. For example, toxic arsenate or selenate can be reduced to less toxic compounds by various anaerobic bacteria via anaerobic respiration. The reduction of chlorinated chemical pollutants, such as vinyl chloride and carbon tetrachloride, also occurs through anaerobic respiration.\n\nAnaerobic respiration is useful in generating electricity in microbial fuel cells, which employ bacteria that respire solid electron acceptors (such as oxidized iron) to transfer electrons from reduced compounds to an electrode. This process can simultaneously degrade organic carbon waste and generate electricity.\n\n"}
{"id": "12305030", "url": "https://en.wikipedia.org/wiki?curid=12305030", "title": "Anderson impurity model", "text": "Anderson impurity model\n\nThe Anderson impurity model is a Hamiltonian that is used to describe magnetic impurities embedded in metallic hosts. It is often applied to the description of Kondo-type problems, such as heavy fermion systems and Kondo insulators. In its simplest form, the model contains a term describing the kinetic energy of the conduction electrons, a two-level term with an on-site Coulomb repulsion that models the impurity energy levels, and a hybridization term that couples conduction and impurity orbitals. For a single impurity, the Hamiltonian takes the form\n\nformula_1,\n\nwhere the formula_2 operator corresponds to the annihilation operator of an impurity, and formula_3 corresponds to a conduction electron annihilation operator, and formula_4 labels the spin. The onsite Coulomb repulsion is formula_5, which is usually the dominant energy scale, and formula_6 is the hopping strength from site formula_7 to site formula_8. A significant feature of this model is the hybridization term formula_9, which allows the formula_2 electrons in heavy fermion systems to become mobile, although they are separated by a distance greater than the Hill limit.\n\nFor heavy-fermion systems, a lattice of impurities is described by the periodic Anderson model:\n\nformula_11\n\nThere are other variants of the Anderson model, for instance the SU(4) Anderson model, which is used to describe impurities which have an orbital, as well as a spin, degree of freedom. This is relevant in carbon nanotube quantum dot systems. The SU(4) Anderson model Hamiltonian is\n\nformula_12\n\nwhere i and i' label the orbital degree of freedom (which can take one of two values), and n represents a number operator.\n\n\n"}
{"id": "25383607", "url": "https://en.wikipedia.org/wiki?curid=25383607", "title": "California Volcano Observatory", "text": "California Volcano Observatory\n\nThe California Volcano Observatory (CalVO) is the volcano observatory that monitors the volcanic and geologic activity of California and Nevada. It is a part of the Volcano Hazards Program of the United States Geological Survey, a scientific agency of the United States government.\n\nOriginally, the volcano observatory was known as the Long Valley Observatory which monitored volcanic activity east of the Sierra Nevada in Mono County, California which included Long Valley Caldera, Mammoth Mountain, and the Mono–Inyo Craters. \n\nIn 2012, the Long Valley Observatory was integrated into the new California Volcano Observatory based in Menlo Park, California which covers the entire states of California and Nevada, this includes the southern Cascade Range volcanoes in the state of California which were previously under the jurisdiction of the Cascades Volcano Observatory.\n\n"}
{"id": "34894149", "url": "https://en.wikipedia.org/wiki?curid=34894149", "title": "Cambridge Water Company", "text": "Cambridge Water Company\n\nThe Cambridge Water Company is a water supply utility company serving Cambridge and the surrounding area. It was established by \"The Cambridge University and Town Waterworks Act, 1853\" and was privately owned until it became a Public Limited Company in 1996. It was sold to Chung Kong Infrastructure Holdings Limited (CKI) in 2004, but was sold on to HSBC in 2011. The sale was made because CKI wanted to acquire Northumbrian Water, and retaining Cambridge Water would have resulted in the takeover of Northumbrian Water being referred to the Competition Commission. It became part of South Staffordshire Water in 2013.\n\nThe Cambridge Water Company is a water supply company and does not provide wastewater services. Anglian Water provides wastewater services to Cambridge Water customers.\n"}
{"id": "18527533", "url": "https://en.wikipedia.org/wiki?curid=18527533", "title": "Canadian Nuclear Society", "text": "Canadian Nuclear Society\n\nThe Canadian Nuclear Society (CNS) is a not-for-profit organization representing individuals contributing to, or otherwise supporting, nuclear science and engineering in Canada.\n\nThe CNS is a member of the International Nuclear Societies Council (INSC).\n\nThe Canadian Nuclear Society was established in 1979 as \"the technical society of the Canadian Nuclear Association (CNA)\". Although legally a division of the CNA, the CNS operated independently from the start, with its own volunteer Council (Board of Directors), its own mandate, its own activities, and its own budget. In 1998 the CNS incorporated independently as a federal, not-for-profit corporation, following an overwhelming vote from members. Since then the legal name of the CNS is \"Canadian Nuclear Society/Société Nucléaire Canadienne, Inc.\".\n\nThe CNS is dedicated to the exchange of information, both within the nuclear professional and academic community, and with the public, in the field of applied nuclear science and technology. This encompasses all aspects of nuclear energy, uranium, fission and other nuclear technologies such as occupational and environmental protection, medical diagnosis and treatment, the use of radioisotopes, and food preservation. \n\nThe CNS is governed by a council, acting as its board of directors, consisting of an executive committee and nine - sixteen members-at-large. The 2017–2018 executive committee consists of: Daniel Gammage (president), Peter Ozemoyah (past president), John Luxat (1st vice-president and president-elect), Keith Stratton (2nd vice-president), Mohamed Younis (treasurer), Colin Hunt (secretary), Ken Smith (financial administrator), Ben Rouben (executive director), Peter Easton (communications director), and John Roberts (chair of the Education and Communications Committee).\n\nThe CNS membership includes about 1200 individuals, mostly from within Canada.\n\nThe primary category of CNS membership is that of an individual directly involved in the use or development of a nuclear technology in any of the above areas or an individual who is simply interested in nuclear technology. \n\nAnother category of CNS membership is that of an educational institution, such as school or university, or public library, that has an interest in providing timely information on nuclear science and technology to a student body or to the public at large. This type of membership has all the privileges of an individual membership with the exception of voting rights. \n\nThe CNS is structured along five main technical Divisions (Design & Materials, Fuel Technologies, Nuclear Operations & Maintenance, Nuclear Science & Engineering, and Environment & Waste Management), whose main activities are to organize and conduct workshops, courses, symposia, or conferences within their respective technical areas. \n\nGeneral administration and outreach (public, other societies, etc.) are typically carried out by a number of Committees within the CNS.\n\nThe CNS holds an annual conference each June, which includes technical sessions covering all fields, as well as plenary sessions that address topics of broad interest.\n\nAt the local level across the country, the CNS includes fourteen branches (nine in Ontario, one in each of Alberta, Saskatchewan, Manitoba, Quebec, and New Brunswick)\n\nThe CNS is affiliated with several \"sister\" nuclear societies around the world, and is also an organizational member of the Engineering Institute of Canada (EIC). The president of the CNS is an \"ex officio\" voting member of the board of directors of the Canadian Nuclear Association.\n\n"}
{"id": "3085316", "url": "https://en.wikipedia.org/wiki?curid=3085316", "title": "Commonly used gamma-emitting isotopes", "text": "Commonly used gamma-emitting isotopes\n\nRadionuclides which emit gamma radiation are valuable in a range of different industrial, scientific and medical technologies. This article lists some common gamma-emitting radionuclides of technological importance, and their properties.\n\nMany artificial radionuclides of technological importance are produced as fission products within nuclear reactors. A fission product is a nucleus with approximately half the mass of a uranium or plutonium nucleus which is left over after such a nucleus has been \"split\" in a nuclear fission reaction.\n\nCaesium-137 is one such radionuclide. It has a half-life of 30 years, and decays by pure beta decay to a metastable state of barium-137 (). Barium-137m has a half-life of minutes and is responsible for all of the gamma ray emission. The ground state of barium-137 is stable.\n\nThe gamma ray (photon) energy of Ba-137m is about 662 keV. These gamma rays can be used, for example, in radiotherapy such as for the treatment of cancer, in food irradation, or in industrial gauges or sensors. Cs-137 is not widely used for industrial radiography as other nuclides, such as cobalt-60 or iridium-192 for example, offer higher radiation output for a given volume.\n\nIodine-131 is another important gamma-emitting radionuclide produced as a fission product. With a short half-life of 8 days, this radioisotope is not of practical use in radioactive sources in industrial radiography or sensing. However, since iodine is a component of biological molecules such as thyroid hormones, iodine-131 is of great importance in nuclear medicine, and in medical and biological research as a radioactive tracer.\n\nLanthanum-140 is a decay product of barium-140, a common fission product. It is a potent gamma emitter. It was used in high quantities during the Manhattan Project for the RaLa Experiments.\n\nSome radionuclides, for example cobalt-60 and iridium-192, are made by the neutron irradiation of normal non-radioactive cobalt and iridium metal in a nuclear reactor, creating radioactive nuclides of these elements which contain extra neutrons, compared to the original stable nuclides.\n\nIn addition to their uses in radiography, both cobalt-60 (Co-60) and iridium-192 (Ir-192) are used in the radiotherapy of cancer. Cobalt-60 tends to be used in teletherapy units as a higher photon energy alternative to Cs-137, while iridium-192 tends to be used in a different mode of therapy, internal radiotherapy or brachytherapy. The iridium wires for brachytherapy are a palladium-coated iridium/palladium alloy wire made radioactive by neutron activation. This wire is then inserted into a tumor such as a breast tumor, and the tumor is irradiated by gamma ray photons from the wire. At the end of the treatment the wire is removed.\n\nA rare but notable gamma source is sodium-24, this has a very short half-life but it emits photons with very high energies (>2 MeV). It could be used for radiography of thick steel objects if the radiography occurred close to the point of production. In common with Co-60 and Ir-192 it is formed by the neutron activation of the commonly found stable isotope.\n\nAmericium-241 has been used as a source of low energy gamma photons, it has been used in some applications such as portable X-ray fluorescence equipment (XRF) and common household ionizing smoke detectors.\n\nMany years ago radium-226 and radon-222 sources were used as gamma-ray sources for industrial radiography: for instance a radon-222 source was used to examine the mechanisms inside an unexploded V-1 flying bomb, while some of the early Bathyspheres could be examined using radium-226 to check for cracks. Because both radium and radon are very radiotoxic and very expensive due to their natural rarity, these natural radioisotopes have fallen out of use over the last half-century, replaced by artificially created radioisotopes.\n\n"}
{"id": "5150414", "url": "https://en.wikipedia.org/wiki?curid=5150414", "title": "Controlled waste", "text": "Controlled waste\n\nControlled waste is waste that is subject to legislative control in either its handling or its disposal. As a legal term, Controlled waste applies exclusively to the UK but the concept is enshrined in laws of many other countries. The types of waste covered includes domestic, commercial and industrial waste. They are regulated because of their toxicity, their hazardous nature or their capability to do harm to human health or the environment either now or at some time in the future. A prime concern is the effects of biodegradation or biochemical degradation and the by-products produced.\n\n"}
{"id": "3088667", "url": "https://en.wikipedia.org/wiki?curid=3088667", "title": "Danish Ministry of Transport", "text": "Danish Ministry of Transport\n\nThe Danish Ministry of Transport () is the Danish ministry in charge of coordinating and realizing the transport politics of Denmark.\n\nThe Ministry is headed by a Permanent Secretary. The Ministry of Transport employs approximately 140 staff. The daily administration and handling of tasks and assignments on transport are carried out by a number of institutions, executive agencies, corporations, councils and boards. Counting every institution and every corporation the Ministry employs around 40.000 people\n\nThe Ministry of Transport was founded in 1892 under the name Ministry for Public Works (\"\"Ministeriet for offentlige Arbejder\"). In 1987 it changed name to Ministry of Traffic (\"Trafikministeriet\"), though briefly known as Ministry of Traffic and Communication (\"Trafik- og Kommunikationsministeriet\"\") during 1988 to 1989. In 2005 the energy sector was detached from Ministry of the Environment and attached to the Ministry of Traffic. In turn, the name was changed to Ministry of Transport and Energy; the energy department was transferred to what is now known as the Danish Ministry of Climate and Energy in 2007.\n\n\n"}
{"id": "23671037", "url": "https://en.wikipedia.org/wiki?curid=23671037", "title": "Edward L. Morse", "text": "Edward L. Morse\n\nEdward Lewis Morse (born January 5, 1942 in New York City) is an American energy economist. He is currently the Global Head of Commodities Research at Citigroup in New York. From 1969 to 1975, he taught at the Woodrow Wilson School at Princeton University. From 1979 to 1981, Morse served as Deputy Assistant Secretary of State for International Energy Policy. From 2006 to 2008, he was chief energy economist at Lehman Brothers, where he argued the oil price rises of 2007 and 2008 were an unsustainable bubble. He is the author of numerous books and scholarly articles on international relations and energy topics. He was a co-founder of PFC Energy, a Washington-based energy consultancy group.\n\nMorse was born in New York City and graduated from Johns Hopkins University (B.A., 1963; M.A., 1966). He then received his Ph.D. from Princeton University in 1969.\n\nHe taught international relations at Princeton University's Woodrow Wilson School of Public and International Affairs from 1969 to 1975. From 1975 to 1978, Morse was a senior research fellow at the Council on Foreign Relations.\n\nIn 1978, Morse joined the U.S. Department of State, where he was initially executive assistant to the Undersecretary for Economic Affairs. In 1979, Morse became Deputy Assistant Secretary of State for International Energy Policy, and was the U.S. representative at the International Energy Agency, under the Carter and Reagan administrations.\n\nFrom 1981 to 1984, he worked at Phillips Petroleum Co., where he was Director for International Affairs. He was a co-founder and Managing Director at PFC Energy from 1984 to 1996. He was president of the \"Petroleum Intelligence Weekly\", a premier journal for energy intelligence, from 1988 to 1999. In 1999, Morse joined Hess Energy Trading Co. as a senior executive.\n\nIn 2001, Morse chaired a task force on energy security, sponsored jointly by the Council on Foreign Relations and the James A. Baker III Institute at Rice University.\n\nIn 2006, Morse joined Lehman Brothers as a managing director and chief energy economist, where he built a world-class team as part of the #1-ranked fixed-income research group. Upon its bankruptcy in 2008, Morse moved to Louis Capital Markets. In January 2010, Edward Morse joined Credit Suisse as Head of Commodities Research. In 2011, Morse was hired by Citigroup to be their Global Head of Commodities Research.\n\nHe is chairman of the New York Energy Forum, and serves on the academic advisory boards at Columbia University's School of International and Public Affairs and Johns Hopkins University's School of Advanced International Studies. He is a member of Oxford Energy Policy Club, the Council on Foreign Relations, and an editor of The Geopolitics of Energy. For his pioneering contributions to energy research, he was honored by the International Association for Energy Economics.\n\nMorse has a keen interest in ballet, and is a member of the Chairman's Council of the American Ballet Theatre.\n\nHe married Linda Kasle Jones on August 15, 1965. He has two children, Michael Ari and Molly Rachel, and five grandchildren.\n\nIn 2008, as oil prices approached its peak near $150/bbl, Morse and his research team at Lehman Brothers argued this was an unsustainable overshooting fueled by behavioral herding, and that there would be sufficient demand destruction in advanced economies and new supply from non-OPEC countries to cause prices to collapse . As oil prices subsequently plunged, Morse's foresight proved correct .\n\nIn 2009 Morse provided a valuation for the Malaysian state fund 1MDB for petroleum exploration and production assets associated with PetroSaudi International Ltd. The valuation he provided was $2.9 billion, which supported 1MDB's decision to enter a joint venture with PetroSaudi. The same assets were valued earlier that year by PetroSaudi at $0.5 billion. Leaked emails and 1MDB board minutes suggest that Morse's valuation was influenced by PetroSaudi director Patrick Mahony.\n\n\n"}
{"id": "37498273", "url": "https://en.wikipedia.org/wiki?curid=37498273", "title": "Elevational diversity gradient", "text": "Elevational diversity gradient\n\nElevational diversity gradient (EDG) is an ecological pattern where trends in biodiversity occur at different elevations. The EDG states that species richness tends to increase as elevation increases, up to a certain point, creating a \"diversity bulge\" at middle elevations. There have been multiple hypotheses proposed for explaining the EDG, none of which accurately describe the phenomenon in full.\n\nA similar pattern, known as the latitudinal diversity gradient, describes an increase in biodiversity from the poles to the equator. While the EDG generally follows the LDG (i.e., high elevations in tropical regions have greater biodiversity than high elevations in temperate regions), the LDG does not account for elevational changes.\n\nThe first recorded observation of the elevational diversity gradient was by Carl Linnaeus in his treatise \"On the growth of the habitable earth\". In this document, Linnaeus based his predictions on flood geology, assuming most of the world was at one point inundated, leaving only the highest elevations available for terrestrial life. Since, by Linnaeus’ hypothesis, all life was concentrated at high elevations, a higher species diversity would be observed there even as life re-populated lower elevations.\n\nIn 1799, Alexander von Humboldt and Aimé Bonpland described elevational changes along the Andean slopes, noting how climatic changes impacted plant and animal communities. These observations contributed to Leslie R. Holdridge’s \"life zone concept\" (1947). Climatic variables shaping life zones include mean potential temperature, total annual precipitation, and the ratio of mean annual evapotranspiration to mean annual precipitation. These variables, most notably precipitation and temperature, vary along an elevational gradient, resulting in the distribution of different ecosystems.\n\nMuch of the current literature correlates elevational diversity to gradients in single climatic or biotic variables including \"rainfall, temperature, productivity, competition, resource abundance, habitat complexity, or habitat diversity\".\n\nA pattern in species richness is also observed as one moves along an elevational gradient; generally, species richness is thought to decline with increasing elevation. Whether this decline is monotonic or if it assumes different shapes based on the taxa or region being studied is still a topic of debate. In a review of previous studies looking at elevational diversity gradients, Rahbek noted the importance of other factors contributing to the shape of a gradient, distinguishing elevational patterns from those described by the latitudinal diversity gradient.\n\nFor certain taxa and regions, there is a mid elevational peak in species richness. This pattern has received empirical support for small mammals, spiders, ants and plants. Alternatively, microbes have been shown to exhibit not only monotonically decreasing diversity when moving from low to high elevations, but also increasing, hump-shaped, and U-shaped elevational patterns in diversity.\nOne explanation for a mid elevational peak includes mid elevational condensation zones. Under the assumption that natural boundaries can limit species distributions in varying degrees (for example, a mountain can present absolute elevational limits), Collwell and Lees explained the mid domain effect with geometric theory. In the context of a mountain, geometric boundary constraints will naturally result in the increasing overlap of species ranges nearing the midpoint of the mountain. Using vascular epiphytes in Costa Rica, Cardelus et al. (2008) noted that the elevational species richness pattern observed was substantially due to the mid domain effect; there was a bulge in epiphyte species richness at 1000m (The cloud forest).\nThis elevational pattern, however, was less consistent for species with small ranges, suggesting that environmental factors may be more clearly accounted for when constraints on domain boundary are loosened. In cases where geometric models fail to explain the location of the midpoint or the trend in species richness, other explanations need to be explored. An example of this can be seen with microbes, which have been shown to exhibit monotonically decreasing diversity when moving from low to high elevations.\n\nThe mountain-mass effect (also known as the Massenerhebung effect or mass-elevation effect) was proposed in 1904 by A. de Quarvain. This phenomenon recognizes the correlation between mountain mass and the occurrence of physiognomically similar vegetation types; similarity in vegetation type is observed at higher elevations on large mountain masses. Furthermore, under a climatically driven mountain–mass effect, there is a “positive linear trend observed in the elevation of highest diversity with mountain height”. This trend is most evident on isolated mountain peaks.\n\nAnother hypothesis that is cited to explain the upper limit of the elevational diversity gradient is the area hypothesis, which states that larger areas are able to support more species. As elevation increases, total area decreases; thus, there are more species present at middle elevations than high elevations.\n\nHowever, this hypothesis does not account for differences between lowland areas and middle elevations, as lowlands tend to have more area than middle elevations and thus would be expected by this hypothesis to have higher species diversity, an assertion that runs counter to the EDG. Additionally, the area hypothesis does not take climatic conditions or resource availability into account.\n\nThis hypothesis states that diversity increases with increasing rainfall, however the correlation between rainfall and plant diversity varies from region to region. The consistency of rainfall seems to correlate more with species richness than total annual rainfall. Species diversity appears to level off when annual rainfall reaches about 4,000mm, however this could be due to sampling limitations. Rainfall and soil richness affect productivity trends which are also believed to affect diversity. A mid elevation peak is usually seen in mean annual rainfall.\n\nThe resource diversity hypothesis states an increase in diversity can be seen when an increase in the diversity of available resources such as soil and food is present. In this hypothesis diversity increases in an area of higher resource diversity even when resource abundance is constant. However resource diversity, especially pertaining to food, could be a result of other influences, such as rainfall and productivity; as such, it may be inappropriate to consider the resource diversity hypothesis as a mechanism acting independently of other factors influencing diversity gradients.\n\nThe productivity hypothesis states that diversity increases with increased productivity. There is some contradiction to this as other research suggests that after a certain point increasing productivity actually correlates with a decrease in diversity.\n\nIt is generally thought that productivity decreases with an increase in elevation, however there is some research that shows a peak in productivity at mid elevation which may be related to a peak in rainfall within the same area.\n\nThe temperature hypothesis correlates increasing temperature with an increase in species diversity, mainly because of temperature's effect on productivity. However increasing temperatures due to climate change have begun to be linked to the spread of chytrid among frogs in the Tropics. Another concern is that higher-elevation species will become extinct as their ranges become more and more restricted with an increase in temperature. This hypothesis is an important factor in considering the effects of global warming.\n\nThere are conflicting views on the effect of competition on species diversity. Some hold the view that an increase in interspecies competition leads to local extinctions and a decrease in diversity. Others view competition as a means of species specialization and niche partitioning, resulting in increase diversity.\n\nIn other studies the competition between plant species at high elevations has been shown to facilitate the movement of plant species into high stress environments. The competition between plant species leads to hardier species spreading into the high stress environment. These founder species then provide shelter and facilitate the movement of less hardy species into the area. This may result in the movement of plant species up a mountainside.\n\nCurrent research illuminates a variety of mechanisms than can be used to explain elevational diversity gradients. No one factor can be used to explain the present of diversity gradients within and among taxa; in many cases, we must consider more than one hypothesis or mechanism to fully understand a pattern in elevational diversity. The emerging macroecological experiments along environmental gradients (for example, mountain elevation gradients) are an important tool in ecological research because they allow for the disentangling the effects of individual environmental drivers on biodiversity, the independent effects of which are not be easily separated due to their covariance in nature. For instance, microcosm experimental setups in subtropical and subarctic regions (China and Norway, respectively) showed clear segregation of bacterial species along temperature gradients, and interactive effects of temperature and nutrients on biodiversity along mountain elevation gradients. A more expansive research program for mountain biogeography may be extremely beneficial for conservation biologists seeking to understand factors driving biodiversity in known hot spots. Further research and reviews are also needed to address contradictions in the scientific literature, and to identify the extent of interactions between current explanations and hypotheses.\n"}
{"id": "11802351", "url": "https://en.wikipedia.org/wiki?curid=11802351", "title": "Energy Research and Development Administration", "text": "Energy Research and Development Administration\n\nThe United States Energy Research and Development Administration (ERDA) was a United States government organization formed from the split of the Atomic Energy Commission (AEC) in 1975. It assumed the functions of the AEC not assumed by the Nuclear Regulatory Commission.\n\nThe agency was created as part of the Energy Reorganization Act of 1974, which was passed on October 11, 1974 in the wake of the 1973 oil crisis. The act split the Atomic Energy Commission into two new agencies: the Nuclear Regulatory Commission would regulate the commercial nuclear power industry, while the ERDA would manage the energy research and development, nuclear weapons, and naval reactors programs.\n\nThe Energy Research and Development Administration was activated on January 19, 1975. The first Administrator was Robert Seamans, followed by Robert W. Fri.\n\nIn 1977, ERDA was combined with the Federal Energy Administration to form the United States Department of Energy.\n\n"}
{"id": "9860594", "url": "https://en.wikipedia.org/wiki?curid=9860594", "title": "Fender Bassman", "text": "Fender Bassman\n\nThe Fender Bassman is a bass amplifier introduced by Fender during 1952. Initially intended to amplify bass guitars, the 5B6 Bassman was used by musicians for other instrument amplification, including the electric guitar, harmonica, and pedal steel guitars. Besides being a popular and important amplifier in its own right, the Bassman also became the foundation on which Marshall and other companies built their high-gain tube amplifiers.\n\nDuring 1952, the Fender 5B6 Bassman amplifier was introduced as a combo amplifier cabinet that included the amplifier chassis combined with one 15\" speaker. The 1952–1954 5B6 Bassman amplifiers had two 6SC7 or 6SL7GT pre-amp tubes, two 5881 power tubes and a single 5U4G rectifier tube. It was designed to generate 26 watts at an 8 ohm impedance load, and offered a cathode-based bias.\n\nFrom 1952 through the spring of 1954, Fender produced approximately 660 model 5B6 Bassman amplifiers (serial numbers #0001–0660). The earlier cabinets have been called \"TV Front\" designs, with a front panel that had a rectangular grill cloth with rounded corners and looked much like a television of that era. In 1953 the cabinet designs were changed to the so-called \"Wide Panel\" design, with a 5 inch wide tweed covered panel above and below a wider swath of grill cloth. Fender ceased production of 5B6 Bassman amplifiers during the spring of 1954.\n\nDuring November 1954, Fender introduced the newly designed 5D6 Bassman amplifier offering four ten inch speakers and was designed utilizing two rectifier tubes. The 5D6 was a major departure from the earlier 5B6 Fender Bassman model. Designed by Freddie Tavares, longtime R&D man at Fender, the new circuit included two rectifier tubes and became known as the Dual Rectifier Bassman. Instead of the single 15\" speaker, four 10\" Jensen Alnico P10R speakers were used. The circuit had two innovations: a fixed bias for the power tubes, which increased power in comparison to the earlier cathode bias design, and a cathodyne phase inverter, using half of the 12AX7 tube and allowing a third gain stage on the other half.\n\nThe first 4x10 Bassman amplifiers started with a batch of prototypes in November and December 1954, model 5D6. No schematic for the 5D6 circuit has ever been found, but Ken Fox and Frank Roy have created a few from originals, and copies are freely available online. Only 11 of these early 5D6 Bassman examples are known to have survived. The lowest serial number known to still exist is 0013 (Frank Roy), 0035 (Albert Talley), 0075 (Jim Cornett), 0077 (Perry Tate), 0089 (Mark Grandfield), 0701, 0745 (Walter Horton), 0769 (Hayes Kolb), 0780 (sold on eBay Nov 2006), 0783, and 0785 (Hayes Kolb) are among those still known to exist.\n\nFender began making other models with tweed covering, a similar open backed cabinet with a rectangular grill cloth and a narrow (just over an inch wide) tweed covered panel at the top and bottom. Produced from 1954 until 1960, these models are called the \"narrow panel\" tweed amps .\n\nFender introduced the model 5D6 \"DK\" in November 1954 followed by the 5E6 Bassman Amp during early 1955. The 5E6-A Bassman model was introduced later that year and included some evolutionary improvements. Demand for the tweed Bassman amp grew, so Fender increased production. By the middle of 1957 more than 1,500 examples of the 5E6 series had been sold.\n\nIn July 1957, Fender introduced the model 5F6 Bassman. This model also had four Jensen P10R speakers, but the power supply was redesigned around a single 83 mercury vapor rectifier tube, and a new preamp circuit was introduced that included a three knob tone stack, with separate controls for Treble, Mid and Bass. The power amp included a \"long tailed pair\" phase inverter, an innovation that noticeably increased the \"headroom\" or clean power output capability of the amplifier. Similar preamp changes were also incorporated in the 5F8 Twin Amp at about the same time, but not on other large size Fender amps.\n\nDuring 1958, Fender introduced the model 5F6-A Bassman model. This final 1950s Tweed Bassman model product line included a change from the 5Y3 to the GZ34 rectifier tube, as well as a modification within the Presence control circuit. During early 1960, Fender began producing the 5F6-A Bassman with Jensen P10Q speakers. The P10Q Jensen speakers are more able to manage stronger electrical input power and generate better \"clean\" output sounds than previous installed P10R Jensen speakers. The P10R Jensen speakers were shipped within all Fender Bassmen from late 1954 until early 1960. Many professional music industry analysts have heralded the 1950s Fender 4×10 Bassman amps as the greatest guitar amp ever. The first 1954 Fender Tweed 5D6 4×10 circuit generated further Tweed Bassman amplifier development through 1960. Several Bassman models were progressively influenced by the 5D6 through the last Fender Tweed 5F6-A Bassman's circuit design. The 5F6-A Bassman's design was directly copied by Marshall Amplifiers within their JTM-45 amplifier during the early 1960s.\n\nIn 1990, Fender began reissuing the 5F6-A Bassman. The first series of the reissue were made at the Corona, California facility, and came equipped with four Eminence-made 10\" blue frame AlNiCo speakers, and a solid state rectifier unit. Later on, production was moved to Ensenada, Baja California, and the model name was altered to \"59 Bassman LTD\". The LTD came equipped with the original 5AR4 rectifier tube, and four Jensen P10R reissue alnico speakers, which was period correct for the original amp.\n\nIn late 1960, Fender introduced a completely redesigned model 6G6 Bassman Amp, using the \"piggy-back\" design, in which the amplifier chassis is housed in a small cabinet, attached by metal clips to a larger separate speaker enclosure.\n\nThe early models were called \"Brownface\" because of the dark brown color used on the control panel. The 6G6 model was covered in rough Blonde colored Tolex material with Oxblood colored grill cloth. It had a single GZ34 rectifier, two 5881/6L6GC power tubes and four 12AX7 preamp tubes. The output was 50 watts at 8 ohm into a single 12 inch speaker, with a \"Tone Ring\" baffle in the speaker cabinet. In early 1961, model 6G6-A was introduced with a solid state rectifier replacing the GZ34, and two 12 inch speakers with a conventional baffle in a slightly larger cabinet (wired in parallel) with a 4 ohm output. In 1962, model 6G6-B was introduced, which incorporated circuit changes but used the same speaker configuration. In 1963 smooth Blonde Tolex covering was used instead of the early rough texture cover, and a light tan grill cloth. In late, 1963, Fender changed the cosmetics to what is commonly known as the \"blackface\" scheme. This amp still had the presence knob and same circuit (designated 6G6-B) as the smooth Blonde Tolex Bassman, but the faceplace was now black, the Tolex was black, and the grillcloth had moved to a silver cloth with black thread. The logo had also transitioned from the flat cast tin Fender with the brown paint in the tail, to a plastic logo with faux chrome and more 3-D shape.\n\nIn 1964 Fender introduced the AA864 circuit, and changed the appearance to the \"Blackface\" design, with black tolex covering and a black painted control panel. Fender was sold to CBS in 1965, and the AA165 circuit was briefly introduced, before being replaced by the AB165 circuit. The \"Blackface\" design continued until the \"Silverface\" model was introduced in 1968. Early \"Drip-Edge\" Silverface Bassmen used the same AB165 circuitry as the previous Blackface versions. The Brownface, Blackface, and Silverface \"piggyback head\" (except the Bassman 10 and 20, which were also combo amplifiers) versions of the 1960s, 1970s, and early 1980s generally followed a trend toward cleaner sound and more headroom.\n\n\n"}
{"id": "10822", "url": "https://en.wikipedia.org/wiki?curid=10822", "title": "Fermium", "text": "Fermium\n\nFermium is a synthetic element with symbol Fm and atomic number 100. It is an actinide and the heaviest element that can be formed by neutron bombardment of lighter elements, and hence the last element that can be prepared in macroscopic quantities, although pure fermium metal has not yet been prepared. A total of 19 isotopes are known, with Fm being the longest-lived with a half-life of 100.5 days.\n\nIt was discovered in the debris of the first hydrogen bomb explosion in 1952, and named after Enrico Fermi, one of the pioneers of nuclear physics. Its chemistry is typical for the late actinides, with a preponderance of the +3 oxidation state but also an accessible +2 oxidation state. Owing to the small amounts of produced fermium and all of its isotopes having relatively short half-lives, there are currently no uses for it outside basic scientific research.\n\nFermium was first discovered in the fallout from the 'Ivy Mike' nuclear test (1 November 1952), the first successful test of a hydrogen bomb. Initial examination of the debris from the explosion had shown the production of a new isotope of plutonium, : this could only have formed by the absorption of six neutrons by a uranium-238 nucleus followed by two β decays. At the time, the absorption of neutrons by a heavy nucleus was thought to be a rare process, but the identification of raised the possibility that still more neutrons could have been absorbed by the uranium nuclei, leading to new elements.\n\nElement 99 (einsteinium) was quickly discovered on filter papers which had been flown through the cloud from the explosion (the same sampling technique that had been used to discover ). It was then identified in December 1952 by Albert Ghiorso and co-workers at the University of California at Berkeley. They discovered the isotope Es (half-life 20.5 days) that was made by the capture of 15 neutrons by uranium-238 nuclei – which then underwent seven successive beta decays:\n\nSome U atoms, however, could capture another amount of neutrons (most likely, 16 or 17).\n\nThe discovery of fermium () required more material, as the yield was expected to be at least an order of magnitude lower than that of element 99, and so contaminated coral from the Enewetak atoll (where the test had taken place) was shipped to the University of California Radiation Laboratory in Berkeley, California, for processing and analysis. About two months after the test, a new component was isolated emitting high-energy α-particles (7.1 MeV) with a half-life of about a day. With such a short half-life, it could only arise from the β decay of an isotope of einsteinium, and so had to be an isotope of the new element 100: it was quickly identified as Fm ().\n\nThe discovery of the new elements, and the new data on neutron capture, was initially kept secret on the orders of the U.S. military until 1955 due to Cold War tensions. Nevertheless, the Berkeley team was able to prepare elements 99 and 100 by civilian means, through the neutron bombardment of plutonium-239, and published this work in 1954 with the disclaimer that it was not the first studies that had been carried out on the elements. The \"Ivy Mike\" studies were declassified and published in 1955.\n\nThe Berkeley team had been worried that another group might discover lighter isotopes of element 100 through ion-bombardment techniques before they could publish their classified research, and this proved to be the case. A group at the Nobel Institute for Physics in Stockholm independently discovered the element, producing an isotope later confirmed to be Fm (\"t\" = 30 minutes) by bombarding a target with oxygen-16 ions, and published their work in May 1954. Nevertheless, the priority of the Berkeley team was generally recognized, and with it the prerogative to name the new element in honour of the recently deceased Enrico Fermi, the developer of the first artificial self-sustained nuclear reactor.\n\nThere are 20 isotopes of fermium listed in N 2016, with atomic weights of 241 to 260, of which Fm is the longest-lived with a half-life of 100.5 days. Fm has a half-life of 3 days, while Fm of 5.3 h, Fm of 25.4 h, Fm of 3.2 h, Fm of 20.1 h, and Fm of 2.6 hours. All the remaining ones have half-lives ranging from 30 minutes to less than a millisecond.\nThe neutron-capture product of fermium-257, Fm, undergoes spontaneous fission with a half-life of just 370(14) microseconds; Fm and Fm are also unstable with respect to spontaneous fission (\"t\" = 1.5(3) s and 4 ms respectively). This means that neutron capture cannot be used to create nuclides with a mass number greater than 257, unless carried out in a nuclear explosion. As Fm is an α-emitter, decaying to Cf, and no known fermium isotopes undergo beta minus decay to the next element, mendelevium, fermium is also the last element that can be prepared by a neutron-capture process. Because of this impediment in forming heavier isotopes, these short-lived isotopes Fm constitute the so-called \"fermium gap.\" \n\nFermium is produced by the bombardment of lighter actinides with neutrons in a nuclear reactor. Fermium-257 is the heaviest isotope that is obtained via neutron capture, and can only be produced in picogram quantities. The major source is the 85 MW High Flux Isotope Reactor (HFIR) at the Oak Ridge National Laboratory in Tennessee, USA, which is dedicated to the production of transcurium (\"Z\" > 96) elements. Lower mass fermium isotopes are available in greater quantities, however, these isotopes (254 and 255) are very short lived. In a \"typical processing campaign\" at Oak Ridge, tens of grams of curium are irradiated to produce decigram quantities of californium, milligram quantities of berkelium and einsteinium and picogram quantities of fermium. However, nanogram quantities of fermium can be prepared for specific experiments. The quantities of fermium produced in 20–200 kiloton thermonuclear explosions is believed to be of the order of milligrams, although it is mixed in with a huge quantity of debris; 4.0 picograms of Fm was recovered from 10 kilograms of debris from the \"Hutch\" test (16 July 1969). The Hutch experiment produced an estimated total of 250 micrograms of Fm.\n\nAfter production, the fermium must be separated from other actinides and from lanthanide fission products. This is usually achieved by ion-exchange chromatography, with the standard process using a cation exchanger such as Dowex 50 or T eluted with a solution of ammonium α-hydroxyisobutyrate. Smaller cations form more stable complexes with the α-hydroxyisobutyrate anion, and so are preferentially eluted from the column. A rapid fractional crystallization method has also been described.\n\nAlthough the most stable isotope of fermium is Fm, with a half-life of 100.5 days, most studies are conducted on Fm (\"t\" = 20.07(7) hours), since this isotope can be easily isolated as required as the decay product of Es (\"t\" = 39.8(12) days).\n\nThe analysis of the debris at the 10-megaton \"Ivy Mike\" nuclear test was a part of long-term project, one of the goals of which was studying the efficiency of production of transuranium elements in high-power nuclear explosions. The motivation for these experiments was as follows: synthesis of such elements from uranium requires multiple neutron capture. The probability of such events increases with the neutron flux, and nuclear explosions are the most powerful neutron sources, providing densities of the order 10 neutrons/cm within a microsecond, i.e. about 10 neutrons/(cm·s). In comparison, the flux of the HFIR reactor is 5 neutrons/(cm·s). A dedicated laboratory was set up right at Enewetak Atoll for preliminary analysis of debris, as some isotopes could have decayed by the time the debris samples reached the U.S. The laboratory was receiving samples for analysis, as soon as possible, from airplanes equipped with paper filters which flew over the atoll after the tests. Whereas it was hoped to discover new chemical elements heavier than fermium, those were not found after a series of megaton explosions conducted between 1954 and 1956 at the atoll.\nThe atmospheric results were supplemented by the underground test data accumulated in the 1960s at the Nevada Test Site, as it was hoped that powerful explosions conducted in confined space might result in improved yields and heavier isotopes. Apart from traditional uranium charges, combinations of uranium with americium and thorium have been tried, as well as a mixed plutonium-neptunium charge. They were less successful in terms of yield that was attributed to stronger losses of heavy isotopes due to enhanced fission rates in heavy-element charges. Isolation of the products was found to be rather problematic, as the explosions were spreading debris through melting and vaporizing rocks under the great depth of 300–600 meters, and drilling to such depth in order to extract the products was both slow and inefficient in terms of collected volumes.\n\nAmong the nine underground tests, which were carried between 1962 and 1969 and codenamed Anacostia (5.2 kilotons, 1962), Kennebec (<5 kilotons, 1963), Par (38 kilotons, 1964), Barbel (<20 kilotons, 1964), Tweed (<20 kilotons, 1965), Cyclamen (13 kilotons, 1966), Kankakee (20-200 kilotons, 1966), Vulcan (25 kilotons, 1966) and Hutch (20-200 kilotons, 1969), the last one was most powerful and had the highest yield of transuranium elements. In the dependence on the atomic mass number, the yield showed a saw-tooth behavior with the lower values for odd isotopes, due to their higher fission rates. The major practical problem of the entire proposal was however collecting the radioactive debris dispersed by the powerful blast. Aircraft filters adsorbed only about 4 of the total amount and collection of tons of corals at Enewetak Atoll increased this fraction by only two orders of magnitude. Extraction of about 500 kilograms of underground rocks 60 days after the Hutch explosion recovered only about 10 of the total charge. The amount of transuranium elements in this 500-kg batch was only 30 times higher than in a 0.4 kg rock picked up 7 days after the test. This observation demonstrated the highly nonlinear dependence of the transuranium elements yield on the amount of retrieved radioactive rock. In order to accelerate sample collection after explosion, shafts were drilled at the site not after but before the test, so that explosion would expel radioactive material from the epicenter, through the shafts, to collecting volumes near the surface. This method was tried in the Anacostia and Kennebec tests and instantly provided hundreds kilograms of material, but with actinide concentration 3 times lower than in samples obtained after drilling; whereas such method could have been efficient in scientific studies of short-lived isotopes, it could not improve the overall collection efficiency of the produced actinides.\n\nAlthough no new elements (apart from einsteinium and fermium) could be detected in the nuclear test debris, and the total yields of transuranium elements were disappointingly low, these tests did provide significantly higher amounts of rare heavy isotopes than previously available in laboratories. So 6 atoms of Fm could be recovered after the Hutch detonation. They were then used in the studies of thermal-neutron induced fission of Fm and in discovery of a new fermium isotope Fm. Also, the rare Cm isotope was synthesized in large quantities, which is very difficult to produce in nuclear reactors from its progenitor Cm – the half-life of Cm (64 minutes) is much too short for months-long reactor irradiations, but is very \"long\" on the explosion timescale.\n\nBecause of the short half-life of all isotopes of fermium, any primordial fermium, that is fermium that could be present on the Earth during its formation, has decayed by now. Synthesis of fermium from naturally occurring actinides uranium and thorium in the Earth crust requires multiple neutron capture, which is an extremely unlikely event. Therefore, most fermium is produced on Earth in scientific laboratories, high-power nuclear reactors, or in nuclear weapons tests, and is present only within a few months from the time of the synthesis. The transuranic elements from americium to fermium did occur naturally in the natural nuclear fission reactor at Oklo, but no longer do so.\n\nThe chemistry of fermium has only been studied in solution using tracer techniques, and no solid compounds have been prepared. Under normal conditions, fermium exists in solution as the Fm ion, which has a hydration number of 16.9 and an acid dissociation constant of 1.6 (p\"K\" = 3.8). Fm forms complexes with a wide variety of organic ligands with hard donor atoms such as oxygen, and these complexes are usually more stable than those of the preceding actinides. It also forms anionic complexes with ligands such as chloride or nitrate and, again, these complexes appear to be more stable than those formed by einsteinium or californium. It is believed that the bonding in the complexes of the later actinides is mostly ionic in character: the Fm ion is expected to be smaller than the preceding An ions because of the higher effective nuclear charge of fermium, and hence fermium would be expected to form shorter and stronger metal–ligand bonds.\n\nFermium(III) can be fairly easily reduced to fermium(II), for example with samarium(II) chloride, with which fermium(II) coprecipitates. The electrode potential has been estimated to be similar to that of the ytterbium(III)/(II) couple, or about −1.15 V with respect to the standard hydrogen electrode, a value which agrees with theoretical calculations. The Fm/Fm couple has an electrode potential of −2.37(10) V based on polarographic measurements.\n\nAlthough few people come in contact with fermium, the International Commission on Radiological Protection has set annual exposure limits for the two most stable isotopes. For fermium-253, the ingestion limit was set at 10 becquerels (1 Bq is equivalent to one decay per second), and the inhalation limit at 10 Bq; for fermium-257, at 10 Bq and 4000 Bq respectively.\n\n\n"}
{"id": "28675152", "url": "https://en.wikipedia.org/wiki?curid=28675152", "title": "Finsterwalde Solar Park", "text": "Finsterwalde Solar Park\n\nThe Finsterwalde Solar Park was, in November 2010, the world’s largest photovoltaic plant with 80.7 MW. The project is located in Finsterwalde, Germany and is equipped with Q-Cells modules and LDK solar wafers.\n\nThe first phase of the project was commissioned in 2009, the second and third in 2010. All phases were developed by Unlimited Energy GmbH. Phase I was sold to LQ Energy GmbH, a joint venture of Q-Cells International and LDK Solar. In 2011, Finsterwalde I was sold to LHI Leasing, a joint venture of Landesbank Baden-Württemberg and Nord/LB.\n\nIn 2010, Unlimited Energy sold phase II and III to Q-Cells International. In 2011, Q-Cells sold Finsterwalde II and III to Blue Forrest Solar Holding, a joint venture of DIF Infrastructure and the NIBC European Infrastructure Fund.\n\n"}
{"id": "46496928", "url": "https://en.wikipedia.org/wiki?curid=46496928", "title": "Fotmal", "text": "Fotmal\n\nThe fotmal (,  \"foot-measure\"; ), also known as the foot ('), formel, fontinel, and fotmell, was an English unit of variable weight particularly used in measuring production, sales, and duties of lead.\n\nUnder the Assize of Weights and Measures, it was equal to 70 Merchants' pounds and made up of a load of lead. Elsewhere, it was made of 70 avoirdupois pounds and made up load. According to Kiernan, in 16th-century Derbyshire, the fotmal was divided into \"boles\" and made up of a fother, meaning it was considered to be 84 avoirdupois pounds.\n\nIt continued to be used until the 16th century.\n"}
{"id": "37250264", "url": "https://en.wikipedia.org/wiki?curid=37250264", "title": "GEN Energija", "text": "GEN Energija\n\nGEN energija, d.o.o. is a state-owned power company in Slovenia. It was established 2001 as Eles Gen, a subsidiary of Elektro-Slovenija, for holding Slovenian shares in the Krško Nuclear Power Plant. In February 2006 the company was separated from Elektro-Slovenija and it became directly owned state-owned company.\n\nGEN Energija has invested in the Krško Nuclear Power Plant, Brestanica thermal power plant, \"Savske elektrarne Ljubljana\" and \"Hidroelektrarne na Spodnji Savi\" hydroplants, and small-scale photovoltaic power plants.\n\n"}
{"id": "32599956", "url": "https://en.wikipedia.org/wiki?curid=32599956", "title": "Galeru Nagari Sujala Sravanthi Project", "text": "Galeru Nagari Sujala Sravanthi Project\n\nGaleru Nagari Sujala Sravanthi Project or GNSS project is an irrigation project in Kadapa and Chitoor districts of Andhra Pradesh.\n\nIt is envisaged to irrigate in two phases besides providing drinking water facilities to villages and towns en route the canal alignment. The main canal, Gandikota, Vamikonda and Sarvarajasagar reservoirs are under construction.\n\nThe scheme envisages drawl of 40 TMC surplus flood waters of river Krishna from the foreshore of Srisailam Reservoir through Srisailam Right Bank Canal (SRBC) system up to Gorakallu Reservoir and thereafter through an independent flood flow canal to feed nine storage reservoirs en route and utilize the stored water for irrigation during the Rabi season.\n\nThe project construction started in 2005, but the work is still in progress. This project doesn't have any assured water allocation and the Rayalaseema region people are strongly demanding the government to allocate assured water to the project.\n"}
{"id": "7096097", "url": "https://en.wikipedia.org/wiki?curid=7096097", "title": "Gas thermometer", "text": "Gas thermometer\n\nA gas thermometer measures temperature by the variation in volume or pressure of a gas. \n\nThis thermometer functions by Charles's Law. Charles's Law states that when the temperature of a gas increases, so does the volume. \n\nUsing Charles's Law, the temperature can be measured by knowing the volume of gas at a certain temperature by using the formula, written below. Translating it to the correct levels of the device that is holding the gas. This works on the same principle as mercury thermometers. \n\nor\n\nT is the temperature.\n\nV is the volume.\n\nk is the constant for the system. k is not a fixed constant across all systems and therefore needs to be found experimentally for a given system through testing with known temperature values. \n\nThe constant volume gas thermometer plays a crucial role in understanding how absolute zero could be discovered long before the advent of cryogenics. Consider a graph of pressure versus temperature made not far from standard conditions (well above absolute zero) for three different samples of any ideal gas \"(a, b, c)\". To the extent that the gas is ideal, the pressure depends linearly on temperature, and the extrapolation to zero pressure occurs at absolute zero. Note that data could have been collected with three different amounts of the same gas, which would have rendered this experiment easy to do in the eighteenth century.\n\n"}
{"id": "14679467", "url": "https://en.wikipedia.org/wiki?curid=14679467", "title": "HSPF", "text": "HSPF\n\nHSPF (Heating Seasonal Performance Factor) is a term used in the heating and cooling industry. HSPF is specifically used to measure the efficiency of air source heat pumps.\n\nThe efficiency of air conditioners is often rated by the HSPF as defined by the Air Conditioning, Heating, and Refrigeration Institute in its standard \"210/240 Performance Rating of Unitary Air-Conditioning and Air-Source Heat Pump Equipment\".<ref name=\"ahri210/240\"></ref>\n\nThe higher the HSPF rating of a unit, the more energy efficient it is. HSPF is a ratio of BTU heat output over the heating season to watt-hours of electricity used. It has units of BTU/watt-hr.\n\nDepending on the system, an HSPF ≥ 8 can be considered high efficiency and worthy of a US Energy Tax Credit.\n\nThe HSPF is related to the non-dimensional Coefficient of Performance (COP) for a heat pump, which measures the ratio of heat energy delivered to electrical energy supplied, independently of the units used to measure energy. The HSPF can be converted to a seasonally-averaged COP by converting both the BTU heat output and the electrical input to a common energy unit (e.g. joules). Since 1 BTU = 1055.056 J, and 1 watt-hour = 3600 J, the seasonally-averaged COP is given by:\n\nAvg COP = Heat transferred / electrical energy supplied = (HSPF * 1055.056 J/BTU) / (3600 J/watt-hour) = 0.29307111 HSPF.\n\nThus, a system which delivers an HSPF of 7.7 will transfer 2.25 times as much heat as electricity consumed over a season. In Europe the term Seasonal Performance Factor (\"SPF\") is used to mean the same as the average COP over the heating season. Thus a system which transfers 2.25 times as much heat as the electricity consumed is said to have an SPF of 2.25. A well designed ground source heat pump installation should achieve an SPF of 3.5, or over 5 if linked to a solar-assisted thermal bank.\n\nExample: For a heat pump delivering 120,000,000 BTU during the season, when consuming 15,000 kWh, the HSPF can be calculated as :\n\n"}
{"id": "39409527", "url": "https://en.wikipedia.org/wiki?curid=39409527", "title": "Hsinta Power Plant", "text": "Hsinta Power Plant\n\nThe Hsinta Power Plant or Hsing-ta Power Plant () is a coal-fired power plant in Yong'an District and Qieding District in Kaohsiung, Taiwan. With a total installed capacity of 4,326 MW, the plant is Taiwan's second largest coal-fired power plant after the 5,500 MW Taichung Power Plant (coal-generated power only).\n\nThe coal yards of the power plant was designed as an indoor type to mitigate local environment impacts. It consists of four coal domes with 170,000 tonnes of storage capacity each, which enables the continuous supply for all of the generation units for 50 days of operation.\n\nThe coal handling system of the power plant consists of stacker, reclaimer and two conveyor belts. The capacity of the stacker is 4,000 tonnes/hour and reclaimer is 2,000 tonnes/hour. The first conveyor belt has a total length of 4.8 km with a capacity of 4,000 tonnes/hour, handling between the coal domes and barge, and the second conveyor belt has a total length of 5.2 km with a capacity of 2,000 tonnes/hour, handling between the coal dome and wharf.\n\nGenerators in four units of the plant tripped at 8:18 a.m following the 2010 Kaohsiung earthquakes.\n\nHsinta Power Plant is accessible West from TRA Luzhu Station.\n\n"}
{"id": "8991205", "url": "https://en.wikipedia.org/wiki?curid=8991205", "title": "Indian Ocean Dipole", "text": "Indian Ocean Dipole\n\nThe Indian Ocean Dipole (IOD), also known as the Indian Niño, is an irregular oscillation of sea-surface temperatures in which the western Indian Ocean becomes alternately warmer and then colder than the eastern part of the ocean.\n\nMonsoon in India is generally affected by the temperature between bay of Bengal in the east and The Arabian sea in the west.\n\nThe IOD involves an aperiodic oscillation of sea-surface temperatures (SST), between \"positive\", \"neutral\" and \"negative\" phases. A positive phase sees greater-than-average sea-surface temperatures and greater precipitation in the western Indian Ocean region, with a corresponding cooling of waters in the eastern Indian Ocean—which tends to cause droughts in adjacent land areas of Indonesia and Australia. The negative phase of the IOD brings about the opposite conditions, with warmer water and greater precipitation in the eastern Indian Ocean, and cooler and drier conditions in the west.\n\nThe IOD also affects the strength of monsoons over the Indian subcontinent. A significant positive IOD occurred in 1997–98, with another in 2006. The IOD is one aspect of the general cycle of global climate, interacting with similar phenomena like the El Niño-Southern Oscillation (ENSO) in the Pacific Ocean.\n\nThe IOD phenomenon was first identified by climate researchers in 1999. \n\nAn average of four each positive-negative IOD events occur during each 30-year period with each event lasting around six months. However, there have been 12 positive IODs since 1980 and no negative events from 1992 until a strong negative event in late 2010. The occurrence of consecutive positive IOD events is extremely rare with only two such events recorded, 1913–1914 and the three consecutive events from 2006 to 2008 which preceded the Black Saturday bushfires. Modelling suggests that consecutive positive events could be expected to occur twice over a 1,000-year period. The positive IOD in 2007 evolved together with La Niña, which is a very rare phenomenon that has happened only once in the available historical records (in 1967). A strong negative IOD developed in October 2010, which, coupled with a strong and concurrent La Niña, caused the 2010–2011 Queensland floods and the 2011 Victorian floods.\n\nIn 2008, Nerilie Abram used coral records from the eastern and western Indian Ocean to construct a coral Dipole Mode Index extending back to 1846 AD. This extended perspective on IOD behaviour suggested that positive IOD events increased in strength and frequency during the 20th century.\n\nA 2009 study by Ummenhofer \"et al.\" at the University of New South Wales (UNSW) Climate Change Research Centre has demonstrated a significant correlation between the IOD and drought in the southern half of Australia, in particular the south-east. Every major southern drought since 1889 has coincided with positive-neutral IOD fluctuations including the 1895–1902, 1937–1945 and the 1995–2009 droughts.\n\nThe research shows that when the IOD is in its negative phase, with cool Indian Ocean water west of Australia and warm Timor Sea water to the north, winds are generated that pick up moisture from the ocean and then sweep down towards southern Australia to deliver higher rainfall. In the IOD-positive phase, the pattern of ocean temperatures is reversed, weakening the winds and reducing the amount of moisture picked up and transported across Australia. The consequence is that rainfall in the south-east is well below average during periods of a positive IOD.\n\nThe study also shows that the IOD has a much more significant effect on the rainfall patterns in south-east Australia than the El Niño-Southern Oscillation (ENSO) in the Pacific Ocean as already shown in several recent studies.\n\nA 2018 study by Hameed et al. at the University of Aizu simulated the impact of a positive IOD event on Pacific surface wind and SST variations. They show that IOD induced surface wind anomalies can produce El Nino like SST anomalies, with the IOD's impact on SST being the strongest in the far-eastern Pacific. They further demonstrated that IOD-ENSO interaction is a key for the generation of Super El Ninos.\n\n\n\n"}
{"id": "10962898", "url": "https://en.wikipedia.org/wiki?curid=10962898", "title": "Indium(III) selenide", "text": "Indium(III) selenide\n\nIndium(III) selenide is a compound of indium and selenium. It has potential for use in photovoltaic devices and it has been the subject of extensive research. The two most common phases, α and β, have a layered structure, while γ is a \"defect wurtzite structure.\" In all, there are five known forms (α, β, γ, δ, κ). The α- β phase transition is accompanied by a change in electrical conductivity. The band-gap of γ-InSe is approximately 1.9 eV.\nThe crystalline form of a sample can depend on the method of production, for example thin films of pure γ-InSe have been produced from trimethylindium, InMe, and hydrogen selenide, HSe, using MOCVD techniques.\n\n\n"}
{"id": "548173", "url": "https://en.wikipedia.org/wiki?curid=548173", "title": "Instability", "text": "Instability\n\nIn numerous fields of study, the component of instability within a system is generally characterized by some of the outputs or internal states growing without bounds. Not all systems that are not stable are unstable; systems can also be marginally stable or exhibit limit cycle behavior.\n\nIn structural engineering, a structure can become unstable when excessive load is applied. Beyond a certain threshold, structural deflections magnify stresses, which in turn increases deflections. This can take the form of buckling or crippling. The general field of study is called structural stability.\n\nAtmospheric instability is a major component of all weather systems on Earth.\n\nIn the theory of dynamical systems, a state variable in a system is said to be unstable if it evolves without bounds. A system itself is said to be unstable if at least one of its state variables is unstable.\n\nIn continuous time control theory, a system is unstable if any of the roots of its characteristic equation has real part greater than zero (or if zero is a repeated root). This is equivalent to any of the eigenvalues of the state matrix having either real part greater than zero, or, for the eigenvalues on the imaginary axis, the algebraic multiplicity being larger than the geometric multiplicity. The equivalent condition in discrete time is that at least one of the eigenvalues is greater than 1 in absolute value, or that two or more eigenvalues are equal and of unit absolute value.\n\n\nFluid instabilities occur in liquids, gases and plasmas, and are often characterized by the shape that form; they are studied in fluid dynamics and magnetohydrodynamics. Fluid instabilities include:\n\n\nPlasma instabilities can be divided into two general groups (1) hydrodynamic instabilities (2) kinetic instabilities. Plasma instabilities are also categorised into different modes – see this paragraph in plasma stability.\n\nGalaxies and star clusters can be unstable, if small perturbations in the gravitational potential cause changes in the density that reinforce the original perturbation. Such instabilities usually require that the motions of stars be highly correlated, so that the perturbation is not \"smeared out\" by random motions. After the instability has run its course, the system is typically \"hotter\" (the motions are more random) or rounder than before. Instabilities in stellar systems include:\n\n\nThe most common residual disability after any sprain in the body is instability. Mechanical instability includes insufficient stabilizing structures and mobility that exceed the physiological limits. Functional instability involves recurrent sprains or a feeling of giving way of the injured joint. Injuries cause proprioceptive deficits and impaired postural control in the joint. Individuals with muscular weakness, occult instability, and decreased postural control are more susceptible to injury than those with better postural control. Instability leads to an increase in postural sway, the measurement of the time and distance a subject spends away from an ideal center of pressure. The measurement of a subject’s postural sway can be calculated through testing center of pressure (CoP), which is defined as the vertical projection of center of mass on the ground. Investigators have theorized that if injuries to joints cause deafferentation, the interruption of sensory nerve fibers, and functional instability, then a subject’s postural sway should be altered. Joint stability can be enhanced by the use of an external support system, like a brace, to alter body mechanics. The mechanical support provided by a brace provides cutaneous afferent feedback in maintaining postural control and increasing stability.\n\n"}
{"id": "1738289", "url": "https://en.wikipedia.org/wiki?curid=1738289", "title": "Intarsia", "text": "Intarsia\n\nIntarsia is a form of wood inlaying that is similar to marquetry. The start of the practice dates before the seventh century.\n\nThe technique of intarsia inlays sections of wood (at times with contrasting ivory or bone, or mother-of-pearl) within the solid stone matrix of floors and walls or of table tops and other furniture; by contrast marquetry assembles a pattern out of veneers glued upon the carcass. It is thought that the word 'intarsia' is derived from the Latin word 'interserere' which means \"to insert\".\n\nWhen Egypt came under Arab rule in the seventh century, indigenous arts of intarsia and wood inlay, which lent themselves to non-representational decors and tiling patterns, spread throughout the maghreb. The technique of intarsia was already perfected in Islamic North Africa before it was introduced into Christian Europe through Sicily and Andalusia. The art was further developed in Siena and by Sienese masters at the cathedral of Orvieto, where figurative intarsia made their first appearance, c. 1330 and continuing into the 15th century and in northern Italy in the fifteenth and sixteenth centuries, spreading to German centers and introduced into London by Flemish craftsmen in the later sixteenth century. The most elaborate examples of intarsia can be found in cabinets of this period, which were items of great luxury and prestige. After about 1620, marquetry tended to supplant intarsia in urbane cabinet work.\n\nIn the 1980s, intarsia began to gain popularity in the United States as a technique for creating wooden art using a band saw or scroll saw. \nEarly practitioners made money both by selling their art, and also selling patterns used to create intarsia. In France proposed a new method which revolutionise the marquetry. Contrary to all the other techniques, based on the generally accepted idea of a decoration \"flat\" made of wood or other matters, George VRIZ brings an important innovation: Thanks to the superposition of the layers of wood, and with the possibility offered by plating to create \"transparencies\", these means make it possible to bring thus sometimes the light, the color, a veil, a depth. These made impossible to create with a traditional method are made using judicious but controlled sandpaperings.\n\nIntarsia is a woodworking technique that uses varied shapes, sizes, and species of wood fitted together to create a mosaic-like picture with an illusion of depth. Intarsia is created through the selection of different types of wood, using their natural grain pattern and color (but can involve the use of stains and dyes but is (not really considered intarsia)) to create variations in the pattern. After selecting the specific woods to be used within the pattern, each piece is then individually cut, shaped, and finished. Sometimes areas of the pattern are raised to create more depth. Once the individual pieces are complete, they are fitted together like a jig-saw puzzle and glued to a wooden backer-board, cut to the outline of the pattern, often with the intention of creating a three-dimensional effect as seen in the studiolo of the Palazzo Ducale, Urbino.\n\nMarble intarsia (\"opere di commessi\"), called \"pietre dura\" in English for the semi-precious hardstones combined with colored marbles that are employed, is an intarsia of coloured stones inlaid in white or black marble. Early examples in Florence date from the mid fifteenth century and reached a peak of refinement and complexity in revetments of the Medici Chapel, produced under Medici patronage in the \"Opificio delle Pietre Dure\", which was established by Ferdinando I de’ Medici. Later complex designs and refinement of the art developed in Naples circa the beginning of the 17th century. The floor of St. Peter's Basilica in Rome is a particularly notable example of marble intarsia. Later this form of decoration became a feature of baroque interior design, particularly so in the Sicilian Baroque designs following the earthquake of 1693.\n\nToday intarsia can be made from purchased patterns. To make intarsia from a pattern, first wood is chosen based on color and grain pattern. Next the pattern is transferred onto the wood and individual pieces are precisely cut out on the band saw or scroll saw. The pieces are then sanded individually or in groups to add depth to the piece. Once the sanding is completed, the wood pieces are fitted together to form the final result. A finish (for example a clear gel stain) can be applied to the individual pieces before gluing, or to the glued final version.\n\nIntarsia is also used to refer to a similar technique used with small, highly polished stones set in a marble matrix also called pietre dure.\n\n\n"}
{"id": "901162", "url": "https://en.wikipedia.org/wiki?curid=901162", "title": "Interactive evolutionary computation", "text": "Interactive evolutionary computation\n\nInteractive evolutionary computation (IEC) or aesthetic selection is a general term for methods of evolutionary computation that use human evaluation. Usually human evaluation is necessary when the form of fitness function is not known (for example, visual appeal or attractiveness; as in Dawkins, 1986) or the result of optimization should fit a particular user preference (for example, taste of coffee or color set of the user interface).\n\nThe number of evaluations that IEC can receive from one human user is limited by user fatigue which was reported by many researchers as a major problem. In addition, human evaluations are slow and expensive as compared to fitness function computation. Hence, one-user IEC methods should be designed to converge using a small number of evaluations, which necessarily implies very small populations. Several methods were proposed by researchers to speed up convergence, like interactive constrain evolutionary search (user intervention) or fitting user preferences using a convex function. IEC human-computer interfaces should be carefully designed in order to reduce user fatigue. There is also evidence that the addition of computational agents can successfully counteract user fatigue.\n\nHowever IEC implementations that can concurrently accept evaluations from many users overcome the limitations described above. An example of this approach is an interactive media installation by Karl Sims that allows one to accept preferences from many visitors by using floor sensors to evolve attractive 3D animated forms. Some of these multi-user IEC implementations serve as collaboration tools, for example HBGA.\n\nIEC methods include interactive evolution strategy, interactive genetic algorithm, interactive genetic programming, and human-based genetic algorithm.,\n\nAn interactive genetic algorithm (IGA) is defined as a genetic algorithm that uses human evaluation. These algorithms belong to a more general category of Interactive evolutionary computation. The main application of these techniques include domains where it is hard or impossible to design a computational fitness function, for example, evolving images, music, various artistic designs and forms to fit a user's aesthetic preferences. Interactive computation methods can use different representations, both linear (as in traditional genetic algorithms) and tree-like ones (as in genetic programming).\n\n\n\n"}
{"id": "14940135", "url": "https://en.wikipedia.org/wiki?curid=14940135", "title": "Japan Crude Cocktail", "text": "Japan Crude Cocktail\n\nThe Japan Customs-cleared Crude (JCC) is the average price of customs-cleared crude oil imports into Japan (formerly the average of the top twenty crude oils by volume) as reported in customs statistics; nicknamed the \"Japanese Crude Cocktail\". \n\nIt is a commonly used index in long term LNG contracts in Japan, Korea and Taiwan, and replaced the Government Selling Price of crude oil as the standard index. \n\nThe data to calculate JCC is published by the Japanese government every month. This is the raw and crude oil import prices in yen per kilolitre, the dollar yen exchange rate and the total Japanese imports of all commodities for the month. \n\nJCC prices are available from the Petroleum Association of Japan.\n"}
{"id": "262116", "url": "https://en.wikipedia.org/wiki?curid=262116", "title": "Jojoba", "text": "Jojoba\n\nJojoba , with the botanical name Simmondsia chinensis, and also known as goat nut, deer nut, pignut, wild hazel, quinine nut, coffeeberry, and gray box bush, is native to Southwestern North America. \"Simmondsia chinensis\" is the sole species of the family Simmondsiaceae, placed in the order Caryophyllales.\n\nJojoba is grown commercially to produce jojoba oil, a liquid wax ester extracted from its seed.\n\nThe plant is a native shrub of the Sonoran Desert, Colorado Desert, Baja California Desert, and California chaparral and woodlands habitats in the Peninsular Ranges and San Jacinto Mountains. It is found in southern California, Arizona, and Utah (U.S.), and Baja California state (Mexico).\n\nJojoba is endemic to North America, and occupies an area of approximately between latitudes 25° and 31° North and between longitudes 109° and 117° West.\n\n\"Simmondsia chinensis\", or jojoba, typically grows to tall, with a broad, dense crown, but there have been reports of plants as tall as .\n\nThe leaves are opposite, oval in shape, long and broad, thick, waxy, and glaucous gray-green in color.\n\nThe flowers are small and greenish-yellow, with 5–6 sepals and no petals. The plant typically blooms from March to May.\n\nEach plant is dioecious, with hermaphrodites being extremely rare. The fruit is an acorn-shaped ovoid, three-angled capsule long, partly enclosed at the base by the sepals. The mature seed is a hard oval that is dark brown and contains an oil (liquid wax) content of approximately 54%. An average-sized bush produces of pollen, to which few humans are allergic.\n\nThe female plants produce seed from flowers pollinated by the male plants. Jojoba leaves have an aerodynamic shape, creating a spiral effect, which brings wind-born pollen from the male flower to the female flower. In the Northern Hemisphere, pollination occurs during February and March. In the Southern Hemisphere, pollination occurs during August and September.\n\nSomatic cells of jojoba are tetraploid; the number of chromosomes is 2\"n\" = 4\"x\" = 52.\n\nDespite its scientific name \"Simmondsia chinensis\", the plant is not native to China. The botanist Johann Link originally named the species \"Buxus chinensis\", after misreading a collection label \"Calif\", referring to California, as \"China.\" Jojoba was collected again in 1836 by Thomas Nuttall who described it as a new genus and species in 1844, naming it \"Simmondsia californica\", but priority rules require that the original specific epithet be used.\n\nThe common name \"jojoba\" originated from O'odham name \"Hohowi\". The common name should not be confused with the similarly written jujube (\"Ziziphus zizyphus\"), an unrelated plant species, which is commonly grown in China.\n\nJojoba foliage provides year-round food for many animals, including deer, javelina, bighorn sheep, and livestock. Its nuts are eaten by squirrels, rabbits, other rodents, and larger birds.\n\nOnly Bailey's pocket mouse, however, is known to be able to digest the wax found inside the jojoba nut. In large quantities, jojoba seed meal is toxic to many mammals, later this effect was found to be due to simmondsin, which inhibits hunger. The indigestible wax acts as a laxative in humans.\n\nNative Americans first made use of jojoba. During the early 18th century Jesuit missionaries on the Baja California Peninsula observed indigenous peoples heating jojoba seeds to soften them. They then used a mortar and pestle to create a salve or buttery substance. The latter was applied to the skin and hair to heal and condition. The O'odham people of the Sonoran Desert treated burns with an antioxidant salve made from a paste of the jojoba nut.\n\nNative Americans also used the salve to soften and preserve animal hides. Pregnant women ate jojoba seeds, believing they assisted during childbirth. Hunters and raiders ate jojoba on the trail to keep hunger at bay.\n\nThe Seri, who utilize nearly every edible plant in their domain, do not regard the beans as real food and in the past ate it only in emergencies.\n\nJojoba is grown for the liquid wax, commonly called jojoba oil, in its seeds. This oil is rare in that it is an extremely long (C36–C46) straight-chain wax ester and not a triglyceride, making jojoba and its derivative jojoba esters more similar to whale oil than to traditional vegetable oils. It has been discussed as a possible biodiesel fuel. Jojoba cannot be cultivated on a scale to compete with traditional fossil fuels, and its use is regulated to personal care products.\n\nPlantations of jojoba have been established in a number of desert and semi-desert areas, predominantly in Argentina, Australia, Israel, Mexico, Peru and the United States. It is currently the Sonoran Desert's second most economically valuable native plant (overshadowed only by \"Washingtonia filifera\"—California fan palms, used as ornamental trees).\n\nJojoba prefers light, coarsly textured soils. Good drainage and water penetration is necessary. It tolerates salinity and nutrient-poor soils. Soil pH should be between 5 and 8. High temperatures are tolerated by jojoba, but frost can damage or kill plants. Requirements are poor because jojoba plants do not need an intensive cultivation. Weed problems only occur during the first two years after planting and there is little damage by insects. Supplemental irrigation could maximize production if rainfall as less than 400 mm. There is no need for high fertilisation, but, especially in the first year, nitrogen increases growth. Jojoba is normally harvested by hand because seeds do not all mature in the same time. Yield is around 3.5 t/ha depending on the age of the plantation.\n\nSelective breeding is developing plants that produce more beans with higher wax content, as well as other characteristics that will facilitate harvesting.\n\nIts ability to withstand high salinity (up to 12 dS m at pH 9) and the high value of jojoba products make jojoba an interesting plant for the use of desertification control. It has been used to combat and prevent desertification in the Thar Desert in India.\n\n"}
{"id": "15399989", "url": "https://en.wikipedia.org/wiki?curid=15399989", "title": "Load-loss factor", "text": "Load-loss factor\n\nLoad-loss factor (LLF) is a factor which when multiplied by energy lost at time of peak and the number of load periods will give overall average energy lost.\n\nIt is calculated as the ratio of the average load loss to the peak load loss.\n\nFor electricity utilities, expect about 0.03.\nIn the days when computers were uncommon, estimates were derived from the Load Factor using an empirical formula.\n\n"}
{"id": "894238", "url": "https://en.wikipedia.org/wiki?curid=894238", "title": "Mercury Sable", "text": "Mercury Sable\n\nThe Mercury Sable is a range of automobiles that were manufactured and marketed by the Mercury brand of Ford Motor Company. Introduced in 1986 as the replacement for the Mercury Marquis, the Sable marked the transition of the mid-size Mercury product range to front-wheel drive. From 1986 to 2005, the Sable was produced as a mid-size four-door sedan and station wagon, serving as the Mercury counterpart of the Ford Taurus (no Sable version of the Taurus SHO was produced). For 2006, the Sable was withdrawn, replaced by the smaller Mercury Milan and larger Mercury Montego. For 2008, alongside the return of the Ford Taurus, the Sable nameplate was revived; as a mid-cycle revision of the Montego, the Sable became a full-size sedan slotted below the Grand Marquis. \n\nDue to low sales, Mercury discontinued the Sable after 2009, with the final vehicle produced on May 21, 2009. In total, 2,112,374 Sables were produced during its 1986-2005 production. As of the 2018 model year, the Ford Taurus/SHO remains in production in North America, though the 2010 closure of Mercury left the Sable without a direct successor. \n\nThe Mercury Sable derives its name from the sable, a weasel-like mammal from Russia that is valued for its smooth, dark fur.\n\n \nThe Sable was a very important sedan for both Mercury and the American auto industry. Ford had lagged in introducing mid-size front wheel drive cars to compete against General Motors' Chevrolet Citation and its best-selling Chevrolet Celebrity/Pontiac 6000/Oldsmobile Cutlass/Buick Century quartet as well as Chrysler's well-received K cars and Japanese offerings from Honda, Datsun/Nissan and Toyota. The Mercury brand suffered even more from this delay. In 1983, Ford launched the redesigned Mercury Cougar to start a reinvigoration of the Mercury brand with new aerodynamic designs, and started development of the Sable. Because of this design, the Sable was a resounding success and launched Mercury into a new design era, as well as influencing the other American automakers to follow suit and create more aerodynamic cars, thus ending the \"boxy\" cars of the 1970s and 1980s.\n\nThe Taurus and Sable siblings used flush aerodynamic composite headlights. Ford was the first to produce and sell vehicles with such headlights in the U.S., when it introduced the Lincoln Mark VII in 1984. To do so, Ford (among other automakers) had to lobby the National Highway Traffic Safety Administration (NHTSA) to have them approved. The Taurus and Sable were the first domestically-produced, mainstream sedans to use the new lights. They also went beyond the Audi 5000, with which they were often compared, to adopt a grille-less \"bottom breather\" nose, first pioneered by the Citroën DS in the 1950s, and also used briefly on the Mustang.\n\nThe Sable was unveiled along with the Taurus in a resounding fashion. For its aerodynamic shape, the launch was held in MGM Studios Soundstage 85, where \"Gone with the Wind\" was filmed. Ford workers came into the room, which was decorated in space-age decor, holding cups shaped like flying saucers and the Taurus and Sable were sitting behind a curtain, their outlines silhouetting. Then, with the flashing of strobe lights and a drum-roll, the curtain was pulled back and the two cars were revealed to the public.\n\nThe bodyshell was smooth and aerodynamic. The Mercury Sable was given its own front fascia; in place of a grille, the Sable was styled with a low-wattage \"lightbar\" between the headlights and fitted with clear turn signals, effectively wrapping around most of the distance from wheel to wheel. The design was also adopted in the Mercury Topaz and Tracer, with other automakers adopting variations of the styling into the early 1990s.\n\nAircraft-style doors were used to reduce wind noise, and the handles were recessed. The Sable also had large glass areas with slim pillars, and were flush with the body. The rear glass wrapped fully around, and the B-pillars were painted black to give the illusion that the front and rear glass were connected. The interior was available with bucket seats — very rare for most U.S. midsize sedans — and the dashboard wrapped around the driver and fed into the door panels to create more of a \"cockpit\" feel.\n\nThe Sable was first introduced as a 1986 model in December 1985, to strong sales and fanfare. It came in two models, base GS and high-end LS. Initial Sable sales were strong, and the Sable sold around 300,000 units its first year.\n\nFor the first year on the market, Sable buyers had the choice of a 90 hp HSC 4-cylinder mated to a three-speed automatic transaxle or a 140 hp Vulcan V6 with a four-speed automatic, with the latter having much higher sales. 4-cylinder Sable sales were so poor that the engine was dropped in 1987 (it remained an option for the Taurus until 1991). Ford's 3.8 L \"Essex\" V6 was added to the line-up in 1988. Although the power output was rated at the same 140 hp (104 kW) as the 3.0 L engine, this large V6 produced 215 ft·lbf (291 N·m) of torque, a welcome addition, especially in the heavier station wagons. However, the 3.8 suffered from premature head gasket failure, which was primarily a fault with Ford's supplier of gaskets, not with the engine itself. Some also attribute this to reduced under-hood cooling. Unlike the Taurus, no manual transmission was offered in the Sable.\n\nThe Sable had just received small changes over the years, mostly in terms of equipment and cosmetics. In 1991, sales dipped to just over 100,000 units, so a new generation of Sable was launched.\n\nThis generation of the Sable wasn't sold in Mexico. It was sold with Mercury badges as the Ford Taurus up until 1995, especially with the second generation.\n\nThe Sable was on Car and Driver magazine's Ten Best list on its release in 1986 and again in 1990 and 1991.\n\nThe Sable received its first significant cosmetic update in 1992, which modernized the interior and the front and rear fascias. The operation cost Ford $650 million at the time. With the older model facing slumping sales, this new model brought sales back up again, with 410,000 examples sold during 1992, a number unheard of even today. While the design was basically the same, every body panel on the sedan except for the doors was changed.; on the station wagon all the sheet metal to the rear of the cowl was the same as that of the 1986-1995 Ford Taurus wagon. The interior was also redesigned, and included an optional passenger-side airbag, a first in its class. The Taurus, sister car of the Sable, was the best-selling car in the United States for every year of this cosmetic update.\n\nThe base \"GS\" and luxury \"LS\" trim levels were carried over from the previous generation. A front cloth bench seat was standard on GS sedans and wagons, although cloth bucket seats were available on GS sedans only. Higher-end cloth bucket seats were standard on LS sedans, but a bench seat was a no cost option. A front bench was standard on LS wagons, with bucket seats optional. Leather seating surfaces were available on all LS Sables.\n\nIn 1993, unpopular optional features such as the \"InstaClear\" heated windshield were eliminated. For 3.0 L V6 engines, the drive belt system became a single-belt setup for 1993 (previously, the 3.0 L alternator had used a separate belt). A passenger-side airbag became standard for 1993, and a redesigned drivers side airbag and steering wheel came in 1994. Also in 1994, some 3.0 L models began receiving the new AX4N transmission.\n\nThe wagon version was available with mostly the same options as the sedan versions. Wagons had a maximum of 81.1 cubic feet of cargo area with the 60/40 split rear seat folded down. They featured a 2-way liftgate (raise the entire liftgate or just the window), a roof rack with crossbar and tie-downs, an optional rear-facing third seat, a lockable under-floor compartment, and an optional fold-out picnic table. With both rear split seats in the upright position, standard cargo capacity was 45.7 cubic feet. Wagons that were equipped with the front bench seat and rear folding seat could seat eight people.\n\nThe last year of this updated Sable generation was 1995. For the 1995 model year, the rare LTS trim level was added. It featured leather bucket seats, Taurus LX-style alloy wheels, special cladding, and many leather wrapped interior trim parts. The LTS trim had either the standard 3.0 L Vulcan V6 or the optional 3.8 L Essex V6.\n\nThe 1996 model year saw the first complete redesign for the Sable. Ford hoped the radical redesign would lead to the same success it had had with the 1986 Sable. The controversial oval theme was not well received by the press and the public, and is ultimately blamed as the reason for a substantial dip in sales. For this generation, the Sable tried to move slightly upmarket, and as a result, prices rose considerably, also driving away potential buyers. The 1996 Sable was the first model to share sheetmetal with the Taurus. Differences from the Taurus included different front and rear fascias, and the elimination of the rear quarter window. Although the Sable used a less oval based styling, sales still fell.\n\nThe 1996 model could be equipped with the brand new 200 hp (149 kW) 3.0 L DOHC \"Duratec 30\" V6 as an option. Trim lines stayed the same, with GS as the entry level model and LS as the most luxurious model. The LTS was eliminated. Although all 1998 models had the option of the DOHC Duratec engine, it was only available on the LS for 1999. That same year front bucket seats became optional on the GS. Mercury claimed that the 1999 Duratec had less power than the 2000 Duratec in hopes to increase sales of the 2000 Sable.\nIn an effort to reverse the declining sales of the Sable, Mercury did major cost cutting for the 1997 model. They carried this further for 1998, by giving it a front end facelift, and cutting the price up to $2,000 in 1999. Mercury also continued to cut costs, eliminating some options for 1999.\n\nThe Sable received another redesign in 2000, which minimized some of the oval design elements from the 1996 model, replacing them with more conventional styling. The redesign also featured a taller roof over the rear-passenger space, to increase passenger headroom that had been sacrificed by the tapered 1996 design. The taller and roomier trunk also served to make the vehicle more functional. The interior was completely changed for a much more conservative design. Certain elements of the interior were retained from the 1996 model, such as the integrated control console, which combined the sound system and climate controls into one panel; but the shape of that panel was changed from the controversial oval to a more conventional and conservative trapezoid. The suspension was also softened to appeal to a broader, non-sporting audience. To reduce the price and increase profitability, many features such as four-wheel disc brakes were eliminated on the sedan; station wagons retained four-wheel disc brakes.\n\nThe 2002 Sable included extra equipment on every trim level, including a CD player and power driver's seat on the GS, and a power moonroof or leather interior on the LS. Side airbags and traction control were added as options on all models. For 2004, the Sable received minor cosmetic changes to the front and rear fascias, most noticeably the grille was made fully chrome. Inside were a new instrument cluster and steering wheel.\n\nDue to the Mercury brand's discontinuation in Canada, the fourth generation Sable was never available in the Canadian market. Thus it was unique to the US and Mexico.\n\nThe 2005 Mercury Montego and 2006 Milan were launched as replacements for the Sable. Shortly after the Montego's introduction the Sable was discontinued, along with the Taurus wagon; the Taurus sedan continued to be produced, but primarily for the fleet market. The last Sable left the Atlanta plant on April 29, 2005.\n\nAt the Chicago Auto Show on February 7, 2007, Ford CEO Alan Mulally unveiled a refreshed version of the Mercury Montego sedan. In response to dealer demand and in a move towards better nameplate recognition, the Montego name was dropped in favor of a revived Sable.\n\nEntering sale on July 2007 as a 2008 model, the revived Sable was joined by a revived Ford Taurus (replacing the Ford Five Hundred) and Ford Taurus X (replacing the Ford Freestyle, effectively replacing the Taurus/Sable station wagons). Although externally smaller than the Grand Marquis, the fifth-generation Sable was the first version produced as a full-size sedan.\n\nIn the transition to the Montego to the Sable, a number of changes were made to the body and chassis. Alongside the addition of the 263 hp 3.5L V6 and 6-speed 6F automatic, the exterior underwent several revisions, including a new front bumper and grille, redesigned headlamps; the LED taillamps were retained, but given white lenses.\n\nA few rare special editions of the Sable were made, all consisting of first generation models.\n\nIn 1987, Mercury created a special edition of the Sable called the \"LS Monochrome Edition\", which as an option would color the bumpers, side trim, and wheels white. It was only offered in 1987; the production quantity is not known and it is also unknown how many still exist.\n\nIn 1989, Mercury created a \"50th Anniversary\" edition of the Sable, to celebrate Mercury's 50th Anniversary. Keeping with the name, only 50 were sold, combined between GS and LS models. This Sable was actually a test bed for creating a Luxury sports version of the Sable called the LTS, similar to that of the Ford Taurus SHO. It was meant to use the SHO's chassis, interior, and suspension, but not the engine. After the launch of the SHO, and all the publicity and praise it got, Ford shelved the Sable LTS to focus on the SHO, and because they were afraid it would take sales away from the SHO. The Sable LTS remained in a \"development hell\" until mid-1994 when it was introduced as a high end version of the Sable, but by then, it was just a highly optioned LS. An unknown number of these Sables still exist, but a pristine condition GS in this trim was sold on eBay in 2007.\n\nA special one-of-a-kind Sable convertible was created in 1988 for the Detroit SAE auto show. It was built from a sedan chassis and featured a completely custom two-door body with a custom folding top. However, it was shelved; the only one sat in a warehouse for years until it was given a VIN, titled, and driven. It was sold on eBay in 2006.\n\nIn an article in Automotive News (circa 1990) an all aluminum \"body in white\" was made for a Sable. In an accompanying photo it is shown being held up by two middle aged women, leading to the belief it would weigh less than 600 lbs. At the time Audi had just released the A8, so it might have been an engineering exercise for constructing all aluminum frames, as Jaguar has now. Whatever became of the \"Aluminum Sable Unibody\", or if there was more than one, is unknown.\n\nIn the movie \"Coneheads\", the main character, Beldar, drives a 1992 Sable GS. The car is equipped with a removable sunroof to accommodate his \"cone.\" At the end of the movie, the car is taken to planet Remulak, and the owner's guide given as a gift to the Conehead Highmaster. The car is described as \"a personal conveyance named after its inventor, an assassinated ruler, a character from Greco-Roman myth and a small furry mammal.\" (Henry Ford, Abraham Lincoln, Mercury, and Sable, respectively).\n\n\n"}
{"id": "47122678", "url": "https://en.wikipedia.org/wiki?curid=47122678", "title": "National Clean Energy Business Plan Competition", "text": "National Clean Energy Business Plan Competition\n\nThe National Clean Energy Business Plan Competition (NCEBPC) is a U.S. Department of Energy (DOE) program in six regions in the United States in Building Technologies, Advanced Manufacturing, Vehicle Technologies, Federal Energy Management Program, Weatherization and Intergovernmental, Biomass Program, Geothermal Technologies, Fuel Cells Technologies, Solar Energy Technologies and Wind and Hydropower Technologies, as recognized by the Office of Energy Efficiency and Renewable Energy.\n\nIt was launched in 2011, with Obama Administration's Startup America Initiative, a White House campaign to inspire and promote entrepreneurship. Each regional winner receives DOE prize money and a chance to compete for a National Grand Prize at a competition held in Washington, D.C..\n\nIn the competition, student ventures the top U.S. universities form a complete business plan that commercializes a technology developed at their school or one of the National Labs. Venture teams are rank the plans on their clean energy impact, solution creativity, execution and financial strategy, market and customer knowledge, and team composition, chemistry and commitment.\n\n\n"}
{"id": "26366290", "url": "https://en.wikipedia.org/wiki?curid=26366290", "title": "November 2009 nor'easter", "text": "November 2009 nor'easter\n\nThe November 2009 nor'easter (also referred to as \"Nor'Ida\") was a powerful autumn nor'easter that caused widespread damage throughout the east coast of the United States. This extratropical cyclone formed in relation to Hurricane Ida's mid-level circulation across southeastern Georgia and moved east-northeast offshore North Carolina before slowly dropping south and southeast over the succeeding several days.\n\nThe origins of the nor'easter originated from with the remnants of Hurricane Ida, a storm that formed on November 4 over the southern Caribbean Sea. After tracking through Nicaragua as a Category 1 hurricane, the system attained Category 2 status over the Yucatán Channel. Once in the Gulf of Mexico, the combination of increasing wind shear and cooler waters caused Ida to weaken. The system eventually moved over the southeastern United States on November 10 before transitioning into an extratropical cyclone. Ida eventually dissipated over the Florida Panhandle. However, Ida's mid-level circulation led to the formation of a new low over southeastern Georgia, which eventually moved off the coast of North Carolina. This new low quickly intensified and became a powerful nor'easter that caused substantial damage throughout the Mid-Atlantic States. Due to the rapid succession of these systems, United States media referred to the nor'easter as \"Nor'Ida\". By November 12, the system attained a minimum pressure of 992 mbar (hPa; 29.29 inHg) along with winds of 65 mph (100 km/h). In combination with a large area of high pressure, a long stretch of easterly, onshore winds impacted areas from Virginia to southern New England. Tracking parallel to the North Carolina coastline, the system eventually moved onshore near Cape Hatteras by November 13. Due to the high pressure system situated over Vermont, the low turned southeastward, bringing its center back over water. Gradual weakening took place during this period, though heavy rains continued to fall across much of the Chesapeake Bay area. On November 14, a brief secondary low developed off the coast of Delaware. Continuing to weaken, the cyclone resumed a northward track after the high weakened and persisted through November 17, by which time it had moved over Atlantic Canada.\n\nAs the remnants of Ida began to weaken within the developing nor'easter on November 11, flood warnings were already in force from Alabama to Georgia and watches extended northward into the Mid-Atlantic states. Coastal flood watches and high wind warnings were also in effect from North Carolina to Delaware. Flood warnings were later expanded into South Carolina and coastal advisories were extended to New Jersey and Long Island. Gale warnings continued to grow in coverage, encompassing areas from North Carolina to New Jersey by the afternoon of November 12. By November 13, the watches and warnings gradually began to be discontinued as the low moved offshore. Although the Hydrometeorological Prediction Center issued their final advisory on the system later on November 13, flood advisories remained in effect due to residual impacts from the cyclone.\n\nDue to the location of the storm, south east of the Chesapeake Bay, persistent onshore flows brought elevated water levels to some areas for up to four days. This also brought a storm surge to much of the region and in some cases, these surges reached record levels set by Hurricane Isabel in 2003. In Norfolk, Virginia, a maximum storm surge of was measured on November 13. Five coastal measuring stations recorded record-high water levels during the event and three were within . Despite the nor'easter not being nearly as intense as Hurricane Isabel, water levels rivaled that of the hurricane because of persistent onshore flows, elevating water levels for several days.\n\nAlong the east coast of the United States, a nor'easter spawned by the remnants of Ida resulted in widespread damage along coastal areas. Minor damage was reported in South Carolina as winds up to 45 mph (75 km/h) and heavy rains, amounting to in most of the state, impacted the region. One person was killed after his vehicle collided with a downed tree in. Flash flooding took place in some areas due to the heavy rains and previously saturated grounds. In North Carolina strong winds downed several trees loosened in saturated soil. In Rockingham County, one person was killed after being struck by a branch while driving. In the Outer Banks, four homes were destroyed and over 500 others were damaged by the system, leaving at least $5.8 million in losses.\n\nAlong the Delmarva Peninsula, waves up to caused some coastal damage and high winds left roughly 13,000 without power. In Delaware alone, damage was estimated at $45 million. The most severe damage took place in New Jersey where coastal losses were estimated to be at least $180 million. Extensive sand loss was reported at numerous beaches, including 7 million cubic yards in Ocean City alone. In New York, one person drowned after being caught in rough seas off Rockaway Beach. Total beach losses in the state reached $8.2 million. Further north, the remnants of the cyclone brought heavy rains to portions of New England, resulting in flash flooding. In Maine, the highest rainfall total was recorded in Wells at . In Cumberland County, one river rose above flood-stage, inundating nearby areas.\n\nWidespread coastal damage and major flooding took place in Virginia as rainfall exceeding fell in many places and large waves affected beaches. A maximum rainfall of fell in Hampton during the storm. In some areas, roads were closed multiple times due to flooding. Minor damage was also reported as a few homes were inundated with up to of water. Some areas reported a storm surge comparable to that of Hurricanes Gloria in 1985 and Isabel in 2003. Damage from the storm in Virginia was estimated to be at least $38.8 million, of which $25 million was in Norfolk alone. According to the National Weather Service, of rain fell in Norfolk between November 11 and 13, nearly three times the monthly average for November; in those three days alone, the total rainfall surpassed the monthly record of set in 1951. Hurricane-force winds also affected the state, with a peak gust of occurring in Oceana.\n\nFollowing the widespread flooding caused by the storm, a major disaster declaration was signed by President Barack Obama on December 9 to provide residents in Virginia with federal assistance. According to the Federal Emergency Management Agency (FEMA), the cost of federal public assistance in the state would reach $11,227,376.\n\n\n"}
{"id": "14970392", "url": "https://en.wikipedia.org/wiki?curid=14970392", "title": "Philip Diehl (inventor)", "text": "Philip Diehl (inventor)\n\nPhilip H. Diehl (January 29, 1847 – April 7, 1913) was a German-American mechanical engineer and inventor who held several U.S. patents, including electric incandescent lamps, electric motors for sewing machines and other uses, and ceiling fans. Diehl was a contemporary of Thomas Edison and his inventions caused Edison to reduce the price of his incandescent bulb.\n\nHe occasionally spelled his first name 'Phillip'.\n\nPhilip H. Diehl was born in Dalsheim, Germany. \n\nIn July 1868, he immigrated to New York City where he worked in several machine shops before finding work as an apprentice with the Singer Manufacturing Company. In 1870 or 1871 he was transferred to Chicago, Illinois and worked at Remington Machine Company until 1875. He lost all of his possessions in the Great Chicago Fire of 1871. In 1873, Diehl married Emilie Loos in Chicago.\n\nIn 1875, Diehl moved to Elizabeth, New Jersey and took charge of experimental work improving sewing machines at the Singer plant. His daughter, Clara Elvira, was born April 2, 1876.\n\nWhile working at Singer in Elizabeth, Diehl experimented at work and at his home. This resulted in several inventions.\n\nWorking in the basement of his home on Orchard Street in Elizabeth, New Jersey, Diehl invented a lamp that was different from Thomas Edison's incandescent electric lamp, which was patented in 1879. Diehl's lamp had no lead-in wires. In 1882 Diehl obtained the first patent on this induction incandescent lamp. The base of the lamp contained a wire coil that coupled with a primary coil in the lamp socket, causing current to flow through the lamp without the need for lead-in wires. Two additional patents were granted in 1883, followed by patents for electrical lighting systems in 1885 and 1886.\n\nFollowing is a partial list of lamp or lighting related patents issued to Philip Diehl:\n\nDiehl erected the city's first arc light in front of the Corey Building in Elizabeth, which still stands at 109 Broad Street.\n\nDiehl's invention of the induction lamp was used by George Westinghouse to force royalty concessions from Thomas Edison. The Westinghouse Company bought Diehl's patent rights for $25,000. Although Diehl's lamp could not be made and sold at a price to compete with the Edison lamp, the Westinghouse Company used the Diehl bulb to force the holders of the Edison patent to charge a more reasonable rate for the use of the Edison patent rights.\n\nTogether with Lebbeus B. Miller, Diehl invented and patented the \"oscillating shuttle\" bobbin driver designed and a sewing machine build around it.\nDiehl's work at Singer to improve the sewing machine led to developments in electric motors, first to power sewing machines and later for other uses as well. In 1884 at the Franklin Institute in Philadelphia, Pennsylvania he demonstrated a dynamo, modeled after his smaller motor, which generated a current for arc lamps, sewing machine motors and incandescent lamps, all covered by his patents. The judicial committee at the exhibition judged it to be one of the best dynamos exhibited.\n\nThe fan was invented in 1882 by Schuyler Skaats Wheeler. A few years later, Philip Diehl mounted a fan blade on a sewing machine motor and attached it to the ceiling, inventing the ceiling fan, which he patented in 1887. Later, he added a light fixture to the ceiling fan. Later in 1904,Diehl and Co. added a split-ball joint, allowing it to be redirected; three years later, this developed into the first oscillating fan.\n\nPhilip Diehl died on April 7, 1913 in Elizabeth, New Jersey.\n\nIn 1889 the American Institute of New York awarded Philip Diehl a bronze medal, which bears the inscription \"The Medal of Merit, awarded to Philip Diehl for Electric Fans and Dynamos, 1889.\"\n\n"}
{"id": "24035669", "url": "https://en.wikipedia.org/wiki?curid=24035669", "title": "RESCO", "text": "RESCO\n\nA Renewable Energy Service Company (RESCO) is an ESCO Energy service company which provides energy to the consumers from renewable energy sources, usually solar photovoltaics, wind power or micro hydro.\n\nRESCOs include investor owned, publicly owned, cooperatives, and community organisations.\n\nThe main characteristics of a RESCO are:\n\nThe concept is much like that of a conventional electric utility in that the generation equipment is not owned by the user and the electricity that is generated is made available to the customer for a fee. The fee charged to the user includes any required capital replacement cost and all operating, maintenance and repair costs plus a profit for the operating organisation.\n\nThere are two significant differences between the conventional utility approach and that of the RESCO. For a RESCO:\n\nRESCOs have been very successful in the expansion of rural electrification projects worldwide because:\n\nAs a result of all this, donors are prepared to contribute with funding to the RESCO concept because it makes their aid (1) effective, (2) sustainable and (3) accountable.\n\nPacific Governments have provided rural electrification to remote locations by means of four main institutional set-ups, which are:\n\n\n"}
{"id": "38549505", "url": "https://en.wikipedia.org/wiki?curid=38549505", "title": "Renewable energy in France", "text": "Renewable energy in France\n\nUnder its commitment to the EU renewable energy directive of 2009, France has a target of producing 23% of its total energy needs from renewable energy by 2020. This figure breaks down to renewable energy providing 33% of energy used in the heating and cooling sector, 27% of the electricity sector and 10.5% in the transport sector. By the end of 2014, 14.3% of France's total energy requirements came from renewable energy, a rise from 9.6% in 2005.\n\nThe outlook for renewable electricity in France received a boost following the publication in October 2016 of the \"Programmation pluriannuelle de l'énergie\", showing a commitment to re-balancing the electricity mix towards renewables. According to the report, renewable electricity capacity is planned to grow from 41 GW in 2014 to between 71 and 78 GW by 2023. Historically the electricity sector in France has been dominated by the country's longstanding commitment to nuclear power. However, the report emphasizes that by 2025 more than half of France's nuclear power capacity will come from stations that will be 40 years or older, and subject to closure or refurbishment to extend their operation. Thus, there is a need to look to other sources, including renewables, to meet the expected generating-capacity shortfall. \n\nA key component of France's renewable target is the commitment to greatly increase energy efficiency, particularly for buildings and thermal insulation. Heat wastage is targeted to be reduced by 38% by 2020. The renewable targets are also intended to stimulate new trades and changes to existing trades to enable green growth. \nThe PPE plan targets the reduction of the consumption of primary fossil energy by 22% in 2023 from 2012 levels (reference scenario) or a fallback scenario of an 11% reduction under less-favorable conditions (variant scenario). \nIn terms of the reduction in primary consumption, petroleum products are targeted to fall by 23% between 2012 and 2023 (reference scenario) or 9.5% (variant scenario), gas by 16% (9% variant scenario) and coal by 37% (30% variant scenario).\n\nIn the transport sector, France has a range of initiatives designed to promote renewable energy use and increase efficiency. \nThese include changing transport behavior, such as a target of 10% of tele-worked days by 2030 to reduce consumption. \nBy 2023, the country aims to have a fleet of 2.4 million rechargeable electric and hybrid vehicles and for 3% of heavy-duty applications to use natural gas vehicles (NGVs). \nBiofuels blended with petrol are set for 1.8% in 2018 and 3.4% in 2023, and for diesel 1% in 2018 and 2.3% in 2023. \nBy 2030, non-road freight transport is targeted to reach 20% of all goods. \nInitiatives to increase walking and cycling are also being undertaken. Car pooling and digital services will be promoted to increase occupancy rates to between 1.8 and 2 people by 2030. The country is also pursuing research and development of autonomous vehicles, particularly in public transport.\n\nDuring 2016 renewable electricity accounted for 19.6% of France's total domestic power consumption, of which 12.2% was provided by hydroelectricity, 4.3% by wind power, 1.7% by solar power and 1.4% by bio energy. According to the report \"Programmation pluriannuelle de l'énergie\" renewable electricity capacity is targeted to grow from 41 GW capacity in 2014 to 52 GW by 2018 and between 71 and 78 GW by 2023. The target for 2023 includes a high and low scenario to take into account external factors such as cost and consultations that may affect future deployment. The sources that are planned to grow fastest are wind and solar photovoltaic (PV) power; 500 MW of offshore wind power is expected to be grid-connected by 2018. Onshore wind power is set to grow from around 9 GW in 2014 to between approximately 22 and 26 GW by 2023. Offshore wind power is targeted to grow from zero capacity in 2014 to between 3.5 GW and 9 GW by 2023, and up to an additional 2 GW of marine energy. Solar PV power is projected to grow from around 5.3 GW in 2014 to between 18.2 GW and 20.2 GW by 2023. Hydroelectric power is already well developed in France but is targeted to grow 500–750 MW by 2023.\n\nSolid biomass accounted for the largest share of renewable energy consumption in the heating and cooling sector at 8,661 ktoe (thousand tonnes of oil equivalent) in 2014. The next-largest source was provided by heat pumps at 1,794 ktoe. Heat accounts for about 95% of the energy produced by solid biomass, while the remaining 5% is used to produce electricity. Energy from wood and wood products accounts for almost all of this production, of which 73% is used to heat family dwellings. During 2015, heat consumption in France (excluding dependencies) from solid biomass amounted to 8,836 ktoe, of which 8,115 ktoe were accounted for by direct use of end user, and 721 ktoe from district heating sources. District heating networks were supplied during 2015 by both heat-only plants (326 ktoe), and combined heat and power plants (395 ktoe).\n\nThe Thassalia marine geothermal plant is located in the Grand Port Maritime de Marseille and uses marine thermal energy to provide heating and cooling to buildings connected to its network. The first phase of the network was inaugurated in October 2016 and covered . The network is planned to be expanded to cover around of Marseille. The plant pumps seawater from the port of Marseille and extracts the natural heat from the water using large-scale heat pumps to provide heating for the town. The process can be reversed to provide cooling during the hot Mediterranean summer. The project is regarded as a flagship example and it is hoped more will follow, including a much-larger geothermal marine project on the island of La Réunion to supply air conditioning utilizing seawater piped from .\n\nBiodiesel provided the largest share of renewable energy in the transport sector at 2,541 ktoe in 2014. In the same year bioethanol provided the next-largest share at 414 ktoe followed by renewable electricity at 251 ktoe.\n\nThe stock of light-duty plug-in electric vehicles registered in France passed the 100,000-unit milestone in October 2016, making the country the second-largest plug-in market in Europe after Norway, and fifth worldwide.\n\nHydroelectric power is the largest single source of renewable electricity in France accounting for 12.2% of total domestic power consumption in 2016. According to industry sources in 2014 there were around 2,600 hydroelectric plants of widely varying capacity accounting for 25,400 MW of installed capacity, 436 of these plants were run by EDF (Électricité de France, a French-based utility company largely owned by the state) and accounted for around 19,900 MW of the total capacity. In 2014 France was the world's tenth-largest producer of hydroelectricity, and Europe's second-largest after Norway, producing 69 TWh including pumped storage production. In 2016 aggregated hydroelectric plants of greater than 1 MW capacity of the run-of-the-river or poundage type accounted for 10,327 MW, the water reservoir type accounted for 8,231 MW and pumped-storage type 4,965 MW.\n\nFrance has the second-largest wind potential in Europe. Wind power capacity grew from 3,577 MW in 2008 to 10,358 MW by 2015 as France continues to develop this potential. As of year end 2015 all wind power in France is onshore, total onshore capacity is planned to more than double by 2023. France is committed to developing a large offshore capability, with the first 500 MW of capacity scheduled to come online by 2018. By 2023 France could have up to 11 GW of offshore wind and marine energy.\n\nSolar photovoltaic (PV) power grew from 104 MW capacity in 2008 to 6,549 MW by year end 2015 at which time France had the seventh-largest solar PV installed capacity in the world. France and is set to undergo significant expansion of its solar power with a target of around 18–20 GW installed capacity by 2023. In January 2016, President François Hollande and the Prime Minister of India, Narendra Modi, laid the foundation stone for the headquarters of the International Solar Alliance (ISA) in Gwalpahari, Gurgaon, India. The ISA will focus on promoting and developing solar energy and solar products for countries lying wholly or partially between the Tropic of Cancer and the Tropic of Capricorn.\n\nIn 2018 EDF had plans to invest up to 25 billion in PV power generation, and introduce green electricity tariffs.\n\nFrance opened Rance Tidal Power Station, the world's first tidal power station, in 1966. It remained the world's largest tidal station until 2011. Its 24 turbines reach a peak output of 240 MW with an annual output of around 500 GWh. The dam traverses the estuary of the Rance River in Brittany, connecting the tourist towns of Dinard and Saint Malo, providing both a roadbridge and footbridge. In addition the barrage is a popular destination in its own right amongst both tourists and anglers providing a pleasant walkway across the entire estuary.\n\nFrance has an overall target of producing 23% of its total energy needs from renewable energy by 2020, encompassing 33% in the heating and cooling sector, 27% in the electricity sector and 10.5% in the transport sector.\n\nBy 2014 France had achieved a 14.3% renewable energy share of its total energy use, a figure a little below its target figure of 16% by that year. Figures for the transport and electricity sectors were at or near their targets whilst the shortfall in the heating and cooling sector was around 4.1%. This may partially be explained by the ambitious rates targeted for this sector, where other countries typically have higher percentage targets in the electricity sector, a sector that has proven easier to raise the share of renewable energy.\n\n\n"}
{"id": "175146", "url": "https://en.wikipedia.org/wiki?curid=175146", "title": "Rudolf Clausius", "text": "Rudolf Clausius\n\nRudolf Julius Emanuel Clausius (; 2 January 1822 – 24 August 1888) was a German physicist and mathematician and is considered one of the central founders of the science of thermodynamics. By his restatement of Sadi Carnot's principle known as the Carnot cycle, he gave the theory of heat a truer and sounder basis. His most important paper, \"On the Moving Force of Heat\", published in 1850, first stated the basic ideas of the second law of thermodynamics. In 1865 he introduced the concept of entropy. In 1870 he introduced the virial theorem which applied to heat.\n\nClausius was born in Köslin (now Koszalin in Poland) in the Province of Pomerania in Prussia. His father was a Protestant pastor and school inspector, and Rudolf studied in the school of his father. After a few years, he went to the Gymnasium in Stettin (now Szczecin). Clausius graduated from the University of Berlin in 1844 where he studied mathematics and physics with, among others, Gustav Magnus, Peter Gustav Lejeune Dirichlet and Jakob Steiner. He also studied history with Leopold von Ranke. During 1847, he got his doctorate from the University of Halle on optical effects in Earth's atmosphere. He then became professor of physics at the Royal Artillery and Engineering School in Berlin and Privatdozent at the Berlin University. In 1855 he became professor at the ETH Zürich, the Swiss Federal Institute of Technology in Zürich, where he stayed until 1867. During that year, he moved to Würzburg and two years later, in 1869 to Bonn.\n\nIn 1870 Clausius organized an ambulance corps in the Franco-Prussian War. He was wounded in battle, leaving him with a lasting disability. He was awarded the Iron Cross for his services.\n\nHis wife, Adelheid Rimpham, died in childbirth in 1875, leaving him to raise their six children. He continued to teach, but had less time for research thereafter.\n\nIn 1886, he married Sophie Sack, and then had another child.\n\nTwo years later, on 24 August 1888, he died in Bonn, Germany.\n\nClausius's PhD thesis concerning the refraction of light proposed that we see a blue sky during the day, and various shades of red at sunrise and sunset (among other phenomena) due to reflection and refraction of light. Later, Lord Rayleigh would show that it was in fact due to the scattering of light, but regardless, Clausius used a far more mathematical approach than some have used.\n\nHis most famous paper, \"Ueber die bewegende Kraft der Wärme\" (\"On the Moving Force of Heat and the Laws of Heat which may be Deduced Therefrom\")\nwas published in 1850, and dealt with the mechanical theory of heat. In this paper, he showed that there was a contradiction between Carnot's principle and the concept of conservation of energy. Clausius restated the two laws of thermodynamics to overcome this contradiction (the third law was developed by Walther Nernst, during the years 1906–1912). This paper made him famous among scientists.\n\nClausius' most famous statement of thermodynamics second law was published in German in 1854, and in English in 1856. \nDuring 1857, Clausius contributed to the field of kinetic theory after refining August Krönig's very simple gas-kinetic model to include translational, rotational and vibrational molecular motions. In this same work he introduced the concept of 'Mean free path' of a particle.\n\nClausius deduced the Clausius–Clapeyron relation from thermodynamics. This relation, which is a way of characterizing the phase transition between two states of matter such as solid and liquid, had originally been developed in 1834 by Émile Clapeyron.\n\nIn 1865, Clausius gave the first mathematical version of the concept of entropy, and also gave it its name. Clausius chose the word because the meaning (from Greek ἐν \"en\" \"in\" and τροπή \"tropē\" \"transformation\") is \"\"content transformative\" or \"transformation content\" (\"Verwandlungsinhalt\"\"). He used the now abandoned unit 'Clausius' (symbol: Cl) for entropy.\n\nThe landmark 1865 paper in which he introduced the concept of entropy ends with the following summary of the first and second laws of thermodynamics:\n\n\n\n\n"}
{"id": "6423129", "url": "https://en.wikipedia.org/wiki?curid=6423129", "title": "S4W reactor", "text": "S4W reactor\n\nThe S4W reactor is a naval reactor plant used by the United States Navy to provide electricity generation and propulsion on warships. The S4W designation stands for:\n\n\nThis nuclear reactor plant utilized an S3W reactor in an alternate equipment arrangement.\n\nTwo boats of the \"Skate\" class were built with S4W reactor plants: USS \"Swordfish\" (SSN-579) and USS \"Seadragon\" (SSN-584) .\n\n"}
{"id": "48417668", "url": "https://en.wikipedia.org/wiki?curid=48417668", "title": "Steppe Geoglyphs", "text": "Steppe Geoglyphs\n\nThe Steppe Geoglyphs are a number of earth constructions in the Turgai Trough area of Turgai in northern Kazakhstan. There are at least 260 of these earthworks.\n\nMany or all of them consist of smaller earthworks (mounds, trenches, and ramparts) arranged with each other to make geometric and other shapes (composite figures). These shapes are squares, rings, and three others. The composite figures range from slightly under 90 m in length to over 400 m in diameter. Besides being made of earth dug out and piled up, some of the geoglyphs are made by placing stones next to each other.\n\nSome of the large shapes have been given names, including Bestamskoe Ring, Ushtogaysky (\"or\" Ushtogay) Square, Turgay triradial swastika, Large cross Ashtasti, Ekedyn cross, Ashutasti ring, Kyzyloba line, Koga cross, and Shili square.\n\nThese shapes are large enough to be easily visible on Google Earth. The Ushtogay Square is at . The Turgay triradial is at , only about from the town of Urpek. Both are inside Amangeldi District, Kostanay Region.\n\nThe earthworks were discovered in 2007 by Dimitriy Dey. He found them by searching Google Earth's satellite images for pyramids and similar configurations in Kazakhstan. They were first reported to the scientific community in 2014.\n\nOptical dating (optically stimulated luminescence) has been used to determine that one of the mounds dates to around 800 B.C. Dey has proposed they were produced by the Mahandzhar culture between seven and nine thousand years ago.\n\n"}
{"id": "41995305", "url": "https://en.wikipedia.org/wiki?curid=41995305", "title": "Switchyard reactor", "text": "Switchyard reactor\n\nFor transmission lines, the space between overhead line and ground forms a capacitor parallel to transmission line, which causes an increase in voltage as the distance increases. When a network becomes larger, sometimes the short-circuit current on transmission line exceeds the short-circuit rating of the equipment. To offset the capacitive effect of the transmission line and to regulate the voltage and reactive power of the power system, reactors are connected either at line terminals or at the middle, thereby improving the voltage profile of transmission line.\n\nA shunt reactor is connected in parallel with a transmission line or other load. A series reactor is connected between a load and source.\n\nA bus reactor is a type of air core inductor, or in some cases, oil filled, connected between two buses or two sections of the same bus in order to limit the voltage transients on either bus. It is installed in a bus to maintain system voltage when the load of the bus changes. It adds inductance to the system to offset the capacitance of the line which varies due to load, humidity, weather, generator excitation and temperature.\n\nA line reactor is placed in line at the point of use or just after a transformer to maintain a stable amperage to the user. When a line is disconnected from the system, the line reactor is also disconnected from the system. Line reactors are often used to compensate line capacitance, mitigate voltage transients due to switching, and to limit fault currents, especially in case of underground transmission lines.\n\nA bus reactor and a line reactor are interchangeable as long as they are rated for the same voltage which is dependent upon substation's physical layout, and bus configuration.\n\nShunt reactors are used in power systems to counteract the effect of the line parasitic capacitance, thereby stabilizing the system voltage within acceptable limits. For short lines, we can basically ignore the impact of capacitive current from a voltage regulation point of view, but medium and long lines can have voltages at their receiving end much higher than the sending end, thus creating issues such as over-fluxing of power transformers and over stressing of line insulators. Under light-load conditions, the line produces more VARs, resulting in receiving end voltage being higher than sending end voltage. In order to consume the excess VARs when system is lightly loaded, an inductor is added to the system. Since inductors absorbs VARs, a reactor is connected in parallel with shunt capacitance of the line known as Shunt Reactor,\n\nA Controlled shunt reactor (CSR) is a variable inductance, smoothly regulated by magnetic biasing of ferromagnetic elements of magnetic circuit. The magnetic system of a CSR single phase consists of two cores. Each core is equipped with control and power windings. In case of regulated DC voltage source connection to the control windings, biasing flow is increasing and directed to different sides in the adjacent cores. This resulted in saturation of CSR cores at relevant half-period of the current. Core saturation is resulted in initiation and increase of the current in the power winding due to non-linear characteristics of the magnetic core. Change in biasing current value leads to the power winding current change, due to which a stepless variation of voltage levels in CSR connection point as well as the value of reactive power consumed by the reactor is ensured.\n\nSeries reactors are used as current limiting reactors to increase the impedance of a system. They are also used for neutral earthing. Such reactors are also used to limit the starting currents of synchronous electric motors and to compensate Reactive Power in order to improve the transmission capacity of power lines.\n"}
{"id": "49792797", "url": "https://en.wikipedia.org/wiki?curid=49792797", "title": "Telechrome", "text": "Telechrome\n\nTelechrome was the first all-electronic single-tube color television system. It was invented by well-known Scottish television engineer, John Logie Baird, who had previously made the first public television broadcast, as well as the first color broadcast using a pre-Telechrome system.\n\nTelechrome used two electron guns aimed at either side of a thin, semi-transparent mica sheet. One of the sides was covered in cyan phosphor and the other red-orange, producing a limited color gamut, but well suited to displaying skin tones. With minor modifications, the system could also be used to produce 3D images. Telechrome was selected as the basis for a UK-wide television standard by a committee in 1944, but the difficult task of converting the two-color system to three-color RGB was still underway when Baird died in 1946.\n\nThe introduction of the shadow mask design by RCA produced a workable solution for color television, albeit one with considerably less image brightness. Interest in alternative systems like the Telechrome or Geer tube faded by the late 1950s. The only alternatives to see widespread use were General Electric's slot-mask, and Sony's Trinitron, both were modifications of the RCA concept. All CRT-based methods have since been almost completely replaced by LCD television, starting in the 1990s.\n\nBaird performed one of the earliest public demonstrations of color television system on 3 July 1928 using an all-mechanical system with three Nipkow disk scanners synchronized with a single disk on the receiving end and three colored lights that were turned on and off in synchronicity with the broadcaster. The same basic system was used on 4 February 1938 to create the first color broadcast transmissions from The Crystal Palace to the Dominion Theatre in London. Baird was not the only one to experiment with mechanical color television, and a number of similar devices were demonstrated throughout this period, but Baird is recorded as the first to show a real over-the-air transmission in a public demonstration.\n\nIn 1940 he introduced a much better solution using a system known today as hybrid color. This used a traditional black and white CRT with a rotating colored filter in front. Three frames, sent one after the other in a system known as \"sequential scan\", were displayed on the CRT while the colored wheel was spun in synchronicity. This design was physically very long, leading to deep receiver chassis, but later versions folded the optical path using mirrors to produce a somewhat more practical system. Again, Baird was not the only one to produce such a system, with CBS displaying a very similar system at almost the same time. However, Baird was not happy with the design later stated that a fully electronic device would be better.\n\nThe basic problem facing designers of color televisions was this: Sending each frame of the moving image meant sending three signals, red, green and blue. The sequential systems, like Baird's earlier efforts, sent the three images one after another. In order for motion to appear smooth, images must change at least 16 times a second. To reduce flicker, over 40 frames per second (fps) is mandatory. In sequential systems, each color requires a separate field. For this reason, very high refresh (field) rates were necessary. CBS' system refreshed at 144 fps. (Peter Goldmark's CBS team tried several field rates. Within the 6 MHz allowable channel bandwidth, the most acceptable rate was 144 fps. This rate made pictures incompatible with existing systems working at 50 or 60 Hz.\n\nA system sending all three signals at the same time at a conventional refresh rate would be greatly preferable. Transmitting such a signal could be accomplished by using three camera tubes, each with a color filter in front of them, using mirrors or prisms to aim at the same scene through a single lens. Each signal would then be separately broadcast using three conventional TV channels, and using the luminance concept, one of those could be received on a conventional black and white set. This would use a considerable amount of bandwidth, but this was a small cost in the era of only a few television channels.\n\nThe problem, however, was how to combine the three separate signals back into a single display. The system used in the cameras, with three separate tubes combined together optically, was not practical due to the cost of a receiver set with three CRTs as well as the unwieldily chassis needed to contain them. One such example was the RCA Triniscope, which produced useful images but was extremely complex, required constant adjustment, and was the size of a contemporary refrigerator to produce a display. A number of experiments were carried out using more conventional tubes and then filtering them, but the low output of the CRTs produced very dim images that were dismissed as impractical.\n\nBaird had previously worked on a high-intensity CRT system known as the \"teapot tube\" that saw some use in the UK and US as a projection system in theatres. These were normally built with two such CRTs side-by-side, with one acting as a hot backup in case the primary tube failed. In 1941 Baird converted such a projector to produce a two-color image simply by placing filters in front of the two tubes and projecting them onto a smaller screen to improve the effective intensity. He first showed this in 1941, and in 1942 the BBC described the resulting color image as \"entirely natural\". The image, of Paddy Naismith, is the first known image of color television to be published.\n\nA projection system with two CRTs was better than three, but still not practical for a home receiver. Baird continued to consider other solutions. One used a single conventional CRT with the two images displayed in a single frame, with the top half of the image containing the image for one color and the bottom the other. Lens systems focused on the display were positioned to see only the top or bottom image, passed them through filters, and then recombined them on a screen. There were drawings showing a similar system with three frames. Like many similar efforts from other experimenters, Baird abandoned this approach.\n\nStill searching for a single-tube solution that was bright enough for direct viewing, in 1942 Baird hit upon the Telechrome concept. His solution was essentially to combine two tubes into one large spherical enclosure. In the center of the enclosure was a translucent mica sheet forming the display, covered on one side with cyan phosphor, the other with an orange-red color, producing a limited but useful color gamut. Two electron guns arranged on either side of the screen fired at it, producing the two colors. The image was viewed from one side, seeing one of the colors directly and the other being transmitted through the screen from the other side. This was the first single-tube color television system.\n\nThe earliest test models used screens only a few inches across and had the guns arranged almost at right angles to it, making for a very large tube. Later models were built inside very large Hackbridge-Hewittic (H-H) vacuum tubes, which the company originally designed for use as high-power rectifiers in power supplies. Arthur Johnson, a glass blower who had previously worked for both Baird and H-H, produced the new models. These had screens ten inches across, comparable to monochrome screens of the era. The guns fired upward at about a 45 degree angle. As in the teapot tube, the necks were very long.\n\nBaird also demonstrated the use of the two-gun tube as the basis for stereoscopic 3D television. In this use, the two color-filtered television cameras were separated to produce a measurable angle, and then broadcast normally. Viewers wore colored glasses to re-separate the images into the left and right eye, in a fashion identical to Anaglyph 3D movies.\n\nThe colour gamut of the two-gun system was limited, unable to produce strong greens or blues. To produce a wider gamut, a system using the three primary colours would be needed. For two colours one can aim at either side of the screen, but for three there is no \"third side\" that can be used. Baird's solution used a variation on his two-colour system, using one side of the screen as-is, and patterning the other side with a series of horizontal triangular ridges. One gun, normally shown as red in most diagrams, fired onto the flat side of the screen, as in the two-colour model. The other two guns were arranged above and below the ridged side of the screen, so they fired onto one side or the other of the ridges. These were coloured green and blue. When all three guns fired, the image would be combined into a single display.\n\nThe problem with this approach was that it was very difficult to focus the electron guns on the ridges without the signal bleeding over to the ridges on either side. This problem was compounded by the changing angle between the gun and the ridges as the signal progressed down the screen. Similar designs were attempted by a number of researchers, the best known was the Geer tube that used pyramid-shaped patterns with three guns arranged around the back. None of these systems could ever be made to work reliably, with focusing and alignment being continual problems.\n\nThere is no documentary evidence that a successful version of the three-gun Telechrome tube was ever built, although images of Baird holding what is claimed to be a prototype are widely duplicated. The image shows a three-neck tube, but the third neck is the original Hewittic port, now used to hold the internal screen. Burns published a typical photo of the two-gun, three-neck tube.\n\nBaird also described a system using the ridged tube that eliminated the need for glasses. In this case, the tube was rotated so the peaks ran vertically instead of horizontally and the red gun was removed. The guns formerly used for green and blue were now used for left and right images. The basic concept is identical to the lenticular printing system used in magazines and other printed materials to produce 3D images. However, there is no evidence such a system was ever trialled.\n\nBaird gave a number of demonstrations of the two-color system throughout the war, and held a full press demonstration on 12 August 1944. These were generally reported in glowing terms, notably an October 1944 report in \"Electronics\" that described the images as bright and the 3D effect \"excellent\".\n\nNot all reports were so positive. One concluded that Baird had \"done a real service in demonstrating the value of colour television\", but suggested that the two-color system would ultimately have to be replaced with a three-color system. They went on to note:\n\nThey also dismissed the 3D work, which Baird had apparently ended by this point, as a \"stunt\".\n\nIn 1943, with the war clearly turning in the Allies favor, Winston Churchill formed a series of committees to consider post-war redevelopment. Among these were plans to re-open the Alexandra Palace broadcaster, and more widely, nationwide television. To consider this, in September 1943 Churchill formed the Television Committee, better known to history as the Hankey Committee.\n\nThe Committee met numerous times during the next year, and asked Baird to prepare a number of papers on the topic of post-war broadcasting. Among his suggestions, he stated that the BBC's monopoly should be ended and independent broadcasters should be licensed, which was delivered along with a request to start such a service. The Committee agreed with this position. He also described the Telechrome system, and this appears to have had a great impact on the Committee.\n\nIn his comments to the Hankey Television Committee, Baird suggested two-color, 1,000-line pictures. Such pictures would have required considerable radio bandwidth. The pictures would be incompatible with the pre-war, EMI / BBC, 405-line system. Before the Hankey Committee, Baird also considered the possibility of compatible color systems. We can imagine two ways to adapt Telechrome as an EMI / BBC compatible color system. Method 1 would use one field of the EMI / BBC interlaced image for red. The other field would convey cyan picture information. Yet this method would reduce the effective resolution from 405 to 202.5 lines. The video field rate would drop from 50 to 25 Hz (40 mS). In areas of solid color, flicker would increase. Method 2 would broadcast full, 405-line images. But in this method, frame rate would fall to a mere 12.5 fps (80 mS). Flicker would then be considerable.\nIn December 1944, the committee delivered its preliminary report. The report called for a system that had \"on the order of 1,000 lines\" of resolution. The system would optionally be capable of color and 3D displays. The system also be able to run beside the pre-war 405-line system by Marconi and EMI.\n\nBaird was called to a 29 February 1944 meeting of Cable and Wireless (C&W) to discuss the formation of a color television studio. After some discussion, C&W chairman Edward Wilshaw noted that there was an agreement in place that precluded Marconi from entering the market until 1949, which would place them at a significant disadvantage compared to other companies. He suggested that the matter be deferred, as any immediate changes would produce friction between C&W, the General Post Office and the BBC. The matter was dropped, and it would not be until the Television Act 1954 that the possibility was again considered.\n\nFrom 1944 Baird was suffering from increasingly poor health, and late that year he suffered an attack of fever that left him almost invalid. Nevertheless, he formed a new company, John Logie Baird Ltd., with offices and labs in a downtown London house. Baird visited the lab less and less frequently over time, and his wife noticed why in a November 1945 visit when he was seen to have to stop and pant after climbing every stair of the building's four stories. He caught a cold over Christmas 1945, and suffered a stroke in February 1946. He was ordered bedridden but refused to stay there, and continued to deteriorate until his death on 14 June.\n\nTelechrome died with Baird, but the company by this time had introduced its first truly successful product. This combined a black and white television, a radio receiver and a record-changing record player in a single large cabinet. The company was purchased in 1948 and switched hands several times, eventually being used as a brand name by Thorn Electrical Industries for a time.\n\nMany years later, former Baird employee Edward Anderson was quoted as saying that they \"had the equivalent of the Sony Trinitron tube on the drawing board\". This has been used by a number of non-technical authors to suggest that the Trinitron is in some way technically related to the Telechrome in spite of the two systems having nothing in common.\n\n\n"}
{"id": "425768", "url": "https://en.wikipedia.org/wiki?curid=425768", "title": "The Living Soil", "text": "The Living Soil\n\nThe Living Soil (1943) by Lady Eve Balfour is considered a seminal classic in organic agriculture and the organic movement. The book is based on the initial findings of the first three years of the Haughley Experiment, the first formal, side-by-side farm trial to compare organic and chemical-based farming, started in 1939 by Balfour (with Alice Debenham), on two adjoining farms in Haughley Green, Suffolk, England.\n\n\"The Living Soil\" was also published as The Living Soil and the Haughley Experiment.\n\n"}
{"id": "352311", "url": "https://en.wikipedia.org/wiki?curid=352311", "title": "Timber slide", "text": "Timber slide\n\nA timber slide is a device for moving timber past rapids and waterfalls. Their use in Canada was widespread in the 18th and 19th century timber trade. At this time, cut timber would be floated down rivers in large timber rafts from logging camps to ports such as Montreal and Saint John, New Brunswick. Rapids and waterfalls would, however, damage the wood and could potentially cause log jams. Thus at these locations timber slides were constructed. These were thin water filled chutes that would run parallel to the river. They would usually only be wide enough for a single log and one at a time the logs would be directed down it. The idea is attributed to Ruggles Wright who introduced the first one in 1829 not far from what is today down-town Hull, Quebec, Canada. Later, the slides could often be up to a kilometre in length. They were most commonly found on the Ottawa River system.\n\nIn some areas the timber slide became a tourist attraction, the most notable being the 1.2 km chute bypassing the Chaudière Falls on the Ottawa River in Ottawa. Its most notable visitors are the Duke of York, who later became King George V, and his wife, the Duchess of York Mary of Teck.\n\nTimber slides disappeared after the construction of canal networks and the decline of the timber trade. They were almost all out of service by the First World War.\n\n\n"}
{"id": "782653", "url": "https://en.wikipedia.org/wiki?curid=782653", "title": "Treen (wooden)", "text": "Treen (wooden)\n\nTreen, literally \"of a tree\" is a generic name for small handmade functional household objects made of wood. Treen is distinct from furniture, such as chairs, and cabinetry, as well as clocks and cupboards. Before the late 17th-century, when silver, pewter, and ceramics were introduced for tableware, most small household items, boxes and tableware were carved from wood. Today, treen is highly collectable for its beautiful patina and tactile appeal.\n\nAnything from wooden plates and bowls, snuff boxes and needle cases, spoons and stay busks to shoehorns and chopping boards can be classed as treen. Domestic and agricultural wooden tools are also usually classed with treen.\n\nBefore the advent of cheap metal wares in industrialized societies, and later plastic, wood played a much greater part as the raw material for common objects. Turning and carving were the key manufacturing techniques. The selection of wood species was important, and close-grained native hardwoods such as box, beech and sycamore were particularly favoured, with occasional use of exotics, such as lignum vitae for mallet heads.\n\nWooden objects have survived relatively less well than those of metal or stone, and their study by archaeologists and historians has been somewhat neglected until recently. Their strongly functional and undecorated forms have, however, been highly regarded by designers and collectors.\n\nThe scholarly study of treen was greatly advanced by Edward Pinto (1901–1972), who started collecting in his childhood and wrote a definitive book on the subject. In 1965, when Birmingham Museum & Art Gallery purchased his collection, it contained over 7,000 items.\n\nIn North America, Native Americans carved tree burls into durable wooden objects with uniquely marbled grain. Burls were rare in Europe because the old-growth forests where they are commonly found had largely been logged out of existence. Burl treen was found in Europe occasionally, particularly in objects intended for celebration or the upper class, but was not in wide-scale use.\n\nIn contrast, burls were widely available in the virgin forests of North America. Native Americans worked these burls into domestic objects like bowls and ladles with tools such as stone blades, hot coals, and beaver teeth. Native Americans traded these wooden items with European colonists, who later learned to harvest burl and carve them into treen in the style of their home countries. Burl treen is considered an indigenous North American craft, and examples are found in museums and private collections of Americana.\n\nThe snarled and interlaced grain of a burl makes the resulting objects stronger and less likely to split. They were strong enough to be passed down over generations. A variety of trees produce burls, but almost all North American burl treen (upwards of ninety percent) is made from black ash. Another five percent is made from maple, with other woods such as cherry wood, white cedar, oak, and birch making up the remainder. Woodworker Michael Combs has speculated that black ash burl was favored because it is easy to work on a lathe.\n\n\n"}
{"id": "30680516", "url": "https://en.wikipedia.org/wiki?curid=30680516", "title": "Tropical Warm Pool", "text": "Tropical Warm Pool\n\nThe Tropical Warm Pool (TWP) or Indo-Pacific Warm Pool is a mass of ocean water located in the western Pacific Ocean and eastern Indian Ocean which consistently exhibits the highest water temperatures over the largest expanse of the Earth's surface. Its intensity and extent appear to oscillate over a time period measured in decades.\n\n"}
{"id": "48215543", "url": "https://en.wikipedia.org/wiki?curid=48215543", "title": "Ulu Jelai Power Station", "text": "Ulu Jelai Power Station\n\nThe Ulu Jelai Power Station is a hydroelectric power station under construction in the district of Cameron Highlands, Pahang, Malaysia. It is one of the entry point projects under the Economic Transformation Programme.\n\nThe project is located approximately 150 km north of Kuala Lumpur. The nearest town is Ringlet, 40 km away. The under-construction station is accessible from route connecting the towns of Ringlet and Sungai Koyan. It is located within Ulu Jelai and Bukit Jerut forest reserves, near Cameron Highlands-Lipis district border.\n\nThe power station has a maximum generating capacity of 372 MW. Water from 3 rivers - Sungai Bertam, Sungai Telom and Sungai Lemoi is used for electricity generation. A dam is built to impound Sungai Bertam. Weirs and diversion tunnels are built on Sungai Telom and Sungai Lemoi to divert water into the main reservoir at Sungai Bertam. From the reservoir, water is channeled into a series of tunnels 15 km-long to generate electricity before being released back into Sungai Telom.\n\nThe underground power cavern houses 2 units of Francis turbines each with a generating capacity of 186 MW.\n\nTM-Salini Consortium has been appointed as the main contractor. Construction commenced in March 2011. The 88m-high roller compacted concrete (RCC) dam on Sungai Bertam was built after Sungai Bertam has been successfully diverted. The RCC mix is of low cementitious content and aggregates were sourced from a nearby quarry.\n\nAll tunnels and underground caverns were excavated using drill and blast method except for the diversion tunnels which utilised a 3m-diameter tunnel boring machine. Lining of tunnels depended upon the rock conditions and water pressure - concrete and steel are used as lining materials. The surge shaft was excavated using the raised boring technique. Unit 1 is expected to be commissioned in December 2015 while Unit 2 in March 2016.\n\nThe construction of this project resulted in the relocation of 3 orang Asli villages - Kampung Susu, Kampung Tiat and Kampung Pinang.\n\nThe reservoir impounding started on 18 January 2016 and reach the full supply level on 16 May 2016.\n\nUnit 2 was the first unit being commissioned due to some issues with the commissioning of Unit 1. Commercial operation of Unit 2 was at 0000 hours on 15 August 2016.\n\nSedimentation is a major issue for Sungai Bertam and Sungai Telom due to uncontrolled clearing of land. A check dam has been constructed upstream of the main reservoir on Sungai Bertam. The check dam is able to reduce the amount of sediment entering the reservoir and prolonging the reservoir life. A desanding system has been constructed at the intake of Sungai Telom for the same purpose.\n\n"}
{"id": "196376", "url": "https://en.wikipedia.org/wiki?curid=196376", "title": "Velomobile", "text": "Velomobile\n\nA velomobile, velomobiel, velo, or bicycle car, is a human-powered vehicle (HPV) enclosed for aerodynamic advantage and protection from weather and collisions. They are similar to recumbent bicycles and tricycles, but with a full fairing (aerodynamic shell). A fairing may be added to a non-faired cycle, or the fairing may be an integral part of the structure, monocoque like that of an airplane. \n\nNot to be confused with purpose built for racing or speed records fully fared vehicles with two wheels, generally called streamliners. Streamliners have set many speed and distance records Though fast in their own right, Velomobiles are consider much more streetable machines. Using three or more wheels can have advantages for everyday use, including the ability to stop and start unaided, better stability, cross-wind handling, etc. Though there are arguments made that the multiple track machines (3+ wheels) have aerodynamic disadvantages due to the drag of the extra wheels and the surface contact points. A good discussion on bicycle wheel aerodynamics is here In practice though Velomobiles continue to be close to their 2 wheel cousins in performance \n\nThere are few velomobile manufacturers; some are home-built. Some models have the operator's head exposed; this has the advantage of giving the operator unobstructed vision, hearing, and some cooling, with the disadvantage of being potentially more exposed to weather and less aerodynamic. Fully enclosed machines can suffer from heat or humidity issues as well as potential noise issues. \n\nThe typical drive train of a velomobile is not unlike a bicycle or recumbent. It will consist of a front with 1 or more , a rear Depending on the configuration of the Velomobile there may be any number of idler pulleys, and chaintubes along the drive train to manage and protect the chain. One of the defining characteristics of most velomobiles is the are protected from weather and the road. \n\nBefore World War I, Charles Mochet built a small four-wheeled 'bike'-car for his son. Mochet built many models of small vehicles called \"Velocar\". Some models had two seats, most were pedal powered, but as the years went by, many were fitted with small engines. Mochet Velocars use a thin wood/plywood body on a steel frame.\n\nSome other early velomobiles use a fabric body or \"skin\" sewn to fit loosely on closely spaced wires or tubes, then painted or \"doped\" with a liquid that dries and shrinks the fabric to a tight fit on the wire/tube supports. The approach was used widely on early airplanes, and has the advantage of light weight with relatively low-technology materials. It is sometimes called \"bird cage\" construction because the support looks similar to closely spaced wires used in construction of bird cages, and because the wire/tube support outline shows through once the fabric is tight. Some disadvantages of this approach are the cost of construction, due to the many interconnected supports; and that the shape is that of many flat panels, which limits smoothness of the skin and thus limits the aerodynamics.\n\nIn the 1970s, the People Powered Vehicle was produced. It was a two-seat, \"sociable\" tandem with a steel sub frame and molded plastic body. It was well designed and weighed something over ; a recently restored version weighs . However, it had flaws in the execution that doomed it as a practical, everyday vehicle. Positive features, such as easily adjustable and comfortable seats, independent pedalling for both passenger and driver, adequate cargo space and relatively good weather protection, could not overcome the negative features, such as a complex, heavy and badly spaced three-speed gear box, ineffective brakes, and pedals that slid on sleeve bearings on steel shafts, which made it difficult to use as an everyday vehicle.\n\nIn Sweden, a two-seat design called Fantom was sold as blueprints and became very popular; over 100,000 copies of the blueprints were sold, but few were actually completed. The downfall of these early 'bicycle' cars came when the economy improved and people chose motorised transport.\n\nBuilders continued to make \"one-off\" velomobiles, but for a time none were available commercially. In the 1970s, Carl-Georg Rasmussen\nrediscovered \"Fantomen\"; he redesigned it and in 1983 started selling a production version called Leitra. Leitra velomobiles have been in continuous production since then (as of 2017), with current models evolved/improved from the originals.\n\nThere are many ways to build a velomobile. One modern design is \"body-on-frame\", in which a velomobile is made from a not-faired cycle plus a body. A standard cycle may be used, but often a custom cycle is used with special fittings to mount the body; the use of special fittings tends to improve fit and durability, and can also reduce weight. Body-on-frame construction allows flexible configuration: the body may be of any construction, as it does not need to be self-supporting, and various bodies can be used with various frames. Also, the body may be removed so the cycle alone can be used. However, the overall weight of body-on-frame is often higher than alternatives, as the body does have some intrinsic strength, yet this is not used to reduce the weight of the frame.\n\nAnother modern design is the , using aluminum sheet formed and riveted to make the fairing and the structure in one piece. This approach is sometimes called monocoque or \"unit\" construction; it was used in airplanes before 1920 and has been used commonly in automobiles since the 1970s. Labor costs to build an Alleweder are significant due to the many rivets and rivet holes. Also, the choices of aerodynamic shapes are limited by the formability of the aluminum sheet. That said, aluminum is relatively inexpensive, and as of 2017 Alleweders can be often be bought for less than other designs; they can also be bought as kits, to reduce out-of-pocket cost. Aluminum can also be repaired relatively easily, and aluminum, which is homogeneous, can be recycled more easily than many composite materials.\n\nAnother common modern design is a monocoque shell, often made of fiber-reinforced plastic or \"FRP\", plus sub-frames of welded aluminum tubes. FRP can be used to produce a wide range of shapes, and thus can improve aerodynamics over approaches such as \"birdcage\" and aluminum-sheet monocoque. FRP can also use fibers with high strength-to-weight ratio that in velomobiles can save several kilograms compared to other designs. A wide range of fibers may be used, but those which reduce weight while retaining strength and toughness often increase the price significantly—e.g., a premium of 1000 Euros to save 3 kg. In addition, it is often hard to segregate and recycle the FRP materials. But despite the cost and other issues, the aerodynamic and weight advantages mean that (as of 2017) monocoque FRP is a common way to build velomobiles.\n\nAs of 2017, most velomobiles are tricycles with two front wheels. A tricycle has the advantage over a bicycle that it does not fall over when stopped. In addition, when a cross-wind hits a fairing, it makes a big force; faired tricycles are less likely to get blown over than faired bicycles. Although three wheels have practical advantages, they also have more aerodynamic drag than two wheels, so land speed record cycles are often bicycles. Although four wheels were used as far back as the Velocar, they are not today common. As of 2017, there is at least one 4-wheel production model, the QuattroVelo. Four wheels tends to further hurt aerodynamics and weight compared to three; but for a given width, four wheels is much more laterally stable than three wheels. Also, four wheels can be placed in a way that increases luggage capacity a lot compared to three-wheel designs.\n\nMost velomobiles drive the rear wheel(s). This approach is simple and can often use many standard bicycle parts. Velomobiles with two rear wheels may drive just one wheel, or may drive both. Driving a single wheel is the simplest and lightest approach. Driving both wheels can improve traction but also increases complexity, cost, and weight. A solid axle may be used, but increases friction/drag in corners and thus can slow down the vehicle. One alternative is a differential, which is the approach used in most cars. A second alternative is to use a pair of ratchets, where the slower wheel is driven and the faster wheel coasts. Rear drive often uses \"idler\" pulleys to route the chain; front-wheel drive can eliminate idlers, so offers less friction and also less weight. It can also increase luggage capacity. However, front drive with two front wheels uses some non-standard components, and as of 2017 is used only rarely.\n\nAs with other cycles, a velomobile may use suspension. Suspension tends to improve rider comfort, and can also improve speed—it takes energy to \"bounce\" the velomobile and rider, so a suspension can reduce the energy lost to bouncing. However, suspension adds cost, weight, and maintenance. Common velomobile designs include no suspension, front-only suspension, and front+rear suspension.\n\nAs of 2017, there are several commercial makers of velomobiles. At the same time, there are still many \"one-off\" makers. Individual designs (both one-off and production) emphasize specific features. For example, some emphasize aerodynamics and light weight to improve average speed, even if cost, ease of entry/exit, comfort, and other \"practical vehicle\" attributes are reduced. By analogy, many automobile makers make performance cars with limited seating and cargo and worse emissions and fuel economy. In contrast, other velomobile designs sacrifice performance features in order to improve cost, entry/exit, comfort, carrying capacity, and so on. By analogy, many automobile makers offer cargo vans. As of 2017, individual makers sometimes offer models spanning a range of features—for example, there are Milan models used in racing but also a Milan \"cargo\" model with enough luggage volume to carry a (not-pedaling) human passenger and other bulky items.\n\nAll current (2017) velomobiles are produced in low volume, with \"big\" makers producing one or a few velomobiles per week. Velomobiles use some standard bicycle parts, but also many parts specific to velomobiles, and thus made in low volume. The use of \"more parts\" (e.g., 3 wheels instead of 2) and \"more low-volume\" parts makes velomobiles more expensive. The only attempt at a mass-produced velomobile, which was in the mid-1980s, flopped. This was the Sinclair C5. The C5 was a delta trike (one front, two rear wheels) with electric assist designed to be mass-produced and sold for a low price. The C5 was poorly designed; it was heavy, had only one gear and had no adjustment for the distance between the pedals and the seat, which is important to get a comfortable pedalling position.\n\nA concept and a potential assessment concerning low-cost velomobiles for daily short trips as well as strategies for reaching a critical lot size for mass production was the subject of a research project called RegInnoMobil.\n\nVelomobiles have also been used in Australian HPV Super Series since 1985, and more recently, other events in Australia like the RACV Energy Breakthrough, the Fraser Coast Technology Challenge, and the Victorian HPV Series.\n\nThe Milan was used by Christian von Ascheberg, current record holder in the 24 hours (1219 km) and 1000 km (19h27m) category.\n\nin 2018 Dave Lewis set a new race record at the Sebring 24 hour race using a DF velomobile made by \"Intercitybike.nl\" Velomobiles also competed in the ultra endurance Trans Am Bike race and came in 1st (setting a new record) and 4th places. 1st place was Marcel Graber and 4th Dave Lewis the full results can be found here - though at the time of this writing it is only current through 2017\n\nThe velomobile fairing adds weight compared to standard upright cycles or unfaired recumbent cycles. For a given terrain, the added weight demands lower gearing and makes the velomobile slower climbing hills than its unfaired counterpart.\n\nSome velomobile fairings are mainly for weather protection. However, if the velomobile fairing is substantially streamlined, then improved aerodynamics means the speeds on flats and down hills may be substantially higher than its unfaired counterpart, and often enough faster to make up for the slower climbing due to weight.\n\nAn aerodynamic fairing must be well-shaped, but minimizing the frontal area of the velomobile is also important to reduced drag: a fairing with half the frontal area may approach half the air drag. In turn, aerodynamic velomobiles use a laid-back or recumbent riding position, with the rider's head much lower than on regular bicycles. In turn, velomobile is much easier to accidentally \"hide\" behind a car or road-side shrubbery, fences, etc.\n\nVelomobile bodies are typically light enough that the center of mass is similar to the center of mass on an unfaired recumbent cycle. This makes cornering stability similar to similar unfaired cycles. However, minimizing velomobile width also contributes to reduced frontal area and thus drag; so there is an additional incentive to make the velomobile narrow. The narrowest velomobiles are only slightly wider than the rider's shoulders, and so the width approaches that of an upright bicycle. However, an upright bicycle still has a significantly narrower \"useful\" width, as the road contact is in the center and so the rider's hands/elbows/etc. might overhang the edge of the roadway or path without causing problems. In contrast, the wheel track of a velomobile is very nearly as wide as the vehicle itself, and so cannot overhang the edge.\n\nAs of 2017, most velomobiles use a delta tricycle configuration—mainly to reduce component weight and improve wheel aerodynamics. However, some use a four-wheel or quadracycle configuration. The extra wheel significantly improves cornering stability and can also improve luggage capacity. As of 2017 there are not many four-wheel velomobiles with highly aerodynamic fairings, but there are a few, and some riders report speeds are close to three-wheel velomobiles with highly aerodynamic fairings.\n\nTwo-wheel \"streamliner\" configurations can have much less aerodynamic drag: wheels are hard to make aerodynamic; each wheel entry/exit to the fairing adds drag; and velomobiles with two front wheels are necessarily wider or longer than the rider, while two-wheel streamliners can be barely wider than the rider. A common way to describe aerodynamic drag is \"CdA\"; in one comparison of racing cycles, there were several two-wheel streamliners with less than half the CdA drag of the best three-wheeler. Air drag is most significant for high-speed events; as of 2016, the world record for a 200-metre sprint on near-level ground is about 145 km/h for a two-wheel streamliner and about 120 km/h for any vehicle with more than two wheels, meaning the two-wheel vehicle was about 20% faster. Aerodynamic power is roughly cubic in speed, so at lower speeds the difference is much less pronounced. At the same time, two-wheel streamliners require a way to stay upright when stopped and at very low speed, and are more sensitive to blowing over in cross-winds. These factors limit the use of streamliners, despite their aerodynamic advantage.\n\nThe fairing on a velomobile sometimes makes it more subject to cross-winds than a similar unfaired cycle. The effect of cross-winds is complicated because the force of the wind can act as a steering force, as-if the rider had tried to steer the cycle. \"Wind steer\" can be a safety issue and may also hurt performance, as a serpentine path is longer and thus slower than a straight line. Thus, a design with inferior aerodynamics may be better overall. For example, it is common for time-trial bicycles to use a solid disk rear wheel for best aerodynamics, and a spoked front wheel that has worse aerodynamics than a disk, but is less likely to steer the bicycle in a cross-wind. Velomobile fairings have analogous concerns, leading to trade-offs in fairing design. For example, a longer \"tail\" on the fairing will increase the total side-wind profile and total side forces, but can reduce the percentage force acting on the steered wheels and so an improve stability in side winds. A larger fairing also hurts weight and has more skin drag), but as with time-trial bicycles, worse aerodynamics but better handling is sometimes a good trade-off.\n\nVelomobiles with fairings that are mainly for weather protection can use a more upright seating position. This tends to improve both the ability to see and be seen. However, to retain stability against tipping (both cornering and cross-winds), the wheel track needs to be wider than a comparable velomobile with a low seating position. In turn, this may make the velomobile quite a bit wider than a conventional cycle.\n\n\"Weather\" protection includes cold and wet, but also shading from sun. Since the rider is doing work, it is typically desirable to have at least some cooling. Many velomobiles have vents and ducts which allow cooling while keeping out water; and the ducts/vents may be closed or covered in cold weather so the rider can stay comfortable even without a secondary heat source. In warm and hot weather, the fairing provides sun protection, but blocks the rider from cooling air, and so may be much warmer for a given level of effort. In some situations, a rider's power output (for any type of cycle) is limited by body temperature, and the worse cooling of a velomobile can limit the rider's power output more than on an unfaired cycle. With an aerodynamic fairing, the velomobile rider at reduced power output may still be faster than an unfaired cycle, due to the lower aerodynamic drag.\n\nVelomobiles for cargo use often have heavy frames to carry load, plus the weight of the cargo itself. In turn, the weight of the fairing may be relatively less important. Also, bulky loads often have poor aerodynamics, and so the quality of aerodynamics of the fairing is less important. This may enable use of a fairing which in hot weather can be converted to a canopy. A canopy provides no aerodynamic benefits, but improves cooling compared to a faired configuration, while also reducing sun exposure compared to riding without a canopy. Thus, a \"high speed\" velomobile may benefit most from better aerodynamics, even when aerodynamics hurts rider cooling; while at the same time a \"high load\" velomobile may benefit most from improved cooling (to maximize power output) even if that hurts aerodynamics.\n\nThe steered wheels on a velomobile will hit the fairing if steered sharply enough. Making the fairing wider can give space to steer the wheels more sharply, but has down-sides for aerodynamics and width. Although sharp steering is not needed at speed, many aerodynamic velomobiles have a much worse turning circle than an equivalent unfaired cycle. In contrast an unfaired cycle cannot have fairing interference, and so even with the same wheel and rider configuration can steer a much tighter circle. Steering only the rear wheel(s) would avoid fairing interference, but it is hard to build a stable vehicle using only rear-wheel steering. The Velayo uses a tricycle configuration and steers only the rear wheel; but it was produced only in small numbers. The experimental Kingsbury Fortuna and Quattro velomobiles steered all wheels; this approach avoids some of the stability problems of rear-wheel steering while still reducing the steering angle of the front wheels. However, this approach has not (as of 2017) gained wider use in velomobiles.\n\nA velomobile's fairing shields the drivetrain from weather, as well as the rider. Drivetrain maintenance is often reduced compared to other cycles, especially unfaired bicycles, where the front wheel kicks up grit-containing dust, mud, and dirty water that lands directly on the chain and increases the rate of abrasive wear on the drivetrain—including chain and sprockets, but sometimes also derailleurs. The fairing of a velomobile tends to limit both the amount and the kinds of grit landing on the drivetrain. Some cycles use tooth-belt drive, which is less affected by grit, is quieter than a chain, and may be lighter. However, belts are only available in pre-selected sizes. Many recumbent cycles, including most velomobiles, have a long drivetrain for which no suitable tooth belts are available.\n\nVelomobiles are significantly bulkier than conventional cycles. Further, the bodywork typically cannot be disassembled much, whereas conventional cycles often can be disassembled to fit in a box or bag of dimensions similar to the frame. In turn, this makes velomobiles more difficult to transport.\n\nVelomobiles are often built using some standard bicycle parts, but also many parts specific to velomobiles or even specific to a particular make or model. In addition, the bodywork is large and may be around half the weight of a velomobile. Thus, to reduce weight, the body is often made of lighter but more expensive materials. Also, production volumes are low, so for both parts and labor there are not benefits of mass production—as of 2017, many velomobile makers have yearly production on the order of tens or maybe a few hundred velomobiles. Taken together, these factors mean velomobiles are often much more expensive than other cycle types.\n\nAs an example of prices and price/weight tradeoffs, as of April 2017 the maker Trisled offers their \"Rotovelo\" model either with a rotational molded plastic fairing or with a carbon fiber fairing (as well as some other weight-saving changes). The body shapes and underlying framework are similar; the rotational-molded version weighs 33 kilograms and has a list price of Au$6500, while the carbon fiber version weighs 20 kilograms and has a list price of Au$10900.\n\nMuch of the cycle-related infrastructure design is based on the typical configuration of an upright bicycle. For example, multi-modal transportation such as bike/train/bike routes often use bicycle racks in the train, and the dimensions of racks and also the train ingress/egress assume a conventional cycle. Similarly, cycle paths often have bollards or S-bend paths to prevent motor vehicle entry, and the entry is often spaced for upright bicycles, but may be too narrow or require too sharp of a turn to allow through some velomobiles.\n\nSome velomobiles have been converted to provide electric assist. Electric assist means that a small battery-operated electric-propulsion system is provided to assist the driver's leg muscle effort. Most electric-assist propulsion motors are of the in-wheel design in the rear wheel, such as geared hub motors (like eZee, Heinzmann, Bafang, BMC, etc.) and direct-drive hub motors (like Crystalyte, BionX, 9Continent, etc.) but mid-drive units (like Sunstar, Cyclone, Ecospeed, etc.) are used as well due to design constraints in velomobile models with one-sided rear wheel mounting like the Quest, Strada and Mango or front wheel drive in the Velayo, or better efficiency by using the multiple speeds of the chain drive or internal geared hubs (e.g. Rohloff 14 speed hub).\n\nWhile an electric-assist unit does add extra weight to the velomobile, it is somewhat offset by the flexibility it also provides, especially during hill climbs and stop-and-go traffic. Due to vastly better aerodynamics of velomobiles the range of a similar electric assist unit and similar battery in a velomobile can be about 50% to 100% higher compared to upright bicycles or unfaired recumbents.\n\nIn events like the RACV Energy Breakthrough and the Fraser Coast Technology Challenge, there are entire categories dedicated to electric and other hybrid powered velomobiles.\n\nThe legal definition of \"bicycle\" often includes velomobiles, but laws covering cycles with electric assist vary widely across countries and often within a country and even between cities in a region. For example, a specific vehicle may be a \"bicycle\" in one area, a \"low-speed pedal-assisted cycle\" in another area, and a \"moped\" in yet a third. Similarly, going from 3 wheels to 4 wheels can change the category of an otherwise identical vehicle. One reason for the varying treatment is that many laws are older than widespread use of power-assisted velomobiles and so the laws were not written to consider such vehicles. In some areas, laws are being rewritten to include power-assist velomobiles and to harmonize treatment with nearby laws.\n\nWith a growing DIY-community and an increasing interest in environmentally friendly \"green energy\", some hobbyists have endeavored to build their own velomobiles from kits, sourced components, or from scratch. When compared to similar sized commercial velomobiles, the DIY velomobiles tend to be less expensive.\n\nProbably the most built velomobile kits are the various models of the made from prefabricated aluminium sheet metal due to its affordable price. Some velomobile manufacturers offer their models as kits for self-assembly (Räderwerk Milan Mk2 + Milan SL, Beyss Go-One Evo K + Go-One Evo Ks, Alleweder A9/Sunrider Mk2 for instance) at reduced price.\n\nMany amateur velomobile builders are also recumbent bike riders. In more recent years many online velomobile groups, some of them focusing on specific regions, have appeared, on Facebook and on other platforms. Engaging actively in those communities is probably the best way to get informed and eventually own a velomobile.\n\n"}
{"id": "33405609", "url": "https://en.wikipedia.org/wiki?curid=33405609", "title": "Zerotracer", "text": "Zerotracer\n\nThe Zerotracer is a purpose built Electric vehicle for the Zero Emissions Race which went around the world in 2010/11. The vehicle seats two in a closed cabin, while its driving characteristics are more similar to a motorbike. The race started under UNEP patronage on 16 August 2010 in front of the Palace of Nations at Geneva where it ended on 24 February 2011 completing 80 days of travel - inspired obviously by Jules Verne's novel. \nOf the only three vehicles completing the race, Zerotracer accumulated the highest score of points to win.\n\nGeneva - Bruxelles - Berlin - Kiev - Moscow - Chelyabinsk - Almaty - Ürümqi - Shanghai - Vancouver - US West Coast - Cancún 2010 United Nations Climate Change Conference - Casablanca - Geneva.\n\nZerotracer covered the through route from Geneva to Shanghai without a break caused by a technical problem. \nThis is due to its fully developed concept deriving from the \"Monotracer\", a very similar concept using a petrol engine. Manufacturer Designwerk evaluates the production of the vehicle.\n"}
