{"id": "52002637", "url": "https://en.wikipedia.org/wiki?curid=52002637", "title": "Alata River", "text": "Alata River\n\nAlata River () is small river in Erdemli ilçe (district) of Mersin Province, Turkey\n\nThe headwaters are around the village of Toros (Küçüksorgun) in the Toros Mountains at an altitude of about . In the upper reaches it is usually called Sorgun Creek (). Flowing to south for , it empties into the Mediterranean sea at . Presently the estuary is in the urban fabric of Erdemli.\n\nAccording to the 1892 map drawn by Vital Cuinet and the 1897-salname (provencial annual) of Adana, Alata was the border line between Mersin and İçil sanjaks (which were later merged with). At the estuary there was an active commercial pier. According to researcher Mehmet Mazak, during 1880-1920 term, Alata pier was the only Mediterranean pier between Mersin and Taşucu piers. Presently there is only a small fishing pier.\n\nA dam on the Alata River under the supervision of Turkish State Hydraulic Works is under construction. The dam will serve for irrigation as well as energy production. The altitude of the rockfill dam is . 52410 decare of land will be irrigated and 37678 GW-hour of electric energy will be produced.\n"}
{"id": "35495563", "url": "https://en.wikipedia.org/wiki?curid=35495563", "title": "Aubanel Wind Project", "text": "Aubanel Wind Project\n\nThe Aubanel Wind Project is a complex of proposed wind farms in Tecate Municipality, Baja California, Mexico. The wind farms to be located near La Rumorosa, approximately east of San Diego and south of the United States – Mexico border. The project is developed by the Cannon Power Group and Gamesa Corporación Tecnológica. In total, about 1,000 MW of capacities will be installed.\n"}
{"id": "2201417", "url": "https://en.wikipedia.org/wiki?curid=2201417", "title": "Bulk density", "text": "Bulk density\n\nBulk density is a property of powders, granules, and other \"divided\" solids, especially used in reference to mineral components (soil, gravel), chemical substances, (pharmaceutical) ingredients, foodstuff, or any other masses of corpuscular or particulate matter. It is defined as the mass of many particles of the material divided by the total volume they occupy. The total volume includes particle volume, inter-particle void volume, and internal pore volume.\n\nBulk density is not an intrinsic property of a material; it can change depending on how the material is handled. For example, a powder poured into a cylinder will have a particular bulk density; if the cylinder is disturbed, the powder particles will move and usually settle closer together, resulting in a higher bulk density. For this reason, the bulk density of powders is usually reported both as \"freely settled\" (or \"poured\" density) and \"tapped\" density (where the tapped density refers to the bulk density of the powder after a specified compaction process, usually involving vibration of the container.)\n\nThe bulk density of soil depends greatly on the mineral make up of soil and the degree of compaction. The density of quartz is around 2.65 g/cm³ but the (dry) bulk density of a mineral soil is normally about half that density, between 1.0 and 1.6 g/cm³. Soils high in organics and some friable clay may have a bulk density well below 1 g/cm³. The reason why soils rich in soil organic carbon do have lower bulk density is due to the low density of organic materials. For instance peat soils have bulk densities of around 0.02 g/cm³\n\nBulk density of soil is usually determined from a core sample which is taken by driving a metal corer into the soil at the desired depth and horizon. This gives a soil sample of known total volume, formula_1. From this sample the wet bulk density and the dry bulk density can be determined.\n\nFor the wet bulk density (total bulk density) this sample is weighed, giving the mass formula_2. For the dry bulk density, the sample is oven dried and weighed, giving the mass of soil solids, formula_3. The relationship between these two masses is formula_4, where formula_5 is the mass of substances lost on oven drying (often, mostly water). The dry and wet bulk densities are calculated as\n\nDry bulk density = mass of soil/ volume as a whole\n\nWet bulk density = mass of soil plus liquids/ volume as a whole\n\nThe dry bulk density of a soil is inversely related to the porosity of the same soil: the more pore space in a soil the lower the value for bulk density. Bulk density of a region in the interior of the earth is also related to the seismic velocity of waves travelling through it: for P-waves, this has been quantified with Gardner's relation. The higher the density, the faster the velocity.\n\n\n"}
{"id": "36871501", "url": "https://en.wikipedia.org/wiki?curid=36871501", "title": "Cascaded Arc Plasma Source", "text": "Cascaded Arc Plasma Source\n\nThe cascaded arc is a wall-stabilized thermal electric arc discharge that produces a high density, low temperature plasma.\n\nThe cascaded arc source, developed at the Eindhoven University of Technology, is shown in the figure below. Compared to plasma sources in other linear plasma generators, this source can produce high-density argon and hydrogen plasmas (respectively 10 – 10 and 10 – 10 m) at a relatively low electron temperature (~1 eV). Due to the high collision frequency of the particles in the source, the plasma is in thermal equilibrium and reasonably homogeneous.\n\nThe cascaded arc consists of a gas inlet, three tungsten cathodes, cascaded plates, a nozzle and an anode. Via the gas inlet, the working gas -argon or hydrogen- can flow into the cathode chamber. The source is typically running at 0.5–3.0 slm ( –  particles per second) and a discharge current of 100–300 A. The cascaded plates in between the cathode and anode are electrically insulated from each other by 1 mm thick Boron nitride plates. The voltage of these plates is the floating potential. Both nozzle and anode are grounded.\n\n\n"}
{"id": "9221221", "url": "https://en.wikipedia.org/wiki?curid=9221221", "title": "Ceramic capacitor", "text": "Ceramic capacitor\n\nA ceramic capacitor is a fixed-value capacitor in which ceramic material acts as the dielectric. It is constructed of two or more alternating layers of ceramic and a metal layer acting as the electrodes. The composition of the ceramic material defines the electrical behavior and therefore applications. Ceramic capacitors are divided into two application classes:\n\nCeramic capacitors, especially multilayer ceramic capacitors (MLCCs), are the most produced and used capacitors in electronic equipment that incorporate approximately one trillion (10) pieces per year.\n\nCeramic capacitors of special shapes and styles are used as capacitors for RFI/EMI suppression, as feed-through capacitors and in larger dimensions as power capacitors for transmitters.\nSince the beginning of the study of electricity non conductive materials such as glass, porcelain, paper and mica have been used as insulators. These materials some decades later were also well-suited for further use as the dielectric for the first capacitors.\n\nEven in the early years of Marconi's wireless transmitting apparatus, porcelain capacitors were used for high voltage and high frequency application in the transmitters. On the receiver side, the smaller mica capacitors were used for resonant circuits. Mica dielectric capacitors were invented in 1909 by William Dubilier. Prior to World War II, mica was the most common dielectric for capacitors in the United States.\n\nMica is a natural material and not available in unlimited quantities. So in the mid-1920s the deficiency of mica in Germany and the experience in porcelain—a special class of ceramic—led in Germany to the first capacitors using ceramic as dielectric, founding a new family of ceramic capacitors. Paraelectric titanium dioxide (rutile) was used as the first ceramic dielectric because it had a linear temperature dependence of capacitance for temperature compensation of resonant circuits and can replace mica capacitors. 1926 these ceramic capacitors were produced in small quantities with increasing quantities in the 1940s. The style of these early ceramics was a disc with metallization on both sides contacted with tinned wires. This style predates the transistor and was used extensively in vacuum-tube equipment (e.g., radio receivers) from about 1930 through the 1950s.\n\nBut this paraelectric dielectric had relatively low permittivity so that only small capacitance values could be realized. The expanding market of radios in the 1930s and 1940s create a demand for higher capacitance values but below electrolytic capacitors for HF decoupling applications. Discovered in 1921, the ferroelectric ceramic material barium titanate with a permittivity in the range of 1,000, about ten times greater than titanium dioxide or mica, began to play a much larger role in electronic applications.\n\nThe higher permittivity resulted in much higher capacitance values, but this was coupled with relatively unstable electrical parameters. Therefore, these ceramic capacitors only could replace the commonly used mica capacitors for applications where stability was less important. Smaller dimensions, as compared to the mica capacitors, lower production costs and independence from mica availability accelerated their acceptance.\n\nThe fast-growing broadcasting industry after the Second World War drove deeper understanding of the crystallography, phase transitions and the chemical and mechanical optimization of the ceramic materials. Through the complex mixture of different basic materials, the electrical properties of ceramic capacitors can be precisely adjusted. To distinguish the electrical properties of ceramic capacitors, standardization defined several different application classes (Class 1, Class 2, Class 3). It is remarkable that the separate development during the War and the time afterwards in the US and the European market had led to different definitions of these classes (EIA vs IEC), and only recently (since 2010) has a worldwide harmonization to the IEC standardization taken place.\n\nThe typical style for ceramic capacitors beneath the disc (at that time called condensers) in radio applications at the time after the War from the 1950s through the 1970s was a ceramic tube covered with tin or silver on both the inside and outside surface. It included relatively long terminals forming, together with resistors and other components, a tangle of open circuit wiring.\n\nThe easy-to-mold ceramic material facilitated the development of special and large styles of ceramic capacitors for high-voltage, high-frequency (RF) and power applications\nWith the development of semiconductor technology in the 1950s, barrier layer capacitors, or IEC class 3/EIA class IV capacitors, were developed using doped ferroelectric ceramics. Because this doped material was not suitable to produce multilayers, they were replaced decades later by Y5V class 2 capacitors.\n\nThe early style of the ceramic disc capacitor could be more cheaply produced than the common ceramic tube capacitors in the 1950s and 1970s. An American company in the midst of the Apollo program, launched in 1961, pioneered the stacking of multiple discs to create a monolithic block. This \"multi-layer ceramic capacitor\" (MLCC) was compact and offered high-capacitance capacitors. The production of these capacitors using the tape casting and ceramic-electrode cofiring processes was a great manufacturing challenge. MLCCs expanded the range of applications to those requiring larger capacitance values in smaller cases. These ceramic chip capacitors were the driving force behind the conversion of electronic devices from through-hole mounting to surface-mount technology in the 1980s. Polarized electrolytic capacitors could be replaced by non-polarized ceramic capacitors, simplifying the mounting.\n\n, more than 10 MLCCs were manufactured each year. Along with the style of ceramic chip capacitors, ceramic disc capacitors are often used as safety capacitors in electromagnetic interference suppression applications. Besides these, large ceramic power capacitors for high voltage or high frequency transmitter applications are also to be found.\n\nNew developments in ceramic materials have been made with anti-ferroelectric ceramics. This material has a nonlinear antiferroelectric/ferroelectric phase change that allows increased energy storage with higher volumetric efficiency. They are used for energy storage (for example, in detonators).\n\nThe different ceramic materials used for ceramic capacitors, paraelectric or ferroelectric ceramics, influences the electrical characteristics of the capacitors. Using mixtures of paraelectric substances based on titanium dioxide results in very stable and linear behavior of the capacitance value within a specified temperature range and low losses at high frequencies. But these mixtures have a relatively low permittivity so that the capacitance values of these capacitors are relatively small.\n\nHigher capacitance values for ceramic capacitors can be attained by using mixtures of ferroelectric materials like barium titanate together with specific oxides. These dielectric materials have much higher permittivities, but at the same time their capacitance value are more or less nonlinear over the temperature range, and losses at high frequencies are much higher. These different electrical characteristics of ceramic capacitors requires to group them into \"application classes\". The definition of the application classes comes from the standardization. As of 2013, two sets of standards were in use, one from International Electrotechnical Commission (IEC) and the other from the now-defunct Electronic Industries Alliance (EIA).\n\nThe definitions of the application classes given in the two standards are different. The following table shows the different definitions of the application classes for ceramic capacitors:\n\nManufacturers, especially in the US, preferred Electronic Industries Alliance (EIA) standards. In many parts very similar to the IEC standard, the EIA RS-198 defines four application classes for ceramic capacitors.\n\nThe different class numbers within both standards are the reason for a lot of misunderstandings interpreting the class descriptions in the datasheets of many manufacturers. The EIA ceased operations on February 11, 2011, but the former sectors continue to serve international standardization organizations.\n\nIn the following, the definitions of the IEC standard will be preferred and in important cases compared with the definitions of the EIA standard.\n\nClass 1 ceramic capacitors are accurate, temperature-compensating capacitors. They offer the most stable voltage, temperature, and to some extent, frequency. They have the lowest losses and therefore are especially suited for resonant circuit applications where stability is essential or where a precisely defined temperature coefficient is required, for example in compensating temperature effects for a circuit. The basic materials of class 1 ceramic capacitors are composed of a mixture of finely ground granules of paraelectric materials such as Titanium dioxide (), modified by additives of Zinc, Zirconium, Niobium, Magnesium, Tantalum, Cobalt and Strontium, which are necessary to achieve the capacitor's desired linear characteristics.\n\nThe general capacitance temperature behavior of class 1 capacitors depends on the basic paraelectric material, for example . The additives of the chemical composition are used to adjust precisely the desired temperature characteristic.\nClass 1 ceramic capacitors have the lowest volumetric efficiency among ceramic capacitors. This is the result of the relatively low permittivity (6 to 200) of the paraelectric materials. Therefore, class 1 capacitors have capacitance values in the lower range.\n\nClass 1 capacitors have a temperature coefficient that is typically fairly linear with temperature. These capacitors have very low electrical losses with a dissipation factor of approximately 0.15%. They undergo no significant aging processes and the capacitance value is nearly independent of the applied voltage. These characteristics allow applications for high Q filters, in resonant circuits and oscillators (for example, in phase-locked loop circuits).\n\nThe EIA RS-198 standard codes ceramic class 1 capacitors with a three character code that indicates temperature coefficient. The first letter gives the significant figure of the change in capacitance over temperature (temperature coefficient α) in ppm/K. The second character gives the multiplier of the temperature coefficient. The third letter gives the maximum tolerance from that in ppm/K. All ratings are from 25 to 85 °C:\n\nIn addition to the EIA code, the temperature coefficient of the capacitance dependence of class 1 ceramic capacitors is commonly expressed in ceramic names like \"NP0\", \"N220\" etc. These names include the temperature coefficient (α). In the IEC/EN 60384-8/21 standard, the temperature coefficient and tolerance are replaced by a two digit letter code (see table) in which the corresponding EIA code is added.\n\nFor instance, an \"NP0\" capacitor with EIA code \"C0G\" will have 0 drift, with a tolerance of ±30 ppm/K, while an \"N1500\" with the code \"P3K\" will have −1500 ppm/K drift, with a maximum tolerance of ±250 ppm/°C. Note that the IEC and EIA capacitor codes are industry capacitor codes and not the same as military capacitor codes.\n\nClass 1 capacitors include capacitors with different temperature coefficients α. Especially, NP0/CG/C0G capacitors with an α ±0•10 /K and an α tolerance of 30 ppm are technically of great interest. These capacitors have a capacitance variation dC/C of ±0.54% within the temperature range −55 to +125 °C. This enables accurate frequency response over a wide temperature range (in, for example, resonant circuits). The other materials with their special temperature behavior are used to compensate a counter temperature run of parallel connected components like coils in oscillator circuits. Class 1 capacitors exhibit very small tolerances of the rated capacitance.\n\nClass 2 ceramic capacitors have a dielectric with a high permittivity and therefore a better volumetric efficiency than class 1 capacitors, but lower accuracy and stability. The ceramic dielectric is characterized by a nonlinear change of capacitance over the temperature range. The capacitance value also depends on the applied voltage. They are suitable for bypass, coupling and decoupling applications or for frequency discriminating circuits where low losses and high stability of capacitance are less important. They typically exhibit microphony.\n\nClass 2 capacitors are made of ferroelectric materials such as barium titanate () and suitable additives such as aluminium silicate, magnesium silicate and aluminium oxide. These ceramics have high to very high permittivity (200 to 14,000), which depends on the field strength. Hence the capacitance value of class 2 capacitors is nonlinear. It depends on temperature and voltage applied. Additionally class 2 capacitors age over time.\n\nHowever, the high permittivity supports high capacitance values in small devices. Class 2 capacitors are significantly smaller than class 1 devices at the equal rated capacitance and voltage. They are suitable for applications that require the capacitor to maintain only a minimum value of capacitance, for example, buffering and filtering in power supplies and coupling and decoupling of electric signals.\n\nClass 2 capacitors are labeled according to the change in capacitance over the temperature range. The most widely used classification is based on the EIA RS-198 standard and uses a three-digit code. The first character is a letter that gives the low-end operating temperature. The second gives the high-end operating temperature, and the final character gives capacitance change over that temperature range:\n\nFor instance, a Z5U capacitor will operate from +10 °C to +85 °C with a capacitance change of at most +22% to −56%. An X7R capacitor will operate from −55 °C to +125 °C with a capacitance change of at most ±15%.\n\nSome commonly used class 2 ceramic capacitor materials are listed below:\n\nThe IEC/EN 60384 -9/22 standard uses another two-digit-code.\n\nIn most cases it is possible to translate the EIA code into the IEC/EN code. Slight translation errors occur, but normally are tolerable.\n\nBecause class 2 ceramic capacitors have lower capacitance accuracy and stability, they require higher tolerance.\n\nFor military types the class 2 dielectrics specify temperature characteristic (TC) but not temperature-voltage characteristic (TVC). Similar to X7R, military type BX cannot vary more than 15% over temperature, and in addition, must remain within +15%/-25 % at maximum rated voltage. Type BR has a TVC limit of +15%/-40%.\n\nClass 3 barrier layer or semiconductive ceramic capacitors have very high permittivity, up to 50,000 and therefore a better volumetric efficiency than class 2 capacitors. However, these capacitors have worse electrical characteristics, including lower accuracy and stability. The dielectric is characterized by very high nonlinear change of capacitance over the temperature range. The capacitance value additionally depends on the voltage applied. As well, they have very high losses and age over time.\n\nBarrier layer ceramic capacitors are made of doped ferroelectric materials such as barium titanate (). As this ceramic technology improved in the mid-1980s, barrier layer capacitors became available in values of up to 100 µF, and at that time it seemed that they could substitute for smaller electrolytic capacitors.\n\nBecause it is not possible to build multilayer capacitors with this material, only leaded single layer types are offered in the market.\n\nCeramic capacitors are composed of a mixture of finely ground granules of paraelectric or ferroelectric materials, appropriately mixed with other materials to achieve the desired characteristics. From these powder mixtures, the ceramic is sintered at high temperatures. The ceramic forms the dielectric and serves as a carrier for the metallic electrodes. The minimum thickness of the dielectric layer, which today (2013) for low voltage capacitors is in the size range of 0.5 micrometers is limited downwards by the grain size of the ceramic powder. The thickness of the dielectric for capacitors with higher voltages is determined by the dielectric strength of the desired capacitor.\n\nThe electrodes of the capacitor are deposited on the ceramic layer by metallization. For MLCCs alternating metallized ceramic layers are stacked one above the other. The outstanding metallization of the electrodes at both sides of the body are connected with the contacting terminal. A lacquer or ceramic coating protects the capacitor against moisture and other ambient influences.\n\nCeramic capacitors come in various shapes and styles. Some of the most common are:\n\nAn MLCC consists of a number of individual capacitors stacked together in parallel and contacted via the terminal surfaces. The starting material for all MLCC chips is a mixture of finely ground granules of paraelectric or ferroelectric raw materials, modified by accurately determined additives. These powdered materials are mixed homogeneously. The composition of the mixture and the size of the powder particles, as small as 10 nm, reflect the manufacturer's expertise.\n\nA thin ceramic foil is cast from a suspension of the powder with a suitable binder. This foil is rolled up for transport. Unrolled again, it is cut into equal-sized sheets, which are screen printed with a metal paste. These sheets become the electrodes. In an automated process, these sheets are stacked in the required number of layers and solidified by pressure. Besides the relative permittivity, the size and number of layers determines the later capacitance value. The electrodes are stacked in an alternating arrangement slightly offset from the adjoining layers so that they each can later be connected on the offset side, one left, one right. The layered stack is pressed and then cut into individual components. High mechanical precision is required, for example, to produce a 500 or more layer stack of size \"0201\" (0.5 mm × 0.3 mm).\n\nAfter cutting, the binder is burnt out of the stack. This is followed by sintering at temperatures between 1,200 and 1,450 °C producing the final, mainly crystalline, structure. This burning process creates the desired dielectric properties. Burning is followed by cleaning and then metallization of both end surfaces. Through the metallization, the ends and the inner electrodes are connected in parallel and the capacitor gets its terminals. Finally a 100% measuring of the electrical values will be done and the taping for automated processing in a manufacturing device are performed.\n\nThe capacitance formula (\"C\") of a MLCC capacitor is based on the formula for a plate capacitor enhanced with the number of layers:\nformula_1 \nwhere \"ε\" stands for dielectric permittivity; \"A\" for electrode surface area; n for the number of layers; and \"d\" for the distance between the electrodes.\n\nA thinner dielectric or a larger electrode area each increase the capacitance value, as will a dielectric material of higher permittivity.\n\nWith the progressive miniaturization of digital electronics in recent decades, the components on the periphery of the integrated logic circuits have been scaled down as well. Shrinking an MLCC involves reducing the dielectric thickness and increasing the number of layers. Both options require huge efforts and are connected with a lot of expertise.\n\nIn 1995 the minimum thickness of the dielectric was 4 µm. By 2005 some manufacturers produced MLCC chips with layer thicknesses of 1 µm. , the minimum thickness is about 0.5 µm. The field strength in the dielectric increased to 35 V/µm.\n\nThe size reduction of these capacitors is achieved reducing powder grain size, the assumption to make the ceramic layers thinner. In addition, the manufacturing process became more precisely controlled, so that more and more layers can be stacked.\n\nBetween 1995 and 2005, the capacitance of a Y5V MLCC capacitor of size 1206 was increased from 4.7 μF to 100 μF. Meanwhile, (2013) a lot of producers can deliver class 2 MLCC capacitors with a capacitance value of 100 μF in the chip-size 0805.\n\nMLCCs don’t have leads, and as a result they are usually smaller than their counterparts with leads. They don’t require through-hole access in a PCB to mount and are designed to be handled by machines rather than by humans. As a result, surface-mount components like MLCCs are typically cheaper.\n\nMLCCs are manufactured in standardized shapes and sizes for comparable handling. Because the early standardization was dominated by American EIA standards the dimensions of the MLCC chips were standardized by EIA in units of inches. A rectangular chip with the dimensions of 0.06 inch length and 0.03 inch width is coded as \"0603\". This code is international and in common use. JEDEC (IEC/EN), devised a second, metric code. The EIA code and the metric equivalent of the common sizes of multilayer ceramic chip capacitors, and the dimensions in mm are shown in the following table. Missing from the table is the measure of the height \"H\". This is generally not listed, because the height of MLCC chips depends on the number of layers and thus on the capacitance. Normally, however, the height H does not exceed the width W.\n\nA particular problem in the production of multilayer ceramic chip capacitors at the end of the 1990s was a strong price increase of the metals used for the electrodes and terminals. The original choices were the non-oxidizable noble metals silver and palladium which can withstand high sintering temperatures of 1200 to 1400 °C. They were called \"NME\" (Noble Metal Electrode) and offered very good electrical properties to class 2 capacitors. The price increase of these metals greatly increased capacitor prices.\n\nCost pressures led to the development of BME (Base Metal Electrodes) using the much cheaper materials nickel and copper.\n\nBut BME metallization produced different electrical properties; for example, the voltage dependence of X7R capacitors increased significantly (see picture). Even the loss factor and the impedance behavior of class 2 ceramic capacitors were lessened by BME metallization.\n\nFor class 2 ceramic capacitors, because of their use in applications where it is usually not very important for the stability of the electrical properties, these negative changes, for cost reasons, were finally accepted by the market, while the NME metallization was maintained in the class 1 ceramic capacitors.\n\nCapacitance of MLCC chips depends on the dielectric, the size and the required voltage (rated voltage). Capacitance values start at about 1pF. The maximum capacitance value is determined by the production technique. For X7R that is 47 µF, for Y5V: 100 µF.\n\nThe picture right shows the maximum capacitance for class 1 and class 2 multilayer ceramic chip capacitors. The following two tables, for ceramics NP0/C0G and X7R each, list for each common case size the maximum available capacitance value and rated voltage of the leading manufacturers Murata, TDK, KEMET, AVX. (Status April 2017)\n\nIn the region of its resonance frequency, a capacitor has the best decoupling properties for noise or electromagnetic interference. The resonance frequency of a capacitor is determined by the inductance of the component. The inductive parts of a capacitor are summarized in the equivalent series inductance, or ESL. (Note that L is the electrical symbol for inductance.) The smaller the inductance, the higher the resonance frequency.\n\nBecause, especially in digital signal processing, switching frequencies have continued to rise, the demand for high frequency decoupling or filter capacitors increases. With a simple design change the ESL of a MLCC chip can be reduced. Therefore, the stacked electrodes are connected on the longitudinal side with the connecting terminations. This reduces the distance that the charge carriers flow over the electrodes, which reduces inductance of the component.\n\nFor example, the result for X7R with 0.1 µF in the size of 0805, with a resonance frequency of about 16 MHz increases to about 22 MHz if the chip has a 0508-size with terminations at the longitudinal side.\n\nAnother possibility is to form the device as an array of capacitors. Here, several individual capacitors are built in a common housing. Connecting them in parallel, the resulting ESL as well as ESR values of the components are reduced.\n\nA standard multi-layer ceramic capacitor has many opposing electrode layers stacked inside connected with two outer terminations. The X2Y ceramic chip capacitor however is a 4 terminal chip device. It is constructed like a standard two-terminal MLCC out of the stacked ceramic layers with an additional third set of shield electrodes incorporated in the chip. These shield electrodes surround each existing electrode within the stack of the capacitor plates and are low ohmic contacted with two additional side terminations across to the capacitor terminations. The X2Y construction results in a three-node capacitive circuit that provides simultaneous line-to-line and line-to-ground filtering.\n\nCapable of replacing 2 or more conventional devices, the X2Y ceramic capacitors are ideal for high frequency filtering or noise suppression of supply voltages in digital circuits, and can prove invaluable in meeting stringent EMC demands in dc motors, in automotive, audio, sensor and other applications.\n\nThe X2Y footprint results in lower mounted inductance. This is particularly of interest for use in high-speed digital circuits with clock rates of several 100 MHz and upwards. There the decoupling of the individual supply voltages on the circuit board is difficult to realize due to parasitic inductances of the supply lines. A standard solution with conventional ceramic capacitors requires the parallel use of many conventional MLCC chips with different capacitance values. Here X2Y capacitors are able to replace up to five equal-sized ceramic capacitors on the PCB. However, this particular type of ceramic capacitor is patented, so these components are still comparatively expensive.\n\nAn alternative to X2Y capacitors may be a three-terminal capacitor.\n\nCeramic is on the one hand a very solid material; on the other hand, it breaks even at relatively low mechanical stress. MLCC chips as surface-mounted components are susceptible to flexing stresses since they are mounted directly on the substrate. They are stuck between soldered joints on the printed circuit board (PCB), and are often exposed to mechanical stresses, for example, if vibration or a bump impacts the circuit board. They are also more sensitive to thermal stresses than leaded components. Excess solder fillet height can multiply these stresses and cause chip cracking. Of all influencing factors, causing a mechanical shock stress to the PCB proved to be the most critical one. The reason is that forces induced by those kinds of stresses are more or less transmitted undampened to the components via the PCB and solder joints.\n\nThe capability of MLCC chips to withstand mechanical stress is tested by a so-called substrate bending test. Here, a test PCB with a soldered MLCC chip between two support points is bent by a punch at a path length of 1 to 3mm. The path length depends on the requirements that come out from the application. If no crack appears, the capacitors are able to withstand the wanted requirements. Cracks are usually detected by a short circuit or a change of the capacitance value in the deflected state.\n\nThe bending strength of the MLCC chip differs by the property of the ceramic, the size of the chip and the design of the capacitors. Without any special design features, NP0/C0G class 1 ceramic MLCC chips reach a typical bending strength of 2mm while larger types of X7R, Y5V class 2 ceramic chips achieved only a bending strength of approximately 1mm. Smaller chips, such as the size of 0402, reached in all types of ceramics larger bending strength values.\n\nWith special design features, particularly by special design of the electrodes and the terminations, the bending strength can be improved. For example, an internal short circuit arises by the contact of two electrodes with opposite polarity, which will be produced at the break of the ceramic in the region of the terminations. This can be prevented when the overlap surfaces of the electrodes are reduced. This is achieved e.g. by an \"Open Mode Design\" (OMD). Here a break in the region of the terminations only reduce the capacitance value a little bit (AVX, KEMET).\n\nWith a similar construction called \"Floating Electrode Design\" (FED) or \"Multi-layer Serial Capacitors\" (MLSC), also, only capacitance reduction results if parts of the capacitor body break. This construction works with floating electrodes without any conductive connection to the termination. A break doesn’t lead to a short, only to capacitance reduction.\nHowever, both structures lead to larger designs with respect to a standard MLCC version with the same capacitance value.\n\nThe same volume with respect to standard MLCCs is achieved by the introduction of a flexible intermediate layer of a conductive polymer between the electrodes and the termination called \"Flexible Terminations\" (FT-Cap) or \"Soft Terminations\". In this construction, the rigid metallic soldering connection can move against the flexible polymer layer, and thus can absorb the bending forces, without resulting in a break in the ceramic.\n\nSuppression capacitors are effective interference reduction components because their electrical impedance decreases with increasing frequency, so that at higher frequencies they short circuit electrical noise and transients between the lines, or to ground. They therefore prevent equipment and machinery (including motors, inverters, and electronic ballasts, as well as solid-state relay snubbers and spark quenchers) from sending and receiving electromagnetic and radio frequency interference as well as transients in across-the-line (X capacitors) and line-to-ground (Y capacitors) connections. X capacitors effectively absorb symmetrical, balanced, or differential interference. Y capacitors are connected in a line bypass between a line phase and a point of zero potential, to absorb asymmetrical, unbalanced, or common-mode interference.\n<gallery widths=\"280\" heights=\"140\" caption=\"RFI/EMI suppression with X- and Y-capacitors for equipment without and with additional safety insulation\">\nFile:Safety caps-Appliance Class I.svg|Appliance Class I capacitor connection\nFile:Safety caps-Appliance Class II.svg|Appliance Class II capacitor connection\n</gallery>\nEMI/RFI suppression capacitors are designed so that any remaining interference or electrical noise does not exceed the limits of EMC directive EN 50081. Suppression components are connected directly to mains voltage for 10 to 20 years or more and are therefore exposed to potentially damaging overvoltages and transients. For this reason, suppression capacitors must comply with the safety and non-flammability requirements of international safety standards such as\nRFI capacitors that fulfill all specified requirements are imprinted with the certification mark of various national safety standards agencies. For power line applications, special requirements are placed on the non-flammability of the coating and the epoxy resin impregnating or coating the capacitor body. To receive safety approvals, X and Y powerline-rated capacitors are destructively tested to the point of failure. Even when exposed to large overvoltage surges, these safety-rated capacitors must fail in a fail-safe manner that does not endanger personnel or property.\n\nAlthough the materials used for large power ceramic capacitors mostly are very similar to those used for smaller ones, ceramic capacitors with high to very high power or voltage ratings for applications in power systems, transmitters and electrical installations are often classified separately, for historical reasons. The standardization of ceramic capacitors for lower power is oriented toward electrical and mechanical parameters as components for use in electronic equipment. The standardization of power capacitors, contrary to that, is strongly focused on protecting personnel and equipment, given by the local regulating authority.\n\nAs modern electronic equipment gained the ability to handle power levels that were previously the exclusive domain of \"electrical power\" components, the distinction between the \"electronic\" and \"electrical\" power ratings has become less distinct. In the past, the boundary between these two families was approximately at a reactive power of 200 volt-amps, but modern power electronics can handle increasing amounts of power.\n\nPower ceramic capacitors are mostly specified for much higher than 200 volt-amps. The great plasticity of ceramic raw material and the high dielectric strength of ceramics deliver solutions for many applications and are the reasons for the enormous diversity of styles within the family of power ceramic capacitors. These power capacitors have been on the market for decades. They are produced according to the requirements as class 1 power ceramic capacitors with high stability and low losses or class 2 power ceramic capacitors with high volumetric efficiency.\n\nClass 1 power ceramic capacitors are used for resonant circuit application in transmitter stations. Class 2 power ceramic capacitors are used for circuit breakers, for power distribution lines, for high voltage power supplies in laser-applications, for induction furnaces and in voltage-doubling circuits. Power ceramic capacitors can be supplied with high rated voltages in the range of 2 kV up to 100 kV.\n\nThe dimensions of these power ceramic capacitors can be very large. At high power applications the losses of these capacitors can generate a lot of heat. For this reason some special styles of power ceramic capacitors have pipes for water-cooling.\n\nAll electrical characteristics of ceramic capacitors can be defined and specified by a series equivalent circuit composed out of an idealized capacitance and additional electrical components, which model all losses and inductive parameters of a capacitor. In this series-equivalent circuit the electrical characteristics of a capacitors is defined by\n\n\nThe use of a series equivalent circuit instead of a parallel equivalent circuit is defined in IEC/EN 60384-1.\n\nThe \"rated capacitance\" C or \"nominal capacitance\" C is the value for which the capacitor has been designed. The actual capacitance depends on the measuring frequency and the ambient temperature. Standardized conditions for capacitors are a low-voltage AC measuring method at a temperature of 20 °C with frequencies of\n\n\nCapacitors are available in different, geometrically increasing preferred values as specified in the E series standards specified in IEC/EN 60063. According to the number of values per decade, these were called the E3, E6, E12, E24, etc. series. The units used to specify capacitor values includes everything from picofarad (pF), nanofarad (nF), microfarad (µF) and farad (F).\n\nThe percentage of allowed deviation of the capacitance from the rated value is called capacitance tolerance. The actual capacitance value must be within the tolerance limits, or the capacitor is out of specification. For abbreviated marking in tight spaces, a letter code for each tolerance is specified in IEC/EN 60062.\n\nThe required capacitance tolerance is determined by the particular application. The narrow tolerances of E24 to E96 will be used for high-quality class 1 capacitors in circuits such as precision oscillators and timers. On the other hand, for general applications such as non-critical filtering or coupling circuits, for class 2 capacitors the tolerance series E12 down to E3 are sufficient.\n\nCapacitance of ceramic capacitors varies with temperature. The different dielectrics of the many capacitor types show great differences in temperature dependence. The temperature coefficient is expressed in parts per million (ppm) per degree Celsius for class 1 ceramic capacitors or in percent (%) over the total temperature range for class 2 capacitors.\n\nMost discrete capacitor types have greater or smaller capacitance changes with increasing frequencies. The dielectric strength of class 2 ceramic and plastic film diminishes with rising frequency. Therefore, their capacitance value decreases with increasing frequency. This phenomenon is related to the dielectric relaxation in which the time constant of the electrical dipoles is the reason for the frequency dependence of permittivity. The graph on the right hand side shows typical frequency behavior for class 2 vs class 1 capacitors.\n\nCapacitance of ceramic capacitors may also change with applied voltage. This effect is more prevalent in class 2 ceramic capacitors. The ferroelectric material depends on the applied voltage. The higher the applied voltage, the lower the permittivity. Capacitance measured or applied with higher voltage can drop to values of -80% of the value measured with the standardized measuring voltage of 0.5 or 1.0 V. This behavior is a small source of nonlinearity in low-distortion filters and other analog applications. In audio applications this can be the reason for harmonic distortions.\n\nThe voltage dependence of capacitance in the both diagrams above shows curves from ceramic capacitors with NME metallization. For capacitors with BME metallization the voltage dependence of capacitance increased significantly.\n\nFor most capacitors, a physically conditioned dielectric strength or a breakdown voltage usually could be specified for each dielectric material and thickness. This is not possible with ceramic capacitors. The breakdown voltage of a ceramic dielectric layer may vary depending on the electrode material and the sintering conditions of the ceramic up to a factor of 10. A high degree of precision and control of process parameters is necessary to keep the scattering of electrical properties for today's very thin ceramic layers within specified limits.\n\nThe voltage proof of ceramic capacitors is specified as rated voltage (UR). This is the maximum DC voltage that may be continuously applied to the capacitor up to the upper temperature limit. This guaranteed voltage proof is tested according to the voltages shown in the adjacent table.\n\nFurthermore, in periodic life time tests (endurance tests) the voltage proof of ceramic capacitors is tested with increased test voltage (120 to 150% of U) to ensure safe construction.\n\nThe frequency dependent AC resistance of a capacitor is called impedance formula_2 and is a complex ratio of voltage to current in an AC circuit. Impedance extends the concept of Ohm's law to AC circuits, and possesses both magnitude and phase at a particular frequency, unlike resistance, which has only magnitude.\n\nImpedance is a measure of the ability of the capacitor to pass alternating currents. In this sense impedance can be used like Ohms law\n\nto calculate either the peak or the effective value of the current or the voltage.\n\nAs shown in the series-equivalent circuit of a capacitor, the real-world component includes an ideal capacitor formula_4, an inductance formula_5 and a resistor formula_6.\n\nTo calculate the impedance formula_2 the resistance and the both reactances have to be added geometrically\nIn the special case of resonance, in which both reactive resistances have the same value (formula_9), then the impedance will only be determined by formula_10.\nData sheets of ceramic capacitors only specify the impedance magnitude formula_2. The typical impedance curve shows that with increasing frequency, impedance decreases, down to a minimum. The lower the impedance, the more easily alternating currents can pass through the capacitor. At the minimum point of the curve, the point of resonance, where X has the same value as X, the capacitor exhibits its lowest impedance value. Here only the ohmic ESR determines the impedance. With frequencies above the resonance, impedance increases again due to the ESL.\n\nThe summarized losses in ceramic capacitors are ohmic AC losses. DC losses are specified as \"leakage current\" or \"insulating resistance\" and are negligible for an AC specification. These AC losses are nonlinear and may depend on frequency, temperature, age, and for some special types, on humidity. The losses result from two physical conditions,\n\nThe largest share of these losses in larger capacitors is usually the frequency dependent ohmic dielectric losses. Regarding the IEC 60384-1 standard, the ohmic losses of capacitors are measured at the same frequency used to measure capacitance. These are:\n\n\nResults of the summarized resistive losses of a capacitor may be specified either as equivalent series resistance (ESR), as dissipation factor (DF, tan δ), or as quality factor (Q), depending on the application requirements.\n\nClass 2 capacitors are mostly specified with the dissipation factor, tan δ. The dissipation factor is determined as the tangent of the reactance formula_12 - formula_13 and the ESR, and can be shown as the angle δ between the imaginary and impedance axes in the above vector diagram, see paragraph \"Impedance\".\n\nIf the inductance formula_14 is small, the dissipation factor can be approximated as:\n\nClass 1 capacitors with very low losses are specified with a dissipation factor and often with a quality factor (Q). The quality factor is defined as the reciprocal of the dissipation factor.\n\nThe Q factor represents the effect of electrical resistance, and characterizes a resonator's bandwidth formula_17 relative to its center or resonant frequency formula_18. A high Q value is a mark of the quality of the resonance for resonant circuits.\n\nIn accordance with IEC 60384-8/-21/-9/-22 ceramic capacitors may not exceed the following dissipation factors:\n\nThe ohmic losses of ceramic capacitors are frequency, temperature and voltage dependent. Additionally, class 2 capacitor measurements change because of aging. Different ceramic materials have differing losses over the temperature range and the operating frequency. The changes in class 1 capacitors are in the single-digit range while class 2 capacitors have much higher changes.\n\nElectrical resonance occurs in an ceramic capacitor at a particular resonance frequency where the imaginary parts of the capacitor impedance and admittances cancel each other.\nThis frequency where X is as high as X is called the self-resonant frequency and can be calculated with:\n\nformula_19\n\nwhere ω = 2π\"f\", in which \"f\" is the resonance frequency in Hertz, \"L\" is the inductance in henries, and \"C\" is the capacitance in farads.\n\nThe smaller the capacitance C and the inductance L the higher is the resonance frequency.\nThe self-resonant frequency is the lowest frequency at which impedance passes through a minimum. For any AC application the self-resonant frequency is the highest frequency at which a capacitor can be used as a capacitive component. At frequencies above the resonance, the impedance increases again due to ESL: the capacitor becomes an inductor with inductance equal to capacitor's ESL, and resistance equal to ESR at the given frequency.\n\nESL in industrial capacitors is mainly caused by the leads and internal connections used to connect the plates to the outside world. Larger capacitors tend to higher ESL than small ones, because the distances to the plate are longer and every millimeter increases inductance.\n\nCeramic capacitors, which are available in the range of very small capacitance values (pF and higher) are already out of their smaller capacitance values suitable for higher frequencies up to several 100 MHz (see formula above).\nDue to the absence of leads and proximity to the electrodes, MLCC chips have significantly lower parasitic inductance than f. e. leaded types, which makes them suitable for higher frequency applications. A further reduction of parasitic inductance is achieved by contacting the electrodes on the longitudinal side of the chip instead of the lateral side.\n\nSample self-resonant frequencies for one set of NP0/C0G and one set of X7R ceramic capacitors are:\n\nNote that X7Rs have better frequency response than C0Gs. It makes sense, however, since class 2 capacitors are much smaller than class 1, so they ought to have lower parasitic inductance.\n\nIn ferroelectric class 2 ceramic capacitors capacitance decreases over time. This behavior is called \"aging\". Aging occurs in ferroelectric dielectrics, where domains of polarization in the dielectric contribute to total polarization. Degradation of the polarized domains in the dielectric decreases permittivity over time so that the capacitance of class 2 ceramic capacitors decreases as the component ages.\nThe aging follows a logarithmic law. This law defines the decrease of capacitance as a percentage for a time decade after the soldering recovery time at a defined temperature, for example, in the period from 1 to 10 hours at 20 °C. As the law is logarithmic, the percentage loss of capacitance will twice between 1 h and 100 h and 3 times between 1 h and 1000 h and so on. So aging is fastest near the beginning, and the capacitance value effectively stabilizes over time.\n\nThe rate of aging of class 2 capacitors mainly depends on the materials used. A rule of thumb is, the higher the temperature dependence of the ceramic, the higher the aging percentage. The typical aging of X7R ceramic capacitors is about 2.5% per decade The aging rate of Z5U ceramic capacitors is significantly higher and can be up to 7% per decade.\n\nThe aging process of class 2 capacitors may be reversed by heating the component above the Curie point.\n\nClass 1 capacitors do not experience ferroelectric aging like Class 2's. But environmental influences such as higher temperature, high humidity and mechanical stress can, over a longer period of time, lead to a small irreversible decline in capacitance, sometimes also called aging. The change of capacitance for P 100 and N 470 Class 1's is lower than 1%, for capacitors with N 750 to N 1500 ceramics it is ≤ 2%.\n\nThe resistance of the dielectric is never infinite, leading to some level of DC \"leakage current\", which contributes to self-discharge. For ceramic capacitors this resistance, placed in parallel with the capacitor in the series-equivalent circuit of capacitors, is called \"insulation resistance R\". The insulation resistance must not be confused with the outer isolation with respect to the environment.\n\nThe rate of self-discharge with decreasing capacitor voltage follows the formula\nWith the stored DC voltage formula_21 and the self-discharge constant\nThat means, after formula_23 capacitor voltage formula_21 dropped to 37% of the initial value.\n\nThe insulation resistance given in the unit MΩ (10 Ohm) as well as the self-discharge constant in seconds is an important parameter for the quality of the dielectric insulation. These time values are important, for example, when a capacitor is used as timing component for relays or for storing a voltage value as in a sample and hold circuits or operational amplifiers.\n\nIn accordance with the applicable standards, Class 1 ceramic capacitors have an R ≥ 10,000 MΩ for capacitors with C ≤ 10 nF or τ ≥ 100 s for capacitors with C > 10 nF. Class 2 ceramic capacitors have an R ≥ 4,000 MΩ for capacitors with C ≤ 25 nF or τ ≥ 100 s for capacitors with C > 25 nF.\n\nInsulation resistance and thus the self-discharge time rate are temperature dependent and decrease with increasing temperature at about 1 MΩ per 60 °C.\n\nDielectric absorption is the name given to the effect by which a capacitor, which has been charged for a long time, discharges only incompletely. Although an ideal capacitor remains at zero volts after discharge, real capacitors will develop a small voltage coming from time-delayed dipole discharging, a phenomenon that is also called dielectric relaxation, \"soakage\" or \"battery action\".\n\nIn many applications of capacitors dielectric absorption is not a problem but in some applications, such as long-time-constant integrators, sample-and-hold circuits, switched-capacitor analog-to-digital converters and very low-distortion filters, it is important that the capacitor does not recover a residual charge after full discharge, and capacitors with low absorption are specified. The voltage at the terminals generated by dielectric absorption may in some cases possibly cause problems in the function of an electronic circuit or can be a safety risk to personnel. In order to prevent shocks, most very large capacitors like power capacitors are shipped with shorting wires that are removed before use.\n\nAll class 2 ceramic capacitors using ferroelectric ceramics exhibit piezoelectricity, and have a piezoelectric effect called microphonics, microphony or in audio applications squealing. Microphony describes the phenomenon wherein electronic components transform mechanical vibrations into an electrical signal which in many cases is undesired noise. Sensitive electronic preamplifiers generally use class 1 capacitors to avoid this effect.\n\nIn the reverse microphonic effect, the varying electric field between the capacitor plates exerts a physical force, moving them as a speaker. High current impulse loads or high ripple currents can generate audible acoustic sound coming from the capacitor, but discharges the capacitor and stresses the dielectric.\n\nCeramic capacitors may experience changes to their electrical parameters due to soldering stress. The heat of the solder bath, especially for SMD styles, can cause changes of contact resistance between terminals and electrodes. For ferroelectric class 2 ceramic capacitors, the soldering temperature is above the Curie point. The polarized domains in the dielectric are going back and the aging process of class 2 ceramic capacitors is starting again.\n\nHence after soldering a recovery time of approximately 24 hours is necessary. After recovery some electrical parameters like capacitance value, ESR, leakage currents are changed irreversibly. The changes are in the lower percentage range depending on the style of capacitor.\n\nThe standardization for all electrical, electronic components and related technologies follows the rules given by the International Electrotechnical Commission (IEC), a non-profit, non-governmental international standards organization.\n\nThe definition of the characteristics and the procedure of the test methods for capacitors for use in electronic equipment are set out in the generic specification:\n\nThe tests and requirements to be met by ceramic capacitors for use in electronic equipment for approval as standardized types are set out in the following sectional specifications:\n\n\nMultilayer ceramic capacitors are increasingly used to replace tantalum and low capacitance aluminium electrolytic capacitors in applications such as bypass or high frequency switched-mode power supplies as their cost, reliability and size becomes competitive. In many applications, their low ESR allows the use of a lower nominal capacitance value.\n\nFor features and disadvantages of ceramic capacitors see main article Types of capacitor#Capacitor features comparisons\n\nIf space permits ceramic capacitors, like most other electronic components, have imprinted markings to indicate the manufacturer, the type, their electrical and thermal characteristics and their date of manufacture. In the ideal case, if they are large enough, the capacitor will be marked with:\n\nSmaller capacitors use a shorthand notation, to display all the relevant information in the limited space. The most commonly used format is: XYZ J/K/M VOLTS V, where XYZ represents the capacitance (calculated as XY × 10 pF), the letters J, K or M indicate the tolerance (±5%, ±10% and ±20% respectively) and VOLTS V represents the working voltage.\n\n\nCapacitance, tolerance and date of manufacture can be identified with a short code according to IEC/EN 60062. Examples of short-marking of the rated capacitance (microfarads):\n\nThe date of manufacture is often printed in accordance with international standards.\nYear code: \"R\" = 2003, \"S\"= 2004, \"T\" = 2005, \"U\" = 2006, \"V\" = 2007, \"W\" = 2008, \"X\" = 2009, \"A\" = 2010, \"B\" = 2011, \"C\" = 2012, \"D\" = 2013 etc.\n\nMonth code: \"1\" to \"9\" = Jan. to Sept., \"O\" = October, \"N\" = November, \"D\" = December\n\n\"X5\" is then \"2009, May\"\n\nFor very small capacitors like MLCC chips no marking is possible. Here only the traceability of the manufacturers can ensure the identification of a type.\n\nThe identification of modern capacitors has no detailed color coding.\n\nAn overview of worldwide operating manufacturers and their product ranges is given in the following table:\n\n"}
{"id": "24573098", "url": "https://en.wikipedia.org/wiki?curid=24573098", "title": "Chatthin Wildlife Sanctuary", "text": "Chatthin Wildlife Sanctuary\n\nChatthin Wildlife Sanctuary is a wildlife reserve of Burma. It is located in Kanbalu Township in Sagaing Division. It occupies an area of and was established in 1941.\n\nThe endangered Thamin deer is found in this protected area. The Indochinese leopard was considered almost locally extinct by 2000.\n\n"}
{"id": "5659", "url": "https://en.wikipedia.org/wiki?curid=5659", "title": "Chemical element", "text": "Chemical element\n\nA chemical element is a species of atoms having the same number of protons in their atomic nuclei (that is, the same atomic number, or \"Z\"). For example, the atomic number of oxygen is 8, so the element oxygen consists of all atoms which have exactly 8 protons.\n\n118 elements have been identified, of which the first 94 occur naturally on Earth with the remaining 24 being synthetic elements. There are 80 elements that have at least one stable isotope and 38 that have exclusively radionuclides, which decay over time into other elements. Iron is the most abundant element (by mass) making up Earth, while oxygen is the most common element in the Earth's crust.\n\nChemical elements constitute all of the ordinary matter of the universe. However astronomical observations suggest that ordinary observable matter makes up only about 15% of the matter in the universe: the remainder is dark matter; the composition of this is unknown, but it is not composed of chemical elements.\nThe two lightest elements, hydrogen and helium, were mostly formed in the Big Bang and are the most common elements in the universe. The next three elements (lithium, beryllium and boron) were formed mostly by cosmic ray spallation, and are thus rarer than heavier elements. Formation of elements with from 6 to 26 protons occurred and continues to occur in main sequence stars via stellar nucleosynthesis. The high abundance of oxygen, silicon, and iron on Earth reflects their common production in such stars. Elements with greater than 26 protons are formed by supernova nucleosynthesis in supernovae, which, when they explode, blast these elements as supernova remnants far into space, where they may become incorporated into planets when they are formed.\n\nThe term \"element\" is used for atoms with a given number of protons (regardless of whether or not they are ionized or chemically bonded, e.g. hydrogen in water) as well as for a pure chemical substance consisting of a single element (e.g. hydrogen gas). For the second meaning, the terms \"elementary substance\" and \"simple substance\" have been suggested, but they have not gained much acceptance in English chemical literature, whereas in some other languages their equivalent is widely used (e.g. French ', Russian '). A single element can form multiple substances differing in their structure; they are called allotropes of the element.\n\nWhen different elements are chemically combined, with the atoms held together by chemical bonds, they form chemical compounds. Only a minority of elements are found uncombined as relatively pure minerals. Among the more common of such native elements are copper, silver, gold, carbon (as coal, graphite, or diamonds), and sulfur. All but a few of the most inert elements, such as noble gases and noble metals, are usually found on Earth in chemically combined form, as chemical compounds. While about 32 of the chemical elements occur on Earth in native uncombined forms, most of these occur as mixtures. For example, atmospheric air is primarily a mixture of nitrogen, oxygen, and argon, and native solid elements occur in alloys, such as that of iron and nickel.\n\nThe history of the discovery and use of the elements began with primitive human societies that found native elements like carbon, sulfur, copper and gold. Later civilizations extracted elemental copper, tin, lead and iron from their ores by smelting, using charcoal. Alchemists and chemists subsequently identified many more; all of the naturally occurring elements were known by 1950.\n\nThe properties of the chemical elements are summarized in the periodic table, which organizes the elements by increasing atomic number into rows (\"periods\") in which the columns (\"groups\") share recurring (\"periodic\") physical and chemical properties. Save for unstable radioactive elements with short half-lives, all of the elements are available industrially, most of them in low degrees of impurities.\n\nThe lightest chemical elements are hydrogen and helium, both created by Big Bang nucleosynthesis during the first 20 minutes of the universe in a ratio of around 3:1 by mass (or 12:1 by number of atoms), along with tiny traces of the next two elements, lithium and beryllium. Almost all other elements found in nature were made by various natural methods of nucleosynthesis. On Earth, small amounts of new atoms are naturally produced in nucleogenic reactions, or in cosmogenic processes, such as cosmic ray spallation. New atoms are also naturally produced on Earth as radiogenic daughter isotopes of ongoing radioactive decay processes such as alpha decay, beta decay, spontaneous fission, cluster decay, and other rarer modes of decay.\n\nOf the 94 naturally occurring elements, those with atomic numbers 1 through 82 each have at least one stable isotope (except for technetium, element 43 and promethium, element 61, which have no stable isotopes). Isotopes considered stable are those for which no radioactive decay has yet been observed. Elements with atomic numbers 83 through 94 are unstable to the point that radioactive decay of all isotopes can be detected. Some of these elements, notably bismuth (atomic number 83), thorium (atomic number 90), and uranium (atomic number 92), have one or more isotopes with half-lives long enough to survive as remnants of the explosive stellar nucleosynthesis that produced the heavy metals before the formation of our Solar System. At over 1.9 years, over a billion times longer than the current estimated age of the universe, bismuth-209 (atomic number 83) has the longest known alpha decay half-life of any naturally occurring element, and is almost always considered on par with the 80 stable elements. The very heaviest elements (those beyond plutonium, element 94) undergo radioactive decay with half-lives so short that they are not found in nature and must be synthesized.\n\nAs of 2010, there are 118 known elements (in this context, \"known\" means observed well enough, even from just a few decay products, to have been differentiated from other elements). Of these 118 elements, 94 occur naturally on Earth. Six of these occur in extreme trace quantities: technetium, atomic number 43; promethium, number 61; astatine, number 85; francium, number 87; neptunium, number 93; and plutonium, number 94. These 94 elements have been detected in the universe at large, in the spectra of stars and also supernovae, where short-lived radioactive elements are newly being made. The first 94 elements have been detected directly on Earth as primordial nuclides present from the formation of the solar system, or as naturally occurring fission or transmutation products of uranium and thorium.\n\nThe remaining 24 heavier elements, not found today either on Earth or in astronomical spectra, have been produced artificially: these are all radioactive, with very short half-lives; if any atoms of these elements were present at the formation of Earth, they are extremely likely, to the point of certainty, to have already decayed, and if present in novae, have been in quantities too small to have been noted. Technetium was the first purportedly non-naturally occurring element synthesized, in 1937, although trace amounts of technetium have since been found in nature (and also the element may have been discovered naturally in 1925). This pattern of artificial production and later natural discovery has been repeated with several other radioactive naturally occurring rare elements.\n\nList of the elements are available by name, atomic number, density, melting point, boiling point and by symbol, as well as ionization energies of the elements. The nuclides of stable and radioactive elements are also available as a list of nuclides, sorted by length of half-life for those that are unstable. One of the most convenient, and certainly the most traditional presentation of the elements, is in the form of the periodic table, which groups together elements with similar chemical properties (and usually also similar electronic structures).\n\nThe atomic number of an element is equal to the number of protons in each atom, and defines the element. For example, all carbon atoms contain 6 protons in their atomic nucleus; so the atomic number of carbon is 6. Carbon atoms may have different numbers of neutrons; atoms of the same element having different numbers of neutrons are known as isotopes of the element.\n\nThe number of protons in the atomic nucleus also determines its electric charge, which in turn determines the number of electrons of the atom in its non-ionized state. The electrons are placed into atomic orbitals that determine the atom's various chemical properties. The number of neutrons in a nucleus usually has very little effect on an element's chemical properties (except in the case of hydrogen and deuterium). Thus, all carbon isotopes have nearly identical chemical properties because they all have six protons and six electrons, even though carbon atoms may, for example, have 6 or 8 neutrons. That is why the atomic number, rather than mass number or atomic weight, is considered the identifying characteristic of a chemical element.\n\nThe symbol for atomic number is \"Z\".\n\nIsotopes are atoms of the same element (that is, with the same number of protons in their atomic nucleus), but having \"different\" numbers of neutrons. Thus, for example, there are three main isotopes of carbon. All carbon atoms have 6 protons in the nucleus, but they can have either 6, 7, or 8 neutrons. Since the mass numbers of these are 12, 13 and 14 respectively, the three isotopes of carbon are known as carbon-12, carbon-13, and carbon-14, often abbreviated to C, C, and C. Carbon in everyday life and in chemistry is a mixture of C (about 98.9%), C (about 1.1%) and about 1 atom per trillion of C.\n\nMost (66 of 94) naturally occurring elements have more than one stable isotope. Except for the isotopes of hydrogen (which differ greatly from each other in relative mass—enough to cause chemical effects), the isotopes of a given element are chemically nearly indistinguishable.\n\nAll of the elements have some isotopes that are radioactive (radioisotopes), although not all of these radioisotopes occur naturally. The radioisotopes typically decay into other elements upon radiating an alpha or beta particle. If an element has isotopes that are not radioactive, these are termed \"stable\" isotopes. All of the known stable isotopes occur naturally (see primordial isotope). The many radioisotopes that are not found in nature have been characterized after being artificially made. Certain elements have no stable isotopes and are composed \"only\" of radioactive isotopes: specifically the elements without any stable isotopes are technetium (atomic number 43), promethium (atomic number 61), and all observed elements with atomic numbers greater than 82.\n\nOf the 80 elements with at least one stable isotope, 26 have only one single stable isotope. The mean number of stable isotopes for the 80 stable elements is 3.1 stable isotopes per element. The largest number of stable isotopes that occur for a single element is 10 (for tin, element 50).\n\nThe mass number of an element, \"A\", is the number of nucleons (protons and neutrons) in the atomic nucleus. Different isotopes of a given element are distinguished by their mass numbers, which are conventionally written as a superscript on the left hand side of the atomic symbol (e.g. U). The mass number is always a whole number and has units of \"nucleons\". For example, magnesium-24 (24 is the mass number) is an atom with 24 nucleons (12 protons and 12 neutrons).\n\nWhereas the mass number simply counts the total number of neutrons and protons and is thus a natural (or whole) number, the atomic mass of a single atom is a real number giving the mass of a particular isotope (or \"nuclide\") of the element, expressed in atomic mass units (symbol: u). In general, the mass number of a given nuclide differs in value slightly from its atomic mass, since the mass of each proton and neutron is not exactly 1 u; since the electrons contribute a lesser share to the atomic mass as neutron number exceeds proton number; and (finally) because of the nuclear binding energy. For example, the atomic mass of chlorine-35 to five significant digits is 34.969 u and that of chlorine-37 is 36.966 u. However, the atomic mass in u of each isotope is quite close to its simple mass number (always within 1%). The only isotope whose atomic mass is exactly a natural number is C, which by definition has a mass of exactly 12, because u is defined as 1/12 of the mass of a free neutral carbon-12 atom in the ground state.\n\nThe standard atomic weight (commonly called \"atomic weight\") of an element is the \"average\" of the atomic masses of all the chemical element's isotopes as found in a particular environment, weighted by isotopic abundance, relative to the atomic mass unit. This number may be a fraction that is \"not\" close to a whole number. For example, the relative atomic mass of chlorine is 35.453 u, which differs greatly from a whole number as it is an average of about 76% chlorine-35 and 24% chlorine-37. Whenever a relative atomic mass value differs by more than 1% from a whole number, it is due to this averaging effect, as significant amounts of more than one isotope are naturally present in a sample of that element.\n\nChemists and nuclear scientists have different definitions of a \"pure element\". In chemistry, a pure element means a substance whose atoms all (or in practice almost all) have the same atomic number, or number of protons. Nuclear scientists, however, define a pure element as one that consists of only one stable isotope.\n\nFor example, a copper wire is 99.99% chemically pure if 99.99% of its atoms are copper, with 29 protons each. However it is not isotopically pure since ordinary copper consists of two stable isotopes, 69% Cu and 31% Cu, with different numbers of neutrons. However, a pure gold ingot would be both chemically and isotopically pure, since ordinary gold consists only of one isotope, Au.\n\nAtoms of chemically pure elements may bond to each other chemically in more than one way, allowing the pure element to exist in multiple chemical structures (spatial arrangements of atoms), known as allotropes, which differ in their properties. For example, carbon can be found as diamond, which has a tetrahedral structure around each carbon atom; graphite, which has layers of carbon atoms with a hexagonal structure stacked on top of each other; graphene, which is a single layer of graphite that is very strong; fullerenes, which have nearly spherical shapes; and carbon nanotubes, which are tubes with a hexagonal structure (even these may differ from each other in electrical properties). The ability of an element to exist in one of many structural forms is known as 'allotropy'.\n\nThe standard state, also known as reference state, of an element is defined as its thermodynamically most stable state at a pressure of 1 bar and a given temperature (typically at 298.15 K). In thermochemistry, an element is defined to have an enthalpy of formation of zero in its standard state. For example, the reference state for carbon is graphite, because the structure of graphite is more stable than that of the other allotropes.\n\nSeveral kinds of descriptive categorizations can be applied broadly to the elements, including consideration of their general physical and chemical properties, their states of matter under familiar conditions, their melting and boiling points, their densities, their crystal structures as solids, and their origins.\n\nSeveral terms are commonly used to characterize the general physical and chemical properties of the chemical elements. A first distinction is between metals, which readily conduct electricity, nonmetals, which do not, and a small group, (the \"metalloids\"), having intermediate properties and often behaving as semiconductors.\n\nA more refined classification is often shown in colored presentations of the periodic table. This system restricts the terms \"metal\" and \"nonmetal\" to only certain of the more broadly defined metals and nonmetals, adding additional terms for certain sets of the more broadly viewed metals and nonmetals. The version of this classification used in the periodic tables presented here includes: actinides, alkali metals, alkaline earth metals, halogens, lanthanides, transition metals, post-transition metals, metalloids, reactive nonmetals, and noble gases. In this system, the alkali metals, alkaline earth metals, and transition metals, as well as the lanthanides and the actinides, are special groups of the metals viewed in a broader sense. Similarly, the reactive nonmetals and the noble gases are nonmetals viewed in the broader sense. In some presentations, the halogens are not distinguished, with astatine identified as a metalloid and the others identified as nonmetals.\n\nAnother commonly used basic distinction among the elements is their state of matter (phase), whether solid, liquid, or gas, at a selected standard temperature and pressure (STP). Most of the elements are solids at conventional temperatures and atmospheric pressure, while several are gases. Only bromine and mercury are liquids at 0 degrees Celsius (32 degrees Fahrenheit) and normal atmospheric pressure; caesium and gallium are solids at that temperature, but melt at 28.4 °C (83.2 °F) and 29.8 °C (85.6 °F), respectively.\n\nMelting and boiling points, typically expressed in degrees Celsius at a pressure of one atmosphere, are commonly used in characterizing the various elements. While known for most elements, either or both of these measurements is still undetermined for some of the radioactive elements available in only tiny quantities. Since helium remains a liquid even at absolute zero at atmospheric pressure, it has only a boiling point, and not a melting point, in conventional presentations.\n\nThe density at a selected standard temperature and pressure (STP) is frequently used in characterizing the elements. Density is often expressed in grams per cubic centimeter (g/cm). Since several elements are gases at commonly encountered temperatures, their densities are usually stated for their gaseous forms; when liquefied or solidified, the gaseous elements have densities similar to those of the other elements.\n\nWhen an element has allotropes with different densities, one representative allotrope is typically selected in summary presentations, while densities for each allotrope can be stated where more detail is provided. For example, the three familiar allotropes of carbon (amorphous carbon, graphite, and diamond) have densities of 1.8–2.1, 2.267, and 3.515 g/cm, respectively.\n\nThe elements studied to date as solid samples have eight kinds of crystal structures: cubic, body-centered cubic, face-centered cubic, hexagonal, monoclinic, orthorhombic, rhombohedral, and tetragonal. For some of the synthetically produced transuranic elements, available samples have been too small to determine crystal structures.\n\nChemical elements may also be categorized by their origin on Earth, with the first 94 considered naturally occurring, while those with atomic numbers beyond 94 have only been produced artificially as the synthetic products of man-made nuclear reactions.\n\nOf the 94 naturally occurring elements, 83 are considered primordial and either stable or weakly radioactive. The remaining 11 naturally occurring elements possess half lives too short for them to have been present at the beginning of the Solar System, and are therefore considered transient elements. Of these 11 transient elements, 5 (polonium, radon, radium, actinium, and protactinium) are relatively common decay products of thorium and uranium. The remaining 6 transient elements (technetium, promethium, astatine, francium, neptunium, and plutonium) occur only rarely, as products of rare decay modes or nuclear reaction processes involving uranium or other heavy elements.\n\nElements with atomic numbers 1 through 40 are all stable, while those with atomic numbers 41 through 82 (except technetium and promethium) are metastable. The half-lives of these metastable \"theoretical radionuclides\" are so long (at least 100 million times longer than the estimated age of the universe) that their radioactive decay has yet to be detected by experiment. Elements with atomic numbers 83 through 94 are unstable to the point that their radioactive decay can be detected. Three of these elements, bismuth (element 83), thorium (element 90), and uranium (element 92) have one or more isotopes with half-lives long enough to survive as remnants of the explosive stellar nucleosynthesis that produced the heavy elements before the formation of our solar system. For example, at over 1.9 years, over a billion times longer than the current estimated age of the universe, bismuth-209 has the longest known alpha decay half-life of any naturally occurring element. The very heaviest 24 elements (those beyond plutonium, element 94) undergo radioactive decay with short half-lives and cannot be produced as daughters of longer-lived elements, and thus they do not occur in nature at all.\n\nThe properties of the chemical elements are often summarized using the periodic table, which powerfully and elegantly organizes the elements by increasing atomic number into rows (\"periods\") in which the columns (\"groups\") share recurring (\"periodic\") physical and chemical properties. The current standard table contains 118 confirmed elements as of 10 April 2010.\n\nAlthough earlier precursors to this presentation exist, its invention is generally credited to the Russian chemist Dmitri Mendeleev in 1869, who intended the table to illustrate recurring trends in the properties of the elements. The layout of the table has been refined and extended over time as new elements have been discovered and new theoretical models have been developed to explain chemical behavior.\n\nUse of the periodic table is now ubiquitous within the academic discipline of chemistry, providing an extremely useful framework to classify, systematize and compare all the many different forms of chemical behavior. The table has also found wide application in physics, geology, biology, materials science, engineering, agriculture, medicine, nutrition, environmental health, and astronomy. Its principles are especially important in chemical engineering.\n\nThe various chemical elements are formally identified by their unique atomic numbers, by their accepted names, and by their symbols.\n\nThe known elements have atomic numbers from 1 through 118, conventionally presented as Arabic numerals. Since the elements can be uniquely sequenced by atomic number, conventionally from lowest to highest (as in a periodic table), sets of elements are sometimes specified by such notation as \"through\", \"beyond\", or \"from ... through\", as in \"through iron\", \"beyond uranium\", or \"from lanthanum through lutetium\". The terms \"light\" and \"heavy\" are sometimes also used informally to indicate relative atomic numbers (not densities), as in \"lighter than carbon\" or \"heavier than lead\", although technically the weight or mass of atoms of an element (their atomic weights or atomic masses) do not always increase monotonically with their atomic numbers.\n\nThe naming of various substances now known as elements precedes the atomic theory of matter, as names were given locally by various cultures to various minerals, metals, compounds, alloys, mixtures, and other materials, although at the time it was not known which chemicals were elements and which compounds. As they were identified as elements, the existing names for anciently-known elements (e.g., gold, mercury, iron) were kept in most countries. National differences emerged over the names of elements either for convenience, linguistic niceties, or nationalism. For a few illustrative examples: German speakers use \"Wasserstoff\" (water substance) for \"hydrogen\", \"Sauerstoff\" (acid substance) for \"oxygen\" and \"Stickstoff\" (smothering substance) for \"nitrogen\", while English and some romance languages use \"sodium\" for \"natrium\" and \"potassium\" for \"kalium\", and the French, Italians, Greeks, Portuguese and Poles prefer \"azote/azot/azoto\" (from roots meaning \"no life\") for \"nitrogen\".\n\nFor purposes of international communication and trade, the official names of the chemical elements both ancient and more recently recognized are decided by the International Union of Pure and Applied Chemistry (IUPAC), which has decided on a sort of international English language, drawing on traditional English names even when an element's chemical symbol is based on a Latin or other traditional word, for example adopting \"gold\" rather than \"aurum\" as the name for the 79th element (Au). IUPAC prefers the British spellings \"aluminium\" and \"caesium\" over the U.S. spellings \"aluminum\" and \"cesium\", and the U.S. \"sulfur\" over the British \"sulphur\". However, elements that are practical to sell in bulk in many countries often still have locally used national names, and countries whose national language does not use the Latin alphabet are likely to use the IUPAC element names.\n\nAccording to IUPAC, chemical elements are not proper nouns in English; consequently, the full name of an element is not routinely capitalized in English, even if derived from a proper noun, as in californium and einsteinium. Isotope names of chemical elements are also uncapitalized if written out, \"e.g.,\" carbon-12 or uranium-235. Chemical element \"symbols\" (such as Cf for californium and Es for einsteinium), are always capitalized (see below).\n\nIn the second half of the twentieth century, physics laboratories became able to produce nuclei of chemical elements with half-lives too short for an appreciable amount of them to exist at any time. These are also named by IUPAC, which generally adopts the name chosen by the discoverer. This practice can lead to the controversial question of which research group actually discovered an element, a question that delayed the naming of elements with atomic number of 104 and higher for a considerable amount of time. (See element naming controversy).\n\nPrecursors of such controversies involved the nationalistic namings of elements in the late 19th century. For example, \"lutetium\" was named in reference to Paris, France. The Germans were reluctant to relinquish naming rights to the French, often calling it \"cassiopeium\". Similarly, the British discoverer of \"niobium\" originally named it \"columbium,\" in reference to the New World. It was used extensively as such by American publications before the international standardization (in 1950).\n\nBefore chemistry became a science, alchemists had designed arcane symbols for both metals and common compounds. These were however used as abbreviations in diagrams or procedures; there was no concept of atoms combining to form molecules. With his advances in the atomic theory of matter, John Dalton devised his own simpler symbols, based on circles, to depict molecules.\n\nThe current system of chemical notation was invented by Berzelius. In this typographical system, chemical symbols are not mere abbreviations—though each consists of letters of the Latin alphabet. They are intended as universal symbols for people of all languages and alphabets.\n\nThe first of these symbols were intended to be fully universal. Since Latin was the common language of science at that time, they were abbreviations based on the Latin names of metals. Cu comes from Cuprum, Fe comes from Ferrum, Ag from Argentum. The symbols were not followed by a period (full stop) as with abbreviations. Later chemical elements were also assigned unique chemical symbols, based on the name of the element, but not necessarily in English. For example, sodium has the chemical symbol 'Na' after the Latin \"natrium\". The same applies to \"W\" (wolfram) for tungsten, \"Fe\" (ferrum) for iron, \"Hg\" (hydrargyrum) for mercury, \"Sn\" (stannum) for tin, \"K\" (kalium) for potassium, \"Au\" (aurum) for gold, \"Ag\" (argentum) for silver, \"Pb\" (plumbum) for lead, \"Cu\" (cuprum) for copper, and \"Sb\" (stibium) for antimony.\n\nChemical symbols are understood internationally when element names might require translation. There have sometimes been differences in the past. For example, Germans in the past have used \"J\" (for the alternate name Jod) for iodine, but now use \"I\" and \"Iod\".\n\nThe first letter of a chemical symbol is always capitalized, as in the preceding examples, and the subsequent letters, if any, are always lower case (small letters). Thus, the symbols for californium and einsteinium are Cf and Es.\n\nThere are also symbols in chemical equations for groups of chemical elements, for example in comparative formulas. These are often a single capital letter, and the letters are reserved and not used for names of specific elements. For example, an \"X\" indicates a variable group (usually a halogen) in a class of compounds, while \"R\" is a radical, meaning a compound structure such as a hydrocarbon chain. The letter \"Q\" is reserved for \"heat\" in a chemical reaction. \"Y\" is also often used as a general chemical symbol, although it is also the symbol of yttrium. \"Z\" is also frequently used as a general variable group. \"E\" is used in organic chemistry to denote an electron-withdrawing group or an electrophile; similarly \"Nu\" denotes a nucleophile. \"L\" is used to represent a general ligand in inorganic and organometallic chemistry. \"M\" is also often used in place of a general metal.\n\nAt least two additional, two-letter generic chemical symbols are also in informal usage, \"Ln\" for any lanthanide element and \"An\" for any actinide element. \"Rg\" was formerly used for any rare gas element, but the group of rare gases has now been renamed noble gases and the symbol \"Rg\" has now been assigned to the element roentgenium.\n\nIsotopes are distinguished by the atomic mass number (total protons and neutrons) for a particular isotope of an element, with this number combined with the pertinent element's symbol. IUPAC prefers that isotope symbols be written in superscript notation when practical, for example C and U. However, other notations, such as carbon-12 and uranium-235, or C-12 and U-235, are also used.\n\nAs a special case, the three naturally occurring isotopes of the element hydrogen are often specified as H for H (protium), D for H (deuterium), and T for H (tritium). This convention is easier to use in chemical equations, replacing the need to write out the mass number for each atom. For example, the formula for heavy water may be written DO instead of HO.\n\nOnly about 4% of the total mass of the universe is made of atoms or ions, and thus represented by chemical elements. This fraction is about 15% of the total matter, with the remainder of the matter (85%) being dark matter. The nature of dark matter is unknown, but it is not composed of atoms of chemical elements because it contains no protons, neutrons, or electrons. (The remaining non-matter part of the mass of the universe is composed of the even more mysterious dark energy).\n\nThe universe's 94 naturally occurring chemical elements are thought to have been produced by at least four cosmic processes. Most of the hydrogen, helium and a very small quantity of lithiumin the universe was produced primordially in the first few minutes of the Big Bang. Other three recurrently occurring later processes are thought to have produced the remaining elements. Stellar nucleosynthesis, an ongoing process inside stars, produces all elements from carbon through iron in atomic number, but little lithium, beryllium, or boron. Elements heavier in atomic number than iron, as heavy as uranium and plutonium, are produced by explosive nucleosynthesis in supernovas and other cataclysmic cosmic events. Cosmic ray spallation (fragmentation) of carbon, nitrogen, and oxygen is important to the production of lithium, beryllium and boron.\n\nDuring the early phases of the Big Bang, nucleosynthesis of hydrogen nuclei resulted in the production of hydrogen-1 (protium, H) and helium-4 (He), as well as a smaller amount of deuterium (H) and very minuscule amounts (on the order of 10) of lithium and beryllium. Even smaller amounts of boron may have been produced in the Big Bang, since it has been observed in some very old stars, while carbon has not. It is generally agreed that no heavier elements than boron were produced in the Big Bang. As a result, the primordial abundance of atoms (or ions) consisted of roughly 75% H, 25% He, and 0.01% deuterium, with only tiny traces of lithium, beryllium, and perhaps boron. Subsequent enrichment of galactic halos occurred due to stellar nucleosynthesis and supernova nucleosynthesis. However, the element abundance in intergalactic space can still closely resemble primordial conditions, unless it has been enriched by some means.\n\nOn Earth (and elsewhere), trace amounts of various elements continue to be produced from other elements as products of nuclear transmutation processes. These include some produced by cosmic rays or other nuclear reactions (see cosmogenic and nucleogenic nuclides), and others produced as decay products of long-lived primordial nuclides. For example, trace (but detectable) amounts of carbon-14 (C) are continually produced in the atmosphere by cosmic rays impacting nitrogen atoms, and argon-40 (Ar) is continually produced by the decay of primordially occurring but unstable potassium-40 (K). Also, three primordially occurring but radioactive actinides, thorium, uranium, and plutonium, decay through a series of recurrently produced but unstable radioactive elements such as radium and radon, which are transiently present in any sample of these metals or their ores or compounds. Three other radioactive elements, technetium, promethium, and neptunium, occur only incidentally in natural materials, produced as individual atoms by nuclear fission of the nuclei of various heavy elements or in other rare nuclear processes.\n\nHuman technology has produced various additional elements beyond these first 94, with those through atomic number 118 now known.\n\nThe following graph (note log scale) shows the abundance of elements in our Solar System. The table shows the twelve most common elements in our galaxy (estimated spectroscopically), as measured in parts per million, by mass. Nearby galaxies that have evolved along similar lines have a corresponding enrichment of elements heavier than hydrogen and helium. The more distant galaxies are being viewed as they appeared in the past, so their abundances of elements appear closer to the primordial mixture. As physical laws and processes appear common throughout the visible universe, however, scientist expect that these galaxies evolved elements in similar abundance.\n\nThe abundance of elements in the Solar System is in keeping with their origin from nucleosynthesis in the Big Bang and a number of progenitor supernova stars. Very abundant hydrogen and helium are products of the Big Bang, but the next three elements are rare since they had little time to form in the Big Bang and are not made in stars (they are, however, produced in small quantities by the breakup of heavier elements in interstellar dust, as a result of impact by cosmic rays). Beginning with carbon, elements are produced in stars by buildup from alpha particles (helium nuclei), resulting in an alternatingly larger abundance of elements with even atomic numbers (these are also more stable). In general, such elements up to iron are made in large stars in the process of becoming supernovas. Iron-56 is particularly common, since it is the most stable element that can easily be made from alpha particles (being a product of decay of radioactive nickel-56, ultimately made from 14 helium nuclei). Elements heavier than iron are made in energy-absorbing processes in large stars, and their abundance in the universe (and on Earth) generally decreases with their atomic number.\n\nThe abundance of the chemical elements on Earth varies from air to crust to ocean, and in various types of life. The abundance of elements in Earth's crust differs from that in the Solar system (as seen in the Sun and heavy planets like Jupiter) mainly in selective loss of the very lightest elements (hydrogen and helium) and also volatile neon, carbon (as hydrocarbons), nitrogen and sulfur, as a result of solar heating in the early formation of the solar system. Oxygen, the most abundant Earth element by mass, is retained on Earth by combination with silicon. Aluminum at 8% by mass is more common in the Earth's crust than in the universe and solar system, but the composition of the far more bulky mantle, which has magnesium and iron in place of aluminum (which occurs there only at 2% of mass) more closely mirrors the elemental composition of the solar system, save for the noted loss of volatile elements to space, and loss of iron which has migrated to the Earth's core.\n\nThe composition of the human body, by contrast, more closely follows the composition of seawater—save that the human body has additional stores of carbon and nitrogen necessary to form the proteins and nucleic acids, together with phosphorus in the nucleic acids and energy transfer molecule adenosine triphosphate (ATP) that occurs in the cells of all living organisms. Certain kinds of organisms require particular additional elements, for example the magnesium in chlorophyll in green plants, the calcium in mollusc shells, or the iron in the hemoglobin in vertebrate animals' red blood cells.\n\nThe concept of an \"element\" as an undivisible substance has developed through three major historical phases: Classical definitions (such as those of the ancient Greeks), chemical definitions, and atomic definitions.\n\nAncient philosophy posited a set of classical elements to explain observed patterns in nature. These \"elements\" originally referred to \"earth\", \"water\", \"air\" and \"fire\" rather than the chemical elements of modern science.\n\nThe term 'elements' (\"stoicheia\") was first used by the Greek philosopher Plato in about 360 BCE in his dialogue Timaeus, which includes a discussion of the composition of inorganic and organic bodies and is a speculative treatise on chemistry. Plato believed the elements introduced a century earlier by Empedocles were composed of small polyhedral forms: tetrahedron (fire), octahedron (air), icosahedron (water), and cube (earth).\n\nAristotle, c. 350 BCE, also used the term \"stoicheia\" and added a fifth element called aether, which formed the heavens. Aristotle defined an element as:\n\nIn 1661, Robert Boyle proposed his theory of corpuscularism which favoured the analysis of matter as constituted by irreducible units of matter (atoms) and, choosing to side with neither Aristotle's view of the four elements nor Paracelsus' view of three fundamental elements, left open the question of the number of elements. The first modern list of chemical elements was given in Antoine Lavoisier's 1789 \"Elements of Chemistry\", which contained thirty-three elements, including light and caloric. By 1818, Jöns Jakob Berzelius had determined atomic weights for forty-five of the forty-nine then-accepted elements. Dmitri Mendeleev had sixty-six elements in his periodic table of 1869.\n\nFrom Boyle until the early 20th century, an element was defined as a pure substance that could not be decomposed into any simpler substance. Put another way, a chemical element cannot be transformed into other chemical elements by chemical processes. Elements during this time were generally distinguished by their atomic weights, a property measurable with fair accuracy by available analytical techniques.\n\nThe 1913 discovery by English physicist Henry Moseley that the nuclear charge is the physical basis for an atom's atomic number, further refined when the nature of protons and neutrons became appreciated, eventually led to the current definition of an element based on atomic number (number of protons per atomic nucleus). The use of atomic numbers, rather than atomic weights, to distinguish elements has greater predictive value (since these numbers are integers), and also resolves some ambiguities in the chemistry-based view due to varying properties of isotopes and allotropes within the same element. Currently, IUPAC defines an element to exist if it has isotopes with a lifetime longer than the 10 seconds it takes the nucleus to form an electronic cloud.\n\nBy 1914, seventy-two elements were known, all naturally occurring. The remaining naturally occurring elements were discovered or isolated in subsequent decades, and various additional elements have also been produced synthetically, with much of that work pioneered by Glenn T. Seaborg. In 1955, element 101 was discovered and named mendelevium in honor of D.I. Mendeleev, the first to arrange the elements in a periodic manner. Most recently, the synthesis of element 118 (since named oganesson) was reported in October 2006, and the synthesis of element 117 (tennessine) was reported in April 2010.\n\nTen materials familiar to various prehistoric cultures are now known to be chemical elements: Carbon, copper, gold, iron, lead, mercury, silver, sulfur, tin, and zinc. Three additional materials now accepted as elements, arsenic, antimony, and bismuth, were recognized as distinct substances prior to 1500 AD. Phosphorus, cobalt, and platinum were isolated before 1750.\n\nMost of the remaining naturally occurring chemical elements were identified and characterized by 1900, including:\n\nElements isolated or produced since 1900 include:\n\nThe first transuranium element (element with atomic number greater than 92) discovered was neptunium in 1940. Since 1999 claims for the discovery of new elements have been considered by the IUPAC/IUPAP Joint Working Party. As of January 2016, all 118 elements have been confirmed as discovered by IUPAC. The discovery of element 112 was acknowledged in 2009, and the name \"copernicium\" and the atomic symbol \"Cn\" were suggested for it. The name and symbol were officially endorsed by IUPAC on 19 February 2010. The heaviest element that is believed to have been synthesized to date is element 118, oganesson, on 9 October 2006, by the Flerov Laboratory of Nuclear Reactions in Dubna, Russia. Tennessine, element 117 was the latest element claimed to be discovered, in 2009. On 28 November 2016, scientists at the IUPAC officially recognized the names for four of the newest chemical elements, with atomic numbers 113, 115, 117, and 118.\n\nThe following sortable table shows the 118 known chemical elements.\n\n"}
{"id": "4934568", "url": "https://en.wikipedia.org/wiki?curid=4934568", "title": "Cirrocumulus undulatus", "text": "Cirrocumulus undulatus\n\nCirrocumulus undulatus is a variety of cirrocumulus cloud. The name \"cirrocumulus undulatus\" is derived from Latin, meaning \"diversified as with waves\". They have a rippled appearance due to wind shear and usually cover only a small portion of the sky. They appear in bands as small patches or layers. Occasionally, they comprise two or more wave forms superposed upon one another. The individual cloudlets can either be circular, or elongated in the direction of the rows.\n\n\n"}
{"id": "47512", "url": "https://en.wikipedia.org/wiki?curid=47512", "title": "Climate change", "text": "Climate change\n\nClimate change is a change in the statistical distribution of weather patterns when that change lasts for an extended period of time (i.e., decades to millions of years). Climate change may refer to a change in average weather conditions, or in the time variation of weather within the context of longer-term average conditions. Climate change is caused by factors such as biotic processes, variations in solar radiation received by Earth, plate tectonics, and volcanic eruptions. Certain human activities have been identified as primary causes of ongoing climate change, often referred to as global warming. There is no general agreement in scientific, media or policy documents as to the precise term to be used to refer to anthropogenic forced change; either \"global warming\" or \"climate change\" may be used.\n\nScientists actively work to understand past and future climate by using observations and theoretical models. A climate record—extending deep into the Earth's past—has been assembled, and continues to be built up, based on geological evidence from borehole temperature profiles, cores removed from deep accumulations of ice, floral and faunal records, glacial and periglacial processes, stable-isotope and other analyses of sediment layers, and records of past sea levels. More recent data are provided by the instrumental record. General circulation models, based on the physical sciences, are often used in theoretical approaches to match past climate data, make future projections, and link causes and effects in climate change.\n\nFactors that can shape climate are called climate forcings or \"forcing mechanisms\". These can be either \"internal\" or \"external\". Internal forcing mechanisms are natural processes within the climate system itself (e.g., the thermohaline circulation). External forcing mechanisms can be either anthropogenic—caused by humans—(e.g. increased emissions of greenhouse gases and dust) or natural (e.g., changes in solar output, the earth's orbit, volcano eruptions).\n\nPhysical evidence to observe climate change includes a range of parameters. Global records of surface temperature are available beginning from the mid-late 19th century. For earlier periods, most of the evidence is indirect—climatic changes are inferred from changes in proxies, indicators that reflect climate, such as ice cores, dendrochronology, sea level change, and glacial geology. Other physical evidence includes arctic sea ice decline, cloud cover and precipitation, vegetation, animals and historical and archaeological evidence.\n\nThe most general definition of climate change is a change in the statistical properties (principally its mean and spread) of the climate system when considered over long periods of time, regardless of cause. Accordingly, fluctuations over periods shorter than a few decades, such as El Niño, do not represent climate change.\n\nThe term \"climate change\" is often used to refer specifically to anthropogenic climate change (also known as global warming). Anthropogenic climate change is caused by human activity, as opposed to changes in climate that may have resulted as part of Earth's natural processes.\nIn this sense, especially in the context of environmental policy, the term climate change has become synonymous with anthropogenic global warming. Within scientific journals, global warming refers to surface temperature increases while climate change includes global warming and everything else that increasing greenhouse gas levels affect.\n\nA related term, \"climatic change\", was proposed by the World Meteorological Organization (WMO) in 1966 to encompass all forms of climatic variability on time-scales longer than 10 years, but regardless of cause. During the 1970s, the term climate change replaced climatic change to focus on anthropogenic causes, as it became clear that human activities had a potential to drastically alter the climate. Climate change was incorporated in the title of the Intergovernmental Panel on Climate Change (IPCC) and the UN Framework Convention on Climate Change (UNFCCC). Climate change is now used as both a technical description of the process, as well as a noun used to describe the problem.\n\nOn the broadest scale, the rate at which energy is received from the Sun and the rate at which it is lost to space determine the equilibrium temperature and climate of Earth. This energy is distributed around the globe by winds, ocean currents, and other mechanisms to affect the climates of different regions.\n\nFactors that can shape climate are called climate forcings or \"forcing mechanisms\". These include processes such as variations in solar radiation, variations in the Earth's orbit, variations in the albedo or reflectivity of the continents, atmosphere, and oceans, mountain-building and continental drift and changes in greenhouse gas concentrations. There are a variety of climate change feedbacks that can either amplify or diminish the initial forcing. Some parts of the climate system, such as the oceans and ice caps, respond more slowly in reaction to climate forcings, while others respond more quickly. There are also key threshold factors which when exceeded can produce rapid change.\n\nForcing mechanisms can be either \"internal\" or \"external\". Internal forcing mechanisms are natural processes within the climate system itself (e.g., the thermohaline circulation). External forcing mechanisms can be either anthropogenic (e.g. increased emissions of greenhouse gases and dust) or natural (e.g., changes in solar output, the earth's orbit, volcano eruptions).\n\nWhether the initial forcing mechanism is internal or external, the response of the climate system might be fast (e.g., a sudden cooling due to airborne volcanic ash reflecting sunlight), slow (e.g. thermal expansion of warming ocean water), or a combination (e.g., sudden loss of albedo in the Arctic Ocean as sea ice melts, followed by more gradual thermal expansion of the water). Therefore, the climate system can respond abruptly, but the full response to forcing mechanisms might not be fully developed for centuries or even longer.\n\nScientists generally define the five components of earth's climate system to include atmosphere, hydrosphere, cryosphere, lithosphere (restricted to the surface soils, rocks, and sediments), and biosphere. Natural changes in the climate system (\"internal forcings\") result in internal \"climate variability\". Examples include the type and distribution of species, and changes in ocean-atmosphere circulations.\n\nThe ocean and atmosphere can work together to spontaneously generate internal climate variability that can persist for years to decades at a time. Examples of this type of variability include the El Niño–Southern Oscillation, the Pacific decadal oscillation, and the Atlantic Multidecadal Oscillation. These variations can affect global average surface temperature by redistributing heat between the deep ocean and the atmosphere and/or by altering the cloud/water vapor/sea ice distribution which can affect the total energy budget of the earth.\n\nThe oceanic aspects of these circulations can generate variability on centennial timescales due to the ocean having hundreds of times more mass than in the atmosphere, and thus very high thermal inertia. For example, alterations to ocean processes such as thermohaline circulation play a key role in redistributing heat in the world's oceans. Due to the long timescales of this circulation, ocean temperature at depth is still adjusting to effects of the Little Ice Age which occurred between the 1600 and 1800s.\n\nLife affects climate through its role in the carbon and water cycles and through such mechanisms as albedo, evapotranspiration, cloud formation, and weathering. Examples of how life may have affected past climate include:\n\nIn the context of climate variation, anthropogenic factors are human activities which affect the climate. The scientific consensus on climate change is \"that climate is changing and that these changes are in large part caused by human activities\",\nand it \"is largely irreversible\".\nOf most concern in these anthropogenic factors is the increase in CO levels. This is due to emissions from fossil fuel combustion, followed by aerosols (particulate matter in the atmosphere), and the CO released by cement manufacture. Other factors, including land use, ozone depletion, animal husbandry (ruminant animals such as cattle produce methane, as do termites), and deforestation, are also of concern in the roles they play—both separately and in conjunction with other factors—in affecting climate, microclimate, and measures of climate variables.\n\nSlight variations in Earth's motion lead to changes in the seasonal distribution of sunlight reaching the Earth's surface and how it is distributed across the globe. There is very little change to the area-averaged annually averaged sunshine; but there can be strong changes in the geographical and seasonal distribution. The three types of kinematic change are variations in Earth's eccentricity, changes in the tilt angle of Earth's axis of rotation, and precession of Earth's axis. Combined together, these produce Milankovitch cycles which affect climate and are notable for their correlation to glacial and interglacial periods, their correlation with the advance and retreat of the Sahara, and for their appearance in the stratigraphic record.\n\nThe IPCC notes that Milankovitch cycles drove the ice age cycles, CO followed temperature change \"with a lag of some hundreds of years\", and that as a feedback amplified temperature change. The depths of the ocean have a lag time in changing temperature (thermal inertia on such scale). Upon seawater temperature change, the solubility of CO in the oceans changed, as well as other factors affecting air-sea CO exchange.\n\nThe Sun is the predominant source of energy input to the Earth. Other sources include geothermal energy from the Earth's core, tidal energy from the Moon and heat from the decay of radioactive compounds. Both long- and short-term variations in solar intensity are known to affect global climate.\n\nThree to four billion years ago, the Sun emitted only 75% as much power as it does today. If the atmospheric composition had been the same as today, liquid water should not have existed on Earth. However, there is evidence for the presence of water on the early Earth, in the Hadean and Archean eons, leading to what is known as the faint young Sun paradox. Hypothesized solutions to this paradox include a vastly different atmosphere, with much higher concentrations of greenhouse gases than currently exist. Over the following approximately 4 billion years, the energy output of the Sun increased and atmospheric composition changed. The Great Oxygenation Event—oxygenation of the atmosphere around 2.4 billion years ago—was the most notable alteration. Over the next five billion years from the present, the Sun's ultimate death as it becomes a red giant and then a white dwarf will have large effects on climate, with the red giant phase possibly ending any life on Earth that survives until that time.\nSolar output varies on shorter time scales, including the 11-year solar cycle and longer-term modulations. Solar intensity variations, possibly as a result of the Wolf, Spörer, and the Maunder Minima, are considered to have been influential in triggering the Little Ice Age. This event extended from 1550 to 1850 AD and was marked by relative cooling and greater glacier extent than the centuries before and afterward. Solar variation may also have affected some of the warming observed from 1900 to 1950. The cyclical nature of the Sun's energy output is not yet fully understood; it differs from the very slow change that is happening within the Sun as it ages and evolves.\n\nSome studies point toward solar radiation increases from cyclical sunspot activity affecting global warming, and climate may be influenced by the sum of all effects (solar variation, anthropogenic radiative forcings, etc.).\n\nA 2010 study suggests \"that the effects of solar variability on temperature throughout the atmosphere may be contrary to current expectations\".\n\nIn 2011, CERN announced the initial results from its CLOUD experiment in the \"Nature\" journal. The results indicate that ionisation from cosmic rays significantly enhances aerosol formation in the presence of sulfuric acid and water, but in the lower atmosphere where ammonia is also required, this is insufficient to account for aerosol formation and additional trace vapours must be involved. The next step is to find more about these trace vapours, including whether they are of natural or human origin.\n\nThe eruptions considered to be large enough to affect the Earth's climate on a scale of more than 1 year are the ones that inject over 100,000 tons of SO into the stratosphere. This is due to the optical properties of SO and sulfate aerosols, which strongly absorb or scatter solar radiation, creating a global layer of sulfuric acid haze. On average, such eruptions occur several times per century, and cause cooling (by partially blocking the transmission of solar radiation to the Earth's surface) for a period of several years.\n\nThe eruption of Mount Pinatubo in 1991, the second largest terrestrial eruption of the 20th century, affected the climate substantially, subsequently global temperatures decreased by about 0.5 °C (0.9 °F) for up to three years. Thus, the cooling over large parts of the Earth reduced surface temperatures in 1991–93, the equivalent to a reduction in net radiation of 4 watts per square meter. The Mount Tambora eruption in 1815 caused the Year Without a Summer. Much larger eruptions, known as large igneous provinces, occur only a few times every fifty – one hundred million years – through flood basalt, and caused in Earth past global warming and mass extinctions.\n\nSmall eruptions, with injections of less than 0.1 Mt of sulfur dioxide into the stratosphere, affect the atmosphere only subtly, as temperature changes are comparable with natural variability. However, because smaller eruptions occur at a much higher frequency, they too significantly affect Earth's atmosphere.\n\nSeismic monitoring maps current and future trends in volcanic activities, and tries to develop early warning systems. In climate modelling the aim is to study the physical mechanisms and feedbacks of volcanic forcing.\n\nVolcanoes are also part of the extended carbon cycle. Over very long (geological) time periods, they release carbon dioxide from the Earth's crust and mantle, counteracting the uptake by sedimentary rocks and other geological carbon dioxide sinks. The US Geological Survey estimates are that volcanic emissions are at a much lower level than the effects of current human activities, which generate 100–300 times the amount of carbon dioxide emitted by volcanoes. A review of published studies indicates that annual volcanic emissions of carbon dioxide, including amounts released from mid-ocean ridges, volcanic arcs, and hot spot volcanoes, are only the equivalent of 3 to 5 days of human-caused output. The annual amount put out by human activities may be greater than the amount released by supererruptions, the most recent of which was the Toba eruption in Indonesia 74,000 years ago.\n\nAlthough volcanoes are technically part of the lithosphere, which itself is part of the climate system, the IPCC explicitly defines volcanism as an external forcing agent.\n\nOver the course of millions of years, the motion of tectonic plates reconfigures global land and ocean areas and generates topography. This can affect both global and local patterns of climate and atmosphere-ocean circulation.\n\nThe position of the continents determines the geometry of the oceans and therefore influences patterns of ocean circulation. The locations of the seas are important in controlling the transfer of heat and moisture across the globe, and therefore, in determining global climate. A recent example of tectonic control on ocean circulation is the formation of the Isthmus of Panama about 5 million years ago, which shut off direct mixing between the Atlantic and Pacific Oceans. This strongly affected the ocean dynamics of what is now the Gulf Stream and may have led to Northern Hemisphere ice cover. During the Carboniferous period, about 300 to 360 million years ago, plate tectonics may have triggered large-scale storage of carbon and increased glaciation. Geologic evidence points to a \"megamonsoonal\" circulation pattern during the time of the supercontinent Pangaea, and climate modeling suggests that the existence of the supercontinent was conducive to the establishment of monsoons.\n\nThe size of continents is also important. Because of the stabilizing effect of the oceans on temperature, yearly temperature variations are generally lower in coastal areas than they are inland. A larger supercontinent will therefore have more area in which climate is strongly seasonal than will several smaller continents or islands.\n\nThe Earth receives an influx of ionized particles known as cosmic rays from a variety of external sources, including the Sun. A hypothesis holds that an increase in the cosmic ray flux would increase the ionization in the atmosphere, leading to greater cloud cover. This, in turn, would tend to cool the surface. The non-solar cosmic ray flux may vary as a result of a nearby supernova event, the solar system passing through a dense interstellar cloud, or the oscillatory movement of the Sun's position with respect to the galactic plane. The latter can increase the flux of high-energy cosmic rays coming from the Virgo cluster.\n\nEvidence exists that the Chicxulub impact some 66 million years ago had severely affected the Earth's climate. Large quantities of sulfate aerosols were kicked up into the atmosphere, decreasing global temperatures by up to 26 °C and producing sub-freezing temperatures for a period of 3−16 years. The recovery time for this event took more than 30 years.\n\nEvidence for climatic change is taken from a variety of sources that can be used to reconstruct past climates. Reasonably complete global records of surface temperature are available beginning from the mid-late 19th century. For earlier periods, most of the evidence is indirect—climatic changes are inferred from changes in proxies, indicators that reflect climate, such as vegetation, ice cores, dendrochronology, sea level change, and glacial geology.\n\nThe instrumental temperature record from surface stations was supplemented by radiosonde balloons, extensive atmospheric monitoring by the mid-20th century, and, from the 1970s on, with global satellite data as well. Taking the record as a whole, most of the 20th century had been unprecedentedly warm, while the 19th and 17th centuries were quite cool.\n\nThe O/O ratio in calcite and ice core samples used to deduce ocean temperature in the distant past is an example of a temperature proxy method, as are other climate metrics noted in subsequent categories.\n\nGlaciers are considered among the most sensitive indicators of climate change. Their size is determined by a mass balance between snow input and melt output. As temperatures warm, glaciers retreat unless snow precipitation increases to make up for the additional melt; the converse is also true.\n\nGlaciers grow and shrink due both to natural variability and external forcings. Variability in temperature, precipitation, and englacial and subglacial hydrology can strongly determine the evolution of a glacier in a particular season. Therefore, one must average over a decadal or longer time-scale and/or over many individual glaciers to smooth out the local short-term variability and obtain a glacier history that is related to climate.\n\nA world glacier inventory has been compiled since the 1970s, initially based mainly on aerial photographs and maps but now relying more on satellites. This compilation tracks more than 100,000 glaciers covering a total area of approximately 240,000 km, and preliminary estimates indicate that the remaining ice cover is around 445,000 km. The World Glacier Monitoring Service collects data annually on glacier retreat and glacier mass balance. From this data, glaciers worldwide have been found to be shrinking significantly, with strong glacier retreats in the 1940s, stable or growing conditions during the 1920s and 1970s, and again retreating from the mid-1980s to the present.\n\nThe most significant climate processes since the middle to late Pliocene (approximately 3 million years ago) are the glacial and interglacial cycles. The present interglacial period (the Holocene) has lasted about 11,700 years. Shaped by orbital variations, responses such as the rise and fall of continental ice sheets and significant sea-level changes helped create the climate. Other changes, including Heinrich events, Dansgaard–Oeschger events and the Younger Dryas, however, illustrate how glacial variations may also influence climate without the orbital forcing.\n\nGlaciers leave behind moraines that contain a wealth of material—including organic matter, quartz, and potassium that may be dated—recording the periods in which a glacier advanced and retreated. Similarly, by tephrochronological techniques, the lack of glacier cover can be identified by the presence of soil or volcanic tephra horizons whose date of deposit may also be ascertained.\n\nData from NASA's Grace satellites show that the land ice sheets in both Antarctica (upper chart) and Greenland (lower) have been losing mass since 2002. Both ice sheets have seen an acceleration of ice mass loss since 2009.\n\nThe decline in Arctic sea ice, both in extent and thickness, over the last several decades is further evidence for rapid climate change. Sea ice is frozen seawater that floats on the ocean surface. It covers millions of square kilometers in the polar regions, varying with the seasons. In the Arctic, some sea ice remains year after year, whereas almost all Southern Ocean or Antarctic sea ice melts away and reforms annually. Satellite observations show that Arctic sea ice is now declining at a rate of 13.2 percent per decade, relative to the 1981 to 2010 average. The 2007 Arctic summer sea ice retreat was unprecedented. Decades of shrinking and thinning in a warm climate has put the Arctic sea ice in a precarious position, it is now vulnerable to atmospheric anomalies. \"Both extent and volume anomaly fluctuate little from January to July and then decrease steeply in August and September\". This decrease is because of lessened ice production as a result of the unusually high SAT. During the Arctic summer, a slower rate of sea ice production is the same as a faster rate of sea ice melting. \n\nGlobal sea level change for much of the last century has generally been estimated using tide gauge measurements collated over long periods of time to give a long-term average. More recently, altimeter measurements—in combination with accurately determined satellite orbits—have provided an improved measurement of global sea level change. To measure sea levels prior to instrumental measurements, scientists have dated coral reefs that grow near the surface of the ocean, coastal sediments, marine terraces, ooids in limestones, and nearshore archaeological remains. The predominant dating methods used are uranium series and radiocarbon, with cosmogenic radionuclides being sometimes used to date terraces that have experienced relative sea level fall. In the early Pliocene, global temperatures were 1–2˚C warmer than the present temperature, yet sea level was 15–25 meters higher than today.\n\nAccording to recent studies, global-mean sea level rose by 195 mm during the period from 1870 to 2004. Since 2004, satellite-based records indicate that there has been a further 43 mm of global-mean sea levels rise, .\n\nAnalysis of ice in a core drilled from an ice sheet such as the Antarctic ice sheet, can be used to show a link between temperature and global sea level variations. The air trapped in bubbles in the ice can also reveal the CO variations of the atmosphere from the distant past, well before modern environmental influences. The study of these ice cores has been a significant indicator of the changes in CO over many millennia, and continues to provide valuable information about the differences between ancient and modern atmospheric conditions.\n\nPast precipitation can be estimated in the modern era with the global network of precipitation gauges. Surface coverage over oceans and remote areas is relatively sparse, but, reducing reliance on interpolation, satellite clouds and precipitation data has been available since the 1970s. Quantification of climatological variation of precipitation in prior centuries and epochs is less complete but approximated using proxies such as marine sediments, ice cores, cave stalagmites, and tree rings. In July 2016 scientists published evidence of increased cloud cover over polar regions, as predicted by climate models.\n\nClimatological temperatures substantially affect cloud cover and precipitation. For instance, during the Last Glacial Maximum of 18,000 years ago, thermal-driven evaporation from the oceans onto continental landmasses was low, causing large areas of extreme desert, including polar deserts (cold but with low rates of cloud cover and precipitation). In contrast, the world's climate was cloudier and wetter than today near the start of the warm Atlantic Period of 8000 years ago.\n\nEstimated global land precipitation increased by approximately 2% over the course of the 20th century, though the calculated trend varies if different time endpoints are chosen, complicated by ENSO and other oscillations, including greater global land cloud cover precipitation in the 1950s and 1970s than the later 1980s and 1990s despite the positive trend over the century overall.\nSimilar slight overall increase in global river runoff and in average soil moisture has been perceived.\n\nA change in the type, distribution and coverage of vegetation may occur given a change in the climate. Some changes in climate may result in increased precipitation and warmth, resulting in improved plant growth and the subsequent sequestration of airborne CO. A gradual increase in warmth in a region will lead to earlier flowering and fruiting times, driving a change in the timing of life cycles of dependent organisms. Conversely, cold will cause plant bio-cycles to lag. Larger, faster or more radical changes, however, may result in vegetation stress, rapid plant loss and desertification in certain circumstances. An example of this occurred during the Carboniferous Rainforest Collapse (CRC), an extinction event 300 million years ago. At this time vast rainforests covered the equatorial region of Europe and America. Climate change devastated these tropical rainforests, abruptly fragmenting the habitat into isolated 'islands' and causing the extinction of many plant and animal species. Such stress can alter the growth rate of trees, which allows scientists to infer climate trends by analyzing the growth rate of tree rings. This branch of climate science is called dendroclimatology, and is one of the many ways they research climate trends prior to written records.\n\nEven though this is a field with many uncertainties, it is expected that over the next 50 years climate changes will have an effect on the diversity of forest genetic resources and thereby on the distribution of forest tree species and the composition of forests. Diversity of forest genetic resources enables the potential for a species (or a population) to adapt to climatic changes and related future challenges such as temperature changes, drought, pests, diseases and forest fire. However, species are not naturally capable to adapt in the pace of which the climate is changing and the increasing temperatures will most likely facilitate the spread of pests and diseases, creating an additional threat to forest trees and their populations. To inhibit these problems human interventions, such as transfer of forest reproductive material, may be needed.\n\nPalynology is the study of contemporary and fossil palynomorphs, including pollen. Palynology is used to infer the geographical distribution of plant species, which vary under different climate conditions. Different groups of plants have pollen with distinctive shapes and surface textures, and since the outer surface of pollen is composed of a very resilient material, they resist decay. Changes in the type of pollen found in different layers of sediment in lakes, bogs, or river deltas indicate changes in plant communities. These changes are often a sign of a changing climate. As an example, palynological studies have been used to track changing vegetation patterns throughout the Quaternary glaciations and especially since the last glacial maximum.\n\nRemains of beetles are common in freshwater and land sediments. Different species of beetles tend to be found under different climatic conditions. Given the extensive lineage of beetles whose genetic makeup has not altered significantly over the millennia, knowledge of the present climatic range of the different species, and the age of the sediments in which remains are found, past climatic conditions may be inferred.\nThe studies of the impact in vertebrates are few mainly from developing countries, where there are the fewest studies; between 1970 and 2012, vertebrates declined by 58 percent, with freshwater, marine, and terrestrial populations declining by 81, 36, and 35 percent, respectively.\n\nSimilarly, the historical abundance of various fish species has been found to have a substantial relationship with observed climatic conditions. Changes in the primary productivity of autotrophs in the oceans can affect marine food webs.\n\nClimate change has already led to the alteration in geographical distribution of various human disease vectors. Both migration to cooler climates and higher elevations has been observed. It has been suggested that these migration events could in some cases lead to greater disease transmission. However, extinction events may also be expected which could possibly decrease the number of vectors in a given area.\n\nTemperature alone can have an effect on vector biting rates, reproductive cycles, and survival rates. It has been suggested that an increase in global temperature may lessen the potential for seasonally lower temperatures which cyclically decrease vector populations. Humidity and rainfall also have an effect on vector population dynamics. Temperature also affects the survival of the pathogens carried by vectors.\n\nThere is significant variability in how various vector borne diseases are impacted by climate change. Climate change has had a mixed effect on Malaria in Africa. Drought in some areas has led to decreased malaria transmission risk, whilst other areas have become more suitable to transmission through increased rainfall. Many confounding variables also make the association between climate change and malaria transmission in Africa difficult to assess. Improved infrastructure and socioeconomic factors along with basic healthcare and preventive care can decrease the risk of transmission and mortality. However, the lack of effective healthcare interventions and other protective factors make Dengue fever more prone to the effects of climate change than malaria. Similarly to malaria, an increase in precipitation and temperature has led to a higher population density of the mosquitoes responsible for Dengue fever and an increase in transmission rates. But, in contrast with malaria, urbanization appears to be positively associated with transmission of the virus. It has therefore been suggested that efforts to control the spread of Dengue in the wake of climate change may be less effective than those directed towards malaria.\n\nChanges in human and animal migration patterns due to climate change have caused an increased in prevalence of vector borne diseases. For example, drought and higher temperatures have led to human migration to water sources, where fly vectors for Leishmaniasis preside. Thus, behavioral alterations due to climate change can cause an increase in prevalence of vector borne diseases. Climate change can also affect migration patterns of vectors, such as those that carry hemorrhagic fever viruses. Increasing temperatures at higher altitudes have led to migration of new species into these areas which can carry vector borne diseases.\n\nClimate change has been shown to cause changes to weather patterns, affecting temperature, wind patterns, precipitation, etc. These changes in weather affect human health outcomes by increasing the rate of major natural disasters, physical trauma, and infections, especially impacting vulnerable, lower income communities . According to a World Health Organization (WHO) report, climate change has already caused 150,000 deaths and lost 5.5 million disability adjusted life years (DALYs), a measure of years of life affected by disability rather than death . It has been estimated that by 2020, an increase in the number of climate change related deaths would be seen due to heat wave induced cardiovascular disease, floods, and vector borne diseases, like malaria. By 2030, it is estimated that adverse health outcomes would double due to climate change .\n\n   The rise in temperatures due to climate change, estimated to be around 1.4 to 5.88 degrees Celsius, may increase the frequency and severity of heat waves. Heat waves are associated with higher mortality rates, especially in vulnerable populations . The elderly population are more likely to be impacted by the higher temperatures in a heatwave, often perishing from cardiovascular, respiratory, and cerebrovascular causes of death. Other vulnerable populations, such as immunocompromised individuals, the mentally ill population, and children, have an increased mortality rate during heat waves. Urban islands, pockets of land in urban areas where human changes to the landscape can exacerbate the effect of increasing temperatures, are also associated with higher mortality rates during heat waves. Heatwaves can also cause an increase in air pollution and humidity levels, thus increasing rates of mortality. Despite the increase in death rates during heat waves, adaptations for higher temperatures, like increased quality of healthcare and awareness of public health, are known to decrease the effect of climate change on the number of deaths due to heat waves.\n\n   Climate change can cause an increase in precipitation, increasing the likelihood of rapid rising floods. These floods raise mortality rates by increasing drowning related deaths. Mortality rates also increase due to infectious diseases and exposure of toxic pollutants after these floods. The increase in rainfall leads to pollutants entering the water system, often contaminating drinking water with sewage, animal feces, pathogens, etc.. Floods also lead to growth of fungal species and habituation of vectors of infectious diseases in previously unexposed areas, propagating the spread of vector borne diseases. Long term effects on human health are also known to be caused by flooding. Malnutrition and mental disorders, along with gastrointestinal and respiratory problems are known to increase after flooding. This most commonly occurs in less wealthy countries or areas that have more people residing in vulnerable areas and a lack of governmental aid for natural disasters and public health structures. It has been shown that the due increased precipitation from climate change, the number of people worldwide at risk of a flood would increase from 75 million to 200 million. \n\n   The changing weather patterns due to climate change cause more droughts, by decreasing levels of groundwater. The lack of groundwater leads to a decrease in health of forest trees, leading to an increase risk of wildfires. Wildfires increase the risk of physical and respiratory damage to the human body. Changing weather patterns caused by climate change can also damage crops leading to malnutrition. New wind patterns can present crops with novel pathogens and decrease the number of available pollinators which usually serve a protective role. Habitats are often affected by these changes of weather. Changes in temperature and rainfall have damaged coral reefs by introducing new pathogens and inducing physical trauma by storms. The damaged reefs increase the levels of salt that are taken up by tropical fishes eaten by locals, which may lead to adverse health outcomes.\n\nClimate change also causes more extreme weather. It is stated that climate change increases the severity of tropical storms, like Hurricane Katrina. Winter storms may become more severe because climate change increases precipitation levels and the strength of winds. Stronger storms lead to more problems with traveling and increase chances of physical trauma.\n\nThe transmission of infectious diseases are affected by changes in climate, by changing levels of humidity, precipitation, and temperature. Warmer temperatures cause land species to inhabit previously cold areas and invade areas closer to human dwellings, increasing the risk of transmission of vector borne diseases. Other factors like overcrowding and poverty levels can multiply the effect of climate change on outbreaks infectious diseases.\n\nClimate change also affects air pollution. Due to increased temperature caused by climate change, ozone pollutants are formed faster. Increasing levels of ozone lead to a rise in mortality rate caused by these pollutants. Changing wind patterns and levels of precipitation affect distribution of air pollutants, and may cause more wildfires that increase the risk of physical and respiratory trauma. Climate change also increases rates of asthma by increasing temperatures and changing wind patterns. These changes increase the levels and distribution of plant based irritants, like pollen and fungi. Climate change also raises levels of carbon dioxide, which affects the growth cycle of fungi, causing higher levels fungi based allergens.\n\nClimate change in the recent past may be detected by corresponding changes in settlement and agricultural patterns. Archaeological evidence, oral history and historical documents can offer insights into past changes in the climate. Climate change effects have been linked to the rise and also the collapse of various civilizations.\nHistorical climatology is the study of historical changes in climate and their effect on human history and development. The primary sources include written records such as sagas, chronicles, maps and local history literature as well as pictorial representations such as paintings, drawings and even rock art. This differs from paleoclimatology which encompasses climate change over the entire history of Earth. Notable climate events known to paleoclimatology are provided in this list of periods and events in climate history.\n\nPrior to the 18th century, scientists had not suspected that prehistoric climates were different from the modern period. By the late 18th century, geologists found evidence of a succession of geological ages with changes in climate. There were various competing theories about these changes, and James Hutton, whose ideas of cyclic change over huge periods of time were later dubbed uniformitarianism, was among those who found signs of past glacial activity in places too warm for glaciers in modern times. In 1815 Jean-Pierre Perraudin described for the first time how glaciers might be responsible for the giant boulders seen in alpine valleys.\n\nBy the end of the 19th century, scientific opinion had turned decisively against any belief in a human influence on climate. And whatever the regional effects, few imagined that humans could affect the climate of the planet as a whole. However, in 1899 Thomas Chrowder Chamberlin developed at length the idea that changes in climate could result from changes in the concentration of atmospheric carbon dioxide.\n\nIn 1967, taking advantage of the ability of digital computers to integrate absorption curves numerically, Syukuro Manabe and Richard Wetherald made the first detailed calculation of the greenhouse effect incorporating convection (the \"Manabe-Wetherald one-dimensional radiative-convective model\"). They found that, in the absence of unknown feedbacks such as changes in clouds, a doubling of carbon dioxide from the current level would result in approximately 2 °C increase in global temperature.\n\nIn 1985 a joint UNEP/WMO/ICSU Conference on the \"Assessment of the Role of Carbon Dioxide and Other Greenhouse Gases in Climate Variations and Associated Impacts\" concluded that greenhouse gases \"are expected\" to cause significant warming in the next century and that some warming is inevitable. Since the 1990s, scientific research on climate change has included multiple disciplines and has expanded. Research during this period has been summarized in the Assessment Reports by the Intergovernmental Panel on Climate Change.\n\n\n"}
{"id": "18859574", "url": "https://en.wikipedia.org/wiki?curid=18859574", "title": "Constant current", "text": "Constant current\n\nA constant current (steady current, time-independent current, stationary current) is a type of Direct Current (DC) that does not change its intensity with time.\n\nIf the load is constant, a steady current can be obtained via a constant voltage source. If the load is varying, a steady current can be obtained via a constant current supply source. \n\nAn electrochemical cell is a device capable of either generating electrical energy from chemical reactions or facilitating chemical reactions through the introduction of electrical energy. A common example of an electrochemical cell is a standard 1.5-volt cell meant for consumer use. This type of device is known as a single Galvanic cell, so an obsolete name for steady current was galvanic current. A \"battery\" consists of two or more cells, connected in either parallel or series pattern.\n\nA homopolar generator is an electrical generator comprising an electrically conductive disc or cylinder rotating in a plane perpendicular to a uniform static magnetic field. A magnetohydrodynamic generator directly extracts electric power from moving hot gases through a magnetic field, without the use of rotating electromagnetic machinery.\n\nAC generators can be used as sources of steady current with a rectifier and a good ripple filter. Pulsed DC generators can be used as sources of steady current with a good ripple filter.\n\nIn electronics, a constant current system is one that varies the voltage across a load to maintain a constant electric current. When a component is indicated to be driven by a constant current, the driver circuit is, in essence, a current regulator and must appear to the component as a current source of suitable reliability. \n\nAn important usage of constant current power supplies is with LEDs. While a high series resistance is sufficient to light an LED, sometimes the design needs to guard against high current (or risk burning out the LEDs). \n\nAnother use is in fluorescent lamps which have very dynamic electrical resistance and are optimally operated within a short range of currents. Other uses include shielded metal arc welding and gas tungsten arc welding.\n\n\n"}
{"id": "17807600", "url": "https://en.wikipedia.org/wiki?curid=17807600", "title": "Copper–chlorine cycle", "text": "Copper–chlorine cycle\n\nThe copper–chlorine cycle (Cu–Cl cycle) is a four-step thermochemical cycle for the production of hydrogen. The Cu–Cl cycle is a hybrid process that employs both thermochemical and electrolysis steps.\nIt has a maximum temperature requirement of about 530 degrees Celsius. \n\nThe Cu–Cl cycle involves four chemical reactions for water splitting, whose net reaction decomposes water into hydrogen and oxygen. All other chemicals are recycled. The Cu–Cl process can be linked with nuclear plants or other heat sources such as solar and industrial waste heat to potentially achieve higher efficiencies, lower environmental impact and lower costs of hydrogen production than any other conventional technology. \n\nThe Cu–Cl cycle is one of the prominent thermochemical cycles under development within the Generation IV International Forum (GIF). Through GIF, over a dozen countries around the world are developing the next generation of nuclear reactors for highly efficient production of both electricity and hydrogen.\n\nThe four reactions in the Cu–Cl cycle are listed as follows:\n\nLegend: (\"g\")—gas; (\"l\")—liquid; (\"aq\")—aqueous solution; the balance of the species are in a solid phase. Atomic Energy of Canada Limited has demonstrated experimentally a CuCl electrolyzer in which hydrogen is produced electrolytically at the cathode and Cu(I) is oxidized to Cu(II) at the anode, thereby combining above steps 1 and 4 to eliminate the intermediate production and subsequent transport of solid copper.\n\nApproximately 50% of the heat required to drive this reaction can be captured from the reaction itself. The other heat can be provided by any suitable process. Recent research has focused on a cogeneration scheme using the waste heat from nuclear reactors, specifically the CANDU supercritical water reactor.\n\nAdvantages of the copper–chlorine cycle include lower operating temperatures, the ability to use low-grade waste heat to improve energy efficiency, and potentially lower cost materials. In comparison with other thermochemical cycles, the Cu–Cl process requires relatively low temperatures of up to . \n\nAnother significant merit of this cycle is a relatively low voltage (thus low electrical energy expenditure) that is required for the electrochemical step (0.6 to 1.0 V, perhaps even 0.5 if lower current density can be achieved). The overall efficiency of the Cu–Cl cycle has been estimated to be just over 43%, excluding the additional potential gains of utilizing waste heat in the cycle.\n\nSolids handling between processes and corrosive working fluids present unique challenges for the engineering equipment development. Among others, the following materials are being currently used: spray coatings, nickel alloys, glass-lined steel, refractory materials, and other advanced materials.\n\n"}
{"id": "52925485", "url": "https://en.wikipedia.org/wiki?curid=52925485", "title": "Cubinder", "text": "Cubinder\n\nIn four-dimensional geometry, the cubinder (otherwise known as cubical cylinder or hypercylinder) is one way to generalise the 3D cylinder to 4D. Like the duocylinder and spherinder, it is also analogous to a cylinder in 3-space, which is the Cartesian product of a disk with a line segment.\n\nThe Cubinder is the Cartesian Product of a circle and a square, and it is a rotachoron. It can be constructed by extrusion of a 3D cylinder along the W- axis by a unit distance. The net of a cubinder is given by a single cube, surrounded by 4 cylinders.\n\nThe square torus that binds the cubinder forms a circular surface on which the cubinder can roll. Like a circle and a cylinder, it can only roll the space of a line. The cubinder cannot roll on the 4 cylinders, as they are flat in 4D.\nThe diagram is a perspective projection of the cubinder. The cubinder is rotated 45 degrees in the ZW plane. This allows us to observe that the cubinder is made up of four cylinders. However the square torus joining the cylinders cannot be observed from this perspective. \n\nIn 4-space, there are three intermediate forms between the tesseract (1 ball × 1 ball × 1 ball × 1 ball) and the hypersphere (4-ball). These are as follows:\n\nThe cubinder is also a rotatope (more specifically a rotachoron), along with other shapes such as the tesseract, duocylinder, spherinder, and glome. A rotachoron is a four dimensional shape which can by formed by extensions and rotations. The cubinder is a rotachoron as it can be formed via extension of a cylinder or rotation of a cube.\n\nThe rotation of cubinder to 5D is non-unique: rotating it around a cubic cross-section will lead to the shape, spherisquare. However, if you rotate it around its cylindrical cross-section, you will get a different rotatope called \"dual cylinder\", which can be also gotten by extending the duocylinder.\n\n"}
{"id": "28877298", "url": "https://en.wikipedia.org/wiki?curid=28877298", "title": "Deogyusan National Park", "text": "Deogyusan National Park\n\nDeogyusan National Park () is located in the provinces of Jeollabuk-do and Gyeongsangnam-do, South Korea. It was designated as the 10th national park in 1975. The park is home to a total of 1,067 plant species, 32 mammal species, 130 bird species, 9 amphibian species, 13 reptile species, 28 fish species, and 1,337 insect species. Endangered animals in the park include Flying squirrel, Marten and Otter.\n\n\n"}
{"id": "2604753", "url": "https://en.wikipedia.org/wiki?curid=2604753", "title": "Diquat", "text": "Diquat\n\nDiquat is a contact herbicide that produces desiccation and defoliation most often available as the dibromide, diquat dibromide. Brand names for this formulation include Aquacide, Dextrone, Preeglone, Deiquat, Spectracide, Detrone, Reglone, Reglon, Reglox, Tribune, Ortho-Diquat, Weedtrine-D, Weedol 2, and in combination with glyphosate, Resolva.\n\nDiquat is a nonselective herbicide that acts quickly to damage only those parts of the plant to which it is applied. It has been used in pre-harvest crop desiccation. It bonds strongly to mineral and organic particles in soil and water, where it remains without significant degradation for years. However, bound to clays, diquat is biologically inactive at concentrations typically observed in agricultural soils.\n\nDiquat dibromide is moderately toxic. It may be fatal to humans if swallowed, inhaled, or absorbed through the skin in large quantities.\n\nPyridine is oxidatively coupled to 2,2′-bipyridine over a heated Raney nickel catalyst. The ethylene bridge is formed by the reaction with 1,2-dibromoethane:\n\n"}
{"id": "26425260", "url": "https://en.wikipedia.org/wiki?curid=26425260", "title": "Directorate-General for Energy", "text": "Directorate-General for Energy\n\nThe Directorate-General for Energy (DG Ener), also internally the abbreviation ENER is used, is a Directorate-General of the European Commission. The DG Ener is in operation since 17 February 2010 when it was split from the Transport DG (it had been merged with Transport since 2000).\n\nDG ENER is focused on creating a competitive internal energy market to lower prices, to develop renewable energy sources, to reduce energy dependence and to reduce energy consumption.\n\nThe Directorate-General for Energy, based in Brussels, reports to Miguel Arias Cañete, European Commissioner for Energy and Climate Action and to Vice President of the Commission Maroš Šefčovič, responsible for Energy Union. The Directorate-General was headed by Phillip Lowe from March 2010 and by Dominique Ristori since January 2014.\n\nThe Directorate-General is made up of 6 Directorates (two of which deal with EURATOM issues), and the Euratom Supply Agency.\n\nThe Green Car Initiative (GCI) is included in the European Economic Recovery Plan (EERP) presented November 2008. Electrification of transport (electromobility) figures prominently in the GCI. DG TREN is supporting a large European \"electromobility\" project on electric vehicles and related infrastructure with a total budget of around € 50million as part of the Green Car Initiative.\n\n\n"}
{"id": "42683109", "url": "https://en.wikipedia.org/wiki?curid=42683109", "title": "Eastern Alberta Transmission Line", "text": "Eastern Alberta Transmission Line\n\nEastern Alberta Transmission Line is a 485 km long, 500 kV, bipolar, high-voltage direct current, overhead transmission line interconnecting Newell HVDC static inverter plant near Brooks, Alberta with Heathfield static inverter plant near Gibbons, Alberta - northeast of Edmonton. The transmission line includes 1387 transmission towers and the construction of the 500kV DC transmission line is completed since december 2014 at a total cost of C$1.8 billion. Work continues on the EATL converter stations. The project is the first newly built, long-distance, HVDC line in Canada since the completion of the Quebec – New England Transmission Project, and is owned by ATCO Electric, being built by Siemens. See also Western Alberta Transmission Line (WATL).<br>\nEastern Alberta Transmission Line uses as Western Alberta Transmission Line in opposite to other HVDC schemes as Nelson River Bipole an insulated, metallic conductor for the neutral pole, in order to avoid any risk of electrochemical corrosion of oil and gas pipelines.\n"}
{"id": "3574959", "url": "https://en.wikipedia.org/wiki?curid=3574959", "title": "Ecological pyramid", "text": "Ecological pyramid\n\nAn ecological pyramid (also trophic pyramid, eltonian pyramid, energy pyramid, or sometimes food pyramid) is a graphical representation designed to show the biomass or bio productivity at each trophic level in a given ecosystem.\n\n\"Biomass pyramids\" show how much biomass (the amount of living or organic matter present in an organism) is present in the organisms at each trophic level, while \"productivity pyramids\" show the procreation or turnover in biomass. There is also \"pyramid of numbers\" which represent the number of organisms in each trophic level. They may be upright (e.g. Grassland ecosystem), inverted (parasitic ecosystem) or dumbbell shaped (forest ecosystem).\n\nEnergy pyramids begin with producers on the bottom (such as plants) and proceed through the various trophic levels (such as herbivores that eat plants, then carnivores that eat flesh, then omnivores that eat both plants and flesh, and so on). The highest level is the top of the food chain.\n\nThe concept of pyramid of numbers (\"Eltonian pyramid\") was developed by Charles Elton (1927). Later, it would also be expressed in terms of biomass by Bodenheimer (1938).\n\nThe idea of pyramid of productivity or energy relies on works of G. Evelyn Hutchinson and Raymond Lindeman (1942).\n\nA \"pyramid of numbers\" shows graphically the population of each level in a food chain. It is an upright pyramid given in an ecosystem, where usually the producers are more in number than any other Trophic level. This shows the number of organisms in each trophic level without any consideration for their size. This type of pyramid can be convenient, as counting is often a simple task and can be done over the years to observe the changes in a particular ecosystem. However, some types of organisms are difficult to count, especially when it comes to some juvenile forms. Unit: number of organisms.\n\nA \"pyramid of biomass\" shows the relationship between biomass and trophic level by quantifying the biomass present at each trophic level of an energy community at a particular time. It is a graphical representation of biomass (total amount of living or organic matter in an ecosystem) present in unit area in different tropic levels. Typical units are grams per meter, or calories per meter.\nThe pyramid of biomass may be \"inverted\". For example, in a pond ecosystem, the standing crop of phytoplankton, the major producers, at any given point will be lower than the mass of the heterotrophs, such as fish and insects. This is explained as the phytoplankton reproduce very quickly, but have much shorter individual lives.\n\nOne problem with biomass pyramids is that they can make a trophic level appear to contain more energy than it actually does. For example, all birds have beaks and skeletons, which despite having mass are not typically digested by the next trophic level.\n\nA \"pyramid of productivity\" is often more useful, showing the production or turnover (the rate at which energy or mass is transferred from one trophic level to the next) of biomass at each trophic level. Instead of showing a single snapshot in time, productivity pyramids show the flow of energy through the food chain. Typical units are grams per meter per year or calories per meter per year. As with the others, this graph shows producers at the bottom and higher trophic levels on top.\n\nWhen an ecosystem is healthy, this graph produces a standard \"ecological pyramid\". This is because in order for the ecosystem to sustain itself, there must be more energy at lower trophic levels than there is at higher trophic levels. This allows organisms on the lower levels to not only to maintain a stable population, but also to transfer energy up the pyramid. The exception to this generalization is when portions of a food web are supported by inputs of resources from outside the local community. In small, forested streams, for example, the volume of higher levels is greater than could be supported by the local primary production.\n\nWhen energy is transferred to the next trophic level, typically only 10% or 12% of it is used to build new biomass, becoming stored energy (the rest going to metabolic processes) (Pauly and Christensen, 1995). In this case, in the \"pyramid of productivity\" each step will be 10% the size of the previous step (100,000, 10,000, 1,000, 100, 10, 1, .1, .01).\n\nThe advantages of the \"pyramid of productivity\" as a representation:\n\nThe disadvantages of the \"pyramid of productivity\" as a representation:\n\nNonetheless, productivity pyramids usually provide more insight into an ecological community when the necessary information is available.\n\n\n\n\n ( pyramid of energy) "}
{"id": "156706", "url": "https://en.wikipedia.org/wiki?curid=156706", "title": "Effective mass (solid-state physics)", "text": "Effective mass (solid-state physics)\n\nIn solid state physics, a particle's effective mass (often denoted ) is the mass that it \"seems\" to have when responding to forces, or the mass that it seems to have when interacting with other identical particles in a thermal distribution. One of the results from the band theory of solids is that the movement of particles in a periodic potential, over long distances larger than the lattice spacing, can be very different from their motion in a vacuum.\nThe effective mass is a quantity that is used to simplify band structures by modeling the behavior of a free particle with that mass.\nFor some purposes and some materials, the effective mass can be considered to be a simple constant of a material. In general, however, the value of effective mass depends on the purpose for which it is used, and can vary depending on a number of factors.\n\nFor electrons or electron holes in a solid, the effective mass is usually stated in units of the rest mass of an electron, \"m\" (9.11×10 kg). In these units it is usually in the range 0.01 to 10, but can also be lower or higher—for example, reaching 1,000 in exotic heavy fermion materials, or anywhere from zero to infinity (depending on definition) in graphene. As it simplifies the more general band theory, the electronic effective mass can be seen as an important basic parameter that influences measurable properties of a solid, including everything from the efficiency of a solar cell to the speed of an integrated circuit.\n\nAt the highest energies of the valence band in many semiconductors (Ge, Si, GaAs, ...), and the lowest energies of the conduction band in some semiconductors (GaAs, ...), the band structure can be locally approximated as\n\nwhere is the energy of an electron at wavevector in that band, is a constant giving the edge of energy of that band, and is a constant (the effective mass).\n\nIt can be shown that the electrons placed in these bands behave as free electrons except with a different mass, as long as their energy stays within the range of validity of the approximation above. As a result, the electron mass in models such as the Drude model must be replaced with the effective mass.\n\nOne remarkable property is that the effective mass can become \"negative\", when the band curves downwards away from a maximum. As a result of the negative mass, the electrons respond to electric and magnetic forces by gaining velocity in the opposite direction compared to normal; even though these electrons have negative charge, they move in trajectories as if they had positive charge (and positive mass). This explains the existence of valence-band holes, the positive-charge, positive-mass quasiparticles that can be found in semiconductors.\n\nIn any case, if the band structure has the simple parabolic form described above, then the value of effective mass is unambiguous. Unfortunately, this parabolic form is not valid for describing most materials. In such complex materials there is no single definition of \"effective mass\" but instead multiple definitions, each suited to a particular purpose. The rest of the article describes these effective masses in detail.\n\nIn some important semiconductors (notably, silicon) the lowest energies of the conduction band are not symmetrical, as the constant-energy surfaces are now ellipsoids, rather than the spheres in the isotropic case. Each conduction band minimum can be approximated only by\n\nwhere \"x\", \"y\", and \"z\" axes are aligned to the principal axes of the ellipsoids, and , and are the inertial effective masses along these different axes. The offsets , , and reflect that the conduction band minimum is no longer centered at zero wavevector. (These effective masses correspond to the principal components of the inertial effective mass tensor, described later.)\n\nIn this case, the electron motion is no longer directly comparable to a free electron; the speed of an electron will depend on its direction, and it will accelerate to a different degree depending on the direction of the force. Still, in crystals such as silicon the overall properties such as conductivity appear to be isotropic. This is because there are multiple valleys (conduction-band minima), each with effective masses rearranged along different axes. The valleys collectively act together to give an isotropic conductivity. It is possible to average the different axes' effective masses together in some way, to regain the free electron picture. However, the averaging method turns out to depend on the purpose:\n\nIn general the dispersion relation cannot be approximated as parabolic, and in such cases the effective mass should be precisely defined if it is to be used at all.\nHere a commonly stated definition of effective mass is the \"inertial\" effective mass tensor defined below; however, in general it is a matrix-valued function of the wavevector, and even more complex than the band structure itself.\nOther effective masses are more relevant to directly measurable phenomena.\n\nA classical particle under the influence of a force accelerates according to Newton's second law, .\nThis intuitive principle appears identically in semiclassical approximations derived from band structure. However, each of the symbols has a slightly modified meaning; acceleration becomes the rate of change in group velocity:\n\nwhere is the del operator in reciprocal space, and force gives a rate of change in crystal momentum :\n\nwhere is the \"reduced Planck constant\". Combining these two equations yields\n\nExtracting the th element from both sides gives\n\nwhere is the th element of , is the th element of , and are the th and th elements of , respectively, and is the total energy of the particle according to the Planck–Einstein relation. The index is contracted by the use of Einstein notation (there is an implicit summation over ). Since Newton's second law uses the inertial mass (not the gravitational mass), we can identify the inverse of this mass in the equation above as the tensor\n\nThis tensor expresses the change in group velocity due to a change in crystal momentum. Its inverse, , is known as the effective mass tensor.\n\nThe inertial expression for effective mass is commonly used, but note that its properties can be counter-intuitive:\n\nClassically, a charged particle in a magnetic field moves in a helix along the magnetic field axis. The period \"T\" of its motion depends on its mass \"m\" and charge \"e\",\n\nwhere \"B\" is the magnetic flux density.\n\nFor particles in asymmetrical band structures, the particle no longer moves exactly in a helix, however its motion transverse to the magnetic field still moves in a closed loop (not necessarily a circle). Moreover, the time to complete one of these loops still varies inversely with magnetic field, and so it is possible to define a \"cyclotron effective mass\" from the measured period, using the above equation.\n\nThe semiclassical motion of the particle can be described by a closed loop in k-space. Throughout this loop, the particle maintains a constant energy, as well as a constant momentum along the magnetic field axis. By defining to be the area enclosed by this loop (this area depends on the energy , the direction of the magnetic field, and the on-axis wavevector ), then it can be shown that the cyclotron effective mass depends on the band structure via the derivative of this area in energy:\n\nTypically, experiments that measure cyclotron motion (cyclotron resonance, de Haas–van Alphen effect, etc.) are restricted to only probe motion for energies near the Fermi level.\n\nIn two-dimensional electron gases, the cyclotron effective mass is defined only for one magnetic field direction (perpendicular) and the out-of-plane wavevector drops out. The cyclotron effective mass therefore is only a function of energy, and it turns out to be exactly related to the density of states at that energy via the relation formula_10, where is the valley degeneracy. Such a simple relationship does not apply in three-dimensional materials.\n\nIn semiconductors with low levels of doping, the electron concentration in the conduction band is in general given by \n\nwhere is the Fermi level, is the minimum energy of the conduction band, and is a concentration coefficient that depends on temperature. The above relationship for can be shown to apply for any conduction band shape (including non-parabolic, asymmetric bands), provided the doping is weak (); this is a consequence of Fermi–Dirac statistics limiting towards Maxwell–Boltzmann statistics.\n\nThe concept of effective mass is useful to model the temperature dependence of , thereby allowing the above relationship to be used over a range of temperatures. In an idealized three-dimensional material with a parabolic band, the concentration coefficient is given by\n\nIn semiconductors with non-simple band structures, this relationship is used to define an effective mass, known as the density of states effective mass of electrons. The name \"density of states effective mass\" is used since the above expression for is derived via the density of states for a parabolic band.\n\nIn practice, the effective mass extracted in this way is not quite constant in temperature ( does not exactly vary as ). In silicon, for example, this effective mass varies by a few percent between absolute zero and room temperature because the band structure itself slightly changes in shape. These band structure distortions are a result of changes in electron-phonon interaction energies, with the lattice's thermal expansion playing a minor role.\n\nSimilarly, the number of holes in the valence band, and the density of states effective mass of holes are defined by:\n\nwhere is the maximum energy of the valence band. Practically, this effective mass tends to vary greatly between absolute zero and room temperature in many materials (e.g., a factor of two in silicon), as there are multiple valence bands with distinct and significantly non-parabolic character, all peaking near the same energy.\n\nTraditionally effective masses were measured using cyclotron resonance, a method in which microwave absorption of a semiconductor immersed in a magnetic field goes through a sharp peak when the microwave frequency equals the cyclotron frequency formula_14. In recent years effective masses have more commonly been determined through measurement of band structures using techniques such as angle-resolved photo emission (ARPES) or, most directly, the de Haas–van Alphen effect. Effective masses can also be estimated using the coefficient γ of the linear term in the low-temperature electronic specific heat at constant volume formula_15. The specific heat depends on the effective mass through the density of states at the Fermi level and as such is a measure of degeneracy as well as band curvature. Very large estimates of carrier mass from specific heat measurements have given rise to the concept of heavy fermion materials. Since carrier mobility depends on the ratio of carrier collision lifetime formula_16 to effective mass, masses can in principle be determined from transport measurements, but this method is not practical since carrier collision probabilities are typically not known a priori. The optical Hall effect is an emerging technique for measuring the free charge carrier density, effective mass and mobility parameters in semiconductors. The optical Hall effect measures the analogue of the quasi-static electric-field-induced electrical Hall effect at optical frequencies in conductive and complex layered materials. The optical Hall effect also permits characterization of the anisotropy (tensor character) of the effective mass and mobility parameters.\n\nA variety of theoretical methods including density functional theory, k·p perturbation theory, and others are used to supplement and support the various experimental measurements described in the previous section, including interpreting, fitting, and extrapolating these measurements. Some of these theoretical methods can also be used for \"ab initio\" predictions of effective mass in the absence of any experimental data, for example to study materials that have not yet been created in the laboratory.\n\nThe effective mass is used in transport calculations, such as transport of electrons under the influence of fields or carrier gradients, but it also is used to calculate the carrier density and density of states in semiconductors. These masses are related but, as explained in the previous sections, are not the same because the weightings of various directions and wavevectors are different. These differences are important, for example in thermoelectric materials, where high conductivity, generally associated with light mass, is desired at the same time as high Seebeck coefficient, generally associated with heavy mass. Methods for assessing the electronic structures of different materials in this context have been developed.\n\nCertain III-V compounds such as GaAs and InSb have far smaller effective masses than tetrahedral group IV materials like Si and Ge. In the simplest Drude picture of electronic transport, the maximum obtainable charge carrier velocity is inversely proportional to the effective mass: formula_17 where formula_18 with formula_19 being the electronic charge. The ultimate speed of integrated circuits depends on the carrier velocity, so the low effective mass is the fundamental reason that GaAs and its derivatives are used instead of Si in high-bandwidth applications like cellular telephony.\n\nIn April 2017, researchers at Washington State University claimed to have created a fluid with negative effective mass inside a Bose–Einstein condensate, by engineering the dispersion relation.\n\n\n"}
{"id": "24878773", "url": "https://en.wikipedia.org/wiki?curid=24878773", "title": "Element Markets", "text": "Element Markets\n\nElement Markets, LLC is a limited liability company that provides environmental asset management and compliance services to institutional clients in North America. Its services span the greenhouse gas, emissions, and renewable energy credit markets. Element Markets also develops renewable energy and greenhouse gas emission reduction (carbon offset) projects. Element Markets was founded in 2005 and is headquartered in Houston, Texas.\n\nElement Markets is among the major companies in the environmental markets and has been recognized as such in industry publications, such as \"Environmental Finance\" and \"Energy Risk\" magazines.\n\nIn December 2008, Element Markets installed a solar photovoltaic power system on the roof of the Shops at Mission Viejo, a California mall owned by the Simon Property Group. At the time of installation, it was the largest rooftop solar project ever done by a mall operator.\n\nOn August 5, 2009, Element Markets completed the first-ever physical delivery of a Renewable Energy Credit (REC) futures contract on the Chicago Climate Futures Exchange.\n\n\n"}
{"id": "8418197", "url": "https://en.wikipedia.org/wiki?curid=8418197", "title": "Energy Saving Module", "text": "Energy Saving Module\n\nEnergy Saving Modules (ESM) reduce the electricity consumption (kWh) and maximum demand (kW) of air conditioning and refrigeration compressors. The concept was developed in Australia in 1983 by Abbotly Technologies and is now distributed by Smartcool Systems Inc. The system works in conjunction with existing HVAC controls ensuring that compressors work at maximum efficiency, while maintaining desired temperature levels. By preventing over-cycling, known as ‘Compressor Optimisation' consumption of electricity is cut by between 15% and 25%.\n\nConventional controls, including Building and Energy Management Systems and state-of-the-art refrigeration controls, often operate only on reaching pre-programmed static values to switch compressors off and on or adjust capacity. When the measured medium is within the dead band, the system and controllers remain idle. \n\nThe \"energy saving module\" is a computer that records the switching values of the primary controller and also measures the 'rate of change' of both the rise and fall of temperatures during the operating cycle. With this data the \"energy saving module\" computes a reference heat load to match the cooling capacity and then calculates operating parameters. This calculation is used to minimize compressor operation within the switching values, with a resultant reduction in refrigeration and air conditioning compressor run time and reduced electricity consumption. By dynamically measuring the heat load and adjusting the control differential in proportion to the cooling demand it's possible to dynamically control the cycle rate of the compressors. This is achieved while maintaining the desiredoperating temperature.\n\n\n\n"}
{"id": "8808087", "url": "https://en.wikipedia.org/wiki?curid=8808087", "title": "Fatty acid synthase", "text": "Fatty acid synthase\n\nFatty acid synthase (FAS) is an enzyme that in humans is encoded by the \"FASN\" gene.\n\nFatty acid synthase is a multi-enzyme protein that catalyzes fatty acid synthesis. It is not a single enzyme but a whole enzymatic system composed of two identical 272 kDa multifunctional polypeptides, in which substrates are handed from one functional domain to the next.\n\nIts main function is to catalyze the synthesis of palmitate (C16:0, a long-chain saturated fatty acid) from acetyl-CoA and malonyl-CoA, in the presence of NADPH.\n\nFatty acids are aliphatic acids fundamental to energy production and storage, cellular structure and as intermediates in the biosynthesis of hormones and other biologically important molecules. They are synthesized by a series of decarboxylative Claisen condensation reactions from acetyl-CoA and malonyl-CoA. Following each round of elongation the beta keto group is reduced to the fully saturated carbon chain by the sequential action of a ketoreductase (KR), dehydratase (DH), and enoyl reductase (ER). The growing fatty acid chain is carried between these active sites while attached covalently to the phosphopantetheine prosthetic group of an acyl carrier protein (ACP), and is released by the action of a thioesterase (TE) upon reaching a carbon chain length of 16 (palmitic acid).\n\nThere are two principal classes of fatty acid synthases. \n\nThe mechanism of FAS I and FAS II elongation and reduction is the same, as the domains of the FAS II enzymes are largely homologous to their domain counterparts in FAS I multienzyme polypeptides. However, the differences in the organization of the enzymes - integrated in FAS I, discrete in FAS II - gives rise to many important biochemical differences.\n\nThe evolutionary history of fatty acid synthases are very much intertwined with that of polyketide synthases (PKS). Polyketide synthases use a similar mechanism and homologous domains to produce secondary metabolite lipids. Furthermore, polyketide synthases also exhibit a Type I and Type II organization. FAS I in animals is thought to have arisen through modification of PKS I in fungi, whereas FAS I in fungi and the CMN group of bacteria seem to have arisen separately through the fusion of FAS II genes.\n\nMammalian FAS consists of a homodimer of two identical protein subunits, in which three catalytic domains in the N-terminal section (-ketoacyl synthase (KS), malonyl/acetyltransferase (MAT), and dehydrase (DH)), are separated by a core region of 600 residues from four C-terminal domains (enoyl reductase (ER), -ketoacyl reductase (KR), acyl carrier protein (ACP) and thioesterase (TE)).\n\nThe conventional model for organization of FAS (see the 'head-to-tail' model on the right) is largely based on the observations that the bifunctional reagent 1,3-dibromopropanone (DBP) is able to crosslink the active site cysteine thiol of the KS domain in one FAS monomer with the phosphopantetheine prosthetic group of the ACP domain in the other monomer. Complementation analysis of FAS dimers carrying different mutations on each monomer has established that the KS and MAT domains can cooperate with the ACP of either monomer. and a reinvestigation of the DBP crosslinking experiments revealed that the KS active site Cys161 thiol could be crosslinked to the ACP 4'-phosphopantetheine thiol of either monomer. In addition, it has been recently reported that a heterodimeric FAS containing only one competent monomer is capable of palmitate synthesis.\n\nThe above observations seemed incompatible with the classical 'head-to-tail' model for FAS organization, and an alternative model has been proposed, predicting that the KS and MAT domains of both monomers lie closer to the center of the FAS dimer, where they can access the ACP of either subunit (see figure on the top right).\n\nA low resolution X-ray crystallography structure of both pig (homodimer) and yeast FAS (heterododecamer) along with a ~6 Å resolution electron cryo-microscopy (cryo-EM) yeast FAS structure have been solved.\n\nThe solved structures of yeast FAS and mammalian FAS show two distinct organizations of highly conserved catalytic domains/enzymes in this multi-enzyme cellular machine. Yeast FAS has a highly efficient rigid barrel-like structure with 6 reaction chambers which synthesize fatty acids independently, while the mammalian FAS has an open flexible structure with only two reaction chambers. However, in both cases the conserved ACP acts as the mobile domain responsible for shuttling the intermediate fatty acid substrates to various catalytic sites. A first direct structural insight into this substrate shuttling mechanism was obtained by cryo-EM analysis, where ACP is observed bound to the various catalytic domains in the barrel-shaped yeast fatty acid synthase. The cryo-EM results suggest that the binding of ACP to various sites is asymmetric and stochastic, as also indicated by computer-simulation studies\n\nMetabolism and homeostasis of fatty acid synthase is transcriptionally regulated by Upstream Stimulatory Factors (USF1 and USF2) and sterol regulatory element binding protein-1c (SREBP-1c) in response to feeding/insulin in living animals.\n\nAlthough liver X receptor (LXRs) modulate the expression of sterol regulatory element binding protein-1c (SREBP-1c) in feeding, regulation of FAS by SREBP-1c is USF-dependent.\n\nAcylphloroglucinols isolated from the fern \"Dryopteris crassirhizoma\" show a fatty acid synthase inhibitory activity.\n\nThe gene that codes for FAS has been investigated as a possible oncogene. FAS is upregulated in breast cancers and as well as being an indicator of poor prognosis may also be worthwhile as a chemotherapeutic target. FAS inhibitors are therefore an active area of drug discovery research.\n\nFAS may also be involved in the production of an endogenous ligand for the nuclear receptor PPARalpha, the target of the fibrate drugs for hyperlipidemia, and is being investigated as a possible drug target for treating the metabolic syndrome. Orlistat which is a gastrointenstinal lipase inhibitor also inhibits FAS and has a potential as a medicine for cancer.\n\nIn some cancer cell lines, this protein has been found to be fused with estrogen receptor alpha (ER-alpha), in which the N-terminus of FAS is fused in-frame with the C-terminus of ER-alpha.\n\nAn association with uterine leiomyomata has been reported.\n\n\n\n"}
{"id": "690861", "url": "https://en.wikipedia.org/wiki?curid=690861", "title": "Flash freezing", "text": "Flash freezing\n\nIn physics and chemistry, flash freezing is a naturally occurring phenomenon used commonly in the food industry and by meteorologists for the purpose of forecasting.\n\nThe process is also of great importance in atmospheric science, as its study is necessary for a proper climate model for the formation of ice clouds in the upper troposphere, which effectively scatter incoming solar radiation and prevent Earth from becoming overheated by the sun.\n\nFlash freezing is closely related to classical nucleation theory, which helps us understand many materials, phenomena and theories in related situations.\n\nFlash freezing refers to the process whereby objects are frozen in just a few hours by subjecting them to cryogenic temperatures, or through direct contact with liquid nitrogen at .\n\nWhen water is supercooled to temperatures below , it must freeze.\n\nFreezing water is a central issue for climate, geology and life. On earth, ice and snow cover 10% of the land and up to 50% of the Northern Hemisphere in winter. Polar ice caps reflect up to 90% of the sun’s incoming radiation. The science of freezing water depends on multiple factors, including how water droplets freeze, how much water is in the atmosphere, if water is in a liquid or crystal state, at what temperature it freezes, and whether it crystallizes from within or from the surface.\n\nThe freezing of nanoscale water or silicon liquid drops is initiated at a number of different distances from the centre of the droplet, providing new insights on a long-standing dispute in the field of material and chemical physics.\n\nWhen water is in a conventional freezer, a dynamic phase transition is triggered. The resulting ice depends on how quickly the system is cooled: If the water is cooled below its freezing point slowly, an ice crystal will result, rather than the poly-crystalline solid that flash freezing results in.\n\nFlash freezing is used in the food industry to quickly freeze perishable food items (see frozen food). In this case, food items are subjected to temperatures well below water's melting/freezing point. Thus, smaller ice crystals are formed, causing less damage to cell membranes.\n\nFlash freezing techniques are used to freeze biological samples quickly so that large ice crystals cannot form and damage the sample. This rapid freezing is done by submerging the sample in liquid nitrogen or a mixture of dry ice and ethanol.\n\nAmerican inventor Clarence Birdseye developed the \"quick-freezing\" process of food preservation in the 20th century. This process was further developed by American inventor Daniel Tippmann by producing a vacuum and drawing the cold air through palletized food.\n\nThe results have important implications in climate control research. One of the current debates is whether the formation of ice occurs near the surface or within the micrometre-sized droplets suspended in clouds. If it is the former, effective engineering approaches may be able to be taken to tune the surface tension of water so that the ice crystallization rate can be controlled.\n\nThere are phenomena like supercooling, in which the water is cooled below its freezing point, but the water remains liquid, if there are too few defects to seed crystallization. One can therefore observe a delay until the water adjusts to the new, below-freezing temperature. Supercooled liquid water must become ice at minus 48 C (minus 55 F) not just because of the extreme cold, but because the molecular structure of water changes physically to form tetrahedron shapes, with each water molecule loosely bonded to four others. This suggests the structural change from liquid to \"intermediate ice\". The crystallization of ice from supercooled water is generally initiated by a process called nucleation. Because of the speed and size of nucleation, which occurs within nanoseconds and nanometers.\n\nThe surface environment does not play a decisive role in the formation of ice and snow. The density fluctuations inside drops result in that the possible freezing regions cover the middle and the surface regions. The freezing from the surface or from within may be random. However, in the strange world of water, tiny amounts of liquid water theoretically still are present, even as temperatures go below minus 48 C (minus 55 F) and almost all the water has turned solid, either into crystalline ice or amorphous water. Below minus 48 C (minus 55 F), ice is crystallizing too fast for any property of the remaining liquid to be measured. The freezing speed directly influences the nucleation process and ice crystal size. A supercooled liquid will stay in a liquid state below the normal freezing point when it has little opportunity for nucleation; that is, if it is pure enough and has a smooth enough container. Once agitated it will rapidly become a solid.\nDuring the final stage of freezing, an ice drop develops a pointy tip, which is not observed for most other liquids, arises because water expands as it freezes. Once the liquid is completely frozen, the sharp tip of the drop attracts water vapor in the air, much like a sharp metal lightning rod attracts electrical charges. The water vapor collects on the tip and a tree of small ice crystals starts to grow. An opposite effect has been shown to preferentially extract water molecules from the sharp edge of potato wedges in the oven.\n\nIf a microscopic droplet of water is cooled very fast, it forms what is called a glass (low-density amorphous ice) in which all the tetrahedrons of water molecules are not lined up, but amorphous. The change in structure of water controls the rate at which ice forms. Depending on its temperature and pressure, water ice has 16 different crystalline forms in which water molecules cling to each other with hydrogen bonds. When water is cooled down, its structure becomes closer to the structure of ice, which is why the density goes down, and this should be reflected in an increased crystallization rate showing these crystalline forms.\n\nFor the understanding of flash freezing, various related quantities might be useful.\n\nCrystal growth or nucleation is the formation of a new thermodynamic phase or a new structure via self-assembly. Nucleation is often found to be very sensitive to impurities in the system. For nucleation of a new thermodynamic phase, such as the formation of ice in water below 0 °C, if the system is not evolving with time and nucleation occurs in one step, then the probability that nucleation has not occurred should undergo exponential decay. This can also be observed in the nucleation of ice in supercooled small water droplets. The decay rate of the exponential gives the nucleation rate and is given by\n\nformula_1\n\nWhere \n\nClassical nucleation theory is a widely used approximate theory for estimating these rates, and how they vary with variables such as temperature. It correctly predicts that the time needed for nucleation decreases extremely rapidly when supersaturated.\n\nNucleation can be divided into homogeneous nucleation and heterogeneous nucleation. First comes homogeneous nucleation, because this is much simpler. Classical nucleation theory assumes that for a microscopic nucleus of a new phase, the free energy of a droplet can be written as the sum of a bulk term, proportional to a volume and surface term.\n\nformula_6\n\nThe first term is the volume term, and, assuming that the nucleus is spherical, this is the volume of a sphere of radius formula_7. formula_8 is the difference in free energy per unit volume between the thermodynamic phase nucleation is occurring in, and the phase that is nucleating.\n\ncritical nucleus radius, at some intermediate value of formula_7, the free energy goes through a maximum, and so the probability of formation of a nucleus goes through a minimum. There is a least-probable nucleus occurs, i.e., the one with the highest value of formula_10 where\n\nformula_11\n\nThis is called the critical nucleus and occurs at a critical nucleus radius\n\nformula_12\n\nAddition of new molecules to nuclei larger than this critical radius decreases the free energy, so these nuclei are more probable.\n\nHeterogeneous nucleation, nucleation with the nucleus at a surface, is much more common than homogeneous nucleation. Heterogeneous nucleation is typically much faster than homogeneous nucleation because the nucleation barrier formula_2 is much lower at a surface. This is because the nucleation barrier comes from the positive term in the free energy formula_10, which is the surface term. Thus, in conclusion, the nucleation probability is highest at a surface instead of the centre of a liquid.\n\nThe Laplace pressure is the pressure difference between the inside and the outside of a curved surface between a gas region and a liquid region. The Laplace pressure is determined from the Young–Laplace equation given as\n\nformula_15.\n\nwhere formula_16 and formula_17 are the principal radii of curvature and formula_18 (also denoted as formula_19) is the surface tension.\n\nThe surface tension can be defined in terms of force or energy. The surface tension of a liquid is the ratio of the change in the energy of the liquid, and the change in the surface area of the liquid (that led to the change in energy). It can be defined as formula_20. This work W is interpreted as the potential energy.\n"}
{"id": "1669606", "url": "https://en.wikipedia.org/wiki?curid=1669606", "title": "HVDC Thailand–Malaysia", "text": "HVDC Thailand–Malaysia\n\nThe HVDC Thailand–Malaysia is a 110 kilometer long HVDC powerline between Khlong Ngae in Thailand at and Gurun in Malaysia. The HVDC Thailand–Malaysia, which crosses the border between Malaysia and Thailand at , serves for the coupling of the asynchronously operated power grids of Thailand and Malaysia and went in service in June 2002. The HVDC connection Thailand–Malaysia is a monopolar 300 kV overhead line with a maximum transmission rate of 300 megawatts. The terminal of the HVDC is situated east of Gurun at . The inverter hall is designed as Chinese style building.\n\n"}
{"id": "24844728", "url": "https://en.wikipedia.org/wiki?curid=24844728", "title": "Iași I Power Station", "text": "Iași I Power Station\n\nThe Iaşi I Power Station is a large thermal power plant located in Iaşi, having 4 generation groups, 2 of 50 MW each and 2 of 25 MW having a total electricity generation capacity of 150 MW.\n"}
{"id": "30938449", "url": "https://en.wikipedia.org/wiki?curid=30938449", "title": "Ice storage air conditioning", "text": "Ice storage air conditioning\n\nIce storage air conditioning is the process of using ice for thermal energy storage. This is practical because of water's large heat of fusion: one metric ton of water (one cubic metre) can store 334 megajoules (MJ) (317,000 BTU) of energy, equivalent to 93 kWh (26.4 ton-hours). \n\nIce was originally obtained from mountains or cut from frozen lakes and transported to cities for use as a coolant. The original definition of a \"ton of cooling capacity\" (heat flow) was the heat needed to melt one ton of ice in a 24-hour period. This heat flow is what one would expect in a house in Boston in the summer. This definition has since been replaced by less archaic units: one ton HVAC capacity is equal to 12,000 BTU per hour. A small storage facility can hold enough ice to cool a large building from one day to one week, whether that ice is produced by anhydrous ammonia chillers or hauled in by horse-drawn carts. \n\nGround freezing can also be utilized; this may be done in ice form where the ground is saturated. Systems will also work with pure rock. Wherever ice forms, the ice formation's heat of fusion is not used, as the ice remains solid throughout the process. The method based on ground freezing is widely used for mining and tunneling to solidify unstable ground during excavations. The ground is frozen using bore holes with concentric pipes that carry brine from a chiller at the surface. Cold is extracted in a similar way using brine and used in the same way as for conventional ice storage, normally with a brine-to-liquid heat exchanger, to bring the working temperatures up to usable levels at higher volumes. The frozen ground can stay cold for months or longer, allowing cold storage for extended periods at negligible structure cost.\n\nReplacing existing air conditioning systems with ice storage offers a cost-effective energy storage method, enabling surplus wind energy and other such intermittent energy sources to be stored for use in chilling at a later time, possibly months later.\n\nThe most widely used form of this technology can be found in campus-wide air conditioning or chilled water systems of large buildings. Air conditioning systems, especially in commercial buildings, are the biggest contributors to peak electrical loads seen on hot summer days in various countries. In this application, a standard chiller runs at night to produce an ice pile. Water then circulates through the pile during the day to produce chilled water that would normally be the chiller's daytime output.\n\nA partial storage system minimizes capital investment by running the chillers nearly 24 hours a day. At night, they produce ice for storage and during the day they chill water for the air conditioning system. Water circulating through the melting ice augments their production. Such a system usually runs in ice-making mode for 16 to 18 hours a day and in ice-melting mode for six hours a day. Capital expenditures are minimized because the chillers can be just 40 - 50% of the size needed for a conventional design. Ice storage sufficient to store half a day's rejected heat is usually adequate.\n\nA full storage system minimizes the cost of energy to run that system by entirely shutting off the chillers during peak load hours. The capital cost is higher, as such a system requires somewhat larger chillers than those from a partial storage system, and a larger ice storage system. Ice storage systems are inexpensive enough that full storage systems are often competitive with conventional air conditioning designs.\n\nThe air conditioning chillers' efficiency is measured by their coefficient of performance (COP). In theory, thermal storage systems could make chillers more efficient because heat is discharged into colder nighttime air rather than warmer daytime air. In practice, heat loss overpowers this advantage, since it melts the ice.\n\nAir conditioning thermal storage has been shown to be somewhat beneficial in society. The fuel used at night to produce electricity is a domestic resource in most countries, so less imported fuel is used. Also, studies show that this process significantly reduces the emissions associated with producing the power for air conditioners, since in the evening, inefficient \"peaker\" plants are replaced by low-emission base load facilities. The plants that produce this power often work more efficiently than the gas turbines that provide peaking power during the day. As well, since the load factor on the plants is higher, fewer plants are needed to service the load.\n\nA new twist on this technology uses ice as a condensing medium for the refrigerant. In this case, regular refrigerant is pumped to coils where it is used. Rather than needing a compressor to convert it back into a liquid, however, the low temperature of ice is used to chill the refrigerant back into a liquid. This type of system allows existing refrigerant-based HVAC equipment to be converted to Thermal Energy Storage systems, something that could not previously be easily done with chill water technology. In addition, unlike water-cooled chill water systems that do not experience a tremendous difference in efficiency from day to night, this new class of equipment typically displaces daytime operation of air-cooled condensing units. In areas where there is a significant difference between peak day time temperatures and off peak temperatures, this type of unit is typically more energy efficient than the equipment that it replaces. \n\nThermal energy storage is also used for combustion gas turbine air inlet cooling. Instead of shifting electrical demand to the night, this technique shifts generation capacity to the day. To generate ice at night, the turbine is often mechanically connected to a large chiller's compressor. During peak daytime loads, water is circulated between the ice pile and a heat exchanger in front of the turbine air intake, cooling the intake air to near freezing temperatures. Since the air is colder, the turbine can compress more air with a given amount of compressor power. Typically, both the generated electrical power and turbine efficiency rise when the inlet cooling system is activated. This system is similar to the compressed air energy storage system.\n\n"}
{"id": "17549613", "url": "https://en.wikipedia.org/wiki?curid=17549613", "title": "Joss Garman", "text": "Joss Garman\n\nJoss Garman (born 1985) is a British environmental and humanitarian campaigner who has worked as a campaign leader for Greenpeace UK, and as a director of The Syria Campaign. At the Institute for Public Policy Research he worked as Associate Director for energy, transport and climate change before becoming adviser to the UK Shadow Secretary of State for Energy & Climate Change Lisa Nandy MP.\n\nBorn in Radnorshire, Mid-Wales, he attended his local comprehensive school before going on to read Politics at SOAS, University of London.\n\nHis father, David Garman, is the inventor of the world’s first bath lift and in 2015 was appointed an OBE for services to the healthcare industry.\n\nIn August 2007 ahead of the Camp for Climate Action, Garman was named in a High Court injunction by airport operator BAA in a bid to prevent environmental protests at Heathrow.\n"}
{"id": "26298081", "url": "https://en.wikipedia.org/wiki?curid=26298081", "title": "Leonard L. Northrup Jr.", "text": "Leonard L. Northrup Jr.\n\nLeonard \"Lynn\" L. Northrup Jr. (March 18, 1918 – March 24, 2016) was an American engineer who was a pioneer of the commercialization of solar thermal energy. Influenced by the work of John Yellott,\nMaria Telkes, and Harry Tabor, Northrup's company designed, patented, developed and manufactured some of the first commercial solar water heaters, solar concentrators, solar-powered air conditioning systems, solar power towers and photovoltaic thermal hybrid systems in the United States. The company he founded became part of ARCO Solar, which in turn became BP Solar, which became the largest solar energy company in the world. Northrup was a prolific inventor with 14 US patents.\n\nLynn Northrup Jr., a fourth generation Texan, was the son of L. L. Northrup Sr., an inventor in his own right, and Dolly McKaskle Northrup, a retail entrepreneur, both members of pioneer Texas families. He was educated at Woodrow Wilson High School in Dallas, Texas, and received a BA from Southern Methodist University, a MS from the University of Denver, and a Master of Business Administration from Harvard Business School. Northrup served as a captain the United States Army Corps of Engineers during and shortly after World War II.\n\nAfter the War, Northrup went to work for Storm Vulcan, a Dallas company, where he invented a machine to clean aircraft engines. He also embarked on a venture to fit cars with air conditioning equipment, putting the machinery in the trunk and piping the cooled air through tubes in the headliner. This caught the interest of engineers from General Motors, who copied the system in Cadillacs in the late 1940s. \"After market\" automotive AC units were manufactured in Texas until the 1980s. He also sold some of the first air conditioning units, built by the Curtis Mathes Corporation, an early leader in manufacturing window units. Northrup married Jane Keliher and started a family in Dallas, where he designed and built one of the first single-family houses in the United States with central air conditioning. He founded a company to install air conditioning in residential and commercial buildings. With a Marketing Plan from G F Sweetman (CEO of American Awards Co), he became one of largest suppliers of Curtis Mathes fans and compressors in the nation. He also developed a company to manage, install, update, and clean air filtration systems.\n\nIn the late 1960s, Northrup bought a controlling interest in Donmark Corporation, a manufacturer of residential air conditioning and heating equipment from Curtis Mathes, his lifelong friend. Northrup promoted the use of “all electric” central heating and cooling equipment, building a manufacturing facility in Dallas and later in Hutchins, Texas and selling primarily to apartment developers. In designing these systems, Northrup focused on the total installed cost of the unit, including the framing and plumbing costs.\n\nDuring the mid-1970s, Northrup became interested in boosting the efficiency of air conditioning systems, and began looking at novel approaches, including water-source geothermal heat pumps, and the innovative use of scroll compressors in split system central air conditioning systems to achieve a higher efficiency rating, which have since become the standard compressor for high-efficiency residential air conditioning equipment.\n\nIn the early 1970s, before the Arab Oil Embargo and the spike in oil prices, Northrup became interested in the commercialization of solar thermal systems, particularly for heating potable water and swimming pools. Such systems had already been commercialized in other countries where climatic conditions were favorable, energy costs were high, and there was a tradition of scientific innovation notably solar power in Israel. Work in the United States had been limited to academia and a few companies in Arizona, Texas, and California. Northrup began experimenting with solar collectors to heat air, using finned heat exchangers, and engaged solar pioneer Professor John Yellott as a consultant on the absorptivity and emissivity or various surfaces and configurations, and on the transparency of various glasses and glazing material that exhibit the \"greenhouse effect\" - transparent to incoming solar radiation, but opaque to the re-radiation of infrared from the heated surface - hence a thermal trap or collector that exhibits the \"greenhouse effect\".\n\nAdditionally, he hired Maria Telkes, an expert on phase change materials, particularly molten salts, as a way to store thermal energy, and consulted with Israeli solar thermal pioneer Harry Tabor on surface coatings, including “black chrome” for solar panels. This work lead to the commercialization of flat panel solar water heaters, and solar pool heaters, marketed as Northrup Energy products directly and via dealers, with particular success in Hawaii, where solar thermosiphon systems could be used with antifreeze. With low temperature products in production and distribution, Northrup turned his attention to achieving higher temperatures – which would entail various methods of concentrating incoming insolation and tracking the sun - with varying degrees of success.\n\nNorthrup’s break-through technology was a collector that used a long curved acrylic fresnel lens to concentrate or focus sunlight at a theoretical ratio of approximately 12 to 1 onto a linear flat copper tube, coated with a variant of Dr. Tabor's “black chrome” absorptive surface. The array, approximately 10’ long, tracked the movement of the sun during the day (east to west), automatically, with the elevation generally fixed (at approximately the same angle from horizontal as the latitude of the installation). The tracking device was ingenious – it consisted of two photoelectric cells at the base of a tube with a baffle between them. Current from the cells went to a control board that controlled the tracking motor. When the cell output was equalized, the baffle and the tube would be pointing at the sun. This was sufficient to enable the array to track the sun’s azimuth and generate considerable heat, as reported in tests published in the ASHRAE Journal, which noted that \". . . the array of collectors . . .follow the sun from just after sunrise to just before sunset and often results in the collection of twice as many usable BTUs of energy at a higher temperature than provided by high quality flat plate collectors.\".\nThese arrays proved popular – and were used to drive absorption refrigeration equipment on large commercial installations at Trinity University in San Antonio, Texas, at Frenchman’s Reef Hotel in St Thomas, USVI, residences, and were sold to prominent individuals, including movie actor and entrepreneur Steve McQueen, actor Stuart Whitman and environmentalist Robert Redford.\n\nThe early success of these concentrating collectors was due in part to grants from the Department of Energy and its predecessor the Energy Research and Development Administration. They created a good deal of publicity for Northrup, Inc., including the cover of Popular Science Magazine and an article in Fortune Magazine that noted, \"By squeezing (sic) sunshine optically, Lynn Northrup's unique new rooftop solar collector produces higher temperatures than are obtainable from most solar heating systems now on the market\". Technically, these concentrating collectors were among the first commercially successful “east-west” tracking solar collectors. The fundamentals of their systems are still in use in daily azimuth and elevation tracking parabolic collectors. Most linear concentrating collectors are, as of this writing in 2010, of the less costly and less complicated seasonal tracking variety in the form of parabolic troughs.\n\nEmboldened with his success in concentrating collectors, Northrup turned his attention to achieving higher temperatures, with azimuth and elevation tracking mirrors \"heliostats\" focused on a central boiler – ie. solar thermal power towers. The most advanced power tower at the time was an experimental tower in France, the THEMIS solar furnace at Odeillo, in the Pyrenees Orientales, which like other power towers, had been used solely for scientific purposes. Northrup attended a conference there in 1973 along with Professor Yellott and Floyd Blake, a former Martin Marietta senior aerospace engineer who had become interested in solar thermal research.\n\nBy the late 1970s, just five years after testing the first low temperature solar thermal collectors, Northrup Energy had become the preeminent developer of solar thermal technology. This attracted the attention of investors, and suitors, including the Atlantic Richfield Company, “ARCO”. ARCO's Chairman, Robert O Anderson was personally interested in solar technology and visited the Northrup Energy facility. Northrup, Inc. merged with Atlantic Richfield, and ARCO Ventures changed its name to ARCO Solar. The Northrup Energy team under Floyd Blake and Jerry Anderson went on to design and build some of the first commercial solar power tower installations, notably \"Solar One\" near Barstow, California. The heat from another project was used to generate steam for tertiary oil recovery in Kern County, California and for electric power generation. Some heliostats were built to directly track the sun with arrays of photovoltaic cells to generate power for the utility grid, near Hesperia, California. The seven million watt installation near Barstow was later dismantled and shipped to Europe's first commercial solar thermal power station. This unit was the largest solar electric power generation facility in the world. Northrup and Blake were then featured in a documentary on solar energy, \"Harnessing the Sun\", narrated by actress Joan Hackett.\n\nNorthrup’s designs and work in flat plate solar thermal collectors, east–west tracking collectors and heliostats are still in use today and serve as the foundations of solar thermal technology. To acknowledge Prof. John Yellott's contributions to the advancements of solar thermal systems, Northrup endowed a chair at Arizona State in Yellott's honor. Northrup has, in turn, been honored for his contributions to the commercialization of solar technologies by the American Institute of Architects and the American Society of Heating, Refrigerating and Air-Conditioning Engineers.\n\n After the merger of Northrup, Inc. into ARCO, Northrup became engaged in real estate development, including the assembly of one of the largest tracts of land in downtown Dallas, where he entered into a joint venture with James Rouse’s Enterprise Development Company to build a festival marketplace. This project was sold to a Belgian investment group who failed to pursue the project with Rouse. Northrup helped fund the establishment of Rouse's Enterprise Foundation affordable housing projects in Dallas. He also assembled tracts on 1½ miles of shoreline on Lewisville Lake, a part of which later became the city of The Colony. Northrup acquired 5,000 acres outside San Antonio, Texas, which was put into a trust for his family, and assembled land now occupied by the regional Rolex headquarters, among others.\nNorthrup subsequently started American Limestone, an innovator in using surface quarried Texas limestone in building facades, using a patented method that employs thin panels of limestone as a veneer, attached to a metal grid without mortar. This popularized the use of rough cut limestone as a thin veneer in residential and commercial applications, including store fronts, such as Brooks Brothers, churches, bank buildings and municipal buildings. At the other extreme in size, Northrup has utilized massive blocks of limestone for their passive heating / cooling characteristics, and free-standing structural qualities, most notably donating the stone and devising the construction technique for the Cistercian Chapel in Irving, Texas, which reflects both a classic design aesthetic and construction technique of mortar-less masonry - a thoroughly modern building built in the rational style of Cistercian architecture. The architect, Gary Cunningham, said \"We wanted to build a church that would literally last for the next 900 years\".\n\nEngineering Achievement Award, American Society of Heating, Refrigeration, and Air Conditioning Engineers\n\nCitation of Honor Award, American Institute of Architects for pioneering development of solar energy as a viable industry\n\nHonorary Member of Beta Beta Beta, biological honor society in recognition of significant contributions and outstanding service to society and biology\n\nNorthrup's May 2006 patent describes a method with the potential for using water as a general refrigerant, as well as an economical method of desalinating water.\n\n\"Harnessing The Sun\" (1980) \n\n\n"}
{"id": "4554946", "url": "https://en.wikipedia.org/wiki?curid=4554946", "title": "List of Lepidoptera that feed on Silene", "text": "List of Lepidoptera that feed on Silene\n\nSilene species are used as food plants by the larvae of some Lepidoptera species including:\n\n\n\n\n\n"}
{"id": "47266228", "url": "https://en.wikipedia.org/wiki?curid=47266228", "title": "List of U.S. rivers by discharge", "text": "List of U.S. rivers by discharge\n\nThis is a list of rivers in the continental United States by average discharge (streamflow) in cubic feet per second. All rivers with average discharge more than 15,000 cubic feet per second are listed. Estimates are approximate, because data are variable with time period measured and also because many rivers lack a gauging station near their point of outflow.\n\n"}
{"id": "8384010", "url": "https://en.wikipedia.org/wiki?curid=8384010", "title": "List of fluid mechanics journals", "text": "List of fluid mechanics journals\n\nThis List of fluid mechanics journals consists of some of the leading scientific journals related to the field of fluid mechanics.\n\n\n"}
{"id": "50104507", "url": "https://en.wikipedia.org/wiki?curid=50104507", "title": "Locale (geographic)", "text": "Locale (geographic)\n\nLocale is the geographic place at which there is or was human activity. It does not include populated places, mines, and dams. Locale indicates locations of present more dispersed, periodic or temporary human activity, such as a crossroad, a camp, a farm, a landing, a railroad siding, a ranch, a windmill or one of any of the various types of agricultural, communication, infrastructure or transport stations where human activities are carried out. \n\nLocale also indicates locations of former locales and incidents of human activity, such as a battlefield or historic site, former locations of populated places such as a ghost town or ruins or an archaeological site.\n"}
{"id": "13077113", "url": "https://en.wikipedia.org/wiki?curid=13077113", "title": "MV Shelly", "text": "MV Shelly\n\nMV \"Shelly\" was a cargo ship that was built in Bulgaria in 1973. She sank off the Mediterranean coast of Israel in 2007 after the cruise ship rammed her and broke her in two. Two of \"Shelly\"s crew were killed.\n\nThe Ivan Dimitrov shipyard on the River Danube in Ruse, Bulgaria built the ship and launched her in 1973 as \"Zlatograd\". She changed hands in 1999 and was renamed \"Loti\". She changed hands again in 2002 and was renamed \"Dora\". In 2003 Israeli owners bought her, renamed her \"Shelly\" and registered her in Slovakia. She was crewed not by Israelis but by other nationalities, mainly Slovaks and Ukrainians.\n\nOn 30 August 2007, \"Shelly\" was at anchor about off Israeli coast, near the port of Haifa, when at about 10pm she was accidentally rammed by the Cypriot passenger ship \"Salamis Glory\", which had left port at Haifa several minutes before. \"Shelly\" sank quickly after the collision, which the Israel Broadcasting Authority said broke her in half. 11 crew members escaped, and most climbed aboard a rescue launch lowered by \"Salamis Glory\" and were subsequently rescued by the Israeli Navy. The rest were rescued by helicopter. The survivors refused to be taken to a local hospital for treatment. \"Salamis Glory\" subsequently returned to port in Haifa, showing slight damage to her hull. None of \"Salamis Glory\"s 700 or so passengers and crew was injured.\n\nThe remaining two crew members were declared missing, prompting a search and rescue operation involving six naval vessels, multiple aircraft and divers. 12 hours after the sinking their bodies were recovered from the wreck by divers in of water. The dead were identified as the ship's Indonesian First Mate and Ukrainian engineer.\n\nThe vessel's sinking released an oil spill that moved down the coast, causing authorities to warn the public not to bathe at the nearby Zevulun beach.\n\nOn 31 August Salamis Lines, owners of \"Salamis Glory\", arranged for 148 stranded Cypriot passengers from the ship to be flown back to Cyprus on a Cyprus Airways jet.\n\nIsraeli Police launched a full investigation into the cause of the accident, aided by Cypriot authorities. All of \"Salamis Glory\"s crew were interviewed. One possibility being considered was that there was a fault in the ship's navigation system, with some reports of a loss of steering control aboard \"Salamis Glory\".\n"}
{"id": "26582432", "url": "https://en.wikipedia.org/wiki?curid=26582432", "title": "March 2010 nor'easter", "text": "March 2010 nor'easter\n\nThe March 2010 nor'easter or St. Patrick's Day nor'easter was a powerful nor'easter that impacted the Northeastern United States and Eastern Canada from March 12–16, 2010, resulting in at least nine deaths. The slow-moving storm produced over of rain in New England, causing widespread flooding of urban and low-lying areas. Winds of up to snapped trees and power lines, resulting in over 1 million homes and businesses left without electricity. The storm also caused extensive coastal flooding and beach erosion. The nor'easter was the fifth major winter storm to impact the Mid-Atlantic and New England in the 2009–10 North American winter storm season.\n\nThe winter storm that would impact the Northeastern United States evolved when an area of low pressure moved northeastward from Texas to the Great Lakes region on March 10 and 11. A secondary low pressure center developed near Cape Hatteras, North Carolina and drifted northward to a position south of Cape Cod, Massachusetts by March 14. The system contained abundant moisture feeds from the tropical Pacific Ocean and Gulf of Mexico. Unlike the previous three winter storms that affected the region in February, there was a lack of cold air with this system and precipitation with this storm fell primarily as rain.\n\nLight to moderate rain spread north across the entire region through the day on Friday as the low pressure drifted north and slowly strengthened. With a strong fetch off the Atlantic Ocean, rainfall rates became heavy overnight Friday and through the evening of Saturday March 13th, resulting in small stream and eventually major river flooding. Meanwhile, a high pressure system anchored in the Canadian Maritimes also strengthened and a very strong pressure gradient developed between these pressure systems overnight Friday. This resulted in strong, damaging easterly winds across much of the area through the day Saturday, especially along the New Jersey coast where minor to moderate coastal flooding also occurred. The strong winds and widespread heavy rains slackened off overnight Saturday as the surface low was stationary across the Delmarva region. Showers and even a few thunderstorms continued to rotate in off of the Atlantic through the day on Sunday March 14th and continued into Monday March 15th as the low slowly moved eastward and finally out to sea by Tuesday morning March 16th.\n\nWinds of up to toppled trees and snapped power lines, with the heaviest damage reported in Fairfield County. Falling trees damaged or destroyed several homes in Connecticut, and the destruction was the worst experienced in Fairfield County since Hurricane Gloria struck the state in 1985. At the height of the storm on March 14, more than 110,000 customers were without electricity. By March 16, Connecticut Light and Power (CL&P) reported that 40,000 customers remained in the dark, and public schools remained closed for the entire week. Governor M. Jodi Rell criticized CL&P and promised an investigation after reports surfaced that the company delayed efforts to restore power to reduce employee overtime costs.\n\nUp to of rain combined with rapidly melting snow from earlier storms caused widespread urban flooding and forced rivers out of their banks across the state. Governor Deval Patrick declared a state of emergency and activated the National Guard to assist in the storm's aftermath. Flooding also shut down sections of commuter rail lines heading into and out of Boston, and caused sewage to overflow from treatment plants and into Boston Harbor.\n\nNew Jersey was particularly hard hit with flooding and wind damage and a state of emergency was declared as a result. The strong winds which frequently gusted 50-60 mph in most areas and up to 70 mph in spots wreaked the most havoc statewide, toppling numerous trees onto roads, cars, houses, and power lines causing widespread power outages. In Middlesex County, a large tree fell on a vehicle injuring two people. Wind and rain forced the closure of parts of the New Jersey Turnpike, a near complete shutdown of the NJ Transit system, and toppled a high rise crane in Atlantic City, NJ causing dangerous debris to drop to the ground. PSE&G reported about 459,000 customers lost power during the height of the storm on Saturday March 13th, making it the worst storm in the utility's history.\n\nOn Friday morning March 19th, nearly a week after the storm, 2,200 people in the state were still without power and many folks along the Passaic river in northeastern New Jersey were still dealing with major flooding. Thousands of businesses and residents in flood prone areas across the state received damage where major flooding occurred. Officials say more than 1,300 buildings in Morris County alone were damaged because of flooding along the Passaic, Ramapo and Pompton rivers. Preliminary estimates from flooding and wind damage in New Jersey alone are in the millions of dollars. Elsewhere in the area, flooding and wind damage was not as severe, but impacts from minor river flooding and downed trees and power lines were still felt in many areas.\n\nThe New York Metro area experienced intense conditions during the height of the storm which dropped on average 3-6\" of rain along with wind gusts over 75 mph. The hardest hit location in the metro area was Brooklyn, NY which experienced over 6\" of rain and wind gusts up to 85 mph. Long Island got hammered with hurricane-force wind gusts of 75-85 mph and heavy bands of rain. Many power outages were reported.\n\nIn southeastern Pennsylvania, PECO said that 135,000 customers lost power in the Philadelphia region, with Bucks County bearing the brunt. All of these customers had their power restored by Tuesday March 16th.\n\nThe state of Rhode Island received very heavy rain from the storm, and the Pawtuxet River flooded many towns in the state. Winds exceeded 50 mph at times and roadways were closed in Rhode Island and Connecticut.\n"}
{"id": "5532777", "url": "https://en.wikipedia.org/wiki?curid=5532777", "title": "Neurophysins", "text": "Neurophysins\n\nNeurophysins are carrier proteins which transport the hormones oxytocin and vasopressin to the posterior pituitary from the paraventricular and supraoptic nucleus of the hypothalamus, respectively. \n\nNeurophysins are also secreted out of the posterior pituitary hypothalamus, each carrying their respective associated passenger hormone. When the posterior pituitary hypothalamus secretes vasopressin and its neurophysin carrier, it also secretes a glycopeptide.\n\nThere are two types:\n\n\n"}
{"id": "1216060", "url": "https://en.wikipedia.org/wiki?curid=1216060", "title": "Nitride", "text": "Nitride\n\nIn chemistry, a nitride is a compound of nitrogen where nitrogen has a formal oxidation state of −3. Nitrides are a large class of compounds with a wide range of properties and applications.\n\nThe nitride ion, N, is never encountered in protic solution because it is so basic that it would be protonated immediately. Its ionic radius is estimated to be 140 pm.\n\nLike carbides, nitrides are often refractory materials owing to their high lattice energy which reflects the strong attraction of \"N\" for the metal cation. Thus, titanium nitride and silicon nitride are used as cutting materials and hard coatings. Hexagonal boron nitride, which adopts a layered structure, is a useful high-temperature lubricant akin to molybdenum disulfide. Nitride compounds often have large band gaps, thus nitrides are usually insulators or wide bandgap semiconductors; examples include boron nitride and silicon nitride. The wide band gap material gallium nitride is prized for emitting blue light in LEDs. Like some oxides, nitrides can absorb hydrogen and have been discussed in the context of hydrogen storage, e.g. lithium nitride.\n\nClassification of such a varied group of compounds is somewhat arbitrary. Compounds where nitrogen is not assigned −3 oxidation state are not included, such as nitrogen trichloride where the oxidation state is +3; nor are ammonia and its many organic derivatives.\n\nOnly one alkali metal nitride is stable, the purple-reddish lithium nitride (LiN), which forms when lithium burns in an atmosphere of N. Sodium nitride has been generated, but remains a laboratory curiosity. The nitrides of the alkaline earth metals have the formula MN are however numerous. Examples include BeN, MgN, CaN, and SrN. The nitrides of electropositive metals (including Li, Zn, and the alkaline earth metals) readily hydrolyze upon contact with water, including the moisture in the air:\n\nBoron nitride exists as several forms (polymorphs). Nitrides of silicon and phosphorus are also known, but only the former is commercially important. The nitrides of aluminium, gallium, and indium adopt diamond-like wurtzite structure in which each atom occupies tetrahedral sites. For example, in aluminium nitride, each aluminium atom has four neighboring nitrogen atoms at the corners of a tetrahedron and similarly each nitrogen atom has four neighboring aluminium atoms at the corners of a tetrahedron. This structure is like hexagonal diamond (lonsdaleite) where every carbon atom occupies a tetrahedral site (however wurtzite differs from sphalerite and diamond in the relative orientation of tetrahedra). Thallium(I) nitride, TlN is known, but thallium(III) nitride, TlN, is not.\n\nFor the group 3 metals, ScN and YN are both known. Group 4, 5, and 6 transition metals, that is the titanium, vanadium and chromium groups all form nitrides. They are refractory, with high melting point and are chemically stable. Representative is titanium nitride. Sometimes these materials are called \"interstitial nitrides.\"\n\nNitrides of the Group 7 and 8 transition metals decompose readily. For example, iron nitride, FeN decomposes at 200 °C. Platinum nitride and osmium nitride may contain N units, and as such should not be called nitrides.\nNitrides of heavier members from group 11 and 12 are less stable than copper nitride, CuN and ZnN: dry silver nitride (AgN) is a contact explosive which may detonate from the slightest touch, even a falling water droplet.\n\nMany metals form molecular nitrido complexes, as discussed in the specialized article. The main group elements also form some molecular nitrides. Cyanogen ((CN)) and tetrasulfur tetranitride (SN) are rare examples of a molecular binary (containing one element aside from nitrogen) nitrides. They dissolve in nonpolar solvents. Both undergo polymerization. SN is also unstable with respect to the elements, but less so that the isostructural SeN. Heating SN gives a polymer, and a variety of molecular sulfur nitride anions and cations are also known.\n\nRelated to but distinct from nitride is pernitride, .\n"}
{"id": "14668772", "url": "https://en.wikipedia.org/wiki?curid=14668772", "title": "Oil megaprojects (2014)", "text": "Oil megaprojects (2014)\n\nThis page summarizes projects that propose to bring more than of new liquid fuel capacity to market with the first production of fuel beginning in 2014. This is part of the Wikipedia summary of Oil Megaprojects.\n\nTerminology\n"}
{"id": "39368969", "url": "https://en.wikipedia.org/wiki?curid=39368969", "title": "Phases of fluorine", "text": "Phases of fluorine\n\nFluorine forms diatomic molecules () that are gaseous at room temperature with a density about 1.3 times that of air. Though sometimes cited as yellow-green, pure fluorine gas is actually a very pale yellow. The color can only be observed in concentrated fluorine gas when looking down the axis of long tubes, as it appears transparent when observed from the side in normal tubes or if allowed to escape into the atmosphere. The element has a \"pungent\" characteristic odor that is noticeable in concentrations as low as 20 ppb.\n\nFluorine condenses to a bright yellow liquid at −188 °C (−307 °F), which is near the condensation temperatures of oxygen and nitrogen. \n\nThe solid state of fluorine relies on Van der Waals forces to hold molecules together, which, because of the small size of the fluorine molecules, are relatively weak. Consequently, the solid state of fluorine is more similar to that of oxygen or the noble gases than to those of the heavier halogens.\nFluorine solidifies at −220 °C (−363 °F) into a cubic structure, called beta-fluorine. This phase is transparent and soft, with significant disorder of the molecules; its density is 1.70 g/cm. At −228 °C (−378 °F) fluorine undergoes a solid–solid phase transition into a monoclinic structure called alpha-fluorine. This phase is opaque and hard, with close-packed layers of molecules, and is denser at 1.97 g/cm. The solid state phase change requires more energy than the melting point transition and can be violent, shattering samples and blowing out sample holder windows.\nSolid fluorine received significant study in the 1920s and 30s, but relatively less until the 1960s. The crystal structure of alpha-fluorine given, which still has some uncertainty, dates to a 1970 paper by Linus Pauling.\n\n"}
{"id": "212141", "url": "https://en.wikipedia.org/wiki?curid=212141", "title": "Power station", "text": "Power station\n\nA power station, also referred to as a power plant or powerhouse and sometimes generating station or generating plant, is an industrial facility for the generation of electric power. Most power stations contain one or more generators, a rotating machine that converts mechanical power into electrical power. The relative motion between a magnetic field and a conductor creates an electrical current. The energy source harnessed to turn the generator varies widely. Most power stations in the world burn fossil fuels such as coal, oil, and natural gas to generate electricity. Others use nuclear power, but there is an increasing use of cleaner renewable sources such as solar, wind, wave and hydroelectric.\n\nIn 1870 a hydroelectric power station was designed and built by Lord Armstrong at Cragside, England. It used water from lakes on his estate to power Siemens dynamos. The electricity supplied power to lights, heating, produced hot water, ran an elevator as well as labor-saving devices and farm buildings.\n\nIn the early 1870s Belgian inventor Zénobe Gramme invented a generator powerful enough to produce power on a commercial scale for industry.\n\nIn the autumn of 1882, a central station providing public power was built in Godalming, England. It was proposed after the town failed to reach an agreement on the rate charged by the gas company, so the town council decided to use electricity. It used hydroelectric power for street lighting and household lighting. The system was not a commercial success and the town reverted to gas.\n\nIn 1882 the world's first coal-fired public power station, the Edison Electric Light Station, was built in London, a project of Thomas Edison organized by Edward Johnson. A Babcock & Wilcox boiler powered a 125-horsepower steam engine that drove a 27-ton generator. This supplied electricity to premises in the area that could be reached through the culverts of the viaduct without digging up the road, which was the monopoly of the gas companies. The customers included the City Temple and the Old Bailey. Another important customer was the Telegraph Office of the General Post Office, but this could not be reached though the culverts. Johnson arranged for the supply cable to be run overhead, via Holborn Tavern and Newgate.\n\nIn September 1882 in New York, the Pearl Street Station was established by Edison to provide electric lighting in the lower Manhattan Island area. The station ran until destroyed by fire in 1890. The station used reciprocating steam engines to turn direct-current generators. Because of the DC distribution, the service area was small, limited by voltage drop in the feeders. In 1886 George Westinghouse began building an alternating current system that used a transformer to step up voltage for long-distance transmission and then stepped it back down for indoor lighting, a more efficient and less expensive system which is similar to modern system. The War of Currents eventually resolved in favor of AC distribution and utilization, although some DC systems persisted to the end of the 20th century. DC systems with a service radius of a mile (kilometer) or so were necessarily smaller, less efficient of fuel consumption, and more labor-intensive to operate than much larger central AC generating stations.\n\nAC systems used a wide range of frequencies depending on the type of load; lighting load using higher frequencies, and traction systems and heavy motor load systems preferring lower frequencies. The economics of central station generation improved greatly when unified light and power systems, operating at a common frequency, were developed. The same generating plant that fed large industrial loads during the day, could feed commuter railway systems during rush hour and then serve lighting load in the evening, thus improving the system load factor and reducing the cost of electrical energy overall. Many exceptions existed, generating stations were dedicated to power or light by the choice of frequency, and rotating frequency changers and rotating converters were particularly common to feed electric railway systems from the general lighting and power network.\n\nThroughout the first few decades of the 20th century central stations became larger, using higher steam pressures to provide greater efficiency, and relying on interconnections of multiple generating stations to improve reliability and cost. High-voltage AC transmission allowed hydroelectric power to be conveniently moved from distant waterfalls to city markets. The advent of the steam turbine in central station service, around 1906, allowed great expansion of generating capacity. Generators were no longer limited by the power transmission of belts or the relatively slow speed of reciprocating engines, and could grow to enormous sizes. For example, Sebastian Ziani de Ferranti planned what would have been the largest reciprocating steam engine ever built for a proposed new central station, but scrapped the plans when turbines became available in the necessary size. Building power systems out of central stations required combinations of engineering skill and financial acumen in equal measure. Pioneers of central station generation include George Westinghouse and Samuel Insull in the United States, Ferranti and Charles Hesterman Merz in UK, and many others.\n\nIn thermal power stations, mechanical power is produced by a heat engine that transforms thermal energy, often from combustion of a fuel, into rotational energy. Most thermal power stations produce steam, so they are sometimes called steam power stations. Not all thermal energy can be transformed into mechanical power, according to the second law of thermodynamics; therefore, there is always heat lost to the environment. If this loss is employed as useful heat, for industrial processes or district heating, the power plant is referred to as a cogeneration power plant or CHP (combined heat-and-power) plant. In countries where district heating is common, there are dedicated heat plants called heat-only boiler stations. An important class of power stations in the Middle East uses by-product heat for the desalination of water.\n\nThe efficiency of a thermal power cycle is limited by the maximum working fluid temperature produced. The efficiency is not directly a function of the fuel used. For the same steam conditions, coal-, nuclear- and gas power plants all have the same theoretical efficiency. Overall, if a system is on constantly (base load) it will be more efficient than one that is used intermittently (peak load). Steam turbines generally operate at higher efficiency when operated at full capacity.\n\nBesides use of reject heat for process or district heating, one way to improve overall efficiency of a power plant is to combine two different thermodynamic cycles in a combined cycle plant. Most commonly, exhaust gases from a gas turbine are used to generate steam for a boiler and a steam turbine. The combination of a \"top\" cycle and a \"bottom\" cycle produces higher overall efficiency than either cycle can attain alone.\n\n\n\nPower plants that can be dispatched (scheduled) to provide energy to a system include:\n\nNon-dispatchable plants include such sources as wind and solar energy; while their long-term contribution to system energy supply is predictable, on a short-term (daily or hourly) base their energy must be used as available since generation cannot be deferred. Contractual arrangements (\"take or pay\") with independent power producers or system interconnections to other networks may be effectively non-dispatchable.\n\nAll thermal power plants produce waste heat energy as a byproduct of the useful electrical energy produced. The amount of waste heat energy equals or exceeds the amount of energy converted into useful electricity. Gas-fired power plants can achieve as much as 65 percent conversion efficiency, while coal and oil plants achieve around 30 to 49 percent. The waste heat produces a temperature rise in the atmosphere, which is small compared to that produced by greenhouse-gas emissions from the same power plant. Natural draft wet cooling towers at many nuclear power plants and large fossil fuel-fired power plants use large hyperboloid chimney-like structures (as seen in the image at the right) that release the waste heat to the ambient atmosphere by the evaporation of water.\n\nHowever, the mechanical induced-draft or forced-draft wet cooling towers in many large thermal power plants, nuclear power plants, fossil-fired power plants, petroleum refineries, petrochemical plants, geothermal, biomass and waste-to-energy plants use fans to provide air movement upward through downcoming water, and are not hyperboloid chimney-like structures. The induced or forced-draft cooling towers are typically rectangular, box-like structures filled with a material that enhances the mixing of the upflowing air and the downflowing water.\n\nIn areas with restricted water use, a dry cooling tower or directly air-cooled radiators may be necessary, since the cost or environmental consequences of obtaining make-up water for evaporative cooling would be prohibitive. These coolers have lower efficiency and higher energy consumption to drive fans, compared to a typical wet, evaporative cooling tower.\n\nElectric companies often prefer to use cooling water from the ocean, a lake, or a river, or a cooling pond, instead of a cooling tower. This single pass or once-through cooling system can save the cost of a cooling tower and may have lower energy costs for pumping cooling water through the plant's heat exchangers. However, the waste heat can cause thermal pollution as the water is discharged. Power plants using natural bodies of water for cooling are designed with mechanisms such as fish screens, to limit intake of organisms into the cooling machinery. These screens are only partially effective and as a result billions of fish and other aquatic organisms are killed by power plants each year. For example, the cooling system at the Indian Point Energy Center in New York kills over a billion fish eggs and larvae annually.\n\nA further environmental impact is that aquatic organisms which adapt to the warmer discharge water may be injured if the plant shuts down in cold weather.\n\nWater consumption by power stations is a developing issue.\n\nIn recent years, recycled wastewater, or grey water, has been used in cooling towers. The Calpine Riverside and the Calpine Fox power stations in Wisconsin as well as the Calpine Mankato power station in Minnesota are among these facilities.\n\nPower stations can also generate electrical energy from renewable energy sources.\n\nIn a hydroelectric power station water flows through turbines using hydropower to generate hydroelectricity. Power is captured from the gravitational force of water falling through penstocks to water turbines connected to generators. The amount of power available is a combination of height and flow. A wide range of Dams may be built to raise the water level, and create a lake for storing water.\nHydropower is produced in 150 countries, with the Asia-Pacific region generating 32 percent of global hydropower in 2010. China is the largest hydroelectricity producer, with 721 terawatt-hours of production in 2010, representing around 17 percent of domestic electricity use.\n\nSolar energy can be turned into electricity either directly in solar cells, or in a concentrating solar power plant by focusing the light to run a heat engine.\n\nA solar photovoltaic power plant converts sunlight into direct current electricity using the photoelectric effect. Inverters change the direct current into alternating current for connection to the electrical grid. This type of plant does not use rotating machines for energy conversion.\n\nSolar thermal power plants are another type of solar power plant. They use either parabolic troughs or heliostats to direct sunlight onto a pipe containing a heat transfer fluid, such as oil. The heated oil is then used to boil water into steam, which turns a turbine that drives an electrical generator. The central tower type of solar thermal power plant uses hundreds or thousands of mirrors, depending on size, to direct sunlight onto a receiver on top of a tower. Again, the heat is used to produce steam to turn turbines that drive electrical generators.\n\nWind turbines can be used to generate electricity in areas with strong, steady winds, sometimes offshore. Many different designs have been used in the past, but almost all modern turbines being produced today use a three-bladed, upwind design. Grid-connected wind turbines now being built are much larger than the units installed during the 1970s. They thus produce power more cheaply and reliably than earlier models. With larger turbines (on the order of one megawatt), the blades move more slowly than older, smaller, units, which makes them less visually distracting and safer for birds.\n\nMarine energy or marine power (also sometimes referred to as ocean energy or ocean power) refers to the energy carried by ocean waves, tides, salinity, and ocean temperature differences. The movement of water in the world’s oceans creates a vast store of kinetic energy, or energy in motion. This energy can be harnessed to generate electricity to power homes, transport and industries.\n\nThe term marine energy encompasses both wave power — power from surface waves, and tidal power — obtained from the kinetic energy of large bodies of moving water. Offshore wind power is not a form of marine energy, as wind power is derived from the wind, even if the wind turbines are placed over water.\n\nThe oceans have a tremendous amount of energy and are close to many if not most concentrated populations. Ocean energy has the potential of providing a substantial amount of new renewable energy around the world.\n\nSalinity gradient energy is called pressure-retarded osmosis. In this method, seawater is pumped into a pressure chamber that is at a pressure lower than the difference between the pressures of saline water and fresh water. Freshwater is also pumped into the pressure chamber through a membrane, which increases both the volume and pressure of the chamber. As the pressure differences are compensated, a turbine is spun creating energy. This method is being specifically studied by the Norwegian utility Statkraft, which has calculated that up to 25 TWh/yr would be available from this process in Norway. Statkraft has built the world's first prototype osmotic power plant on the Oslo fiord which was opened on November 24, 2009.\n\nBiomass energy can be produced from combustion of waste green material to heat water into steam and drive a steam turbine. Bioenergy can also be processed through a range of temperatures and pressures in gasification, pyrolysis or torrefaction reactions. Depending on the desired end product, these reactions create more energy-dense products (syngas, wood pellets, biocoal) that can then be fed into an accompanying engine to produce electricity at a much lower emission rate when compared with open burning.\n\nIt is possible to store energy and produce the electricity at a later time like in Pumped-storage hydroelectricity, Thermal energy storage, Flywheel energy storage, Battery storage power station and so on.\n\nThe worlds largest form of storage for excess electricity, pumped-storage is a reversible hydroelectric plant. They are a net consumer of energy but provide storage for any source of electricity, effectively smoothing peaks and troughs in electricity supply and demand. Pumped storage plants typically use \"spare\" electricity during off peak periods to pump water from a lower reservoir to an upper reservoir. Because the pumping takes place \"off peak\", electricity is less valuable than at peak times. This less valuable \"spare\" electricity comes from uncontrolled wind power and base load power plants such as coal, nuclear and geothermal, which still produce power at night even though demand is very low. During daytime peak demand, when electricity prices are high, the storage is used for peaking power, where water in the upper reservoir is allowed to flow back to a lower reservoir through a turbine and generator. Unlike coal power stations, which can take more than 12 hours to start up from cold, a hydroelectric generator can be brought into service in a few minutes, ideal to meet a peak load demand. Two substantial pumped storage schemes are in South Africa, Palmiet Pumped Storage Scheme and another in the Drakensberg, Ingula Pumped Storage Scheme.\n\nThe power generated by a power station is measured in multiples of the watt, typically megawatts (10 watts) or gigawatts (10 watts). Power stations vary greatly in capacity depending on the type of power plant and on historical, geographical and economic factors. The following examples offer a sense of the scale.\n\nMany of the largest operational onshore wind farms are located in the USA. As of 2011, the Roscoe Wind Farm is the second largest onshore wind farm in the world, producing 781.5 MW of power, followed by the Horse Hollow Wind Energy Center (735.5 MW). As of July 2013, the London Array in United Kingdom is the largest offshore wind farm in the world at 630 MW, followed by Thanet Offshore Wind Project in United Kingdom at 300 MW.\n\n, the largest photovoltaic (PV) power plants in the world are led by Longyangxia Dam Solar Park in China, rated at 850 megawatts.\n\nSolar thermal power stations in the U.S. have the following output:\nLarge coal-fired, nuclear, and hydroelectric power stations can generate hundreds of megawatts to multiple gigawatts. Some examples:\n\nGas turbine power plants can generate tens to hundreds of megawatts. Some examples:\n\nThe rated capacity of a power station is nearly the maximum electrical power that that power station can produce.\nSome power plants are run at almost exactly their rated capacity all the time, as a non-load-following base load power plant, except at times of scheduled or unscheduled maintenance.\n\nHowever, many power plants usually produce much less power than their rated capacity.\n\nIn some cases a power plant produces much less power than its rated capacity because it uses an intermittent energy source.\nOperators try to pull maximum available power from such power plants, because their marginal cost is practically zero, but the available power varies widely—in particular, it may be zero during heavy storms at night.\n\nIn some cases operators deliberately produce less power for economic reasons.\nThe cost of fuel to run a load following power plant may be relatively high, and the cost of fuel to run a peaking power plant is even higher—they have relatively high marginal costs.\nOperators keep power plants turned off (\"operational reserve\") or running at minimum fuel consumption (\"spinning reserve\") most of the time.\nOperators feed more fuel into load following power plants only when the demand rises above what lower-cost plants (i.e., intermittent and base load plants) can produce, and then feed more fuel into peaking power plants only when the demand rises faster than the load following power plants can follow.\n\nOperating staff at a power station have several duties. Operators are responsible for the safety of the work crews that frequently do repairs on the mechanical and electrical equipment. They maintain the equipment with periodic inspections and log temperatures, pressures and other important information at regular intervals. Operators are responsible for starting and stopping the generators depending on need. They are able to synchronize and adjust the voltage output of the added generation with the running electrical system, without upsetting the system. They must know the electrical and mechanical systems to troubleshoot problems in the facility and add to the reliability of the facility. Operators must be able to respond to an emergency and know the procedures in place to deal with it.\n\n"}
{"id": "43125367", "url": "https://en.wikipedia.org/wiki?curid=43125367", "title": "Predictive control of switching power converters", "text": "Predictive control of switching power converters\n\nPredictive Control of Switching Power Converters. Predictive controllers rely on optimum control systems theory and aim to solve a cost functional minimization problem. Predictive controllers are relatively easy to numerically implement but electronic power converters are non-linear time varying dynamic systems, so a different approach to predictive must be taken.\n\nThe first step to design a predictive controller is to derive a detailed direct dynamic model (including non-linearities) of the switching power converter. This model must contain enough detail of the converter dynamics to allow, from initial conditions, a forecast in real time and with negligible error, of the future behavior of the converter.\n\nSliding mode control of switching power converters chooses a vector to reach sliding mode as fast as possible (high switching frequency).\n\nIt would be better to choose a vector to ensure zero error at the end of the sampling period Δt.\n\nTo find such a vector, a previous calculation can be made (prediction);\n\nThe converter has a finite number of vectors (states) and is usually non-linear: one way is to try all vectors to find the one that minimizes the control errors, prior to the application of that vector to the converter.\n\nReceding Horizon Optimum Predictive Control\n\n\n"}
{"id": "594990", "url": "https://en.wikipedia.org/wiki?curid=594990", "title": "Psychrophile", "text": "Psychrophile\n\nPsychrophiles or cryophiles (adj. psychrophilic or cryophilic) are extremophilic organisms that are capable of growth and reproduction in low temperatures, ranging from −200 °C to +10 °C. They are found in places that are permanently cold, such as the polar regions and the deep sea. They can be contrasted with thermophiles, which are organisms that thrive at unusually high temperatures. Psychrophile is Greek for 'cold-loving'.\n\nMany such organisms are bacteria or archaea, but some eukaryotes such as lichens, snow algae, fungi, and wingless midges, are also classified as psychrophiles.\n\nThe cold environments that psychrophiles inhabit are ubiquitous on Earth, as a large fraction of our planetary surface experiences temperatures lower than 15 °C. They are present in permafrost, polar ice, glaciers, snowfields and deep ocean waters. These organisms can also be found in pockets of sea ice with high salinity content. Microbial activity has been measured in soils frozen below −39 °C. In addition to their temperature limit, psychrophiles must also adapt to other extreme environmental constraints that may arise as a result of their habitat. These constraints include high pressure in the deep sea, and high salt concentration on some sea ice.\n\nPsychrophiles are protected from freezing and the expansion of ice by ice-induced desiccation and vitrification (glass transition), as long as they cool slowly. Free living cells desiccate and vitrify between −10 °C and −26 °C. Cells of multicellular organisms may vitrify at temperatures below −50 °C. The cells may continue to have some metabolic activity in the extracellular fluid down to these temperatures, and they remain viable once restored to normal temperatures.\n\nThey must also overcome the stiffening of their lipid cell membrane, as this is important for the survival and functionality of these organisms. To accomplish this, psychrophiles adapt lipid membrane structures that have a high content of short, unsaturated fatty acids. Compared to longer saturated fatty acids, incorporating this type of fatty acid allows for the lipid cell membrane to have a lower melting point, which increases the fluidity of the membranes. In addition, carotenoids are present in the membrane, which help modulate the fluidity of it.\n\nAntifreeze proteins are also synthesized to keep psychrophiles' internal space liquid, and to protect their DNA when temperatures drop below water's freezing point. By doing so, the protein prevents any ice formation or recrystallization process from occurring.\n\nThe enzymes of these organisms have been hypothesized to engage in a activity-stability-flexibility relationship as a method for adapting to the cold; the flexibility of their enzyme structure will increase as a way to compensate for the freezing effect of their environment.\n\nCertain cryophiles, such as Gram-negative bacteria \"Vibrio\" and \"Aeromonas\" spp., can transition into a viable but nonculturable (VBNC) state. During VBNC, a micro-organism can respirate and use substrates for metabolism – however, it cannot replicate. An advantage of this state is that it is highly reversible. It has been debated whether VBNC is an active survival strategy or if eventually the organism's cells will no longer be able to be revived. There is proof however it may be very effective – Gram positive bacteria Actinobacteria have been shown to have lived about 500,000 years in the permafrost conditions of Antarctica, Canada, and Serbia.\n\nPsychrophiles include bacteria, lichens, fungi, and insects.\n\nAmong the bacteria that can tolerate extreme cold are \"Arthrobacter\" sp., \"Psychrobacter\" sp. and members of the genera \"Halomonas\", \"Pseudomonas\", \"Hyphomonas\", and \"Sphingomonas\". Another example is \"Chryseobacterium greenlandensis\", a psychrophile that was found in 120,000-year-old ice.\n\n\"Umbilicaria antarctica\" and \"Xanthoria elegans\" are lichens that have been recorded photosynthesizing at temperatures ranging down to −24 °C, and they can grow down to around −10 °C. Some multicellular eukaryotes can also be metabolically active at sub-zero temperatures, such as some conifers; those in the \"Chironomidae\" family are still active at −16 °C.\n\n\"Penicillium\" is a genus of fungi found in a wide range of environments including extreme cold.\n\nAmong the psychrophile insects, the Grylloblattidae or icebugs, found on mountaintops, have optimal temperatures between 1-4 °C. The wingless midge (Chironomidae) \"Belgica antarctica\" is the only insect, indeed the only terrestrial animal, endemic to Antarctica; it can tolerate salt, being frozen and strong ultraviolet, and has the smallest known genome of any insect. The small genome, of 99 million base pairs, is thought to be adaptive to extreme environments.\n\nPsychrotrophic bacteria are capable of surviving or even thriving in extremely cold environment. They provide an estimation of the product's shelf life, also they can be found in soils, in surface and deep sea waters, in Antarctic ecosystems, and in foods.\nThey are responsible for spoiling refrigerated foods.\n\nPsychrotrophic bacteria are of particular concern to the dairy industry. Most are killed by pasteurization; however, they can be present in milk as post-pasteurization contaminants due to less than adequate sanitation practices. According to the Food Science Department at Cornell University, psychrotrophs are bacteria capable of growth at temperatures at or less than . At freezing temperatures, growth of psychrotrophic bacteria becomes negligible or virtually stops.\n\nAll three subunits of the RecBCD enzyme are essential for physiological activities of the enzyme in the Antarctic \"Pseudomonas syringae\", namely, repairing of DNA damage and supporting the growth at low temperature. The RecBCD enzymes are exchangeable between the psychrophilic \"P. syringae\" and the mesophilic \"E. coli\" when provided with the entire protein complex from same species. However, the RecBC proteins (RecBCPs and RecBCEc) of the two bacteria are not equivalent; the RecBCEc is proficient in DNA recombination and repair, and supports the growth of \"P. syringae\" at low temperature, while RecBCPs is insufficient for these functions. Finally, both helicase and nuclease activity of the RecBCDPs are although important for DNA repair and growth of \"P. syringae\" at low temperature, the RecB-nuclease activity is not essential in vivo.\n\nIn 1940, ZoBell and Conn stated that they had never encountered \"true psychrophiles\" or organisms that grow best at relatively low temperatures. In 1958, J. L. Ingraham supported this by concluding that there are very few or possibly no bacteria that fit the textbook definitions of psychrophiles. Richard Y. Morita emphasizes this by using the term \"psychrotroph\" to describe organisms that do not meet the definition of psychrophiles. The confusion between the terms \"psychrotrophs\" and \"psychrophiles\" was started because investigators were unaware of the thermolability of psychrophilic organisms at the laboratory temperatures. Due to this, early investigators did not determine the cardinal temperatures for their isolates.\n\nThe similarity between these two is that they are both capable of growing at zero, but optimum and upper temperature limits for the growth are lower for psychrophiles compared to psychrotrophs. Psychrophiles are also more often isolated from permanently cold habitats compared to psychrotrophs. Although psychrophilic enzymes remain under-used because the cost of production and processing at low temperatures is higher than for the commercial enzymes that are presently in use, the attention and resurgence of research interest in psychrophiles and psychrotrophs will be a contributor to the betterment of the environment and the desire to conserve energy.\n\n\n"}
{"id": "49544421", "url": "https://en.wikipedia.org/wiki?curid=49544421", "title": "REScoop.be", "text": "REScoop.be\n\nREScoop.be is the Belgian branch of the European group of cooperatives for renewable energy, REScoop.eu.\n\nIt was established in 2011 on the initiative of the Flemish energy cooperative Ecopower.\n\nOn the Belgian level it coordinates the cooperative initiatives of its members REScoop Vlaanderen and REScoop Wallonie.\n"}
{"id": "18505995", "url": "https://en.wikipedia.org/wiki?curid=18505995", "title": "Resource productivity", "text": "Resource productivity\n\nResource productivity is the quantity of good or service (outcome) that is obtained through the expenditure of unit resource. This can be expressed in monetary terms as the monetary yield per unit resource.\n\nFor example, when applied to crop irrigation it is the yield of crop obtained through use of a given volume of irrigation water, the “crop per drop”, which could also be expressed as monetary return from product per use of unit irrigation water.\n\nResource productivity and resource intensity are key concepts used in sustainability measurement as they attempt to decouple the direct connection between resource use and environmental degradation. Their strength is that they can be used as a metric for both economic and environmental cost. Although these concepts are two sides of the same coin, in practice they involve very different approaches and can be viewed as reflecting, on the one hand, the efficiency of resource production as outcome per unit of resource use (resource productivity) and, on the other hand, the efficiency of resource consumption as resource use per unit outcome (resource intensity). The sustainability objective is to maximize resource productivity while minimizing resource intensity. Scientific and political debates on resource productivity are regularly held at, among others, the World Resources Forum conferences.\n\n\n"}
{"id": "33335738", "url": "https://en.wikipedia.org/wiki?curid=33335738", "title": "Saturate, aromatic, resin and asphaltene", "text": "Saturate, aromatic, resin and asphaltene\n\nSaturate, Aromatic, Resin and Asphaltene (SARA) is an analysis method that divides crude oil components according to their polarizability and polarity. The saturate fraction consists of nonpolar material including linear, branched, and cyclic saturated hydrocarbons (paraffins). Aromatics, which contain one or more aromatic rings, are slightly more polarizable. The remaining two fractions, resins and asphaltenes, have polar substituents. The distinction between the two is that asphaltenes are insoluble in an excess of heptane (or pentane) whereas resins are miscible with heptane (or pentane).\n\nThere are three main methods to obtain SARA results. One has lately emerged as the most popular. That technology is known as the Iatroscan TLC-FID, it combines TLC with flame ionization detection (TLC-FID). It is referred to as IP-143. Other analysis giving SARA numbers might not correspond to the numbers obtained in IP-143. It is therefore always important to know the analysis method when comparing SARA numbers.\n\nIt is the only method that is 100 times more sensitive than any old method and faster. Takes 30 seconds for 1 sample instead of 1 day.\n\n"}
{"id": "31803650", "url": "https://en.wikipedia.org/wiki?curid=31803650", "title": "Sergio Focardi", "text": "Sergio Focardi\n\nSergio Focardi (1932 – 22 June 2013) was an Italian physicist and professor emeritus at the University of Bologna.\nHe led the Department of Bologna of the (Italian) National Institute for Nuclear Physics and the Faculty of Mathematical, Physical and Natural Sciences at the University of Bologna. \n\nIn the early 1960s Focardi spent time at CERN in Geneva. \n\nHe was a member of the President's Board of the Italian Physical Society.\nFrom 1992 he had been working on cold fusion with nickel-hydrogen reactors. From 2007 until his death, Focardi collaborated with inventor Andrea Rossi on the development of the Energy Catalyzer (E-Cat).\n\nIn the early 90s Sergio Focardi, together with physicists Roberto Habel and Francesco Piantelli, started to develop a nickel-hydrogen exothermal reactor. The results of their research were presented in 1994, and published on the peer-reviewed scientific journal \"Il Nuovo Cimento A\".\n\n\n"}
{"id": "34144367", "url": "https://en.wikipedia.org/wiki?curid=34144367", "title": "Shou (character)", "text": "Shou (character)\n\nShòu () is the Chinese word/character for \"longevity\".\n\nThree of the most important goals in life in Chinese traditional thought are the propitious blessings of happiness (\"fú\" 福), professional success or prosperity (\"lù\" 禄), and longevity (\"shòu\" 寿). These are visually represented by the three \"star gods\" of the same names (\"Fú, Lù, Shòu\"), commonly depicted as three male figurines (each wearing a distinctive garment and holding an object that enables them to be differentiated), or the Chinese ideographs/characters themselves, or various homophones or objects with relevant attributes. \"Shòu\" is instantly recognizable. \"He holds in his hand a large peach, and attached to his long staff are a gourd and a scroll. The stag and the bat both indicate \"fu\" happiness. The peach, gourd, and scroll are symbols of longevity.\" His most striking characteristic is, however, his large and high forehead, which earned him the title \"Longevity Star Old-pate\".\n\nThe Chinese character \"shòu\" (寿) is usually found on textiles, furniture, ceramics and jewelry, generally in its more complex ideograph (壽), but also in its simplified (post-1950) form (寿). The ideograph may appear alone or be surrounded by flowers, bats, or other good luck symbols, but will always hold a central position. \n\nLongevity is commonly recognized as one of the Five Blessings (\"wǔfú\" 五福 - longevity, wealth, health, love of virtue, a peaceful death) of Chinese belief that are often depicted in the homophonous rendition of five flying bats because the word for \"bat\" in Chinese (\"fú\" 蝠) sounds like the word for \"good fortune\" or \"happiness\" or in this case, \"blessings\". In this arrangement, the \"shòu\" ideograph sometimes takes the dominant central position, replacing the fifth bat.\n\nOther symbols in Chinese iconography that represent longevity include pine trees, cranes, spotted deer, special collectors' stones (\"shòushí\" 寿石), peaches, and tortoises. These are often depicted in small groupings to emphasize the central, symbolic meaning of the picture (for example, cranes standing amongst pine trees). \n\nPerhaps the most common Chinese auspicious saying concerning longevity is that found on scrolls in nearly every Chinese calligraphy shop in the world: \"shòu shān fú hǎi\" (寿山福海), which can be translated as \"May your life be as steadfast as the mountains and your good fortune as limitless as the seas\".\n\nSince 2017, the version 10 of the Unicode Standard features a rounded version of the symbol in the \"Enclosed Ideographic Supplement\" block, at code point U+1F262 (ROUNDED SYMBOL FOR SHOU).\n\nAs a sign for a resonant cultural concept, the character became a part of many Chinese names (e.g. Palace of Tranquil Longevity in Beijing). The Japanese equivalent is Kotobuki 寿 (see Nakajima Kotobuki, Tsukasa Kotobuki). See also Jurōjin (Shou Laoren) and Fukurokuju.\n\n"}
{"id": "57965729", "url": "https://en.wikipedia.org/wiki?curid=57965729", "title": "Water oxidation catalysis", "text": "Water oxidation catalysis\n\nWater oxidation catalysis (WOC) is the acceleration (catalysis) of the conversion of water into oxygen and protons:\nMany catalysts are effective, both homogeneous catalysts and heterogeneous catalysts. The oxygen evolving complex in photosynthesis is the premier example. There is no interest in generating oxygen by water oxidation since oxygen is readily obtained from air. Instead, interest in water oxidation is motivated by its relevance to water splitting, which would provide \"solar hydrogen,\" i.e. water oxidation would generate the electrons and protons for the production of hydrogen. An ideal WOC would operate rapidly at low overpotential, exhibit high stability and be of low cost, derived from nontoxic components.\n\nWater is more difficult to oxidize than its conjugate base hydroxide. Hydroxide is stabilized by coordination to metal cations. Some metal hydroxides, those featuring redox-active metal centers, can oxidize to give metal oxo complexes. Attack of water on metal oxo centers represents one pathway for the formation of the O-O bond, leading to dioxygen. Alternatively, the crutial O-O bond forming step can arise by coupling suitably positioned pairs of metal hydroxo centers. The molecular mechanism of the OEC has not been elucidated.\n\nThe conversion of even metal hydroxo complexes to O requires very strong oxidants. In photosynthesis, such oxidants are provided by electron holes on porphyrin radical cations. For device applications, the aspirational oxidant is a photovoltaic material. For screening WOCs, ceric ammonium nitrate is a typical electron acceptor. \nA number of ruthenium-aqua complexes catalyze the oxidation of water. Most catalysts feature bipyridine and terpyridine ligands. Catalysts containing pyridine-2-carboxylate exhibit rates (300 s) comparable to that of photosystem II. Work in this area has ushered in many new polypyridyl ligands.\nEarly examples of cobalt-based WOCs suffered from instability. A homogeneous WOC [Co(Py)(HO)](ClO) operates by a proton-coupled electron transfer to form a [Co--OH] species, which on further oxidation forms a Co intermediate. The intermediate formed reacts with water to liberate O. The cobalt-polyoxometalate complex [Co(HO)(α-PWO)] is highly efficient WOC.\n\nSome iron complexes catalyze water oxidation. A water-soluble complex [Fe(OTf)(MePytacn)] (Pytacn=pyridine-substituted trimethyltriazacyclononane; OTf= triflate) is an efficient WOC. The concentration of the catalyst and the oxidant were found to be strongly affecting the oxidation process. Many related complexes with cis labile sites are active catalysts. Most complexes were found to undergo degradation in a few hours. The number and stereochemistry of reactive coordination sites on Fe have been evaluated but few guidelines have emerged.\n\nThe complexes [Ir(ppy)(OH)] (ppy = 2-phenylpyridine) exhibit high turnover numbers, but low catalytic rates. Replacing ppy with Cp* (CMe) results in increased catalytic activity but decreased the turnover number. Water nucleophilic attack on Ir=O species was found to be responsible for the O formation.\n\nIridium oxide is a stable bulk WOC catalyst with low overpotential.\n\nNi-based oxide film liberates oxygen in quasi-neutral conditions at an overpotential of ~425 mV and shows long lasting stability. X-ray spectroscopy revealed the presence of di-µ-oxide bridging between Ni/Ni ions but no evidence of mono-µ-oxide bridging was found between the ions. Similar structures can be found in Co-WOC films and Mn-WOC catalysts.\n\nCobalt oxides (CoO) have been investigated to work on the same pattern as other cobalt salts. Cobalt phosphates are also active WOCs at neutral pH. Stable and highly active WOCs can be prepared by adsorbing Co on silica nanoparticles.\n\n"}
{"id": "1149933", "url": "https://en.wikipedia.org/wiki?curid=1149933", "title": "Weight gain", "text": "Weight gain\n\nWeight gain is an increase in body weight. This can involve an increase in muscle mass, fat deposits, excess fluids such as water or other factors. Weight gain can be a symptom of a serious medical condition.\n\nIf enough weight is gained due to increased body fat deposits, one may become overweight or obese, generally defined as having more body fat (adipose tissue) than is considered good for health. The Body Mass Index (BMI) measures body weight in proportion to the square of height and defines optimal, insufficient, and excessive weight based on the ratio.\n\nWeight gain has a latency period. The effect that eating has on weight gain can vary greatly depending on the following factors: energy (calorie) density of foods, exercise regimen, amount of water intake, amount of salt contained in the food, time of day eaten, age of individual, individual's country of origin, individual's overall stress level, and amount of water retention in ankles/feet. Typical latency periods vary from three days to two weeks after ingestion.\n\nHaving excess adipose tissue (fat) is a common condition, especially where food supplies are plentiful and lifestyles are sedentary. As much as 64% of the United States adult population is considered either overweight or obese, and this percentage has increased over the last four decades.\n\nA commonly asserted \"rule\" for weight gain or loss is based on the assumption that one pound of human fat tissue contains about 3,500 kilocalories (often simply called \"calories\" in the field of nutrition). Thus, eating 500 fewer calories than one needs per day should result in a loss of about a pound per week. Similarly, for every 3500 calories consumed above the amount one needs, a pound will be gained.\n\nThe assumption that a pound of human fat tissue represents about 3500 calories in the context of weight loss or gain is based on a review of previous observations and experiments by Max Wishnofsky published in 1958. He notes that previous research suggested that a pound of human adipose tissue is 87% fat, which equals 395 grams of fat. He further assumes that animal fat contains 9.5 calories per gram. Thus one pound of human fat tissue should contain 3750 calories. He then critically analyzes the relevant literature and applies a number of additional assumptions, including that the diet contains sufficient protein and that the person is in glycogen and nitrogen (protein) equilibrium, leading to most weight loss stemming from the catabolism of fat. He concludes that a 3500 calorie excess or deficit for a person meeting his assumptions, would lead to the gain or loss, respectively, of one pound of body weight. He notes that if the assumptions he makes are not met, a deficit of 3500 calories would not necessarily equate to a pound of weight loss.\n\nIn any case, Wishnofsky did not take into account numerous aspects of human physiology and biochemistry which refute this simple equivalence. Unfortunately, the claim has achieved the status of a rule of thumb and is repeated in numerous sources, used for diet planning by dietitians and misapplied at the population level as well.\n\nIn regard to adipose tissue increases, a person generally gains weight by increasing food consumption, becoming physically inactive, or both. When energy intake exceeds energy expenditure (when the body is in positive energy balance), the body can store the excess energy as fat. However, the physiology of weight gain and loss is complex involving numerous hormones, body systems and environmental factors. Other factors beside energy balance that may contribute to gaining weight include:\n\nA study, involving more than 12,000 people tracked over 32 years, found that social networks play a surprisingly powerful role in determining an individual's chances of gaining weight, transmitting an increased risk of becoming obese from wives to husbands, from brothers to brothers and from friends to friends.\nThe human microbiota facilitates fermentation of indigestible carbohydrates to short-chain fatty acids, SCFAs, contributing to weight gain. A change in the proportion of Bacteroidetes and Firmicutes may determine host’s risk of obesity.\n\nLack of sufficient sleep has been suggested as a cause for weight gain or the difficulty in maintaining a healthy weight. Two hormones responsible for regulating hunger and metabolism are leptin, which inhibits appetite and increases energy expenditure, and ghrelin, which increases appetite and reduces energy expenditure. Studies have shown that chronic sleep deprivation is associated with reduced levels of leptin and elevated levels of ghrelin, which together result in increased appetite, especially for high fat and high carbohydrate foods. As a result, sleep deprivation over time may contribute to increased caloric intake and decreased self-control over food cravings, leading to weight gain.\n\nWeight gain is a common side-effect of certain psychiatric medications.\n\nPathological causes of weight gain include Cushing's syndrome, hypothyroidism, insulinoma, and craniopharyngioma. Genetic reasons can relate to Prader–Willi syndrome, Bardet–Biedl syndrome, Alström syndrome, Cohen syndrome, and Carpenter syndrome.\n\nMedications that list headaches and/or fatigue as side effects can indirectly contribute to weight gain since they decrease the motivation for physical activity.\n\nExcess adipose tissue can lead to medical problems; however, a round or large figure does not necessarily imply a medical problem, and is sometimes not primarily caused by adipose tissue. If too much weight is gained, serious health side-effects may follow. A large number of medical conditions have been associated with obesity. Health consequences are categorised as being the result of either increased fat mass (osteoarthritis, obstructive sleep apnea, social stigma) or increased number of fat cells (diabetes, some forms of cancer, cardiovascular disease, non-alcoholic fatty liver disease).\n\nIn centuries past, a degree of plumpness has been seen as indicative of personal or family prosperity: \"Calories were scarce, physical labor was hard, and most people were as lean as greyhounds.\" Only in the early 20th Century did fatness lose this appeal. The connection of fatness with financial well-being persists today in some less-developed countries. Indeed, it may be on the rise.\n\nDespite the connotations that excess weight had in the past, it has for some time been seen as \"unacceptable\" in contemporary Western society. An expansive market has taken root since the mid-20th century, focusing on weight loss regimens, products and surgeries. This market has been aided by the rising number of overweight and obese citizens in the United States. Data from the CDC's National Health and Nutrition Examination Survey, indicates that the average weight of women between ages 30 and 60 has increased by 20 pounds, or 14%, since 1976. Among women who weigh 300 pounds or more, the increase was 18%.\n\nHowever, some research has indicated the opposite pattern. It has been suggested that obesity among women residing in the U.S. has become more socially acceptable. According to a study published in the July issue of Economic Inquiry, this is likely because more than one-third of women aged 20 and older are obese in the United States. The study found that the average woman weighed 147 pounds in 1994, but stated that she wanted to weigh 132 pounds. By 2002, the average women weighed 153 pounds, but said that she wanted to weigh 135 pounds. \"The fact that even the desired weight of women has increased suggests there is less social pressure to lose weight,\" the researchers noted. However, the difference between women's average weight and desired weight had increased as well.\n\nIn any case, weight gain and weight loss are still charged topics. The ever-present social stigma concerning weight gain, can have lasting and harmful effects on individuals, especially among young women. These are thought to include eating disorders and body dysmorphia.\n\nWeight gain is seen in professional sports most notably in combat sports because of their weight divisions. It occurs mostly in boxing, mixed martial arts, puroresu and professional wrestling.\n\n"}
{"id": "454209", "url": "https://en.wikipedia.org/wiki?curid=454209", "title": "Wind gradient", "text": "Wind gradient\n\nIn common usage, wind gradient, more specifically wind speed gradient\nor wind velocity gradient,\nor alternatively shear wind,\nis the vertical gradient of the mean horizontal wind speed in the lower atmosphere. It is the rate of increase of wind strength with unit increase in height above ground level. In metric units, it is often measured in units of meters per second of speed, per kilometer of height (m/s/km), which reduces to the standard unit of shear rate, inverse seconds (s).\n\nSurface friction forces the surface wind to slow and turn near the surface of the Earth, blowing directly towards the low pressure, when compared to the winds in the nearly frictionless flow well above the Earth's surface. This layer, where surface friction slows the wind and changes the wind direction, is known as the planetary boundary layer. Daytime solar heating due to insolation thickens the boundary layer as winds at the surface become increasingly mixed with winds aloft. Radiative cooling overnight decouples the winds at the surface from the winds above the boundary layer, increasing vertical wind shear near the surface, also known as wind gradient.\n\nTypically, due to aerodynamic drag, there is a wind gradient in the wind flow just a few hundred meters above the Earth's surface—the surface layer of the planetary boundary layer. Wind speed increases with increasing height above the ground, starting from zero due to the no-slip condition. Flow near the surface encounters obstacles that reduce the wind speed, and introduce random vertical and horizontal velocity components at right angles to the main direction of flow.\nThis turbulence causes vertical mixing between the air moving horizontally at one level, and the air at those levels immediately above and below it, which is important in dispersion of pollutants and in soil erosion.\n\nThe reduction in velocity near the surface is a function of surface roughness, so wind velocity profiles are quite different for different terrain types. Rough, irregular ground, and man-made obstructions on the ground, retard movement of the air near the surface, reducing wind velocity. Because of low surface roughness on the relatively smooth water surface, wind speeds do not increase as much with height above sea level as they do on land. Over a city or rough terrain, the wind gradient effect could cause a reduction of 40% to 50% of the geostrophic wind speed aloft; while over open water or ice, the reduction may be only 20% to 30%.\n\nFor engineering purposes, the wind gradient is modeled as a simple shear exhibiting a vertical velocity profile varying according to a power law with a constant exponential coefficient based on surface type. The height above ground where surface friction has a negligible effect on wind speed is called the \"gradient height\" and the wind speed above this height is assumed to be a constant called the \"gradient wind speed\". For example, typical values for the predicted gradient height are 457 m for large cities, 366 m for suburbs, 274 m for open terrain, and 213 m for open sea.\n\nAlthough the power law exponent approximation is convenient, it has no theoretical basis. When the temperature profile is adiabatic, the wind speed should vary logarithmically with height, Measurements over open terrain in 1961 showed good agreement with the logarithmic fit up to 100 m or so, with near constant average wind speed up through 1000 m.\n\nThe shearing of the wind is usually three-dimensional, that is, there is also a change in direction between the 'free' pressure-driven geostrophic wind and the wind close to the ground. This is related to the Ekman spiral effect. \nThe cross-isobar angle of the diverted ageostrophic flow near the surface ranges from 10° over open water, to 30° over rough hilly terrain, and can increase to 40°-50° over land at night when the wind speed is very low.\n\nAfter sundown the wind gradient near the surface increases, with the increasing stability.\nAtmospheric stability occurring at night with radiative cooling tends to contain turbulent eddies vertically, increasing the wind gradient. The magnitude of the wind gradient is largely influenced by the height of the convective boundary layer and this effect is even larger over the sea, where there is no diurnal variation of the height of the boundary layer as there is over land.\nIn the convective boundary layer, strong mixing diminishes vertical wind gradient.\n\nThe design of buildings must account for wind loads, and these are affected by wind gradient. The respective gradient levels, usually assumed in the Building Codes, are 500 meters for cities, 400 meters for suburbs, and 300 m for flat open terrain. For engineering purposes, a power law wind speed profile may be defined as follows:\n\nwhere:\n\nWind turbines are affected by wind gradient. Vertical wind-speed profiles result in different wind speeds at the blades nearest to the ground level compared to those at the top of blade travel, and this in turn affects the turbine operation. The wind gradient can create a large bending moment in the shaft of a two bladed turbine when the blades are vertical. The reduced wind gradient over water means shorter and less expensive wind turbine towers can be used in shallow seas. It would be preferable for wind turbines to be tested in a wind tunnel simulating the wind gradient that they will eventually see, but this is rarely done.\n\nFor wind turbine engineering, a polynomial variation in wind speed with height can be defined relative to wind measured at a reference height of 10 meters as:\n\nwhere:\n\nThe Hellmann exponent depends upon the coastal location and the shape of the terrain on the ground, and the stability of the air. Examples of values of the Hellmann exponent are given in the table below:\n\nSource:\n\"Renewable energy: technology, economics, and environment\" by\nMartin Kaltschmitt, Wolfgang Streicher, Andreas Wiese, (Springer, 2007, , ), page 55\n\nIn gliding, wind gradient affects the takeoff and landing phases of flight of a glider.\nWind gradient can have a noticeable effect on ground launches. If the wind gradient is significant or sudden,\nor both, and the pilot maintains the same pitch attitude, the indicated airspeed will increase, possibly exceeding\nthe maximum ground launch tow speed. The pilot must adjust the airspeed to deal with the effect of the\ngradient.\n\nWhen landing, wind gradient is also a hazard, particularly when the winds are strong. As the glider descends through the wind gradient on final approach to landing, airspeed decreases while sink rate increases, and there is insufficient time to accelerate prior to ground contact. The pilot must anticipate the wind gradient and use a higher approach speed to compensate for it.\n\nWind gradient is also a hazard for aircraft making steep turns near the ground. It is a particular problem for gliders which have a relatively long wingspan, which exposes them to a greater wind speed difference for a given bank angle. The different airspeed experienced by each wing tip can result in an aerodynamic stall on one wing, causing a loss of control accident. The rolling moment generated by the different airflow over each wing can exceed the aileron control authority, causing the glider to continue rolling into a steeper bank angle.\n\nIn sailing, wind gradient affects sailboats by presenting a different wind speed to the sail at different heights along the mast. The direction also varies with height, but sailors refer to this as \"wind shear.\"\n\nThe mast head instruments indication of apparent wind speed and direction is different from what the sailor sees and feels near the surface. Sailmakers may introduce sail twist in the design of the sail, where the head of the sail is set at a different angle of attack from the foot of the sail in order to change the lift distribution with height. The effect of wind gradient can be factored into the selection of twist in the sail design, but this can be difficult to predict since the wind gradient may vary widely in different weather conditions. Sailors may also adjust the trim of the sail to account for wind gradient, for example using a boom vang.\n\nAccording to one source, the wind gradient is not significant for sailboats when the wind is over 6 knots (because a wind speed of 10 knots at the surface corresponds to 15 knots at 300 meters, so the change in speed is negligible over the height of a sailboat's mast). According to the same source, the wind increases steadily with height up to about 10 meters in 5 knot winds but less if there is less wind. That source states that in winds with average speeds of six knots or more, the change of speed with height is confined almost entirely to the one or two meters closest to the surface. This is consistent with another source, which shows that the change in wind speed is very small for heights over 2 meters and with a statement by the Australian Government Bureau of Meteorology according to which differences can be as little as 5% in unstable air.\n\nIn kitesurfing, the wind gradient is even more important, because the power kite is flown on 20-30m lines, and the kitesurfer can use the kite to jump off the water, bringing the kite to even greater heights above the sea surface.\n\nWind gradient can have a pronounced effect upon sound propagation in the lower atmosphere. This effect is important in understanding sound propagation from distant sources, such as foghorns, thunder, sonic booms, gunshots or other phenomena like mistpouffers. It is also important in studying noise pollution, for example from roadway noise and aircraft noise, and must be considered in the design of noise barriers.\nWhen wind speed increases with altitude, wind blowing towards the listener from the source will refract sound waves downwards, resulting in increased noise levels downwind of the barrier. These effects were first quantified in the field of highway engineering to address variations of noise barrier efficacy in the 1960s.\n\nWhen the sun warms the Earth's surface, there is a negative temperature gradient in atmosphere. The speed of sound decreases with decreasing temperature, so this also creates a negative sound speed gradient. The sound wave front travels faster near the ground, so the sound is refracted upward, away from listeners on the ground, creating an acoustic shadow at some distance from the source. The radius of curvature of the sound path is inversely proportional to the velocity gradient.\n\nA wind speed gradient of 4 (m/s)/km can produce refraction equal to a typical temperature lapse rate of 7.5 °C/km. Higher values of wind gradient will refract sound downward toward the surface in the downwind direction, eliminating the acoustic shadow on the downwind side. This will increase the audibility of sounds downwind. This downwind refraction effect occurs because there is a wind gradient; the sound is not being carried along by the wind.\n\nThere will usually be both a wind gradient and a temperature gradient. In that case, the effects of both might add together or subtract depending on the situation and the location of the observer.\nThe wind gradient and the temperature gradient can also have complex interactions. For example, a foghorn can be audible at a place near the source, and a distant place, but not in a sound shadow between them.\nIn the case of transverse sound propagation, wind gradients do not sensibly modify sound propagation relative to the windless condition; the gradient effect appears to be important only in upwind and downwind configurations.\n\nFor sound propagation, the exponential variation of wind speed with height can be defined as follows:\n\nwhere:\n\nIn the 1862 American Civil War Battle of Iuka, an acoustic shadow, believed to have been enhanced by a northeast wind, kept two divisions of Union soldiers out of the battle, because they could not hear the sounds of battle only six miles downwind.\n\nScientists have understood the effect of wind gradient upon refraction of sound since the mid-1900s; however, with the advent of the U.S. Noise Control Act, the application of this refractive phenomena became applied widely beginning in the early 1970s, chiefly in the application to noise propagation from highways and resultant design of transportation facilities.\n\nWind gradient soaring, also called dynamic soaring, is a technique used by soaring birds including albatrosses. If the wind gradient is of sufficient magnitude, a bird can climb into the wind gradient, trading ground speed for height, while maintaining airspeed. By then turning downwind, and diving through the wind gradient, they can also gain energy.\n"}
