{"id": "15731782", "url": "https://en.wikipedia.org/wiki?curid=15731782", "title": "2000 Southwest Georgia tornado outbreak", "text": "2000 Southwest Georgia tornado outbreak\n\nThe 2000 Southwest Georgia tornado outbreak occurred between February 13 and February 14, 2000. Three deadly tornadoes raked several counties of southwest Georgia (USA), killing 18 and causing extensive damage to neighborhoods on the south edge of Camilla, where eleven died, as well as the area north of Meigs, where six died. It was the single deadliest tornado outbreak in the United States between June 1999 and October 2002. Other tornadoes struck Arkansas, Alabama, Tennessee, Florida and the Carolinas during the 12-hour outbreak.\n\n\n"}
{"id": "19391039", "url": "https://en.wikipedia.org/wiki?curid=19391039", "title": "Abo Elementary School", "text": "Abo Elementary School\n\nAbo Elementary School in Artesia, New Mexico, United States, was the first public school in the United States constructed entirely underground and equipped to function as an advanced fallout shelter.Designed at the height of the Cold War and completed in 1962, the school had a concrete slab roof which doubled as the school's playground. It contained a large storage facility with room for emergency rations and supplies for up to 2,160 people in the event of nuclear warfare or other catastrophe. The building was listed on the National Register of Historic Places in 1999.\n\nAbo Elementary School was built partly to further the development of American fallout shelter design and to further knowledge about the long-term effects of life underground in a shelter environment. The US civil defense film \"Duck and Cover\" was produced with students in mind, in the hopes that they would learn how, in the event of a nuclear blast, to be shielded from glass and blast waves using desks and chairs. However, educators, school administrators, and government officials soon realized that such measures would be inadequate, especially if a school received a direct hit from an atomic blast or was within the immediate blast radius of the weapon. One official argued that his school district was \"in no position to guarantee physical protection… from a thermonuclear explosion or radioactive fallout.\" Many state departments of education viewed the school shelter plans as \"worthless\". California's Department of Education, for example, was given designs intended to reduce radiation levels inside the school to 1/100 the level outdoors. These plans were rejected. When the California Department of Education then specified protection which would increase the protection factor to 1/1,000, it judged the costs to be too high, and the plans were rejected. Other departments of education and administrators rejected such plans because of their concern for the psychological well-being of their students, who, they believed, would be \"constantly reminded of the possibility of a nuclear war\" if kept in such a school for extended periods of time.\n\nBuilding the Abo Elementary School required that it be constructed with concrete reinforcing walls and a concrete outer shell to protect the inner parts of the school. To fulfill all requirements, Abo contained multiple drinking-water wells, a cafeteria, food storage, bedding and supplies for up to 2,160 people, air-filtration systems, an emergency power-generation system, decontamination systems, carpet, and a morgue.\n\nArchitect Frank Standhardt, in designing the school said, \"I consider my profession derelict on civil defense. We've had ten years of grace and done nothing about it.\" Standhardt had built multiple aboveground windowless schools before Abo, believing them to positively influence pupils' ability to concentrate. He also cited reduced maintenance costs since there were no windows. Construction cost estimates were inconsistent: a \"Time\" magazine article (September 5, 1960) quotes Standhardt as estimating the costs at 10% above the cost of an average above-ground school, while Loretta Hall in \"Underground Buildings: More than Meets the Eye. This school has carpet\" suggests a cost increase at 30 percent. Regardless, the U.S. Office of Civil Defense contributed the excess cost, assuming that it would benefit from any empirical testing performed on the students in the underground environment.\n\nStandhardt designed the school in such a way as to make every element serve multiple purposes. To reduce cost of concrete, for example, the concrete shell roof of the school would double as a basketball court, and the drinking wells were designed to pump water into the air conditioning systems during peacetime. Two-way radio systems, Geiger counters, and fire fighting equipment were also built into the design.\n\nWithin Artesia, Abo Elementary School was lauded by teachers and many parents. Teachers often described Abo's students as less likely to cause trouble, more attentive, and less likely to require discipline. On the other hand, many of those same students described heightened awareness of the possibility of nuclear war, and some were terrified that they could be orphaned in the event of war. One of the most substantial fears raised by students involved the 2,160 person capacity of the school. In the event of a war, only the first 2,160 people would be allowed into the school for shelter, which would likely have left the majority of Artesia's nearly 12,000 residents, including the parents of some of Abo's students, without any shelter in the event of a nuclear attack.\n\nOutside of Artesia, Abo Elementary School was condemned by many councils and groups, some of whom rejected the concept of an underground school entirely. Notwithstanding, the President of Artesia's Board of Education, C.P. Bunch, called the school \"more a matter of insurance than fear\", and expressed hope that future schools built in Artesia would follow its example. Federal studies concluded that the students suffered no long-term effects from their time in Abo, and many students who suffered from chronic allergies or asthma were transferred to Abo as its advanced air filtration systems reduced the impact of dust storms and allergens. Indeed, these studies concluded that many students' health improved as a result of extended time in the school.\n\nAbo Elementary School was shut down in 1995 as a result of increased maintenance costs, aging mechanical equipment and difficulties associated with removing asbestos insulation from the underground, windowless structure. A new school, Yeso Elementary School, was built next door and Abo was converted into a storage facility.\n\n\n"}
{"id": "39782170", "url": "https://en.wikipedia.org/wiki?curid=39782170", "title": "Accommodation platform", "text": "Accommodation platform\n\nAn accommodation platform is an offshore platform which supports living quarters for offshore personnel. These are often associated with the petroleum industry, although other industries use them as well, such as the wind farm Horns Rev 2.\n"}
{"id": "40201057", "url": "https://en.wikipedia.org/wiki?curid=40201057", "title": "Arrott plot", "text": "Arrott plot\n\nIn condensed matter physics, an Arrott plot is a plot of the square of the magnetization formula_1 of a substance, against the ratio of the applied magnetic field to magnetization formula_2 at one (or several) fixed temperature(s). Arrott plots are an easy way of determining the presence of ferromagnetic order in a material. They are named after American physicist Anthony Arrott who introduced them as a technique for studying magnetism in 1957.\n\nAccording to the Ginzburg-Landau mean field picture for magnetism, the free energy of a ferromagnetic material close to a phase transition can be written as:\n\nformula_3\n\nwhere formula_4, the magnetization, is the order parameter, formula_5 is the applied magnetic field, formula_6 is the critical temperature, and formula_7 are material constants.\n\nClose to the phase transition, this gives a relation for the magnetization order parameter:\n\nformula_8\n\nwhere formula_9 is a dimensionless measure of the temperature.\n\nThus in a graph plotting formula_1 vs. formula_2 for various temperatures, the line without an intercept corresponds to the dependence at the critical temperature. Thus along with providing evidence for the existence of a ferromagnetic phase, the Arrott plot can also be used to determine the critical temperature for the phase transition.\n"}
{"id": "19690850", "url": "https://en.wikipedia.org/wiki?curid=19690850", "title": "Cadmium telluride photovoltaics", "text": "Cadmium telluride photovoltaics\n\nCadmium telluride (CdTe) photovoltaics describes a photovoltaic (PV) technology that is based on the use of cadmium telluride, a thin semiconductor layer designed to absorb and convert sunlight into electricity. Cadmium telluride PV is the only thin film technology with lower costs than conventional solar cells made of crystalline silicon in multi-kilowatt systems.\n\nOn a lifecycle basis, CdTe PV has the smallest carbon footprint, lowest water use and shortest energy payback time of all solar technologies. CdTe's energy payback time of less than a year allows for faster carbon reductions without short-term energy deficits.\n\nThe toxicity of cadmium is an environmental concern mitigated by the recycling of CdTe modules at the end of their life time, though there are still uncertainties and the public opinion is skeptical towards this technology. The usage of rare materials may also become a limiting factor to the industrial scalability of CdTe technology in the mid-term future. The abundance of tellurium—of which telluride is the anionic form—is comparable to that of platinum in the earth's crust and contributes significantly to the module's cost.\n\nCdTe photovoltaics are used in some of the world's largest photovoltaic power stations, such as the Topaz Solar Farm. With a share of 5.1% of worldwide PV production, CdTe technology accounted for more than half of the thin film market in 2013. A prominent manufacturer of CdTe thin film technology is the company First Solar, based in Tempe, Arizona.\n\nThe dominant PV technology has always been based on crystalline silicon wafers. Thin films and concentrators were early attempts to lower costs. Thin films are based on using thinner semiconductor layers to absorb and convert sunlight. Concentrators lower the number of panels by using lenses or mirrors to put more sunlight on each panel.\n\nThe first thin film technology to be extensively developed was amorphous silicon. However, this technology suffers from low efficiencies and slow deposition rates (leading to high capital costs). Instead, the PV market reached some 4 gigawatts in 2007 with crystalline silicon comprising almost 90% of sales. The same source estimated that about 3 gigawatts were installed in 2007.\n\nDuring this period cadmium telluride and copper indium diselenide or CIS-alloys remained under development. The latter is beginning to be produced in volumes of 1–30 megawatts per year due to very high, small-area cell efficiencies approaching 20% in the laboratory. CdTe cell efficiency is approaching 20% in the laboratory with a record of 22.1% as of 2016.\n\nResearch in CdTe dates back to the 1950s, because its band gap (~1.5 eV) is almost a perfect match to the distribution of photons in the solar spectrum in terms of conversion to electricity. A simple heterojunction design evolved in which p-type CdTe was matched with n-type cadmium sulfide (CdS). The cell was completed by adding top and bottom contacts. Early leaders in CdS/CdTe cell efficiencies were GE in the 1960s, and then Kodak, Monosolar, Matsushita, and AMETEK.\n\nBy 1981, Kodak used close spaced sublimation (CSS) and made the first 10% cells and first multi-cell devices (12 cells, 8% efficiency, 30 cm). Monosolar and AMETEK used electrodeposition, a popular early method. Matsushita started with screen printing but shifted in the 1990s to CSS. Cells of about 10% sunlight-to-electricity efficiency were produced by the early 1980s at Kodak, Matsushita, Monosolar and AMETEK.\n\nAn important step forward occurred when cells were scaled-up in size to make larger area products called modules. These products required higher currents than small cells and it was found that an additional layer, called a transparent conducting oxide (TCO), could facilitate the movement of current across the top of the cell (instead of a metal grid). One such TCO, tin oxide, was available for other uses (thermally reflective windows). Made more conductive for PV, tin oxide became and remains the norm in CdTe PV modules.\n\nCdTe cells achieved above 15% in 1992 by adding a buffer layer to the TCO/CdS/CdTe stack and then thinned the CdS to admit more light. Chu used resistive tin oxide as the buffer layer and then thinned the CdS from several micrometres to under half a micrometre in thickness. Thick CdS, as it was used in prior devices, blocked about 5 mA/cm of light, or about 20% of the light usable by a CdTe device. The additional layer did not compromise the device's other properties.\n\nIn the early 1990s, other players experienced mixed results. Golden Photon held the record for a short period for the best CdTe module measured at NREL at 7.7% using a spray deposition technique. Matsushita claimed an 11% module efficiency using CSS and then dropped the technology. A similar efficiency and fate eventually occurred at BP Solar. BP used electrodeposition (inherited from Monosolar by a circuitous route when it purchased SOHIO, Monosolar's acquirer). BP Solar dropped CdTe in November 2002. Antec was able to make about 7%-efficient modules, but went bankrupt when it started producing commercially during a short, sharp market downturn in 2002. However, as of 2014 Antec still made CdTe PV modules.\n\nCdTe start-ups include Calyxo (formerly owned by Q-Cells), \"PrimeStar Solar\", in Arvada, Colorado (acquired by First Solar from GE), Arendi (Italy). Including Antec, their total production represents less than 70 megawatts per year. Empa, the Swiss Federal Laboratories for Materials Testing and Research, focuses on the development of CdTe solar cells on flexible substrates and demonstrated cell efficiencies of 13.5% and 15.6% for flexible plastic foil and glass substrates, respectively.\n\nThe major commercial success was by Solar Cells Incorporated (SCI). Its founder, Harold McMaster, envisioned low-cost thin films made on a large scale. After trying amorphous silicon, he shifted to CdTe at the urging of Jim Nolan and founded Solar Cells Inc., which later became First Solar. McMaster championed CdTe for its high-rate, high-throughput processing. SCI shifted from an adaptation of the CSS method then shifted to vapor transport. In February 1999, McMaster sold the company to True North Partners, who named it First Solar.\n\nIn its early years First Solar suffered setbacks, and initial module efficiencies were modest, about 7%. Commercial product became available in 2002. Production reached 25 megawatts in 2005. The company manufactured in Perrysburg, Ohio and Germany. In 2013, First Solar acquired GE's thin film solar panel technology in exchange for a 1.8% stake in the company. Today, First Solar manufactures over 3 gigawatts with an average module efficiency of 16.4% in 2016.\n\nIn August 2014 First Solar announced a device with 21.1% conversion efficiency. In February 2016, First Solar announced that they had reached a record 22.1% conversion efficiency in their CdTe cells. In 2014, the record module efficiency was also raised by First Solar from 16.1% up to 17.0%. At this time, the company projected average production line module efficiency for its CdTe PV to be 17% by 2017, but by 2016, they predicted a module efficiency closer to ~19.5%.\n\nSince CdTe has the optimal band gap for single-junction devices, efficiencies close to 20% (such as already shown in CIS alloys) may be achievable in practical CdTe cells.\n\nProcess optimization improved throughput and lowered costs. Improvements included broader substrates (since capital costs scale sublinearly and installation costs can be reduced), thinner layers (to save material, electricity, and processing time), and better material utilization (to save material and cleaning costs). 2014 CdTe module costs were about $72 per , or about $90 per module.\n\nModule efficiencies are measured in laboratories at standard testing temperatures of 25 °C, however in the field modules are often exposed to much higher temperatures. CdTe’s relatively low temperature coefficient protects performance at higher temperatures. CdTe PV modules experience half the reduction of crystalline silicon modules, resulting in an increased annual energy output of 5-9%.\n\nAlmost all thin film photovoltaic module systems to-date have been non-solar tracking, because module output was too low to offset tracker capital and operating costs. But relatively inexpensive single-axis tracking systems can add 25% output per installed watt. In addition, depending on the Tracker Energy Gain, the overall eco-efficiency of the PV system can be enhanced by lowering both system costs and environmental impacts. This is climate-dependent. Tracking also produces a smoother output plateau around midday, better matching afternoon peaks.\n\nCadmium (Cd), a toxic heavy metal considered a hazardous substance, is a waste byproduct of mining, smelting and refining sulfidic ores of zinc during zinc refining, and therefore its production does not depend on PV market demand. CdTe PV modules provide a beneficial and safe use for cadmium that would otherwise be stored for future use or disposed of in landfills as hazardous waste. Mining byproducts can be converted into a stable CdTe compound and safely encapsulated inside CdTe PV solar modules for years. A large growth in the CdTe PV sector has the potential to reduce global cadmium emissions by displacing coal and oil power generation.\n\nTellurium (Te) production and reserves estimates are subject to uncertainty and vary considerably. Tellurium is a rare, mildly toxic metalloid that is primarily used as a machining additive to steel. Te is almost exclusively obtained as a by-product of copper refining, with smaller amounts from lead and gold production. Only a small amount, estimated to be about 800 metric tons per year, is available. According to USGS, global production in 2007 was 135 metric tons. One gigawatt (GW) of CdTe PV modules would require about 93 metric tons (at current efficiencies and thicknesses). Through improved material efficiency and increased PV recycling, the CdTe PV industry has the potential to fully rely on tellurium from recycled end-of-life modules by 2038. In the last decade, new supplies have been located, e.g., in Xinju, China as well as in Mexico and Sweden. In 1984 astrophysicists identified tellurium as the universe's most abundant element having an atomic number over 40. Certain undersea ridges are rich in tellurium.\n\nThe manufacture of a CdTe cell includes a thin coating with cadmium chloride () to increase the cell's overall efficiency. Cadmium chloride is toxic, relatively expensive and highly soluble in water, posing a potential environmental threat during manufacture. In 2014 research discovered that abundant and harmless magnesium chloride () performs as well as cadmium chloride. This research may lead to cheaper and safer CdTe cells.\n\nBy themselves, cadmium and tellurium are toxic and carcinogenic, but CdTe forms a crystalline lattice that is highly stable, and is several orders of magnitude less toxic than cadmium. The glass plates surrounding CdTe material sandwiched between them (as in all commercial modules) seal during a fire and do not allow any cadmium release. All other uses and exposures related to cadmium are minor and similar in kind and magnitude to exposures from other materials in the broader PV value chain, e.g., to toxic gases, lead solder, or solvents (most of which are not used in CdTe manufacturing).\n\nDue to the exponential growth of photovoltaics the number of worldwide installed PV systems has increased significantly. First Solar established the first global and comprehensive recycling program in the PV industry in 2005. Its recycling facilities operate at each of First Solar’s manufacturing plants and recover up to 95% of semiconductor material for reuse in new modules and 90% of glass for reuse in new glass products. A life cycle assessment of CdTe module recycling by the University of Stuttgart showed a reduction in primary energy demand in End-Of-Life from 81 MJ/m to -12 MJ/m, a reduction of around 93 MJ/m, and in terms of global warming potential from 6 kg CO2-equiv./m to -2.5 CO2-equiv./m, a reduction of around -8.5 CO2-equiv./m. These reductions show a highly beneficial change in the overall environmental profile of CdTe photovoltaic module. The LCA also showed that the main contributors to considered environmental impact categories are due to required chemicals and energy within the processing of CdTe modules.\n\nGrain boundary is the interface between two grains of a crystalline material and occur when two grains meet. They are a type of crystalline defect. It is often assumed that the open-circuit voltage gap seen in CdTe, in comparison to both single crystal GaAs and the theoretical limit, may be in some way attributable to the grain boundaries within the material. There have however been a number of studies which have suggested not only that GBs are not deleterious to performance but may in fact be beneficial as sources of enhanced carrier collection. So, the exact role of the grain boundaries in limitation of performance of CdTe-based solar cells remains unclear and the research is ongoing to address this question.\n\nSuccess of cadmium telluride PV has been due to the low cost achievable with the CdTe technology, made possible by combining adequate efficiency with lower module area costs. Direct manufacturing cost for CdTe PV modules reached $0.57 per watt in 2013, and capital cost per new watt of capacity is near $0.9 per watt (including land and buildings).\n\nUtility-scale CdTe PV solutions were claimed to be able to compete with peaking fossil fuel generation sources depending on irradiance levels, interest rates and other factors such as development costs. Recent installations of large First Solar CdTe PV systems were claimed to be competitive with other forms of solar energy:\n\n\n"}
{"id": "43523820", "url": "https://en.wikipedia.org/wiki?curid=43523820", "title": "Caliber (mathematics)", "text": "Caliber (mathematics)\n\nIn mathematics, the caliber or calibre of a topological space \"X\" is a cardinal \"κ\" such that for every set of \"κ\" nonempty open subsets of \"X\" there is some point of \"X\" contained in \"κ\" of these subsets. This concept was introduced by .\n\nThere is a similar concept for posets. A pre-caliber of a poset \"P\" is a cardinal \"κ\" such that for any collection of elements of \"P\" indexed by \"κ\", there is a subcollection of cardinality \"κ\" that is centered. Here a subset of a poset is called centered if for any finite subset there is an element of the poset less than or equal to all of them.\n\n"}
{"id": "47889743", "url": "https://en.wikipedia.org/wiki?curid=47889743", "title": "Conductive elastomer", "text": "Conductive elastomer\n\nA conductive elastomer is a form of elastomer, often natural rubber or other rubber substitute, that is manufactured to conduct electricity. This is commonly accomplished by distributing carbon or other conductive particles throughout the raw material prior to setting it.\n\nConductive elastomers are often pressure-sensitive, with their conductivity varying with the amount of pressure put on it, and can be used to make pressure sensors.\n\nOther uses of conductive elastomers include conductive flexible seals and gaskets, and conductive mats used to prevent electrostatic damage to electronic devices. These elastomers also have uses in the energy industry, where they could be used to make flexible solar cells or stretchable devices for converting mechanical energy to electrical energy. Making solar cells and various sensors able to stretch and bend would allow them to be incorporated into wearable electronics.\n\n"}
{"id": "4002783", "url": "https://en.wikipedia.org/wiki?curid=4002783", "title": "Conservation Commons", "text": "Conservation Commons\n\nThe Conservation Commons is the expression of a cooperative effort of non-governmental organizations, international and multi-lateral organizations, governments, academia, and the private sector, to improve open access to and unrestricted use of, data, information and knowledge related to the conservation of biodiversity, with the belief that this will contribute to improving conservation outcomes. At its simplest, it encourages organizations and individuals alike to ensure open access to data, information, expertise and knowledge related to the conservation of biodiversity. The goal of the Conservation Commons is to promote conscious, effective, and equitable sharing of knowledge resources to advance conservation.\n\n\n\n"}
{"id": "11366900", "url": "https://en.wikipedia.org/wiki?curid=11366900", "title": "Crude Impact", "text": "Crude Impact\n\nCrude Impact is a 2006 film written and directed by James Jandak Wood. It is a documentary about the effect of fossil fuels on issues such as global warming, the environmental crisis, society and the questionable practices of oil companies.\n\nCrude Impact was an official selection at over thirty film festivals around the world. The film had a limited theatrical release in the United States and Canada. It has been broadcast on television in several countries. Crude Impact has been translated into French, Spanish, Czech, Turkish and Finnish.\n\n\n\n"}
{"id": "29549786", "url": "https://en.wikipedia.org/wiki?curid=29549786", "title": "Designed landscape", "text": "Designed landscape\n\nA designed landscape is an area of land which has been modified by people for primarily aesthetic effect. The term is used by historians to denote various types of site, such as gardens, parks, cemeteries, and estates. Such sites are often protected for their historic or artistic value. A designed landscape may comprise landform, water, built structures, trees and plants, all of which may be naturally occurring or introduced. \n\nMany designed landscapes take advantage of existing geographical features, emphasising them through the planting of woodlands, or the creation of artificial lakes. For example, the parklands created by landscape gardeners such as Lancelot \"Capability\" Brown, are designed landscapes. \n\nThey may also be more subtle, resulting from the enclosure of land, and the planting of functional woodlands such as shelter belts. Patterns of such features may be of use to historians in identifying the extent of country estates, and in dating agricultural improvements.\n\n"}
{"id": "38445240", "url": "https://en.wikipedia.org/wiki?curid=38445240", "title": "Dust Storm Warning", "text": "Dust Storm Warning\n\nA Dust Storm Warning (SAME code: DSW) is issued by the National Weather Service in the United States when blowing dust is expected to frequently reduce visibility to or less, generally with winds of or more.\n\nBeginning November 1st, 2018, the National Weather Service will issue these Dust Storm Warnings in an expirimental polygon and storm-based format, similar to the format of Severe Thunderstorm and Tornado Warnings. The zone-based Dust Storm Warning was replaced by the new Blowing Dust Warning product. In addition to the new Dust Storm Warning format, a lesser-impact Dust Advisory will be issued by the National Weather Service if the criteria for a warning isn’t met and if travel impacts are still expected.\n\nThe following is the first issuance of the new \"storm-based\" Dust Storm Warnings, which is in use as of June 19, 2018.\n\nThe following is an example of the previous format Dust Storm Warning issued by the National Weather Service in Tucson Arizona.\n"}
{"id": "32418626", "url": "https://en.wikipedia.org/wiki?curid=32418626", "title": "EnviroCAB", "text": "EnviroCAB\n\nEnviroCAB is a taxicab service provider based in Arlington County, Virginia, which provides service exclusively with a fleet of hybrid electric vehicles. When the company began operations in February 2008 it became the first all-hybrid taxicab fleet in the United States, and the first carbon-negative taxicab company in the world.\n\nIn September 2007 the Arlington County Board authorized EnviroCAB, then a new taxi company, to operate with an all-hybrid fleet of 50 vehicles. In addition, the Board authorized existing companies permission to add 35 hybrid taxis. The introduction of green taxis is part of a county campaign known as Fresh AIRE, or Arlington Initiative to Reduce Emissions. AIRE aims to cut production of greenhouse gases from county buildings and vehicles by 10% by 2012.\nEnviroCAB taxi fleet consist of Toyota Priuses, Toyota Camry Hybrids, Toyota Highlander Hybrids, and Ford Escape Hybrids.\n\nThe company claims to be the first carbon-negative taxicab company in the world, as it will completely offset its own emissions by purchasing \"clean-source\" offset credits. Also, EnviroCAB expects to offset the emissions of 100 of the approximately 685 non-hybrid taxis operating in Arlington by March 2008.\n\n"}
{"id": "57801212", "url": "https://en.wikipedia.org/wiki?curid=57801212", "title": "Evolutionary Ecology (journal)", "text": "Evolutionary Ecology (journal)\n\nEvolutionary Ecology is a bimonthly peer-reviewed scientific journal covering the study of ecology from an evolutionary perspective. It was established in 1987 and is published by Springer Science+Business Media. The editor-in-chief is Matthew Symonds (Deakin University). According to the \"Journal Citation Reports\", the journal has a 2017 impact factor of 2.133. \n"}
{"id": "1437371", "url": "https://en.wikipedia.org/wiki?curid=1437371", "title": "Fatty acid metabolism", "text": "Fatty acid metabolism\n\nFatty acid metabolism consists of catabolic processes that generate energy, and anabolic processes that create biologically important molecules (triglycerides, phospholipids, second messengers, local hormones and ketone bodies).\nFatty acids are a family of molecules classified within the lipid macronutrient class. One role of fatty acids in animal metabolism is energy production, captured in the form of adenosine triphosphate (ATP). When compared to other macronutrient classes (carbohydrates and protein), fatty acids yield the most ATP on an energy per gram basis, when they are completely oxidized to CO and water by beta oxidation and the citric acid cycle. Fatty acids (mainly in the form of triglycerides) are therefore the foremost storage form of fuel in most animals, and to a lesser extent in plants. In addition, fatty acids are important components of the phospholipids that form the phospholipid bilayers out of which all the membranes of the cell are constructed (the cell wall, and the membranes that enclose all the organelles within the cells, such as the nucleus, the mitochondria, endoplasmic reticulum, and the Golgi apparatus). Fatty acids can also be cleaved, or partially cleaved, from their chemical attachments in the cell membrane to form second messengers within the cell, and local hormones in the immediate vicinity of the cell. The prostaglandins made from arachidonic acid stored in the cell membrane, are probably the most well known group of these local hormones.\n\nFatty acids are released, between meals, from the fat depots in adipose tissue, where they are stored as triglycerides, as follows:\n\n\n\n\n\n\nIn the liver oxaloacetate can be wholly or partially diverted into the gluconeogenic pathway during fasting, starvation, a low carbohydrate diet, prolonged strenuous exercise, and in uncontrolled type 1 diabetes mellitus. Under these circumstances oxaloacetate is hydrogenated to malate which is then removed from the mitochondrion to be converted into glucose in the cytoplasm of the liver cells, from where it is released into the blood. In the liver, therefore, oxaloacetate is unavailable for condensation with acetyl-CoA when significant gluconeogenesis has been stimulated by low (or absent) insulin and high glucagon concentrations in the blood. Under these circumstances acetyl-CoA is diverted to the formation of acetoacetate and beta-hydroxybutyrate. Acetoacetate, beta-hydroxybutyrate, and their spontaneous breakdown product, acetone, are frequently, but confusingly, known as ketone bodies (as they are not \"bodies\" at all, but water-soluble chemical substances). The ketones are released by the liver into the blood. All cells with mitochondria can take ketones up from the blood and reconvert them into acetyl-CoA, which can then be used as fuel in their citric acid cycles, as no other tissue can divert its oxaloacetate into the gluconeogenic pathway in the way that this can occur in the liver. Unlike free fatty acids, ketones can cross the blood-brain barrier and are therefore available as fuel for the cells of the central nervous system, acting as a substitute for glucose, on which these cells normally survive. The occurrence of high levels of ketones in the blood during starvation, a low carbohydrate diet, prolonged heavy exercise and uncontrolled type 1 diabetes mellitus is known as ketosis, and, in its extreme form, in out-of-control type 1 diabetes mellitus, as ketoacidosis.\n\nFatty acids, stored as triglycerides in an organism, are an important source of energy because they are both reduced and anhydrous. The energy yield from a gram of fatty acids is approximately 9 kcal (37 kJ), compared to 4 kcal (17 kJ) for carbohydrates. Since the hydrocarbon portion of fatty acids is hydrophobic, these molecules can be stored in a relatively anhydrous (water-free) environment. Carbohydrates, on the other hand, are more highly hydrated. For example, 1 g of glycogen can bind approximately 2 g of water, which translates to 1.33 kcal/g (4 kcal/3 g). This means that fatty acids can hold more than six times the amount of energy per unit of storage mass. Put another way, if the human body relied on carbohydrates to store energy, then a person would need to carry 31 kg (67.5 lb) of hydrated glycogen to have the energy equivalent to 4.6 kg (10 lb) of fat.\n\nHibernating animals provide a good example for utilizing fat reserves as fuel. For example, bears hibernate for about 7 months, and, during this entire period, the energy is derived from degradation of fat stores. Migrating birds similarly build up large fat reserves before embarking on their intercontinental journeys.\n\nThus the young adult human’s fat stores average between about 10–20 kg, but varies greatly depending on age, gender, and individual disposition. By contrast the human body stores only about 400 g of glycogen, of which 300 g is locked inside the skeletal muscles and is unavailable to the body as a whole. The 100 g or so of glycogen stored in the liver is depleted within one day of starvation. Thereafter the glucose that is released into the blood by the liver for general use by the body tissues, has to be synthesized from the glucogenic amino acids and a few other gluconeogenic substrates, which do not include fatty acids. Please note however that lipolysis releases glycerol which can enter the pathway of gluconeogenesis.\n\nFatty acids are broken down to acetyl-CoA by means of beta oxidation inside the mitochondria, whereas fatty acids are synthesized from acetyl-CoA outside the mitochondria, in the cytosol. The two pathways are distinct, not only in where they occur, but also in the reactions that occur, and the substrates that are used. The two pathways are mutually inhibitory, preventing the acetyl-CoA produced by beta-oxidation from entering the synthetic pathway via the acetyl-CoA carboxylase reaction. It can also not be converted to pyruvate as the pyruvate dehydrogenase complex reaction is irreversible. Instead the acetyl-CoA produced by the beta-oxidation of fatty acids condenses with oxaloacetate, to enter the citric acid cycle. During each turn of the cycle, two carbon atoms leave the cycle as CO in the decarboxylation reactions catalyzed by isocitrate dehydrogenase and alpha-ketoglutarate dehydrogenase. Thus each turn of the citric acid cycle oxidizes an acetyl-CoA unit while regenerating the oxaloacetate molecule with which the acetyl-CoA had originally combined to form citric acid. The decarboxylation reactions occur before malate is formed in the cycle. Only plants possess the enzymes to convert acetyl-CoA into oxaloacetate from which malate can be formed to ultimately be converted to glucose.\n\nHowever acetyl-CoA can be converted to acetoacetate, which can decarboxylate to acetone (either spontaneously, or by acetoacetate decarboxylase). It can then be further metabolized to isopropanol which is excreted in breath/urine, or by CYP2E1 into hydroxyacetone (acetol). Acetol can be converted to propylene glycol. This converts to formate and acetate (the latter converting to glucose), or pyruvate (by two alternative enzymes), or propionaldehyde, or to -lactaldehyde then -lactate (the common lactate isomer). Another pathway turns acetol to methylglyoxal, then to pyruvate, or to -lactaldehyde (via -lactoyl-glutathione or otherwise) then -lactate. D-lactate metabolism (to glucose) is slow or impaired in humans, so most of the D-lactate is excreted in the urine; thus -lactate derived from acetone can contribute significantly to the metabolic acidosis associated with ketosis or isopropanol intoxication. -Lactate can complete the net conversion of fatty acids into glucose. The first experiment to show conversion of acetone to glucose was carried out in 1951. This, and further experiments used carbon isotopic labelling. Up to 11% of the glucose can be derived from acetone during starvation in humans.\n\nThe glycerol released into the blood during the lipolysis of triglycerides in adipose tissue can only be taken up by the liver. Here it is converted into glycerol 3-phosphate by the action of glycerol kinase which hydrolyzes one molecule of ATP per glycerol molecule which is phosphorylated. Glycerol 3-phosphate is then oxidized to dihydroxyacetone phosphate, which is, in turn, converted into glyceraldehyde 3-phosphate by the enzyme triose phosphate isomerase. From here the three carbon atoms of the original glycerol can be oxidized via glycolysis, or converted to glucose via gluconeogenesis.\n\nFatty acids are an integral part of the phospholipids that make up the bulk of the plasma membranes, or cell membranes, of cells. These phospholipids can be cleaved into diacylglycerol (DAG) and inositol trisphosphate (IP) through hydrolysis of the phospholipid, phosphatidylinositol 4,5-bisphosphate (PIP), by the cell membrane bound enzyme phospholipase C (PLC).\n\nAn example of a diacyl-glycerol shown on the right. This DAG is 1-palmitoyl-2-oleoyl-glycerol, which contains side-chains derived from palmitic acid and oleic acid. Diacylglycerols can also have many other combinations of fatty acids attached at either the C-1 and C-2 positions or the C-1 and C-3 positions of the glycerol molecule. 1,2 disubstituted glycerols are always chiral, 1,3 disubstituted glycerols are chiral if the substituents are different from each other.\n\nInositol trisphosphate (IP) functions as an intracellular second messenger, which initiates the intracellular release of calcium ions (which activates intracellular enzymes, causes the release of hormones and neurotransmitters from the cells in which they are stored, and causes smooth muscle contraction when released by IP), and the activation of protein kinase C (PKC), which is then translocated from the cell cytoplasm to the cell membrane. Although inositol trisphosphate, (IP), diffuses into the cytosol, diacylglycerol (DAG) remains within the plasma membrane, due to its hydrophobic properties. IP stimulates the release of calcium ions from the smooth endoplasmic reticulum, whereas DAG is a physiological activator of protein kinase C (PKC), promoting its translocation from the cytosol to the plasma membrane. PKC is a multifunctional protein kinase which phosphorylates serine and threonine residues in many target proteins. However PKC is only active in the presence of calcium ions, and it is DAG that increases the affinity of PKC for Ca and thereby renders it active at the physiological intracellular levels of this ion.\n\nDiacylglycerol and IP act transiently because both are rapidly metabolized. This is important as their message function should not linger after the message has been” received” by their target molecules. DAG can be phosphorylated to phosphatidate or it can be it can be hydrolysed to glycerol and its constituent fatty acids. IP is rapidly converted into derivatives that do not open calcium ion channels.\n\nThe prostaglandins are a group of physiologically active lipid compounds having diverse hormone-like effects in animals. Prostaglandins have been found in almost every tissue in humans and other animals. They are enzymatically derived from arachidonic acid a 20-carbon polyunsaturated fatty acid. Every prostaglandin therefore contains 20 carbon atoms, including a 5-carbon ring. They are a subclass of eicosanoids and form the prostanoid class of fatty acid derivatives.\n\nThe prostaglandins are synthesized in the cell membrane by the cleavage of arachidonate from the phospholipids that make up the membrane. This is catalyzed either by phospholipase A acting directly on a membrane phospholipid, or by a lipase acting on DAG (diacyl-glycerol). The arachidonate is then acted upon by the cyclooxygenase component of prostaglandin synthase. This forms a cyclopentane ring in roughly the middle of the fatty acid chain. The reaction also adds 4 oxygen atoms derived from two molecules of O. The resulting molecule is prostaglandin G which is converted by the hydroperoxidase component of the enzyme complex into prostaglandin H. This highly unstable compound is rapidly transformed into other prostaglandins, prostacyclin and thromboxanes. These are then released into the interstitial fluids surrounding the cells that have manufactured the eicosanoid hormone.\n\nIf arachidonate is acted upon by a lipoxygenase instead of cyclooxygenase, Hydroxyeicosatetraenoic acids and leukotrienes are formed. They also act as local hormones.\n\nProstaglandins were originally believed to leave the cells via passive diffusion because of their high lipophilicity. The discovery of the prostaglandin transporter (PGT, SLCO2A1), which mediates the cellular uptake of prostaglandin, demonstrated that diffusion alone cannot explain the penetration of prostaglandin through the cellular membrane. The release of prostaglandin has now also been shown to be mediated by a specific transporter, namely the multidrug resistance protein 4 (MRP4, ABCC4), a member of the ATP-binding cassette transporter superfamily. Whether MRP4 is the only transporter releasing prostaglandins from the cells is still unclear.\n\nThe structural differences between prostaglandins account for their different biological activities. A given prostaglandin may have different and even opposite effects in different tissues. The ability of the same prostaglandin to stimulate a reaction in one tissue and inhibit the same reaction in another tissue is determined by the type of receptor to which the prostaglandin binds. They act as autocrine or paracrine factors with their target cells present in the immediate vicinity of the site of their secretion. Prostaglandins differ from endocrine hormones in that they are not produced at a specific site but in many places throughout the human body.\n\nProstaglandins have two derivatives: prostacyclins and thromboxanes. Prostacyclins are powerful locally acting vasodilators and inhibit the aggregation of blood platelets. Through their role in vasodilation, prostacyclins are also involved in inflammation. They are synthesized in the walls of blood vessels and serve the physiological function of preventing needless clot formation, as well as regulating the contraction of smooth muscle tissue. Conversely, thromboxanes (produced by platelet cells) are vasoconstrictors and facilitate platelet aggregation. Their name comes from their role in clot formation (thrombosis).\n\nA significant proportion of the fatty acids in the body are obtained from the diet, in the form of triglycerides of either animal or plant origin. The fatty acids in the fats obtained from land animals tend to be saturated, whereas the fatty acids in the triglycerides of fish and plants are often polyunsaturated and therefore present as oils.\n\nThese triglycerides, cannot be absorbed by the intestine. They are broken down into mono- and di-glycerides plus free fatty acids (but no free glycerol) by pancreatic lipase, which forms a 1:1 complex with a protein called colipase (also a constituent of pancreatic juice), which is necessary for its activity. The activated complex can work only at a water-fat interface. Therefore, it is essential that fats are first emulsified by bile salts for optimal activity of these enzymes. The digestion products consisting of a mixture of tri-, di- and monoglycerides and free fatty acids, which, together with the other fat soluble contents of the diet (e.g. the fat soluble vitamins and cholesterol) and bile salts form mixed micelles, in the watery duodenal contents (see diagrams on the right).\n\nThe contents of these micelles (but not the bile salts) enter the enterocytes (epithelial cells lining the small intestine) where they are resynthesized into triglycerides, and packaged into chylomicrons which are released into the lacteals (the capillaries of the lymph system of the intestines). These lacteals drain into the thoracic duct which empties into the venous blood at the junction of the left jugular and left subclavian veins on the lower left hand side of the neck. This means that the fat soluble products of digestion are discharged directly into the general circulation, without first passing through the liver, as all other digestion products do. The reason for this peculiarity is unknown.\n\nThe chylomicrons circulate throughout the body, giving the blood plasma a milky, or creamy appearance after a fatty meal. Lipoprotein lipase on the endothelial surfaces of the capillaries, especially in adipose tissue, but to a lesser extent also in other tissues, partially digests the chylomicrons into free fatty acids, glycerol and chylomicron remnants. The fatty acids are absorbed by the adipocytes, but the glycerol and chylomicron remnants remain in the blood plasma, ultimately to be removed from the circulation by the liver. The free fatty acids released by the digestion of the chylomicrons are absorbed by the adipocytes, where they are resynthesized into triglycerides using glycerol derived from glucose in the glycolytic pathway. These triglycerides are stored, until needed for the fuel requirements of other tissues, in the fat droplet of the adipocyte.\n\nThe liver absorbs a proportion of the glucose from the blood in the portal vein coming from the intestines. After the liver has replenished its glycogen stores (which amount to only about 100 g of glycogen when full) much of the rest of the glucose is converted into fatty acids as described below. These fatty acids are combined with glycerol to form triglycerides which are packaged into droplets very similar to chylomicrons, but known as very low-density lipoproteins (VLDL). These VLDL droplets are handled in exactly the same manner as chylomicrons, except that the VLDL remnant is known as an intermediate-density lipoprotein (IDL), which is capable of scavenging cholesterol from the blood. This converts IDL into low-density lipoprotein (LDL), which is taken up by cells that require cholesterol for incorporation into their cell membranes or for synthetic purposes (e.g. the formation of the steroid hormones). The remainder of the LDLs is removed by the liver.\n\nAdipose tissue and lactating mammary glands also take up glucose from the blood for conversion into triglycerides. This occurs in the same way as it does in the liver, except that these tissues do not release the triglycerides thus produced as VLDL into the blood. Adipose tissue cells store the triglycerides in their fat droplets, ultimately to release them again as free fatty acids and glycerol into the blood (as described above), when the plasma concentration of insulin is low, and that of glucagon and/or epinephrine is high. Mammary glands discharge the fat (as cream fat droplets) into the milk that they produce under the influence of the anterior pituitary hormone prolactin.\n\nAll cells in the body need to manufacture and maintain their membranes and the membranes of their organelles. Whether they rely for this entirely on free fatty acids absorbed from the blood, or are able to synthesize their own fatty acids from the blood glucose, is not known. The cells of the central nervous system will almost certainly have the capability of manufacturing their own fatty acids, as these molecules cannot reach them through the blood brain barrier, while, on the other hand, no cell in the body can manufacture the required essential fatty acids which have to be obtained from the diet and delivered to each cell via the blood.\n\nMuch like beta-oxidation, straight-chain fatty acid synthesis occurs via the six recurring reactions shown below, until the 16-carbon palmitic acid is produced.\n\nThe diagrams presented show how fatty acids are synthesized in microorganisms and list the enzymes found in Escherichia coli. These reactions are performed by fatty acid synthase II (FASII), which in general contain multiple enzymes that act as one complex. FASII is present in prokaryotes, plants, fungi, and parasites, as well as in mitochondria.\n\nIn animals, as well as some fungi such as yeast, these same reactions occur on fatty acid synthase I (FASI), a large dimeric protein that has all of the enzymatic activities required to create a fatty acid. FASI is less efficient than FASII; however, it allows for the formation of more molecules, including \"medium-chain\" fatty acids via early chain termination. Enzymes, acyltransferases and transacylases, incorporate fatty acids in phospholipids, triacylglycerols, etc. by transferring fatty acids between an acyl acceptor and donor. They also have the job of synthesizing bioactive lipids as well as their precursor molecules.\n\nOnce a 16:0 carbon fatty acid has been formed, it can undergo a number of modifications, resulting in desaturation and/or elongation. Elongation, starting with stearate (18:0), is performed mainly in the ER by several membrane-bound enzymes. The enzymatic steps involved in the elongation process are principally the same as those carried out by FAS, but the four principal successive steps of the elongation are performed by individual proteins, which may be physically associated.\n\nAbbreviations: ACP – Acyl carrier protein, CoA – Coenzyme A, NADP – Nicotinamide adenine dinucleotide phosphate.\n\nNote that during fatty synthesis the reducing agent is NADPH, whereas NAD is the oxidizing agent in beta-oxidation (the breakdown of fatty acids to acetyl-CoA). This difference exemplifies a general principle that NADPH is consumed during biosynthetic reactions, whereas NADH is generated in energy-yielding reactions. (Thus NADPH is also required for the synthesis of cholesterol from acetyl-CoA; while NADH is generated during glycolysis.) The source of the NADPH is two-fold. When malate is oxidatively decarboxylated by “NADP-linked malic enzyme\" pyruvate, CO and NADPH are formed. NADPH is also formed by the pentose phosphate pathway which converts glucose into ribose, which can be used in synthesis of nucleotides and nucleic acids, or it can be catabolized to pyruvate.\n\nIn humans, fatty acids are formed from carbohydrates predominantly in the liver and adipose tissue, as well as in the mammary glands during lactation. The cells of the central nervous system probably also make most of the fatty acids needed for the phospholipids of their extensive membranes from glucose, as blood-born fatty acids cannot cross the blood brain barrier to reach these cells. However, how the essential fatty acids, which mammals cannot synthesize themselves, but are nevertheless important components of cell membranes (and other functions described above) reach them is unknown.\n\nThe pyruvate produced by glycolysis is an important intermediary in the conversion of carbohydrates into fatty acids and cholesterol. This occurs via the conversion of pyruvate into acetyl-CoA in the mitochondrion. However, this acetyl CoA needs to be transported into cytosol where the synthesis of fatty acids and cholesterol occurs. This cannot occur directly. To obtain cytosolic acetyl-CoA, citrate (produced by the condensation of acetyl CoA with oxaloacetate) is removed from the citric acid cycle and carried across the inner mitochondrial membrane into the cytosol. There it is cleaved by ATP citrate lyase into acetyl-CoA and oxaloacetate. The oxaloacetate is returned to mitochondrion as malate (and then converted back into oxaloacetate to transfer more acetyl-CoA out of the mitochondrion). The cytosolic acetyl-CoA is carboxylated by acetyl CoA carboxylase into malonyl CoA, the first committed step in the synthesis of fatty acids.\n\nAcetyl-CoA is formed into malonyl-CoA by acetyl-CoA carboxylase, at which point malonyl-CoA is destined to feed into the fatty acid synthesis pathway. Acetyl-CoA carboxylase is the point of regulation in saturated straight-chain fatty acid synthesis, and is subject to both phosphorylation and allosteric regulation. Regulation by phosphorylation occurs mostly in mammals, while allosteric regulation occurs in most organisms. Allosteric control occurs as feedback inhibition by palmitoyl-CoA and activation by citrate. When there are high levels of palmitoyl-CoA, the final product of saturated fatty acid synthesis, it allosterically inactivates acetyl-CoA carboxylase to prevent a build-up of fatty acids in cells. Citrate acts to activate acetyl-CoA carboxylase under high levels, because high levels indicate that there is enough acetyl-CoA to feed into the Krebs cycle and produce energy.\n\nHigh plasma levels of insulin in the blood plasma (e.g. after meals) cause the dephosphorylation and activation of acetyl-CoA carboxylase, thus promoting the formation of malonyl-CoA from acetyl-CoA, and consequently the conversion of carbohydrates into fatty acids, while epinephrine and glucagon (released into the blood during starvation and exercise) cause the phosphorylation of this enzyme, inhibiting lipogenesis in favor of fatty acid oxidation via beta-oxidation.\n\nDisorders of fatty acid metabolism can be described in terms of, for example, hypertriglyceridemia (too high level of triglycerides), or other types of hyperlipidemia. These may be familial or acquired.\n\nFamilial types of disorders of fatty acid metabolism are generally classified as inborn errors of lipid metabolism. These disorders may be described as fatty oxidation disorders or as a \"lipid storage disorders\", and are any one of several inborn errors of metabolism that result from enzyme defects affecting the ability of the body to oxidize fatty acids in order to produce energy within muscles, liver, and other cell types.\n"}
{"id": "32879174", "url": "https://en.wikipedia.org/wiki?curid=32879174", "title": "Finndøla Hydroelectric Power Station", "text": "Finndøla Hydroelectric Power Station\n\nThe Finndøla Power Station is a hydroelectric power station located in Fyresdal, Telemark, Norway. It operates at an installed capacity of , with an average annual production of about 254 GW·h.\n"}
{"id": "15657581", "url": "https://en.wikipedia.org/wiki?curid=15657581", "title": "Flood warning", "text": "Flood warning\n\nA Flood warning is closely linked to the task of flood forecasting. The distinction between the two is that the outcome of flood forecasting is a set of forecast time-profiles of channel flows or river levels at various locations, while \"flood warning\" is the task of making use of these forecasts to make decisions about whether warnings of floods should be issued to the general public or whether previous warnings should be rescinded or retracted.\n\nThe task of providing warning for floods is divided into two parts:\n\nThe decisions made by someone responsible for initiating flood warnings must be influenced by a number of factors, which include:\n\nA computer system for flood warning will usually contain sub-systems for:\n\nThe type of flood warning service available varies greatly from country to country, and a location may receive warnings from more than one service.\n\nArrangements for flood warnings vary across the United Kingdom with several agencies leading on warnings for emergency responders and the public. The Environment Agency, Natural Resources Wales and Scottish Environment Protection Agency all undertake location specific flood warning activities for communities at risk depending upon the scale of flood risk, technical challenges and investment needed to deliver a reliable service.\n\nPrior to issuing a flood warning consideration is given to:\n\nDissemination of flood warnings has moved towards a service whereby those at risk can pre-register to receive warnings by phone, email or text message from an automatic system, Floodline. Both warnings and updates about current conditions are also carried by local radio stations. In addition, live updates are carried by the Environment Agency's website, showing which locations have flood warnings in place and the severity of these warnings.\n\nThere is currently no flood warning system in Northern Ireland, but the Met Office does issue weather warnings. Flood risk management is the responsibility of Rivers Agency in Northern Ireland. Consideration will be given to the introduction of a warning system as part of the implementation of the EU Floods directive.\n\nIn the United States, the National Weather Service issues flood watches and warnings for large-scale, gradual river flooding. Watches are issued when flooding is possible or expected within 12–48 hours, and warnings are issued when flooding over a large area or river flooding is imminent or occurring. Both can be issued on a county-by-county basis or for specific rivers or points along a river. When rapid flooding from heavy rain or a dam failure is expected, flash flood watches and warnings are issued. \n\nIn the U.S. and Canada, dissemination of flood warnings is covered by Specific Area Message Encoding (SAME) code FLW, which is used by the U.S. Emergency Alert System and NOAA Weather Radio network and in Canada's Weatheradio Canada network.\n\n\"Flood statements\" are issued by the National Weather Service to inform the public of flooding along major streams in which there is not a serious threat to life or property. They may also follow a flood warning to give later information.\n\nThe following is an example of a \"Flood Warning.\" The South Chickamauga Creek is used as an example:\n\nSource:\n\nSource:\n\nThe Iowa Flood Center at the University of Iowa operates the largest real-time flood monitoring system of its kind in the world. It includes more than 200 real-time stream stage sensors that feed data into the Iowa Flood Information System where data can be viewed, online, by disaster management staff and the general public. The stream stage sensors, mounted on bridges and culverts, use ultrasonic sensors to monitor stream and river levels.\n\n"}
{"id": "49520086", "url": "https://en.wikipedia.org/wiki?curid=49520086", "title": "Georg Matthias Bose", "text": "Georg Matthias Bose\n\n\"Not to be confused with the unrelated Georg Bose.\"\n\nGeorg Matthias Bose (22 September 1710 – 17 September 1761), also known as Mathias Bose, was a famous electrical experimenter in the early days of the development of electrostatics. He is credited with being the first to develop a way of temporarily storing static charges by using an insulated conductor (called a prime conductor). His demonstrations and experiments raised the interests of the German scientific community and the public in the development of electrical research.\n\nHe was born the son of a merchant and educated at the University of Leipzig, receiving his Master's degree in 1727. In 1738, he became the professor of natural philosophy at the University of Wittenberg. As part of his course in physics, he revived experimentation with a glass-globe machine following the design of Francis Hauksbee the Elder, and he later vastly improved on the machine by adding a \"prime conductor\" which allowed the machine to accumulate the generated static charge at a higher level.\n\nHe was said to be a flamboyant demonstrator and an assiduous self-promoter, and he corresponded extensively with the Royal Society in London and with similar groups in Prussia, France, Italy and even Istanbul.\n\nBose's initial prime conductor consisted of an assistant standing on a block of resin (an effective insulator) holding a metal bar in one hand while touching the spinning globe with the other. The friction-generated charge would have flowed through the assistant to the metal bar and accumulated on the external surface of the bar. In effect, this was an extension of Stephen Gray's 1730 Flying Boy demonstration, but with the addition of a metal conductor which, over time, became the only effective storage device.\n\nWhat was novel was Bose's use of metal (the bar) at a time when it had long been accepted that only insulators (then called \"electrics\") could successfully accumulate static electricity. Metal conductors were known to dissipate any charge relatively quickly and the need to insulate charged objects from electrical contact with the earth was not well appreciated. Later experiments by other experimenters showed that it was not the mass of the prime conductor which set the limit of static accumulation so much as its exterior dimensions—since like-charges repel and so the cumulative effect only exists on the external skin of the conductor.\n\nFor three years, from 1742 to 1745, Bose promoted the study of electricity in Germany, and became famous for spectacular experiments including one where he set alight alcoholic spirits floating on the surface of water via a spark generated by his friction machine which passed through the water. Since water and fire were seen as direct opposites, this created a minor sensation among the observers and became widely mentioned in scientific correspondence.\n\nBose's \"Electric Kiss\" (also called \"Electric Venus\") demonstration was immensely popular with spectators, and it was little more than a variation on Stephen Gray's \"Flying Boy\" demonstration. An attractive young lady was invited to stand on a block of insulating resin, and she was given a moderate static charge from a spinning globe. A young man from the audience was then invited to give her a kiss, and, in the process, the pair received a reasonable shock.\n\nThis demonstration combined both the scientific illustration of charge accumulation with the naughtiness of a stolen kiss, so it became a mainstay of all electrical showmen and scientific demonstrators.\n\nOne of the public demonstrations developed by Bose became known as \"Beatification\", and Bose purposefully concealed the means by which he generated the effect. Eventually he was accused of fabrication, and so he revealed the technique to a colleague who published it in the Royal Society's \"Philosophical Transactions\".\n\nIn essences this was an extension of his Prime Conductor developments. A person dressed in a metal helmet or suit of armour would sit on a highly insulated chair and receive a high level of static charge—enough to produce sparkling points and possibly plasma glows around conductive surfaces. Bose eventually admitted to exaggeration, and explained the effect:\n\nThe news of Bose's alcohol experiment quickly spread to Pomerania, a small independent state on the Baltic coast, where the dean of the Camin cathedral was also experimenting with a friction generator. J.G. von Kleist (aka Jurgen Georg) had suspended a large stove-pipe above his machine on silk threads which, by coincidence rather than design, turned out to be the best possible shape and mass for a prime conductor in its ability to accumulate charge.\n\nVon Kleist wondered whether Bose's experiment at transmitting electricity through water to the flammable spirits would extend to capturing electricity in a small medicine bottle filled with alcohol, since he knew that the glass of the bottle would act as an insulator and prevent the electricity from escaping. He therefore fitted his bottle with a cork through which protruded a nail in contact with the fluid to provide a point of entry.\n\nHolding the bottle in one hand, he applied the nail to the prime conductor. Nothing happened immediately, but while carrying the bottle back into a dark room he noticed a slight plasma glow which suggested to him that the alcohol was charge with St Elmo's fire. When he touched the nail with his spare hand he experienced a most violent shock. He said in correspondence that it threw him across the room.\n\nVon Kleist had invented the Leyden Jar, but he did not understand the significance of his cupped hand held around the outside of the bottle which provided the one exterior conductive element needed to allow the Leyden Jar to accumulate a massive amount of charge.\n\nHaving experienced such a shock, he was naturally loath to hold the bottle in his hand again and he seems not to have been able to duplicate the experiment. When he advised his scientific correspondents, they were also naturally very wary of holding the jar, and for some time they failed to duplicate the experiment. He is known to have corresponded his finding to a: Dr Lieberkuhn in Berlin, Mr Winckler at Lepzick (Leipzig), Mr Świetlicki of Dantzick (Danzig) and Mr Krugar of Hall as well as the professors of the academy of Lignitz.(sic)\n\nVon Kleist had been educated at the University of Leyden, and it is now widely assumed that he would also have corresponded with fellow ex-students Andreas Cunaeus (a lawyer), and Prof. Jean Allamand (theologian professor) at the University. Allmand attempted to duplicate the experiment with a glass of beer and eventually succeeded.\n\nCunaeus and Allmand worked occasionally with the professor of physics at Leyden University, Pieter Musschenbroek on new electrical experiments, and it took time for them to discover the secret. They finally worked out the importance of the outside conduction surface provided by the hand which was needed to prevent the development of electrostatic backpressure, and, as a result, it was the Leyden University experimenters who received the bulk of the credit.\n\nIn 1744, Bose published his major works on electricity in the form of pamphlets released in London and Paris. However, many of his manuscripts were later lost during the Thirty Years War. He is now mainly celebrated for his spectacular demonstrations rather than for his scientific contributions. In 1757 he was elected a Fellow of the Royal Society of London.\n\nIn 1760, during the Seven Years' War with Prussia, Bose was kidnapped from Wittenberg as a strategic asset, and taken to Magdeburg where he was held as a hostage. This, ironically, was the site of Otto von Guericke's famous sulphur ball experiment. He died at Magdeburg two years later.\n\n\n"}
{"id": "40687534", "url": "https://en.wikipedia.org/wiki?curid=40687534", "title": "Great Green Fleet", "text": "Great Green Fleet\n\nThe US Navy's Great Green Fleet was an energy cost saving measure (ECM) announced in 2009 to begin using a combination of conventional diesel fuel and biofuels in a 50/50 mixture. The first demonstration by the USS Nimitz carrier task group during RIMPAC (Rim of the Pacific exercise) in 2012 was completed without incident. The Great Green Fleet is the popular nickname is an homage to the Great White Fleet of the early 20th century.\n\nSince the 1970s the world has begun to be acutely aware of the impact of climate change and the influence fossil fuels has had on the progress of the natural migrations of climate. \nIt is with that in mind that the US military is seeking to change the profile of its energy usage. \nWhile each branch of the military has its own goals and plans, the Navy's goals are particularly lofty:\n\nThe Navy's efforts are highlighted by the design of the Great Green Fleet. The carrier, the USS Nimitz, was nuclear powered, but everything else, including the Nimitz's strike aircraft, ran on a 50:50 mix of petroleum and biofuel derived from cooking oil and algae. Set to fully deploy in 2016, the fleet will combine advances in fuels, equipment, and navigation all in an effort to deploy the most energy efficient and modern fleet anywhere in the world.\n\nMore than six years after it was first announced, the Great Green Fleet made its maiden voyage on late January 20, 2016.\nOverall, the 2016 Great Green Fleet initiative was a year-long event. The Department of the Navy obtained 77.66 million gallons of cost-competitive, drop-in biofuels blends in support of the launch of the Great Green Fleet at $2.05 per gal. One of the goals of the Navy in its biofuels program is “not to have to sail to the Middle East every time we re-fuel.\" The Navy aims to deploy a permanent green strike force after 2016.\n\n\"The Great Green Fleet will signal to the world America's continued naval supremacy, unleashed from the tether of foreign oil.\" - Ray Mabus, Secretary of the Navy\n\nThe 2012 fleet was composed of the aircraft carrier USS \"Nimitz\" (CVN 68), the cruiser USS \"Princeton\" (CG 59), the two destroyers USS \"Chafee\" (DDG 90) and USS \"Chung Hoon\" (DDG 93), and the fuel tanker USNS \"Henry J. Kaiser\" (T-AO-187).\n\nThe 2016 fleet was composed of the aircraft carrier USS \"John C. Stennis\" (CVN 74), guided-missile cruiser USS \"Mobile Bay\" (CG 53), and guided-missile destroyers USS \"Chung Hoon\" (DDG 93), USS \"Stockdale\" (DDG 106) and USS \"William P. Lawrence\" (DDG 110) are all operating in the Indo-Asia-Pacific using alternative fuel.\n\nThere are reservations from some politicians, as well as military officials that the transition to alternative fuel sources would be too costly.\nBeyond the strict fuel costs, there are also concerns about the time and resources which would be required to create the necessary support structure to produce the volume of fuel sufficient for the military's needs.\nDuring the oil embargo of 1973 there were extensive and costly efforts to develop alternative fuels. A market for these fuels failed to develop, and economical fuels and research and development was not pursued. Many economists and scientists fear that this atmosphere will not develop for the current fuels.\n\nThe Navy's goals will further their own objectives, but will also have far-reaching effects in the US and beyond. Reducing U.S reliance on foreign petroleum has numerous strategic advantages, and could work to avert future conflicts centered around the acquisition of petroleum stores. The call for alternative fuels has already begun to drive innovation, with companies striving to create efficient processes to bring biofuel availability to the necessary scale the military requires. Energy companies Solazyme (CA) and Dynamic Fuels (LA) are working towards numerous fuels and competing for the contracts offered by the U.S Department of Defense, and continued studies by military and civilian researchers will build momentum in both the scientific community, as well as the energy marketplace. Studies have also shown a decrease in particulate emissions from the use of algal biofuels versus naval diesel fuel. Crossover and cooperation with the civilian marketplace is already underway with comparable studies and efforts underway in civilian aviation and maritime fleets. As all these efforts come together, we will see public acceptance for biofuels grow, and a demand for cleaner, more efficient fuels from populations across the globe.\n"}
{"id": "45382688", "url": "https://en.wikipedia.org/wiki?curid=45382688", "title": "HVDC DolWin2", "text": "HVDC DolWin2\n\nHVDC DolWin2 is a high voltage direct current (HVDC) link to transmit offshore wind power to the power grid of the German mainland. The project differs from most HVDC systems in that one of the two converter stations is built on a platform in the sea. Voltage-Sourced Converters with DC ratings of 900 MW, ±320 kV are used and the total cable length is 135 km. The project is similar to the HVDC DolWin1 project but has a slightly higher power rating and uses a different design of offshore platform. The platform was designed by Norwegian company Aibel and built by Drydocks World in Dubai and transported to Europe in the summer of 2014 to be fitted out at the Aibel yard in Haugesund in Norway. The platform, which is of a floating, self-installing design not previously used in an HVDC project, sailed out from Haugesund on 1 August 2015 and was installed in the North Sea ten days later.\n\nThe overall project was built by ABB and was handed over to its owner, TenneT, in 2017.\n\n\n"}
{"id": "56821216", "url": "https://en.wikipedia.org/wiki?curid=56821216", "title": "Habiba Al Marashi", "text": "Habiba Al Marashi\n\nHabiba Al Marashi is an Emirati environmentalist. In 1991 she founded the Emirates Environmental Group, which she continues to chair. In 2004 she founded the Arabia CSR Network (ACSRN), devoted to corporate social responsibility across the Arab region.\n\n"}
{"id": "11345064", "url": "https://en.wikipedia.org/wiki?curid=11345064", "title": "Harold Denton", "text": "Harold Denton\n\nHarold Ray Denton (February 24, 1936 – February 13, 2017) was the Director of the Office of Nuclear Reactor Regulation at the United States Nuclear Regulatory Commission (USNRC) and is best known for his role as President Jimmy Carter's personal adviser for the Three Mile Island (TMI) accident.\n\nAfter graduating in 1958 with a Bachelor of Science in nuclear engineering from North Carolina State University College of Engineering, Denton first worked at DuPont as an engineer for several years, before being hired by the USNRC. After 10 years, he became the Director of the Office of Nuclear Reactor Regulation, a position he held until his retirement in 1998.\n\nIn 1979, President Carter sent Denton to the Three Mile Island nuclear power plant in Harrisburg, as his personal representative. The arrival of Denton seemed to immediately calm the frayed nerves of public officials and stem the anger of a frustrated press corps. As reporter Steve Liddick of WCMB radio explained to writer Mark Stephens, \"Harold Denton was trusted because he looked like a regular, down-to-earth kind of guy. And people wanted someone to believe.\"\n\nIt was Denton's task to inform Pennsylvania Governor Dick Thornburgh and the President about the discovery of a possibly explosive hydrogen bubble above the cooling water, at the top of the reactor pressure vessel. The debate over whether the bubble would mix with oxygen and set off an explosion, fueled speculation of a meltdown.\n\nAt the time of Carter's arrival Sunday morning on 1 April, whether the bubble would explode was still under debate. Denton informed the President of the risk just as he was preparing to enter the plant. \"...I briefed the President on this bubble and the possibility of an explosive mixture and tried to give him the two sides that were out there, but we still didn't have [a] single view on that,\" Denton recalled.\n\nDenton has won multiple awards for his contribution at Three Mile Island, including the James N. Landis Medal. In an interview with Dick Thornburgh concerning TMI, Thornburg said, \"[Denton] proved to be a genuine hero with respect to this event. He was a much needed source of information for those of us who had the ultimate responsibility for the safety of the people in the area and the quality of the environment.\"\n\nOn 11 July 1959, he married Lucinda Vaden Oliver, and they subsequently had three children.\n\nHAROLD R. DENTON\n\nMr. Denton had over 30 years experience in the U.S. Nuclear Regulatory Commission’s nuclear safety regulation program. He was the Director of the Office of Nuclear Reactor Regulation for almost a decade. In this position, he was responsible for assuring the safety of all commercial nuclear power reactors, test reactors and research reactors, and was the official authorized by the Atomic Energy Act to issue construction permits and operating licenses. He directed a technical staff of over 500 reactor safety and environmental professionals. Later in his career, he directed the Commission's interactions with Congress, State Governments, the media and other countries.\n\nImmediately following the accident at the Three Mile Island nuclear power plant in Harrisburg, Pennsylvania, President Carter appointed Mr. Denton as his personal representative at the TMI site. He subsequently worked closely with Gov. Thornburgh and other public officials. He was the news media spokesman for the federal government during the crisis and directed NRC staff activities to assess and mitigate the consequences. As a nuclear safety expert, he has visited essentially all U.S. power reactors and has traveled extensively to nuclear facilities worldwide. He was in the first group of Americans allowed to visit the Chernobyl site and was instrumental in establishing nuclear safety cooperation with the former USSR.\n\nIn a ceremony in the Rose Garden at the White House, President Carter presented him with the Presidential Distinguished Executive Award for extraordinary accomplishment and leadership.\n\nMr. Denton issued operating licenses to over forty nuclear power reactors during his career. He often testified before Senate and House committees concerning NRC's positions and activities. Later in his career, he directed the Commission's interactions with Congress, State Governments, foreign countries, the Nuclear Energy Agency, the International Atomic Energy Agency and the media.\nHe attended North Carolina State College, earning a Bachelor of Science degree in nuclear engineering and did graduate work at the University of Maryland. Before joining the Atomic Energy Commission (AEC), he was employed as a reactor physicist by the Dupont Company at the AEC’s Savannah River Plant in South Carolina. He was awarded honorary \ndegrees from Gettysburg College, Lebanon Valley College in Pennsylvania and the University of Pennsylvania.\nSince retiring from the NRC, he has provided advice on nuclear safety matters to a variety of national and international clients, including the US Department of Energy and its National Laboratories, the National Science Foundation, private power companies, and to the Governments of Austria, Japan, Taiwan, and Turkey.\n\nHe was awarded the James M. Landis Medal by the American Society of Mechanical Engineers for outstanding personal performance related to designing, constructing, and managing the operation of major steam-powered electric stations using nuclear.\n\nIn December 2011, he participated in a symposium sponsored by the Japanese Society of Mechanical Engineers in Tokyo. This included a visit to several reactor sites damaged by the March 11 earthquake and tsunami where restoration and enhanced safety counter measures are in progress.\n\nDenton died at his home in Knoxville, Tennessee from complications of Alzheimer's disease and chronic obstructive pulmonary disease, aged 80.\n\n\n"}
{"id": "5116788", "url": "https://en.wikipedia.org/wiki?curid=5116788", "title": "Hayashi track", "text": "Hayashi track\n\nThe Hayashi track is a luminosity–temperature relationship obeyed by infant stars of less than in the pre-main-sequence phase (PMS phase) of stellar evolution. It is named after Japanese astrophysicist Chushiro Hayashi. On the Hertzsprung–Russell diagram, which plots luminosity against temperature, the track is a nearly vertical curve. After a protostar ends its phase of rapid contraction and becomes a T Tauri star, it is extremely luminous. The star continues to contract, but much more slowly. While slowly contracting, the star follows the Hayashi track downwards, becoming several times less luminous but staying at roughly the same surface temperature, until either a radiative zone develops, at which point the star starts following the Henyey track, or nuclear fusion begins, marking its entry onto the main sequence.\n\nThe shape and position of the Hayashi track on the Hertzsprung–Russell diagram depends on the star's mass and chemical composition. For solar-mass stars, the track lies at a temperature of roughly 4000 K. Stars on the track are nearly fully convective and have their opacity dominated by hydrogen ions. Stars less than are fully convective even on the main sequence, but their opacity begins to be dominated by Kramers' opacity law after nuclear fusion begins, thus moving them off the Hayashi track. Stars between 0.5 and develop a radiative\nzone prior to reaching the main sequence. Stars between 3 and are fully radiative at the beginning of the pre-main-sequence. Even heavier stars are born onto the main sequence, with no PMS evolution.\n\nAt an end of a low- or intermediate-mass star's life, the star follows an analogue of the Hayashi track, but in reverse—it increases in luminosity, expands, and stays at roughly the same temperature, eventually becoming a red giant.\n\nIn 1961, Professor Chushiro Hayashi published two papers that led to the concept of the pre-main-sequence and form the basis of the modern understanding of early stellar evolution. Hayashi realized that the existing model, in which stars are assumed to be in radiative equilibrium with no substantial convection zone, cannot explain the shape of the red giant branch. He therefore replaced the model by including the effects of thick convection zones on a star's interior.\n\nA few years prior, Osterbrock proposed deep convection zones with efficient convection, analyzing them using the opacity of H- ions (the dominant opacity source in cool atmospheres) in temperatures below 5000K. However, the earliest numerical models of Sun-like stars did not follow up on this work and continued to assume radiative equilibrium.\n\nIn his 1961 papers, Hayashi showed that the convective envelope of a star is determined by:\n\nformula_1\n\nwhere E is unitless, and not the energy. Modelling stars as polytropes with index 3/2—in other words, assuming they follow a pressure-density relationship of formula_2—he found that E=45 is the maximum for a quasistatic star. If a star is not contracting rapidly, E=45 defines a curve on the HR diagram, to the right of which the star cannot exist. He then computed the evolutionary tracks and isochrones (luminosity-temperature distributions of stars at a given age) for a variety of stellar masses and noted that NGC2264, a very young star cluster, fits the isochrones well. In particular, he calculated much lower ages for solar-type stars in NGC2264 and predicted that these stars were rapidly contracting T Tauri stars.\n\nIn 1962, Hayashi published a 183-page review of stellar evolution. Here, he discussed the evolution of stars born in the forbidden region. These stars rapidly contract due to gravity before settling to a quasistatic, fully convective state on the Hayashi tracks.\n\nIn 1965, numerical models by Iben and Ezer & Cameron realistically simulated pre-main-sequence evolution, including the Henyey track that stars follow after leaving the Hayashi track. These standard PMS tracks can still be found in textbooks on stellar evolution.\n\nThe forbidden zone is the region on the HR diagram to the right of the Hayashi track where no star can be in hydrostatic equilibrium, even those that are partially or fully radiative. Newborn protostars start out in this zone, but are not in hydrostatic equilibrium and will rapidly move towards the Hayashi track.\n\nBecause stars emit light via blackbody radiation, the power per unit surface area they emit is given by the Stefan-Boltzmann law:\n\nThe star's luminosity is therefore given by:\n\nFor a given L, a lower temperature implies a larger radius, and vice versa. Thus, the Hayashi track separates the HR diagram into two regions: the allowed region to the left, with high temperatures and smaller radii for each luminosity, and the forbidden region to the right, with lower temperatures and correspondingly higher radii. The Hayashi limit can refer to either the lower bound in temperature or the upper bound on radius defined by the Hayashi track.\n\nThe region to the right is forbidden because it can be shown that a star in the region must have a temperature gradient of:\nwhere formula_6 for a monatomic ideal gas undergoing adiabatic expansion or contraction. A temperature gradient greater than 0.4 is therefore called superadiabatic.\n\nConsider a star with a superadiabatic gradient. Imagine a parcel of gas that starts at radial position r, but moves upwards to r+dr in a sufficiently short time that it exchanges negligible heat with its surroundings—in other words, the process is adiabatic. The pressure of the surroundings, as well as that of the parcel, decreases by some amount dP. The parcel's temperature changes by formula_7. The temperature of the surroundings also decreases, but by some amount dT' that is greater than dT. The parcel therefore ends up being hotter than its surroundings. Since the ideal gas law can be written formula_8, a higher temperature implies a lower density at the same pressure. The parcel is therefore also less dense than its surroundings. This will cause it to rise even more, and the parcel will become even less dense than its new surroundings.\n\nClearly, this situation is not stable. In fact, a superadiabatic gradient causes convection. Convection tends to lower the temperature gradient because the rising parcel of gas will eventually be dispersed, dumping its excess thermal and kinetic energy into its surroundings and heating up said surroundings. In stars, the convection process is known to be highly efficient, with a typical formula_9 that only exceeds the adiabatic gradient by 1 part in 10 million.\n\nIf a star is placed in the forbidden zone, with a temperature gradient much greater than 0.4, it will experience rapid convection that brings the gradient down. Since this convection will drastically change the star's pressure and temperature distribution, the star is not in hydrostatic equilibrium, and will contract until it is.\n\nA star far to the left of the Hayashi track has a temperature gradient smaller than adiabatic. This means that if a parcel of gas rises a tiny bit, it will be more dense than its surroundings and sink back to where it came from. Convection therefore does not occur, and almost all energy output is carried radiatively.\n\nStars form when small regions of a giant molecular cloud collapse under their own gravity, becoming protostars. The collapse releases gravitational energy, which heats up the protostar. This process occurs on the free-fall timescale, which is roughly 100,000 years for solar-mass protostars, and ends when the protostar reaches approximately 4000 K. This is known as the Hayashi boundary, and at this point, the protostar is on the Hayashi track. At this point, they are known as T Tauri stars and continue to contract, but much more slowly. As they contract, they decrease in luminosity because less surface area becomes available for emitting light. The Hayashi track gives the resulting change in temperature, which will be minimal compared to the change in luminosity because the Hayashi track is nearly vertical. In other words, on the HR diagram, a T Tauri star starts out on the Hayashi track with a high luminosity and moves downward along the track as time passes.\n\nThe Hayashi track describes a fully convective star. This is a good approximation for very young pre-main-sequence stars they are still cool and highly opaque, so that radiative transport is insufficient to carry away the generated energy and convection must occur. Stars less massive than remain fully convective, and therefore remain on the Hayashi track, throughout their pre-main-sequence stage, joining the main sequence at the bottom of the Hayashi track. Stars heavier than have higher interior temperatures, which decreases their central opacity and allows radiation to carry away large amounts of energy. This allows a radiative zone to develop around the star's core. The star is then no longer on the Hayashi track, and experiences a period of rapidly increasing temperature at nearly constant luminosity. This is called the Henyey track, and ends when temperatures are high enough to ignite hydrogen fusion in the core. The star is then on the main sequence.\n\nLower-mass stars follow the Hayashi track until the track intersects with the main sequence, at which point hydrogen fusion begins and the star follows the main sequence. Even lower-mass 'stars' never achieve the conditions necessary to fuse hydrogen and become brown dwarfs.\n\nThe exact shape and position of the Hayashi track can only be computed\nnumerically using computer models. Nevertheless, we can make an extremely\ncrude analytical argument that captures most of the track's properties. The\nfollowing derivation loosely follows that of Kippenhahn, Weigert, and Weiss in\n\"Stellar Structure and Evolution\".\n\nIn our\nsimple model, a star is assumed to consist of a fully convective interior\ninside of a fully radiative atmosphere.\n\nThe convective interior is assumed to be an ideal monatomic gas with a perfectly adiabatic temperature gradient:\n\nThis quantity is sometimes labelled formula_11. The following\nadiabatic equation therefore holds true for the entire interior:\n\nwhere formula_13 is the adiabatic gamma, which is 5/3 for an ideal\nmonatomic gas. The ideal gas law says:\n\nwhere formula_17 is the molecular weight per particle and H is (to a very good\napproximation) the mass of a hydrogen atom. This equation represents a\npolytrope of index 1.5, since a polytrope is defined by \nformula_18, where n=1.5 is the polytropic index. Applying \nthe equation to the center of the star gives:\nformula_19\nWe can solve for C:\n\nBut for any polytrope, formula_21,\nformula_22, and \nformula_23. formula_24 and K are all constants independent of pressure and density,\nand the average density is defined as \nformula_25. Plugging all 3 equations\ninto the equation for C, we have:\n\nwhere all multiplicative constants have been ignored. Recall that our original\ndefinition of C was:\n\nWe therefore have, for any star of mass M and radius R:\n\nWe need another relationship between P, T, M, and R, in order to eliminate P.\nThis relationship will come from the atmosphere model.\n\nThe atmosphere is assumed to be thin, with average opacity k. Opacity is\ndefined to be optical depth divided by density. Thus, by definition, the\noptical depth of the stellar surface, also called the photosphere, is:\n\nwhere R is the stellar radius, also known as the position of the photosphere.\nThe pressure at the surface is:\n\nThe optical depth at the photosphere turns out to be formula_35. By\ndefinition, the temperature of the photosphere is formula_36 where effective\ntemperature is given by formula_37. Therefore,\nthe pressure is:\n\nWe can approximate the opacity to be:\n\nwhere a=1, b=3. Plugging this into the pressure equation, we get:\n\nFinally, we need to eliminate R and introduce L, the luminosity. This can be\ndone with the equation:\n\nEquation and can now be combined by\nsetting formula_42 and formula_43 in Equation 1, then eliminating formula_44.\nR can be eliminated using Equation . After some algebra,\nand after setting formula_45, we get:\n\nwhere\n\nIn cool stellar atmospheres (T < 5000 K) like those of newborn stars, \nthe dominant source of opacity is the H- ion, for which \nformula_49 and formula_50, we get \nformula_51 and formula_52.\n\nSince A is much smaller than\n1, the Hayashi track is extremely steep: if the luminosity changes by a factor\nof 2, the temperature only changes by 4 percent. The fact that B is positive\nindicates that the Hayashi track shifts left on the HR diagram, towards higher\ntemperatures, as mass increases. Although this model is extremely crude, these\nqualitative observations are fully supported by numerical simulations.\n\nAt high temperatures, the atmosphere's opacity begins to be dominated by\nKramers' opacity law instead of the H- ion, with a=1 and b=-4.5 In that\ncase, A=0.2 in our crude model, far higher than 0.05, and the star is no longer\non the Hayashi track.\n\nIn \"Stellar Interiors\", Hansen, Kawaler, and Trimble go through a similar\nderivation without neglecting multiplicative constants, \nand arrived at:\n\nwhere formula_17 is the molecular weight per particle. The authors note that the coefficient of 2600K is too\nlow—it should be around 4000K—but this equation nevertheless shows that\ntemperature is nearly independent of luminosity.\n\nThe diagram at the top of this article shows numerically computed stellar \nevolution\ntracks for various masses. The vertical portions of each track is the Hayashi\ntrack. The endpoints of each track lie on the main sequence.\nThe horizontal segments for higher-mass stars show the Henyey track.\n\nIt is approximately true that: \n\nThe diagram to the right shows how Hayashi tracks change with changes in\nchemical composition. Z is the star's metallicity, the mass fraction not\naccounted for by hydrogen or helium. For any given hydrogen mass fraction,\nincreasing Z leads to increasing molecular weight. The dependence of \ntemperature on molecular weight is extremely steep—it is approximately\nDecreasing Z by a factor of 10 shifts the track right, changing \nformula_57 by about 0.05.\n\nChemical composition affects the Hayashi track in a few ways. The\ntrack depends strongly on the atmosphere's opacity, and this opacity is\ndominated by the H- ion. The abundance of the H- ion is proportional to the\ndensity of free electrons, which, in turn, is higher if there are more metals\nbecause metals are easier to ionize than hydrogen or helium.\n\nObservational evidence of the Hayashi track comes from color-magnitude plots—the observational equivalent of HR diagrams—of young star clusters. For\nHayashi, NGC 2264 provided the first evidence of a population of contracting\nstars. In 2012, data from NGC 2264 was re-analyzed to account for dust\nreddening and extinction. The resulting color-magnitude plot is shown at\nright.\n\nIn the upper diagram, the isochrones are curves along which stars of a certain \nage\nare expected to lie, assuming that all stars evolve along the Hayashi track.\nAn isochrone is created by taking stars of every conceivable mass, evolving\nthem forwards to the same age, and plotting all of them on the color-magnitude\ndiagram.\nMost of the stars in NGC 2264 are already on the main sequence (black line), \nbut a substantial population lies between the isochrones for 3.2 million and 5\nmillion years, indicating that the cluster is 3.2-5 million years old and a large population of T Tauri stars is still on their respective Hayashi tracks.\nSimilar results have been obtained for NGC 6530, IC 5146, and NGC 6611.\n\nThe lower diagram shows Hayashi tracks for various masses, along with T Tauri\nobservations collected from a variety of sources. Note the bold curve to\nthe right, representing a stellar birthline. Even though some Hayashi tracks\ntheoretically extend above the birthline, few stars are above it. In effect,\nstars are 'born' onto the birthline before evolving downwards along their\nrespective Hayashi tracks.\n\nThe birthline exists because stars formed from overdense cores of giant molecular\nclouds in an inside-out manner. That is, a small central region first\ncollapses in on itself while the outer shell is still nearly static. The outer\nenvelope then accretes onto the central protostar. Before the accretion is \nover, the protostar is hidden from view, and therefore not plotted on the\ncolor-magnitude diagram. When the envelope finishes accreting, the star is\nrevealed and appears on the birthline.\n"}
{"id": "52626653", "url": "https://en.wikipedia.org/wiki?curid=52626653", "title": "High voltage interface relays", "text": "High voltage interface relays\n\nHigh voltage interface relays, a.k.a., \"interface relays\": or \"coupling relays\" or \"insulating interfaces\" is a special class of electrical relays designed to provide informational and electrical compatibility between functional components isolated from each other and not allowing for a direct connection due to a high difference of potentials. A common design principle of these devices is a special galvanic isolation module between the input (control) and the output (switching) circuits of the relay. Interface relays are widely used in control and protection systems of high voltage (10-100 kV) electronic and electrophysical equipment and in high power installations.\n\nAny electromagnetic relay has a certain level of isolation between the input and output circuits. However, in ordinary relays, this function is not prevalent and, hence, not considered in the existing system of relay classification. In interface relays, however, the property of galvanic isolation (decoupling) between the input and output circuits is significantly bolstered, and parameters of the galvanic isolation have an utmost importance from standpoint of the functions performed by this relay. On the other hand, the parameters associated with switching capacity are secondary and can significantly vary in interface relays with the same level of galvanic decoupling.\n\nIn this respect, categorization of interface relays into existing classes of ordinary relays is arguable. Rather, it seems more appropriate to categorize them as a separate class of electrical relays and classify according to characteristics of the galvanic decoupling unit\n\nby insulation voltage level:\nby construction of galvanic isolation module:\nby operational (execution) speed:\n\nAlthough such classification may seem arbitrary, it fully reflects the most important properties of interface relays that have a critical effect on the functions performed by them.\n\nThe developmental trends of interface relay technology suggest the use of opto-isolator as the prevailing design principle of interface relays. An opto-isolator can be implemented in terms of an LED and a phototransistor (or photothyristor or a photodiode) or a lamp and a photoresistor. An example of an opto-electronic interface relay is shown in the figure. The transparent high voltage insulating barrier provides galvanic isolation of the circuits under the difference of potentials up to 5-7 kV. For higher voltages, they use an optical fiber, the length of which (depending on the voltage level) can take from dozens of centimeters to several meters.\n\nCriticism\n\nIt is agreed that the most important characteristic of opto-electronic systems is their noise robustness and insensitivity to electromagnetic fields. What is not considered, however, is that, in addition to the fiber optic line and the output actuator, such a system includes the source of light pulses on the transmitting side and the amplifier on the receiving side that are generally based on micro-circuitry. It is precisely these elements, with low trigger levels, that get damaged by pulse noise (interference, voltage spikes and discharges) of the high voltage power equipment, which negates the main advantage of opto-electronic systems. Moreover, the optical fibers themselves are subject to a severe negative effect of ionizing radiation and external mechanical impacts (which is critically important in military applications). The arrangement of input and output circuits of such systems needs to be widely spaced (requiring a lengthy optical fiber), which drives up the overall dimensions of interface unit. As such, the preferred use of an opto-electronic galvanic decoupling module in interface relays is not always warranted, and is merely the consequence of a stereotypical thinking of design engineers\n\nA special kind of high voltage (HV) interface relays (which do not fall under the existing classification discussed above) are called \"gerkotrones\" — see the figure on the right. They were designed and developed by Vladimir Gurevich and offer a number of benefits over other types of interface relays. These include: design parsimony; mechanical, environmental and operational robustness; reliability and relatively low cost. Another important advantage of gerkotrones is the possibility of their installation directly on HV buses, which minimizes the dimensions of a protection system (unlike the aforementioned opto-electronic interfaces that require lengthy optical fibers).\n\nThese advantages command gerkotrones' widespread use in commercial and military applications in on-board, mobile and stationary powerful radio-electronic equipment, in relay protection and automation systems of electrical networks, in electrophysical installations, in power converter technology, etc.\n"}
{"id": "23012271", "url": "https://en.wikipedia.org/wiki?curid=23012271", "title": "International Council on Large Electric Systems", "text": "International Council on Large Electric Systems\n\nThe International Council on Large Electric Systems (, \"CIGRÉ\") is a global organization in the field of high voltage electricity. It was founded in Paris, France in 1921. The scope of its activities include the technical and economical aspects of the electrical grid, as well as the environmental and regulatory aspects.\n\nMore specifically, the objectives of CIGRÉ are to:\n\nCIGRÉ membership is open to individuals, companies and organisations involved with any aspect of high voltage engineering. Member organisations and companies are known as collective members.\n\nThe activities of CIGRÉ are divided into sixteen study committees(SC) :\n\nIn addition, CIGRÉ has (as of 2012) 57 \"National Committees\" (NC) which support the Study Committees in identifying experts to participate in working groups. Beginning with the United Kingdom, Netherlands and Italy in 1923, National Committees have developed progressively around the world to give CIGRÉ a global footprint. In 2014 the Turkish National Committee was established.\n\nCIGRÉ organises several types of conference, of which the biennial \"Sessions\", which take place in Paris in even-numbered years, are the most important and broad-ranging. The first CIGRÉ session took place on 21–28 November 1921 at 7 Rue de Madrid, Paris and was attended by 231 high voltage engineers and technicians.\n\nThe 46th CIGRÉ session will be held at the Palais des congrès de Paris on 21–26 August 2016. The CIGRE Session and its Technical Exhibition bring together more than 8500 senior executives, engineers and experts from the worldwide Power Industry. The CIGRE Session provides a unique opportunity to listen to contributions from international senior executives as well as experts and specialists through official presentations, panel discussions, technical meetings and poster sessions. The Session also offers a social program to engage further discussions in a relaxed atmosphere.\n\nIn parallel of the Session, a Technical Exhibition is held in the same location on levels 1, 2 and 3. The exhibition offers the opportunity to all visitors, including CIGRE delegates, to discover new services, tools, equipment and materials as well as the most advanced technologies in the field of power systems.\n\nIn addition to the biennial sessions, CIGRÉ organises several other types of conference in locations other than Paris, including: \n\nThe Study Committees of CIGRÉ appoint \"Working Groups\" of internationally recognised experts to investigate and publish the state of the art in their chosen field. The output of Working Groups is in the form of \"Technical Brochures\". Technical Brochures are frequently used to inform and act as precursor documents for the activities of national and international Standards organizations, notably the International Electrotechnical Commission (IEC).\n\nCIGRÉ publishes a bimonthly magazine called \"Electra\" which contains executive summaries of recently published Technical Brochures, as well as selected scientific papers and invited papers.\n\nThe Technical Committee of CIGRE recently updated the CIGRE White Paper “Network of the Future” issued in 2011 and published in Electra (N°256). This summary paper provides CIGRE’s views on the know-how needed to manage the transition towards future energy supply systems].\n\n"}
{"id": "53651947", "url": "https://en.wikipedia.org/wiki?curid=53651947", "title": "Ion funnel", "text": "Ion funnel\n\nIn mass spectrometry, an ion funnel is a device used to focus a beam of ions using a series of stacked ring electrodes with decreasing inner diameter. A combined radio frequency and fixed electrical potential is applied to the grids.\n\nProton transfer reaction mass spectrometry has traditionally used drift tubes as ion traps. However, radio frequency ion funnels offer an attractive alternative, as they improve compound specific sensitivity significantly. This is due to increasing the effective reaction time and focusing the ions. The same pressure ranges are required for ion funnels and drift tubes, so the technology is not difficult to implement. Ion funnels have been shown to favor transmission of ions with high m/z.\n\nBreath analysis is a convenient and non-invasive way to determine whether a person is intoxicated, monitor the levels of anesthetics in the body during surgical procedures, identify performance-enhancing substances in the system of athletes, etc. However, conventional techniques are ineffective at low concentrations. An electrospray ionization interface assisted by an ion funnel used in a linear trap quadrupole Fourier-transform ion cyclotron resonance mass spectrometer was shown to greatly increase sensitivity with high resolution.\n\n"}
{"id": "674796", "url": "https://en.wikipedia.org/wiki?curid=674796", "title": "JFE Holdings", "text": "JFE Holdings\n\nAt the time JFE Holdings was created in 2002, NKK Corporation was Japan's second largest steelmaker and Kawasaki Steel was the third largest steelmaker.\nBoth companies were major military vessel manufacturers during World War II.\n\nJFE's main business is steel production. It also engages in engineering, ship building, real-estate redevelopment, and LSi business. The company also operates several overseas subsidiaries, including California Steel Industries in the United States, Fujian Sino-Japan Metal in China, and Minas da Serra Geral in Brazil. Other than steel, they are also known for products such as the bicycle tree.\n\nJFE Holdings owns JFE Steel, the fifth largest steel maker in the world with revenue in excess of US$30 billion. JFE Holdings has other subsidiaries including JFE Engineering, JFE Steel and JFE Shoji , and part-owns Japan Marine United, a major shipbuilding company. \n\nNKK and Siderca S.A. of Argentina established a seamless pipe joint venture by spinning off the seamless pipe division of NKK's Keihin Works in 2000. In November 2009, JFE agreed to partner with JSW Steel, India's third-largest steel producer, to construct a joint steel plant in West Bengal.\n\nIts shipbuilding unit, Universal Shipbuilding was created in 2002 when NKK Corporation a predecessor of JFE, merged its shipbuilding unit with that of Hitachi Zosen. In 2012, JFE merged its ship building unit, Universal Shipbuilding Corporation, with Marine United Inc. of IHI after discussion started in April 2008 to form Japan Marine United Corporation It aims to become Japan’s largest shipbuilder.\n\nJFE Engineering Corporation is developing a quick charge system that it claims can take a battery from zero charge to 50% full in about 3 minutes. It has two batteries, one that stores electrical energy from the grid and another that delivers it to the car at extremely high current (500-600 amps), which allows it to use a low-voltage power supply. The company claims that even though one station costs about $63,000, that’s roughly 40% less than the competing CHAdeMO system.\n\nThe bicycle tree is an automatic storage system for bicycles that can hold up to 6,000 bikes. The systems works by fitting the bicycle with an electronic tag and a computer saves the owner's data. Then a mechanical arm pulls the bike into a cylindrical well and stores it in a free location. When the owner wants to retrieve the bike, a card is swiped through a reader and the computer retrieves the bike based on the data.\n\n"}
{"id": "41090920", "url": "https://en.wikipedia.org/wiki?curid=41090920", "title": "Korea Invisible Mass Search", "text": "Korea Invisible Mass Search\n\nThe Korea Invisible Mass Search (KIMS Collaboration), is a South Korean experiment, led by Sun Kee Kim, searching for weakly interacting massive particles (WIMPs), one of the candidates for dark matter. The experiments use CsI(Tl) crystals at Yangyang Underground Laboratory (Y2L), in tunnels from a preexisting underground power plant. KIMS is supported by the Creative Research Initiative program of the Korea Science and Engineering Foundation. It is the first physics experiment located, and largely built, in Korea.\n\nOther research topics include detector development for a neutrinoless double beta decay search and the creation of an extreme low temperature diamond calorimeter.\n\nThe KIMS experiment was funded in 2000 to search for WIMP dark matter. To avoid the cost of creating a new underground tunnel for testing, the Yangyang Pumped Storage Power Plant belonging to Korea Middleland Power Co. in Yangyang, Korea was used. Construction was completed in 2003. The CsI(Tl) scintillating crystal used has a high light yield is affordable for large mass. After a substantial effort for the initial setup and crystal development, KIMS began recording data in 2004 with one full-size 6 kg crystal. A 4 crystal setup was run in 2005-2006 to optimize the WIMP search. In 2008, the 12 crystal array with mass 103.4 kg was completed and ran until December 2012 for a detector upgrade replacing the PMTs.\n\nThe first WIMP cross section search was published in 2006 using the one crystal data. New limits were presented in 2007 and 2012, inconsistent with the DAMA signal reports for masses above 20 GeV. Using 24324.3 kg days exposure, low-mass WIMP signals below 20 GeV were disfavored in 2014.\n\nThe KIMS and DM-Ice groups have joined forces to make a new detector consisting of an array of NaI(Tl) scintillating crystals to confirm or refute the DAMA/LIBRA results. , the 100 kg COSINE-100 experiment had been installed at Y2L. In September 2016, physics data started to be collected. The next version of the COSINE detector, COSINE-200, will be constructed in Yemi Laboratory in Jeongseon County.\n\n"}
{"id": "5318004", "url": "https://en.wikipedia.org/wiki?curid=5318004", "title": "Kunstformen der Natur", "text": "Kunstformen der Natur\n\nKunstformen der Natur (known in English as Art Forms in Nature) is a book of lithographic and halftone prints by German biologist Ernst Haeckel.\n\nOriginally published in sets of ten between 1899 and 1904 and collectively in two volumes in 1904, it consists of 100 prints of various organisms, many of which were first described by Haeckel himself. Over the course of his career, over 1000 engravings were produced based on Haeckel's sketches and watercolors; many of the best of these were chosen for \"Kunstformen der Natur\", translated from sketch to print by lithographer Adolf Giltsch.\n\nA second edition of \"Kunstformen\", containing only 30 prints, was produced in 1924.\n\nAccording to Haeckel scholar Olaf Breidbach, the work was \"not just a book of illustrations but also the summation of his view of the world.\" The over-riding themes of the \"Kunstformen\" plates are symmetry and level of organization. The subjects were selected to embody these to the full, from the scale patterns of boxfishes to the spirals of ammonites to the perfect symmetries of jellies and microorganisms, while images composing each plate are arranged for maximum visual impact.\n\nAmong the notable prints are numerous radiolarians, which Haeckel helped to popularize among amateur microscopists; at least one example is found in almost every set of 10. Cnidaria also feature prominently throughout the book, including sea anemones as well as Siphonophorae, Semaeostomeae, and other medusae. The first set included \"Desmonema annasethe\" (now \"Cyanea annasethe\"), a particularly striking jellyfish that Haeckel observed and described shortly after the death of his wife Anna Sethe.\n\n\"Kunstformen der Natur\" was influential in early 20th-century art, architecture, and design, bridging the gap between science and art. In particular, many artists associated with Art Nouveau were influenced by Haeckel's images, including René Binet, Karl Blossfeldt, Hans Christiansen, and Émile Gallé. One prominent example is the Amsterdam Commodities Exchange designed by Hendrik Petrus Berlage: it was in part inspired by \"Kunstformen\" illustrations.\n\nHaeckel's original classifications appear in \"italics\".\n\n\n"}
{"id": "3825096", "url": "https://en.wikipedia.org/wiki?curid=3825096", "title": "Lamri Ali", "text": "Lamri Ali\n\nDatuk Lamri Ali is a former Director of Sabah Parks. At the December 1999 World Conservation Union (IUCN)'s regional meeting staged in Pakse, Laos, Ali was awarded the Fred M. Packard WCPA-IUCN award. The award recognises his contribution to nature conservation and protected area movement in Malaysia. Datuk Lamri is the only Malaysian recipient of this award.\n\n\"Nepenthes × alisaputrana\", a hybrid of two well-known Bornean pitcher plant species, is named in his honour.\n"}
{"id": "15428528", "url": "https://en.wikipedia.org/wiki?curid=15428528", "title": "List of inquiries into uranium mining in Australia", "text": "List of inquiries into uranium mining in Australia\n\nThis is a List of Australian inquiries and reports relating to uranium mining issues.\n\nFor several decades uranium mining has been a major part of the Australian political landscape, with opposition groups citing the wide ranging environmental impacts, indigenous land access and nuclear proliferation as reasons for ceasing or restricting the industry. The debate has resulted in limitations on mining and export activities, with Federal and State governments occasionally flip-flopping on public policy. In the meantime, mining companies have pursued exploration activities, and in some instances stockpiled mined ore.\n\n\n"}
{"id": "7737918", "url": "https://en.wikipedia.org/wiki?curid=7737918", "title": "Load bank", "text": "Load bank\n\nA load bank is a device which develops an electrical load, applies the load to an electrical power source and converts or dissipates the resultant power output of the source.\nA load bank includes load elements with protection, control, metering and accessory devices required for operation. Load banks can either be permanently installed at a facility and permanently connected to a power source or portable versions can be used for testing power sources such as standby generators and batteries. Load banks are the best way to replicate, prove and verify the real-life demands on critical power systems. . \n\nLoad banks are used in a variety of applications, including:\n\n\nThe three most common types of load banks are resistive, inductive, and capacitive. Both inductive and capacitive loads create what is known as reactance in an AC circuit. Reactance is a circuit element's opposition to an alternating current, caused by the buildup of electric or magnetic fields in the element due to the current and is the \"imaginary\" component of impedance, or the resistance to AC signals at a certain frequency. Capacitive reactance is equal to 1/(2⋅π⋅f⋅C), and inductive reactance is equal to 2⋅π⋅f⋅L. The unit of reactance is the ohm. Inductive reactance resists the change to current, causing the circuit current to lag voltage. Capacitive reactance resists the change to voltage, causing the circuit current to lead voltage. \n\nA resistive load bank, the most common type, provides equivalent loading for both generators and prime movers. That is, for each kilowatt (or horsepower) of load applied to the generator by the load bank, an equal amount of load is applied to the prime mover by the generator. A resistive load bank, therefore, removes energy from the complete system: load bank from generator—generator from prime mover—prime mover from fuel. Additional energy is removed as a consequence of resistive load bank operation: waste heat from coolant, exhaust and generator losses and energy consumed by accessory devices. A resistive load bank impacts upon all aspects of a generating system.\n\nThe load of a resistive load bank is created by the conversion of electrical energy to heat via high-power resistors such as grid resistors. This heat must be dissipated from the load bank, either by air or by water, by forced means or convection.\n\nIn a testing system, a resistive load simulates real-life resistive loads, such as incandescent lighting and heating loads as well as the resistive or unity power factor component of magnetic (motors, transformers) loads.\n\nThe most common type uses wire resistance, usually with fan cooling, and this type is often portable and moved from generator to generator for test purposes. Sometimes a load of this type is built into a building, but this is unusual.\n\nRarely a salt water rheostat is used. It can be readily improvised, which makes it useful in remote locations.\n\nFor testing automotive batteries, a carbon pile load bank allows an adjustable load to be placed on the battery or charging system, allowing accurate simulation of the heavy load on the battery during cranking of the engine. Such devices are usually portable and may include metering to show voltage and current. \n\nAn inductive load includes inductive (lagging power factor) loads.\n\nAn inductive load consists of an iron-core reactive element which, when used in conjunction with a resistive load bank, creates a lagging power factor load. Typically, the inductive load will be rated at a numeric value 75% that of the corresponding resistive load such that when applied together a resultant 0.8 power factor load is provided. That is to say, for each 100 kW of resistive load, 75 kVAr of inductive load is provided. Other ratios are possible to obtain other power factor ratings. An inductive load is used to simulate a real-life mixed commercial loads consisting of lighting, heating, motors, transformers, etc. With a resistive-inductive load bank, full power system testing is possible, because the provided impedance supplies currents out of phase with voltage and allows for performance evaluation of generators, voltage regulators, load tap changers, conductors, switchgear and other equipment.\nA capacitive load bank or capacitor bank is similar to an inductive load bank in rating and purpose, except leading power factor loads are created, so reactive power is supplied from these loads to the system, hence improves the power factor. These loads simulate certain electronic or non-linear loads typical of telecommunications, computer or UPS industries.\n\nA combined load bank usually consists of both resistive elements and inductors that can be used to provide load testing at non-unity PF (lagging) including the capability to test the generator set fully at 100% nameplate kVA rating. Combined load banks incorporate resistors and inductors all in a single construction which can be independently switched to allow resistive only, inductive only, or varying lagging power factor testing. Combined load banks are rated in kilovolt-amperes (kVA). It’s worth noting that combined load banks can consist of resistive, inductive, and capacitive (RLC) also.\n\nTypically, facilities require motor-driven devices, transformers and capacitors. If this is the case, then the load banks used for testing require reactive power compensation. The ideal solution is a combination of both resistive and reactive elements in one load bank package. \n\nResistive/reactive loads are able to mimic motor loads and electromagnetic devices within a power system, as well as provide purely resistive loads.\n\nMany backup generators and turbines need to be commissioned at nameplate capacity using a combination of resistive and reactive load to fully qualify their operating capability. Using a resistive/reactive load bank enables comprehensive testing from a single unit. A range of resistive/reactive load banks are available to simulate these types of loads on a power source and the transformers, relays and switches which will distribute the power throughout the facility.\n\nResistive/reactive load banks are great choices for testing turbines, switchgear, rotary UPS, generators and UPS systems. They can also be used for integrated system testing of utility substation protection systems, particularly for more complex relays like distance, directional overcurrent, power directional and others. A resistive/reactive inductive and/or capacitive load is often required to test solar inverters to ensure solar panels can be stopped from producing electricity in the event of a power outage. The resistive/reactive combination load banks are used to test the engine generator set at its rated power factor. In most cases this is 0.8 power factor.\n\nAn electronic load bank tends to be a fully programmable, air- or water-cooled design used to simulate a solid state load and to provide constant power and current loading on circuits for precision testing.\n\nWhere a diesel-electric locomotive is equipped for dynamic braking, the braking resistor may be used as a load bank for testing the engine-generator set.\n\n"}
{"id": "19989867", "url": "https://en.wikipedia.org/wiki?curid=19989867", "title": "Meghri Dam", "text": "Meghri Dam\n\nThe Meghri Dam (also known as the Aras Dam or Aras Watershed Dam) is a hydroelectric dam currently under construction on the Aras River near Armenia's southern town of Meghri on the Armenia–Iran border.\n\nThe joint Iranian–Armenia project was proposed in the 1990s and was discussed between Iranian and Armenian authorities. Intergovernmental agreements between Armenia and Iran were signed in 2007 and 2008. On 14 April 2009, energy ministries of Armenia and Iran signed a memorandum on financing construction of the Meghri power station of Armenian side. The dam and power stations construction contract was signed on 16 October 2010 with Farab Sepasad Company. A ceremony on 17 November 2012, attended by Iran's Minister of Energy and the Armenian President, officially began construction.\n\nThe basic studies were carried out by Arm-Hydroenergo-Project Company and Mahab-E-Qods Consulting Engineers Company in 1999. The Meghri station will be operated by Iran for 15 years after which it will be returned to Armenia free of charge.\n\nThe project is expected to cost US$400 million.\n\nThe complex will consists of two hydroelectric power stations — Ghare Chiler (also transcribed Gharachilar or Karachinar) on the Iranian side and Meghri on Armenia side. Due to the sufficient difference in elevation and the presence of Aras Dam on the upstream, the power stations will be run-of-river type. Both power stations will have two turbines with a capacity of each.\n\n"}
{"id": "30472532", "url": "https://en.wikipedia.org/wiki?curid=30472532", "title": "Moringa stenopetala", "text": "Moringa stenopetala\n\nMoringa stenopetala, commonly called the cabbage-tree (along with a number of other species), is a tree in the flowering plant genus \"Moringa\", native to Kenya and Ethiopia. It is now extirpated in the wild in Ethiopia, though still grown there as a crop on the terraces built to conserve water high up the mountains. It is a multipurpose tree producing edible leaves, seeds used for the purification of water, and traditional medicinal products.\n\nThe cabbage-tree was planted by agriculturalists on the complex system of terraces built high up in the Ethiopian Highlands, where they became domesticated and were bred to improve productivity, the taste of their leaves, and the size of their seeds. Since then, the improved trees have been introduced into other areas such as the Rift Valley.\n\nThe cabbage-tree is a small tree up to , with a many-branched crown and sometimes with multiple trunks. The leaves are bipinnate or tripinnate, with about five pairs of pinnae and three to nine elliptic or ovate leaflets on each pinna. The fragrant flowers have creamy-pink sepals, white or yellow petals, and white stamens. The fruits are long reddish pods with a greyish bloom.\n\n\"Moringa stenopetala\" is mostly known for its importance as a nutritious vegetable food crop in the terraced fields of Konso, Ethiopia. In this way, it is similar to its Indian relative, \"Moringa oleifera\". It is also used for shading of \"Capsicum\" and \"Sorghum\" crops, as a companion plant; and additionally in folk medicine.\n\nAnother use is the clarification and purification of water to make it potable. A powder made by grinding the seeds is found to be more effective at coagulating substances in suspension than the seeds of the closely related horseradish tree (\"M. oleifera\"), which is used for this purpose in India.\n"}
{"id": "11393612", "url": "https://en.wikipedia.org/wiki?curid=11393612", "title": "Morské oko (Slovakia)", "text": "Morské oko (Slovakia)\n\nMorské oko (called \"Veľké Vihorlatské jazero\" in the past; literally Sea Eye) is a lake in the Vihorlat Mountains in east Slovakia. It is at 618 m, covers 0.13 km² with a maximum depth of 25.1 m. It is a national nature reserve (covering 1.08 km²) since 1984 and part of the Vihorlat Protected Landscape Area. Among the many fish are trout.\n\n\n"}
{"id": "29655906", "url": "https://en.wikipedia.org/wiki?curid=29655906", "title": "National Hydrocarbons Commission", "text": "National Hydrocarbons Commission\n\nThe National Hydrocarbons Commission (CNH) is an agency of the Federal Executive, with legal personality, budgetary autonomy and self-sufficiency technique whose operates in strict compliance with maximum transparency and accountability principles. These principles are established in the Constitution, the Coordinated Energy Regulatory Bodies Law, as well as its internal regulations, setting a new national and international benchmark.\n\nThe CNH was founded as a Federal Executive Agency, with full legal capacities, technical autonomy and budgetary self-sufficiency. CNH´s highest authority is the Board of Commissioners, composed of seven members. For each of their appointments, the President nominates three candidates and the Senate designates one of them. Currently, this board is made up of Juan Carlos Zepeda, President Commissioner and Commissioners Alma America Porres Moon, Nestor Martinez Romero, Hector Felix Alberto Acosta, Sergio Pimentel Vargas, Hector Moreira Rodriguez and Franco Gaspar\n\n\nOn April 8, 2008 the Executive presented a series of reforms to Article 27 of the Constitution of Mexico and to secondary laws regulation the oil sector in Mexico.\n\nAmong the proposals to Congress the creation of a National Hydrocarbons Commission which shall endeavor to found \"that the exploration and extraction are carried maximizing oil rents in the extraction of crude oil and natural gas.\" The amendment was approved in October 2008, including as regards the Commission, which is formally established with the publication in the Official Journal of the Federation of Law of the National Hydrocarbons Commission on November 28, 2008\n\nCNH was formally installed on May 20, 2009 when Felipe Calderon, President of Mexico, appointed by the five members of the commissioners who formed its governing. This first Board of Commissioners were Juan Carlos Zepeda Molina, Edgar Rene Rangel Germán, Javier Humberto Estrada Estrada, Guillermo Cruz Dominguez Vargas and Eduardo Alfredo Guzman. The first was appointed as commissioner in May presidente.\n\nThe Reform Proposals of the government of Enrique Peña Nieto on energy carried out under the so-called Pact for Mexico, whose goals include reforming the functions of the National Hydrocarbons Commission to give greater strength and give authority to sign contracts and tenders in energy. Also given powers of supervision and promotion of activities petroleras. In early April 2014 the drafting of the amendments to articles 25, 27 and 28 of the Constitution and the Law on the National Commission of Hydrocarbons you were finished and It is waiting to be approved by both Cámaras.\n\nOn April 29, 2014 Juan Carlos Zepeda Molina was reelected as president of the Commission for a period of 5 years.\n\nThe current composition of Board of Commissioners is:\n\n\n"}
{"id": "38169758", "url": "https://en.wikipedia.org/wiki?curid=38169758", "title": "Neptunium(VI) fluoride", "text": "Neptunium(VI) fluoride\n\nNeptunium hexafluoride (NpF) is the highest fluoride of neptunium, it is also one of seventeen known binary hexafluorides. It is an orange volatile crystalline solid. It is relatively hard to handle, being very corrosive, volatile and radioactive. \n\nAt normal pressure , it melts at 54.4 ° C and boils at 55.18 ° C. It is the only neptunium compound that is easily converted into the gas phase. Due to these properties, it is possible to separate neptunium from spent fuel . This quickly increased the interest in his presentation and in the exact examination of its properties.\n\nIt is prepared by fluorination of neptunium(IV) fluoride (NpF) by powerful fluorinating agents such as elemental fluorine.\n\nIt can also be obtained by fluorination of neptunium(III) fluoride.\n"}
{"id": "43460990", "url": "https://en.wikipedia.org/wiki?curid=43460990", "title": "Nigeria Renewable Energy Master Plan", "text": "Nigeria Renewable Energy Master Plan\n\nThe Nigeria Renewable Energy Master Plan (REMP) is a policy being implemented by Nigeria's Federal Ministry of Environment that aims to increase the contribution of renewable energy to account for 10% of Nigerian total energy consumption by 2025. The Renewable Energy Masterplan for Nigeria was produced in 2006 with support\nfrom the UNDP.\n\nThe Renewable Energy Master Plan (REMP) articulates Nigeria’s vision and sets out a road map for increasing the role of renewable energy in achieving sustainable development. The policy primarily addresses Nigeria's need for increased electricity supply, improved grid reliability and security.\n\nTargets for Renewable Energy Contribution to Electricity Generation (MW) in Nigeria\n\n"}
{"id": "411683", "url": "https://en.wikipedia.org/wiki?curid=411683", "title": "Polyamide", "text": "Polyamide\n\nA polyamide is a macromolecule with repeating units linked by amide bonds.\n\nPolyamides occur both naturally and artificially. Examples of naturally occurring polyamides are proteins, such as wool and silk. Artificially made polyamides can be made through step-growth polymerization or solid-phase synthesis yielding materials such as nylons, aramids, and sodium poly(aspartate). Synthetic polyamides are commonly used in textiles, automotive applications, carpets and sportswear due to their high durability and strength. The transportation manufacturing industry is the major consumer, accounting for 35% of polyamide (PA) consumption.\n\nPolymers of amino acids are known as polypeptides or proteins.\n\nAccording to the composition of their main chain, synthetic polyamides are classified as follows:\n\nAll polyamides are made by the formation of an amide function to link two molecules of monomer together. The monomers can be amides themselves (usually in the form of a cyclic lactam such as caprolactam), α,ω-amino acids or a stoichiometric mixture of a diamine and a diacid. Both these kinds of precursors give a homopolymer. Polyamides are easily copolymerized, and thus many mixtures of monomers are possible which can in turn lead to many copolymers. Additionally many nylon polymers are miscible with one another allowing the creation of blends.\n\nProduction of polymers requires the repeated joining of two groups to form an amide linkage. In this case this specifically involves amide bonds, and the two groups involved are an amine group, and a terminal carbonyl component of a functional group. These react to produce a carbon-nitrogen bond, creating a singular amide linkage. This process involves the elimination of other atoms previously part of the functional groups. The carbonyl-component may be part of either a carboxylic acid group or the more reactive acyl halide derivative. The amine group and the carboxylic acid group can be on the same monomer, or the polymer can be constituted of two different bifunctional monomers, one with two amine groups, the other with two carboxylic acid or acid chloride groups.\n\nThe condensation reaction is used to synthetically produce nylon polymers in industry. Nylons must specifically include a straight chain (aliphatic) monomer. The amide link is produced from an amine group (alternatively known as an amino group), and a carboxylic acid group. The hydroxyl from the carboxylic acid combines with a hydrogen from the amine, and gives rise to water, the elimination byproduct that is the namesake of the reaction. \n\nAs an example of condensation reactions, consider that in living organisms, Amino acids are condensed with one another by an enzyme to form amide linkages (known as peptides). The resulting polyamides are known as proteins or polypeptides. In the diagram below, consider the amino-acids as single aliphatic monomers reacting with identical molecules to form a polyamide, focusing on solely the amine and acid groups. Ignore the substituent R groups – under the assumption the difference between the R groups are negligible:\n\nFor fully aromatic polyamides or 'aramids' e.g. Kevlar, the more reactive acyl chloride is used as a monomer. The polymerization reaction with the amine group eliminates hydrogen chloride. The acid chloride route can be used as a laboratory synthesis to avoid heating and obtain an almost instantaneous reaction. The aromatic moiety itself does not participate in elimination reaction, but it does increase the rigidity and strength of the resulting material which leads to Kevlar's renowned strength.\n\nIn the diagram below, Aramid is made from two different monomers which continuously alternate to form the polymer. Aramid is an aromatic polyamide:\n\nPolyamides can also be synthesized from dinitriles using acid catalysis via an application of the Ritter reaction. This method is applicable for preparation of nylon 1,6 from adiponitrile, formaldehyde and water. Additionally, polyamides can be synthesized from glycols and dinitriles using this method as well.\n\n\n"}
{"id": "30463820", "url": "https://en.wikipedia.org/wiki?curid=30463820", "title": "REK Bitola", "text": "REK Bitola\n\nMining and Energy Combine Bitola (), abbr. MEC Bitola (\"РЕК Битола\", REK Bitola) is an energy company in the city of Bitola, Republic of Macedonia. Its main facility is located in the Novaci Municipality bordering Bitola.\n\nMEC Bitola operates both:\n\nThe power plant is the primary source of electrical power in Macedonia, providing for 70% of the country's needs. The plant, which was built from 1982 to 1988, generates around 4.34 million megawatt hours of electricity per year. It is regulated by the Balkan Environmental Regulatory Compliance and Enforcement Network (BERCEN).\n\n"}
{"id": "390864", "url": "https://en.wikipedia.org/wiki?curid=390864", "title": "Rhinogradentia", "text": "Rhinogradentia\n\nRhinogradentia is a fictitious order of mammal invented by German zoologist Gerolf Steiner. Members of the order, known as rhinogrades or snouters, are characterized by a nose-like feature called a \"nasorium\", which evolved to fulfill a wide variety of functions in different species. Steiner also created a fictional persona, naturalist Harald Stümpke, who is credited as author of the 1957 book \"Bau und Leben der Rhinogradentia\" (translated into English in 1967 as \"The Snouters: Form and Life of the Rhinogrades\"). According to Steiner, it is the only remaining record of the animals, which were wiped out, along with all the world's Rhinogradentia researchers, when the small Pacific archipelago they inhabited sank into the ocean due to nearby atomic bomb testing.\n\nSuccessfully mimicking a genuine scientific work, Rhinogradentia has appeared in several publications without any note of its fictitious nature, sometimes in connection with April Fools' Day.\n\nRhinogradentia, their island home of Hy-yi-yi, zoologist Harald Stümpke, and a host of other people, places, and documents are fictional creations of Gerolf Steiner (1908–2009), a German zoologist. Steiner is best known for his fictional work as Stümpke, but he was an accomplished zoologist in his own right. He held a professorship at the University of Heidelberg and later the Technical University of Karlsruhe, where he occupied the department chair from 1962 to 1973.\n\nAccording to Bud Webster, Steiner's motivation for the work was instructional, to illustrate \"how animals evolve in isolation\".\nSteiner's fictional author, credited as \"quondam curator of the Museum of the Darwin Institute of Hy-yi-yi, Mairuwili,\" provides a very detailed account of the order and individual species, written in a dry, scholarly tone. The evidently expert voice of the author, his competent writing, and apparent familiarity with conventions of academic literature set the work apart as a rare example at the intersection of fiction and scholarship. Steiner credits himself by name as illustrator of the book, and explains how that role led him to possess the only remaining record of Rhinogradentia.\n\nAccording to Stümpke, Rhinogradentia were native to Hy-yi-yi, a small Pacific archipelago comprising eighteen islands: Annoorussawubbissy, Awkoavussa, Hiddudify, Koavussa, Lowlukha, Lownunnoia, Mara, Miroovilly, Mittuddinna, Naty, Nawissy, Noorubbissy, Osovitissy, Ownavussa, Owsuddowsa, Shanelukha, Towteng-Awko, and Vinsy. The islands occupied and the archipelago's highest peak, , was on its main island, Hiddudify (Hy-dud-dye-fee).\n\nThe first description of Hy-yi-yi published in Europe was that of Einar Pettersson-Skämtkvist, a Swedish explorer who arrived in Hiddudify by chance in 1941, after escaping from a Japanese prisoner of war camp. Each of the islands was home to distinctive fauna, dominated by Rhinogradentia, the only mammals other than humans. In the time after the war, a number of scientists took interest in the rhinogrades and began formal research into their physiology, morphology, behaviors, and evolution.\n\nIn the late 1950s, nearby nuclear weapons testing by the United States military accidentally caused all of the islands of Hy-yi-yi to sink into the ocean, destroying all traces of the rhinogrades and their unique ecosystem. Also killed were all the world's Rhinogradentia researchers, who were attending a conference on Hy-yi-yi at the time. The book's epilogue, credited to Steiner in his capacity as the book's illustrator, explains that Stümpke had sent the book's materials to Steiner to serve as the basis for illustrations in preparation for publication. Following the disaster, it is the only remaining record of the subjects it describes.\n\nRhinogrades are mammals characterized by a nose-like feature called a \"nasorium\", the form and function of which vary significantly between species. According to Stümpke, the order's remarkable variety was the natural outcome of evolution acting over millions of years in the remote Hy-yi-yi islands. All the 14 families and 189 known snouter species descended from a small shrew-like animal, which gradually evolved and diversified to fill most of the ecological niches in the archipelago — from tiny worm-like beings to large herbivores and predators.\n\nMany rhinogrades used their nose for locomotion, for example the \"snout leapers\" like \"Hopsorrhinus aureus\", whose nasorium was used for jumping, or the \"earwings\" like \"Otopteryx\", which flew backwards by flapping its ears and used its nose as a rudder. Some species used their nasorium for catching food, for example by using it to fish or to attract and trap insects. Other species included the fierce \"Tyrannonasus imperator\" and the shaggy \"Mammontops\".\n\nPettersson-Skämtkvist's early descriptions of the animals he encountered on Hy-yi-yi led zoologists to name them after the title creature in a short nonsense poem by Christian Morgenstern, \"The Nasobame\" (\"Das Nasobēm\"). In the poem, which exists outside of this fictional universe and also served as an inspiration for Steiner, the Nasobame is seen \"striding on its noses\" (\"auf seinen Nasen schreitet\").\n\nStümpke's book classifies 138 species of rhinograde in the following genera:\nSteiner's books as Stümpke have been translated into other languages, sometimes crediting other names based on the country of publication. Harald Stümpke, Massimo Pandolfi, Hararuto Shutyunpuke, and Karl D. S. Geeste are pseudonyms, while translator names are authentic.\n\nRhinogradentia is considered one of the best known biological hoaxes and scientific jokes and Steiner's pseudonymous works on the subject continue to be reprinted and translated.\n\nSince the book's original publication several scientists and publishers have written about Rhinogradentia as though Steiner's account were true, though it is unclear how many of those who continued and popularized the joke did so intentionally. Wulf Ankle wrote that the order \"is not a poetic invention, but has really lived\". Erich von Holst celebrated the discovery of \"a completely new animal world\". Timothy E. Lawlor's widely read textbook \"Handbook to the Orders and Families of Living Mammals\", includes an entry for Rhinogradentia that does not acknowledge its fictional nature.\nPrior to the publication of Leigh Chadwick's English translation, an abbreviated version ran in the April 1967 edition of \"Natural History\", a magazine published by the American Museum of Natural History. It comprised material from the book's introduction, first chapter, selected descriptions of genera, and the epilogue, and was presented as the lead story, without qualification, by the normally serious publication. The following month, \"The New York Times\" ran a story about the snouters on the front page, based on the \"Natural History\" article. According to the magazine's editorial director, they had \"received more than 100 letters and telegraphs about the snouters, most of them from people who forgot that the article was published on April Fool's Day.\" \"Natural History\" printed several letters to the editor in its June–July issue, and conveyed to the \"Times\" the content of several more, ranging from skeptical to fascinated and continuations of the joke. One reader, entomologist Alice Gray, expressed thanks for the article, which enabled her family to identify an animal-shaped metal bracelet from the South Pacific as having been modeled after a \"Hoop Snouter\", and included a drawing to preserve the record because, she said, it had been melted down with some toy soldiers and a spoon by a young cousin with a new casting set.\n\nDecades later, papers are still published purporting to continue Stümpke's research or otherwise paying homage to Steiner's hoax. In a 2004 paper in the \"Russian Journal of Marine Biology\", authors Kashkina & Bukashkina claim to have discovered two new marine genera: \"Dendronasus\" and an as yet unnamed parasitic taxon. On April Fools' Day in 2012, the National Museum of Natural History in France announced the discovery of a wood-eating termite-like genera, \"Nasoperferator\", with a rotating nose resembling a drill.\n\nRhinogradentia has been included in a number of museum exhibitions and collections. The National Museum of Natural History's \"Nasoperferator\" announcement was accompanied by a two-month exhibit honoring the animals, featuring purported stuffed specimens in its gallery of extinct species. Mock taxidermies of rhinogrades have also been included in an exhibit at the Musée d'ethnographie de Neuchâtel, and in the permanent collections of the Musée zoologique de la ville de Strasbourg and the Salzburg .\n\nTwo real species have been named after Steiner and Stümpke: \"Rhinogradentia steineri\", a snout moth, and \"Hyorhinomys stuempkei\", a shrew rat also known as the Sulawesi snouter, while the specific epithet of \"Tateomys rhinogradoides\" pays homage to the fictitious order.\n\nIn popular culture, Steiner's work influenced or inspired works of art. Japanese noise musician Merzbow, for example, gave the name Rhinogradentia to both a song and an album in the \"Merzbox\" box set.\n\n\n"}
{"id": "1460229", "url": "https://en.wikipedia.org/wiki?curid=1460229", "title": "Storm Track", "text": "Storm Track\n\nStorm Track was the first magazine for and about storm chasing. The magazine was in circulation between 1977 and 2002.\n\n\"Storm Track\" was started in 1977 by chasing pioneer David Hoadley following an informal meeting of storm chasers at an American Meteorological Society conference. In the beginning, it was published in newsletter format but in time assumed a magazine format and was published bimonthly throughout its history. In 1986, editorship was handed over to Tim Marshall, a storm damage engineer (and meteorologist). Production of paper issues ceased in 2002 after a 25-year run; however, an accompanying website started in 1996 and continues primarily in the form of a large discussion board.\n\n\"Storm Track\", among other topics, published storm chase accounts, discussions of issues affecting storm chasing, history of storm chasing and meteorology, meteorological analysis and case studies, climatology, reviews, biographies, photography, cartoons, poetry, and classifieds.\n\n\"Storm Track\" was a non-profit publication aimed at scientists and amateurs interested in severe storms. Rich Herzog was an associate editor since 1991 and Phil Sherman an assistant editor from 1986-1990. Another associate editor and a founding member was Randy Zipser. Gene Rhoden contributed significantly to the cover design in 1986. It was published with Master Graphics in Dallas, Texas. Tim Vasquez was online editor. Most articles and photographs were submitted by subscribers. More than 180 people wrote articles for the magazine. David Hoadley made all the drawings and sketches and did many of the cartoons which were known as \"Funnel Funnies\". It began with 10 subscribers in 1977 and grew to several hundred over the years. Circulation peaked at nearly 1,000 in mid-1996 in association with the release of \"Twister\".\n\nIn 2015, ownership of the \"Storm Track\" Brand was transferred to Steve Miller (OK). Since the 2015 change in ownership, the discussion board has been updated to the newest edition of Xenforo. Additionally, a Storm Track mobile app was released, along with the launch of a Discord server which has over 900 members. The leadership team, post-2015, consists of Shane Adams, Sean Bargmann, B. Dean Berry, Mark Blue, Harrison Cater, Jeff Duda, Ryan Hickman, Ben Holcomb, Devin Pitts, Jesse Risley, Dan Robinson, Jared Thompson, and John Wetter.\n\nAs of April 2018, the support functions of the Spotter Network forums have been housed at the \"Storm Track\" forums..\n\n\n"}
{"id": "4767789", "url": "https://en.wikipedia.org/wiki?curid=4767789", "title": "Superconductor Insulator Transition", "text": "Superconductor Insulator Transition\n\nThe Superconductor Insulator Transition is an example of a quantum phase transition, whereupon tuning some parameter in the Hamiltonian, a dramatic change in the behavior of the electrons occurs. The nature of how this transition occurs is disputed, and many studies seek to understand how the order parameter, formula_1, changes. Here formula_2 is the amplitude of the order parameter, and formula_3 is the phase. Most theories involve either the destruction of the amplitude of the order parameter - by a reduction in the density of states at the Fermi surface, or by destruction of the phase coherence; which results from the proliferation of vortices.\n\nIn two dimensions, the subject of superconductivity becomes very interesting because the existence of true long-range order is not possible. How then is superconductivity obtained? In the 70's, Kosterlitz and Thouless (along with Berezinski) showed that a different kind of long-range order could exist - topological order - which showed power law correlations (meaning that by measuring the two-point correlation function formula_4 it decays algebraically).\n\nThis picture changes if disorder is included. Kosterlitz-Thouless behavior can be obtained, but the fluctuations of the order parameter are greatly enhanced, and the transition temperature is suppressed.\n\nThe model to keep in mind in the understanding of how superconductivity occurs in a two-dimensional disordered superconductor is the following. At high temperatures, the system is in the normal state. As the system is cooled towards its transition temperature, superconducting grains begin to fluctuate in and out of existence. When one of these grains \"pops\" into existence, it is accelerated without dissipation for a time formula_5 before decaying back into the normal state. This has the effect of increasing the conductivity even before the system has condensed into the superconducting state. This increased conductivity above formula_6 is referred to as paraconductivity, or fluctuation conductivity, and was first correctly described by Aslamazov and Larkin. As the system is cooled further, the lifetime of these fluctuations increase, and becomes comparable to the Ginzburg-Landau time formula_7. Eventually, the amplitude formula_2 of the order parameter becomes well defined (it is non-zero wherever there are superconducting patches), and it can begin to support phase fluctuations. These phase fluctuations set in at a lower temperature, and are caused by vortices - which are topological defects in the order parameter. It is the motion of vortices that gives rise to inflation of resistance below formula_9. Eventually the system is cooled further, below the Kosterlitz-Thouless temperature formula_10, all of the free vortices become bound into vortex-antivortex pairs, and the systems attains a state with zero resistance.\n\nCooling the system to formula_11 and turning on a magnetic field has certain effects. For very small fields (formula_12) the magnetic field is shielded from the interior of the sample. Above formula_13 however, the energy cost to keep out the external field becomes too great, and the superconductor allows the field to penetrate in quantized fluxons. Now the superconductor has transitioned into the \"mixed state\", in which there is a superfluid along with vortices - which now have only one circulation.\n\nIncreasing the field adds vortices to the system. Eventually the density of vortices becomes so large that they overlap. The core of the vortex contains normal electrons (i.e. the amplitude of the superconducting order parameter is zero), so when they overlap, superconductivity is killed by destroying the amplitude of the order parameter. Increasing the field further leads to a very interesting possibility - in two-dimensions where the fluctuations are enhanced - that the vortices may condense into a Bose-condensate, which localizes the superconducting pairs.\n\n"}
{"id": "3551648", "url": "https://en.wikipedia.org/wiki?curid=3551648", "title": "Surface layering", "text": "Surface layering\n\nSurface layering is a quasi-crystalline structure at the surfaces of otherwise disordered liquids, where atoms or molecules of even the simplest liquid are stratified into well-defined layers parallel to the surface. While in crystalline solids such atomic layers can extend periodically throughout the entire dimension of a crystal, surface layering decays rapidly away from the surface and is limited to just a few near-surface region layers. Another difference between surface layering and crystalline structure is that atoms or molecules of surface-layered liquids are not ordered in-plane, while in crystalline solids they are.\n\nSurface layering was predicted theoretically by Stuart Rice at the University of Chicago in 1983 and has been experimentally discovered by Peter Pershan (Harvard) and his group, working in collaboration with Ben Ocko (Brookhaven) and Moshe Deutsch (Bar-Ilan) in 1995 in elemental liquid mercury and liquid gallium using x-ray reflectivity techniques.\n\nMore recently layering has been shown to arise from electronic properties of metallic liquids, rather than thermodynamic variables such as surface tension, since surfaces of low-surface tension metallic liquids such as liquid potassium are layered, while those of dielectric liquids such as water, are not.\n"}
{"id": "25777123", "url": "https://en.wikipedia.org/wiki?curid=25777123", "title": "Utilization factor", "text": "Utilization factor\n\nThe utilization factor or use factor is the ratio of the time that a piece of equipment is in use to the total time that it could be in use. It is often averaged over time in the definition such that the ratio becomes the amount of energy used divided by the maximum possible to be used. These definitions are equivalent.\n\nIn electrical engineering, \"utilization factor\", formula_1, is the ratio of the maximum load which could be drawn to the rated capacity of the system. This is closely related to the concept of Load factor. The Load factor is the ratio of the load that a piece of equipment actually draws (time averaged) when it is in operation to the load it could draw (which we call full load).\n\nFor example, an oversized motor - 15 kW - drives a constant 12 kW load whenever it is on. The motor load factor is then 12/15 = 80%. The motor above may only be used for eight hours a day, 50 weeks a year. The hours of operation would then be 2800 hours, and the motor use factor for a base of 8760 hours per year would be 2800/8760 = 31.96%. With a base of 2800 hours per year, the motor use factor would be 100%.\n\nIn offshore pipeline engineering, it is the ratio of the maximum allowable stress to the stress generally modelled at that section.\n\nIn power plant, utilization varies according to the demand on the plant from the electricity market.\n\n"}
{"id": "1805751", "url": "https://en.wikipedia.org/wiki?curid=1805751", "title": "Vicinal (chemistry)", "text": "Vicinal (chemistry)\n\nIn chemistry the descriptor vicinal (from Latin \"vicinus\" = neighbor), abbreviated \"vic\", describes any two functional groups bonded to two adjacent carbon atoms (i.e., in a 1,2-relationship). For example, the molecule 2,3-dibromobutane carries two vicinal bromine atoms and 1,3-dibromobutane does not. Mostly, the use of the term vicinal is restricted to two \"identical\" functional groups.\n\nLikewise in a \"gem-\"dibromide the prefix \"gem\", an abbreviation of geminal, signals that both bromine atoms are bonded to the \"same\" atom (i.e., in a 1,1-relationship). For example, 1,1-dibromobutane is geminal. While comparatively less common, the term hominal has been suggested as a descriptor for groups in a 1,3-relationship.\n\nLike other such descriptors as syn, anti, exo or endo, the description \"vicinal\" helps explain how different parts of a molecule are related to each other either structurally or spatially. The vicinal adjective is sometimes restricted to those molecules with two \"identical\" functional groups. The term can also be extended to substituents on aromatic rings.\n\nIn H NMR spectroscopy, the coupling of two hydrogen atoms on adjacent carbon atoms is called vicinal coupling. The vicinal coupling constant is referred to as \"J\" because the hydrogen atoms couple through three bonds. Depending on the other substituents, the vicinal coupling constant assumes values between 0 and +20 Hz. The dependence of the vicinal coupling constant on the dihedral angle formula_1 is described by the Karplus relation.\n\n"}
{"id": "19984658", "url": "https://en.wikipedia.org/wiki?curid=19984658", "title": "Vlorë Wind Farm", "text": "Vlorë Wind Farm\n\nThe Vlorë Wind Farm is a wind farm in Albania. It has 250 individual wind turbines with a nominal output of around 2 MW each, which deliver up to 500 MW of power.\n\n"}
{"id": "6986688", "url": "https://en.wikipedia.org/wiki?curid=6986688", "title": "Wildlife Warriors", "text": "Wildlife Warriors\n\nWildlife Warriors, originally called the Steve Irwin Conservation Foundation, is a conservationist organisation that was established in 2002 by Steve Irwin and his wife, Terri Irwin, to involve and educate others in the protection of injured, threatened or endangered wildlife. Terri Irwin is still involved in the organisation as patron and significant advisor.\n\n\n\nAustralia Zoo's Wildlife Warriors is represented by a number of celebrities, including:\n\n\nThe logo represents the pugmarks of five endangered animals:\nThey surround a human footprint.\n\nAfter the death of Steve Irwin on 4 September 2006, thousands of people from around the world offered their support and donations to the conservation group. On 14 October 2006, Wildlife Warriors executive manager Michael Hornby reported that donations to the fund in the past month had reached $2 million – enough to fund its animal hospital and international programs for six to nine months. The conservationist's one-hour public memorial service, which aired worldwide from Australia Zoo in September, has also been made into a DVD which was released across Australia on 14 October, all proceeds of which are to be used to fund the future of the charity.\n\nIrwin's daughter Bindi Irwin, then nine years old, became the new public face of Wildlife Warriors after his death.\n\n\n"}
