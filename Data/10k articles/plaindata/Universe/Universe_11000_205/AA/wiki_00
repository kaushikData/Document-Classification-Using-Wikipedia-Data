{"id": "57061065", "url": "https://en.wikipedia.org/wiki?curid=57061065", "title": "2018 Balikpapan oil spill", "text": "2018 Balikpapan oil spill\n\nThe 2018 Balikpapan oil spill was an oil spill off coast from the city of Balikpapan, Indonesia. It was caused by a cracked pipeline linked to a Pertamina refinery in the city.\n\nA blaze which occurred when the oil spill caught on fire had killed five residents who were in the bay, in addition to causing respiratory problems in the city.\nThe spilled occurred in Balikpapan Bay, on the coast of which the Pertamina Refinery Unit V is located. Balikpapan, which sits on the bay, is an energy and mining hub on the island and is populated by over 700,000 people.\nInitially, Pertamina denied responsibility for the spill for four days, claiming that tests showed the oil originated from marine shipments and that divers sent had not discovered any leakage on their pipelines. Instead, the company claimed that the oil originated from \"MV Ever Judger\", a Panama-flagged bulk carrier carrying coal to Malaysia. However, on 5 April, it admitted responsibility, adding that it discovered that the oil samples were crude oil and not marine fuel oil after its 10th sample.\n\nAccording to Pertamina on an April 4 statement, a distribution pipeline carrying crude oil from a terminal in Lawe-Lawe, North Penajam Paser to Balikpapan which was installed in 1998 burst, having been displaced by 120 meters from its original position. The leaked pipeline was located at a depth of 25 meters, with a diameter of 20 inches and a thickness of 12 millimeters. Officials claimed that an anchor dropped by of a coal ship flying a Panamianian flag dragged the pipe, although it did not name the ship. Pertamina denied any negligence in the spill, adding that the pipe had been inspected as recently as December 2017.\n\nDuring initial attempts on 31 March, some of the oil spill caught on fire with the blaze reaching as high as . Five people were killed when the oil spill caught on fire. According to a local search and rescue worker, the flame was sparked in an attempt to clear the spill by burning it. Director-general of oil and gas Djoko Siswanto stated that they suspected the ship dragging the pipeline had caught on fire, hence igniting the spill.\n\nAccording to the Ministry of Environment and Forestry, about 70 cubic meters of spilled oil have been contained by night on 3 April.\n\nEnvironmental organizations and campaigners criticized the Indonesian government for its slow response, with Greenpeace claiming it had not received any information from the authorities on the fifth day of the spill.\n\nThe city's residents participated in the cleanup, using simple equipment such as buckets to scoop oil from the local beaches.\nBalikpapan city secretary Sayid MN Fadli described that the bay was \"like a gas station\". The city itself declared a state of emergency, with over 1,000 people reporting nausea and breathing problem due to the smoke caused by a fire on the spill. Pertamina released a statement declaring that the operations of its refinery, which processes 260,000 barrels of crude oil daily, is unaffected.\n\nHead of the People's Representative Council Bambang Soesatyo demanded an explanation from the Embassy of China in Jakarta, regarding a theory that the fire on the \"Ever Judger\" resulted in the blaze and hence fatalities.\n\nOn 20 April, Pertamina's Chief Executive Elia Massa Manik was removed from his position. Among other factors, the oil spill incident was quoted as one of the reasons for the dismissal.\nFive people were killed when their ship was trapped in the blaze caused by the oil catching on fire. Two of the bodies were discovered immediately afterwards, with two more being discovered on the third day and the last the following day. The victims had rented a boat to fish around the bay. All victims were residents of the city.\n\nThe MV \"Ever Judger\" was also caught in the blaze, with its inflatable life raft and the rope binding it to the ship igniting. Its crew of 20 Chinese nationals were evacuated with 1 suffering from burns. While the ship, which carried more than 70,000 tons of coal, did not catch on fire fully, ship's port side was severely damaged. \n\nIndonesian vice-minister of energy and mineral resources Arcandra Tahar stated that the pipeline damage may cause up to 200,000 barrels per day in lost production. Pertamina stated that the crude oil flow was diverted to a smaller, 16-inch pipe, although it confirmed the loss of some production in the refinery.\n\nAt least 34 hectares of mangrove swamps were covered in the spill. A dead endangered Irrawaddy dolphin washed up to the city's beach, seemingly poisoned by the oil spill. Environmental and oceanographical experts have expressed concerns of long-lasting damages to the oceanic ecosystem including coral reefs and seagrass.\n\nDue to the spill, the Ministry of Maritime Affairs and Fisheries received reports of oil pollution across the country, such as in the Bay of Jakarta and in Bintan.\n"}
{"id": "13998811", "url": "https://en.wikipedia.org/wiki?curid=13998811", "title": "Advanced Stirling radioisotope generator", "text": "Advanced Stirling radioisotope generator\n\nThe advanced Stirling radioisotope generator (ASRG) was a radioisotope power system first developed at NASA's Glenn Research Center. It uses a Stirling power conversion technology to convert radioactive-decay heat into electricity for use on spacecraft. The energy conversion process used by an ASRG is about four times more efficient than in previous radioisotope systems to produce a similar amount of power, and allows it to use about one quarter of the plutonium-238 as other similar generators.\n\nDespite termination of the ASRG flight development contract in 2013, NASA continues a small investment testing by private companies. Flight-ready Stirling-based units are expected by 2028.\n\nDevelopment was undertaken in 2000 under joint sponsorship by the United States Department of Energy (DoE), Lockheed Martin Space Systems, and the Stirling Research Laboratory at NASA's Glenn Research Center (GRC) for potential future space missions.\n\nIn 2012, NASA chose a solar-powered mission (InSight) for Discovery 12 interplanetary mission, which would have otherwise needed a radioisotope power system for the planned 2016 launch (which as it happens has been delayed until 2018).\n\nThe DOE cancelled the Lockheed contract in late 2013, after the cost had risen to over $260 million, $110 million more than originally expected. It was also decided to make use of remaining program hardware in constructing and testing a second engineering unit (for testing and research), which was completed in August 2014 in a close-out phase and shipped to GRC. Testing done in 2015 showed power fluctuations after just 175 hr of operation, becoming more frequent and larger in magnitude.\n\nNASA also needed more funding for continued plutonium-238 production (which will be used in existing MMRTGs for long-range probes in the meantime) and decided to use the savings from the ASRG cancellation to do so rather than take funding from science missions.\n\nDespite termination of the ASRG flight development contract, NASA continues a small investment testing Sterling converter technologies developed by Sunpower Inc. and Infinia Corporation, in addition to the unit supplied by Lockheed and a variable-conductance heat pipe supplied by Advanced Cooling Technologies, Inc. Flight-ready units based on Stirling technology are not expected until 2028.\nThe higher conversion efficiency of the Stirling cycle compared with that of radioisotope thermoelectric generators (RTGs) used in previous missions (Viking, Pioneer, Voyager, Galileo, Ulysses, Cassini, New Horizons, and Mars Science Laboratory) would have offered an advantage of a fourfold reduction in PuO fuel, at half the mass of an RTG. It would have produced 140 watts of electricity using a quarter of the plutonium an RTG or MMRTG needs.\n\nThe two finished units had these expected specifications:\n\nASRGs can be installed on a wide variety of vehicles, from orbiters, landers and rovers to balloons and planetary boats. A spacecraft proposed to use this generator was the TiME boat-lander mission to Titan, the largest moon of the planet Saturn, with a launch intended for January 2015, or 2023. In February 2009 it was announced that NASA/ESA had given Europa Jupiter System Mission (EJSM/Laplace) mission priority ahead of the Titan Saturn System Mission (TSSM), which could have included TiME. In August 2012, TiME also lost the 2016 Discovery class competition to the InSight Mars lander.\n\nThe HORUS mission was proposing to use three ASRGs to power an orbiter for the Uranian system. Another Uranus probe concept using the ASRG was MUSE which has been evaluated as both an ESA L-Class mission and New Frontiers enhanced mission. The Jupiter Europa Orbiter mission proposed using four ASRG to power an orbiter in the Jovian system. Another possibility was the Mars Geyser Hopper.\n\nIt was proposed in 2013 to fly three ASRG units onbord the FIRE probe to study Jupiter's moon Io for the New Frontiers program Mission 4.\n\n\n"}
{"id": "2704841", "url": "https://en.wikipedia.org/wiki?curid=2704841", "title": "Air preheater", "text": "Air preheater\n\nAn air preheater (APH) is any device designed to heat air before another process (for example, combustion in a boiler) with the primary objective of increasing the thermal efficiency of the process. They may be used alone or to replace a recuperative heat system or to replace a steam coil.\n\nIn particular, this article describes the combustion air preheaters used in large boilers found in thermal power stations producing electric power from e.g. fossil fuels, biomass or waste.\n\nThe purpose of the air preheater is to recover the heat from the boiler flue gas which increases the thermal efficiency of the boiler by reducing the useful heat lost in the flue gas. As a consequence, the flue gases are also conveyed to the flue gas stack (or chimney) at a lower temperature, allowing simplified design of the conveyance system and the flue gas stack. It also allows control over the temperature of gases leaving the stack (to meet emissions regulations, for example).It is installed between the economizer and chimney.\n\nThere are two types of air preheaters for use in steam generators in thermal power stations: One is a tubular type built into the boiler flue gas ducting, and the other is a regenerative air preheater. These may be arranged so the gas flows horizontally or vertically across the axis of rotation.\n\nAnother type of air preheater is the \"regenerator\" used in iron or glass manufacture.\n\nTubular preheaters consist of straight tube bundles which pass through the outlet ducting of the boiler and open at each end outside of the ducting. Inside the ducting, the hot furnace gases pass around the preheater tubes, transferring heat from the exhaust gas to the air inside the preheater. Ambient air is forced by a fan through ducting at one end of the preheater tubes and at other end the heated air from inside of the tubes emerges into another set of ducting, which carries it to the boiler furnace for combustion.\n\nThe tubular preheater ductings for cold and hot air require more space and structural supports than a rotating preheater design. Further, due to dust-laden abrasive flue gases, the tubes outside the ducting wear out faster on the side facing the gas current. Many advances have been made to eliminate this problem such as the use of ceramic and hardened steel.\n\nMany new circulating fluidized bed (CFB) and bubbling fluidized bed (BFB) steam generators are currently incorporating tubular air heaters offering an advantage with regards to the moving parts of a rotary type.\n\nDew point corrosion occurs for a variety of reasons. The type of fuel used, its sulfur content and moisture content are contributing factors. However, by far the most significant cause of dew point corrosion is the metal temperature of the tubes. If the metal temperature within the tubes drops below the acid saturation temperature, usually at between 190 °F (88 °C)and 230 °F (110 °C), but sometimes at temperatures as high as 260 °F (127 °C), then the risk of dew point corrosion damage becomes considerable.\n\nThere are two types of regenerative air preheaters: the rotating-plate regenerative air preheaters (RAPH) and the stationary-plate regenerative air preheaters (Rothemuhle).\n\nThe rotating-plate design (RAPH) consists of a central rotating-plate element installed within a casing that is divided into two (\"bi-sector\" type), three (\"tri-sector\" type) or four (\"quad-sector\" type) sectors containing seals around the element. The seals allow the element to rotate through all the sectors, but keep gas leakage between sectors to a minimum while providing separate gas air and flue gas paths through each sector.\n\nTri-sector types are the most common in modern power generation facilities. In the tri-sector design, the largest sector (usually spanning about half the cross-section of the casing) is connected to the boiler hot gas outlet. The hot exhaust gas flows over the central element, transferring some of its heat to the element, and is then ducted away for further treatment in dust collectors and other equipment before being expelled from the flue gas stack. The second, smaller sector, is fed with ambient air by a fan, which passes over the heated element as it rotates into the sector, and is heated before being carried to the boiler furnace for combustion. The third sector is the smallest one and it heats air which is routed into the pulverizers and used to carry the coal-air mixture to coal boiler burners. Thus, the total air heated in the RAPH provides: heating air to remove the moisture from the pulverised coal dust, carrier air for transporting the pulverised coal to the boiler burners and the primary air for combustion.\nThe rotor itself is the medium of heat transfer in this system, and is usually composed of some form of steel and/or ceramic structure. It rotates quite slowly (around 1-2 RPM) to allow optimum heat transfer first from the hot exhaust gases to the element, then as it rotates, from the element to the cooler air in the other sectors.\n\nIn this design the whole air preheater casing is supported on the boiler supporting structure itself with necessary expansion joints in the ducting.\n\nThe vertical rotor is supported on thrust bearings at the lower end and has an oil bath lubrication, cooled by water circulating in coils inside the oil bath. This arrangement is for cooling the lower end of the shaft, as this end of the vertical rotor is on the hot end of the ducting. The top end of the rotor has a simple roller bearing to hold the shaft in a vertical position.\n\nThe rotor is built up on the vertical shaft with radial supports and cages for holding the baskets in position. Radial and circumferential seal plates are also provided to avoid leakages of gases or air between the sectors or between the duct and the casing while in rotation.\n\nFor on line cleaning of the deposits from the baskets steam jets are provided such that the blown out dust and ash are collected at the bottom ash hopper of the air preheater. This dust hopper is connected for emptying along with the main dust hoppers of the dust collectors.\n\nThe rotor is turned by an air driven motor and gearing, and is required to be started before starting the boiler and also to be kept in rotation for some time after the boiler is stopped, to avoid uneven expansion and contraction resulting in warping or cracking of the rotor. The station air is generally totally dry (dry air is required for the instrumentation), so the air used to drive the rotor is injected with oil to lubricate the air motor.\n\nSafety protected inspection windows are provided for viewing the preheater's internal operation under all operating conditions.\n\nThe baskets are in the sector housings provided on the rotor and are renewable. The life of the baskets depend on the ash abrasiveness and corrosiveness of the boiler outlet gases.\n\nThe boiler flue gas contains many dust particles (due to high ash content) not contributing towards combustion, such as silica, which cause abrasive wear of the baskets, and may also contain corrosive gases depending on the composition of the fuel. For example, Indian coals generally result in high levels of ash and silica in the flue gas. The wear of the baskets therefore is generally more than other, cleaner-burning fuels.\n\nIn this RAPH, the dust laden, corrosive boiler gases have to pass between the elements of air preheater baskets. The elements are made up of zig zag corrugated plates pressed into a steel basket giving sufficient annular space in between for the gas to pass through. These plates are corrugated to give more surface area for the heat to be absorbed and also to give it the rigidity for stacking them into the baskets. Hence frequent replacements are called for and new baskets are always kept ready. In the early days, Cor-ten steel was being used for the elements. Today due to technological advance many manufacturers may use their own patents.\nSome manufacturers supply different materials for the use of the elements to lengthen the life of the baskets.\n\nIn certain cases the unburnt deposits may occur on the air preheater elements causing it to catch fire during normal operations of the boiler, giving rise to explosions inside the air preheater. Sometimes mild explosions may be detected in the control room by variations in the inlet and outlet temperatures of the combustion air.\n\nThe heating plate elements in this type of regenerative air preheater are also installed in a casing, but the heating plate elements are stationary rather than rotating. Instead the air ducts in the preheater are rotated so as to alternatively expose sections of the heating plate elements to the upflowing cool air.\n\nAs indicated in the adjacent drawing, there are rotating inlet air ducts at the bottom of the stationary plates similar to the rotating outlet air ducts at the top of the stationary plates.\n\nStationary-plate regenerative air preheaters are also known as Rothemuhle preheaters, manufactured for over 25 years by Balke-Dürr GmbH of Ratingen, Germany.\n\nA regenerator consists of a brick checkerwork: bricks laid with spaces equivalent to a brick's width between them, so that air can flow relatively easily through the checkerwork. The idea is that as hot exhaust gases flow through the checkerwork, they give up heat to the bricks. The airflow is then reversed, so that the hot bricks heat up the incoming combustion air and fuel. For a glass-melting furnace, a regenerator sits on either side of the furnace, often forming an integral whole. For a blast furnace, the regenerators (commonly called Cowper stoves) sit separate to the furnace. A furnace needs no less than two stoves, but may have three. One of the stoves is 'on gas', receiving hot gases from the furnace top and heating the checkerwork inside, whilst the other is 'on blast', receiving cold air from the blowers, heating it and passing it to the blast furnace.\n\n\n"}
{"id": "48666625", "url": "https://en.wikipedia.org/wiki?curid=48666625", "title": "Bamboo Forest (Kyoto, Japan)", "text": "Bamboo Forest (Kyoto, Japan)\n\nBamboo Forest, or Arashiyama Bamboo Grove or Sagano Bamboo Forest, is a natural forest of bamboo in Arashiyama, Kyoto, Japan. The forest consists of several pathways for tourists and visitors. The Ministry of the Environment considers it a part of the soundscape of Japan.\n\nPrior to 2015 there was a charge to access the area.\n\nThe forest is not far from Tenryū-ji Temple, which is the location of Rinzai School, and the Nonomiya Shrine.\n\n"}
{"id": "2238741", "url": "https://en.wikipedia.org/wiki?curid=2238741", "title": "Bioactive glass", "text": "Bioactive glass\n\nBioactive glasses are a group of surface reactive glass-ceramic biomaterials and include the original bioactive glass, bioglass. The biocompatibility and bioactivity of these glasses has led them to be investigated extensively for use as implant device in the human body to repair and replace diseased or damaged bones.\n\nThere is tentative evidence that bioactive glass may also be useful in long bone infections. Support from randomized controlled trials; however, is still not available as of 2015.\n\nSolid state NMR spectroscopy has been very useful in elucidating the structure of amorphous solids. Bioactive glasses have been studied by Si and P solid state MAS NMR spectroscopy. The chemical shift from MAS NMR is indicative of the type of chemical species present in the glass. The Si MAS NMR spectroscopy showed that Bioglass 45S5 was a Q2 type-structure with a small amount of Q3 ; i.e., silicate chains with a few crosslinks. The P MAS NMR revealed predominately Q0 species; i.e., PO; subsequent MAS NMR spectroscopy measurements have shown that Si-O-P bonds are below detectable levels \n\nThere have been many variations on the original composition which was Food and Drug Administration (FDA) approved and termed Bioglass. This composition is known as 45S5. Other compositions are in the list below.\n\nThe underlying mechanisms that enable bioactive glasses to act as materials for bone repair have been investigated since the first work of Hench et al. at the University of Florida. Early attention was paid to changes in the bioactive glass surface. Five inorganic reaction stages are commonly thought to occur when a bioactive glass is immersed in a physiological environment:\n\n1) Ion exchange in which modifier cations (mostly Na) in the glass exchange with hydronium ions in the external solution.\n\n2) Hydrolysis in which Si-O-Si bridges are broken, forming Si-OH silanol groups, and the glass network is disrupted.\n\n3) Condensation of silanols in which the disrupted glass network changes its morphology to form a gel-like surface layer, depleted in sodium and calcium ions.\n\n4) Precipitation in which an amorphous calcium phosphate layer is deposited on the gel.\n\n5) Mineralization in which the calcium phosphate layer gradually transforms into crystalline hydroxyapatite, that mimics the mineral phase naturally contained with vertebrate bones.\n\nLater, it was discovered that the morphology of the gel surface layer was a key component in determining the bioactive response. This was supported by studies on bioactive glasses derived from sol-gel processing. Such glasses could contain significantly higher concentrations of SiO than traditional melt-derived bioactive glasses and still maintain bioactivity (i.e., the ability to form a mineralized hydroxyapatite layer on the surface). The inherent porosity of the sol-gel-derived material was cited as a possible explanation for why bioactivity was retained, and often enhanced with respect to the melt-derived glass.\n\nSubsequent advances in DNA microarray technology enabled an entirely new perspective on the mechanisms of bioactivity in bioactive glasses. Previously, it was known that a complex interplay existed between bioactive glasses and the molecular biology of the implant host, but the available tools did not provide a sufficient quantity of information to develop a holistic picture. Using DNA microarrays, researchers are now able to identify entire classes of genes that are regulated by the dissolution products of bioactive glasses, resulting in the so-called \"genetic theory\" of bioactive glasses. The first microarray studies on bioactive glasses demonstrated that genes associated with osteoblast growth and differentiation, maintenance of extracellular matrix, and promotion of cell-cell and cell-matrix adhesion were up-regulated by conditioned cell culture media containing the dissolution products of bioactive glass.\n\nLarry Hench and colleagues at the University of Florida first developed these materials in the late 1960s and they have been further developed by his research team at the Imperial College London and other researchers worldwide.\n\nBioglass 8625, also called Schott 8625, is a soda-lime glass used for encapsulation of implanted devices. The most common use of Bioglass 8625 is in the housings of RFID transponders for use in human and animal microchip implants. It is patented and manufactured by Schott AG. Bioglass 8625 is also used for some piercings.\n\nBioglass 8625 does not bond to tissue or bone, it is held in place by fibrous tissue encapsulation. After implantation, a calcium-rich layer forms on the interface between the glass and the tissue. Without additional antimigration coating it is subject to migration in the tissue. The antimigration coating is a material that bonds to both the glass and the tissue. Parylene, usually parylene type C, is often used as such material.\n\nBioglass 8625 has a significant content of iron, which provides infrared light absorption and allows sealing by a light source, e.g. a Nd:YAG laser or a mercury-vapor lamp. The content of FeO yields high absorption with maximum at 1100 nm, and gives the glass a green tint. The use of infrared radiation instead of flame or contact heating helps preventing contamination of the device.\n\nAfter implantation, the glass reacts with the environment in two phases, in the span of about two weeks. In the first phase, alkali metal ions are leached from the glass and replaced with hydrogen ions; small amount of calcium ions also diffuses from the material. During the second phase, the Si-O-Si bonds in the silica matrix undergo hydrolysis, yielding a gel-like surface layer rich on Si-O-H groups. A calcium phosphate-rich passivation layer gradually forms over the surface of the glass, preventing further leaching.\n\nIt is used in microchips for tracking of many kinds of animals, and recently in some human implants. The U.S. Food and Drug Administration (FDA) approved use of Bioglass 8625 in humans in 1994.\n\nBioglass 45S5, one of the most important formulations, is composed of SiO, NaO, CaO and PO. Professor Larry Hench developed Bioglass at the University of Florida in the late 1960s. He was challenged by a MASH army officer to develop a material to help regenerate bone, as many Vietnam war veterans suffered badly from bone damage, such that most of them injured in this way lost their limbs.\n\nThe composition was originally selected because of being roughly eutectic.\n\nThe 45S5 name signifies glass with 45 wt.% of SiO and 5:1 molar ratio of Calcium to Phosphorus. Lower Ca/P ratios do not bond to bone.\n\nThe key composition features of Bioglass is that it contains less than 60 mol% SiO, high NaO and CaO contents, high CaO/PO ratio, which makes Bioglass highly reactive to aqueous medium and bioactive.\n\nHigh bioactivity is the main advantage of Bioglass, while its disadvantages includes mechanical weakness, low fracture resistance due to amorphous 2-dimensional glass network. The bending strength of most Bioglass is in the range of 40–60 MPa, which is not enough for load-bearing application. Its Young's modulus is 30–35 GPa, very close to that of cortical bone, which can be an advantage. Bioglass implants can be used in non-load-bearing applications, for buried implants loaded slightly or compressively. Bioglass can be also used as a bioactive component in composite materials or as powder. Sometimes, Bioglass can be converted into an artificial cocaine. This has no known side-effects.\n\nThe first successful surgical use of Bioglass 45S5 was in replacement of ossicles in middle ear, as a treatment of conductive hearing loss. The advantage of 45S5 is in no tendency to form fibrous tissue. Other uses are in cones for implantation into the jaw following a tooth extraction. Composite materials made of Bioglass 45S5 and patient's own bone can be used for bone reconstruction.\n\nBioglass is comparatively soft in comparison to other glasses. It can be machined, preferably with diamond tools, or ground to powder. Bioglass has to be stored in a dry environment, as it readily absorbs moisture and reacts with it.\n\nBioglass 45S5 is manufactured by conventional glass-making technology, using platinum or platinum alloy crucibles to avoid contamination. Contaminants would interfere with the chemical reactivity in organism. Annealing is a crucial step in forming bulk parts, due to high thermal expansion of the material.\n\nHeat treatment of Bioglass reduces the volatile alkali metal oxide content and precipitates apatite crystals in the glass matrix. The resulting glass–ceramic material, named Ceravital, has higher mechanical strength and lower bioactivity.\n\n"}
{"id": "4606867", "url": "https://en.wikipedia.org/wiki?curid=4606867", "title": "Bottom ash", "text": "Bottom ash\n\nBottom ash is part of the non-combustible residue of combustion in a furnace or incinerator. In an industrial context, it has traditionally referred to coal combustion and comprises traces of combustibles embedded in forming clinkers and sticking to hot side walls of a coal-burning furnace during its operation. The portion of the ash that escapes up the chimney or stack is, however, referred to as \"fly ash\". The clinkers fall by themselves into the bottom hopper of a coal-burning furnace and are cooled. The above portion of the ash is referred to as bottom ash too. Additionally, modern municipal waste incinerators try in reducing the production of dioxins by incinerating at 850 to 950 (degrees Celsius) for at least two seconds forming bottom ash as byproduct. \n\nIn a conventional water impounded hopper (WIH) system, the clinker lumps get crushed to small sizes by clinker grinders mounted under water and fall down into a trough from where a water ejector takes them out to a sump. From there it is pumped out by suitable rotary pumps to dumping yard far away. In another arrangement a continuous link chain scrapes out the clinkers from under water and feeds them to clinker grinders outside the bottom ash hopper.\n\nMore modern systems adopt a continuous removal philosophy. Essentially, a heavy duty chain conveyor submerged in a water bath below the furnace which quenches hot ash as it falls from the combustion chamber and removes the wet ash continuously up to a de-watering slope before onward discharge into mechanical conveyors or directly to storage silos.\n\nThese days bottom ash can be extracted, cooled and conveyed using dry ash technology from various companies. Dry ash handling has many benefits. When left dry the ash can be used to make concrete and other useful materials. There are also several environmental benefits.\n\nBottom ash may be used as raw alternative material, replacing earth or sand or aggregates, for example in road construction and in cement kilns (clinker production). A noticeable other use is as growing medium in horticulture (usually after sieving). In the United Kingdom it is known as furnace bottom ash (FBA), to distinguish it from incinerator bottom ash (IBA), the non-combustible elements remaining after incineration. A pioneer use of bottom ash was in the production of concrete blocks used to construct many high-rise flats in London in the 1960s.\n\n\n"}
{"id": "35588945", "url": "https://en.wikipedia.org/wiki?curid=35588945", "title": "Brandenburg-Briest Solarpark", "text": "Brandenburg-Briest Solarpark\n\nBrandenburg-Briest Solarpark is photovoltaic power station, located at a former military airfield in Brandenburg, Germany. At the time of its completion, it was the largest solar park in Europe. Equipped with Q-Cells solar modules, it consists of three sections, namely\nthat add up to a total installed capacity of 91 megawatts (MW), sufficient to supply the electricity needs of more than 11,500 households.\n\n"}
{"id": "5214591", "url": "https://en.wikipedia.org/wiki?curid=5214591", "title": "Braniff Flight 250", "text": "Braniff Flight 250\n\nBraniff Airways Flight 250 crashed near Falls City, Nebraska, on August 6, 1966, en route to Omaha from Kansas City, Missouri. Thirty-eight passengers and four crew members were killed in the crash, which occurred in a farm field late on a Saturday night. In-flight structural failure due to extreme turbulence in an avoidable weather hazard was cited as the cause.\n\nThe aircraft involved was a BAC 1-11-203AE, registration ; the aircraft was manufactured in December 1965. The cockpit crew consisted of Captain Donald Pauly, 47, and First Officer James Hilliker, 39. \nCaptain Pauly was highly experienced with 20,767 flying hours, 549 of which was in the BAC-1-11. He possessed type ratings in other aircraft including the DC-3, the DC-6, DC-7 and the Convair family. \nFirst Officer Hilliker was less experienced, with 9,269 flying hours, 685 in the BAC-1-11. According to the NTSB report, he had two type ratings in the BAC-1-11 and the Convair family.\n\nFlight 250 was operated by Braniff between New Orleans and Minneapolis with stops in between at Shreveport, Fort Smith, Tulsa, Kansas City, and Omaha. It departed Kansas City at 22:55 on an IFR clearance to Omaha at FL200. However, the crew asked if they could remain at because of the weather. The flight remained at until permission was received at 23:06 to descend to 5,000 feet. At 23:08 the crew contacted a company flight that had just departed Omaha. This flight reported moderate to light turbulence. About four minutes later the aircraft entered an updraft within an area of active squall line of severe thunderstorms. The 1-11 violently accelerated upward and in a left roll. At this time the right tailplane and the fin failed. The aircraft then pitched nose down and within one or two seconds the right wing failed as well. The plane tumbled down in flames until entering a flat spin before impacting the ground. The probable cause was in-flight structural failure caused by extreme turbulence during operation of the aircraft in an area of avoidable hazardous weather.\n\nThe crash occurred on a farm about north-northeast of Falls City, in a soybean field only from a farmhouse.\n\nBraniff regulations prohibited a plane from being dispatched into an area with a solid line of thunderstorms; nonetheless, the company forecast was somewhat inaccurate with respect to the number and intensity of thunderstorms and the intensity of the associated turbulence. Braniff dispatchers were aware that their flight 255 had delayed departing Sioux City for Omaha by one hour to allow the storm to pass Omaha; they also knew that their flight 234 from St. Louis to Des Moines had diverted to Kansas City due to the storm. They did not inform the crew of these events believing they were too far from the route of flight 250 to be relevant. The crew was aware of the severe weather, however, and the first officer suggested that they divert around the activity. The captain instead elected to continue the flight into the edges of the squall line.\n\nDr. Ted Fujita, a renowned weather researcher and professor of meteorology at the University of Chicago, was hired by British Aircraft Corporation, the manufacturer of the BAC 1-11, to study how the weather affected the jet. Dr. Fujita is recognized as the discoverer of downbursts and microbursts and also developed the Fujita scale, which differentiates tornado intensity and links tornado damage with wind speed. Notably, the accident was the first with a U.S.-registered aircraft in which a cockpit voice recorder was used to aid in the investigation. Just before the breakup, the device recorded Captain Pauly instructing First Officer Hilliker to adjust the engine power settings. He was interrupted mid-sentence by buffeting so severe that no more dialog could be discerned on the recording, which continued even after the wings and tail separated from the aircraft. Since the Flight Data Recorder was destroyed in the crash, the changes in the buffeting sound would later be used to estimate the airplane's changes in speed and altitude during the accident sequence.\n\nAt its fortieth anniversary in 2006, a memorial was placed at the crash site.\n\nThis crash is covered in detail in the book \"Air Disaster (Vol. 1)\" by Macarthur Job, illustrated by Matthew Tesch. US television drama \"Mad Men\" referenced this accident briefly in the season 5 episode \"Signal 30\". In the series, client Mohawk Airlines also operated the BAC 1-11.\n\n\n\n"}
{"id": "54852772", "url": "https://en.wikipedia.org/wiki?curid=54852772", "title": "Breakthrough curve", "text": "Breakthrough curve\n\nA breakthrough curve in adsorption is the course of the effluent adsorptive concentration at the outlet of a fixed bed adsorber Breakthrough curves are important for adsorptive separation technologies and for the characterization of porous materials.\n\nSince almost all adsorptive separation processes are dynamic -meaning, that they are running under flow - testing porous materials for those applications for their separation performance has to be tested under flow as well. Since separation processes run with mixtures of different components, measuring several breakthrough curves results in thermodynamic mixture equilibria - mixture sorption isotherms, that are hardly accessible with static manometric sorption characterization. This enables the determination of sorption selectivities in gaseous and liquid phase.\n\nThe determination of breakthrough curves is the foundation of many other processes, like the pressure swing adsorption. Within this process, the loading of one adsorber is equivalent to a breakthrough experiment.\n\nA fixed bed of porous materials (e.g. activated carbons and zeolites) is pressurized and purged with a carrier gas. After becoming stationary one or more adsorptives are added to the carrier gas, resulting in a step-wise change of the inlet concentration. This is in contrast to chromatographic separation processes, where pulse-wise changes of the inlet concentrations are used. The course of the adsorptive concentrations at the outlet of the fixed bed are monitored.\n\nIntegration of the area above the entire breakthrough curve gives the maximum loading of the adsorptive material. Additionally, the duration of the breakthrough experiment until a certain threshold of the adsorptive concentration at the outlet can be measured, which enables the calculation of a technically usable sorption capacity. Up to this time, the quality of the product stream can be maintained. The shape of the breakthrough curves contains information about the mass transfer properties of the adsorptive-adsorbent system. These properties can by evaluated by applying simplified models and fitting to experimental data by stimulations.\n"}
{"id": "12242497", "url": "https://en.wikipedia.org/wiki?curid=12242497", "title": "Brunsbüttel Nuclear Power Plant", "text": "Brunsbüttel Nuclear Power Plant\n\nBrunsbüttel Nuclear Power Plant is a nuclear power plant in Brunsbüttel near Hamburg, Germany. It is owned 67% by Vattenfall and 33% by E.ON. It started operation in 1976 and has a gross power production of 806 MW. During its lifetime, it produced 130,000 GW hours of electricity, about double the electricity production of Austria. The value of this electricity is about 9.1 billion Euros.\n\nAs part of the nuclear power phase-out, it was taken out of service in 2007.\n"}
{"id": "2635689", "url": "https://en.wikipedia.org/wiki?curid=2635689", "title": "Cerebos", "text": "Cerebos\n\nCerebos is a brand of salt and, more recently, of other flavourings and nutritional supplements. The company is now owned by a Japanese company, Suntory K.K. The product was developed by George Weddell, a Scottish chemist working at the British company Mawson & Swan, and sold under the Cerebos brand by a new partnership, Mawson, Swan & Weddell. The company Cerebos Ltd was later registered in 1894. Its slogan was \"See How It Runs\", because the salt contained anti-caking agents. At the time of its introduction, salt was sold in large blocks from which the user would scrape what they needed. Free-running salt was a novelty because, left for any length of time, pure sodium chloride crystals would absorb sufficient moisture from the air to cause them to stick together, a phenomenon called \"caking\". A boy chasing a bird and pouring salt over it is an icon that has become synonymous with the brand.\n\nIt is sold in Western Europe (including France where it is spelt Cérébos), Australia, New Zealand and South Africa.\n\nThe Australian and New Zealand Food and Beverage operations are part of Cerebos Pacific (Singapore), which in turn is part of Suntory Holdings Limited (Japan) established in 2009, and includes the well known local brands:\n\nThe Cerebos salt company invented 'Bisto' gravy powder product (a mixture of salt, flavourings and colourings), at its salt factory in Middlewich, Cheshire in the United Kingdom. It was acquired by RHM in 1968, which later sold its stake in Cerebos Pacific to Suntory in 1990.\n\n"}
{"id": "5406236", "url": "https://en.wikipedia.org/wiki?curid=5406236", "title": "Charta emporetica", "text": "Charta emporetica\n\nIn pharmacy, etc., charta emporetica was a kind of paper made very soft and porous, used as a filter. It was also used as packing paper.\n"}
{"id": "6193204", "url": "https://en.wikipedia.org/wiki?curid=6193204", "title": "Cooper–Harper rating scale", "text": "Cooper–Harper rating scale\n\nThe Cooper–Harper rating scale is a set of criteria used by test pilots and flight test engineers to evaluate the handling qualities of aircraft during flight test. The scale ranges from 1 to 10, with 1 indicating the best handling characteristics and 10 the worst. The criteria are evaluative and thus the scale is considered subjective.\n\nAfter World War II, the various U.S. military branches sent different models of their operational aircraft to the Ames Aeronautical Laboratory located at the Moffett Federal Airfield in Mountain View, California for evaluation of the planes' flight performance and flying qualities. The laboratory was operated by NACA, the predecessor of NASA. Most of the flights were conducted by George Cooper, Bob Innis, and Fred Drinkwater and took place at the remote test site at the Crows Landing Naval Auxiliary Landing Field in the central valley area east of Moffett Field.\n\nWhat may be the most important contribution of the flying qualities evaluation programs and experiments conducted on the variable stability aircraft at Ames was George Cooper's standardized system for rating an aircraft's flying qualities. Cooper developed his rating system over several years as a result of the need to quantify the pilot's judgment of an aircraft's handling in a fashion that could be used in the stability and control design process. This came about because of his perception of the value that such a system would have, and because of the encouragement of his colleagues in the United States and England who were familiar with his initial attempts.\n\nCooper's approach forced a specific definition of the pilot's task and of its performance standards. Furthermore, it accounted for the demands the aircraft placed on the pilot in accomplishing a given task to some specified degree of precision. The Cooper Pilot Opinion Rating Scale was initially published in 1957. After several years of experience gained in its application to many flight and flight simulator experiments, and through its use by the military services and aircraft industry, the scale was modified in collaboration with Robert (Bob) Harper of the Cornell Aeronautical Laboratory (Now Calspan) and became the Cooper-Harper Flying Qualities Rating Scale in 1969, a scale which remains the standard for measuring flying qualities.\n\nIn recognition of his many contributions to aviation safety, Cooper received the Admiral Luis de Florez Flight Safety Award in 1966 and the Richard Hansford Burroughs, Jr. Test Pilot Award in 1971. After he retired, both he and Bob Harper were selected by the American Institute of Aeronautics and Astronautics to reprise the Cooper-Harper Rating Scale in the 1984 Wright Brothers Lectureship in Aeronautics.\n\nWhile the Cooper–Harper scale remains the only well-established scale for assessing aircraft flying qualities, its unidimensional format lacks diagnostic power and has also been criticised for exhibiting poor reliability. The Cranfield Aircraft Handling Qualities Rating Scale (CAHQRS), developed at Cranfield University's School of Engineering, is a multidimensional evaluative system. It was developed by combining concepts from two previously established scales, the NASA-TLX workload scale and the Cooper–Harper. A series of validation trials in an engineering flight simulator with a range of control laws showed that the CAHQRS was at least as effective as the Cooper–Harper scale. However, the CAHQRS also demonstrated greater diagnostic ability and reliability than the Cooper–Harper. This new scale's acceptance by the aerospace industry at large, though, remains to be demonstrated.\n\nIt is important to note that a Handling Qualities Rating (HQR) can not be assigned to an aircraft, as in \"That aircraft is a HQR 5 aircraft.\" Any HQR that is assigned requires a well defined, repeatable task, a well trained pilot that is actively engaged in accomplishing that task, and an aircraft.\n\n"}
{"id": "50825027", "url": "https://en.wikipedia.org/wiki?curid=50825027", "title": "Current sensing techniques", "text": "Current sensing techniques\n\nIn electrical engineering, current sensing is any one of several techniques used to measure electric current. The measurement of current ranges from picoamps to tens of thousands of amperes. The selection of a current sensing method depends on requirements such as magnitude, accuracy, bandwidth, robustness, cost, isolation or size. The current value may be directly displayed by an instrument, or converted to digital form for use by a monitoring or control system.\n\nCurrent sensing techniques include shunt resistor, current transformers and Rogowski coils, magnetic-field based transducers and others.\n\nCurrent sensing technologies must fulfill various requirements, for various applications. Generally, the common requirements are:\n\nThe measurement of the electric current can be classified depending upon the underlying fundamental physical principles such as,\n\n\nOhm's Law is the observation that the voltage drop across a resistor is proportional to the current going through it.\nThis relationship can be used to sense currents. Sensors based on this simple relationship are well known for their lower costs, and reliability due to this simple principle.\nThe common and simple approach to current sensing is the use of a shunt resistor. The voltage drop across the shunt is proportional to its current flow. Both alternating currents (AC) and direct currents (DC) can be measured with the shunt resistor.The high performance coaxial shunt have been widely used for many applications fast rise-time transient currents and high amplitudes but, highly integrated electronic devices prefer low-cost surface mounted devices (SMDs), because of their small sizes and relatively low prices.\nThe parasitic inductance present in the shunt affects high precision current measurement. Although this affects only the magnitude of the impedance at relatively high frequency, but also its effect on the phase at line frequency causes a noticeable error at a low power factor. The low cost and high reliability make the low resistance current shunt a very popular choice for current measurement system. The major disadvantage of using the shunt is that fundamentally a shunt is a resistive element, the power loss is thus proportional to the square of the current passing through it and consequently it is a rarity amongst high current measurements.\nFast-response for measuring high-impulse or heavy-surge currents is the common requirement for shunt resistors. In 1981 Malewski, designed a circuit to eliminate the skin effect and later in 1999 the flap-strap sandwich shunt (FSSS) was introduced from a flat-strap sandwich resistor. The properties of the FSSS in terms of response time, power loss and frequency characteristics, are the same as the shunt resistor but the cost is lower and the construction technique is less sophisticated, compared to Malewski and the coaxial shunt.\n\nThe intrinsic resistance of a conducting element, usually a copper trace in Printed circuit Board(PCB) can be used as sensing element instead of a shunt resistor. Since no additional resistor is required this approach promises a low-cost and space saving configuration with no additional power losses either. Naturally, the voltage drop of a copper trace is very low due to its very low resistance, making the presence of a high gain amplifier mandatory in order to get a useful signal.\nThere are several physical effects which may alter the current measurement process: thermal drift of the copper trace, initial conditions of the trace resistance etc. Therefore, this approach is not suitable for applications that require a reasonable accuracy due to the large thermal drift. In order to overcome the problems associated with the temperature drift, a digital controller can be used for thermal drift compensation and calibration of the copper trace.\nA significant drawback of this kind of current sensor is the unavoidable electrical connection between the current to be measured and the sense circuit. By employing a so-called isolation amplifier, electrical isolation can be added. However, these amplifiers are expensive and can also deteriorate the bandwidth, accuracy and thermal drift of the original current sensing technique. For these reasons, current sensing techniques based on physical principles that provide intrinsic electrical isolation deliver a better performance at lower costs in applications where isolation is required.\n\nFaraday's Law of induction – that states: the total electromotive force induced in a closed circuit is proportional to the time rate of change of the total magnetic flux linking the circuit – has been largely employed in current sensing techniques. Two major sensing devices based on Faraday’s law are Current transformers (CTs) and Rogowski coils. These sensors provide an intrinsic electrical isolation between the current to be measured and the output signal, thus making these current sensing devices mandatory, where safety standards demand electrical isolation.\n\nThe CT is based on the principle of a transformer and converts a high primary current into a smaller secondary current and is common among high AC current measurement system. As this device is a passive device, no extra driving circuitry is needed in its implementation. \nAnother major advantage is that it can measure very high current while consuming little power. The disadvantage of the CT is that a very high primary current or a substantial DC component in the current can saturate the ferrite material used in the core ultimately corrupting the signal. Another problem is that once the core is magnetized, it will contain hysteresis and the accuracy will degrade unless it is demagnetized again.\n\nRogowski coil is based on the principle of Faraday’s law of induction and the output voltage V of the Rogowski coil is determined by integrating the current I to be measured. It is given by, \nwhere A is the cross-sectional area of the coil N is the number of turns,\nThe Rogowski coil has a low sensitivity and is due to the absence of a high permeability magnetic core, that the current transformer can take advantage of. However, this can be compensated by adding more turns on the Rogowski coil or using an integrator with a higher gain k. More turns increase the self-capacitance and self-inductance, and higher integrator gain means an amplifier with a large gain-bandwidth product. As always in engineering, trade-offs must be made depending on specific applications.\n\nHall effect sensors are devices based on the Hall-effect, which was discovered by Edwin Hall in 1879 based on the physical principle of the Lorentz force. They are activated by an external magnetic field. In this generalized device, the Hall sensor senses the magnetic field produced by the magnetic system. This system responds to the quantity to be sensed (current, temperature, position, velocity, etc.) through the input interface. The Hall element is the basic magnetic field sensor. It requires signal conditioning to make the output usable for most applications. The signal conditioning electronics needed are an amplifier stage and temperature compensation. Voltage regulation is needed when operating from an unregulated supply. If the Hall voltage is measured when no magnetic field is present, the output should be zero. However, if voltage at each output terminal is measured with respect to ground, a non-zero voltage will appear. This is the common mode voltage (CMV), and is the same at each output terminal. The output interface then converts the electrical signal from the Hall sensor; the Hall voltage: a signal that is significant to the application context. The Hall voltage is a low level signal on the order of 30 μvolts in the presence of one gauss magnetic field. This low-level output requires an amplifier with low noise, high input impedance and moderate gain. A differential amplifier with these characteristics can be readily integrated with the Hall element using standard bipolar transistor technology. Temperature compensation is also easily integrated.\n\nFlux gate sensors or Saturable inductor current sensors work on the same measurement principle as Hall-effect-based current sensors: the magnetic field created by the primary current to be measured is detected by a specific sensing element. The design of the saturable inductor current sensor is similar to that of a closed-loop Hall-effect current sensor; the only difference is that this method uses the saturable inductor instead of the Hall-effect sensor in the air gap.\n\nSaturable inductor current sensor is based on the detection of an inductance change. The saturable inductor is made of small and thin magnetic core wound with a coil around it. The saturable inductor operates into its saturation region. It is designed in such a way that the external and internal flux density will affect its saturation level. Change in the saturation level of a saturable inductor will alter core’s permeability and, consequently, its inductance L. The value of saturable inductance (L) is high at low currents (based on the permeability of the core) and low at high currents (the core permeability becomes unity when saturated). Fluxgate detectors rely on the property of many magnetic materials to exhibit a non-linear relationship between the magnetic field strength H and the flux density B.\n\nIn this technique, high frequency performance is achieved by using two cores without air gaps. One of the two main cores is used to create a saturable inductor and the other is used to create a high frequency transformer effect. In another approach, three cores can be used without air gap. Two of the three cores are used to create saturable inductor, and the third core is used to create a high frequency transformer effect. Advantages of saturable inductor sensors include high resolution, high accuracy, low offset and gain drift, and large bandwidth (up to 500 kHz). Drawbacks of saturable inductor technologies include limited bandwidth for simpler design, relatively high secondary power consumption, and risk of current or voltage noise injection into the primary conductor.\n\nA magneto-resistor (MR) is a two terminal device which changes its resistance parabolically with applied magnetic field. This variation of the resistance of MR due to the magnetic field is known as the Magnetoresistive Effect. It is possible to build structures in which the electrical resistance varies as a function of applied magnetic field. These structures can be used as magnetic sensors. Normally these resistors are assembled in a bridge configuration to compensate for thermal drift.\nPopular magneto resistance-based sensors are: Anisotropic Magneto Resistance (AMR), Giant Magneto Resistance (GMR), Giant Magneto Impendence (GMI) and Tunnel Magneto Resistance (TMR). All these MR-based sensors have higher sensitivity compared to Hall-effect sensors. Despite this, these sensors (GMR, CMR, and TMR) are still more expensive than Hall-effect devices, have serious drawbacks related with nonlinear behavior, distinct thermal drift, and a very strong external field can permanently alter the sensor behavior (GMR). GMI and TMR sensors are even more sensitive than GMR based sensors, but still in the test phase and no commercial products are available as of 2016-06.\n\n\n"}
{"id": "53647399", "url": "https://en.wikipedia.org/wiki?curid=53647399", "title": "Danube Program", "text": "Danube Program\n\nThe Danube Program (Romanian: \"Programul Dunărea\") was a secret Romanian military project to develop their own nuclear weapons. The project began in 1981, and lasted until 1989.\n\nIn 1970 Romania ratified the Nuclear Non-Proliferation Treaty, which banned them from developing and building their own nuclear weapons. However Nicolae Ceaușescu started the Danube Program in secret.\n\nFrom 1981 until 1989, the Danube Program was active, using nuclear reactors to create plutonium out of Highly Enriched Uranium, for the purpose of creating a nuclear bomb. In 1992, after the Romanian Revolution, the newly established Romanian Republic reported the infraction to the International Atomic Energy Agency voluntarily, who then reported it to the UN Security Council.\n\nThe Danube Program acquired Highly Enriched Uranium (HEU) from multiple sources, most notably America's Atoms for Peace program, which gave highly enriched uranium to many countries. The project made use of a nuclear reactor, that had been given to them by the United States, to create plutonium from the HEU. Although the project succeeded in creating plutonium, it did not actually construct any nuclear bombs, although it is estimated that with the materials the project had, they could have made up to 240 plutonium bombs, assuming that of plutonium would be used for every bomb.\n"}
{"id": "13931137", "url": "https://en.wikipedia.org/wiki?curid=13931137", "title": "Dracontomelon", "text": "Dracontomelon\n\nDracontomelon (Vietnamese:Chi Sấu) is a genus of flowering plants in the family Anacardiaceae.\n\nThe most commonly eaten species is \"Dracontomelon duperreanum\", which produces an edible fruit that is eaten in Cambodia, Vietnam and China. In Vietnamese, the plant is called \"\" and is a common urban tree in Hanoi; the fruit is called \"quả sấu\". The fruit is used in Vietnamese cuisine both as a souring agent and a candied treat similar to the Japanese umeboshi. The treat is popular among youths. In Chinese, the fruit is called 仁面.\n\n\nThe following are known synonyms:\n\n"}
{"id": "24994958", "url": "https://en.wikipedia.org/wiki?curid=24994958", "title": "Earth orientation parameters", "text": "Earth orientation parameters\n\nIn geodesy, earth orientation parameters (EOP) are a collection of parameters that describe irregularities in the rotation of the Earth.\n\nThe Earth's rotational velocity is not constant over time. Any motion of mass in or on the Earth causes a slowdown or speedup of the rotation speed, or a change of rotation axis. Small motions produce changes too small to be measured, but movements of very large mass, like sea currents or tides, can produce discernible changes in the rotation and can change very precise astronomical observations. Global simulations of atmosphere, ocean, and land dynamics are used to create effective angular momentum (EAM) functions that can be used to predict changes in EOP.\n\nThe collection of earth orientation parameters is fitted to describe the observed rotation irregularities. Technically, they provide the rotational transform from the International Terrestrial Reference System (ITRS) to the International Celestial Reference System (ICRS), or vice versa, as a function of time.\n\nUniversal time (UT1) tracks the Earth's rotation in time, which performs one revolution in about 24 hours. The Earth's rotation is uneven, so UT is not linear with respect to atomic time. It is practically proportional to the sidereal time, which is also a direct measure of Earth rotation. The excess revolution time is called length of day (LOD).\n\nDue to the very slow pole motion of the Earth, the Celestial Ephemeris Pole (CEP, or celestial pole) does not stay still on the surface of the Earth. The Celestial Ephemeris Pole is calculated from observation data, and is averaged, so it differs from the instantaneous rotation axis by quasi-diurnal terms, which are as small as under 0.01\" (see ). In setting up a coordinate system, a static terrestrial point called the IERS Reference Pole, or IRP, is used as the origin; the x-axis is in the direction of IRM, the IERS Reference Meridian; the y-axis is in the direction 90 degrees West longitude. x and y are the coordinates of the CEP relative to the IRP.\n\nCelestial pole offsets are described in the IAU Precession and Nutation models. The observed differences with respect to the conventional celestial pole position defined by the models are monitored and reported by the IERS.\n\n"}
{"id": "7978633", "url": "https://en.wikipedia.org/wiki?curid=7978633", "title": "Earthquake valve", "text": "Earthquake valve\n\nAn earthquake valve (or seismic valve) is an automatic method to shut off the low pressure regulated gas supply to a structure during a major earthquake and/or if a pipe is broken. These are applicable both to utility-supplied natural gas and to gas from liquefied petroleum gas (LPG). These small devices are installed on the property gas meter (usually between the utility company's metered installation and the structure piping) and are designed to instantly stop the natural gas supply in order to protect the structure if a gas leak or line break occurs during an earthquake.\n\nFires or explosions due to gas line breaks can be more damaging than the actual earthquake itself. Gas supply companies recommend that the gas supply be cut off immediately if there is a smell of gas after an earthquake; if nobody is in place to do this, an unattended earthquake valve will instantly cut off the gas.\n\nTwo types of valve are commonly employed: one is sensitive to motion and the other to excessive gas flow. One of each type can be connected sequentially for maximum reliability.\n\nA metal ball is retained away from an orifice by sitting upon a ring. Any shaking of the mechanism will cause the ball to roll off its ledge and fall down to block the orifice. It is reset using either an external magnetic device or an internal lift mechanism. If it is too sensitive, it may be triggered by normal vibrations such as passing vehicles. After a severe seismic event, gas piping may be damaged, requiring complete re-inspection for breaks or leaks.\n\nA valve is closed when the flow exceeds a certain limit that is appropriate to the application. This will only operate when a pipe is broken and there is significant leakage. It may not operate in case of a small, though still dangerous, leak.\n\n"}
{"id": "27694293", "url": "https://en.wikipedia.org/wiki?curid=27694293", "title": "Electricity North West", "text": "Electricity North West\n\nElectricity North West is a British electricity distribution network operator, responsible for the administration and maintenance of the network, that distributes electricity throughout the North West of England.\n\nElectricity North West Limited is a private limited company registered in England and Wales. The company is ultimately owned by long term infrastructure funds managed by Colonial First State Global Asset Management (a member of the Commonwealth Bank of Australia Group) and by JP Morgan Investment Management Inc.\n\nThe principal activity of the group is the distribution of electricity in North West England on behalf of the electricity supply companies. Customers receive their electricity bill from their suppliers who pay for use of the electricity network. Electricity North West delivers electricity to 5 million people in 2.4 million properties throughout the North West. \n\nThe distribution network had originated as NORWEB (from 1948 to 1990, the North West Electricity Board).\nOn 19 December 2007, United Utilities Group plc sold United Utilities Electricity Limited to North West Electricity Networks (Jersey) Limited, a company advised by Colonial First State Global Asset Management and the Infrastructure Investment Group, which is advised by JP Morgan Asset Management.\n\nUnited Utilities operated and maintained North West England's electricity network on behalf of Electricity North West Limited, until June 2010. On 30 June 2010, the company completed the purchase of United Utilities Electricity Services Limited (‘UUES’) from United Utilities Group PLC (‘UU’). \n\nThe purchase of UUES, which had previously been contracted to operate and maintain the network, established one group which owns, operates, manages and maintains its network. UUES was subsequently renamed Electricity North West Services Limited (‘ENWSL’).\n\nAs the distribution network operator for North West England, Electricity North West owns and is responsible for the construction and maintenance of the network that distributes electricity throughout the region. This includes the inspection and maintenance of assets which include 13,000 km of overhead lines, 43,000 km of underground cables, and 38,000 transformers.\n"}
{"id": "1388228", "url": "https://en.wikipedia.org/wiki?curid=1388228", "title": "Electromagnetic forming", "text": "Electromagnetic forming\n\nElectromagnetic forming (EM forming or magneforming) is a type of high velocity, cold process for electrically conductive metals, most commonly copper and aluminium. The workpiece is reshaped by high intensity pulsed magnetic fields that induce a current in the workpiece and a corresponding repulsive magnetic field, rapidly repelling portions of the workpiece. The workpiece can be reshaped without any contact from a tool, although in some instances the piece may be pressed against a die or former. The technique is sometimes called \"high velocity forming\" or \"electromagnetic pulse technology\".\n\nA special coil is placed near the metallic workpiece, replacing the pusher in traditional forming. When the system releases its intense magnetic pulse , the coil generates a magnetic field which in turn accelerates the workpiece to hyper speed and onto the die.\nThe magnetic pulse and the extreme deformation speed transforms the metal into a visco-plastic state – increasing formability without affecting the native strength of the material magnetic pulse forming illustration.\n\nA rapidly changing magnetic field induces a circulating electric current within a nearby conductor through electromagnetic induction. The induced current creates a corresponding magnetic field around the conductor (see Pinch (plasma physics)). Because of Lenz's Law, the magnetic fields created within the conductor and work coil strongly repel each other.\n\nIn practice the metal workpiece to be fabricated is placed in proximity to a heavily constructed coil of wire (called the \"work coil\"). A huge pulse of current is forced through the work coil by rapidly discharging a high voltage capacitor bank using an ignitron or a spark gap as a switch. This creates a rapidly oscillating, ultrastrong electromagnetic field around the work coil.\n\nThe high work coil current (typically tens or hundreds of thousands of amperes) creates ultrastrong magnetic forces that easily overcome the yield strength of the metal work piece, causing permanent deformation. The metal forming process occurs extremely quickly (typically tens of microseconds) and, because of the large forces, portions of the workpiece undergo high acceleration reaching velocities of up to 300 m/s.\n\nThe forming process is most often used to shrink or expand cylindrical tubing, but it can also form sheet metal by repelling the work piece onto a shaped die at a high velocity. High-quality joints can be formed, either by electromagnetic pulse crimping with a mechanical interlock or by electromagnetic pulse welding with a true metallurgical weld. Since the forming operation involves high acceleration and deceleration, mass of the work piece plays a critical role during the forming process. The process works best with good electrical conductors such as copper or aluminum, but it can be adapted to work with poorer conductors such as steel.\n\nElectromagnetic forming has a number of advantages and disadvantages compared to conventional mechanical forming techniques.\n\nSome of the advantages are;\n\nThe principle disadvantages are;\n\n\n"}
{"id": "15344883", "url": "https://en.wikipedia.org/wiki?curid=15344883", "title": "Envipco", "text": "Envipco\n\nEnvipco (\"Environmental Products Corporation\") is a global recycling company. Envipco's corporate headquarters are located in Naugatuck, Connecticut. \nEnvipco USA is an American manufacturer and distributor of reverse vending machines and other reverse vending solutions for all size clients - from convenience stores to Redemption Centers and everything in between.\n\nEnvipco products include Reverse vending machines and customized solutions for its clients. Reverse vending machines (RVMs) collect, compact and sort customers' empty beverage containers and, where applicable, issue a voucher redeemable for cash. The Envipco RVMs can also have interactive advertising, couponing and integration with Retailer POS.\nEnvipco's reverse vending machines differ based on the market in which they appear. In deposit-and-return markets, the reverse vending machines use bar code and/or camera technology to identify the product being recycled. In non-deposit markets the reverse vending machines identify the product as being PET or aluminum.In the United States, Envipco machines are most common in the 10 states that require bottle deposits. Envipco's products can be found around the globe in both deposit and non-deposit markets - including USA, Canada, the UK, France, Sweden, Greece, Cyprus, Australia and Japan. \nEnvipco's current flagship product is the Quantum bulk-feed reverse vending machine. The Quantum is the first successful bulk-feed RVM in the industry - a quantum leap in the consumer experience, allowing emptying of entire bags of containers into the machine (no feeding one container at a time). It is the most efficient return of containers in quantities processing over 100 cans and plastic bottles/ minute.\nThe Ultra 48 and HDS product lines are ADA compliant and the mainstay for high volume, single-feed RVMs in the Deposit Market generating a cash voucher for a refund of the deposit paid on the container. \nFor smaller shops and stores, the Flex product is the right model. It is the smallest footprint RVM in Envipco's product portfolio with the lowest price tag. This RVM is best suited for convenience stores, pharmacies, gas stations and other venues where container returns are typically low volume.\nIn states that do not have bottle deposits, as well as outside of the United States, reverse vending machines can generate coupons, prizes or vouchers for donations to schools.\n\nEnvipco's products are also featured in Recycling Centers. The Quantum Outdoor RVM itself can be installed in less than 3 standard parking spaces. It allows storage of approximately 60,000 containers before it needs to be emptied. Envipco's Ultra 48 and Ultra HDS are also featured in outdoor kiosks - especially prolific in the Greek (non-deposit) and Australian (deposit) markets.\n"}
{"id": "51493094", "url": "https://en.wikipedia.org/wiki?curid=51493094", "title": "FAB Link", "text": "FAB Link\n\nFAB Link is a proposed France-Alderney-Britain cable link. A European project, part of a series of grid ‘interconnectors’ between mainland Europe and UK, part of the High-voltage direct current (HVDC) power interconnector system.\n\nIn 2016 only one link (IFA 2000MW) connects France with the United Kingdom and one (Britned 1000MW) from the Netherlands to the United Kingdom.\n\nThe project is being jointly developed by Réseau de Transport d'Électricité (RTE) and FAB Link Limited.\n\nTwin 700 MW cables giving a capacity of 1,400 MW.\n\nAccording to plans, 25 km AC land cables in France, landfall will be made at Siouville-Hague, 30 km HVDC submarine cables between France and Alderney. 1 km land cables across Alderney, 140 km HVDC submarine cables with landfall at Budleigh Salterton with 20 km HVDC land cables to a converter station planned to be located near Exeter Airport with some AC land cables to connect to the grid in England. \n\nLand cables carry 400 Kv AC Voltage, The submarine voltage will be 320 Kv DC Voltage.\n\nVoltage Sourced Converters (VSC) are the preferred converter technology.\n\nThe cost is estimated at €750m.\n\nThe FAB project has received funding from the European Commission through the Connecting Europe Facility of €7.235m\n\nIn January 2017 applications for licences from the Marine Management Organisation, East Devon District Council and the States of Guernsey were submitted.\n\nApproval for the project is expected after Ofgem give their final project assessment decision in July 2017 with the final investment decision made in late 2017.\n\nThe project is scheduled to commence construction in 2018 with a proposed completion by 2022.\n\nThe FAB Link in Alderney is a set of cables, each 5 inches in diameter, buried 1–2 metres deep crossing Alderney at Longis Common. It connects to the undersea cables from France to Alderney and from Alderney to England.\n\nFAB Link Limited is a joint venture between Alderney Renewable Energy Limited and Transmission Investment LLP. Transmission Investment is a leading independent transmission business spanning origination, project development, acquisition management, financial structuring and asset management.\n\nFAB will facilitate the export of electricity from Tidal Turbines in Alderney’s waters.\n\nSubmarine power cables need to incorporate fibre optic cables to monitor performance, which gives an excess capacity that can be used for high-speed communications.\n\nAlderney would receive rent of around £70,000 per annum for the facilities they provide together with a possible reduction in electricity prices paid by consumers.\n\nUnresolved conflicts about the appropriateness of this project to the island resulted in the delaying of controversial planning law changes in 2016. The unprecedented change of law would open the area known as the \"Green Belt\" to industrial utility development, granting FAB Link permission to lay cables across a rural part of Alderney. The retention of the preservation laws would result in the cables having to by-pass Alderney altogether.\n\nSome Alderney residents believe the character of Alderney would be ruined, damaging the island's seasonal tourism industry; protest demonstrations took place in 2016 and in July 2017 to this end. Others believe the link will assist future tidal turbine projects and financially benefit construction and energy production contractors.\n\n\n"}
{"id": "28602593", "url": "https://en.wikipedia.org/wiki?curid=28602593", "title": "Fyn Power Station", "text": "Fyn Power Station\n\nThe Fyn Power Station () is a coal, straw and municipal waste-fired power station operated by Vattenfall in Odense, Denmark. It has eight units, three of which were operating : unit 3, unit 7, and unit 8. Unit 3 has a power of 235 MW (coal), unit 7 of 362 MW (coal), unit 8 of 35 MW biomass), and Odense CHP plant 24 MW. Unit 7 has a tall chimney, which is the second-tallest in Denmark, unit 3 a tall chimney.\n\n\n"}
{"id": "2710659", "url": "https://en.wikipedia.org/wiki?curid=2710659", "title": "HE0450-2958", "text": "HE0450-2958\n\nHE0450-2958 is an unusual quasar. It has been called the \"naked quasar\" and the \"quasar without a home\" because it appears to lack a host galaxy. It is estimated to lie approximately one billion parsecs away.\n\nA team of researchers led by Pierre Magain of the Universite de Liege, Belgium announced their findings in the September 14, 2005 issue of the journal \"Nature\". The quasar lies close in the sky to a disturbed, starburst galaxy (see figure, upper left). However, no galaxy was seen around the quasar itself (figure, middle), leading the authors to speculate\n\nOne might suggest that the host galaxy has disappeared from our view as a result of the collision [which formed the disturbed galaxy], but it is hard to imagine how the complete disruption of a galaxy could happen.\n\nIn order for the quasar's host galaxy to have escaped detection, Magain et al. estimated that it would need to be approximately five magnitudes (100 times) dimmer than expected for such a quasar, or to have a radius of 300 light-years or less (typical quasars are embedded in galaxies 5000 to 50,000 light-years across).\n\nShortly after Magain et al.'s paper was published, three theoretical papers appeared, all in the week of November 6, 2005, which claimed to explain the peculiar properties of this object. Two of the papers—from groups in Cambridge, Massachusetts and Cambridge, England —suggested that the quasar was a supermassive black hole that had been ejected from the center of the nearby, disturbed galaxy, either by gravitational radiation recoil or by an interaction involving three black holes. The ejection velocity would have to be approximately 1000 km/s in order to place the quasar so far from its original host galaxy.\n\nThe third paper, from a team led by David Merritt, critically examined the ejection hypothesis and concluded that it could not be correct. The two main arguments were: (1) The quasar spectrum reveals it to be a narrow-line Seyfert 1 galaxy. NLS1's are believed to have abnormally small black holes; since black hole size is strongly correlated with galaxy size, the host galaxy of the quasar should also be abnormally small, explaining why it had not been detected by Magain et al. (2) The quasar spectrum also reveals the presence of a classic, narrow emission line region (NLR). The gas producing the narrow lines lies roughly a thousand light-years from the black hole, and such gas could not remain bound to the black hole following a kick large enough to remove it from its host galaxy. These authors concluded that the \"naked\" quasar was in fact a perfectly normal, narrow-line Seyfert galaxy that happened to lie close on the sky to a disturbed galaxy.\n\nA number of scientific studies since 2005 have supported this conclusion. (1) Kim et al. (2006) made a more careful attempt to find the quasar's host galaxy. They concluded that it was impossible to rule out the presence of a galaxy given the confusing light from the quasar. (2) Zhou et al. (2007) observed the X-ray emission from the quasar and used it to estimate the mass of the black hole. They confirmed a small mass for the black hole, implying an even fainter host galaxy than predicted by Merritt et al. (3) Feain et al. (2007) detected radio emission from the quasar, which they interpreted as indicating ongoing star formation, which \"contradicts any suggestion that this is a 'naked' quasar'\".\n\nThe current scientific consensus is that HE0450-2958 probably does have a host galaxy but that it is difficult to see behind the bright quasar light.\n\nRecently, the consensus has been questioned after a European Southern Observatory study.\n\n\n"}
{"id": "44277627", "url": "https://en.wikipedia.org/wiki?curid=44277627", "title": "High energy density physics", "text": "High energy density physics\n\nHigh-energy-density physics (HEDP) is a new subfield of physics intersecting nuclear physics, astrophysics and plasma physics.\nIt has been defined as the physics of matter and radiation at energy densities in excess of about 100 GJ/m^3.\n\n"}
{"id": "25119101", "url": "https://en.wikipedia.org/wiki?curid=25119101", "title": "History of X-ray astronomy", "text": "History of X-ray astronomy\n\nThe history of X-ray astronomy begins in the 1920s, with interest in short wave communications for the U.S. Navy. This was soon followed by extensive study of the earth's ionosphere. By 1927, interest in the detection of X-ray and ultraviolet (UV) radiation at high altitudes inspired researchers to launch Goddard's rockets into the upper atmosphere to support theoretical studies and data gathering. The first successful rocket flight equipped with instrumentation able to detect solar ultraviolet radiation occurred in 1946. X-ray solar studies began in 1949. By 1973 a solar instrument package orbited on Skylab providing significant solar data.\n\nIn 1965 the Goddard Space Flight Center program in X-ray astronomy was initiated with a series of balloon-borne experiments. In the 1970s this was followed by high altitude sounding rocket experiments, and that was followed by orbiting (satellite) observatories.\n\nThe first rocket flight to successfully detect a cosmic source of X-ray emission was launched in 1962 by a group at American Science and Engineering (AS&E).\n\nX-ray wavelengths reveal information about the bodies (sources) that emit them.\n\nThe Naval Research Laboratory (NRL) opened in 1923. After E.O. Hulburt (1890-1982) arrived there in 1924 he studied physical optics. The NRL was conducting research on the properties of the ionosphere (\"Earth's reflecting layer\") because of interest in short wave radio communications. Hubert (Hulburt ?) produced a series of mathematical descriptions of the ionosphere during the 1920s and 1930s. In 1927, at the Carnegie Institution of Washington, Hulburt, Gregory Breit and Merle Tuve explored the possibility of equipping Robert Goddard's rockets to explore the upper atmosphere. In 1929 Hulburt proposed an experimental program in which a rocket might be instrumented to explore the upper atmosphere. This proposal included detection of ultraviolet radiation and X rays at high altitudes.\n\nHerbert Friedman began X-ray solar studies in 1949 and soon reported that the energy of \"the solar X-ray spectrum ... is adequate to account for all of E-layer ionization.\" Thus one of Hulburt's original questions, the source and behavior of the radio-reflecting layer, began to find its answer in space research.\n\nAt the end of the 1930s other studies included the inference of an X-ray corona by optical methods and, in 1949, more direct evidence by detecting X-ray photons.\n\nBecause the Earth's atmosphere blocks X-rays at ground level, Wilhelm Röntgen's discovery had no effect on observational astronomy for the first 50 years. X-ray astronomy became possible only with the capability to use rockets that far exceeded the altitudes of balloons. In 1948 U.S. researchers used a German-made V-2 rocket to gather the first records of solar x-rays.\n\nThe NRL has placed instruments in rockets, satellites, Skylab, and Spacelab 2\n\nThrough the 1960s, 70s, 80s, and 90s, the sensitivity of detectors increased greatly during the 60 years of X-ray astronomy. In addition, the ability to focus X-rays has developed enormously—allowing the production of high-quality images. \n\nThe study of astronomical objects at the highest energies of X-rays and gamma rays began in the early 1960s. Before then, scientists knew only that the Sun was an intense source in these wavebands. Earth's atmosphere absorbs most X-rays and gamma rays, so rocket flights that could lift scientific payloads above Earth's atmosphere were needed. The first rocket flight to successfully detect a cosmic source of X-ray emission was launched in 1962 by a group at American Science and Engineering (AS&E). The team of scientists on this project included Riccardo Giacconi, Herbert Gursky, Frank Paolini, and Bruno Rossi. This rocket flight used a small X-ray detector, which found a very bright source they named Scorpius X-1, because it was the first X-ray source found in the constellation Scorpius.\n\nIn the 1970s, dedicated X-ray astronomy satellites, such as Uhuru, Ariel 5, SAS-3, OSO-8 and HEAO-1, developed this field of science at an astounding pace. Scientists hypothesized that X-rays from stellar sources in our galaxy were primarily from a neutron star in a binary system with a normal star. In these \"X-ray binaries,\" the X-rays originate from material traveling from the normal star to the neutron star in a process called accretion. The binary nature of the system allowed astronomers to measure the mass of the neutron star. For other systems, the inferred mass of the X-ray emitting object supported the idea of the existence of black holes, as they were too massive to be neutron stars. Other systems displayed a characteristic X-ray pulse, just as pulsars had been found to do in the radio regime, which allowed a determination of the spin rate of the neutron star.\n\nFinally, some of these galactic X-ray sources were found to be highly variable. In fact, some sources would appear in the sky, remain bright for a few weeks, and then fade again from view. Such sources are called X-ray transients. The inner regions of some galaxies were also found to emit X-rays. The X-ray emission from these active galactic nuclei is believed to originate from ultra-relativistic gas near a very massive black hole at the galaxy's center. Lastly, a diffuse X-ray emission was found to exist all over the sky.\n\nThe study of X-ray astronomy continued to be carried out using data from a host of satellites that were active from the 1980s to the early 2000s: the HEAO Program, EXOSAT, Ginga, RXTE, ROSAT, ASCA, as well as BeppoSAX, which detected the first afterglow of a gamma-ray burst (GRB). Data from these satellites continues to aid our further understanding of the nature of these sources and the mechanisms by which the X-rays and gamma rays are emitted. Understanding these mechanisms can in turn shed light on the fundamental physics of our universe. By looking at the sky with X-ray and gamma-ray instruments, we collect important information in our attempt to address questions such as how the universe began and how it evolves, and gain some insight into its eventual fate.\n\nIn 1965, at the suggestion of Frank McDonald, Elihu Boldt initiated Goddard's program in X-ray astronomy with a series of balloon-borne experiments. At an early stage he was joined by Peter Serlemitsos, who had just completed his PhD space physics thesis on magnetospheric electrons, and by Guenter Riegler, a University of Maryland physics graduate student interested in doing his dissertation research in astrophysics.\n\nFrom 1965 to 1972 there were over a dozen balloon-borne experiments (mostly from New Mexico), including the first such to take place from Australia (1966), one in which hard X-ray emission was discovered (albeit with crude angular resolution) from a region towards the Galactic Center whose centroid is located among subsequently identified sources GX1+4, GX3+1, and GX5-1. A balloon-borne experiment in 1968 was based on the multi-anode multi-layer xenon gas proportional chamber that had recently been developed in our lab and represented the first use of such a high performance instrument for X-ray astronomy.\n\nDue to the attenuation of soft X-rays by the residual atmosphere at balloon altitudes these early experiments were restricted to energies above ~20 keV. Observations down to lower energies were begun with a series of high altitude sounding rocket experiments; by this stage Steve Holt had already joined the program. A 1972 rocket-borne observation of Cas A, the youngest supernova remnant in our galaxy, yielded the first detection of an X-ray spectral line, iron K-line emission at ~7 keV.\n\nThe figure to the right shows 15-second samples of the raw counts (per 20.48ms) observed in a 1973 sounding-rocket-borne exposure to three of the X-ray brightest binary sources in our galaxy: Her X-1 (1.7 days), Cyg X-3 (0.2 day), and Cyg X-1 (5.6 days). The 1.24 second pulsar period associated with Her X-1 is immediately evident from the data, while the rate profile for Cyg X-3 is completely consistent with the statistical fluctuations in counts expected for a source that is constant, at least for the 15s duration of the exposure shown; the Cyg X-1 data, on the other hand, clearly exhibit the chaotic \"shot noise\" behavior characteristic of this black-hole candidate and also provided preliminary evidence for the additional feature of millisecond \"burst\" sub-structure, noted for the first time in this observation. The sharp cut-off at ~24 keV in the flat spectrum observed for Her X-1 in this exposure provided the first reported evidence for radiative transfer effects to be associated with a highly magnetized plasma near the surface of a neutron star. The black-body spectral component observed for Cyg X-3 during this experiment gave strong evidence that this emission is from the immediate vicinity of a compact object the size of a neutron star.\n\nAn observation of Cyg X-3 a year later with the same instrument yielded an optically thin thermal spectrum for this source and provided the first evidence for strong spectral iron K-line emission from an X-ray binary.\n\nOur large area PCA (Proportional Counter Array) on the current RXTE (Rossi X-ray Timing Explorer) mission genuinely reflects the heritage of our sounding rocket program. RXTE continues to provide very valuable data as it enters the second decade of successful operation. \nGoddard's ASM (All-Sky Monitor) pin-hole X-ray camera on Ariel-5 (1974-1980) was the first X-ray astronomy experiment to use imaging proportional counters (albeit one-dimensional); it provided information on transient sources and the long-term behavior of several bright objects. Jean Swank joined the program in time for the beginning of our OSO-8 experiment (1975-1978), the first broadband (2-40 keV) orbiting observatory based on multi-anode multi-layer proportional chambers, one that showed the power of X-ray spectroscopy; for example, it established that iron K-line emission is a ubiquitous feature of clusters of galaxies.\n\nThe HEAO-1 A2 full-sky cosmic X-ray experiment (1977-1979) provided the most comprehensive data (still the most definitive) on the cosmic X-ray background broadband spectrum and large-scale structure, and a much used complete sample of the brightest extragalactic sources; it posed the challenging \"spectral paradox\" just now being unraveled with new results on evolution (from deep surveys) and on individual source spectra extending into the gamma-ray band. The SSS (Solid State Spectrometer) at the focus of the HEAO-2 Einstein Observatory (1978-1981) grazing incidence telescope was the first high spectral resolution non-dispersive spectrometer to be used for X-ray astronomy, here for energies up to ~3 keV, limited by the telescope optics.\n\nBy the use of conical foil optics, developed in our lab, the response of a grazing incidence X-ray telescope was extended to 12 keV, amply covering the crucial iron K-band of emission. A cooled Si(Li) solid state detector was used at the focus of such a telescope for the BBXRT (Broad Band X-Ray Telescope) on the Astro-1 shuttle mission (STS-35) on Columbia in December 1990, the first broadband (0.3-12keV) X-ray observatory to use focusing optics.\n\nIn collaboration with X-ray astronomers in Japan, Goddard supplied conical foil X-ray optics have been used for the joint Japanese and American ASCA mission (1993-2000). It was the first broadband imaging observatory using CCD non-dispersive spectrometers.\n\nSubstantial improvement in the capability of solid-state non-dispersive spectrometers has been achieved in our lab (in collaboration with the University of Wisconsin) by the successful development of quantum calorimeters with resolution better than 10 eV (FWHM). Such spectrometers have been used in a sounding-rocket-borne experiment to study spectral lines from the hot interstellar medium of our galaxy and will soon play a major role in the joint Japanese/American Suzaku orbiting X-ray observatory launched in July 2005.\n\nThe critical early stages of this program benefited from highly dedicated technical support by Dale Arbogast, Frank Birsa, Ciro Cancro, Upendra Desai, Henry Doong, Charles Glasser, Sid Jones, and Frank Shaffer. More than 20 graduate students (mostly from the University of Maryland at College Park) have successfully carried out their PhD dissertation research within our X-ray astronomy program. Almost all of these former students have remained actively involved with astrophysics.\n\nThe beginning of the search for X-ray sources from above the Earth's atmosphere was on August 5, 1948 12:07 GMT. A US Army V-2 as part of Project Hermes was launched from White Sands Proving Grounds Launch Complex (LC) 33. In addition to carrying experiments of the US Naval Research Laboratory for cosmic and solar radiation, temperature, pressure, ionosphere, and photography, there was on board a solar X-ray test detector, which functioned properly. The missile reached an apogee of 166 km.\n\nAs part of a collaboration between the US Naval Research Laboratory (NRL) and the Signal Corps Engineering Laboratory (SCEL) of the University of Michigan, another V-2 (V-2 42 configuration) was launched from White Sands LC33 on December 9, 1948 at 16:08 GMT (09:08 local time). The missile reached an apogee of 108.7 km and carried aeronomy (winds, pressure, temperature), solar X-ray and radiation, and biology experiments.\n\nOn January 28, 1949, an NRL X-ray detector (Blossom) was placed in the nose cone of a V-2 rocket and launched at White Sands Missile Range in New Mexico. X-rays from the Sun were detected. Apogee: 60 km.\n\nA second collaborative effort (NRL/SCEL) using a V-2 UM-3 configuration launched on April 11, 1949 at 22:05 GMT. Experiments included solar X-ray detection, apogee: 87.4 km.\n\nNRL Ionosphere 1 solar X-ray, ionosphere, meteorite mission launched a V-2 on September 29, 1949 from White Sands at 16:58 GMT and reached 151.1 km.\n\nUsing V-2 53 configuration a solar X-ray experiment was launched on February 17, 1950 from White Sands LC 33 at 18:01 GMT reaching an apogee of 148 km.\n\nThe last V-2 launch number TF2/TF3 came on August 22, 1952 07:33 GMT from White Sands reaching an apogee of 78.2 km and carried experiments\n\nThe first successful launch of an Aerobee occurred on May 5, 1952 13:44 GMT from White Sands Proving Grounds launch complex LC35. It was an Aerobee RTV-N-10 configuration reaching an apogee of 127 km with NRL experiments for solar X-ray and ultraviolet detection.\n\nOn April 19, 1960, an Office of Naval Research Aerobee Hi made a series of X-ray photographs of the Sun from an altitude of 208 km. The mainstay of the US IGY rocket stable was the Aerobee Hi, which was modified and improved to create the Aerobee 150.\n\nAn Aerobee 150 rocket launched on June 12, 1962 detected the first X-rays from other celestial sources (Scorpius X-1).\n\nStarting on June 21, 1959 from Kapustin Yar, with a modified V-2 designated the R-5V, the USSR launched a series of four vehicles to detect solar X-rays: a R-2A on July 21, 1959 and two R-11A at 02:00 GMT and 14:00 GMT.\n\nThe British Skylark was probably the most successful of the many sounding rocket programs. The first launched in 1957 from Woomera, Australia and its 441st and final launch took place from Esrange, Sweden on 2 May 2005. Launches were carried out from sites in Australia, Europe, and South America, with use by NASA, the European Space Research Organisation (ESRO), and German and Swedish space organizations. Skylark was used to obtain the first good-quality X-ray images of the solar corona.\n\nThe first X-ray surveys of the sky in the Southern Hemisphere were provided by Skylark launches. It was also used with high precision in September and October 1972 in an effort to locate the optical counterpart of X-ray source GX3+1 by lunar occultation.\n\nThe French Véronique was successfully launched on April 14, 1964 from Hammaguira, LC Blandine carrying experiments to measure UV and X-ray intensities and the FU110 to measure UV intensity from the atomic H (Lyman-α) line, and again on November 4, 1964.\n\nThe SOLar RADiation satellite program (SOLRAD) was conceived in the late 1950s to study the Sun's effects on Earth, particularly during periods of heightened solar activity. Solrad 1 was launched on June 22, 1960 aboard a Thor Able from Cape Canaveral at 1:54 a.m. EDT. As the world's first orbiting astronomical observatory, SOLRAD I determined that radio fade-outs were caused by solar X-ray emissions.\n\nThe first in a series of 8 successfully launched Orbiting Solar Observatories (OSO 1, launched on March 7, 1963) had as its primary mission to measure solar electromagnetic radiation in the UV, X-ray, and gamma-ray regions.\n\nThe first USA satellite which detected cosmic X-rays was the Third Orbiting Solar Observatory, or OSO-3, launched on March 8, 1967. It was intended primarily to observe the Sun, which it did very well during its 2-year lifetime, but it also detected a flaring episode from the source Sco X-1 and measured the diffuse cosmic X-ray background.\n\nOSO 5 was launched on January 22, 1969, and lasted until July 1975. It was the 5th satellite put into orbit as part of the Orbiting Solar Observatory program. This program was intended to launch a series of nearly identical satellites to cover an entire 11-year solar cycle. The circular orbit had an altitude of 555 km and an inclination of 33°. The spin rate of the satellite was 1.8 s. The data produced a spectrum of the diffuse background over the energy range 14-200 keV.\n\nOSO 6 was launched on August 9, 1969. Its orbital period was ~95 min. The spacecraft had a spin rate of 0.5 rps. On board was a hard X-ray detector (27-189 keV) with a 5.1 cm NaI(Tl) scintillator, collimated to 17° × 23° FWHM. The system had 4 energy channels (separated 27-49-75-118-189 keV). The detector spun with the spacecraft on a plane containing the Sun direction within ± 3.5°. Data were read with alternate 70 ms and 30 ms integrations for 5 intervals every 320 ms.\n\nTD-1A was put in a nearly circular polar sun-synchronous orbit, with apogee 545 km, perigee 533 km, and inclination 97.6°. It was ESRO's first 3-axis stabilized satellite, with one axis pointing to the Sun to within ±5°. The optical axis was maintained perpendicular to the solar pointing axis and to the orbital plane. It scanned the entire celestial sphere every 6 months, with a great circle being scanned every satellite revolution. After about 2 months of operation, both of the satellite's tape recorders failed. A network of ground stations was put together so that real-time telemetry from the satellite was recorded for about 60% of the time. After 6 months in orbit, the satellite entered a period of regular eclipses as the satellite passed behind the Earth—cutting off sunlight to the solar panels. The satellite was put into hibernation for 4 months, until the eclipse period passed, after which systems were turned back on and another 6 months of observations were made. TD-1A was primarily a UV mission however it carried both a cosmic X-ray and a gamma-ray detector. TD-1A reentered on January 9, 1980.\n\nOSO 7 was primarily a solar observatory designed to point a battery of UV and X-ray telescopes at the Sun from a platform mounted on a cylindrical wheel. The detectors for observing cosmic X-ray sources were X-ray proportional counters. The hard X-ray telescope operated over the energy range 7 - 550 keV. OSO 7 performed an X-ray All-sky survey and discovered the 9-day periodicity in Vela X-1 which led to its optical identification as a HMXRB. OSO 7 was launched on September 29, 1971 and operated until May 18, 1973.\n\nSkylab, a science and engineering laboratory, was launched into Earth orbit by a Saturn V rocket on May 14, 1973. Detailed X-ray studies of the Sun were performed. The S150 experiment performed a faint X-ray source survey. The S150 was mounted atop the SIV-B upper stage of the Saturn 1B rocket which orbited briefly behind and below Skylab on July 28, 1973. The entire SIV-B stage underwent a series of preprogrammed maneuvers, scanning about 1° every 15 seconds, to allow the instrument to sweep across selected regions of the sky. The pointing direction was determined during data processing, using the inertial guidance system of the SIV-B stage combined with information from two visible star sensors which formed part of the experiment. Galactic X-ray sources were observed with the S150 experiment. The experiment was designed to detect 4.0-10.0 nm photons. It consisted of a single large (~1500 cm) proportional counter, electrically divided by fine wire ground planes into separate signal-collecting areas and looking through collimator vanes. The collimators defined 3 intersecting fields of view (~2 × 20°) on the sky, which allowed source positions to be determined to ~ 30'. The front window of the instrument consisted of a 2 µm thick plastic sheet. The counter gas was a mixture of argon and methane. Analysis of the data from the S150 experiment provided strong evidence that the soft X-ray background cannot be explained as the cumulative effect of many unresolved point sources.\n\nSkylab's solar studies: UV and X-ray solar photography for highly ionized atoms, X-ray spectrography of solar flares and active regions, and X-ray emissions of lower solar corona.\n\nSalyut 4 space station was launched on December 26, 1974. It was in an orbit of 355 × 343 km, with an orbital period of 91.3 minutes, inclined at 51.6°. The X-ray telescope began observations on January 15, 1975.\n\nOrbiting Solar Observatory (OSO 8) was launched on June 21, 1975. While OSO 8's primary objective was to observe the Sun, four instruments were dedicated to observations of other celestial X-ray sources brighter than a few milliCrab. A sensitivity of 0.001 of the Crab nebula source (= 1 \"mCrab\"). OSO 8 ceased operations on October 1, 1978.\n\nAlthough several earlier X-ray observatories initiated the endeavor to study X-ray source variability, once the catalogs of X-ray sources were firmly established, more extensive studies could commence.\n\nPrognoz 6 carried two NaI(Tl) scintillators (2-511 keV, 2.2-98 keV), and a proportional counter (2.2-7 keV) to study solar X-rays.\n\nThe Space Test Program spacecraft P78-1 or Solwind was launched on February 24, 1979 and continued operating until September 13, 1985, when it was shot down in orbit during an Air Force ASM-135 ASAT test. The platform was of the Orbiting Solar Observatory (OSO) type, with a solar-oriented sail and a rotating wheel section. P78-1 was in a noon-midnight, Sun-synchronous orbit at 600 km altitude. The orbital inclination of 96° implied that a substantial fraction of the orbit was spent at high latitude, where the particle background prevented detector operation. In-flight experience showed that good data were obtained between 35° N and 35° S geomagnetic latitude outside the South Atlantic Anomaly. This yields an instrument duty cycle of 25-30%. Telemetry data were obtained for about 40-50% of the orbits, yielding a net data return of 10-15%. Though this data rate appears low, it means that about 10 seconds of good data reside in the XMON data base.\n\nData from the P78-1 X-Ray Monitor experiment offered source monitoring with a sensitivity comparable to that of instruments flown on SAS-3, OSO-8, or Hakucho, and the advantages of longer observing times and unique temporal coverage. Five fields of inquiry were particularly well suited for investigation with P78-1 data:\n\nLaunched on February 21, 1981, the Hinotori satellite observations of the 1980s pioneered hard X-ray imaging of solar flares.\n\nTenma was the second Japanese X-ray astronomy satellite launched on February 20, 1983. Tenma carried GSFC detectors which had an improved energy resolution (by a factor of 2) compared to proportional counters and performed the first sensitive measurements of the iron spectral region for many astronomical objects. Energy range: 0.1-60 keV; gas scintillator proportional counter: 10 units of 80 cm each, FOV ~ 3° (FWHM), 2-60 keV; transient source monitor: 2-10 keV.\n\nThe Soviet Astron orbital station was designed primarily for UV and X-ray astrophysical observations. It was injected into orbit on March 23, 1983. The satellite was put into a highly elliptical orbit, ~200,000 × 2,000 km. The orbit kept the craft far away from the Earth for 3.5 out of every 4 days. It was outside of the Earth's shadow and radiation belts for 90% of the time. The second major experiment, SKR-02M, aboard Astron was an X-ray spectrometer, which consisted of a proportional counter sensitive to 2-25 keV X-rays, with an effective area of 0.17 m. The FOV was 3° × 3° (FWHM). Data could be telemetered in 10 energy channels. The instrument began taking data on April 3, 1983.\n\nSpacelab 1 was the first Spacelab mission in orbit in the payload bay of the Space Shuttle (STS-9) between November 28 and December 8, 1983. An X-ray spectrometer, measuring 2-30 keV photons (although 2-80 keV was possible), was on the pallet. The primary science objective was to study detailed spectral features in cosmic sources and their temporal changes. The instrument was a gas scintillation proportional counter (GSPC) with ~ 180 cm area and energy resolution of 9% at 7 keV. The detector was collimated to a 4.5° (FWHM) FOV. There were 512 energy channels.\n\nSpartan 1 was deployed from the Space Shuttle Discovery (STS-51G) on June 20, 1985 and retrieved 45.5 hours later. The X-ray detectors aboard the Spartan platform were sensitive to the energy range 1-12 keV. The instrument scanned its target with narrowly collimated (5' × 3°) GSPCs. There were 2 identical sets of counters, each having ~ 660 cm effective area. Counts were accumulated for 0.812 s into 128 energy channels. The energy resolution was 16% at 6 keV. During its 2 days of flight, Spartan-1 observed the Perseus cluster of galaxies and the Galactic Center region.\n\nGinga was launched on February 5, 1987. The primary instrument for observations was the Large Area Proportional Counter (LAC).\n\nThe European Retrievable Carrier (EURECA) was launched July 31, 1992 by the Space Shuttle Atlantis, and put into an orbit at an altitude of 508 km. It began its scientific mission on August 7, 1992. EURECA was retrieved on July 1, 1993 by the Space Shuttle Endeavour and returned to Earth. On board was the WATCH or Wide Angle Telescope for Cosmic Hard X-rays instrument. The WATCH instrument was sensitive to 6-150 keV photons. The total field of view covered 1/4 of the celestial sphere. During its 11-month lifetime, EURECA tracked the Sun and WATCH gradually scanned across the entire sky. Some 2 dozen known X-ray sources were monitored—some for more than 100 days—and a number of new X-ray transients were discovered.\n\nThe Diffuse X-ray Spectrometer (DXS) STS-54 package was flown as an attached payload in January, 1993 to obtain spectra of the diffuse soft X-ray background. DXS obtained the first-ever high resolution spectra of the diffuse soft X-ray background in the energy band from 0.15 to 0.28 keV (4.3-8.4 nm).\n\nAs all-sky surveys are performed and analyzed or once the first extrasolar X-ray source in each constellation is confirmed, it is designated X-1, e.g., Scorpius X-1 or Sco X-1. There are 88 official constellations. Often the first X-ray source is a transient.\n\nAs X-ray sources have been better located, many of them have been isolated to extragalactic regions such as the Large Magellanic Cloud (LMC). When there are often many individually discernible sources, the first one identified is usually designated as the extragalactic source X-1, e.g., Small Magellanic Cloud (SMC) X-1, a HMXRB, at 011514 -734222.\n\nThese early X-ray sources still are studied and often produce significant results. For example, Serpens X-1.\n\nAs of August 27, 2007 discoveries concerning asymmetric iron line broadening and their implications for relativity have been a topic of much excitement. With respect to the asymmetric iron line broadening, Edward Cackett of the University of Michigan commented, \"We're seeing the gas whipping around just outside the neutron star's surface,\". \"And since the inner part of the disk obviously can't orbit any closer than the neutron star's surface, these measurements give us a maximum size of the neutron star's diameter. The neutron stars can be no larger than 18 to 20.5 miles across, results that agree with other types of measurements.\"\n\n\"We've seen these asymmetric lines from many black holes, but this is the first confirmation that neutron stars can produce them as well. It shows that the way neutron stars accrete matter is not very different from that of black holes, and it gives us a new tool to probe Einstein's theory\", says Tod Strohmayer of NASA's Goddard Space Flight Center.\n\n\"This is fundamental physics\", says Sudip Bhattacharyya also of NASA's in Greenbelt, Maryland and the University of Maryland. \"There could be exotic kinds of particles or states of matter, such as quark matter, in the centers of neutron stars, but it's impossible to create them in the lab. The only way to find out is to understand neutron stars.\"\n\nUsing XMM-Newton, Bhattacharyya and Strohmayer observed Serpens X-1, which contains a neutron star and a stellar companion. Cackett and Jon Miller of the University of Michigan, along with Bhattacharyya and Strohmayer, used Suzaku's superb spectral capabilities to survey Serpens X-1. The Suzaku data confirmed the XMM-Newton result regarding the iron line in Serpens X-1.\n\nCatalogs of X-ray sources have been put together for a variety of purposes including chronology of discovery, confirmation by X-ray flux measurement, initial detection, and X-ray source type.\n\nOne of the first catalogs of X-ray sources published came from workers at the US Naval Research Laboratory in 1966 and contained 35 X-ray sources. Of these only 22 had been confirmed by 1968. An additional astronomical catalog of discrete X-ray sources over the celestial sphere by constellation contains 59 sources as of December 1, 1969, that at the least had an X-ray flux published in the literature.\n\nEach of the major observatory satellites had its own catalog of detected and observed X-ray sources. These catalogs were often the result of large area sky surveys. Many of the X-ray sources have names that come from a combination of a catalog abbreviation and the Right Ascension (RA) and Declination (Dec) of the object. For example, 4U 0115+63, 4th Uhuru catalog, RA=01 hr 15 min, Dec=+63°; 3S 1820-30 is the SAS-3 catalog; EXO 0748-676 is an Exosat catalog entry; HEAO 1 uses H; Ariel 5 is 3A; Ginga sources are in GS; general X-ray sources are in the X catalog. Of the early satellites, the Vela series X-ray sources have been cataloged.\n\nThe Uhuru X-ray satellite made extensive observations and produced at least 4 catalogs wherein previous catalog designations were improved and relisted: 1ASE or 2ASE 1615+38 would appear successively as 2U 1615+38, 3U 1615+38, and 4U 1615+3802, for example. After over a year of initial operation the first catalog (2U) was produced. The third Uhuru catalog was published in 1974. The fourth and final \"Uhuru\" catalog included 339 sources.\n\nAlthough apparently not containing extrasolar sources from the earlier OSO satellites, the MIT/OSO 7 catalog contains 185 sources from the OSO 7 detectors and sources from the 3U catalog.\n\nThe 3rd Ariel 5 SSI Catalog (designated 3A) contains a list of X-ray sources detected by the University of Leicester's Sky Survey Instrument (SSI) on the Ariel 5 satellite. This catalog contains both low and high galactic latitude sources and includes some sources observed by HEAO 1, Einstein, OSO 7, SAS 3, Uhuru, and earlier, mainly rocket, observations. The second Ariel catalog (designated 2A) contains 105 X-ray sources observed before April 1, 1977. Prior to 2A some sources were observed that may not have been included.\n\nThe 842 sources in the HEAO A-1 X-ray source catalog were detected with the NRL Large Area Sky Survey Experiment on the HEAO 1 satellite.\n\nWhen EXOSAT was slewing between different pointed observations from 1983 to 1986, it scanned a number of X-ray sources (1210). From this the EXOSAT Medium Energy Slew Survey catalog was created. From the use of the Gas Scintillation Proportional Counter (GSPC) on board EXOSAT, a catalog of iron lines from some 431 sources was made available.\n\nThe Catalog of High-Mass X-ray Binaries in the Galaxy (4th Ed.) contains source name(s), coordinates, finding charts, X-ray luminosities, system parameters, and stellar parameters of the components and other characteristic properties for 114 HMXBs, together with a comprehensive selection of the relevant literature. About 60% of the high-mass X-ray binary candidates are known or suspected Be/X-ray binaries, while 32% are supergiant/X-ray binaries (SGXB).\n\nFor all the main-sequence and subgiant stars of spectral types A, F, G, and K and luminosity classes IV and V listed in the Bright Star Catalogue (BSC, also known as the HR Catalogue) that have been detected as X-ray sources in the ROSAT All-Sky Survey (RASS), there is the RASSDWARF - RASS A-K Dwarfs/Subgiants Catalog. The total number of RASS sources amounts to ~150,000 and in the BSC 3054 late-type main-sequence and subgiant stars of which 980 are in the catalog, with a chance coincidence of 2.2% (21.8 of 980).\n\n"}
{"id": "4798308", "url": "https://en.wikipedia.org/wiki?curid=4798308", "title": "Hyderabad Electric Supply Company", "text": "Hyderabad Electric Supply Company\n\nHyderabad Electric Supply Company or HESCO is an electric distribution company which supplies electricity to all the districts of Sindh in Pakistan, excluding Karachi.\n\nHyderabad had an Area Electricity Board (AEB) as one of the eight AEBs constituted through amendments in WAPDA Act during 1981. Later on as the Government of Pakistan approved the revamping of the Water and Power Development Authority (WAPDA) power sector in April 1998, the Hyderabad Electric Supply Company took over responsibilities of the Hyderabad Area Electricity Board. Hyderabad Electric Supply Company is owned and operated by WAPDA though. The company was incorporated on 23 April 1998 and certificate for commencement of business was obtained on 1 July 1998 from NEPRA under section 146(2) of Companies Ordinance 1984.\n\n\n"}
{"id": "16792462", "url": "https://en.wikipedia.org/wiki?curid=16792462", "title": "Ingeborg Gjærum", "text": "Ingeborg Gjærum\n\nIngeborg Gjærum (born 16 April 1985) is a Norwegian environmentalist.\n\nShe hails from Ottestad. She broke national news in 2005, when she was a central board member of Natur og Ungdom. She was deputy leader from 2006 to 2008, and became leader in 2008. She remained so until 2009.\n\nIn 2010 she was hired by the public relations firm Burson-Marsteller.\n"}
{"id": "18146395", "url": "https://en.wikipedia.org/wiki?curid=18146395", "title": "Insulated pipe", "text": "Insulated pipe\n\nInsulated pipes (called also preinsulated pipes or bonded pipe ) are widely used for district heating and hot water supply in Europe. They consist of a steel pipe, an insulating layer, and an outer casing. The main purpose of such pipes is to maintain the temperature of the fluid in the pipes. A common application is the hot water from district heating plants. Most commonly used are single insulated pipes, but more recently in Europe it is becoming popular to use two pipes insulated within the same casing. By using insulated pipe supports, direct heat transfer between pipes and their supports are prevented.\n\nThe insulating material usually used is polyurethane foam or similar, with a coefficient of thermal conductivity k=0.033-0.024 W/mK (thermal conductivity). Outer casing is usually high-density polyethylene (HDPE). Production of preinsulated pipes for district heating in the European Union is regulated by the standard EN253. According to EN253:2003, pipes must be produced to work at constant temperature of for 30 years, keeping thermal conductivity less than or equal to 0.033 W/mK. There are three insulation thickness levels.\n\nInsulated pipelines are usually assembled from pipes of , , or in length, laid underground in depth . Efficient working life of district heating pipelines networks is estimated at 25–30 years, after which they need to be replaced with new pipes.\n\nElectrical insulation may be used for metal pipes for corrosion prevention.\n"}
{"id": "2370766", "url": "https://en.wikipedia.org/wiki?curid=2370766", "title": "Ion beam analysis", "text": "Ion beam analysis\n\nIon beam analysis (\"IBA\") is an important family of modern analytical techniques involving the use of MeV ion beams to probe the composition and obtain elemental depth profiles in the near-surface layer of solids. All IBA methods are highly sensitive and allow the detection of elements in the sub-monolayer range. The depth resolution is typically in the range of a few\nnanometers to a few ten nanometers. Atomic depth resolution can be achieved, but requires special equipment. The analyzed depth ranges from a few ten nanometers to a few ten micrometers. IBA methods are always quantitative with an accuracy of a few percent. \nChanneling allows to determine the depth profile of damage in single crystals.\n\n\nThe quantitative evaluation of IBA methods requires the use of specialized simulation and data analysis software. SIMNRA and DataFurnace are popular programs for the analysis of RBS, ERD and NRA, while GUPIX is popular for PIXE. A review of IBA software was followed by an intercomparison of several codes dedicated to RBS, ERD and NRA, organized by the International Atomic Energy Agency.\n\nIBA is an area of active research. The last major Nuclear Microbeam conference in Debrecen (Hungary) was published in NIMB 267(12-13).\n\nIon beam analysis works on the basis that ion-atom interactions are produced by the introduction of ions to the sample being tested. Major interactions result in the emission of products that enable information regarding the number, type, distribution and structural arrangement of atoms to be collected. To use these interactions to determine sample composition a technique must be selected along with irradiation conditions and the detection system that will best isolate the radiation of interest providing the desired sensitivity and detection limits. The basic layout of an ion beam apparatus is an accelerator which produces an ion beam that is feed through an evacuated beam-transport tube to a beam handling device. This device isolates the ion species and charge of interest which then are transported through an evacuated beam-transport tube into the target chamber. This chamber is where the refined ion beam will come into contact with the sample and thus the resulting interactions can be observed. The configuration of the ion beam apparatus can be changed and made more complex with the incorporation of additional components. The techniques for ion beam analysis are designed for specific purposes. Some techniques and ion sources are shown in table 1. Detector types and arrangements for ion beam techniques are shown in table 2.\n\nIon beam analysis has found use in a number of variable applications, ranging from biomedical uses to studying ancient artifacts. The popularity of this technique stems from the sensitive data that can be collected without significant distortion to the system on which it is studying. The unparalleled success found in using ion beam analysis has been virtually unchallenged over the past thirty years until very recently with new developing technologies. Even then, the use of ion beam analysis has not faded, and more applications are being found that take advantage of its superior detection capabilities. In an era where older technologies can become obsolete at an instant, ion beam analysis has remained a mainstay and only appears to be growing as researchers are finding greater use for the technique.\n\nGold nanoparticles have been recently used as a basis for a count of atomic species, especially with studying the content of cancer cells. Ion beam analysis is a great way to count the amount of atomic species per cell. Scientists have found an effective way to make accurate quantitative data available by using ion beam analysis in conjunction with elastic backscattering spectrometry (EBS). The researchers of a gold nanoparticle study were able to find much greater success using ion beam analysis in comparison to other analytical techniques, such as PIXE or XRF. This success is due to the fact that the EBS signal can directly measure depth information using ion beam analysis, whereas this cannot be done with the other two methods. The unique properties of ion beam analysis make great use in a new line of cancer therapy.\nIon beam analysis also has a very unique application in the use of studying archaeological artifacts, also known as archaeometry. For the past three decades, this has been the much preferred method to study artifacts while preserving their content. What many have found useful in using this technique is its offering of excellent analytical performance and non-invasive character. More specifically, this technique offers unparalleled performance in terms of sensitivity and accuracy. Recently however, there have been competing sources for archaeometry purposes using X-ray based methods such as XRF. Nonetheless, the most preferred and accurate source is ion beam analysis, which is still unmatched in its analysis of light elements and chemical 3D imaging applications (i.e. artwork and archaeological artifacts).\n\nA third application of ion beam analysis is in forensic studies, particularly with gunshot residue characterization. Current characterization is done based on heavy metals found in bullets, however, manufacturing changes are slowly making these analyses obsolete. The introduction of techniques such as ion beam analysis are believed to alleviate this issue. Researchers are currently studying the use of ion beam analysis in conjunction with a scanning electron microscope and an Energy Dispersive X-ray spectrometer (SEM-EDS). The hope is that this setup will detect the composition of new and old chemicals that older analyses could not efficiently detect in the past. The greater amount of analytical signal used and more sensitive lighting found in ion beam analysis gives great promise to the field of forensic science.\n\nDating back to the 1960s the data collected via ion beam analysis has been analyzed through a multitude of computer simulation programs. Researchers who frequently use ion beam analysis in conjunction with their work require that this software be accurate and appropriate for describing the analytical process they are observing. Applications of these software programs range from data analysis to theoretical simulations and modeling based on assumptions about the atomic data, mathematics and physics properties that detail the process in question. As the purpose and implementation of ion beam analysis has changed over the years, so has the software and codes used to model it. Such changes are detailed through the five classes by which the updated software are categorized.\n\nIncludes all programs developed in the late 1960s and early 1970s. This class of software solved specific problems in the data; niy did not provide the full potential to analyze a spectrum of a full general case. The prominent pioneering program was IBA, developed by Ziegler and Baglin in 1971. At the time, the computational models only tackled the analysis associated with the back-scattering techniques of ion beam analysis and performed calculation based on a slab analysis. A variety of other programs arose during this time, such as RBSFIT, though due to the lack of in-depth knowledge on ion beam analysis, it became increasingly hard to develop programs that accurate.\n\nA new wave of programs sought to solve this accuracy problem in this next class of software. Developed during the 1980s, programs like SQEAKIE and BEAM EXPERT, afforded an opportunity to solve the complete general case by employing codes to perform direct analysis. This direct approach unfolds the produced spectrum with no assumptions made about the sample. Instead it calculates through separated spectrum signals and solves a set of linear equations for each layer. Problems still arise, though, and adjustments made to reduce noise in the measurements and room for uncertainty.\n\nIn a trip back to square one, this third class of programs, created in the 1990s, take a few principles from Class A in accounting for the general case, however, now through the use of indirect methods. RUMP and SENRAS, for example, use an assumed model of the sample and simulate a comparative theoretical spectra, which afforded such properties as fine structure retention and uncertainty calculations. In addition to the improvement in software analysis tools came the ability to analyze other techniques aside from back-scattering; i.e. ERDA and NRA.\n\nExiting the Class C era and into the early 2000s, software and simulation programs for ion beam analysis were tackling a variety of data collecting techniques and data analysis problems. Following along with the world's technological advancements, adjustments were made to enhance the programs into a state more generalized codes, spectrum evaluation, and structural determination. Programs produced like SIMNRA now account for the more complex interactions with the beam and sample; also providing a known database of scattering data.\n\nThis most recently developed class, having similar characteristics to the previous, makes use of primary principles in the Monte Carlo computational techniques. This class applies molecular dynamic calculations that are able to analyze both low and high energy physical interactions taking place in the ion beam analysis. A key and popular feature that accompanies such techniques is the possibility for the computations to be incorporated in real time with the ion beam analysis experiment itself.\n\n\n"}
{"id": "5021154", "url": "https://en.wikipedia.org/wiki?curid=5021154", "title": "Ishfaq Ahmad", "text": "Ishfaq Ahmad\n\nIshfaq Ahmad, D.Sc., Minister of State, \"\" (3 November 1930 – 18 January 2018), was a Pakistani nuclear physicist, emeritus professor of high-energy physics at the National Center for Physics, and former science advisor to the Government of Pakistan.\n\nA versatile theoretical physicist, Ahmad made significant contributions in the theoretical development of the applications and concepts involving the particle physics, and its relative extension to the quantum electrodynamics, while working as senior research scientist at the CERN in 1960s and 1970s. Joining the PAEC in late 1950s, Ahmad served as the director of the Nuclear Physics Division at the secret Pinstech Institute which developed the first designs of atomic bombs, a clandestine project during the post-1971 war. There, he played an influential role in leading the physics and mathematical calculations in the critical mass of the weapons, and did theoretical work on the implosion method used in the weapons.\n\nSince 1960s and onwards, he has been a high-ranking official at the IAEA as part of the Pakistan Government's official mission, working to make the peaceful use of nuclear power for the industrial development. Having chaired the PAEC from 1991 until 2001, he has been affiliated with the Pakistan Government as a Science adviser to the Prime minister on strategic and scientific programs, with the status of Minister of State. A vehement supporter for the peaceful use of nuclear energy, he earned public and international fame in May 1998 when he oversaw and directed PAEC to perform country's first public atomic tests (see Chagai-I and Chagai-II) in a secret weapon-testing laboratories in Balochistan Province of Pakistan. He died on 18 January 2018, aged 87 in Lahore.\n\nAhmad was born in Gurdaspur, Indian Punjab state of the British India, to a Kakazai family. Ahmad obtained his early education in Jalandhar (Indian Punjab), Faisalabad (then Lyallpur) and Lahore, Pakistan. Ahmad enrolled in the Punjab University in Lahore to study Physics, and earned his undergraduate, B.Sc. degree, in Physics in 1949.\n\nAfter entering in the post graduate school at the Punjab University, Ahmad obtained his M.Sc. degree, in 1951, after submitting his master's thesis on nuclear physics, which was supervised by Rafi Chaudhry. With his master's degree, he obtained Honours diploma and secured a gold medallion for the recognition of his work in physics. He taught various undergraduate physics laboratory courses at the Government College University while working on fundamental concepts in nuclear physics with his university mentor. In 1954, he won the scholarship under the Columbo Plan fellowship program and went to Quebec, Canada for his doctorate studies.\n\nAhmad attended the doctorate school at the Université de Montréal and did a two-year-long course in Particle physics and engaged his research on theoretical physics. In 1959, Ahmad obtained D.Sc. in Nuclear physics after submitting his doctoral works on concepts on advancing on particle physics. His thesis were written on fluent French and English language, and reluctantly returned to Pakistan under the terms of Colombo Plan contract. His DSc thesis were supervised by Prof. and covered a wide range of research in the study of elementary particles by using the deployment of special fine grain nuclear emulsion (AgBr). During his long doctoral studies, Ahmad studied nuclear reaction at the Montreal Laboratory with supervisors and scientists role in the Manhattan Project. Upon his return to Pakistan, he joined the Pakistan Atomic Energy Commission (PAEC) as a senior scientist.\n\nIn 1952, Ahmad served as a visiting professor of mathematics at the Government College University, before accepting the professorship of mathematics at the University of Paris in 1959. He engaged his research in theoretical physics and obtained a one-year-long research fellowship at the Niels Bohr Institute for Theoretical Physics. In 1962–64, he accepted the professorship in physics at the University of Montreal and the University of Ottawa. In Ottawa, he carried out pioneering research in particle resonance and published important publications in theoretical physics.\n\nAhmad also performed experiments on nuclear physics at the Meuse Underground Laboratories of France. In 1965, Ahmad published a research report on absorption of Pion's cross sections and the range of complex atom's energy of the pion particle. He recalled his Cern experience in 1994:\n\nIn the 1990s, Ahmad played a pivotal role in building closer relations with the CERN, and lobbied tirelessly for PAEC to reach an agreement with CERN. In 1997, Ahmad, as chair of PAEC, signed an agreement with CERN in the up gradation of the CMS detector and the financial contribution worth one million SFr for the construction of eight magnetic rings for the detector. This was followed by In 1998, Ishfaq Ahmad, as PAEC chairman, reached another contract with CERN. The signing of the agreement was followed by the state visit of CERN's director Christopher Llewellyn Smith with whom Ahmad signed a collaborative agreement that provided an entry point for Pakistani's scientist (respectively PAEC) into the CMS collaboration.\n\nIn 2000, another treaty between PAEC and CERN was signed that covered the construction of the resistive plate chambers required for the CMS muon system. In Press Conference with Luciano Maiani, Ahmad quoted: \"I very much hope and wish that these developments may eventually lead to Pakistan becoming an associate member of CERN.\"\n\nIn 1960, Ahmad joined the Pakistan Atomic Energy Commission (PAEC) as senior scientist and was allowed to proceed aborad for post-doctoral work at several of the world's most renowned research institutions. Ahmad published papers in physics at the Niels Bohr Institute at Copenhagen; also at the University of Montreal in Canada as well as the University of Paris – Sorbonne in France. Finally, he settled down for work at the Lahore Centre of the PAEC (PAEC) in 1965. Ahmad held the post of Senior Scientific Officer until 1966. From 1969 until 1971, Ahmad was the director of the Atomic Energy Center in Lahore; and then served as secretary of PAEC from 1967 till 1969. In 1971, Ahmad became director of the Institute of Nuclear Science and Technology in Nilore until 1976. In 1976, he became a Science Member of PAEC, raised to the position of Senior Member in 1988. He became Chairman of the Commission in 1991 and remained its Chairman from 13 March 1991 to 19 December 2001.\n\nWhile he was Chairman PAEC, Ahmad has been heading the country's delegation at the International Atomic Energy Agency (IAEA) in Vienna, Austria. \nAt IAEA, he was always very keen for getting technical support and the breaking of the isololation of scientists from third world. On his persuasion IAEA's technical assistance program was adapted to cater for special needs of the developing countries. In this regard a Standing Advisory Group on Technical Assistance and Cooperation (SAGTAC) was established; Ahmad served as the first Chairman of the Group.\n\nAfter the 1971 war with India, the government sent Ishfaq Ahmad to the Pakistan Institute of Nuclear Science and Technology (PINSTECH). When Munir Ahmad Khan became the chairman of PAEC and was put in charge of secret atomic bomb project, Munir Khan appointed Ahmad as the director of PINSTECH, where he remained up to 1976. Ahmad served as the director of the Nuclear Physics Division at the secret Pinstech Institute which developed the first designs of atomic bombs, a clandestine project during the post-1971 war. There, he played an influential role in leading the physics and mathematical calculations in the critical mass of the weapons, and did preliminary theoretical work on the implosion method used in the weapons.\n\nAs early as in 1976, Ahmad, in a seismic team led by geophysicist Ahsan Mubarak conducted a three-dimensional geometrical survey and made several reconnaissance tours of the suitable areas in Balochistan. After a one-year-long survey, the team found a mountain which matched their specifications. The 185-meter high-rise granite mountain was founded in the Ras Koh region of the Chagai Division of Balochistan, which at their highest point rise to a height of 3,009 metres. Ahmad had long noted that the underground weapon-testing laboratories in the mountain should be \"bone dry\" and capable of withstanding a ~20 kilotonne nuclear force from the inside. Within a week, further test experiments were conducted to measure the water content of the mountains and the surrounding area and to measure the capability of the mountain’s rock to withstand a nuclear test. Once this was confirmed, Ishfaq Ahmed finalised the work on a three-dimensional survey of the area.\n\nIn 1976, PAEC succeeded in producing the first local 10kg of Yellowcake and later on produced the Pu, the weapon grade plutonium in 1983, which was later tested with the nuclear device.\n\nAt PINSTECH, Ahmad produced the first Photographic plate to identify the fissile matter in natural uranium when it is explored. However, due to its classified research, the knowledge of such detector is completely classified. The NPD developed the Thermoluminescent Dosimeter to measure the detection of alpha particles emitted in the decay of radon and thoron gases. Ahmad collaborating with Hameed Ahmad Khan —director of Radiation Physics Division – in the development of CR-39, a type of particle detector. Ahmad gained expertise in nuclear emulsion and developed a first classified nuclear emulsion that provided information about the mass, charge and velocity of the particles producing the track.\n\nA first device was physically manufactured by 1983, and transported to Sargodha air force base for a first test. On 11 March 1983, a first cold test, codename Kirana-I, of a device was secretly carried out at the weapon-testing laboratories built inside the Central Ammunition Depot (CAD) of Sargodha AFB. The test was overseen and conducted by a small team of scientists led by Ahmad, while calculations on quantum oscillator was conducted by Theoretical physics group. Other invitees and attendees included the Munir Ahmad Khan, Samar Mubarakmand, and Masud Ahmad of PAEC whilst others were high-ranking civilians officials of elite civil bureaucracy and the active-duty officer of the Pakistan military.\n\nIn 1991, Ahmad was officially approved as the chairman of PAEC by the Prime minister of Pakistan after Munir Khan retired. During this time, he had been a senior scientist and acted as official science advisor to the government of Pakistan on many occasions. In 1998, Ahmad visited Canada to deliver lecture on quantum physics at the Montreal Laboratory when the news of surprise nuclear tests, codename Pokhran-II, of India reached to him. On 16 May 1998, Ahmad cut short his trip and returned to Pakistan to attend meeting with Prime minister Nawaz Sharif, and arranged his meeting with Prime minister on 17 May 1998. The message was bestowed to him by the Joint Headquarters at Rawalpindi, informing him to remain on stand-by a meeting with the Prime Minister. After commencing the meeting with the Prime minister, Ahmad received green signal from the government of Pakistan to conduct country's first test as a suitable reply to Indian nuclear aggression.\n\nAhmad personally supervised the test preparations as he also suggests the codenames of the tests. On 28 May 1998, the PAEC, sided by KRL and corps of engineers, performed the first nuclear tests, codename \"Chagai-I\" which was followed by \"Chagai-II\" two days later, on May 1998. Evidently, the fission devices were had contained the boosted-fission HEU nuclear process, that came from the KRL. But, on 30 May, the second test, codename Chagai-II, was performed completely under the command and control management of the PAEC. The fission devices, on a second test, were reportedly had contained the weapon grade plutonium, producing around at ~20kt of nuclear force. All together, the superposition of sum of the forces and the total blast yield was ranged at the nearly ~40kt of nuclear force, according to the PAEC scientific data.\n\nAfter his retirement from PAEC, Ahmad developed keen scientific interest in the science of climate change. This interest lead to the creation of 2 new centre viz Global Change Impact Studies Centre (GCISC) and Center for Earthquake Studies (CES), both initially attached to the National Center for Physics (NCP) in Islamabad. Ahmad sereved as elected President of the Pakistan Academy of Sciences and is the lifetime Chairman of the Board of Governors of the National Center for Physics (NCP)— a research institute established on the pattern of International Center for Theoretical Physics (ICTP) at Trieste, Italy.\n\nHe also put Pakistan on the governing Council of the International Institute for Applied Systems Analysis (IIASA), Austria, which conducts policy related research using mathematical modeling and simulation tools.\n\nDr Ishfaq Ahmad’s efforts led to the creation of the Global Change Impact Studies Centre (GCISC) in Islamabad where, for the first time, research on policy issues related to Climate Change is being undertaken in Pakistan. The centre, an autonomous organisation under the federal govt, works in collaboration with national institutions such as Pakistan Meteorological Department (PMD), National Agricultural Research Centre (NARC), WAPDA and PCRWR etc. The centre has also established collaborative relationship with international institutions, most importantly The Abdus Salam International Centre for Theoretical Physics (ICTP), Trieste, Italy. GCISC, with Dr. Arshad M Khan as its Executive Director, also serves as the Secretariat of the Prime Minister’s Committee on Climate Change.\n\nAfter the 8 October 2005, Kashmir earthquake, the Government has decided to establish a Center for Earthquake studies in Islamabad, under the technical direction of Dr. Ishfaq Ahmad.\nThe centre under the directorship of Mr. Shahid Ashraf and Dr. Ahsan Mubarak started work in collaboration with world leading scientists such as Prof. Elchin Khalilov of Azerbaijan. The centre conducts research using a Gravitational Wave Recorder housed at the National Centre for Physics, Islamabad.\n\nIshfaq Ahmad is internationally known for his long-standing public advocacy for the nuclear power plants for the industrial and socio-economic growth. On international forums, Ahmad deterred the international pressure mounted on Pakistan after conducting its tests, instead highlighted the achievements gained by Pakistan on its nuclear power infrastructure in the country as well as the need of Pakistan's usage of nuclear power for its economical growth. In 2012, Ahmad lobbied for the HMC-3 consortium to be listed as first commercial nuclear power corporation and helped the consortium to acquired its first license to manufacture nuclear materials for industrial power plants.\n\nIn 1989, Ishfaq Ahmad was bestowed with first state honour, \"Sitara-e-Imtiaz\" by Benazir Bhutto; and Hilal-e-Imtiaz in 1995. In 1998, Ahmad received the highest state honour, Nishan-e-Imtiaz, given to any national of Pakistan, for his services to the country in a graceful state ceremony. The same year, he was awarded gold medallion by the Institute of Leadership and Management in Lahore.\n\n\n\nD.Sc. Thesis (UQAM): Structure and Identification of trajectories in fine grain ionographic emulsions, under the direction of Pierre Demers, Faculty of Science, University of Montreal, Canada, 1958.\n\n\n6. L'INFLUENCE DU DÉVELOPPEMENT SUR LA STRUCTURE DES TRAJECTOIRES ET SUR LE VOILE DANS LES ÉMULSIONS À GRAINS FINS, Canadian Journal of Physics, 1959, 37(12). pp. 1548–1552. (http://www.nrcresearchpress.com/doi/pdf/10.1139/p59-171)\n\nnucléaire sur la structure des lacunes. Ahmad Ishfaq and Max Morand. \nComptes rendus hebdomadaires des séances de l'Académie des sciences, France, \n1959, Vol. 1–3 (T248, part 1), pp. 1798–1800 \n(http://gallica.bnf.fr/ark:/12148/bpt6k32002/f1836.image).\n\n1964, Ann. ACFAS, 31, 76–7, 1965.\n\n\n13. The role of pre-irradiation annealing in changing the track development characteristics of glass track detectors. Nuclear Instruments and Methods, Vol.131(1), 1975, pp. 89–92.\n\n19–24 February 2007, Cyprus (https://www.springer.com/environment/sustainable+development/book/978-3-540-95990-8).\n\n\n"}
{"id": "21405814", "url": "https://en.wikipedia.org/wiki?curid=21405814", "title": "Kanger", "text": "Kanger\n\nA kanger (; also known as kangri or kangid or kangir) is an earthen pot woven around with wicker filled with hot embers used by Kashmiris beneath their traditional clothing to keep the chill at bay, which is also regarded as a work of art. It is normally kept inside the Phiran, the Kashmiri cloak, or inside a blanket. If a person is wearing a jacket, it may be used as a hand warmer. It is about in diameter and reaches a temperature of about . It comes in different variants, small ones for children and large ones for adults.\n\nAfter the earthen pots are moulded, and backed, the artisans complete the wickerwork around them, by erecting two arms to handle the pot, proping the back side with strong wicker sticks, and colour it (optionally) to give an aesthetically delicate shape. The final product then goes to the market.\n\nIt is generally believed that Kashmiris learnt the use of the \"kangri\" from the Italians who were in the retinue of the Mughal emperors, and usually visited the Valley during summer. In Italy (where a similar device was known as a \"scaldino\") and Spain, braziers were made in a great variety of shapes and were profusely ornamented. Historical data, however, contradicts the claim that \"kangri\" came to Kashmir from Italy, but it is known that it was used in the time of the Mughal Empire. Those visiting Kashmir for the first time during the winter season are surprised to find people carrying firepots in their hands or in their laps but every Kashmiri knows how to handle the apparatus with care. It is a part of Kashmiri tradition and even in modern times it sees a huge demand, and is even used in public or private offices during winters.\n\nKashmiri Pandits burn \"kangri\" on the occasion of a local festival called Teela Aetham, marking the end of winter season. Isband (Peganum harmala), aromatic seeds believed to push away negative energies, are burnt in a kanger to mark a good beginning to a party.\n\nBeyond Kashmir, people of the erstwhile Hill states of Himachal, Uttarakhand, and some parts of Nepal also use other local variants of Kangri.\n\nIn 2015, a shopkeeper in Srinagar commissioned a \"kangri\", described as the world's largest, to attract customers to his textile shop. \"Kashmir Life\" reported that the size, over a metre long, posed technical challenges to the wicker-weavers.\n\nThis Kashmiri proverb, \"what Laila was on Majnun’s bosom (Legendary Lovers), so is the Kanger to a Kashmiri\", sums up the relationship between a Kashmiri and the Kanger and its cultural importance, which is also shown by this verse:\n\nRegular use of the kanger can cause a specific skin cancer known as kangri cancer. This effect was first studied by W. J. Elmslie in 1866 and was thought to be caused by burns, but it is now thought to be the result of a carcinogenic distillation product of woodcoal.\n\n"}
{"id": "32474711", "url": "https://en.wikipedia.org/wiki?curid=32474711", "title": "Launceston Gasworks", "text": "Launceston Gasworks\n\nThe Launceston Gasworks is a former industrial site located in the CBD of Launceston, Tasmania. The site was the principal supplier of gas to the City of Launceston before the importation of LPG in the 1970s. The gasworks produced gas by heating coal and siphoning off the gas that it released before refining and storing it on site in a set of 3, steel frame gasometers. The first buildings on site were the horizontal retort buildings built in 1860 from sandstone and local brick. The site was later used by Origin Energy as their Launceston LPG outlet. The site is instantly recognizable by its 1930s, steel braced, vertical retort building with the words \"COOK WITH GAS\" in the brickwork.\n\nStarting in 1826, Launceston was lit with lamps running on sperm whale oil. These lamps were unpopular and local butchers soon replaced these with \"slush\" lamps that burned animal fat. These lamps were still disliked by many so in 1844 a local man, Doctor William Russ Pugh (a statue of him is located in Launceston's Prince's Square), produced his own coal gas for his house and a year later Benjamin Hyrons lit the Angel Inn with methane gas. As early as 1854, the Examiner newspaper urged locals to consider the creation of a gasworks in Launceston pointing out the numerous benefits and cheaper costs of coal gas as a means of lighting. In 1856 the Launceston City Council engaged Scottish-born engineer, William Falconer of the Hobart Gas Company, to prepare plans for the proposed gasworks.\n\nAt a public meeting at the Cornwall Hotel on the 18 May 1858, the Launceston Gas Company was formed. The company purchased a marshy paddock near Cimitier Street the same year due to its proximity to the North Esk River in order to build the new gasworks. Machinery from England and suitable builders were assembled in 1859 with the Horizontal Retort Buildings completed in early 1860. On the 5 April 1860, Launceston turned on its new gas street lighting for the first time and oil lamps were replaced by gas.\n\nThe main source of gas was Newcastle Coal from New South Wales. The gas was extracted by heating the coal until gas was produced. The site of the gasworks was also directly opposite the TMLR rail yards on Willis Street which was also convenient for the delivery of coal. The demand for coal gas continued to grow even after the Duck Reach Power Station was commissioned in 1896. In 1932, the Vertical Retort House was added to the site to increase productivity. The site was expanded to the west in the mid 1900s which involved the removal of Wescombe Street and the historic cottages that lined it in order to make a 3rd gasometer and later an LPG cylinder yard when the site was taken over by Boral and eventually Origin Energy. In 2007 the site was sold and the 3 gasometers were largely dismantled. The cylinder yard has now been built over by the new Centrelink building and car park with additional developments planned to take place in the future.\n\nThe chief engineer of the site used to live in an ornate cottage on the NE corner of the site. The first engineer was William Falconer from Scotland and many more followed. An unusual feature associated with the cottage was the chief engineer's private, heated swimming pool. It is said to still be there, buried under the courtyard between the cottage and the gas laboratories. The pool was heated by the Vertical Retort Building's boilers and was strictly out of bounds to all workers unless they were needed to fix it. The cottage is now restored\nand serves as a private residence.\n\nThe company office is located next to the chief engineer's cottage, and once looked out across the North Esk until the levee system was built over the wharves. The original building was a brick cottage but in the 1880s it had its facade rebuilt in a Victorian style. The headquarters have now been restored along with the other buildings on the site's north facing Boland Street.\n\nOut in front of the company office is one of the original gas lamps used in the city. The company operated 123 gas lamps across the city as well as a special one outside the office. The lamps were removed when the city turned to electric lighting but during restoration work, an original lamp was located and returned to the site where it now stands.\n\nBuilt from local brick with sandstone quoining and arches, the Horizontal Retort building was erected in 1860 and is one of the oldest buildings on site. The central portion of the building used to have a 39.3 m (129 ft) brick chimney and housed 18 retorts with room for an additional 6. On either side of the main building were two wings: the western one for the storage of coal imported form NSW and the other being a purifying house which also contained the \"smiths\" and gasfitters shops. A second coal store, located where the Carbureted Water Gas Building now stands, closely followed the same design. Gas produced in this building was stored in the 15 m (50 ft) diameter No.1 and No.2 Gasometers. The brick chimney was built by A Henderson with Francis and Miller being contracted to build the main building, all at a total cost of £3333. In 1929, this building was flooded to a depth of 4 ft during the great floods of 1929, after which damage was repaired quickly.\n\nWhen the Vertical Retort was built in the 1930s, the Horizontal Retorts were removed along with chimney and replaced with a boiler and a large tar plant in the mid portion of the building with associated 3 oil tanks outside on the southern face. The brick and masonry portion of the eastern wing was re-fitted with 3 exhausters and the steel and galvanized iron portion upgraded to contain 4 more purifiers adding to a total of 8.\n\nToday the building is falling into disrepair despite minor restoration work being carried out a few years earlier and most of the sandstone is heavily eroded. The north face of the building is only partially original. The last restoration attempts from the 1980s saw the purifiers removed and converted to conference rooms; closing the originally open air structure with galvanized iron and plate glass bearing the new Gasworks emblem, based on the ventilation hole patterns in the Vertical Retort's walls.\n\nLocated along the southern perimeter of the site facing Cimitier Street, the gasometers were where gas produced at the gasworks was stored at pressure for later distribution. At present, three of the original five gasometers are on site in various levels of salvage. The newest No.5 Gasometer built in the mid 1940s is mostly untouched except for the gas bell has been removed and the pit filled. This gasometer is mounted on a concrete base with a 12 sided steel frame mounted on top that has been painted red. A few meters to the east are the brick and sandstone foundations of two smaller gasometers; the middle dating from the late 1800s and the furthest to the east being one of the original 2 built in 1860. These 2 gas holders were smaller than the newer one and were last painted Boral's yellow and green colour scheme before they were dismantled in 2007. Shortly after dismantling, a proposal was forwarded to council to develop all 3 gasometers as a single 6 storey apartment block but was cancelled. The other one of the first 2 gasometers on site was dismantled in the early 1900s with the foundations and water pit being converted into a covered tar and liquor well located in the eastern most corner of the site (now vacant). In the 1946 plan for the site, a final 250 000 cubic ft. gasometer was proposed to be built to the northwest of the No.5 Gasometer but was canceled along with the rest of the proposal.\n\n Capacity: 35 000 cubic ft. (991 cubic meters)\n\n Capacity: 35 000 cubic ft. (later 70 000 cubic ft. (1982 cubic meters))\n\n Capacity: 70 000 cubic ft. (1982 cubic meters)\n\n Capacity: 140 000 cubic ft. (3964 cubic meters)\n\n Capacity: 250 000 cubic ft. (7079 cubic meters)\n\n Capacity: 250 000 cubic ft. (7079 cubic meters)\n\nLocated on the south of the site bordering Cimitier Street, this small galvanized iron and timber building was used as the gaswork's distribution point. As with the rest of the southern half of the site, this building is in disrepair with all of its windows broken by vandals. Inside, most of the original machinery is still intact such as 2 miniature Braddock Gasometers which are largely intact and are ornately decorated. The miniature gasometer on the eastern side of the building is connected onto the inlet mains for the main Gasometers on site (from the point it leaves the purifiers in the Horizontal Retort Building), with the second being directly connected to the outlet mains between the Gasometers and the main governor. The main governor is located at the back of the building along with an exhauster and various meters and pressure gauges. The original governor was first installed in 1860 when the gasworks were first built and changed little since then though the building itself has been rebuilt. The governor originally distributed gas into 10 gas mains (2 large and 8 smaller) with the largest 2 just visible on the external surface of the building.\n\nAs part of a development proposal from 2007, this building was meant to be restored as a public historic site with interpretive panels but was canceled in 2008.\n\nThe Carburetted Water Gas Building is located next to the Vertical Retort House and was last used as the offices for Origin Energy after the Carbureted Water Gas Plant was removed from within. Though built later in the gasworks history (post 1940s), closely imitates the same style as the Vertical Retort through the use of steel and brick. The building is 3 storeys high with a small 4th level on the roof.\n\nWhen built in 1956, the building was primarily built around a single Carbureted Water Gas Plant with plans for a later second. The plant itself dominated the first 3 levels of the building with its massive steel furnace and generator blocks (steel structures lined with firebrick on the interior). The first floor (ground level) housed many associated machinery items including both electric and steam powered exhausters, a wash box and tram cart. The upper most level was used as a coal store with gravity feeders connecting it to the plant.\n\nAs part of the current development, the Carbureted Water Gas Building is being restored and connected onto the Vertical Retort House via a glass atrium mimicking the profile of the buildings' rooflines.\n\nThe Meter Shop was where the new gas meters were assembled and tested. By 1958, there were 6333 meters across Launceston. The Gas Fitters did the meter rounds by bicycle, often loading the bicycles with everything they needed from 20 ft pipes to plywood. The Engineering staff and the company fitters also used the building to write up the daily logbooks.\nThe Meter Shop also played a secondary role as the staff training center and under Origin Energy ownership in the late 1900s, was solely used for this purpose. Gas Fitters, Meter Makers and Repairers had to do an apprenticeship that lasted 5 years with most men working for the company their whole life. Even the management stayed with a total of four Company Secretaries in 120 years. At its peak, the company employed up to 80 people. The company encouraged a family feeling among its employees, often organising annual Christmas picnics.\n\nThe 29m high (not including stack height) Vertical Retort Building was built in 1932 as the primary gas production facility for the Launceston Gasworks site. The building was officially opened on 19 March 1932 and was described by the company director as a masterpiece of modern chemistry and engineering. During the opening ceremony, the buildings moving components were switched on and visitors were taken to the top of the retorts to view what was the most advanced gasworks of its time. During later operation, the building was described as a harsh place to work in due to blazing heat, noise and gas fumes.\n\nThe vertical retort is the most recognisable building on site and is an iconic building in Launceston. The building is built with a steel frame forming a grid pattern, supporting individual \"panes\" of brick walls. The inscription \"COOK WITH GAS\" is written in the top floor's brickwork. The building climbs five stories with the coal conveyor still intact running up the front of the building, though the breaker pit at its base has been filled in. The original building was shorter in length and contained only 4 retorts but foundations for later extensions were included and soon used as the building was extended. The small rear section plus the first connected division of the building were part of this extension with the divide visible by a slight change in brick colour.\nInside, the coal was transported to the top level where it was deposited in a long hopper at the top level of the building (which also contains the conveyor drive and water tank). From the hopper, the coal was fed via a single chute into the vertical retort block which was then divided into 8 separate retorts, where it was heated by furnaces (and later an additional steel furnace in the back room) located on the first floor (ground level). The extracted gas was collected in two parallel \"Collector Mains\" running along the top of the retort block. Accumulated tar and liquor was drained from the collector mains into a small liquor separating tank on the 3rd floor on the western side of the building. The collected gas was forced through the retort governor (recently removed for scrap) and into the foul main that runs down the back of the retort house where it was purified elsewhere on the site. Like the Horizontal Retorts, the Vertical Retort was run on Newcastle Coal imported from New South Wales. In the 1950s, modifications were made to the gasworks to allow the site to use Fingal Coal from the Fingal Valley in north east Tasmania.\n\nUnlike many gasworks sites in Australia that were either demolished or stripped after closure, the vertical retort house in Launceston still retained most of its original machinery after it was abandoned. Both the electric coke extractor eccentric and conveyor drive remained intact within the building along with most of the fourth floor machinery. The waste heat boiler on sublevel 3 had been reduced in size due to salvage operations in the 1980s as with the waste heat boiler on the first floor (ground level) which was also partially dismantled. The original brick furnaces (producer blocks) at the front of the building were mostly dismantled with the exception of a now free-standing, brick archway which used to allow the conveyor to pass through the furnace block. Original lighting and electronic equipment still remained within the building both on the first floor and the electrical room on the 3rd floor at the back of the building. The coke extractors on the underside of the retort blocks were in various levels of salvage with the most complete being located at the rear of the building.\n\nSince it was abandoned in favor of LPG in 1977, the vertical retort was left to fall into ruin with large piles of droppings cover the floor and machinery due to the pigeons that used the building as a roost because of the many openings in its brickwork. In late July 2012, most of the building underwent cleaning to remove the accumulated waste from the interior before scrapping and internal demolition stripped the building of its heritage value to make way for a building proposal.\n\nLocated between the Chief Engineers Cottage and the Vertical Retort, the Laboratory was an essential part of the site. Every day, samples of gas were tested in this building for quality. The laboratory was no safe place, before switching to Butane for town gas in 1978, the coal gas process produced many hazardous by-product gasses such as Ammonia and Hydrogen Sulfide which were always a potential hazard.\nThis building was also fitted with a drafting table where the Lab Assistant drew up site plans when they were needed. All the distribution maps for Launceston's gas mains were produced here.\n\nDuring restoration in 2007, the iron-clad, timber frame building was lifted from its foundations to make way for a new laneway into the site. The building was relocated a few meters to the north where it was placed on new foundations and a new, modern section of similar proportions was added to it. The Laboratory and Workshop has since been restored and in one of its windows is displayed various items used on the gasworks site that were uncovered during restoration.\n\nSince the site was sold by Origin Energy in 2007, the Launceston Gasworks have become a potential site for development being located in the CBD close to Launceston's City Park and the Inveresk precinct.\n\nThe gasworks have currently been partially developed as part of a large 3 part scheme costing $35 million. The first stage costing $8 million involving the restoration of the cottages on the north of the site as part of the new Centrelink complex has already been completed though the second and third stages were cancelled in 2008 due to the global financial crisis. The second stage was to involve the construction of a whole new set of buildings consisting of offices, retail outlets, restaurants, and apartments. These buildings were to be built on the south of the site facing Cimitier Street within the foundations of the gasometers as well as a free standing structure next to the Governor Cottage and a glass office building connecting the Vertical Retort House to the Carburetted Water Gas Building. As part of the second stage, the Governor Cottage and Horizontal Retort were meant to be restored to allow access to the general public. The third stage was disapproved by the Launceston Council but was to involve the development of the abandoned Origin Energy parking lot across Willis Street into more shops but has been converted into a public parking lot instead.\n\nIn June 2012 a proposal by developer Ross Harrison was put forward to the Launceston City Council to incorporate the Vertical Retort House and Carburetted Water Gas Building into a restaurant and bar with additional commercial space. In late July 2012, the site changed zoning from industrial to commercial and work commenced on cleaning the building of the accumulated pigeon droppings and polystyrene blocks that were dumped inside it. In mid-September removal of the Corrugated Asbestos Roofing started. On the 10 October 2012, the original 1932 machinery was blow-torched out of the building to be taken away followed by disassembly of the fire-brick retorts except for the coke extractors which were partially retained for aesthetic value. The completed works now link the two buildings with a glass atrium as the earlier proposal had intended. A bar and reception area are currently located on the 1st floor (ground level) of the Vertical Retort House under the former retort blocks with restaurant space occupying the area between the two buildings and into the Carburetted Water Gas Building. The upper levels of both buildings have been remodeled to support retail and office spaces. The machinery within the Vertical Retort House underwent a heritage survey to assess the historic value of the surviving equipment and from that, local architectural firm, ARTAS, was left to incorporate this into the final design. Removed machinery was intended to be kept as museum exhibits but has yet to be utilized for such purposes. At present all the machinery in the Vertical Retort has been removed except for a few components on the ground level and the waste heat boiler on the floors above, with salvaged components stored in the Horizontal Retort House.\n\n\nMost information sourced from the newly restored Gasworks Historic Site, Launceston.\n"}
{"id": "45180368", "url": "https://en.wikipedia.org/wiki?curid=45180368", "title": "LiHe", "text": "LiHe\n\nLiHe is a compound of helium and lithium. The substance is a cold low density gas made of molecules each composed of a helium atom and lithium atom bound by van der Waals force. The preparation of LiHe opens up the possibility to prepare other Helium dimers, and beyond that multi-atom clusters that could be used to investigate Efimov states and Casimir retardation effects.\n\nIt was detected in 2013. Previously LiHe was predicted to have a binding energy of 0.0039 cm (7.7×10eV, 1.2×10J or 6 mK), and a bond length of 28 Å. Other van der Waals bound helium molecules were previously known including AgHe and He. Detection of LiHe was done via fluorescence. The Lithium atom in the XΣ state was excited to AΠ. The spectrum showed a pair of lines, each split into two with the hyperfine structure of Li. The lines had wavenumbers of 14902.563. 14902.591 14902.740 and 14902.768 cm. The two pairs are separated by 0.177 cm. This is explained by two different vibrational states of the LiHe molecule: 1/2 and 3/2. The bonding between the atoms is so low that it cannot withstand any rotation or greater vibration without breaking apart. The lowest rotation states would have energies of 40 and 80 mK, greater than the binding energy around 6 mK.\n\nLiHe was formed by laser ablation of lithium metal into a cryogenic helium buffer gas at a temperature between 1 and 5 K. The proportion of LiHe molecules was proportional to the density of He, and declined as the temperature increased.\n\nLiHe is polar and paramagnetic.\n"}
{"id": "212330", "url": "https://en.wikipedia.org/wiki?curid=212330", "title": "Lye", "text": "Lye\n\nA lye is a metal hydroxide traditionally obtained by leaching ashes (containing largely potassium carbonate or \"potash\"), or a strong alkali which is highly soluble in water producing caustic basic solutions. \"Lye\" is commonly an alternative name of sodium hydroxide (NaOH) or historically potassium hydroxide (KOH), though the term \"lye\" refers to any member of a broad range of metal hydroxides.\n\nToday, lye is commercially manufactured using a membrane cell chloralkali process. It is supplied in various forms such as flakes, pellets, microbeads, coarse powder or a solution.\n\nLyes are used to cure many types of food, including the traditional Nordic lutefisk, olives (making them less bitter), canned mandarin oranges, hominy, lye rolls, century eggs, pretzels, bagels, and the traditional Turkish pumpkin dessert Kabak tatlısı (creating a hard crust while the inside remains soft). They are also used as a tenderizer in the crust of baked Cantonese moon cakes, in \"zongzi\" (glutinous rice dumplings wrapped in bamboo leaves), in chewy southern Chinese noodles popular in Hong Kong and southern China, and in Japanese ramen noodles. They are also used in kutsinta, a type of rice cake from the Philippines together with pitsi-pitsî.\n\nIn the United States, food-grade lye must meet the requirements outlined in the Food Chemicals Codex (FCC),as prescribed by the U.S. Food and Drug Administration (FDA). Lower grades of lye which are unsuitable for use in food preparation are commonly used as drain de-cloggers and oven cleaners.\n\nLye in the form of both sodium hydroxide and potassium hydroxide is used in making soap. Potassium hydroxide soaps are softer and more easily dissolved in water than sodium hydroxide soaps. Sodium hydroxide and potassium hydroxide are not interchangeable in either the proportions required or the properties produced in making soaps.\n\n\"Hot process\" soap making also uses lye as the main ingredient. Lye is added to water, cooled for a few minutes and then added to oils and butters. The mixture is then cooked over a period of time (1–2 hours), typically in a slow cooker, and then placed into a mold. This method is much quicker than cold process, as it doesn't take several weeks to complete.\n\nThe ancient use of lye for soap-making and as a detergent is the origin of the English word, deriving from Proto-Germanic \"*laugo\" and ultimately from the Proto-Indo-European root \"*leue-\", \"to wash.\" Relatives in other Germanic languages, besides their words for lye, include the Nordic languages' words for Saturday (\"laugardagur, lördag, lørdag\"), meaning \"washing day\".\n\nLyes are also valued for their cleaning effects. Sodium hydroxide is commonly the major constituent in commercial and industrial oven cleaners and clogged drain openers, due to its grease-dissolving abilities. Lyes decompose greases via alkaline ester hydrolysis, yielding water-soluble residues that are easily removed by rinsing.\n\nSodium or potassium hydroxide can be used to digest tissues of animal carcasses. Often referred to as alkaline hydrolysis, the process involves placing the carcass or body into a sealed chamber, adding a mixture of lye and water and the application of heat to accelerate the process. After several hours the chamber will contain a liquid with coffee-like appearance, and the only solids that remain are very fragile bone hulls of mostly calcium phosphate, which can be mechanically crushed to a fine powder with very little force. Sodium hydroxide is frequently used in the process of decomposing roadkill dumped in landfills by animal disposal contractors. Due to its low cost and availability, it has also been used to dispose of corpses by criminals. Italian serial killer Leonarda Cianciulli used this chemical to turn dead bodies into soap. In Mexico, a man who worked for drug cartels admitted to having disposed of more than 300 bodies with it.\n\nA 3–10% solution of potassium hydroxide (KOH) gives a color change in some species of mushrooms:\nSee: Chemical test in mushroom identification\n\nSources recommend immediate removal of contaminated clothing/materials, gently brushing/wiping excess off of skin, and then flushing the area of exposure with running water for 15–60 minutes while contacting emergency services.\n\nPersonal protective equipment including safety glasses, chemical-resistant gloves, and adequate ventilation are required for the safe handling of lyes. When in proximity to a lye that is dissolving in an open container of water, the use of a vapor-resistant face mask is recommended. Adding lye to water too quickly can cause the solution to boil.\n\nSolid lyes are deliquescents and have a strong affinity for air moisture. Solid lyes will deliquesce or dissolve when exposed to open air, absorbing a relatively large amount of water vapour. Accordingly, lyes are stored in air-tight plastic containers. Glass is not a good material to be used for storage as lyes are mildly corrosive to it. Similar to the case of other corrosives, the containers should be labeled to indicate the potential danger of the contents and stored away from children, pets, heat, and moisture.\n\nThe majority of safety concerns with lye are also common with most corrosives, such as their potentially destructive effects on living tissues; examples are the skin, flesh, and the cornea. Solutions containing lyes can cause chemical burns, permanent injuries, scarring and blindness, immediately upon contact. Lyes may be harmful or even fatal if swallowed; ingestion can cause esophageal stricture. Moreover, the solvation of dry solid lyes is highly exothermic; the resulting heat may cause additional burns or ignite flammables.\n\nThe reaction between sodium hydroxide and a few metals is also hazardous. Aluminium reacts with lyes to produce hydrogen gas. Since hydrogen is flammable, mixing a large quantity of a lye such as sodium hydroxide with aluminum in a closed container is dangerous—especially when the system is at a high temperature, which speeds up the reaction. In addition to aluminum, lyes may also react with magnesium, zinc, tin, chromium, brass or bronze—producing hydrogen gas.\n\n"}
{"id": "20731495", "url": "https://en.wikipedia.org/wiki?curid=20731495", "title": "Lyonium ion", "text": "Lyonium ion\n\nIn chemistry, a lyonium ion is the cation derived by the protonation of a solvent molecule. For example, a hydronium ion is formed from the protonation of water and is the cation formed by the protonation of methanol. \nLyate ion is the anion formed by the deprotonation of a solvent molecule. These ions, resulting from molecular autoionization, contribute to the molar conductivity of protolytic solvents. \n"}
{"id": "2846153", "url": "https://en.wikipedia.org/wiki?curid=2846153", "title": "Meter Point Administration Number", "text": "Meter Point Administration Number\n\nA Meter Point Administration Number, also known as MPAN, Supply Number or S-Number, is a 21-digit reference used in Great Britain to uniquely identify electricity supply points such as individual domestic residences. The gas equivalent is the Meter Point Reference Number. The system was introduced in 1998 in order to provide a competitive environment for the electricity companies, and allows consumers to switch their supplier easily as well as simplifying administration. Although the name suggests that an MPAN refers to a particular meter, an MPAN can have several meters associated with it, or indeed none where it is an unmetered supply. A supply receiving power from the network operator (DNO) has an Import MPAN, while generation and microgeneration projects feeding back into the DNO network are given Export MPANs.\n\nAn MPAN is commonly separated into two sections: the core and the top line data. The core is the final 13 digits and is the unique identifier. The top line data gives information about the characteristics of the supply and is the responsibility of the supplier.\n\nThe full MPAN is required to be depicted on electricity bills (the boxes on the top and bottom line are generally unaligned):\n\nThe core data is on the second line, the supplementary data on the first.\n\nThe first two digits of a full MPAN reflect its profile class.\nProfile class 00 supplies are half-hourly (HH) metered, i.e. they record electricity consumption for every half hour of every day, and supplies of the other profile classes are non-half-hourly (NHH) metered. A NHH supply must be upgraded to HH for:\n\nHH data is recorded by the meter and will be collected usually by either an onsite download, or via a GSM, SMS, GPRS or telephone line.\n\nDomestic NHH import MPANs will always have a profile class of 01 or 02. Domestic NHH export MPANs are allocated a profile class of 08.\n\nThe MTC is a 3 digit code that reflects the various registers a meter may have, whether it be a Single Rate, Day Night split, or even a Seasonal Time of Day.\n\nThe Line Loss Factor Class or LLFC is used to identify the related Distribution Use of System (DUoS) charges for the MPAN. The figure reflects both the amount of distribution infrastructure used to supply the exit point and the amount of energy lost through heating of cables, transformers, etc.\n\nThe MPAN core is the final 13 digits of the MPAN, and uniquely identifies an exit point. It consists of the two-digit Distributor ID, followed by an eight-digit unique identifier, then by two digits and a single check digit.\n\nGreat Britain is divided into fourteen distribution areas. For each area a single company, the distribution network operator, has a licence to distribute electricity. They effectively carry electricity from the National Grid to the exit point (each has a unique MPAN and a possibility of several meters) where the customers are. The owner of the distribution network charges electricity suppliers for carrying the electricity in their network. Their DNO licensed regions are the same geographic areas as the old nationalised electricity boards.\n\nIn addition to the distribution network operators noted above who are licensed for a specific geographic area there are also independent distribution network operators (IDNO). IDNOs own and operate electricity distribution networks which are mostly network extensions connected to the existing distribution network, e.g. to serve new housing developments. Scottish Hydro Electric Power Distribution also provide distribution services in South Scotland as an IDNO and Southern Electric Power Distribution provide IDNO services in all other England and Wales areas. There are five IDNOs with no \"base\" area and these are detailed in the table below:\n\nThe final digit in the MPAN is the check digit, and validates the previous 12 (the core) using a modulus 11 test. The check digit is calculated thus:\n\n\nThe supply identified by the MPAN can exist in one of four states: disconnected, de-energised, live, and new.\n\n\nThese terms are by no means standardised. For example, a disconnected supply might be referred to as a 'dead' supply.\n\nThe vast majority of MPANs are import MPANs, used where energy is being consumed. However, if a supply exports to the distribution network, then an export MPAN is issued. If a supply both imports and exports, then both an import MPAN and export MPAN are issued.\n\nFormerly, export MPANs required a half-hourly compliant meter to be installed. Since 2003, it has been possible for microgeneration projects, with a capacity of 30 kW or below to have a non-half-hourly meter, to export back into the distribution network. The uptake was slow with the first microgeneration export MPAN being issued in June 2005. Some suppliers may not bother to register the export MPAN in MPAS as the revenue is so small. Export capacity over 30 kW is required to be half hourly metered.\n\nThe Metered Supply Point (MSP) is the point at which the meter measuring a customer's consumption is located. It is thus also the point at which either the distribution network operator's supply, or the building network operator's lateral cable terminates and the customer's equipment begins. In order to firmly establish a supply's MSP, the MPAN needs to be associated with a meter serial number.\n\nAlthough it is common for an MPAN to be associated with one meter serial number, in some cases there is a many-to-many relationship. For example, one meter could be associated with both an import and an export MPAN, or one MPAN could be measured by three separate meters.\n\nIt is possible for small predictable supplies to be unmetered. Typical unmetered supplies are street lights, traffic signals, signs and bollards but can also include telephone kiosks, CCTV and advertising displays.\n\nFor a piece of equipment to be connected to the distribution networks via an unmetered connection it should not exceed 500 watts and operate in a predictable nature and cannot be manually turned on at the end users request, typically the equipment would either be in operation and taking a supply of electricity 24 hours a day or be controlled by a photocell. A photocell is most commonly used with street lights and is the small dome unit which can be seen on the very top of the lighting column.\n\nIt is the customer's responsibility to maintain an accurate and up-to-date inventory of unmetered supplies, and to inform the UMSO (UnMetered Supplies Operator) of all changes to the connected equipment.\n\nLarger local authorities tend to trade their unmetered energy on a half-hourly basis. To do so, they employ a meter administrator who will use daily data from a PECU Array which is then used to calculate the energy consumptions.\n\nA PECU Array is a device that holds a representative account of the photocells that authority uses on their street lights or traffic signals they operate, there are many different types of photocell (electronic photocell, thermal and hybrid for example) that can be used. Until 2014, the process of retrieving data from the PECU array relied on the meter administrator using the LAMP or Lailoken equivalent meter programs, or for the local authority who own the array to use the Ganieda software. The array only held a limited amount of data in its 'rolling barrel' data log. For this reason, it was important that whichever software was used, it had to be downloaded daily to avoid the risk of losing event data. In June 2014, ICTIS (UK) Ltd (bought out by Tym Huckin Ltd in December 2015) launched the PECU Array 2.0, which stores all its data off the array, in the cloud. Not only did this provide an unlimited data store, but it made it possible for the PECU Array owners to have direct and immediate access to their data via a dedicated website. It also enabled Lailoken to retrieve the event data more efficiently and reduce the overall processing time for the Meter Administrators.\n\nBy trading energy as unmetered half-hourly the authority will accurately pay for the energy consumed by their declared unmetered equipment, and because the data is downloaded daily the authorities will see their energy invoices change throughout the year to represent the changes in season and daily lighting levels.\n\nOnce the daily calculations have been performed by the Meter Administrator the new revised energy consumption are sent to the appointed Data Collector who will in turn provided them to the appointed electricity supplier who invoices the customer for the electricity used. Some Meter Administrators can provide instant access to the settlement data for the Local Authorities to see and download at their convenience. This sort of service enables the Lighting Authorities to be more proactive in their energy management.\n\nIf however the unmetered supplies are being traded as non half hourly the UMSO undertakes the responsibility to calculate an EAC (Estimated Annual Consumption), this is done using a simple formula which takes into account the circuit watts of the equipment and the annual hours of operation.\n\nFor example, a piece of equipment that is in use 24hrs per day will have annual hours of 8766. If we say a CCTV camera is rated at 24 circuit watts and operating 24/7 the EAC would be 210.384kWh, the calculation is circuit watts x annual hours / by 1000 = Kilowatt Hours (kWh).\n\nExample 24 x 8766 / 1000 = 210.384kWh, 1kWh is a unit of electricity.\n\nIf the equipment is street lighting the same process is used however the annual hours will change as each photocell is assigned a set number of Annual Hours which indicate how and when the lights turn on and off. These annual hours have been set by Elexon and are not locally agreed with the UMSO by the customer.\n\nOnce an EAC calculation has taken place an EAC Certificate is provided to the customer's appointed electricity supplier for billing, with an electronic copy of the EAC being sent to the appointed data collector.\n\nThe UMSO does not make a charge to the unmetered customer which is why an appointed supplier invoices for the electricity consumptions, however the DNO (for whom the UMSO is part of) do levy a charge to the electricity supplier for the delivery of the electricity to the customer's unmetered equipment. These are known as DUoS charges (distribution use of system).\n\nThe electricity supplier pays a DUoS charge based on the information held by the data collector for settlement purposes.\n\nEach non-half-hourly supply has a four digit code called the Standard Settlement Configuration (SSC), which specifies the number of registers a meter has, and the times that each register is recording electricity usage. The times that a register is recording is specified with a five digit code Time Pattern Regime (TPR). So for example a supply with SSC 0943 has two registers with TPRs 00404 and 00405. The 00404 TPR register records from 01:00 to 02:30 and 08:00 to 23:30, and the 00405 register records for the rest of the time.\n\nEach DNO operates a Meter Point Administration System (MPAS) which holds the following information for each MPAN:\n\n\nMPRS is the name of the software package that implements the MPAS system for all DNOs. Since MPRS is used by most DNOs it is often used interchangeably with the term MPAS.\n\nECOES (Electricity Central Online Enquiry Service) is a website that allows users and authorised industry parties to search for supply details (past and present) using such things as the 13-digit MPAN bottom line number, the meter serial number or the postcode. The user can determine a wide range of data relating to the supply including the full address, meter details, the current energisation status and also the appointed parties (i.e. the supplier, distributor, MOP, DC and DA). The site is populated from information sent from the supplier regarding the metering system.\n\nOnly non-domestic users (with two valid MPAN's that are not Class 1 or 2) can register to access this service. To apply for access to ECOES, just visit www.ecoes.co.uk and follow the process for application detailed there.\n\n\n"}
{"id": "1636581", "url": "https://en.wikipedia.org/wiki?curid=1636581", "title": "Miter joint", "text": "Miter joint\n\nA miter joint (mitre in British English, sometimes shortened to miter) is a joint made by beveling each of two parts to be joined, usually at a 45° angle, to form a corner, usually a 90° angle. For woodworking, a disadvantage of a miter joint is its weakness, but it can be strengthened with a spline. There are two common variations of a splined miter joint, one where the spline is long and runs the length of the mating surfaces and another where the spline is perpendicular to the joined edges.\n\nCommon applications include picture frames, pipes, and molding.\n\nFor miter joints occurring at angles other than 90°, for materials of the same cross-section the proper cut angle must be determined so that the two pieces to be joined meet flush (i.e. one piece's mitered end is not longer than the adjoining piece). To find the cut angle divide the angle at which the two pieces meet by two. Technically, two different cut angles are required; one for each piece, where the second angle is 90° plus the aforementioned cut angle, but due to angular limitations in common cutting implements (hand circular saws, table saws) a single angle is required and is used to cut the first piece in one direction and the second piece in the opposite direction.\n\nWhen a piece is beveled at both ends, such that the two attached pieces do not lie in the same plane, a three-dimensional structure is obtained. In that case, it is also necessary either to rotate the piece in its longitudinal axis, or, to tilt the saw blade before beveling the second end.\n\nWhen employing the miter joint to connect two pieces that have a non-circular cross-section it typically is desirable to have the longitudinal edges of the joined pieces match up properly at the joint. It always is possible to close a structure constructed with pieces having non-circular cross section into a loop through \"properly matched\" miter joints (e.g. a picture frame), however, a three-dimensional loop from pieces with non-circular cross section need not close properly when attempting to miter it all the way around. In general, a twist occurs, causing the edges at the last joint to be misaligned.\n\n\n"}
{"id": "10251117", "url": "https://en.wikipedia.org/wiki?curid=10251117", "title": "New Zealand Journal of Ecology", "text": "New Zealand Journal of Ecology\n\nThe New Zealand Journal of Ecology is a biannual peer-reviewed scientific journal publishing ecological research relevant to New Zealand and the South Pacific. It has been published since 1952, firstly as a 1952 issue of \"New Zealand Science Review\" and then as the Proceedings of the New Zealand Ecological Society until 1977. The Journal is published by the New Zealand Ecological Society, and is covered by Current Contents/Agriculture, Biology and Environmental Science, GEOBASE, and Geo Abstracts. Kevin C. Burns is the journal's current editor, with Anne Austin as technical editor.\n\nHosted by the Royal Society of New Zealand, the website's server runs free and open source software including Linux, PHP, and mySQL. Free access is available to all issues over three years old in PDF format. The compilation of PDF files from issues dating back to 1953 was funded by the New Zealand Government's \"Terrestrial and Freshwater Biodiversity Information System\". Current issues are also available free online for a limited period of time, but will be subject to subscription in the near future.\n"}
{"id": "6391934", "url": "https://en.wikipedia.org/wiki?curid=6391934", "title": "Nitroethane", "text": "Nitroethane\n\nNitroethane is an organic compound having the chemical formula CHNO. Similar in many regards to nitromethane, nitroethane is an oily liquid at standard temperature and pressure. Pure nitroethane is colorless and has a fruity odor.\n\nNitroethane is produced industrially by treating propane with nitric acid at 350–450 °C. This exothermic reaction produces four industrially significant nitroalkanes: nitromethane, nitroethane, 1-nitropropane, and 2-nitropropane. The reaction involves free radicals, such as CHCHCHO, which arise via homolysis of the corresponding nitrite ester. These alkoxy radicals are susceptible to C—C fragmentation reactions, which explains the formation of a mixture of products.\n\nAlternatively, nitroethane can be produced by the Victor Meyer reaction with haloethanes like Chloroethane, bromoethane, or iodoethane with silver nitrite in diethyl ether or THF.\nAdditionally with the Kornblum Modification which uses the prior mentioned alkyl halides uses the less soluble sodium nitrite salt in either a dimethyl sulfoxide or dimethylformamide solvent.\n\nVia condensations like the Henry reaction, nitroethane converts to several compounds of commercial interest. Condensation with 3,4-dimethoxybenzaldehyde affords the precursor to the antihypertensive drug methyldopa; condensation with unsubstituted benzaldehyde yields phenyl-2-nitropropene. Nitroethane condenses with two equivalents of formaldehyde to give, after hydrogenation, 2-amino-2-methyl-1,3-propanediol, which in turn condenses with oleic acid to give an oxazoline, which protonates to give a cationic surfactant.\n\nLike some other nitrated organic compounds, nitroethane is also used as a fuel additive and a precursor to explosives.\n\nNitroethane is a useful solvent for polymers such as polystyrene and is particularly useful for dissolving cyanoacrylate adhesives. In niche applications, it has been used as a component in artificial nail remover and in overhead ceiling sealant sprays.\n\nNitroethane has been used as a chemical feedstock in clandestine laboratories for synthesis of controlled substances such as amphetamines.\n\nNitroethane is suspected to cause genetic damage and be harmful to the nervous system. Typical TLV/TWA is 100 ppm. Typical STEL is 150 ppm. Skin contact causes dermatitis in humans. In animal studies, nitroethane exposure was observed to cause lacrimation, dyspnea, pulmonary rales, edema, liver and kidney injury, and narcosis. Children have been poisoned by accidental ingestion of artificial nail remover.\n\nThe for rats is reported as 1100 mg/kg.\n\n"}
{"id": "22594", "url": "https://en.wikipedia.org/wiki?curid=22594", "title": "Omega-3 fatty acid", "text": "Omega-3 fatty acid\n\nOmega−3 fatty acids, also called ω−3 fatty acids or \"n\"−3 fatty acids, are polyunsaturated fatty acids (PUFAs). The fatty acids have two ends, the carboxylic acid (-COOH) end, which is considered the beginning of the chain, thus \"alpha\", and the methyl (-CH) end, which is considered the \"tail\" of the chain, thus \"omega\". One way in which a fatty acid is named is determined by the location of the first double bond, counted from the tail, that is, the omega (ω-) or the n- end. Thus, in omega-3 fatty acids the first double bond is between the third and fourth carbon atoms from the tail end. However, the standard (IUPAC) chemical nomenclature system starts from the carboxyl end.\nThe three types of omega−3 fatty acids involved in human physiology are α-linolenic acid (ALA), found in plant oils, and eicosapentaenoic acid (EPA) and docosahexaenoic acid (DHA), both commonly found in marine oils. Marine algae and phytoplankton are primary sources of omega−3 fatty acids. Common sources of plant oils containing ALA include walnut, edible seeds, clary sage seed oil, algal oil, flaxseed oil, Sacha Inchi oil, \"Echium\" oil, and hemp oil, while sources of animal omega−3 fatty acids EPA and DHA include fish, fish oils, eggs from chickens fed EPA and DHA, squid oils, and krill oil. Dietary supplementation with omega−3 fatty acids does not appear to affect the risk of death, cancer or heart disease. Furthermore, fish oil supplement studies have failed to support claims of preventing heart attacks or strokes or any vascular disease outcomes.\nOmega−3 fatty acids are important for normal metabolism. Mammals are unable to synthesize omega−3 fatty acids, but can obtain the shorter-chain omega−3 fatty acid ALA (18 carbons and 3 double bonds) through diet and use it to form the more important long-chain omega−3 fatty acids, EPA (20 carbons and 5 double bonds) and then from EPA, the most crucial, DHA (22 carbons and 6 double bonds). The ability to make the longer-chain omega−3 fatty acids from ALA may be impaired in aging. In foods exposed to air, unsaturated fatty acids are vulnerable to oxidation and rancidity.\nSupplementation does not appear to be associated with a lower risk of all-cause mortality.\n\nThe evidence linking the consumption of marine omega−3 fats to a lower risk of cancer is poor. With the possible exception of breast cancer, there is insufficient evidence that supplementation with omega−3 fatty acids has an effect on different cancers. The effect of consumption on prostate cancer is not conclusive. There is a decreased risk with higher blood levels of DPA, but an increased risk of more aggressive prostate cancer was shown with higher blood levels of combined EPA and DHA. In people with advanced cancer and cachexia, omega−3 fatty acids supplements may be of benefit, improving appetite, weight, and quality of life.\n\nEvidence in the population generally does not support a beneficial role for omega−3 fatty acid supplementation in preventing cardiovascular disease (including myocardial infarction and sudden cardiac death) or stroke. A 2018 meta-analysis found no support that daily intake of one gram of omega-3 fatty acid in individuals with a history of coronary heart disease prevents fatal coronary heart disease, nonfatal myocardial infarction or any other vascular event. However, omega−3 fatty acid supplementation greater than one gram daily for at least a year may be protective against cardiac death, sudden death, and myocardial infarction in people who have a history of cardiovascular disease. No protective effect against the development of stroke or all-cause mortality was seen in this population. Eating a diet high in fish that contain long chain omega−3 fatty acids does appear to decrease the risk of stroke. Fish oil supplementation has not been shown to benefit revascularization or abnormal heart rhythms and has no effect on heart failure hospital admission rates. Furthermore, fish oil supplement studies have failed to support claims of preventing heart attacks or strokes.\n\nEvidence suggests that omega−3 fatty acids modestly lower blood pressure (systolic and diastolic) in people with hypertension and in people with normal blood pressure. Some evidence suggests that people with certain circulatory problems, such as varicose veins, may benefit from the consumption of EPA and DHA, which may stimulate blood circulation and increase the breakdown of fibrin, a protein involved in blood clotting and scar formation.\nOmega−3 fatty acids reduce blood triglyceride levels but do not significantly change the level of LDL cholesterol or HDL cholesterol in the blood. The American Heart Association position (2011) is that borderline elevated triglycerides, defined as 150–199 mg/dL, can be lowered by 0.5-1.0 grams of EPA and DHA per day; high triglycerides 200–499 mg/dL benefit from 1-2 g/day; and >500 mg/dL be treated under a physician's supervision with 2-4 g/day using a prescription product.\n\nALA does not confer the cardiovascular health benefits of EPA and DHAs.\n\nThe effect of omega−3 polyunsaturated fatty acids on stroke is unclear, with a possible benefit in women.\n\nA 2013 systematic review found tentative evidence of benefit for lowering inflammation levels in healthy adults and in people with one or more biomarkers of metabolic syndrome. Consumption of omega−3 fatty acids from marine sources lowers blood markers of inflammation such as C-reactive protein, interleukin 6, and TNF alpha.\n\nFor rheumatoid arthritis, one systematic review found consistent, but modest, evidence for the effect of marine n−3 PUFAs on symptoms such as \"joint swelling and pain, duration of morning stiffness, global assessments of pain and disease activity\" as well as the use of non-steroidal anti-inflammatory drugs. The American College of Rheumatology has stated that there may be modest benefit from the use of fish oils, but that it may take months for effects to be seen, and cautions for possible gastrointestinal side effects and the possibility of the supplements containing mercury or vitamin A at toxic levels. The National Center for Complementary and Integrative Health has concluded that \"[n]o dietary supplement has shown clear benefits for rheumatoid arthritis\", but that there is preliminary evidence that fish oil may be beneficial, but needs further study.\n\nAlthough not supported by current scientific evidence as a primary treatment for attention deficit hyperactivity disorder (ADHD), autism, and other developmental disabilities, omega−3 fatty acid supplements are being given to children with these conditions.\n\nOne meta-analysis concluded that omega−3 fatty acid supplementation demonstrated a modest effect for improving ADHD symptoms. A Cochrane review of PUFA (not necessarily omega−3) supplementation found \"there is little evidence that PUFA supplementation provides any benefit for the symptoms of ADHD in children and adolescents\", while a different review found \"insufficient evidence to draw any conclusion about the use of PUFAs for children with specific learning disorders\". Another review concluded that the evidence is inconclusive for the use of omega−3 fatty acids in behavior and non-neurodegenerative neuropsychiatric disorders such as ADHD and depression.\n\nFish oil has only a small benefit on the risk of premature birth. A 2015 meta-analysis of the effect of omega−3 supplementation during pregnancy did not demonstrate a decrease in the rate of preterm birth or improve outcomes in women with singleton pregnancies with no prior preterm births. A systematic review and meta-analysis published the same year reached the opposite conclusion, specifically, that omega−3 fatty acids were effective in \"preventing early and any preterm delivery\".\n\nThere is some evidence that omega−3 fatty acids are related to mental health, including that they may tentatively be useful as an add-on for the treatment of depression associated with bipolar disorder. Significant benefits due to EPA supplementation were only seen, however, when treating depressive symptoms and not manic symptoms suggesting a link between omega−3 and depressive mood. There is also preliminary evidence that EPA supplementation is helpful in cases of depression. The link between omega−3 and depression has been attributed to the fact that many of the products of the omega−3 synthesis pathway play key roles in regulating inflammation (such as prostaglandin E3) which have been linked to depression. This link to inflammation regulation has been supported in both in vitro and in vivo studies as well as in meta-analysis studies. The exact mechanism in which omega−3 acts upon the inflammatory system is still controversial as it was commonly believed to have anti-inflammatory effects.\n\nThere is, however, significant difficulty in interpreting the literature due to participant recall and systematic differences in diets. There is also controversy as to the efficacy of omega−3, with many meta-analysis papers finding heterogeneity among results which can be explained mostly by publication bias. A significant correlation between shorter treatment trials was associated with increased omega−3 efficacy for treating depressed symptoms further implicating bias in publication.\n\nA study in 2013, (Stafford, Jackson, Mayo-Wilson, Morrison, Kendall), stated the following in its conclusion: \"Although evidence of benefits for any specific intervention is not conclusive, these findings suggest that it might be possible to delay or prevent transition to psychosis. Further research should be undertaken to establish conclusively the potential for benefit of psychological interventions in the treatment of people at high risk of psychosis.\"`\n\nEpidemiological studies are inconclusive about an effect of omega−3 fatty acids on the mechanisms of Alzheimer's disease. There is preliminary evidence of effect on mild cognitive problems, but none supporting an effect in healthy people or those with dementia.\n\nBrain function and vision rely on dietary intake of DHA to support a broad range of cell membrane properties, particularly in grey matter, which is rich in membranes. A major structural component of the mammalian brain, DHA is the most abundant omega−3 fatty acid in the brain. It is under study as a candidate essential nutrient with roles in neurodevelopment, cognition, and neurodegenerative disorders.\n\nResults of studies investigating the role of LCPUFA supplementation and LCPUFA status in the prevention and therapy of atopic diseases (allergic rhinoconjunctivitis, atopic dermatitis and allergic asthma) are controversial; therefore, at the present stage of our knowledge (as of 2013) we cannot state either that the nutritional intake of n−3 fatty acids has a clear preventive or therapeutic role, or that the intake of n-6 fatty acids has a promoting role in context of atopic diseases.\n\nPeople with PKU often have low intake of omega−3 fatty acids, because nutrients rich in omega−3 fatty acids are excluded from their diet due to high protein content.\n\nAs of 2015, there was no evidence that taking omega−3 supplements can prevent asthma attacks in children.\n\nAn omega−3 fatty acid is a fatty acid with multiple double bonds, where the first double bond is between the third and fourth carbon atoms from the end of the carbon atom chain. \"Short chain\" omega−3 fatty acids have a chain of 18 carbon atoms or less, while \"long chain\" omega−3 fatty acids have a chain of 20 or more.\n\nThree omega−3 fatty acids are important in human physiology, α-linolenic acid (18:3, \"n\"-3; ALA), eicosapentaenoic acid (20:5, \"n\"-3; EPA), and docosahexaenoic acid (22:6, \"n\"-3; DHA). These three polyunsaturates have either 3, 5, or 6 double bonds in a carbon chain of 18, 20, or 22 carbon atoms, respectively. As with most naturally-produced fatty acids, all double bonds are in the \"cis\"-configuration, in other words, the two hydrogen atoms are on the same side of the double bond; and the double bonds are interrupted by methylene bridges (--), so that there are two single bonds between each pair of adjacent double bonds.\n\nThis table lists several different names for the most common omega−3 fatty acids found in nature.\n\nOmega−3 fatty acids occur naturally in two forms, triglycerides and phospholipids. In the triglycerides, they, together with other fatty acids, are bonded to glycerol; three fatty acids are attached to glycerol. Phospholipid omega−3 is composed of two fatty acids attached to a phosphate group via glycerol.\n\nThe triglycerides can be converted to the free fatty acid or to methyl or ethyl esters, and the individual esters of omega−3 fatty acids are available.\n\nDHA in the form of lysophosphatidylcholine is transported into the brain by a membrane transport protein, MFSD2A, which is exclusively expressed in the endothelium of the blood–brain barrier.\n\nThe 'essential' fatty acids were given their name when researchers found that they are essential to normal growth in young children and animals. The omega−3 fatty acid DHA, also known as docosahexaenoic acid, is found in high abundance in the human brain. It is produced by a desaturation process, but humans lack the desaturase enzyme, which acts to insert double bonds at the ω and ω position. Therefore, the ω and ω polyunsaturated fatty acids cannot be synthesized, are appropriately called essential fatty acids, and must be obtained from the diet.\n\nIn 1964, it was discovered that enzymes found in sheep tissues convert omega−6 arachidonic acid into the inflammatory agent, prostaglandin E, which is involved in the immune response of traumatized and infected tissues. By 1979, eicosanoids were further identified, including thromboxanes, prostacyclins, and leukotrienes. The eicosanoids typically have a short period of activity in the body, starting with synthesis from fatty acids and ending with metabolism by enzymes. If the rate of synthesis exceeds the rate of metabolism, the excess eicosanoids may have deleterious effects. Researchers found that certain omega−3 fatty acids are also converted into eicosanoids and docosanoids, but at a slower rate. If both omega−3 and omega−6 fatty acids are present, they will \"compete\" to be transformed, so the ratio of long-chain omega−3:omega−6 fatty acids directly affects the type of eicosanoids that are produced.\n\nHumans can convert short-chain omega−3 fatty acids to long-chain forms (EPA, DHA) with an efficiency below 5%. The omega−3 conversion efficiency is greater in women than in men, but less studied. Higher ALA and DHA values found in plasma phospholipids of women may be due to the higher activity of desaturases, especially that of delta-6-desaturase.\n\nThese conversions occur competitively with omega−6 fatty acids, which are essential closely related chemical analogues that are derived from linoleic acid. They both utilize the same desaturase and elongase proteins in order to synthesize inflammatory regulatory proteins. The products of both pathways are vital for growth making a balanced diet of omega−3 and omega−6 important to an individual's health. A balanced intake ratio of 1:1 was believed to be ideal in order for proteins to be able to synthesize both pathways sufficiently, but this has been controversial as of recent research.\n\nThe conversion of ALA to EPA and further to DHA in humans has been reported to be limited, but varies with individuals. Women have higher ALA-to-DHA conversion efficiency than men, which is presumed to be due to the lower rate of use of dietary ALA for beta-oxidation. One preliminary study showed that EPA can be increased by lowering the amount of dietary linoleic acid, and DHA can be increased by elevating intake of dietary ALA.\n\nHuman diet has changed rapidly in recent centuries resulting in a reported increased diet of omega−6 in comparison to omega−3. The rapid evolution of human diet away from a 1:1 omega−3 and omega−6 ratio, such as during the Neolithic Agricultural Revolution, has presumably been too fast for humans to have adapted to biological profiles adept at balancing omega−3 and omega−6 ratios of 1:1. This is commonly believed to be the reason why modern diets are correlated with many inflammatory disorders. While omega−3 polyunsaturated fatty acids may be beneficial in preventing heart disease in humans, the level of omega−6 polyunsaturated fatty acids (and, therefore, the ratio) does not matter.\n\nBoth omega−6 and omega−3 fatty acids are essential: humans must consume them in their diet. Omega−6 and omega−3 eighteen-carbon polyunsaturated fatty acids compete for the same metabolic enzymes, thus the omega−6:omega−3 ratio of ingested fatty acids has significant influence on the ratio and rate of production of eicosanoids, a group of hormones intimately involved in the body's inflammatory and homeostatic processes, which include the prostaglandins, leukotrienes, and thromboxanes, among others. Altering this ratio can change the body's metabolic and inflammatory state. In general, grass-fed animals accumulate more omega−3 than do grain-fed animals, which accumulate relatively more omega−6. Metabolites of omega−6 are more inflammatory (esp. arachidonic acid) than those of omega−3. This necessitates that omega−6 and omega−3 be consumed in a balanced proportion; healthy ratios of omega−6:omega−3, according to some authors, range from 1:1 to 1:4. Other authors believe that a ratio of 4:1 (4 times as much omega−6 as omega−3) is already healthy. Studies suggest the evolutionary human diet, rich in game animals, seafood, and other sources of omega−3, may have provided such a ratio.\n\nTypical Western diets provide ratios of between 10:1 and 30:1 (i.e., dramatically higher levels of omega−6 than omega−3). The ratios of omega−6 to omega−3 fatty acids in some common vegetable oils are: canola 2:1, hemp 2–3:1, soybean 7:1, olive 3–13:1, sunflower (no omega−3), flax 1:3, cottonseed (almost no omega−3), peanut (no omega−3), grapeseed oil (almost no omega−3) and corn oil 46:1.\n\nAlthough omega−3 fatty acids have been known as essential to normal growth and health since the 1930s, awareness of their health benefits has dramatically increased since the 1980s.\n\nOn September 8, 2004, the U.S. Food and Drug Administration gave \"qualified health claim\" status to EPA and DHA omega−3 fatty acids, stating, \"supportive but not conclusive research shows that consumption of EPA and DHA [omega−3] fatty acids may reduce the risk of coronary heart disease\". This updated and modified their health risk advice letter of 2001 (see below).\n\nThe Canadian Food Inspection Agency has recognized the importance of DHA omega−3 and permits the following claim for DHA: \"DHA, an omega−3 fatty acid, supports the normal physical development of the brain, eyes and nerves primarily in children under two years of age.\"\n\nHistorically, whole food diets contained sufficient amounts of omega−3, but because omega−3 is readily oxidized, the trend to shelf-stable, processed foods has led to a deficiency in omega−3 in manufactured foods.\n\nIn the United States, the Institute of Medicine publishes a system of Dietary Reference Intakes, which includes Recommended Dietary Allowances (RDAs) for individual nutrients, and Acceptable Macronutrient Distribution Ranges (AMDRs) for certain groups of nutrients, such as fats. When there is insufficient evidence to determine an RDA, the institute may publish an Adequate Intake (AI) instead, which has a similar meaning, but is less certain. The AI for α-linolenic acid is 1.6 grams/day for men and 1.1 grams/day for women, while the AMDR is 0.6% to 1.2% of total energy. Because the physiological potency of EPA and DHA is much greater than that of ALA, it is not possible to estimate one AMDR for all omega−3 fatty acids. Approximately 10 percent of the AMDR can be consumed as EPA and/or DHA. The Institute of Medicine has not established a RDA or AI for EPA, DHA or the combination, so there is no Daily Value (DVs are derived from RDAs), no labeling of foods or supplements as providing a DV percentage of these fatty acids per serving, and no labeling a food or supplement as an excellent source, or \"High in...\" As for safety, there was insufficient evidence as of 2005 to set an upper tolerable limit for omega−3 fatty acids, although the FDA has advised that adults can safely consume up to a total of 3 grams per day of combined DHA and EPA, with no more than 2 g from dietary supplements.\n\nThe American Heart Association (AHA) has made recommendations for EPA and DHA due to their cardiovascular benefits: individuals with no history of coronary heart disease or myocardial infarction should consume oily fish two times per week; and \"Treatment is reasonable\" for those having been diagnosed with coronary heart disease. For the latter the AHA does not recommend a specific amount of EPA + DHA, although it notes that most trials were at or close to 1000 mg/day. The benefit appears to be on the order of a 9% decrease in relative risk. The European Food Safety Authority (EFSA) approved a claim \"EPA and DHA contributes to the normal function of the heart\" for products that contain at least 250 mg EPA + DHA. The report did not address the issue of people with pre-existing heart disease. The World Health Organization recommends regular fish consumption (1-2 servings per week, equivalent to 200 to 500 mg/day EPA + DHA) as protective against coronary heart disease and ischaemic stroke.\n\nHeavy metal poisoning by the body's accumulation of traces of heavy metals, in particular mercury, lead, nickel, arsenic, and cadmium, is a possible risk from consuming fish oil supplements. Also, other contaminants (PCBs, furans, dioxins, and PBDEs) might be found, especially in less-refined fish oil supplements. However, heavy metal toxicity from consuming fish oil supplements is highly unlikely, because heavy metals selectively bind with protein in the fish flesh rather than accumulate in the oil. An independent test in 2005 of 44 fish oils on the US market found all of the products passed safety standards for potential contaminants.\n\nThroughout their history, the Council for Responsible Nutrition and the World Health Organization have published acceptability standards regarding contaminants in fish oil. The most stringent current standard is the International Fish Oils Standard. Fish oils that are molecularly distilled under vacuum typically make this highest-grade; levels of contaminants are stated in parts per billion per trillion.\n\nThe most widely available dietary source of EPA and DHA is oily fish, such as salmon, herring, mackerel, anchovies, menhaden, and sardines. Oils from these fish have a profile of around seven times as much omega−3 as omega−6. Other oily fish, such as tuna, also contain \"n\"-3 in somewhat lesser amounts. Consumers of oily fish should be aware of the potential presence of heavy metals and fat-soluble pollutants like PCBs and dioxins, which are known to accumulate up the food chain. After extensive review, researchers from Harvard's School of Public Health in the \"Journal of the American Medical Association\" (2006) reported that the benefits of fish intake generally far outweigh the potential risks. Although fish are a dietary source of omega−3 fatty acids, fish do not synthesize them; they obtain them from the algae (microalgae in particular) or plankton in their diets. In the case of farmed fish, omega-3 fatty acids is provided by fish oil; In 2009, 81% of the global fish oil production is used by aquaculture.\n\nMarine and freshwater fish oil vary in content of arachidonic acid, EPA and DHA. They also differ in their effects on organ lipids.\n\nNot all forms of fish oil may be equally digestible. Of four studies that compare bioavailability of the glyceryl ester form of fish oil vs. the ethyl ester form, two have concluded the natural glyceryl ester form is better, and the other two studies did not find a significant difference. No studies have shown the ethyl ester form to be superior, although it is cheaper to manufacture. \n\nKrill oil is a source of omega−3 fatty acids. The effect of krill oil, at a lower dose of EPA + DHA (62.8%), was demonstrated to be similar to that of fish oil on blood lipid levels and markers of inflammation in healthy humans. While not an endangered species, krill are a mainstay of the diets of many ocean-based species including whales, causing environmental and scientific concerns about their sustainability.\n\nTable 1. ALA content as the percentage of the seed oil.\nTable 2. ALA content as the percentage of the whole food.\n\nFlaxseed (or linseed) (\"Linum usitatissimum\") and its oil are perhaps the most widely available botanical source of the omega−3 fatty acid ALA. Flaxseed oil consists of approximately 55% ALA, which makes it six times richer than most fish oils in omega−3 fatty acids. A portion of this is converted by the body to EPA and DHA, though the actual converted percentage may differ between men and women.\n\nIn 2013 Rothamsted Research in the UK reported they had developed a genetically modified form of the plant Camelina that produced EPA and DHA. Oil from the seeds of this plant contained on average 11% EPA and 8% DHA in one development and 24% EPA in another.\n\nEggs produced by hens fed a diet of greens and insects contain higher levels of omega−3 fatty acids than those produced by chickens fed corn or soybeans. In addition to feeding chickens insects and greens, fish oils may be added to their diets to increase the omega−3 fatty acid concentrations in eggs.\n\nThe addition of flax and canola seeds to the diets of chickens, both good sources of alpha-linolenic acid, increases the omega−3 content of the eggs, predominantly DHA.\n\nThe addition of green algae or seaweed to the diets boosts the content of DHA and EPA, which are the forms of omega−3 approved by the FDA for medical claims. A common consumer complaint is \"Omega−3 eggs can sometimes have a fishy taste if the hens are fed marine oils\".\n\nOmega−3 fatty acids are formed in the chloroplasts of green leaves and algae. While seaweeds and algae are the source of omega−3 fatty acids present in fish, grass is the source of omega−3 fatty acids present in grass fed animals. When cattle are taken off omega−3 fatty acid rich grass and shipped to a feedlot to be fattened on omega−3 fatty acid deficient grain, they begin losing their store of this beneficial fat. Each day that an animal spends in the feedlot, the amount of omega−3 fatty acids in its meat is diminished.\n\nThe omega−6:omega−3 ratio of grass-fed beef is about 2:1, making it a more useful source of omega−3 than grain-fed beef, which usually has a ratio of 4:1.\n\nIn a 2009 joint study by the USDA and researchers at Clemson University in South Carolina, grass-fed beef was compared with grain-finished beef. The researchers found that grass-finished beef is higher in moisture content, 42.5% lower total lipid content, 54% lower in total fatty acids, 54% higher in beta-carotene, 288% higher in vitamin E (alpha-tocopherol), higher in the B-vitamins thiamin and riboflavin, higher in the minerals calcium, magnesium, and potassium, 193% higher in total omega−3s, 117% higher in CLA (cis-9, trans-11 octadecenoic acid, a cojugated linoleic acid, which is a potential cancer fighter), 90% higher in vaccenic acid (which can be transformed into CLA), lower in the saturated fats linked with heart disease, and has a healthier ratio of omega−6 to omega−3 fatty acids (1.65 vs 4.84). Protein and cholesterol content were equal.\n\nIn most countries, commercially available lamb is typically grass-fed, and thus higher in omega−3 than other grain-fed or grain-finished meat sources. In the United States, lamb is often finished (i.e., fattened before slaughter) with grain, resulting in lower omega−3.\n\nThe omega−3 content of chicken meat may be enhanced by increasing the animals' dietary intake of grains high in omega−3, such as flax, chia, and canola.\n\nKangaroo meat is also a source of omega−3, with fillet and steak containing 74 mg per 100 g of raw meat.\n\nSeal oil is a source of EPA, DPA, and DHA. According to Health Canada, it helps to support the development of the brain, eyes, and nerves in children up to 12 years of age. Like all seal products, it is not allowed to be imported into the European Union.\n\nA recent trend has been to fortify food with omega−3 fatty acid supplements. Global food companies have launched omega−3 fatty acid fortified bread, mayonnaise, pizza, yogurt, orange juice, children's pasta, milk, eggs, popcorn, confections, and infant formula.\n\nThe microalgae \"Crypthecodinium cohnii\" and \"Schizochytrium\" are rich sources of DHA but not EPA, and can be produced commercially in bioreactors. Oil from brown algae (kelp) is a source of EPA. The alga \"Nannochloropsis\" also has high levels of EPA.\n\nIn 2006 the Journal of Dairy Science published a study which found that butter made from the milk of grass-fed cows contains substantially more α-linolenic acid than butter made from the milk of cows that have limited access to pasture.\n\n"}
{"id": "24458", "url": "https://en.wikipedia.org/wiki?curid=24458", "title": "Polyvinyl chloride", "text": "Polyvinyl chloride\n\nPolyvinyl chloride (; colloquial: polyvinyl, vinyl; abbreviated: PVC) is the world's third-most widely produced synthetic plastic polymer, after polyethylene and polypropylene. About 40 million tonnes are produced per year.\n\nPVC comes in two basic forms: rigid (sometimes abbreviated as RPVC) and flexible. The rigid form of PVC is used in construction for pipe and in profile applications such as doors and windows. It is also used in making bottles, non-food packaging, and cards (such as bank or membership cards). It can be made softer and more flexible by the addition of plasticizers, the most widely used being phthalates. In this form, it is also used in plumbing, electrical cable insulation, imitation leather, flooring, signage, phonograph records, inflatable products, and many applications where it replaces rubber. With cotton or linen, it is used to make canvas.\n\nPure polyvinyl chloride is a white, brittle solid. It is insoluble in alcohol but slightly soluble in tetrahydrofuran.\n\nPVC was accidentally synthesized in 1872 by German chemist Eugen Baumann. The polymer appeared as a white solid inside a flask of vinyl chloride that had been left exposed to sunlight. In the early 20th century the Russian chemist Ivan Ostromislensky and Fritz Klatte of the German chemical company Griesheim-Elektron both attempted to use PVC in commercial products, but difficulties in processing the rigid, sometimes brittle polymer thwarted their efforts. Waldo Semon and the B.F. Goodrich Company developed a method in 1926 to plasticize PVC by blending it with various additives. The result was a more flexible and more easily processed material that soon achieved widespread commercial use.\n\nPolyvinyl chloride is produced by polymerization of the vinyl chloride monomer (VCM), as shown.\nAbout 80% of production involves suspension polymerization. Emulsion polymerization accounts for about 12%, and bulk polymerization accounts for 8%. Suspension polymerization affords particles with average diameters of 100–180 μm, whereas emulsion polymerization gives much smaller particles of average size around 0.2 μm. VCM and water are introduced into the reactor along with a polymerization initiator and other additives. The contents of the reaction vessel are pressurized and continually mixed to maintain the suspension and ensure a uniform particle size of the PVC resin. The reaction is exothermic and thus requires cooling. As the volume is reduced during the reaction (PVC is denser than VCM), water is continually added to the mixture to maintain the suspension.\n\nThe polymerization of VCM is started by compounds called initiators that are mixed into the droplets. These compounds break down to start the radical chain reaction. Typical initiators include dioctanoyl peroxide and dicetyl peroxydicarbonate, both of which have fragile O-O bonds. Some initiators start the reaction rapidly but decay quickly, and other initiators have the opposite effect. A combination of two different initiators is often used to give a uniform rate of polymerization. After the polymer has grown by about 10 times, the short polymer precipitates inside the droplet of VCM, and polymerization continues with the precipitated, solvent-swollen particles. The weight average molecular weights of commercial polymers range from 100,000 to 200,000, and the number average molecular weights range from 45,000 to 64,000.\n\nOnce the reaction has run its course, the resulting PVC slurry is degassed and stripped to remove excess VCM, which is recycled. The polymer is then passed through a centrifuge to remove water. The slurry is further dried in a hot air bed, and the resulting powder is sieved before storage or pelletization. Normally, the resulting PVC has a VCM content of less than 1 part per million. Other production processes, such as micro-suspension polymerization and emulsion polymerization, produce PVC with smaller particle sizes (10 μm vs. 120–150 μm for suspension PVC) with slightly different properties and with somewhat different sets of applications.\n\nPVC may be manufactured from either naphtha or ethylene feedstock.\n\nThe polymers are linear and are strong. The monomers are mainly arranged head-to-tail, meaning that there are chlorides on alternating carbon centres. PVC has mainly an atactic stereochemistry, which means that the relative stereochemistry of the chloride centres are random. Some degree of syndiotacticity of the chain gives a few percent crystallinity that is influential on the properties of the material. About 57% of the mass of PVC is chlorine. The presence of chloride groups gives the polymer very different properties from the structurally related material polyethylene.\n\nAbout half of the world's PVC production capacity is in China, but many Chinese PVC plants have been shut down due to issues complying with environmental regulations. The largest single producer of PVC as of 2018 is Shin-Etsu Chemical of Japan, with a global share of around 30%.\n\nThe product of the polymerization process is unmodified PVC. Before PVC can be made into finished products, it always requires conversion into a compound by the incorporation of additives (but not necessarily all of the following) such as heat stabilizers, UV stabilizers, plasticizers, processing aids, impact modifiers, thermal modifiers, fillers, flame retardants, biocides, blowing agents and smoke suppressors, and, optionally, pigments. The choice of additives used for the PVC finished product is controlled by the cost performance requirements of the end use specification (underground pipe, window frames, intravenous tubing and flooring all have very different ingredients to suit their performance requirements). Previously, polychlorinated biphenyls (PCBs) were added to certain PVC products as flame retardants and stabilizers.\n\nMost vinyl products contain plasticizers which dramatically improve their performance characteristic. The most common plasticizers are derivatives of phthalic acid. The materials are selected on their compatibility with the polymer, low volatility levels, and cost. These materials are usually oily colourless substances that mix well with the PVC particles. About 90% of the plasticizer market, estimated to be millions of tons per year worldwide, is dedicated to PVC.\n\nLiquid mixed metal stabilisers are used in several PVC flexible applications such as calendered films, extruded profiles, injection moulded soles and footwear, extruded hoses and plastisols where PVC paste is spread on to a backing (flooring, wall covering, artificial leather). Liquid mixed metal stabiliser systems are primarily based on barium, zinc and calcium carboxylates. In general liquid mixed metals like BaZn, CaZn require the addition of co-stabilisers, antioxidants and organo-phosphites to provide optimum performance.\n\nBaZn stabilisers have successfully replaced cadmium-based stabilisers in Europe in many PVC semi-rigid and flexible applications.\n\nIn Europe, particularly Belgium, there has been a commitment to eliminate the use of cadmium (previously used as a part component of heat stabilizers in window profiles) and phase out lead-based heat stabilizers (as used in pipe and profile areas) such as liquid autodiachromate and calcium polyhydrocummate by 2015. According to the final report of Vinyl 2010 cadmium was eliminated across Europe by 2007. The progressive substitution of lead-based stabilizers is also confirmed in the same document showing a reduction of 75% since 2000 and ongoing. This is confirmed by the corresponding growth in calcium-based stabilizers, used as an alternative to lead-based stabilizers, more and more, also outside Europe.\n\nTin-based stabilizers are mainly used in Europe for rigid, transparent applications due to the high temperature processing conditions used. The situation in North America is different where tin systems are used for almost all rigid PVC applications.\nTin stabilizers can be divided into two main groups, the first group containing those with tin-oxygen bonds and the second group with tin-sulphur bonds.\n\nOne of the most crucial additives are heat stabilizers. These agents minimize loss of HCl, a degradation process that starts above 70 °C. Once dehydrochlorination starts, it is autocatalytic. Many diverse agents have been used including, traditionally, derivatives of heavy metals (lead, cadmium). Increasingly, metallic soaps (metal \"salts\" of fatty acids) are favored, species such as calcium stearate. Addition levels vary typically from 2% to 4%.\nThe choice of the best heat stabilizer depends on its cost effectiveness in the end use application, performance specification requirements, processing technology and regulatory approvals.\n\nDi-2ethylhexylphthalate (DEHP) has been medically approved for many years for use in medical devices; the PVC-DEHP combination proving to be very suitable for making blood bags because DEHP stabilises red blood cells, minimising haemolysis (red blood cell rupture). However, DEHP is coming under increasing pressure in Europe. The assessment of potential risks related to phthalates, and in particular the use of DEHP in PVC medical devices, was subject to scientific and policy review by the European Union authorities, and on 21 March 2010, a specific labeling requirement was introduced across the EU for all devices containing phthalates that are classified as CMR (carcinogenic, mutagenic or toxic to reproduction). The label aims to enable healthcare professionals to use this equipment safely, and, where needed, take appropriate precautionary measures for patients at risk of over-exposure.\n\nDEHP alternatives, which are gradually replacing it, are Adipates, Butyryltrihexylcitrate (BTHC), Cyclohexane-1,2-dicarboxylic acid, diisononylester (DINCH), Di(2-ethylhexyl)terephthalate, polymerics and trimellitic acid, 2-ethylhexylester (TOTM).\n\nPVC is a thermoplastic polymer. Its properties are usually categorized based on rigid and flexible PVCs.\nPVC has high hardness and mechanical properties. The mechanical properties enhance with the molecular weight increasing but decrease with the temperature increasing. The mechanical properties of rigid PVC (uPVC) are very good; the elastic modulus can reach 1500-3,000 MPa. The soft PVC (flexible PVC) elastic limit is 1.5–15 MPa.\n\nThe heat stability of raw PVC is very poor, so the addition of a heat stabilizer during the process is necessary in order to ensure the product's properties. Traditional product PVC has a maximum operating temperature around 140°F (60°C) when heat distortion begins to occur. Melting temperatures range from 212°F to 500°F (100°C to 260°C) depending upon manufacture additives to the PVC. The linear expansion coefficient of rigid PVC is small and has good flame retardancy, the limiting oxygen index (LOI) being up to 45 or more. The LOI is the minimum concentration of oxygen, expressed as a percentage, that will support combustion of a polymer and noting that air has 20% content of oxygen.\n\nAs a thermoplastic, PVC has an inherent insulation that aids in reducing condensation formation and resisting internal temperature changes for hot and cold liquids.\n\nPVC is a polymer with good insulation properties, but because of its higher polar nature the electrical insulating property is inferior to non polar polymers such as polyethylene and polypropylene.\n\nSince the dielectric constant, dielectric loss tangent value, and volume resistivity are high, the corona resistance is not very good, and it is generally suitable for medium or low voltage and low frequency insulation materials.\n\nPVC is chemically resistant to acids, salts, bases, fats, and alcohols, making it resistant to the corrosive effects of sewage, which is why it is so extensively utilized in sewer piping systems. It is also resistant to some solvents, this, however, is reserved mainly for uPVC (unplasticized PVC). Plasticized PVC, also known as PVC-P, is in some cases less resistant to solvents. For example, PVC is resistant to fuel and some paint thinners. Some solvents may only swell it or deform it but not dissolve it, but some, like tetrahydrofuran or acetone, may damage it.\n\nRoughly half of the world's polyvinyl chloride resin manufactured annually is used for producing pipes for municipal and industrial applications. In the water distribution market, it accounts for 66% of the market in the U.S., and in sanitary sewer pipe applications, it accounts for 75%. Buried PVC pipes in both water and sanitary sewer applications that are in diameter and larger are typically joined by means of a gasket-sealed joint. The most common type of gasket utilized in North America is a metal reinforced elastomer, commonly referred to as a Rieber sealing system. Its light weight, low cost, and low maintenance make it attractive. However, it must be carefully installed and bedded to ensure longitudinal cracking and overbelling does not occur. Additionally, PVC pipes can be fused together using various solvent cements, or heat-fused (butt-fusion process, similar to joining high-density polyethylene (HDPE) pipe), creating permanent joints that are virtually impervious to leakage.\n\nIn February 2007 the California Building Standards Code was updated to approve the use of chlorinated polyvinyl chloride (CPVC) pipe for use in residential water supply piping systems. CPVC has been a nationally accepted material in the U.S. since 1982; California, however, has permitted only limited use since 2001. The Department of Housing and Community Development prepared and certified an environmental impact statement resulting in a recommendation that the commission adopt and approve the use of CPVC. The commission's vote was unanimous, and CPVC has been placed in the 2007 California Plumbing Code.\n\nPVC is commonly used as the insulation on electrical cables such as teck; PVC used for this purpose needs to be plasticized.\nFlexible PVC coated wire and cable for electrical use has traditionally been stabilised with lead, but these are being replaced with calcium-based systems.\n\nIn a fire, PVC-coated wires can form hydrogen chloride fumes; the chlorine serves to scavenge free radicals and is the source of the material's fire retardance. While hydrogen chloride fumes can also pose a health hazard in their own right, it dissolves in moisture and breaks down onto surfaces, particularly in areas where the air is cool enough to breathe, and is not available for inhalation. Frequently in applications where smoke is a major hazard (notably in tunnels and communal areas), PVC-free cable insulation is preferred, such as low smoke zero halogen (LSZH) insulation.\n\nPVC is a common, strong but lightweight plastic used in construction. It is made softer and more flexible by the addition of plasticizers. If no plasticizers are added, it is known as uPVC (unplasticized polyvinyl chloride) or rigid PVC.\n\nuPVC is extensively used in the building industry as a low-maintenance material, particularly in Ireland, the United Kingdom, in the United States and Canada. In the U.S. and Canada it is known as vinyl or vinyl siding. The material comes in a range of colors and finishes, including a photo-effect wood finish, and is used as a substitute for painted wood, mostly for window frames and sills when installing insulated glazing in new buildings; or to replace older single-glazed windows, as it does not decompose and is weather-resistant. Other uses include fascia, and siding or weatherboarding. This material has almost entirely replaced the use of cast iron for plumbing and drainage, being used for waste pipes, drainpipes, gutters and downspouts. uPVC is known as having strong resistance against chemicals, sunlight, and oxidation from water.\nPolyvinyl chloride is formed in flat sheets in a variety of thicknesses and colors. As flat sheets, PVC is often expanded to create voids in the interior of the material, providing additional thickness without additional weight and minimal extra cost (see Closed-cell PVC foamboard). Sheets are cut using saws and rotary cutting equipment. Plasticized PVC is also used to produce thin, colored, or clear, adhesive-backed films referred to simply as vinyl. These films are typically cut on a computer-controlled plotter (see Vinyl cutter) or printed in a wide-format printer. These sheets and films are used to produce a wide variety of commercial signage products, including car body stripes and stickers.\n\nPVC fabric is water-resistant, used for its weather-resistant qualities in coats, skiing equipment, shoes, jackets, aprons, and sports bags.\n\nPVC fabric has a niche role in specialty clothing, to either create a artificial leather material or at times simply for its effect. PVC clothing is common in Goth, Punk, clothing fetish and alternative fashions. PVC is less expensive than rubber, leather, and latex which it is used to simulate.\n\nThe two main application areas for single-use medically approved PVC compounds are flexible containers and tubing: containers used for blood and blood components, for urine collection or for ostomy products and tubing used for blood taking and blood giving sets, catheters, heart-lung bypass sets, hemodialysis sets etc. In Europe the consumption of PVC for medical devices is approximately 85,000 tons each year. Almost one third of plastic-based medical devices are made from PVC.\nThe reasons for using flexible PVC in these applications for over 50 years are numerous and based on cost effectiveness linked to transparency, light weight, softness, tear strength, kink resistance, suitability for sterilization and biocompatibility.\n\nFlexible PVC flooring is inexpensive and used in a variety of buildings, including homes, hospitals, offices, and schools. Complex and 3D designs are possible, which are then protected by a clear wear layer. A middle vinyl foam layer also gives a comfortable and safe feel. The smooth, tough surface of the upper wear layer prevents the buildup of dirt, which prevents microbes from breeding in areas that need to be kept sterile, such as hospitals and clinics.\n\nPVC may be extruded under pressure to encase wire rope and aircraft cable used for general purpose applications. PVC coated wire rope is easier to handle, resists corrosion and abrasion, and may be color-coded for increased visibility. It is found in a variety of industries and environments both indoor and out.\n\nPVC has been used for a host of consumer products. One of its earliest mass-market consumer applications was vinyl record production. More recent examples include wallcovering, greenhouses, home playgrounds, foam and other toys, custom truck toppers (tarpaulins), ceiling tiles and other kinds of interior cladding.\n\nPVC piping is cheaper than metals used in musical instrument making; it is therefore a common alternative when making instruments, often for leisure or for rarer instruments such as the contrabass flute.\n\nPVC can be usefully modified by chlorination, which increases its chlorine content to 67%. CPVC, as it is called, is produced by chlorination of aqueous solution of suspension PVC particles followed by exposure to UV light which initiates the free-radical chlorination. An extensive market for CPVC is in pipe for use in office building, apartment and condominium fire protection. CPVC also has a higher heat resistance so is primarily used for hot water pipe and fittings, but it is more expensive and it is found only in niche applications, such as some water heaters and specialized clothing.\n\nDegradation during service life, or after careless disposal, is a chemical change that drastically reduces the average molecular weight of the polyvinyl chloride polymer. Since the mechanical integrity of a plastic depends on its high average molecular weight, wear and tear inevitably weakens the material. Weathering degradation of plastics results in their surface embrittlement and microcracking, yielding microparticles that continue on in the environment. Also known as microplastics, these particles act like sponges and soak up persistent organic pollutants (POPs) around them. Thus laden with high levels of POPs, the microparticles are often ingested by organisms in the biosphere.\n\nHowever, there is evidence that three of the polymers (HDPE, LDPE, and PP) consistently soaked up POPs at concentrations an order of magnitude higher than did the remaining two (PVC and PET). After 12 months of exposure, for example, there was a 34-fold difference in average total POPs amassed on LDPE compared to PET at one location. At another site, average total POPs adhered to HDPE was nearly 30 times that of PVC. The researchers think that differences in the size and shape of the polymer molecules can explain why some accumulate more pollutants than others. \nThe fungus Aspergillus fumigatus effectively degrades plasticized PVC. Phanerochaete chrysosporium was grown on PVC in a mineral salt agar. Phanerochaete chrysosporium, Lentinus tigrinus, Aspergillus niger, and Aspergillus sydowii can effectively degrade PVC.\n\nPhthalates, which are incorporated into plastics as plasticizers, comprise approximately 70% of the U.S. plasticizer market; phthalates are by design not covalently bound to the polymer matrix, which makes them highly susceptible to leaching. Phthalates are contained in plastics at high percentages. For example, they can contribute up to 40% by weight to intravenous medical bags and up to 80% by weight in medical tubing. Vinyl products are pervasive—including toys, car interiors, shower curtains, and flooring—and initially release chemical gases into the air. Some studies indicate that this outgassing of additives may contribute to health complications, and have resulted in a call for banning the use of DEHP on shower curtains, among other uses. Japanese car companies Toyota, Nissan, and Honda eliminated the use of PVC in car interiors since 2007.\n\nIn 2004 a joint Swedish-Danish research team found a statistical association between allergies in children and indoor air levels of DEHP and BBzP (butyl benzyl phthalate), which is used in vinyl flooring. In December 2006, the European Chemicals Bureau of the European Commission released a final draft risk assessment of BBzP which found \"no concern\" for consumer exposure including exposure to children.\n\nRisk assessments have led to the classification of low molecular weight and labeling as Category 1B Reproductive agents. Three of these phthalates, DBP, BBP and DEHP were included on annex XIV of the REACH regulation in February 2011 and will be phased out by the EU by February 2015 unless an application for authorisation is made before July 2013 and an authorisation granted. DIBP is still on the REACH Candidate List for Authorisation. Environmental Science & Technology, a peer reviewed journal published by the American Chemical Society states DEHP poses a serious risk to human health.\n\nIn 2008 the European Union's Scientific Committee on Emerging and Newly Identified Health Risks (SCENIHR) reviewed the safety of DEHP in medical devices. The SCENIHR report states that certain medical procedures used in high risk patients result in a significant exposure to DEHP and concludes there is still a reason for having some concerns about the exposure of prematurely born male babies to medical devices containing DEHP. The Committee said there are some alternative plasticizers available for which there is sufficient toxicological data to indicate a lower hazard compared to DEHP but added that the functionality of these plasticizers should be assessed before they can be used as an alternative for DEHP in PVC medical devices. Risk assessment results have shown positive results regarding the safe use of High Molecular Weight Phthalates. They have all been registered for REACH and do not require any classification for health and environmental effects, nor are they on the Candidate List for Authorisation. High phthalates are not CMR (carcinogenic, mutagenic or toxic for reproduction), neither are they considered endocrine disruptors.\n\nIn the EU Risk Assessment the European Commission has confirmed that Di-isononyl phthalate (DINP) and Di-isodecyl phthalate (DIDP) pose no risk to either human health or the environment from any current use.\nThe European Commission's findings (published in the EU Official Journal on 13 April 2006) confirm the outcome of a risk assessment involving more than 10 years of extensive scientific evaluation by EU regulators.\nFollowing the recent adoption of EU legislation with the regard to the marketing and use of DINP in toys and childcare articles, the risk assessment conclusions clearly state that there is no need for any further measures to regulate the use of DINP.\nIn Europe and in some other parts of the world, the use of DINP in toys and childcare items has been restricted as a precautionary measure. In Europe, for example, DINP can no longer be used in toys and childcare items that can be put in the mouth even though the EU scientific risk assessment concluded that its use in toys does not pose a risk to human health or the environment.\nThe rigorous EU risk assessments, which include a high degree of conservatism and built-in safety factors, have been carried out under the strict supervision of the European Commission and provide a clear scientific evaluation on which to judge whether or not a particular substance can be safely used.\n\nThe FDA Paper titled \"Safety Assessment of Di(2-ethylhexyl)phthalate (DEHP)Released from PVC Medical Devices\" states that [3.2.1.3] Critically ill or injured patients may be at increased risk of developing adverse health effects from DEHP, not only by virtue of increased exposure relative to the general population, but also because of the physiological and pharmacodynamic changes that occur in these patients compared to healthy individuals.\n\nLead had previously been frequently added to PVC to improve workability and stability. Lead has been shown to leach into drinking water from PVC pipes.\n\nIn Europe (EU 28) the use of lead-based stabilizers was gradually replaced by the end of 2015, under the VinylPlus voluntary commitment, ESPA members completed the replacement of Pb-based stabilisers.\n\nIn the early 1970s, the carcinogenicity of vinyl chloride (usually called vinyl chloride monomer or VCM) was linked to cancers in workers in the polyvinyl chloride industry. Specifically workers in polymerization section of a B.F. Goodrich plant near Louisville, Kentucky, were diagnosed with liver angiosarcoma also known as hemangiosarcoma, a rare disease. Since that time, studies of PVC workers in Australia, Italy, Germany, and the UK have all associated certain types of occupational cancers with exposure to vinyl chloride, and it has become accepted that VCM is a carcinogen. Technology for removal of VCM from products has become stringent, commensurate with the associated regulations.\n\nPVC produces HCl upon combustion almost quantitatively related to its chlorine content. Extensive studies in Europe indicate that the chlorine found in emitted dioxins is not derived from HCl in the flue gases. Instead, most dioxins arise in the condensed solid phase by the reaction of inorganic chlorides with graphitic structures in char-containing ash particles. Copper acts as a catalyst for these reactions.\n\nStudies of household waste burning indicate consistent increases in dioxin generation with increasing PVC concentrations. According to the EPA dioxin inventory, landfill fires are likely to represent an even larger source of dioxin to the environment. A survey of international studies consistently identifies high dioxin concentrations in areas affected by open waste burning and a study that looked at the homologue pattern found the sample with the highest dioxin concentration was \"typical for the pyrolysis of PVC\". Other EU studies indicate that PVC likely \"accounts for the overwhelming majority of chlorine that is available for dioxin formation during landfill fires.\"\n\nThe next largest sources of dioxin in the EPA inventory are medical and municipal waste incinerators. Various studies have been conducted that reach contradictory results. For instance a study of commercial-scale incinerators showed no relationship between the PVC content of the waste and dioxin emissions. Other studies have shown a clear correlation between dioxin formation and chloride content and indicate that PVC is a significant contributor to the formation of both dioxin and PCB in incinerators.\n\nIn February 2007, the Technical and Scientific Advisory Committee of the US Green Building Council (USGBC) released its report on a PVC avoidance related materials credit for the LEED Green Building Rating system. The report concludes that \"no single material shows up as the best across all the human health and environmental impact categories, nor as the worst\" but that the \"risk of dioxin emissions puts PVC consistently among the worst materials for human health impacts.\"\n\nIn Europe the overwhelming importance of combustion conditions on dioxin formation has been established by numerous researchers. The single most important factor in forming dioxin-like compounds is the temperature of the combustion gases. Oxygen concentration also plays a major role on dioxin formation, but not the chlorine content.\n\nThe design of modern incinerators minimises PCDD/F formation by optimising the stability of the thermal process. To comply with the EU emission limit of 0.1 ng I-TEQ/m3 modern incinerators operate in conditions minimising dioxin formation and are equipped with pollution control devices which catch the low amounts produced. Recent information is showing for example that dioxin levels in populations near incinerators in Lisbon and Madeira have not risen since the plants began operating in 1999 and 2002 respectively.\n\nSeveral studies have also shown that removing PVC from waste would not significantly reduce the quantity of dioxins emitted. The European Union Commission published in July 2000 a Green Paper on the Environmental Issues of PVC. \" The Commission states (page 27) that it has been suggested that the reduction of the chlorine content in the waste can contribute to the reduction of dioxin formation, even though the actual mechanism is not fully understood. The influence on the reduction is also expected to be a second or third order relationship. It is most likely that the main incineration parameters, such as the temperature and the oxygen concentration, have a major influence on the dioxin formation\". The Green Paper states further that at the current levels of chlorine in municipal waste, there does not seem to be a direct quantitative relationship between chlorine content and dioxin formation.\n\nA study commissioned by the European Commission on \"Life Cycle Assessment of PVC and of principal competing materials\" states that \"Recent studies show that the presence of PVC has no significant effect on the amount of dioxins released through incineration of plastic waste.\"\n\nThe European waste hierarchy refers to the five steps included in the article 4 of the Waste Framework Directive:\n\n\nThe European Commission has set new rules to promote the recovery of PVC waste for use in a number of construction products. It says: \"The use of recovered PVC should be encouraged in the manufacture of certain construction products because it allows the reuse of old PVC ... This avoids PVC being discarded in landfills or incinerated causing release of carbon dioxide and cadmium in the environment\".\n\nIn Europe, developments in PVC waste management have been monitored by Vinyl 2010, established in 2000. Vinyl 2010's objective was to recycle 200,000 tonnes of post-consumer PVC waste per year in Europe by the end of 2010, excluding waste streams already subject to other or more specific legislation (such as the European Directives on End-of-Life Vehicles, Packaging and Waste Electric and Electronic Equipment).\n\nSince June 2011, it is followed by VinylPlus, a new set of targets for sustainable development. Its main target is to recycle 800,000 tonnes/year of PVC by 2020 including 100,000 tonnes of \"difficult to recycle\" waste. One facilitator for collection and recycling of PVC waste is Recovinyl. The reported and audited recycled PVC tonnage in 2016 was 568,695 tonnes.\nOne approach to address the problem of waste PVC is also through the process called Vinyloop. It is a mechanical recycling process using a solvent to separate PVC from other materials. This solvent turns in a closed loop process in which the solvent is recycled. Recycled PVC is used in place of virgin PVC in various applications: coatings for swimming pools, shoe soles, hoses, diaphragms tunnel, coated fabrics, PVC sheets. This recycled PVC's primary energy demand is 46 percent lower than conventional produced PVC. So the use of recycled material leads to a significant better ecological footprint. The global warming potential is 39 percent lower.\n\nIn November 2005 one of the largest hospital networks in the U.S., Catholic Healthcare West, signed a contract with B. Braun Melsungen for vinyl-free intravenous bags and tubing.\n\nIn January 2012 a major U.S. West Coast healthcare provider, Kaiser Permanente, announced that it will no longer buy intravenous (IV) medical equipment made with polyvinyl chloride (PVC) and DEHP (di-2-ethyl hexyl phthalate) type plasticizers.\n\nIn 1998, the U.S. Consumer Product Safety Commission (CPSC) arrived at a voluntary agreement with manufacturers to remove phthalates from PVC rattles, teethers, baby bottle nipples and pacifiers.\n\nPlasticized PVC is a common material for medical gloves. Due to vinyl gloves having less flexibility and elasticity, several guidelines recommend either latex or nitrile gloves for clinical care and procedures that require manual dexterity and/or that involve patient contact for more than a brief period. Vinyl gloves show poor resistance to many chemicals, including glutaraldehyde-based products and alcohols used in formulation of disinfectants for swabbing down work surfaces or in hand rubs. The additives in PVC are also known to cause skin reactions such as allergic contact dermatitis. These are for example the antioxidant bisphenol A, the biocide benzisothiazolinone, propylene glycol/adipate polyester and ethylhexylmaleate.\n\nPVC is made from petroleum. The production process also uses sodium chloride. Recycled PVC is broken down into small chips, impurities removed, and the product refined to make pure white PVC. It can be recycled roughly seven times and has a lifespan of around 140 years.\n\nIn the UK, approximately 400 tonnes of PVC are recycled every month. Property owners can recycle it through nationwide collection depots. The Olympic Delivery Authority (ODA), for example, after initially rejecting PVC as material for different temporary venues of the London Olympics 2012, has reviewed its decision and developed a policy for its use. This policy highlighted that the functional properties of PVC make it the most appropriate material in certain circumstances while taking into consideration the environmental and social impacts across the whole life cycle, e.g. the rate for recycling or reuse and the percentage of recycled content. Temporary parts, like roofing covers of the Olympic Stadium, the Water Polo Arena, and the Royal Artillery Barracks, would be deconstructed and a part recycled in the VinyLoop process.\n\n"}
{"id": "286262", "url": "https://en.wikipedia.org/wiki?curid=286262", "title": "Precipitation (chemistry)", "text": "Precipitation (chemistry)\n\nPrecipitation is the creation of a solid from a solution. When the reaction occurs in a liquid solution, the solid formed is called the 'precipitate'. The chemical that causes the solid to form is called the 'precipitant'. Without sufficient force of gravity (settling) to bring the solid particles together, the precipitate remains in suspension. After sedimentation, especially when using a centrifuge to press it into a compact mass, the precipitate may be referred to as a 'pellet'. Precipitation can be used as a medium. The precipitate-free liquid remaining above the solid is called the 'supernate' or 'supernatant'. Powders derived from precipitation have also historically been known as 'flowers'. When the solid appears in the form of cellulose fibers which have been through chemical processing, the process is often referred to as regeneration.\n\nSometimes the formation of a precipitate indicates the occurrence of a chemical reaction. If silver nitrate solution is poured into a solution of sodium chloride, a chemical reaction occurs forming a white precipitate of silver chloride. When potassium iodide solution reacts with lead(II) nitrate solution, a yellow precipitate of lead(II) iodide is formed.\n\nPrecipitation may occur if the concentration of a compound exceeds its solubility (such as when mixing solvents or changing their temperature). Precipitation may occur rapidly from a supersaturated solution.\n\nIn solids, precipitation occurs if the concentration of one solid is above the solubility limit in the host solid, due to e.g. rapid quenching or\nion implantation, and the temperature is high enough that diffusion can lead to segregation into precipitates. Precipitation in solids is routinely used to synthesize nanoclusters.\n\nAn important stage of the precipitation process is the onset of nucleation. The creation of a hypothetical solid particle includes the formation of an interface, which requires some energy based on the relative surface energy of the solid and the solution. If this energy is not available, and no suitable nucleation surface is available, supersaturation occurs.\n\nPrecipitation reactions can be used for making pigments, removing salts from water in water treatment, and in classical qualitative inorganic analysis.\n\nPrecipitation is also useful to isolate the products of a reaction during workup. Ideally, the product of the reaction is insoluble in the reaction solvent. Thus, it precipitates as it is formed, preferably forming pure crystals. An example of this would be the synthesis of porphyrins in refluxing propionic acid. By cooling the reaction mixture to room temperature, crystals of the porphyrin precipitate, and are collected by filtration:\nPrecipitation may also occur when an \"antisolvent\" (a solvent in which the product is insoluble) is added, drastically reducing the solubility of the desired product. Thereafter, the precipitate may easily be separated by filtration, decanting, or centrifugation. An example would be the synthesis of chromic tetraphenylporphyrin chloride: water is added to the DMF reaction solution, and the product precipitates. Precipitation is also useful in purifying products: crude bmim-Cl is taken up in acetonitrile, and dropped into ethyl acetate, where it precipitates. Another important application of an antisolvent is in ethanol precipitation of DNA.\n\nIn metallurgy, precipitation from a solid solution is also a useful way to strengthen alloys; this process is known as solid solution strengthening.\n\nAn example of a precipitation reaction: Aqueous silver nitrate (AgNO) is added to a solution containing potassium chloride (KCl), the precipitation of a \nwhite solid, silver chloride (AgCl), is observed. (Zumdahl, 2005)\nThe silver chloride (AgCl) has formed a solid, which is observed as a precipitate.\n\nThis reaction can be written emphasizing the dissociated ions in a combined solution. This is known as the ionic equation.\n\nA final way to represent a precipitate reaction is known as a \"net ionic reaction\".\n\nMany compounds containing metal ions produce precipitates with distinctive colors. The following are typical colors for various metals. However, many of these compounds can produce colors very different from those listed.\n\nOther compounds generally form white precipitates.\n\nPrecipitate formation is useful in the detection of the type of cation in a salt. To do this, an alkali first reacts with the unknown salt to produce a precipitate that is the hydroxide of the unknown salt. To identify the cation, the color of the precipitate and its solubility in excess are noted. Similar processes are often used in sequence – for example, a barium nitrate solution will react with sulfate ions to form a solid barium sulfate precipitate, indicating that it is likely that sulfate ions are present.\n\nDigestion, or \"precipitate ageing\", happens when a freshly formed precipitate is left, usually at a higher temperature, in the solution from which it precipitates. It results in cleaner and bigger particles. The physico-chemical process underlying digestion is called Ostwald ripening.\n\n\n"}
{"id": "211485", "url": "https://en.wikipedia.org/wiki?curid=211485", "title": "Radioisotope thermoelectric generator", "text": "Radioisotope thermoelectric generator\n\nA radioisotope thermoelectric generator (RTG, RITEG) is an electrical generator that uses an array of thermocouples to convert the heat released by the decay of a suitable radioactive material into electricity by the Seebeck effect. This generator has no moving parts.\n\nRTGs have been used as power sources in satellites, space probes, and unmanned remote facilities such as a series of lighthouses built by the former Soviet Union inside the Arctic Circle. RTGs are usually the most desirable power source for unmaintained situations that need a few hundred watts (or less) of power for durations too long for fuel cells, batteries, or generators to provide economically, and in places where solar cells are not practical. Safe use of RTGs requires containment of the radioisotopes long after the productive life of the unit. Notably, RTGs tend to be prohibitively expensive for most things they might otherwise find applications for.\n\nThe RTG was invented in 1954 by Mound Laboratories scientists Ken Jordan and John Birden. They were inducted into the National Inventors Hall of Fame in 2013. Jordan & Birden worked on an Army Signal Corps contract (R-65-8- 998 11-SC-03-91) beginning on January 1, 1957, to conduct research on radioactive materials and thermocouples suitable for the direct conversion of heat to electrical energy using Polonium-210 as the heat source. RTGs were developed in the US during the late 1950s by Mound Laboratories in Miamisburg, Ohio, under contract with the United States Atomic Energy Commission. The project was led by Dr. Bertram C. Blanke.\n\nThe first RTG launched into space by the United States was SNAP 3B in 1961 powered by 96 grams of plutonium-238 metal, aboard the Navy Transit 4A spacecraft. One of the first terrestrial uses of RTGs was in 1966 by the US Navy at uninhabited Fairway Rock in Alaska. RTGs were used at that site until 1995.\n\nA common RTG application is spacecraft power supply. Systems for Nuclear Auxiliary Power (SNAP) units were used for probes that traveled far from the Sun rendering solar panels impractical. As such, they were used with Pioneer 10, Pioneer 11, \"Voyager 1\", \"Voyager 2\", \"Galileo\", \"Ulysses\", \"Cassini\", \"New Horizons\" and the Mars Science Laboratory. RTGs were used to power the two Viking landers and for the scientific experiments left on the Moon by the crews of Apollo 12 through 17 (SNAP 27s). Because the Apollo 13 moon landing was aborted, its RTG rests in the South Pacific Ocean, in the vicinity of the Tonga Trench. RTGs were also used for the Nimbus, Transit and LES satellites. By comparison, only a few space vehicles have been launched using full-fledged nuclear reactors: the Soviet RORSAT series and the American SNAP-10A.\n\nIn addition to spacecraft, the Soviet Union constructed many unmanned lighthouses and navigation beacons powered by RTGs. Powered by strontium-90 (Sr) (a material with potential use in a \"dirty bomb\") they are very reliable and provide a steady source of power. Most have no protection, not even fences or warning signs, and the locations of some of these facilities are no longer known due to poor record keeping. In one instance, the radioactive compartments were opened by a thief. In another case, three woodsmen in Tsalendzhikha Region, Georgia found two ceramic RTG heat sources that had been stripped of their shielding; two of them were later hospitalized with severe radiation burns after carrying the sources on their backs. The units were eventually recovered and isolated. There are approximately 1,000 such RTGs in Russia, all of which have long since exceeded their design operational lives of 10 years. Most of these RTGs likely no longer function, and may need to be dismantled. Some of their metal casings have been stripped by metal hunters, despite the risk of radioactive contamination.\n\nThe United States Air Force uses RTGs to power remote sensing stations for Top-ROCC and SEEK IGLOO radar systems predominantly located in Alaska.\n\nIn the past, small \"plutonium cells\" (very small Pu-powered RTGs) were used in implanted heart pacemakers to ensure a very long \"battery life\". , about 90 were still in use. The Mound Laboratory Cardiac Pacemaker program began on June 1, 1966, in conjunction with NUMEC. When it was recognized that the heat source would not remain intact during cremation, the program was cancelled in 1972 because there was no way to completely ensure that the units would not be cremated with their users' bodies.\n\nThe design of an RTG is simple by the standards of nuclear technology: the main component is a sturdy container of a radioactive material (the fuel). Thermocouples are placed in the walls of the container, with the outer end of each thermocouple connected to a heat sink. Radioactive decay of the fuel produces heat. It is the temperature difference between the fuel and the heat sink that allows the thermocouples to generate electricity.\n\nA thermocouple is a thermoelectric device that can convert thermal energy directly into electrical energy, using the Seebeck effect. It is made of two kinds of metal (or semiconductors) that can both conduct electricity. They are connected to each other in a closed loop. If the two junctions are at different temperatures, an electric current will flow in the loop.\n\nThe radioactive material used in RTGs must have several characteristics:\n\n\nThe first two criteria limit the number of possible fuels to fewer than 30 atomic isotopes within the entire table of nuclides.\n\nPlutonium-238, curium-244 and strontium-90 are the most often cited candidate isotopes, but other isotopes such as polonium-210, promethium-147, caesium-137, cerium-144, ruthenium-106, cobalt-60, curium-242, americium-241 and thulium isotopes have also been studied.\n\nPlutonium-238 has a half-life of 87.7 years, reasonable power density of 0.54 watts per gram, \nand exceptionally low gamma and neutron radiation levels. Pu has the lowest shielding requirements; Only three candidate isotopes meet the last criterion (not all are listed above) and need less than 25 mm of lead shielding to block the radiation. Pu (the best of these three) needs less than 2.5 mm, and in many cases, no shielding is needed in a Pu RTG, as the casing itself is adequate.\nPu has become the most widely used fuel for RTGs, in the form of plutonium(IV) oxide (PuO). \nHowever, plutonium(IV) oxide containing a natural abundance of oxygen emits ~23x10 n/sec/g of plutonium-238. This emission rate is relatively high compared to the neutron emission rate of plutonium-238 metal. The metal containing no light element impurities emits ~2.8x10 n/sec/g of plutonium-238. These neutrons are produced by the spontaneous fission of plutonium-238. \n\nThe difference in the emission rates of the metal and the oxide is due mainly to the alpha, neutron reaction with the oxygen-18 and oxygen-17 present in the oxide. The normal amount of oxygen-18 present in the natural form is 0.204% while that of oxygen-17 is 0.037%. The reduction of the oxygen-17 and oxygen-18 present in plutonium dioxide will result in a much lower neutron emission rate for the oxide; this can be accomplished by a gas phase O exchange method. Regular production batches of PuO particles precipitated as a hydroxide were used to show that large production batches could be effectively O-exchanged on a routine basis. High-fired PuO microspheres were successfully O-exchanged showing that an exchange will take place regardless of the previous heat treatment history of the PuO. \nThis lowering of the neutron emission rate of PuO containing normal oxygen by a factor of 5 was discovered during the Cardiac Pacemaker research at Mound Laboratory in 1966, due in part to the Mound Laboratory's experience with production of stable isotopes beginning in 1960. For production of the large heat sources the shielding required would have been prohibitive without this process.\n\nUnlike the latter RTG fuels, Pu must be specifically synthesized and is not abundant as a nuclear waste product. At present only Russia has maintained consistent Pu production, while the United States restarted production at circa 1.5 kg a year in 2013 after a c. 25-year hiatus. At present these are the only countries with declared production of Pu in quantities useful for RTGs. Pu is produced at typically 85% purity and its purity decreases over time.\n\nStrontium-90 has been used by the Soviet Union in terrestrial RTGs. Sr decays by β emission, with minor γ emission. While its half life of 28.8 years is much shorter than that of Pu, it also has a lower decay energy with a power density of 0.46 watts per gram. Because the energy output is lower it reaches lower temperatures than Pu, which results in lower RTG efficiency. Sr is a high yield waste product of nuclear fission and is available in large quantities at a low price.\n\nSome prototype RTGs, first built in 1958 by the US Atomic Energy Commission, have used polonium-210. This isotope provides phenomenal power density (pure Po emits 140 W/g) because of its high decay rate, but has limited use because of its very short half-life of 138 days. A half-gram sample of Po reaches temperatures of over .\n\nAmericium-241 is a potential candidate isotope with a longer half-life than Pu: Am has a half-life of 432 years and could hypothetically power a device for centuries. However, the power density of Am is only 1/4 that of Pu, and Am produces more penetrating radiation through decay chain products than Pu and needs more shielding. Even so, its shielding requirements in an RTG are the second lowest of all possible isotopes: only Pu requires less. With a current global shortage of Pu, Am is being studied as RTG fuel by ESA. An advantage over Pu is that it is produced as nuclear waste and is nearly isotopically pure. Prototype designs of Am RTGs expect 2-2.2 W/kg for 5-50 W RTGs design, putting Am RTGs at parity with Pu RTGs within that power range.\n\nMost RTGs use Pu, which decays with a half-life of 87.7 years. RTGs using this material will therefore diminish in power output by a factor of 1−0.5, or 0.787%, per year.\n\nOne example is the RTG used by the Voyager probes. In the year 2000, 23 years after production, the radioactive material inside the RTG had decreased in power by 16.6%, i.e. providing 83.4% of its initial output; starting with a capacity of 470 W, after this length of time it would have a capacity of only 392 W. A related loss of power in the Voyager RTGs is the degrading properties of the bi-metallic thermocouples used to convert thermal energy into electrical energy; the RTGs were working at about 67% of their total original capacity instead of the expected 83.4%. By the beginning of 2001, the power generated by the Voyager RTGs had dropped to 315 W for \"Voyager 1\" and to 319 W for \"Voyager 2\".\n\nNASA is developing a Multi-Mission Radioisotope Thermoelectric Generator in which the thermocouples would be made of skutterudite, which can function with a smaller temperature difference than the current tellurium designs. This would mean that an otherwise similar RTG would generate 25% more power at the beginning of a mission and at least 50% more after seventeen years. NASA hopes to use the design on the next New Frontiers mission.\n\nRTGs use thermoelectric generators to convert heat from the radioactive material into electricity. Thermoelectric modules, though very reliable and long-lasting, are very inefficient; efficiencies above 10% have never been achieved and most RTGs have efficiencies between 3–7%. Thermoelectric materials in space missions to date have included silicon–germanium alloys, lead telluride and tellurides of antimony, germanium and silver (TAGS). Studies have been done on improving efficiency by using other technologies to generate electricity from heat. Achieving higher efficiency would mean less radioactive fuel is needed to produce the same amount of power, and therefore a lighter overall weight for the generator. This is a critically important factor in spaceflight launch cost considerations.\nA thermionic converter—an energy conversion device which relies on the principle of thermionic emission—can achieve efficiencies between 10–20%, but requires higher temperatures than those at which standard RTGs run. Some prototype Po RTGs have used thermionics, and potentially other extremely radioactive isotopes could also provide power by this means, but short half-lives make these unfeasible. Several space-bound nuclear reactors have used thermionics, but nuclear reactors are usually too heavy to use on most space probes.\n\nThermophotovoltaic cells work by the same principles as a photovoltaic cell, except that they convert infrared light emitted by a hot surface rather than visible light into electricity. Thermophotovoltaic cells have an efficiency slightly higher than thermoelectric modules (TEMs) and can be overlaid on top of themselves, potentially doubling efficiency. Systems with radioisotope generators simulated by electric heaters have demonstrated efficiencies of 20%, but have not yet been tested with radioisotopes. Some theoretical thermophotovoltaic cell designs have efficiencies up to 30%, but these have yet to be built or confirmed. Thermophotovoltaic cells and silicon TEMs degrade faster than metal TEMs, especially in the presence of ionizing radiation.\n\nDynamic generators can provide power at more than 4 times the conversion efficiency of RTGs. NASA and DOE have been developing a next-generation radioisotope-fueled power source called the Stirling Radioisotope Generator (SRG) that uses free-piston Stirling engines coupled to linear alternators to convert heat to electricity. SRG prototypes demonstrated an average efficiency of 23%. Greater efficiency can be achieved by increasing the temperature ratio between the hot and cold ends of the generator. The use of non-contacting moving parts, non-degrading flexural bearings, and a lubrication-free and hermetically sealed environment have, in test units, demonstrated no appreciable degradation over years of operation. Experimental results demonstrate that an SRG could continue running for decades without maintenance. Vibration can be eliminated as a concern by implementation of dynamic balancing or use of dual-opposed piston movement. Potential applications of a Stirling radioisotope power system include exploration and science missions to deep-space, Mars, and the Moon.\n\nThe increased efficiency of the SRG may be demonstrated by a theoretical comparison of thermodynamic properties, as follows. These calculations are simplified and do not account for the decay of thermal power input due to the long half-life of the radioisotopes used in these generators. The assumptions for this analysis include that both systems are operating at steady state under the conditions observed in experimental procedures (see table below for values used). Both generators can be simplified to heat engines to be able to compare their current efficiencies to their corresponding Carnot efficiencies. The system is assumed to be the components, apart from the heat source and heat sink.\n\nThe thermal efficiency, denoted η, is given by:\n\nWhere primes ( ' ) denote the time derivative.\n\nFrom a general form of the First Law of Thermodynamics, in rate form:\n\nAssuming the system is operating at steady state and formula_3,\n\nη, then, can be calculated to be 110 W / 2000 W = 5.5% (or 140 W / 500 W = 28% for the SRG). Additionally, the Second Law efficiency, denoted η, is given by:\n\nWhere η is the Carnot efficiency, given by:\n\nIn which T is the external temperature (which has been measured to be 510 K for the MMRTG (Multi-Mission RTG) and 363 K for the SRG) and T is the temperature of the MMRTG, assumed 823 K (1123 K for the SRG). This yields a Second Law efficiency of 14.46% for the MMRTG (or 41.37% for the SRG).\n\nRTGs pose a risk of radioactive contamination: if the container holding the fuel leaks, the radioactive material may contaminate the environment.\n\nFor spacecraft, the main concern is that if an accident were to occur during launch or a subsequent passage of a spacecraft close to Earth, harmful material could be released into the atmosphere; therefore their use in spacecraft and elsewhere has attracted controversy. \n\nHowever, this event is not considered likely with current RTG cask designs. For instance, the environmental impact study for the Cassini–Huygens probe launched in 1997 estimated the probability of contamination accidents at various stages in the mission. The probability of an accident occurring which caused radioactive release from one or more of its 3 RTGs (or from its 129 radioisotope heater units) during the first 3.5 minutes following launch was estimated at 1 in 1,400; the chances of a release later in the ascent into orbit were 1 in 476; after that the likelihood of an accidental release fell off sharply to less than 1 in a million. If an accident which had the potential to cause contamination occurred during the launch phases (such as the spacecraft failing to reach orbit), the probability of contamination actually being caused by the RTGs was estimated at about 1 in 10. The launch was successful and \"Cassini–Huygens\" reached Saturn.\n\nTo minimize the risk of the radioactive material being released, the fuel is stored in individual modular units with their own heat shielding. They are surrounded by a layer of iridium metal and encased in high-strength graphite blocks. These two materials are corrosion- and heat-resistant. Surrounding the graphite blocks is an aeroshell, designed to protect the entire assembly against the heat of reentering the Earth's atmosphere. The plutonium fuel is also stored in a ceramic form that is heat-resistant, minimising the risk of vaporization and aerosolization. The ceramic is also highly insoluble.\n\nBetween 1961—2011, 28 U.S. space missions safely flew radioisotope energy sources.\n\nThe plutonium-238 used in these RTGs has a half-life of 87.74 years, in contrast to the 24,110 year half-life of plutonium-239 used in nuclear weapons and reactors. A consequence of the shorter half-life is that plutonium-238 is about 275 times more radioactive than plutonium-239 (i.e. /g compared to /g). For instance, 3.6 kg of plutonium-238 undergoes the same number of radioactive decays per second as 1 tonne of plutonium-239. Since the morbidity of the two isotopes in terms of absorbed radioactivity is almost exactly the same, plutonium-238 is around 275 times more toxic by weight than plutonium-239.\n\nThe alpha radiation emitted by either isotope will not penetrate the skin, but it can irradiate internal organs if plutonium is inhaled or ingested. Particularly at risk is the skeleton, the surface of which is likely to absorb the isotope, and the liver, where the isotope will collect and become concentrated.\nThere have been several known accidents involving RTG-powered spacecraft:\nOne RTG, the SNAP-19C, was lost near the top of Nanda Devi mountain in India in 1965 when it was stored in a rock formation near the top of the mountain in the face of a snowstorm before it could be installed to power a CIA remote automated station collecting telemetry from the Chinese rocket testing facility. The seven capsules were carried down the mountain onto a glacier by an avalanche and never recovered. It is most likely that they melted through the glacier and were pulverized, whereupon the plutonium zirconium alloy fuel oxidized soil particles that are moving in a plume under the glacier.\n\nThe SNAP-27 heat source traveled to the moon in a graphite cask attached to the lander leg from which an astronaut removed it with a handling tool after a successful landing and placed it in the RTG.\n\nMany Beta-M RTGs produced by the Soviet Union to power lighthouses and beacons have become orphaned sources of radiation. Several of these units have been illegally dismantled for scrap metal (resulting in the complete exposure of the Sr-90 source), fallen into the ocean, or have defective shielding due to poor design or physical damage. The US Department of Defense cooperative threat reduction program has expressed concern that material from the Beta-M RTGs can be used by terrorists to construct a dirty bomb.\n\nRTGs and nuclear power reactors use very different nuclear reactions. Nuclear power reactors use controlled nuclear fission in a chain reaction. The rate of the reaction can be controlled with neutron absorbers, so power can be varied with demand or shut off entirely for maintenance. However, care is needed to avoid uncontrolled operation at dangerously high power levels.\n\nChain reactions do not occur in RTGs, so heat is produced at an unchangeable, though steadily decreasing rate that depends only on the amount of fuel isotope and its half-life. An accidental power excursion is impossible. However, if a launch or re-entry accident occurs and the fuel is dispersed, the combined power output of the radionuclides now set free does not drop. In an RTG, heat generation cannot be varied with demand or shut off when not needed. Therefore, auxiliary power supplies (such as rechargeable batteries) may be needed to meet peak demand, and adequate cooling must be provided at all times including the pre-launch and early flight phases of a space mission.\n\nDue to the shortage of plutonium-238, a new kind of RTG assisted by subcritical reactions has been proposed. In this kind of RTG, the alpha decay from the radioisotope is also used in alpha-neutron reactions with a suitable element such as beryllium. This way a long-lived neutron source is produced. Because the system is working with a criticality close to but less than 1, i.e. K < 1, a subcritical multiplication is achieved which increases the neutron background and produces energy from fission reactions. Although the number of fissions produced in the RTG is very small (making their gamma radiation negligible), because each fission reaction releases almost 30 times more energy than each alpha decay (200 MeV compared to 6 MeV), up to a 10% energy gain is attainable, which translates into a reduction of the Pu needed per mission. The idea was proposed to NASA in 2012 for the yearly NASA NSPIRE competition, which translated to Idaho National Laboratory at the Center for Space Nuclear Research (CSNR) in 2013 for studies of feasibility.. However the essentials are unmodified.\n\nRTG have been proposed for use on realistic interstellar precursor missions and interstellar probes. An example of this is the Innovative Interstellar Explorer (2003–current) proposal from NASA.\nAn RTG using Am was proposed for this type of mission in 2002. This could support mission extensions up to 1000 years on the interstellar probe, because the power output would decline more slowly over the long term than plutonium. Other isotopes for RTG were also examined in the study, looking at traits such as watt/gram, half-life, and decay products. An interstellar probe proposal from 1999 suggested using three advanced radioisotope power sources (ARPS).\n\nThe RTG electricity can be used for powering scientific instruments and communication to Earth on the probes. One mission proposed using the electricity to power ion engines, calling this method radioisotope electric propulsion (REP).\n\nA power enhancement for radioisotope heat sources based on a self-induced electrostatic field has been proposed. According to the authors, enhancements of up to 10% could be attainable using beta sources.\n\nA typical RTG is powered by radioactive decay and features electricity from thermoelectric conversion, but for the sake of knowledge, some systems with some variations on that concept are included here:\n\n<nowiki>*</nowiki> The ASRG is not really an RTG: it uses a Stirling power device that runs on radioisotope (see Stirling radioisotope generator).\n\n<nowiki>**</nowiki> The BES-5 Buk () reactor was a fast breeder reactor which used thermocouples based on semiconductors to convert heat directly into electricity.\n\n<nowiki>***</nowiki> The SNAP-10A used enriched uranium fuel, zirconium hydride as a moderator, liquid sodium potassium alloy coolant, and was activated or deactivated with beryllium reflectors. Reactor heat fed a thermoelectric conversion system for electrical production.\n\nKnown spacecraft/nuclear power systems and their fate. Systems face a variety of fates, for example, Apollo's SNAP-27 were left on the Moon. Some other spacecraft also have small radioisotope heaters, for example each of the Mars Exploration Rovers have a 1 watt radioisotope heater. Spacecraft use different amounts of material, for example MSL \"Curiosity\" has 4.8 kg of plutonium-238 dioxide, while the Cassini spacecraft had 32.7 kg.\n\n\n\n"}
{"id": "16864252", "url": "https://en.wikipedia.org/wiki?curid=16864252", "title": "Superinsulator", "text": "Superinsulator\n\nA superinsulator is a material that at low temperatures under certain conditions has an infinite resistance and no current will pass through it. The superinsulating state has many parallels to the superconducting state, and can be destroyed (in a sudden phase transition) by increased temperature, magnetic fields and voltage.\n\nThe superinsulating state was first observed in a titanium nitride film in April 2008 by Russian scientists Valerii Vinokur and Tatyana Baturina working at Argonne National Laboratory, United States. Currently it is not known if the superinsulation state they observed means the dielectric permittivity of the material approaches infinity, or whether the material just has zero conduction as would be found in a vacuum.\n\nOther researchers have seen the same phenomenon in disordered indium oxide films, but have proposed a different explanation for their observations.\n\nBoth superconductivity and superinsulation are caused by the pairing of conduction electrons at low temperatures into Cooper pairs. In superconductors, all the pairs move in unison, allowing current without resistance. In superinsulators the Cooper pairs avoid each other, preventing current from flowing.\n\nSuperinsulators could potentially be used to create batteries that do not lose charge when not in use. Combined with superconductors, superinsulators could be used to create electrical circuits with no energy lost as heat.\n\nIt has been suggested by several authors that the \"superinsulator\" may not be a fundamentally new state of solid, but is rather caused due to the non-equilibrium heating of the electrons with respect to the phonons at low temperatures. This notion gets support from the fact that the jump in the current-voltage characteristics, a hallmark of the superinsulating state, is also observed in other systems, such as YSi, where no known superconducting correlations exist.\n\n"}
{"id": "89761", "url": "https://en.wikipedia.org/wiki?curid=89761", "title": "Thai baht", "text": "Thai baht\n\nThe baht (; , ; sign: ฿; code: THB) is the official currency of Thailand. It is subdivided into 100 \"satang\" (, ). The issuance of currency is the responsibility of the Bank of Thailand.\n\nAccording to SWIFT, as of February 2017, the Thai baht is ranked as the 10th most frequently used world payment currency.\n\nAccording to a report in the \"South China Morning Post\", the China Banknote Printing and Minting Corporation produces at least some Thai banknotes and coins.\n\nThe Thai baht, like the pound, originated from a traditional unit of mass. Its currency value was originally expressed as that of silver of corresponding weight (now defined as 15 grams), and was in use probably as early as the Sukhothai period in the form of bullet coins known in Thai as \"phot duang\" (). These were pieces of solid silver cast to various weights corresponding to a traditional system of units related by simple fractions and multiples, one of which is the baht. These are listed in the following table:\n\nThat system was in use up until 1897, when the decimal system devised by Jayanta Mongkol, in which one baht = 100 satang, was introduced by King Chulalongkorn. However, coins denominated in the old units were issued until 1910, and the amount of 25 satang is still commonly referred to as a \"salueng\", as is the 25-satang coin.\n\nUntil 27 November 1902, the baht was fixed on a purely silver basis, with 15 grams of silver to the baht. This caused the value of the currency to vary relative to currencies on a gold standard. In 1857, the values of certain foreign silver coins were fixed by law, with the one baht = 0.6 Straits dollar and five baht = seven Indian rupees. Before 1880 the exchange rate was fixed at eight baht per pound sterling, falling to 10 to the pound during the 1880s.\n\nIn 1902, the government began to increase the value of the baht by following all increases in the value of silver against gold but not reducing it when the silver price fell. Beginning at 21.75 baht = one pound sterling, the currency rose in value until, in 1908, a fixed peg to the British pound sterling was established of 13 baht = one pound. This was revised to 12 baht in 1919 and then, after a period of instability, to 11 baht in 1923. During World War II, the baht was fixed at a value of one Japanese yen.\n\nFrom 1956 until 1973, the baht was pegged to the U.S. dollar at an exchange rate of 20.8 baht = one dollar and at 20 baht = 1 dollar until 1978. A strengthening US economy caused Thailand to re-peg its currency at 25 to the dollar from 1984 until 2 July 1997, when the country was affected by the 1997 Asian financial crisis. The baht was floated and halved in value, reaching its lowest rate of 56 to the dollar in January 1998. It has since risen to about 30 per dollar.\n\nThe baht was originally known to foreigners by the term \"tical\", which was used in English language text on banknotes until 1925.\n\nRama III (1824-1851) was the first king to consider the use of a flat coin. He did so not for the convenience of traders, but because he was disturbed that the creatures living in the cowrie shells were killed. When he learned of the use of flat copper coins in Singapore in 1835, he contacted a Scottish trader, who had two types of experimental coins struck in England. The king rejected both designs. The name of the country put on these first coins was \"Muang Thai\", not \"Siam\".\n\nCowrie shells from the Mekong River had been used as currency for small amounts since the Sukhothai period. Before 1860, Thailand did not produce coins using modern methods. Instead, a so-called \"bullet\" coinage was used, consisting of bars of metal, thicker in the middle, bent round to form a complete circle on which identifying marks were stamped. Denominations issued included , , , , , , 1, , 2, , 4, , 8, 10, 20, 40, and 80 baht in silver and , , , , 1, , 2, and 4 baht in gold. One gold baht was generally worth 16 silver baht. Between 1858 and 1860, foreign trade coins were also stamped by the government for use in Thailand.\n\nIn 1860, modern style coins were introduced. These were silver 1 sik, 1 fuang, 1 and 2 salung, 1, 2, and 4 baht, with the baht weighing 15.244 grams and the others weight related. Tin 1 solot and 1 att followed in 1862, with gold , 4, and 8 baht introduced in 1863 and copper 2 and 4 att in 1865. Copper replaced tin in the 1 solot and 1 att in 1874, with copper 4 att introduced in 1876. The last gold coins were struck in 1895.\n\nIn 1897, the first coins denominated in satang were introduced, cupronickel , 5, 10, and 20 satang. However, 1 solot, 1 and 2 att coins were struck until 1905 and 1 fuang coins were struck until 1910. In 1908, holed 1, 5, and 10 satang coins were introduced, with the 1 satang in bronze and the 5 and 10 satang in nickel. The 1 and 2 salung were replaced by 25 and 50 satang coins in 1915. In 1937, holed, bronze satang were issued.\n\nIn 1941, a series of silver coins was introduced in denominations of 5, 10, and 20 satang, due to a shortage of nickel caused by World War II. The next year, tin coins were introduced for 1, 5, and 10 satang, followed by 20 satang in 1945 and 25 and 50 satang in 1946. In 1950, aluminium-bronze 5, 10, 25, and 50 satang were introduced whilst, in 1957, bronze 5 and 10 satang were issued, along with 1 baht coins struck in an unusual alloy of copper, nickel, silver, and zinc. Several Thai coins were issued for many years without changing the date. These include the tin 1942 1 satang and the 1950 5 and 10 satang, struck until 1973, the tin 1946 25 satang struck until 1964, the tin 50 satang struck until 1957, and the aluminium bronze 1957 5, 10, 25, and 50 satang struck until the 1970s. Cupronickel 1 baht coins were introduced in 1962 and struck without date change until 1982.\n\nIn 1972, cupronickel 5 baht coins were introduced, switching to cupronickel-clad copper in 1977. Between 1986 and 1988, a new coinage was introduced, consisting of aluminium 1, 5, and 10 satang, aluminium-bronze 25 and 50 satang, cupronickel 1 baht, cupronickel-clad-copper 5 baht and bimetallic 10 baht. Cupronickel-clad-steel 2 baht were introduced in 2005.\n\nIn 2008, the Ministry of Finance and the Royal Thai Mint announced the 2009 coin series, which included changes in materials to reduce production costs as well as an update of the image on the obverse to a more recent portrait of the king. The two-baht coin, confusingly similar in color and size to the one-baht coin, was changed from nickel-clad low-carbon steel to aluminium bronze. New two-baht coin was the first of the new series released on February 3, 2009, followed by a satang coin in April, a five-baht coin in May, a ten-baht coin in June, and a one-baht coin in July 2009.\n\nIn 2018, the Royal Thai Mint and the Ministry of Finance issued a new series of general circulation coins, featuring the same standard specifications, but feature a portrait of its current king, Maha Vajiralongkorn. \n\n\nIn February 2010 the Treasury Department of Thailand stated that it has been planning a new circulation 20 baht coin.\n\nIn 1851, the government issued notes for , , , , and 1 tical, followed by 3, 4, 6, and 10 tamlueng in 1853. After 1857, notes for 20 and 40 ticals were issued, also bearing their values in Straits dollars and Indian rupees. Undated notes were also issued before 1868 for 5, 7, 8, 12, and 15 tamlueng, and 1 chang. One att notes were issued in 1874.\n\nIn 1892, the treasury issued notes for 1, 5, 10, 40, 80, 100, 400, and 800 ticals, called \"baht\" in the Thai text.\n\nThe year 1902 marked the introduction of reforms by prince Jayanta Mongkol after his observations of banking practices in Europe, which became an important landmark in the inauguration of paper money in Thailand. On September 19, 1902, the government introduced notes which were printed by Thomas De La Rue & Company Limited, England, during the reigns of Kings Rama V and Rama VI, denominated 5, 10, 20, 100, and 1000 ticals, still called baht in the Thai text — each denomination having many types, with 1 and 50 tical notes following in 1918. In 1925, notes were issued in denominations of 1, 5, 10, 20, 100, and 1,000 baht with the denomination in both Arabic and Thai numerals without English text; English speakers continued to refer to these as \"ticals\".\n\nIn 1942, the Bank of Thailand was founded and took over responsibility for the issuance of paper money. 50 baht notes were briefly reintroduced in 1945, with 50 satang notes issued in 1946. The one baht note was replaced by a coin in 1957 and the five baht was replaced in 1972. 50 baht notes were again reintroduced in 1985, with the 10 baht note replaced by a coin in 1988. The EURion constellation has been used on the reverse of 100 and 1000 baht notes since 2003. Older notes are occasionally still found in circulation, for example, 10 baht notes, and these can usually be spent without problem. In any case, they can be exchanged for free in banks.\n\nOn 27 July 2010, the Bank of Thailand announced that the 16th series banknotes would enter circulation in December 2010. On 9 August 2012, the Bank of Thailand issued a new denomination banknote, 80 baht, to commemorate queen Sirikit's 80th birthday. It was the first Thai banknote that featured Crane's Motion security thread.\n\nIn 2017, the Bank of Thailand announced a new family of banknotes in remembrance of its late king Bhumibol Adulyadej (Rama IX). The notes are the same size and dimensions as the \"Series 16\" banknotes, with the front designs as before, but the back designs featuring images of the king's life in infancy, adolescence and maturity. The new family of banknotes were issued on September 20.\n\nIn 2018, the Bank of Thailand announced a new family of banknotes featuring a portrait of its current king, Maha Vajiralongkorn.\nThe main colors and dimensions of the notes are the same as before, with the back designs featuring images of the Kings of Thailand from past to present. The 20, 50 and 100 baht banknotes were issued on Chakri Memorial Day, April 6, 2018. The final two denominations, 500 and 1,000 baht were issued on the anniversary of the birth of King Maha Vajiralongkorn, July 28, 2018.\n\nImages of banknotes have been removed lest they infringe copyright, but may be viewed at the Thai-language article linked in the margin.\n\nIn addition to the banknotes currently in circulation, numerous commemorative notes have been issued:\n\n\"Ngoen\" (เงิน) is Thai for 'silver' as well as the general term for money, reflecting the fact that the baht (or tical) is foremost a unit of weight for precious metals and gemstones. One baht = 15.244 grams. Since the standard purity of Thai gold is 96.5 percent, the actual gold content of one baht by weight is 15.244 × 0.965 = 14.71046 grams; equivalent to about 0.473 troy ounces. 15.244 grams is used for bullion; in the case of jewellery, one baht should be more than 15.16 grams.\n\nThe Bank of Thailand adopted a series of exchange controls on 19 December 2006, which resulted in a significant divergence between offshore and onshore exchange rates, with spreads of up to 10 percent between the two markets. Controls were broadly lifted on 3 March 2008 and there is now no significant difference between offshore and onshore exchange rates.\n\n\"(Source: usd.fx-exchange.com)\"\n\n\n"}
{"id": "12922162", "url": "https://en.wikipedia.org/wiki?curid=12922162", "title": "Tianwan Nuclear Power Plant", "text": "Tianwan Nuclear Power Plant\n\nTianwan Nuclear Power Station is a nuclear power plant in Lianyungang prefecture level city, Jiangsu province, China. \nIt is located on the coast of the Yellow Sea approximately 30 kilometers east of Lianyungang proper.\nThe nuclear power plant consists of two reactor units each rated at 1,000 MW capacity and constructed by Russia's Atomstroyexport. \nThe first reactor began full operations in 2006 and the second in 2007.\n\nConstruction commenced on 20 October 1999 for the first unit, and on 20 October 2000 for the second reactor unit. The first reactor went critical on 20 December 2005. Construction of the second reactor finished in May 2007 and commercial operation began in August. This is the first time the two countries have co-operated on a nuclear power project.\n\nOn 23 November 2010, Jiangsu Nuclear Power Corporation signed a contract with Atomstroyexport according to which Atomstroyexport will supply 1060 MWe VVER-1000 reactors for units 3 and 4.\nConstruction of unit 3 was delayed by the 2011 nuclear accident in Japan, but finally began in December 2012.\n\nBoth units use VVER pressurized water reactor (PWR) technology supplied from Russia. Together they cost approximately US$3.3 billion. The units are the Russian standard reactor type VVER-1000/392 (also carries the designation of VVER-1000/428) adapted specifically for China.\n\nThese VVER 1000 reactors are housed in a confinement shell capable of being hit by an aircraft weighing 20 tonnes and suffering no expected damage. Reactors also received additional protection from earthquakes. Other important safety features include an emergency core cooling system and core confinement system. Russia delivered initial fuel loads for the Tianwan reactors. China planned to begin indigenous fuel fabrication for the Tianwan plant in 2010, using technology transferred from Russian nuclear fuel producer TVEL.\n\n\"The station has four levels of security. There's a double asbestos cluster, which blocks any kind of emissions. Also there's a revolutionary security improvement called the trap, which prevents any leakage of nuclear fuel in the event of a breakdown\", Alexandr Selikhov, Head of Atomstroyexport's delegation to China\n\nThe Tianwan Nuclear Power Plant uses third party parts. While the reactor and turbo-generators are of Russian design, the control room was designed and built by an international consortium (including Siemens). In this way the plant was brought to meet the toughest recognised safety standards; safety systems were already mostly in place but the previous monitoring of these systems did not meet international safety standards. The new VVER 1000 plant built in China has 94% of its systems automated, meaning the plant can control itself under most situations. Refueling procedures require little human intervention. Five operators are still needed in the control room.\n\nBuilt reactors are Third Generation, except Unit 5 and 6.\n\nThe Tianwan nuclear power plant has four operating units, two more under construction, and two planned future reactors:\n"}
{"id": "3957700", "url": "https://en.wikipedia.org/wiki?curid=3957700", "title": "Tritiated water", "text": "Tritiated water\n\nTritiated water is a radioactive form of water where the usual protium atoms are replaced with tritium. In its pure form it may be called tritium oxide (TO or HO) or super-heavy water. Pure TO is corrosive due to self-radiolysis. Diluted, tritiated water is mainly HO plus some HTO (HOH). It is also used as a tracer for water transport studies in life-science research. Furthermore, since it naturally occurs in minute quantities, it can be used to determine the age of various water-based liquids, such as vintage wines.\n\nThe name \"super-heavy water\" helps distinguish the tritiated material from heavy water, which instead contains deuterium.\n\nTritiated water can be used to measure the total volume of water in one's body. Tritiated water distributes itself into all body compartments relatively quickly. The concentration of tritiated water in urine is assumed to be similar to the concentration of tritiated water in the body. Knowing the original amount of tritiated water that was ingested and the concentration, one can calculate the volume of water in the body.\n\n\nTritiated water contains the radioactive hydrogen isotope tritium. As a low energy beta emitter with a half-life of about 12 years, it is not dangerous externally because its beta particles are unable to penetrate the skin. However, it is a radiation hazard when inhaled, ingested via food or water, or absorbed through the skin. HTO has a short biological half-life in the human body of 7 to 14 days, which both reduces the total effects of single-incident ingestion and precludes long-term bioaccumulation of HTO from the environment. Biological half-life of tritiated water in the human body, which is a measure of body water turnover, varies with season. Studies on the biological half-life of occupational radiation workers for free water tritium in the coastal region of Karnataka, India show that the biological half-life in the winter season is twice that of the summer season.\n"}
{"id": "2524890", "url": "https://en.wikipedia.org/wiki?curid=2524890", "title": "Zirconium nitride", "text": "Zirconium nitride\n\nZirconium nitride () is an inorganic compound used in a variety of ways due to its properties.\n\nZrN grown by physical vapor deposition (PVD) is a light gold color similar to elemental gold. ZrN has a room-temperature electrical resistivity of 12.0 µΩ·cm, a temperature coefficient of resistivity of 5.6·10 Ω·cm/K, a superconducting transition temperature of 10.4 K, and a relaxed lattice parameter of 0.4575 nm. The hardness of single-crystal ZrN is 22.7±1.7 GPa and elastic modulus is 450 GPa.\n\nZirconium nitride is a hard ceramic material similar to titanium nitride and is a cement-like refractory material. Thus it is used in refractories, cermets and laboratory crucibles. When applied using the physical vapor deposition coating process it is commonly used for coating medical devices, industrial parts (notably drill bits), automotive and aerospace components and other parts subject to high wear and corrosive environments.\n\nAlso zirconium nitride was suggested as a hydrogen peroxide fuel tank liner for rockets and aircraft.\n"}
