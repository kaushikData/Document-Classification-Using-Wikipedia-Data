{"id": "57724143", "url": "https://en.wikipedia.org/wiki?curid=57724143", "title": "AMADEE-18", "text": "AMADEE-18\n\nAMADEE-18 is a Mars simulation project. It was launched in February 2018 in a desert in Oman. It is mission of the Austrian Space Forum (OeWF). \n\n"}
{"id": "1387", "url": "https://en.wikipedia.org/wiki?curid=1387", "title": "Alabaster", "text": "Alabaster\n\nAlabaster is a mineral or rock that is soft, often used for carving, and is processed for plaster powder. Archaeologists and the stone processing industry use the word differently from geologists. The former use is in a wider sense that includes varieties of two different minerals: the fine-grained massive type of gypsum and the fine-grained banded type of calcite. Geologists define alabaster only as the gypsum type. Chemically, gypsum is a hydrous sulfate of calcium, while calcite is a carbonate of calcium. \n\nBoth types of alabaster have similar properties. They are usually lightly colored, translucent, and soft stones. They have been used throughout history primarily for carving decorative artifacts.\n\nThe calcite type is also denominated \"onyx-marble\", \"Egyptian alabaster\", and \"Oriental alabaster\" and is geologically described as either a compact banded travertine or \"a stalagmitic limestone marked with patterns of swirling bands of cream and brown\". \"Onyx-marble\" is a traditional, but geologically inaccurate, name because both onyx and marble have geological definitions that are distinct from even the broadest definition of \"alabaster\".\n\nIn general, ancient alabaster is calcite in the wider Middle East, including Egypt and Mesopotamia, while it is gypsum in medieval Europe. Modern alabaster is probably calcite but may be either. Both are easy to work and slightly soluble in water. They have been used for making a variety of indoor artwork and carving, and they will not survive long outdoors.\n\nThe two kinds are readily distinguished by their different hardnesses: gypsum alabaster is so soft that a fingernail scratches it (Mohs hardness 1.5 to 2), while calcite cannot be scratched in this way (Mohs hardness 3), although it yields to a knife. Moreover, calcite alabaster, being a carbonate, effervesces when treated with hydrochloric acid, while gypsum alabaster remains almost unaffected when thus treated.\n\nThe origin of \"alabaster\" is in Middle English through Old French \"\"alabastre\", in turn derived from Latin \"alabaster\", and that from Greek \"ἀλάβαστρος\" (\"alabastros\") or \"ἀλάβαστος\" (\"alabastos\"). The Greek words denoted a vase of alabaster.\n\nThe name may be derived further from Ancient Egyptian \"a-labaste\"\", which refers to vessels of the Egyptian goddess Bast. She was represented as a lioness and frequently depicted as such in figures placed atop these alabaster vessels. Other suggestions include derivation from the name of the ancient town of Alabastron, Egypt, described in sometimes contradictory manner by ancient Roman authors, e. g. Pliny the Elder in \"On Stones\" and Ptolemy in \"Geography\"), the location of which is unknown.\n\nThe purest alabaster is a snow-white material of fine uniform grain, but it often is associated with an oxide of iron, which produces brown clouding and veining in the stone. The coarser varieties of gypsum alabaster are converted by calcination into plaster of Paris, and are sometimes known as \"plaster stone.\"\n\nThe softness of alabaster enables it to be carved readily into elaborate forms, but its solubility in water renders it unsuitable for outdoor work. If alabaster with a smooth, polished surface is washed with dishwashing liquid, it will become rough, dull and whiter, losing most of its translucency and lustre. The finer kinds of alabaster are employed largely as an ornamental stone, especially for ecclesiastical decoration and for the rails of staircases and halls.\n\nAlabaster is mined and then sold in blocks to alabaster workshops. There they are cut to the needed size (\"squaring\"), and then are processed in different techniques: turned on a lathe for round shapes, carved into three-dimensional sculptures, chiselled to produce low relief figures or decoration; and then given an elaborate finish that reveals its transparency, colour, and texture.\n\nIn order to diminish the translucency of the alabaster and to produce an opacity suggestive of true marble, the statues are immersed in a bath of water and heated gradually—nearly to the boiling point—an operation requiring great care, because if the temperature is not regulated carefully, the stone acquires a dead-white, chalky appearance. The effect of heating appears to be a partial dehydration of the gypsum. If properly treated, it very closely resembles true marble and is known as \"marmo di Castellina\".\n\nAlabaster is a porous stone and can be \"dyed\" into any colour or shade, a technique used for centuries. For this the stone needs to be fully immersed in various pigmentary solutions and heated to a specific temperature. The technique can be used to disguise alabaster. In this way a very misleading imitation of coral that is called \"alabaster coral\" is produced.\n\nTypically only one type is sculpted in any particular cultural environment, but sometimes both have been worked to make similar pieces in the same place and time. This was the case with small flasks of the alabastron type made in Cyprus from the Bronze Age into the Classical period.\n\nWhen cut in thin sheets, alabaster is translucent enough to be used for small windows. It was used for this purpose in Byzantine churches and later in medieval ones, especially in Italy. Large sheets of Aragonese gypsum alabaster are used extensively in the contemporary Cathedral of Our Lady of the Angels, which was dedicated in 2002 by the Los Angeles, California Archdiocese. The cathedral incorporates special cooling to prevent the panes from overheating and turning opaque. The ancients used the calcite type, while the modern Los Angeles cathedral is using gypsum alabaster.\n\nCalcite alabaster, harder than the gypsum variety, was the kind primarily used in ancient Egypt and the wider Middle East (but not Assyrian palace reliefs), and is also used in modern times. It is found as either a stalagmitic deposit from the floor and walls of limestone caverns, or as a kind of travertine, similarly deposited in springs of calcareous water. Its deposition in successive layers gives rise to the banded appearance that the marble often shows on cross-section, from which its name is derived: onyx-marble or alabaster-onyx, or sometimes simply (and wrongly) as onyx.\n\nEgyptian alabaster has been worked extensively near Suez and Assiut.\n\nThis stone variety is the \"alabaster\" of the ancient Egyptians and Bible and is often termed \"Oriental alabaster\", since the early examples came from the Far East. The Greek name \"alabastrites\" is said to be derived from the town of Alabastron in Egypt, where the stone was quarried. The locality probably owed its name to the mineral; the origin of the mineral name is obscure (though see above).\n\nThe \"Oriental\" alabaster was highly esteemed for making small perfume bottles or ointment vases called alabastra; the vessel name has been suggested as a possible source of the mineral name. In Egypt, craftsmen used alabaster for canopic jars and various other sacred and sepulchral objects. A sarcophagus discovered in the tomb of Seti I near Thebes is on display in Sir John Soane's Museum, London; it is carved in a single block of translucent calcite alabaster from Alabastron.\n\nAlgerian onyx-marble has been quarried largely in the province of Oran.\n\nIn Mexico, there are famous deposits of a delicate green variety at La Pedrara, in the district of Tecali, near Puebla. Onyx-marble occurs also in the district of Tehuacán and at several localities in the US including California, Arizona, Utah, Colorado and Virginia.\n\nGypsum alabaster is the softer of the two varieties, the other being calcite alabaster. It was used primarily in medieval Europe, and is also used in modern times.\n\n\"Mosul marble\" is a kind of gypsum alabaster found in the north of modern Iraq, which was used for the Assyrian palace reliefs of the 9th to 7th centuries BC; these are the largest type of alabaster sculptures to have been regularly made. The relief is very low and the carving detailed, but large rooms were lined with continuous compositions on slabs around high. The \"Lion Hunt of Ashurbanipal\" and military Lachish reliefs, both 7th century and in the British Museum, are some of the best known.\n\nGypsum alabaster was very widely used for small sculpture for indoor use in the ancient world, especially in ancient Egypt and Mesopotamia. Fine detail could be obtained in a material with an attractive finish without iron or steel tools. Alabaster was used for vessels dedicated for use in the cult of the deity Bast in the culture of the ancient Egyptians, and thousands of gypsum alabaster artifacts dating to the late 4th millennium BC also have been found in Tell Brak (present day Nagar), in Syria.\n\nIn Mesopotamia, gypsum alabaster was the typical material for figures of deities and devotees from temples, as in a figure believed to represent the deity, Abu, dating to the first half of the 3rd millennium BC in New York.\n\nMuch of the world's alabaster extraction is performed in the centre of the Ebro Valley in Aragon, Spain, which has the world's largest known exploitable deposits.\nAccording to a brochure published by the Aragon government, alabaster has elsewhere either been depleted, or its extraction is so difficult that it has almost been abandoned or is carried out at a very high cost.\nThere are two separate sites in Aragon, both are located in Tertiary basins.\nThe most important site is the Fuentes-Azaila area, in the Tertiary Ebro Basin.\nThe other is the Calatayud-Teruel Basin, which divides the Iberian Range in two main sectors (NW and SE).\n\nThe abundance of Aragonese alabaster was crucial for its use in architecture, sculpture and decoration.\nThere is no record of likely use by pre-Roman cultures, so perhaps the first ones to use alabaster in Aragon were the Romans, who produced vessels from alabaster following the Greek and Egyptian models.\nIt seems that since the reconstruction of the Roman Wall in Zaragoza in the 3rd century AD with alabaster, the use of this material became common in building for centuries.\nMuslim Saraqusta (today, Zaragoza) was also called \"Medina Albaida\", the White City, due to the appearance of its alabaster walls and palaces, which stood out among gardens, groves and orchards by the Ebro and Huerva Rivers.\n\nThe oldest remains in the Aljafería Palace, together with other interesting elements like capitals, reliefs and inscriptions, were made using alabaster, but it was during the artistic and economic blossoming of the Renaissance that Aragonese alabaster reached its golden age.\nIn the 16th century sculptors in Aragon chose alabaster for their best works. They were adept at exploiting its lighting qualities and generally speaking the finished art pieces retained their natural color.\n\nIn Europe, the centre of the alabaster trade today is Florence, Italy. Tuscan alabaster occurs in nodular masses embedded in limestone, interstratified with marls of Miocene and Pliocene age. The mineral is worked largely by means of underground galleries, in the district of Volterra. Several varieties are recognized—veined, spotted, clouded, agatiform, and others. The finest kind, obtained principally from Castellina, is sent to Florence for figure-sculpture, while the common kinds are carved locally, into vases, lights, and various ornamental objects. These items are objects of extensive trade, especially in Florence, Pisa, and Livorno.\n\nIn the 3rd century BC the Etruscans used the alabaster of Tuscany from the area of modern-day Volterra to produce funeral urns, possibly taught by Greek artists. During the Middle Ages the craft of alabaster was almost completely forgotten. A revival started in the mid-16th century, and until the beginning of the 17th century alabaster work was strictly artistic and did not expand to form a large industry.\n\nIn the 17th and 18th centuries production of artistic, high-quality Renaissance-style artifacts stopped altogether, being replaced by less sophisticated, cheaper items better suited for large-scale production and commerce. The new industry prospered, but the reduced need of skilled craftsmen left only few still working. The 19th century brought a boom to the industry, largely due to the \"traveling artisans\" who went and offered their wares to the palaces of Europe, as well as to America and the East.\n\nIn the 19th century new processing technology was also introduced, allowing for the production of custom-made, unique pieces, as well as the combination of alabaster with other materials. Apart from the newly developed craft, artistic work became again possible, chiefly by Volterran sculptor Albino Funaioli. After a short slump, the industry was revived again by the sale of mass-produced mannerist Expressionist sculptures, and was further enhanced in the 1920s by a new branch creating ceiling and wall lamps in the Art Deco style and culminating in the participation at the 1925 International Exposition of Modern Industrial and Decorative Arts from Paris. Important names from the evolution of alabaster use after World War II are Volterran Umberto Borgna, the \"first alabaster designer\", and later on the architect and industrial designer Angelo Mangiarotti.\n\nGypsum alabaster is a common mineral, which occurs in England in the Keuper marls of the Midlands, especially at Chellaston in Derbyshire, at Fauld in Staffordshire, and near Newark in Nottinghamshire. Deposits at all of these localities have been worked extensively.\n\nIn the 14th and 15th centuries its carving into small statues and sets of relief panels for altarpieces was a valuable local industry in Nottingham, as well as a major English export. These were usually painted, or partly painted. It was also used for the effigies, often life size, on tomb monuments, as the typical recumbent position suited the material's lack of strength, and it was cheaper and easier to work than good marble. After the English Reformation the making of altarpiece sets was discontinued, but funerary monument work in reliefs and statues continued.\n\nBesides examples of these carvings still in Britain (especially at the Nottingham Castle Museum, British Museum, and Victoria and Albert Museum), trade in mineral alabaster (rather than just the antiques trade) has scattered examples in the material that may be found as far afield as the Musée de Cluny, Spain, and Scandinavia.\n\nAlabaster also is found, although in smaller quantity, at Watchet in Somerset, near Penarth in Glamorganshire, and elsewhere. In Cumbria it occurs largely in the New Red rocks, but at a lower geological horizon. The alabaster of Nottinghamshire and Derbyshire is found in thick nodular beds or \"floors\" in spheroidal masses known as \"balls\" or \"bowls\" and in smaller lenticular masses termed \"cakes.\" At Chellaston, where the local alabaster is known as \"Patrick,\" it has been worked into ornaments under the name of \"Derbyshire spar\"―a term more properly applied to fluorspar.\n\n\"Black alabaster\" is a rare anhydrite form of the gypsum-based mineral. This black form is found in only three veins in the world, one each in United States, Italy, and China.\n\nAlabaster Caverns State Park, near Freedom, Oklahoma is home to a natural gypsum cave in which much of the gypsum is in the form of alabaster. There are several types of alabaster found at the site, including pink, white, and the rare black alabaster.\n\n\nChronological list of examples:\n\n\n"}
{"id": "25043612", "url": "https://en.wikipedia.org/wiki?curid=25043612", "title": "Amsterdam and Saint-Paul Islands temperate grasslands", "text": "Amsterdam and Saint-Paul Islands temperate grasslands\n\nThe Amsterdam and Saint-Paul Islands temperate grasslands is an ecoregion comprising two volcanic islands in the southern Indian Ocean. The only way to visit the islands is on the French research vessel \"Marion Dufresne II\" which services the Martin-de-Viviès research station on Amsterdam Island.\n\nÎle Amsterdam and Île Saint-Paul are two volcanoes 83 km from each other lying in the centre of a triangle between Australia, Antarctica and southern Africa. The islands are remote, situated about 3000 km (1860 mi) from each neighboring continent. They have cool oceanic climates with temperatures ranging from 13 °C (55 °F) in August to 17 °C (63 °F) in February, rainfall of 1,100 mm (43 in), persistent westerly winds, and high humidity levels.\n\nPlant life changes with elevation; at lower levels the volcanoes are covered with grass and tussock grasslands and sedge meadows and, on Amsterdam, the tree \"Phylica arborea\" mixed with ferns. Higher up, on the Plateau des Tourbières, there are shrubs, bogs, and mosses.\n\nThese isolated islands are not rich in wildlife diversity but are home to a large population of subantarctic fur seal. They are an important breeding ground for the Indian yellow-nosed albatross, flesh-footed shearwater, gentoo penguin, northern rockhopper penguin \"(Eudyptes moseleyi)\" great skua, Antarctic tern and the endemic Amsterdam albatross. They were formerly home to two endemic ducks: the Amsterdam wigeon and an undescribed species on Île Saint-Paul. \n\nAlthough the islands are remote and therefore safe from most human activity and pollution, several introduced species of both flora and fauna have damaged the environment; the feral cattle, in particular, grazed on young and regenerating plants and trampled on bird eggs. Five cattle had been brought to Amsterdam in January 1871; they were abandoned later that year and subsequently increased to a wild population of 2,000. As part of the French Southern and Antarctic Lands the islands are home to a research base which is working to preserve the original plant and animal life. Initially, they restricted the cattle to the northern half of Amsterdam. When the discovery was made in 2007 that the native flora and fauna had returned to the areas no longer grazed by the cattle, a plan to cull the remaining cattle on the island was launched in 2008. By 2010, the eradication program was complete, and no cattle remain on the island. The cattle population itself had become of scientific interest as it was a rare example of a feral, unmanned herd of cattle. Humans have caused other damage to the islands' ecosystems, as much of Amsterdam's woodland was cleared in the 18th and 19th centuries by whalers, sealer, and visitors from passing ships and is struggling to recover. Seal populations have recovered from the commercial seal hunting, and are no longer threatened.\n"}
{"id": "1327", "url": "https://en.wikipedia.org/wiki?curid=1327", "title": "Antiparticle", "text": "Antiparticle\n\nIn particle physics, every type of particle has an associated antiparticle with the same mass but with opposite physical charges (such as electric charge). For example, the antiparticle of the electron is the antielectron (which is often referred to as \"positron\"). While the electron has a negative electric charge, the positron has a positive electric charge, and is produced naturally in certain types of radioactive decay. The opposite is also true: the antiparticle of the positron is the electron.\n\nSome particles, such as the photon, are their own antiparticle. Otherwise, for each pair of antiparticle partners, one is designated as normal matter (the kind we are made of), and the other (usually given the prefix \"anti-\") as \"antimatter\".\n\nParticle–antiparticle pairs can annihilate each other, producing photons; since the charges of the particle and antiparticle are opposite, total charge is conserved. For example, the positrons produced in natural radioactive decay quickly annihilate themselves with electrons, producing pairs of gamma rays, a process exploited in positron emission tomography.\n\nThe laws of nature are very nearly symmetrical with respect to particles and antiparticles. For example, an antiproton and a positron can form an antihydrogen atom, which is believed to have the same properties as a hydrogen atom. This leads to the question of why the formation of matter after the Big Bang resulted in a universe consisting almost entirely of matter, rather than being a half-and-half mixture of matter and antimatter. The discovery of Charge Parity violation helped to shed light on this problem by showing that this symmetry, originally thought to be perfect, was only approximate.\n\nBecause charge is conserved, it is not possible to create an antiparticle without either destroying another particle of the same charge (as is for instance the case when antiparticles are produced naturally via beta decay or the collision of cosmic rays with Earth's atmosphere), or by the simultaneous creation of both a particle \"and\" its antiparticle, which can occur in particle accelerators such as the Large Hadron Collider at CERN.\n\nAlthough particles and their antiparticles have opposite charges, electrically neutral particles need not be identical to their antiparticles. The neutron, for example, is made out of quarks, the antineutron from antiquarks, and they are distinguishable from one another because neutrons and antineutrons annihilate each other upon contact. However, other neutral particles are their own antiparticles, such as photons, Z bosons,  mesons, and hypothetical gravitons and some hypothetical WIMPs.\n\nIn 1932, soon after the prediction of positrons by Paul Dirac, Carl D. Anderson found that cosmic-ray collisions produced these particles in a cloud chamber— a particle detector in which moving electrons (or positrons) leave behind trails as they move through the gas. The electric charge-to-mass ratio of a particle can be measured by observing the radius of curling of its cloud-chamber track in a magnetic field. Positrons, because of the direction that their paths curled, were at first mistaken for electrons travelling in the opposite direction. Positron paths in a cloud-chamber trace the same helical path as an electron but rotate in the opposite direction with respect to the magnetic field direction due to their having the same magnitude of charge-to-mass ratio but with opposite charge and, therefore, opposite signed charge-to-mass ratios.\n\nThe antiproton and antineutron were found by Emilio Segrè and Owen Chamberlain in 1955 at the University of California, Berkeley. Since then, the antiparticles of many other subatomic particles have been created in particle accelerator experiments. In recent years, complete atoms of antimatter have been assembled out of antiprotons and positrons, collected in electromagnetic traps.\n\nSolutions of the Dirac equation contained negative energy quantum states. As a result, an electron could always radiate energy and fall into a negative energy state. Even worse, it could keep radiating infinite amounts of energy because there were infinitely many negative energy states available. To prevent this unphysical situation from happening, Dirac proposed that a \"sea\" of negative-energy electrons fills the universe, already occupying all of the lower-energy states so that, due to the Pauli exclusion principle, no other electron could fall into them. Sometimes, however, one of these negative-energy particles could be lifted out of this Dirac sea to become a positive-energy particle. But, when lifted out, it would leave behind a \"hole\" in the sea that would act exactly like a positive-energy electron with a reversed charge. These holes were interpreted as \"negative-energy electrons\" by Paul Dirac and by mistake he identified them with protons in his 1930 paper \"A Theory of Electrons and Protons\" However, these \"negative-energy electrons\" turned out to be positrons, and not protons.\n\nThis picture implied an infinite negative charge for the universe—a problem of which Dirac was aware. Dirac tried to argue that we would perceive this as the normal state of zero charge. Another difficulty was the difference in masses of the electron and the proton. Dirac tried to argue that this was due to the electromagnetic interactions with the sea, until Hermann Weyl proved that hole theory was completely symmetric between negative and positive charges. Dirac also predicted a reaction  +  →  + , where an electron and a proton annihilate to give two photons. Robert Oppenheimer and Igor Tamm proved that this would cause ordinary matter to disappear too fast. A year later, in 1931, Dirac modified his theory and postulated the positron, a new particle of the same mass as the electron. The discovery of this particle the next year removed the last two objections to his theory.\n\nHowever, the problem of infinite charge of the universe remains. Also, as we now know, bosons also have antiparticles, but since bosons do not obey the Pauli exclusion principle (only fermions do), hole theory does not work for them. A unified interpretation of antiparticles is now available in quantum field theory, which solves both these problems.\n\nIf a particle and antiparticle are in the appropriate quantum states, then they can annihilate each other and produce other particles. Reactions such as  +  →   +  (the two-photon annihilation of an electron-positron pair) are an example. The single-photon annihilation of an electron-positron pair,  +  → , cannot occur in free space because it is impossible to conserve energy and momentum together in this process. However, in the Coulomb field of a nucleus the translational invariance is broken and single-photon annihilation may occur. The reverse reaction (in free space, without an atomic nucleus) is also impossible for this reason. In quantum field theory, this process is allowed only as an intermediate quantum state for times short enough that the violation of energy conservation can be accommodated by the uncertainty principle. This opens the way for virtual pair production or annihilation in which a one particle quantum state may \"fluctuate\" into a two particle state and back. These processes are important in the vacuum state and renormalization of a quantum field theory. It also opens the way for neutral particle mixing through processes such as the one pictured here, which is a complicated example of mass renormalization.\n\nQuantum states of a particle and an antiparticle can be interchanged by applying the charge conjugation (C), parity (P), and time reversal (T) operators. If formula_1 denotes the quantum state of a particle (n) with momentum p, spin J whose component in the z-direction is σ, then one has\nwhere n denotes the charge conjugate state, that is, the antiparticle. This behaviour under CPT symmetry is the same as the statement that the particle and its antiparticle lie in the same irreducible representation of the Poincaré group. Properties of antiparticles can be related to those of particles through this. If T is a good symmetry of the dynamics, then\nwhere the proportionality sign indicates that there might be a phase on the right hand side. In other words, particle and antiparticle must have\n\nOne may try to quantize an electron field without mixing the annihilation and creation operators by writing\n\nwhere we use the symbol \"k\" to denote the quantum numbers \"p\" and σ of the previous section and the sign of the energy, \"E(k)\", and \"a\" denotes the corresponding annihilation operators. Of course, since we are dealing with fermions, we have to have the operators satisfy canonical anti-commutation relations. However, if one now writes down the Hamiltonian\n\nthen one sees immediately that the expectation value of \"H\" need not be positive. This is because \"E(k)\" can have any sign whatsoever, and the combination of creation and annihilation operators has expectation value 1 or 0.\n\nSo one has to introduce the charge conjugate \"antiparticle\" field, with its own creation and annihilation operators satisfying the relations\n\nwhere \"k\" has the same \"p\", and opposite σ and sign of the energy. Then one can rewrite the field in the form\n\nwhere the first sum is over positive energy states and the second over those of negative energy. The energy becomes\n\nwhere \"E\" is an infinite negative constant. The vacuum state is defined as the state with no particle or antiparticle, \"i.e.\", formula_11 and formula_12. Then the energy of the vacuum is exactly \"E\". Since all energies are measured relative to the vacuum, H is positive definite. Analysis of the properties of \"a\" and \"b\" shows that one is the annihilation operator for particles and the other for antiparticles. This is the case of a fermion.\n\nThis approach is due to Vladimir Fock, Wendell Furry and Robert Oppenheimer. If one quantizes a real scalar field, then one finds that there is only one kind of annihilation operator; therefore, real scalar fields describe neutral bosons. Since complex scalar fields admit two different kinds of annihilation operators, which are related by conjugation, such fields describe charged bosons.\n\nBy considering the propagation of the negative energy modes of the electron field backward in time, Ernst Stueckelberg reached a pictorial understanding of the fact that the particle and antiparticle have equal mass m and spin J but opposite charges q. This allowed him to rewrite perturbation theory precisely in the form of diagrams. Richard Feynman later gave an independent systematic derivation of these diagrams from a particle formalism, and they are now called Feynman diagrams. Each line of a diagram represents a particle propagating either backward or forward in time. This technique is the most widespread method of computing amplitudes in quantum field theory today.\n\nSince this picture was first developed by Stueckelberg, and acquired its modern form in Feynman's work, it is called the Feynman–Stueckelberg interpretation of antiparticles to honor both scientists.\n\n\n"}
{"id": "307552", "url": "https://en.wikipedia.org/wiki?curid=307552", "title": "Bio-based material", "text": "Bio-based material\n\nA bio-based material is a material intentionally made from substances derived from living (or once-living) organisms. These materials are sometimes referred to as biomaterials, but this word also has another meaning. Strictly the definition could include many common materials such as wood and leather, but it typically refers to modern materials that have undergone more extensive processing. Unprocessed materials may be called biotic material. Bio-based materials or biomaterials fall under the broader category of bioproducts or bio-based products which includes materials, chemicals and energy derived from renewable biological resources.\n\nBio-based materials are often biodegradable, but this is not always the case.\n\nExamples include:\n"}
{"id": "31180783", "url": "https://en.wikipedia.org/wiki?curid=31180783", "title": "Biochore", "text": "Biochore\n\nA biochore is a subdivision of the biosphere consisting of biotopes that resemble one another and thus are colonized by similar vegetation.\n"}
{"id": "25252698", "url": "https://en.wikipedia.org/wiki?curid=25252698", "title": "Calathea allouia", "text": "Calathea allouia\n\nCalathea allouia, known as lerén or lairén in Spanish, and also known in English as Guinea arrowroot, and sweet corn root, is a plant in the arrowroot family, native to northern South America and the Caribbean, The name \"allouia\" is derived from the Carib name for the plant Leren is a minor food crop in the American tropics, but was one of the earliest plants domesticated by pre-historic Indians in South America. \nLeren is considered native to Cuba, Hispaniola, Puerto Rico, the Lesser Antilles, Trinidad & Tobago, Venezuela, Colombia, Ecuador, Peru and Brazil. It is reportedly naturalized in Jamaica\n\nLeren has been introduced as a minor root crop in tropical regions around the world.\n\nLeren is a perennial plant, approximately in height. It produces egg-shaped tuberous roots to long at the end of fibrous roots. The leaves are large, up to long and wide. Indigenous people of the Americas have used the durable leaves to make traditional medicines and as baby clothing. Leren usually reproduces itself through rhizomes which produce shoots and new plants.\n\nLeren is adapted to a tropical climate with alternating rainy and dry seasons. It sprouts with the first rains and grows rapidly, forming tubers which are harvested as the foliage begins to die back eight or nine months after the initial sprouting. The rhizomes, harvested at the same time, are tolerant of both drying and flooding, and divided and replanted again at the onset of the rainy season. Frequent irrigation is necessary during dry periods. Leren is often planted in shade or partial shade but can grow in full sun with adequate moisture and nutrients.\n\nLeren is traditionally cultivated on a small scale. Its cultivation is declining as it has been replaced by other crops.\n\nLeren is usually cooked by boiling the tubers for 15 to 60 minutes, As food, leren is often compared to water chestnut (Eleocharis dulcis) because leren, like the water chestnut, retains its crispness despite being cooked. Boiled leren has a taste similar to sweet corn, hence one of its common English names. The cooked tuber is covered with a thin, edible skin which is most easily peeled after cooking. Leren is mostly eaten as an hors d'oeuvre or appetizer. Leren tubers can be stored at room temperatures for up to three months, but do not tolerate refrigeration well.\n\nThe nutritional value of leren has not been thoroughly studied, but the tubers have a starch content of 13-15 percent and a protein content of 6.6 percent.\n\nArchaeologists have discovered that leren was one of the first plants domesticated in prehistoric South America. Leren, along with arrowroot (\"Maranta arundinacea\"), squash (\"Cucurbita moschata\"), and bottle gourd (\"Lagenaria siceraria\") were being cultivated in northern South American and Panama between 8200 BCE and 5600 BCE. It appears that the cultivation of leren spread to places where it was not likely native. For example, the people of the Las Vegas culture on the arid and semi-arid Santa Elena Peninsula of Ecuador grew leren by about 7000 BCE. Leren was being grown there to be eaten raw, dried, or ground into flour.\n"}
{"id": "21691608", "url": "https://en.wikipedia.org/wiki?curid=21691608", "title": "Circuit total limitation", "text": "Circuit total limitation\n\nCircuit total limitation (CTL) is one of the present-day standards for electrical panels sold in the United States according to the National Electrical Code. This standard requires an electrical panel to provide a physical mechanism to prevent installing more circuit breakers than it was designed for. This has generally been implemented by restricting the use of tandem (duplex) breakers to replace standard single pole breakers.\n\nThe 1965 edition of the NEC, article 384-15 was the first reference to the circuit total limitation of panelboards. , the location of this language is at Article 408.54 now titled \"Maximum Number of Overcurrent Devices.\"\nNon-CTL panels have not been made by reputable manufacturers since 1965.\n\nCircuitboards and panelboards built prior to 1965 did not have circuit total limiting devices or features built-in. To support these old panels, non-CTL circuit breakers that bypass the rejection feature are still sold \"for replacement use only.\" As a result, numerous unsafe situations have resulted where panels were dangerously overloaded because these non-CTL breakers continue to be used. With the use of non-CTL breakers, panels can be configured with the total number of circuits in excess of the designed capacity of that panel.\n\nThe 2008 code did away with the previous 42 circuit limitation on panelboards. One can now order panelboards with as many as 84 circuit places, and a corresponding ampacity rating. If a panelboard with a sufficient number of breaker positions is installed in the first place, the need for non-CTL breakers should be eliminated.\n\n"}
{"id": "7722", "url": "https://en.wikipedia.org/wiki?curid=7722", "title": "Compactron", "text": "Compactron\n\nCompactrons are a type of thermionic valve, or vacuum tube, which contain multiple electrode structures packed into a single enclosure. They were designed to compete with early transistor electronics and were used in televisions, radios, and similar roles.\n\nThe Compactron was a trade name applied to multi-electrode structure tubes specifically constructed on a 12-pin Duodecar base. This vacuum tube family was introduced in 1961 by General Electric in Owensboro, Kentucky to compete with transistorized electronics during the solid state transition. Television sets were a primary application. The idea of multi-electrode tubes itself was far from new and indeed the Loewe company of Germany was producing multi-electrode tubes as far back as 1926, and they even included all of the required passive components as well.\n\nUse was prevalent in televisions because transistors were slow to achieve the high power and frequency capabilities needed particularly in color television sets. The first portable color television, the General Electric Porta-Color, was designed using 13 tubes, 10 of which were Compactrons. Even before the compactron design was unveiled, nearly all tube based electronic equipment used multi-electrode tubes of one type or another. Virtually every AM/FM radio receiver of the 1950's and 60's used a 6AK8 (EABC80) tube (or equivalent) consisting of three diodes and a triode which was designed in 1954.\n\nCompactron's integrated valve design helped lower power consumption and heat generation (they were to tubes what integrated circuits were to transistors). Compactrons were also used in a few high end Hi-Fi stereos. They were also used by the Ampeg guitar amplifier company in some of their guitar amps. No modern tube based Hi-Fi systems are known to use this tube type, as simpler and more readily available tubes have again filled this niche.\n\nA distinguishing feature of most Compactrons is the placement of the evacuation tip on the bottom end, rather than the top end as was customary with \"miniature\" tubes, and a characteristic 3/4\" diameter circle pin pattern.\n\nExamples of Compactrons type types include:\n\nDue to their specific applications in television circuits, many different Compactron types were produced. Almost all were assigned using standard US tube numbers.\n\nIntegrated circuits (of the analogue and digital type) gradually took over all of the functions that the Compactron was designed for. \"Hybrid\" television sets produced in the early to mid-1970s made use of a combination of tubes (typically Compactrons), transistors, and integrated circuits in the same set. By the mid-1980s this type of tube was functionally obsolete. Compactrons simply don't exist in any TV sets designed after 1986. Other specialist uses of the tube declined in parallel with the television set manufacture. Manufacture of Compactrons ceased in the early 1990s. New old stock replacements for almost all Compactron types produced are easily found for sale on the Internet.\n"}
{"id": "1263779", "url": "https://en.wikipedia.org/wiki?curid=1263779", "title": "Consolamentum", "text": "Consolamentum\n\nConsolamentum, known as heretication to its Catholic opponents, was the unique sacrament of the Cathars. In common with Christianity, Cathars believed in original sin, and—like Gnostics—believed temporal pleasure to be sinful or unwise. The process of living thus inevitably incurred \"regret\" that required \"consolation to move nearer to God or to approach heaven. It occurred only twice in a lifetime: upon confirmation in the faith and upon impending death. It was available to both men and women who made a commitment to the faith. Following the ceremony the consoled individual became a \"Cathar Perfect\".\n\nAccording to the Albigenses and other Cathars, the \"consolamentum\" was an immersion (or baptism) in the Holy Spirit. It implied reception of all spiritual gifts including absolution from sin, spiritual regeneration, the power to preach and elevation to a higher plane of perfection.\n\nReference to the trinity was systematically replaced with the name of Christ since the doctrine of the Albigenses and Cathars professed a single unified deity (their Christology resembled modalistic monarchism in the West and adoptionism in the East). \n\nThe ritual took various forms; some used the entire New Testament scripture whilst others relied on extracts such as the Gospel attributed to John while administering consolation. There were reportedly some remote cases where holy water was used as a cleansing agent during \"consolamentum\" being profusely poured over the recipient's head until he/she was completely wet (as opposed to sprinkling).\n\nIn contrast to Catholic ceremonies, the form used by the majority of Cathars only required verbal blessings and scriptures administered to the person to be consoled, and did not involve tokens such as consecrated bread or wine because these would pass through the body and become befouled. Dying persons might abstain from food in order that their bodies be as pure as possible as it passed into eternity.\n\nAccording to a few known cases in the latter years of Catharism, the terminally ill would voluntarily undertake a complete fast known as the endura. It was only undertaken when death was clearly inevitable. It was a form of purification and separation from the material world which was controlled by the evil one. They believed that this final sacrifice ensured their reunification with the Good God.\n\nLaying on of hands was always part of the ceremony. Some historians have stated that incidents of ecstatic utterances during \"consolamentum\" was actually glossolalia, or \"speaking in tongues,\" which demanded that the rite be even more secretly guarded since this phenomenon occurring outside of the Catholic Church was considered witchcraft and was punishable by death. \n\nOnce consoled, Parfaits were required to be vegetarian, to be celibate, and to dedicate their lives to travelling and teaching Albigensian and Cathar doctrines. These Parfaits were the leaders of the Albigensian and other Cathar communities (Albigenses were a branch of Cathari in what was to later become southern France. Most people use the term Albigenses when referring to those that adhered to a branch of Cathari that was less dualistic and more closely resembled orthodoxy).\n\nThe vast majority of the population did not receive \"consolamentum\" until on the verge of death. Once given the \"consolamentum\", the same rules applied to them, though they were obviously not expected to travel or preach from their deathbed. This allowed most believers to live somewhat normal lives and receive \"consolamentum\" shortly before passing away.\n"}
{"id": "41311585", "url": "https://en.wikipedia.org/wiki?curid=41311585", "title": "Copenhagen Wheel", "text": "Copenhagen Wheel\n\nThe Copenhagen Wheel is a self-contained rear wheel electric bicycle system which transforms a traditional bicycle into a hybrid e-bike. The app-connected Wheel is equipped with an electric motor, battery, and suite of sensors that work together to seamlessly amplify a rider’s pedal power by up to ten times. The Copenhagen Wheel was developed at MIT’s Senseable City Lab in 2009 in partnership with the city of Copenhagen, and unveiled at the 2009 United Nations Climate Change Conference. In December 2012, Assaf Biderman, a co-inventor of the Wheel and Associate Director of the MIT Senseable City Lab, founded Superpedestrian Inc. with an exclusive license to commercialize the Wheel. After several years of engineering, testing, and validation, the Copenhagen Wheel officially launched in the U.S. in April 2017, and in Europe in October 2017\nThe Copenhagen Wheel contains a custom brushless motor, advanced sensors, control systems, and a lithium-ion battery, all enclosed within the rear wheel hub. The control system interfaces with a range of sensors measuring actual torque, power, cadence, pedal position, and acceleration to monitor a rider’s effort when pedaling. The Wheel responds to a rider’s inputs by providing the appropriate level of assistance at each moment, creating a seamless ride experience. The wheel’s battery is charged via an external cord that fits a standard wall outlet. Electronic Braking Assistance while riding will partially recharge the wheel when coasting or backpedaling. With a full charge, the wheel’s reported range is up to 50km (31mi), with variations depending on assist mode and terrain. Extensions in range are possible when using low-power modes.\n\nIn order to gain assist, the Wheel must be connected to Superpedestrian’s Wheel App, available for iOS and Android. The Wheel connects to the App via Bluetooth, and enables riders to personalize their cycling experience from their smartphone. The Wheel App tracks rides and calorie information, and allows riders to toggle between ride modes such as Turbo, Eco, and Exercise. A smartphone also acts as a digital key, activating the Wheel automatically when ready to ride, and communicating with the cloud in real time. A self-diagnostic safety system monitors components within the Wheel and proactively responds to events within milliseconds, protecting both rider and Wheel. Superpedestrian releases frequent updates to the Wheel App.\n\nCopenhagen Wheels are custom built to fit each customer’s bicycle. The Wheel is compatible with steel or aluminum bicycle frames, and requires rear rim brakes. The Copenhagen Wheel is available in both single and multi-speed versions. The multi-speed version supports a 7, 8, 9 or 10-speed Shimano/SRAM-compatible (external) multi-speed cassette. Copenhagen Wheels come in two wheel sizes -  622mm or 559mm - with either road, hybrid, or mountain bike tires. Available rim colors include silver and black.\n\nSuperpedestrian is currently working on future versions of the Copenhagen Wheel with updated features and wider compatibility. In the meantime, over-the-air firmware updates and app updates are available to current riders.\n\nThe Copenhagen Wheel is available throughout the U.S., Canada, and Europe. Superpedestrian offers a complete bike product as well, called Wheel + Bike. Customers can choose from a wide range of bicycle frames, from OEM’s like Marin, Public, Montague, Fyxation, Tern, Cinelli, and Fortified to pair with the Copenhagen Wheel. Superpedestrian also works with over 260 independent bicycle retailers across the United States and Europe to provide demos, maintenance, and installation services. The MSRP of the wheel as of August, 2018 was US$1749. \n\nThe Copenhagen Wheel has been covered in several dozen publications, including the New York Times, Boston Globe, and Fast Company. It was featured on NPR’s Here and Now, and got a free advertising boost when it was shown in the seventh season of Showtime’s \"Weeds\".\n\n\n"}
{"id": "170757", "url": "https://en.wikipedia.org/wiki?curid=170757", "title": "Cyclotron radiation", "text": "Cyclotron radiation\n\nCyclotron radiation is electromagnetic radiation emitted by accelerating charged particles deflected by a magnetic field. The Lorentz force on the particles acts perpendicular to both the magnetic field lines and the particles' motion through them, creating an acceleration of charged particles that causes them to emit radiation as a result of the acceleration they undergo as they spiral around the lines of the magnetic field.\n\nThe name of this radiation derives from the cyclotron, a type of particle accelerator used since the 1930s to create highly energetic particles for study. The cyclotron makes use of the circular orbits that charged particles exhibit in a uniform magnetic field. Furthermore, the period of the orbit is independent of the energy of the particles, allowing the cyclotron to operate at a set frequency. Cyclotron radiation is emitted by all charged particles travelling through magnetic fields, not just those in cyclotrons. Cyclotron radiation from plasma in the interstellar medium or around black holes and other astronomical phenomena is an important source of information about distant magnetic fields.\n\nThe power (energy per unit time) of the emission of each electron can be calculated:\n\nwhere \"E\" is energy, \"t\" is time, formula_2 is the Thomson cross section (total, not differential), \"B\" is the magnetic field strength, \"v\" is the velocity perpendicular to the magnetic field, \"c\" is the speed of light and formula_3 is the permeability of free space.\n\nCyclotron radiation has a spectrum with its main spike at the same fundamental frequency as the particle's orbit, and harmonics at higher integral factors. Harmonics are the result of imperfections in the actual emission environment, which also create a broadening of the spectral lines. The most obvious source of line broadening is non-uniformities in the magnetic field; as an electron passes from one area of the field to another, its emission frequency will change with the strength of the field. Other sources of broadening include collisional broadening as the electron will invariably fail to follow a perfect orbit, distortions of the emission caused by interactions with the surrounding plasma, and relativistic effects if the charged particles are sufficiently energetic. When the electrons are moving at relativistic speeds, cyclotron radiation is known as synchrotron radiation.\n\nThe recoil experienced by a particle emitting cyclotron radiation is called radiation reaction. Radiation reaction acts as a resistance to motion in a cyclotron; and the work necessary to overcome it is the main energetic cost of accelerating a particle in a cyclotron. Cyclotrons are prime examples of systems which experience radiation reaction.\n\nIn the context of magnetic fusion energy, cyclotron radiation losses translate into a requirement for a minimum plasma energy density in relation to the magnetic field energy density.\n\nCyclotron radiation would likely be produced in a high altitude nuclear explosion. Gamma rays produced by the explosion would ionize atoms in the upper atmosphere and those free electrons would interact with the Earth's magnetic field to produce cyclotron radiation in the form of an electromagnetic pulse (EMP). This phenomenon is of concern to the military as the EMP may damage solid state electronic equipment.\n\n"}
{"id": "729824", "url": "https://en.wikipedia.org/wiki?curid=729824", "title": "Edward Davy", "text": "Edward Davy\n\nEdward Davy (16 June 1806 – 26 January 1885) was an English physician, scientist, and inventor who played a prominent role in the development of telegraphy, and invented an electric relay.\n\nDavy was born in Ottery St Mary, Devonshire, England, son of Thomas Davy (medical practitioner and house surgeon at Guy's Hospital, London). Edward Davy was educated at a school run by his maternal uncle in Tower Street, London. He was then apprenticed to Dr C. Wheeler, house surgeon at St Bartholomew's Hospital. Davy won the prize for botany in 1825, was licensed by the Worshipful Society of Apothecaries in 1828 and the Royal College of Surgeons in 1829. Soon after graduating, Davy began trading as an operative chemist under the name of Davy & Co. In 1836 he published a small book \"Experimental Guide to Chemistry\", at the end of which was a catalogue of goods supplied by his firm.\n\nDavy published \"Outline of a New Plan of Telegraphic Communication\" in 1836 and carried out telegraphic experiments the following year. He demonstrated the operation of the telegraph over a mile of wire in Regent's Park. In 1837 he demonstrated a working model of the telegraph in Exeter Hall. He was granted a patent for his telegraph in 1838. However, he was soon obliged to drop his investigations of telegraphy for personal reasons. His patent was purchased by the Electric Telegraph Company in 1847 for £600. Davy also invented an electric relay. He used a magnetic needle which dipped into a mercury contact when an electric current passed through the surrounding coil. In recognition of this he was elected in 1885 as an honorary member of the Society of Telegraph Engineers and was informed of this by telegraph shortly before his death.\n\nIn 1838 Davy migrated to South Australia without his first wife and son. He was editor of the \"Adelaide Examiner\" from June to July 1842 and was elected president of the Port Adelaide Mechanics' Institute at its inaugural meeting in 1851. Davy was a director and manager of the Adelaide Smelting Company and became chief assayer of the Government Assay Office in Adelaide in February 1852.\n\nDavy was appointed assay master in Melbourne in July 1853 until the office was abolished in October 1854. For a short while, he took up farming near Malmsbury, Victoria then moved into Malmsbury where he practised as a physician for the rest of his life. He was three times mayor of Malmbury.\n\n"}
{"id": "27420170", "url": "https://en.wikipedia.org/wiki?curid=27420170", "title": "Electric discharge in gases", "text": "Electric discharge in gases\n\nElectric discharge in gases occurs when electric current flows through a gaseous medium due to ionization of the gas. Depending on several factors, the discharge may radiate visible light. The properties of electric discharges in gases are studied in connection with design of lighting sources and in the design of high voltage electrical equipment.\n\nIn cold cathode tubes, the electric discharge in gas has three regions, with distinct current-voltage characteristics:\n\nGlow discharge is facilitated by electrons striking the gas atoms and ionizing them. For formation of glow discharge, the mean free path of the electrons has to be reasonably long but shorter than the distance between the electrodes; glow discharges therefore do not readily occur at both too low and too high gas pressures.\n\nThe breakdown voltage for the glow discharge depends nonlinearly on the product of gas pressure and electrode distance according to Paschen's law. For a certain pressure × distance value, there is a lowest breakdown voltage. The increase of strike voltage for shorter electrode distances is related to too long mean free path of the electrons in comparison with the electrode distance.\n\nA small amount of a radioactive element may be added into the tube, either as a separate piece of material (e.g. nickel-63 in krytrons) or as addition to the alloy of the electrodes (e.g. thorium), to preionize the gas and increase the reliability of electrical breakdown and glow or arc discharge ignition. A gaseous radioactive isotope, e.g. krypton-85, can also be used. Ignition electrodes and keepalive discharge electrodes can also be employed.\n\nThe E/N ratio between the electric field E and the concentration of neutral particles N is often used, because the mean energy of electrons (and therefore many other properties of discharge) is a function of E/N. Increasing the electric intensity E by some factor q has the same consequences as lowering gas density N by factor q.\n\nIts SI unit is V·cm, but the Townsend unit (Td) is frequently used.\n\nThe use of a glow discharge for solution of certain mapping problems was described in 2002.\n"}
{"id": "23675159", "url": "https://en.wikipedia.org/wiki?curid=23675159", "title": "EnergySmart Home Scale", "text": "EnergySmart Home Scale\n\nThe EnergySmart Home Scale (E-Scale) is a standard applied to homes in the USA that meet the Builders Challenge program requirements. The E-Scale visually shows the energy performance of the labeled home. Homes need to score a 70 or less on the E-Scale to qualify for the Builders Challenge.\n\nThe E-Scale is based on the 2004 International Energy Conservation Code, with a 100 equating to a code built home. A home that scores a 70 on the E-Scale is 30% more energy efficient than a code built home. The E-Scale allows for an easy comparison between homes, very similar to a MPG sticker for a car but for a home. Homes meeting the Challenge will receive an E-Scale with a Sunburst displaying the E-Scale rating of the home, enabling home buyers to easily compare the energy performance of homes.\n\nThe E-Scale ends at zero, which equates to the Builders Challenge goal to provide a “net zero energy home (NZEH) anywhere in the United States - a grid-connected home that, over the course of a year, produces as much energy as it uses” \n\nThe Builders Challenge program and E-Scale requirements are based on years of research results from the United States Department of Energy’s Building America program. Building America research “is expected to achieve cost-neutral Net-Zero Energy Homes by 2020. \"Cost-neutral\" means that the added first costs of system enhancements (when amortized over a 30 year period) are equal to the monthly energy cost savings that result from these enhancements. While the Building America program develops and field-tests the technologies needed to cost-effectively move to higher levels of efficiency, the Builders Challenge will promote the implementation of proven strategies that builders can implement in any climate region.” \n\n"}
{"id": "14352487", "url": "https://en.wikipedia.org/wiki?curid=14352487", "title": "Energy in Romania", "text": "Energy in Romania\n\nEnergy in Romania describes energy and electricity production, consumption and import in Romania.\n\nRomania has significant oil and gas reserves, substantial coal deposits and it has substantial hydroelectric power installed. However, Romania imports oil and gas from Russia and other first world countries, (it mainly imports from the EU). To ease this dependency Romania seeks to use nuclear power as an alternative to electricity generation. So far, the country has two nuclear reactors, located at Cernavodă, accounting for about 18–20% of the country's electricity production, with the second one online in 2007. Nuclear waste is stored on site at reprocessing facilities.\n\nElectric power in Romania is dominated by government enterprises, although privately operated coal mines and oil refineries also existed. Accordingly, Romania placed an increasingly heavy emphasis on developing nuclear power generation. Electric power was provided by the \"Romanian Electric Power Corporation\" (CONEL). Energy used in electric power generation consisted primarily of nuclear, coal, oil, and liquefied natural gas (LNG). Of the electricity generated in 2007, 13.1 percent came from nuclear plants then in operation, 41.69 percent from thermal plants (oil and coal), and 25.8 percent from hydroelectric sites. It was predicted in 2007 that the generation structure by the year 2010 would be 10.2 percent hydroelectric, 12.2 percent oil, 22.9 percent coal, 10.2 percent LNG, and 44.5 percent nuclear.\n\nAccording to the National Energy Strategy adopted by the government in September 2007, investments in upgrading power plants would top EUR 35 bln in the 2007–2020 period. EUR 8.6 bln will be invested in the electricity generation.\n\nIn the decade between 1989 and 1999, Romania saw decrease of its greenhouse gas emissions by 55%. This can be accounted for by a 45% decrease in energy use due to languishing economy, and a 15% decrease in its carbon intensity of energy use. In this period of time the carbon intensity of Romania's economy decreased by 40%, while Romania's GDP declined 15%. Romania's GDP has recovered significantly since then.\n\nPossessing substantial oil refining capacities, Romania is particularly interested in the Central Asia – Europe pipelines and seeks to strengthen its relations with some Persian Gulf states. With 10 refineries and an overall refining capacity of approximately , Romania has the largest refining industry in the region. Romania's refining capacity far exceeds domestic demand for refined petroleum products, allowing the country to export a wide range of oil products and petrochemicals—such as lubricants, bitumen, and fertilizers—throughout the region.\n\nEnergy producers were dominated by government enterprises, although privately operated coal mines and oil refineries also existed. Accordingly, Romania placed an increasingly heavy emphasis on developing nuclear power generation.\n\nAccording to the data displayed by Electrica Furnizare SA in August 2017 (source www.electricafurnizare.ro), the structure of electricity production of Romania in 2016 was provided by:\n\n1. High-carbon energy sources: 40.13%, as follows\n\n2. Low-carbon sources: 59.87%, as follows\n\nRomania has an estimated total usable hydropower of 36,000 GWh per year.\n\nRomania placed a heavy emphasis on nuclear power generation. The country's first nuclear power plant, the Cernavodă Number One located near Cernavodă, opened in 1993. Two reactors were operational in 2007 when atomic power generation was an estimated 21,158 million kilowatts, or 23.1 percent of total electric power.\n\nTo cover the increasing energy needs of its population and ensure the continued raising of its living standard, Romania plans several nuclear power plants. Nuclear power proposals were presented as early as in the 1990s, but plans were repeatedly canceled even after bids were made by interested manufacturers because of high costs and safety concerns.\n\nBesides the nuclear power plant in Cernavodă, which consists of two nuclear reactors, the Government has recently announced that it plans to build another nuclear power plant which would most likely be located near one of the major rivers in Transylvania. The new nuclear power plant would consist of two or four nuclear reactors and would have a total output of 2,400 MW. The feasibility studies will be ready by mid-2009.\n\nRomania has always chosen CANDU nuclear reactors because they use natural unenriched uranium which is cheap and available locally and because they can be refueled online.\n\n"}
{"id": "32181898", "url": "https://en.wikipedia.org/wiki?curid=32181898", "title": "Energy policy of Iraq", "text": "Energy policy of Iraq\n\nEnergy policy of Iraq describes the politics of Iraq related to energy. Energy in Iraq describes the energy and electricity production, consumption and import in Iraq. Electricity sector in Iraq is the main article of electricity in Iraq.\n\nOil revenues are the major income in the economy of Iraq.\n\nBased on data from BP at the end of 2009 the highest proved oil reserves including the non-conventional oil deposits are in 1) Saudi Arabia (18 per cent of global reserves) 2) Canada (12%, mostly oil sands) 3) Venezuela (12%, mostly tar sands) 4) Iran (9%) and 5) Iraq (8%)\n\nIraq is a member of OPEC.\n\nThe global oil and gas prices have been strongly influenced by political decisions and events. For example, the oil embargo 1967 and 1973 oil crisis during the 1970s, the Iran-Iraq War in the 1980s, the Iraq-Kuwait War in the 1990s and the Iraq War from 2003.\n\nOne of the corruption risks is that the oil resources are publicly owned but often privately produced. The complex system of licenses and fees may drive corruption incentives. According to Transparency International Bribe Payers Index 2008, the oil and gas industry in general is highly vulnerable to 1) bribery of public officials and 2) undue influence on the legislative process and government policies. IMF Working Paper confirms the relationship between oil rents and corruption. Higher increases in oil rents tends to increase corruption and erode political rights. Open Budget Survey 2008 by International Budget Partnership confirmed that the oil- and gas-dependent countries tend to be less transparent.\n"}
{"id": "13113155", "url": "https://en.wikipedia.org/wiki?curid=13113155", "title": "Estación de Fotobiología Playa Unión", "text": "Estación de Fotobiología Playa Unión\n\nThe Estación de Fotobiología Playa Unión (EFPU) (in English: 'Playa Unión Photobiology Station') is a non-profit organization, devoted to scientific research about the effects of ultraviolet radiation on aquatic ecosystems.\n\nEFPU is located at Playa Unión, Chubut province, Argentina.\n\n"}
{"id": "11042", "url": "https://en.wikipedia.org/wiki?curid=11042", "title": "Fat", "text": "Fat\n\nFat is one of the three main macronutrients, along with the other two: carbohydrate and protein. Fats molecules consist of primarily carbon and hydrogen atoms, thus they are all hydrocarbon molecules. Examples include cholesterol, phospholipids and triglycerides.\n\nThe terms \"lipid\", \"oil\" and \"fat\" are often confused. \"Lipid\" is the general term, though a lipid is not necessarily a triglyceride. \"Oil\" normally refers to a lipid with short or unsaturated fatty acid chains that is liquid at room temperature, while \"fat\" (in the strict sense) may specifically refer to lipids that are solids at room temperature – however, \"fat\" (in the broad sense) may be used in food science as a synonym for lipid. Fats, like other lipids, are generally hydrophobic, and are soluble in organic solvents and insoluble in water.\n\nFat is an important foodstuff for many forms of life, and fats serve both structural and metabolic functions. They are a necessary part of the diet of most heterotrophs (including humans) and are the most energy dense, thus the most efficient form of energy storage and do not bind water thus do not increase body mass as much as proteins, especially carbohydrates, both of which bind a lot more water. \n\nSome fatty acids that are set free by the digestion of fats are called essential because they cannot be synthesized in the body from simpler constituents. There are two essential fatty acids (EFAs) in human nutrition: alpha-linolenic acid (an omega-3 fatty acid) and linoleic acid (an omega-6 fatty acid). Other lipids needed by the body can be synthesized from these and other fats. Fats and other lipids are broken down in the body by enzymes called lipases produced in the pancreas.\n\nFats and oils are categorized according to the number and bonding of the carbon atoms in the aliphatic chain. Fats that are saturated fats have no double bonds between the carbons in the chain. Unsaturated fats have one or more double bonded carbons in the chain. The nomenclature is based on the non-acid (non-carbonyl) end of the chain. This end is called the omega end or the n-end. Thus alpha-linolenic acid is called an omega-3 fatty acid because the 3rd carbon from that end is the first double bonded carbon in the chain counting from that end. Some oils and fats have multiple double bonds and are therefore called polyunsaturated fats. Unsaturated fats can be further divided into cis fats, which are the most common in nature, and trans fats, which are rare in nature. Unsaturated fats can be altered by reaction with hydrogen effected by a catalyst. This action, called hydrogenation, tends to break all the double bonds and makes a fully saturated fat. To make vegetable shortening, then, liquid \"cis\"-unsaturated fats such as vegetable oils are hydrogenated to produce saturated fats, which have more desirable physical properties e.g., they melt at a desirable temperature (30–40 °C), and store well, whereas polyunsaturated oils go rancid when they react with oxygen in the air. However, trans fats are generated during hydrogenation as contaminants created by an unwanted side reaction on the catalyst during partial hydrogenation.\n\nSaturated fats can stack themselves in a closely packed arrangement, so they can solidify easily and are typically solid at room temperature. For example, animal fats tallow and lard are high in saturated fatty acid content and are solids. Olive and linseed oils on the other hand are unsaturated and liquid. Fats serve both as energy sources for the body, and as stores for energy in excess of what the body needs immediately. Each gram of fat when burned or metabolized releases about 9 food calories (37 kJ = 8.8 kcal). Fats are broken down in the healthy body to release their constituents, glycerol and fatty acids. Glycerol itself can be converted to glucose by the liver and so become a source of energy.\n\nThere are many different kinds of fats, but each is a variation on the same chemical structure. All fats are derivatives of fatty acids and glycerol. Most fats are glycerides, particularly triglycerides (triesters of glycerol). One chain of fatty acid is bonded to each of the three -OH groups of the glycerol by the reaction of the carboxyl end of the fatty acid (-COOH) with the alcohol; I.e. three chains per molecule. Water is eliminated and the carbons are linked by an -O- bond through dehydration synthesis. This process is called esterification and fats are therefore esters. As a simple visual illustration, if the kinks and angles of these chains were straightened out, the molecule would have the shape of a capital letter E. The fatty acids would each be a horizontal line; the glycerol \"backbone\" would be the vertical line that joins the horizontal lines. Fats therefore have \"ester\" bonds.\n\nThe properties of any specific fat molecule depend on the particular fatty acids that constitute it. Fatty acids form a family of compounds that are composed of increasing numbers of carbon atoms linked into a zig-zag chain (hydrogen atoms to the side). The more carbon atoms there are in any fatty acid, the longer its chain will be. Long chains are more susceptible to intermolecular forces of attraction (in this case, van der Waals forces), and so the longer ones melt at a higher temperature (melting point).\n\nFatty acid chains may also differ by length, often categorized as short to very long.\nAny of these aliphatic fatty acid chains may be glycerated and the resultant fats may have tails of different lengths from very short triformin to very long, e.g., cerotic acid, or \"hexacosanoic acid\", a 26-carbon long-chain saturated fatty acid. Long chain fats are exemplified by tallow (lard) whose chains are 17 carbons long. Most fats found in food, whether vegetable or animal, are made up of medium to long-chain fatty acids, usually of equal or nearly equal length. Many cell types can use either glucose or fatty acids for this energy. In particular, heart and skeletal muscle prefer fatty acids. Despite long-standing assertions to the contrary, fatty acids can also be used as a source of fuel for brain cells.\n\nFats are also sources of essential fatty acids, an important dietary requirement. They provide energy as noted above. Vitamins A, D, E, and K are fat-soluble, meaning they can only be digested, absorbed, and transported in conjunction with fats. Fats play a vital role in maintaining healthy skin and hair, insulating body organs against shock, maintaining body temperature, and promoting healthy cell function. Fat also serves as a useful buffer against a host of diseases. When a particular substance, whether chemical or biotic, reaches unsafe levels in the bloodstream, the body can effectively dilute—or at least maintain equilibrium of—the offending substances by storing it in new fat tissue. This helps to protect vital organs, until such time as the offending substances can be metabolized or removed from the body by such means as excretion, urination, accidental or intentional bloodletting, sebum excretion, and hair growth.\n\nIn animals, adipose tissue, or fatty tissue is the body's means of storing metabolic energy over extended periods of time. Adipocytes (fat cells) store fat derived from the diet and from liver metabolism. Under energy stress these cells may degrade their stored fat to supply fatty acids and also glycerol to the circulation. These metabolic activities are regulated by several hormones (e.g., insulin, glucagon and epinephrine). Adipose tissue also secretes the hormone leptin. \n\nThe location of the tissue determines its metabolic profile: visceral fat is located within the abdominal wall (i.e., beneath the wall of abdominal muscle) whereas \"subcutaneous fat\" is located beneath the skin (and includes fat that is located in the abdominal area beneath the skin but \"above\" the abdominal muscle wall). Visceral fat was recently discovered to be a significant producer of signaling chemicals (i.e., hormones), among which several are involved in inflammatory tissue responses. One of these is resistin which has been linked to obesity, insulin resistance, and Type 2 diabetes. This latter result is currently controversial, and there have been reputable studies supporting all sides on the issue.\n\nDietary consumption of fatty acids has effects on human health. Studies have found that replacing saturated fats with \"cis\" unsaturated fats in the diet reduces risk of cardiovascular disease. For example, a 2015 systematic review of randomized control trials by the Cochrane Library concluded: \"Lifestyle advice to all those at risk of cardiovascular disease and to lower risk population groups should continue to include permanent reduction of dietary saturated fat and partial replacement by unsaturated fats.\"\n\nNumerous studies have also found that consumption of \"trans\" fats increases risk of cardiovascular disease. The Harvard School of Public Health advises that replacing \"trans\" fats and saturated fats with \"cis\" monounsaturated and polyunsaturated fats is beneficial for health.\n\n"}
{"id": "39946273", "url": "https://en.wikipedia.org/wiki?curid=39946273", "title": "Fuel pricing software", "text": "Fuel pricing software\n\nFuel Pricing Software is a business tool intended to allow retail fuel marketers to determine the most appropriate price at which to offer fuel based on their pricing strategies.\n\nThe software solutions were developed to help fuel retailers manage margins, sales and stock volumes in the face of fuel market price volatility, unfavorable supply arrangements, and price sensitivity of retail customers. This can even include managing fuel order placement and monitoring overall site traffic.\n\nFactors such as pricing of competitors, analysis of current market costs and sales for each grade of fuel are all considerations that affect the outcome of fuel prices. In addition to this, sales by related convenience stores can effectively subsidize the fuel, lowering the price. Some software is designed to integrate with point-of-sale systems, pumps, wetstock systems providers, and electronic price signs to automate instant price changes. This saves store staff the inconvenience of changing gas price signs manually and allows retailers to more responsively post optimal pricing, even monitoring the market in real time.\nFuel pricing software is intended to replace manual or spreadsheet-based processes that could delay the update of fuel costs and jeopardize profit margins. Delayed updates of fuels costs can cause fuel buyers to pay more than necessary, with day-to-day price swings occurring at 3 cents nearly 50% of the time and 5 cents at just over 25%.\n\nFuel pricing mobile applications for consumers, such as GasBuddy and Fuel Finder, are intended to locate the best prices on fuel according to their current location.\n\n"}
{"id": "31311930", "url": "https://en.wikipedia.org/wiki?curid=31311930", "title": "Ghana Atomic Energy Commission", "text": "Ghana Atomic Energy Commission\n\nThe Ghana Atomic Energy Commission (GAEC) is the state organization in Ghana involved with surveillance of the use of nuclear energy in Ghana. It is similar in aim to the Ghana Nuclear Society (GNS), with the difference being that the GNS is a nonprofit organisation, whereas the GAEC is part of the parliament of Ghana. Its primary objectives were set out by the parliament act 588, which involve investigating the use of nuclear energy for Ghana and supporting research and development both in Ghana and abroad.\n\n\nResponsible for preservation, maintenance and enhancement of nuclear knowledge in Ghana and Africa through the provision of high quality teaching, research, entrepreneurship training, service and development of postgraduate programmes in the nuclear sciences and technology.\n\nAuthorize, inspect and control all activities and practices involving sealed radiation sources, ionizing radiation and other sources, radioactive materials and x-rays used in hospitals in Ghana. Implementation of safety culture by providing adequate human resource development in radiation and waste safety for management and operating organizations. Conduct research and technical services in radiation and waste safety.\n\n\n\n"}
{"id": "57203691", "url": "https://en.wikipedia.org/wiki?curid=57203691", "title": "Green strength", "text": "Green strength\n\nGreen strength, or handling strength, can be defined as the strength of a material it is processed to form its final ultimate tensile strength. This strength is usually considerably lower than the final ultimate strength of a material. The term green strength is usually referenced when discussing non-metallic materials such as adhesives and elastomers (such as rubber). Recently, it has also been referenced in metallurgy applications such as powdered metallurgy.\n\nA joint made through the use of an adhesive can be referred to as an adhesive joint or bond.\n\nThe green strength of adhesives is the early development of bond strength of an adhesive. It indicated \"that the adhesive bond is strong enough to be handled a short time after the adherents are mated but much before full cure is obtained.\" Usually, this strength is significantly lower than the final curing strength. Most adhesives typically have an initial green strength and a final ultimate tensile strength listed for their application. For household adhesives, this data is usually reflected on the packaging.\n\nThe best example of this is seen in typical epoxies from a local hardware stores. During curing, the epoxy will travel into an initial curing phase, also called \"green phase\", when it begins to gel. At that point, the epoxy is no longer workable and will move from being tacky to a firm rubber-like texture. While the epoxy is only partially cured at this point, it has formed a lower green strength. Normally, this process occurs within 30 minutes to 1 hour. At this time, the part in question can be handled, but cannot handle large loads or stress. It typically takes up to 24 hours for a standard epoxy to cure to its final and complete strength.\n\nTemperature is an important factor in the time it takes for an adhesive to form the green strength. While this can vary from adhesive to adhesive, general speaking, heat can speed up the process to form the green strength and the overall curing time. Time-Temperature-Transformation Diagrams exist for various adhesives that relate the time and temperature to the state of the adhesive during curing. This allows for a proper understanding of when the green strength will be reached for an adhesive joint based on certain conditions.\n\nMechanical testing can be used to verify the green strength of a material. This will allow the user the understand the amount of load that can be applied in the green phase before final cure.\n\nTensile loading can be verified by various testing methods. Multiple ASTM specifications exist for the tensile testing of adhesives that are relatively easy to follow. Such tests include the process of attaching the adhesive to two adherents (typically wood or steel) then testing the joint with a pull-type test. One example is the use us ASTM Test Method D2095. In this test, two metal rod ends are polished so it contains no burrs that could affect the adhesive bond. It also machined so the surfaces are perfectly parallel. The rods are then butted against each with the adhesive joining them. As it cures and sets, the fulfillment of green strength can be tested by a pull test, putting the bond in full tension load.\n\nShear loading can also be tested in respect to green strength. Most adhesive bonds used in design require the bond to typically be in a state of shear, not tensile. Because of this, it is very important to understand the shear loading of a joint in relation to its green strength and final strength. Just like in tensile loading, ASTM provides very specific testing methods for a joint in shear loading. The standard lap shear specimen test is described in ASTM D1002. This test is the single common and discussed test method for adhesive bonds. In this method, the surface is prepped and cleaned for each specimen. The adhesive is then applied to the area that will be lapped. This lap length is generally 0.5\" and the bond width is 1\". The bond is then fixed and allowed to cure. For green strength testing, the fixture can be removed, at the appropriate time, and the specimen can be loaded in shear until it finally fails. This will verify the green strength of the material.\n\nOther testing, such as cleavage loading and peel test, can be used to determine both the green strength and final strength of a material. These are typically not reflected on the data sheet for standard adhesives, but can be used for testing of adhesives based on their applications in residential and commercial environments.\n\nGreen strength is a term in the elastomer industry used to describe the strength of an elastomer an unvulcanized and uncured state. The most popular, referenced type of elastomer is rubber.\nFor rubber composites, the green strength is essential during the formation and manufacturing of materials such as radial tires, tank tracks, etc. These rubbers must be stretched from one mill to another during processing to form the final vulcanized product. The green strength allows this to happen without tearing or wrinkling of the material before final curing is complete.\n\nTo improve the green strength of elastomers and prevent issues during forming, various additives and compounds are typically added to the composite. Also, fabrication and forming techniques have been modified to reduce the amount of stress on the material before it is vulcanized. These techniques are a pertinent component of the tire making industry because it is a process that requires lots of forming, stretching, and bending during fabrication before the final curing is complete.\nGreen strength of metals is typically referenced in the field of powder metallurgy. Powder metallurgy refers to the fabrication of materials or components from powders. In powder metallurgy, the initial green strength is formed during compacting and forming. Increased complexity of parts and geometry have created a need for a higher green strength during this process. \n\nThere are several limitations that restrict the ability to increase green strength in powder metallurgy components. Characteristics such as particle size and compressibility pose limits on the final green strength. \n\nThere are currently various studies in place to improve the green strength of powder metallurgy. The use of advanced lubricants and the addition of high alloys have shown that it is possible to increase the green strength of these materials. \n"}
{"id": "1666688", "url": "https://en.wikipedia.org/wiki?curid=1666688", "title": "HVDC Sileru–Barsoor", "text": "HVDC Sileru–Barsoor\n\nThe HVDC Sileru–Barsoor is a high voltage direct current transmission system between Sileru and Barsoor in India. It is in service since 1989 as the first HVDC line in the country. The HVDC Sileru–Barsoor is a bipolar HVDC with a voltage of 200 kV and a transmission rate of 400 megawatts. The HVDC Sileru–Barsoor couples two asynchronously operated parts of Indian electricity mains over a long overhead line, which was originally a double-circuit 220 kV AC line from which three conductors are paralleled.\n\nThis HVDC line is not in use for a long time. On 1 January 2014, the NEW grid is synchronised with the Southern regional grid making this HVDC link redundant. 200 kV AC line of NEW grid can be directly connected to the 200 kV line of Southern grid bypassing the HVDC converter stations. Thus the energy losses taking place in the converter stations can be avoided and these HVDC converter stations can be shifted to elsewhere to export/import power from other countries.\n\nThese unused converter stations owned by APTransCo, can be used to generate hydro electricity from Srisailam Dam or Nagarjuna Sagar Dam hydro power stations when the water level in the reservoirs are below the minimum rated water head by generating power at lower/under frequency (< 50 Hz). The lower frequency power is converted to normal grid frequency power with these converter stations. Thus more water available in the dead storage of these reservoirs can be used for additional power generation to meet peaking demand in summer months.\n\n"}
{"id": "4802230", "url": "https://en.wikipedia.org/wiki?curid=4802230", "title": "Hydrogenated jojoba oil", "text": "Hydrogenated jojoba oil\n\nHydrogenated jojoba oil is the end product of the complete hydrogenation of jojoba oil.\n\nHydrogenated jojoba oil is a straight-chain wax ester of 36 to 46 carbons in length, an ester bond in the approximate middle of the chain, no branching, no points of unsaturation, and terminal methyl groups at each end. As with jojoba oil, there is no triglyceride component of hydrogenated jojoba oil.\n\nHydrogenated jojoba oil is a hard, crystalline wax ester. The melting point is 68-70 °C and the iodine value is < 2.0, making it one of the very few commercially available, high-melting-point wax esters of botanical origin. Hydrogenated jojoba oil is relatively colourless and odourless. Hydrogenated jojoba oil contains no trans isomers.\n\nHydrogentated Jojoba Oil has particular functionality in cosmetics, due to its ability to strengthen the wax matricies of \"stick\" formulations such as lipstick, eyeliner, lip balm et al. Hydrogenated jojoba oil is often used as a material for exfoliation particles due to its uniform color, hardness, and controllable crystallinity.\n\n"}
{"id": "52842055", "url": "https://en.wikipedia.org/wiki?curid=52842055", "title": "Hygrophyte", "text": "Hygrophyte\n\nA Hygrophyte (Greek \"hygros\" = wet + \"phyton\" = plant) is a plant living above ground that are adapted to the conditions of abundant moisture pads of surrounding air. These plants inhabit mainly wet and dark forests and islands darkened swamp and very humid and floody meadows. Within the group of all types of terrestrial plants, they are at least resistant to drought.\n\nAccording to the environmental attributes are a group of plants between categories hydrophytes (aquatic plants) and mesophytes (plants in moderate environmental conditions)\nPlants living in the or moist habitats typically lack xeromorphic features.\n\n"}
{"id": "28379285", "url": "https://en.wikipedia.org/wiki?curid=28379285", "title": "International Convention on Civil Liability for Oil Pollution Damage", "text": "International Convention on Civil Liability for Oil Pollution Damage\n\nThe International Convention on Civil Liability for Oil Pollution Damage, 1969, renewed in 1992 and often referred to as the CLC Convention, is an international maritime treaty admistered by the International Maritime Organization that was adopted to ensure that adequate compensation would be available where oil pollution damage was caused by maritime casualties involving oil tankers (i.e. ships that carry oil as cargo).\n\nThe convention introduces strict liability for shipowners.\n\nIn cases when the shipowner is deemed guilty of fault for an instance of oil pollution, the convention does not cap liability.\n\nWhen the shipowner is not at fault, the convention caps liability at between 3 million special drawing rights (SDR) for a ship of to 59.7 million SDR for ships over .\n\nThe 2000 Amendments\n\nAdoption: 18 October 2000\n\nEntry into force: 1 November 2003\n\nThe amendments raised the compensation limits by 50 percent compared to the limits set in the 1992 Protocol, as follows: \nFor a ship not exceeding 5,000 gross tonnage, liability is limited to 4.51 million SDR (US$5.78 million)\nFor a ship 5,000 to 140,000 gross tonnage: liability is limited to 4.51 million SDR plus 631 SDR for each additional gross tonne over 5,000\nFor a ship over 140,000 gross tonnage: liability is limited to 89.77 million SDR\n\nIf a ship carries more than 2000 tons of oil in cargo, CLC requires shipowners to maintain \"insurance or other financial security\" sufficient to cover the maximum liability for one oil spill\n\nAs of September 2016, 136 states, representing 97.5 per cent of the world fleet, are contracting parties to the CLC Protocol of 1992, which amends the original CLC Convention. Bolivia, North Korea, Honduras, and Lebanon—which are generally flag of convenience states—have not ratified the treaty.\n\nThe United States of America is not a signatory to CLC, despite considerable involvement in its formulation. This is due to significant nation legislation such as the Oil Pollution Act, 1990, so signing the CLC was deemed unnecessary.\n"}
{"id": "2091702", "url": "https://en.wikipedia.org/wiki?curid=2091702", "title": "Iron sulfide", "text": "Iron sulfide\n\nIron sulfide or Iron sulphide can refer to a range of chemical compounds composed of iron and sulfur.\n\nBy increasing order of stability: \n\n"}
{"id": "3174627", "url": "https://en.wikipedia.org/wiki?curid=3174627", "title": "Isodesmic crystal", "text": "Isodesmic crystal\n\nAn isodesmic crystal is a crystal in which all the bonds have the same electrostatic valency. This means that all the bonds are of the same strength. Diamonds and halite have isodesmic crystals. The opposite of an isodesmic crystal is an anisodesmic crystal, in which anions more strongly bonded to central coordinating cation. Graphite is an example of an anisodesmic crystal.\n"}
{"id": "33776080", "url": "https://en.wikipedia.org/wiki?curid=33776080", "title": "Kigumba Petroleum Institute", "text": "Kigumba Petroleum Institute\n\nKigumba Petroleum Institute, also referred to as Uganda Petroleum Institute or as Uganda Petroleum Institute, Kigumba (UPIK), is a government-owned, national center for training, research and consultancy in the field of petroleum exploration, recovery, refinement and responsible utilization in Uganda.\n\nThe institute is located approximately , by road, north of the town of Kigumba, off of the Kigumba–Karuma Road, in Kiryandongo District, Western Uganda. This location lies approximately , by road, northeast of Masindi, the nearest large town in the sub-region. Uganda Petroleum Institute is located approximately , by road, northwest of Kampala, Uganda's capital and largest city. The coordinates of the Institute's campus are: 01°50'34.0\"N, 32°01'09.0\"E (Latitude:1.842778; Longitude:32.019167).\n\nThe institute was established in 2009 and admitted the first batch of students in 2010, with the objective of training personnel in petroleum-related skills, at certificate, diploma and undergraduate levels. In 2011, increased budgettary allocations were made towards the elevation of the institute from a vocational school to a fully-fledged International University. Financial assistance to the tune of US$8 million (UGX:20 billion), will be sought from the World Bank and Irish Aid, to achieve this goal. In November 2011, the Uganda Government began the process of elevating the Institute to University status.\n\nIn 2014, the institute introduced five new internationally recognized programs to graduate \"highly qualified and specialized\" technicians needed by oil companies across the world. The new plan proposes wide ranging overhaul of the curriculum and the introduction of five new diploma courses in oil studies. The institute also plans to work in close collaboration with the Ugandan oil industry to graduate over 220 students annually by the year 2019, up from 54 in 2014.\n\n"}
{"id": "3735181", "url": "https://en.wikipedia.org/wiki?curid=3735181", "title": "Kingsbridge Wind Power Project", "text": "Kingsbridge Wind Power Project\n\nKingsbridge Wind Power Project, also referred to as Kingsbridge 1 Wind facility , refers to a large wind farm owned and operated by Capital Power Corporation. Kingsbridge 1 Wind facility is located between Goderich and Kincardine, Ontario. The Kingsbridge 1 Wind facility is located on the southeast shore of Lake Huron in the township of Ashfield-Colborne-Wawanosh and consists of 22 1.8 MW Vestas V80 wind turbines with a capacity of 39.6 MW. Each tower is over 78 meters tall and weigh around 195 metric tonnes. The project consists of two phases and currently provides power to on average 12,500 homes.\n\nIn 2004, the first phase was awarded a generation contract by the Government of Ontario as part of its renewable energy RFP. Construction has since been completed and the project is fully operational. \n\nIn 2005, the second phase received a generation contract from the provincial government.\n\n\n"}
{"id": "30578928", "url": "https://en.wikipedia.org/wiki?curid=30578928", "title": "Lai Châu Dam", "text": "Lai Châu Dam\n\nThe Lai Châu Dam is a hydroelectric dam on the Black River inaugurated on 20 December 2016 in Nậm Nhùn District, Lai Chau Province, Vietnam. The owner of the power station is Vietnam Electricity.\n\nDam was designed by Hydroproject, a Russian hydrotechnical design firm, in collaboration with the Vietnamese company Power Construction No.1 JSC. The ground clearance works and relocation of people was completed by September 2010. Construction started on 5 January 2011; it was expected to be operational by 2017 with the first turbine put into operation in 2016, but the plant was inaugurated on 20 December 2016, one year ahead of the schedule, in a ceremony attended by the Deputy Prime Minister Trịnh Đình Dũng and other important personalities.\n\nThe power station has three Francis turbines with a capacity of 400 MW each. Turbines have been manufactured at the Alstom's plant in Tianjin, China. The total capacity of the station is 1,200 MW. It is the third largest hydroelectric power station in Vietnam and the third on the Da river. It is connected with the Hòa Bình Dam completed in 1994, and the Sơn La Dam, inaugurated in 2012. The Lai Chau hydropower projects costed VND 35,700 billion.\n"}
{"id": "53018706", "url": "https://en.wikipedia.org/wiki?curid=53018706", "title": "Langvann Hydroelectric Power Station", "text": "Langvann Hydroelectric Power Station\n\nThe Langvann Hydroelectric Power Station ( or \"Langvann kraftstasjon\") is a hydroelectric power station in the municipality of Gildeskål in Nordland county, Norway. It is sometimes referred to as the \"Langvatn kraftverk\", which should not be confused with the Langvatn Hydroelectric Power Station in Rana.\n\nThe plant utilizes a drop of between two lakes: \"Fellvatnet\" (), regulated between an elevation of and , and \"Langvatnet\" (), regulated between an elevation of and .\n\nThe plant has a 5 MW turbine and an average annual production of about 21 GWh. Its catchment area is . The plant is owned by Salten Kraftsamband and it came into operation in 1979. The plant is built into the mountainside at the end of a drain tunnel and the water it discharges is later used by the Sundsfjord Hydroelectric Power Station.\n"}
{"id": "6611405", "url": "https://en.wikipedia.org/wiki?curid=6611405", "title": "List of stars in Apus", "text": "List of stars in Apus\n\nThis is the list of notable stars in the constellation Apus, sorted by decreasing brightness.\n\n"}
{"id": "24573257", "url": "https://en.wikipedia.org/wiki?curid=24573257", "title": "Loimwe National Park", "text": "Loimwe National Park\n\nLoimwe National Park is a national park in the Shan Hills, Burma. It is located near Loi Mwe or Lwemwe, meaning \"misty mountain\", the former location of the headquarters of the British District Commissioner in colonial Burma.\n\n"}
{"id": "39546686", "url": "https://en.wikipedia.org/wiki?curid=39546686", "title": "Mass dimension one fermions", "text": "Mass dimension one fermions\n\nIn theoretical physics and cosmology the mass dimension one fermions of spin one half are a dark matter candidate. These fermions are fundamentally different from the hitherto known matter particles, like electrons or neutrinos. Despite being endowed with spin one half they are not described by the celebrated Dirac formalism but, instead, by a spinorial Klein-Gordon formalism.\n\nIn 2004 Dharam Vir Ahluwalia (IIT Guwahati) in collaboration with Daniel Grumiller presented an unexpected theoretical discovery of spin one-half fermions with mass dimension one. See, and. In the decade that followed a significant number of groups explored intriguing mathematical and physical properties\nof the new construct while Ahluwalia and his students developed the formalism further.\n\nHowever, the formalism suffered from two troubling features, that of non-locality and a subtle violation of Lorentz symmetry. The origin of both of these issues has now been traced to a hidden freedom in the definition of duals of spinors and the associated field adjoints. As a result there now exists an entirely new quantum theory of spin one-half fermions that is free from all the mentioned issues. The interactions of the new fermions are restricted to dimension-four quartic self interaction, and also to a dimension-four coupling with the Higgs. A generalised Yukawa coupling of the new fermions with neutrinos provides an hitherto unsuspected source of lepton-number violation. The new fermions thus present a first-principle dark matter partner to Dirac fermions of the standard model with contrasting mass dimensions — that of three halves for the latter versus one of the former without mutating the statistics from fermionic to bosonic.\n\nMass dimension one fermionic field of spin one half uses ELKO as its expansion coefficients. ELKO is an acronym of the original German term \"Eigenspinoren des Ladungskonjugationsoperators\", designating spinors that are eigenspinors of the charge conjugation operator.\n\nSince the new fermions have a mass dimensionality mismatch with standard model matter fields they were suggested as a dark matter candidate. As a result of their scalar-like mass dimension they differ significantly from the mass dimension 3/2 Dirac fermions.\n\nMass dimension one fermions have unexpected implications for cosmology by providing first principle dark matter and dark energy fields. Immediately after the publication of the Ahluwalia-Grumiller papers in 2005, Christian Boehmer pioneered application of Elko to cosmology and argued that Elko \"are not only prime dark matter candidates but also prime candidates for inflation.\" Einstein–Cartan–Elko system was first introduced in cosmology by Boehmer. Saulo Pereira and colleagues have shown that Elko can also induce a time varying cosmological constant.\nAbhishek Basak and colleagues have argued that the fast-roll inflation attractor point is unique for Elko and it is independent of the form of the potential. The subject is further pursued in references and. \nRoldao da Rocha has argued that Elko can also be used as a tool for probing exotic topological features of spacetime. Elko localization on the branes has been investigated in, and.\nThe following references serve as a guide to the lively activity on Elko, and mass dimension one fermions:\n\nEarlier history of Elko is summarized in references: and .\n\nHow Weinberg no go theorem is evaded is explained by Ahluwalia in 2017.. Also in 2017, it was shown that mass-dimension-one fermions, even in the absence of a cosmological constant, can induce a 'cosmological constant' term by quantum effects. These effects, leading to the non-vanishing Λ could be responsible for the inflationary phase at early universe stages. Furthermore, for the late time evolution, corresponding to a model with a time varying cosmological term, such quantum effects are in agreement with a previous recent work .\n"}
{"id": "16806606", "url": "https://en.wikipedia.org/wiki?curid=16806606", "title": "Minerotrophic", "text": "Minerotrophic\n\nMinerotrophic soils and vegetation receive their water supply mainly from streams or springs. This water has flowed over or through rocks or other minerals, often acquiring dissolved chemicals which raise the nutrient levels and reduce the acidity. If these chemicals include chemical bases such as calcium or magnesium ions, the water is referred to as \"base-rich\" and is neutral or alkaline.\n\nIn contrast to minerotrophic environments, ombrotrophic environments get their water mainly from precipitation, and so are very low in nutrients and more acidic.\n\n"}
{"id": "30387875", "url": "https://en.wikipedia.org/wiki?curid=30387875", "title": "Nationale Plattform Elektromobilität", "text": "Nationale Plattform Elektromobilität\n\nThe German National Platform for Electric Mobility (\"Nationale Plattform Elektromobilität\") is an advisory council of the German Federal Government for electric vehicle introduction. It consists of the top representatives of industry (10 Members), politics (6), science (3), associations (3) and unions (1). It was officially established on 3 May 2010 during a meeting with German chancellor Angela Merkel. Its task is to push on the National Development Plan for Electric Mobility (\"Nationaler Entwicklungsplan Elektromobilität\"). The goal for 2020 of the NPE is to develop Germany to the leading supplier and lead market for electric mobility and to gain an employment effect of 30,000 additional jobs.\n\nThe foundation for the promotion of electric mobility in Germany has been set with the Integrated Energy and Climate Programme of the Federal Government decided in 2007. Concrete measures were proposed in connection with the National Strategy Conference on Electromobility at the end of 2008. First fundings followed under the second economic stimulus package in early 2009. Before that, the industry had formed a so-called Innovation Alliance \"LIB 2015\". This industry consortium committed to invest 360 million euros for research and development of lithium-ion batteries.\n\nThe economic stimulus package brought up an amount of 500 million euros for projects in 15 topics. funded by the Federal Ministries of Economics and Technology (BMWi), of Transport, Building and Urban Affairs (BMVBS), for the Environment, Nature Conservation and Nuclear Safety (BMU), of Education and Research (BMBF) and of Food, Agriculture and Consumer protection (BMELV). The actual implementation was mostly coordinated by partners like the VDI/VDE-IT that already coordinated previous developments in electric mobility like research and development of batteries.\n\nIn September 2009 Annette Schavan, Federal minister of Education and Research, inaugurated the Forum Electric Mobility (\"Forum Elektromobilität\") as part of the joint project System Research for Electromobility of the Fraunhofer Society. The forum was assigned to pool the research and development efforts of the 33 Fraunhofer institutes together with the industrial partners. Until 2011 the schemes were funded with 30 million euros from the economic stimulus package. This support constitutes the first step of implementation of the National Development Plan for Electric Mobility.\n\nThe Federal Chancellery gave increasingly top priority to the National Development Plan for Electric Mobility. The Federal Ministry of Economics and Technology and the Federal Ministry of Transport, Building and Urban Affairs established the Joint Agency for Electric Mobility of the Federal Government on 1 February 2010 (\"Gemeinsame Geschäftsstelle Elektromobilität der Bundesregierung\"). With its kick-off it announced the foundation of the German National Platform for Electric Mobility, which was proclaimed on 3. May 2010.\n\nSubsequently the involved partners committed to establish cross-company and cross-sector working groups and to deliver a first interim report until the end of 2010. The coordination panels of the NPE could build on experts of the existing working groups of the German Commission for Electrical, Electronic & Information Technologies of DIN and VDE (DKE) of the Association for Electrical, Electronic and Information Technologies (VDE). The VDE Congress \"E-mobility\" on 8 and 9 December 2010 in Leipzig was used for technical consultation, from which the first interim report for the Federal Government has been derived. It describes the state of development and the expectations of the representatives of industry and research.\n\nThe second report was published in 2011. The report refined the details on development options and proposes support of different projects. In an early statement the German industry expressed the opinion that the current stimulation plan is insufficient to reach the defined goal of 1 million [electric vehicle|electric vehicles] in Germany by 2020. However, the report states that with the right incentive measures the goals of the NPE could still be reached.\n\nAt the International Conference of the German Government for Electric Mobility on 27 and 28 May 2013, the representatives of the Government confirmed the objectives and the planned financing and incentives. With its fourth report (Progress Report 2014 – Review of Pre-Market Phase) the NPE completed the pre-market phase (2010 – 2014) and showed the progress so far. At the same time the NPE presented a package of measures for the subsequent phase of the market-ramp-up (2015 - 2017), which showed how Germany could achieve the targets till 2020. The report stresses the need for additional incentives to reach the one million target.\n\nIn June 2015 the German Bundestag passed the first act on electric mobility (\"Elektromobilitätsgesetz\"). At the National Conference for Electric Mobility of the Federal Government in 2015, Chancellor Merkel announced a decision on further support measures for the end of the year. In spring of 2016 the decision on a support program was made, which directly addresses buyers of electric vehicles by financial incentives.\n\nThe platform consists of the steering committee, an editorial team and six topic-specific working groups. The steering committee coordinates the working groups. It is the decision-making body and reflects the structure of the NPE, which is made up of members of economy, science, politics and associations. Its members are the chairmen of the working groups, representatives of the federal government, the industrial circle electric mobility and the members of the editorial team. The editorial team supports the steering committee in the elaboration of reports and other publications of the NPE. The NPE is supported by the Federal Government’s Joint Agency for Electric Mobility (GGEMO). The Office of the NPE Chair moderates the platform and coordinates the NPE communication.\n\nThe NPE considers electric mobility as a total system of the components vehicle, energy supply, transport infrastructure and urban planning, which transcends the limits of traditional industry branches. This corresponds to the cross-sector and cross-disciplinary organization and operation of the NPE. The aim of the NPE is a self-sustaining market, which requires no political monetary support. To achieve the goals, the NPE recommends certain measures to the federal government and conducts regular reviews to their stage of development. The basis for their recommendations to support electric mobility is a three-phase model that reflects the different market stages.\n\nThe National Platform for Electric Mobility has the mandate to develop concrete proposals to achieve the objectives of the National Development Plan for Electric Mobility. Therefore the six working groups address the following main topics:\n\n\nThe experts in the working groups develop common positions and represent them in status analyzes, roadmaps and recommendation papers that summarize the need for action in electric mobility. The NPE hands over the publications to the federal government and opens it to the interested public. On 30 November 2010, a first interim report was published. The second report was published on 16 May 2011 and the third on 1 June 2012. The Progress Report 2014 – Review of Pre-Market Phase was passed to the Federal Government on 2 December 2014.\n\nThe members of the steering committee are (as at July 2016):\n\n\n\n"}
{"id": "21277", "url": "https://en.wikipedia.org/wiki?curid=21277", "title": "Neptunium", "text": "Neptunium\n\nNeptunium is a chemical element with symbol Np and atomic number 93. A radioactive actinide metal, neptunium is the first transuranic element. Its position in the periodic table just after uranium, named after the planet Uranus, led to it being named after Neptune, the next planet beyond Uranus. A neptunium atom has 93 protons and 93 electrons, of which seven are valence electrons. Neptunium metal is silvery and tarnishes when exposed to air. The element occurs in three allotropic forms and it normally exhibits five oxidation states, ranging from +3 to +7. It is radioactive, poisonous, pyrophoric, and can accumulate in bones, which makes the handling of neptunium dangerous.\n\nAlthough many false claims of its discovery were made over the years, the element was first synthesized by Edwin McMillan and Philip H. Abelson at the Berkeley Radiation Laboratory in 1940. Since then, most neptunium has been and still is produced by neutron irradiation of uranium in nuclear reactors. The vast majority is generated as a by-product in conventional nuclear power reactors. While neptunium itself has no commercial uses at present, it is used as a precursor for the formation of plutonium-238, used in radioisotope thermal generators to provide electricity for spacecraft. Neptunium has also been used in detectors of high-energy neutrons.\n\nThe most stable isotope of neptunium, neptunium-237, is a by-product of nuclear reactors and plutonium production. It, and the isotope neptunium-239, are also found in trace amounts in uranium ores due to neutron capture reactions and beta decay.\n\nNeptunium is a hard, silvery, ductile, radioactive actinide metal. In the periodic table, it is located to the right of the actinide uranium, to the left of the actinide plutonium and below the lanthanide promethium. Neptunium is a hard metal, having a bulk modulus of 118 GPa, comparable to that of manganese. Neptunium metal is similar to uranium in terms of physical workability. When exposed to air at normal temperatures, it forms a thin oxide layer. This reaction proceeds more rapidly as the temperature increases. Neptunium has been determined to melt at 639±3 °C: this low melting point, a property the metal shares with the neighboring element plutonium (which has melting point 639.4 °C), is due to the hybridization of the 5f and 6d orbitals and the formation of directional bonds in the metal. The boiling point of neptunium is not empirically known and the usually given value of 4174 °C is extrapolated from the vapor pressure of the element. If accurate, this would give neptunium the largest liquid range of any element (3535 K passes between its melting and boiling points).\n\nNeptunium is found in at least three allotropes. Some claims of a fourth allotrope have been made, but they are so far not proven. This multiplicity of allotropes is common among the actinides. The crystal structures of neptunium, protactinium, uranium, and plutonium do not have clear analogs among the lanthanides and are more similar to those of the 3d transition metals.\n\nα-neptunium takes on an orthorhombic structure, resembling a highly distorted body-centered cubic structure. Each neptunium atom is coordinated to four others and the Np–Np bond lengths are 260 pm. It is the densest of all the actinides and the fifth-densest of all naturally occurring elements, behind only rhenium, platinum, iridium, and osmium. α-neptunium has semimetallic properties, such as strong covalent bonding and a high electrical resistivity, and its metallic physical properties are closer to those of the metalloids than the true metals. Some allotropes of the other actinides also exhibit similar behaviour, though to a lesser degree. The densities of different isotopes of neptunium in the alpha phase are expected to be observably different: α-Np should have density 20.303 g/cm; α-Np, density 20.389 g/cm; α-Np, density 20.476 g/cm.\n\nβ-neptunium takes on a distorted tetragonal close-packed structure. Four atoms of neptunium make up a unit cell, and the Np–Np bond lengths are 276 pm. γ-neptunium has a body-centered cubic structure and has Np–Np bond length of 297 pm. The γ form becomes less stable with increased pressure, though the melting point of neptunium also increases with pressure. The β-Np/γ-Np/liquid triple point occurs at 725 °C and 3200 MPa.\n\nDue to the presence of valence 5f electrons, neptunium and its alloys exhibit very interesting magnetic behavior, like many other actinides. These can range from the itinerant band-like character characteristic of the transition metals to the local moment behavior typical of scandium, yttrium, and the lanthanides. This stems from 5f-orbital hybridization with the orbitals of the metal ligands, and the fact that the 5f orbital is relativistically destabilized and extends outwards. For example, pure neptunium is paramagnetic, NpAl is ferromagnetic, NpGe has no magnetic ordering, and NpSn behaves fermionically. Investigations are underway regarding alloys of neptunium with uranium, americium, plutonium, zirconium, and iron, so as to recycle long-lived waste isotopes such as neptunium-237 into shorter-lived isotopes more useful as nuclear fuel.\n\nOne neptunium-based superconductor alloy has been discovered with formula NpPdAl. This occurrence in neptunium compounds is somewhat surprising because they often exhibit strong magnetism, which usually destroys superconductivity. The alloy has a tetragonal structure with a superconductivity transition temperature of −268.3 °C (4.9 K).\n\nNeptunium has five ionic oxidation states ranging from +3 to +7 when forming chemical compounds, which can be simultaneously observed in solutions. It is the heaviest actinide that can lose all its valence electrons in a stable compound. The most stable state in solution is +5, but the valence +4 is preferred in solid neptunium compounds. Neptunium metal is very reactive. Ions of neptunium are prone to hydrolysis and formation of coordination compounds.\n\nA neptunium atom has 93 electrons, arranged in the configuration <nowiki>[</nowiki>Rn<nowiki>]</nowiki>5f6d7s. This differs from the configuration expected by the Aufbau principle in that one electron is in the 6d subshell instead of being as expected in the 5f subshell. This is because of the similarity of the electron energies of the 5f, 6d, and 7s subshells. In forming compounds and ions, all the valence electrons may be lost, leaving behind an inert core of inner electrons with the electron configuration of the noble gas radon; more commonly, only some of the valence electrons will be lost. The electron configuration for the tripositive ion Np is [Rn] 5f, with the outermost 7s and 6d electrons lost first: this is exactly analogous to neptunium's lanthanide homolog promethium, and conforms to the trend set by the other actinides with their [Rn] 5f electron configurations in the tripositive state. The first ionization potential of neptunium was measured to be at most in 1974, based on the assumption that the 7s electrons would ionize before 5f and 6d; more recent measurements have refined this to 6.2657 eV.\n\n20 neptunium radioisotopes have been characterized with the most stable being Np with a half-life of 2.14 million years, Np with a half-life of 154,000 years, and Np with a half-life of 396.1 days. All of the remaining radioactive isotopes have half-lives that are less than 4.5 days, and the majority of these have half-lives that are less than 50 minutes. This element also has at least four meta states, with the most stable being Np with a half-life of 22.5 hours.\n\nThe isotopes of neptunium range in atomic weight from 225.0339 u (Np) to 244.068 u (Np). Most of the isotopes that are lighter than the most stable one, Np, decay primarily by electron capture although a sizable number, most notably Np and Np, also exhibit various levels of decay via alpha emission to become protactinium. Np itself, being the beta-stable isobar of mass number 237, decays almost exclusively by alpha emission into Pa, with very rare (occurring only about once in trillions of decays) spontaneous fission and cluster decay (emission of Mg to form Tl). All of the known isotopes except one that are heavier than this decay exclusively via beta emission. The lone exception, Np, exhibits a rare (>0.12%) decay by isomeric transition in addition to the beta emission. Np eventually decays to form bismuth-209 and thallium-205, unlike most other common heavy nuclei which decay into isotopes of lead. This decay chain is known as the neptunium series. This decay chain had long been extinct on Earth due to the short half-lives of all of its isotopes above bismuth-209, but is now being resurrected thanks to artificial production of neptunium on the tonne scale.\n\nThe isotopes neptunium-235, -236, and -237 are predicted to be fissile; only neptunium-237's fissionability has been experimentally shown, with the critical mass being about 60 kg, only about 10 kg more than that of the commonly used uranium-235. Calculated values of the critical masses of neptunium-235, -236, and -237 respectively are 66.2 kg, 6.79 kg, and 63.6 kg: the neptunium-236 value is even lower than that of plutonium-239. In particular Np also has a low neutron cross section. Despite this, a neptunium atomic bomb has never been built: uranium and plutonium have lower critical masses than Np and Np, and Np is difficult to purify as it is not found in quantity in spent nuclear fuel and is nearly impossible to separate in any significant quantities from its parent Np.\n\nSince all isotopes of neptunium have half-lives that are many times shorter than the age of the Earth, any primordial neptunium should have decayed by now. After only about 80 million years, the concentration of even the longest lived isotope, Np, would have been reduced to less than one-trillionth (10) of its original amount; and even if the whole Earth had initially been made of pure Np (and ignoring that this would be well over its critical mass of 60 kg), 2100 half-lives would have passed since the formation of the Solar System, and thus all of it would have decayed. Thus neptunium is present in nature only in negligible amounts produced as intermediate decay products of other isotopes.\n\nTrace amounts of the neptunium isotopes neptunium-237 and -239 are found naturally as decay products from transmutation reactions in uranium ores. In particular, Np and Np are the most common of these isotopes; they are directly formed from neutron capture by uranium-238 atoms. These neutrons come from the spontaneous fission of uranium-238, naturally neutron-induced fission of uranium-235, cosmic ray spallation of nuclei, and light elements absorbing alpha particles and emitting a neutron. The half-life of Np is very short, although the detection of its much longer-lived daughter Pu in nature in 1951 definitively established its natural occurrence. In 1952, Np was identified and isolated from concentrates of uranium ore from the Belgian Congo: in these minerals, the ratio of neptunium-237 to uranium is less than or equal to about 10 to 1.\n\nMost neptunium (and plutonium) now encountered in the environment is due to atmospheric nuclear explosions that took place between the detonation of the first atomic bomb in 1945 and the ratification of the Partial Nuclear Test Ban Treaty in 1963. The total amount of neptunium released by these explosions and the few atmospheric tests that have been carried out since 1963 is estimated to be around 2500 kg. The overwhelming majority of this is composed of the long-lived isotopes Np and Np since even the moderately long-lived Np (half-life 396 days) would have decayed to less than one-billionth (10) its original concentration over the intervening decades. An additional very small amount of neptunium, created by neutron irradiation of natural uranium in nuclear reactor cooling water, is released when the water is discharged into rivers or lakes. The concentration of Np in seawater is approximately 6.5 × 10 millibecquerels per liter: this concentration is between 0.1% and 1% that of plutonium.\n\nOnce in the environment, neptunium generally oxidizes fairly quickly, usually to the +4 or +5 state. Regardless of its oxidation state, the element exhibits a much greater mobility than the other actinides, largely due to its ability to readily form aqueous solutions with various other elements. In one study comparing the diffusion rates of neptunium(V), plutonium(IV), and americium(III) in sandstone and limestone, neptunium penetrated more than ten times as well as the other elements. Np(V) will also react efficiently in pH levels greater than 5.5 if there are no carbonates present and in these conditions it has also been observed to readily bond with quartz. It has also been observed to bond well with goethite, ferric oxide colloids, and several clays including kaolinite and smectite. Np(V) does not bond as readily to soil particles in mildly acidic conditions as its fellow actinides americium and curium by nearly an order of magnitude. This behavior enables it to migrate rapidly through the soil while in solution without becoming fixed in place, contributing further to its mobility. Np(V) is also readily absorbed by concrete, which because of the element's radioactivity is a consideration that must be addressed when building nuclear waste storage facilities. When absorbed in concrete, it is reduced to Np(IV) in a relatively short period of time. Np(V) is also reduced by humic acid if it is present on the surface of goethite, hematite, and magnetite. Np(IV) is absorbed efficiently by tuff, granodiorite, and bentonite; although uptake by the latter is most pronounced in mildly acidic conditions. It also exhibits a strong tendency to bind to colloidal particulates, an effect that is enhanced when in soil with a high clay content. The behavior provides an additional aid in the element's observed high mobility.\n\nWhen the first periodic table of the elements was published by Dmitri Mendeleev in the early 1870s, it showed a \" — \" in place after uranium similar to several other places for then-undiscovered elements. Other subsequent tables of known elements, including a 1913 publication of the known radioactive isotopes by Kasimir Fajans, also show an empty place after uranium, element 92.\n\nUp to and after the discovery of the final component of the atomic nucleus, the neutron in 1932, most scientists did not seriously consider the possibility of elements heavier than uranium. While nuclear theory at the time did not explicitly prohibit their existence, there was little evidence to suggest that they did. However, the discovery of induced radioactivity by Irène and Frédéric Joliot-Curie in late 1933 opened up an entirely new method of researching the elements and inspired a small group of Italian scientists led by Enrico Fermi to begin a series of experiments involving neutron bombardment. Although the Joliot-Curies' experiment involved bombarding a sample of Al with alpha particles to produce the radioactive P, Fermi realized that using neutrons, which have no electrical charge, would most likely produce even better results than the positively charged alpha particles. Accordingly, in March 1934 he began systematically subjecting all of the then-known elements to neutron bombardment to determine whether others could also be induced to radioactivity.\n\nAfter several months of work, Fermi's group had tentatively determined that lighter elements would disperse the energy of the captured neutron by emitting a proton or alpha particle and heavier elements would generally accomplish the same by emitting a gamma ray. This latter behavior would later result in the beta decay of a neutron into a proton, thus moving the resulting isotope one place up the periodic table. When Fermi's team bombarded uranium, they observed this behavior as well, which strongly suggested that the resulting isotope had an atomic number of 93. Fermi was initially reluctant to publicize such a claim, but after his team observed several unknown half-lives in the uranium bombardment products that did not match those of any known isotope, he published a paper entitled \"Possible Production of Elements of Atomic Number Higher than 92\" in June 1934. In it he proposed the name ausonium (atomic symbol Ao) for element 93, after the Greek name \"Ausonia\" (Italy).\n\nSeveral theoretical objections to the claims of Fermi's paper were quickly raised; in particular, the exact process that took place when an atom captured a neutron was not well understood at the time. This and Fermi's accidental discovery three months later that nuclear reactions could be induced by slow neutrons cast further doubt in the minds of many scientists, notably Aristid von Grosse and Ida Noddack, that the experiment was creating element 93. While von Grosse's claim that Fermi was actually producing protactinium (element 91) was quickly tested and disproved, Noddack's proposal that the uranium had been shattered into two or more much smaller fragments was simply ignored by most because existing nuclear theory did not include a way for this to be possible. Fermi and his team maintained that they were in fact synthesizing a new element, but the issue remained unresolved for several years.\nAlthough the many different and unknown radioactive half-lives in the experiment's results showed that several nuclear reactions were occurring, Fermi's group could not prove that element 93 was being created unless they could isolate it chemically. They and many other scientists attempted to accomplish this, including Otto Hahn and Lise Meitner who were among the best radiochemists in the world at the time and supporters of Fermi's claim, but they all failed. Much later, it was determined that the main reason for this failure was because the predictions of element 93's chemical properties were based on a periodic table which lacked the actinide series. This arrangement placed protactinium below tantalum, uranium below tungsten, and further suggested that element 93, at that point referred to as eka-rhenium, should be similar to the group 7 elements, including manganese and rhenium. Thorium, protactinium, and uranium, with their dominant oxidation states of +4, +5, and +6 respectively, fooled scientists into thinking they belonged below hafnium, tantalum, and tungsten, rather than below the lanthanide series, which was at the time viewed as a fluke, and whose members all have dominant +3 states; neptunium, on the other hand, has a much rarer, more unstable +7 state, with +4 and +5 being the most stable. Upon finding that plutonium and the other transuranic elements also have dominant +3 and +4 states, along with the discovery of the f-block, the actinide series was firmly established.\n\nWhile the question of whether Fermi's experiment had produced element 93 was stalemated, two additional claims of the discovery of the element appeared, although unlike Fermi, they both claimed to have observed it in nature. The first of these claims was by Czech engineer Odolen Koblic in 1934 when he extracted a small amount of material from the wash water of heated pitchblende. He proposed the name bohemium for the element, but after being analyzed it turned out that the sample was a mixture of tungsten and vanadium. The other claim, in 1938 by Romanian physicist Horia Hulubei and French chemist Yvette Cauchois, claimed to have discovered the new element via spectroscopy in minerals. They named their element sequanium, but the claim was discounted because the prevailing theory at the time was that if it existed at all, element 93 would not exist naturally. However, as neptunium does in fact occur in nature in trace amounts, as demonstrated when it was found in uranium ore in 1952, it is possible that Hulubei and Cauchois did in fact observe neptunium.\n\nAlthough by 1938 some scientists, including Niels Bohr, were still reluctant to accept that Fermi had actually produced a new element, he was nevertheless awarded the Nobel Prize in Physics in November 1938 \"for his demonstrations of the existence of new radioactive elements produced by neutron irradiation, and for his related discovery of nuclear reactions brought about by slow neutrons\". A month later, the almost totally unexpected discovery of nuclear fission by Hahn, Meitner, and Otto Frisch put an end to the possibility that Fermi had discovered element 93 because most of the unknown half-lives that had been observed by Fermi's team were rapidly identified as fission products.\n\nPerhaps the closest of all attempts to produce the missing element 93 was that conducted by the Japanese physicist Yoshio Nishina working with chemist Kenjiro Kimura in 1940, just before the outbreak of the Pacific War in 1941: they bombarded U with fast neutrons. However, while slow neutrons tend to induce neutron capture through a (n, γ) reaction, fast neutrons tend to induce a \"knock-out\" (n, 2n) reaction, where one neutron is added and two more are removed, resulting in the net loss of a neutron. Nishina and Kimura, having tested this technique on Th and successfully produced the known Th and its long-lived beta decay daughter Pa (both occurring in the natural decay chain of U), therefore correctly assigned the new 6.75-day half-life activity they observed to the new isotope U. They confirmed that this isotope was also a beta emitter and must hence decay to the unknown nuclide 93. They attempted to isolate this nuclide by carrying it with its supposed lighter congener rhenium, but no beta or alpha decay was observed from the rhenium-containing fraction: Nishina and Kimura thus correctly speculated that the half-life of 93, like that of Pa, was very long and hence its activity would be so weak as to be unmeasurable by their equipment, thus concluding the last and closest unsuccessful search for transuranic elements.\n\nAs research on nuclear fission progressed in early 1939, Edwin McMillan at the Berkeley Radiation Laboratory of the University of California, Berkeley decided to run an experiment bombarding uranium using the powerful 60-inch (1.52 m) cyclotron that had recently been built at the university. The purpose was to separate the various fission products produced by the bombardment by exploiting the enormous force that the fragments gain from their mutual electrical repulsion after fissioning. Although he did not discover anything of note from this, McMillan did observe two new beta decay half-lives in the uranium trioxide target itself, which meant that whatever was producing the radioactivity had not violently repelled each other like normal fission products. He quickly realized that one of the half-lives closely matched the known 23-minute decay period of uranium-239, but the other half-life of 2.3 days was unknown. McMillan took the results of his experiment to chemist and fellow Berkeley professor Emilio Segrè to attempt to isolate the source of the radioactivity. Both scientists began their work using the prevailing theory that element 93 would have similar chemistry to rhenium, but Segrè rapidly determined that McMillan's sample was not at all similar to rhenium. Instead, when he reacted it with hydrogen fluoride (HF) with a strong oxidizing agent present, it behaved much like members of the rare earths. Since these elements comprise a large percentage of fission products, Segrè and McMillan decided that the half-life must have been simply another fission product, titling the paper \"An Unsuccessful Search for Transuranium Elements\".\nHowever, as more information about fission became available, the possibility that the fragments of nuclear fission could still have been present in the target became more remote. McMillan and several scientists, including Philip H. Abelson, attempted again to determine what was producing the unknown half-life. In early 1940, McMillan realized that his 1939 experiment with Segrè had failed to test the chemical reactions of the radioactive source with sufficient rigor. In a new experiment, McMillan tried subjecting the unknown substance to HF in the presence of a reducing agent, something he had not done before. This reaction resulted in the sample precipitating with the HF, an action that definitively ruled out the possibility that the unknown substance was a rare earth. \nShortly after this, Abelson, who had received his graduate degree from the university, visited Berkeley for a short vacation and McMillan asked the more able chemist to assist with the separation of the experiment's results. Abelson very quickly observed that whatever was producing the 2.3-day half-life did not have chemistry like any known element and was actually more similar to uranium than a rare earth. This discovery finally allowed the source to be isolated and later, in 1945, led to the classification of the actinide series. As a final step, McMillan and Abelson prepared a much larger sample of bombarded uranium that had a prominent 23-minute half-life from U and demonstrated conclusively that the unknown 2.3-day half-life increased in strength in concert with a decrease in the 23-minute activity through the following reaction:\n\nThis proved that the unknown radioactive source originated from the decay of uranium and, coupled with the previous observation that the source was different chemically from all known elements, proved beyond all doubt that a new element had been discovered. McMillan and Abelson published their results in a paper entitled \"Radioactive Element 93\" in the \"Physical Review\" on May 27, 1940. They did not propose a name for the element in the paper, but they soon decided on the name \"neptunium\" since Neptune is the next planet beyond Uranus in our solar system. McMillan and Abelson's success compared to Nishina and Kimura's near miss can be attributed to the favorable half-life of Np for radiochemical analysis and quick decay of U, in contrast to the slower decay of U and extremely long half-life of Np.\n\nIt was also realized that the beta decay of Np must produce an isotope of element 94 (now called plutonium), but the quantities involved in McMillan and Abelson's original experiment were too small to isolate and identify plutonium along with neptunium. The discovery of plutonium had to wait until the end of 1940, when Glenn T. Seaborg and his team identified the isotope plutonium-238.\n\nNeptunium's unique radioactive characteristics allowed it to be traced as it moved through various compounds in chemical reactions, at first this was the only method available to prove that its chemistry was different from other elements. As the first isotope of neptunium to be discovered has such a short half-life, McMillan and Abelson were unable to prepare a sample that was large enough to perform chemical analysis of the new element using the technology that was then available. However, after the discovery of the long-lived Np isotope in 1942 by Glenn Seaborg and Arthur Wahl, forming weighable amounts of neptunium became a realistic endeavor. Its half-life was initially determined to be about 3 million years (later revised to 2.144 million years), confirming the predictions of Nishina and Kimura of a very long half-life.\n\nEarly research into the element was somewhat limited because most of the nuclear physicists and chemists in the United States at the time were focused on the massive effort to research the properties of plutonium as part of the Manhattan Project. Research into the element did continue as a minor part of the project and the first bulk sample of neptunium was isolated in 1944.\n\nMuch of the research into the properties of neptunium since then has been focused on understanding how to confine it as a portion of nuclear waste. Because it has isotopes with very long half-lives, it is of particular concern in the context of designing confinement facilities that can last for thousands of years. It has found some limited uses as a radioactive tracer and a precursor for various nuclear reactions to produce useful plutonium isotopes. However, most of the neptunium that is produced as a reaction byproduct in nuclear power stations is considered to be a waste product.\n\nThe vast majority of the neptunium that currently exists on Earth was produced in artificial nuclear reactions. Neptunium-237 is the most commonly synthesized isotope due to it being the only one that both can be created via neutron capture and also has a half-lifelong enough to allow weighable quantities to be easily isolated. As such, it is by far the most common isotope to be utilized in chemical studies of the element.\n\n\nHeavier isotopes of neptunium decay quickly, and lighter isotopes of neptunium cannot be produced by neutron capture, so chemical separation of neptunium from cooled spent nuclear fuel gives nearly pure Np. The short-lived heavier isotopes Np and Np, useful as radioactive tracers, are produced through neutron irradiation of Np and U respectively, while the longer-lived lighter isotopes Np and Np are produced through irradiation of U with protons and deuterons in a cyclotron.\n\nArtificial Np metal is usually isolated through a reaction of NpF with liquid barium or lithium at around 1200 °C and is most often extracted from spent nuclear fuel rods in kilogram amounts as a by-product in plutonium production.\n\nBy weight, neptunium-237 discharges are about 5% as great as plutonium discharges and about 0.05% of spent nuclear fuel discharges. However, even this fraction still amounts to more than fifty tons per year globally.\n\nRecovering uranium and plutonium from spent nuclear fuel for reuse is one of the major processes of the nuclear fuel cycle. As it has a long half-life of just over 2 million years, the alpha emitter Np is one of the major isotopes of the minor actinides separated from spent nuclear fuel. Many separation methods have been used to separate out the neptunium, operating on small and large scales. The small-scale purification operations have the goals of preparing pure neptunium as a precursor of metallic neptunium and its compounds, and also to isolate and preconcentrate neptunium in samples for analysis.\n\nMost methods that separate neptunium ions exploit the differing chemical behaviour of the differing oxidation states of neptunium (from +3 to +6 or sometimes even +7) in solution. Among the methods that are or have been used are: solvent extraction (using various extractants, usually multidentate β-diketone derivatives, organophosphorus compounds, and amine compounds), chromatography using various ion-exchange or chelating resins, coprecipitation (possible matrices include LaF, BiPO, BaSO, Fe(OH), and MnO), electrodeposition, and biotechnological methods. Currently, commercial reprocessing plants use the Purex process, involving the solvent extraction of uranium and plutonium with tributyl phosphate.\n\nWhen it is in an aqueous solution, neptunium can exist in any of its five possible oxidation states (+3 to +7) and each of these show a characteristic color. The stability of each oxidation state is strongly dependent on various factors, such as the presence of oxidizing or reducing agents, pH of the solution, presence of coordination complex-forming ligands, and even the concentration of neptunium in the solution.\n\nIn acidic solutions, the neptunium(III) to neptunium(VII) ions exist as Np, Np, , , and . In basic solutions, they exist as the oxides and hydroxides Np(OH), NpO, NpOOH, NpO(OH), and . Not as much work has been done to characterize neptunium in basic solutions. Np and Np can easily be reduced and oxidized to each other, as can and .\n\nNp(III) or Np exists as hydrated complexes in acidic solutions, . It is a dark blue-purple and is analogous to its lighter congener, the pink rare-earth ion Pm. In the presence of oxygen, it is quickly oxidized to Np(IV) unless strong reducing agents are also present. Nevertheless, it is the second-least easily hydrolyzed neptunium ion in water, forming the NpOH ion. Np is the predominant neptunium ion in solutions of pH 4–5.\n\nNp(IV) or Np is pale yellow-green in acidic solutions, where it exists as hydrated complexes (). It is quite unstable to hydrolysis in acidic aqueous solutions at pH 1 and above, forming NpOH. In basic solutions, Np tends to hydrolyze to form the neutral neptunium(IV) hydroxide (Np(OH)) and neptunium(IV) oxide (NpO).\n\nNp(V) or is green-blue in aqueous solution, in which it behaves as a strong Lewis acid. It is a stable ion and is the most common form of neptunium in aqueous solutions. Unlike its neighboring homologues and , does not spontaneously disproportionate except at very low pH and high concentration:\n\nIt hydrolyzes in basic solutions to form NpOOH and .\n\nNp(VI) or , the neptunyl ion, shows a light pink or reddish color in an acidic solution and yellow-green otherwise. It is a strong Lewis acid and is the main neptunium ion encountered in solutions of pH 3–4. Though stable in acidic solutions, it is quite easily reduced to the Np(V) ion, and it is not as stable as the homologous hexavalent ions of its neighbours uranium and plutonium (the uranyl and plutonyl ions). It hydrolyzes in basic solutions to form the oxo and hydroxo ions NpOOH, , and .\n\nNp(VII) is dark green in a strongly basic solution. Though its chemical formula in basic solution is frequently cited as , this is a simplification and the real structure is probably closer to a hydroxo species like . Np(VII) was first prepared in basic solution in 1967. In strongly acidic solution, Np(VII) is found as ; water quickly reduces this to Np(VI). Its hydrolysis products are uncharacterized.\n\nThe oxides and hydroxides of neptunium are closely related to its ions. In general, Np hydroxides at various oxidation levels are less stable than the actinides before it on the periodic table such as thorium and uranium and more stable than those after it such as plutonium and americium. This phenomenon is because the stability of an ion increases as the ratio of atomic number to the radius of the ion increases. Thus actinides higher on the periodic table will more readily undergo hydrolysis.\n\nNeptunium(III) hydroxide is quite stable in acidic solutions and in environments that lack oxygen, but it will rapidly oxidize to the IV state in the presence of air. It is not soluble in water. Np(IV) hydroxides exist mainly as the electrically neutral Np(OH) and its mild solubility in water is not affected at all by the pH of the solution. This suggests that the other Np(IV) hydroxide, , does not have a significant presence.\n\nBecause the Np(V) ion is very stable, it can only form a hydroxide in high acidity levels. When placed in a 0.1 M sodium perchlorate solution, it does not react significantly for a period of months, although a higher molar concentration of 3.0 M will result in it reacting to the solid hydroxide NpOOH almost immediately. Np(VI) hydroxide is more reactive but it is still fairly stable in acidic solutions. It will form the compound NpO· HO in the presence of ozone under various carbon dioxide pressures. Np(VII) has not been well-studied and no neutral hydroxides have been reported. It probably exists mostly as .\n\nThree anhydrous neptunium oxides have been reported, NpO, NpO, and NpO, though some studies have stated that only the first two of these exist, suggesting that claims of NpO are actually the result of mistaken analysis of NpO. However, as the full extent of the reactions that occur between neptunium and oxygen has yet to be researched, it is not certain which of these claims is accurate. Although neptunium oxides have not been produced with neptunium in oxidation states as high as those possible with the adjacent actinide uranium, neptunium oxides are more stable at lower oxidation states. This behavior is illustrated by the fact that NpO can be produced by simply burning neptunium salts of oxyacids in air.\n\nThe greenish-brown NpO is very stable over a large range of pressures and temperatures and does not undergo phase transitions at low temperatures. It does show a phase transition from face-centered cubic to orthorhombic at around 33-37GPa, although it returns to is original phase when pressure is released. It remains stable under oxygen pressures up to 2.84 MPa and temperatures up to 400 °C. \nNpO is black-brown in color and monoclinic with a lattice size of 418×658×409 picometres. It is relatively unstable and decomposes to NpO and O at 420-695 °C. Although NpO was initially subject to several studies that claimed to produce it with mutually contradictory methods, it was eventually prepared successfully by heating neptunium peroxide to 300-350 °C for 2–3 hours or by heating it under a layer of water in an ampoule at 180 °C.\n\nNeptunium also forms a large number of oxide compounds with a wide variety of elements, although the neptunate oxides formed with alkali metals and alkaline earth metals have been by far the most studied. Ternary neptunium oxides are generally formed by reacting NpO with the oxide of another element or by precipitating from an alkaline solution. LiNpO has been prepared by reacting LiO and NpO at 400 °C for 16 hours or by reacting LiO with NpO · HO at 400 °C for 16 hours in a quartz tube and flowing oxygen. Alkali neptunate compounds KNpO, CsNpO, and RbNpO are all created by a similar reaction: \n\nThe oxide compounds KNpO, CsNpO, and RbNpO are formed by reacting Np(VII) () with a compound of the alkali metal nitrate and ozone. Additional compounds have been produced by reacting NpO and water with solid alkali and alkaline peroxides at temperatures of 400 - 600 °C for 15–30 hours. Some of these include Ba(NpO), BaNaNpO, and BaLiNpO. Also, a considerable number of hexavelant neptunium oxides are formed by reacting solid-state NpO with various alkali or alkaline earth oxides in an environment of flowing oxygen. Many of the resulting compounds also have an equivalent compound that substitutes uranium for neptunium. Some compounds that have been characterized include NaNpO, NaNpO, NaNpO, and NaNpO. These can be obtained by heating different combinations of NpO and NaO to various temperature thresholds and further heating will also cause these compounds to exhibit different neptunium allotropes. The lithium neptunate oxides LiNpO and LiNpO can be obtained with similar reactions of NpO and LiO.\n\nA large number of additional alkali and alkaline neptunium oxide compounds such as CsNpO and CsNpO have been characterized with various production methods. Neptunium has also been observed to form ternary oxides with many additional elements in groups 3 through 7, although these compounds are much less well studied.\n\nAlthough neptunium halide compounds have not been nearly as well studied as its oxides, a fairly large number have been successfully characterized. Of these, neptunium fluorides have been the most extensively researched, largely because of their potential use in separating the element from nuclear waste products. Four binary neptunium fluoride compounds, NpF, NpF, NpF, and NpF, have been reported. The first two are fairly stable and were first prepared in 1947 through the following reactions:\nLater, NpF was obtained directly by heating NpO to various temperatures in mixtures of either hydrogen fluoride or pure fluorine gas. NpF is much more difficult to create and most known preparation methods involve reacting NpF or NpF compounds with various other fluoride compounds. NpF will decompose into NpF and NpF when heated to around 320 °C.\n\nNpF or neptunium hexafluoride is extremely volatile, as are its adjacent actinide compounds uranium hexafluoride (UF) and plutonium hexafluoride (PuF). This volatility has attracted a large amount of interest to the compound in an attempt to devise a simple method for extracting neptunium from spent nuclear power station fuel rods. NpF was first prepared in 1943 by reacting NpF and gaseous fluorine at very high temperatures and the first bulk quantities were obtained in 1958 by heating NpF and dripping pure fluorine on it in a specially prepared apparatus. Additional methods that have successfully produced neptunium hexafluoride include reacting BrF and BrF with NpF and by reacting several different neptunium oxide and fluoride compounds with anhydrous hydrogen fluorides.\n\nFour neptunium oxyfluoride compounds, NpOF, NpOF, NpOF, and NpOF, have been reported, although none of them have been extensively studied. NpOF is a pinkish solid and can be prepared by reacting NpO · HO and NpF with pure fluorine at around 330 °C. NpOF and NpOF can be produced by reacting neptunium oxides with anhydrous hydrogen fluoride at various temperatures. Neptunium also forms a wide variety of fluoride compounds with various elements. Some of these that have been characterized include CsNpF, RbNpF, NaNpF, and KNpOF.\n\nTwo neptunium chlorides, NpCl and NpCl, have been characterized. Although several attempts to create NpCl have been made, they have not been successful. NpCl is created by reducing neptunium dioxide with hydrogen and carbon tetrachloride (CCl) and NpCl by reacting a neptunium oxide with CCl at around 500 °C. Other neptunium chloride compounds have also been reported, including NpOCl, CsNpCl, CsNpOCl, and CsNaNpCl. Neptunium bromides NpBr and NpBr have also been created; the latter by reacting aluminium bromide with NpO at 350 °C and the former in an almost identical procedure but with zinc present. The neptunium iodide NpI has also been prepared by the same method as NpBr.\n\nNeptunium chalcogen and pnictogen compounds have been well studied primarily as part of research into their electronic and magnetic properties and their interactions in the natural environment. Pnictide and carbide compounds have also attracted interest because of their presence in the fuel of several advanced nuclear reactor designs, although the latter group has not had nearly as much research as the former.\n\nA wide variety of neptunium sulfide compounds have been characterized, including the pure sulfide compounds NpS, NpS, NpS, NpS, NpS, and NpS. Of these, NpS, prepared by reacting NpO with hydrogen sulfide and carbon disulfide at around 1000 °C, is the most well-studied and three allotropic forms are known. The α form exists up to around 1230 °C, the β up to 1530 °C, and the γ form, which can also exist as NpS, at higher temperatures. NpS can be created by reacting NpS and neptunium metal at 1600 °C and NpS can be prepared by the decomposition of NpS at 500 °C or by reacting sulfur and neptunium hydride at 650 °C. NpS is made by heating a mixture of NpS and pure sulfur to 500 °C. All of the neptunium sulfides except for the β and γ forms of NpS are isostructural with the equivalent uranium sulfide and several, including NpS, α−NpS, and β−NpS are also isostructural with the equivalent plutonium sulfide. The oxysulfides NpOS, NpOS, and NpOS have also been created, although the latter three have not been well studied. NpOS was first prepared in 1985 by vacuum sealing NpO, NpS, and pure sulfur in a quartz tube and heating it to 900 °C for one week.\n\nNeptunium selenide compounds that have been reported include NpSe, NpSe, NpSe, NpSe, NpSe, and NpSe. All of these have only been obtained by heating neptunium hydride and selenium metal to various temperatures in a vacuum for an extended period of time and NpSe is only known to exist in the γ allotrope at relatively high temperatures. Two neptunium oxyselenide compounds are known, NpOSe and NpOSe, are formed with similar methods by replacing the neptunium hydride with neptunium dioxide. The known neptunium telluride compounds NpTe, NpTe, NpTe, NpTe, and NpOTe are formed by similar procedures to the selenides and NpOTe is isostructural to the equivalent uranium and plutonium compounds. No neptunium−polonium compounds have been reported.\n\nNeptunium nitride (NpN) was first prepared in 1953 by reacting neptunium hydride and ammonia gas at around 750 °C in a quartz capillary tube. Later, it was produced by reacting different mixtures of nitrogen and hydrogen with neptunium metal at various temperatures. It has also been created by the reduction of neptunium dioxide with diatomic nitrogen gas at 1550 °C. NpN is isomorphous with uranium mononitride (UN) and plutonium mononitride (PuN) and has a melting point of 2830 °C under a nitrogen pressure of around 1 MPa. Two neptunium phosphide compounds have been reported, NpP and NpP. The first has a face centered cubic structure and is prepared by converting neptunium metal to a powder and then reacting it with phosphine gas at 350 °C. NpP can be created by reacting neptunium metal with red phosphorus at 740 °C in a vacuum and then allowing any extra phosphorus to sublimate away. The compound is non-reactive with water but will react with nitric acid to produce Np(IV) solution.\n\nThree neptunium arsenide compounds have been prepared, NpAs, NpAs, and NpAs. The first two were first created by heating arsenic and neptunium hydride in a vacuum-sealed tube for about a week. Later, NpAs was also made by confining neptunium metal and arsenic in a vacuum tube, separating them with a quartz membrane, and heating them to just below neptunium's melting point of 639 °C, which is slightly higher than the arsenic's sublimation point of 615 °C. NpAs is prepared by a similar procedure using iodine as a transporting agent. NpAs crystals are brownish gold and NpAs is black. The neptunium antimonide compound NpSb was created in 1971 by placing equal quantities of both elements in a vacuum tube, heating them to the melting point of antimony, and then heating it further to 1000 °C for sixteen days. This procedure also created trace amounts of an additional antimonide compound NpSb. One neptunium-bismuth compound, NpBi, has also been reported.\n\nThe neptunium carbides NpC, NpC, and NpC (tentative) have been reported, but have not characterized in detail despite the high importance and utility of actinide carbides as advanced nuclear reactor fuel. NpC is a non-stoichiometric compound, and could be better labelled as NpC (0.82 ≤ \"x\" ≤ 0.96). It may be obtained from the reaction of neptunium hydride with graphite at 1400 °C or by heating the constituent elements together in an electric arc furnace using a tungsten electrode. It reacts with excess carbon to form pure NpC. NpC is formed from heating NpO in a graphite crucible at 2660–2800 °C.\n\nNeptunium reacts with hydrogen in a similar manner to its neighbor plutonium, forming the hydrides NpH (face-centered cubic) and NpH (hexagonal). These are isostructural with the corresponding plutonium hydrides, although unlike PuH, the lattice parameters of NpH become greater as the hydrogen content (\"x\") increases. The hydrides require extreme care in handling as they decompose in a vacuum at 300 °C to form finely divided neptunium metal, which is pyrophoric.\n\nBeing chemically stable, neptunium phosphates have been investigated for potential use in immobilizing nuclear waste. Neptunium pyrophosphate (α-NpPO), a green solid, has been produced in the reaction between neptunium dioxide and boron phosphate at 1100 °C, though neptunium(IV) phosphate has so far remained elusive. The series of compounds NpM(PO), where M is an alkali metal (Li, Na, K, Rb, or Cs), are all known. Some neptunium sulfates have been characterized, both aqueous and solid and at various oxidation states of neptunium (IV through VI have been observed). Additionally, neptunium carbonates have been investigated to achieve a better understanding of the behavior of neptunium in geological repositories and the environment, where it may come into contact with carbonate and bicarbonate aqueous solutions and form soluble complexes.\n\nA few organoneptunium compounds are known and chemically characterized, although not as many as for uranium due to neptunium's scarcity and radioactivity. The most well known organoneptunium compounds are the cyclopentadienyl and cyclooctatetraenyl compounds and their derivatives. The trivalent cyclopentadienyl compound Np(CH)·THF was obtained in 1972 from reacting Np(CH)Cl with sodium, although the simpler Np(CH) could not be obtained. Tetravalent neptunium cyclopentadienyl, a reddish-brown complex, was synthesized in 1968 by reacting neptunium(IV) chloride with potassium cyclopentadienide:\n\nIt is soluble in benzene and THF, and is less sensitive to oxygen and water than Pu(CH) and Am(CH). Other Np(IV) cyclopentadienyl compounds are known for many ligands: they have the general formula (CH)NpL, where L represents a ligand.\nNeptunocene, Np(CH), was synthesized in 1970 by reacting neptunium(IV) chloride with K(CH). It is isomorphous to uranocene and plutonocene, and they behave chemically identically: all three compounds are insensitive to water or dilute bases but are sensitive to air, reacting quickly to form oxides, and are only slightly soluble in benzene and toluene. Other known neptunium cyclooctatetraenyl derivatives include Np(RCH) (R = ethanol, butanol) and KNp(CH)·2THF, which is isostructural to the corresponding plutonium compound. In addition, neptunium hydrocarbyls have been prepared, and solvated triiodide complexes of neptunium are a precursor to many organoneptunium and inorganic neptunium compounds.\n\nThere is much interest in the coordination chemistry of neptunium, because its five oxidation states all exhibit their own distinctive chemical behavior, and the coordination chemistry of the actinides is heavily influenced by the actinide contraction (the greater-than-expected decrease in ionic radii across the actinide series, analogous to the lanthanide contraction).\n\nFew neptunium(III) coordination compounds are known, because Np(III) is readily oxidized by atmospheric oxygen while in aqueous solution. However, sodium formaldehyde sulfoxylate can reduce Np(IV) to Np(III), stabilizing the lower oxidation state and forming various sparingly soluble Np(III) coordination complexes, such as ·11HO, ·HO, and .\n\nMany neptunium(IV) coordination compounds have been reported, the first one being , which is isostructural with the analogous uranium(IV) coordination compound. Other Np(IV) coordination compounds are known, some involving other metals such as cobalt (·8HO, formed at 400 K) and copper (·6HO, formed at 600 K). Complex nitrate compounds are also known: the experimenters who produced them in 1986 and 1987 produced single crystals by slow evaporation of the Np(IV) solution at ambient temperature in concentrated nitric acid and excess 2,2′-pyrimidine.\n\nThe coordination chemistry of neptunium(V) has been extensively researched due to the presence of cation–cation interactions in the solid state, which had been already known for actinyl ions. Some known such compounds include the neptunyl dimer ·8HO and neptunium glycolate, both of which form green crystals.\n\nNeptunium(VI) compounds range from the simple oxalate (which is unstable, usually becoming Np(IV)) to such complicated compounds as the green . Extensive study has been performed on compounds of the form , where M represents a monovalent cation and An is either uranium, neptunium, or plutonium.\n\nSince 1967, when neptunium(VII) was discovered, some coordination compounds with neptunium in the +7 oxidation state have been prepared and studied. The first reported such compound was initially characterized as ·\"n\"HO in 1968, but was suggested in 1973 to actually have the formula ·2HO based on the fact that Np(VII) occurs as in aqueous solution. This compound forms dark green prismatic crystals with maximum edge length 0.15–0.4 mm.\n\nMost neptunium coordination complexes known in solution involve the element in the +4, +5, and +6 oxidation states: only a few studies have been done on neptunium(III) and (VII) coordination complexes. For the former, NpX and (X = Cl, Br) were obtained in 1966 in concentrated LiCl and LiBr solutions, respectively: for the latter, 1970 experiments discovered that the ion could form sulfate complexes in acidic solutions, such as and ; these were found to have higher stability constants than the neptunyl ion (). A great many complexes for the other neptunium oxidation states are known: the inorganic ligands involved are the halides, iodate, azide, nitride, nitrate, thiocyanate, sulfate, carbonate, chromate, and phosphate. Many organic ligands are known to be able to be used in neptunium coordination complexes: they include acetate, propionate, glycolate, lactate, oxalate, malonate, phthalate, mellitate, and citrate.\n\nAnalogously to its neighbours, uranium and plutonium, the order of the neptunium ions in terms of complex formation ability is Np > ≥ Np > . (The relative order of the middle two neptunium ions depends on the ligands and solvents used.) The stability sequence for Np(IV), Np(V), and Np(VI) complexes with monovalent inorganic ligands is F > > SCN > > Cl > ; the order for divalent inorganic ligands is > > . These follow the strengths of the corresponding acids. The divalent ligands are more strongly complexing than the monovalent ones. can also form the complex ions [] (M = Al, Ga, Sc, In, Fe, Cr, Rh) in perchloric acid solution: the strength of interaction between the two cations follows the order Fe > In > Sc > Ga > Al. The neptunyl and uranyl ions can also form a complex together.\n\nAn important of use of Np is as a precursor in plutonium production, where it is irradiated with neutrons to create Pu, an alpha emitter for radioisotope thermal generators for spacecraft and military applications. Np will capture a neutron to form Np and beta decay with a half-life of just over two days to Pu.\n\nPu also exists in sizable quantities in spent nuclear fuel but would have to be separated from other isotopes of plutonium. Irradiating neptunium-237 with electron beams, provoking bremsstrahlung, also produces quite pure samples of the isotope plutonium-236, useful as a tracer to determine plutonium concentration in the environment.\n\nNeptunium is fissionable, and could theoretically be used as fuel in a fast neutron reactor or a nuclear weapon, with a critical mass of around 60 kilograms. In 1992, the U.S. Department of Energy declassified the statement that neptunium-237 \"can be used for a nuclear explosive device\". It is not believed that an actual weapon has ever been constructed using neptunium. As of 2009, the world production of neptunium-237 by commercial power reactors was over 1000 critical masses a year, but to extract the isotope from irradiated fuel elements would be a major industrial undertaking.\n\nIn September 2002, researchers at the Los Alamos National Laboratory briefly created the first known nuclear critical mass using neptunium in combination with shells of enriched uranium (uranium-235), discovering that the critical mass of a bare sphere of neptunium-237 \"ranges from kilogram weights in the high fifties to low sixties,\" showing that it \"is about as good a bomb material as [uranium-235].\" The United States Federal government made plans in March 2004 to move America's supply of separated neptunium to a nuclear-waste disposal site in Nevada.\n\nNp is used in devices for detecting high-energy (MeV) neutrons.\nNeptunium accumulates in commercial household ionization-chamber smoke detectors from decay of the (typically) 0.2 microgram of americium-241 initially present as a source of ionizing radiation. With a half-life of 432 years, the americium-241 in an ionization smoke detector includes about 3% neptunium after 20 years, and about 15% after 100 years.\n\nNeptunium-237 is the most mobile actinide in the deep geological repository environment. This makes it and its predecessors such as americium-241 candidates of interest for destruction by nuclear transmutation. Due to its long half-life, neptunium will become the major contributor of the total radiotoxicity in 10,000 years. As it is unclear what happens to the containment in that long time span, an extraction of the neptunium would minimize the contamination of the environment if the nuclear waste could be mobilized after several thousand years.\nNeptunium does not have a biological role, as it has a short half-life and occurs only in small traces naturally. Animal tests showed that it is not absorbed via the digestive tract. When injected it concentrates in the bones, from which it is slowly released.\n\nFinely divided neptunium metal presents a fire hazard because neptunium is pyrophoric; small grains will ignite spontaneously in air at room temperature.\n\n\n\n"}
{"id": "35864967", "url": "https://en.wikipedia.org/wiki?curid=35864967", "title": "Nimaben Acharya", "text": "Nimaben Acharya\n\nNimaben Bhaveshbhai Acharya is a Member of Legislative assembly from Anjar constituency in Gujarat for its 12th legislative assembly. She previously served on the Gujarat family planning council.\n\nIn April 2017, Acharya was traveling to a funeral when her vehicle was attacked by unidentified males who hurled stones at it, breaking the glass of the car, but Acharya and her driver were unharmed.\n"}
{"id": "41783230", "url": "https://en.wikipedia.org/wiki?curid=41783230", "title": "Nine Maneaters And One Rogue", "text": "Nine Maneaters And One Rogue\n\nNine Maneaters And One Rogue is the first book of jungle tales and man-eaters written by Kenneth Anderson, first published in 1954 by George Allen & Unwin Ltd.\n\n\"To the memory of the jungle of the Southern India, their birds and animals, particularly elephant, tiger and panther, and their forest-people, Chensoos, Sholagas, Karumbas and Poojarees, I proudly and gratefully dedicate this book, in return for the twenty-five years of unadulterated joy they have given me in making and keeping their acquaintance\".\n\nIntroduction\nAnderson discusses the causes of man-eating in tigers and panthers, as well as possible causes of an elephant becoming rogue.\n\nThe Maneater of Jowlagiri\nA tigress turns man-eater after being wounded by a poacher in the Jowlagiri area. After the death toll reaches around fifteen, Anderson is contacted to track down the beast. Anderson has an agonizing wait sat over a human kill, followed by a lucky escape when the man-eater starts to stalk him. The man-eater is not heard of again for another five months, and when a fresh human kill is reported Anderson heads back to Jowlagiri - where he manages to lure the tigress to her death by imitating a tiger's mating call.\n\nThe Spotted Devil of Gummlapur\nOver an area of some , a man-eating leopard is responsible for some 42 human deaths. When Anderson arrives to dispatch the animal, villagers are reluctant to assist as they believed the 'shaitan' would hear of it and hasten their death. Anderson spends his nights waiting up for the panther, first using himself as bait and then changing tact and using a dummy. On one such night he comes across a stray dog, which he keeps with him for company in his hut. The dog ends up saving his life by signalling when the panther finally arrives at Anderson's hut - in repayment Anderson takes the dog home and names him 'Nipper'.\n\nThe Striped Terror of Chamala Valley\nIn 1937 a tiger is responsible for the death of seven people in the space of six months. Anderson locates the tiger by tying out baits, and though not fully equipped he decides to sit up over a kill and await the tigers return. Successfully killing the tiger, the locals help to carry it back to the village, though Anderson begins to doubt if in fact this is the correct tiger. Days later a report reaches him of a further tiger attack in the region, Anderson arrives on the scene and tracks the trail of the tiger up a dry stream bed... this time he has found the right tiger.\n\nThe Hosdurga-Holalkere Man-eater\nAnderson recounts the cat and mouse game that ensued in the hunt for the Hosdurga-Holalkere man-eating tiger along with his friend Mac. Whilst sitting over human remains atop a large cluster of boulders, Anderson sits with his back to a sheer drop of 12 foot and as such believes himself safe from an attack from the rear - but unexpectedly that is where the man-eater decides to attack.\n\nThe Rogue Elephant of Panapatti\nAn elephant with one whole tusk and one half tusk turns rogue in the Panapatti area and is responsible for multiple human deaths. Anderson first encounters the elephant when out trying to bag a peacock - armed only with a shotgun. Anderson escapes unscathed, though the elephant goes on to attack a camp of people - causing the government to double its reward for the animal. Anderson spends four laborious days tracking the elephant through the jungle, finally coming across a lone elephant - but is unsure if it is the right animal until it turns to face him...\n\nThe Maneater of Segur\nAfter first recounting a story of a pack of wild dogs taking on a tigress in the Nilgiri Hills, Anderson goes on to detail his hunt for the man-eating tiger of the same region. After a run in with a sloth bear family, and two failed vigils over two different human kills, Anderson comes across the one-eyed tiger by chance when a sambar deer sounds its warning.\n\nThe Maneater of Yemmaydoddi\nEarly in 1946 a small male tiger appeared in the Yemmaydoddi locality and started lifting local cattle. In 1948 after breaking down in Tiptur, Anderson and his friend Alfie finally arrive in Birur to find they are given the choice of shooting either a cattle lifting panther or a cattle lifting tiger. Unfortunately they opt for the panther, as after they leave the region the cattle lifting tiger is wounded badly in the lower jaw by a local villager protecting his cattle and turns man-eater. Anderson returns to the region and months go by of monotonous nights of waiting, until finally Anderson gets a shot at the tiger. Only managing to wound the animal, he returns the next day to track the tiger with the help of a herd of buffalo.\n\nThe Killer of Jalahalli\nDuring a rabbit beat in the Jalahalli region, a leopard (previously wounded by a policeman) caught between the nets and the beaters with dogs, succeeds in mauling six people in order to escape. Locals persist in trying to bring the leopard to bag, but the leopard is a fighter and survives their assaults - in the progress managing to maul a total of 11 people and kill 3. Anderson arrives in the area and the leopard again manages to escape the attempt on his life, obtaining more wounds in the progress. The next day Anderson follows circling vultures to the body of the leopard which had painfully succumbed to its many wounds.\n\nThe Hermit of Devarayandurga\nA local tigress is nicknamed 'the hermit' due to its shabby appearance and choice of abode. Not a recorded man-eater, it reportedly was very aggressive towards humans, and killed two men and one woman. After spending some very cold all night vigils sat over baits, Anderson gets a shot at the tigress. Badly wounding the animal, he has a tense morning following its distinctive blood trail until he puts an end to 'the hermit'.\n\nByra the Poojaree\nAnderson recounts how a chance meeting with a poacher, turns into a close friendship of over 25 years and who appears in many of his hunting stories. Anderson shares some such tales about Byra, including a vicious bear attack and Byra's part in hunting a man-eater in the Muthur area.\n\nThe Tigers of Tagarthy \nAnderson found the village of Tagarthy to have no less than 4 tigers operating in the area, and on one single day - eight cattle kills were reported to have been made by tigers. Anderson recounts his own close encounters with the tigers of the region, including the story of how local man Sham Rao Bapat comes to shoot one of these tigers in his garden, and Anderson's own hunt for the hostile, cattle lifting tiger of Goowja.\n\n"}
{"id": "18005010", "url": "https://en.wikipedia.org/wiki?curid=18005010", "title": "Niobium dioxide", "text": "Niobium dioxide\n\nNiobium dioxide, is the chemical compound with the formula NbO. It is a bluish black non-stoichiometric solid with a composition range of NbO-NbO It can be prepared by reacting NbO with H at 800–1350 °C. An alternative method is reaction of NbO with Nb powder at 1100 °C.\n\nThe room temperature form NbO has a tetragonal, rutile-like structure with short Nb-Nb distances indicating Nb-Nb bonding. High temp form also has a rutile-like structure with short Nb-Nb distances.\nTwo high pressure phases have been reported one with a rutile-like structure, again with short Nb-Nb distances, and a higher pressure with baddeleyite-related structure.\n\nNbO is insoluble in water and is a powerful reducing agent, reducing carbon dioxide to carbon and sulfur dioxide to sulfur. In an industrial process for the production of niobium metal , NbO is produced as an intermediate, by the hydrogen reduction of NbO. The NbO is subsequently reacted with magnesium vapour to produce niobium metal.\n"}
{"id": "9227446", "url": "https://en.wikipedia.org/wiki?curid=9227446", "title": "Ombrotrophic", "text": "Ombrotrophic\n\nOmbrotrophic (\"cloud-fed\") soils or vegetation receive all of their water and nutrients from precipitation, rather than from streams or springs. Such environments are hydrologically isolated from the surrounding landscape, and since rain is acidic and very low in nutrients, they are home to organisms tolerant of acidic, low-nutrient environments. The vegetation of ombrotrophic peatlands is often bog, dominated by \"Sphagnum\" mosses. The hydrology of these environments are directly related to their climate, as precipitation is the water and nutrient source, and temperatures dictate how quickly water evaporates from these systems.\n\nOmbrotrophic circumstances may occur even in landscapes composed of limestone or other nutrient-rich substrates – for example, in high-rainfall areas, limestone boulders may be capped by acidic ombrotrophic bog vegetation. Epiphytic vegetation (plants growing on other plants) is ombrotrophic.\n\nIn contrast to ombrotrophic environments, minerotrophic environments are those where the water supply comes mainly from streams or springs. This water has flowed over or through rocks often acquiring dissolved chemicals which raise the nutrient levels and reduce the acidity, which leads to different vegetation such as fen or poor fen.\n\n\n"}
{"id": "11197155", "url": "https://en.wikipedia.org/wiki?curid=11197155", "title": "Partial linear space", "text": "Partial linear space\n\nA partial linear space (also semilinear or near-linear space) is a basic incidence structure in the field of incidence geometry, that carries slightly less structure than a linear space.\nThe notion is equivalent to that of a linear hypergraph.\nLet formula_1 an incidence structure, for which the elements of formula_2 are called \"points\" and the elements of formula_3 are called \"lines\". \"S\" is a partial linear space, if the following axioms hold:\n\nIf there is a unique line incident with every pair of distinct points, then we get a linear space. \n\nThe De Bruijn–Erdős theorem (incidence geometry) shows that in any finite linear space formula_4 which is not a single point or a single line, we have formula_5. \n\n\n\n"}
{"id": "36502688", "url": "https://en.wikipedia.org/wiki?curid=36502688", "title": "Portuguese Natural Gas Association", "text": "Portuguese Natural Gas Association\n\nThe Portuguese Natural Gas Association, founded in December 2010, is a non-profit association, based in Lisbon, Portugal. Its bylaws define AGN as a scientific, technical and professional association, establishing itself as the representative body for the sector.\n\nThe Governing Board is the association's highest representative body, and fulfills decisions made in the General Assembly. The Governing Board is formed by seven to nine members elected in the General Assembly: one President, two Vice-Presidents, and, variably, four to six advisors. The Governing Board will be assembled as many times as considered necessary and has at least one meeting per month.\n\nAGN has two Standing Committees formed by specialists of associated companies; one is dedicated to natural gas infra-structure issues, and the other to retail issues. Other working committees may be established for specific issues. The committees are a permanent part of AGN’s Governing Body.\n\n"}
{"id": "9990961", "url": "https://en.wikipedia.org/wiki?curid=9990961", "title": "Strong link/weak link", "text": "Strong link/weak link\n\nA strong link/weak link and exclusion zone nuclear detonation mechanism is a type of safety mechanism employed in the arming and firing mechanisms of modern nuclear weapons. It is a form of automatic safety interlock. \n\nThe safety mechanism starts by enclosing the electronics and mechanical components used to arm and fire the nuclear weapon with a mechanical and electrical isolation barrier, forming the \"exclusion zone\". This is insulated from mechanical, thermal, and electrical disruptions (such as static electricity, lightning, fire).\n\nBetween the exclusion zone and the actual detonators, a normally disconnected link mechanism is used, such as a switch which has a built in motor to activate it. The arming system has to activate the switch in order to connect the firing circuits to the detonators in the weapon. This disconnection, which requires the arming mechanism to operate, is called the \"strong link\".\n\nIt is possible for an accident (rocket explosion, airplane crash, accident while weapon is being moved) to disrupt the weapon and break the integrity of the exclusion zone. As a safety mechanism, a \"weak link\" is also built into the system. This is a set of components designed to fail at lower stresses (thermal, mechanical, and electrical) than will cause failure of the strong link mechanisms. The weak link acts to break the connection to the detonators before the strong link could be disrupted and fail by the stress of an accident.\n\nThese mechanisms do not prevent misuse of the weapon, which is restricted by Permissive Action Link code systems, or an accident from physically causing initiation of the explosives or detonators directly from extremely high temperatures, impact forces, or electrical disturbance such as lightning. The risk of accidental direct detonation is significantly reduced by using insensitive high explosives such as TATB, which is extremely unlikely to detonate due to fire or impact or electricity. While TATB may decompose or burn in a fire, it is extremely unlikely to detonate as a result of that action.\n\n"}
{"id": "8407270", "url": "https://en.wikipedia.org/wiki?curid=8407270", "title": "Total dynamic head", "text": "Total dynamic head\n\nIn fluid dynamics, Total Dynamic Head (TDH) is the total equivalent height that a fluid is to be pumped, taking into account friction losses in the pipe.\n\nformula_1\n\nTDH = Static Height + Static Lift + Friction Loss\n\nwhere:\n\"Static Height\" is the maximum height reached by the pipe after the pump (also known as the 'discharge head').\n\n\"Static Lift\" is the height the water will rise before arriving at the pump (also known as the \"suction head\").\n\"Friction Loss\" (or Head Loss).\nThis equation can be derived from Bernoulli's Equation.\n\nFor a relatively incompressible fluid such as water, TDH is simply the pressure head difference between the inlet and outlet of the pump, if measured at the same elevation and with inlet and outlet of equal diameter.\n\nTDH is also the work done by the pump per unit weight, per unit volume of fluid.\n\n\n"}
{"id": "3341108", "url": "https://en.wikipedia.org/wiki?curid=3341108", "title": "Transposition (telecommunications)", "text": "Transposition (telecommunications)\n\nTransposition is the periodic swapping of positions of the conductors of a transmission line, in order to reduce crosstalk and otherwise improve transmission. In telecommunications this applies to balanced pairs whilst in power transmission lines three conductors are periodically transposed.\n\nFor cables, the swapping is gradual and continuous; that is the two or three conductors are twisted around each other. For communication cables this is called twisted pair. For overhead power lines or open pair communication lines, the conductors are exchanged at pylons, for example at transposition towers or at utility poles, respectively.\n\nThe mutual influence of electrical conductors is reduced by transposition. Transposition also equalizes their impedance relative to ground, thus avoiding one-sided loads in three-phase electric power systems. Transposing is an effective measure for the reduction of inductively linked normal mode interferences.\n\nFor longer powerlines without branches, wires are transposed according to the transposing scheme. At closely branched grids and where several electric circuits share a route (in particular when the lines operate at different voltages) on the same pylons the outside unbalance of the line, which is caused by the other electric circuits, dominates. In these cases one finds large deviations from the transposing schemes. For example, in some such transpositions, only two of the three conductors on the pylons change their place. Also transpositions on pylons near power substations are used to get an optimal arrangement of the feeding system without crossing of conductors.\n\nAs the mutual influence of electric circuits can change after new lines are installed or old lines dismantled, certain transpositions may disappear or be added after new construction in electricity mains. In the case of a twisted line the individual conductors of an electric circuit swap places, either in their whole course (at cables) or at certain points (at overhead lines). The mutual influence of electrical conductors is reduced by transposing. The unbalance of the line, which can lead to one-sided loads in three-phase systems, is also reduced. Transposing of overhead lines is usually realized at so-called transposing pylons. Transposing is an effective measure for the reduction of inductively linked normal mode interferences.\nA transposing scheme is a pattern by which the conductors of overhead power lines are transposed at transposing structures. In order to ensure balanced capacitance of a three-phase line, each of the three conductors must hang once at each position of the overhead line.\n\nAt a transposition tower, the conductors change their relative places in the line. A transposing structure may be a standard structure with special cross arms, or may be a dead-end structure. The transposing is necessary as there is capacitance between conductors, as well as between conductors and ground. This is typically not symmetrical across phases. By transposing, the overall capacitance for the whole line is approximately balanced. Transposings also reduce effects to communiciations circuits.\n\nIn communications cables, transposition is used to reduce coupling between circuits in the same cable. The principal measure is the pitch or lay length, the distance over which the pairs of a circuit are twisted. By twisting, the wires become longer than the cable. The stranding factor indicates the relationship of single wire length to cable length; it amounts to with communication cables about 1.02 to 1.04.\n\nIn practice the following kinds of stranding occur more frequently: \n\nThe kinds of stranding have different transmission characteristics. The capacity of a stranding affects itself, for example the two conductors of a quadruple run parallel over the entire cable length in star quad twisting. Capacitance between the conductors is thus substantially higher than with Dieselhorst Martin (DHM) - stranding in which the situation of the conductors to each other in the cable changes repeatedly. Because of the smaller work capacity of the DHM stranding it is possible to form additional electric circuits with the help of a phantom circuit. Since the phantom transducers are turned on to in the middle of the master transducers, the currents of the phantom circuit on the two coming Rome circles compensate themselves.\n\n"}
{"id": "34076322", "url": "https://en.wikipedia.org/wiki?curid=34076322", "title": "Wood splitting", "text": "Wood splitting\n\nWood splitting (\"riving\", cleaving) is an ancient technique used in carpentry to make lumber for making wooden objects, some basket weaving, and to make firewood. Unlike wood sawing, the wood is split along the grain using tools such as a hammer and wedges, splitting maul, cleaving axe, side knife, or froe.\n\nIn woodworking carpenters use a wooden siding which gets its name, clapboard, from originally being split from logs—the sound of the plank against the log being a clap. This is used in clapboard architecture and for wainscoting. Coopers use oak clapboards to make barrel staves. Split-rail fences are made with split wood.\n\nSome Native Americans traditionally make baskets from Black ash by pounding the wood with a mallet and pulling long strips from the log.\n\nLog splitting is the act of splitting firewood from logs that have been pre-cut into sections (rounds, bolts, billets). This can be done by hand, using an axe or maul, or by using a mechanical log splitter. When splitting a log by hand, it is best to aim for the cracks (called checks), if there are any visible. Some types of wood are harder to split than others, including extremely hard woods, as well as types like gum which an axe will often bounce off of, and cherry, which is typically so twisted it's near impossible to get a clean split, and elm. Any type of wood, being thick or tall, having large knots or twisted grain can make it difficult to split. In some cases, it is easiest to aim for the edges and split the log into multiple pieces. Batoning is splitting small pieces of wood for kindling or other purposes sometimes with a \"batoning chisel\", a special chisel with one sharp side used for splitting.\n\nThe advantages of splitting wood along its grain, rather than sawing it is that the wood is much stronger. Due to this, it was historically used for building ships (e.g. drekars) and traditional skis. A defining feature of shakes, which are like shingles, are that they are split rather than sawn and because the cell structure of the wood remains intact may be more durable, and similarly trunnels when split are stronger than when sawn.\n\nSometimes wood splitting is undesirable. Methods to prevent splitting in woodworking are the butterfly joint, truss connector plates, or metal straps. Columns may be hollowed in the center to prevent splitting. Nail points may be blunted or pilot holes drilled to prevent splitting of lumber while nailing or screwing. \"End grain sealers\" are liquid products usually containing wax which helps prevent rapid drying of the ends of lumber resulting in splits. Metal end plates or S-shaped pieces of metal may be driven into the butt ends of a timber. Splitting is the primary reason building codes do not allow notching in the bottom of joists and beams.\n\n"}
