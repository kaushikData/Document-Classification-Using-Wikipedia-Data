{"id": "56700109", "url": "https://en.wikipedia.org/wiki?curid=56700109", "title": "An Inconvenient Sequel (book)", "text": "An Inconvenient Sequel (book)\n\nAn Inconvenient Sequel: Truth to Power: Your Action Handbook to Learn the Science, Find Your Voice, and Help Solve the Climate Crisis is a book by former Vice President and environmental activist Al Gore, written in conjunction with his 2017 documentary film, \".\" The book is a sequel to his 2006 book \"An Inconvenient Truth\", published concurrently with his documentary of the same name. It was published on July 25, 2017 by Rodale Books.\n\nThe book is intended to encourage and inform readers on how they can help fight anthropogenic global warming and climate change, and is a more in-depth analysis than the film. The book describes how humans have further damaged the environment since the release of \"An Inconvenient Truth\", and makes more predictions about what will happen in the near future if humans fail to act. The book also describes advancements that have been made so far in the effort against global warming, such as developments in alternative energy sources. The book explores other aspects of the climate crisis, such as climate change denial and the corporate influence of money in politics, and ends by reasoning that it is not too late to solve the crisis.\n"}
{"id": "25529629", "url": "https://en.wikipedia.org/wiki?curid=25529629", "title": "Automatic waste container", "text": "Automatic waste container\n\nAn automatic waste container is a waste container which is automatic. This helps prevent the bin lids becoming clogged with trash.\n\nIt has an infrared system that allows it to detect human presence and open or close the waste container.\n\nThe waste container also comes with its own ability to neutralize odors.\n\nThese wastes containers are mostly made of stainless steel.\n\n"}
{"id": "31362947", "url": "https://en.wikipedia.org/wiki?curid=31362947", "title": "Avoided burden", "text": "Avoided burden\n\nAvoided burden is an approach used in life-cycle assessment (LCA), especially in the context of allocating environmental burden in the presence of recycling or reuse. When determining the overall environmental impact of a product, the product is given \"credit\" for the potential recycled material included. For instance, PET bottles may be given environmental credit for the PET they contain, as this material will eventually be recycled back into further PET products. This credit is termed \"avoided burden\" because it refers to the impact of virgin material production that is avoided by the use of recycled material.\n\nThe extent to which recycled material displaces production of virgin material of a similar or different type is the subject of current research. In case studies, practitioners typically utilize heuristic, arbitrary rules, such as assigning 100%, 50%, or 0% displacement of virgin material by recycled material. However, accurate information on the displacement rates of various materials is largely unknown. This information is important to LCA practitioners, as it can often tip the balance between two compared alternative products or materials.\n\nAlthough the calculation of avoided burden is not without some degree of subjectivity, it is usually calculated as follows: \n"}
{"id": "1970281", "url": "https://en.wikipedia.org/wiki?curid=1970281", "title": "Bevel", "text": "Bevel\n\nA bevelled edge (UK) or beveled edge (US) refers to an edge of a structure that is not perpendicular to the faces of the piece. The words bevel and chamfer overlap in usage; in general usage they are often interchanged, while in technical usage they may sometimes be differentiated as shown in the image at right. A bevel is typically used to soften the edge of a piece for the sake of safety, wear resistance, or aesthetics; or to facilitate mating with another piece.\n\nMost cutting tools have a bevelled edge which is apparent when one examines the grind.\n\nBevel angles can be duplicated using a sliding T bevel.\n\nTypographic bevels are shading and artificial shadows that emulate the appearance of a 3-dimensional letter. The bevel is a relatively common effect in graphic editors such as Photoshop. As such, it is in widespread use in mainstream logos and other design elements.\n\nBevelled edges are a common aesthetic nicety added to window panes and mirrors.\n\nGeologists refer to any slope of land into a stratum of different elevation as a bevel.\n\nIn waterskiing, a bevel is the transition area between the side of the ski and the bottom of the ski. Beginners tend to prefer sharp bevels, which allow the ski to glide on the water surface.\n\nIn Disc Golf, the 'beveled edge' was patented in 1983 by Dave Dunipace who founded Innova Champion Discs. This element transformed the Frisbee into the farther flying golf discs the sport uses today.\n\nIn modeling, a bevel is a classic modeling pose (e.g., used by the Rockettes). Bevels are part of choreography and transitions. Beveling is done off the stage quite often—it’s a more formal pose used in photo shoots and during on-camera interviews. Bevels can be open or closed depending on leg position.\n\nWith a deck of cards, you can slide the top portion back so that the back of the deck is at an angle. This can be used in card tricks.\n\nIn the semiconductor industry, wafers have two typical edge types: a slanted beveled shape or a rounded bullet shape. The edges on the beveled types are called the bevel region, and they are typically ground at a 22 degree angle.\n\nBeveling and chamfering (along with other profiles) are applied to thicker pieces of metal prior to welding, see Welding_joint#V-joints. The bevel provides a smooth clean edge to the plate or pipe and allows a weld of the correct shape (to prevent center-line cracking) to join the separate pieces of metal. \n\nSimple bevels can be used with a backup strip (thin removable sheet behind the plate joint) with chamfers (and a small land) being used on open root welds. Particularly thick plate will have a \"J\" shaped chamfer or \"U\" shaped groove to reduce the amount of welding filler metal used.\n\nCruciform joint preparation can involve a double bevel to permit full penetration of each weld to the other, removing a possible void space in the center.\n\n\n"}
{"id": "42232", "url": "https://en.wikipedia.org/wiki?curid=42232", "title": "Calliope (music)", "text": "Calliope (music)\n\nA calliope (see below for pronunciation) is a musical instrument that produces sound by sending a gas, originally steam or more recently compressed air, through large whistles—originally locomotive whistles.\n\nA calliope is typically very loud. Even some small calliopes are audible for miles. There is no way to vary tone or loudness. Musically, the only expression possible is the pitch, rhythm, and duration of the notes.\n\nThe steam calliope is also known as a steam organ or steam piano. The air-driven calliope is sometimes called a calliaphone, the name given to it by Norman Baker, but the \"Calliaphone\" name is registered by the Miner Company for instruments produced under the Tangley name.\n\nIn the age of steam, the steam calliope was particularly used on riverboats and in circuses. In both cases, a steam supply was already available for other purposes. Riverboats supplied steam from their propulsion boilers. Circus calliopes were sometimes installed in steam-drive carousels, or supplied with steam from a traction engine. The traction engine could also supply electric power for lighting, and tow the calliope in the circus parade, where it traditionally came last. Other circus calliopes were self-contained, mounted on a carved, painted and gilded wagon pulled by horses, but the presence of other steam boilers in the circus meant that fuel and expertise to run the boiler were readily available. Steam instruments often had keyboards made from brass. This was in part to resist the heat and moisture of the steam, but also for the golden shine of the highly polished keys.\n\nCalliopes can be played by a player at a keyboard or mechanically. Mechanical operation may be by a drum similar to a music box drum, or by a roll similar to that of a player piano. Some instruments have both a keyboard and a mechanism for automated operation, others only one or the other. Some calliopes can also be played via a MIDI interface.\n\nThe whistles of a calliope are tuned to a chromatic scale, although this process is difficult and must be repeated often to maintain quality sound. Since the pitch of each note is largely affected by the temperature of the steam, accurate tuning is nearly impossible; however, the off-pitch notes (particularly in the upper register) have become something of a trademark of the steam calliope. A calliope may have anywhere from 25 to 67 whistles, but 32 is traditional for a steam calliope.\n\nJoshua C. Stoddard of Worcester, Massachusetts patented the calliope on October 9, 1855, though his design echos previous concepts, such as an 1832 instrument called a \"steam trumpet,\" later known as a train whistle. In 1851, William Hoyt of Dupont, Indiana claimed to have conceived of a device similar to Stoddard's calliope, but he never patented it. Later, an employee of Stoddard's American Music, Arthur S. Denny, attempted to market an \"Improved Kalliope\" in Europe, but it did not catch on. In 1859, he demonstrated this instrument in Crystal Palace, London. Unlike other calliopes before or since, Denny's Improved Kalliope let the player control the steam pressure, and therefore the volume of the music, while playing.\n\nWhile Stoddard originally intended the calliope to replace bells at churches, it found its way onto riverboats during the paddlewheel era. While only a small number of working steamboats still exist, each has a steam calliope. These boats include the \"Delta Queen\", the \"Belle of Louisville\", and \"President\". Their calliopes are played regularly on river excursions. Many surviving calliopes were built by Thomas J. Nichol, Cincinnati, Ohio, who built calliopes from 1890 until 1932. The Thomas J. Nichol calliopes featured rolled sheet copper (as used in roofing) for the resonant tube (the bell) of the whistle, lending a sweeter tone than cast bronze or brass, which were the usual materials for steam whistles of the day. David Morecraft pioneered a resurgence in the building of authentic steam calliopes of the Thomas J. Nichol style beginning in 1985 in Peru, Indiana. These calliopes are featured in Peru's annual Circus City Parade. Morecraft passed away on December 5, 2016.\n\nStoddard's original calliope was attached to a metal roller set with pins in the manner familiar to Stoddard from the contemporary clockwork music box. The pins on the roller opened valves that admitted steam into the whistles. Later, Stoddard replaced the cylinder with a keyboard, so that the calliope could be played like an organ.\n\nStarting in the 1900s, calliopes began using music rolls instead of a live musician. The music roll operated in a similar manner to a piano roll in a player piano, mechanically operating the keys. Many of these mechanical calliopes retained keyboards, allowing a live musician to play them if needed. During this period, compressed air began to replace steam as the vehicle of producing sound.\n\nMost calliopes disappeared in the mid-20th century, as steam power was replaced with other power sources. Without the demand for technicians that mines and railroads supplied, no support was available to keep boilers running. Only a few calliopes have survived, and these are rarely played.\n\nThe pronunciation of the word has long been disputed, and often it is pronounced differently inside and outside the groups that use it. The Greek muse by the same name is pronounced , but the instrument was usually pronounced by people who played it. A nineteenth century magazine, \"Reedy's Mirror\", attempted to settle the dispute by publishing this rhyme:\nThis, in turn, came from a poem by Vachel Lindsay, called \"The Kallyope Yell\",\nin which Lindsay uses both pronunciations.\n\nIn the song \"Blinded by the Light\", written in 1972, Bruce Springsteen used the four-syllable () pronunciation when referring to a fairground organ, and this was repeated by Manfred Mann's Earth Band in their 1976 cover.\n\nThe calliope is similar to the pyrophone. The difference between the two is that the calliope is an external combustion instrument and the pyrophone is an internal combustion instrument.\n\nAt 1998's Burning Man, a pyrophone referred to as \"Satan's Calliope\" was powered by ignition of propane inside resonant cavities. This device was incorrectly referred to as a \"calliope\", since a calliope is an external combustion instrument.\n\nThe Calliaphone is an invention of Norman Baker. He developed\nan air-blown (versus steam) instrument that could be easily transported.\n\nThe \"lustre chantant\" (literally \"singing chandelier\") or musical lamp, invented by Frederik Kastner, was a large chandelier with glass pipes of varying lengths each illuminated and heated by an individual gas jet. A keyboard allowed the player to turn down individual jets; as the glass tube cooled, a note was produced. Kastner installed several such instruments in Paris.\n\nThe Beatles, in recording \"Being for the Benefit of Mr. Kite!\" from the album \"Sgt. Pepper's Lonely Hearts Club Band,\" used tapes of calliope music to create the atmosphere of a circus. Beatles producer George Martin recalled, \"When we first worked on 'Being for the Benefit of Mr. Kite!' John had said that he wanted to 'smell the sawdust on the floor', wanted to taste the atmosphere of the circus. I said to him, 'What we need is a calliope.' 'A what?' 'Steam whistles, played by a keyboard. Unable to find an authentic calliope, Martin resorted to tapes of calliopes playing Sousa marches. \"[I] chopped the tapes up into small sections and had Geoff Emerick throw them up into the air, re-assembling them at random.\"\n\nThe song \"The Tears of a Clown\" from Smokey Robinson & the Miracles, first released in 1967 and whose music was composed by Stevie Wonder and Hank Cosby, features a distinctive circus calliope motif, which inspired Smokey Robinson with the lyrical theme of the sad clown.\n\nThe art rockers of the United States of America used the instrument on several tracks of their eponymous 1968 album (recorded 1967).\n\nAmerican rock band Kiss used a calliope on their song 'Flaming Youth' from their 1976 album Destroyer.\n\nTom Waits' 2002 release \"Blood Money\" features a track written for trumpet and calliope.\n\nVernian Process' 2011 single \"Something Wicked (That Way Went)\" features a sampled calliope throughout.\n\nIn the \"Thomas & Friends\" episode \"Percy and the Calliope\", Percy the Small Engine saves a calliope from the scrapyard.\n\nIn \"In the Court of the Crimson King\" by King Crimson, the main theme of the title song is played on a calliope towards the end of the piece.\n\nDuring Madonna's The Girlie Show tour, during the encore for \"Holiday\", the credits include the statement, \"contains excerpts from \"Holiday for Calliope\",\" which was, in general, the hook for \"Holiday\" played with a calliope. Additionally, a generalized circus theme was played with a calliope sound through part of the song.\n\nIn the web series \"Bravest Warriors\", there is a theoretical system (similar to string theory) humorously dubbed the \"Space Time Calliope\" in which an infinite number of universes and timelines exist. The name is possibly referencing the mechanical complexity of calliopes.\n\nThe Bruce Springsteen song more popularly covered by Manfred Mann's Earth Band \"Blinded by the Light\" contains the line \"the calliope crashed to the ground\".\n\nThe Barclay James Harvest song \"Medicine Man\" uses the lyric \"And didn't anybody want to ask the calliope to call the tune\". This song was a great concert favourite and concerned a sinister travelling fair and carousel.\n\nAt one point in the \"SpongeBob SquarePants\" episode \"Free Samples\", SpongeBob uses a calliope to lure potential customers to his free samples stand. Instead, the calliope does the opposite.\n\nIn \"The Red Green Show\" episode \"Out of the Woods\", Red makes a calliope using a v8 engine and an assortment of old exhaust pipes.\n\nIn \"Girl Genius\", the main character Agatha Heterodyne is given a calliope to repair by a travelling circus that had been wrecked, and thought to be beyond repair, in volumes 4 and 5. In the climax of volume 6, it is revealed she had modified it to control an army of small robots.\n\nThe Italian alternative metal band Ravenscry released a song called \"Calliope\" on their 2009 self-titled album.\n\nIn Larry Niven's \"The Ringworld Engineers\": \"The puppeteer wasn't in sight, but presently Louis heard the sound of a steam calliope dying in agony.\"\n\n\n"}
{"id": "2363171", "url": "https://en.wikipedia.org/wiki?curid=2363171", "title": "Centrifugal pump", "text": "Centrifugal pump\n\nCentrifugal pumps are a sub-class of dynamic axisymmetric work-absorbing turbomachinery. Centrifugal pumps are used to transport fluids by the conversion of rotational kinetic energy to the hydrodynamic energy of the fluid flow. The rotational energy typically comes from an engine or electric motor. The fluid enters the pump impeller along or near to the rotating axis and is accelerated by the impeller, flowing radially outward into a diffuser or volute chamber (casing), from where it exits.\n\nCommon uses include water, sewage, agriculture, petroleum and petrochemical pumping. Centrifugal pumps are often chosen for their high flow rate capabilities, abrasive solution compatibility, mixing potential, as well as their relatively simple engineering. A centrifugal fan is commonly used to implement a vacuum cleaner. The reverse function of the centrifugal pump is a water turbine converting potential energy of water pressure into mechanical rotational energy.\n\nAccording to Reti, the first machine that could be characterized as a centrifugal pump was a mud lifting machine which appeared as early as 1475 in a treatise by the Italian Renaissance engineer Francesco di Giorgio Martini. True centrifugal pumps were not developed until the late 17th century, when Denis Papin built one using straight vanes. The curved vane was introduced by British inventor John Appold in 1851.\n\nLike most pumps, a centrifugal pump converts rotational energy, often from a motor, to energy in a moving fluid. A portion of the energy goes into kinetic energy of the fluid. Fluid enters axially through eye of the casing, is caught up in the impeller blades, and is whirled tangentially and radially outward until it leaves through all circumferential parts of the impeller into the diffuser part of the casing. The fluid gains both velocity and pressure while passing through the impeller. The doughnut-shaped diffuser, or scroll, section of the casing decelerates the flow and further increases the pressure. It is important to note that the water is not pushed radially outward by centrifugal force (non-existent force), but rather by inertia, the natural tendency of an object to continue in a straight line (tangent to the radius) when traveling around circle. This can be compared to the way a spin-cycle works in a washing machine.\n\nA consequence of Newton’s second law of mechanics is the conservation of the angular momentum (or the “moment of momentum”) which is of fundamental significance to all turbomachines. Accordingly, the change of the angular momentum is equal to the sum of the external moments. Angular momentums ρ×Q×r×cu at inlet and outlet, an external torque M and friction moments due to shear stresses Mτ are acting on an impeller or a diffuser.\n\nSince no pressure forces are created on cylindrical surfaces in the circumferential direction, it is possible to write Eq. (1.10) as:\n\nBased on Eq.(1.13) Euler developed the head pressure equation created by the impeller see Fig.2.2\n\nIn Eq. (2) the sum of 4 front element number call static pressure,the sum of last 2 element number call velocity pressure look carefully on the Fig 2.2 and the detail equation.\n\nH theory head pressure ; g = between 9.78 and 9.82 m/s2 depending on latitude, conventional standard value of exactly 9.80665 m/s2 barycentric gravitational acceleration\n\nu=r.ω the peripheral circumferential velocity vector\nu=r.ω the inlet circumferential velocity vector\n\nω=2π.n angular velocity\n\nw inlet relative velocity vector\n\nw outlet relative velocity vector\n\nc inlet absolute velocity vector\n\nc outlet absolute velocity vector\n\nThe color triangle formed by velocity vector u,c,w called \"velocity triangle\". This rule was helpful to detail Eq.(1) become Eq.(2) and wide explained how the pump works.\n\nFig 2.3 (a) shows triangle velocity of forward curved vanes impeller ; Fig 2.3 (b) shows triangle velocity of radial straight vanes impeller. It illustrates rather clearly energy added to the flow (shown in vector c) inversely change upon flow rate Q (shown in vector c).\n\nformula_4,\n\nwhere:\nThe head added by the pump (formula_8) is a sum of the static lift, the head loss due to friction and any losses due to valves or pipe bends all expressed in metres of fluid. Power is more commonly expressed as kilowatts (10 W, kW) or horsepower (hp*0.746 = kW). The value for the pump efficiency, formula_12, may be stated for the pump itself or as a combined efficiency of the pump and motor system.\n\nVertical centrifugal pumps are also referred to as cantilever pumps. They utilize a unique shaft and bearing support configuration that allows the volute to hang in the sump while the bearings are outside the sump. This style of pump uses no stuffing box to seal the shaft but instead utilizes a \"throttle bushing\". A common application for this style of pump is in a parts washer.\n\nIn the mineral industry, or in the extraction of oilsand, froth is generated to separate the rich minerals or bitumen from the sand and clays. Froth contains air that tends to block conventional pumps and cause loss of prime. Over history, industry has developed different ways to deal with this problem. In the pulp and paper industry holes are drilled in the impeller. Air escapes to the back of the impeller and a special expeller discharges the air back to the suction tank. The impeller may also feature special small vanes between the primary vanes called split vanes or secondary vanes. Some pumps may feature a large eye, an inducer or recirculation of pressurized froth from the pump discharge back to the suction to break the bubbles.\n\nA centrifugal pump containing two or more impellers is called a multistage centrifugal pump. The impellers may be mounted on the same shaft or on different shafts. At each stage, the fluid is directed to the center before making its way to the discharge on the outer diameter.\n\nFor higher pressures at the outlet, impellers can be connected in series. For higher flow output, impellers can be connected in parallel.\n\nA common application of the multistage centrifugal pump is the boiler feedwater pump. For example, a 350 MW unit would require two feedpumps in parallel. Each feedpump is a multistage centrifugal pump producing 150 l/s at 21 MPa.\n\nAll energy transferred to the fluid is derived from the mechanical energy driving the impeller. This can be measured at isentropic compression, resulting in a slight temperature increase (in addition to the pressure increase).\n\nThe energy usage in a pumping installation is determined by the flow required, the height lifted and the length and friction characteristics of the pipeline.\nThe power required to drive a pump (formula_13), is defined simply using SI units by:\n\nwhere:\nThe head added by the pump (formula_8) is a sum of the static lift, the head loss due to friction and any losses due to valves or pipe bends all expressed in metres of fluid. Power is more commonly expressed as kilowatts (10 W, kW) or horsepower (hp = kW/0.746). The value for the pump efficiency, formula_12, may be stated for the pump itself or as a combined efficiency of the pump and motor system.\n\nThe energy usage is determined by multiplying the power requirement by the length of time the pump is operating.\n\nThese are some difficulties faced in centrifugal pumps:\n\nAn oilfield solids control system needs many centrifugal pumps to sit on or in mud tanks. The types of centrifugal pumps used are sand pumps, submersible slurry pumps, shear pumps, and charging pumps. They are defined for their different functions, but their working principle is the same.\n\nMagnetically coupled pumps, or magnetic drive pumps, vary from the traditional pumping style, as the motor is coupled to the pump by magnetic means rather than by a direct mechanical shaft. The pump works via a drive magnet, 'driving' the pump rotor, which is magnetically coupled to the primary shaft driven by the motor.[7] They are often used where leakage of the fluid pumped poses a great risk (e.g., aggressive fluid in the chemical or nuclear industry, or electric shock - garden fountains). They have no direct connection between the motor shaft and the impeller, so no gland is needed. There is no risk of leakage, unless the casing is broken. Since the pump shaft is not supported by bearings outside the pump's housing, support inside the pump is provided by bushings. The pump size of a magnetic drive pumps can go from few Watts power to a giant 1MW.\n\nMost centrifugal pumps are not self-priming. In other words, the pump casing must be filled with liquid before the pump is started, or the pump will not be able to function. If the pump casing becomes filled with vapors or gases, the pump impeller becomes gas-bound and incapable of pumping. To ensure that a centrifugal pump remains primed and does not become gas-bound, most centrifugal pumps are located below the level of the source from which the pump is to take its suction. The same effect can be gained by supplying liquid to the pump suction under pressure supplied by another pump placed in the suction line.\n\nIn normal conditions, common centrifugal pumps are unable to evacuate the air from an inlet line leading to a fluid level whose geodetic altitude is below that of the pump. Self-priming pumps have to be capable of evacuating air (see Venting) from the pump suction line without any external auxiliary devices.\n\nCentrifugal pumps with an internal suction stage such as water jet pumps or side channel pumps are also classified as self-priming pumps.\n\nCentrifugal pumps which are not designed with an internal or external self-priming stage can only start to pump the fluid after the pump has initially been primed with the fluid. Sturdier but slower, their impellers are designed to move water which is far denser than air, leaving them unable to operate when air is present. In addition, a suction-side swing check valve or a vent valve must be fitted to prevent any siphon action and ensure that the fluid remains in the casing when the pump has been stopped. In self-priming centrifugal pumps with a separation chamber the fluid pumped and the entrained air bubbles are pumped into the separation chamber by the impeller action.\n\nThe air escapes through the pump discharge nozzle whilst the fluid drops back down and is once more entrained by the impeller. The suction line is thus continuously evacuated. The design required for such a self-priming feature has an adverse effect on pump efficiency. Also, the dimensions of the separating chamber are relatively large. For these reasons this solution is only adopted for small pumps, e.g. garden pumps. More frequently used types of self-priming pumps are side channel and water ring pumps. Another type of self-priming pump is a centrifugal pump with two casing chambers and an open impeller. This design is not only used for its self-priming capabilities but also for its degassing effects when pumping twophase mixtures (air/gas and liquid) for a short time in process engineering or when handling polluted fluids, for example when draining water from construction pits.\n\nThis pump type operates without a foot valve and without an evacuation device on the suction side. The pump has to be primed with the fluid to be handled prior to commissioning. \nTwo-phase mixture is pumped until the suction line has been evacuated and the fluid level has been pushed into the front suction intake chamber by atmospheric pressure. During normal pumping operation this pump works like an ordinary centrifugal pump.\n\n\n\n"}
{"id": "39443568", "url": "https://en.wikipedia.org/wiki?curid=39443568", "title": "Changsheng Power Plant", "text": "Changsheng Power Plant\n\nThe Changsheng Power Plant or Ever Power Power Plant () is a gas-fired power plant in Luzhu District, Taoyuan City, Taiwan.\n\n"}
{"id": "24107566", "url": "https://en.wikipedia.org/wiki?curid=24107566", "title": "Charles Secrett", "text": "Charles Secrett\n\nCharles Secrett is an environmental activist, head of Friends of the Earth England, Wales and Northern Ireland in 1993-2003. He is an author and broadcaster on environmental topics.\n\nSecrett was a member of the Labour government's Commission for Sustainable Development, and sits on the Advisory Boards for \"The Ecologist\" magazine and the Environmental Law Foundation in Britain, and for the Environment Programme of the University of North Carolina in the United States. In 1999, \"The Observer\" newspaper ranked him the 36th most influential person in Britain in its annual ‘Power 300 List'; in 2000, he was ranked 200th.\n\nSecrett has also campaigned against nuclear power.\n\nPrior to the 2015 general election, he was one of several celebrities who endorsed the parliamentary candidacy of the Green Party's Caroline Lucas.\n\n\n"}
{"id": "2009032", "url": "https://en.wikipedia.org/wiki?curid=2009032", "title": "Cherrapunji", "text": "Cherrapunji\n\nCherrapunji (; with the native name Sohra being more commonly used, and can also be spelled Cherrapunjee or Cherrapunji) is a subdivisional town in the East Khasi Hills district in the Indian state of Meghalaya. It is the traditional capital of aNongkhlaw \"hima\" (Khasi tribal chieftainship constituting a petty state), both known as Sohra or Churra.\n\nCherrapunji has often been credited as being the wettest place on Earth, but for now nearby Mawsynram currently holds that distinction. Cherrapunji still holds the all-time record for the most rainfall in a calendar month and in a year, however: it received in July 1861 and between 1 August 1860 and 31 July 1861.\n\nThe history of the Khasi people – native inhabitants of Cherrapunji – may be traced from the early part of the 16th century. Between the 16th and 18th centuries, these people were ruled by their tribal 'Syiems (rajas or chiefs) of Khyriem' in the Khasi Hills. The Khasis hills came under British authority in 1883 with the submission of the last of the important Syiems, Tirot Sing.\n\nThe main pivot on which the entire superstructure of Khasi society rests is the matrilineal system.\n\nThe original name for this town was Sohra (soh-ra), which was pronounced \"Churra\" by the British. This name eventually evolved into the current name, Cherrapunji, meaning 'land of oranges', which was first used by tourists from other parts of India.\n\nDespite abundant rainfall, Cherrapunji faces an acute water shortage and the inhabitants often have to trek very long distances to obtain potable water. Irrigation is hampered due to excessive rain washing away the topsoil as a result of human encroachment into the forests.\n\nThe Meghalaya state government has renamed Cherrapunji back to its original name, \"Sohra\". There is a monument to David Scott (British Administrator in NE India, 1802–31) in the Cherrapunji cemetery.\n\nCherrapunji is located at . It has an average elevation of and sits on a plateau in the southern part of the Khasi Hills, facing the plains of Bangladesh. The plateau rises 660 meters above the surrounding valleys.\n\nSoils on the plateau are poor owing to deforestation and washout caused by heavy rainfall. Owing to winter droughts, the vegetation in this location is even xerophytic in spite of the fame of Cherrapunji as an extremely wet place. Additional pressure on local ecosystems is created by the rapid increase of the population — from a Sohra-area population of 7,000 in 1960, it grew to over 100,000 by 2000.\n\nValleys around Cherrapunji, however, are covered with lush and very diverse vegetation, containing numerous endemic species of plants, including Meghalaya subtropical forests. Although there is heavy rainfall in Cherrapunji, there is a high scarcity of clean water.\n\nThere are some interesting living root bridges in Cherrapunjee like the Umshiang root bridge, Mawsaw root bridge, Ritymmen root bridge etc. \n\nThe Shillong Plateau is an uplifted horst-like feature, bounded by the E-W Main Boundary Thrust (MBT) to the North, the N-S Jamuna fault in the west, and the NW-SE kopilli fracture zone in the east.\n\nSohra or Cherrapunji has a mild subtropical highland climate (Köppen \"Cwb\"), with monsoonal influences typical of India. The city's annual rainfall average stands at . This figure places it behind only nearby Mawsynram, Meghalaya, whose average is . Cherrapunji receives both the southwest and northeast monsoonal winds, giving it a single monsoon season. It lies on the windward side of the Khasi Hills, so the resulting orographic lift enhances precipitation. In the winter months it receives the northeast monsoon showers that travel down the Brahmaputra valley. The driest months are November, December, January and February. \n\nTemperatures average in January and in August, and the annual mean is \n\nCherrapunji holds two Guinness world records for receiving the maximum amount of rainfall in a single year: of rainfall between August 1860 and July 1861 and for receiving the maximum amount of rainfall in a single month: in July 1861.\n\nCherrapunji receives rains from the Bay of Bengal arm of the Indian summer monsoon. The monsoon clouds fly unhindered over the plains of Bangladesh for about 400 km. Thereafter, they hit the Khasi Hills which rise abruptly from the plains to a height of about 1,370 m above mean sea level within 2 to 5 km. The geography of the hills with many deep valley channels encompassing the low-flying (150–300 m) moisture-laden clouds from a wide area converges over Cherrapunji. The winds push the rain clouds through these gorges and up the steep slopes. The rapid ascent of the clouds into the upper atmosphere hastens the cooling and helps vapours to condense. Most of Cherrapunji's rain is the result of air being lifted as a large body of water vapour. The extreme amount of rainfall at Cherrapunji is perhaps the best-known feature of orographic rain in northeast India.\n\nOccasionally, cloudbursts can occur in one part of Cherrapunji while other areas may be totally or relatively dry, reflecting the high spatial variability of the rainfall. Atmospheric humidity is extremely high during the peak monsoon period.\n\nThe major part of the rainfall at Cherrapunji can be attributed to the orographic features. When the clouds are blown over the hills from the south, they are funneled through the valley. The clouds strike Cherrapunjee perpendicularly and the low flying clouds are pushed up the steep slopes. It is not surprising to find that the heaviest rainfalls occur when the winds blow directly on the Khasi Hills.\n\nA notable feature of monsoon rain at Cherrapunji is that most of it falls in the morning. This could be partly due to two air masses coming together. During the monsoon months, the prevailing winds along the Brahmaputra valley generally blow from the east or the northeast, but the winds over Meghalaya are from the south. These two winds systems usually come together in the vicinity of the Khasi Hills. Apparently, the winds that are trapped in the valley at night begin their upward ascent only after they are warmed during the day. This partially explains the frequency of morning rainfall. Apart from orographic features, atmospheric convection plays an important role during the monsoon and the period just preceding it.\n\n India census, Cherrapunji had a population of 10,086, with males 48.75% of the total population and females 51.25%. Cherrapunji has an average literacy rate of 74%, higher than the national average of 59.5%, with a male literacy rate of 72.4% and a female one of 73.9%.\n\nThe locals living in and around Cherrapunji are known as Khasis. It is a matrilineal culture. After the wedding, the husband of the youngest daughter goes to live with his wife's family, who own the property of the family, while others live on their own getting a bit of the share. The children take on the surname of the mother.\n\nCherrapunji is also famous for its living bridges. Over hundreds of years the people in Cherrapunji have developed techniques for growing roots of trees into large bridges. The process takes 10 to 15 years and the bridges typically last hundreds of years, the oldest ones in use being over 500 years old.\n\nSohra Government College is the only general degree college in Cherrapunji.\n\nCherrapunji has an All India Radio relay station known as Akashvani Cherrapunji. It broadcasts on FM frequencies.\n\n\n"}
{"id": "40193797", "url": "https://en.wikipedia.org/wiki?curid=40193797", "title": "Christi Craddick", "text": "Christi Craddick\n\nChristi Leigh Craddick (born July 1, 1970) is one of three members of the Texas Railroad Commission, the elected regulatory body over oil, natural gas, utilities, and surface mining first established in 1891. She is a Republican. The commission ended all controls over railroads in 2005 but is still known as the \"Railroad Commission\" for historical reasons.\n\nA native of Midland, Texas, Craddick won her seat in the general election held on November 6, 2012, in conjunction with the presidential contest.\n\nCraddick's father is State Representative Tom Craddick, a Midland businessman who was the Speaker of the Texas House from 2003 to 2009. Her mother is the former Nadine Nayfa, of Lebanese descent and originally from Sweetwater in Nolan County in West Texas. She has a brother, Thomas Russell Craddick, Jr. (born 1973). Craddick has a daughter, Catherine. The Craddicks are Roman Catholic.\n\nCraddick graduated from Midland High School, obtained her undergraduate degree from the University of Texas at Austin, and received her Juris Doctor from the University of Texas School of Law.\n\nCraddick's path to victory surged in the Republican runoff election held on July 31, 2012, when she easily defeated then State Representative Warren Chisum of Pampa in Gray County in the Texas Panhandle. Chisum is a former state legislative lieutenant of Tom Craddick. In that same election, most of the attention had focused not on the Craddick-Chisum race but on conservative Ted Cruz, who defeated Lieutenant Governor David Dewhurst for the Republican nomination to succeed U.S. Senator Kay Bailey Hutchison. Craddick raised triple the campaign contributions of Chisum, more than $1 million compared to $375,000, but Chisum had access to another $600,000 that he had accumulated earlier as a legislator. Craddick enjoyed the support of such wealthy donors as the entrepreneur James R. Leininger of San Antonio and the late homebuilder Bob J. Perry of Houston.\n\nCraddick polled 589,211 votes (60 percent); Chisum, 396,858 ballots (40 percent). \n\nCraddick thereafter defeated the Democratic nominee, Dale P. Henry (born 1930), a retired petroleum engineer from Lampasas in Central Texas. Craddick polled 4,336,499 votes (56 percent); Henry, 3,057,733 (40 percent). The remaining 4 percent was cast for two minor-party candidates.\n\nCraddick succeeded Elizabeth Ames Jones of San Antonio, who vacated the seat in February 2012. Jones ran for the District 25 seat in the Texas State Senate, which was ultimately won by the Republican physician Donna Campbell of New Braunfels, who unseated incumbent Senator Jeff Wentworth, a Moderate Republican from San Antonio, in the party runoff on July 31. Interim commissioner Buddy Garcia, an appointee of Governor Rick Perry, stepped down several weeks after the 2012 general election, and Perry named Craddick to complete the few days remaining in Jones' term.\n\nCraddick's two Republican colleagues on the railroad commission were David J. Porter of Giddings in Lee County, formerly of Midland, and the former chairman, Barry Smitherman, formerly of Houston. Smitherman, elected to a two-year unexpired term in 2012, did not seek a full six-year term in 2014; he instead ran for Texas attorney general to succeed Greg Abbott, but came in third place in the Republican primary. Since 1995, when veteran Democratic member James E. Nugent was unseated by Charles R. Matthews, all railroad commissioners have been Republicans. Both Craddick and Porter have ties to the oil-rich Permian Basin of Midland/Odessa.\n\nMark Jones, a political scientist at Rice University, attributed Craddick's victory over Chisum to the \"respect\" within the GOP for her father. Tom Craddick is the longest-serving Republican legislator in Austin, having first been elected in 1968. He lost the Speakership in 2009 to Joe Straus, a moderate Republican from San Antonio, who initially prevailed through a coalition of mostly Democrats and sixteen maverick Republicans.\n\nFormer Midland Mayor Ernest Angelo, a one-time Texas Republican National Committeeman, said that Craddick succeeded because she gained credibility with large Republican donors and traveled by highway to meet with the conservative grassroots and women's groups. According to Angelo, Tom Craddick's neighbor of many years, Christi Craddick \"showed she will do what it takes to win a state primary. She earned it.\"\n\nFrom the start of her term as commissioner, Craddick has been critical of federal intervention into the energy industries: \"Texas knows how energy regulation is done. People ought to be modeling themselves after us, instead of ... the EPA,\" she told an energy policy group in Austin.\"\n\nIn August 2014, she was unanimously elected as the chairman of the Texas Railroad Commission.\n\nOn March 16, 2018, Craddick with nearly 76 percent of the vote defeated her Republican primary opponent, Weston Martinez of San Antonio. She then defeated the Democrat Roman McAllen in the November 6 general election. Craddick polled 4,356,658 votes (53.2 percent) to McAllen's 3,588,625 ballots (43.9 percent). Another 236,720 votes (2.9 percent) went to the Libertarian Party nominee, Mike Wright.\n"}
{"id": "773672", "url": "https://en.wikipedia.org/wiki?curid=773672", "title": "Compressed-air vehicle", "text": "Compressed-air vehicle\n\nA compressed-air vehicle (CAV) is a transport mechanism fueled by tanks of pressurized atmospheric gas and propelled by the release and expansion of the gas within a Pneumatic motor. CAV's have found application in torpedoes, locomotives used in digging tunnels, and early prototype submarines. Potential environmental advantages have generated public interest in CAV's as passenger cars, but they have not been competitive due to the low energy density of compressed air and inefficiency of the compression / expansion process.\n\nCompressed-air propulsion may also be incorporated in hybrid systems, such as with battery electric propulsion. This kind of system is called a hybrid-pneumatic electric propulsion. Additionally, regenerative braking can also be used in conjunction with this system.\n\nThe tanks must be designed to safety standards appropriate for a pressure vessel, such as ISO 11439.\n\nThe storage tank may be made of metal or composite materials. The fiber materials are considerably lighter than metals but generally more expensive. Metal tanks can withstand a large number of pressure cycles, but must be checked for corrosion periodically.\n\nOne company stores air in tanks at 4,500 pounds per square inch (about 30 MPa) and hold nearly 3,200 cubic feet (around 90 cubic metres) of air.\n\nThe tanks may be refilled at a service station equipped with heat exchangers, or in a few hours at home or in parking lots, plugging the car into the electrical grid via an on-board compressor. The cost of driving such a car is typically projected to be around €0.75 per 100 km, with a complete refill at the \"tank-station\" at about US$3.\n\nCompressed air has a low energy density. In 300 bar containers, about 0.1 MJ/L and 0.1 MJ/kg is achievable, comparable to the values of electrochemical lead-acid batteries. While batteries can somewhat maintain their voltage throughout their discharge and chemical fuel tanks provide the same power densities from the first to the last litre, the pressure of compressed air tanks falls as air is drawn off. A consumer-automobile of conventional size and shape typically consumes 0.3–0.5 kWh (1.1–1.8 MJ) at the drive shaft per mile of use, though unconventional sizes may perform with significantly less.\n\nLike other non-combustion energy storage technologies, an air vehicle displaces the emission source from the vehicle's tail pipe to the central electrical generating plant. Where low emissions sources are available, net production of pollutants can be reduced. Emission control measures at a central generating plant may be more effective and less costly than treating the emissions of widely dispersed vehicles.\n\nSince the compressed air is filtered to protect the compressor machinery, the air discharged has less suspended dust in it, though there may be carry-over of lubricants used in the engine. The car works when gas expands.\n\nCompressed air has been used since the 19th century to power mine locomotives and trams in cities such as Paris (via a central, city-level, compressed air energy distribution system), and was previously the basis of naval torpedo propulsion.\n\nDuring the construction of the Gotthardbahn from 1872 to 1882, pneumatic locomotives were used in the construction of the Gotthard Rail Tunnel and other tunnels of the Gotthardbahn.\n\nIn 1903, the Liquid Air Company located in London England manufactured a number of compressed-air and liquified-air cars. The major problem with these cars and all compressed-air cars is the lack of torque produced by the \"engines\" and the cost of compressing the air.\n\nSince 2010, several companies have started to develop compressed air cars including hybrid types that also include a petrol driven engine; none has been released to the public, or have been tested by third parties.\n\nCompressed-air vehicles are comparable in many ways to electric vehicles, but use compressed air to store the energy instead of batteries. Their potential advantages over other vehicles include:\n\nThe principal disadvantage is the indirect use of energy. Energy is used to compress air, which – in turn – provides the energy to run the motor. Any conversion of energy between forms results in loss. For conventional combustion motor cars, the energy is lost when oil is converted to usable fuel – including drilling, refinement, labor, storage, eventually transportation to the end-user. For compressed-air cars, energy is lost when electrical energy is converted to compressed air, and when fuel, whether coal, natural gas or nuclear, is burned to drive the electrical generators.\n\nCompressed-air vehicles operate according to a thermodynamic process because air cools down when expanding and heats up when being compressed. Since it is not practical to use a theoretically ideal process, losses occur and improvements may involve reducing these, e.g., by using large heat exchangers in order to use heat from the ambient air and at the same time provide air cooling in the passenger compartment. At the other end, the heat produced during compression can be stored in water systems, physical or chemical systems and reused later.\n\nIt may be possible to store compressed air at lower pressure using an absorption material within the tank. Absorption materials like Activated carbon, or a metal organic framework is used for storing compressed natural gas at 500 psi instead of 4500 psi, which amounts to a large energy saving.\n\nSeveral companies are investigating and producing prototypes including hybrid compressed-air/gasoline-combustion vehicles. As of August 2017, none of the developers have yet gone into production, although Tata has indicated they will begin selling vehicles from 2020 and MDI's US distributor Zero Pollution Motors says production of the AIRPod will commence in Europe in 2018.\n\nIn 2008, a compressed air and natural gas powered vehicle designed by engineering students at Deakin University in Australia was joint winner of the Ford Motor Company T2 competition to produce a car with a 200 km range and a cost of less than $7,000.\n\nAustralian company Engineair has produced a number of vehicle types - moped, small car, small carrier, go-cart - around the rotary compressed air engine created by Angelo Di Pietro. The company is seeking partners to utilise its engine.\n\nA compressed-air powered motorcycle, called the Green Speed Air Powered Motorcycle was made by Edwin Yi Yuan, based on the Suzuki GP100 and using the Angelo Di Pietro compressed-air engine.\n\nThree mechanical engineering students from San Jose State University; Daniel Mekis, Dennis Schaaf and Andrew Merovich, designed and built a bike that runs on compressed air. The total cost of the prototype was under $1000 and was sponsored by Sunshops (on the Boardwalk in Santa Cruz, California) and NO DIG NO RIDE (from Aptos, California.). The top speed of the maiden voyage in May 2009 was 23 mph https://www.youtube.com/watch?v=NBeky4EuyBc.\nWhile their design was simple (https://www.youtube.com/watch?v=NBeky4EuyBc), these three pioneers of compressed air powered vehicles helped pave the way for French automaker Peugeot Citreon to invent a brand new air-powered hybrid. The 'Hybrid Air' system uses compressed air to move the car's wheels when driving under 43 mph. Peugeot says the new hybrid system should get up to 141 miles per gallon of gas. Models should roll out as early as 2016. http://screen.yahoo.com/hybrid-car-runs-air-001013615.html. The head of the project left Peugeot in 2014 and in 2015 the company said it had been unable to find a partner to share the development costs, effectively ending the project. .\n\n\"Ku:Rin\" named air-compressed three-wheeler vehicle was created by Toyota in 2011. The speciality about this vehicle is it has registered a record-breaking highest speed 129.2 km/h (80 mph) even if it has engine which uses only compressed air. This car was developed by the companies \"Dream car workshop\". This car is nicknamed as \"sleek rocket\", or \"pencil shaped rocket\".\n\nAs part of the TV-show Planet Mechanics, Jem Stansfield and Dick Strawbridge converted a regular scooter to a compressed air moped. This has been done by equipping the scooter with a compressed-air engine and air tank.\n\nIn 2010, Honda presented the Honda Air concept car at the LA Auto Show.\n\nCompressed-air locomotives are a kind of fireless locomotive and have been used in mining and tunnel boring.\n\nVarious compressed-air-powered trams were trialled, starting in 1876. In Nantes and Paris such trams ran in regular service for 30 years.\n\nCurrently, no water or air vehicles exist that make use of the compressed air engine. Historically certain torpedoes were propelled by compressed-air engines.\n\n"}
{"id": "48565512", "url": "https://en.wikipedia.org/wiki?curid=48565512", "title": "Cotton production in China", "text": "Cotton production in China\n\nCotton production in China pertains to cotton which is the prime cash crop of China. Its production is the highest in the world (in excess of 20 percent of world production) since 1982 until recently overtaken by India. Twenty four of the 35 provinces of China grow cotton with nearly 300 million people involved in its production. It is the main input for textile industry of the country with cotton textiles now accounting for 80% use vis-a-vis 95% in the 1950s. The cotton textile industry employs over 10 million people. Its production in 2012–13 was 7.6 million tons but in 2013–14 it dropped to 7 million tons. Historically, its introduction in China is considered a 'southernization', crop which originated from southern Asia (India) during the \"proto-globalization\" period.\n\nCotton, which is called \"mian\" () or \"mumian\" () in Chinese was first reported from an area now known as Yunnan, some time around 200 BC. Lao-ai tribe in the southwest border region is reported to have produced quality cotton cloth around 25–220 CE. In the Tarim Basin archaeological finds of cotton cloth on mummies dated around 1,000 BC have also been reported. In the early Han dynasty period in Siachen in Western China, around 100 BC, quality cotton cloth has been noted. From all these documented and archaeological evidences, a historian has called the introduction of cotton in the country as \"southernization\" during which period the cotton species introduced were \"Gossypium herbaceum\" (an Afro-Asian species) and \"Gossypium arboreum\" and \"Gossypium hardense\" from India; the latter species were grown in Guangdong and Fujian provinces during the ninth century AD but were blocked for introduction into Sichuan province due to a strong silk lobby. However, adoption of scientific methods of cultivation came into practice only from 1949.\n\nThe cotton sown area accounts for about 30 percent of the total sown area of all the various cash crops. The major regions, according where cotton is a prime crop are the Xinjiang Autonomous Region, the Yangtze River Basin (including provinces of Jiangxi and Hubei), and the Huang-Huai Region (mainly in provinces of Hebei, Henan, and Shandong). However, according to agroclimatic regions they are under three zones of the northwest inland cotton region, the Yellow River valley region and the Yangtze River valley region, which together account for 99.5% area under cotton with a total yield of 99.7%. The area planted in 2012 was 5.3 million hectares and the average lint yield was per ha.\n\nBoth Bt cotton and non Bt cotton varieties are used in China. Bt cotton is a major Genetically modified crop, which is adopted for commercial production. This was introduced after several field trials in cotton growing provinces and found to be economically profitable in terms of not only production but also revenue. It was introduced in 2002 in about 45% of country’s total area under cotton as it also proved to have better pest resistance. The Hybrid (F1) Bt cotton, developed after crossing with a non-Bt line, has found favour in southern China because of higher yield due to better methods of farming such as seedling transplantation and planting at lower population density. Now Bollgard Bt cotton is adopted in 55% of cropped area while 45% area is planted with China’s own Bt cotton.\n\nAccording to the FAO statistics for 2012, cotton production was 6.84 million tonnes with cotton seed production of 13.68 million tonnes. The crops grown are under intensive cotton cultivation practices, typically with Chinese adaptations covering special seedling transplanting (since 1950's), plastic mulching since 1979, plant training and double cropping of the cotton–wheat (\"Triticum aestivum\") system (from 1980s to 1990s). Another practice introduced extensively since 1980 is the adoption of high-yielding cultivation pattern known as \"short-dense-early\" in the northwestern inland areas which is stated to have contributed to China achieving the status of world number one in cotton production.\n"}
{"id": "5179212", "url": "https://en.wikipedia.org/wiki?curid=5179212", "title": "Crookwell Wind Farm", "text": "Crookwell Wind Farm\n\nCrookwell Wind Farm, located at Crookwell west of Goulburn, New South Wales, consists of eight 600 kW wind turbines giving a total capacity of 4.8 MW. It was the first grid-connected wind farm in Australia when built by Pacific Power in 1998. It is now owned by Tilt Renewables.\n\nThe farm was the largest wind farm in Australia when built, with the energy produced bought and on-sold to customers by then energy retailer Great Southern Energy.\n\nPhase two of the Crookwell Wind Farm, planned to have an installed capacity of 92 MW, is under construction since 2009.\n\n\n"}
{"id": "2635884", "url": "https://en.wikipedia.org/wiki?curid=2635884", "title": "Current sheet", "text": "Current sheet\n\nA current sheet is an electric current that is confined to a surface, rather than being spread through a volume of space. Current sheets feature in magnetohydrodynamics (MHD), the study of the behavior of electrically conductive fluids: if there is an electric current through part of the volume of such a fluid, magnetic forces tend to expel it from the fluid, compressing the current into thin layers that pass through the volume.\n\nThe largest occurring current sheet in the Solar System is the so-called Heliospheric current sheet, which is about 10,000 km thick, and extends from the Sun and out beyond the orbit of Pluto.\n\nIn astrophysical plasmas such as the solar corona, current sheets theoretically might have an aspect ratio (breadth divided by thickness) as high as 100,000:1. By contrast, the pages of most books have an aspect ratio close to 2000:1. Because current sheets are so thin in comparison to their size, they are often treated as if they have zero thickness; this is a result of the simplifying assumptions of ideal MHD. In reality, no current sheet may be infinitely thin because that would require infinitely fast motion of the charge carriers whose motion causes the current.\n\nCurrent sheets in plasmas store energy by increasing the energy density of the magnetic field. Many plasma instabilities arise near strong current sheets, which are prone to collapse, causing magnetic reconnection and rapidly releasing the stored energy. This process is the cause of solar flares and is one reason for the difficulty of magnetic confinement fusion, which requires strong electric currents in a hot plasma.\n\nAn infinite current sheet can be modelled as an infinite number of parallel wires all carrying the same current. Assuming each wire carries current \"I\", and there are \"N\" wires per unit length, the magnetic field can be derived using Ampère's law:\n\nR is a rectangular loop surrounding the current sheet, perpendicular to the plane and perpendicular to the wires. In the two sides perpendicular to the sheet, formula_3 since formula_4. In the other two sides, formula_5, so if S is one parallel side of the rectangular loop of dimensions L x W, the integral simplifies to:\nSince \"B\" is constant due to the chosen path, it can be pulled out of the integral:\nThe integral is evaluated:\nSolving for \"B\", plugging in for \"I\" (total current enclosed in path \"R\") as \"I\"*\"N\"*\"L\", and simplifying:\nNotably, the magnetic field strength of an infinite current sheet does not depend on the distance from it.\n\nThe direction of B can be found via the right-hand rule.\n\nA well-known one-dimensional current sheet equilibrium is Harris current sheet, which is a stationary solution to the Maxwell-Vlasov system. The magnetic field profile is given by\n\n"}
{"id": "293065", "url": "https://en.wikipedia.org/wiki?curid=293065", "title": "Desiccant", "text": "Desiccant\n\nA desiccant is a hygroscopic substance that induces or sustains a state of dryness (desiccation) in its vicinity; it is the opposite of a humectant. Commonly encountered pre-packaged desiccants are solids that absorb water. Desiccants for specialized purposes may be in forms other than solid, and may work through other principles, such as chemical bonding of water molecules. They are commonly encountered in foods to retain crispness. Industrially, desiccants are widely used to control the level of water in gas streams.\n\nAlthough some desiccants are chemically inert, others are extremely reactive and require specialized handling techniques. The most common desiccant is silica, an otherwise inert, nontoxic, water-insoluble white solid. Tens of thousands of tons are produced annually for this purpose. Other common desiccants include activated charcoal, calcium sulfate (Drierite), calcium chloride, and molecular sieves (typically, zeolites).\n\nOne measure of desiccant efficiency is the ratio (or percentage) of water storable in the desiccant relative to the mass of desiccant.\n\nAnother measure is the residual relative humidity of the air or other fluid being dried.\n\nThe performance of any desiccant varies with temperature and both relative humidity and absolute humidity. To some extent, desiccant performance can be precisely described, but most commonly, the final choice of which desiccant best suits a given situation, how much of it to use, and in what form, is made based on testing and practical experience.\n\nSometimes a humidity indicator is included in the desiccant to show, by color changes, the degree of water-saturation of the desiccant. One commonly used indicator is cobalt chloride (CoCl). Anhydrous cobalt chloride is blue. When it bonds with two water molecules, (CoCl•2HO), it turns purple. Further hydration results in the pink hexaaquacobalt(II) chloride complex [Co(HO)]Cl.\n\nOne example of desiccant usage is in the manufacture of insulated windows where zeolite spheroids fill a rectangular spacer tube at the perimeter of the panes of glass. The desiccant helps to prevent the condensation of moisture between the panes. Another use of zeolites is in the dryer component of air conditioning systems to help maintain the efficacy of the refrigerant. Desiccants are also commonly used to protect goods in shipping containers against moisture damage. Hygroscopic cargo, such as cocoa, coffee, and various nuts and grains, are particularly susceptible to mold and rot when exposed to condensation and humidity. Because of this, shippers often take precautionary measures to protect against cargo loss.\n\nDesiccants induce dryness in any environment and reduce the amount of moisture present in air. Desiccants come in various forms and have found widespread use in the food, pharmaceuticals, packing, electronics and many manufacturing industries.\n\nAir conditioning systems can be made based on desiccants.\n\nDesiccants are used in different kinds of livestock farming to dry newborn animals, such as piglets. The use of a good desiccant can help them dry quicker and save energy, which can be crucial for the animal's development. Another use is to reduce bacteria and pathogens that thrive in wet surfaces, reducing bacteria pressure. However, some desiccants have a very high pH-level, which can be harmful for an animal's skin.\n\nDesiccants are also used to remove water from solvents, typically required by chemical reactions that do not tolerate water, e.g., the Grignard reaction. The method generally, though not always, involves mixing the solvent with the solid desiccant. Studies show that molecular sieves are superior as desiccants relative to chemical drying reagents such as sodium-benzophenone. Sieves offer the advantages of being safe in air and recyclable.\n\n\n"}
{"id": "15636975", "url": "https://en.wikipedia.org/wiki?curid=15636975", "title": "ECOS (CSIRO magazine)", "text": "ECOS (CSIRO magazine)\n\nECOS is an Australian environmental magazine which presents articles on sustainability research and issues from across Australia and the Asia Pacific region, published bi-monthly online by CSIRO.\n\n\"ECOS\" was founded in 1974. The magazine won the Banksia Award for Communication in 2000. Until May June 2011 the magazine was also printed.\n"}
{"id": "16984813", "url": "https://en.wikipedia.org/wiki?curid=16984813", "title": "Ecocho", "text": "Ecocho\n\nEcocho was a search engine with the aim of offsetting carbon emissions by donating 70% of revenues to 'carbon offset credits'. The site launched on 14 April 2008.\n\nIt is owned by the Found Agency, a search engine optimisation company.\n\nThe Ecocho search is powered by Yahoo! after a termination of their account with Google.\n\nFor every 1,000 searches, Ecocho claims that 1 ton of greenhouse gases are offset through the planting of trees. However, the precise amount is not given, nor are the number of searches performed on the site. Rather, the website maintains a tally of trees planted and of kilograms of offset.\n\nOn 22 April 2008, Ecocho was informed by Google that they would be denied the use of Google's search technologies for multiple past violations of Google's policies. The issue has been discussed on the Ecocho blog. The mixed reactions from users can be seen in the blog comments, some taking further action by complaining to Google, and others agreeing with Google.\n\n\n"}
{"id": "35034187", "url": "https://en.wikipedia.org/wiki?curid=35034187", "title": "Film capacitor", "text": "Film capacitor\n\nFilm capacitors, plastic film capacitors, film dielectric capacitors, or polymer film capacitors, generically called “film caps” as well as power film capacitors, are electrical capacitors with an insulating plastic film as the dielectric, sometimes combined with paper as carrier of the electrodes.\n\nThe dielectric films, depending on the desired dielectric strength, are drawn in a special process to an extremely thin thickness, and are then provided with electrodes. The electrodes of film capacitors may be metallized aluminum or zinc applied directly to the surface of the plastic film, or a separate metallic foil. Two of these conductive layers are wound into a cylinder shaped winding, usually flattened to reduce mounting space requirements on a printed circuit board, or layered as multiple single layers stacked together, to form a capacitor body. Film capacitors, together with ceramic capacitors and electrolytic capacitors, are the most common capacitor types for use in electronic equipment, and are used in many AC and DC microelectronics and electronics circuits.\n\nA related component type is the power (film) capacitor. Although the materials and construction techniques used for large power film capacitors are very similar to those used for ordinary film capacitors, capacitors with high to very high power ratings for applications in power systems and electrical installations are often classified separately, for historical reasons. As modern electronic equipment gained the capacity to handle power levels that were previously the exclusive domain of \"electrical power\" components, the distinction between the \"electronic\" and \"electrical\" power ratings has become less distinct. In the past, the boundary between these two families was approximately at a reactive power of 200 volt-amperes, but modern power electronics can handle increasing amounts of power.\n\nFilm capacitors are made out of two pieces of plastic film covered with metallic electrodes, wound into a cylindrical shaped winding, with terminals attached, and then encapsulated. In general, film capacitors are not polarized, so the two terminals are interchangeable. There are two different types of plastic film capacitors, made with two different electrode configurations:\n\n\nA key advantage of modern film capacitor internal construction is direct contact to the electrodes on both ends of the winding. This contact keeps all current paths to the entire electrode very short. The setup behaves like a large number of individual capacitors connected in parallel, thus reducing the internal ohmic losses (ESR) and the parasitic inductance (ESL). The inherent geometry of film capacitor structure results in very low ohmic losses and a very low parasitic inductance, which makes them especially suitable for applications with very high surge currents (snubbers) and for AC power applications, or for applications at higher frequencies.\n\nAnother feature of film capacitors is the possibility of choosing different film materials for the dielectric layer to select for desirable electrical characteristics, such as stability, wide temperature range, or ability to withstand very high voltages. Polypropylene film capacitors are specified because of their low electrical losses and their nearly linear behavior over a very wide frequency range, for stability Class 1 applications in resonant circuits, comparable only with ceramic capacitors. For simple high frequency filter circuits, polyester capacitors offer low-cost solutions with excellent long-term stability, allowing replacement of more expensive tantalum electrolytic capacitors. The film/foil variants of plastic film capacitors are especially capable of handling high and very high current surges.\n\nTypical capacitance values of smaller film capacitors used in electronics start around 100 picofarads and extend upwards to microfarads.\n\nUnique mechanical properties of plastic and paper films in some special configurations allow them to be used in capacitors of very large dimensions. The larger film capacitors are used as power capacitors in electrical power installations and plants, capable of withstanding very high power or very high applied voltages. The dielectric strength of these capacitors can reach into the four-digit voltage range.\n\nThe formula for capacitance (\"C\") of a plate capacitor is:\nformula_1 \n(\"ε\" stands for dielectric permittivity; \"A\" for electrode surface area; and \"d\" for the distance between the electrodes).\n\nAccording to the equation, a thinner dielectric or a larger electrode area both will increase the capacitance value, as will a dielectric material of higher permittivity.\n\nThe following example describes a typical manufacturing process flow for wound metallized plastic film capacitors.\n\nThe production of wound film/metal foil capacitors with metal foil instead of metallized films is done in a very similar way.\n\nAs an alternative to the traditional wound construction of film capacitors, they can also be manufactured in a “stacked” configuration. For this version, the two metallized films representing the electrodes are wound on a much larger core with a diameter of more than 1 m. So-called multi-layer capacitors (MLP, Multilayer Polymer Capacitors) can be produced by sawing this large winding into many smaller single segments. The sawing causes defects on the collateral sides of the capacitors which are later burned out (self-healing) during the manufacturing process. Low-cost metallized plastic film capacitors for general purpose applications are produced in this manner. This technique is also used to produce capacitor \"dice\" for Surface Mount Device (SMD) packaged components.\n\nMetallized film capacitors have \"self-healing\" properties, which are not available from film/foil configurations. When sufficient voltage is applied, a point-defect short-circuit between the metallized electrodes vaporizes due to high arc temperature, since both the dielectric plastic material at the breakdown point and the metallized electrodes around the breakdown point are very thin (about 0.02 to 0.05 µm). The point-defect cause of the short-circuit is burned out, and the resulting vapor pressure blows the arc away, too. This process can complete in less than 10 μs, often without interrupting the useful operation of the afflicted capacitor.\n\nThis property of self-healing allows the use of a single-layer winding of metallized films without any additional protection against defects, and thereby leads to a reduction in the amount of the physical space required to achieve a given performance specification. In other words, the so-called \"volumetric efficiency\" of the capacitor is increased.\n\nThe self-healing capability of metallized films is used multiple times during the manufacturing process of metallized film capacitors. Typically, after slitting the metallized film to the desired width, any resulting defects can be burned out (healed) by applying a suitable voltage before winding. The same method is also used after the metallization of the contact surfaces (\"schoopage\") to remove any defects in the capacitor caused by the secondary metallization process.\n\nThe \"pinholes\" in the metallization caused by the self-healing arcs reduce the capacitance of the capacitor very slightly. However, the magnitude of this reduction is quite low; even with several thousand defects to be burned out, this reduction usually is much smaller than 1% of the total capacitance of the capacitor.\n\nFor larger film capacitors with very high standards for stability and long lifetime, such as snubber capacitors, the metallization can be made with a special fault isolation pattern. In the picture on the right hand side, such a metallization formed into a “T” pattern is shown. Each of these “T” patterns produces a deliberately narrowed cross-section in the conductive metallization. These restrictions work like microscopic fuses so that if a point-defect short-circuit between the electrodes occurs, the high current of the short only burns out the fuses around the fault. The affected sections are thus disconnected and isolated in a controlled manner, without any explosions surrounding a larger short-circuit arc. Therefore, the area affected is limited and the fault is gently controlled, significantly reducing internal damage to the capacitor, which can thus remain in service with only an infinitesimal reduction in capacitance.\n\nIn field installations of electrical power distribution equipment, capacitor bank fault tolerance is often improved by connecting multiple capacitors in parallel, each protected with an internal or external fuse. Should an individual capacitor develop an internal short, the resulting fault current (augmented by capacitive discharge from neighboring capacitors) blows the fuse, thus isolating the failed capacitor from the remaining devices. This technique is analogous to the \"T metallization\" technique described above, but operating at a larger physical scale. More-complex series and parallel arrangements of capacitor banks are also used to allow continuity of service despite individual capacitor failures at this larger scale.\n\nThe rated voltage of different film materials depends on factors such as the thickness of the film, the quality of the material (freedom from physical defects and chemical impurities), the ambient temperature, and frequency of operation, plus a safety margin against the breakdown voltage (dielectric strength). But to a first approximation, the voltage rating of a film capacitor depends primarily on the thickness of the plastic film. For example, with the minimum available film thickness of polyester film capacitors (about 0.7 µm), it is possible to produce capacitors with a rated voltage of 400 VDC. If higher voltages are needed, typically a thicker plastic film will be used. But the breakdown voltage for dielectric films is usually \"nonlinear\". For thicknesses greater than about 5 mils, the breakdown voltage only increases approximately with the \"square-root\" of the film thickness. On the other hand, the capacitance \"decreases\" linearly with increased film thickness. For reasons of availability, storage and existing processing capabilities, it is desirable to achieve higher breakdown voltages while using existing available film materials. This can be achieved by a one-sided partial metallization of the insulating films in such a manner that an internal series connection of capacitors is produced. By using this series connection technique, the total breakdown voltage of the capacitor can be multiplied by an arbitrary factor, but the total capacitance is also reduced by the same factor.\n\nThe breakdown voltage can be increased by using one-sided partially metallized films, or the breakdown voltage of the capacitor can be increased by using double-sided metallized films. Double-sided metallized films also can be combined with internal series-connected capacitors by partial metallization. These multiple technique designs are especially used for high-reliability applications with polypropylene films.\n\nAn important property of film capacitors is their ability to withstand high peak voltage or peak current surge pulses. This capability depends on all internal connections of the film capacitor withstanding the peak current loads up to the maximum specified temperature. The collateral contact layers (schoopage) with the electrodes can be a potential limitation of peak current carrying capacity.\n\nThe electrode layers are wound slightly offset from each other, so that the edges of the electrodes can be contacted using a face contacting method “schoopage” at the collateral end faces of the winding. This internal connection is ultimately made by multiple point-shaped contacts at the edge of the electrode, and can be modeled as a large number of individual capacitors all connected in parallel. The many individual resistance (ESR) and inductance (ESL) losses are connected \"in parallel\", so that these total undesirable parasitic losses are minimized.\n\nHowever, ohmic contact resistance heating is generated when peak current flows through these individual microscopic contacting points, which are critical areas for the overall internal resistance of the capacitor. If the current gets too high, \"hot spots\" can develop and cause burning of the contact areas.\n\nA second limitation of the current-carrying capacity is caused by the ohmic bulk resistance of the electrodes themselves. For metallized film capacitors, which have layer thicknesses from 0.02 to 0.05 μm the current-carrying capacity is limited by these thin layers.\nThe surge current rating of film capacitors can be enhanced by various internal configurations. Because metallization is the cheapest way of producing electrodes, optimizing the shape of the electrodes is one way to minimize the internal resistance and to increase the current-carrying capacity. A slightly thicker metallization layer at the schoopage contact sides of the electrodes results in a lower overall contact resistance and increased surge current handling, without losing the self-healing properties throughout the remainder of the metallization.\n\nAnother technique to increase the surge current rating for film capacitors is a double-sided metallization. This can double the peak current rating. This design also halves the total self-inductance of the capacitor, because in effect, two inductors are connected in parallel, which allows less-unimpeded passage of faster pulses (higher so-called \"dV/dt\" rating). \n\nThe double-sided metallized film is electrostatically field-free because the electrodes have the same voltage potential on both sides of the film, and therefore does not contribute to the total capacitance of the capacitor. This film can therefore be made of a different and less expensive material. For example, a polypropylene film capacitor with double-sided metallization on a polyester film carrier makes the capacitor not only cheaper but also smaller, because the thinner polyester foil improves the volumetric efficiency of the capacitor. Film capacitors with a double-sided metallized film effectively have thicker electrodes for higher surge current handling, but still do retain their self-healing properties, in contrast to the film/foil capacitors.\n\nThe highest surge-current rated film capacitors are film/foil capacitors with a metal foil construction. These capacitors use thin metal foils, usually aluminum, as the electrodes overlying the polymer film. The advantage of this construction is the easy and robust connection of the metal foil electrodes. In this design, the contact resistance in the area of the schoopage is the lowest.\n\nHowever, metal foil capacitors \"do not have self-healing properties\". A breakdown in the dielectric film of a film/foil capacitor leads to an irreversible short circuit. To avoid breakdowns caused by weak spots in the dielectric, the insulating film chosen is always thicker than theoretically required by the specific breakdown voltage of the material. Films of less than 4 µm generally are not used for film/foil capacitors because of their excessively high numbers of point defects. Also. the metallic foils only can be produced down to about 25 µm in thickness. These tradeoffs make the film/foil capacitor the most robust but also the most expensive method for increasing surge current handling.\n\nFilm capacitors for use in electronic equipment are packaged in the common and usual industry styles: axial, radial, and SMD. Traditional axial type packages are less used today, but are still specified for point-to-point wiring and some traditional through-hole printed circuit boards. The most common form factor is the radial type (single ended), with both terminals on one side of the capacitor body. To facilitate automated insertion, radial plastic film capacitors are commonly constructed with terminal spacings at standardized distances, starting with 2.5 mm pitch and increasing in 2.5 mm steps. Radial capacitors are available potted in plastic cases, or dipped in an epoxy resin to protect the capacitor body against environmental influences. Although the transient heat of reflow soldering induces high stress in the plastic film materials, film capacitors able to withstand such temperatures are available in Surface Mounted Device“ (SMD) packages.\n\nBefore the introduction of plastic films, capacitors made by sandwiching a strip of impregnated paper between strips of metal, and rolling the result into a cylinder–paper capacitors–were commonly used; their manufacture started in 1876, and they were used from the early 20th century as decoupling capacitors in telecommunications (telephony).\n\nWith the development of plastic materials by organic chemists during the Second World War, the capacitor industry began to replace paper with thinner polymer films. One very early development in film capacitors was described in British Patent 587,953 in 1944. The introduction of plastics in plastic film capacitors was approximately in the following historic order: polystyrene (PS) in 1949, polyethylene terephthalate (PET/\"polyester\") and cellulose acetate (CA) in 1951, polycarbonate (PC/Lexan) in 1953, polytetrafluoroethylene (PTFE/Teflon) in 1954, polyparylene in 1954, polypropylene (PP) in 1954, polyethylene (PE) in 1958, and polyphenylene sulphide (PPS) in 1967. By the mid-1960s there was a wide range of different plastic film capacitors offered by many, mostly European and US manufacturers. German manufacturers such as WIMA, Roederstein, Siemens and Philips were trend-setters and leaders in a world market driven by consumer electronics.\n\nOne of the great advantages of plastic films for capacitor fabrication is that plastic films have considerably fewer defects than paper sheets used in paper capacitors. This allows the manufacture of plastic film capacitors with only a single layer of plastic film, whereas paper capacitors need a double layer of paper. Plastic film capacitors were significantly smaller in physical size (better volumetric efficiency), with the same capacitance value and the same dielectric strength as comparable paper capacitors. Then-new plastic materials also showed further advantages compared with paper. Plastic is much less hygroscopic than paper, reducing the deleterious effects of imperfect sealing. Additionally, most plastics are subject to fewer chemical changes over long periods, providing long-term stability of their electrical parameters. Since around 1980, paper and metallized paper capacitors (MP capacitors) have almost completely been replaced by PET film capacitors for most low-power DC electronic applications. Paper is now used only in RFI suppression or motor run capacitors, or as a mixed dielectric combined with polypropylene films in large AC and DC capacitors for high-power applications.\n\nAn early special type of plastic film capacitors were the cellulose acetate film capacitors, also called MKU capacitors. The polar insulating dielectric cellulose acetate was a synthetic resin that could be made for metallized capacitors in paint film thickness down to about 3 µm. A liquid layer of cellulose acetate was first applied to a paper carrier, then covered with wax, dried and then metallized. During winding of the capacitor body, the paper was removed from the metallized film. The remaining thin cellulose acetate layer had a dielectric breakdown of 63 V, enough for many of general purpose applications. The very small thickness of the dielectric decreased the overall dimensions of these capacitors compared to other film capacitors of the time. MKU film capacitors are no longer manufactured, because polyester film capacitors can now be produced in the smaller sizes that were the market niche of the MKU type.\n\nFilm capacitors have become much smaller since the beginning of the technology. Through development of thinner plastic films, for example, the dimensions of metallized polyester film capacitors were decreased by a factor of approximately 3 to 4.\n\nThe most important advantages of film capacitors are the stability of their electrical values over long durations, their reliability, and lower cost than some other types for the same applications. Especially for applications with high current pulse loads or high AC loads in electrical systems, heavy-duty film capacitors, here called \"power capacitors\", are available with dielectric ratings of several kilovolts.\n\nBut the manufacture of film capacitors does have a critical dependency on the materials supply chain. Each of the plastic film materials used for film capacitors worldwide is produced by only two or three large suppliers. The reason for this is that the mass quantities required by the market for film caps are quite small compared to typical chemical company production runs. This leads to a great dependency of the capacitor manufacturers on relatively few chemical companies as raw material suppliers. For example, in the year 2000 Bayer AG stopped their production of polycarbonate films, due to unprofitably low sales volumes. Most of the producers of polycarbonate film capacitors had to quickly change their product offerings to another type of capacitor, and a lot of expensive testing approvals for new designs were required.\n\nAs of 2012, only five plastic materials continued to be widely used in the capacitor industry as films for capacitors: PET, PEN, PP, PPS and PTFE. Other plastic materials are no longer in common use, either because they are no longer manufactured, or they have been replaced with better materials. Even the long-time manufactured polystyrene (PS) and polycarbonate (PC) film capacitors have been largely replaced by the previously-mentioned film types, though at least one PC capacitor manufacturer retains the ability to make its own films from raw polycarbonate feedstock. The less-common plastic films are described briefly here, since they are still encountered in older designs, and are still available from some suppliers.\n\nFrom simple beginnings film capacitors developed into a very broad and highly specialized range of different types. By the end of the 20th century mass production of most film capacitors had shifted to the Far East. A few large companies still produce highly specialized film capacitors in Europe and in the US, for power and AC applications.\n\nThe following table identifies the most commonly used dielectric polymers for film capacitors.\n\nAlso, different film materials can be mixed to produce capacitors with particular properties.\n\nThe most used film materials are polypropylene, with a market share of 50%, followed by polyester, with a 40% share. The remaining 10% share is accounted for by the other dielectric materials, including polyphenylene sulfide and paper, with roughly 3% each.\n\nPolycarbonate film capacitors are no longer manufactured because the dielectric material is no longer available.\n\nThe electrical characteristics, and the temperature and frequency behavior of film capacitors are essentially determined by the type of material that forms the dielectric of the capacitor. The following table lists the most important characteristics of the principal plastic film materials in use today. Characteristics of mixed film materials are not listed here.\n\nThe figures in this table are extracted from specifications published by various different manufacturers of film capacitors for industrial electronic applications.\n\nThe large range of values for the dissipation factor includes both typical and maximum specifications from data sheets of the various manufacturers. Typical electrical values for power and large AC capacitors were not included in this table.\n\nPolypropylene film capacitors have a dielectric made of the thermoplastic, non-polar, organic and partially crystalline polymer material Polypropylene (PP), trade name Treofan, from the family of polyolefins. They are manufactured both as metallized wound and stacked versions, as well as film/foil types. Polypropylene film is the most-used dielectric film in industrial capacitors and also in power capacitor types. The polypropylene film material absorbs less moisture than polyester film and is therefore also suitable for \"naked\" designs without any coating or further packaging. But the maximum temperature of 105 °C hinders use of PP films in SMD packaging.\n\nThe temperature and frequency dependencies of electrical parameters for polypropylene film capacitors are very low. Polypropylene film capacitors have a linear, negative temperature coefficient of capacitance of ±2,5 % within their temperature range. Therefore, polypropylene film capacitors are suitable for applications in Class 1 frequency-determining circuits, filters, oscillator circuits, audio circuits, and timers. They are also useful for compensation of inductive coils in precision filter applications, and for high-frequency applications.\n\nIn addition to the application class qualification for the film/foil version of PP film capacitors, the standard IEC/EN 60384-13 specifies three “stability classes”. These stability classes specify the tolerance on temperature coefficients together with the permissible change of capacitance after defined tests. They are divided into different temperature coefficient grades (α) with associated tolerances and preferred values of permissible change of capacitance after mechanical, ambient (moisture) and life time tests.\n\nThe table is not valid for capacitance values smaller than 50 pF.\n\nIn addition, PP film capacitors have the lowest dielectric absorption, which makes them suitable for applications such as VCO timing capacitors, sample-and-hold applications, and audio circuits. They are available for these precision applications in very narrow capacitance tolerances.\n\nThe dissipation factor of PP film capacitors is smaller than that of other film capacitors. Due to the low and very stable dissipation factor over a wide temperature and frequency range, even at very high frequencies, and their high dielectric strength of 650 V/µm, PP film capacitors can be used in metallized and in film/foil versions as capacitors for pulse applications, such as CRT-scan deflection circuits, or as so-called \"snubber\" capacitors, or in IGBT applications. In addition, polypropylene film capacitors are used in AC power applications, such as motor run capacitors or PFC capacitors.\n\nPolypropylene film capacitors are widely used for EMI suppression, including direct connection to the power supply mains. In this latter application, they must meet special testing and certification requirements concerning safety and non-flammability.\n\nMost power capacitors, the largest capacitors made, generally use polypropylene film as the dielectric. PP film capacitors are used for high-frequency high-power applications such as induction heating, for pulsed power energy discharge applications, and as AC capacitors for electrical distribution. The AC voltage ratings of these capacitors can range up to 400 kV.\n\nThe relatively low permittivity of 2.2 is a slight disadvantage, and PP film capacitors tend to be somewhat physically larger than other film caps.\n\nThe capacitor grade films are produced up to 20 µm in thickness with width of roll up to 140 mm. Rolls are carefully vacuum packed in pairs according to the specifications required for the capacitor.\n\nPolyester film capacitors are film capacitors using a dielectric made of the thermoplastic polar polymer material polyethylene terephthalate (PET), trade names Hostaphan or Mylar®, from the polyester family. They are manufactured both as metallized wound and stacked versions, as well as film/foil types. The polyester film adsorbs very little moisture, and this feature makes it suitable for \"naked\" designs without any further coating needed. They are the low-cost mass-produced capacitors in modern electronics, featuring relatively small dimensions with relatively high capacitance values. PET capacitors are mainly used as general purpose capacitors for DC applications, or for semi-critical circuits with operating temperatures up to 125  °C. The maximum temperature rating of 125 °C also allows SMD film capacitors to be made with PET films. The low cost of polyester and the relatively compact dimensions are the main reasons for the high prevalence of PET film capacitors in modern designs.\n\nThe small physical dimensions of PET film capacitors are the result of a high relative permittivity of 3.3, combined with a relatively high dielectric strength leads to a relatively high volumetric efficiency. This advantage of compactness comes with some disadvantages. The capacitance temperature dependence of polyester film capacitors is relatively high compared to other film capacitors, ±5% over the entire temperature range. The capacitance frequency dependence of polyester film capacitors compared with the other film capacitors is -3% in the range from 100 Hz to 100 kHz at the upper limit. Also, the temperature and frequency dependence of the dissipation factor are higher for polyester film capacitors compared with the other film capacitor types.\n\nPolyester film capacitors are mainly used for general purpose applications or semi-critical circuits with operating temperatures up to 125 °C.\n\nPolyethylene naphthalate film capacitors are film capacitors using a dielectric made of the thermoplastic biaxial polymer material polyethylene naphthalate (PEN), trade names Kaladex®, Teonex®. They are produced only as metallized types. PEN, like PET, belongs to the polyester family, but has better stability at high temperatures. Therefore, PEN film capacitors are more suitable for high temperature applications and for SMD packaging.\n\nThe temperature and frequency dependence of the electrical characteristics for capacitance and dissipation factor of PEN film capacitors are similar to the PET film capacitors. Because of the smaller relative permittivity and lower dielectric strength of the PEN polymer, PEN film capacitors are physically larger for a given capacitance and rated voltage value. In spite of this, PEN film capacitors are preferred over PET when the ambient temperature during operation of the capacitors is permanently above 125 °C. The special PEN \"high voltage\" (HV) dielectric offers excellent electrical properties during the life tests at high voltages and high temperatures (175 °C). PEN capacitors are mainly used for non-critical filtering, coupling and decoupling in electronic circuits, when the temperature dependencies do not matter.\n\nPolyphenylene sulfide film capacitors are film capacitors with dielectric made of the thermoplastic, organic, and partially crystalline polymer material Poly(p-phenylene sulfide) (PPS), trade name Torelina. They are only produced as metallized types.\n\nThe temperature dependence of the capacitance of PPS film capacitors over the entire temperature range is very small (± 1.5%) compared with other film capacitors. Also the frequency dependence in the range from 100 Hz to 100 kHz of the capacitance of the PPS film capacitors is ± 0.5%, very low compared with other film capacitors. The dissipation factor of PPS film capacitors is quite small, and the temperature and frequency dependence of the dissipation factor over a wide range is very stable. Only at temperatures above 100 °C does the dissipation factor increase to larger values. The dielectric absorption performance is excellent, behind only PTFE and PS dielectric capacitors.\n\nPolyphenylene sulfide film capacitors are well-suited for applications in frequency-determining circuits and for high-temperature applications. Because of their good electrical properties, PPS film capacitors are an ideal replacement for polycarbonate film capacitors, whose production since 2000 has been largely discontinued.\n\nIn addition to their excellent electrical properties, PPS film capacitors can withstand temperatures up to 270 °C without damaging the film quality, so that PPS film capacitors are suitable for surface mount devices (SMD), and can tolerate the increased reflow soldering temperatures for lead-free soldering mandated by the RoHS 2002/95/EC directive.\n\nCost of a PPS film capacitor is usually higher compared to a PP film capacitor.\n\nPolytetrafluoroethylene film capacitors are made with a dielectric of the synthetic fluoropolymer polytetrafluoroethylene (PTFE), a hydrophobic solid fluorocarbon. They are manufactured both as metallized and as film/foil types, although poor adherence to the film makes metallization difficult. PTFE is often known by the DuPont trademark Teflon.\n\nPolytetrafluoroethylene film capacitors feature a very high temperature resistance up to 200 °C, and even further up to 260 °C, with a voltage derating. The dissipation factor 2 • 10 is quite small. The change in capacitance over the entire temperature range of +1% to -3% is a little bit higher than for polypropylene film capacitors. However, since the smallest available film thickness for PTFE films is 5.5 µm, approximately twice of the thickness of polypropylene films, the PTFE film capacitors are physically bulkier than PP film capacitors. It added that the film thickness on the surface is not constant, so that Teflon films are difficult to produce.\n\nPTFE film capacitors are available with rated voltages of 100 V to 630 V DC. They are used in military equipment, in aerospace, in geological probes, in burn-in circuits and in high-quality audio circuits. Main producers of PTFE film capacitors are located in the USA.\n\nPolystyrene film capacitors, sometimes known as \"Styroflex Capacitors\", were well known for many years as inexpensive film capacitors for general purpose applications, in which high capacitance stability, low dissipation factor and low leakage currents were needed. But because the film thickness could be not made thinner than 10 µm, and the maximum temperature ratings reached only 85 °C, the PS film capacitors have mostly been replaced by polyester film capacitors as of 2012. However, some manufacturers may still offer PS film capacitors in their production program, backed by large amounts of polystyrene film stocked in their warehouse.\nPolystyrene capacitors have an important advantage - they have a temperature coefficient near zero and so are useful in tuned circuits where drift with temperature must be avoided.\n\nPolycarbonate film capacitors are film capacitors with a dielectric made of the polymerized esters of carbonic acid and dihydric alcohols polycarbonate (PC), sometimes given the trademarked name Makrofol. They are manufactured as wound metallized as well as film/foil types.\n\nThese capacitors have a low dissipation factor and because of their relatively temperature-independent electrical properties of about ±80 ppm over the entire temperature range, they had many applications for low-loss and temperature-stable applications such as timing circuits, precision analog circuits, and signal filters in applications with tough environmental conditions. PC film capacitors had been manufactured since the mid-1950s, but the main supplier of polycarbonate film for capacitors had ceased the production of this polymer in film form as of the year 2000. As a result, most of the manufacturers of polycarbonate film capacitors worldwide had to stop their production of PC film capacitors and changed to polypropylene film capacitors instead. Most of the former PC capacitor applications have found satisfactory substitutes with PP film capacitors.\n\nHowever, there are exceptions. The manufacturer Electronic Concepts Inc, (New Jersey, US) claims to be an in-house producer of its own polycarbonate film, and continues to produce PC film capacitors. In addition to this manufacturer of polycarbonate film capacitors, there are other mostly US-based specialty manufacturers.\n\nHistorically, the first “film” type capacitors were paper capacitors of film/foil configuration. They were fairly bulky, and not particularly reliable. As of 2012, paper is used in the form of metallized paper for MP capacitors with self-healing properties used for EMI suppression. Paper is also used as an insulating mechanical carrier of metallized-layer electrodes, and combined with polypropylene dielectric, mostly in power capacitors rated for high current AC and high voltage DC applications.\n\nPaper as carrier of the electrodes has the advantages of lower cost and somewhat better adherence of metallization to paper than to polymer films. But paper alone as dielectric in capacitors is not reliable enough for the growing quality requirements of modern applications. The combination of paper together with polypropylene film dielectric is a cost-effective way to improve quality and performance. The better adhering of metallization on paper is advantageous especially at high current pulse loads, and the polypropylene film dielectric increases the voltage rating.\n\nHowever, the roughness of a metallized paper surface can cause many small air-filled bubbles between the dielectric and the metallization, decreasing the breakdown voltage of the capacitor. For this reason, larger film capacitors or power capacitors using paper as carrier of the electrodes usually are filled with an insulating oil or gas, to displace the air bubbles for a higher breakdown voltage.\n\nHowever, since almost every major manufacturer offers its own proprietary film capacitors with mixed film materials, it is difficult to give a universal and general overview of the specific properties of mixed film capacitors.\n\nBesides the above-described films ((Polypropylene (PP), Polyethylene Terephthalate Polyester PET), Polyphenylene Sulfide (PPS), Polyethylene Naphthalate (PEN), Polycarbonate (PP), Polystyrene (PS) and Polytetrafluoroethylene (PTFE)), some other plastic materials may be used as the dielectric in film capacitors. Thermoplastic polymers such as Polyimide (PI), Polyamide (PA, better known as Nylon or Perlon), Polyvinylidene fluoride (PVDF), Siloxane, Polysulfone (PEx) and Aromatic Polyester (FPE) are described in the technical literature as possible dielectric films for capacitors. The primary reason for considering new film materials for capacitors is the relative low permittivity of commonly used materials. With a higher permittivity, film capacitors could be made even smaller, an advantage in the market for more-compact portable electronic devices.\n\nIn 1984, a new film capacitor technology that makes use of vacuum-deposited electron-beam cross-linked acrylate materials as dielectric in film capacitors was announced as a patent in the press. But as of 2012, only one manufacturer markets a specific acrylate SMD film capacitor, as an X7R MLCC replacement.\n\nPolyimide (PI), a thermoplastic polymer of imide monomers, is proposed for film capacitors called Polyimide-, PI- or Kapton capacitors. Kapton is the trade name of polyimide from DuPont. This material is of interest because its high temperature resistance up to 400 °C. But as of 2012, no specific PI capacitor series film capacitors have been announced. The offered film capacitor, Kapton CapacitorCL11, announced from “dhgate” is a “Type: Polypropylene Film Capacitor”. Another very strange Kapton capacitor can be found at YEC, a Chinese producer of capacitors. Here the announced “Kapton capacitors” are in reality supercapacitors, a completely different technology Perhaps the Kapton film in these supercapacitors is used as a separator between the electrodes of this double-layer capacitor. Kapton films are often offered as an adhesive film for the outer insulation of capacitor packages.\n\nPolyvinylidene fluoride (PVDF) has a very high permittivity of 18 to 20, which allows large amounts of energy to be stored in a small space (volumetric efficiency). However, it has a Curie temperature of only 60 °C, which limits its usability. Film capacitors with PVDF are described for one very special application, in portable defibrillators.\n\nFor all the other previously named plastic materials such as PA, PVDF, Siloxane, PEx or FPE, specific series of film capacitors with these plastic films are not known to be produced in commercial quantities, as of 2012.\n\nThe standardization for all electrical, electronic components and related technologies follows the rules given by the International Electrotechnical Commission (IEC), a non-profit, non-governmental international standards organization. The IEC standards are harmonized with European standards EN.\n\nThe definition of the characteristics and the procedure of the test methods for capacitors for use in electronic equipment are set out in the generic specification:\n\nThe tests and requirements to be met by film capacitors for use in electronic equipment for approval as standardized types are set out in the following sectional specifications:\n\nThe standardization of power capacitors is strongly focused on rules for the safety of personnel and equipment, given by the local regulating authority. The concepts and definitions to guarantee safe application of power capacitors are published in the following standards:\n\nThe text above is directly extracted from the relevant IEC standards, which use the abbreviations \"d.c.\" for Direct Current (DC) and \"a.c.\" for Alternating Current (AC).\n\nDuring the early development of film capacitors, some large manufacturers have tried to standardize the names of different film materials. This resulted in a former German standard (DIN 41 379), since withdrawn, in which an abbreviated code for each material and configuration type were prescribed. Many manufacturers continue to use these de facto standard abbreviations.\n\nHowever, with the relocation of mass-market business in the passive components industry, which includes film capacitors, many of the new manufacturers in the Far East use their own abbreviations that differ from the previously established abbreviations.\n\nThe manufacturers Wima, Vishay and TDK Epcos specify the electrical parameters of their film capacitors in a general technical information sheet.\n\nThe electrical characteristics of capacitors are harmonized by the international generic specification IEC/EN 60384-1. In this standard, the electrical characteristics of capacitors are described by an idealized series-equivalent circuit with electrical components which model all ohmic losses, capacitive and inductive parameters of a film capacitor:\n\n\nThe two reactive resistances have following relations with the angular frequency “ω”:\n\n\nThe rated capacitance is the value for which the capacitor has been designed. The actual capacitance of film capacitors depends on the measuring frequency and the ambient temperature. Standardized conditions for film capacitors are a measuring frequency of 1 kHz and a temperature of 20 °C. The percentage of allowed deviation of the capacitance from the rated value is called capacitance tolerance. The actual capacitance value of a capacitor should be within the tolerance limits, or the capacitor is out of specification.\n\nFilm capacitors are available in different tolerance series, whose values are specified in the E series standards specified in IEC/EN 60063. For abbreviated marking in tight spaces, a letter code for each tolerance is specified in IEC/EN 60062.\n\n\nThe required capacitance tolerance is determined by the particular application. The narrow tolerances of E24 to E96 will be used for high-quality circuits like precision oscillators and timers. On the other hand, for general applications such as non-critical filtering or coupling circuits, the tolerance series E12 or E6 are sufficient.\n\nThe different film materials have temperature- and frequency-dependent differences in their characteristics. The graphs below show typical temperature and frequency behavior of the capacitance for different film materials.\n\nThe rated DC voltage V is the maximum DC voltage, or peak value of pulse voltage, or the sum of an applied DC voltage and the peak value of a superimposed AC voltage, which may be applied continuously to a capacitor at any temperature between the category temperature and the rated temperature.\n\nThe breakdown voltage of film capacitors decreases with increasing temperature. When using film capacitors at temperatures between the upper rated temperature and the upper category temperature, only a temperature-derated category voltage V is allowed. The derating factors apply to both DC and AC voltages. Some manufacturers may have quite different derating curves for their capacitors compared with the generic curves given in the picture at the right.\n\nThe allowable peak value of a superimposed alternating voltage, called the \"rated ripple voltage\", is frequency-dependent. The applicable standards specify the following conditions, regardless of the type of dielectric film.\n\nFilm capacitors are not polarized and are suitable for handling an alternating voltage. Because the rated AC voltage is specified as an RMS value, the nominal AC voltage must be smaller than the rated DC voltage. Typical figures for DC voltages and nominally related AC voltages are given in the table below:\n\nAn AC voltage will cause an AC current (with an applied DC bias this is also called \"ripple current\"), with cyclic charging and discharging of the capacitor causing oscillating motion of the electric dipoles in the dielectric. This results in dielectric losses, which are the principal component of the ESR of film capacitors, and which produce heat from the alternating current. The maximum RMS alternating voltage at a given frequency which may be applied continuously to a capacitor (up to the rated temperature) is defined as the rated AC voltage U. Rated AC voltages usually are specified at the mains frequency of a region (50 or 60 Hz).\n\nThe rated AC voltage is generally calculated so that an internal temperature rise of 8 to 10 °K sets the allowed limit for film capacitors. These losses increase with increasing frequency, and manufacturers specify curves for derating maximum AC voltages permissible at higher frequencies.\n\nCapacitors, including film types, designed for continuous operation at low-frequency (50 or 60 Hz) mains voltage, typically between line and neutral or line and ground for interference suppression, are required to meet standard safety ratings; e.g., X2 is designed to operate between line and neutral at 200-240 VAC, and Y2 between line and ground. These types are designed for reliability, and, in case of failure, to fail safely (open-, rather than short-circuit). A non-catastrophic failure mode in this application is due to the corona effect: the air enclosed in the winding element becomes ionized and consequently more conductive, allowing partial discharges on the metallized surface of the film, which causes local vaporization of the metallization. This occurs repeatedly, and can cause significant loss of capacitance (C-decay) over one or two years. International standard IEC60384-14 specifies a limit of 10% C-decay per 1,000 test hours (41 days of permanent connection). Some capacitors are designed to minimise this effect. One method, at the expense of increased size and cost, is for a capacitor operating at 200-240 VAC to consist internally of two parts in series, each at a voltage of 100-120 VAC, insufficient to cause ionisation. Manufacturers also adopt cheaper and smaller construction intended to avoid corona effect without series-connected sections, for example minimising enclosed air.\n\nFor metallized film capacitors, the maximum possible pulse voltage is limited because of the limited current-carrying capacity between contact of the electrodes and the electrodes themselves. The rated pulse voltage V is the peak value of the pulse voltage which may be applied continuously to a capacitor at the rated temperature and at a given frequency. The pulse voltage capacity is given as pulse voltage rise time dV/dT in V/µs and also implies the maximum pulse current capacity. The values on the pulse rise time refer to the rated voltage. For lower operating voltages, the permissible pulse rise times may decrease. The permissible pulse load capacity of a film capacitor is generally calculated so that an internal temperature rise of 8 to 10 °K is acceptable.\n\nThe maximum permissible pulse rise time of film capacitors which may be applied within the rated temperature range is specified in the relevant data sheets. Exceeding the maximum specified pulse load can lead to the destruction of the capacitor.\n\nFor each individual application, the pulse load must be calculated. A general rule for calculating the power handling of film capacitors is not available because of vendor-related differences stemming from the internal construction details of different capacitors. Therefore, the calculation procedure of the manufacturer WIMA is referenced as an example of the generally applicable principles.\n\nThe impedance formula_4 is the complex ratio of the voltage to the current in an alternating current (AC) circuit at a given frequency.\n\nIn data sheets of film capacitors, only the \"magnitude\" of the impedance |Z| will be specified, and simply written as “Z”. The \"phase\" of the impedance is specified as dissipation factor formula_5.\n\nIf the series-equivalent values of a capacitor formula_6 and formula_2 and formula_3, and the frequency are known, then the impedance can be calculated with these values. The impedance formula_9 is then the sum of the geometric (complex) addition of the real and the reactive resistances.\n\nIn the special case of resonance, in which the both reactive resistances formula_2 and formula_3 have the same value (formula_14), then the impedance will only be determined by formula_6.\n\nThe impedance is a measure of the ability of the capacitor to pass alternating currents. The lower the impedance, the more easily alternating currents can be passed through the capacitor. Film capacitors are characterized by very small impedance values and very high resonant frequencies, especially when compared to electrolytic capacitors.\n\nThe equivalent series resistance (ESR) summarizes all resistive losses of the capacitor. These are the supply line resistances, the contact resistance of the electrode contact, the line resistance of the electrodes and the dielectric losses in the dielectric film. The largest share of these losses is usually the dissipative losses in the dielectric.\n\nFor film capacitors, the dissipation factor tan \"δ\" will be specified in the relevant data sheets, instead of the ESR. The dissipation factor is determined by the tangent of the phase angle between the capacitive reactance \"X\" minus the inductive reactance \"X\" and the \"ESR\".\n\nIf the inductance \"ESL\" is small, the dissipation factor can be approximated as:\n\nThis reason for using the dissipation factor instead of the ESR is, that film capacitors were originally used mainly in frequency-determining resonant circuits. The reciprocal value of the dissipation factor is defined as the quality factor “Q”. A high Q value is for resonant circuits a mark of the quality of the resonance.\n\nThe dissipation factor for film/foil capacitors is lower than for metallized film capacitors, due to lower contact resistance to the foil electrode compared to the metallized film electrode.\n\nThe dissipation factor of film capacitors is frequency-, temperature- and time-dependent. While the frequency- and temperature-dependencies arise directly from physical laws, the time dependence is related to aging and moisture adsorption processes.\n\nA charged capacitor discharges over time through its own internal insulation resistance R. The multiplication of the insulation resistance together with the capacitance of the capacitor results in a time constant which is called the “self-discharge time constant”: (τ = R•C). This is a measure of the quality of the dielectric with respect to its insulating properties, and is dimensioned in seconds. Usual values for film capacitors range from 1000 s up to 1,000,000 s. These time constants are always relevant if capacitors are used as time-determining elements (such as timing delay), or for storing a voltage value as in sample-and-hold circuits or integrators.\n\nDielectric absorption is the name given to the effect by which a capacitor that has been charged for a long time discharges only incompletely when briefly discharged. It is a form of hysteresis in capacitor voltages. Although an ideal capacitor would remain at zero volts after being discharged, real capacitors will develop a small residual voltage, a phenomenon that is also called \"soakage\".\n\nThe following table lists typical values of the dielectric absorption for common film materials\n\nPolypropylene film capacitors have the lowest voltage values generated by dielectric absorption. Therefore, they are ideally suited for precision analog circuits, or for integrators and sample-and-hold circuits.\n\nFilm capacitors are subject to certain very small but measurable aging processes. The primary degradation process is a small amount of plastic film shrinkage, which occurs mainly during the soldering process, but also during operation at high ambient temperatures or at high current load. Additionally, some moisture absorption in the windings of the capacitor may take place under operating conditions in humid climates.\n\nThermal stress during the soldering process can change the capacitance value of leaded film capacitors by 1% to 5% from initial value, for example. For surface mount devices, the soldering process may change the capacitance value by as much as 10%. The dissipation factor and insulation resistance of film capacitors may also be changed by the above-described external factors, particularly by moisture absorption in high humidity climates.\n\nThe manufacturers of film capacitors can slow the aging process caused by moisture absorption, by using better encapsulation. This more expensive fabrication processing may account for the fact that film capacitors with the same basic body design can be supplied in different life time stability ratings called Performance grades. Performance grade 1 capacitors are \"longlife\", Performance grade 2 capacitors are \"general purpose\" capacitors. The specifications behind this grades are defined in the relevant standard of IEC/EN 60384-x (see standards).\n\nThe permissible changes of capacitance, dissipation factor and insulation resistance vary with the film material, and are specified in the relevant data sheet. Variations over the course of time which exceed the specified values are considered as a degradation failure.\n\nFilm capacitors generally are very reliable components with very low failure rates, with predicted life expectancies of decades under normal conditions. The life expectancy for film capacitors is usually specified in terms of applied voltage, current load, and temperature.\n\nColor-coded film capacitors have been produced, but it is usual to print more detailed information on the body. According to the IEC standard 60384.1, capacitors should be marked with imprints of the following information:\n\n\nMains-voltage RFI suppression capacitors must also be marked with the appropriate safety agency approvals.\n\nCapacitance, tolerance, and date of manufacture can be marked with short codes. Capacitance is often indicated with the sub-multiple indicator replacing an easily erased decimal point, as: n47 = 0.47 nF, 4n7 = 4.7 nF, 47n = 47 nF\n\nIn comparison with the other two main capacitor technologies, ceramic and electrolytic capacitors, film capacitors have properties that make them particularly well suited for many general-purpose and industrial applications in electronic equipment.\nTwo main advantages of film capacitors are very low ESR and ESL values. Film capacitors are physically larger and more expensive than aluminum electrolytic capacitors (e-caps), but have much higher surge and pulse load capabilities. As film capacitors are not polarized, they can be used in AC voltage applications without DC bias, and they have much more stable electrical parameters. Polypropylene film capacitors have relatively little temperature dependence of capacitance and dissipation factor, so that they can be applied in frequency-stable Class 1 applications, replacing Class 1 ceramic capacitors.\n\nPolypropylene film capacitors meet the criteria for stability Class 1 capacitors, and have low electrical losses and nearly linear behavior over a very wide temperature and frequency range. They are used for oscillators and resonant circuits; for electronic filter applications with high quality factor (Q) such as high-pass filters, low-pass filters and band-pass filters as well as for tuning circuits; for audio crossovers in loudspeakers; in sample and hold A/D converters and in peak voltage detectors. Tight capacitance tolerances are required for timing applications in signal lights or pulse width generators to control the speed of motors, PP film capacitors are also well-suited because of their very low leakage current.\n\nClass 1 PP film capacitors are able to handle higher current than stability Class 1 ceramic capacitors. The precise negative temperature characteristics of polypropylene make PP capacitors useful to compensate for temperature-induced changes in other components.\n\nFast pulse rise time rating, high dielectric strength (breakdown voltage), and low dissipation factor (high Q) are the reasons for the use of polypropylene film capacitors in fly-back tuning and S-correction applications in older CRT tube television and display equipment. For similar reasons, PP film capacitors, often in versions with special terminals for high peak currents, work well as snubbers for power electronic circuits. Because of their high pulse surge capabilities, PP capacitors are suitable for use in applications where high-current pulses are needed, such as in time-domain reflectometer (TDR) cable fault locators, in welding machines, defibrillators, in high-power pulsed lasers, or to generate high-energy light or X-ray flashes.\n\nIn addition, polypropylene film capacitors are used in many AC applications such as phase shifters for power factor correction in fluorescent lamps, or as a motor-run capacitors.\n\nFor simple higher-frequency filter circuits, or in voltage regulator or voltage doubler circuits, low-cost metallized polyester film capacitors provide long-term stability, and can replace more-expensive tantalum capacitors. Because capacitors pass AC signals but block DC, film capacitors with their high insulation resistance and low self-inductance are well-suited as signal coupling capacitors for higher frequencies. For similar reasons, film capacitors are widely used as decoupling capacitors to suppress noise or transients.\n\nFilm capacitors made with lower-cost plastics are used for non-critical applications which do not require ultra-stable characteristics over a wide temperature range, such as for smoothing or AC signal coupling. Polyester film (KT) capacitors of the \"stacked\" type are often used now instead of polystyrene capacitors (KS), which have become less available.\n\nMetallized film capacitors have self-healing properties, and small imperfections do not lead to the destruction of the component, which makes these capacitors suitable for RFI/EMI suppression capacitors with fault protection against electrical shock and flame propagation, although repeated corona discharges which self-heal can lead to significant loss of capacitance.\n\nPTFE film capacitors are used in applications that must withstand extremely high temperatures. such as in military equipment, in aerospace, in geological probes, or burn-in circuits.\n\n<gallery caption=\"RFI/EMI suppression capacitors\" class=\"centered\">\nFile:MP3-X2-P1180582b.JPG|Metallized paper RFI suppression capacitors (MP3) with safety marks for “X2” safety standard\nFile:MKP Capacitor P10705333b.jpg|Metallized polypropylene RFI suppression capacitor (MKP) for \"X2\" safety standard\nFile:Entstoerkondensator xy IMGP5363.jpg|Combined XY-RFI suppression capacitor\n</gallery>\nElectromagnetic Interference (EMI) or Radio Frequency Interference (RFI) suppression film capacitors, also known as \"AC line filter safety capacitors\" or \"Safety capacitors\", are used as crucial components to reduce or suppress electrical noise caused by the operation of electrical or electronic equipment, while also limited providing protection against electrical shocks.\n\nA suppression capacitor is an effective interference reduction component because its electrical impedance decreases with increasing frequency, so that at higher frequencies they short circuit electrical noise and transients between the lines, or to ground. They therefore prevent equipment and machinery (including motors, inverters, and electronic ballasts, as well as solid-state relay snubbers and spark quenchers) from sending and receiving electromagnetic and radio frequency interference as well as transients in across-the-line (X capacitors) and line-to-ground (Y capacitors) connections. X capacitors effectively absorb symmetrical, balanced, or differential interference. On the other hand, Y capacitors are connected in a line bypass between a line phase and a point of zero potential, to absorb asymmetrical, unbalanced, or common-mode interference.\n<gallery widths=\"280\" heights=\"140\" caption=\"RFI/EMI suppression with X- and Y-capacitors for equipment without and with additional safety insulation\">\nFile:Safety caps-Appliance Class I.svg|Appliance Class I capacitor connection\nFile:Safety caps-Appliance Class II.svg|Appliance Class II capacitor connection\n</gallery>\nEMI/RFI suppression capacitors are designed and installed so that remaining interference or electrical noise does not exceed the limits of EMC directive EN 50081 Suppression components are connected directly to mains voltage semi-permanently for 10 to 20 years or more, and are therefore exposed to overvoltages and transients which could damage the capacitors. For this reason, suppression capacitors must comply with the safety and inflammability requirements of international safety standards such as the following:\n\n\nRFI capacitors which fulfill all specified requirements are imprinted with the certification mark of various national safety standards agencies. For power line applications, special requirements are placed on the inflammability of the coating and the epoxy resin impregnating or coating the capacitor body. To receive safety approvals, X and Y powerline-rated capacitors are destructively tested to the point of failure. Even when exposed to large overvoltage surges, these safety-rated capacitors must fail in a fail-safe manner that will not endanger personnel or property.\n\nMost EMI/RFI suppression film capacitors are metallized polypropylene film capacitors. However, some types of metallized paper capacitors (MP) are still used for this application, because they still have some advantages in flame resistance.\n\nA lighting ballast is a device to provide proper starting and operating electrical conditions to light one or more fluorescent lamps, while also limiting the amount of current. A familiar and widely used example is the traditional inductive ballast used in fluorescent lamps, to limit the current through the tube, which would otherwise rise to destructive levels due to the tube's negative resistance characteristic. A disadvantage of using an inductor is that current is shifted out of phase with the voltage, producing a poor power factor.\n\nModern electronic ballasts usually change the frequency of the power from a standard mains frequency of 50 or 60  Hz up to 40  kHz or higher, often using a Switched Mode Power Supply (SMPS) circuit topology with Power Factor Correction (PFC). First the AC input power is rectified to DC, and then it is chopped at a high frequency to improve the power factor. In more expensive ballasts, a film capacitor is often paired with the inductor to correct the power factor. In the picture at right, the flat grey rectangular component in the middle of the ballast circuit is a polyester film capacitor used for PFC.\n\nSnubber capacitors are designed for the high peak current operation required for protection against transient voltages. Such voltages are caused by the high \"di/dt\" current slew rate generated in switching power electronics applications.\n\nSnubbers are energy-absorbing circuits used to eliminate voltage spikes caused by circuit inductance when a switch opens. The purpose of the snubber is to improve electromagnetic compatibility (EMC) by eliminating the voltage transient that occurs when a switch abruptly opens, or by suppressing sparking of switch contacts (such as an automotive ignition coil with mechanical interrupter), or by limiting the voltage slew rate of semiconductor switches like thyristors, GTO thyristors, IGBTs and bipolar transistors. Snubber capacitors (or higher power \"damping capacitors\") require a very low self-inductance and very low ESR capacitor construction. These devices are also expected to be highly reliable because, if the snubber RC circuitry fails, a power semiconductor will be destroyed in most cases.\n\nSnubber circuits usually incorporate film capacitors, mostly polypropylene film caps. The most important criteria for this application are a low self-inductance, low ESR, and very high peak current capability. The so-called \"snubber” capacitors sometimes have some additional special construction features. The self-inductance is reduced by slimmer designs with narrower width of the electrodes. By double-sided metallization or the film/foil construction of the electrodes, the ESR also can be reduced, increasing the peak current capability. Specially widened terminals which can be mounted directly beneath semiconductor packages can help to increase current handling and decrease inductance.\n\nThe most popular simple snubber circuit consists out of a film capacitor and a resistor in series, connected in parallel with a semiconductor component to suppress or damp undesirable voltage spikes. The capacitor absorbs the inductive turn-off peak current temporarily, so that the resulting voltage spike is limited. But the trend in modern semiconductor technology is towards higher power applications, which increases the peak currents and switching speeds. In this case, the boundary between a standard electronic film capacitor and a power capacitor is blurred, so that larger snubber capacitors belong more in the area of power systems, electrical installations and plants.\n\nThe overlapping categories of film and power capacitors are visible when they are applied as snubber capacitors in the growing market for high power electronics with IGBT’s and thyristors. Although the power capacitors use polypropylene film, like the smaller snubber film capacitors, they belong to the family of power capacitors, and are called “damping” capacitors.\n\n \n\nThe relatively simple fabrication technique of winding gives film capacitors the possibility of attaining even very large sizes for applications in the high power range, as so-called \"power capacitors\". Although the materials and the construction of power capacitors are mostly similar to the smaller film capacitors, they are specified and marketed differently for historical reasons.\n\nThe \"film capacitors\" were developed together with the growing market of broadcast and electronic equipment technology in the mid-20th century. These capacitors are standardized under the rules of IEC/EN 60384-1 “Capacitors for use in electronic equipment” and different \"film materials\" have their own sub standards, the IEC/EN 60384-\"n\" series. The \"power capacitors\" begin at a power handling capacity of approximately 200 volt-amps, such as for ballast capacitors in fluorescent lamps. The standardization of power capacitors follows the rules of IEC/EN 61071 and IEC/EN 60143-1, and have for various different applications their own sub standards, such as for railway applications.\n\nPower capacitors can be used for a wide variety of applications, even where extremely non-sinusoidal voltages and pulsed currents are present. Both AC and DC capacitors are available. AC capacitors serve as damping or snubbing capacitors when connected in series with a resistor, and are also specified for the damping of undesirable voltage spikes caused by the so-called charge carrier storage effect during switching of power semiconductors. In addition, AC capacitors are used in low-detuned or close-tuned filter circuits for filtering or absorbing harmonics. As pulse discharge capacitors, they are useful in applications with reversing voltages, such as in magnetizing equipment.\n\nThe scope of application for DC capacitors is similarly diverse. Smoothing capacitors are used to reduce the AC component of fluctuating DC voltage (such as in power supplies for radio and television transmitters), and for high voltage testing equipment, DC controllers, measurement and control technology and cascaded circuits for generation of high DC voltage. Supporting capacitors, DC-filter or buffer circuit capacitors are used for energy storage in intermediate DC circuits, such as in frequency converters for poly-phase drives, and transistor and thyristor power converters. They must be able to absorb and release very high currents within short periods, the peak values of currents being substantially greater than the RMS values.\n\nSurge (pulse) discharge capacitors are also capable of supplying or absorbing extreme short-duration current surges. They are usually operated in discharge applications with non-reversing voltages, and at low repetition frequencies, such as in laser technology and lighting generators.\n\nPower capacitors can reach quite large physical dimensions. Rectangular housings with internally interconnected individual capacitors can reach sizes of L×W×H = (350×200×1000) mm and above.\n\n\n\n\nThis article draws heavily on a corresponding article in the , accessed in the version of 12 March 2012.\n"}
{"id": "34957804", "url": "https://en.wikipedia.org/wiki?curid=34957804", "title": "Ford F-Series (ninth generation)", "text": "Ford F-Series (ninth generation)\n\nThe ninth generation Ford F-Series is a line of full-size and medium-duty commercial trucks that were produced by Ford from 1991 to 1997. While still based on the basic design dating from late 1979 (for the 1980 model year), the 1992 F-Series brought a number of minor changes to the exterior and interior (where most enthusiasts consider this a facelift for the same existing truck that first appeared in 1979 as a 1980 model instead of a redesign). This is the last generation of the F-Series that was produced as a complete range of trucks from a half-ton pickup (F-150) to a medium-duty Class 6 truck (F-250 and above). As this generation was replaced during the 1997–1998 model years, the larger models of the F-Series (F-250 and above) were split from the F-150; these became the Ford Super Duty trucks, related to the latter with a few powertrain components.\n\nIn the interest of aerodynamics, the lines of the hood, front fenders, and grille were rounded off for 1992. Along with the larger grille, the headlights were enlarged (with the turn signals again moving below). Inside, the interior was updated with a redesigned dashboard along with new seats. Extended-cab (SuperCab) models received larger rear side windows. A notable change included the reintroduction of the Flareside bed that returned for production since 1987. Instead of the previous classic-style bed, the Flareside bed was now a narrow-body version of the dual rear-wheel bed; the rear fenders were repositioned to fit the width of the cab.\n\nThe 1994 models brought a slightly updated dashboard and the addition of a standard driver's-side airbag on F150s only, Center high mount stop lamp (CHMSL) third brake light/cargo light, brake-shift interlock and CFC-free air conditioning. New options for 1994 included remote keyless entry with alarm, a compact disc player fitted into the regular stereo system, and a power driver's seat; an electrochromic inside rear view mirror was also offered for 1994 and 1995 as part of a luxury light package.\n\nFord trailed their rival General Motors in combined truck sales for much of the ninth generation, though sales steadily rose each year. 500,000 F-Series trucks were sold in 1992, but this rose to nearly 800,000 by 1996. Meaning that Ford had overtaken the combined Truck sales of Chevrolet and GMC for the first time in a decade.\nIn the second half of 1997 The F-250 HD (Heavy Duty) was in the same series as the F-350.\n\n\nThe monochromatic \"Nite\" package introduced in 1990 continued, but was dropped at the end of the 1992 model year. As before, it featured an all-black exterior with either a pink or blue/purple stripe and \"Nite\" decal on the sides of the cargo box.\n\nFor 1993 the Custom model was dropped, as the XL became the new base model. Following the lead of the Aerostar, Ford Bronco, and Explorer, the \"Eddie Bauer\" trim line — featuring plusher trim and increased standard features — was reintroduced for 1995.\n\nIn 1993, the \"SVT Lightning\" was introduced, slotting itself in between the Chevrolet 454SS and GMC Syclone. Ford Special Vehicles Team upgraded the Lightning from the regular F-series with heavy-duty suspension and brakes. Powertrain upgrades came from heavy-duty trucks, with a 240 hp version of the 5.8L V8 and the E4OD overdrive transmission normally paired with the 7.3L diesel and 460 7.5L V8s.\n\nThe F-150, F-250, F-250 HD, F-350, and F-Super Duty were available in many different configurations from chassis cab base models, up to XLT trimmed models with their chrome and plush seating. The trucks came with a variety of gas and diesel engines. The F-150 could be had with one of 4 gas engines, the 4.9L (300 cid) I6, the 5.0L (302 cid) V8, and the 5.8L (351 cid) V8. The same gas engine options were also available for the heavy duty trucks along with the 7.5L (460 cid), or you could opt for the 7.3L (444 cid) diesel. The first version was the 7.3L IDI (Indirect Injected) V8 (1992 - 1993.5), which was produced by International Harvester. In 1993.5-1994.5 a turbocharged 7.3L IDI V8 with stronger internals was offered as emissions, power, and torque demands were increasing. In the second half of 1994, the new Direct Injected 7.3L Power Stroke V8 Turbodiesel replaced the 7.3L IDI V8. Also built by International Harvester, and used in International Trucks, known as the T444 E Engine. The F-250 HD was available from 1996 to 1997, and differed from the earlier F-250 in the trim that was on it, and it also had a transmission cooler like the later Ford Super Duty trucks, it also received a heavier rear axle, and heavier springs and shocks. Because of these changes in design, service technicians started to refer to the first powerstrokes as an OBS or Old Body Style to avoid confusion from the similar Super Duty 7.3 Power Stroke parts. As part of the 4x4 offroad package, they were available with several skid plates underneath. (After 1997, the heavier-duty models were splitting from the Ford F-150. The line of trucks were called the 1999 Ford Super Duty.) The F-150 could be had with either short (6.5') or long (8') beds with either regular or extended cab. The F-250 and F-350 trucks were only available with longbeds (8 ft.) except in 1996 and 1997, the F-250 HD could be had in an extended cab/shortbed or crew cab/shortbed model. The crew cab shortbed and extended cab shortbed trucks are very rare, as they were only produced for little over a year. Production of these trucks stopped in Fall of 1997.\n\nFor the 1994.5 model year, the exterior of the medium-duty F-Series was changed for the first time since 1979. Available only as a tilting cowl, the new hood featured a much larger grille. Instead of being mounted on the fenders, the turn signals were now mounted beside the headlights. Instead of the model designation, the cowl badge was replaced by an \"F-Series\" one. Inside, the interior was largely carried over from 1980.\n\nWhile still available with a 7.0L gasoline V8, many medium-duty F-Series were diesel-powered. Instead of the Navistar T444E V8 engine seen in the F-250/F-350, the medium-duty models offered two inline-6 engines (the Caterpillar 3126 and the Cummins 6BT/ISB). In 1999, these trucks were discontinued, along with the B-Series bus chassis; while the bus chassis was not replaced, Ford re-entered the medium-duty truck market with the Ford F-650/750. Built in a joint venture with Navistar, they were integrated into the Super Duty lineup.\nThe 1992 redesign left the powertrain lineup from the previous generation; the gasoline lineup of the 4.9L Inline-6, 5.0L and 5.8L Windsor V8s, and the 7.5L Big Block V8 were all carried over. A 1993 model year option, the 7.3L International IDI diesel V8 gained a turbocharger for the first time. The 1994 model year engine lineup received a retune to increase output. During the 1994 model year, the IDI diesel V8 was replaced by the T444E V8. Dubbed the Powerstroke by Ford, the new diesel was again supplied by Navistar International. Despite sharing identical displacement with its IDI predecessor, the turbocharged Powerstroke/T444E was an all-new design with direct fuel injection. One thing worth noting is that the 7.3 Powerstroke was the second diesel motor with electronic fuel injection to be put into a light duty truck. The GM 6.5L TurboDiesel with the Stanadyne DS-4 injection pump was the first, appearing in 1992. The Dodge Ram did not offer EFI in its diesel engines until 1998.\n\nAs before, the 5.0L V8 was not offered above 8500 GVWR and the F-Superduty was 7.5 and diesel only. The diesels and 7.5 were above 8500 GVWR only (F250HD and heavier.) The 4.9 was available in the F350 through 1996 as a delete option.\n\nThe 4wd F150 continued the use of the Dana 44 Twin-Traction Beam axle from the 80–91 trucks, and the Ford 8.8\" Rear Straight axle. The 4wd F250 carried the Dana 50 Twin Traction Beam axle, the Sterling 10.25 from the previous generation for the rear; full float on heavy duty 3/4 ton trucks and the 4wd F350 used the Dana 60 Straight Axle front, and the Sterling 10.25\" rear Straight axle.\n\nThe SVT Lightning is a sports/performance version of the F-150, released by Ford's SVT (Special Vehicle Team) division.\nFord introduced the Lightning in 1992 to compete with primarily the Chevrolet 454 SS, in an effort to enhance the sporty, personal-use image of the Ford F-Series pickup. This initial Lightning featured performance handling developed by world-champion driver Jackie Stewart. The Lightning was powered by a special version of the V8 engine. The Lightning shared its basic platform structure with the regular F-150, but modifications were made to many vehicle systems. To enhance the Lightning chassis, the thicker frame rails from the 4-wheel drive F-250 used to increase rigidity. The stock Lightning was capable of achieving 0.88 g lateral acceleration, yet it retained almost all the hauling and trailering capabilities of the parent F-Series. A \"Windsor\" V8 producing and of torque replaced the standard F-150 engine. The engine was based on an existing block, but Ford engineers fitted it with high flow rate \"GT40\" heads and used hypereutectic pistons to increase response, output and durability. The engine was also fitted with stainless steel \"shorty\" headers.\n\nAn upgraded Ford E4OD automatic transmission was the only available transmission. An aluminum driveshaft connected it to 4.10:1 rear limited slip gears. The suspension had front and rear anti-rollbars and a special leaf, in the rear, tipped with a rubber snubber, that acted as a ladder bar and controlled rear wheel hop during hard acceleration. Special 17\" aluminum wheels with Firestone Firehawk tires, Lightning badging, a front air dam with fog lamps, a speedometer and a special intake manifold all differentiated the Lightning from normal F-150s. Bucket seats with electrically-adjustable side bolsters and lumbar supports were part of the package. Suspension modifications provided a 1 in front and 2.5 in rear drop in ride height.\n\nThe 1993 Lightning, launched on 15 December 1992 by Ford President Ed Hagenlocker, received more than 150 favorable articles in America's newspapers, magazines, and television outlets, and helped Ford retain leadership in the personal-use truck market. Three-time World Champion driver Jackie Stewart was highly involved in fine-tuning of the Lightning's handling. Key Ford engineers, managers, and executives involved in developing the original Lightning Performance Truck were Jim Mason, Robert Burnham, Jim Englehart, Terry DeJonckheere, Rory Carpenter, Bob Hommel, Terrell Edgar, Dick Liljestrand, Deb Neill, Adolfo Mejia, and Fred Gregg.\n"}
{"id": "35784528", "url": "https://en.wikipedia.org/wiki?curid=35784528", "title": "French National School of Forestry", "text": "French National School of Forestry\n\nThe French National School of Forestry (\"École nationale des eaux et forêts\", or National School of Water Resources and Forestry), established in Nancy, France, in 1824, was the first national training institute for foresters in France, and a premier early school of forestry in Europe and globally.\n\nIn 1964, it was merged into the National School of Rural Engineering, Water Resources and Forestry (\"École nationale du génie rural, des eaux et des forêts\", or ENGREF), which in turn became part of AgroParisTech (\"Institut des sciences et industries du vivant et de l’environnement\", or Paris Institute of Technology for Life, Food and Environmental Sciences) in 2006.\n\n\n"}
{"id": "157606", "url": "https://en.wikipedia.org/wiki?curid=157606", "title": "Glass fiber", "text": "Glass fiber\n\nGlass fiber (or glass fibre) is a material consisting of numerous extremely fine fibers of glass.\n\nGlassmakers throughout history have experimented with glass fibers, but mass manufacture of glass fiber was only made possible with the invention of finer machine tooling. In 1893, Edward Drummond Libbey exhibited a dress at the World's Columbian Exposition incorporating glass fibers with the diameter and texture of silk fibers. Glass fibers can also occur naturally, as Pele's hair.\n\nGlass wool, which is one product called \"fiberglass\" today, was invented in 1932–1933 by Russell Games Slayter of Owens-Corning, as a material to be used as thermal building insulation. It is marketed under the trade name Fiberglas, which has become a genericized trademark. Glass fiber when used as a thermal insulating material, is specially manufactured with a bonding agent to trap many small air cells, resulting in the characteristically air-filled low-density \"glass wool\" family of products.\n\nGlass fiber has roughly comparable mechanical properties to other fibers such as polymers and carbon fiber. Although not as rigid as carbon fiber, it is much cheaper and significantly less brittle when used in composites. Glass fibers are therefore used as a reinforcing agent for many polymer products; to form a very strong and relatively lightweight fiber-reinforced polymer (FRP) composite material called glass-reinforced plastic (GRP), also popularly known as \"fiberglass\". This material contains little or no air or gas, is more dense, and is a much poorer thermal insulator than is glass wool.\n\nGlass fiber is formed when thin strands of silica-based or other formulation glass are extruded into many fibers with small diameters suitable for textile processing. The technique of heating and drawing glass into fine fibers has been known for millennia; however, the use of these fibers for textile applications is more recent. Until this time, all glass fiber had been manufactured as staple (that is, clusters of short lengths of fiber).\n\nThe modern method for producing glass wool is the invention of Games Slayter working at the Owens-Illinois Glass Co. (Toledo, Ohio). He first applied for a patent for a new process to make glass wool in 1933. The first commercial production of glass fiber was in 1936. In 1938 Owens-Illinois Glass Company and Corning Glass Works joined to form the Owens-Corning Fiberglas Corporation. When the two companies joined to produce and promote glass fiber, they introduced continuous filament glass fibers. Owens-Corning is still the major glass-fiber producer in the market today.\n\nThe most common types of glass fiber used in fiberglass is E-glass, which is alumino-borosilicate glass with less than 1% w/w alkali oxides, mainly used for glass-reinforced plastics. Other types of glass used are A-glass (Alkali-lime glass with little or no boron oxide), E-CR-glass (Electrical/Chemical Resistance; alumino-lime silicate with less than 1% w/w alkali oxides, with high acid resistance), C-glass (alkali-lime glass with high boron oxide content, used for glass staple fibers and insulation), D-glass (borosilicate glass, named for its low Dielectric constant), R-glass (alumino silicate glass without MgO and CaO with high mechanical requirements as reinforcement), and S-glass (alumino silicate glass without CaO but with high MgO content with high tensile strength).\n\nPure silica (silicon dioxide), when cooled as fused quartz into a glass with no true melting point, can be used as a glass fiber for fiberglass, but has the drawback that it must be worked at very high temperatures. In order to lower the necessary work temperature, other materials are introduced as \"fluxing agents\" (i.e., components to lower the melting point). Ordinary A-glass (\"A\" for \"alkali-lime\") or soda lime glass, crushed and ready to be remelted, as so-called cullet glass, was the first type of glass used for fiberglass. E-glass (\"E\" because of initial electrical application), is alkali free, and was the first glass formulation used for continuous filament formation. It now makes up most of the fiberglass production in the world, and also is the single largest consumer of boron minerals globally. It is susceptible to chloride ion attack and is a poor choice for marine applications. S-glass (\"S\" for \"Strength\") is used when high tensile strength (modulus) is important, and is thus important in composites for building and aircraft construction. The same substance is known as R-glass (\"R\" for \"reinforcement\") in Europe. C-glass (\"C\" for \"chemical resistance\") and T-glass (\"T\" is for \"thermal insulator\" – a North American variant of C-glass) are resistant to chemical attack; both are often found in insulation-grades of blown fiberglass.\n\nThe basis of textile-grade glass fibers is silica, SiO. In its pure form it exists as a polymer, (SiO). It has no true melting point but softens up to 1200 °C, where it starts to degrade. At 1713 °C, most of the molecules can move about freely. If the glass is extruded and cooled quickly at this temperature, it will be unable to form an ordered structure. In the polymer it forms SiO groups which are configured as a tetrahedron with the silicon atom at the center, and four oxygen atoms at the corners. These atoms then form a network bonded at the corners by sharing the oxygen atoms.\n\nThe vitreous and crystalline states of silica (glass and quartz) have similar energy levels on a molecular basis, also implying that the glassy form is extremely stable. In order to induce crystallization, it must be heated to temperatures above 1200 °C for long periods of time.\n\nAlthough pure silica is a perfectly viable glass and glass fiber, it must be worked with at very high temperatures, which is a drawback unless its specific chemical properties are needed. It is usual to introduce impurities into the glass in the form of other materials to lower its working temperature. These materials also impart various other properties to the glass that may be beneficial in different applications. The first type of glass used for fiber was soda lime glass or A-glass (\"A\" for the alkali it contains). It is not very resistant to alkali. A newer, alkali-free (<2%) type, E-glass, is an alumino-borosilicate glass. C-glass was developed to resist attack from chemicals, mostly acids that destroy E-glass. T-glass is a North American variant of C-glass. AR-glass is alkali-resistant glass. Most glass fibers have limited solubility in water but are very dependent on pH. Chloride ions will also attack and dissolve E-glass surfaces.\n\nE-glass does not actually melt, but softens instead, the softening point being \"the temperature at which a 0.55–0.77 mm diameter fiber 235 mm long, elongates under its own weight at 1 mm/min when suspended vertically and heated at the rate of 5 °C per minute\". The strain point is reached when the glass has a viscosity of 10 poise. The annealing point, which is the temperature where the internal stresses are reduced to an acceptable commercial limit in 15 minutes, is marked by a viscosity of 10 poise.\n\nFabrics of woven glass fibers are useful thermal insulators because of their high ratio of surface area to weight. However, the increased surface area makes them much more susceptible to chemical attack. By trapping air within them, blocks of glass fiber make good thermal insulation, with a thermal conductivity of the order of 0.05 W/(m·K).\n\nThe strength of glass is usually tested and reported for \"virgin\" or pristine fibers—those that have just been manufactured. The freshest, thinnest fibers are the strongest because the thinner fibers are more ductile. The more the surface is scratched, the less the resulting tenacity. Because glass has an amorphous structure, its properties are the same along the fiber and across the fiber. Humidity is an important factor in the tensile strength. Moisture is easily adsorbed and can worsen microscopic cracks and surface defects, and lessen tenacity.\n\nIn contrast to carbon fiber, glass can undergo more elongation before it breaks. Thinner filaments can bend further before they break. The viscosity of the molten glass is very important for manufacturing success. During drawing, the process where the hot glass is pulled to reduce the diameter of the fiber, the viscosity must be relatively low. If it is too high, the fiber will break during drawing. However, if it is too low, the glass will form droplets instead of being drawn out into a fiber.\n\nThere are two main types of glass fiber manufacture and two main types of glass fiber product. First, fiber is made either from a direct melt process or a marble remelt process. Both start with the raw materials in solid form. The materials are mixed together and melted in a furnace. Then, for the marble process, the molten material is sheared and rolled into marbles which are cooled and packaged. The marbles are taken to the fiber manufacturing facility where they are inserted into a can and remelted. The molten glass is extruded to the bushing to be formed into fiber. In the direct melt process, the molten glass in the furnace goes directly to the bushing for formation.\n\nThe bushing plate is the most important part of the machinery for making the fiber. This is a small metal furnace containing nozzles for the fiber to be formed through. It is almost always made of platinum alloyed with rhodium for durability. Platinum is used because the glass melt has a natural affinity for wetting it. When bushings were first used they were 100% platinum, and the glass wetted the bushing so easily that it ran under the plate after exiting the nozzle and accumulated on the underside. Also, due to its cost and the tendency to wear, the platinum was alloyed with rhodium. In the direct melt process, the bushing serves as a collector for the molten glass. It is heated slightly to keep the glass at the correct temperature for fiber formation. In the marble melt process, the bushing acts more like a furnace as it melts more of the material.\n\nBushings are the major expense in fiber glass production. The nozzle design is also critical. The number of nozzles ranges from 200 to 4000 in multiples of 200. The important part of the nozzle in continuous filament manufacture is the thickness of its walls in the exit region. It was found that inserting a counterbore here reduced wetting. Today, the nozzles are designed to have a minimum thickness at the exit. As glass flows through the nozzle, it forms a drop which is suspended from the end. As it falls, it leaves a thread attached by the meniscus to the nozzle as long as the viscosity is in the correct range for fiber formation. The smaller the annular ring of the nozzle and the thinner the wall at exit, the faster the drop will form and fall away, and the lower its tendency to wet the vertical part of the nozzle. The surface tension of the glass is what influences the formation of the meniscus. For E-glass it should be around 400 mN/m.\n\nThe attenuation (drawing) speed is important in the nozzle design. Although slowing this speed down can make coarser fiber, it is uneconomic to run at speeds for which the nozzles were not designed.\n\nIn the continuous filament process, after the fiber is drawn, a size is applied. This size helps protect the fiber as it is wound onto a bobbin. The particular size applied relates to end-use. While some sizes are processing aids, others make the fiber have an affinity for a certain resin, if the fiber is to be used in a composite. Size is usually added at 0.5–2.0% by weight. Winding then takes place at around 1 km/min.\n\nFor staple fiber production, there are a number of ways to manufacture the fiber. The glass can be blown or blasted with heat or steam after exiting the formation machine. Usually these fibers are made into some sort of mat. The most common process used is the rotary process. Here, the glass enters a rotating spinner, and due to centrifugal force is thrown out horizontally. The air jets push it down vertically, and binder is applied. Then the mat is vacuumed to a screen and the binder is cured in the oven.\n\nGlass fiber has increased in popularity since the discovery that asbestos causes cancer and its subsequent removal from most products. However, the safety of glass fiber is also being called into question, as research shows that the composition of this material (asbestos and glass fiber are both silicate fibers) can cause similar toxicity as asbestos.\n\n1970s studies on rats found that fibrous glass of less than 3 μm in diameter and greater than 20 μm in length is a \"potent carcinogen\". Likewise, the International Agency for Research on Cancer found it \"may reasonably be anticipated to be a carcinogen\" in 1990. The American Conference of Governmental Industrial Hygienists, on the other hand, says that there is insufficient evidence, and that glass fiber is in group A4: \"Not classifiable as a human carcinogen\".\n\nThe North American Insulation Manufacturers Association (NAIMA) claims that glass fiber is fundamentally different from asbestos, since it is man-made instead of naturally occurring. They claim that glass fiber \"dissolves in the lungs\", while asbestos remains in the body for life. Although both glass fiber and asbestos are made from silica filaments, NAIMA claims that asbestos is more dangerous because of its crystalline structure, which causes it to cleave into smaller, more dangerous pieces, citing the U.S. Department of Health and Human Services:\n\nA 1998 study using rats found that the biopersistence of synthetic fibers after one year was 0.04–10%, but 27% for amosite asbestos. Fibers that persisted longer were found to be more carcinogenic.\n\nGlass-reinforced plastic (GRP) is a composite material or fiber-reinforced plastic made of a plastic reinforced by fine glass fibers. Like graphite-reinforced plastic, the composite material is commonly referred to as fiberglass. The glass can be in the form of a chopped strand mat (CSM) or a woven fabric.\n\nAs with many other composite materials (such as reinforced concrete), the two materials act together, each overcoming the deficits of the other. Whereas the plastic resins are strong in compressive loading and relatively weak in tensile strength, the glass fibers are very strong in tension but tend not to resist compression. By combining the two materials, GRP becomes a material that resists both compressive and tensile forces well. The two materials may be used uniformly or the glass may be specifically placed in those portions of the structure that will experience tensile loads.\n\nUses for regular glass fiber include mats and fabrics for thermal insulation, electrical insulation, sound insulation, high-strength fabrics or heat- and corrosion-resistant fabrics. It is also used to reinforce various materials, such as tent poles, pole vault poles, arrows, bows and crossbows, translucent roofing panels, automobile bodies, hockey sticks, surfboards, boat hulls, and paper honeycomb. It has been used for medical purposes in casts. Glass fiber is extensively used for making FRP tanks and vessels.\n\nOpen-weave glass fiber grids are used to reinforce asphalt pavement. Non-woven glass fiber/polymer blend mats are used saturated with asphalt emulsion and overlaid with asphalt, producing a waterproof, crack-resistant membrane. Use of glass-fiber reinforced polymer rebar instead of steel rebar shows promise in areas where avoidance of steel corrosion is desired.\n\nManufacturers of glass-fiber insulation can use recycled glass. Recycled glass fiber contains up to 40% recycled glass.\n\n\n"}
{"id": "35550122", "url": "https://en.wikipedia.org/wiki?curid=35550122", "title": "Governance Initiative for Rights and Accountability in Forest Management", "text": "Governance Initiative for Rights and Accountability in Forest Management\n\nThe Governance Initiative for Rights & Accountability in Forest Management (GIRAF) is a project executed in Ghana with funds from the European Union (EU) and its partners CARE Denmark, CIVIC Response, Friends of the Earth-Ghana and Centre for Indigenous Knowledge.\n"}
{"id": "45010796", "url": "https://en.wikipedia.org/wiki?curid=45010796", "title": "Greenly Island Conservation Park", "text": "Greenly Island Conservation Park\n\nGreenly Island Conservation Park is a protected area associated with Greenly Island located off the west coast of Eyre Peninsula in South Australia about west of Coffin Bay. It was declared in 1972 under the \"National Parks and Wildlife Act 1972\" ‘to protect the island’s delicate ecology and Australian Sea-lion and New Zealand Fur-seal haul-out areas’ and continuing protected area status for the island which was first declared in 1919. The conservation park is classified as an IUCN Category Ia protected area.\n\n"}
{"id": "1065627", "url": "https://en.wikipedia.org/wiki?curid=1065627", "title": "Ground and neutral", "text": "Ground and neutral\n\nAs the neutral point of an electrical supply system is often connected to earth ground, ground and neutral are closely related. Under certain conditions, a conductor used to connect to a system neutral is also used for grounding (earthing) of equipment and structures. Current carried on a grounding conductor can result in objectionable or dangerous voltages appearing on equipment enclosures, so the installation of grounding conductors and neutral conductors is carefully defined in electrical regulations. Where a neutral conductor is used also to connect equipment enclosures to earth, care must be taken that the neutral conductor never rises to a high voltage with respect to local ground.\n\nGround or earth in a mains (AC power) electrical wiring system is a conductor that provides a low-impedance path to the earth to prevent hazardous voltages from appearing on equipment (high voltage spikes). The terms and are used synonymously in this section; is more common in North American English, and is more common in British English. Under normal conditions, a grounding conductor does not carry current. Grounding is an integral path for home wiring also because it causes circuit breakers to trip more quickly (ie, GFI), which is safer. Adding new grounds requires a qualified electrician with information particular to a power company distribution region.\n\nNeutral is a circuit conductor that normally carries current back to the source. Neutral is usually connected to ground (earth) at the main electrical panel, street drop, or meter, and also at the final step-down transformer of the supply. That is for simple single panel installations; for multiple panels the situation is more complex.\n\nIn the electrical trade, the conductor of a 2-wire circuit connected to the supply neutral point and earth ground is referred to as the . \n\nIn a polyphase (usually three-phase) AC system, the neutral conductor is intended to have similar voltages to each of the other circuit conductors, but may carry very little current if the phases are balanced.\n\nThe United States' National Electrical Code and Canadian electrical code only define neutral as the grounded, not the polyphase common connection. In North American use, the polyphase definition is used in less formal language but not in official specifications. In the United Kingdom the Institution of Engineering and Technology defines a neutral conductor as one connected to the supply system neutral point, which includes both these uses.\n\nAs per Indian CEAR means that conductor of a multi-wire system, the voltage of which is normally intermediate between the voltages of the other conductors of the system and shall also include return wire of the single phase system.\n\nAll neutral wires of the same earthed (grounded) electrical system should have the same electrical potential, because they are all connected through the system ground. Neutral conductors are usually insulated for the same voltage as the line conductors, with interesting exceptions.\n\nNeutral wires are usually connected at a neutral bus within panelboards or switchboards, and are \"bonded\" to earth ground at either the electrical service entrance, or at transformers within the system. For electrical installations with split-phase (three-wire single-phase service), the neutral point of the system is at the center-tap on the secondary side of the service transformer. For larger electrical installations, such as those with polyphase service, the neutral point is usually at the common connection on the secondary side of delta/wye connected transformers. Other arrangements of polyphase transformers may result in no neutral point, and no neutral conductors.\n\nThe IEC standard (IEC 60364) codifies methods of installing neutral and ground conductors in a building, where these earthing systems are designated with letter symbols. The letter symbols are common in countries using IEC standards, but North American practices rarely refer to the IEC symbols. The differences are that the conductors may be separate over their entire run from equipment to earth ground, or may be combined over all or part of their length. Different systems are used to minimize the voltage difference between neutral and local earth ground. Current flowing in a grounding conductor will produce a voltage drop along the conductor, and grounding systems seek to ensure this voltage does not reach unsafe levels. \n\nIn the TN-S system, separate neutral and protective earth conductors are installed between the equipment and the source of supply (generator or electric utility transformer). Normal circuit currents flow only in the neutral, and the protective earth conductor bonds all equipment cases to earth to intercept any leakage current due to insulation failure. The neutral conductor is connected to earth at the building point of supply, but no common path to ground exists for circuit current and the protective conductor.\n\nIn the TN-C system, a common conductor provides both the neutral and protective grounding. The neutral conductor is connected to earth ground at the point of supply, and equipment cases are connected to the neutral. The danger exists that a broken neutral connection will allow all the equipment cases to rise to a dangerous voltage if any leakage or insulation fault exists in any equipment. This can be mitigated with special cables but the cost is then higher. \n\nIn the TN-C-S system, each piece of electrical equipment has both a protective ground connection to its case, and a neutral connection. These are all brought back to some common point in the building system, and a common connection is then made from that point back to the source of supply and to the earth. \n\nIn a TT system, no lengthy common protective ground conductor is used, instead each article of electrical equipment (or building distribution system) has its own connection to earth ground.\n\nAs per Indian CEAR , Rule 41, the grounding system to follow:\n\n- Neutral conductor of a 3-phase, 4-wire system and the middle conductor of a 2- phase, 3-wire system to have minimum two separate and distinct earth connections with a minimum of two different earth electrodes to have the earth resistance to a satisfactory value. \n\n- The earth electrodes to be inter-connected to reduce earth resistance.\n\n- Neutral conductor shall also be earthed at one or more points along the distribution system or service line in addition to any connection at user end.\n\nStray voltages created in grounding (earthing) conductors by currents flowing in the supply utility neutral conductors can be troublesome. For example, special measures may be required in barns used for milking dairy cattle. Very small voltages, not usually perceptible to humans, may cause low milk yield, or even mastitis (inflammation of the udder).\nSo-called \"tingle voltage filters\" may be required in the electrical distribution system for a milking parlour.\n\nConnecting the neutral to the equipment case provides some protection against faults, but may produce a dangerous voltage on the case if the neutral connection is broken.\n\nCombined neutral and ground conductors are commonly used in electricity supply companies' wiring and occasionally for fixed wiring in buildings and for some specialist applications where there is little alternative, such as railways and trams. Since normal circuit currents in the neutral conductor can lead to objectionable or dangerous differences between local earth potential and the neutral, and to protect against neutral breakages, special precautions such as frequent rodding down to earth (multiple ground rod connections), use of cables where the combined neutral and earth completely surrounds the phase conductor(s), and thicker than normal equipotential bonding must be considered to ensure the system is safe.\n\nIn the United States, the cases of some kitchen stoves (ranges, ovens), cook tops, clothes dryers and other specifically \"listed\" appliances were grounded through their neutral wires as a measure to conserve copper from copper cables during World War II. This practice was removed from the NEC in the 1996 edition, but existing installations (called \"old work\") may still allow the cases of such \"listed\" appliances to be connected to the neutral conductor for grounding. (Canada did not adopt this system and instead during this time and into the present uses separate neutral and ground wires.)\n\nThis practice arose from the three-wire system used to supply both 120 volt and 240 volt loads. Because these \"listed\" appliances often have components that use either 120, or both 120 and 240 volts, there is often some current on the neutral wire. This differs from the protective grounding wire, which only carries current under fault conditions. Using the neutral conductor for grounding the equipment enclosure was considered safe since the devices were permanently wired to the supply and so the neutral was unlikely to be broken without also breaking both supply conductors. Also, the unbalanced current due to lamps and small motors in the appliances was small compared to the rating of the conductors and therefore unlikely to cause a large voltage drop in the neutral conductor.\n\nIn North American and European practice, small portable equipment connected by a cord set is permitted under certain conditions to have merely two conductors in the attachment plug. A polarized plug can be used to maintain the identity of the neutral conductor into the appliance but neutral is never used as a chassis/case ground. The small cords to lamps, etc., often have one or more molded ridges or embedded strings to identify the neutral conductor, or may be identified by colour. Portable appliances never use the neutral conductor for case grounding, and often feature \"double-insulated\" construction.\n\nIn places where the design of the plug and socket cannot ensure that a system neutral conductor is connected to particular terminals of the device (\"unpolarized\" plugs), portable appliances must be designed on the assumption that either pole of each circuit may reach full main voltage with respect to the ground.\n\nIn North American practice, equipment connected by a cord set must have three wires, if supplied exclusively by 240 volts, or must have four wires (including neutral and ground), if supplied by 120/240 volts.\n\nThere are special provisions in the NEC for so-called technical equipment, mainly professional grade audio and video equipment supplied by so-called \"balanced\" 120 volt circuits. The center tap of a transformer is connected to ground, and the equipment is supplied by two line wires each 60 volts to ground (and 120 volts between line conductors). The center tap is not distributed to the equipment and no neutral conductor is used. These cases generally use a grounding conductor which is separated from the safety grounding conductor specifically for the purposes of noise and \"hum\" reduction.\n\nAnother specialized distribution system was formerly specified in patient care areas of hospitals. An isolated power system was furnished, from a special isolation transformer, with the intention of minimizing any leakage current that could pass through equipment directly connected to a patient (for example, an electrocardiograph for monitoring the heart). The neutral of the circuit was not connected to ground. The leakage current was due to the distributed capacitance of the wiring and capacitance of the supply transformer. Such distribution systems were monitored by permanently installed instruments to give an alarm when high leakage current was detected.\n\nA shared neutral is a connection in which a plurality of circuits use the same neutral connection. This is also known as a common neutral, and the circuits and neutral together are sometimes referred to as an Edison circuit.\n\nIn a three-phase circuit, a neutral is shared between all three phases. Commonly the system neutral is connected to the star point on the feeding transformer. This is the reason that the secondary side of most three-phase distribution transformers is wye- or star-wound. Three-phase transformers and their associated neutrals are usually found in industrial distribution environments.\n\nA system could be made entirely ungrounded. In this case a fault between one phase and ground would not cause any significant current. In fact, this is not a good scheme. Commonly the neutral is grounded (earthed) through a bond between the neutral bar and the earth bar. It is common on larger systems to monitor any current flowing through the neutral-to-earth link and use this as the basis for neutral fault protection.\nThe connection between neutral and earth allows any phase-to-earth fault to develop enough current flow to \"trip\" the circuit overcurrent protection device. In some jurisdictions, calculations are required to ensure the fault loop impedance is low enough so that fault current will trip the protection (In Australia, this is referred to in AS3000:2007 Fault loop impedance calculation). This may limit the length of a branch circuit.\nIn the case of two phases sharing one neutral, the worst-case current draw is one side has zero load and the other has full load, or when both sides have full load. The latter case results in 1 + 1@120deg = 1@60deg, i.e. the magnitude of the current in the neutral equals that of the other two wires.\n\nIn a three-phase linear circuit with three identical resistive or reactive loads, the neutral carries no current. The neutral carries current if the loads on each phase are not identical. In some jurisdictions, the neutral is allowed to be reduced in size if no unbalanced current flow is expected. If the neutral is smaller than the phase conductors, it can be overloaded if a large unbalanced load occurs.\n\nThe current drawn by non-linear loads, such as fluorescent & HID lighting and electronic equipment containing switching power supplies, often contains harmonics. Triplen harmonic currents (odd multiples of the third harmonic) are additive, resulting in more current in the shared neutral conductor than in any of the phase conductors. In the absolute worst case, the current in the shared neutral conductor can be triple that in each phase conductor. Some jurisdictions prohibit the use of shared neutral conductors when feeding single-phase loads from a three-phase source; others require that the neutral conductor be substantially larger than the phase conductors. It is good practice to use four-pole circuit breakers (as opposed to the standard three-pole) where the fourth pole is the neutral phase, and is hence protected against overcurrent on the neutral conductor.\n\nIn split-phase wiring, for example a duplex receptacle in a North American kitchen, devices may be connected with a cable that has three conductors, in addition to ground. The three conductors are usually coloured red, black, and white. The white serves as a common neutral, while the red and black each feed, separately, the top and bottom hot sides of the receptacle. Typically such receptacles are supplied from two circuit breakers in which the handles of two poles are tied together for a common trip. If two large appliances are used at once, current passes through both and the neutral only carries the difference in current. The advantage is that only three wires are required to serve these loads, instead of four. If one kitchen appliance overloads the circuit, the other side of the duplex receptacle will be shut off as well. This is called a multiwire branch circuit. Common trip is required when the connected load uses more than one phase simultaneously. The common trip prevents overloading of the shared neutral if one device draws more than rated current.\n\nA ground connection that is missing or of inadequate capacity may not provide the protective functions as intended during a fault in the connected equipment. Extra connections between ground and circuit neutral may result in circulating current in the ground path, stray current introduced in the earth or in a structure, and stray voltage. Extra ground connections on a neutral conductor may bypass the protection provided by a ground-fault circuit interrupter. Signal circuits that rely on a ground connection will not function or will have erratic function if the ground connection is missing.\n\n\n\n"}
{"id": "41602263", "url": "https://en.wikipedia.org/wiki?curid=41602263", "title": "Guohua Taishan Power Station", "text": "Guohua Taishan Power Station\n\nGuohua Taishan Power Station () is a coal-fired power station in Taishan, Jiangmen, Guangdong, China. With an installed capacity of 5,000 MW, it is the fourth largest coal-fired power station in the world.\n\n"}
{"id": "15821650", "url": "https://en.wikipedia.org/wiki?curid=15821650", "title": "Independent Power Producer", "text": "Independent Power Producer\n\nAn independent power producer (IPP) or non-utility generator (NUG) is an entity, which is not a public utility, but which owns facilities to generate electric power for sale to utilities and end users. NUGs may be privately held facilities, corporations, cooperatives such as rural solar or wind energy producers, and non-energy industrial concerns capable of feeding excess energy into the system.\n\nFor the majority of IPPs, particularly in the renewable energy industry, a feed-in Tariff or Power Purchase Agreement provides a long term price guarantee.\n\nHas been uncommon in Germany for decades but since the EEG (for renewable energy) the business model gets more common. It depends on finding a partner for distributing the produced energy to the customer. \n\nIn 2002, the BC government stipulated that new clean renewable energy generation in the province would be developed by \"independent power producers\" (IPPs) not BC Hydro, save for large hydro-electric facilities. The role of the private sector in developing BC’s \"public\" resources is one of the more controversial issues that British Columbians are currently grappling with.\n\nThe liberalization of Taiwan electricity market was done in January 1995. Currently there are nine IPP companies operating in Taiwan.\n\nPrior to the US Public Utility Regulatory Policies Act (PURPA) of 1978, NUGs were rare, and the few that existed were seldom able to distribute power, as the cost of building the conveyance infrastructure was prohibitive. Public utilities generated power and owned the generating facilities, the transmission lines, and the local delivery systems.\nCongress Passed the PURPA in 1978, establishing a class of non‐utility generators, called Qualifying Facilities (QF), which were permitted to produce power for resale. \n\nPURPA was intended to reduce domestic dependence on foreign energy, to encourage energy conservation, and to reduce the ability of electric utilities to abuse the purchase of power from QFs. A QF is defined as a generating facility that produces electricity and another form of useful thermal energy through the sequential use of energy, and meets certain ownership, operating, and efficiency criteria established by the Federal Energy Regulatory Commission (FERC).\n\nSection 210 of PURPA now requires utilities to purchase energy from NUGs which qualify (qualifying facilities) at the utility's avoided cost. This allows NUGs to receive a reasonable to excellent price for the energy they produce and ensures that energy generated by small producers won't be wasted.\n\nIn 1994, Government of Pakistan announced an investor friendly policy to develop IPPs based on oil, coal and gas, which helped in establishment of 16 IPPS. Later in 1995, a hydro power policy was announced which resulted in development of country's first Hydro IPP. \n\nIn 2002, new government adopted new policy, by which another 12 IPPs started operations. \n\nIn 2015, Pakistan adopted a new power policy by which another 13 IPPs were established, mostly by Chinese companies. A transmission policy for development of transmission line in private sector was also announced. \n\nAs of 2018, currently more than 40 IPPs are operating in Pakistan.\n\nIndia also has many IPP's like ReNew Power, Adani, Hero, Mytrah, Ostro, Greenko Etc.\n"}
{"id": "40370137", "url": "https://en.wikipedia.org/wiki?curid=40370137", "title": "International Oil and Gas University", "text": "International Oil and Gas University\n\nInternational Oil and Gas University () is a university located in Ashgabat, the main university of the Turkmenistan oil and gas community. Founded May 25, 2012 as the Turkmen State Institute of Oil and Gas. August 10, 2013 has become an international university. A branch of the University operate in Balkanabat.\n\nCreated in order to improve existing work on the diversification of exports to the world market of Turkmen minerals, the implementation of high-quality level of development programs of oil and gas industry. The decree № PP-6081 was signed by President of Turkmenistan on the establishment of the Turkmen State Institute of Oil and Gas. Was placed under the supervision to the Ministry of Education of Turkmenistan.\n\nAugust 10, 2013 \"in order to radically improve the training of highly qualified specialists for the oil and gas industry\" was renamed the International University of Oil and Gas.\n\nThe institute created about twenty specialties in eight areas: geology, exploration and mining, chemical engineering, computer technology, construction, architecture, manufacturing machinery and equipment, energy, economics and management in industry, management.\n\nThe institute seven faculties and 27 departments. Taught by some 250 teachers, including 6 doctors, including 5 professors and 33 candidates of sciences, including 14 professors.\n\n\nThe building was built in the southern part of Ashgabat, where a new business and cultural center of the capital of Turkmenistan. 17-storey office building was built by Turkish company «Renaissance». The project started in 2010. The opening ceremony of the buildings took place September 1, 2012 with the participation of President of Turkmenistan Gurbanguly Berdimuhamedov.\n\nThe building is symbolically resembles an oil rig. The complex of buildings of the University covers an area of 30 hectares, consists of a main 18-storey building and five academic buildings, 86 classrooms at the same time be able to learn to 3,000 students. The institute located assembly and meeting rooms, museum, archive, library (with 250 seats) equipped with multimedia equipment reading rooms, Center for Information Technology, cafe, clinic, grocery and department store. Classrooms and laboratories equipped with modern equipment.\n\nThe university has a museum, where the archive is created fund, telling about the oil and gas production, oil and gas development and the national economy of Turkmenistan.\n\nThe building in 2012, recognized as the best building of the CIS according to the International Union of Architects Association of CIS.\n\n6 dormitories constructed for each faculty, designed for 230 seats. Rooms - double the terms, kitchen room is equipped with household appliances.\n\nOperates an indoor sports complex, with a boxing ring, gym and swimming pool. The multi-purpose sports hall has fields for football, basketball, volleyball, tennis and other sports. Separately located gymnasium. There are showers. On the sports field with natural grass flooring classes are held in the open air.\n"}
{"id": "17173057", "url": "https://en.wikipedia.org/wiki?curid=17173057", "title": "Joint BioEnergy Institute", "text": "Joint BioEnergy Institute\n\nThe Joint BioEnergy Institute (JBEI) is a research institute funded by the United States Department of Energy. It is led by the Lawrence Berkeley National Laboratory, and includes participation from the Sandia National Laboratory, Lawrence Livermore National Laboratory, as well as UC Berkeley, UC Davis and the Carnegie Institute. It is located in Emeryville, California.\n\nThe goal of the Institute is to develop biofuels, bio-synthesized from cellulosic materials (see Second generation biofuels) as an alternative to fossil fuels.\n\n"}
{"id": "26006419", "url": "https://en.wikipedia.org/wiki?curid=26006419", "title": "List of pumped-storage hydroelectric power stations", "text": "List of pumped-storage hydroelectric power stations\n\nThe following page lists all pumped-storage hydroelectric power stations that are larger than in installed generating capacity, which are currently operational or under construction. Those power stations that are smaller than , and those that are decommissioned or only at a planning/proposal stage may be found in regional lists, listed at the end of the page. \n\n \n\nThe table below lists currently operational power stations. Some of these may have additional units under construction, but only current installed capacity is listed.\n\nThis table lists future 1,000 MW or larger stations that are under construction; some may be partially operational with a current installed capacity under 1,000 MW.\n\n"}
{"id": "22482181", "url": "https://en.wikipedia.org/wiki?curid=22482181", "title": "Maritime Electric", "text": "Maritime Electric\n\nMaritime Electric is the supplier of electricity in Prince Edward Island, Canada. Maritime Electric is a public utility, and is regulated the Island Regulatory and Appeals Commission (IRAC) under the Electric Power Act and the Renewable Energy Act. The utility operates two generating stations on the island: the Charlottetown Thermal Generating Station and the Borden Generating Station.\n\nOn November 13, 2009, it was announced that the PEI government was in discussion with the province of Quebec, with regard to providing electric power between the two provinces, which could lead to a long-term supply contract with Hydro-Québec, the construction of a submarine transmission line linking PEI and the Magdalen Islands, and, pending Fortis' involvement, the sale of Maritime Electric to Hydro-Québec. This followed the announcement of Hydro-Québec's proposed purchase of most of NB Power's assets two weeks earlier (which failed in March 2010).\n\n"}
{"id": "49511662", "url": "https://en.wikipedia.org/wiki?curid=49511662", "title": "Metal-phenolic network", "text": "Metal-phenolic network\n\nA metal-phenolic network is a supramolecular coordination structure which consists of metal ions and polyphenols. These materials were first reported by an Australian group at The University of Melbourne. MPN materials can be coated on versatile substrates to form nanostructured films, and the toolbox of metal ions has been further extended across the periodic table. Unlike many other coating materials, which form covalent bonds with specific substrate molecules, MPNs adsorb to a wide variety of surfaces due to noncovalent forces. Due to their significant versatile coating and multifunctional properties, MPN-based materials has been one of the fastest growing fields in chemistry and materials science. The applications of MPN-based materials have been extended to a wide spectrum of fields, such as drug delivery, bioimaging, and biotechnology. In 2016, the same group reported the new property of MPN (and polydopamine) in the particle assembly driven by polyphenol-based modular functionalization and interfacial molecular interactions.\n"}
{"id": "34294319", "url": "https://en.wikipedia.org/wiki?curid=34294319", "title": "N. K. Sukumaran Nair", "text": "N. K. Sukumaran Nair\n\nN. K. Sukumaran Nair (born 6 June 1942) is an acclaimed Environmental Activist and General secretary of Pampa Samrakshana Samithi (PSS). He was born at Poovathur in Pathanamthitta District, Kerala State. He is the recipient of Jaiji Peter Foundation Gold medal and citation for the State's best environmental activist in the year 2007. He has been spending his time and energy for the past several years to save the Pamba River.\n\n"}
{"id": "188888", "url": "https://en.wikipedia.org/wiki?curid=188888", "title": "Neutron source", "text": "Neutron source\n\nA neutron source is any device that emits neutrons, irrespective of the mechanism used to produce the neutrons. Neutron sources are used in physics, engineering, medicine, nuclear weapons, petroleum exploration, biology, chemistry, and nuclear power.\n\nNeutron source variables include the energy of the neutrons emitted by the source, the rate of neutrons emitted by the source, the size of the source, the cost of owning and maintaining the source, and government regulations related to the source.\n\nCertain isotopes undergo spontaneous fission with emission of neutrons. The most commonly used spontaneous fission source is the radioactive isotope californium-252. Cf-252 and all other spontaneous fission neutron sources are produced by irradiating uranium or another transuranic element in a nuclear reactor, where neutrons are absorbed in the starting material and its subsequent reaction products, transmuting the starting material into the SF isotope. Cf-252 neutron sources are typically 1/4\" to 1/2\" in diameter and 1\" to 2\" in length. When purchased new a typical Cf-252 neutron source emits between 1×10 to 1×10 neutrons per second but, with a half life of 2.6 years, this neutron output rate drops to half of this original value in 2.6 years. The price of a typical Cf-252 neutron source is from $15,000 to $20,000.\n\nNeutrons are produced when alpha particles impinge upon any of several low-atomic-weight isotopes including isotopes of beryllium, carbon, and oxygen. This nuclear reaction can be used to construct a neutron source by mixing a radioisotope that emits alpha particles such as radium, polonium, or americium with a low-atomic-weight isotope, usually by blending powders of the two materials. Typical emission rates for alpha reaction neutron sources range from 1×10 to 1×10 neutrons per second. As an example, a representative alpha-beryllium neutron source can be expected to produce approximately 30 neutrons for every one million alpha particles. The useful lifetime for these types of sources is highly variable, depending upon the half life of the radioisotope that emits the alpha particles. The size and cost of these neutron sources are comparable to spontaneous fission sources. Usual combinations of materials are plutonium-beryllium (PuBe), americium-beryllium (AmBe), or americium-lithium (AmLi).\n\nGamma radiation with an energy exceeding the neutron binding energy of a nucleus can eject a neutron (a photoneutron). Two example reactions are:\n\nSome accelerator-based neutron generators induce fusion between beams of deuterium and/or tritium ions and metal hydride targets which also contain these isotopes.\n\nThe dense plasma focus neutron source produces controlled nuclear fusion by creating a dense plasma within which heats ionized deuterium and/or tritium gas to temperatures sufficient for creating fusion.\n\nInertial electrostatic confinement devices such as the Farnsworth-Hirsch fusor use an electric field to heat a plasma to fusion conditions and produce neutrons. Various applications from a hobby enthusiast scene up to commercial applications have developed, mostly in the US.\n\nTraditional particle accelerators with hydrogen (H), deuterium (D), or tritium (T) ion sources may be used to produce neutrons using targets of deuterium, tritium, lithium, beryllium, and other low-Z materials. Typically these accelerators operate with energies in the > 1 MeV range.\n\nNeutrons are produced when photons above the nuclear binding energy of a substance are incident on that substance, causing it to undergo giant dipole resonance after which it either emits a neutron (photoneutron) or undergoes fission (photofission). The number of neutrons released by each fission event is dependent on the substance. Typically photons begin to produce neutrons on interaction with normal matter at energies of about 7 to 40 MeV, which means that radiotherapy facilities using megavoltage X-rays also produce neutrons, and some require neutron shielding. In addition, electrons of energy over about 50 MeV may induce giant dipole resonance in nuclides by a mechanism which is the inverse of internal conversion, and thus produce neutrons by a mechanism similar to that of photoneutrons.\n\nNuclear fission which takes place within in a reactor produces very large quantities of neutrons and can be used for a variety of purposes including power generation and experiments.\n\nNuclear fusion, the combining of the heavy isotopes of hydrogen, also has the potential to produces large quantities of neutrons. Small scale fusion systems exist for (plasma) research purposes at many universities and laboratories around the world. A small number of large scale nuclear fusion experiments also exist including the National Ignition Facility in the USA, JET in the UK, and soon the ITER experiment currently under construction in France. None are yet used as neutron sources.\n\nInertial confinement fusion has the potential to produce orders of magnitude more neutrons than spallation. This could be useful for neutron radiography which can be used to locate hydrogen atoms in structures, resolve atomic thermal motion and study collective excitations of nuclei more effectively than X-rays.\n\nA spallation source is a high-flux source in which protons that have been accelerated to high energies hit a target material, prompting the emission of neutrons.\n\nFor most applications, a higher neutron flux is better (since it reduces the time required to conduct the experiment, acquire the image, etc.). Amateur fusion devices, like the fusor, generate only about 300 000 neutrons per second. Commercial fusor devices can generate on the order of 10 neutrons per second, which corresponds to a usable flux of less than 10 n/(cm² s). Large neutron beamlines around the world achieve much greater flux. Reactor-based sources now produce 10 n/(cm² s), and spallation sources generate greater than 10 n/(cm² s).\n\n\n"}
{"id": "180279", "url": "https://en.wikipedia.org/wiki?curid=180279", "title": "Ounce", "text": "Ounce\n\nThe ounce (abbreviated oz; apothecary symbol: ℥) is a unit of mass, weight, or volume used in most British derived customary systems of measurement. The common avoirdupois ounce (approximately ) is of a common avoirdupois pound; this is the United States customary and British imperial ounce. It is primarily used in the United States to measure packaged foods and food portions, postal items, areal density of fabric and paper, boxing gloves, and so on; but sometimes also elsewhere in the Anglosphere.\n\nBesides the common ounce, several other ounces are in current use:\n\nHistorically, a variety of different ounces measuring mass or volume were used in different jurisdictions by different trades.\n\n\"Ounce\" derives from Latin , a unit that was one-twelfth () of the Roman pound (). \"Ounce\" was borrowed twice: first into Old English as or from an unattested Vulgar Latin form with \"ts\" for \"c\" before \"i\" (palatalization) and second into Middle English through Anglo-Norman and Middle French (). The abbreviation \"oz\" came later from the cognate Italian word (now spelled ).\n\n\"Inch\" comes from the same Latin word, but differs because it was borrowed into Old English and underwent i-mutation or umlaut ( → ) and palatalization ( → ).\n\nHistorically, in different parts of the world, at different points in time, and for different applications, the ounce (or its translation) has referred to broadly similar but different standards of mass.\n\nThe international avoirdupois ounce is defined as exactly 28.349523125 g under the international yard and pound agreement of 1959, signed by the United States and countries of the Commonwealth of Nations.\n\nIn the avoirdupois system, sixteen ounces make up an avoirdupois pound, and the avoirdupois pound is defined as 7000 grains; one avoirdupois ounce is therefore equal to 437.5 grains.\n\nThe ounce is still a standard unit in the United States. In the United Kingdom it ceased to be a legal unit of measure in 2000, but is still in general usage on an informal basis. In addition it is the normal measure for portion sizes in British restaurants.\n\nA troy ounce is equal to 480 grains. Consequently, the international troy ounce is equal to exactly 31.1034768 grams. There are 12 troy ounces in the now obsolete troy pound.\n\nToday, the troy ounce is used only to express the mass of precious metals such as gold, platinum, palladium, rhodium or silver. Bullion coins are the most common products produced and marketed in troy ounces, but precious metal bars also exist in gram and kilogram (kg) sizes. (A kilogram bullion bar contains 32.15074657 troy ounces.)\n\nFor historical measurement of gold,\n\nSome countries have redefined their ounces in the metric system. For example, the German apothecaries ounce of 30 grams, is very close to the previously widespread Nuremberg ounce, but the divisions and multiples come out in metric.\n\nIn 1820, the Dutch redefined their ounce (in Dutch, \"ons\") as 100 grams. Dutch amendments to the metric system, such as an \"ons\" or 100 grams, has been inherited, adopted, and taught in Indonesia beginning in elementary school. It is also listed as standard usage in Indonesia's national dictionary, the \"Kamus Besar Bahasa Indonesia\", and the government's official elementary‐school curriculum.\n\nThe obsolete apothecaries' ounce (abbreviated ℥) equivalent to the troy ounce, was formerly used by apothecaries.\n\n\"Maria Theresa ounce\" was once introduced in Ethiopia and some European countries, which was equal to the weight of one Maria Theresa thaler, or 28.0668 g. Both the weight and the value are the definition of one \"birr\", still in use in present-day Ethiopia and formerly in Eritrea.\n\nThe Spanish pound (Spanish \"libra\") was 460 g. The Spanish ounce (Spanish \"onza\") was of a pound, i.e. 28.75 g.\n\nThe Tower ounce of 450 grains was used in the English mints, the principal one being in the Tower of London. It dates back to the Anglo-Saxon coinage weight standard. It was abolished in favour of the Troy ounce by Henry VIII in 1527.\n\nAn ounce-force is of a pound-force, or .\n\nThe \"ounce\" in \"ounce-force\" is equivalent to an avoirdupois ounce; ounce-force is a measurement of force using avoirdupois ounces. However, it is not necessary to identify it as such or to differentiate it in that way because there is no equivalent measure of force using troy or any other \"ounce\".\n\nA fluid ounce (abbreviated fl oz, fl. oz. or oz. fl.) is a unit of volume equal to about 28.4 ml in the imperial system or about 29.6 ml in the US system. The fluid ounce is sometimes referred to simply as an \"ounce\" in applications where its use is implicit, such as bartending.\n\nOunces are also used to express the \"weight\", or more accurately the areal density, of a textile fabric in North America, Asia or the UK, as in \"16 oz denim\". The number refers to the weight in ounces of a given amount of fabric, either a yard of a given width, or a square yard.\n\nThe most common unit of measure for the copper thickness on a printed circuit board (PCB) is ounces (oz). It is the resulting thickness when 1 oz of copper is pressed flat and spread evenly over a one-square-foot area. This roughly equals 34.7 µm.\n\n"}
{"id": "1355057", "url": "https://en.wikipedia.org/wiki?curid=1355057", "title": "Overhead power line", "text": "Overhead power line\n\nAn overhead power line is a structure used in electric power transmission and distribution to transmit electrical energy along large distances. It consists of one or more conductors (commonly multiples of three) suspended by towers or poles. Since most of the insulation is provided by air, overhead power lines are generally the lowest-cost method of power transmission for large quantities of electric energy.\n\nTowers for support of the lines are made of wood (as-grown or laminated), steel or aluminum (either lattice structures or tubular poles), concrete, and occasionally reinforced plastics. The bare wire conductors on the line are generally made of aluminum (either plain or reinforced with steel, or composite materials such as carbon and glass fiber), though some copper wires are used in medium-voltage distribution and low-voltage connections to customer premises. A major goal of overhead power line design is to maintain adequate clearance between energized conductors and the ground so as to prevent dangerous contact with the line, and to provide reliable support for the conductors, resilience to storms, ice loads, earthquakes and other potential damage causes.\nToday overhead lines are routinely operated at voltages exceeding 765,000 volts between conductors, with even higher voltages possible in some cases.\n\nOverhead power transmission lines are classified in the electrical power industry by the range of voltages:\n\nStructures for overhead lines take a variety of shapes depending on the type of line. Structures may be as simple as wood poles directly set in the earth, carrying one or more cross-arm beams to support conductors, or \"armless\" construction with conductors supported on insulators attached to the side of the pole. Tubular steel poles are typically used in urban areas. High-voltage lines are often carried on lattice-type steel towers or pylons. For remote areas, aluminum towers may be placed by helicopters. Concrete poles have also been used. Poles made of reinforced plastics are also available, but their high cost restricts application.\n\nEach structure must be designed for the loads imposed on it by the conductors. The weight of the conductor must be supported, as well as dynamic loads due to wind and ice accumulation, and effects of vibration. Where conductors are in a straight line, towers need only resist the weight since the tension in the conductors approximately balances with no resultant force on the structure. Flexible conductors supported at their ends approximate the form of a catenary, and much of the analysis for construction of transmission lines relies on the properties of this form.\n\nA large transmission line project may have several types of towers, with \"tangent\" (\"suspension\" or \"line\" towers, UK) towers intended for most positions and more heavily constructed towers used for turning the line through an angle, dead-ending (terminating) a line, or for important river or road crossings. Depending on the design criteria for a particular line, semi-flexible type structures may rely on the weight of the conductors to be balanced on both sides of each tower. More rigid structures may be intended to remain standing even if one or more conductors is broken. Such structures may be installed at intervals in power lines to limit the scale of cascading tower failures.\n\nFoundations for tower structures may be large and costly, particularly if the ground conditions are poor, such as in wetlands. Each structure may be stabilized considerably by the use of guy wires to counteract some of the forces applied by the conductors.\n\nPower lines and supporting structures can be a form of visual pollution. In some cases the lines are buried to avoid this, but this \"undergrounding\" is more expensive and therefore not common.\n\nFor a single wood utility pole structure, a pole is placed in the ground, then three crossarms extend from this, either staggered or all to one side. The insulators are attached to the crossarms. For an \"H\"-type wood pole structure, two poles are placed in the ground, then a crossbar is placed on top of these, extending to both sides. The insulators are attached at the ends and in the middle. Lattice tower structures have two common forms. One has a pyramidal base, then a vertical section, where three crossarms extend out, typically staggered. The strain insulators are attached to the crossarms. Another has a pyramidal base, which extends to four support points. On top of this a horizontal truss-like structure is placed.\n\nA grounded wire is sometimes strung along the tops of the towers to provide lightning protection. An optical ground wire is a more advanced version with embedded optical fibers for communication. Overhead wire markers can be mounted on the ground wire to meet International Civil Aviation Organization recommendations.\nSome markers include flashing lamps for night-time warning.\n\nA \"single-circuit transmission line\" carries conductors for only one circuit. For a three-phase system, this implies that each tower supports three conductors.\n\nA \"double-circuit transmission line\" has two circuits. For three-phase systems, each tower supports and insulates six conductors. Single phase AC-power lines as used for traction current have four conductors for two circuits. Usually both circuits operate at the same voltage.\n\nIn HVDC systems typically two conductors are carried per line, but in rare cases only one pole of the system is carried on a set of towers.\n\nIn some countries like Germany most power lines with voltages above 100 kV are implemented as double, quadruple or in rare cases even hextuple power line as rights of way are rare. Sometimes all conductors are installed with the erection of the pylons; often some circuits are installed later. A disadvantage of double circuit transmission lines is that maintenance can be difficult, as either work in close proximity of high voltage or switch-off of two circuits is required. In case of failure, both systems can be affected.\n\nThe largest double-circuit transmission line is the Kita-Iwaki Powerline.\n\nInsulators must support the conductors and withstand both the normal operating voltage and surges due to switching and lightning. Insulators are broadly classified as either pin-type, which support the conductor above the structure, or suspension type, where the conductor hangs below the structure. The invention of the strain insulator was a critical factor in allowing higher voltages to be used.\n\nAt the end of the 19th century, the limited electrical strength of telegraph-style pin insulators limited the voltage to no more than 69,000 volts. Up to about 33 kV (69 kV in North America) both types are commonly used. At higher voltages only suspension-type insulators are common for overhead conductors.\n\nInsulators are usually made of wet-process porcelain or toughened glass, with increasing use of glass-reinforced polymer insulators. However, with rising voltage levels, polymer insulators (silicone rubber based) are seeing increasing usage. China has already developed polymer insulators having a highest system voltage of 1100 kV and India is currently developing a 1200 kV (highest system voltage) line which will initially be charged with 400 kV to be upgraded to a 1200 kV line.\n\nSuspension insulators are made of multiple units, with the number of unit insulator disks increasing at higher voltages. The number of disks is chosen based on line voltage, lightning withstand requirement, altitude, and environmental factors such as fog, pollution, or salt spray. In cases where these conditions are suboptimal, longer insulators must be used. Longer insulators with longer creepage distance for leakage current, are required in these cases. Strain insulators must be strong enough mechanically to support the full weight of the span of conductor, as well as loads due to ice accumulation, and wind.\n\nPorcelain insulators may have a semi-conductive glaze finish, so that a small current (a few milliamperes) passes through the insulator. This warms the surface slightly and reduces the effect of fog and dirt accumulation. The semiconducting glaze also ensures a more even distribution of voltage along the length of the chain of insulator units.\n\nPolymer insulators by nature have hydrophobic characteristics providing for improved wet performance. Also, studies have shown that the specific creepage distance required in polymer insulators is much lower than that required in porcelain or glass. Additionally, the mass of polymer insulators (especially in higher voltages) is approximately 50% to 30% less than that of a comparative porcelain or glass string. Better pollution and wet performance is leading to the increased use of such insulators.\n\nInsulators for very high voltages, exceeding 200 kV, may have grading rings installed at their terminals. This improves the electric field distribution around the insulator and makes it more resistant to flash-over during voltage surges.\n\nThe most common conductor in use for transmission today is aluminum conductor steel reinforced (ACSR). Also seeing much use is all-aluminum-alloy conductor (AAAC). Aluminum is used because it has about half the weight of a comparable resistance copper cable (though larger diameter due to lower specific conductivity), as well as being cheaper. \nCopper was more popular in the past and is still in use, especially at lower voltages and for grounding.\n\nWhile larger conductors may lose less energy due to lower electrical resistance, they are more costly than smaller conductors. An optimization rule called \"Kelvin's Law\" states that the optimum size of conductor for a line is found when the cost of the energy wasted in the conductor is equal to the annual interest paid on that portion of the line construction cost due to the size of the conductors. The optimization problem is made more complex by additional factors such as varying annual load, varying cost of installation, and the discrete sizes of cable that are commonly made.\n\nSince a conductor is a flexible object with uniform weight per unit length, the geometric shape of a conductor strung on towers approximates that of a catenary. The sag of the conductor (vertical distance between the highest and lowest point of the curve) varies depending on the temperature and additional load such as ice cover. A minimum overhead clearance must be maintained for safety. Since the temperature of the conductor increases with increasing heat produced by the current through it, it is sometimes possible to increase the power handling capacity (uprate) by changing the conductors for a type with a lower coefficient of thermal expansion or a higher allowable operating temperature.\n\nTwo such conductors that offer reduced thermal sag are known as composite core conductors (ACCR and ACCC conductor). In lieu of steel core strands that are often used to increase overall conductor strength, the ACCC conductor uses a carbon and glass fiber core that offers a coefficient of thermal expansion about 1/10 of that of steel. While the composite core is nonconductive, it is substantially lighter and stronger than steel, which allows the incorporation of 28% more aluminum (using compact trapezoidal-shaped strands) without any diameter or weight penalty. The added aluminum content helps reduce line losses by 25 to 40% compared to other conductors of the same diameter and weight, depending upon electric current. The carbon core conductor's reduced thermal sag allows it to carry up to twice the current (\"ampacity\") compared to all-aluminum conductor (AAC) or ACSR.\n\nThe power lines and their surroundings must be maintained by linemen, sometimes assisted by helicopters with pressure washers or circular saws which may work three times faster. However this work often occurs in the dangerous areas of the Helicopter height–velocity diagram, and the pilot must be qualified for this \"human external cargo\" method.\n\nFor transmission of power across long distances, high voltage transmission is employed. Transmission higher than 132 kV poses the problem of corona discharge, which causes significant power loss and interference with communication circuits. To reduce this corona effect, it is preferable to use more than one conductor per phase, or bundled conductors.\n\nBundle conductors consist of several parallel cables connected at intervals by spacers, often in a cylindrical configuration. The optimum number of conductors depends on the current rating, but typically higher-voltage lines also have higher current. American Electric Power is building 765 kV lines using six conductors per phase in a bundle. Spacers must resist the forces due to wind, and magnetic forces during a short-circuit.\n\nBundled conductors reduce the voltage gradient in the vicinity of the line. This reduces the possibility of corona discharge. At extra high voltage, the electric field gradient at the surface of a single conductor is high enough to ionize air, which wastes power, generates unwanted audible noise and interferes with communication systems. The field surrounding a bundle of conductors is similar to the field that would surround a single, very large conductor—this produces lower gradients which mitigates issues associated with high field strength. The transmission efficiency is improved as loss due to corona effect is countered.\n\nBundled conductors cool themselves more efficiently due to the increased surface area of the conductors, further reducing line losses. When transmitting alternating current, bundle conductors also avoid the reduction in ampacity of a single large conductor due to the skin effect. A bundle conductor also has lower reactance, compared to a single conductor.\n\nWhile wind resistance is higher, wind-induced oscillation can be damped at bundle spacers. The ice and wind loading of bundled conductors will be greater than a single conductor of the same total cross section, and bundled conductors are more difficult to install than single conductors.\n\nOverhead power lines are often equipped with a ground conductor (shield wire, static wire, or overhead earth wire). The ground conductor is usually grounded (earthed) at the top of the supporting structure, to minimize the likelihood of direct lightning strikes to the phase conductors. In circuits with earthed neutral, it also serves as a parallel path with the earth for fault currents. Very high-voltage transmission lines may have two ground conductors. These are either at the outermost ends of the highest cross beam, at two V-shaped mast points, or at a separate cross arm. Older lines may use surge arresters every few spans in place of a shield wire; this configuration is typically found in the more rural areas of the United States. By protecting the line from lightning, the design of apparatus in substations is simplified due to lower stress on insulation. Shield wires on transmission lines may include optical fibers (optical ground wires/OPGW), used for communication and control of the power system.\nAt some HVDC converter stations, the ground wire is used also as the electrode line to connect to a distant grounding electrode. This allows the HVDC system to use the earth as one conductor. The ground conductor is mounted on small insulators bridged by lightning arrestors above the phase conductors. The insulation prevents electrochemical corrosion of the pylon.\n\nMedium-voltage distribution lines may also use one or two shield wires, or may have the grounded conductor strung below the phase conductors to provide some measure of protection against tall vehicles or equipment touching the energized line, as well as to provide a neutral line in Wye wired systems.\n\nOn some power lines for very high voltages in the former Soviet Union, the ground wire is used for PLC-radio systems and mounted on insulators at the pylons.\n\nOverhead insulated cables are rarely used, usually for short distances (less than a kilometer). Insulated cables can be directly fastened to structures without insulating supports. An overhead line with bare conductors insulated by air is typically less costly than a cable with insulated conductors.\n\nA more common approach is \"covered\" line wire. It is treated as bare cable, but often is safer for wildlife, as the insulation on the cables increases the likelihood of a large-wing-span raptor to survive a brush with the lines, and reduces the overall danger of the lines slightly. These types of lines are often seen in the eastern United States and in heavily wooded areas, where tree-line contact is likely. The only pitfall is cost, as insulated wire is often costlier than its bare counterpart. Many utility companies implement covered line wire as jumper material where the wires are often closer to each other on the pole, such as an underground riser/pothead, and on reclosers, cutouts and the like.\n\nA compact overhead transmission line requires a smaller right of way than a standard overhead powerline. Conductors must not get too close to each other. This can be achieved either by short span lengths and insulating crossbars, or by separating the conductors in the span with insulators. The first type is easier to build as it does not require insulators in the span, which may be difficult to install and to maintain.\n\nExamples of compact lines are:\n\nCompact transmission lines may be designed for voltage upgrade of existing lines to increase the power that can be transmitted on an existing right of way.\n\nLow voltage overhead lines may use either bare conductors carried on glass or ceramic insulators or an aerial bundled cable system. The number of conductors may be anywhere between two (most likely a phase and neutral) up to as many as six (three phase conductors, separate neutral and earth plus street lighting supplied by a common switch); a common case is four (three phase and neutral, where the neutral might also serve as a protective earthing conductor).\n\nOverhead lines or overhead wires are used to transmit electrical energy to trams, trolleybuses or trains. Overhead line is designed on the principle of one or more overhead wires situated over rail tracks. Feeder stations at regular intervals along the overhead line supply power from the high-voltage grid. For some cases low-frequency AC is used, and distributed by a special traction current network.\n\nOverhead lines are also occasionally used to supply transmitting antennas, especially for efficient transmission of long, medium and short waves. For this purpose a staggered array line is often used. Along a staggered array line the conductor cables for the supply of the earth net of the transmitting antenna are attached on the exterior of a ring, while the conductor inside the ring, is fastened to insulators leading to the high-voltage standing feeder of the antenna.\n\nUse of the area below an overhead line is limited because objects must not come too close to the energized conductors. Overhead lines and structures may shed ice, creating a hazard. Radio reception can be impaired under a power line, due both to shielding of a receiver antenna by the overhead conductors, and by partial discharge at insulators and sharp points of the conductors which creates radio noise.\n\nIn the area surrounding the overhead lines it is dangerous to risk interference; e.g. flying kites or balloons, using ladders or operating machinery.\n\nOverhead distribution and transmission lines near airfields are often marked on maps, and the lines themselves marked with conspicuous plastic reflectors, to warn pilots of the presence of conductors.\n\nConstruction of overhead power lines, especially in wilderness areas, may have significant environmental effects. Environmental studies for such projects may consider the effect of bush clearing, changed migration routes for migratory animals, possible access by predators and humans along transmission corridors, disturbances of fish habitat at stream crossings, and other effects.\n\nGeneral aviation, hang gliding, paragliding, skydiving, balloon, and kite flying must avoid accidental contact with power lines. Nearly every kite product warns users to stay away from power lines. Deaths occur when aircraft crash into power lines. Some power lines are marked with obstruction makers, especially near air strips or over waterways that may support floatplane operations. The placement of power lines sometimes use up sites that would otherwise be used by hang gliders.\n\nThe first transmission of electrical impulses over an extended distance was demonstrated on July 14, 1729 by the physicist Stephen Gray. The demonstration used damp hemp cords suspended by silk threads (the low resistance of metallic conductors not being appreciated at the time).\n\nHowever the first practical use of overhead lines was in the context of telegraphy. By 1837 experimental commercial telegraph systems ran as far as 20 km (13 miles). Electric power transmission was accomplished in 1882 with the first high-voltage transmission between Munich and Miesbach (60 km). 1891 saw the construction of the first three-phase alternating current overhead line on the occasion of the International Electricity Exhibition in Frankfurt, between Lauffen and Frankfurt.\n\nIn 1912 the first 110 kV-overhead power line entered service followed by the first 220 kV-overhead power line in 1923. In the 1920s RWE AG built the first overhead line for this voltage and in 1926 built a Rhine crossing with the pylons of Voerde, two masts 138 meters high.\nIn 1953, the first 345 kV line was put into service by American Electric Power in the United States. In Germany in 1957 the first 380 kV overhead power line was commissioned (between the transformer station and Rommerskirchen). In the same year the overhead line traversing of the Strait of Messina went into service in Italy, whose pylons served the Elbe crossing 1. This was used as the model for the building of the Elbe crossing 2 in the second half of the 1970s which saw the construction of the highest overhead line pylons of the world. Earlier, in 1952, the first 380 kV line was put into service in Sweden, in 1000 km (625 miles) between the more populated areas in the south and the largest hydroelectric power stations in the north.\nStarting from 1967 in Russia, and also in the USA and Canada, overhead lines for voltage of 765 kV were built. In 1982 overhead power lines were built in Soviet Union between Elektrostal and the power station at Ekibastuz, this was a three-phase alternating current line at 1150 kV (Powerline Ekibastuz-Kokshetau). In 1999, in Japan the first powerline designed for 1000 kV with 2 circuits were built, the Kita-Iwaki Powerline. In 2003 the building of the highest overhead line commenced in China, the Yangtze River Crossing.\n\nAn overhead power line is one example of a transmission line. At power system frequencies, many useful simplifications can be made for lines of typical lengths. For analysis of power systems, the distributed resistance, series inductance, shunt leakage resistance and shunt capacitance can be replaced with suitable lumped values or simplified networks.\n\nA short length of a power line (less than 80 km) can be approximated with a resistance in series with an inductance and ignoring the shunt admittances. This value is not the total impedance of the line, but rather the series impedance per unit length of line. For a longer length of line (80–250 km), a shunt capacitance is added to the model. In this case it is common to distribute half of the total capacitance to each side of the line. As a result, the power line can be represented as a two-port network, such as with ABCD parameters.\n\nThe circuit can be characterized as\nwhere\nThe medium line has an additional shunt admittance\nwhere\n\n\n"}
{"id": "15677755", "url": "https://en.wikipedia.org/wiki?curid=15677755", "title": "Photovoltaic system", "text": "Photovoltaic system\n\nA photovoltaic system, also PV system or solar power system, is a power system designed to supply usable solar power by means of photovoltaics. It consists of an arrangement of several components, including solar panels to absorb and convert sunlight into electricity, a solar inverter to change the electric current from DC to AC, as well as mounting, cabling, and other electrical accessories to set up a working system. It may also use a solar tracking system to improve the system's overall performance and include an integrated battery solution, as prices for storage devices are expected to decline. Strictly speaking, a solar array only encompasses the ensemble of solar panels, the visible part of the PV system, and does not include all the other hardware, often summarized as balance of system (BOS). Moreover, PV systems convert light directly into electricity and shouldn't be confused with other technologies, such as concentrated solar power or solar thermal, used for heating and cooling.\n\nPV systems range from small, rooftop-mounted or building-integrated systems with capacities from a few to several tens of kilowatts, to large utility-scale power stations of hundreds of megawatts. Nowadays, most PV systems are grid-connected, while off-grid or stand-alone systems only account for a small portion of the market.\n\nOperating silently and without any moving parts or environmental emissions, PV systems have developed from being niche market applications into a mature technology used for mainstream electricity generation. A rooftop system recoups the invested energy for its manufacturing and installation within 0.7 to 2 years and produces about 95 percent of net clean renewable energy over a 30-year service lifetime.\n\nDue to the exponential growth of photovoltaics, prices for PV systems have rapidly declined in recent years. However, they vary by market and the size of the system. In 2014, prices for residential 5-kilowatt systems in the United States were around $3.29 per watt, while in the highly penetrated German market, prices for rooftop systems of up to 100 kW declined to €1.24 per watt. Nowadays, solar PV modules account for less than half of the system's overall cost, leaving the rest to the remaining BOS-components and to soft costs, which include customer acquisition, permitting, inspection and interconnection, installation labor and financing costs.\n\nA photovoltaic system converts the sun's radiation into usable electricity. It comprises the solar array and the balance of system components. PV systems can be categorized by various aspects, such as, grid-connected vs. stand alone systems, building-integrated vs. rack-mounted systems, residential vs. utility systems, distributed vs. centralized systems, rooftop vs. ground-mounted systems, tracking vs. fixed-tilt systems, and new constructed vs. retrofitted systems. Other distinctions may include, systems with microinverters vs. central inverter, systems using crystalline silicon vs. thin-film technology, and systems with modules from Chinese vs. European and U.S.-manufacturers.\n\nAbout 99 percent of all European and 90 percent of all U.S. solar power systems are connected to the electrical grid, while off-grid systems are somewhat more common in Australia and South Korea. PV systems rarely use battery storage. This may change soon, as government incentives for distributed energy storage are being implemented and investments in storage solutions are gradually becoming economically viable for small systems. A solar array of a typical residential PV system is rack-mounted on the roof, rather than integrated into the roof or facade of the building, as this is significantly more expensive. Utility-scale solar power stations are ground-mounted, with fixed tilted solar panels rather than using expensive tracking devices. Crystalline silicon is the predominant material used in 90 percent of worldwide produced solar modules, while rival thin-film has lost market-share in recent years. About 70 percent of all solar cells and modules are produced in China and Taiwan, leaving only 5 percent to European and US-manufacturers. The installed capacity for both, small rooftop systems and large solar power stations is growing rapidly and in equal parts, although there is a notable trend towards utility-scale systems, as the focus on new installations is shifting away from Europe to sunnier regions, such as the Sunbelt in the U.S., which are less opposed to ground-mounted solar farms and cost-effectiveness is more emphasized by investors.\n\nDriven by advances in technology and increases in manufacturing scale and sophistication, the cost of photovoltaics is declining continuously. There are several million PV systems distributed all over the world, mostly in Europe, with 1.4 million systems in Germany alone– as well as North America with 440,000 systems in the United States, The energy conversion efficiency of a conventional solar module increased from 15 to 20 percent over the last 10 years and a PV system recoups the energy needed for its manufacture in about 2 years. In exceptionally irradiated locations, or when thin-film technology is used, the so-called energy payback time decreases to one year or less.\nNet metering and financial incentives, such as preferential feed-in tariffs for solar-generated electricity, have also greatly supported installations of PV systems in many countries. The levelised cost of electricity from large-scale PV systems has become competitive with conventional electricity sources in an expanding list of geographic regions, and grid parity has been achieved in about 30 different countries.\n\nAs of 2015, the fast-growing global PV market is rapidly approaching the 200 GW mark – about 40 times the installed capacity of 2006. Photovoltaic systems currently contribute about 1 percent to worldwide electricity generation. Top installers of PV systems in terms of capacity are currently China, Japan and the United States, while half of the world's capacity is installed in Europe, with Germany and Italy supplying 7% to 8% of their respective domestic electricity consumption with solar PV. The International Energy Agency expects solar power to become the world's largest source of electricity by 2050, with solar photovoltaics and concentrated solar thermal contributing 16% and 11% to the global demand, respectively.\n\nA grid connected system is connected to a larger independent grid (typically the public electricity grid) and feeds energy directly into the grid. This energy may be shared by a residential or commercial building before or after the revenue measurement point. The difference being whether the credited energy production is calculated independently of the customer's energy consumption (feed-in tariff) or only on the difference of energy (net metering). Grid connected systems vary in size from residential (2–10 kW) to solar power stations (up to 10s of MW). This is a form of decentralized electricity generation. The feeding of electricity into the grid requires the transformation of DC into AC by a special, synchronising grid-tie inverter. In kilowatt-sized installations the DC side system voltage is as high as permitted (typically 1000V except US residential 600 V) to limit ohmic losses. Most modules (60 or 72 crystalline silicon cells) generate 160 W to 300 W at 36 volts. It is sometimes necessary or desirable to connect the modules partially in parallel rather than all in series. One set of modules connected in series is known as a 'string'.\n\nPhotovoltaic systems are generally categorized into three distinct market segments: residential rooftop, commercial rooftop, and ground-mount utility-scale systems. Their capacities range from a few kilowatts to hundreds of megawatts. A typical residential system is around 10 kilowatts and mounted on a sloped roof, while commercial systems may reach a megawatt-scale and are generally installed on low-slope or even flat roofs. Although rooftop mounted systems are small and display a higher cost per watt than large utility-scale installations, they account for the largest share in the market. There is, however, a growing trend towards bigger utility-scale power plants, especially in the \"sunbelt\" region of the planet.\n\n\n\n\nUncertainties in revenue over time relate mostly to the evaluation of the solar resource and to the performance of the system itself. In the best of cases, uncertainties are typically 4% for year-to-year climate variability, 5% for solar resource estimation (in a horizontal plane), 3% for estimation of irradiation in the plane of the array, 3% for power rating of modules, 2% for losses due to dirt and soiling, 1.5% for losses due to snow, and 5% for other sources of error. Identifying and reacting to manageable losses is critical for revenue and O&M efficiency. Monitoring of array performance may be part of contractual agreements between the array owner, the builder, and the utility purchasing the energy produced. Recently, a method to create \"synthetic days\" using readily available weather data and verification using the Open Solar Outdoors Test Field make it possible to predict photovoltaic systems performance with high degrees of accuracy. This method can be used to then determine loss mechanisms on a local scale - such as those from snow or the effects of surface coatings (e.g. hydrophobic or hydrophilic) on soiling or snow losses. (Although in heavy snow environments with severe ground interference can result in annual losses from snow of 30%.)\nAccess to the Internet has allowed a further improvement in energy monitoring and communication. Dedicated systems are available from a number of vendors. For solar PV systems that use microinverters (panel-level DC to AC conversion), module power data is automatically provided. Some systems allow setting performance alerts that trigger phone/email/text warnings when limits are reached. These solutions provide data for the system owner and the installer. Installers are able to remotely monitor multiple installations, and see at-a-glance the status of their entire installed base.\n\nA photovoltaic system for residential, commercial, or industrial energy supply consists of the solar array and a number of components often summarized as the balance of system (BOS). This term is synonymous with \"Balance of plant\" q.v. BOS-components include power-conditioning equipment and structures for mounting, typically one or more DC to AC power converters, also known as inverters, an energy storage device, a racking system that supports the solar array, electrical wiring and interconnections, and mounting for other components.\n\nOptionally, a balance of system may include any or all of the following: renewable energy credit revenue-grade meter, maximum power point tracker (MPPT), battery system and charger, GPS solar tracker, energy management software, solar irradiance sensors, anemometer, or task-specific accessories designed to meet specialized requirements for a system owner. In addition, a CPV system requires optical lenses or mirrors and sometimes a cooling system.\n\nThe terms \"solar array\" and \"PV system\" are often incorrectly used interchangeably, despite the fact that the solar array does not encompass the entire system. Moreover, \"solar panel\" is often used as a synonym for \"solar module\", although a panel consists of a string of several modules. The term \"solar system\" is also an often used misnomer for a PV system.\n\nConventional c-Si solar cells, normally wired in series, are encapsulated in a solar module to protect them from the weather. The module consists of a tempered glass as cover, a soft and flexible encapsulant, a rear backsheet made of a weathering and fire-resistant material and an aluminium frame around the outer edge. Electrically connected and mounted on a supporting structure, solar modules build a string of modules, often called solar panel. A solar array consists of one or many such panels. A photovoltaic array, or solar array, is a linked collection of solar modules. The power that one module can produce is seldom enough to meet requirements of a home or a business, so the modules are linked together to form an array. Most PV arrays use an inverter to convert the DC power produced by the modules into alternating current that can power lights, motors, and other loads. The modules in a PV array are usually first connected in series to obtain the desired voltage; the individual strings are then connected in parallel to allow the system to produce more current. Solar panels are typically measured under STC (standard test conditions) or PTC (PVUSA test conditions), in watts. Typical panel ratings range from less than 100 watts to over 400 watts. The array rating consists of a summation of the panel ratings, in watts, kilowatts, or megawatts.\n\n\nA typical \"150 watt\" PV module is about a square meter in size. Such a module may be expected to produce 0.75  kilowatt-hour (kWh) every day, on average, after taking into account the weather and the latitude, for an insolation of 5 sun hours/day. In the last 10 years, the efficiency of average commercial wafer-based crystalline silicon modules increased from about 12% to 16% and CdTe module efficiency increased from 9% to 13% during same period. Module output and life degraded by increased temperature. Allowing ambient air to flow over, and if possible behind, PV modules reduces this problem. Effective module lives are typically 25 years or more. The payback period for an investment in a PV solar installation varies greatly and is typically less useful than a calculation of return on investment. While it is typically calculated to be between 10 and 20 years, the financial payback period can be far shorter with incentives.\n\nDue to the low voltage of an individual solar cell (typically ca. 0.5V), several cells are wired \"(also see copper used in PV systems)\" in series in the manufacture of a \"laminate\". The laminate is assembled into a protective weatherproof enclosure, thus making a photovoltaic module or solar panel. Modules may then be strung together into a photovoltaic array. In 2012, solar panels available for consumers can have an efficiency of up to about 17%, while commercially available panels can go as far as 27%. It has been recorded that a group from The Fraunhofer Institute for Solar Energy Systems have created a cell that can reach 44.7% efficiency, which makes scientists' hopes of reaching the 50% efficiency threshold a lot more feasible.\n\n\nPhotovoltaic cell electrical output is extremely sensitive to shading. The effects of this shading are well known. When even a small portion of a cell, module, or array is shaded, while the remainder is in sunlight, the output falls dramatically due to internal 'short-circuiting' (the electrons reversing course through the shaded portion of the p-n junction). If the current drawn from the series string of cells is no greater than the current that can be produced by the shaded cell, the current (and so power) developed by the string is limited. If enough voltage is available from the rest of the cells in a string, current will be forced through the cell by breaking down the junction in the shaded portion. This breakdown voltage in common cells is between 10 and 30 volts. Instead of adding to the power produced by the panel, the shaded cell absorbs power, turning it into heat. Since the reverse voltage of a shaded cell is much greater than the forward voltage of an illuminated cell, one shaded cell can absorb the power of many other cells in the string, disproportionately affecting panel output. For example, a shaded cell may drop 8 volts, instead of adding 0.5 volts, at a particular current level, thereby absorbing the power produced by 16 other cells. It is, thus important that a PV installation is not shaded by trees or other obstructions.\n\nSeveral methods have been developed to determine shading losses from trees to PV systems over both large regions using LiDAR, but also at an individual system level using sketchup.\nMost modules have bypass diodes between each cell or string of cells that minimize the effects of shading and only lose the power of the shaded portion of the array. The main job of the bypass diode is to eliminate hot spots that form on cells that can cause further damage to the array, and cause fires. Sunlight can be absorbed by dust, snow, or other impurities at the surface of the module. This can reduce the light that strikes the cells. In general these losses aggregated over the year are small even for locations in Canada. Maintaining a clean module surface will increase output performance over the life of the module. Google found that cleaning the flat mounted solar panels after 15 months increased their output by almost 100%, but that the 5% tilted arrays were adequately cleaned by rainwater.\n\nSolar insolation is made up of direct, diffuse, and reflected radiation. The absorption factor of a PV cell is defined as the fraction of incident solar irradiance that is absorbed by the cell. At high noon on a cloudless day at the equator, the power of the sun is about 1 kW/m², on the Earth's surface, to a plane that is perpendicular to the sun's rays. As such, PV arrays can track the sun through each day to greatly enhance energy collection. However, tracking devices add cost, and require maintenance, so it is more common for PV arrays to have fixed mounts that tilt the array and face solar noon (approximately due south in the Northern Hemisphere or due north in the Southern Hemisphere). The tilt angle, from horizontal, can be varied for season, but if fixed, should be set to give optimal array output during the peak electrical demand portion of a typical year for a stand-alone system. This optimal module tilt angle is not necessarily identical to the tilt angle for maximum annual array energy output. The optimization of the photovoltaic system for a specific environment can be complicated as issues of solar flux, soiling, and snow losses should be taken into effect. In addition, recent work has shown that spectral effects can play a role in optimal photovoltaic material selection. For example, the spectral albedo can play a significant role in output depending on the surface around the photovoltaic system and the type of solar cell material. For the weather and latitudes of the United States and Europe, typical insolation ranges from 4 kWh/m²/day in northern climes to 6.5 kWh/m²/day in the sunniest regions. A photovoltaic installation in the southern latitudes of Europe or the United States may expect to produce 1 kWh/m²/day. A typical 1 kW photovoltaic installation in Australia or the southern latitudes of Europe or United States, may produce 3.5–5 kWh per day, dependent on location, orientation, tilt, insolation and other factors. In the Sahara desert, with less cloud cover and a better solar angle, one could ideally obtain closer to 8.3 kWh/m²/day provided the nearly ever present wind would not blow sand onto the units. The area of the Sahara desert is over 9 million km². 90,600 km², or about 1%, could generate as much electricity as all of the world's power plants combined.\n\nModules are assembled into arrays on some kind of mounting system, which may be classified as ground mount, roof mount or pole mount. For solar parks a large rack is mounted on the ground, and the modules mounted on the rack.\nFor buildings, many different racks have been devised for pitched roofs. For flat roofs, racks, bins and building integrated solutions are used. Solar panel racks mounted on top of poles can be stationary or moving, see Trackers below. Side-of-pole mounts are suitable for situations where a pole has something else mounted at its top, such as a light fixture or an antenna. Pole mounting raises what would otherwise be a ground mounted array above weed shadows and livestock, and may satisfy electrical code requirements regarding inaccessibility of exposed wiring. Pole mounted panels are open to more cooling air on their underside, which increases performance. A multiplicity of pole top racks can be formed into a parking carport or other shade structure. A rack which does not follow the sun from left to right may allow seasonal adjustment up or down.\n\nDue to their outdoor usage, solar cables are specifically designed to be resistant against UV radiation and extremely high temperature fluctuations and are generally unaffected by the weather. A number of standards specify the usage of electrical wiring in PV systems, such as the IEC 60364 by the International Electrotechnical Commission, in section 712 \"Solar photovoltaic (PV) power supply systems\", the British Standard BS 7671, incorporating regulations relating to microgeneration and photovoltaic systems, and the US UL4703 standard, in subject 4703 \"Photovoltaic Wire\".\n\nA solar tracking system tilts a solar panel throughout the day. Depending on the type of tracking system, the panel is either aimed directly at the sun or the brightest area of a partly clouded sky. Trackers greatly enhance early morning and late afternoon performance, increasing the total amount of power produced by a system by about 20–25% for a single axis tracker and about 30% or more for a dual axis tracker, depending on latitude.\nTrackers are effective in regions that receive a large portion of sunlight directly. In diffuse light (i.e. under cloud or fog), tracking has little or no value. Because most concentrated photovoltaics systems are very sensitive to the sunlight's angle, tracking systems allow them to produce useful power for more than a brief period each day. Tracking systems improve performance for two main reasons. First, when a solar panel is perpendicular to the sunlight, it receives more light on its surface than if it were angled. Second, direct light is used more efficiently than angled light. Special Anti-reflective coatings can improve solar panel efficiency for direct and angled light, somewhat reducing the benefit of tracking.\n\nTrackers and sensors to optimise the performance are often seen as optional, but tracking systems can increase viable output by up to 45%. PV arrays that approach or exceed one megawatt often use solar trackers. Accounting for clouds, and the fact that most of the world is not on the equator, and that the sun sets in the evening, the correct measure of solar power is insolation – the average number of kilowatt-hours per square meter per day. For the weather and latitudes of the United States and Europe, typical insolation ranges from 2.26 kWh/m²/day in northern climes to 5.61 kWh/m²/day in the sunniest regions.\n\nFor large systems, the energy gained by using tracking systems can outweigh the added complexity (trackers can increase efficiency by 30% or more). For very large systems, the added maintenance of tracking is a substantial detriment. Tracking is not required for flat panel and low-concentration photovoltaic systems. For high-concentration photovoltaic systems, dual axis tracking is a necessity. Pricing trends affect the balance between adding more stationary solar panels versus having fewer panels that track. When solar panel prices drop, trackers become a less attractive option.\n\nSystems designed to deliver alternating current (AC), such as grid-connected applications need an inverter to convert the direct current (DC) from the solar modules to AC. Grid connected inverters must supply AC electricity in sinusoidal form, synchronized to the grid frequency, limit feed in voltage to no higher than the grid voltage and disconnect from the grid if the grid voltage is turned off. Islanding inverters need only produce regulated voltages and frequencies in a sinusoidal waveshape as no synchronisation or co-ordination with grid supplies is required.\n\nA solar inverter may connect to a string of solar panels. In some installations a solar micro-inverter is connected at each solar panel. For safety reasons a circuit breaker is provided both on the AC and DC side to enable maintenance. AC output may be connected through an electricity meter into the public grid. The number of modules in the system determines the total DC watts capable of being generated by the solar array; however, the inverter ultimately governs the amount of AC watts that can be distributed for consumption. For example, a PV system comprising 11 kilowatts DC (kW) worth of PV modules, paired with one 10-kilowatt AC (kW) inverter, will be limited to the inverter's output of 10 kW. As of 2014, conversion efficiency for state-of-the-art converters reached more than 98 percent. While string inverters are used in residential to medium-sized commercial PV systems, central inverters cover the large commercial and utility-scale market. Market-share for central and string inverters are about 50 percent and 48 percent, respectively, leaving less than 2 percent to micro-inverters.\n\nMaximum power point tracking (MPPT) is a technique that grid connected inverters use to get the maximum possible power from the photovoltaic array. In order to do so, the inverter's MPPT system digitally samples the solar array's ever changing power output and applies the proper resistance to find the optimal \"maximum power point\".\n\nAnti-islanding is a protection mechanism that immediately shuts down the inverter preventing it from generating AC power when the connection to the load no longer exists. This happens, for example, in the case of a blackout. Without this protection, the supply line would become an \"island\" with power surrounded by a \"sea\" of unpowered lines, as the solar array continues to deliver DC power during the power outage. Islanding is a hazard to utility workers, who may not realize that an AC circuit is still powered, and it may prevent automatic re-connection of devices.\n\nAlthough still expensive, PV systems increasingly use rechargeable batteries to store a surplus to be later used at night. Batteries used for grid-storage also stabilize the electrical grid by leveling out peak loads, and play an important role in a smart grid, as they can charge during periods of low demand and feed their stored energy into the grid when demand is high.\n\nCommon battery technologies used in today's PV systems include the valve regulated lead-acid battery– a modified version of the conventional lead–acid battery, nickel–cadmium and lithium-ion batteries. Compared to the other types, lead-acid batteries have a shorter lifetime and lower energy density. However, due to their high reliability, low self discharge as well as low investment and maintenance costs, they are currently the predominant technology used in small-scale, residential PV systems, as lithium-ion batteries are still being developed and about 3.5 times as expensive as lead-acid batteries. Furthermore, as storage devices for PV systems are stationary, the lower energy and power density and therefore higher weight of lead-acid batteries are not as critical as, for example, in electric transportation Other rechargeable batteries that are considered for distributed PV systems include sodium–sulfur and vanadium redox batteries, two prominent types of a molten salt and a flow battery, respectively. In 2015, Tesla motors launched the Powerwall, a rechargeable lithium-ion battery with the aim to revolutionize energy consumption.\n\nPV systems with an integrated battery solution also need a charge controller, as the varying voltage and current from the solar array requires constant adjustment to prevent damage from overcharging. Basic charge controllers may simply turn the PV panels on and off, or may meter out pulses of energy as needed, a strategy called PWM or pulse-width modulation. More advanced charge controllers will incorporate MPPT logic into their battery charging algorithms. Charge controllers may also divert energy to some purpose other than battery charging. Rather than simply shut off the free PV energy when not needed, a user may choose to heat air or water once the battery is full.\n\nThe metering must be able to accumulate energy units in both directions or two meters must be used. Many meters accumulate bidirectionally, some systems use two meters, but a unidirectional meter (with detent) will not accumulate energy from any resultant feed into the grid. In some countries, for installations over 30 kW a frequency and a voltage monitor with disconnection of all phases is required. This is done where more solar power is being generated than can be accommodated by the utility, and the excess can not either be exported or stored. Grid operators historically have needed to provide transmission lines and generation capacity. Now they need to also provide storage. This is normally hydro-storage, but other means of storage are used. Initially storage was used so that baseload generators could operate at full output. With variable renewable energy, storage is needed to allow power generation whenever it is available, and consumption whenever it is needed. \nThe two variables a grid operator have are storing electricity for \"when\" it is needed, or transmitting it to \"where\" it is needed. If both of those fail, installations over 30kWp can automatically shut down, although in practice all inverters maintain voltage regulation and stop supplying power if the load is inadequate. Grid operators have the option of curtailing excess generation from large systems, although this is more commonly done with wind power than solar power, and results in a substantial loss of revenue. Three-phase inverters have the unique option of supplying reactive power which can be advantageous in matching load requirements.\n\nPhotovoltaic systems need to be monitored to detect breakdown and optimize their operation. There are several photovoltaic monitoring strategies depending on the output of the installation and its nature. Monitoring can be performed on site or remotely. It can measure production only, retrieve all the data from the inverter or retrieve all of the data from the communicating equipment (probes, meters, etc.). Monitoring tools can be dedicated to supervision only or offer additional functions. Individual inverters and battery charge controllers may include monitoring using manufacturer specific protocols and software. Energy metering of an inverter may be of limited accuracy and not suitable for revenue metering purposes. A third-party data acquisition system can monitor multiple inverters, using the inverter manufacturer's protocols, and also acquire weather-related information. Independent smart meters may measure the total energy production of a PV array system. Separate measures such as satellite image analysis or a solar radiation meter (a pyranometer) can be used to estimate total insolation for comparison. \nData collected from a monitoring system can be displayed remotely over the World Wide Web, such as OSOTF.\n\nThis section includes systems that are either highly specialized and uncommon or still an emerging new technology with limited significance. However, standalone or off-grid systems take a special place. They were the most common type of systems during the 1980s and 1990s, when PV technology was still very expensive and a pure niche market of small scale applications. Only in places where no electrical grid was available, they were economically viable. Although new stand-alone systems are still being deployed all around the world, their contribution to the overall installed photovoltaic capacity is decreasing. In Europe, off-grid systems account for 1 percent of installed capacity. In the United States, they account for about 10 percent. Off-grid systems are still common in Australia and South Korea, and in many developing countries.\n\nConcentrator photovoltaics (CPV) and \"high concentrator photovoltaic\" (HCPV) systems use optical lenses or curved mirrors to concentrate sunlight onto small but highly efficient solar cells. Besides concentrating optics, CPV systems sometime use solar trackers and cooling systems and are more expensive.\n\nEspecially HCPV systems are best suited in location with high solar irradiance, concentrating sunlight up to 400 times or more, with efficiencies of 24–28 percent, exceeding those of regular systems. Various designs of CPV and HCPV systems are commercially available but not very common. However, ongoing research and development is taking place.\n\nCPV is often confused with CSP (concentrated solar power) that does not use photovoltaics. Both technologies favor locations that receive much sunlight and are directly competing with each other.\n\nA hybrid system combines PV with other forms of generation, usually a diesel generator. Biogas is also used. The other form of generation may be a type able to modulate power output as a function of demand. However more than one renewable form of energy may be used e.g. wind. The photovoltaic power generation serves to reduce the consumption of non renewable fuel. Hybrid systems are most often found on islands. Pellworm island in Germany and Kythnos island in Greece are notable examples (both are combined with wind). The Kythnos plant has reduced diesel consumption by 11.2%.\n\nIn 2015, a case-study conducted in seven countries concluded that in all cases generating costs can be reduced by hybridising mini-grids and isolated grids. However, financing costs for such hybrids are crucial and largely depend on the ownership structure of the power plant. While cost reductions for state-owned utilities can be significant, the study also identified economic benefits to be insignificant or even negative for non-public utilities, such as independent power producers.\n\nThere has also been recent work showing that the PV penetration limit can be increased by deploying a distributed network of PV+CHP hybrid systems in the U.S. The temporal distribution of solar flux, electrical and heating requirements for representative U.S. single family residences were analyzed and the results clearly show that hybridizing CHP with PV can enable additional PV deployment above what is possible with a conventional centralized electric generation system. This theory was reconfirmed with numerical simulations using per second solar flux data to determine that the necessary battery backup to provide for such a hybrid system is possible with relatively small and inexpensive battery systems. In addition, large PV+CHP systems are possible for institutional buildings, which again provide back up for intermittent PV and reduce CHP runtime.\n\n\nFloating solar arrays are PV systems that float on the surface of drinking water reservoirs, quarry lakes, irrigation canals or remediation and tailing ponds. These systems are called \"floatovoltaics\" when used only for electrical production or \"aquavoltaics\" when such systems are used to synergistically enhance aquaculture. A small number of such systems exist in France, India, Japan, South Korea, the United Kingdom, Singapore and the United States.\n\nThe systems are said to have advantages over photovoltaics on land. The cost of land is more expensive, and there are fewer rules and regulations for structures built on bodies of water not used for recreation. Unlike most land-based solar plants, floating arrays can be unobtrusive because they are hidden from public view. They achieve higher efficiencies than PV panels on land, because water cools the panels. The panels have a special coating to prevent rust or corrosion.\n\nIn May 2008, the Far Niente Winery in Oakville, California, pioneered the world's first floatovoltaic system by installing 994 solar PV modules with a total capacity of 477 kW onto 130 pontoons and floating them on the winery's irrigation pond. The primary benefit of such a system is that it avoids the need to sacrifice valuable land area that could be used for another purpose. In the case of the Far Niente Winery, it saved three-quarters of an acre that would have been required for a land-based system. Another benefit of a floatovoltaic system is that the panels are kept at a cooler temperature than they would be on land, leading to a higher efficiency of solar energy conversion. The floating PV array also reduces the amount of water lost through evaporation and inhibits the growth of algae.\n\nUtility-scale floating PV farms are starting to be built. The multinational electronics and ceramics manufacturer Kyocera will develop the world's largest, a 13.4 MW farm on the reservoir above Yamakura Dam in Chiba Prefecture using 50,000 solar panels. Salt-water resistant floating farms are also being considered for ocean use, with experiments in Thailand. The largest so far announced floatovoltaic project is a 350 MW power station in the Amazon region of Brazil.\n\nDC grids are found in electric powered transport: railways trams and trolleybuses. A few pilot plants for such applications have been built, such as the tram depots in Hannover Leinhausen, using photovoltaic contributors and Geneva (Bachet de Pesay). The 150 kW Geneva site feeds 600V DC directly into the tram/trolleybus electricity network whereas before it provided about 15% of the electricity at its opening in 1999.\n\nA stand-alone or off-grid system is not connected to the electrical grid. Standalone systems vary widely in size and application from wristwatches or calculators to remote buildings or spacecraft. If the load is to be supplied independently of solar insolation, the generated power is stored and buffered with a battery. In non-portable applications where weight is not an issue, such as in buildings, lead acid batteries are most commonly used for their low cost and tolerance for abuse.\n\nA charge controller may be incorporated in the system to avoid battery damage by excessive charging or discharging. It may also help to optimize production from the solar array using a maximum power point tracking technique (MPPT). However, in simple PV systems where the PV module voltage is matched to the battery voltage, the use of MPPT electronics is generally considered unnecessary, since the battery voltage is stable enough to provide near-maximum power collection from the PV module.\nIn small devices (e.g. calculators, parking meters) only direct current (DC) is consumed. In larger systems (e.g. buildings, remote water pumps) AC is usually required. To convert the DC from the modules or batteries into AC, an inverter is used.\n\nIn agricultural settings, the array may be used to directly power DC pumps, without the need for an inverter. In remote settings such as mountainous areas, islands, or other places where a power grid is unavailable, solar arrays can be used as the sole source of electricity, usually by charging a storage battery. Stand-alone systems closely relate to microgeneration and distributed generation.\n\n\n\n\n\n\n\n\n\nThe cost of producing photovotaic cells have dropped due to economies of scale in production and technological advances in manufacturing. For large-scale installations, prices below $1.00 per watt were common by 2012. A price decrease of 50% had been achieved in Europe from 2006 to 2011 and there is a potential to lower the generation cost by 50% by 2020. Crystal silicon solar cells have largely been replaced by less expensive multicrystalline silicon solar cells, and thin film silicon solar cells have also been developed recently at lower costs of production. Although they are reduced in energy conversion efficiency from single crystalline \"siwafers\", they are also much easier to produce at comparably lower costs.\n\nThe table below shows the total cost in US cents per kWh of electricity generated by a photovoltaic system. The row headings on the left show the total cost, per peak kilowatt (kW), of a photovoltaic installation. Photovoltaic system costs have been declining and in Germany, for example, were reported to have fallen to USD 1389/kW by the end of 2014. The column headings across the top refer to the annual energy output in kWh expected from each installed kW. This varies by geographic region because the average insolation depends on the average cloudiness and the thickness of atmosphere traversed by the sunlight. It also depends on the path of the sun relative to the panel and the horizon. \nPanels are usually mounted at an angle based on latitude, and often they are adjusted seasonally to meet the changing solar declination. Solar tracking can also be utilized to access even more perpendicular sunlight, thereby raising the total energy output.\n\nThe calculated values in the table reflect the total cost in cents per kWh produced. They assume a 10% total capital cost (for instance 4% interest rate, 1% operating and maintenance cost, and depreciation of the capital outlay over 20 years). Normally, photovoltaic modules have a 25-year warranty.\n\nIn its 2014 edition of the \"Technology Roadmap: Solar Photovoltaic Energy\" report, the International Energy Agency (IEA) published prices in US$ per watt for residential, commercial and utility-scale PV systems for eight major markets in 2013.\n\nPhotovoltaic systems demonstrate a learning curve in terms of Levelized Cost of Electricity (LCOE), reducing its cost per kWh by 32.6% for every doubling of capacity. From the data of LCOE and cumulative installed capacity from International Renewable Energy Agency (IRENA) from 2010 to 2017, the learning curve equation for photovoltaic systems is given as\n\nformula_1\n\n\nIncreasing use of photovoltaic systems and integration of photovoltaic power into existing structures and techniques of supply and distribution increases the value of general standards and definitions for photovoltaic components and systems. The standards are compiled at the International Electrotechnical Commission (IEC) and apply to efficiency, durability and safety of cells, modules, simulation programs, plug connectors and cables, mounting systems, overall efficiency of inverters etc.\n\nWhile article 690 of the National Electric Code provides general guidelines for the installation of photovoltaic systems, these guidelines may be superseded by local laws and regulations. Often a permit is required necessitating plan submissions and structural calculations before work may begin. Additionally, many locales require the work to be performed under the guidance of a licensed electrician. Check with the local City/County AHJ (Authority Having Jurisdiction) to ensure compliance with any applicable laws or regulations.\n\nIn the United States, the Authority Having Jurisdiction (AHJ) will review designs and issue permits, before construction can lawfully begin. Electrical installation practices must comply with standards set forth within the National Electrical Code (NEC) and be inspected by the AHJ to ensure compliance with building code, electrical code, and fire safety code. Jurisdictions may require that equipment has been tested, certified, listed, and labeled by at least one of the Nationally Recognized Testing Laboratories (NRTL). Despite the complicated installation process, a recent list of solar contractors shows a majority of installation companies were founded since 2000.\n\n\nIn the UK, PV installations are generally considered permitted development and don't require planning permission. If the property is listed or in a designated area (National Park, Area of Outstanding Natural Beauty, Site of Special Scientific Interest or Norfolk Broads) then planning permission is required.\n\n\nIn the US, many localities require a permit to install a photovoltaic system. A grid-tied system normally requires a licensed electrician to make the connection between the system and the grid-connected wiring of the building. Installers who meet these qualifications are located in almost every state. The State of California prohibits homeowners' associations from restricting solar devices.\n\n\nAlthough Spain generates around 40% of its electricity via photovoltaic and other renewable energy sources, and cities such as Huelva and Seville boast nearly 3,000 hours of sunshine per year, Spain has issued a solar tax to account for the debt created by the investment done by the Spanish government. Those who do not connect to the grid can face up to a fine of 30 million euros ($40 million USD).\n\nPV has been a well-known method of generating clean, emission free electricity. PV systems are often made of PV modules and inverter (changing DC to AC). PV modules are mainly made of PV cells, which has no fundamental difference to the material for making computer chips. The process of producing PV cells (computer chips) is energy intensive and involves highly poisonous and environmental toxic chemicals. There are few PV manufacturing plants around the world producing PV modules with energy produced from PV. This measure greatly reduces the carbon footprint during the manufacturing process. Managing the chemicals used in the manufacturing process is subject to the factories' local laws and regulations.\n\nWith the increasing levels of rooftop photovoltaic systems, the energy flow becomes 2-way. When there is more local generation than consumption, electricity is exported to the grid. However, electricity network traditionally is not designed to deal with the 2- way energy transfer. Therefore, some technical issues may occur. For example, in Queensland Australia, there have been more than 30% of households with rooftop PV by the end of 2017. The famous Californian 2020 duck curve appears very often for a lot of communities from 2015 onwards. An over-voltage issue may come out as the electricity flows from these PV households back to the network. There are solutions to manage the over voltage issue, such as regulating PV inverter power factor, new voltage and energy control equipment at electricity distributor level, re-conductor the electricity wires, demand side management, etc. There are often limitations and costs related to these solutions.\n\nThere is no silver bullet in electricity or energy demand and bill management, because customers (sites) have different specific situations, e.g. different comfort/convenience needs, different electricity tariffs, or different usage patterns. Electricity tariff may have a few elements, such as daily access and metering charge, energy charge (based on kWh, MWh) or peak demand charge (e.g. a price for the highest 30min energy consumption in a month). PV is a promising option for reducing energy charge when electricity price is reasonably high and continuously increasing, such as in Australia and Germany. However, for sites with peak demand charge in place, PV may be less attractive if peak demands mostly occur in the late afternoon to early evening, for example residential communities. Overall, energy investment is largely an economical decision and it is better to make investment decisions based on systematical evaluation of options in operational improvement, energy efficiency, onsite generation and energy storage.\n\n"}
{"id": "34535570", "url": "https://en.wikipedia.org/wiki?curid=34535570", "title": "Plagioclimax community", "text": "Plagioclimax community\n\nA Plagioclimax community is an area or habitat in which the influences of the humans have prevented the ecosystem from developing further. The ecosystem may have been stopped from reaching its full climatic climax or deflected towards a different climax by activities such as:\n\n\nThese are known as disturbances, or arresting factors.\n\nIn each case, human activity has led to a community which is not the climax community expected in such an area. If the human activity continues, the community will be held in a stable position and further succession will not occur until the human activity ceases.\n\nAn example may be in a beach dune system where the impact of the human race has caused footpath erosion to occur, affecting the vegetation so that feet trampling on the dune plants eventually destroys them.\n\nThe uplands of Northern England were once covered by deciduous woodland. Some heather would have been present, but in relatively small amounts. Gradually the forests were removed during the early Middle Ages for timber and fuel purposes, and to create space for agricultural activities. The soil deteriorated as a result and heather came to dominate the plant community. Sheep grazing was the major form of agriculture in the area at the time and the sheep prevented the re-growth of woodland by destroying any young saplings.\n\nIn more recent times the process of controlled burning of the heather has taken place. The heather is burnt after 15 years of its life cycle before it becomes mature and allows colonisation of the area by other plants. The ash adds to the soil fertility and the new growth that results increase the productivity of the ecosystem and provides the sheep with a more nutritious diet than is provided by the elder heather. This controlled burning maintains a plant community which is not the climatic climax of the area, and is therefore a plagioclimax.\n\nIn Studland Heath in Dorset, England, the management agency prevents the climax community (in this case trees) becoming established. At Studland the aim is to keep the area as heathland, so that the small reptiles that inhabit the dune system continue to survive. If the area is allowed to develop naturally into woodland, the reptiles would be replaced by other species of animals.\n\nAnother good example of a plagioclimax community is Braunton Burrows, England.\n\n"}
{"id": "25023930", "url": "https://en.wikipedia.org/wiki?curid=25023930", "title": "Pre-consumer recycling", "text": "Pre-consumer recycling\n\nPre-consumer recycling is the reclamation of waste materials that were created during the process of manufacturing or delivering goods prior to their delivery to a consumer. Pre-consumer recycled materials can be broken down and remade into similar or different materials, or can be sold \"as is\" to third-party buyers who then use those materials for consumer products. One of the largest contributing industries to pre-consumer recycling is the textile industry, which recycles fibers, fabrics, trims and unsold \"new\" garments to third-party buyers.\n\nThere are generally two types of recycling: post-consumer and pre-consumer. Post-consumer recycling is the most heavily practiced form of recycling, where the materials being recycled have already passed through to the consumer.\n\nAccording to the Council for Textile Recycling, each year 750,000 tons of textile waste is recycled (pre- and post-consumer) into new raw materials for the automotive, furniture, mattress, coarse yarn, home furnishings, paper and other industries. Although this amount accounts for 75% of textile waste in the United States, there is little research on textile excess produced in countries that play a larger role in global textile production, such as China, Vietnam, Thailand, India and Bangladesh.\n\n"}
{"id": "10473105", "url": "https://en.wikipedia.org/wiki?curid=10473105", "title": "Prometheus Gas", "text": "Prometheus Gas\n\nPrometheus Gas S.A. is a Greek – Russian joint company, established in 1991 in Athens. The Russian natural gas export company Gazprom Export, on behalf of Gazprom, holds the 50% of the shares. Mr. Dimitrios Ch. Copelouzos, Chairman & Managing Director of Copelouzos Group, holds the 50% of shares.\n\nAmong the basic sectors of the business activity of the Copelouzos Group and of Prometheus Gas are the importation and marketing of Russian natural gas in the Greek Market, the participation in the development of the necessary infrastructure with active involvement in the construction of natural gas transmission pipelines, compressor stations and city natural gas distribution networks. Within the interests of the company also fall the promotion and the implementation of business plans, aiming at creating new major natural gas consumers such as the modern natural gas fired combined cycle power stations.\n\nPrometheus Gas has secured for at least 50 years, through Gazprom, large quantities of gas in excess of those contracted to be imported into Greece by the state utility DEPA. These quantities amount to 3 billion cubic meters (bcm) annually until 2016 and 7 bcm annually beyond this period.\n\n"}
{"id": "4661416", "url": "https://en.wikipedia.org/wiki?curid=4661416", "title": "REN21", "text": "REN21\n\nREN21 (Renewable Energy Policy Network for the 21st Century) is a global renewable energy policy multi-stakeholder network that connects a wide range of key actors. \nREN21’s goal is to facilitate knowledge exchange, policy development and joint action towards a rapid global transition to renewable energy. REN21 brings together governments, non-governmental organisations, research and academic institutions, international organisations and industry to learn from one another and build on successes that advance renewable energy. To assist policy decision-making, REN21 provides high-quality information, catalyses discussion and debate, and supports the development of thematic networks. REN21 facilitates the collection of comprehensive and timely information on renewable energy. This information reflects diverse viewpoints from both private and public sector actors, serving to dispel myths about renewable energy and to catalyse policy change. It does this through six product lines: Renewables Global Status Report (GSR): Regional Reports; Renewables Interactive Map; Global Futures Reports (GFR); Renewables Academy; International Renewable Energy Conference (IREC). \nThe REN21 Secretariat is based at UN Environment in Paris, France, and is a registered non-profit association under German law (e.V.). The organisation has more than 60 members and with its governance structure consists of the Bureau, the Steering Committee and REN21 Members. \n\nREN21 was launched in June 2004 as an outcome of the International Conference for Renewable Energies in Bonn, Germany. Paul Hugo Suding, was the first Executive Secretary upon REN21’s formation in 2006. He was succeeded by Virginia Sonntag O'Brien (2008-2011). Christine Lins was Executive Secretary 2011-2018. The current Executive Secretary is Rana Adib. \n\nThe Renewables Global Status Report (GSR) is the flagship product of REN21 and is released on a yearly basis, giving a status report of the deployment of renewable technologies globally. The GSR has been produced since 2005 and includes the three sectors power, heating & cooling and transport, reporting on policy development, energy industry, investment and market. The report is based on data and information produced by the REN21 member network of more than 800 experts and researchers from around the world. The GSR is openly peer-reviewed by the expert and research community, which contributes to the high transparency of the report. Today, the GSR is the most frequently referenced report on the renewable energy market, industry and policy trends. \nThe full report is available on a microsite along with its colourful infographics. References are easily accessible – users can place their mouse over a reference and see all the sources used and access them directly via individual hyperlinks. The infographics are incorporated throughout the text and the simple navigation structure allows the reader to jump easily from chapter to chapter. The new Google translate function allows the user to translate the full content into the reader’s native language.\n\nThe GSR report is complemented by a Highlights Report that presents the overarching trends and developments. It outlines what is happening to drive the energy transition and details why it is not happening fast enough or as fast as possible. This document draws extensively on the meticulously documented data found in the GSR. It is available in multiple languages \n\nThe Regional Status Report focus on the progress of renewable energy deployment in specific regions. The report process encourages and supports regional data collection processes as well as informed decision making. Regional status reports have been produced since 2009 and include: China (2009); India (2010); Middle East and Northern Africa (MENA, 2009); Economic Community of West African States (ECOWAS, 2014); Southern African Development Community (SADC, 2015); United Nations Economic Commission for Europe (UNECE, 2015); East African Community (EAC, 2016) and UNECE (2017). \n\nIn its Thematic Report series , REN21 looks at a certain topic in-depth. Past reports include a publication on mini-grid policies, a ten year overview of renewable energy developments, as well as tendering and community power in Latin America and the Caribbean. \nThe Renewable Interactive Map is a research tool for tracking the development of renewable energy worldwide. The interactive map complements the perspectives and findings of the Renewable Global Status Report and Regional Status Report series. The data includes statistics like renewable share of final energy demand, carbon pricing policy tracking and annual average renewable growth rate, on a global level, and can be extracted as a datapack. \n\nIn the Global Futures Report (GFR) series energy experts from around the world share their views and perspectives on the feasibility and challenges of achieving a world fed by renewable energy. The 2013 report provides a range of thinking about the future of renewables. The 2017 report documents global views about the feasibility of achieving a 100% renewable energy future by mid-century. The GFR does not include any forecasts; rather it aims to spur debate about opportunities and challenges of a 100% renewable future. \nREN21’s first Renewables Academy was held 10-12 November, 2014, in Bonn, Germany. It was held exclusively for REN21’s community, offering a venue for brainstorming on future-oriented policy solutions. The Academy offers a rich environment, stimulating active contribution and new ideas on issues central to a renewable energy transition as well as identifying its key policy drivers. A summary of what was discussed on the Academy of 2014 in Bonn can be found here. Plans are underway for an Academy in the latter half of 2018.\n\nInitiated at the renewables 2004 conference in Bonn, IRECs a high-level political conference series exclusively dedicated to renewable energy sector. The conference is held on a biennial basis, hosted by a national government and convened by REN21. \nThese multi-stakeholder events act as a common platform for government, private sector and civil society leaders to jointly address the goal of advancing renewable energy and have provided the impulse for several momentous initiatives over the past decade.\nIRECs have been held in the following countries: Beijing, China (BIREC, 2005); Washington, the U.S (WIREC, 2008); Delhi, India (DIREC, 2010); Abu Dhabi, the United Arab Emirates (ADIREC, 2013); South Africa (SAIREC, 2015); Mexico City, Mexico (MEXIREC, 2017). The next IREC will be hosted by the Republic of Korea, to be held 23-26 October 2019, in Seoul. \n\nREN21 works in cooperation with other organizations. A sister report to REN21’s GSR is the Global Trends in Renewable Energy Investment Report (GTR) produced by the Frankfurt School – UNEP Collaborating Center for Climate & Sustainable Energy Finance. REN21 is also a partner of the Global Tracking Framework (GTF), which is convened by the UN Secretary General’s Sustainable Energy for All (SEforAll) Initiative. REN21 also collaborates closely with other intergovernmental organisations such as the IEA, the World Bank and IRENA.\n\nThe diverse and interactive member network of REN21 is vital to the success of its work. By having contributing members from many different sectors, makes the work of REN21 unique and transparent. The network includes more than 60 members from industry associations, international organisations, national governments, NGOs and science and academia. REN21 is chaired by Arthouros Zervos; its Executive Secretary is Rana Adib. \n\n\n\n"}
{"id": "1815563", "url": "https://en.wikipedia.org/wiki?curid=1815563", "title": "Rockfall", "text": "Rockfall\n\nA rockfall or rock-fall refers to quantities of rock falling freely from a cliff face. The term is also used for collapse of rock from roof or walls of mine or quarry workings. A rockfall is a fragment of rock (a block) detached by sliding, toppling, or falling, that falls along a vertical or sub-vertical cliff, proceeds down slope by bouncing and flying along ballistic trajectories or by rolling on talus or debris slopes,” (Varnes, 1978). Alternatively, a \"rockfall is the natural downward motion of a detached block or series of blocks with a small volume involving free falling, bouncing, rolling, and sliding\". The mode of failure differs from that of a rockslide.\n\nFavourable geology and climate are the principal causal mechanisms of rockfall, factors that include intact condition of the rock mass, discontinuities within the rockmass, weathering susceptibility, ground and surface water, freeze-thaw, root-wedging, and external stresses. A tree may be blown by the wind, and this causes a pressure at the root level and this loosens rocks and can trigger a fall. The pieces of rock collect at the bottom creating a \"talus\" or \"scree\". Rocks falling from the cliff may dislodge other rocks and serve to create another mass wasting process, for example an avalanche.\n\nA cliff that has favorable geology to a rockfall may be said to be incompetent. One that is not favorable to a rockfall, which is better consolidated, may be said to be competent.\n\nTypically, rockfall events are mitigated in one of two ways: either by passive mitigation or active mitigation. Passive mitigation is where only the effects of the rockfall event are mitigated and are generally employed in the deposition or run-out zones, such as through the use of drape nets, rockfall catchment fences, diversion dams, etc. The rockfall still takes place but an attempt is made to control the outcome. In contrast, active mitigation is carried out in the initiation zone and prevents the rockfall event from ever occurring. Some examples of these measures are rock bolting, slope retention systems, shotcrete, etc. Other active measures might be by changing the geographic or climatic characteristics in the initiation zone, e.g. altering slope geometry, dewatering the slope, revegetation, etc.\n\nDesign guides of passive measures have been proposed by Ritchie (1963), Pierson et al. (2001), Pantelidis (2010) and Bar et al. (2016)\n\nThe effect of rockfalls on trees can be seen in several ways. The tree roots may rotate, via the rotational energy of the rockfall. The tree may move via the application of translational energy. And lastly deformation may occur, either elastic or plastic. Dendrochronology can reveal a past impact, with missing tree rings, as the tree rings grow around and close over a gap; the callus tissue can be seen microscopically. A macroscopic section can be used for dating of avalanche and rockfall events.\n\n"}
{"id": "3051050", "url": "https://en.wikipedia.org/wiki?curid=3051050", "title": "Runaway breakdown", "text": "Runaway breakdown\n\nRunaway breakdown is a theory of lightning initiation proposed by Alex Gurevich in 1992.\n\nElectrons in air have a mean free path of ~1 cm. Fast electrons which move at a large fraction of the speed of light have a mean free path up to 100 times longer. Given the long free paths, an electric field can accelerate these electrons to energies far higher than that of initially static electrons. If they strike air molecules, more relativistic electrons will be released, creating an avalanche multiplication of \"runaway\" electrons. This process, relativistic runaway electron avalanche, has been hypothesized to lead to electrical breakdown in thunderstorms, but only when a source of high-energy electrons from a cosmic ray is present to start the \"runaway\" process.\n\nThe resulting conductive plasma trail, many tens of meters long, is suggested to supply the \"seed\" which triggers a lightning flash.\n\n\n"}
{"id": "13693372", "url": "https://en.wikipedia.org/wiki?curid=13693372", "title": "Solano (wind)", "text": "Solano (wind)\n\nThe Solano is a south to south-easterly wind in the southern sector of Spain. In the spring and summer, from June to September, it carries hot, dry, suffocating weather over La Mancha and the Andalusian plain with the cities of Sevilla and Cádiz. It is also known as the Levant.\n"}
{"id": "31655758", "url": "https://en.wikipedia.org/wiki?curid=31655758", "title": "Spray (liquid drop)", "text": "Spray (liquid drop)\n\nA spray is a dynamic collection of drops dispersed in a gas. The process of forming a spray is known as atomization. A spray nozzle is the device used to generate a spray. The two main uses of sprays are to distribute material over a cross-section and to generate liquid surface area. There are thousands of applications in which sprays allow material to be used most efficiently. The spray characteristics required must be understood in order to select the most appropriate technology, optimal device and size.\n\nSpray atomization can be formed by several methods. The most common method is through a spray nozzle which typically has a fluid passage that is acted upon by different mechanical forces that atomize the liquid. The first atomization nozzle was invented by Thomas A. DeVilbiss of Toledo, Ohio in the late 1800s His invention was bulb atomizer that used pressure to impinge upon a liquid, breaking the liquid into a fine mist. Spray formation has taken on several forms, the most common being, pressure sprayers, centrifugal, electrostatic and ultrasonic nozzle.\n\nSprays of hydrocarbon liquids (fossil fuels) are among the most economically significant applications of sprays. Examples include fuel injectors for gasoline and diesel engines, atomizers for jet engines (gas turbines), atomizers for injecting heavy fuel oil into combustion air in steam boiler injectors, and rocket engine injectors. Drop size is critical because the large surface area of a finely atomized spray enhances fuel evaporation rate. Dispersion of the fuel into the combustion air is critical to maximize the efficiency of these systems and minimize emissions of pollutants (soot, NOx, CO).\n\nLimestone slurry is sprayed with single fluid spray nozzles to control acid gas emissions especially sulfur dioxide (SO2) emissions from coal-fired power plants with liquid scrubbers. Calcium hydroxide (lime) is atomized into a spray dryer absorber to remove acid gases (SO2 and HCl) from coal-fired power plants. Water is sprayed to remove particulate solids using a spray tower or a cyclonic spray scrubber Cooling towers use spray nozzles to distribute water.\n\n\nSprays are used extensively in manufacturing. Some typical applications are applying adhesive, lubricating bearings, and cooling tools in machining operations.\n\n\n\n\n\n\n\n\n\nSpray application of herbicides, insecticides, and pesticides is essential to distribute these materials over the intended target surface. Pre-emergent herbicides are sprayed onto soil, but many materials are applied to the plant leaf surface. Agricultural sprays include the spraying of cropland, forest, turf grass, and orchards. The sprayer may be a hand nozzle, on a ground vehicle, or on an aircraft. Herbicides, insecticides and pesticides are spray applied to soil or plant foliage to distribute and disperse these materials. See aerial application, pesticide application, sprayer.\nThe control of spray characteristics is critical to provide the coverage of foliage and to minimize off target drifting of the spray to adjacent areas. (pesticide drift). Spray drift is managed by applying only in appropriate wind conditions and humidity, and by controlling drop size and drop size distribution. Minimizing the height of the spray boom above the crop reduces drift. The spray nozzle type and size and the operating pressure provide the correct application rate of the material and control the amount of driftable fines.\nSpays, single fluid nozzles, are also used to cool animals\n\nAtomizers are used with pump-operated sprays of household cleaning products. The function of these nozzles is to distribute the product over an area. see Aerosol spray and spray can\n"}
{"id": "36389653", "url": "https://en.wikipedia.org/wiki?curid=36389653", "title": "Thorium-based nuclear power", "text": "Thorium-based nuclear power\n\nThorium-based nuclear power generation is fueled primarily by the nuclear fission of the isotope uranium-233 produced from the fertile element thorium. According to proponents, a thorium fuel cycle offers several potential advantages over a uranium fuel cycle—including much greater abundance of thorium on Earth, superior physical and nuclear fuel properties, and reduced nuclear waste production. However, development of thorium power has significant start-up costs. Proponents also cite the lack of weaponization potential as an advantage of thorium, while critics say that development of breeder reactors in general (including thorium reactors, which are breeders by nature) increases proliferation concerns. Since about 2008, nuclear energy experts have become more interested in thorium to supply nuclear fuel in place of uranium to generate nuclear power. This renewed interest has been highlighted in a number of scientific conferences, the latest of which, ThEC13 was held at CERN by iThEC and attracted over 200 scientists from 32 countries.\n\nA nuclear reactor consumes certain specific fissile isotopes to produce energy. The three most practical types of nuclear reactor fuel are:\n\nSome believe thorium is key to developing a new generation of cleaner, safer nuclear power. According to a 2011 opinion piece by a group of scientists at the Georgia Institute of Technology, considering its overall potential, thorium-based power \"can mean a 1000+ year solution or a quality low-carbon bridge to truly sustainable energy sources solving a huge portion of mankind’s negative environmental impact.\"\n\nAfter studying the feasibility of using thorium, nuclear scientists Ralph W. Moir and Edward Teller suggested that thorium nuclear research should be restarted after a three-decade shutdown and that a small prototype plant should be built.\n\nAfter World War II, uranium-based nuclear reactors were built to produce electricity. These were similar to the reactor designs that produced material for nuclear weapons. During that period, the government of the United States also built an experimental molten salt reactor using U-233 fuel, the fissile material created by bombarding thorium with neutrons. The MSRE reactor, built at Oak Ridge National Laboratory, operated critical for roughly 15,000 hours from 1965 to 1969. In 1968, Nobel laureate and discoverer of plutonium, Glenn Seaborg, publicly announced to the Atomic Energy Commission, of which he was chairman, that the thorium-based reactor had been successfully developed and tested.\n\nIn 1973, however, the US government settled on uranium technology and largely discontinued thorium-related nuclear research. The reasons were that uranium-fueled reactors were more efficient, the research was proven, and thorium's breeding ratio was thought insufficient to produce enough fuel to support development of a commercial nuclear industry. As Moir and Teller later wrote, \"The competition came down to a liquid metal fast breeder reactor (LMFBR) on the uranium-plutonium cycle and a thermal reactor on the thorium-233U cycle, the molten salt breeder reactor. The LMFBR had a larger breeding rate ... and won the competition.\" In their opinion, the decision to stop development of thorium reactors, at least as a backup option, “was an excusable mistake.”\n\nScience writer Richard Martin states that nuclear physicist Alvin Weinberg, who was director at Oak Ridge and primarily responsible for the new reactor, lost his job as director because he championed development of the safer thorium reactors. Weinberg himself recalls this period:\n\nMartin explains that Weinberg's unwillingness to sacrifice potentially safe nuclear power for the benefit of military uses forced him to retire:\nDespite the documented history of thorium nuclear power, many of today’s nuclear experts were nonetheless unaware of it. According to \"Chemical & Engineering News\", \"most people—including scientists—have hardly heard of the heavy-metal element and know little about it...,\" noting a comment by a conference attendee that \"it's possible to have a Ph.D. in nuclear reactor technology and not know about thorium energy.\" Nuclear physicist Victor J. Stenger, for one, first learned of it in 2012:\n\nOthers, including former NASA scientist and thorium expert Kirk Sorensen, agree that \"thorium was the alternative path that was not taken … \" According to Sorensen, during a documentary interview, he states that if the US had not discontinued its research in 1974 it could have \"probably achieved energy independence by around 2000.\"\n\nThe World Nuclear Association explains some of the possible benefits\nMoir and Teller agree, noting that the possible advantages of thorium include \"utilization of an abundant fuel, inaccessibility of that fuel to terrorists or for diversion to weapons use, together with good economics and safety features … \"\nThorium is considered the \"most abundant, most readily available, cleanest, and safest energy source on Earth,\" adds science writer Richard Martin.\n\n\nSummarizing some of the potential benefits, Martin offers his general opinion: \"Thorium could provide a clean and effectively limitless source of power while allaying all public concern—weapons proliferation, radioactive pollution, toxic waste, and fuel that is both costly and complicated to process. From an economics viewpoint, UK business editor Ambrose Evans-Pritchard has suggested that \"Obama could kill fossil fuels overnight with a nuclear dash for thorium,\" suggesting a \"new Manhattan Project,\" and adding, \"If it works, Manhattan II could restore American optimism and strategic leadership at a stroke …\" Moir and Teller estimated in 2004 that the cost for their recommended prototype would be \"well under $1 billion with operation costs likely on the order of $100 million per year,\" and as a result a \"large-scale nuclear power plan\" usable by many countries could be set up within a decade.\n\nA report by the Bellona Foundation in 2013 concluded that the economics are quite speculative. Thorium nuclear reactors are unlikely to produce cheaper energy, but the management of spent fuel is likely to be cheaper than for uranium nuclear reactors.\n\nSome experts note possible specific disadvantages of thorium nuclear power:\n\n\nResearch and development of thorium-based nuclear reactors, primarily the Liquid fluoride thorium reactor (LFTR), MSR design, has been or is now being done in the United States, United Kingdom, Germany, Brazil, India, China, France, the Czech Republic, Japan, Russia, Canada, Israel, and the Netherlands. Conferences with experts from as many as 32 countries are held, including one by the European Organization for Nuclear Research (CERN) in 2013, which focuses on thorium as an alternative nuclear technology without requiring production of nuclear waste. Recognized experts, such as Hans Blix, former head of the International Atomic Energy Agency, calls for expanded support of new nuclear power technology, and states, \"the thorium option offers the world not only a new sustainable supply of fuel for nuclear power but also one that makes better use of the fuel's energy content.\"\n\nCANDU reactors are capable of using thorium, and Thorium Power Canada has, in 2013, planned and proposed developing thorium power projects for Chile and Indonesia.\n\nThe proposed 10 MW demonstration reactor in Chile could be used to power a 20 million litre/day desalination plant. All land and regulatory approvals are currently in process.\n\nThorium Power Canada's proposal for the development of a 25 MW thorium reactor in Indonesia is meant to be a \"demonstration power project\" which could provide electrical power to the country’s power grid.\n\nIn 2018, the New Brunswick Energy Solutions Corporation announced the participation of Moltex Energy in the nuclear research cluster that will work on research and development on small modular reactor technology.\n\nAt the 2011 annual conference of the Chinese Academy of Sciences, it was announced that \"China has initiated a research and development project in thorium MSR technology.\" In addition, Dr. Jiang Mianheng, son of China's former leader Jiang Zemin, led a thorium delegation in non-disclosure talks at Oak Ridge National Laboratory, Tennessee, and by late 2013 China had officially partnered with Oak Ridge to aid China in its own development. The World Nuclear Association notes that the China Academy of Sciences in January 2011 announced its R&D program, \"claiming to have the world's largest national effort on it, hoping to obtain full intellectual property rights on the technology.\" According to Martin, \"China has made clear its intention to go it alone,\" adding that China already has a monopoly over most of the world's rare earth minerals.\n\nIn March 2014, with their reliance on coal-fired power having become a major cause of their current \"smog crisis,\" they reduced their original goal of creating a working reactor from 25 years down to 10. \"In the past, the government was interested in nuclear power because of the energy shortage. Now they are more interested because of smog,\" said Professor Li Zhong, a scientist working on the project. \"This is definitely a race,\" he added.\n\nIn early 2012, it was reported that China, using components produced by the West and Russia, planned to build two prototype thorium MSRs by 2015, and had budgeted the project at $400 million and requiring 400 workers.\" China also finalized an agreement with a Canadian nuclear technology company to develop improved CANDU reactors using thorium and uranium as a fuel.\n\nThe German THTR-300 was a prototype commercial power station using thorium as fertile and highly enriched U-235 as fissile fuel. Though named thorium high temperature reactor, mostly U-235 was fissioned. The THTR-300 was a helium-cooled high-temperature reactor with a pebble-bed reactor core consisting of approximately 670,000 spherical fuel compacts each 6 centimetres (2.4 in) in diameter with particles of uranium-235 and thorium-232 fuel embedded in a graphite matrix. \nIt fed power to Germany's grid for 432 days in the late 1980s, before it was shut down for cost, mechanical and other reasons.\n\nIndia has one of the largest supplies of thorium in the world, with comparatively poor quantities of uranium. India has projected meeting as much as 30% of its electrical demands through thorium by 2050.\n\nIn February 2014, Bhabha Atomic Research Centre (BARC), in Mumbai, India, presented their latest design for a \"next-generation nuclear reactor\" that will burn thorium as its fuel ore, calling it the Advanced Heavy Water Reactor (AWHR). They estimated the reactor could function without an operator for 120 days. Validation of its core reactor physics was underway by late 2017.\n\nAccording to Dr R K Sinha, chairman of their Atomic Energy Commission, \"This will reduce our dependence on fossil fuels, mostly imported, and will be a major contribution to global efforts to combat climate change.\" Because of its inherent safety, they expect that similar designs could be set up \"within\" populated cities, like Mumbai or Delhi.\n\nIndia's government is also developing up to 62, mostly thorium reactors, which it expects to be operational by 2025. It is the \"only country in the world with a detailed, funded, government-approved plan\" to focus on thorium-based nuclear power. The country currently gets under 2% of its electricity from nuclear power, with the rest coming from coal (60%), hydroelectricity (16%), other renewable sources (12%) and natural gas (9%). It expects to produce around 25% of its electricity from nuclear power. In 2009 the chairman of the Indian Atomic Energy Commission said that India has a \"long-term objective goal of becoming energy-independent based on its vast thorium resources.\"\n\nIn late June 2012, India announced that their \"first commercial fast reactor\" was near completion making India the most advanced country in thorium research.\" We have huge reserves of thorium. The challenge is to develop technology for converting this to fissile material,\" stated their former Chairman of India's Atomic Energy Commission. That vision of using thorium in place of uranium was set out in the 1950s by physicist Homi Bhabha. India's first commercial fast breeder reactor — the 500 MWe Prototype Fast Breeder Reactor (PFBR) — is approaching completion at the Indira Gandhi Centre for Atomic Research, Kalpakkam, Tamil Nadu.\n\nAs of July 2013 the major equipment of the PFBR had been erected and the loading of \"dummy\" fuels in peripheral locations was in progress. The reactor was expected to go critical by September 2014. The Centre had sanctioned Rs. 5,677 crore for building the PFBR and “we will definitely build the reactor within that amount,” Mr. Kumar asserted. The original cost of the project was Rs. 3,492 crore, revised to Rs. 5,677 crore. Electricity generated from the PFBR would be sold to the State Electricity Boards at Rs. 4.44 a unit. BHAVINI builds breeder reactors in India.\n\nIn 2013 India's 300 MWe AHWR (pressurized heavy water reactor) was slated to be built at an undisclosed location. The design envisages a start up with reactor grade plutonium that will breed U-233 from Th-232. Thereafter thorium is to be the only fuel. As of 2017, the design is in the final stages of validation.\n\nBy Nov 2015 the PFBR was built and expected to \nDelays have since postponed the commissioning [criticality?] of the PFBR to Sept 2016, but India's commitment to long-term nuclear energy production is underscored by the approval in 2015 of ten new sites for reactors of unspecified types, though procurement of primary fissile material – preferably plutonium – may be problematic due to India's low uranium reserves and capacity for production.\n\nIn May 2010, researchers from Ben-Gurion University of the Negev in Israel and Brookhaven National Laboratory in New York began to collaborate on the development of thorium reactors, aimed at being self-sustaining, \"meaning one that will produce and consume about the same amounts of fuel,\" which is not possible with uranium in a light water reactor.\n\nIn June 2012, Japan utility Chubu Electric Power wrote that they regard thorium as \"one of future possible energy resources.\"\n\nIn late 2012, Norway's privately owned Thor Energy, in collaboration with the government and Westinghouse, announced a four-year trial using thorium in an existing nuclear reactor.\" In 2013, Aker Solutions purchased patents from Nobel Prize winning physicist Carlo Rubbia for the design of a proton accelerator-based thorium nuclear power plant.\n\nIn Britain, one organisation promoting or examining research on thorium-based nuclear plants is The Alvin Weinberg Foundation. House of Lords member Bryony Worthington is promoting thorium, calling it “the forgotten fuel” that could alter Britain’s energy plans. However, in 2010, the UK’s National Nuclear Laboratory (NNL) concluded that for the short to medium term, \"...the thorium fuel cycle does not currently have a role to play,\" in that it is \"technically immature, and would require a significant financial investment and risk without clear benefits,\" and concluded that the benefits have been \"overstated.\" Friends of the Earth UK considers research into it as \"useful\" as a fallback option.\n\nIn its January 2012 report to the United States Secretary of Energy, the Blue Ribbon Commission on America's Future notes that a \"molten-salt reactor using thorium [has] also been proposed.\" That same month it was reported that the US Department of Energy is \"quietly collaborating with China\" on thorium-based nuclear power designs using an MSR.\n\nSome experts and politicians want thorium to be \"the pillar of the U.S. nuclear future.\" Senators Harry Reid and Orrin Hatch have supported using $250 million in federal research funds to revive ORNL research. In 2009, Congressman Joe Sestak unsuccessfully attempted to secure funding for research and development of a destroyer-sized reactor [reactor of a size to power a destroyer] using thorium-based liquid fuel.\n\nAlvin Radkowsky, chief designer of the world’s second full-scale atomic electric power plant in Shippingport, Pennsylvania, founded a joint US and Russian project in 1997 to create a thorium-based reactor, considered a \"creative breakthrough.\" In 1992, while a resident professor in Tel Aviv, Israel, he founded the US company, Thorium Power Ltd., near Washington, D.C., to build thorium reactors.\n\nThe primary fuel of the proposed HTR research project near Odessa, Texas, United States, will be ceramic-coated thorium beads. The earliest the reactor would become operational was 2015.\n\nOn the research potential of thorium-based nuclear power, Richard L. Garwin, winner of the Presidential Medal of Freedom, and Georges Charpak advise further study of the Energy amplifier in their book Megawatts and Megatons (2001), pages 153-163.\n\nThorium is mostly found with the rare earth phosphate mineral, monazite, which contains up to about 12% thorium phosphate, but 6-7% on average. World monazite resources are estimated to be about 12 million tons, two-thirds of which are in heavy mineral sands deposits on the south and east coasts of India. There are substantial deposits in several other countries \"(see table \"World thorium reserves\")\". Monazite is a good source of REEs (Rare Earth Element), but monazites are currently not economical to produce because the radioactive thorium that is produced as a byproduct would have to be stored indefinitely. However, if thorium-based power plants were adopted on a large-scale, virtually all the world's thorium requirements could be supplied simply by refining monazites for their more valuable REEs.\n\nAnother estimate of reasonably assured reserves (RAR) and estimated additional reserves (EAR) of thorium comes from OECD/NEA, Nuclear Energy, \"Trends in Nuclear Fuel Cycle\", Paris, France (2001). \"(see table \"IAEA Estimates in tons\")\"\nThe preceding figures are reserves and as such refer to the amount of thorium in high-concentration deposits inventoried so far and estimated to be extractable at current market prices; millions of times more total exist in Earth's 3 tonne crust, around 120 trillion tons of thorium, and lesser but vast quantities of thorium exist at intermediate concentrations. Proved reserves are a good indicator of the total future supply of a mineral resource.\n\nAccording to the World Nuclear Association, there are seven types of reactors that can be designed to use thorium as a nuclear fuel. Six of these have all entered into operational service at some point. The seventh is still conceptual, although currently in development by many countries:\n\n\n"}
{"id": "3629797", "url": "https://en.wikipedia.org/wiki?curid=3629797", "title": "Vacuum furnace", "text": "Vacuum furnace\n\nA vacuum furnace is a type of furnace in which the product in the furnace is surrounded by a vacuum during processing. The absence of air or other gases prevents oxidation, heat loss from the product through convection, and removes a source of contamination. This enables the furnace to heat materials (typically metals and ceramics) to temperatures as high as with select materials. Maximum furnace temperatures and vacuum levels depend on melting points and vapor pressures of heated materials. Vacuum furnaces are used to carry out processes such as annealing, brazing, sintering and heat treatment with high consistency and low contamination.\n\nCharacteristics of a vacuum furnace are:\n\n\nHeating metals to high temperatures in open to atmosphere normally causes rapid oxidation, which is undesirable. A vacuum furnace removes the oxygen and prevents this from happening.\n\nAn inert gas, such as Argon, is often used to quickly cool the treated metals back to non-metallurgical levels (below ) after the desired process in the furnace. This inert gas can be pressurized to two times atmosphere or more, then circulated through the hot zone area to pick up heat before passing through a heat exchanger to remove heat. This process continues until the desired temperature is reached.\n\nVacuum furnaces are used in a wide range of applications in both production industries and research laboratories.\n\nAt temperatures below 1200 °C, a vacuum furnace is commonly used for the heat treatment of steel alloys. Many general heat treating applications involve the hardening and tempering of a steel part to make it strong and tough through service. Hardening involves heating the steel to a predetermined temperature, then cooling it rapidly.\n\nA further application for vacuum furnaces is Vacuum Carburizing also known as Low Pressure Carburizing or LPC. In this process, a gas (such as acetylene) is introduced as a partial pressure into the hot zone at temperatures typically between . The gas disassociates into its constituent elements (in this case carbon and hydrogen). The carbon is then diffused into the surface area of the part. This function is typically repeated, varying the duration of gas input and diffusion time. Once the workload is properly \"cased\", the metal is quenched using oil or high pressure gas (HPGQ). For HPGQ, nitrogen or, for faster quench helium, is commonly used. This process is also known as case hardening.\n\nAnother low temperature application of vacuum furnaces is debinding, a process for the removal of binders. Heat is applied under a vacuum in a sealed chamber, melting or vaporizing the binder from the aggregate. The binder is evacuated by the pumping system and collected or purged downstream. The material with a higher melting point is left behind in a purified state and can be further processed.\n\nVacuum furnaces capable of temperatures above 1200 °C are used in various industry sectors such as electronics, medical, crystal growth, energy and artificial gems. The processing of high temperature materials, both of metals and nonmetals, in a vacuum environment allows annealing, brazing, purification, sintering and other processes to take place in a controlled manner.\n"}
{"id": "3824919", "url": "https://en.wikipedia.org/wiki?curid=3824919", "title": "Vapor recovery", "text": "Vapor recovery\n\nVapor (or vapour) recovery is the process of recovering the vapors of gasoline or other fuels, so that they do not escape into the atmosphere. This is often done (or required by law) at filling stations, in order to reduce noxious and potentially explosive fumes and pollution. \n\nThe negative pressure created in the (underground) tank by the withdrawal is usually used to pull in the vapors. They are drawn-in through holes in the side of the nozzle and travel through special hoses which have a return path.\n\n\nVapor recovery is also used in the chemical process industry to remove and recover vapors from storage tanks. The vapors are usually either environmentally hazardous, or valuable to be recovered. The process consists of a closed venting system from the storage tank ullage space to a vapor recovery unit (VRU) which will recover the vapors for return to the process or destroy them, usually by oxidation.\n\nVapor recovery units are also becoming commonly used in the oil and gas industry as a means of recovering natural gas vapor and making it a usable and profitable product. Specifically a newer form of vapor recovery technology, Ejector Vapor Recovery Units create a closed loop system which not only recovers valuable vapor, but also reduces methane and VOC emissions. \n\nIn the Australian region vapor recovery has become mandatory in major urban areas. There are 2 categories - VR1 and VR2. VR1 is to be installed in fuel stations that pump less than 500,000 litres annually, VR2 is for fuel quantities over 500,000 litres per annum, or as designated by various EPA bodies. \n\nVapor recovery towers are also used in the oil and gas industry to provide flash gas recovery at near atmospheric pressure without the potential of oxygen ingress from the top of the storage tanks. The ability to create the vapor flash inside the vapor recovery tower often reduces storage tank emissions to less than 6 tons per year, exempting the tank battery from Quad O reporting requirements. \n\n\n"}
{"id": "192331", "url": "https://en.wikipedia.org/wiki?curid=192331", "title": "Vegetable oil", "text": "Vegetable oil\n\nVegetable oils, or vegetable fats, are fats extracted from seeds, or less often, from other parts of fruits. Like animal fats, vegetable fats are \"mixtures\" of triglycerides. Soybean oil, rapeseed oil, and cocoa butter are examples of fats from seeds. Olive oil, palm oil, and rice bran oil are example of fats from other parts of fruits. In common usage, vegetable \"oil\" may refer exclusively to vegetable fats which are liquid at room temperature.\n\nOils extracted from plants have been used since ancient times and in many cultures. As an example, in a 4,000-year-old kitchen unearthed in Indiana's Charlestown State Park, archaeologist Bob McCullough of Indiana University-Purdue University Fort Wayne found evidence that large slabs of rock were used to crush hickory nuts and the oil was then extracted with boiling water. Archaeological evidence shows that olives were turned into olive oil by 6000 BC and 4500 BC in present-day Israel and Palestine.\n\nMany vegetable oils are consumed directly, or indirectly as ingredients in food – a role that they share with some animal fats, including butter, ghee, lard, and schmaltz. The oils serve a number of purposes in this role:\n\nSecondly, oils can be heated and used to cook other foods. Oils suitable for this objective must have a high flash point. Such oils include the major cooking oils – soybean, rapeseed, canola, sunflower, safflower, peanut, cottonseed, etc. Tropical oils, such as coconut, palm, and rice bran oils, are particularly valued in Asian cultures for high-temperature cooking, because of their unusually high flash points.\n\nUnsaturated vegetable oils can be transformed through partial or complete \"hydrogenation\" into oils of higher melting point. The hydrogenation process involves \"sparging\" the oil at high temperature and pressure with hydrogen in the presence of a catalyst, typically a powdered nickel compound. As each carbon–carbon double-bond is chemically reduced to a single bond, two hydrogen atoms each form single bonds with the two carbon atoms. The elimination of double bonds by adding hydrogen atoms is called saturation; as the degree of saturation increases, the oil progresses toward being fully hydrogenated. An oil may be hydrogenated to increase resistance to rancidity (oxidation) or to change its physical characteristics. As the degree of saturation increases, the oil's viscosity and melting point increase.\n\nThe use of hydrogenated oils in foods has never been completely satisfactory. Because the center arm of the triglyceride is shielded somewhat by the end fatty acids, most of the hydrogenation occurs on the end fatty acids, thus making the resulting fat more brittle. A margarine made from naturally more saturated oils will be more plastic (more \"spreadable\") than a margarine made from hydrogenated soy oil. While \"full hydrogenation\" produces largely saturated fatty acids, partial hydrogenation results in the transformation of unsaturated cis fatty acids to unsaturated trans fatty acids in the oil mixture due to the heat used in hydrogenation. Partially hydrogenated oils and their trans fats have been linked to an increased risk of mortality from coronary heart disease, among other increased health risks.\n\nIn the US, the Standard of Identity for a product labeled as \"vegetable oil margarine\" specifies only canola, safflower, sunflower, corn, soybean, or peanut oil may be used. Products not labeled \"vegetable oil margarine\" do not have that restriction.\n\nVegetable oils are used as an ingredient or component in many manufactured products.\n\nMany vegetable oils are used to make soaps, skin products, candles, perfumes and other personal care and cosmetic products. Some oils are particularly suitable as drying oils, and are used in making paints and other wood treatment products. Dammar oil (a mixture of linseed oil and dammar resin), for example, is used almost exclusively in treating the hulls of wooden boats. Vegetable oils are increasingly being used in the electrical industry as insulators as vegetable oils are not toxic to the environment, biodegradable if spilled and have high flash and fire points. However, vegetable oils are less stable chemically, so they are generally used in systems where they are not exposed to oxygen, and they are more expensive than crude oil distillate. Synthetic tetraesters, which are similar to vegetable oils but with four fatty acid chains compared to the normal three found in a natural ester, are manufactured by Fischer esterification. Tetraesters generally have high stability to oxidation and have found use as engine lubricants. Vegetable oil is being used to produce biodegradable hydraulic fluid and lubricant.\n\nOne limiting factor in industrial uses of vegetable oils is that all such oils are susceptible to becoming rancid. Oils that are more stable, such as ben oil or mineral oil, are thus preferred for industrial uses. Castor oil has numerous industrial uses, owing to the presence of hydroxyl group on the fatty acid. Castor oil is a precursor to Nylon 11.\n\nVegetable oil is used in production of some pet foods. AAFCO defines vegetable oil, in this context, as the product of vegetable origin obtained by extracting the oil from seeds or fruits which are processed for edible purposes.\n\nVegetable oils are also used to make biodiesel, which can be used like conventional diesel. Some vegetable oil blends are used in unmodified vehicles but straight vegetable oil, also known as pure plant oil, needs specially prepared vehicles which have a method of heating the oil to reduce its viscosity. The use of vegetable oils as alternative energy is growing and the availability of biodiesel around the world is increasing.\n\nThe NNFCC estimate that the total net greenhouse gas savings when using vegetable oils in place of fossil fuel-based alternatives for fuel production, range from 18 to 100%.\n\nThe production process of vegetable oil involves the removal of oil from plant components, typically seeds. This can be done via mechanical extraction using an oil mill or chemical extraction using a solvent. The extracted oil can then be purified and, if required, refined or chemically altered.\n\nOils can be removed via mechanical extraction, termed \"crushing\" or \"pressing.\" This method is typically used to produce the more traditional oils (e.g., olive, coconut etc.), and it is preferred by most \"health-food\" customers in the United States and in Europe. There are several different types of mechanical extraction. Expeller-pressing extraction is common, though the screw press, ram press, and ghani (powered mortar and pestle) are also used. Oilseed presses are commonly used in developing countries, among people for whom other extraction methods would be prohibitively expensive; the ghani is primarily used in India. The amount of oil extracted using these methods varies widely, as shown in the following table for extracting mowrah butter in India:\n\nThe processing of vegetable oil in commercial applications is commonly done by chemical extraction, using solvent extracts, which produces higher yields and is quicker and less expensive. The most common solvent is petroleum-derived hexane. This technique is used for most of the \"newer\" industrial oils such as soybean and corn oils.\n\nSupercritical carbon dioxide can be used as a non-toxic alternative to other solvents.\n\nOils may be partially hydrogenated to produce various ingredient oils. Lightly hydrogenated oils have very similar physical characteristics to regular soy oil, but are more resistant to becoming rancid. Margarine oils need to be mostly solid at 32 °C (90 °F) so that the margarine does not melt in warm rooms, yet it needs to be completely liquid at 37 °C (98 °F), so that it doesn't leave a \"lardy\" taste in the mouth.\n\nHardening vegetable oil is done by raising a blend of vegetable oil and a catalyst in near-vacuum to very high temperatures, and introducing hydrogen. This causes the carbon atoms of the oil to break double-bonds with other carbons, each carbon forming a new single-bond with a hydrogen atom. Adding these hydrogen atoms to the oil makes it more solid, raises the smoke point, and makes the oil more stable.\n\nHydrogenated vegetable oils differ in two major ways from other oils which are equally saturated. During hydrogenation, it is easier for hydrogen to come into contact with the fatty acids on the end of the triglyceride, and less easy for them to come into contact with the center fatty acid. This makes the resulting fat more brittle than a tropical oil; soy margarines are less \"spreadable\". The other difference is that trans fatty acids (often called trans fat) are formed in the hydrogenation reactor, and may amount to as much as 40 percent by weight of a partially hydrogenated oil. Hydrogenated oils, especially partially hydrogenated oils with their higher amounts of trans fatty acids are increasingly thought to be unhealthy.\n\nIn the processing of edible oils, the oil is heated under vacuum to near the smoke point, and water is introduced at the bottom of the oil. The water immediately is converted to steam, which bubbles through the oil, carrying with it any chemicals which are water-soluble. The steam sparging removes impurities that can impart unwanted flavors and odors to the oil. Deodorization is key to the manufacture of vegetable oils. Nearly all soybean, corn, and canola oils found on supermarket shelves go through a deodorization stage that removes trace amounts of odors and flavors, and lightens the color of the oil.\n\nPeople can breathe in vegetable oil mist in the workplace. The U.S. Occupational Safety and Health Administration (OSHA) has set the legal limit (permissible exposure limit) for vegetable oil mist exposure in the workplace as 15 mg/m total exposure and 5 mg/m respiratory exposure over an 8-hour workday. The U.S. National Institute for Occupational Safety and Health (NIOSH) has set a recommended exposure limit (REL) of 10 mg/m total exposure and 5 mg/m respiratory exposure over an 8-hour workday.\n\nThe following triglyceride vegetable oils account for almost all worldwide production, by volume. All are used as both cooking oils and as SVO or to make biodiesel. According to the USDA, the total world consumption of major vegetable oils in 2007/08 was:\nNote that these figures include industrial and animal feed use. The majority of European rapeseed oil production is used to produce biodiesel, or used directly as fuel in diesel cars which may require modification to heat the oil to reduce its higher viscosity. The suitability of the fuel should come as little surprise, as Rudolf Diesel originally designed his engine to run on peanut oil.\n\nOther significant triglyceride oils include:\n\n\n\nSuch oils have been part of human culture for millennia. Poppy seed, rapeseed, linseed, almond oil, sesame seed, safflower, and cotton seed were used since at least the bronze age throughout the Middle East and Central Asia. In 1780, Carl Wilhelm Scheele demonstrated that fats were derived from glycerol. Thirty years later Michel Eugène Chevreul deduced that these fats were esters of fatty acids and glycerol.\n\nIn modern times, cottonseed oil was marketed by Procter & Gamble as a creamed shortening in 1911. Ginning mills were happy to have someone haul away the cotton seeds. The extracted oil was refined and partially hydrogenated to give a solid at room temperature and thus mimic natural lard, and can it under nitrogen gas. Compared to the rendered lard Procter & Gamble was already selling to consumers, Crisco was cheaper, easier to stir into a recipe, and could be stored at room temperature for two years without turning rancid.\n\nSoybeans were an exciting new crop from China in the 1930s. Soy was protein-rich, and the medium viscosity oil was high in polyunsaturates. Henry Ford established a soybean research laboratory, developed soybean plastics and a soy-based synthetic wool, and built a car \"almost entirely\" out of soybeans. Roger Drackett had a successful new product with Windex, but he invested heavily in soybean research, seeing it as a smart investment. By the 1950s and 1960s, soybean oil had become the most popular vegetable oil in the US.\n\nIn the mid-1970s, Canadian researchers developed a low-erucic-acid rapeseed cultivar. Because the word \"rape\" was not considered optimal for marketing, they coined the name \"canola\" (from \"Canada Oil low acid\"). The U.S. Food and Drug Administration approved use of the canola name in January 1985, and U.S. farmers started planting large areas that spring. Canola oil is lower in saturated fats, and higher in monounsaturates and is a better source of omega-3 fats than other popular oils. Canola is very thin (unlike corn oil) and flavorless (unlike olive oil), so it largely succeeds by displacing soy oil, just as soy oil largely succeeded by displacing cottonseed oil.\n\nA large quantity of used vegetable oil is produced and recycled, mainly from industrial deep fryers in potato processing plants, snack food factories and fast food restaurants.\n\nRecycled oil has numerous uses, including use as a direct fuel, as well as in the production of biodiesel, soap, animal feed, pet food, detergent, and cosmetics. It is traded as the commodity, yellow grease.\n\nSince 2002, an increasing number of European Union countries have prohibited the inclusion of recycled vegetable oil from catering in animal feed. Used cooking oils from food manufacturing, however, as well as fresh or unused cooking oil, continue to be used in animal feed.\n\nDue to their susceptibility to oxidation from the exposure to oxygen, heat and light, resulting in the formation of oxidation products, such as peroxides and hydroperoxides, plant oils rich in polyunsaturated fatty acids have a limited shelf-life.\n\nIn Canada, palm oil is one of five vegetable oils, along with palm kernel oil, coconut oil, peanut oil and cocoa butter, which must be specifically named in the list of ingredients for a food product. Also, oils in Canadian food products which have been modified or hydrogenated must contain the word \"modified\" or \"hydrogenated\" when listed as an ingredient. A mix of oils other than the aforementioned exceptions may simply be listed as \"vegetable oil\" in Canada; however, if the food product is a cooking oil, salad oil or table oil, the type of oil must be specified and listing \"vegetable oil\" as an ingredient is not acceptable.\n\nFrom December 2014, all food products produced in the European Union were legally required to indicate the specific vegetable oil used in their manufacture, following the introduction of the Food Information to Consumers Regulation.\n\n"}
{"id": "33140676", "url": "https://en.wikipedia.org/wiki?curid=33140676", "title": "Vinton Furnace State Experimental Forest", "text": "Vinton Furnace State Experimental Forest\n\nThe Vinton Furnace State Experimental Forest and Raccoon Ecological Management Area (REMA) is a state forest in Vinton County, Ohio, United States. It comprises 15,849 acres, the largest remaining intact block of forest in Ohio still available for permanent protection. Since 2000, data collected at the forest has been cited in nearly 200 academic articles.\n"}
