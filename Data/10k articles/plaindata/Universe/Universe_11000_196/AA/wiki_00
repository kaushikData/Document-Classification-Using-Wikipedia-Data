{"id": "40333762", "url": "https://en.wikipedia.org/wiki?curid=40333762", "title": "3,11-Dihydroxydodecanoic acid", "text": "3,11-Dihydroxydodecanoic acid\n\n3,11-Dihydroxydodecanoic acid is a chemical found in royal jelly.\n\n"}
{"id": "899", "url": "https://en.wikipedia.org/wiki?curid=899", "title": "Actinium", "text": "Actinium\n\nActinium is a chemical element with symbol Ac and atomic number 89. It was first isolated by French chemist André-Louis Debierne in 1899. Friedrich Oskar Giesel later independently isolated it in 1902 and, unaware that it was already known, gave it the name emanium. Actinium gave the name to the actinide series, a group of 15 similar elements between actinium and lawrencium in the periodic table. It is also sometimes considered the first of the 7th-period transition metals, although lawrencium is less commonly given that position. Together with polonium, radium, and radon, actinium was one of the first non-primordial radioactive elements to be isolated.\n\nA soft, silvery-white radioactive metal, actinium reacts rapidly with oxygen and moisture in air forming a white coating of actinium oxide that prevents further oxidation. As with most lanthanides and many actinides, actinium assumes oxidation state +3 in nearly all its chemical compounds. Actinium is found only in traces in uranium and thorium ores as the isotope Ac, which decays with a half-life of 21.772 years, predominantly emitting beta and sometimes alpha particles, and Ac, which is beta active with a half-life of 6.15 hours. One tonne of natural uranium in ore contains about 0.2 milligrams of actinium-227, and one tonne of thorium contains about 5 nanograms of actinium-228. The close similarity of physical and chemical properties of actinium and lanthanum makes separation of actinium from the ore impractical. Instead, the element is prepared, in milligram amounts, by the neutron irradiation of in a nuclear reactor. Owing to its scarcity, high price and radioactivity, actinium has no significant industrial use. Its current applications include a neutron source and an agent for radiation therapy targeting cancer cells in the body and killing them.\n\nAndré-Louis Debierne, a French chemist, announced the discovery of a new element in 1899. He separated it from pitchblende residues left by Marie and Pierre Curie after they had extracted radium. In 1899, Debierne described the substance as similar to titanium and (in 1900) as similar to thorium. Friedrich Oskar Giesel independently discovered actinium in 1902 as a substance being similar to lanthanum and called it \"emanium\" in 1904. After a comparison of the substances half-lives determined by Debierne, Harriet Brooks in 1904, and Otto Hahn and Otto Sackur in 1905, Debierne's chosen name for the new element was retained because it had seniority, despite the contradicting chemical properties he claimed for the element at different times.\n\nArticles published in the 1970s and later suggest that Debierne's results published in 1904 conflict with those reported in 1899 and 1900. Furthermore, the now-known chemistry of actinium precludes its presence as anything other than a minor constituent of Debierne's 1899 and 1900 results; in fact, the chemical properties he reported make it likely that he had, instead, accidentally identified protactinium, which would not be discovered for another fourteen years, only to have it disappear due to its hydrolysis and adsorption onto his laboratory equipment. This has led some authors to advocate that Giesel alone should be credited with the discovery. A less confrontational vision of scientific discovery is proposed by Adloff. He suggests that hindsight criticism of the early publications should be mitigated by the then nascent state of radiochemistry: highlighting the prudence of Debierne's claims in the original papers, he notes that nobody can contend that Debierne's substance did not contain actinium. Debierne, who is now considered by the vast majority of historians as the discoverer, lost interest in the element and left the topic. Giesel, on the other hand, can rightfully be credited with the first preparation of radiochemically pure actinium and with the identification of its atomic number 89.\n\nThe name actinium originates from the Ancient Greek \"aktis, aktinos\" (ακτίς, ακτίνος), meaning beam or ray. Its symbol Ac is also used in abbreviations of other compounds that have nothing to do with actinium, such as acetyl, acetate and sometimes acetaldehyde.\n\nActinium is a soft, silvery-white, radioactive, metallic element. Its estimated shear modulus is similar to that of lead. Owing to its strong radioactivity, actinium glows in the dark with a pale blue light, which originates from the surrounding air ionized by the emitted energetic particles. Actinium has similar chemical properties to lanthanum and other lanthanides, and therefore these elements are difficult to separate when extracting from uranium ores. Solvent extraction and ion chromatography are commonly used for the separation.\n\nThe first element of the actinides, actinium gave the group its name, much as lanthanum had done for the lanthanides. The group of elements is more diverse than the lanthanides and therefore it was not until 1928 that Charles Janet proposed the most significant change to Dmitri Mendeleev's periodic table since the recognition of the lanthanides, by introducing the actinides, a move suggested again in 1945 by Glenn T. Seaborg.\n\nActinium reacts rapidly with oxygen and moisture in air forming a white coating of actinium oxide that impedes further oxidation. As with most lanthanides and actinides, actinium exists in the oxidation state +3, and the Ac ions are colorless in solutions. The oxidation state +3 originates from the [Rn]6d7s electronic configuration of actinium, with three valence electrons that are easily donated to give the stable closed-shell structure of the noble gas radon. The rare oxidation state +2 is only known for actinium dihydride (AcH); even this may in reality be an electride compound like its lighter congener LaH and thus have actinium(III).\n\nOnly a limited number of actinium compounds are known including AcF, AcCl, AcBr, AcOF, AcOCl, AcOBr, AcS, AcO and AcPO, due to actinium's intense radioactivity. Except for AcPO, they are all similar to the corresponding lanthanum compounds. They all contain actinium in the oxidation state +3. In particular, the lattice constants of the analogous lanthanum and actinium compounds differ by only a few percent.\n\nHere \"a\", \"b\" and \"c\" are lattice constants, No is space group number and \"Z\" is the number of formula units per unit cell. Density was not measured directly but calculated from the lattice parameters.\n\nActinium oxide (AcO) can be obtained by heating the hydroxide at 500 °C or the oxalate at 1100 °C, in vacuum. Its crystal lattice is isotypic with the oxides of most trivalent rare-earth metals.\n\nActinium trifluoride can be produced either in solution or in solid reaction. The former reaction is carried out at room temperature, by adding hydrofluoric acid to a solution containing actinium ions. In the latter method, actinium metal is treated with hydrogen fluoride vapors at 700 °C in an all-platinum setup. Treating actinium trifluoride with ammonium hydroxide at 900–1000 °C yields oxyfluoride AcOF. Whereas lanthanum oxyfluoride can be easily obtained by burning lanthanum trifluoride in air at 800 °C for an hour, similar treatment of actinium trifluoride yields no AcOF and only results in melting of the initial product.\n\nActinium trichloride is obtained by reacting actinium hydroxide or oxalate with carbon tetrachloride vapors at temperatures above 960 °C. Similar to oxyfluoride, actinium oxychloride can be prepared by hydrolyzing actinium trichloride with ammonium hydroxide at 1000 °C. However, in contrast to the oxyfluoride, the oxychloride could well be synthesized by igniting a solution of actinium trichloride in hydrochloric acid with ammonia.\n\nReaction of aluminium bromide and actinium oxide yields actinium tribromide:\n\nand treating it with ammonium hydroxide at 500 °C results in the oxybromide AcOBr.\n\nActinium hydride was obtained by reduction of actinium trichloride with potassium at 300 °C, and its structure was deduced by analogy with the corresponding LaH hydride. The source of hydrogen in the reaction was uncertain.\n\nMixing monosodium phosphate (NaHPO) with a solution of actinium in hydrochloric acid yields white-colored actinium phosphate hemihydrate (AcPO·0.5HO), and heating actinium oxalate with hydrogen sulfide vapors at 1400 °C for a few minutes results in a black actinium sulfide AcS. It may possibly be produced by acting with a mixture of hydrogen sulfide and carbon disulfide on actinium oxide at 1000 °C.\n\nNaturally occurring actinium is composed of two radioactive isotopes; (from the radioactive family of ) and (a granddaughter of ). decays mainly as a beta emitter with a very small energy, but in 1.38% of cases it emits an alpha particle, so it can readily be identified through alpha spectrometry. Thirty-six radioisotopes have been identified, the most stable being with a half-life of 21.772 years, with a half-life of 10.0 days and with a half-life of 29.37 hours. All remaining radioactive isotopes have half-lives that are less than 10 hours and the majority of them have half-lives shorter than one minute. The shortest-lived known isotope of actinium is (half-life of 69 nanoseconds) which decays through alpha decay and electron capture. Actinium also has two known meta states. The most significant isotopes for chemistry are Ac, Ac, and Ac.\n\nPurified comes into equilibrium with its decay products after about a half of year. It decays according to its 21.772-year half-life emitting mostly beta (98.62%) and some alpha particles (1.38%); the successive decay products are part of the actinium series. Owing to the low available amounts, low energy of its beta particles (maximum 44.8 keV) and low intensity of alpha radiation, is difficult to detect directly by its emission and it is therefore traced via its decay products. The isotopes of actinium range in atomic weight from 206 u () to 236 u ().\n\nActinium is found only in traces in uranium ores – one tonne of uranium in ore contains about 0.2 milligrams of Ac – and in thorium ores, which contain about 5 nanograms of Ac per one tonne of thorium. The actinium isotope Ac is a transient member of the uranium-actinium series decay chain, which begins with the parent isotope U (or Pu) and ends with the stable lead isotope Pb. The isotope Ac is a transient member of the thorium series decay chain, which begins with the parent isotope Th and ends with the stable lead isotope Pb. Another actinium isotope (Ac) is transiently present in the neptunium series decay chain, beginning with Np (or U) and ending with thallium (Tl) and near-stable bismuth (Bi); even though all primordial Np has decayed away, it is continuously produced by neutron knock-out reactions on natural U.\n\nThe low natural concentration, and the close similarity of physical and chemical properties to those of lanthanum and other lanthanides, which are always abundant in actinium-bearing ores, render separation of actinium from the ore impractical, and complete separation was never achieved. Instead, actinium is prepared, in milligram amounts, by the neutron irradiation of in a nuclear reactor.\nThe reaction yield is about 2% of the radium weight. Ac can further capture neutrons resulting in small amounts of Ac. After the synthesis, actinium is separated from radium and from the products of decay and nuclear fusion, such as thorium, polonium, lead and bismuth. The extraction can be performed with thenoyltrifluoroacetone-benzene solution from an aqueous solution of the radiation products, and the selectivity to a certain element is achieved by adjusting the pH (to about 6.0 for actinium). An alternative procedure is anion exchange with an appropriate resin in nitric acid, which can result in a separation factor of 1,000,000 for radium and actinium vs. thorium in a two-stage process. Actinium can then be separated from radium, with a ratio of about 100, using a low cross-linking cation exchange resin and nitric acid as eluant.\n\nAc was first produced artificially at the Institute for Transuranium Elements (ITU) in Germany using a cyclotron and at St George Hospital in Sydney using a linac in 2000. This rare isotope has potential applications in radiation therapy and is most efficiently produced by bombarding a radium-226 target with 20–30 MeV deuterium ions. This reaction also yields Ac which however decays with a half-life of 29 hours and thus does not contaminate Ac.\n\nActinium metal has been prepared by the reduction of actinium fluoride with lithium vapor in vacuum at a temperature between 1100 and 1300 °C. Higher temperatures resulted in evaporation of the product and lower ones lead to an incomplete transformation. Lithium was chosen among other alkali metals because its fluoride is most volatile.\n\nOwing to its scarcity, high price and radioactivity, actinium currently has no significant industrial use.\nAc is highly radioactive and was therefore studied for use as an active element of radioisotope thermoelectric generators, for example in spacecraft. The oxide of Ac pressed with beryllium is also an efficient neutron source with the activity exceeding that of the standard americium-beryllium and radium-beryllium pairs. In all those applications, Ac (a beta source) is merely a progenitor which generates alpha-emitting isotopes upon its decay. Beryllium captures alpha particles and emits neutrons owing to its large cross-section for the (α,n) nuclear reaction:\n\nThe AcBe neutron sources can be applied in a neutron probe – a standard device for measuring the quantity of water present in soil, as well as moisture/density for quality control in highway construction. Such probes are also used in well logging applications, in neutron radiography, tomography and other radiochemical investigations.\n\nAc is applied in medicine to produce in a reusable generator or can be used alone as an agent for radiation therapy, in particular targeted alpha therapy (TAT). This isotope has a half-life of 10 days that makes it much more suitable for radiation therapy than Bi (half-life 46 minutes). Not only Ac itself, but also its daughters, emit alpha particles which kill cancer cells in the body. The major difficulty with application of Ac was that intravenous injection of simple actinium complexes resulted in their accumulation in the bones and liver for a period of tens of years. As a result, after the cancer cells were quickly killed by alpha particles from Ac, the radiation from the actinium and its daughters might induce new mutations. To solve this problem, Ac was bound to a chelating agent, such as citrate, ethylenediaminetetraacetic acid (EDTA) or diethylene triamine pentaacetic acid (DTPA). This reduced actinium accumulation in the bones, but the excretion from the body remained slow. Much better results were obtained with such chelating agents as HEHA () or DOTA () coupled to trastuzumab, a monoclonal antibody that interferes with the HER2/neu receptor. The latter delivery combination was tested on mice and proved to be effective against leukemia, lymphoma, breast, ovarian, neuroblastoma and prostate cancers.\n\nThe medium half-life of Ac (21.77 years) makes it very convenient radioactive isotope in modeling the slow vertical mixing of oceanic waters. The associated processes cannot be studied with the required accuracy by direct measurements of current velocities (of the order 50 meters per year). However, evaluation of the concentration depth-profiles for different isotopes allows estimating the mixing rates. The physics behind this method is as follows: oceanic waters contain homogeneously dispersed U. Its decay product, Pa, gradually precipitates to the bottom, so that its concentration first increases with depth and then stays nearly constant. Pa decays to Ac; however, the concentration of the latter isotope does not follow the Pa depth profile, but instead increases toward the sea bottom. This occurs because of the mixing processes which raise some additional Ac from the sea bottom. Thus analysis of both Pa and Ac depth profiles allows researchers to model the mixing behavior.\n\nThere are theoretical predictions that AcH hydrides (in this case with very high pressure) are a candidate for a near room-temperature superconductor as they have T significantly higher than H3S, possibly near 250 K.\n\nAc is highly radioactive and experiments with it are carried out in a specially designed laboratory equipped with a tight glove box. When actinium trichloride is administered intravenously to rats, about 33% of actinium is deposited into the bones and 50% into the liver. Its toxicity is comparable to, but slightly lower than that of americium and plutonium. For trace quantities, fume hoods with good aeration suffice; for gram amounts, hot cells with shielding from the intense gamma radiation emitted by Ac are necessary.\n\n\n"}
{"id": "2181084", "url": "https://en.wikipedia.org/wiki?curid=2181084", "title": "An Bord Pleanála", "text": "An Bord Pleanála\n\nAn Bord Pleanála (ABP) (, meaning \"The Planning Board\") is an independent, statutory, quasi-judicial body that decides on appeals from planning decisions made by local authorities in Ireland. As of 2007, An Bord Pleanála directly decided major strategic infrastructural projects under the provisions of the \"Planning and Development (Strategic Infrastructure) Act 2006\". The Board also hears applications from local authorities for projects which would have a significant environmental impact. The Board was established by the \"Local Government (Planning and Development) Act 1976\", whose provisions have for the most part been carried over into the \"Planning and Development Act, 2000\".\n\n"}
{"id": "3301549", "url": "https://en.wikipedia.org/wiki?curid=3301549", "title": "Animal glue", "text": "Animal glue\n\nAn animal glue is an adhesive that is created by prolonged boiling of animal connective tissue. \n\nThese protein colloid glues are formed through hydrolysis of the collagen from skins, bones, tendons, and other tissues, similar to gelatin. The word \"collagen\" itself derives from Greek κόλλα \"kolla\", glue. These proteins form a molecular bond with the glued object.\n\nStereotypically, the animal in question is a horse, and horses that are put down are often said to have been \"sent to the glue factory\". However, other animals are also used, including rabbits and fish.\n\nAnimal glue has existed since ancient times, although its usage was not widespread. Glue deriving from horse tooth can be dated back nearly 6000 years, but no written records from these times can prove that they were fully or extensively utilized.\n\nThe first known written procedures of making animal glue were written about 2000 BC. Between 1500–1000 BC, it was used for wood furnishings and mural paintings, found even on the caskets of Egyptian Pharaohs. Evidence is in the form of stone carvings depicting glue preparation and use, primarily utilized for the pharaoh’s tomb’s furniture. Egyptian records tell that animal glue would be made by melting it over a fire and then applied with a brush. \n\nGreeks and Romans later used animal and fish glue to develop veneering and marquetry, the bonding of thin sections or layers of wood. Animal glue, known as taurokolla in Greek and gluten taurinum in Latin, were made from the skins of bulls in antiquity. Broken pottery might also be repaired with the use of animal glues, filling the cracks to hide imperfections.\n\nAbout 906–618 BC, China utilized fish, ox, and stag horns to produce adhesives and binders for pigments. Animal glues were employed as binders in paint media during the Tang Dynasty. Records indicate that one of the essential components of lampblack ink was proteinaceous glue. Ox glue and stag-horn glues bound particles of pigments together, acting as a preservative by forming a film over the surface as the ink dried. The Chinese, such as Kao Gong Ji, also researched glue for medicinal purposes.\n\nThe use of animal glue, as well as some other types of glues, largely vanished in Europe after the decline of the Western Roman Empire until the sixteenth to eighteenth centuries, when wooden furniture started to surge as a major craft. During the medieval ages, fish glue remained a source for painting and illuminating manuscripts. Since the 16th century, hide glue has been used in the construction of violins.\n\nNative Americans would use hoof glue primarily as a binder and as a water-resistant coating by boiling it down from leftover animal parts and applying it to exposed surfaces. They occasionally used hide glue as paint to achieve patterns after applying pigments and tanning to hides. Hoof glue would be used for purposes aside from hides, such as a hair preservative. The Assiniboins preferred longer hair, so they would plaster the strands with a mixture of red earth and hoof glue. It would also be used to bind feathers and equipment together.\n\nThe first commercial glue factory opened in Holland circa 1700, manufacturing animal glue from hides. The United States’ first glue factory opened in 1899, established by the Milwaukee Tanning Industry. The L.D. Davis company thrived producing animal glue during the Great Depression after shifting its focus from stenciling, selling to local box makers and other users; L.D. Davis' animal glue formula for bookbinding remains in production. During the 18th and 19th centuries, ranchers disposed of old animals – horses in particular – to glue factories. The advent of synthetic adhesives heralded the collapse of the animal glue industry.\n\nToday, animal glues are sparsely industrialized, but still used for making and restoring objects, paintings, illuminated parchment manuscripts, and other artifacts. Gelatin, a form of animal glue, is found in many contemporary products, such as gelatin desserts, marshmallows, and pharmaceutical capsules, and is used to reinforce sinew wrappings, wood, leather, bark, and paper. Hide glue is also preferred by many luthiers over synthetic glues for its reversibility, creep-resistance and tendency to pull joints closed as it cures.\n\nThis adhesive is mostly used as glue, sizing, or varnish, although it is not as frequently used as other adhesives because it is water-soluble. Other aspects, such as difficulty of storage in a wet state, requirement for fresh raw materials (the animal skin cannot be rotten or grease-burned), make this product more difficult to find and use. Factories now produce other forms of adhesives, as the process for animal glue is complex and tricky to follow. Animal glues will also darken with age and shrink as they dry, giving them the potential to harm wood, paper, or works of art. Too much handling and too many changes in temperature or humidity could cause further harm. Some companies, such as those in Canada, still produce animal, hide and hoof glues from horses. Recently, animal glue has been replaced by other adhesives and plastics, but remains popular for restoration.\n\nAnimal glue was the most common woodworking glue for thousands of years until the advent of synthetic glues, such as polyvinyl acetate (PVA) and other resin glues, in the 20th century. Today it is used primarily in specialty applications, such as lutherie, pipe organ building, piano repairs, and antique restoration. Glass artists take advantage of hide glue's ability to bond with glass, applying hide glue to glass. As the glue hardens it shrinks, chipping the glass. \n\nIt has several advantages and disadvantages compared to other glues. The glue is applied hot, typically with a brush or spatula. Glue is kept hot in a glue pot, which may be an electric unit built for the purpose, a double boiler, or simply a saucepan or crock pot to provide a warm water bath for the container of glue.\n\nMost animal glues are soluble in water, useful for joints which may at some time need to be separated. Alcohol is sometimes applied to such joints to dehydrate the glue, making it more brittle and easier to crack apart. Steam can also be used to soften glue and separate joints.\n\nSpecific types include \"hide glue\", \"bone glue\", \"fish glue\", \"rabbit skin glue\".\n\nHide glue is used in woodworking. It may be supplied as granules, flakes, or flat sheets, which have an indefinite shelf life if kept dry. It is dissolved in water, heated and applied warm, typically around 60°C (140°F). Warmer temperatures quickly destroy the strength of hide glue. Commercial glue pots, simple water baths or double boilers may be used to keep the glue hot while in use. As hide glue cools, it gels quickly. At room temperature, prepared hide glue has the consistency of stiff gelatin, which is in fact a similar composition. Gelled hide glue does not have significant strength, so it is vital to apply the glue, fit the pieces, and hold them steady before the glue temperature drops much below 50 °C (120 °F). All glues have an \"open time\", the amount of time the glue remains liquid and workable. Joining parts after the open time is expired results in a weak bond. Hide glue's open time is usually a minute or less. In practice, this often means having to heat the pieces to be glued, and gluing in a very warm room, though these steps can be dispensed with if the glue and clamp operation can be carried out quickly.\n\nWhere hide glue is in occasional use, excess glue may be held in a freezer, to prevent spoilage from the growth of microorganisms. Hide glue has some gap filling properties, although modern gap-filling adhesives, such as epoxy resin, are better in this regard.\n\nHide glue that is liquid at room temperature is also possible through the addition of urea. In stress tests performed by Mark Schofield of Fine Woodworking Magazine, \"liquid hide glue\" compared favourably to normal hide glue in average strength of bond. \"However, any liquid hide glue over six months old can be suspect because the urea eventually hydrolyzes the protein structure of the glue and weakens it – even though the product was 'protected' with various bactericides and fungicides during manufacture.\"\n\nAnimal hides are soaked in water to produce \"stock.\" The stock is then treated with lime to break down the hides. The hides are then rinsed to remove the lime, any residue being neutralised with a weak acid solution. The hides are heated, in water, to a carefully controlled temperature around 70 degrees Celsius. The 'glue liquor' is then drawn off, more water added, and the process repeated at increasing temperatures.\n\nThe glue liquor is then dried and chipped into pellets.\n\nThe significant disadvantages of hide glue – its thermal limitations, short open time, and vulnerability to micro-organisms – are offset by several advantages. Hide glue joints are reversible and repairable. Recently glued joints will release easily with the application of heat and steam. Hide glue sticks to itself, so the repairer can apply new hide glue to the joint and reclamp it. In contrast, PVA glues do not adhere to themselves once they are cured, so a successful repair requires removal of the old glue first – which usually requires removing some of the material being glued.\n\nHide glue creates a somewhat brittle joint, so a strong shock will often cause a very clean break along the joint. In contrast, cleaving a joint glued with PVA will usually damage the surrounding material, creating an irregular break that is more difficult to repair. This brittleness is taken advantage of by instrument makers. For example, instruments in the violin family require periodic disassembly for repairs and maintenance. The top of a violin is easily removed by prying a palette knife between the top and ribs, and running it all around the joint. The brittleness allows the top to be removed, often without significant damage to the wood. Regluing the top only requires applying new hot hide glue to the joint. If the violin top were glued on with PVA glue, removing the top would require heat and steam to disassemble the joint (causing damage to the varnish), then wood would have to be removed from the joint to ensure no cured PVA glue was remaining before regluing the top.\n\nHide glue also functions as its own clamp. Once the glue begins to gel, it pulls the joint together. Violin makers may glue the center seams of top and back plates together using a \"rubbed joint\" rather than using clamps. This technique involves coating half of the joint with hot hide glue, and then rubbing the other half against the joint until the hide glue starts to gel, at which point the glue becomes tacky. At this point the plate is set aside without clamps, and the hide glue pulls the joint together as it hardens.\n\nHide glue regains its working properties after cooling if it is reheated. This property can be used when the glue's open time does not allow the joint to be glued normally. For example, a cello maker may not be able to glue and clamp a top to the instrument's ribs in the short one-minute open time available. Instead, the builder will lay a bead of glue along the ribs, and allow it to cool. The top is then clamped to the ribs. Moving a few inches at a time, the maker inserts a heated palette knife into the joint, heating the glue. When the glue is liquefied, the palette knife is removed, and the glue cools, creating a bond. A similar process can be used to glue veneers to a substrate. The veneer and/or the substrate is coated with hot hide glue. Once the glue is cold, the veneer is positioned on the substrate. A hot object such as a clothes iron is applied to the veneer, liquefying the underlying glue. When the iron is removed, the glue cools, bonding the veneer to the substrate.\n\nHide glue joints do not creep under loads. PVA glues create plastic joints, which will creep over time if heavy loads are applied to them.\n\nHide glue is supplied in many different gram strengths, each suited to specific applications. Instrument and cabinet builders will use a range from 120 to 200 gram strength. Some hide glues are sold without the gram strength specified. Experienced users avoid this glue as the glue may be too weak or strong for the expected application.\n\nHoof glue is also used today in woodworking, specifically cabinetry.\n\nRabbit-skin glue is more flexible when dry than typical hide glues. It is used in the sizing or priming of oil painters' canvases. It also is used in bookbinding and as the adhesive component of some recipes for gesso and compo.\n\n\n\n"}
{"id": "1954485", "url": "https://en.wikipedia.org/wiki?curid=1954485", "title": "Balloon mail", "text": "Balloon mail\n\nBalloon mail is the transport of mail (usually for weight reasons in the form of a postcard) carrying the name of the sender by means of an unguided hydrogen or helium filled balloon. Since the balloon is not controllable, the delivery of a balloon mail is left to good fortune; often the balloon and postcard are lost. A found balloon should be returned to the sender (by conventional post) with an indication of the discovery site, so that the sender can determine how far their balloon flew. Frequently balloon mail is sent as part of a balloon competition.\n\nHistorically, balloons were used to transport mail from Paris during the Siege of Paris of 1870-71. About 66 unguided mail balloons were released from Paris to communicate with the outside world, of which the great majority succeeded in delivering their cargo. As the Prussian forces surrounded the city, telegraph lines were cut and messengers were captured, shot or turned back. Two services were proposed, by \"ballon monté\" (manned balloon) and \"ballon non-monté\" (unmanned). In practice only manned flights were used. After the siege, Anglo-French scientist Dr Pierre Wesby travelled to Burton-on-Trent, where in 1873 he started a business to transport mail across the Irish Sea to Dublin, from England. It is not known how this venture turned out; the records of Wesby's company were lost in 1916, when a bomb from the Zeppelin L 19 destroyed them.\nIn 1877, a 5-cent stamp for balloon postage was privately printed in Nashville, Tennessee to carry mail on a June 18 flight of the \"Buffalo Balloon\" from that city to Gallatin, Tennessee. Of the three hundred stamps produced, only 23 were used.\n\nBalloon mail was sent from Przemyśl, Poland (near the Ukrainian border) during World War I.\n\nBalloon mail has been used for spreading information and propaganda materials, in particular for spreading propaganda to the population in countries with dictatorial governments. A balloon can be released from outside the sphere of influence of these governments and, wind permitting, can travel several hundred kilometers. This method of balloon mail has been used by private activists to distribute leaflets to Warsaw Pact countries from West Germany in the mid-1950s; and by South Koreans to North Korea discussing the health of their leader, Kim Jong-il.\n"}
{"id": "43317198", "url": "https://en.wikipedia.org/wiki?curid=43317198", "title": "Biospeleology", "text": "Biospeleology\n\nBiospeleology, also known as cave biology, is a branch of biology dedicated to the study of organisms that live in caves and are collectively referred to as troglofauna.\n\nThe first documented mention of a cave organisms dates back to 1689, with the documentation of the olm, a cave salamander. Discovered in a cave in Slovenia, in the region of Carniola, it was mistaken for a baby dragon and was recorded by Johann Weikhard von Valvasor in his work \"The Glory of the Duchy of Carniola\". \nThe first formal study on cave organisms was conducted on the blind cave beetle. Found in 1831 by Luka Čeč, an assistant to the lamplighter, when exploring the newly discovered inner portions of the Postojna cave system in southwestern Slovenia. The specimen was turned over to Ferdinand J. Schmidt, who described it in the paper \"Illyrisches Blatt\" (1832). He named it \"Leptodirus Hochenwartii\" after the donor, and also gave it the Slovene name \"drobnovratnik\" and the German name \"Enghalskäfer\", both meaning \"slender-necked (beetle)\". The article represents the first formal description of a cave animal (the olm, described in 1768, wasn't recognized as a cave animal at the time).\nSubsequent research by Schmidt revealed further previously unknown cave inhabitants, which aroused considerable interest among natural historians. For this reason, the discovery of \"L. hochenwartii\" (along with the olm) is considered as the starting point of biospeleology as a scientific discipline. Biospeleology was formalized as a science in 1907 by Emil Racoviţă with his seminal work \"Essai sur les problèmes biospéologiques\" (\"Essay on biospeleological problems\").\n\nCave organisms fall into three basic classes:\n\nTroglobites are obligatory \"cavernicoles\", specialized for cave life. Some can leave caves for short periods, and may complete parts of their life cycles above ground, but cannot live their entire lives outside of a cave environment. Examples include chemotrophic bacteria, some species of flatworms, collembola, and cavefish.\n\nTroglophiles can live part or all of their lives in caves, but can also complete a life cycle in appropriate environments on the surface. Examples include cave crickets, bats, millipedes, pseudoscorpions and spiders.\n\nTrogloxenes frequent caves, and may require caves for a portion of its life cycle, but must return to the surface (or a \"parahypogean\" zone) for at least some portion of its life. Oilbirds and the Daddy longlegs are trogloxenes.\n\nCave environments fall into three general categories:\n\nEndogean environments are the parts of caves that are in communication with surface soils through cracks and rock seams, groundwater seepage, and root protrusion.\n\nParahypogean environments are the threshold regions near cave mouths that extend to the last penetration of sunlight.\n\nHypogean or \"true\" cave environments. These can be in regular contact with the surface via wind and underground rivers, or the migration of animals, or can be almost entirely isolated. Deep hypogean environments can host autonomous ecologies whose primary source of energy is not sunlight, but chemical energy liberated from limestone and other minerals by chemoautotrophic bacteria.\n\n\n\n"}
{"id": "2591641", "url": "https://en.wikipedia.org/wiki?curid=2591641", "title": "Bow echo", "text": "Bow echo\n\nA bow echo is the characteristic radar return from a mesoscale convective system that is shaped like an archer’s bow. These systems can produce severe straight-line winds and occasionally tornadoes, causing major damage. They can also become derechos.\n\nThe term \"bow echo\" was first used by Dr. Theodore Fujita in his May 1978 paper \"Manual of Downburst Identification for Project NIMROD.\" In 2004, research was done to better anticipate the formation of bow echoes, specifically the formation of bow echoes from weakly organized squall lines and supercells. Researchers determined that bow echoes were most likely to occur in weakly organized cells. A Midwest Bow Echo Workshop was held in 2007, at which meteorologists gathered to share their research to better understand bow echoes.\n\nA bow echo is associated with squall lines or lines of convective thunderstorms. These echoes can range in size from 20 to 200 km, and have a life span of 3 to 6 hours. Bow echoes tend to develop when moderate to strong wind shear exists in the lower 2 to 3 km of the atmosphere. While similar to squall lines, bow echoes are smaller in scale and are moved by the wind inside them. They tend to push outward and after time die out. A bow echo also lowers the chance of a tornado being formed in the storm itself. The \"bow shaped\" echo is a result of focusing of the strong flow at the rear of the system. Especially strong bow echoes that cause devastating damage all along the width of the storm are often called derechos.\n\nThe formation of a bow echo requires a strong elevated rear inflow jet at mid-levels. The strength of the cold pool and mesohigh at the surface as well as warmer temperatures aloft due to convection works to create a mesolow at mid-levels which strengthens the jet. Upon reaching the edge of the convection the jet descends and spreads along the surface, generating straight-line winds.\n\nAfter the rear inflow jet has bowed the storm system, book end or line end vortices develop on either side of the jet. These vortices are similar in strength. Due to the small size of the bow echo, the vortices help enhance the mid-level flow between them. This strengthens the rear inflow jet. The surface winds increase from the descending jet. As the life of the storm increases, the Coriolis force acts to intensify the cyclonic vortex and weaken the anticyclonic vortex. The system then develops an asymmetric comma-shaped echo. Some embedded tornadoes or gustnadoes develop within these vortices.\n\nDamaging straight-line winds often occur near the center of a bow echo. Damage from all severe thunderstorm winds account for half of all severe reports in the lower 48 states of the US and is more common than damage from tornadoes. In a type of long-lived and powerful bow echo known as a derecho, wind speeds can reach up to or exceeding 100 mph (160 km/h) and can produce a damage path extending for hundreds of miles. Bow echoes are capable of producing straight-line winds that are just as strong as many tornadoes. A strong bow echo will produce more widespread and intense damage than the majority of tornadoes. Also, bow echoes in the form of a line echo wave pattern create a favorable environment for tornadoes to form.\n\nThe semiarid climate and rugged terrain in the interior west of the United States do not favour the development of bow echoes. However, on 21 April 2011, a bow echo associated with a fast-moving mid-tropospheric perturbation formed across the Great Salt Lake (GSL) in Utah, producing damaging winds along its path.\n\nIn 1674, the city of Utrecht in the Netherlands was devastated by a storm now thought to have been a bow echo storm. Some of the damage to the city is still visible, and severe storm activity was recorded across other areas of Europe.\n\nA Bow Echo was evident in the radar signature of a line of severe storms that impacted the Mobile Bay area of Alabama on April 25, 2015. This severe weather contributed to the death of six sailors who were competing in the Dauphin Island Regatta.\n\n\n"}
{"id": "9856794", "url": "https://en.wikipedia.org/wiki?curid=9856794", "title": "Bukit Tagar Landfill", "text": "Bukit Tagar Landfill\n\nBukit Tagar Landfill is a landfill in Bukit Tagar, Selangor.\n\n"}
{"id": "23884136", "url": "https://en.wikipedia.org/wiki?curid=23884136", "title": "Bureau of Energy Efficiency", "text": "Bureau of Energy Efficiency\n\nThe Bureau of Energy Efficiency is an agency of the Government of India, under the Ministry of Power created in March 2002 under the provisions of the nation's 2001 Energy Conservation Act. The agency's function is to develop programs which will increase the conservation and efficient use of energy in India. The government has proposed to make it mandatory for certain appliances in India to have ratings by the BEE starting in January 2010.\nThe mission of Bureau of Energy Efficiency is to \"institutionalise\" energy efficiency services, enable delivery mechanisms in the country and provide leadership to energy efficiency in all sectors of the country. The primary objective would be to reduce energy intensity in the economy.\n\nThe broad objectives of BEE are as under:\nTo exert leadership and provide policy recommendation and direction to national energy conservation and efficiency efforts and programs.\nTo coordinate energy efficiency and conservation policies and programs and take it to the stakeholders\nTo establish systems and procedures to measure, monitor and verify energy efficiency results in individual sectors as well as at a macro level.\nTo leverage multi-lateral and bi-lateral and private sector support in implementation of Energy Conservation Act and efficient use of energy and its conservation programs.\nTo demonstrate delivery of energy efficiency services as mandated in the EC bill through private-public partnerships.\nTo interpret, plan and manage energy conservation programs as envisaged in the Energy Conservation Act.\nObjectives\nProvide a policy recommendation and direction to national energy conservation activities\nCoordinate policies and programmes on efficient use of energy with shareholders\nEstablish systems and procedures to verify, measure and monitor Energy Efficiency (EE) improvements\nLeverage multilateral, bilateral and private sector support to implement the EC Act '01\nDemonstrate EE delivery systems through public-private partnerships\n\nEnergy Audit : The Government of India has identified certain energy intensive industries labelled as 'designated consumers', and made it compulsory for them to conduct Energy Audits following the ‘Bureau of Energy Efficiency (Manner and Intervals of Time for Conduct of Energy Audit) Regulations, 2010’\n\n\n"}
{"id": "9862519", "url": "https://en.wikipedia.org/wiki?curid=9862519", "title": "Capital Airlines Flight 75", "text": "Capital Airlines Flight 75\n\nCapital Airlines Flight 75 was a domestic scheduled Capital Airlines flight operating between La Guardia Airport and Atlanta Airport. A Vickers Viscount flying the route crashed in Chase, Maryland, on May 12, 1959, with the loss of all on board. The crash was the second of three involving a Capital Airlines Vickers Viscount in as many years; the other two were Capital Airlines Flight 20 and Capital Airlines Flight 67.\n\nThe flight left the terminal at La Guardia at 3:20 in the afternoon, 20 minutes after it was scheduled to begin, and took off at 3:29. It then climbed to 14,000 feet before coming onto the assigned airway, Victor 3. By 4:02 the crew contacted Washington Center, reporting over Westchester and estimating Westminster at fifteen minutes away. In the same message they noted that there were thunderstorms along the assigned course, and requested permission to stay in the clear a little south of Westminster. The air traffic controller acknowledged the message and gave the go-ahead. At 4:10 the flight called again, the pilots noting that they had slowed somewhat to account for turbulence. This was the last message sent by the flight crew; three minutes later, the plane entered an area of severe turbulence, lost control, and entered a steep descent.\n\nIt is believed that the craft reached an airspeed of 335 knots, fully 15 percent more than the Viscount's never-exceed speed, and about 5 percent in excess of the maximum speed demonstrated when the plane was certified. Consequently, at about 5000 feet both of the horizontal stabilizers failed at once, separating downard. The separation caused the plane to pitch violently downward; the gyroscopic loads combined with inertia to cause all four engine nacelles to break upward. Both wings were then subjected to extreme downloads. Under the pressure the right wing separated, and the integrity of the left was completely destroyed.\n\nWith so much of the aircraft's superstructure gone, the left wing induced drag on the fuselage, yawing it violently to the left. More forces from that direction tore off the vertical fin, which came away with portions of the fuselage, already weakened from losing the left stabilizer, still attached. Further gyrations caused the left wing to disintegrate, opening its fuel tanks and leading to a flash fire. What was left of the fuselage crumbled, and the craft plunged to the ground in rural Maryland.\n\nThe cause of the accident was determined to be a loss of control of the plane in turbulence, resulting in an involuntary steep descent which created aerodynamic loads in excess of those for which the craft had been designed.\n\n"}
{"id": "14280337", "url": "https://en.wikipedia.org/wiki?curid=14280337", "title": "Chemical Automatics Design Bureau", "text": "Chemical Automatics Design Bureau\n\nChemical Automatics Design Bureau (CADB), also KB Khimavtomatika (, KBKhA), is a Russian design bureau founded by the NKAP (People's Commissariat of the Aircraft Industry) in 1941 and led by Semyon Kosberg until his death in 1965. Its origin dates back to a 1940 Moscow carburetor factory, evacuated to Berdsk in 1941, and then relocated to Voronezh city in 1945, where it now operates. Originally designated OKB-296 and tasked to develop fuel equipment for aviation engines, it was redesignated OKB-154 in 1946.\n\nIn 1965 A.D. Konopatov took over leadership. He was succeeded by V.S. Rachuk in 1993, then by Viktor D. Gorokhov (RD-0124 Chief designer) in 2015. During this time the company designed a wide range of high technology products, including liquid propellant rocket engines, a nuclear reactor for space use, the first Soviet laser with an output of 1 MW and the USSR's only operational nuclear rocket engine. The company has designed more than 60 liquid propellant engines with some 30 having entered production.\n\nKB Khimavtomatika's original mandate was to develop aviation fuel systems for Soviet military during World War II. Kosberg had spent ten years working at the \"Central Institute of Aircraft Engine Construction\" on fuel systems and was tapped to run the new bureau. Approaching German armies required the group to relocate to Berdsk, Siberia, where Kosberg and his team of about 30 specialists developed direct injection fuel systems, eventually implemented on the La-5, La-7, Tupolev Tu-2 and Tu-2D. The new fuel systems provided a significant increase in performance over conventional gasoline fuel systems and eliminated carburetor float problems caused by aggressive combat flying. They competed with direct injection systems developed by Daimler Benz at the time. After the end of the war, the design bureau was moved to Voronezh, where it continued to design fuel systems for piston, turboprop and jet aircraft.\n\nSuccessful work results were a basis for the reformation of Plant 154 Design Bureau into the independent company OCB-154. The new enterprise was to develop rocket engines.\nThe works were performed in two directions: development of LREs for space launch vehicles (LV) and missiles.\nStart of works was marked by the meeting of S. Kosberg and S. Korolev on February 10, 1958. The result of this meeting was the joint development of oxygen-kerosene engine RD0105 for LV “Luna” LV stage (engine chief designer V. Koshelnikov). This engine allowed LV to reach second space velocity for the first time in the world, deliver USSR pennon to the Moon surface, make the round flight of the Moon and take pictures of Moon back side. Later on, one of the craters on its backside was named after S. Kosberg.\nKBKhA developed LRE RD0109 for “Vostok” LV third stage (chief designer – V. Koshelnikov) on the basis of engine RD0105. The engine was more reliable and had higher technical specifications due to the creation of the new efficient lightweight combustion chamber. RD 0109 thrusts to orbit space ship Vostok with Y. Gagarin on board, all one-seat manned ships and different military and scientific spacecraft later. The development of space industry in the end of the 50th and beginning of 60th required the creation of more powerful LV for orbiting objects with mass up to 7000 kg. To fulfill this purpose, the Design bureau – on the basis of second stage engine RD0106 of military rocker P-9A - developed engines RD0107, RD0108, and RD0110 (chief designer Y. Gershkovits) for third stages of S. Korolev LVs “Molnia”, “Voshod”, “Soyuz” that ensured launches of interplanetary stations to Mars and Venus, orbiting space ships with 2 and 3 cosmonauts on board. Members of these crews were the first human beings entering into open space, made orbit docking and joint flight of two ships, including American “Apollo”. LV “Soyuz” is used to deliver payload to orbital stations.\nUsing highly reliable engine RD0110, over 1500 LV successful launches were performed. In the beginning of 1965, chief designer S. Kosberg died in a car accident. A. Konopatov was appointed as a lead designer of the Design Bureau.\n\nAnother milestone in the development of Russian space industry was the creation of powerful LV UR500 by General designer V. Chelomey. The LV was able to orbit heavy objects with weight up to 20 tons. For the second stage of LV “Proton” KBKhA created LRE RD0208 and RD0209 (chief designer V. Kozelkov), operating according to oxidizer rich preburner staged combustion schematic. As a prototype, engine RD0206 was used, installed on military missile UR-200. This LV orbited heavy automated stations “Proton”. LV UR500 was later named “Proton”.\nThree-stage “Proton” was a more powerful LV, for whose second stage engines RD0208 and RD0209 were modernized. The modernized engines got indexes RD0210 and RD0211 (chief designer V. Kozelkov). For the third stage engine, RD0212 was renewed (chief designer Y. Gershkovits). Besides, for the position correction of “Almaz” space station, launched by “Proton”, KBKhA created pressure fed engine RD0225 (chief designer V. Borodin) and multiple startup (up to 100 times), with orbit stand-by mode (up to 2 years). These LV delivered Lunar excursion modules to the Moon, interplanetary spacecrafts that took probes of lunar soil and landed on Mars and Venus. It became possible to launch long-stay orbital stations “Salut” and “Mir”, as well as modules “Zarya” and “Zvezda” for International space station. For the moment, over 300 “Proton” LV launches have been performed.\nTechnical perfection of engines RD0110, RD0210, RD0211, RD0212 ensured their long life. For over 40 years these engines have launched different spacecrafts, automated stations, and manned space ships. High energy-weight characteristics and operation simplicity support their position in the best of Russian and foreign engines of the same class.\n\nOne of KBKhA priority directions was the completion of defense contracts – creation of LREs with high energy characteristics and reliability, with low production costs, without servicing during entire life. In 1957, using extensive experience acquired during the development of engines RD0100, RD0101, RD0102 for interceptors, the Design Bureau started the creation of engines for antiaircraft missiles (SAM) on self-ignited components. The first LRE RD0200 (chief designer A. Golubev) was developed for the second stage of S. Lavochkin 5В11 SAM. The engine was designed as open cycle engine with 1 : 10 throttle capability. The engine passed all types of tests and was serially manufactured\nLRE RD0201 (chief designer L. Pozdnyakov) was designed for the third stage of P. Grushin B1100 SAM. The difference of the engine from RD0200 was four swivel combustion chambers due to which flight navigation was performed. In the end of the 50th, the question about the creation of a more powerful rocket R-9 arose, which was to replace rocket 8K72. In 1959-1962 the Design Bureau developed oxygen-kerosene engine RD0106 for LV second stage (block B) (chief designer – Y. Gershkovitz).\nHigh energy characteristics, optimum mounting, relatively small height, simple operation, development time (on ground and flight) were the basis for the development of a variety of engines for Korolev’s space rockets, including RD0110 for the third stage (block И) of Soyuz LV. In the beginning of the 60th, long-term and prolific cooperation of KBKhA and Chelomey Design Bureau started, for whose LVs our design bureau developed about 20 LREs.\nThe creation of powerful LVs during these years required considerable increase of energy characteristics and operational features of LREs. And KBKhA was among the first to start the development of such LREs. In 1961-1964 RD0203 and RD0204 LREs (chief designer V. Kozelkov) for the first stage of rocket UR200 and RD0206 and RD0207 LREs (chief designer L. Pozdnyakov) for the second stage of the same rocket were developed.\nThese new engines were of advanced design, operate on storable fuel components and for the first time staged combustion cycle was used. The application of such schematic allowed double combustion chamber pressure (up to 150 kg/cm2 as compared to 70 kg/cm2 for open cycle engines) and excluded Isp losses for TPA turbine drive.\nPowerful and highly economical engines created in short time, went through ground development and flight tests. The engines were a basis for the creation of new LREs. In 1963, Chelomei Design Bureau started the creation of the new rocket RS-10 for first stage KBKhA developed engines RD0216 and RD0217 were used in 1963-1966 (chief designer V. Koshelnikov). Higher technical and operational requirements to LV defined the necessity of high engine efficiency and reliability, protection of its inner cavities from the environment, etc. All these requirements were fulfilled and confirmed by ground and flight development testing as rocket component.\nThe experience acquired was the basis for development of new generation engines with higher combustion chamber pressures. First engines of this type were RD0233 and RD0234 (chief designer V. Kozelkov, lead designer V. Ezhov), created in 1969-1974 for RS-18 rocket first stage.\nFurther on, two engines were developed: staged combustion RD0235, and open cycle steering engine RD0236 (chief designer V. Kozelkov, lead designer Y. Garmanov) for RS-18 rocket second stage. Engine RD0235 was developed on the basis of RD0216 engine but it is more reliable due to better design and technology possibilities\nThe experience of LRE development was the basis for the engagement of KBKhA in 1967 in the development of engine RD0208 (lead designer Y. Gershkovich) for the second stage of rocket RS-20, designed by general designer M. Yangel. The engine was developed on the basis of a third stage engine RD0212, used in “Proton”, but it was more powerful and was differently applied within the stage.\nThe First Nuclear Rocket Engine\nIn 1965 KBKhA was involved into project of the development of nuclear rocket engines RD0410 and RD0411 (chief designer G. Chursin, lead designers – L. Nikitin, M. Biryukov, A. Belogurov, Y. Mamontov). The engines were specified for the acceleration and deceleration of spacecrafts and orbit correction for deep space explorations. Due to operating fluid high thermodynamic properties and high heating temperatures in the nuclear reactor, (up to 3 000 K), the engine possesses high efficiency (vacuum Isp 910 kg s/kg). For time and costs saving, the nuclear reactor and “cold” engine (feed system, regulation and control components) were developed in parallel. The nuclear reactor is designed according to heterogeneous schematic – its design utilizes block-mounting principle, which allowed to develop uranium-containing (fuel cell) assemblies and reactor separately. The results of the development of RD-0410 nuclear rocket engine were used for development of main turbopump of RD-0120 engine and were the basis for development of multimode space nuclear power plants.\n\nIn early 70s KBKhA began development of continuous high power, gas-dynamic of CO2-lasers (GDL), operating on the transformation of the heat energy of active gaseous medium, obtained with non-equilibrium expansion in supersonic nozzle grid, into electromagnetic radiation. The family of GDL samples was created with radiation energy from 10 to 600 kW and space on-board GDL RD0600 working on gaseous propellant (the leading designers — V.P. Koshelnikov, G.I. Zavision, V.Y. Guterman).\n\nBy 1954 the bureau was designing liquid-propellant rocket engines for super performance and experimental aircraft, the Yak-27V and E-50A, and from 1957 to 1962 they designed engines for anti-aircraft guided missiles. By the early 1960s the bureau was designing Liquid Propellant Rocket Engines (LPREs) for man-rated space launch vehicles.\n\nOver several decades, the CADB became one of the Soviet Union's premier developers of LPREs, designing engines for the SS-11, SS-18 and SS-19 and ballistic missiles, among others. In one unique design, the engine is submerged in the UDMH propellant tank to save space (SS-N-23 submarine-launched ballistic missile). They also designed upper stage engines for the Soyuz and Proton space launch vehicles, along with the core engines for the Energia. The large volume of design work and continuous refinement led to a high degree of technical capability. During this same period in the United States (late 1960s - early 1970s), liquid engines on missiles were dropped in favor of solids, and the only LPRE being developed was the Space Shuttle Main Engine. The Kosberg design bureau parlayed their experience into the RD-0120 - the Soviet's first cryogenic engine with over 40 tonnes thrust. Despite designing mostly LOX/Kerosene or N2O4/UDMH engines, the LOX/LH2 RD-0120 had similar ratings and performance as the SSME, but with a lower cost due to the choice of technology.\n\nCADB is currently offering the RD-0146 to the international market as an alternative to the RL-10. With a reduction in the market for LPRE's, the company has expanded into related fields, designing products for oil and gas, agricultural and medical industries.\n\nKBKhA team possesses productive design experience, highly qualified scientists in staff (6 Doctors of Science and over 50 PhDs), designers, production engineers, and workers who keep on working on the creation of the new rocket engines and power plants.\n\nSince 1993 the development of four-chamber LOX-kerosene LRE RD-0124, 14D23 (the chief designers — V. Koselkov and V. Gorokhov, the lead designers — V. Borodin, A. Plis and V. Gurin) for the third stage of the general designer D. Koslov \"Sojuz-2\" launch vehicle has been conducted. The main engine destination — delivery into the orbit of different payloads: satellites, cargo and manned space vehicles. RD-0124 engine is developed as substitution for RD-0110. It has the practically identical interfaces, overall dimensions and mass, but it offers the higher specific parameters — the best of the developed LRE of this class. The engine operates according to oxidizer rich stage combustion cycle and has highe r (on 33 s) efficiency compared to RD-0110. This will allow to put into orbit larger payloads (~ 950 kg) or to ensure launching of \"Sojuz-2\" launch vehicle from spaceports located to the north of Baikonur. The conducted series of successful stand tests has confirmed the fulfillment of the specification requirements for main parameters. Two test-bench fire tests within LV “Soyuz-2” 3rd stage were performed that completed the 1st phase of on-ground engine development.\nDecember 27, 2006, first flight test of the engine within LV “Soyuz-2b” was performed. In 1998 KBKhA has studied and determined the possibility of using the RD-0124 (the RD-0124A) for the second stage of space rocket complex \"Angara\", created by Khrunichev Design and Research center and aimed for orbiting multiple purpose space vehicles. The main differences from the requirements to base engine are the change of engine operating time of the main and final thrust stage.\nOn December 1, 2007, 150 fire tests were performed, with overall development time over 30,000 seconds, which confirmed the compliance of main parameters with Technical Task requirements.\nRD-0750\nIn 1993-1998 large volume of design, analysis, research and experimental works on development of three-propellant dual-mode engine on the base of RD-0120 have been conducted as a KBKhA initiative. The propellants of the engine are: liquid hydrogen, kerosene, and liquid oxygen.\nStudies and recommendations of the leading Russian R&D Institutes and foreign firms shown an economic feasibility of application of dual-mode three-propellant engines to advanced launch vehicles (especially single-staged) have become the real support for three-propellant engine works performance. The engine according to the first mode operates on oxygen and kerosene with the small addition of hydrogen and at the second operational mode - with oxygen and hydrogen.\nAs a result of this work, for the first time, in practice of LRE development, three-propellant dual-mode preburner (the lead designers – Y.A. Martynenko, V.A. Turtushov) successfully tested in KBKhA and in RD0750D demonstrator conditions at NIICHIMMASH.\n\nIn 1997 KBKhA according to the Khrunichev Space Center Technical Specification has begun the development of the new oxygen-hydrogen engine RD-0146 (the chief designer — N.E. Titkov, the lead designer — I.V. Liplavy) for space boosters of advanced launchers options «Proton» and «Angara». For the first time in Russia the expander cycle engine has been developed with insurance of multiple in-flight starts. Since 2001, 4 engines were manufactured, independent tests of engine subassemblies and chamber with igniter were performed at modes higher than nominal. Altogether 30 fire tests at mode up to 109.5% and with overall operational time 1680 seconds were completed. The development time per each engine was 1604 seconds in 27 tests.\n\nIn 1995 the research work for development expander kerosene-hydrogen LREs for advanced space boost units and interorbital tows has been initiated. It has defined the engine configuration and performances. This work was completed by issue of technical proposal. On the basis of this work RKK «Energia» has issued specification for RD-0126 engine development that was presented in two variants:\nEngine RD0126 - with a traditional Laval nozzle chamber, and RD0126Э with an expansion-deflection nozzle and ring throat (chief designer V. Grokhov, lead designer – I. Liplyavy).\nEngine RD0126Э has the following advantages as compared to traditional LREs:\nequal length, but higher vacuum Isp;\nlighter weight with the same Isp;\npossibility to obtain higher hydrogen temperature in cooling channels, which allows to use it as working medium for TPA turbine rotation;\npossibility of engine ground testing performed under high-altitude conditions without gas-dynamic tube.\nIn 1998, test bench chamber with ring throat was tested. 5 sea level fire tests were performed that confirmed combustion products flow without boundary layer separation within high-altitude nozzle, which makes engine development considerably simpler. The calculated performance data complied with the design figures. Steady state operation process was stable; hardware is in satisfactory operable condition.\n\nSince 1994 according Baranov CIAM Institute specification KBKhA has been developed experimental axial symmetrical scramjet 58L (the lead designers — Y.V. Liplavy, Y.A. Martynenko), for studying of processes of hydrogen combustion at stream velocities 3-6.5 M and altitudes of 20–35 km flight conditions. The liquid hydrogen is an engine fuel passing CC cooling channels and being introduced into the combustion zones. The combustion chamber is an annular and three-zone design. In the first zone the hydrogen combustion takes place in subsonic airflow, in two others — in supersonic flow. The combustion chamber is completely designed and manufactured in KBKhA, and the new and advanced design and technological solutions have been realized. In 1998 the flight tests of scramjet on board Kholod laboratory have been successfully conducted. The engine operation started at flight velocity 3 M, at the end of the flight on 77 s the vehicle velocity reached 6.47 M. For the first time in the world hydrogen combustion has taken place under supersonic flow conditions. Engine has operated according to the test program and without remarks under testing program.\n\nin 2013 the Chemical Automatics Design Bureau successfully conducted a test bench magnetoplasmadynamic engine for long-distance space travel. Magnetoplasmadynamic engine without flaws ion engines.\n\nAt the test facility Chemical Automation Design Bureau has successfully completed a series of initial tests of the high ion electric propulsion. Tests carried out successfully on a special stand vacuum and confirmed the compliance parameters of the engine characteristics, laid down in the specifications.\nWorks with the engine continues: new tests planned for production resources and test the stability of proven performance in continuous operation.\nCreation of electric rocket engines was started in the company in 2012. By developing ion electric propulsion team started after KBKhA won the 2013 competition of the Ministry of Education and Science of the Russian Federation to receive subsidies for the realization of complex projects for the organization of high-tech production. The company was among the winners of the project \"Creation of high-tech production and testing base for the development, Metal processing and industrial production of the new generation of electric propulsion.\" \n\n"}
{"id": "4362087", "url": "https://en.wikipedia.org/wiki?curid=4362087", "title": "Chu space", "text": "Chu space\n\nChu spaces generalize the notion of topological space by dropping the requirements that the set of open sets be closed under union and finite intersection, that the open sets be extensional, and that the membership predicate (of points in open sets) be two-valued. The definition of continuous function remains unchanged other than having to be worded carefully to continue to make sense after these generalizations.\n\nThe name is due to Po-Hsiang Chu, who originally constructed a verification of autonomous categories as a graduate student under the direction of Michael Barr in 1979.\n\nUnderstood statically, a Chu space (\"A\", \"r\", \"X\") over a set \"K\" consists of a set \"A\" of points, a set \"X\" of states, and a function \"r\" : \"A\" × \"X\" → \"K\". This makes it an \"A\" × \"X\" matrix with entries drawn from \"K\", or equivalently a \"K\"-valued binary relation between \"A\" and \"X\" (ordinary binary relations being 2-valued).\n\nUnderstood dynamically, Chu spaces transform in the manner of topological spaces, with \"A\" as the set of points, \"X\" as the set of open sets, and \"r\" as the membership relation between them, where \"K\" is the set of all possible degrees of membership of a point in an open set. The counterpart of a continuous function from (\"A\", \"r\", \"X\") to (\"B\", \"s\", \"Y\") is a pair (\"f\", \"g\") of functions \"f\" : \"A\" → \"B\", \"g\" : \"Y\" → \"X\" satisfying the \"adjointness condition\" \"s\"(\"f\"(\"a\"), \"y\") = \"r\"(\"a\", \"g\"(\"y\")) for all \"a\" ∈ \"A\" and \"y\" ∈ \"Y\". That is, \"f\" maps points forwards at the same time as \"g\" maps states backwards. The adjointness condition makes \"g\" the inverse image function \"f\", while the choice of \"X\" for the codomain of \"g\" corresponds to the requirement for continuous functions that the inverse image of open sets be open. Such a pair is called a Chu transform or morphism of Chu spaces.\n\nA topological space (\"X\", \"T\") where \"X\" is the set of points and \"T\" the set of open sets, can be understood as a Chu space (\"X\",∈,\"T\") over {0, 1}. That is, the points of the topological space become those of the Chu space while the open sets become states and the membership relation \" ∈ \" between points and open sets is made explicit in the Chu space. The condition that the set of open sets be closed under arbitrary (including empty) union and finite (including empty) intersection becomes the corresponding condition on the columns of the matrix. A continuous function \"f\": \"X\" → \"X\"' between two topological spaces becomes an adjoint pair (\"f\",\"g\") in which \"f\" is now paired with a realization of the continuity condition constructed as an explicit witness function \"g\" exhibiting the requisite open sets in the domain of \"f\".\n\nThe category of Chu spaces over \"K\" and their maps is denoted by Chu(Set, \"K\"). As is clear from the symmetry of the definitions, it is a self-dual category: it is equivalent (in fact isomorphic) to its dual, the category obtained by reversing all the maps. It is furthermore a *-autonomous category with dualizing object (\"K\", λ, {*}) where λ : \"K\" × {*} → \"K\" is defined by λ(\"k\", *) = \"k\" (Barr 1979). As such it is a model of Jean-Yves Girard's linear logic (Girard 1987).\n\nThe more general enriched category Chu(\"V\", \"k\") originally appeared in an appendix to Barr (1979). The Chu space concept originated with Michael Barr and the details were developed by his student Po-Hsiang Chu, whose master's thesis formed the appendix. Ordinary Chu spaces arise as the case \"V\" = Set, that is, when the monoidal category \"V\" is specialized to the cartesian closed category Set of sets and their functions, but were not studied in their own right until more than a decade after the appearance of the more general enriched notion. A variant of Chu spaces, called dialectica spaces, due to replaces the map condition (1) with the map condition (2):\n\n\nThe category Top of topological spaces and their continuous functions embeds in Chu(Set, 2) in the sense that there exists a full and faithful functor \"F\" : Top → Chu(Set, 2) providing for each topological space (\"X\", \"T\") its \"representation\" \"F\"((\"X\", \"T\")) = (\"X\", ∈, \"T\") as noted above. This representation is moreover a \"realization\" in the sense of Pultr and Trnková (1980), namely that the representing Chu space has the same set of points as the represented topological space and transforms in the same way via the same functions.\n\nChu spaces are remarkable for the wide variety of familiar structures they realize. Lafont and Streicher (1991) point out that Chu spaces over 2 realize both topological spaces and coherent spaces (introduced by J.-Y. Girard (1987) to model linear logic), while Chu spaces over \"K\" realize any category of vector spaces over a field whose cardinality is at most that of \"K\". This was extended by Vaughan Pratt (1995) to the realization of \"k\"-ary relational structures by Chu spaces over 2. For example, the category Grp of groups and their homomorphisms is realized by Chu(Set, 8) since the group multiplication can be organized as a ternary relation. Chu(Set, 2) realizes a wide range of ``logical`` structures such as semilattices, distributive lattices, complete and completely distributive lattices, Boolean algebras, complete atomic Boolean algebras, etc. Further information on this and other aspects of Chu spaces, including their application to the modeling of concurrent behavior, may be found at \"Chu Spaces\".\n\nChu spaces can serve as a model of concurrent computation in automata theory to express branching time and true concurrency. Chu spaces exhibit the quantum mechanical phenomena of complementarity and uncertainty. The complementarity arises as the duality of information and time, automata and schedules, and states and events. Uncertainty arises when a measurement is defined to be a morphism such that increasing structure in the observed object reduces the clarity of observation. This uncertainty can be calculated numerically from its form factor to yield the usual Heisenberg uncertainty relation. Chu spaces correspond to wavefunctions as vectors of Hilbert space.\n\n\n"}
{"id": "235899", "url": "https://en.wikipedia.org/wiki?curid=235899", "title": "Circuit breaker", "text": "Circuit breaker\n\nA circuit breaker is an automatically operated electrical switch designed to protect an electrical circuit from damage caused by excess current from an overload or short circuit. Its basic function is to interrupt current flow after a fault is detected. Unlike a fuse, which operates once and then must be replaced, a circuit breaker can be reset (either manually or automatically) to resume normal operation.\n\nCircuit breakers are made in varying sizes, from small devices that protect low-current circuits or individual household appliance, up to large switchgear designed to protect high voltage circuits feeding an entire city. The generic function of a circuit breaker, RCD or a fuse, as an automatic means of removing power from a faulty system is often abbreviated as OCPD (Over Current Protection Device).\n\nAn early form of circuit breaker was described by Thomas Edison in an 1879 patent application, although his commercial power distribution system used fuses. Its purpose was to protect lighting circuit wiring from accidental short circuits and overloads. A modern miniature circuit breaker similar to the ones now in use was patented by Brown, Boveri & Cie in 1924. Hugo Stotz, an engineer who had sold his company to BBC, was credited as the inventor on DRP (\"Deutsches Reichspatent\") 458392. Stotz's invention was the forerunner of the modern thermal-magnetic breaker commonly used in household load centers to this day.\n\nInterconnection of multiple generator sources into an electrical grid required the development of circuit breakers with increasing voltage ratings and increased ability to safely interrupt the increasing short-circuit currents produced by networks. Simple air-break manual switches produced hazardous arcs when interrupting high voltages; these gave way to oil-enclosed contacts, and various forms using the directed flow of pressurized air, or of pressurized oil, to cool and interrupt the arc. By 1935, the specially constructed circuit breakers used at the Boulder Dam project use eight series breaks and pressurized oil flow to interrupt faults of up to 2,500 MVA, in three cycles of the AC power frequency.\n\nAll circuit breaker systems have common features in their operation, but details vary substantially depending on the voltage class, current rating and type of the circuit breaker.\n\nThe circuit breaker must first detect a fault condition. In small mains and low voltage circuit breakers, this is usually done within the device itself. Typically, the heating or magnetic effects of electric current are employed. Circuit breakers for large currents or high voltages are usually arranged with protective relay pilot devices to sense a fault condition and to operate the opening mechanism. These typically require a separate power source, such as a battery, although some high-voltage circuit breakers are self-contained with current transformers, protective relays, and an internal control power source.\n\nOnce a fault is detected, the circuit breaker contacts must open to interrupt the circuit; this is commonly done using mechanically stored energy contained within the breaker, such as a spring or compressed air to separate the contacts. Circuit breakers may also use the higher current caused by the fault to separate the contacts, such as thermal expansion or a magnetic field. Small circuit breakers typically have a manual control lever to switch off the load or reset a tripped breaker, while larger units use solenoids to trip the mechanism, and electric motors to restore energy to the springs.\n\nThe circuit breaker contacts must carry the load current without excessive heating, and must also withstand the heat of the arc produced when interrupting (opening) the circuit. Contacts are made of copper or copper alloys, silver alloys and other highly conductive materials. Service life of the contacts is limited by the erosion of contact material due to arcing while interrupting the current. Miniature and molded-case circuit breakers are usually discarded when the contacts have worn, but power circuit breakers and high-voltage circuit breakers have replaceable contacts.\n\nWhen a high current or voltage is interrupted, an arc is generated. The length of the arc is generally proportional to the voltage while the intensity (or heat) is proportional to the current. This arc must be contained, cooled and extinguished in a controlled way, so that the gap between the contacts can again withstand the voltage in the circuit. Different circuit breakers use vacuum, air, insulating gas, or oil as the medium the arc forms in. Different techniques are used to extinguish the arc including:\n\nFinally, once the fault condition has been cleared, the contacts must again be closed to restore power to the interrupted circuit.\n\nLow-voltage miniature circuit breakers (MCB) use air alone to extinguish the arc. These circuit breakers contain so-called arc chutes, a stack of mutually insulated parallel metal plates that divide and cool the arc. By splitting the arc into smaller arcs the arc is cooled down while the arc voltage is increased and serves as an additional impedance that limits the current through the circuit breaker. The current-carrying parts near the contacts provide easy deflection of the arc into the arc chutes by a magnetic force of a current path, although magnetic blowout coils or permanent magnets could also deflect the arc into the arc chute (used on circuit breakers for higher ratings). The number of plates in the arc chute is dependent on the short-circuit rating and nominal voltage of the circuit breaker.\n\nIn larger ratings, oil circuit breakers rely upon vaporization of some of the oil to blast a jet of oil through the arc.\n\nGas (usually sulfur hexafluoride) circuit breakers sometimes stretch the arc using a magnetic field, and then rely upon the dielectric strength of the sulfur hexafluoride (SF) to quench the stretched arc.\n\nVacuum circuit breakers have minimal arcing (as there is nothing to ionize other than the contact material). The arc quenches when it is stretched a very small amount (less than ). Vacuum circuit breakers are frequently used in modern medium-voltage switchgear to 38,000 volts.\n\nAir circuit breakers may use compressed air to blow out the arc, or alternatively, the contacts are rapidly swung into a small sealed chamber, the escaping of the displaced air thus blowing out the arc.\n\nCircuit breakers are usually able to terminate all current very quickly: typically the arc is extinguished between 30 ms and 150 ms after the mechanism has been tripped, depending upon age and construction of the device. The maximum current value and let-through energy determine the quality of the circuit breakers.\n\nCircuit breakers are rated both by the normal current that they are expected to carry, and the maximum short-circuit current that they can safely interrupt. This latter figure is the ampere interrupting capacity (AIC) of the breaker.\n\nUnder short-circuit conditions, the calculated maximum prospective short-circuit current may be many times the normal, rated current of the circuit. When electrical contacts open to interrupt a large current, there is a tendency for an arc to form between the opened contacts, which would allow the current to continue. This condition can create conductive ionized gases and molten or vaporized metal, which can cause further continuation of the arc, or creation of additional short circuits, potentially resulting in the explosion of the circuit breaker and the equipment that it is installed in. Therefore, circuit breakers must incorporate various features to divide and extinguish the arc.\n\nThe maximum short-circuit current that a breaker can interrupt is determined by testing. Application of a breaker in a circuit with a prospective short-circuit current higher than the breaker's interrupting capacity rating may result in failure of the breaker to safely interrupt a fault. In a worst-case scenario the breaker may successfully interrupt the fault, only to explode when reset.\n\nTypical domestic panel circuit breakers are rated to interrupt () short-circuit current.\n\nMiniature circuit breakers used to protect control circuits or small appliances may not have sufficient interrupting capacity to use at a panel board; these circuit breakers are called \"supplemental circuit protectors\" to distinguish them from distribution-type circuit breakers.\n\nCircuit breakers are manufactured in standard sizes, using a system of preferred numbers to cover a range of ratings. Miniature circuit breakers have a fixed trip setting; changing the operating current value requires changing the whole circuit breaker. Larger circuit breakers can have adjustable trip settings, allowing standardized elements to be applied but with a setting intended to improve protection. For example, a circuit breaker with a 400 ampere \"frame size\" might have its overcurrent detection set to operate at only 300 amperes, to protect a feeder cable.\n\nInternational Standards, IEC 60898-1 and European Standard EN 60898-1, define the \"rated current\" \"I\" of a circuit breaker for low voltage distribution applications as the maximum current that the breaker is designed to carry continuously (at an ambient air temperature of 30 °C). The commonly available preferred values for the rated current are1A,2A,4A, 6 A, 10 A, 13 A, 16 A, 20 A, 25 A, 32 A, 40 A, 50 A, 63 A, 80 A, 100 A, and 125 A (similar to the R10 Renard series, but using 6, 13, and 32 instead of 6.3, 12.5, and 31.5 – it includes the 13 A current limit of British BS 1363 sockets). The circuit breaker is labeled with the rated current in amperes, but excluding the unit symbol, A. Instead, the ampere figure is preceded by a letter, \"B\", \"C\", or \"D\", which indicates the \"instantaneous tripping current\" — that is, the minimum value of current that causes the circuit breaker to trip without intentional time delay (i.e., in less than 100 ms), expressed in terms of \"I\":\nCircuit breakers are also rated by the maximum fault current that they can interrupt; this allows use of more economical devices on systems unlikely to develop the high short-circuit current found on, for example, a large commercial building distribution system.\n\nIn the United States, Underwriters Laboratories (UL) certifies equipment ratings, called Series Ratings (or \"integrated equipment ratings\") for circuit breaker equipment used for buildings. Power circuit breakers and medium- and high-voltage circuit breakers used for industrial or electric power systems are designed and tested to ANSI or IEEE standards in the C37 series.\n\nMany classifications of circuit breakers can be made, based on their features such as voltage class, construction type, interrupting type, and structural features.\n\nLow-voltage (less than 1,000 V) types are common in domestic, commercial and industrial application, and include:\n\nThe characteristics of low-voltage circuit breakers are given by international standards such as IEC 947. These circuit breakers are often installed in draw-out enclosures that allow removal and interchange without dismantling the switchgear.\n\nLarge low-voltage molded case and power circuit breakers may have electric motor operators so they can open and close under remote control. These may form part of an automatic transfer switch system for standby power.\n\nLow-voltage circuit breakers are also made for direct-current (DC) applications, such as DC for subway lines. Direct current requires special breakers because the arc is continuous—unlike an AC arc, which tends to go out on each half cycle. A direct current circuit breaker has blow-out coils that generate a magnetic field that rapidly stretches the arc. Small circuit breakers are either installed directly in equipment, or are arranged in a breaker panel.\nThe DIN rail-mounted thermal-magnetic miniature circuit breaker is the most common style in modern domestic consumer units and commercial electrical distribution boards throughout Europe. The design includes the following components:\n\n\n\"Solid-state circuit breakers\", also known as \"Digital circuit breakers\" are a technological innovation which promises advance circuit breaker technology out of the mechanical level, into the electrical. This promises several advantages, such as cutting the circuit in fractions of microseconds, better monitoring of circuit loads and longer lifetimes. \n\n\"Magnetic circuit breakers\" use a solenoid (electromagnet) whose pulling force increases with the current. Certain designs utilize electromagnetic forces in addition to those of the solenoid. The circuit breaker contacts are held closed by a latch. As the current in the solenoid increases beyond the rating of the circuit breaker, the solenoid's pull releases the latch, which lets the contacts open by spring action. They are the most commonly used circuit breakers.\n\n\"Thermal magnetic circuit breakers\", which are the type found in most distribution boards, incorporate both techniques with the electromagnet responding instantaneously to large surges in current (short circuits) and the bimetallic strip responding to less extreme but longer-term over-current conditions. The thermal portion of the circuit breaker provides a time response feature, that trips the circuit breaker sooner for larger overcurrents but allows smaller overloads to persist for a longer time. This allows short current spikes such as are produced when a motor or other non-resistive load is switched on. With very large over-currents during a short-circuit, the magnetic element trips the circuit breaker with no intentional additional delay.\n\nA \"magnetic-hydraulic\" circuit breaker uses a solenoid coil to provide operating force to open the contacts. Magnetic-hydraulic breakers incorporate a hydraulic time delay feature using a viscous fluid. A spring restrains the core until the current exceeds the breaker rating. During an overload, the speed of the solenoid motion is restricted by the fluid. The delay permits brief current surges beyond normal running current for motor starting, energizing equipment, etc. Short-circuit currents provide sufficient solenoid force to release the latch regardless of core position thus bypassing the delay feature. Ambient temperature affects the time delay but does not affect the current rating of a magnetic breaker. \n\nLarge power circuit breakers, applied in circuits of more than 1000 volts, may incorporate hydraulic elements in the contact operating mechanism. Hydraulic energy may be supplied by a pump, or stored in accumulators. These form a distinct type from oil-filled circuit breakers where oil is the arc extinguishing medium. \n\nWhen supplying a branch circuit with more than one live conductor, each live conductor must be protected by a breaker pole. To ensure that all live conductors are interrupted when any pole trips, a \"common trip\" breaker must be used. These may either contain two or three tripping mechanisms within one case, or for small breakers, may externally tie the poles together via their operating handles. Two-pole common trip breakers are common on 120/240-volt systems where 240 volt loads (including major appliances or further distribution boards) span the two live wires. Three-pole common trip breakers are typically used to supply three-phase electric power to large motors or further distribution boards.\n\nTwo- and four-pole breakers are used when there is a need to disconnect multiple phase AC, or to disconnect the neutral wire to ensure that no current flows through the neutral wire from other loads connected to the same network when workers may touch the wires during maintenance. Separate circuit breakers must never be used for live and neutral, because if the neutral is disconnected while the live conductor stays connected, a dangerous condition arises: the circuit appears de-energized (appliances don't work), but wires remain live and some residual-current devices (RCDs) may not trip if someone touches the live wire (because some RCDs need power to trip). This is why only common trip breakers must be used when neutral wire switching is needed.\n\nMedium-voltage circuit breakers rated between 1 and 72 kV may be assembled into metal-enclosed switchgear line ups for indoor use, or may be individual components installed outdoors in a substation. Air-break circuit breakers replaced oil-filled units for indoor applications, but are now themselves being replaced by vacuum circuit breakers (up to about 40.5 kV). Like the high voltage circuit breakers described below, these are also operated by current sensing protective relays operated through current transformers. The characteristics of MV breakers are given by international standards such as IEC 62271. Medium-voltage circuit breakers nearly always use separate current sensors and protective relays, instead of relying on built-in thermal or magnetic overcurrent sensors.\n\nMedium-voltage circuit breakers can be classified by the medium used to extinguish the arc:\n\nMedium-voltage circuit breakers may be connected into the circuit by bolted connections to bus bars or wires, especially in outdoor switchyards. Medium-voltage circuit breakers in switchgear line-ups are often built with draw-out construction, allowing breaker removal without disturbing power circuit connections, using a motor-operated or hand-cranked mechanism to separate the breaker from its enclosure.\nSome important manufacturer of vacuum circuit breakers from 3.3 kV to 38 kV are ABB, Schneider Electric, Eaton, Siemens, HHI (Hyundai Heavy Industry), S&C Electric Company, Jyoti and BHEL.\n\nElectrical power transmission networks are protected and controlled by high-voltage breakers. The definition of \"high voltage\" varies but in power transmission work is usually thought to be 72.5 kV or higher, according to a recent definition by the International Electrotechnical Commission (IEC). High-voltage breakers are nearly always solenoid-operated, with current sensing protective relays operated through current transformers. In substations the protective relay scheme can be complex, protecting equipment and buses from various types of overload or ground/earth fault.\n\nHigh-voltage breakers are broadly classified by the medium used to extinguish the arc:\n\nDue to environmental and cost concerns over insulating oil spills, most new breakers use SF gas to quench the arc.\n\nCircuit breakers can be classified as \"live tank\", where the enclosure that contains the breaking mechanism is at line potential, or \"dead tank\" with the enclosure at earth potential. High-voltage AC circuit breakers are routinely available with ratings up to 765 kV. 1,200 kV breakers were launched by Siemens in November 2011, followed by ABB in April the following year.\n\nHigh-voltage circuit breakers used on transmission systems may be arranged to allow a single pole of a three-phase line to trip, instead of tripping all three poles; for some classes of faults this improves the system stability and availability.\n\nHigh-voltage direct current circuit breakers are still a field of research as of 2015. Such breakers would be useful to interconnect HVDC transmission systems.\n\nA sulfur hexafluoride circuit breaker uses contacts surrounded by sulfur hexafluoride gas to quench the arc. They are most often used for transmission-level voltages and may be incorporated into compact gas-insulated switchgear. In cold climates, supplemental heating or de-rating of the circuit breakers may be required due to liquefaction of the SF6 gas.\n\nThe disconnecting circuit breaker (DCB) was introduced in 2000 and is a high-voltage circuit breaker modeled after the SF-breaker. It presents a technical solution where the disconnecting function is integrated in the breaking chamber, eliminating the need for separate disconnectors. This increases the availability, since open-air disconnecting switch main contacts need maintenance every 2–6 years, while modern circuit breakers have maintenance intervals of 15 years. Implementing a DCB solution also reduces the space requirements within the substation, and increases the reliability, due to the lack of separate disconnectors.\n\nIn order to further reduce the required space of substation, as well as simplifying the design and engineering of the substation, a fiber optic current sensor (FOCS) can be integrated with the DCB. A 420 kV DCB with integrated FOCS can reduce a substation’s footprint with over 50% compared to a conventional solution of live tank breakers with disconnectors and current transformers, due to reduced material and no additional insulation medium.\n\nIn 2012 ABB presented a 75 kV high-voltage breaker that uses carbon dioxide as the medium to extinguish the arc. The carbon dioxide breaker works on the same principles as an SF breaker and can also be produced as a disconnecting circuit breaker. By switching from SF to CO it is possible to reduce the CO emissions by 10 tons during the product’s life cycle.\n\nSeveral firms have looked at adding monitoring for appliances via electronics or using a digital circuit breaker to monitor the breakers remotely. Utility companies in the United States have been reviewing use of the technology to turn on and off appliances, as well as potentially turning off charging of electric cars during periods of high electrical grid load. These devices under research and testing would have wireless capability to monitor the electrical in a house via a smartphone app or other means.\n\nAtom Power, a company based in North Carolina, is also considering adopting \"smart\" circuit breakers for industrial applications, which may have greater benefits. Siemens has publicly said that the circuit breaker technology shows “great promise” for future applications.\n\nThe following types are described in separate articles.\n\n\n"}
{"id": "20957502", "url": "https://en.wikipedia.org/wiki?curid=20957502", "title": "Colloidal crystal", "text": "Colloidal crystal\n\nA colloidal crystal is an ordered array of colloid particles and fine grained materials analogous to a standard crystal whose repeating subunits are atoms or molecules. A natural example of this phenomenon can be found in the gem opal, where spheres of silica assume a close-packed locally periodic structure under moderate compression. Bulk properties of a colloidal crystal depend on composition, particle size, packing arrangement, and degree of regularity. Applications include photonics, materials processing, and the study of self-assembly and phase transitions.\n\nA colloidal crystal is a highly ordered array of particles which can be formed over a long range (to about a centimeter). Arrays such as this appear to be analogous to their atomic or molecular counterparts with proper scaling considerations. A good natural example of this phenomenon can be found in precious opal, where brilliant regions of pure spectral color result from close-packed domains of colloidal spheres of amorphous silicon dioxide, SiO (see above illustration). The spherical particles precipitate in highly siliceous pools and form highly ordered arrays after years of sedimentation and compression under hydrostatic and gravitational forces. The periodic arrays of spherical particles make similar arrays of interstitial voids, which act as a natural diffraction grating for light waves in photonic crystals, especially when the interstitial spacing is of the same order of magnitude as the incident lightwave.\n\nThe origins of colloidal crystals go back to the mechanical properties of bentonite sols, and the optical properties of Schiller layers in iron oxide sols. The properties are supposed to be due to the ordering of monodisperse inorganic particles. Monodisperse colloids, capable of forming long-range ordered arrays, existing in nature. The discovery by W.M. Stanley of the crystalline forms of the tobacco and tomato viruses provided examples of this. Using X-ray diffraction methods, it was subsequently determined that when concentrated by centrifuging from dilute water suspensions, these virus particles often organized themselves into highly ordered arrays.\n\nRod-shaped particles in the tobacco mosaic virus could form a two-dimensional triangular lattice, while a body-centered cubic structure was formed from the almost spherical particles in the tomato Bushy Stunt Virus. In 1957, a letter describing the discovery of \"A Crystallizable Insect Virus\" was published in the journal \"Nature\". Known as the Tipula Iridiscent Virus, from both square and triangular arrays occurring on crystal faces, the authors deduced the face-centered cubic close-packing of virus particles. This type of ordered array has also been observed in cell suspensions, where the symmetry is well adapted to the mode of reproduction of the organism. The limited content of genetic material places a restriction on the size of the protein to be coded by it. The use of a large number of the same proteins to build a protective shell is consistent with the limited length of RNA or DNA content.\n\nIt has been known for many years that, due to repulsive Coulombic interactions, electrically charged macromolecules in an aqueous environment can exhibit long-range crystal-like correlations with interparticle separation distances often being considerably greater than the individual particle diameter. In all of the cases in nature, the same iridescence is caused by the diffraction and constructive interference of visible lightwaves which falls under Bragg’s law.\n\nBecause of the rarity and pathological properties, neither opal nor any of the organic viruses have been very popular in scientific laboratories. The number of experiments exploring the physics and chemistry of these “colloidal crystals” has emerged as a result of the simple methods which have evolved in 20 years for preparing synthetic monodisperse colloids, both polymer and mineral, and, through various mechanisms, implementing and preserving their long-range order formation.\n\nColloidal crystals are receiving increased attention, largely due to their mechanisms of ordering and self-assembly, cooperative motion, structures similar to those observed in condensed matter by both liquids and solids, and structural phase transitions. Phase equilibrium has been considered within the context of their physical similarities, with appropriate scaling, to elastic solids. Observations of the interparticle separation distance has shown a decrease on ordering. This led to a re-evaluation of Langmuir's beliefs about the existence of a long-range attractive component in the interparticle potential.\n\nColloidal crystals have found application in optics as photonic crystals. Photonics is the science of generating, controlling, and detecting photons (packets of light), particularly in the visible and near Infrared, but also extending to the Ultraviolet, Infrared and far IR portions of the electromagnetic spectrum. The science of photonics includes the emission, transmission, amplification, detection, modulation, and switching of lightwaves over a broad range of frequencies and wavelengths. Photonic devices include electro-optic components such as lasers (Light Amplification by Stimulated Emission of Radiation) and optical fiber. Applications include telecommunications, information processing, illumination, spectroscopy, holography, medicine (surgery, vision correction, endoscopy), military (guided missile) technology, agriculture and robotics.\n\nPolycrystalline colloidal structures have been identified as the basic elements of submicrometre colloidal materials science.\nMolecular self-assembly has been observed in various biological systems and underlies the formation of a wide variety of complex biological structures. This includes an emerging class of mechanically superior biomaterials based on microstructure features and designs found in nature.\n\nThe principal mechanical characteristics and structures of biological ceramics, polymer composites, elastomers, and cellular materials are being re-evaluated, with an emphasis on bioinspired materials and structures. Traditional approaches focus on design methods of biological materials using conventional synthetic materials. The uses have been identified in the synthesis of bioinspired materials through processes that are characteristic of biological systems in nature. This includes the nanoscale self-assembly of the components and the development of hierarchical structures.\n\nAggregation in colloidal dispersions (or stable suspensions) has been characterized by the degree of interparticle attraction. For attractions strong relative to the thermal energy (given by kT), Brownian motion produces irreversibly flocculated structures with growth rates limited by the rate of particle diffusion. This leads to a description using such parameters as the degree of branching, ramification or fractal dimensionality. A reversible growth model has been constructed by modifying the cluster-cluster aggregation model with a finite inter-particle attraction energy.\n\nIn systems where forces of attraction forces are buffered to some degree, a balance of forces leads to an equilibrium phase separation, that is particles coexist with equal chemical potential in two distinct structural phases. The role of the ordered phase as an elastic colloidal solid has been evidenced by the elastic (or reversible) deformation due to the force of gravity. This deformation can be quantified by the distortion of the lattice parameter, or inter-particle spacing.\n\nPeriodic ordered lattices behave as linear viscoelastic solids when subjected to small amplitude mechanical deformations. Okano's group experimentally correlated the shear modulus to the frequency of standing shear modes using mechanical resonance techniques in the ultrasonic range (40 to 70 kHz). In oscillatory experiments at lower frequencies (< 40 Hz), the fundamental mode of vibration as well as several higher frequency partial overtones (or harmonics) have been observed. Structurally, most systems exhibit a clear instability toward the formation of periodic domains of relatively short-range order Above a critical amplitude of oscillation, plastic deformation is the primary mode of structural rearrangement.\n\nEquilibrium phase transitions (e.g. order/disorder), an equation of state, and the kinetics of colloidal crystallization have all been actively studied, leading to the development of several methods to control the self-assembly of the colloidal particles. Examples include colloidal epitaxy and space-based reduced-gravity techniques, as well as the use of temperature gradients to define a density gradient. This is somewhat counterintuitive as temperature does not play a role in determining the hard-sphere phase diagram. However, hard-sphere single crystals (size 3 mm) have been obtained from a sample in a concentration regime that would remain in the liquid state in the absence of a temperature gradient.\n\nUsing a single colloidal crystal, phonon dispersion of the normal modes of vibration modes were investigated using photon correlation spectroscopy, or dynamic light scattering. This technique relies on the relaxation or decay of concentration (or density) fluctuations. These are often associated with longitudinal modes in the acoustic range. A distinctive increase in the sound wave velocity (and thus the elastic modulus) by a factor of 2.5 has been observed at the structural transition from colloidal liquid to colloidal solid, or point of ordering.\n\nUsing a single body-centered cubic colloidal crystal, the occurrence of Kossel lines in diffraction patterns were used to monitor the initial nucleation and subsequent motion caused distortion of the crystal. Continuous or homogeneous deformations occurring beyond the elastic limit produce a 'flowing crystal', where the nucleation site density increases significantly with increasing particle concentration. Lattice dynamics have been investigated for longitudinal as well as transverse modes. The same technique was used to evaluate the crystallization process near the edge of a glass tube. The former might be considered analogous to a homogeneous nucleation event—whereas the latter would clearly be considered a heterogeneous nucleation event, being catalyzed by the surface of the glass tube.\n\nSmall-angle laser light scattering has provided information about spatial density fluctuations or the shape of growing crystal grains. In addition, confocal laser scanning microscopy has been used to observe crystal growth near a glass surface. Electro-optic shear waves have been induced by an ac pulse, and monitored by reflection spectroscopy as well as light scattering. Kinetics of colloidal crystallization have been measured quantitatively, with nucleation rates being depending on the suspension concentration. Similarly, crystal growth rates have been shown to decrease linearly with increasing reciprocal concentration.\n\nExperiments performed in microgravity on the Space Shuttle Columbia suggest that the typical face-centered cubic structure may be induced by gravitational stresses. Crystals tend to exhibit the hcp structure alone (random stacking of hexagonally close-packed crystal planes), in contrast with a mixture of (rhcp) and face-centred cubic packing when allowed sufficient time to reach mechanical equilibrium under gravitational forces on Earth. Glassy (disordered or amorphous) colloidal samples have become fully crystallized in microgravity in less than two weeks.\n\nTwo-dimensional (thin film) semi-ordered lattices have been studied using an optical microscope, as well as those collected at electrode surfaces. Digital video microscopy has revealed the existence of an equilibrium hexatic phase as well as a strongly first-order liquid-to-hexatic and hexatic-to-solid phase transition. These observations are in agreement with the explanation that melting might proceed via the unbinding of pairs of lattice dislocations.\n\nLong-range order has been observed in thin films of colloidal liquids under oil—with the faceted edge of an emerging single crystal in alignment with the diffuse streaking pattern in the liquid phase. Structural defects have been directly observed in the ordered solid phase as well as at the interface of the solid and liquid phases. Mobile lattice defects have been observed via Bragg reflections, due to the modulation of the light waves in the strain field of the defect and its stored elastic strain energy.\n\nAll of the experiments have led to at least one common conclusion: colloidal crystals may indeed mimic their atomic counterparts on appropriate scales of length (spatial) and time (temporal). Defects have been reported to flash by in the blink of an eye in thin films of colloidal crystals under oil using a simple optical microscope. But quantitatively measuring the rate of its propagation provides an entirely different challenge, which has been measured at somewhere near the speed of sound.\n\nCrystalline thin-films from non-spherical colloids were produced using convective assembly techniques. Colloid shapes included dumbbell, hemisphere, disc, and sphero-cylinder shapes. Both purely crystalline and plastic crystal phases could be produced, depending on the aspect ratio of the colloidal particle. The particles were crystallized both as 2D (i.e., monolayer) and 3D (i.e., multilayer) structures. The observed lattice and particle orientations experimentally confirmed a body of theoretical work on the condensed phases of non-spherical objects. Assembly of crystals from non-spherical colloids can also be directed via the use of electrical fields.\n\nTechnologically, colloidal crystals have found application in the world of optics as photonic band gap (PBG) materials (or photonic crystals). Synthetic opals as well as inverse opal configurations are being formed either by natural sedimentation or applied forces, both achieving similar results: long-range ordered structures which provide a natural diffraction grating for lightwaves of wavelength comparable to the particle size.\n\nNovel PBG materials are being formed from opal-semiconductor-polymer composites, typically utilizing the ordered lattice to create an ordered array of holes (or pores) which is left behind after removal or decomposition of the original particles. Residual hollow honeycomb structures provide a relative index of refraction (ratio of matrix to air) sufficient for selective filters. Variable index liquids or liquid crystals injected into the network alter the ratio and band gap.\n\nSuch frequency-sensitive devices may be ideal for optical switching and frequency selective filters in the ultraviolet, visible, or infrared portions of the spectrum, as well as higher efficiency antennae at microwave and millimeter wave frequencies.\n\nSelf-assembly is the most common term in use in the modern scientific community to describe the spontaneous aggregation of particles (atoms, molecules, colloids, micelles, etc.) without the influence of any external forces. Large groups of such particles are known to assemble themselves into thermodynamically stable, structurally well-defined arrays, quite reminiscent of one of the 7 crystal systems found in metallurgy and mineralogy (e.g. face-centered cubic, body-centered cubic, etc.). The fundamental difference in equilibrium structure is in the spatial scale of the unit cell (or lattice parameter) in each particular case.\n\nMolecular self-assembly is found widely in biological systems and provides the basis of a wide variety of complex biological structures. This includes an emerging class of mechanically superior biomaterials based on microstructural features and designs found in nature. Thus, self-assembly is also emerging as a new strategy in chemical synthesis and nanotechnology. Molecular crystals, liquid crystals, colloids, micelles, emulsions, phase-separated polymers, thin films and self-assembled monolayers all represent examples of the types of highly ordered structures which are obtained using these techniques. The distinguishing feature of these methods is self-organization.\n\n\n"}
{"id": "12645027", "url": "https://en.wikipedia.org/wiki?curid=12645027", "title": "Eco-running", "text": "Eco-running\n\nEco-running is the variation of recreational running in which the participant collects the litter that is found along the path travelled. The founder of Eco-Running is Samuel Huber of Milwaukee, Wisconsin in the United States.\n"}
{"id": "3801990", "url": "https://en.wikipedia.org/wiki?curid=3801990", "title": "Electric Vehicle Company", "text": "Electric Vehicle Company\n\nElectric Vehicle Company was an American automobile holding company and early pioneering manufacturer of automobiles.\n\nThe Electric Vehicle Company was founded 27 September 1897 as a holding company of battery-powered electric vehicle manufacturers made up of several companies assembled by Isaac Rice. Rice had acquired in May, 1897 another electric cab manufacturer, the Electric Carriage & Wagon Company (E.C.W.C.) in New York. Their vehicles were constructed by \"Henry G. Morris\" and \"Pedro G. Salom\", builders of the Electrobats, the first truly useful electric automobiles in the USA. E.V.W.C. pioneered a cab system that included service stations for quick change of battery sets, and repair work; vehicles were leased only, not sold. Twelve of these cabs were in use in Manhattan in January, 1897. After the merger, E.C.V. concentrated on building heavy but reliable electric cabs which were built in the E.C.W.C. workshops. The rental system was for a short time run by E.V.C. Between 1897 and 1899, there were several hundred E.V.C. vehicles built.\n\nThe company was taken over in 1899 by a syndicate around William C. Whitney, Thomas Fortune Ryan Anthony N. Brady, and P. A. B. Widener, thus forming the so-called \"Lead Cab Trust,\" which hoped to develop a monopoly by placing electric cabs on the streets of major American cities, starting with New York City, Philadelphia, Chicago, Washington, D.C., and Boston. Although by 1899, E.V.C. was the largest motor car manufacturer in the USA - a position lost to Oldsmobile in 1901 - this policy failed soon, as it was even then not able to sell as many vehicles as necessary for the task. The firm actually made and sold about two thousand electric cars, but fell into hard times in 1900 after facing competition from gas-powered cars and legal problems stemming from these monopolistic practices, as well as scandal surrounding the poor performance of its vehicles. Whitney brought in industrial leader Albert Augustus Pope now, who brought the Columbia Automobile Company. The trust was reorganized as the parent company of several vehicle manufacturers, among them Columbia and the Riker Electric Vehicle Company, which was acquired in 1902. Electric Vehicle's chief asset was now the holding of the Selden Patent which established a right to royalties from all manufacturers of internal combustion engine vehicles. While this was initially lucrative, it led inevitably to opposition from the other manufacturers and expensive lawsuits, which ended with bankruptcy in 1907. The patent was valid until 1913, but lost its worth as the appellation court reduced it to vehicles with Brayton engines, of which none was used in a motor vehicle.\n\n\n\n"}
{"id": "19467781", "url": "https://en.wikipedia.org/wiki?curid=19467781", "title": "Eolgen Racovițeni Wind Farm", "text": "Eolgen Racovițeni Wind Farm\n\nThe Eolgen Racovițeni Wind Farm is an under construction wind power project in Buzău County, Romania. It will have 45 individual wind turbines with a nominal output of around 1 MW which will deliver up to 45 MW of power, enough to power over 17,820 homes, with a capital investment required of approximately US$105 million.\n"}
{"id": "23743529", "url": "https://en.wikipedia.org/wiki?curid=23743529", "title": "Esso Refinery, Milford Haven", "text": "Esso Refinery, Milford Haven\n\nThe Esso Refinery at Milford Haven was an oil refinery situated on the Pembrokeshire coast in Wales. Construction started in 1957 and the refinery was opened in 1960 by the Duke of Edinburgh. Construction cost £18million and the refinery had the capacity to process 4.5million tons of crude oil a year.\n\nThe refinery closed down in March 1983. Today, the site has been converted by the owners Exxonmobil into the South Hook LNG terminal.\n\n"}
{"id": "2224335", "url": "https://en.wikipedia.org/wiki?curid=2224335", "title": "Gin Drinkers Bay", "text": "Gin Drinkers Bay\n\nGin Drinkers Bay () or Gin Drinker's Bay, also known as Lap Sap Wan (), was a bay in Kwai Chung, Hong Kong.\n\nThe bay was reclaimed in the 1960s and became Kwai Fong and part of Kwai Hing. At the mouth of the bay stood the island of Pillar Island.\n\nThe bay was a harbour for Tanka fishing junks. They relocated to Tsing Yi Tong and Mun Tsai Tong of Tsing Yi Island before the commencement of reclamation.\n\n\"Lap Sap\" (垃圾) means \"rubbish\" in Cantonese. It is unclear why the bay was named \"rubbish\" in the past. However, it was coincidentally once a dumping area for rubbish after extensive reclamation. It is assumed that in Gin Drinkers Bay Park or Kwai Chung Park near Pillar Island that the area is subject to landfill gas produced deep in the ground even though it is covered with earthen hills. It remains closed due to unsafe levels of landfill gas.\n\nGin Drinkers Bay is known for the Gin Drinkers Line, which formed a defensive line against the Japanese invasion in 1941.\n\n"}
{"id": "26183087", "url": "https://en.wikipedia.org/wiki?curid=26183087", "title": "Hanbit Nuclear Power Plant", "text": "Hanbit Nuclear Power Plant\n\nThe Hanbit Nuclear Power Plant is a large nuclear power station in the Jeollanam-do province of South Korea. The facility runs at an installed capacity of . The power station is currently ranked as the fifth largest nuclear power station in the world.\nThe plant's name was changed from Yeonggwang NPP to Hanbit in 2013 at the request of local fishermen.\n\nAll the units at Hanbit are of the Pressurized Water Reactor (PWR) reactor type. Unit-1 and Unit-2 are 3-loop Westinghouse-designed plants; major components were sourced from foreign firms while auxiliary components and site construction were handled domestically. Unit-3 and Unit-4 are 2-loop Combustion Engineering (C-E) System 80 plants with major components and construction handled domestically under a technology transfer agreement. Unit-5 and Unit-6 are based on the Ulchin (now Hanul) Unit-3 OPR-1000 Korean Standard Nuclear Power Plant design.\n\nIn November 2012 security checkups prompted by the Fukushima Daiichi nuclear disaster revealed that from 2003 eight suppliers had forged quality certificates for a delivered 7,682 items to the plant. Of the plant's six reactors two were affected by more than 5,000 of those parts and were consequently shut down, for an expected eight weeks. According to Yonhap news agency the incident was likely to seriously undermine the confidence in South Korean nuclear reactors and could thus impede the country's export of nuclear power plants. Knowledge Economy Minister Hong Suk-woo responded that the \"government plans to further increase its efforts to export nuclear reactors. In this regard, the government will quickly provide all necessary and accurate facts to prospective foreign buyers to make sure there is not a single shred of doubt left over the safety of the country's nuclear reactors\".\n\n"}
{"id": "45566670", "url": "https://en.wikipedia.org/wiki?curid=45566670", "title": "Hybrid Illinois Device for Research and Applications", "text": "Hybrid Illinois Device for Research and Applications\n\nThe Hybrid Illinois Device for Research and Applications (HIDRA) is a medium-sized toroidal magnetic fusion device currently being assembled within the Center for Plasma Material Interactions in the Department of Nuclear, Plasma and Radiological Engineering at the University of Illinois at Urbana-Champaign, United States. It is anticipated that HIDRA will have first plasma by mid-September 2015 and start full experimental campaigns by December of that year. HIDRA is the former WEGA classical stellarator that was operated at the Max-Planck Institut für Plasmaphsyik in Greifswald Germany from 2001 to 2013.\n\nA unique aspect of HIDRA is that it can not only operate as a stellarator but also as a tokamak, hence the hybrid designation. In fact it should be possible to operate the two modes simultaneously. It is planned to operate up to 30 minutes of continuous plasma, with up to 60 minutes in the future and will concentrate on understanding the complex relationship between the plasma and materials inside the vacuum vessel of a fusion device\n\nHIDRA is probably the most well traveled fusion devices in the world. From its beginnings in France it has operated in 3 countries and 4 cities. The research goals of the device have dramatically changed over the years from doing wave heating studies, to being a testbed for one of the worlds most sophisticated fusion devices and now to studying the way plasmas interact with the inside wall and materials of fusion devices. In fact it will be the first toroidal fusion-relevant device that will be dedicated to the study of plasma wall (PWI) and plasma material interactions (PMI).\n\nHIDRA, in fact, started off as a different machine at the Centre d’Etudes Nucléaires in Grenoble, France in 1972. Back then it was called WEGA with construction of the device from 1972 to 1975. WEGA was a joint project between CEA Grenoble and the Max-Planck-Institut für Plasmaphysik in Germany to study RF heating and lower hybrid heating. There were three vacuum vessel that were built, two tokamak and stellarator. WEGA primarily operated as a tokamak from 1975 to 1982 despite plans to install the stellarator vessel in 1976 (repeairs were needed on the helical coil insulation). Electron and ion temperatures achieved were, \"T\" = 600 – 900 eV and, \"T\" = 150 – 250 eV. Densities of \"n\" = 1.6×10 m with a plasma current of \"I\" = 45 – 60 kA and heating power, \"P\" = 100 – 130 kW and, \"P\" = 100 kW. Typical pulse duration was, \"Δt\" = 5 – 15 ms and an energy confinement time, \"τ\" = 3 – 5 ms.\n\nIn 1982 WEGA moved from Grenoble to the University of Stuttgart in Germany. Unfortunately in Stuttgart it seems that WEGA was not used much. There is not a lot of information from this time and its is very difficult to see what results came out of this 18-year time period. The issues seem to be a lack of enough heating power and cooling. However, IPF did have the stellarator vacuum vessel installed and some of the first magnetic flux surface measurements were performed.\n\nFrom 2000 to 2001 WEGA moved from Stuttgart to Greifswald. At this stage a new institute had been built in the former East German city to stimulate economic, scientific and educational growth in the region. MPIPP Greifswald was designated to house the brand new modular stellarator W7-X. While W7-X was being built WEGA was brought in to be the machine where much of the diagnostic, heating and control work for W7-X would be tested and perfected. It also provided a valuable tool in the training and teaching of future generations of fusion and plasma scientists and engineers. In fact, though the name was the same the acronym for WEGA changed to \"Wendelstein Experiment in Greifswald für Ausbildung\" the Wendelstein experiment in Greifswald for Training.\n\nWEGA's magnetic coil systems were run via transformer and rectifier sets that allowed steady state operations. This made WEGA unique among the smaller toroidal fusion devices that normally are pulsed, and only the larger devices such as LHD and W7-X have that kind of steady state capability. Some of the achievements on WEGA include the development of a OXB heating scheme that allows ECRH heating above the cut off density for the electrons in the plasma. This allowed densities almost 100 times higher to be achieved. The W7-X contol system was tested on WEGA showing the real time capability of measuring plasma parameters and control of the machine. It was dmeonstrated, that despite being a stellarator, a plasma current could be driven via microwave heating of the plasma.\n\nIn 2013 the last experiments were performed and WEGA was slowly decommissioned as operations for W7-X started to ramp up. WEGA was either going to be scrapped or if a suitable research group was found would be donated.\n\nWithe W7-X operations starting in 2014, the space occupied by WEGA and its power systems was taking up much needed real-estate for the Thompson Scattering system and a cryogenic pellet injection system. During the 2013 SOFE conference, Daniel Andruczyk, a former Post-Doc on WEGA, met with some former colleagues and it was suggested that the University of Illinois could potentially take WEGA to the USA. After a year of negotiating between the director of the Center for Plasma Material Interactions, David Ruzic, the University and IPP, the funds were secured to make it happen. Andruczyk was brought on to lead the project of dismantling and shipping WEGA to the Urbana-Champaign Campus in Illinois. Upon arrival the Machine was re-named the Hybrid Illinois Device for Research and Applications. The Hybrid part being that it still has the tokamak capabilities, not only the stellerator ones.\nIt took nearly 8 weeks over the autumn of 2014 of dismantling the device in Germany and was packed and shipped by Rhenus to the US in October. By early 4 November shipping containers worth of fusion device arrived by flat bed-truck to the University of Illinois at the Center for Plasma Material Interactions. Aside from lab personnel, the universities Facilities and services division were heavily involved in helping to off load, move in and assemble much of the larger, heavier components. Construction began in earnest with the transformers and rectifiers for the helical and toroidal magnetic coils moving in first. Once these were established then the base was brought in and the yokes, center-stack, half-tori and vertical field coils all installed over the next ten days.\n\nFrom there it took another 18 months of getting everything else assembled and hooked up. The 20 kV transformer needed for running the coils was made by Cooper electronics and the switching mechanism that to switch it all on was supplied by G&W. Quad-Plus was brought in to commission the rectifiers which allows the field shaping of the device. In this time water cooling tanks were brought in and installed and the universities pipe fitters built the cooling system for the machine and installed the water pump. The control system was written by senior undergrad and grad students and allows the machine to be controlled from one computer via a LabView program. This also sends out the trigger signals that are needed to fire the various data acquisition and diagnostic systems.\n\nIn April 2016, HIDRA had its first plasma, a simple glow plasma discharge, during the Nuclear, Plasma and Radiological Engineering Departments Open house. With nearly 100 dignitaries and guests including the head of department and Dean of Engineering the HIDRA team was able to demonstrate that all the working components of the device we installed and running and that operations were able to begin.\n\nThe first set of data was the characterization of the magnetic flux lines on the machine. This used an in-house electron beam and fluorescing rod method, which was originally developed on WEGA for W7-X, to see what the magnetic field shape, and thus plasma shape, would look like. The beam would be scanned across the minor radius of the vacuum vessel and the rod swept through the vessel. Anywhere an electron beam hit the rod it would light up. a sensitive astronomical camera was used to image the flux surfaces and compare them to a ray-tracing code to see the validity of the plasma and any error fields present. Some gas could also be bled in to visually see the electron beams themselves.\n\nMagnetron Plasmas\n"}
{"id": "4362479", "url": "https://en.wikipedia.org/wiki?curid=4362479", "title": "Hydrodeoxygenation", "text": "Hydrodeoxygenation\n\nHydrodeoxygenation (HDO) is a hydrogenolysis process for removing oxygen from oxygen containing compounds. Typical HDO catalysts commonly are sulfided nickel-molybdenum or cobalt-molybdenum on gamma alumina. An idealized reaction is:\nThe first review on HDO was published in 1983. HDO is of interest in producing biofuels, which are derived from oxygen-rich precursors like sugars or lipids. An example of a biomass refining process employing hydrodeoxygenation is the NEXBTL process.\n"}
{"id": "56656634", "url": "https://en.wikipedia.org/wiki?curid=56656634", "title": "IPIECA", "text": "IPIECA\n\nIPIECA, set up in 1974, originally the \"International Petroleum Industry Environmental Conservation Association\" is a global not-for-profit oil and gas industry association for environmental and social issues, headquartered in London. The full title was abandoned in 2002, when IPIECA recognized that it no longer accurately reflected the scope of the association’s work. IPIECA's mission is to assist \"the oil and gas industry improve its environmental and social performance\". \nCompany members contribute to IPIECAs budget according to an individually agreed percentage based on the volume of crude oil produced and petroleum products sold by each company and the number of geographical areas where the company has interests.\n\nIPIECA is the industry channel into UN's International Panel on Climate Change (IPCC) and the UNFCC, both concerned with climate change. IPIECA promotes an image of an environmental and socially responsible industry, while its members, including BP, Chevron, ConocoPhillips, ExxonMobil, Repsol and Shell, are among the biggest emitters of greenhouse gas.\n\nIts geographical coverage encompasses North America, Asia and the Pacific, Latin America and The Caribbean, Africa, Western Europe, Eastern Europe and the global market.\n\n\n"}
{"id": "42400407", "url": "https://en.wikipedia.org/wiki?curid=42400407", "title": "Jeroen C. J. M. van den Bergh", "text": "Jeroen C. J. M. van den Bergh\n\nJeroen Cornelis Johannes Maria van den Bergh (born August 1, 1965) is an environmental economist of Dutch origin. As of January 2015 he was ICREA Research Professor at Universitat Autònoma de Barcelona and Deputy Director for Research of its Institute of Environmental Science and Technology, and professor of Environmental and Resource Economics at VU University Amsterdam.\n\nVan den Bergh earned a master's degree in Econometrics and Operations Research from Tilburg University in 1988 and a doctorate in economics from VU University Amsterdam (VUA) in 1991. In July 1997 he was appointed professor of Environmental Economics in the Faculty of Economics and Business Administration at VUA and in 2002 he became a professor in the Institute for Environmental Studies of VUA. In 2002 he was awarded the annual Royal Dutch/Shell Prize (Koninklijke/Shell Prijs) with a purse of 100,000 euros by the Royal Netherlands Academy of Arts and Sciences, for his research related to sustainable development and energy. In September 2007 he was appointed ICREA Research Professor at Universitat Autònoma de Barcelona (UAB), and honorary professor of Environmental and Resource Economics at VUA. In 2010 he became the editor-in-chief of the Elsevier journal \"Environmental Innovation and Societal Transitions\" From 2007 to 2009 he was a member of the Energy Council of The Netherlands. In 2004 he was appointed a member of the scientific advisory board of the Austrian Institute of Economic Research.\n\nAccording to his ICREA profile page as of March 2015, his research is on \"the intersection of economics, environmental science and innovation studies. Past work includes dematerialization and recycling, ecological-economic modelling, construction of aggregate economic and environmental performance indicators, the growth-versus-environment debate, and spatial/international aspects of environmental policy. Work in recent years involves evolutionary economics, environmental innovation, and economic analysis of climate policy and a transition to a low-carbon economy.\"\n\nAs of March 2015 he had published about 150 papers and (co-)authored or edited 16 books.\n\nAccording to RePEc, as of February 2015 van den Bergh was ranked #16 among the 1,904 economists of Spain. As of March 2015 he was a member of the Academia Europaea.\n\n\nAs of March 2015 the five most cited articles in Google Scholar by Van den Bergh were:\n"}
{"id": "1249489", "url": "https://en.wikipedia.org/wiki?curid=1249489", "title": "Kip (unit)", "text": "Kip (unit)\n\nA kip is a US customary unit of force. It equals 1000 pounds-force, used primarily by American architects and engineers to measure engineering loads. Although uncommon, it is occasionally also considered a unit of weight, equal to 1000 pounds, i.e., one half of a short ton. One use is as a unit of deadweight to compute shipping charges. \n\nThe name comes from combining the words \"kilo\" and \"pound\"; it is occasionally called a \"kilopound\". Its symbol is kip, or less frequently, klb. When it is necessary to clearly distinguish it as a unit of force rather than mass, it is sometimes called the \"kip-force\" (symbol kipf or klbf).\nNote that the symbol kp usually stands for a different unit of force, the \"kilopond\" or kilogram-force used primarily in Europe prior to the introduction of SI units.\n\nThe kip is also the name of another unit of measure formerly used in Malaysia equal to approximately 9.19 kilograms, which is now obsolete.\n\n"}
{"id": "10655554", "url": "https://en.wikipedia.org/wiki?curid=10655554", "title": "Lacewood", "text": "Lacewood\n\nLacewood is a common name for the wood produced from a number of different trees, with mostly a striking appearance of their „lace-wood“, which gets its name from the lacelike pattern: These include:\n\n"}
{"id": "33759152", "url": "https://en.wikipedia.org/wiki?curid=33759152", "title": "Lesquerolic acid", "text": "Lesquerolic acid\n\nLesquerolic acid is a hydroxy acid that occurs naturally in \"Paysonia lasiocarpa\" and other \"Paysonia\" and \"Physaria\" species. It was first isolated in 1961 by a team from the Northern Regional Research Laboratory. This compound has the at the alcohol-bearing stereocenter, and it is of the \"Z\" configuration at the olefin. Lesquerolic acid is chemically similar to ricinoleic acid, but with two additional carbons at the carboxyl end of the carbon chain. Lesquerolic acid, with other hydroxy fatty acids, has important industrial uses, including making resins, waxes, nylons and plastics.\n"}
{"id": "53821192", "url": "https://en.wikipedia.org/wiki?curid=53821192", "title": "Lower Wonga Solar Farm", "text": "Lower Wonga Solar Farm\n\nThe Lower Wonga Solar Farm is a proposed photovoltaic power station, located in the rural locality of Lower Wonga, Gympie Region, Queensland, Australia. If the station is completed to its final configuration of 3 million solar panels capable of powering about 315,000 homes, it would become Australia's largest solar power station.\n\nThe Lower Wonga Solar Farm would be located on previously cleared lands used for grazing, on the corner of the Wide Bay Highway and Gympie Woolooga Road, Lower Wonga, Queensland. The farm site is also located next to the Woolooga Substation and transmission lines, providing a cheap and easy point to distribute power into the grid.\n"}
{"id": "1988220", "url": "https://en.wikipedia.org/wiki?curid=1988220", "title": "Lucens reactor", "text": "Lucens reactor\n\nThe Lucens reactor was a 6 MW experimental nuclear power reactor built next to Lucens, Vaud, Switzerland. After its connection to the electrical grid on 29 January 1968, the reactor only operated for a few months before it suffered a loss-of-coolant accident on 21 January 1969, leading to the meltdown of one of the five fuel elements and radioactive contamination of the cavern.\n\nIn 1962 the construction of a Swiss-designed pilot nuclear power plant began. The heavy-water moderated, carbon dioxide gas-cooled reactor was built in an underground cavern. It produced 28 MW of heat, which was used to generate 6 MW of electricity, and it became critical 29 December 1966. It was fueled by 0.96% enriched uranium alloyed with chromium cased in magnesium alloy (magnesium with 0.6% zirconium) inserted into a graphite matrix. Carbon dioxide gas was pumped into the top of the channels at 6.28 MPa and 223 °C and exited the channels at a pressure of 5.79 MPa and at a temperature of 378 °C.\n\nIt was intended to operate until the end of 1969, but during a startup on 21 January 1969, it suffered a loss-of-coolant accident, leading to a partial core meltdown and the radioactive contamination of the cavern, which was then sealed. Using the criteria of the International Nuclear Event Scale, introduced in 1990 by the International Atomic Energy Agency, the event has been assessed as a Level 4 \"Accident with local consequences\".\n\nThe accident was caused by water condensation forming on some of the magnesium alloy fuel element components during shutdown and corroding them. The corrosion products from this accumulated in some of the fuel channels. One of the vertical fuel channels was sufficiently blocked by it to impede the flow of carbon dioxide coolant so that the magnesium alloy cladding melted and further blocked the channel. The increase in temperature and exposure of the uranium metal fuel to the coolant eventually caused the fuel to catch fire in the carbon dioxide coolant atmosphere. The pressure tube surrounding the fuel channel split because of overheating and bowing of the burning fuel assembly, and the carbon dioxide coolant leaked out of the reactor.\n\nNo irradiation of workers or the population occurred, though the cavern containing the reactor was seriously contaminated. The cavern was decontaminated and the reactor dismantled over the next few years. The plant was totally decommissioned in 1988 and the last radioactive waste was removed in 2003.\n\n\n"}
{"id": "7204236", "url": "https://en.wikipedia.org/wiki?curid=7204236", "title": "Macedonian Ecological Society", "text": "Macedonian Ecological Society\n\nThe Macedonian Ecological Society (MES) was founded in 1972 in what was then known as the Socialist Republic of Macedonia.\n\n\n\n"}
{"id": "14174594", "url": "https://en.wikipedia.org/wiki?curid=14174594", "title": "Manifold (fluid mechanics)", "text": "Manifold (fluid mechanics)\n\nA manifold is a wide and/or bigger pipe, or channel, into which smaller pipes or channels lead.\n\nTypes of manifolds in engineering include:\n\nIn biology manifolds are found in:\n\nManifolds are used in:\n"}
{"id": "1787483", "url": "https://en.wikipedia.org/wiki?curid=1787483", "title": "Mina (unit)", "text": "Mina (unit)\n\nThe mina (also mĕnē, Aramaic; ) is an ancient Near Eastern unit of weight, which was divided into 50 shekels. The mina, like the shekel, was also a unit of currency. In ancient Greece, it originally equalled 70 drachmae and later was increased to 100 drachmae. The Greek word \"mna\" () was borrowed from Semitic; compare Hebrew \"māneh\", Aramaic \"mĕnē\", Syriac \"manyā\", Ugaritic \"mn\", and Akkadian \"manū\".\nHowever, before it was used as currency, a mina was a unit of measurement, equal to .\n\nFrom earliest Sumerian times, a mina was a unit of weight. At first, talents and shekels had not yet been introduced. By the time of Ur-Nammu, the mina had a value of 1/60 talents as well as 60 shekels. The value of the mina is calculated at . \n\nEvidence from Ugarit indicates that a mina was equivalent to fifty shekels. The prophet Ezekiel refers to a mina ('maneh' in the King James Version) as sixty shekels, in the Book of Ezekiel 45:12. Jesus of Nazareth tells the \"parable of the minas\" in Luke 19:11-27.\n\nFrom the Akkadian period, 2 mina was equal to 1 \"sila\" of water (cf. clepsydra, water clock).\n\n"}
{"id": "2073624", "url": "https://en.wikipedia.org/wiki?curid=2073624", "title": "Monolithic dome", "text": "Monolithic dome\n\nA monolithic dome (from Greek mono- and -lithic, meaning \"one stone\") is a structure cast in a one-piece form. The form may be permanent or temporary and may or may not remain part of the finished structure. Monolithic domes are a form of monolithic architecture.\n\nThe igloo may be the earliest form of monolithic dome. While it is constructed of blocks of compressed snow, these blocks melt and re-freeze to form a strong, homogeneous structure. The dome-like shape of the igloo exhibits the two major advantages of a dome-shaped structure: great strength, and good insulation. The strength is due to the natural strength of the arch, and the insulation is due to the minimal surface area of a spherical section.\n\nThe first modern monolithic dome structure was built in Provo, Utah and opened in 1963 as an ice skating rink. Called \"Turtle Reams\" after its 1967 conversion into a general store by new owner Paul Ream, the building stood until it was demolished in 2006 for new construction.\n\nTurtle Reams was built by first creating a mound of dirt in the desired shape of the shell, an ellipsoidal section long, wide and high. The mound was then covered in a grid of rebar, to provide strength, and a layer of concrete approximately thick. After the concrete was cured, the dirt was excavated through the doorways, leaving the roof standing in its place. The floor was then poured to finish the structure.\n\nToday, monolithic domes are used in a variety of residential, commercial and industrial projects. Because of the strength, durability and economics, they are used to store large amounts of various commodities in the cement, fertilizer, agricultural, power and mining industries. Due to their structural integrity, they are used as the containment buildings at some nuclear power plants. Forms have been made using nearly every common structural material including air pressure supported fabric.\n\nModern construction differs significantly from the original concrete-over-dirt method. The current methods were developed by three brothers from Idaho: David, Barry, and Randy South. The first dome built using these method was constructed in April 1976 in Shelley, Idaho:\n\n\nIn instances where necessity requires economical construction for multiple small and basic dwellings, the dome can be built without insulation and the air form can be removed after completion and re-used to build additional domes.\n\nThe dome, when finished, is earthquake, tornado and hurricane resistant (the US Federal Emergency Management Agency rates them as \"near-absolute protection\" from F5 tornadoes and Category 5 Hurricanes). Recently, a number of monolithic domes constructed using MDI techniques have survived major disasters:\n\nThe demolition of Turtle Reams also demonstrated the durability of the monolithic dome structure. A wrecking ball demolished a strip several feet wide around the perimeter of the structure, without a collapse. When a doorway on one side was pulled down, the dome finally tipped over, and collapsed.\n\nThe monolithic dome, for a number of reasons, is very energy efficient. The spherical sections of the dome offer minimal surface area for the volume they contain, so there is less surface for heat transfer with the outside air. The one piece construction of the monolithic dome also eliminates many of the seams through which air can leak, though this is mitigated to some degree in residential domes by the addition of multiple doors and windows. By placing the insulating foam on the outside of the concrete shell, the concrete acts as a Thermal mass inside the building, reducing interior temperature fluctuations far more than the traditional home's insulation inside of a brick or stone veneer.\n\nWhile the monolithic dome has numerous demonstrated engineering advantages, there are also some disadvantages, both engineering and social.\n\n\nSocial disadvantages of monolithic domes are to a large degree shared by geodesic domes, due to the similar shape and unorthodox construction. These disadvantages are:\n\nThe largest monolithic dome in the world is the home of Faith Chapel Christian Center in Birmingham, AL, which is tall, and in diameter. Inside is a floor area of in two levels.\n\nSome communities in the United States have chosen to use monolithic dome technology in the construction of new schools. A map of recently built monolithic dome schools can be found here: \n\nA residential house, the monolithic \"Dome of a Home\" in Pensacola Beach, Florida, has experienced several hurricanes since it was built.\n\n\n"}
{"id": "21868", "url": "https://en.wikipedia.org/wiki?curid=21868", "title": "Neutronium", "text": "Neutronium\n\nNeutronium (sometimes shortened to neutrium, also referred to as neutrite) is a hypothetical substance composed purely of neutrons. The word was coined by scientist Andreas von Antropoff in 1926 (before the discovery of the neutron) for the conjectured \"element of atomic number zero\" that he placed at the head of the periodic table. However, the meaning of the term has changed over time, and from the last half of the 20th century onward it has been also used to refer to extremely dense substances resembling the neutron-degenerate matter theorized to exist in the cores of neutron stars; hereinafter \"\"degenerate\" neutronium\" will refer to this. Science fiction and popular literature frequently use the term \"neutronium\" to refer to a highly dense phase of matter composed primarily of neutrons.\n\nNeutronium is used in popular literature to refer to the material present in the cores of neutron stars (stars which are too massive to be supported by electron degeneracy pressure and which collapse into a denser phase of matter). This term is very rarely used in scientific literature, for three reasons: there are multiple definitions for the term \"neutronium\"; there is considerable uncertainty over the composition of the material in the cores of neutron stars (it could be neutron-degenerate matter, strange matter, quark matter, or a variant or combination of the above); the properties of neutron star material should depend on depth due to changing pressure (see below), and no sharp boundary between the crust (consisting primarily of atomic nuclei) and almost protonless inner layer is expected to exist.\n\nWhen neutron star core material is presumed to consist mostly of free neutrons, it is typically referred to as neutron-degenerate matter in scientific literature.\n\nThe term \"neutronium\" was coined in 1926 by Andreas von Antropoff for a conjectured form of matter made up of neutrons with no protons or electrons, which he placed as the chemical element of atomic number zero at the head of his new version of the periodic table. It was subsequently placed in the middle of several spiral representations of the periodic system for classifying the chemical elements, such as those of Charles Janet (1928), E. I. Emerson (1944), John D. Clark (1950) and in Philip Stewart's Chemical Galaxy (2005).\n\nAlthough the term is not used in the scientific literature either for a condensed form of matter, or as an element, there have been reports that, besides the free neutron, there may exist two bound forms of neutrons without protons. If neutronium were considered to be an element, then these neutron clusters could be considered to be the isotopes of that element. However, these reports have not been further substantiated.\nAlthough not called \"neutronium\", the National Nuclear Data Center's \"Nuclear Wallet Cards\" lists as its first \"isotope\" an \"element\" with the symbol n and atomic number \"Z\" = 0 and mass number \"A\" = 1. This isotope is described as decaying to element H with a half life of .\n\nDue to the beta (β) decay of mononeutron and extreme instability of aforementioned heavier \"isotopes\", neutron matter is not expected to be stable under ordinary pressures.\n\nFree neutrons decay with a half-life of 10 minutes and 11 seconds. While this lifetime is long enough to permit the study of neutronium's chemical properties, there are serious practical problems. Having no charge or electrons, neutronium would not interact with ordinary low-energy photons (light) and would feel no electrostatic forces, so it would diffuse into the walls of most containers made of ordinary matter. Certain materials are able to resist diffusion or absorption of ultracold neutrons due to nuclear-quantum effects, specifically reflection caused by the strong interaction. In the presence of other elements, low energy (thermal) neutrons readily undergo neutron capture to form heavier (and often radioactive) isotope of the element. Neutronium is left off most periodic tables.\n\nNeutron matter at standard pressure and temperature is predicted by the ideal gas law to be less dense than even hydrogen, with a density of only (roughly 27 times less dense than air). Similar to helium, neutron matter is predicted to remain gaseous down to absolute zero at normal pressures, as the zero-point energy of the system is too high to allow condensation. However, neutron matter should in theory, form a degenerate gaseous Bose–Einstein condensate at these temperatures, composed of neutron pairs called \"dineutrons\". At higher temperatures, neutron matter will condense with sufficient pressure, and solidify with even greater pressure. Such pressures exist in neutron stars, where the extreme pressure causes the neutron matter to become degenerate. However, in the presence of atomic matter compressed to the state of electron degeneracy, the β decay may be inhibited due to the Pauli exclusion principle, thus making free neutrons stable. Also, elevated pressures should make neutrons degenerate themselves. Compared to ordinary elements, neutronium should be more compressible due to the absence of electrically charged protons and electrons. This makes neutronium more energetically favorable than (positive-\"Z\") atomic nuclei and leads to their conversion to (degenerate) neutronium through electron capture, a process which is believed to occur in stellar cores in the final seconds of the lifetime of massive stars, where it is facilitated by cooling via emission. As a result, degenerate neutronium can have a density of , roughly 13 magnitudes denser than the densest known ordinary substances. It was theorized that extreme pressures of order may deform the neutrons into a cubic symmetry, allowing tighter packing of neutrons, or cause a strange matter formation.\n\nThe term \"neutronium\" has been popular in science fiction since at least the middle of the 20th century. It typically refers to an extremely dense, incredibly strong form of matter. While presumably inspired by the concept of neutron-degenerate matter in the cores of neutron stars, the material used in fiction bears at most only a superficial resemblance, usually depicted as an extremely strong solid under Earth-like conditions, or possessing exotic properties such as the ability to manipulate time and space. In contrast, all proposed forms of neutron star core material are fluids and are extremely unstable at pressures lower than that found in stellar cores. According to one analysis, a neutron star with a mass below about 0.2 solar masses would explode.\n\n"}
{"id": "3541812", "url": "https://en.wikipedia.org/wiki?curid=3541812", "title": "Nuclear Liabilities Fund", "text": "Nuclear Liabilities Fund\n\nThe Nuclear Liabilities Fund (formerly the Nuclear Generation Decommissioning Fund) is a fund of the UK Government to provide arrangements for funding certain long-term costs for the decommissioning of eight nuclear power stations formerly owned by British Energy, now EDF Energy. Responsibility for the Fund within government lies with the Shareholder Executive, on behalf of the Department of Energy and Climate Change.\n\nThe Fund is incorporated as a limited company registered in Scotland and is owned by the Nuclear Trust. It consists of five trustees, three appointed by the Secretary of State for Energy and Climate Change and two by the owners of the nuclear power stations, now EDF Energy. The trustees are also directors of the Fund as well as owning the ordinary share capital of the Fund.\n\nThe Nuclear Generation Decommissioning Fund was established on 28 March 1996 by the UK Government as part of the preparations for the privatisation of British Energy. It covered the nuclear power stations owned by the company on 20 March 1996, comprising seven advanced gas cooled reactor (AGR) stations and one pressurised water reactor (PWR) station. The obligations of the Fund were set out in an agreement known as the Nuclear Decommissioning Agreement. The Fund had an initial endowment of £232m and thereafter received £4m a quarter from British Energy, adjusted each year in line with RPI.\n\nOn 14 January 2005, following the restructuring of British Energy after it required financial assistance from the government, the original agreement was terminated. It was replaced by the Contribution Agreement (CA) and the Nuclear Liabilities Funding Agreement (NLFA). Broadly, these agreements resulted in the following:\n\n\nPart of the Fund’s interest in British Energy was realised on 31 May 2007, when it converted approximately 30% of its entitlement to British Energy’s free cashflow into 450m British Energy shares. These were immediately sold to investors at a price of 520p per share, raising £2.34 billion.\n\nOn 19 January 2009, the Fund sold its remaining 36% interest in British Energy following the takeover of the company by EDF Energy. The sale raised a further £4.421 billion, taking the total Fund value at that date to £8.3 billion. This is to be invested to fund the long term decommissioning costs of EDF's eight former British Energy nuclear stations, plus certain other contracted and uncontracted nuclear liabilities relating to the assets as they arise.\n\n\n"}
{"id": "6601335", "url": "https://en.wikipedia.org/wiki?curid=6601335", "title": "Partial specific volume", "text": "Partial specific volume\n\nThe partial specific volume formula_1 express the variation of the extensive volume of a mixture in respect to composition of the masses. It is the partial derivative of volume with respect to the mass of the component of interest.\n\nwhere formula_3 is the partial specific volume of a component formula_4 defined as:\n\nThe PSV is usually measured in milliLiters (mL) per gram (g).\nThe sum of partial specific volumes of a mixture or solution is an inverse of density of the mixture namely the specific volume of the mixture.\n\n"}
{"id": "19614243", "url": "https://en.wikipedia.org/wiki?curid=19614243", "title": "RCA Dimensia", "text": "RCA Dimensia\n\nDimensia was RCA's brand name for their high-end models of television systems and their components (Tuner, VCR, CD Player, etc.) produced from 1984 to 1989, with variations continuing into the early 1990s, superseded by the ProScan model line. After RCA was acquired by General Electric in 1986, GE sold the RCA consumer electronics line to Thomson SA which continued the Dimensia line. They are significant for their wide array of advanced features and for being the first television receiver systems to feature a built in computer, somewhat of an early incarnation of a smart TV, but without internet access (see Technological convergence). In 1985, RCA released the Digital Command Component System, a fully integrated audio system that permitted the full functionality of Dimensia audio components without a Dimensia monitor. The name \"Dimensia\" actually dates back to the early 1970s when RCA used the term for an enhanced spatial stereo effect which they called \"Dimensia IV\". The tagline for the Dimensia was \"The Next Dimension in Sight and Sound.\"\n\nThe RCA Dimensia systems had a wide array of high-end features that were novel for its time and are still not common anywhere.\n\nThe main unique feature of the Dimensia system was the MRT 003, a 32-kilobyte built-in computer module which allowed the monitor to communicate with all Dimensia components and the remote functions. All components were connected via the control bus found on the I/O panel on the back of the TV. The control bus was a unique RCA connector which was colored black. All Dimensia branded components had this control jack and they all interconnected by using RCA plugs that could piggy-back, resulting in a daisy chain which simplified wiring. This was known as the SystemLink, a communication system that had 16 kilobytes of computer memory.\n\nOne of the main features of the Dimensia's was the large input/output panel on the back. This included several RCA composite video terminals as well as multiple unbalanced and balanced RF antenna/cable inputs. This enabled easy connection of all Dimensia system components, each on their own channel. Additionally, the RCA cables were able to be connected piggy-back, resulting in daisy-chain wiring. The components were also connected to the control bus data link via the same piggy-back style RCA connectors. All Dimensia-Intelligent components could interact with the monitor's built-in computer. The first generation console Dimensia I/O panel shown to the left also has a SCART interface, an early multiport A/V interface which is popular in Europe (called EIA Multiport-Stereo Connector by RCA).\n\nThe Dimensia system came with a very large and advanced universal remote, called \"Dimensia Intelligent Audio Video\" or \"Dimensia Digital Control,\" variations of the Digital Command Center. The capabilities of this remote were far more advanced than many (perhaps any) other remotes at that time. For example, with other universal remotes you can control everything separately by controlling one component at a time; one command at a time. With the Dimensia remote, just pushing the VCR button will turn on the VCR, turn on the monitor and then play the tape in the VCR, assuming there is one in the VCR.\n\nThe Dimensia remote was not programmable like most universal remotes today. It was only fully compatible with Dimensia components. These components were referred to as \"Dimensia Intelligent,\" hence the name, \"Dimensia Intelligent Audio Video.\"\n\nDimensia televisions had many unique features that were state of the art at the time and some that are still rare today, including:\n\n\n\n\n\n\nThere are different models of the Dimensia, and there were two \"generations\" of the console and full Dimensia systems.\n\nThis variation of the Dimensia featured just the TV monitor mounted in a heavy wood grain veneer with a large input/output panel. It was on a swivel mount. It was not as commercial as the monitor for the full component-based Dimensia system; however, it was still considered high-end and had the control bus. In other words, the console Dimensia system was intended to be more stand-alone television set than the full Dimensia system. The second generation console version of the Dimensia had the speakers located on the sides rather than underneath the screen (e.g. Model GPR2750P). It had three coaxial cable/antenna posts for separate RF inputs and one output.\n\nAudio: Since the console Dimensia monitor was intended to stand alone (the full system had a 100 or 200 watt amplifier and 3-way tower speakers), but was still a high-end system, it featured a more complete built-in audio system than most monitors of the time. It had both woofers and tweeters in the TV cabinet, whereas almost all standard CRT television sets featured just one (mono) or two (stereo) low fidelity mid-range speaker drivers. It also had selectable external speaker connectors to be powered by the internal amp.\n\nThe commercial models featured a grounded power cord and BNC high-end commercial coaxial inputs. These models were around before the Dimensia system and were called the RCA Lyceum TV. These units were often used in educational facilities or other institutions and had the same chassis as the Colortrak 2000 and Dimensia tabletop model (the primary one). The commercial models had many extensive features such as automatic color balance and an automatic screen brightness adjuster which varied according to the ambient light in the room; at night or with the lights off it would lower its brightness.\n\nRetailed at over $5,000 USD upon its release in October 1984, it came with all the matching Dimensia-intelligent components, including the VCR, CED player (canceled just before release), amplifier, equalizer, speakers, tuner, cassette recorder, CD player and turntable. This was the most remarkable system, as all the components were compatible with the TV's computer and almost any operation could be executed with just the push of a button on the Digital Command Center.\n\nThe two TV sets that were the center of this system (FKC2600E and FKC2601T) were physically identical to the Colortrak 2000 chassis. These monitors were where the systems 32 kilobyte microprocessors were located. They sat in a dedicated entertainment center that suited all the components of this fully integrated system. The system was available in two color schemes; a woodgrain veneer and a black exterior from 1984 to 1986. These monitors featured BTSC system three-channel audio which had just been adopted by the Federal Communications Commission as the U.S. standard for stereo television transmission in 1984, the same year as the release of Dimensia.\n\nIn 1985, RCA released a 40-inch projection monitor for the system with the 32 kilobyte microprocessors. This was otherwise identical to the 26 inch displays that were initially released.\n\nThe initial 1984 Dimensia system came with one of several three-way stereo loudspeaker systems. The first one was the SPK375 which were made to go on optional speaker stands. They were rated at 60 watts RMS and 120 watts maximum and were in a 36-pound acoustic suspension enclosure. These were initially meant to be used with the 100 watt MSA-100 amplifier; then later in 1985 the 200 watt MSA-200 amplifier was released which was compatible with the MGE-160 graphic equalizer. The impedance of the SPK-375 was six ohms and their sensitivity was 91 dB/watt/meter. The frequency response of the speakers built into the TV chassis was 50-15000 Hertz, a standard range for mid-range audio components; the external speakers and amplifiers increased this range to 35-20000 Hertz, a high fidelity range.\n\nThe second-generation Dimensia audio components were also made for the Digital Command Component System.\n\nIn 1985, RCA released a fully integrated audio system known as the \"RCA Audio System\" that used the MSR-140 stereo receiver as its center for control over all the components. This allowed all Dimensia audio components to be controlled and fully functional without the need of a Dimensia television set. The price of this system started at $1,500 and included the Digital Command Center remote control. It also used the Colortrak 2000 monitor.\n\nThe SPK400 and SPK500, released in 1987, were second generation three-way speaker systems. Also released in 1987 for the second generation Dimensia system and for the Digital Command Component System were the MPA-100 and MPA-120 amplifiers; released to replace the MSA-100 and MSA-200, respectively.\n\nIn 1987, RCA released the MSP400 for the second-generation Dimensia audio system, an early Dolby Pro Logic surround sound decoder.\n\nVarious images of a 1987 console model GPR2740T\n\n\nGeneral:\n\n"}
{"id": "18362930", "url": "https://en.wikipedia.org/wiki?curid=18362930", "title": "SeaGen", "text": "SeaGen\n\nSeaGen was the world's first large scale commercial tidal stream generator. It was four times more powerful than any other tidal stream generator in the world at the time of installation.\n\nThe first SeaGen generator was installed in Strangford Narrows between Strangford and Portaferry in Northern Ireland, in April 2008 and was connected to the grid in July 2008. It generates 1.2 MW for between 18 and 20 hours a day while the tides are forced in and out of Strangford Lough through the Narrows. Strangford Lough was also the site of the very first known tide mill in the world, the Nendrum Monastery mill where remains dating from 787 have been excavated.\n\nMarine Current Turbines, the developer of SeaGen, demonstrated first prototype of tidal stream generator in 1994 with a 15 kilowatt system in Loch Linnhe, off the west coast of Scotland. In May 2003, the prototype for SeaGen, \", was installed off the coast of Lynmouth, North Devon, England. Seaflow was a single rotor turbine which generated 300 kW but was not connected to the grid. SeaFlow was the world's first offshore tidal generator, and remained the world's largest until SeaGen was installed.\n\nSeaGen generator weighs . each driving a generator through a gearbox like a hydro-electric or wind turbine. These turbines have a patented feature by which the rotor blades can be pitched through 180 degrees allowing them to operate in both flow directions – on ebb and flood tides. The company claims a capacity factor of 0.59 (average of the last 2000 hours). The power units of each system are mounted on arm-like extensions either side of a tubular steel monopile some in diameter and the arms with the power units can be raised above the surface for safe and easy maintenance access. The SeaGen was built at Belfast's Harland and Wolff's shipyards.\n\nSeaGen has been licensed to operate over a period of 5 years, during which there will be a comprehensive environmental monitoring programme to determine the precise impact on the marine environment.\n\nDuring the commissioning of the system a software error caused the blades of one of the turbines to be damaged. This left the turbine operating at half power until autumn 2008. The incident is being investigated and MCT is confident it will not happen again. Full power operation was finally achieved on 18 Dec 2008.\n\nThe System was removed in 2017 , after Siemens sold the company and technology to rival Altantis Resources in 2015.\n\n"}
{"id": "1467897", "url": "https://en.wikipedia.org/wiki?curid=1467897", "title": "Selenide", "text": "Selenide\n\nA selenide is a chemical compound containing a selenium anion with oxidation number of −2 (Se), much as sulfur does in a sulfide. The chemistry of the selenides and sulfides is similar. Similar to sulfide, in aqueous solution, the selenide ion, Se, is prevalent only in very basic conditions. In neutral conditions, hydrogen selenide ion, HSe, is most common. In acid conditions, hydrogen selenide, HSe, is formed.\n\nSome selenides are reactive to oxidation by air. Owing to the greater reducing power of selenide, metal selenides are more easily decomposed to the elements than are sulfides (tellurides are even more labile). Selenides of electropositive metals: such as aluminium selenide readily hydrolyse, even in moist air, evolving toxic hydrogen selenide gas.\n\nPure selenide minerals are rare, instead selenium tends to partially substitute for sulfide in many sulfide minerals. The degree of substitution is only of commercial interest for copper sulfide ores, in which case selenium is recovered as a by-product of copper refining. Some selenide minerals include ferroselite and umangite.\n\nPolyselenide anions are chains with the composition . Polyselenides also refer to salts of these anions. They are commonly synthesized by melting elements together in a quartz tube. Selenium and an alkali metal react to initially give white, sparingly soluble solids like monoselenides. Excess selenium leads to the formation of soluble diselenides and very soluble polyselenides with even greater amounts of selenium. Alternatively, they can be prepared by dissolving selenium and an alkali metal in a liquid ammonia. Synthesis can also be conducted in high-boiling, polar, aprotic solvents such as DMF, HMPA, and NMP. Aqueous polyselenides undergo salt metathesis with large organic counterions to form crystalline salts that are soluble in organic solvents.\n\nThe structures of polyselenides have been examined by X-ray crystallography. One characteristic feature of the structure is that two terminal Se–Se bonds are shorter than those bonds involving internal selenium atoms. High resolution solid state Se NMR spectroscopy of for [NMe]Se and [NMe]Se suggest similar confirmations of the anions [Se] and Se in the solid state and in solution. The spectra of [NMe]Se show five distinct selenium sites and the [NMe]Se spectra show symmetry with only 3 crystallographically different selenium sites. Single-crystal X-ray structure determination of the two salts support the NMR data.\n\nPolyselenides are prone to decomposition on exposure to air, in which case they are oxidized back to elemental selenium.\nPolyselenides form metal complexes. Se (\"x\" = 4, 5, 6) function as chelating ligands in complexes, e.g. (CH)TiSe, which is analogous to titanocene pentasulfide. Polyselenide anions reacts with organic halides:\n\nMetal selenide quantum dots and nanoparticles can be prepared by a variety of synthetic methods are available, many of which require high temperatures and hazardous precursor compounds.\nThe particles can be adapted for a variety of applications by varying the ligands coordinated to the positively-charged outer layer. Many ligand-exchange reactions are available for use, trading X, L, and Z-type ligands, the mechanism for which is still under study.\n\nQuantum dots based on metal selenides have been extensively for their distinctive spectral properties. Core-shell alloys of cadmium sulfide and selenide are of interest in imaging and phototherapy.\n\n\n"}
{"id": "27471653", "url": "https://en.wikipedia.org/wiki?curid=27471653", "title": "Slab hut", "text": "Slab hut\n\nA slab hut is a kind of dwelling or shed made from slabs of split or sawn timber. It was a common form of construction used by settlers in Australia and New Zealand during their nations' Colonial periods.\n\nFrom the very beginning of European settlement in Australia, improvised methods of building construction were in use. The First Fleet, arriving in 1788, brought with it few carpenters and a meagre supply of poor-quality tools. Nails and other ironmongery were scarce. The colonists were forced to build shelters using whatever skills they possessed, from whatever natural materials they could find. They tried the traditional British wattle and daub (or 'dab') method: posts were set in the ground; thin branches were woven and set between these posts, and clay or mud was plastered over the weave to make a solid wall. Wattle and daub walls were easily destroyed by the drenching rains of Australia's severe summer storms, and for a time, walls of timber slabs took their place. These were soon replaced by brick structures; the Sydney Cove landscape was almost denuded of useful timber. When settlement moved beyond Sydney Cove, an abundance of suitable forest timber became available. Huts and humpies made entirely from timber poles and large sheets of bark were easily erected, but these were often only temporary structures. Local timbers presented a fresh challenge to the European settler. Australian hardwoods were difficult to work, and tools were scarce or inadequate. Australia's colonists were forced to improvise again, and become their own craftsmen.\n\nIn time, buildings of timber slabs became a familiar feature of rural Australia. Some were public and long-lasting structures: shops, schools and churches; even substantial homesteads were built of slabs. Others were no more than hovels. As workmanship and tools improved, the slab structure became more permanent and sophisticated, eventually to become an icon of Colonial Australia, as evocative of time and place and humble beginnings as the thatched cottage of an English village or the log cabin of Early America.\n\nNew Zealand's European settlers also had to adapt to local circumstances, building with whatever materials were available, and employing tools of poor quality, or even none at all. Settlers tended to use the Maori word whare (house), instead of 'hut', for a temporary or pioneer dwelling.\n\n\"Ten pounds will go a long way towards putting up a sod hut; a cabin of outside slabs and refuse timber from the sawmills, or a serviceable tent with timber frame and sod chimney, sufficient to protect the inmates from the weather, and afford a temporary home at all events. There is, too, one great advantage [to] the immigrants hampering themselves at first with only slender households, for they may very soon find it to their interest to change their place of abode, in order to secure higher wages or engage in more congenial occupations...\"\n\nThe usual slab hut was built entirely from timber and bark. Australian settlers found that the most fissile timbers were the Eucalypts: blackbutt, bluegum, stringybark, ironbark and turpentine. Some of these species are also termite resistant. The chimney, too, was often made of wood, although sometimes sods were used. The fireplace may have been given a lining of stones, sometimes covered with a plaster of mud or clay.\n\nSettlers used a thatch of raupo, toitoi, flax, fern, or totara bark; they erected tents from poles, saplings, canvas, and planks or split slabs; and made tree-fern huts or more permanent dwellings from clay, sods, wattle and daub, or stone.\n\nA slab hut is actually a 'slab-walled' structure. Its walls were, strictly speaking, built from 'flitches'. Slabs are sawn from a trunk, flitches are split from it. Hut-builders felled selected trees, and sawed the trunks into suitable lengths. They then split these lengths into flitches using a maul and a wedge. Timber was split tangentially, that is, along the grain, instead of by the traditional British radial method, from the core of the trunk out towards the bark. There was neither time nor tools suitable to properly dress timber into planks, nor to season the timber; it was used green.\n\nRafters would be fixed atop the slab walls, and a pitched roof erected. The dimensions of the hut would be kept small, to avoid the need for roof trusses. Joists were not always laid, and a ceiling was not always included. A Queensland example can be seen here. If a ceiling was added, it was chiefly used for storage. Slab dwellings with a second storey were almost unknown.\n\nA bark roof was common, and was quickly and easily erected.\n\n'... the roof [was] covered with forest box or stringy-bark, which was stripped from the\nliving trees in sheets of about six feet long and from two to four feet\nwide, laid upon rafters composed of small sapling poles just as they came\nfrom being cut in the bush. The sheets of bark, having holes pierced\nthrough each in pairs, were then tied on the rafters with cords twisted\nof the inner rind of the kurrajong tree. The whole framing of the roof\nwas secured as it was needed by wooden pins in order to save the expense\nof nails, which were then both too scarce and too dear to be used by the\nlower order of settlers.\n\nIndeed, all kinds of ironwork were equally inaccessible, and instead of\nhinges to tie doors or window shutters, those appurtenances were all made\nto revolve on wooden pivots in holes, bored a short distance into the\ncorresponding parts of the frames.\n\nThatching was less common, but cumbungi (rushes), and blady grass were used if available. Later, when crops were grown, straw was used. For a more permanent dwelling shingles would be cut. The cabbage tree palm was found most suitable, and later the she-oak. In later years, galvanised iron became a popular roofing material, due to its cheapness and durability. Sometimes this was laid over the original shingles. Mrs Gunn noted that 'Great sheets of bark... were packed a foot deep above the rafters to break the heat reflected from the iron roof, while beneath it the calico ceiling was tacked up.'\nWhether or not a slab hut was lined, inside or out, depended on the economic means, the energy and skill, and the taste of the occupants. Beyond the need for simple weatherproofing lay the desire for some aesthetic satisfaction, the wish to make one's dwelling place pleasing in appearance as well as comfortable to occupy.\n\nBattens might be nailed over the gaps between slabs, or the entire exterior might be clad with weatherboards. The exterior might then be painted, using mixes of materials as diverse as skim milk, quick-lime, lampblack and cement or plastered over entirely. All these measures were less to do with appearance than with preservation of the fabric of the building.\n\n'The split timbers are put in quite rough, and chipped all over with the axe to insure adhesion of the coat of plaster. This plaster is composed of alluvial soil, mixed with a portion of cow-dung to prevent it from cracking, and with chopped grass to enable it to adhere, the coat being put on with a light spade and smoothed over with a plasterer's trowel. It is run over occasionally afterwards with the trowel to fill in the cracks; and on being quite dry, whitewashed with lime, plaster of Paris, or apple-tree ashes and sour milk, the latter forming a tolerable substitute for lime as whitewash.'\n\nThe interior might have a coating of plaster made from a variety of available ingredients: mud, clay, cow-dung. The inside face of the slabs might be whitewashed, or have newspaper pasted over them. More elaborate linings might cover the ceiling, and include sailcloth, hessian, calico, osnaburg, even wallpaper, cretonne or chintz. Mrs Aeneas Gunn describes making 'a huge mosquito-netted dining room, big enough to enclose the table and chairs, so as to ensure our meals in comfort... we hoped to find a paradise at mealtimes in comparison with the purgatory of the last few months.'\n\nFloors might consist of the original ground upon which the hut was erected, but various mixtures of sand, clay, cow-dung, and similar materials were laid to make a firmer, more level, or harder-wearing indoor surface. Termite mounds, crushed and watered, had many of the properties of poured concrete when used as flooring material. Termites mix their saliva, faeces and other substances to bind soil particles and form their mound: this type of flooring was known as 'ant bed'. All of these substances or mixes required regular maintenance, either by watering them to re-solidify the materials, or by spreading a new layer of mixture on top.\n\nTimber slabs might also be laid directly on the earth to form a floor. More sophisticated and permanent dwellings had properly sawn floorboards nailed onto bearers.\n\nThe basic slab hut derived its plan from the vernacular English crofter's hut, a simple rectangular walled shelter with one door, and perhaps holes to allow air to enter. The interior spaces might later be partitioned off. To this design Australian settlers often added a verandah.\n\nMost slab-hut construction techniques could be described as bush carpentry. Few early settlers could afford the time, or possessed the capital, to build any dwelling more impressive than a slab hut: they had first to clear their land and get a crop planted or pasture fenced. In later years, according to the terms of their purchase, selectors had to erect and occupy a dwelling on their land as soon as possible. On the goldfields, or timber-getting, only a temporary dwelling, produced quickly from available materials, was thought necessary.\n\nSince a majority of early settlers had formerly been manual labourers, they brought with them a sound practical ability and aptitude for 'making do'; other settlers observed or helped those more skilled and copied their techniques. The average settler could thus erect a basic hut in two or three weeks, adding to or modifying it later.\n\nThe two preferred methods of slab hut construction differed chiefly in the placement of the wall slabs: vertically or horizontally.\n\nAlexander Harris described the vertical method of slab hut construction:\n\nThe first step of its erection was digging post-holes,\nof about two feet deep... in which were placed posts ten feet high,\nsquared on the four sides with the axe... Along the\nground between these... were laid ground-plates and wall-plates... having a groove of\nabout an inch and a half wide and two inches deep mortised into the flat\nsides their whole length. Into these grooves were fitted the two ends of\nthe eight-feet slabs we had split with the maul and wedges... The flooringboards... were six inches wide and one [inch] thick; timber being used so green, and the heat being so great, boards\nof any greater width turn up at the edges, so as in time to look like a row\nof spouts. The rooms were all joisted at top, and on the joists was spread\na floor of bark, so as to form, over the whole top of the house, the\nsettler's usual first rude granary. Squares of a couple of feet..\nwere left open in the wall in various places for windows... The chimneys were large,\nlike those of old farm-houses, and, for security, had a little wall of rough\nstone and mortar run up inside about three feet; and in the middle of the\nfire-place was a large flag-stone, of a sort capable of resisting the fire,\nwhich constituted the hearth and baking-place. \n\nSurgeon Peter Cunnigham, advising potential settlers, described a similar method, and added:\n\n... by this means a wooden house may be put up without having more than a dozen nails in its composition. I have known the frame of a house of this description, twenty-four feet long by twelve broad, with a back-skilling, or lean-to, of the same length seven feet wide attached to it, put up for the small sum of eight pounds, exclusive of plastering. The house was thatched, had a chimney, and was divided into four compartments ; and with the additional plastering, whitewashing, and fitting of doors and windows, I do not think exceeded twenty pounds... A veranda tends materially to the coolness of the habitation, by sheltering the walls from the sun...\n\nIf only a top plate was used, the top of each slab was pushed up into the groove (a mortise). The bottom of the slab was merely set into a trench. When a wall bottom plate was used, it was also mortised. Each slab was slid in at one end of these plates; on the bottom plate, an extra piece was cut out at one end of the groove to widen it and allow each slab to be fitted in: this piece was replaced after the last slab was inserted. Another method was to make a much deeper mortise in the top plate. In this case, each slab was lifted up into the deep top groove and then dropped into the bottom one. A third method was to nail planks either side of the wall plates to form a channel to hold the slabs, instead of mortising. This was a much quicker method of construction, but it required the use of sawn and dressed timber, and nails. Slabs were sometimes chamfered at one or both ends to fit into the mortises. Each method took more time and labour, and used more material, but produced a progressively more sophisticated and permanent structure.\n\nMrs Aeneas Gunn wrote of their Northern Territory homestead:\n\nThe walls are erected by what is known as the drop-slab-panel system - upright panels formed of three-foot slabs cut from the outside slice of tree-trunks, and dropped horizontally, one above the other, between grooved posts - a simple arrangement, quickly run up and artistic in appearance - outside, a horizontally fluted surface, formed by the natural curves of the timber, and inside, flat, smooth walls. As in every third panel there was a door or a window, and as the horizontal slabs stopped within two feet of the ceiling, the building was exceedingly airy, and open on all sides.\n\nIn this case, too, instead of grooving the posts, a channel might be made by nailing battens either side of the uprights, and the slabs fitted inside these.\n\nIt is not clear which of these two methods was the more popular. Examples of each remain. The shearing shed shown in this illustration c. 1890 has walls of both vertical and horizontal slabs; the latter may have been a later addition. The horizontal method had the advantage that shorter slabs (known as 'billets') of timber could be used, but more uprights had to be erected and mortised to hold these.\n\nThe slab hut is mentioned often in classic Australian literature.\n\nIn works of fiction, Henry Lawson's lives in a slab hut; so does his \"Bush Undertaker,\" and much of \"A Day on a Selection\" is set in or around one. A horizontal-slab shearing shed is the scene for \"Stragglers\", and Lawson remarks of this makeshift structure, '... the whole business reminds us of the \"cubby house\" style of architecture of our childhood.'\n\nMiles Franklin's Sybylla Melvyn grew up in a 'comfortable, wide-veranda'ed, irregularly built slab house' in the Timlinbilly Ranges and she was educated at 'Stringybark Hill Public... a little slab school house.' Richard Mahony hurriedly renovates his goldfields house and general store, so it will be fit for his new wife to occupy 'That her ears should not be polluted by the worst language of the customers he ran up a partition... cutting off the slab-walled portion of the house, with its roof of stringy-bark, from the log and canvas front. He also stopped with putty the worst gaps between the slabs...' Geoffrey Hamlyn recollects 'the old slab hut' at Baroona 'now quite overwhelmed' by the new, long, low house, the result of 'dull, stupid prosperity'.\n\nSteele Rudd's \"Our New Selection\" describes the first house his farming family built:\n\nIt was a slabbed house, with shingled roof, and space enough for two rooms, but the partition wasn't up. The floor was earth, but Dad had a mixture of sand and fresh cow-dung with which he used to keep it level. About once every month he would put it on, and everyone had to keep outside that day till it was dry. There were no locks on the doors. Pegs were put in to keep them fast at night, and the slabs were not very close together, for we could easily see anybody coming on horseback by looking through them. Joe and I used to play at counting the stars through the cracks in the roof.\n\nIn biographical writings, Louisa Anne Meredith considered such 'habitations... the least pleasing objects one meets with in this colony,' but her objections were chiefly to the poor initial construction and subsequent neglect of those dwellings. This arose, she claimed, from the high wages paid due to the shortage of labour, and therefore the idleness and drunkenness of the 'working classes'. Writing of a convict-owned and operated theatre, \"Ralph Rashleigh\" says 'The theatre... had few external charms. It was formed only of slabs and bark; yet the interstices of the walls being filled in with mud, and the whole of the interior whitewashed with pipeclay, of which there was abundance near, it produced no despicable effect by candlelight.'\n\nRachel Henning describes the construction of their slab-built homestead on their Queensland station. The house was relocated during her time there. Henning remarks, 'It is not much to move a slab house; all the woodwork takes down and puts up again; some of the roof may have to be new, but nothing else.'\n\nMrs Aeneas Gunn writes of the satisfaction derived from building their slab homestead, 'beginning at the beginning of things': choosing, felling and sawing their own timber. In his A Fortunate Life, Bert Facey describes his method of building a slab house for a farmer, having watched and helped others to build such structures several times during his life.\n\n\"Frank Melton's Luck\" (1891):\n\nI've bought that big block of land ten miles north of here. Shall want you to go up and manage it. Take up Tom Hardy with you. He'll look after the cattle and cook. Then those two contractor fellows will soon run you up a slab hut. A tent will do till it's ready.\n\n\"In the Shadow of the Bush\" (1899):\n\nA large clearing opened out on the right, and a little way back from the road-line stood a slab hut—or wharé, as it is generally called in New Zealand... A building of but one apartment... constructed entirely of split timber, but neatly put together. The roof was of iron, as was also the chimney. The latter, deep and wide, extended nearly across the whole of one end, and formed almost a small compartment of its own. Its dimensions, however, were but in keeping with the supply of firewood outside; and it is only in the bush districts that such fireplaces are to be seen... Two small windows gave light to the apartment.\n\n\"A Maori Maid\" (1898):\nOn a low hill-side, with a clump of bush close behind, stood the rough whare. The roof was thatched with totara bark. The walls consisted of unplaned slabs of totara wood about six feet long, placed vertically side by side. There was no lining, and there were no flooring boards; only the hard dry clay. The window was a mere opening with a piece of white linen stretched across in place of glass... Almost the whole of one end of the hut consisted of fireplace. The chimney was built of wood. At the bottom large stones, cemented together with clay and mud, formed a rough lining and a protection from the flames... John's present country home was as rough and unpretentious as it well could be. He was pursuing the wise course of putting every available penny into improvements that would bring in some profit... Time enough to build a good homestead when he had a good woolshed... \n\nThe landscapes of Augustus Earle, and S.T. Gill usually show one or more slab structures; Gill even illustrated the process of splitting timber for slabs. William Strutt's sketch of a settler's hut shows the tools used to build it, while John Skinner Prout's \"Interior of Settlers Hut Australia\" emphasizes the crudity of technique and bulkiness of the timbers. It also shows the timber fireplace and chimney. Strutt in 1856, also sketched a New Zealand settler's 'whorry' William Swainson, John Barr Clark Hoyte, Frances Mary Hodges and Charles Blomfield, among others, produced paintings of slab wharves and other structures. \n\nThe deterioration of the hut depicted by Nicholas Chevalier in his \"Buffalo Ranges\" supports Louisa Meredith's observation about poor upkeep by many hut occupants. Unk White's 1960s sketches of Tyrrell's Vineyard in the Hunter Valley include a slab hut dating from 1858.\n\nThe 'backblocks' humour of Australian cartoonists of the \"Smith's Weekly\" school such as Alex Gurney, Percy Leason, Stan Cross and Eric Jolliffe often included slab huts as a backdrop to their gags. Jolliffe also published detailed sketches of slab structures still standing, to preserve Australian heritage. In journalism, illustrations of rural towns and farms in Australian newspapers and magazines of the Colonial era often show slab huts and homes. Examples can be seen in \"The Australasian Sketcher,\" \"The Sydney Mail\" and \"Sydney Punch.\"\n\nThis slab-walled house (Fig. 1) was built in 1992, in the Watagan Ranges of New South Wales.\n\nIt varies from the traditional design in several respects. It is raised off the ground on stumps (Fig. 5); the slab walls are of sawn timber, not flitches split from a trunk (Fig 2.); it uses the nailed 'channel' method of holding the slabs, not mortises; the spaces between the slabs are filled with foam-rubber strips (Fig. 5); no attempt has been made to line or clad the house (Fig. 3); it has no chimney or fireplace as part of the structure; the floor is of chipboard.\n\nMore akin to traditional structures, the roof has no joists, and there is no ceiling; the entire pitch of the roof forms the interior space, allowing for cooling in summer; the gable-ends are framed with studs and filled in with weatherboards (Fig. 4). The walls are kept square by a mezzanine floor, reached by an internal spiral staircase, making the house in effect a two-storey structure (Fig. 3).\n\n\n\n\n \n"}
{"id": "25157839", "url": "https://en.wikipedia.org/wiki?curid=25157839", "title": "Smith-Putnam wind turbine", "text": "Smith-Putnam wind turbine\n\nThe Smith-Putnam wind turbine was the world's first megawatt-size wind turbine. In 1941 it was connected to the local electrical distribution system on Grandpa's Knob in Castleton, Vermont, US. It was designed by Palmer Cosslett Putnam and manufactured by the S. Morgan Smith Company. The 1.25 MW turbine operated for 1100 hours before a blade failed at a known weak point, which had not been reinforced due to wartime material shortages. It would be the largest wind turbine ever built until 1979.\n\nThe turbine had two blades, 175 feet (53 m) in diameter, on the down-wind side of a 120-foot (36 m) steel lattice tower. Each blade was approximately 8 feet (2.4 m) wide and 66 feet (20 m) long, and weighed eight tons. The blades were built on steel spars and covered with a stainless steel skin. The blade spars were hinged at their root attachment to the hub, allowing them to assume a slight cone shape. The generator was a 1250 kW 600 RPM synchronous generator made by General Electric, producing 2,400 V at 60 cycles. The generator and rotor hub were mounted on a pintle beam, which allowed the rotor to capture wind from varying directions. The pitch of the blades was controlled by hydraulic cylinders to maintain constant speed.\nPalmer Putnam became interested in production of electric power from wind after observing high winds at Cape Cod. Putnam was aware of the Balaklava 100 kW turbine and desired to improve on its performance. By 1937 he had enlisted General Electric, and the Central Vermont Public Service Corporation. General Electric provided a generator, and Central Vermont Public Services Corporation was interested in an energy supply that could displace purchased power for meeting peak loads. Only 23 months elapsed between first discussions and production of power.\n\nPalmer concluded that the most promising concept was a two-bladed propeller driving a synchronous AC generator. He developed a preliminary design and cost estimates. Dr. Vannevar Bush, Dean of Engineering at MIT, reacted favorably when shown these calculations in 1937. Bush introduced Putnam to a vice president of General Electric Company, Mr. T. Knight. From this point on Putnam was able to enlist the services of some very taiented people which included Theodore von Karman, a world-famous authority on aerodynamics, to assist in the design, parametric studies, cost analyses, site selection, and determination of wind characteristics.\n\nIn 1939, the Guggenheim Aeronautical Laboratories of the California Institute of Technology (GALCIT) was approached by Palmer C. Putnam, to design the turbine. Theodore von Kármán had William Rees Sears and W. Duncan Rannie carry out the aerodynamic design. Unfortunately, Rannie's analytical findings regarding the stability of the giant windmill were not incorporated in the prototype that was built and tested on the mountain.\n\nPutnam obtained the financial and technical backing of the S. Morgan Smith Company of York, Pennsylvania. The Smith Company manufactured hydroelectric hydraulic turbines. Since the number of feasible sites for hydroelectric development was felt to be declining, the Smith company sought diversification into a new but related product line. The S. Morgan Smith Co. agreed to take on the project as general contractor and financed construction of a pilot turbine.\n\nThe site chosen for the prototype turbine was a previously unnamed 2000 foot (600 m) elevation, named \"Grandpa's Knob\"; this mountain was not so high as to have excessive ice build up, but had high wind speeds. Access to the site required construction of a road with 12 to 15% grade. Due to the impending entry of the United States into World War II, some of the fundamental research and testing process was skipped so that major components could be made before wartime material shortages occurred.\n\nNo-load testing of the unit began in August 1941 to verify mechanical operation of the turbine and the blade control system. The generator was first synchronized to the local electrical grid on the evening of October 19, 1941, and tested under load varying from zero to 700 kW. The unit operated for about 1000 hours between startup and February 1943, when a shaft bearing failed. Due to wartime material priorities, the bearing was not replaced until March 3, 1945, when the unit achieved another three weeks of operation.\n\nIn the early morning of March 26, 1945, the operator on duty in the nacelle of the turbine was thrown down by vibrations. He stopped the turbine. On investigation, it was found one turbine blade had broken off and fallen about 750 feet (229 m) away. The blade had failed at a previously repaired weak point in the spar; due to wartime shortages, it had been impractical to complete a full repair and reinforcement of the blade root.\n\nA study completed in 1945 suggested that a block of six turbines similar to the prototype, producing 9 MW, could be installed in Vermont for around US$190 per kilowatt. However, the economic value to the power utility was only $125 per kilowatt, and the wind turbine was not considered economically viable by a factor of 1.5. Although the S. Morgan Smith company had spent more than US$1.25 million on the prototype turbine, entirely private funding, it concluded that there was insufficient prospect for profit on further development. Repairs were never done after the March 1945 failure. The prototype turbine was dismantled in 1946, leaving only concrete footings and a marker plaque at the site today. In the introduction to Putnam's book, Vannevar Bush stated that the project achieved proof of the concept of synchronous generation of wind power, and projected future commercial use of wind-generated electricity.\n\n\n"}
{"id": "186919", "url": "https://en.wikipedia.org/wiki?curid=186919", "title": "Stirling engine", "text": "Stirling engine\n\nA Stirling engine is a heat engine that operates by cyclic compression and expansion of air or other gas (the \"working fluid\") at different temperatures, such that there is a net conversion of heat energy to mechanical work. More specifically, the Stirling engine is a closed-cycle regenerative heat engine with a permanently gaseous working fluid. \"Closed-cycle\", in this context, means a thermodynamic system in which the working fluid is permanently contained within the system, and \"regenerative\" describes the use of a specific type of internal heat exchanger and thermal store, known as the \"regenerator\". Strictly speaking, the inclusion of the regenerator is what differentiates a Stirling engine from other closed cycle hot air engines.\n\nOriginally conceived in 1816 as an industrial prime mover to rival the steam engine, its practical use was largely confined to low-power domestic applications for over a century.\n\nRobert Stirling is considered as one of the fathers of hot air engines, notwithstanding some earlier predecessors, notably Amontons, who succeeded in building, in 1699, the first working hot air engine.\n\nHe has been later followed by Cayley. This engine type was of those in which the fire is enclosed, and fed by air pumped in beneath the grate in sufficient quantity to maintain combustion, while by far the largest portion of the air enters above the fire, to be heated and expanded; the whole, together with the products of combustion, then acts on the piston, and passes through the working cylinder; and the operation being one of simple mixture only, no heating surface of metal is required, the air to be heated being brought into immediate contact with the fire.\n\nStirling came up with a first air engine in 1816. The principle of the Stirling Air Engine differs from that of Sir George Cayley (1807), in which the air is forced through the furnace and exhausted, whereas in Stirling’s engine the air works in a closed circuit. It was to it that the inventor devoted most of his attention. \nA two horse-power engine, built in 1818 for pumping water at an Ayrshire quarry, continued to work for some time, until a careless attendant allowed the heater to become overheated. This experiment proved to the inventor that, owing to the low working pressure obtainable, the engine could only be adapted to small powers for which there was at that time no demand.\n\nThe Stirling 1816 patent was also about an \"Economiser\", is the predecessor of the regenerator. In this patent (# 4081) he describes the \"economiser\" technology and several applications where such technology can be used. Out of them came a new arrangement for a hot air engine. In 1818, one engine was built to pump water from a quarry in Ayrshire, but due to technical issues, the engine was abandoned for a time.\n\nStirling patented a second hot air engine, together with his brother James, in 1827. They inverted the design so that the hot ends of the displacers were underneath the machinery and they added a compressed air pump so the air within could be increased in pressure to around 20 atmospheres.\n\nThe two Stirling brothers were followed shortly after (1828) by Parkinson & Crossley and Arnott in 1829.\n\nThese precursors, to whom Ericsson should be added, have brought to the world the hot air engine technology and its enormous advantages over the steam engine. Each of them came with his own specific technology, and although the Stirling engine and the Parkinson & Crossley engines were quite similar, Robert Stirling distinguished himself by inventing the regenerator.\n\nParkinson and Crosley introduced the principle of using air of greater density than that of the atmosphere, and so obtained an engine of greater power in the same compass. James Stirling followed this same idea when he built the famous Dundee engine.\n\nThe Stirling patent of 1827 was the base of the Stirling third patent of 1840. The changes from the 1827 patent were minor but essential, and this third patent led to the Dundee engine. \n\nJames Stirling presented his engine to the Institution of Civil Engineers in 1845. The first engine of this kind which, after various modifications, was efficiently constructed and heated, had a cylinder of 12 inches (approx. 30 cm) in diameter, with a length of stroke of 2 feet (approx. 61 cm), and made 40 strokes or revolutions in a minute (40 rpm). This engine moved all the machinery at the Dundee Foundry Company's works for eight or ten months, and was previously found capable of raising 700,000 lbs one foot in a minute (approx. 21 HP).\n\nFinding this power insufficient for their works, the Dundee Foundry Company erected the second engine, with a cylinder of 16 inches (approx. 40 cm) in diameter, a stroke of 4 feet (approx. 1.20 m), and making 28 strokes in a minute. When this engine had been in continual operation for upwards of two years, it had not only performed the work of the foundry in the most satisfactory manner, but had been tested (by a friction brake on a third mover) to the extent of lifting nearly 1,500,000 lbs (approx. 45 HP).\n\nThis gives a consumption of 2.7 lbs. (approx. 1.22 kg) per horse-power per hour; but when the engine was not fully burdened, the consumption was considerably under 2.5 lbs. (approx. 1.13 kg) per horse-power per hour. This performance was at the level of the best steam engines whose efficiency was about 10%. After James Stirling, such efficiency was possible only thanks to the use of the economiser (or regenerator).\n\nThe Stirling engine (or Stirling's air engine as it was known at the time) was invented and patented in 1816. It followed earlier attempts at making an air engine but was probably the first put to practical use when, in 1818, an engine built by Stirling was employed pumping water in a quarry. The main subject of Stirling's original patent was a heat exchanger, which he called an \"economiser\" for its enhancement of fuel economy in a variety of applications. The patent also described in detail the employment of one form of the economiser in his unique closed-cycle air engine design in which application it is now generally known as a \"regenerator\". Subsequent development by Robert Stirling and his brother James, an engineer, resulted in patents for various improved configurations of the original engine including pressurization, which by 1843, had sufficiently increased power output to drive all the machinery at a Dundee iron foundry.\n\nThough it has been disputed, it is widely supposed that the inventor's aims were not only to save fuel but also to create a safer alternative to the steam engines of the time, whose boilers frequently exploded, causing many injuries and fatalities.\n\nThe need for Stirling engines to run at very high temperatures to maximize power and efficiency exposed limitations in the materials of the day, and the few engines that were built in those early years suffered unacceptably frequent failures (albeit with far less disastrous consequences than boiler explosions). For example, the Dundee foundry engine was replaced by a steam engine after three hot cylinder failures in four years.\n\nSubsequent to the replacement of the Dundee foundry engine there is no record of the Stirling brothers having any further involvement with air engine development, and the Stirling engine never again competed with steam as an industrial scale power source. (Steam boilers were becoming safer and steam engines more efficient, thus presenting less of a target for rival prime movers). However, beginning about 1860, smaller engines of the Stirling/hot air type were produced in substantial numbers for applications in which reliable sources of low to medium power were required, such as pumping air for church organs or raising water. These smaller engines generally operated at lower temperatures so as not to tax available materials, and so were relatively inefficient. Their selling point was that unlike steam engines, they could be operated safely by anybody capable of managing a fire. Several types remained in production beyond the end of the century, but apart from a few minor mechanical improvements the design of the Stirling engine in general stagnated during this period.\n\nDuring the early part of the twentieth century the role of the Stirling engine as a \"domestic motor\" was gradually taken over by electric motors and small internal combustion engines. By the late 1930s, it was largely forgotten, only produced for toys and a few small ventilating fans.\n\nAround that time, Philips was seeking to expand sales of its radios into parts of the world where grid electricity and batteries were not consistently available. Philips' management decided that offering a low-power portable generator would facilitate such sales and asked a group of engineers at the company's research lab in Eindhoven to evaluate alternative ways of achieving this aim. After a systematic comparison of various prime movers, the team decided to go forward with the Stirling engine, citing its quiet operation (both audibly and in terms of radio interference) and ability to run on a variety of heat sources (common lamp oil – \"cheap and available everywhere\" – was favored). They were also aware that, unlike steam and internal combustion engines, virtually no serious development work had been carried out on the Stirling engine for many years and asserted that modern materials and know-how should enable great improvements.\nBy 1951, the 180/200 W generator set designated MP1002CA (known as the \"Bungalow set\") was ready for production and an initial batch of 250 was planned, but soon it became clear that they could not be made at a competitive price. Additionally, the advent of transistor radios and their much lower power requirements meant that the original rationale for the set was disappearing. Approximately 150 of these sets were eventually produced. Some found their way into university and college engineering departments around the world giving generations of students a valuable introduction to the Stirling engine.\n\nIn parallel with the Bungalow set, Philips developed experimental Stirling engines for a wide variety of applications and continued to work in the field until the late 1970s, but only achieved commercial success with the \"reversed Stirling engine\" cryocooler. However, they filed a large number of patents and amassed a wealth of information, which they licensed to other companies and which formed the basis of much of the development work in the modern era.\n\nIn 1996, the Swedish navy commissioned three Gotland-class submarines. On the surface, these boats are propelled by marine diesel engines. However, when submerged, they use a Stirling-driven generator developed by Swedish shipbuilder Kockums to recharge batteries and provide electrical power for propulsion. A supply of liquid oxygen is carried to support burning of diesel fuel to power the engine. Stirling engines are also fitted to the Swedish Södermanland-class submarines, the Archer-class submarines in service in Singapore and, license-built by Kawasaki Heavy Industries for the Japanese Sōryū-class submarines. In a submarine application, the Stirling engine offers the advantage of being exceptionally quiet when running.\n\nStirling engines are frequently used in the dish version of Concentrated Solar Power systems. A mirrored dish similar to a very large satellite dish directs and concentrates sunlight onto a thermal receiver, which absorbs and collects the heat and using a fluid transfers it into the Stirling engine. The resulting mechanical power is then used to run a generator or alternator to produce electricity.\n\nStirling engines are forming the core component of micro combined heat and power (CHP) units, as they are more efficient and safer than a comparable steam engine. CHP units are being installed in people's homes.\n\nA Stirling engine is a heat engine that operates by cyclic compression and expansion of air or other gas (the \"working fluid\") at different temperatures, such that there is a net conversion of heat energy to mechanical work. More specifically, the Stirling engine is a closed-cycle regenerative heat engine with a permanently gaseous working fluid. \"Closed-cycle\", in this context, means a thermodynamic system in which the working fluid is permanently contained within the system, and \"regenerative\" describes the use of a specific type of internal heat exchanger and thermal store, known as the \"regenerator\". Strictly speaking, the inclusion of the regenerator is what differentiates a Stirling engine from other closed cycle hot air engines.\n\nStirling engines have a high efficiency compared to internal combustion engines, being able to reach 50% efficiency. They are also capable of quiet operation and can use almost any heat source. The heat energy source is generated external to the Stirling engine rather than by internal combustion as with the Otto cycle or Diesel cycle engines. Because the Stirling engine is compatible with alternative and renewable energy sources it could become increasingly significant as the price of conventional fuels rises, and also in light of concerns such as depletion of oil supplies and climate change. This type of engine is currently generating interest as the core component of micro combined heat and power (CHP) units, in which it is more efficient and safer than a comparable steam engine. However, it has a low power-to-weight ratio, rendering it more suitable for use in static installations where space and weight are not at a premium.\n\nRobert Stirling invented the first practical example of a closed cycle air engine in 1816, and it was suggested by Fleeming Jenkin as early as 1884 that all such engines should therefore generically be called Stirling engines. This naming proposal found little favour, and the various types on the market continued to be known by the name of their individual designers or manufacturers, e.g., Rider's, Robinson's, or Heinrici's (hot) air engine. In the 1940s, the Philips company was seeking a suitable name for its own version of the 'air engine', which by that time had been tested with working fluids other than air, and decided upon 'Stirling engine' in April 1945. However, nearly thirty years later, Graham Walker still had cause to bemoan the fact such terms as \"hot air engine\" remained interchangeable with \"Stirling engine\", which itself was applied widely and indiscriminately, a situation that continues.\n\nLike the steam engine, the Stirling engine is traditionally classified as an external combustion engine, as all heat transfers to and from the working fluid take place through a solid boundary (heat exchanger) thus isolating the combustion process and any contaminants it may produce from the working parts of the engine. This contrasts with an internal combustion engine where heat input is by combustion of a fuel within the body of the working fluid. Most of the many possible implementations of the Stirling engine fall into the category of reciprocating piston engine.\n\nThe engine is designed so the working gas is generally compressed in the colder portion of the engine and expanded in the hotter portion resulting in a net conversion of heat into work. An internal regenerative heat exchanger increases the Stirling engine's thermal efficiency compared to simpler hot air engines lacking this feature.\n\nAs a consequence of closed cycle operation, the heat driving a Stirling engine must be transmitted from a heat source to the working fluid by heat exchangers and finally to a heat sink. A Stirling engine system has at least one heat source, one heat sink and up to five heat exchangers. Some types may combine or dispense with some of these.\n\nThe heat source may be provided by the combustion of a fuel and, since the combustion products do not mix with the working fluid and hence do not come into contact with the internal parts of the engine, a Stirling engine can run on fuels that would damage other engines types' internals, such as landfill gas, which may contain siloxane that could deposit abrasive silicon dioxide in conventional engines.\n\nOther suitable heat sources include concentrated solar energy, geothermal energy, nuclear energy, waste heat and bioenergy. If solar power is used as a heat source, regular solar mirrors and solar dishes may be utilised. The use of Fresnel lenses and mirrors has also been advocated, for example in planetary surface exploration. Solar powered Stirling engines are increasingly popular as they offer an environmentally sound option for producing power while some designs are economically attractive in development projects.\n\nIn small, low power engines this may simply consist of the walls of the hot space(s) but where larger powers are required a greater surface area is needed to transfer sufficient heat. Typical implementations are internal and external fins or multiple small bore tubes.\n\nDesigning Stirling engine heat exchangers is a balance between high heat transfer with low viscous pumping losses, and low dead space (unswept internal volume). Engines that operate at high powers and pressures require that heat exchangers on the hot side be made of alloys that retain considerable strength at high temperatures and that don't corrode or creep.\n\nIn a Stirling engine, the regenerator is an internal heat exchanger and temporary heat store placed between the hot and cold spaces such that the working fluid passes through it first in one direction then the other, taking heat from the fluid in one direction, and returning it in the other. It can be as simple as metal mesh or foam, and benefits from high surface area, high heat capacity, low conductivity and low flow friction. Its function is to retain within the system that heat that would otherwise be exchanged with the environment at temperatures intermediate to the maximum and minimum cycle temperatures, thus enabling the thermal efficiency of the cycle (though not of any practical engine) to approach the limiting Carnot efficiency.\n\nThe primary effect of regeneration in a Stirling engine is to increase the thermal efficiency by 'recycling' internal heat that would otherwise pass through the engine irreversibly. As a secondary effect, increased thermal efficiency yields a higher power output from a given set of hot and cold end heat exchangers. These usually limit the engine's heat throughput. In practice this additional power may not be fully realized as the additional \"dead space\" (unswept volume) and pumping loss inherent in practical regenerators reduces the potential efficiency gains from regeneration.\n\nThe design challenge for a Stirling engine regenerator is to provide sufficient heat transfer capacity without introducing too much additional internal volume ('dead space') or flow resistance. These inherent design conflicts are one of many factors that limit the efficiency of practical Stirling engines. A typical design is a stack of fine metal wire meshes, with low porosity to reduce dead space, and with the wire axes perpendicular to the gas flow to reduce conduction in that direction and to maximize convective heat transfer.\n\nThe regenerator is the key component invented by Robert Stirling and its presence distinguishes a true Stirling engine from any other closed cycle hot air engine. Many small 'toy' Stirling engines, particularly low-temperature difference (LTD) types, do not have a distinct regenerator component and might be considered hot air engines; however a small amount of regeneration is provided by the surface of the displacer itself and the nearby cylinder wall, or similarly the passage connecting the hot and cold cylinders of an alpha configuration engine.\n\nIn small, low power engines this may simply consist of the walls of the cold space(s), but where larger powers are required a cooler using a liquid like water is needed to transfer sufficient heat.\n\nThe larger the temperature difference between the hot and cold sections of a Stirling engine, the greater the engine's efficiency. The heat sink is typically the environment the engine operates in, at ambient temperature. In the case of medium to high power engines, a radiator is required to transfer the heat from the engine to the ambient air. Marine engines have the advantage of using cool ambient sea, lake, or river water, which is typically cooler than ambient air. In the case of combined heat and power systems, the engine's cooling water is used directly or indirectly for heating purposes, raising efficiency.\n\nAlternatively, heat may be supplied at ambient temperature and the heat sink maintained at a lower temperature by such means as cryogenic fluid (see Liquid nitrogen economy) or iced water.\n\nThe displacer is a special-purpose piston, used in Beta and Gamma type Stirling engines, to move the working gas back and forth between the hot and cold heat exchangers. Depending on the type of engine design, the displacer may or may not be sealed to the cylinder, i.e. it may be a loose fit within the cylinder, allowing the working gas to pass around it as it moves to occupy the part of the cylinder beyond.\n\nThere are three major types of Stirling engines, that are distinguished by the way they move the air between the hot and cold areas:\n\n\nAn alpha Stirling contains two power pistons in separate cylinders, one hot and one cold. The hot cylinder is situated inside the high temperature heat exchanger and the cold cylinder is situated inside the low temperature heat exchanger. This type of engine has a high power-to-volume ratio but has technical problems because of the usually high temperature of the hot piston and the durability of its seals. In practice, this piston usually carries a large insulating head to move the seals away from the hot zone at the expense of some additional dead space. The crank angle has a major effect on efficiency and the best angle frequently must be found experimentally. An angle of 90° frequently locks.\n\nThe following diagrams do not show internal heat exchangers in the compression and expansion spaces, which are needed to produce power. A regenerator would be placed in the pipe connecting the two cylinders.\n\nA beta Stirling has a single power piston arranged within the same cylinder on the same shaft as a displacer piston. The displacer piston is a loose fit and does not extract any power from the expanding gas but only serves to shuttle the working gas between the hot and cold heat exchangers. When the working gas is pushed to the hot end of the cylinder it expands and pushes the power piston. When it is pushed to the cold end of the cylinder it contracts and the momentum of the machine, usually enhanced by a flywheel, pushes the power piston the other way to compress the gas. Unlike the alpha type, the beta type avoids the technical problems of hot moving seals, as the power piston is not in contact with the hot gas.\n\nAgain, the following diagrams do not show any internal heat exchangers or a regenerator, which would be placed in the gas path around the displacer. If a regenerator is used in a beta engine, it is usually in the position of the displacer and moving, often as a volume of wire mesh.\n\nA gamma Stirling is simply a beta Stirling with the power piston mounted in a separate cylinder alongside the displacer piston cylinder, but still connected to the same flywheel. The gas in the two cylinders can flow freely between them and remains a single body. This configuration produces a lower compression ratio because of the volume of the connection between the two but is mechanically simpler and often used in multi-cylinder Stirling engines.\n\nOther Stirling configurations continue to interest engineers and inventors.\n\nThe rotary Stirling engine seeks to convert power from the Stirling cycle directly into torque, similar to the rotary combustion engine. No practical engine has yet been built but a number of concepts, models and patents have been produced, such as the Quasiturbine engine.\n\nA hybrid between piston and rotary configuration is a double acting engine. This design rotates the displacers on either side of the power piston. In addition to giving great design variability in the heat transfer area, this layout eliminates all but one external seal on the output shaft and one internal seal on the piston. Also, both sides can be highly pressurized as they balance against each other. \n\nAnother alternative is the Fluidyne engine (Fluidyne heat pump), which uses hydraulic pistons to implement the Stirling cycle. The work produced by a Fluidyne engine goes into pumping the liquid. In its simplest form, the engine contains a working gas, a liquid, and two non-return valves.\n\nThe concept published in 1907 has no rotary mechanism or linkage for the displacer. This is instead driven by a small auxiliary piston, usually a thick displacer rod, with the movement limited by stops.\n\nThe two-cylinder Stirling with Ross yoke is a two-cylinder stirling engine (positioned at 0°, not 90°) connected using a special yoke. The engine configuration/yoke setup was invented by Andy Ross.\n\nThe Franchot engine is a double acting engine invented by Charles-Louis-Félix Franchot in the nineteenth century. In a double acting engine, the pressure of the working fluid acts on both sides of the piston. One of the simplest forms of a double acting machine, the Franchot engine consists of two pistons and two cylinders, and acts like two separate alpha machines. In the Franchot engine, each piston acts in two gas phases, which makes more efficient use of the mechanical components than a single acting alpha machine. However, a disadvantage of this machine is that one connecting rod must have a sliding seal at the hot side of the engine, which is difficult when dealing with high pressures and temperatures.\n\nFree-piston Stirling engines include those with liquid pistons and those with diaphragms as pistons. In a free-piston device, energy may be added or removed by an electrical linear alternator, pump or other coaxial device. This avoids the need for a linkage, and reduces the number of moving parts. In some designs, friction and wear are nearly eliminated by the use of non-contact gas bearings or very precise suspension through planar springs.\n\nFour basic steps in the cycle of a free-piston Stirling engine are:\n\n\nIn the early 1960s, W.T. Beale invented a free piston version of the Stirling engine to overcome the difficulty of lubricating the crank mechanism. While the invention of the basic free piston Stirling engine is generally attributed to Beale, independent inventions of similar types of engines were made by E.H. Cooke-Yarborough and C. West at the Harwell Laboratories of the UKAERE. G.M. Benson also made important early contributions and patented many novel free-piston configurations.\n\nThe first known mention of a Stirling cycle machine using freely moving components is a British patent disclosure in 1876. This machine was envisaged as a refrigerator (i.e., the \"reversed\" Stirling cycle). The first consumer product to utilize a free piston Stirling device was a portable refrigerator manufactured by Twinbird Corporation of Japan and offered in the US by Coleman in 2004.\n\nDesign of the flat double-acting Stirling engine solves the drive of a displacer with the help of the fact that areas of the hot and cold pistons of the displacer are different.\nThe drive does so without any mechanical transmission.\nUsing diaphragms eliminates friction and need for lubricants.\nWhen the displacer is in motion, the generator holds the working piston in the limit position, which brings the engine working cycle close to an ideal Stirling cycle.\nThe ratio of the area of the heat exchangers to the volume of the machine increases by the implementation of a flat design.\nFlat design of the working cylinder approximates thermal process of the expansion and compression closer to the isothermal one.\nThe disadvantage is a large area of the thermal insulation between the hot and cold space.\nThermoacoustic devices are very different from Stirling devices, although the individual path travelled by each working gas molecule does follow a real Stirling cycle. These devices include the thermoacoustic engine and thermoacoustic refrigerator. High-amplitude acoustic standing waves cause compression and expansion analogous to a Stirling power piston, while out-of-phase acoustic travelling waves cause displacement along a temperature gradient, analogous to a Stirling displacer piston. Thus a thermoacoustic device typically does not have a displacer, as found in a beta or gamma Stirling.\n\nStarting in 1986, Infinia Corporation began developing both highly reliable pulsed free-piston Stirling engines, and thermoacoustic coolers using related technology. The published design uses flexural bearings and hermetically sealed Helium gas cycles, to achieve tested reliabilities exceeding 20 years. As of 2010, the corporation had amassed more than 30 patents, and developed a number of commercial products for both combined heat and power, and solar power. According to press release from September 2013, Infinia filed for bankruptcy.\n\nMore recently, NASA has considered nuclear-decay heated Stirling Engines for extended missions to the outer solar system. In 2018, NASA and the United States Department of Energy announced that they had successfully tested a new type of nuclear reactor called KRUSTY, which stands for \"Kilopower Reactor Using Stirling TechnologY\", and which is designed to be able to power deep space vehicles and probes as well as exoplanetary encampments.\nAt the 2012 Cable-Tec Expo put on by the Society of Cable Telecommunications Engineers, Dean Kamen took the stage with Time Warner Cable Chief Technology Officer Mike LaJoie to announce a new initiative between his company Deka Research and the SCTE. Kamen refers to it as a Stirling engine.\n\nThe idealised Stirling cycle consists of four thermodynamic processes acting on the working fluid:\n\n\nTheoretical thermal efficiency equals that of the hypothetical Carnot cycle – i.e. the highest efficiency attainable by any heat engine. However, though it is useful for illustrating general principles, the ideal cycle deviates substantially from practical Stirling engines. It has been argued that its indiscriminate use in many standard books on engineering thermodynamics has done a disservice to the study of Stirling engines in general.\n\nOther real-world issues reduce the efficiency of actual engines, because of limits of convective heat transfer, and viscous flow (friction). There are also practical mechanical considerations, for instance a simple kinematic linkage may be favoured over a more complex mechanism needed to replicate the idealized cycle, and limitations imposed by available materials such as non-ideal properties of the working gas, thermal conductivity, tensile strength, creep, rupture strength, and melting point. A question that often arises is whether the ideal cycle with isothermal expansion and compression is in fact the correct ideal cycle to apply to the Stirling engine. Professor C. J. Rallis has pointed out that it is very difficult to imagine any condition where the expansion and compression spaces may approach isothermal behavior and it is far more realistic to imagine these spaces as adiabatic. An ideal analysis where the expansion and compression spaces are taken to be adiabatic with isothermal heat exchangers and perfect regeneration was analyzed by Rallis and presented as a better ideal yardstick for Stirling machinery. He called this cycle the 'pseudo-Stirling cycle' or 'ideal adiabatic Stirling cycle'. An important consequence of this ideal cycle is that it does not predict Carnot efficiency. A further conclusion of this ideal cycle is that maximum efficiencies are found at lower compression ratios, a characteristic observed in real machines. In an independent work, T. Finkelstein also assumed adiabatic expansion and compression spaces in his analysis of Stirling machinery \n\nSince the Stirling engine is a closed cycle, it contains a fixed mass of gas called the \"working fluid\", most commonly air, hydrogen or helium. In normal operation the engine is sealed and no gas enters or leaves the engine. No valves are required, unlike other types of piston engines. The Stirling engine, like most heat engines, cycles through four main processes: cooling, compression, heating and expansion. This is accomplished by moving the gas back and forth between hot and cold heat exchangers, often with a regenerator between the heater and cooler. The hot heat exchanger is in thermal contact with an external heat source, such as a fuel burner, and the cold heat exchanger is in thermal contact with an external heat sink, such as air fins. A change in gas temperature causes a corresponding change in gas pressure, while the motion of the piston makes the gas alternately expand and compress.\n\nThe gas follows the behaviour described by the gas laws, which describe how a gas's pressure, temperature and volume are related. When the gas is heated the pressure rises (because it is in a sealed chamber) and this pressure then acts on the power piston to produce a power stroke. When the gas is cooled the pressure drops and this drop means that the piston needs to do less work to compress the gas on the return stroke. The difference in work between the strokes yields a net positive power output.\n\nThe ideal Stirling cycle is unattainable in the real world (as with any heat engine); efficiencies of 50% have been reached, similar to the maximum figure for Diesel cycle engines. The efficiency of Stirling machines is also linked to the environmental temperature; higher efficiency is obtained when the weather is cooler, thus making this type of engine less interesting in places with warmer climates. As with other external combustion engines, Stirling engines can use heat sources other than from combustion of fuels.\n\nWhen one side of the piston is open to the atmosphere, the operation is slightly different. As the sealed volume of working gas comes in contact with the hot side, it expands, doing work on both the piston and on the atmosphere. When the working gas contacts the cold side, its pressure drops below atmospheric pressure and the atmosphere pushes on the piston and does work on the gas.\n\nTo summarize, the Stirling engine uses the temperature difference between its hot end and cold end to establish a cycle of a fixed mass of gas, heated and expanded, and cooled and compressed, thus converting thermal energy into mechanical energy. The greater the temperature difference between the hot and cold sources, the greater the thermal efficiency. The maximum theoretical efficiency is equivalent to that of the Carnot cycle, but the efficiency of real engines is less than this value because of friction and other losses.\n\nVery low-power engines have been built that run on a temperature difference of as little as 0.5 K. A displacer type stirling engine has one piston and one displacer. A temperature difference is required between the top and bottom of the large cylinder to run the engine. In the case of the low-temperature difference (LTD) stirling engine, the temperature difference between one's hand and the surrounding air can be enough to run the engine. The power piston in the displacer type stirling engine is tightly sealed and is controlled to move up and down as the gas inside expands. The displacer, on the other hand, is very loosely fitted so that air can move freely between the hot and cold sections of the engine as the piston moves up and down. The displacer moves up and down to cause most of the gas in the displacer cylinder to be either heated, or cooled. \nNote that in the following description of the cycle the heat source at the bottom (the engine would run equally well with the heat source at the top): \n\nIn most high power Stirling engines, both the minimum pressure and mean pressure of the working fluid are above atmospheric pressure. This initial engine pressurization can be realized by a pump, or by filling the engine from a compressed gas tank, or even just by sealing the engine when the mean temperature is lower than the mean operating temperature. All of these methods increase the mass of working fluid in the thermodynamic cycle. All of the heat exchangers must be sized appropriately to supply the necessary heat transfer rates. If the heat exchangers are well designed and can supply the heat flux needed for convective heat transfer, then the engine, in a first approximation, produces power in proportion to the mean pressure, as predicted by the West number, and Beale number. In practice, the maximum pressure is also limited to the safe pressure of the pressure vessel. Like most aspects of Stirling engine design, optimization is multivariate, and often has conflicting requirements. A difficulty of pressurization is that while it improves the power, the heat required increases proportionately to the increased power. This heat transfer is made increasingly difficult with pressurization since increased pressure also demands increased thicknesses of the walls of the engine, which, in turn, increase the resistance to heat transfer.\n\nAt high temperatures and pressures, the oxygen in air-pressurized crankcases, or in the working gas of hot air engines, can combine with the engine's lubricating oil and explode. At least one person has died in such an explosion.\n\nLubricants can also clog heat exchangers, especially the regenerator. For these reasons, designers prefer non-lubricated, low-coefficient of friction materials (such as rulon or graphite), with low normal forces on the moving parts, especially for sliding seals. Some designs avoid sliding surfaces altogether by using diaphragms for sealed pistons. These are some of the factors that allow Stirling engines to have lower maintenance requirements and longer life than internal-combustion engines.\n\nIn contrast to internal combustion engines, Stirling engines have the potential to use renewable heat sources more easily, and to be quieter and more reliable with lower maintenance. They are preferred for applications that value these unique advantages, particularly if the cost per unit energy generated is more important than the capital cost per unit power. On this basis, Stirling engines are cost competitive up to about 100 kW.\n\nCompared to an internal combustion engine of the same power rating, Stirling engines currently have a higher capital cost and are usually larger and heavier. However, they are more efficient than most internal combustion engines. Their lower maintenance requirements make the overall \"energy\" cost comparable. The thermal efficiency is also comparable (for small engines), ranging from 15% to 30%. For applications such as micro-CHP, a Stirling engine is often preferable to an internal combustion engine. Other applications include water pumping, astronautics, and electrical generation from plentiful energy sources that are incompatible with the internal combustion engine, such as solar energy, and biomass such as agricultural waste and other waste such as domestic refuse. However, Stirling engines are generally not price-competitive as an automobile engine, because of high cost per unit power, low power density, and high material costs.\n\nBasic analysis is based on the closed-form Schmidt analysis.\n\n\n\n\nThe gas used should have a low heat capacity, so that a given amount of transferred heat leads to a large increase in pressure. Considering this issue, helium would be the best gas because of its very low heat capacity. Air is a viable working fluid, but the oxygen in a highly pressurized air engine can cause fatal accidents caused by lubricating oil explosions. Following one such accident Philips pioneered the use of other gases to avoid such risk of explosions.\n\nApplications of the Stirling engine range from heating and cooling to underwater power systems. A Stirling engine can function in reverse as a heat pump for heating or cooling. Other uses include combined heat and power, solar power generation, Stirling cryocoolers, heat pump, marine engines, low power aviation engines, and low temperature difference engines.\n\n"}
{"id": "1099759", "url": "https://en.wikipedia.org/wiki?curid=1099759", "title": "Strongly interacting massive particle", "text": "Strongly interacting massive particle\n\nStrongly interacting massive particles (SIMPs) are hypothetical particles that interact strongly between themselves [looks like they scatter off each other - observation of colliding galaxies in the Abell 3827 cluster, where it seemed that dark matter lagged behind the ordinary matter] and weakly with ordinary matter, but could form the inferred dark matter despite this. However, this finding has since been discounted based on further observations and modelling of the cluster.\n\nStrongly interacting massive particles have been proposed as a solution for the ultra-high-energy cosmic-ray problem and the absence of cooling flows in galactic clusters.\n\nVarious experiments and observations have set constraints on SIMP dark matter from 1990 onward.\n\nSIMPs annihilations would produce significant heat. DAMA set limits with NaI(Tl) crystals.\n\nMeasurements of Uranus's heat excess exclude SIMPS from 150 MeV to 10 GeV. Earth's heat flow significantly constrains any cross section.\n\n\n"}
{"id": "28825", "url": "https://en.wikipedia.org/wiki?curid=28825", "title": "Submarine", "text": "Submarine\n\nA submarine (or simply sub) is a watercraft capable of independent operation underwater. It differs from a submersible, which has more limited underwater capability. The term most commonly refers to a large, crewed vessel. It is also sometimes used historically or colloquially to refer to remotely operated vehicles and robots, as well as medium-sized or smaller vessels, such as the midget submarine and the wet sub. The noun \"submarine\" evolved as a shortened form of \"submarine boat\"; by naval tradition, submarines are usually referred to as \"boats\" rather than as \"ships\", regardless of their size (\"boat\" is usually reserved for seagoing vessels of relatively small size).\n\nAlthough experimental submarines had been built before, submarine design took off during the 19th century, and they were adopted by several navies. Submarines were first widely used during World War I (1914–1918), and now figure in many navies large and small. Military uses include attacking enemy surface ships (merchant and military), attacking other submarines, aircraft carrier protection, blockade running, ballistic missile submarines as part of a nuclear strike force, reconnaissance, conventional land attack (for example using a cruise missile), and covert insertion of special forces. Civilian uses for submarines include marine science, salvage, exploration and facility inspection and maintenance. Submarines can also be modified to perform more specialized functions such as search-and-rescue missions or undersea cable repair. Submarines are also used in tourism, and for undersea archaeology.\n\nMost large submarines consist of a cylindrical body with hemispherical (or conical) ends and a vertical structure, usually located amidships, which houses communications and sensing devices as well as periscopes. In modern submarines, this structure is the \"sail\" in American usage and \"fin\" in European usage. A \"conning tower\" was a feature of earlier designs: a separate pressure hull above the main body of the boat that allowed the use of shorter periscopes. There is a propeller (or pump jet) at the rear, and various hydrodynamic control fins. Smaller, deep-diving and specialty submarines may deviate significantly from this traditional layout. Submarines use diving planes and also change the amount of water and air in ballast tanks to change buoyancy for submerging and surfacing.\n\nSubmarines have one of the widest ranges of types and capabilities of any vessel. They range from small autonomous examples and one- or two-person vessels that operate for a few hours, to vessels that can remain submerged for six months—such as the Russian , the biggest submarines ever built. Submarines can work at greater depths than are survivable or practical for human divers. Modern deep-diving submarines derive from the bathyscaphe, which in turn evolved from the diving bell.\n\nAccording to a report in \"Opusculum Taisnieri\" published in 1562:\nIn 1578, the English mathematician William Bourne recorded in his book \"Inventions or Devises\" one of the first plans for an underwater navigation vehicle. A few years later the Scottish mathematician and theologian John Napier wrote in his \"Secret Inventions\" (1596) the following: \"These inventions besides devises of sayling under water with divers, other devises and strategems for harming of the enemyes by the Grace of God and worke of expert Craftsmen I hope to perform.\" It's unclear whether he ever carried out his idea.\n\nThe first submersible of whose construction there exists reliable information was designed and built in 1620 by Cornelis Drebbel, a Dutchman in the service of James I of England. It was propelled by means of oars.\n\nBy the mid-18th century, over a dozen patents for submarines/submersible boats had been granted in England. In 1747, Nathaniel Symons patented and built the first known working example of the use of a ballast tank for submersion. His design used leather bags that could fill with water to submerge the craft. A mechanism was used to twist the water out of the bags and cause the boat to resurface. In 1749, the Gentlemen's Magazine reported that a similar design had initially been proposed by Giovanni Borelli in 1680. By this point of development, further improvement in design necessarily stagnated for over a century, until new industrial technologies for propulsion and stability could be applied.\n\nThe first military submarine was the \"Turtle\" (1775), a hand-powered acorn-shaped device designed by the American David Bushnell to accommodate a single person. It was the first verified submarine capable of independent underwater operation and movement, and the first to use screws for propulsion.\n\nIn 1800, France built a human-powered submarine designed by American Robert Fulton, the . The French eventually gave up on the experiment in 1804, as did the British when they later considered Fulton's submarine design.\n\nIn 1864, late in the American Civil War, the Confederate navy's became the first military submarine to sink an enemy vessel, the Union sloop-of-war . In the aftermath of its successful attack against the ship, the \"Hunley\" also sank, possibly because it was too close to its own exploding torpedo.\n\nIn 1866, the \"Sub Marine Explorer\" was the first submarine to successfully dive, cruise underwater, and resurface under the control of the crew. The design by German American Julius H. Kroehl (in German, \"Kröhl\") incorporated elements that are still used in modern submarines.\n\nIn 1866, the \"Flach\" was built at the request of the Chilean government, by Karl Flach, a German engineer and immigrant. It was the fifth submarine built in the world and, along with a second submarine, was intended to defend the port of Valparaiso against attack by the Spanish navy during the Chincha Islands War.\n\nThe first submarine not relying on human power for propulsion was the French (\"Diver\"), launched in 1863, which used compressed air at 180 psi (1241 kPa). Narcís Monturiol designed the first air–independent and combustion–powered submarine, \"Ictineo II', which was launched in Barcelona, Spain in 1864.\n\nThe submarine became a potentially viable weapon with the development of the Whitehead torpedo, designed in 1866 by British engineer Robert Whitehead, the first practical self-propelled or 'locomotive' torpedo. The spar torpedo that had been developed earlier by the Confederate navy was considered to be impracticable, as it was believed to have sunk both its intended target, and probably \"H. L. Hunley\", the submarine that deployed it.\n\nDiscussions between the English clergyman and inventor George Garrett and the Swedish industrialist Thorsten Nordenfelt led to the first practical steam-powered submarines, armed with torpedoes and ready for military use. The first was \"Nordenfelt I\", a 56-tonne, vessel similar to Garrett's ill-fated \"Resurgam\" (1879), with a range of , armed with a single torpedo, in 1885.\n\nA reliable means of propulsion for the submerged vessel was only made possible in the 1880s with the advent of the necessary electric battery technology. The first electrically powered boats were built by Isaac Peral y Caballero in Spain, Dupuy de Lôme and Gustave Zédé in France, and James Franklin Waddington in England. Peral's design featured torpedoes and other systems that later became standard in submarines.\n\nSubmarines were not put into service for any widespread or routine use by navies until the early 1900s. This era marked a pivotal time in submarine development, and several important technologies appeared. A number of nations built and used submarines. Diesel electric propulsion became the dominant power system and equipment such as the periscope became standardized. Countries conducted many experiments on effective tactics and weapons for submarines, which led to their large impact in World War I.\n\nThe Irish inventor John Philip Holland built a model submarine in 1876 and a full-scale version in 1878, which were followed by a number of unsuccessful ones. In 1896 he designed the Holland Type VI submarine, which used internal combustion engine power on the surface and electric battery power underwater. Launched on 17 May 1897 at Navy Lt. Lewis Nixon's Crescent Shipyard in Elizabeth, New Jersey, \"Holland VI\" was purchased by the United States Navy on 11 April 1900, becoming the Navy's first commissioned submarine, christened .\n\nCommissioned in June 1900, the French steam and electric employed the now typical double-hull design, with a pressure hull inside the outer shell. These 200-ton ships had a range of over underwater. The French submarine \"Aigrette\" in 1904 further improved the concept by using a diesel rather than a gasoline engine for surface power. Large numbers of these submarines were built, with seventy-six completed before 1914.\n\nThe Royal Navy commissioned five s from Vickers, Barrow-in-Furness, under licence from the Holland Torpedo Boat Company from 1901 to 1903. Construction of the boats took longer than anticipated, with the first only ready for a diving trial at sea on 6 April 1902. Although the design had been purchased entirely from the US company, the actual design used was an untested improvement to the original Holland design using a new petrol engine.\n\nThese types of submarines were first used during the Russo-Japanese War of 1904–05. Due to the blockade at Port Arthur, the Russians sent their submarines to Vladivostok, where by 1 January 1905 there were seven boats, enough to create the world's first \"operational submarine fleet\". The new submarine fleet began patrols on 14 February, usually lasting for about 24 hours each. The first confrontation with Japanese warships occurred on 29 April 1905 when the Russian submarine \"Som\" was fired upon by Japanese torpedo boats, but then withdrew.\n\nMilitary submarines first made a significant impact in World War I. Forces such as the U-boats of Germany saw action in the First Battle of the Atlantic, and were responsible for sinking , which was sunk as a result of unrestricted submarine warfare and is often cited among the reasons for the entry of the United States into the war.\n\nAt the outbreak of war Germany had only twenty submarines immediately available for combat, although these included vessels of the diesel-engined \"U-19\" class with the range (5,000 miles) and speed () to operate effectively around the entire British coast. By contrast the Royal Navy had a total of 74 submarines, though of mixed effectiveness. In August 1914, a flotilla of ten U-boats sailed from their base in Heligoland to attack Royal Navy warships in the North Sea in the first submarine war patrol in history.\n\nThe U-boats' ability to function as practical war machines relied on new tactics, their numbers, and submarine technologies such as combination diesel-electric power system developed in the preceding years. More submersibles than true submarines, U-boats operated primarily on the surface using regular engines, submerging occasionally to attack under battery power. They were roughly triangular in cross-section, with a distinct keel to control rolling while surfaced, and a distinct bow. During World War I more than 5,000 Allied ships were sunk by U-boats.\n\nThis British tried to catch up to the Germans in terms of submarine technology with the creation of the K-class submarines. However, these were extremely large and often collided with each other forcing the British to scrap the K-class design shortly after the war.\n\nDuring World War II, Germany used submarines to devastating effect in the Battle of the Atlantic, where it attempted to cut Britain's supply routes by sinking more merchant ships than Britain could replace. (Shipping was vital to supply Britain's population with food, industry with raw material, and armed forces with fuel and armaments.) While U-boats destroyed a significant number of ships, the strategy ultimately failed. Although the U-boats had been updated in the interwar years, the major innovation was improved communications, encrypted using the famous Enigma cipher machine. This allowed for mass-attack naval tactics (\"Rudeltaktik\", commonly known as \"wolfpack\"), but was also ultimately the U-boats' downfall. By the end of the war, almost 3,000 Allied ships (175 warships, 2,825 merchantmen) had been sunk by U-boats. Although successful early in the war, ultimately the U-boat fleet suffered a casualty rate of 73%, almost all fatalities. Germany's U-boat fleet in World War II suffered heavy casualties, losing 793 U-boats and about 28,000 submariners, out of 41,000; a casualty rate of about 70%.\n\nThe Imperial Japanese Navy operated the most varied fleet of submarines of any navy, including \"Kaiten\" crewed torpedoes, midget submarines ( and es), medium-range submarines, purpose-built supply submarines and long-range fleet submarines. They also had submarines with the highest submerged speeds during World War II (s) and submarines that could carry multiple aircraft (s). They were also equipped with one of the most advanced torpedoes of the conflict, the oxygen-propelled Type 95. Nevertheless, despite their technical prowess, Japan chose to use its submarines for fleet warfare, and consequently were relatively unsuccessful, as warships were fast, maneuverable and well-defended compared to merchant ships.\n\nThe submarine force was the most effective anti-ship weapon in the American arsenal. Submarines, though only about 2 percent of the U.S. Navy, destroyed over 30 percent of the Japanese Navy, including 8 aircraft carriers, 1 battleship and 11 cruisers. US submarines also destroyed over 60 percent of the Japanese merchant fleet, crippling Japan's ability to supply its military forces and industrial war effort. Allied submarines in the Pacific War destroyed more Japanese shipping than all other weapons combined. This feat was considerably aided by the Imperial Japanese Navy's failure to provide adequate escort forces for the nation's merchant fleet.\n\nDuring World War II, 314 submarines served in the US Navy, of which nearly 260 were deployed to the Pacific. When the Japanese attacked Hawaii in December 1941, 111 boats were in commission; 203 submarines from the , , and es were commissioned during the war. During the war, 52 US submarines were lost to all causes, with 48 directly due to hostilities. US submarines sank 1,560 enemy vessels, a total tonnage of 5.3 million tons (55% of the total sunk).\n\nThe Royal Navy Submarine Service was used primarily in the classic Axis blockade. Its major operating areas were around Norway, in the Mediterranean (against the Axis supply routes to North Africa), and in the Far East. In that war, British submarines sank 2 million tons of enemy shipping and 57 major warships, the latter including 35 submarines. Among these is the only documented instance of a submarine sinking another submarine while both were submerged. This occurred when engaged ; the \"Venturer\" crew manually computed a successful firing solution against a three-dimensionally maneuvering target using techniques which became the basis of modern torpedo computer targeting systems. Seventy-four British submarines were lost, the majority, forty-two, in the Mediterranean.\n\nThe first launch of a cruise missile (SSM-N-8 Regulus) from a submarine occurred in July 1953, from the deck of , a World War II fleet boat modified to carry the missile with a nuclear warhead. \"Tunny\" and its sister boat, , were the United States' first nuclear deterrent patrol submarines. In the 1950s, nuclear power partially replaced diesel-electric propulsion. Equipment was also developed to extract oxygen from sea water. These two innovations gave submarines the ability to remain submerged for weeks or months. Most of the naval submarines built since that time in the US, the Soviet Union/Russian Federation, Britain, and France have been powered by nuclear reactors.\n\nIn 1959–1960, the first ballistic missile submarines were put into service by both the United States () and the Soviet Union () as part of the Cold War nuclear deterrent strategy.\n\nDuring the Cold War, the US and the Soviet Union maintained large submarine fleets that engaged in cat-and-mouse games. The Soviet Union lost at least four submarines during this period: was lost in 1968 (a part of which the CIA retrieved from the ocean floor with the Howard Hughes-designed ship \"Glomar Explorer\"), in 1970, in 1986, and in 1989 (which held a depth record among military submarines—). Many other Soviet subs, such as (the first Soviet nuclear submarine, and the first Soviet sub to reach the North Pole) were badly damaged by fire or radiation leaks. The US lost two nuclear submarines during this time: due to equipment failure during a test dive while at its operational limit, and due to unknown causes.\n\nDuring India's intervention in the Bangladesh Liberation War, the Pakistan Navy's sank the Indian frigate . This was the first sinking by a submarine since World War II. During the same war, the , a \"Tench\"-class submarine on loan to Pakistan from the US, was sunk by the Indian Navy. It was the first submarine combat loss since World War II. In 1982 during the Falklands War, the Argentine cruiser was sunk by the British submarine , the first sinking by a nuclear-powered submarine in war.\n\nBefore and during World War II, the primary role of the submarine was anti-surface ship warfare. Submarines would attack either on the surface, using deck guns or submerged, using torpedoes. They were particularly effective in sinking Allied transatlantic shipping in both World Wars, and in disrupting Japanese supply routes and naval operations in the Pacific in World War II.\n\nMine-laying submarines were developed in the early part of the 20th century. The facility was used in both World Wars. Submarines were also used for inserting and removing covert agents and military forces, for intelligence gathering, and to rescue aircrew during air attacks on islands, where the airmen would be told of safe places to crash-land so the submarines could rescue them. Submarines could carry cargo through hostile waters or act as supply vessels for other submarines.\n\nSubmarines could usually locate and attack other submarines only on the surface, although managed to sink with a four torpedo spread while both were submerged. The British developed a specialized anti-submarine submarine in WWI, the R class. After WWII, with the development of the homing torpedo, better sonar systems, and nuclear propulsion, submarines also became able to hunt each other effectively.\n\nThe development of submarine-launched ballistic missile and submarine-launched cruise missiles gave submarines a substantial and long-ranged ability to attack both land and sea targets with a variety of weapons ranging from cluster bombs to nuclear weapons.\n\nThe primary defense of a submarine lies in its ability to remain concealed in the depths of the ocean. Early submarines could be detected by the sound they made. Water is an excellent conductor of sound (much better than air), and submarines can detect and track comparatively noisy surface ships from long distances. Modern submarines are built with an emphasis on stealth. Advanced propeller designs, extensive sound-reducing insulation, and special machinery help a submarine remain as quiet as ambient ocean noise, making them difficult to detect. It takes specialized technology to find and attack modern submarines.\n\nActive sonar uses the reflection of sound emitted from the search equipment to detect submarines. It has been used since WWII by surface ships, submarines and aircraft (via dropped buoys and helicopter \"dipping\" arrays), but it reveals the emitter's position, and is susceptible to counter-measures.\n\nA concealed military submarine is a real threat, and because of its stealth, can force an enemy navy to waste resources searching large areas of ocean and protecting ships against attack. This advantage was vividly demonstrated in the 1982 Falklands War when the British nuclear-powered submarine sank the Argentine cruiser . After the sinking the Argentine Navy recognized that they had no effective defense against submarine attack, and the Argentine surface fleet withdrew to port for the remainder of the war, though an Argentine submarine remained at sea.\n\nAlthough the majority of the world's submarines are military, there are some civilian submarines, which are used for tourism, exploration, oil and gas platform inspections, and pipeline surveys. Some are also used in illegal activities.\n\nThe Submarine Voyage ride opened at Disneyland in 1959, but although it ran under water it was not a true submarine, as it ran on tracks and was open to the atmosphere. The first tourist submarine was , which went into service in 1964 at Expo64. By 1997 there were 45 tourist submarines operating around the world. Submarines with a crush depth in the range of are operated in several areas worldwide, typically with bottom depths around , with a carrying capacity of 50 to 100 passengers.\n\nIn a typical operation a surface vessel carries passengers to an offshore operating area and loads them into the submarine. The submarine then visits underwater points of interest such as natural or artificial reef structures. To surface safely without danger of collision the location of the submarine is marked with an air release and movement to the surface is coordinated by an observer in a support craft.\n\nA recent development is the deployment of so-called narco submarines by South American drug smugglers to evade law enforcement detection. Although they occasionally deploy true submarines, most are self-propelled semi-submersibles, where a portion of the craft remains above water at all times. In September 2011, Colombian authorities seized a 16-meter-long submersible that could hold a crew of 5, costing about $2 million. The vessel belonged to FARC rebels and had the capacity to carry at least 7 tonnes of drugs.\n\nAll surface ships, as well as surfaced submarines, are in a positively buoyant condition, weighing less than the volume of water they would displace if fully submerged. To submerge hydrostatically, a ship must have negative buoyancy, either by increasing its own weight or decreasing its displacement of water. To control their displacement, submarines have ballast tanks, which can hold varying amounts of water and air.\n\nFor general submersion or surfacing, submarines use the forward and aft tanks, called Main Ballast Tanks (MBT), which are filled with water to submerge or with air to surface. Submerged, MBTs generally remain flooded, which simplifies their design, and on many submarines these tanks are a section of interhull space. For more precise and quick control of depth, submarines use smaller Depth Control Tanks (DCT) – also called hard tanks (due to their ability to withstand higher pressure), or trim tanks. The amount of water in depth control tanks can be controlled to change depth or to maintain a constant depth as outside conditions (chiefly water density) change. Depth control tanks may be located either near the submarine's center of gravity, or separated along the submarine body to prevent affecting trim.\n\nWhen submerged, the water pressure on a submarine's hull can reach for steel submarines and up to for titanium submarines like , while interior pressure remains relatively unchanged. This difference results in hull compression, which decreases displacement. Water density also marginally increases with depth, as the salinity and pressure are higher. This change in density incompletely compensates for hull compression, so buoyancy decreases as depth increases. A submerged submarine is in an unstable equilibrium, having a tendency to either sink or float to the surface. Keeping a constant depth requires continual operation of either the depth control tanks or control surfaces.\n\nSubmarines in a neutral buoyancy condition are not intrinsically trim-stable. To maintain desired trim, submarines use forward and aft trim tanks. Pumps can move water between the tanks, changing weight distribution and pointing the sub up or down. A similar system is sometimes used to maintain stability.\nThe hydrostatic effect of variable ballast tanks is not the only way to control the submarine underwater. Hydrodynamic maneuvering is done by several surfaces, which can be moved to create hydrodynamic forces when a submarine moves at sufficient speed. The stern planes, located near the propeller and normally horizontal, serve the same purpose as the trim tanks, controlling the trim, and are commonly used, while other control surfaces may not be present on all submarines. The fairwater planes on the sail and/or bow planes on the main body, both also horizontal, are closer to the center of gravity, and are used to control depth with less effect on the trim.\n\nWhen a submarine performs an emergency surfacing, all depth and trim methods are used simultaneously, together with propelling the boat upwards. Such surfacing is very quick, so the sub may even partially jump out of the water, potentially damaging submarine systems.\n\nModern submarines are cigar-shaped. This design, visible in early submarines, is sometimes called a \"teardrop hull\". It reduces the hydrodynamic drag when submerged, but decreases the sea-keeping capabilities and increases drag while surfaced. Since the limitations of the propulsion systems of early submarines forced them to operate surfaced most of the time, their hull designs were a compromise. Because of the slow submerged speeds of those subs, usually well below 10 kt (18 km/h), the increased drag for underwater travel was acceptable. Late in World War II, when technology allowed faster and longer submerged operation and increased aircraft surveillance forced submarines to stay submerged, hull designs became teardrop shaped again to reduce drag and noise. was a unique research submarine that pioneered the American version of the teardrop hull form (sometimes referred to as an \"Albacore hull\") of modern submarines. On modern military submarines the outer hull is covered with a layer of sound-absorbing rubber, or anechoic plating, to reduce detection.\n\nThe occupied pressure hulls of deep diving submarines such as are spherical instead of cylindrical. This allows a more even distribution of stress at the great depth. A titanium frame is usually affixed to the pressure hull, providing attachment for ballast and trim systems, scientific instrumentation, battery packs, syntactic flotation foam, and lighting.\n\nA raised tower on top of a submarine accommodates the periscope and electronics masts, which can include radio, radar, electronic warfare, and other systems including the snorkel mast. In many early classes of submarines (see history), the control room, or \"conn\", was located inside this tower, which was known as the \"conning tower\". Since then, the conn has been located within the hull of the submarine, and the tower is now called the \"sail\". The conn is distinct from the \"bridge\", a small open platform in the top of the sail, used for observation during surface operation.\n\n\"Bathtubs\" are related to conning towers but are used on smaller submarines. The bathtub is a metal cylinder surrounding the hatch that prevents waves from breaking directly into the cabin. It is needed because surfaced submarines have limited freeboard, that is, they lie low in the water. Bathtubs help prevent swamping the vessel.\nModern submarines and submersibles, as well as the oldest ones, usually have a single hull. Large submarines generally have an additional hull or hull sections outside. This external hull, which actually forms the shape of submarine, is called the outer hull (\"casing\" in the Royal Navy) or light hull, as it does not have to withstand a pressure difference. Inside the outer hull there is a strong hull, or pressure hull, which withstands sea pressure and has normal atmospheric pressure inside.\n\nAs early as World War I, it was realized that the optimal shape for withstanding pressure conflicted with the optimal shape for seakeeping and minimal drag, and construction difficulties further complicated the problem. This was solved either by a compromise shape, or by using two hulls; internal for holding pressure, and external for optimal shape. Until the end of World War II, most submarines had an additional partial cover on the top, bow and stern, built of thinner metal, which was flooded when submerged. Germany went further with the Type XXI, a general predecessor of modern submarines, in which the pressure hull was fully enclosed inside the light hull, but optimized for submerged navigation, unlike earlier designs that were optimized for surface operation.\nAfter World War II, approaches split. The Soviet Union changed its designs, basing them on German developments. All post–World War II heavy Soviet and Russian submarines are built with a double hull structure. American and most other Western submarines switched to a primarily single-hull approach. They still have light hull sections in the bow and stern, which house main ballast tanks and provide a hydrodynamically optimized shape, but the main cylindrical hull section has only a single plating layer. Double hulls are being considered for future submarines in the United States to improve payload capacity, stealth and range.\n\nThe pressure hull is generally constructed of thick high-strength steel with a complex structure and high strength reserve, and is separated with watertight bulkheads into several compartments. There are also examples of more than two hulls in a submarine, like the , which has two main pressure hulls and three smaller ones for control room, torpedoes and steering gear, with the missile launch system between the main hulls.\n\nThe dive depth cannot be increased easily. Simply making the hull thicker increases the weight and requires reduction of onboard equipment weight, ultimately resulting in a \"bathyscaphe\". This is acceptable for civilian research submersibles, but not military submarines.\n\nWWI submarines had hulls of carbon steel, with a maximum depth. During WWII, high-strength alloyed steel was introduced, allowing depths. High-strength alloy steel remains the primary material for submarines today, with depths, which cannot be exceeded on a military submarine without design compromises. To exceed that limit, a few submarines were built with titanium hulls. Titanium can be stronger than steel, lighter, and is not ferromagnetic, important for stealth. Titanium submarines were built by the Soviet Union, which developed specialized high-strength alloys. It has produced several types of titanium submarines. Titanium alloys allow a major increase in depth, but other systems must be redesigned to cope, so test depth was limited to for the , the deepest-diving combat submarine. An may have successfully operated at , though continuous operation at such depths would produce excessive stress on many submarine systems. Titanium does not flex as readily as steel, and may become brittle after many dive cycles. Despite its benefits, the high cost of titanium construction led to the abandonment of titanium submarine construction as the Cold War ended. Deep–diving civilian submarines have used thick acrylic pressure hulls.\n\nThe deepest deep-submergence vehicle (DSV) to date is \"Trieste\". On 5 October 1959, \"Trieste\" departed San Diego for Guam aboard the freighter \"Santa Maria\" to participate in \"Project Nekton\", a series of very deep dives in the Mariana Trench. On 23 January 1960, \"Trieste\" reached the ocean floor in the Challenger Deep (the deepest southern part of the Mariana Trench), carrying Jacques Piccard (son of Auguste) and Lieutenant Don Walsh, USN. This was the first time a vessel, manned or unmanned, had reached the deepest point in the Earth's oceans. The onboard systems indicated a depth of , although this was later revised to and more accurate measurements made in 1995 have found the Challenger Deep slightly shallower, at .\n\nBuilding a pressure hull is difficult, as it must withstand pressures at its required diving depth. When the hull is perfectly round in cross-section, the pressure is evenly distributed, and causes only hull compression. If the shape is not perfect, the hull is bent, with several points heavily strained. Inevitable minor deviations are resisted by stiffener rings, but even a one-inch (25 mm) deviation from roundness results in over 30 percent decrease of maximal hydrostatic load and consequently dive depth. The hull must therefore be constructed with high precision. All hull parts must be welded without defects, and all joints are checked multiple times with different methods, contributing to the high cost of modern submarines. (For example, each attack submarine costs US$2.6 billion, over US$200,000 per ton of displacement.)\n\nThe first submarines were propelled by humans. The first mechanically driven submarine was the 1863 French , which used compressed air for propulsion. Anaerobic propulsion was first employed by the Spanish \"Ictineo II\" in 1864, which used a solution of zinc, manganese dioxide, and potassium chlorate to generate sufficient heat to power a steam engine, while also providing oxygen for the crew. A similar system was not employed again until 1940 when the German Navy tested a hydrogen peroxide-based system, the Walter turbine, on the experimental V-80 submarine and later on the naval and type XVII submarines.\n\nUntil the advent of nuclear marine propulsion, most 20th-century submarines used batteries for running underwater and gasoline (petrol) or diesel engines on the surface, and for battery recharging. Early submarines used gasoline, but this quickly gave way to kerosene (paraffin), then diesel, because of reduced flammability. Diesel-electric became the standard means of propulsion. The diesel or gasoline engine and the electric motor, separated by clutches, were initially on the same shaft driving the propeller. This allowed the engine to drive the electric motor as a generator to recharge the batteries and also propel the submarine. The clutch between the motor and the engine would be disengaged when the submarine dived, so that the motor could drive the propeller. The motor could have multiple armatures on the shaft, which could be electrically coupled in series for slow speed and in parallel for high speed (these connections were called \"group down\" and \"group up\", respectively).\n\nEarly submarines used a direct mechanical connection between the engine and propeller, switching between diesel engines for surface running, and battery-driven electric motors for submerged propulsion.\n\nIn 1928, the United States Navy's Bureau of Engineering proposed a diesel-electric transmission. Instead of driving the propeller directly while running on the surface, the submarine's diesel drove a generator that could either charge the submarine's batteries or drive the electric motor. This made electric motor speed independent of diesel engine speed, so the diesel could run at an optimum and non-critical speed. One or more diesel engines could be shut down for maintenance while the submarine continued to run on the remaining engine or battery power. The US pioneered this concept in 1929, in the S-class submarines , , and . The first production submarines with this system were the \"Porpoise\" class of the 1930s, and it was used on most subsequent US diesel submarines through the 1960s. No other navy adopted the system before 1945, apart from the Royal Navy's U-class submarines, though some submarines of the Imperial Japanese Navy used separate diesel generators for low speed running.\n\nOther advantages of such an arrangement were that a submarine could travel slowly with the engines at full power to recharge the batteries quickly, reducing time on the surface or on snorkel. It was then possible to isolate the noisy diesel engines from the pressure hull, making the submarine quieter. Additionally, diesel-electric transmissions were more compact.\n\nDuring World War II the Germans experimented with the idea of the \"schnorchel\" (snorkel) from captured Dutch submarines, but didn't see the need for them until rather late in the war. The \"schnorchel\" was a retractable pipe that supplied air to the diesel engines while submerged at periscope depth, allowing the boats to cruise and recharge their batteries while maintaining a degree of stealth. It was far from a perfect solution, however. There were problems with the device's valve sticking shut or closing as it dunked in rough weather; since the system used the entire pressure hull as a buffer, the diesels would instantaneously suck huge volumes of air from the boat's compartments, and the crew often suffered painful ear injuries. Speed was limited to , lest the device snap from stress. The \"schnorchel\" also created noise that made the boat easier to detect with sonar, yet more difficult for the on-board sonar to detect signals from other vessels. Finally, Allied radar eventually became sufficiently advanced that the \"schnorchel\" mast could be detected beyond visual range.\n\nWhile the snorkel renders a submarine far less detectable, it is not perfect. In clear weather, diesel exhaust can be seen on the surface to a distance of about three miles, while \"periscope feather\" (the wave created by the snorkel or periscope moving through the water) is visible from far off in calm sea conditions. Modern radar is also capable of detecting a snorkel in calm sea conditions.\n\nThe problem of the diesels causing a vacuum in the submarine when the head valve is submerged still exists in later model diesel submarines, but is mitigated by high-vacuum cut-off sensors that shut down the engines when the vacuum in the ship reaches a pre-set point. Modern snorkel induction masts use a fail-safe design using compressed air, controlled by a simple electrical circuit, to hold the \"head valve\" open against the pull of a powerful spring. Seawater washing over the mast shorts out exposed electrodes on top, breaking the control, and shutting the \"head valve\" while it is submerged. US submarines did not adopt the use of snorkels until after WWII.\n\nOne new technology that is being introduced starting with the Japanese Navy's eleventh \"Sōryū\"-class submarine (JS \"Ōryū\") is a more modern battery, the lithium-ion battery. These batteries have about double the electric storage of traditional batteries, and by changing out the lead-acid batteries in their normal storage areas plus filling up the large hull space normally devoted to AIP engine and fuel tanks with many tons of lithium-ion batteries, modern submarines can actually return to a \"pure\" diesel-electric configuration yet have the added underwater range and power normally associated with AIP equipped submarines.\n\nDuring World War II, German Type XXI submarines (also known as \"Elektroboote\") were the first submarines designed to operate submerged for extended periods. Initially they were to carry hydrogen peroxide for long-term, fast air-independent propulsion, but were ultimately built with very large batteries instead. At the end of the War, the British and Soviets experimented with hydrogen peroxide/kerosene (paraffin) engines that could run surfaced and submerged. The results were not encouraging. Though the Soviet Union deployed a class of submarines with this engine type (codenamed by NATO), they were considered unsuccessful.\nThe United States also used hydrogen peroxide in an experimental midget submarine, X-1. It was originally powered by a hydrogen peroxide/diesel engine and battery system until an explosion of her hydrogen peroxide supply on 20 May 1957. X-1 was later converted to use diesel-electric drive.\n\nToday several navies use air-independent propulsion. Notably Sweden uses Stirling technology on the and s. The Stirling engine is heated by burning diesel fuel with liquid oxygen from cryogenic tanks. A newer development in air-independent propulsion is hydrogen fuel cells, first used on the German Type 212 submarine, with nine 34 kW or two 120 kW cells and soon to be used in the new Spanish s.\n\nSteam power was resurrected in the 1950s with a nuclear-powered steam turbine driving a generator. By eliminating the need for atmospheric oxygen, the time that a submarine could remain submerged was limited only by its food stores, as breathing air was recycled and fresh water distilled from seawater. More importantly, a nuclear submarine has unlimited range at top speed. This allows it to travel from its operating base to the combat zone in a much shorter time and makes it a far more difficult target for most anti-submarine weapons. Nuclear-powered submarines have a relatively small battery and diesel engine/generator powerplant for emergency use if the reactors must be shut down.\nNuclear power is now used in all large submarines, but due to the high cost and large size of nuclear reactors, smaller submarines still use diesel-electric propulsion. The ratio of larger to smaller submarines depends on strategic needs. The US Navy, French Navy, and the British Royal Navy operate only nuclear submarines, which is explained by the need for distant operations. Other major operators rely on a mix of nuclear submarines for strategic purposes and diesel-electric submarines for defense. Most fleets have no nuclear submarines, due to the limited availability of nuclear power and submarine technology.\n\nDiesel-electric submarines have a stealth advantage over their nuclear counterparts. Nuclear submarines generate noise from coolant pumps and turbo-machinery needed to operate the reactor, even at low power levels. Some nuclear submarines such as the American can operate with their reactor coolant pumps secured, making them quieter than electric subs. A conventional submarine operating on batteries is almost completely silent, the only noise coming from the shaft bearings, propeller, and flow noise around the hull, all of which stops when the sub hovers in mid-water to listen, leaving only the noise from crew activity. Commercial submarines usually rely only on batteries, since they operate in conjunction with a mother ship.\n\nSeveral serious nuclear and radiation accidents have involved nuclear submarine mishaps. The reactor accident in 1961 resulted in 8 deaths and more than 30 other people were over-exposed to radiation. The reactor accident in 1968 resulted in 9 fatalities and 83 other injuries. The accident in 1985 resulted in 10 fatalities and 49 other radiation injuries.\n\nOil-fired steam turbines powered the British K-class submarines, built during World War I and later, to give them the surface speed to keep up with the battle fleet. The K-class subs were not very successful, however.\n\nToward the end of the 20th century, some submarines—such as the British \"Vanguard\" class—began to be fitted with pump-jet propulsors instead of propellers. Though these are heavier, more expensive, and less efficient than a propeller, they are significantly quieter, providing an important tactical advantage.\n\nMagnetohydrodynamic drive (MHD) was portrayed as the operating principle behind the titular submarine's nearly silent propulsion system in the film adaptation of \"The Hunt for Red October\". However, in the novel the \"Red October\" did not use MHD, but rather something more similar to the above-mentioned pump-jet.\n\nThe success of the submarine is inextricably linked to the development of the torpedo, invented by Robert Whitehead in 1866. His invention is essentially the same now as it was 140 years ago. Only with self-propelled torpedoes could the submarine make the leap from novelty to a weapon of war. Until the perfection of the guided torpedo, multiple \"straight-running\" torpedoes were required to attack a target. With at most 20 to 25 torpedoes stored on board, the number of attacks was limited. To increase combat endurance most World War I submarines functioned as submersible gunboats, using their deck guns against unarmed targets, and diving to escape and engage enemy warships. The importance of guns encouraged the development of the unsuccessful Submarine Cruiser such as the French and the Royal Navy's and M-class submarines. With the arrival of Anti-submarine warfare (ASW) aircraft, guns became more for defense than attack. A more practical method of increasing combat endurance was the external torpedo tube, loaded only in port.\nThe ability of submarines to approach enemy harbours covertly led to their use as minelayers. Minelaying submarines of World War I and World War II were specially built for that purpose. Modern submarine-laid mines, such as the British Mark 5 Stonefish and Mark 6 Sea Urchin, can be deployed from a submarine's torpedo tubes.\n\nAfter World War II, both the US and the USSR experimented with submarine-launched cruise missiles such as the SSM-N-8 Regulus and P-5 Pyatyorka. Such missiles required the submarine to surface to fire its missiles. They were the forerunners of modern submarine-launched cruise missiles, which can be fired from the torpedo tubes of submerged submarines, for example the US BGM-109 Tomahawk and Russian RPK-2 Viyuga and versions of surface–to–surface anti-ship missiles such as the Exocet and Harpoon, encapsulated for submarine launch. Ballistic missiles can also be fired from a submarine's torpedo tubes, for example missiles such as the anti-submarine SUBROC. With internal volume as limited as ever and the desire to carry heavier warloads, the idea of the external launch tube was revived, usually for encapsulated missiles, with such tubes being placed between the internal pressure and outer streamlined hulls.\n\nThe strategic mission of the SSM-N-8 and the P-5 was taken up by submarine-launched ballistic missile beginning with the US Navy's Polaris missile, and subsequently the Poseidon and Trident missiles.\n\nGermany is working on the torpedo tube-launched short-range IDAS missile, which can be used against ASW helicopters, as well as surface ships and coastal targets.\n\nA submarine can have a variety of sensors, depending on its missions. Modern military submarines rely almost entirely on a suite of passive and active sonars to locate targets. Active sonar relies on an audible \"ping\" to generate echoes to reveal objects around the submarine. Active systems are rarely used, as doing so reveals the sub's presence. Passive sonar is a set of sensitive hydrophones set into the hull or trailed in a towed array, normally trailing several hundred feet behind the sub. The towed array is the mainstay of NATO submarine detection systems, as it reduces the flow noise heard by operators. Hull mounted sonar is employed in addition to the towed array, as the towed array can't work in shallow depth and during maneuvering. In addition, sonar has a blind spot \"through\" the submarine, so a system on both the front and back works to eliminate that problem. As the towed array trails behind and below the submarine, it also allows the submarine to have a system both above and below the thermocline at the proper depth; sound passing through the thermocline is distorted resulting in a lower detection range.\n\nSubmarines also carry radar equipment to detect surface ships and aircraft. Submarine captains are more likely to use radar detection gear than active radar to detect targets, as radar can be detected far beyond its own return range, revealing the submarine. Periscopes are rarely used, except for position fixes and to verify a contact's identity.\n\nCivilian submarines, such as the or the Russian \"Mir\" submersibles, rely on small active sonar sets and viewing ports to navigate. The human eye cannot detect sunlight below about underwater, so high intensity lights are used to illuminate the viewing area.\n\nEarly submarines had few navigation aids, but modern subs have a variety of navigation systems. Modern military submarines use an inertial guidance system for navigation while submerged, but drift error unavoidably builds over time. To counter this, the crew occasionally uses the Global Positioning System to obtain an accurate position. The periscope—a retractable tube with a prism system that provides a view of the surface—is only used occasionally in modern submarines, since the visibility range is short. The and s use photonics masts rather than hull-penetrating optical periscopes. These masts must still be deployed above the surface, and use electronic sensors for visible light, infrared, laser range-finding, and electromagnetic surveillance. One benefit to hoisting the mast above the surface is that while the mast is above the water the entire sub is still below the water and is much harder to detect visually or by radar.\n\nMilitary submarines use several systems to communicate with distant command centers or other ships. One is VLF (very low frequency) radio, which can reach a submarine either on the surface or submerged to a fairly shallow depth, usually less than . ELF (extremely low frequency) can reach a submarine at greater depths, but has a very low bandwidth and is generally used to call a submerged sub to a shallower depth where VLF signals can reach. A submarine also has the option of floating a long, buoyant wire antenna to a shallower depth, allowing VLF transmissions by a deeply submerged boat.\n\nBy extending a radio mast, a submarine can also use a \"burst transmission\" technique. A burst transmission takes only a fraction of a second, minimizing a submarine's risk of detection.\n\nTo communicate with other submarines, a system known as Gertrude is used. Gertrude is basically a sonar telephone. Voice communication from one submarine is transmitted by low power speakers into the water, where it is detected by passive sonars on the receiving submarine. The range of this system is probably very short, and using it radiates sound into the water, which can be heard by the enemy.\n\nCivilian submarines can use similar, albeit less powerful systems to communicate with support ships or other submersibles in the area.\n\nWith nuclear power or air-independent propulsion, submarines can remain submerged for months at a time. Conventional diesel submarines must periodically resurface or run on snorkel to recharge their batteries. Most modern military submarines generate breathing oxygen by electrolysis of water (using a device called an \"Elektrolytic Oxygen Generator\"). Atmosphere control equipment includes a CO scrubber, which uses an amine absorbent to remove the gas from air and diffuse it into waste pumped overboard. A machine that uses a catalyst to convert carbon monoxide into carbon dioxide (removed by the CO scrubber) and bonds hydrogen produced from the ship's storage battery with oxygen in the atmosphere to produce water, is also used. An atmosphere monitoring system samples the air from different areas of the ship for nitrogen, oxygen, hydrogen, R-12 and R-114 refrigerants, carbon dioxide, carbon monoxide, and other gases. Poisonous gases are removed, and oxygen is replenished by use of an oxygen bank located in a main ballast tank. Some heavier submarines have two oxygen bleed stations (forward and aft). The oxygen in the air is sometimes kept a few percent less than atmospheric concentration to reduce fire danger.\n\nFresh water is produced by either an evaporator or a reverse osmosis unit. The primary use for fresh water is to provide feedwater for the reactor and steam propulsion plants. It is also available for showers, sinks, cooking and cleaning once propulsion plant needs have been met. Seawater is used to flush toilets, and the resulting \"black water\" is stored in a sanitary tank until it is blown overboard using pressurized air or pumped overboard by using a special sanitary pump. The blackwater–discharge system is difficult to operate, and the German Type VIIC boat was lost with casualties because of human error while using this system. Water from showers and sinks is stored separately in \"grey water\" tanks and discharged overboard using drain pumps.\n\nTrash on modern large submarines is usually disposed of using a tube called a Trash Disposal Unit (TDU), where it is compacted into a galvanized steel can. At the bottom of the TDU is a large ball valve. An ice plug is set on top of the ball valve to protect it, the cans atop the ice plug. The top breech door is shut, and the TDU is flooded and equalized with sea pressure, the ball valve is opened and the cans fall out assisted by scrap iron weights in the cans. The TDU is also flushed with seawater to ensure it is completely empty and the ball valve is clear before closing the valve.\n\nA typical nuclear submarine has a crew of over 80; conventional boats typically have fewer than 40. The conditions on a submarine can be difficult because crew members must work in isolation for long periods of time, without family contact. Submarines normally maintain radio silence to avoid detection. Operating a submarine is dangerous, even in peacetime, and many submarines have been lost in accidents.\n\nMost navies prohibited women from serving on submarines, even after they had been permitted to serve on surface warships. The Royal Norwegian Navy became the first navy to allow women on its submarine crews in 1985. The Royal Danish Navy allowed female submariners in 1988. Others followed suit including the Swedish Navy (1989), the Royal Australian Navy (1998), the Spanish Navy (1999), the German Navy (2001) and the Canadian Navy (2002). In 1995, Solveig Krey of the Royal Norwegian Navy became the first female officer to assume command on a military submarine, HNoMS \"Kobben\".\n\nOn 8 December 2011, British Defence Secretary Philip Hammond announced that the UK's ban on women in submarines was to be lifted from 2013. Previously there were fears that women were more at risk from a build-up of carbon dioxide in the submarine. But a study showed no medical reason to exclude women, though pregnant women would still be excluded. Similar dangers to the pregnant woman and her fetus barred women from submarine service in Sweden in 1983, when all other positions were made available for them in the Swedish Navy. Today, pregnant women are still not allowed to serve on submarines in Sweden. However, the policymakers thought that it was discriminatory with a general ban and demanded that women should be tried on their individual merits and have their suitability evaluated and compared to other candidates. Further, they noted that a woman complying with such high demands is unlikely to become pregnant. In May 2014, three women became the RN's first female submariners.\n\nWomen have served on US Navy surface ships since 1993, and , began serving on submarines for the first time. Until presently, the Navy allowed only three exceptions to women being on board military submarines: female civilian technicians for a few days at most, women midshipmen on an overnight during summer training for Navy ROTC and Naval Academy, and family members for one-day dependent cruises. In 2009, senior officials, including then-Secretary of the Navy Ray Mabus, Joint Chief of Staff Admiral Michael Mullen, and Chief of Naval Operations Admiral Gary Roughead, began the process of finding a way to implement women on submarines. The US Navy rescinded its \"no women on subs\" policy in 2010.\n\nBoth the US and British navies operate nuclear-powered submarines that deploy for periods of six months or longer. Other navies that permit women to serve on submarines operate conventionally powered submarines, which deploy for much shorter periods—usually only for a few months. Prior to the change by the US, no nation using nuclear submarines permitted women to serve on board.\n\nIn 2011, the first class of female submarine officers graduated from Naval Submarine School's Submarine Officer Basic Course (SOBC) at the Naval Submarine Base New London. Additionally, more senior ranking and experienced female supply officers from the surface warfare specialty attended SOBC as well, proceeding to fleet Ballistic Missile (SSBN) and Guided Missile (SSGN) submarines along with the new female submarine line officers beginning in late 2011. By late 2011, several women were assigned to the \"Ohio\"-class ballistic missile submarine . On 15 October 2013, the US Navy announced that two of the smaller \"Virginia\"-class attack submarines, and , would have female crew-members by January 2015.\n\nIn an emergency, submarines can transmit a signal to other ships. The crew can use Submarine Escape Immersion Equipment to abandon the submarine. The crew can prevent a lung injury from the pressure change known as pulmonary barotrauma by exhaling during the ascent. Following escape from a pressurized submarine, the crew is at risk of developing decompression sickness. An alternative escape means is via a Deep Submergence Rescue Vehicle that can dock onto the disabled submarine.\n\n\n1900/Russo-Japanese War 1904–1905\n\n\n"}
{"id": "43141095", "url": "https://en.wikipedia.org/wiki?curid=43141095", "title": "The Houston Story", "text": "The Houston Story\n\nThe Houston Story is a 1956 crime film noir directed by William Castle starring Gene Barry, Barbara Hale and Edward Arnold.\n\nThe film went through a major casting change while in production. Originally set for the lead role was acclaimed character actor Lee J. Cobb. But Cobb suffered a heart attack after filming an exhausting fight sequence, in part due to the August heat in Texas, where the scene was shot. Cobb's part had to be recast and went to Barry. The film's director, William Castle wrote that the producer \"insisted on a relatively new actor in pictures - Gene Barry, a fine actor, but as unlike Lee J. Cobb as anyone could be.\"\n\nFrank Duncan (Barry), a shrewd oil driller from Galveston, Texas, conceives a plan to sneakily siphon millions of dollars' worth of oil from the oil fields and sell it as his own. He goes through nightclub singer Zoe Crane (Hale) to insinuate himself with a Houston mobster, Paul Atlas (Arnold) to get financing for his scheme.\n\nAtlas tells right-hand man Gordon Shay privately that he plans to double-cross Duncan as soon as the money's in hand. Chris Barker, a gunman, robs Duncan and intends to murder him, but Duncan is able to push Barker off an oil rig to his death.\n\nDuncan tries to make a getaway with the help of true-blue girlfriend Madge, but the hard-hearted Zoe steals his money and lies to Madge that Duncan has betrayed her. A pair of Atlas's thugs snatch Zoe, take her money and toss her from a moving car. Shay is killed, but before Duncan can get away, the cops close in on him and he's forced to surrender.\n\n\n\n"}
{"id": "1748253", "url": "https://en.wikipedia.org/wiki?curid=1748253", "title": "Two-man saw", "text": "Two-man saw\n\nA two-man saw (known colloquially as a \"misery whip\") is a saw designed for use by two sawyers. While some modern chainsaws are so large that they require two persons to control, two-man crosscut saws were primarily important when human power was used. Such a saw would typically be long, and sometimes up to , with a handle at each end. In some cases, such as when felling Giant Sequoias, sawblades could be brazed together end-to-end in order to create longer saws. \n\nThe technique in using a two-man saw involved a sawyer standing at each end. Together the sawyers would alternate pulling the saw through the wood. If the kerf began closing, causing the saw to bind, wedges would be inserted behind the sawblade in order to keep the kerf open. Cutting from underneath a suspended log, called \"underbucking\", might also have been used if binding became a big problem.\n\nMany variations on the design were used, but they mainly fell into two types. Felling saws were used to fell the trees, and bucking saws were used to cut felled trees into lumber. The two applications require slightly different designs: a felling saw has a narrower blade, allowing wedges to be more easily inserted, while a bucking saw has a wider blade, giving it more strength.\n\nTwo-man saws were designed to cut in both directions. Careful tooth design was necessary to clear the sawdust during the cut.\n\nTwo-man saws were known to the ancient Romans, but first became common in Europe in the mid-15th century. In America, crosscut saws were used as early as the mid-17th century, but felling saws only began to replace axes for felling trees in the late 19th century. \n\nSome Japanese saws are used by two persons, although they are of a different design.\n\n\n"}
{"id": "57949875", "url": "https://en.wikipedia.org/wiki?curid=57949875", "title": "Vainudden Standing Stone", "text": "Vainudden Standing Stone\n\nLocated on the Vainudden peninsular - part of Hitå - in Southern Sipoo is a small standing stone and evidence of Nordic Bronze Age activity c. 1700–500 BC \n\nThe stones are of rectangular cross-section approximately 50cm x 50cm arranged in a square with single upright stone of same cross-section standing 1m high. The stone is now a marker of the border between properties in the area.\n\nTo the immediate south of the stone is an area thought to have been used for carving stones and possibly camping; two rocks show possible workings by hand. There is cirumstancial evidence of a moat. The rocks are covered in moss and lichen making identification of the area difficult.\n"}
{"id": "31827337", "url": "https://en.wikipedia.org/wiki?curid=31827337", "title": "Valve Amplification Company", "text": "Valve Amplification Company\n\nValve Amplification Company (VAC) is a U.S. manufacturer of high end audio electronics, principally utilizing vacuum tube technology. It was founded in 1990 by Kevin Hayes (b. 1959) and Channing W. Hayes (1923–2009). A Florida corporation, as of 2011 it is located in Sarasota, Florida, U.S.\n\nIt is well known for having produced the Marantz Classic series of amplifiers for Marantz Japan from 1996 to 1998 (Marantz Model 7 / 7C, Model 8B, Model 9), as well as the VAC-designed Model 66 integrated amplifier. In addition, it redeveloped and produced the first 200 recreations of the LA-2A leveling amplifier for Universal Audio in 1998.\n\nVAC was founded in 1990 by Kevin Hayes and his father, Channing W. Hayes in Sarasota, Florida. The first products were the PA45 and PA90 power amplifiers, designed jointly. Beginning with their second product, the CPA1 / CLA1 preamplifier, designs are largely the work of Kevin Hayes, who functions as chief engineer.\n\nVAC continued to operate in Florida until the beginning of the Marantz Classic project, the production phase of which commenced in Durham, NC in January 1996. VAC returned to Sarasota, FL in September 2001.\n\nOn June 10, 2014, Kevin Hayes was issued U.S. Patent 8,749,310 for \"amplifier bias control\". This patent covers the only known technique for observing the true underlying quiescent current (idle current) of an output tube (or transistor) under dynamic signal conditions, and then holding it to the stable target value with a precision of 99% or better. It is the only 'auto bias' system in which the volume and character of the music being played does not alter the idle point of the tube. It is incorporated in VAC power amplifiers as the iQ Intelligent Continuous Automatic Bias System. In addition, the iQ system defends against short circuit tubes, prevents gas current run away tubes, indicates weakening tubes, and minimizes noise and distortion. Current iQ models are the VAC Statement 450 iQ and Signature 200 iQ.\n\nVAC is privately held.\n\n\n"}
{"id": "7579113", "url": "https://en.wikipedia.org/wiki?curid=7579113", "title": "Wax carving", "text": "Wax carving\n\nWax carving is the shaping of wax using tools usually associated with machining: rotary tools, saws, files and burins or gravers. Actual knives can be used and most certainly are, but the hardness of the material is such that they are not the ideal tool, generally.\n\nTo carve wax, the proper size and shape of block or tube is chosen, in the preferred hardness, and cut to a rough size, as needed. Then the design is generally drawn or laid out on that, and saws, files or machine tools are used to work the wax into a finished product. The wax is easily taken to a fine finish in the end using a bit of nylon stocking or steel wool. After the wax product is finished, it may be molded or used in the lost wax casting process to create a final cast product.\n\nThere are a wide variety of wax types used in the lost wax casting process. Generally they fall into three main types, soft, hard and injection waxes. Injection waxes are made and intended to be used for injecting wax under pressure into rubber or other types of molds. They can be carved and worked otherwise, but they are not specifically designed for that use. Their properties more often target good injection properties: flow, low shrinkage, pot life etc.\n\nSoft waxes are sometimes called sculpting waxes, and generally have a consistency resembling clay. Generally the techniques used in working soft waxes are similar to those used with clay and involve the use of wooden or metal spatulas, direct molding with the fingers and the like.\n\nCarving wax is a smooth, non-brittle wax designed for carving and/or machining. Although the formulas for most commercial waxes are proprietary, most suppliers will state that hard waxes are some blend of waxes and plastics. This family of waxes has a hardness and consistency of plastic or softer wood. They can be cut or carved with knives, files and rotary or machine tools. To illustrate the usefulness of this type of wax, if one were to get a candle, mount it on a lathe and feed a tool into it, the wax would slough off like butter, stick to the tool and make a mess. Hard wax, on the other hand, will machine more like soft aluminum, giving fine edges and a fine finish if worked properly.\n\nWaxes come in a wide variety of shapes: blocks, sheets, rods and tubes, and in recent times there are even extruded shapes available. The rods are useful for lathe turning, among other things, and the tubes are useful for making rings in jewelry work. The tubes are available in various sizes, and also with a flat top, which is useful for signet rings.\n"}
{"id": "7332900", "url": "https://en.wikipedia.org/wiki?curid=7332900", "title": "Woolnorth Wind Farm", "text": "Woolnorth Wind Farm\n\nWoolnorth Wind Farm is a wind power development, comprising two wind farms, at the Woolnorth property at Woolnorth (which includes the location known as Cape Grim), in the far north-west of Tasmania, Australia. Both wind farms are operated by Woolnorth Wind Farm Holdings, a joint venture between Hydro Tasmania (who own a 25% share) and Shenhua Group (75% share).\n\nBluff Point Wind Farm was constructed in two stages. The first consisted of six Vestas turbines and was commissioned in 2002. Stage two, commissioned in 2004, expanded the wind farm with a further 31 of the same turbines, for a total generating capacity of .\n\nStudland Bay Wind Farm was commissioned in 2007 and consists of 25 Vestas V90 turbines, for a total capacity of .\n\nTours to the wind farms are available and operated by a private commercial entity.\n\n"}
