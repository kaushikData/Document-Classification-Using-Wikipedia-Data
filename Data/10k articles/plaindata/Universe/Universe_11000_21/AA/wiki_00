{"id": "1784383", "url": "https://en.wikipedia.org/wiki?curid=1784383", "title": "15 kV AC railway electrification", "text": "15 kV AC railway electrification\n\nThe AC railway electrification system is used in Germany, Austria, Switzerland, Sweden and Norway. The high voltage enables high power transmission with the lower frequency reducing the losses of the traction motors that were available at the beginning of the 20th century. Railway electrification in late 20th century tends to use AC systems which has become the preferred standard for new railway electrifications but extensions of the existing networks are not completely unlikely. In particular, the Gotthard Base Tunnel (opened on 1 June 2016) still uses 15 kV, 16.7 Hz electrification.\n\nDue to high conversion costs, it is unlikely that existing systems will be converted to despite the fact that this would reduce the weight of the on-board step-down transformers to one third that of the present devices.\n\nThe first electrified railways used series-wound DC motors, first at 600 V and then 1,500 V. Areas with 3 kV DC catenaries (primarily in Eastern Europe) used two 1,500 V DC motors in series. But even at 3 kV, the current needed to power a heavy train (particularly in rural and mountainous areas) can be excessive. Although increasing the transmission voltage decreases the current and associated resistive losses for a given power, insulation limits make higher voltage traction motors impractical. Transformers on each locomotive are thus required to step high transmission voltages down to practical motor operating voltages. Before the development of suitable ways to efficiently transform DC currents through power electronics, efficient transformers strictly required alternating current (AC); thus high voltage electrified railways adopted AC along with the electric power distribution system (see War of Currents).\n\nThe 50 Hz (60 Hz in North America) AC grid was already established at the beginning of the 20th century. Although series-wound motors can in principle run on AC as well as DC (the reason they are also known as universal motors) large series-wound traction motors had problems with such high frequencies. High inductive reactance of the motor windings caused commutator flashover problems and the non-laminated magnetic pole-pieces originally designed for DC exhibited excessive eddy current losses. Using a lower AC frequency alleviated both problems.\n\nIn the German-speaking countries, high-voltage electrification began at , exactly one third of the national power grid frequency of 50 Hz. This facilitated the operation of rotary converters from the grid frequency and allowed dedicated railway power generators to operate at the same shaft speed as a standard 50 Hz generator by reducing the number of poles by a factor of three. For example, a generator turning at would be wound with two poles rather than six.\n\nSeparate plants supply railway power in Austria, Switzerland and Germany, except for Mecklenburg-Western Pomerania and Saxony-Anhalt; converters powered by the grid supply railway power in those two German states plus Sweden and Norway. Norway also has two hydro-electric power plants dedicated for railway power with output.\n\nThe first generators were synchronous AC generators or synchronous transformers; however, with the introduction of modern double fed induction generators, the control current induced an undesired DC component, leading to pole overheating problems. This was solved by shifting the frequency slightly away from exactly ⅓ the grid frequency; was arbitrarily chosen to remain within the tolerance of existing traction motors. Austria, Switzerland and Southern Germany switched their power plants to 16.7 Hz on 16 October 1995 at 12:00 CET. Note that regional electrified sections run by synchronous generators keep their frequency of just as Sweden and Norway still run their railway networks at throughout.\n\nOne of the disadvantages of locomotives as compared to or locomotives is the heavier transformer required to reduce the overhead line voltage to that used by the motors and their speed control gear. Low frequency transformers need to have heavier magnetic cores and larger windings for the same level of power conversion. (See effect of frequency on the design of transformers.) The heavier transformers also lead to higher axle loads than for those of a higher frequency. This, in turn, leads to increased track wear and increases the need for more frequent track maintenance. The Czech Railways encountered the problem of the reduced power handling of lower frequency transformers when they rebuilt some AC, locomotives (series 340) to operate on AC, lines. As a result of using the same transformer cores (originally designed for ) at the lower frequency, the transformers had to be de-rated to one third of their original power handling capability, thereby reducing the available tractive effort by the same amount (to around ).\n\nThese drawbacks, plus the need for a separate supply infrastructure and the lack of any technical advantages with modern motors and controllers has limited the use of  Hz and 16.7 Hz beyond the original five countries. Most other countries electrified their railways at the utility frequency of 50/60 Hz. Newer European electrification is mostly 25 kV AC at 50 Hz (primarily in Eastern Europe). Conversion to this voltage/frequency requires higher voltage insulators and greater clearance between lines and bridges and other structures. This is now standard for new overhead lines as well as for modernizing old installations.\n\nSimple European standardization with an alignment of voltage/frequency across Europe is not necessarily cost-effective since trans-border traction is more limited by the differing national standards in other areas. To equip an electric locomotive with a transformer for two or more input voltages is cheap compared to the cost of installing multiple train protection systems and to run them through the approval procedure to get access to the railway network in other countries. However, some new high-speed lines to neighbouring countries are already intended to be built to 25 kV (e.g. in Austria to Eastern Europe). Newer locomotives are always built with asynchronous motor control systems that have no problem with a range of input frequencies including DC. However the Deutsche Bahn train operator does still use older models from the standard electric locomotive series - even though some are now as much as 50 years old. As soon as these obsolescent models are decommissioned, it will be easier to standardise, but this may take a few decades to happen. Meanwhile, the Deutsche Bahn tends to order train sets that are capable of running multiple electrification systems.\n\nIn Germany (except Mecklenburg-Western Pomerania and Saxony-Anhalt), Austria and Switzerland, there is a separate single-phase power distribution grid for railway power at ; the voltage is in Germany and Austria and in Switzerland. This system is called the centralized railway energy supply. A separate single-phase power distribution grid makes the recuperation of energy during braking extremely easy in comparison with 25kV 50 Hz system tied to 3 phase distribution grid.\n\nIn Sweden, Norway, Mecklenburg-Western Pomerania and Saxony-Anhalt, the power is taken directly from the three-phase grid ( at ), converted to low frequency single phase and fed into the overhead line. This system is called the decentralized (i.e. local) railway energy supply.\n\nThe centralized system is supplied by special power plants that generate (or in the Swiss system) AC at and by rotary converters or AC/AC converters that are supplied from the national power grid (e.g. , ), they convert it to 55-0-55 kV (or 66-0-66 kV) AC at . The 0 V point is connected to earth through an inductance so that each conductor of the single phase AC power line has a voltage of (or ) with respect to earth potential. This is similar to split-phase electric power systems and results in a balanced line transmission. The inductance through which the earthing is done is designed to limit earth currents in cases of faults on the line. At the transformer substations, the voltage is transformed from (or ) AC to AC and the energy is fed into the overhead line.\n\nThe frequency of depends on the necessity to avoid synchronism in parts of the rotary machine, which consists principally of a three phase asynchronous motor and a single phase synchronous generator. Since synchronism sets in at a frequency of (according to the technical details) in the single phase system, the frequency of the centralized system was set to .\n\nPower plants providing , , are either dedicated to generating this specific single phase AC or have special generators for the purpose, such as the Neckarwestheim nuclear power plant or the Walchensee hydroelectric power station.\n\nThe power for the decentralized system is taken directly from the national power grid and directly transformed and converted into , by synchronous-synchronous-converters or static converters. Both systems need additional transformers. The converters consist of a three-phase synchronous motor and a single-phase synchronous generator. The decentralized system in the north-east of Germany was established by the Deutsche Reichsbahn in the 1980s, because there was no centralized system available in these areas.\n\nGermany, Austria and Switzerland operate the largest interconnected 15 kV AC system with central generation, and central and local converter plants.\n\nIn these facilities electricity is transformed down from 110 kV-level of DB to 15 kV.\nThere is no conversion or generation of power.\n\nStations for connecting/isolating parts of the system.\n\nIn these facilities the AC from the public grid is transformed and converted into the single phase AC and fed into the railway current distribution grid. At some facilities, power is also fed to the overhead line. Conversion is done by rotary converters or electronic inverters.\n\nIn these facilities the AC from the public grid is transformed and converted into the single phase AC and fed to the overhead line. Conversion is done by rotary converters or electronic inverters.\n\nIn these facilities electricity is transformed down from 132 kV or 66 kV to 15 kV.\nThere is no conversion or generation of power.\n\nIn these facilities the AC from public grid is transformed and converted into the single phase AC and fed into the railway current distribution grid. At some facilities, power is also fed to the overhead line. Conversion is done by rotary converters or electronic inverters.\n\nStations for connecting/isolating parts of the system.\n\nIn these facilities electricity is transformed down from 110 kV to 15 kV. No conversion or generation of power takes place.\n\nIn these facilities the AC from the public grid is transformed and converted into the single phase AC and fed into the railway current distribution grid. At some facilities, power is also fed to the overhead line. Conversion is done by rotary converters or electronic inverters.\n\nIn Norway all electric railways use 16 kV 16 Hz AC (except the Thamshavnbanen museum railway which uses 6.6 kV 25 Hz AC). The Oslo T-bane and tramways use 750 V DC power.\n\nIn Sweden most electric railways use 15 kV 16 Hz AC. Exceptions include: Saltsjöbanan and Roslagsbanan (1.5 kV DC), the Stockholm Metro (650 V and 750 V DC) and tramways (750 V DC). The Oresund Bridge linking Sweden and Denmark is electrified at 25 kV, Danish standard; the split is located on the Swedish side near the bridge. Only two-system trains (or diesel trains; rare) can pass the point.\n\n\n"}
{"id": "11126615", "url": "https://en.wikipedia.org/wiki?curid=11126615", "title": "AP600", "text": "AP600\n\nThe AP600 is a model of relatively small, 600 MWe nuclear power plant designed by Westinghouse Electric Company. The AP600 has passive safety features characteristic of the Generation III reactor concept. The projected core damage frequency is nearly 1000 times less than today's Nuclear Regulatory Commission (NRC) requirements, on par with plants currently being considered for construction. The design has been scaled up and improved with the AP1000.\n\nCertification testing and analysis of the AP600 and AP1000 reactor designs for Westinghouse were conducted at the APEX facility at Oregon State University. The one-quarter scale reduced pressure integral system certified the passively safe systems that cool the reactor core using gravity and natural circulation. \n\nThe NRC's final design certification was received in 1999 but no orders were ever placed. A large reason Westinghouse entered development of the AP1000 was to improve the economies of scale that come with larger MWe plants. \nThe more powerful AP1000 was designed to have a similar footprint but a taller containment and a power output of 1000 MWe or greater.\n"}
{"id": "2518272", "url": "https://en.wikipedia.org/wiki?curid=2518272", "title": "Aeroacoustics", "text": "Aeroacoustics\n\nAeroacoustics is a branch of acoustics that studies noise generation via either turbulent fluid motion or aerodynamic forces interacting with surfaces. Noise generation can also be associated with periodically varying flows. A notable example of this phenomenon is the Aeolian tones produced by wind blowing over fixed objects.\n\nAlthough no complete scientific theory of the generation of noise by aerodynamic flows has been established, most practical aeroacoustic analysis relies upon the so-called \"aeroacoustic analogy\", proposed by Sir James Lighthill in the 1950s while at the University of Manchester. whereby the governing equations of motion of the fluid are coerced into a form reminiscent of the wave equation of \"classical\" (i.e. linear) acoustics in the left-hand side with the remaining terms as sources in the right-hand side.\n\nThe modern discipline of aeroacoustics can be said to have originated with the first publication of Lighthill in the early 1950s, when noise generation associated with the jet engine was beginning to be placed under scientific scrutiny.\n\nLighthill rearranged the Navier–Stokes equations, which govern the flow of a compressible viscous fluid, into an inhomogeneous wave equation, thereby making a connection between fluid mechanics and acoustics. This is often called \"Lighthill's analogy\" because it presents a model for the acoustic field that is not, strictly speaking, based on the physics of flow-induced/generated noise, but rather on the analogy of how they might be represented through the governing equations of a compressible fluid.\n\nThe first equation of interest is the conservation of mass equation, which reads\n\nwhere formula_2 and formula_3 represent the density and velocity of the fluid, which depend on space and time, and formula_4 is the substantial derivative.\n\nNext is the conservation of momentum equation, which is given by\n\nwhere formula_6 is the thermodynamic pressure, and formula_7 is the viscous (or traceless) part of the stress tensor from the Navier–Stokes equations.\n\nNow, multiplying the conservation of mass equation by formula_3 and adding it to the conservation of momentum equation gives\n\nNote that formula_10 is a tensor (see also tensor product). Differentiating the conservation of mass equation with respect to time, taking the divergence of the last equation and subtracting the latter from the former, we arrive at\n\nSubtracting formula_12, where formula_13 is the speed of sound in the medium in its equilibrium (or quiescent) state, from both sides of the last equation and rearranging it results in\n\nwhich is equivalent to\n\nwhere formula_16 is the identity tensor, and formula_17 denotes the (double) tensor contraction operator.\n\nThe above equation is the celebrated Lighthill equation of aeroacoustics. It is a wave equation with a source term on the right-hand side, i.e. an inhomogeneous wave equation. The argument of the \"double-divergence operator\" on the right-hand side of last equation, i.e. formula_18, is the so-called \"Lighthill turbulence stress tensor for the acoustic field\", and it is commonly denoted by formula_19.\n\nUsing Einstein notation, Lighthill’s equation can be written as\n\nwhere\n\nand formula_22 is the Kronecker delta. Each of the acoustic source terms, i.e. terms in formula_23, may play a significant role in the generation of noise depending upon flow conditions considered. formula_24 describes unsteady convection of flow (or Reynolds' Stress, developed by Osborne Reynolds), formula_25 describes sound generated by shear, and formula_26 describes non-linear acoustic generation processes.\n\nIn practice, it is customary to neglect the effects of viscosity on the fluid, i.e. one takes formula_27, because it is generally accepted that the effects of the latter on noise generation, in most situations, are orders of magnitude smaller than those due to the other terms. Lighthill provides an in-depth discussion of this matter.\n\nIn aeroacoustic studies, both theoretical and computational efforts are made to solve for the acoustic source terms in Lighthill's equation in order to make statements regarding the relevant aerodynamic noise generation mechanisms present.\n\nFinally, it is important to realize that Lighthill's equation is exact in the sense that no approximations of any kind have been made in its derivation.\n\nIn their classical text on fluid mechanics, Landau and Lifshitz derive an aeroacoustic equation analogous to Lighthill's (i.e., an equation for sound generated by \"turbulent\" fluid motion), but for the incompressible flow of an inviscid fluid. The inhomogeneous wave equation that they obtain is for the \"pressure\" formula_6 rather than for the density formula_2 of the fluid. Furthermore, unlike Lighthill's equation, Landau and Lifshitz's equation is not exact; it is an approximation.\n\nIf one is to allow for approximations to be made, a simpler way (without necessarily assuming the fluid is incompressible) to obtain an approximation to Lighthill's equation is to assume that formula_30, where formula_31 and formula_32 are the (characteristic) density and pressure of the fluid in its equilibrium state. Then, upon substitution the assumed relation between pressure and density into formula_33 we obtain the equation (for an inviscid fluid, σ = 0)\n\nAnd for the case when the fluid is indeed incompressible, i.e. formula_35 (for some positive constant formula_31) everywhere, then we obtain exactly the equation given in Landau and Lifshitz, namely\n\nA similar approximation [in the context of equation formula_38], namely formula_39, is suggested by Lighthill [see Eq. (7) in the latter paper].\n\nOf course, one might wonder whether we are justified in assuming that formula_30. The answer is affirmative, if the flow satisfies certain basic assumptions. In particular, if formula_41 and formula_42, then the assumed relation follows directly from the \"linear\" theory of sound waves (see, e.g., the linearized Euler equations and the acoustic wave equation). In fact, the approximate relation between formula_6 and formula_2 that we assumed is just a linear approximation to the generic barotropic equation of state of the fluid.\n\nHowever, even after the above deliberations, it is still not clear whether one is justified in using an inherently \"linear\" relation to simplify a \"nonlinear\" wave equation. Nevertheless, it is a very common practice in nonlinear acoustics as the textbooks on the subject show: e.g., Naugolnykh and Ostrovsky and Hamilton and Morfey.\n\n\n"}
{"id": "1305131", "url": "https://en.wikipedia.org/wiki?curid=1305131", "title": "Agreste", "text": "Agreste\n\nThe agreste (, \"countryside\") is a narrow zone of Brazil in the states of Rio Grande do Norte, Paraíba, Pernambuco, Alagoas, Sergipe and Bahia between the coastal forest \"zona da mata\" and the semiarid \"sertão\". The agreste fades out after it reaches Rio Grande do Norte due to the break of the mountain-chain that blocks air currents from the Atlantic ocean. This barrier is what induces high rainfalls in the coastal Atlantic forest zone.\n\nMost of the agreste is hilly, its hills becoming higher at south, except near the narrow valley of São Francisco River. This land is mostly used for mixed farming, prevailing fruits, of which melons have especial importance. Like the sertão, the agreste is frequently affected by drought, though generally with less severe effects. Only some highland regions mostly in Pernambuco, where cities like Garanhuns and Triunfo are located, are able to reach temperatures below 10 degrees Celsius for a part of the year, usually coinciding with the south-American winter.\n\nThe climate is hot and sub-humid, with rainfall in the area's principal city of Campina Grande averaging about 700 millimetres per year - ranging from less than 10 millimetres in October and November to about 120 millimetres in May and June.\n\n\nNonfiction\n\nFiction\n"}
{"id": "51144726", "url": "https://en.wikipedia.org/wiki?curid=51144726", "title": "Andreev Bay nuclear accident", "text": "Andreev Bay nuclear accident\n\nThe Andreev Bay nuclear accident took place at Soviet naval base 569 in February 1982. Andreev Bay is a radioactive waste repository, located 55 km (34 mi) northwest of Murmansk and 60 km (37 mi) from the Norwegian border on the western shore of the Zapadnaya Litsa (Kola Peninsula). The repository entered service in 1961. In February 1982, a nuclear accident occurred in which radioactive water was released from a pool in building #5. Cleanup of the accident took place from 1983 to 1989. About 700,000 tonnes (770,000 tons) of highly radioactive water leaked into the Barents Sea during that time period. About 1,000 people took part in the cleanup effort. Vladimir Konstantinovich Bulygin, who was in charge of the naval fleet's radiation accidents, received the Hero of the Soviet Union distinction for his work.\n\nThe repository was constructed in the early 1960s by construction brigade soldiers from central Asian and Caucasian republics. Many did not have professional construction training, and some could not speak the Russian language. The repository is a naval base on the shore of the Zapadnaya Litsa bay. It consists of two piers, a stationary mooring bay, a sanitation facility, spent fuel pools in building #5 (unused since 1989), three dry storage containers, a 40,000-tonne crane, an open-air field for storing spent nuclear fuel containers, a security checkpoint, and other technical facilities.\n\nBuilding #5 (the pool storage facility) contained two pools for storing spent fuel assemblies, encased in steel drums. Each drum contained 5-7 spent fuel assemblies, weighing 350 kg fully loaded. Each of the pools was long, wide, deep, and had a volume of . Each was designed for about 2,000 drums. The drums were suspended underwater from massive chains, which were attached to consoles a certain distance from each other to avoid an uncontrolled nuclear chain reaction from starting. The water served as biological protection. The drums were placed underwater using the chains and a crane cart, but due to the construction's unreliability, drums often fell to the bottom of the pool. As a result, spent nuclear fuel drums dangerously piled up at the bottom.\n\nOne of the Andreev Bay workers recalls:\n\nThe dry storage containers consist of three steel underground blocks: \"3A\", \"2A\", and \"2B\": in diameter, each. Initially, the containers were designed as filtration structures for radioactive water from building #5, but after the 1982 accident, they were refitted to serve as dry storage for nuclear waste from building #5, as well as from nuclear submarines. Each container contains cells made of vertical steel pipes. The pipes are placed a set distance from each other in order to avoid starting an uncontrolled nuclear chain reaction. Each pipe is long, in diameter, and space between the pipes is filled with concrete. The \"3A\" container has 900 cells (for 900 drums), whereas containers \"2A\" and \"2B\" have 1,200 cells each. The drums were loaded using the KPM-40 crane.\n\nAfter the refitting project was approved, the concrete covers over the containers were broken in order to load the drums. Since the containers remained uncovered, loading was conducted in outdoor conditions, where precipitation fell inside unobstructed. The drums were placed into the cells, displacing water, which immediately turned into radioactive vapour, due to the heat emitted from the fuel assemblies, and wind scattered it over the entire territory of the base. In his book \"Andreev Nuclear Bay\", A. N. Safonov writes, citing data, that the bottoms of the containers were not covered in steel and allowed groundwater to seep through. During winters, the groundwater inside the cells froze, deforming their steel covers.\n\n\nSeveral possible causes that led to the destruction of the pools exist:\n\nExperts consider version #4 the most likely — temperature changes in the pool's water stressed welding seams, tearing them. When building #5's repository was designed, it was assumed that the water would be kept at a constant temperature by heat from the nuclear assemblies suspended under the surface. A separate water heating system was thus deemed unnecessary. But the designers were wrong, the harsh Arctic climate covered the pool's surface with a layer of ice in winter. In order to solve this problem, the ice was melted using steam from the boiler, in blatant violation of radiation safety protocol. This was accomplished in the following manner: a hole was drilled in the ice cover, a pipe was inserted into the hole, and steam was pumped through the pipe under the ice, melting it. Radioactive aerosols spread through the whole building, leaking into the air outside.\n\nDuring extraction of drums from the bottom of the pools, an accident occurred that might have cost two workers' lives. After the left-hand pool was covered with protective lids, cleanup workers cut windows inside them with torches in order to feed in a capturing device that lifted drums from the bottom. The windows were covered with iron sheets to protect the workers from radiation and prevent them from falling in. During this work, one of the workers, a starshina 1st stage, inadvertently stepped onto an iron sheet covering one of the windows. The sheet failed to support his weight, and they both fell into the pool's radioactive water. As the worker fell, his legs got caught under some nuclear waste drums, and the water splashed onto the others, who also did not have radiation protection equipment.\n\nFrom the memoirs of the cleanup effort's leader after the accident, A. N. Safonov: No links can be found on this memoir now 2018 Sept 18\n\nA moment later, another worker, a starshina 2nd stage, heroically jumped into the pool to save his comrade's life. A few seconds later they both surfaced, completely soaked in radioactive water. Witnesses say their faces had expressions of utter terror.\n\nThe first worker himself remembers:\n\nBoth workers were sent into the showers for decontamination. The dosimeter's arrow kept passing tens of millions of beta decays. Both workers had hair removed from all parts of their bodies, they slept separately from everyone else and received food in rubber gloves, since their bodies themselves were now sources of gamma radiation. The dose to which they were exposed is unknown, as their dosimeters sank in the pool.\n\nFrom the memoirs of A. N. Safonov:\n\nThe workers received no medical attention.\n\nWhen nuclear fuel drums were unloaded from building #5 to be loaded into the dry storage containers, it often happened that cells, deformed from physical impacts and ice, spilled nuclear fuel. The working sailors then used regular shovels to pour the fuel into the cement-encased vertical steel pipes of the storage containers. These actions lead to accumulation of critical mass and subsequent uncontrolled chain reactions, glowing from Cherenkov radiation and emitting a buzzing sound, which quickly subsided.\n\nHere is how A. N. Safonov describes this:\nBlue-green flashes of light were also observed in the left-hand pool in building #5 during the work on lifting nuclear waste drums from the bottom. That they were uncontrolled chain reactions was confirmed by the physicist, senior lieutenant Leonid Grigorievich Konobritski, who served then in building #5.\n\n"}
{"id": "33223136", "url": "https://en.wikipedia.org/wiki?curid=33223136", "title": "Arecleoch Wind Farm", "text": "Arecleoch Wind Farm\n\nArecleoch Wind Farm is a 60 turbine wind farm in South Ayrshire, Scotland with a total capacity of 120 megawatts (MW), enough to power over 67,000 homes. Construction started in 2009 and it was commissioned in June 2011.\n\nIn January and February 2016 there were two separate incidents of a turbine catching fire overnight while the site was unmanned due to faults in the mechanism housing. The turbines were destroyed and were disassembled and removed from site. Scottish Power reported to the Sunday Mail on 10 April 2016 they were investigating these incidents along with the manufacturer Gamesa. \n\nIn December 2016 a third turbine caught fire during high winds while it was switched off for maintenance. The turbine was left to burn itself out and was destroyed.\n\nIn October 2017 a fourth turbine caught fire and was also left to burn itself out and was destroyed.\n\n"}
{"id": "1390580", "url": "https://en.wikipedia.org/wiki?curid=1390580", "title": "Banana paper", "text": "Banana paper\n\nBanana paper is used in two different senses: one refers to a paper made from the bark of the banana plant and which is mainly used for artistic purposes; the other to paper made from banana fiber obtained, through an industrial process, from stems and the non-utilizable fruits. This paper can be either hand or machine made.\n\nThe banana agricultural industry processes of bananas every year (with planted). As a result of pulling apart the banana bunches from the main stem, there are leftover stems which contain 5% of fiber useful for the manufacture of paper.\n\n"}
{"id": "3982", "url": "https://en.wikipedia.org/wiki?curid=3982", "title": "Bicarbonate", "text": "Bicarbonate\n\nIn inorganic chemistry, bicarbonate (IUPAC-recommended nomenclature: hydrogencarbonate) is an intermediate form in the deprotonation of carbonic acid. It is a polyatomic anion with the chemical formula .\n\nBicarbonate serves a crucial biochemical role in the physiological pH buffering system.\n\nThe term \"bicarbonate\" was coined in 1814 by the English chemist William Hyde Wollaston. The prefix \"bi\" in \"bicarbonate\" comes from an outdated naming system and is based on the observation that there is twice as much carbonate () per sodium ion in sodium bicarbonate (NaHCO) and other bicarbonates than in sodium carbonate (NaCO) and other carbonates. The name lives on as a trivial name.\n\nThe bicarbonate ion (hydrogencarbonate ion) is an anion with the empirical formula and a molecular mass of 61.01 daltons; it consists of one central carbon atom surrounded by three oxygen atoms in a trigonal planar arrangement, with a hydrogen atom attached to one of the oxygens. It is isoelectronic with nitric acid . The bicarbonate ion carries a negative one formal charge and is an amphiprotic species which has both acidic and basic properties. It is both the conjugate base of carbonic acid ; and the conjugate acid of , the carbonate ion, as shown by these equilibrium reactions:\n\nA bicarbonate salt forms when a positively charged ion attaches to the negatively charged oxygen atoms of the ion, forming an ionic compound. Many bicarbonates are soluble in water at standard temperature and pressure; in particular, sodium bicarbonate contributes to total dissolved solids, a common parameter for assessing water quality.\n\nBicarbonate () is alkaline, and a vital component of the pH buffering system of the human body (maintaining acid–base homeostasis). 70–75% of CO in the body is converted into carbonic acid (HCO), which can quickly turn into bicarbonate.\n\nWith carbonic acid as the central intermediate species, bicarbonate – in conjunction with water, hydrogen ions, and carbon dioxide – forms this buffering system, which is maintained at the volatile equilibrium required to provide prompt resistance to pH changes in both the acidic and basic directions. This is especially important for protecting tissues of the central nervous system, where pH changes too far outside of the normal range in either direction could prove disastrous (see acidosis or alkalosis).\n\nBicarbonate also serves much in the digestive system. It raises the internal pH of the stomach, after highly acidic digestive juices have finished in their digestion of food. Bicarbonate also acts to regulate pH in the small intestine. It is released from the pancreas in response to the hormone secretin to neutralize the acidic chyme entering the duodenum from the stomach.\n\nBicarbonate is the dominant form of dissolved inorganic carbon in sea water, and in most fresh waters. As such it is an important sink in the carbon cycle.\n\nIn freshwater ecology, strong photosynthetic activity by freshwater plants in daylight releases gaseous oxygen into the water and at the same time produces bicarbonate ions. These shift the pH upward until in certain circumstances the degree of alkalinity can become toxic to some organisms or can make other chemical constituents such as ammonia toxic. In darkness, when no photosynthesis occurs, respiration processes release carbon dioxide, and no new bicarbonate ions are produced, resulting in a rapid fall in pH.\n\nThe most common salt of the bicarbonate ion is sodium bicarbonate, NaHCO, which is commonly known as baking soda. When heated or exposed to an acid such as acetic acid (vinegar), sodium bicarbonate releases carbon dioxide. This is used as a leavening agent in baking.\n\nThe flow of bicarbonate ions from rocks weathered by the carbonic acid in rainwater is an important part of the carbon cycle.\n\nAmmonium bicarbonate is used in digestive biscuit manufacture.\n\nIn diagnostic medicine, the blood value of bicarbonate is one of several indicators of the state of acid–base physiology in the body. It is measured, along with carbon dioxide, chloride, potassium, and sodium, to assess electrolyte levels in an electrolyte panel test (which has Current Procedural Terminology, CPT, code 80051).\n\nThe parameter \"standard bicarbonate concentration\" (SBC) is the bicarbonate concentration in the blood at a PCO of , full oxygen saturation and 36 °C.\n\n\n"}
{"id": "27024341", "url": "https://en.wikipedia.org/wiki?curid=27024341", "title": "Binalood wind farm", "text": "Binalood wind farm\n\nBinalood wind farm is a wind farm situated in Razavi Khorasan Province of Iran near the city of Nishapur. It currently uses 43 turbines with a generating capacity of 660 kW to produce 28.2 MW of electricity using wind power. The area of the farm is over . The project was initiated in 2002 and the farm came online in 2008. The plant was built by Renewable energy organization of Iran. The plant is currently being expanded by adding 50 more turbines, each with a capacity of 660 kW, increasing its total capacity to 61.2 MW.\n\n\n"}
{"id": "33245032", "url": "https://en.wikipedia.org/wiki?curid=33245032", "title": "Black Gold (2011 Qatari film)", "text": "Black Gold (2011 Qatari film)\n\nBlack Gold (also known as Day of the Falcon and Or noir) is a 2011 French-Qatari epic historical war film, based on Hans Ruesch's 1957 novel \"South of the Heart: A Novel of Modern Arabia\" (also known as \"The Great Thirst\" and \"The Arab\"). It was directed by Jean-Jacques Annaud, produced by Tarak Ben Ammar and co-produced by Doha Film Institute. The film stars Tahar Rahim, Antonio Banderas, Freida Pinto, Mark Strong and Riz Ahmed.\n\nThe film had a budget of US$40 million, making it one of the most expensive films backed by an Arab about an Arab subject.\n\nIn the early 20th century, Emir Nesib (Antonio Banderas), Sultan of Hobeika, and Sultan Amar (Mark Strong) of Salmaah have been in a border war over a vast barren strip they call \"The Yellow Belt\". When Nesib wins he forces Amar to agree to a peace pact: the Yellow Belt will belong to neither, becoming a no-man's-land between their territories, and Emir Nesib will take Sultan Amar's sons, Saleh and Auda, as hostages. Amar reluctantly agrees, knowing the hostages are a sacred trust which binds Nesib as well. They both swear to the pact before God. Nesib promises to rear Amar's sons with his own children, Tariq and Leyla.\n\nSaleh, Amar's eldest son, is a free spirit interested in the traditional pursuits of an Arab emir, while Auda is a dedicated bookworm. Leyla and Auda become good friends, until they are separated at adolescence. Ten years pass. Auda (Tahar Rahim) is still a bookworm, while Saleh (Akin Gazi) longs to go home and be with his father.\n\nSam Thurkettle (Corey Johnson), a geologist working for the western \"Texan Oil\" company, surveys the Yellow Belt and is convinced there is high grade crude oil under its shale. He tells Nesib that the find will make him richer than the King of England, and Nesib is more than willing to listen; he has had to watch, powerless and penniless, as his people suffered a cholera epidemic and his own wife died of it. Nesib allows Thurkettle's company to extract oil from the Yellow Belt - thus violating his peace pact with Sultan Amar.\n\nMoney pours in and Nesib starts to modernize his kingdom with schools, hospitals and electricity. He makes his son Tariq a Colonel, appoints Auda his national librarian, and sends an envoy to Amar to strike a deal to extract oil from the Yellow Belt. But desert tribesmen attack one of the oil sites and kill the crew. Their revenues threatened, Nesib sets about inducing the various tribes to accept the oil extraction, using lavish gifts and gold as inducements.\n\nThe envoy sent to Amar returns and reports to Nesib: Amar considers the exploitation of Yellow Belt a violation of their treaty. Saleh tells Auda that he can convince their father and decides to escape; he kills his guards while making his escape. He is caught, but Ibn Idris kills him in revenge for killing the guards, who were Ibn Idris's cousins.\n\nDesperate to maintain the oil revenues, Nesib executes a brilliant political maneuver: he marries Auda to his daughter Leyla (Freida Pinto). At one stroke he has converted Auda's position from hostage to family member, thereby dissolving his pact with Amar and absolving himself from his religious oath. Auda reluctantly agrees, knowing it is a plot to prevent Amar from attacking Nesib. Nesib decides to send Auda to convince Amar for using the yellow belt. Auda meets Amar, who is surprised to learn that Auda has come as a representative of Nesib. Auda learns more about his father during his stay there. Amar tells Auda that Nesib offered 5%, then 35% of the earnings but he refused the offer. Nissib has even promised to throw in Amar's sons as some kind of property value. When Auda tries to explain to him, he says that everything in his home is made either out of blood or love, but not money and that money has no value. The following day a meeting is held with Amar's allies. They say that by letting foreigners extract oil they let themselves be destroyed their culture will be gone, while Auda unsuccessfully argues by saying if god had not meant it for them to use, he wouldn't have given it in their soil. \n\nAmar sends an envoy, Hassan Dakhil, to Nesib offering to cease all hostilities if Nesib agrees to shut down the oil wells and expel the foreigners. Nesib refuses and makes a counter offer to Hassan Dakhil. Later Amar receives a message from Hassan that seemingly indicates that Hassan has defected to Nesib. Following this setback, Amar makes a plan to send all of his prisoners into the desert with weapon props to act as decoys, in order to lure Nesib's army into the desert by while Amar takes his real army to capture Nesib's city, Hobeika. He offers the leadership of the decoy army to Auda. Auda protests, objecting that to send the prisoners into the desert would mean certain death, but in the face of his father's disapproval he reluctantly agrees and ventures out, accompanied by his half brother Ali, a doctor, who does not seem to share their father's xenophobic mindset. The plan works and Nesib sends six armored cars after them. However the heavy cars are stuck in the sands and Auda's men are able to overpower their occupants, though not without heavy losses. The camel carrying the carrier pigeons is killed and the pigeons escape. One of them manages to make it way back to Salmaah. The blood on the pigeon along with the absence of a written message leads Amar to believe that there have been no survivors. When the armored cars fail to return, Nesib sends a plane with Tariq on board to reconnoiter. Auda sets another decoy, having his men lie down and play dead around a destroyed armored car. When Tariq lands to investigate, Auda's men swarm him. Tariq manages to make it back to the plane, but Auda's men force it down. Auda finds his body in the ensuing wreckage and is overcome with remorse at all the needless deaths. He rallies the remaining prisoners and offers to divide the remaining water between them and set them free. The prisoners decide to follow Auda, who leads them to the sea believing that they would find water there, based from what he had heard from a dying camel rider. \n\nThe army, now Auda's army, finally arrive at the sea but are disheartened to find no fresh water, until Ali finds an underwater spring. Having refilled their water skins, the army moves away and comes upon a Beni Sirri slaver tribe. Auda leads a small group of men to meet the Sheikh of Beni Sirri tribe and during the meeting the Sheikh beats Aicha (Liya Kebede), a slave girl, brutally. The Sheikh seemingly holds the slave's Zamiri tribe in derision as the Zamiri are one of the few tribes that allow women the same freedoms that are given to men. Auda tries to protest and offers to buy the slave girl in exchange for his mother's ring, who belonged to the same tribe as the slave girl. An argument ensues, and the Sheikh reveals that Nesib has already bought the loyalty of the Beni Sirri and that they intended to kill him in exchange of a reward. The rest of Auda's men launch a surprise attack, surrounding and overwhelming the Beni Sirri, leaving Auda to handcuff and disgrace the leaders of Beni Sirri tribe and freeing the slaves. To add insult to injury, Ali relieves the sheikh of the gold watch given to him by Nesib at Auda's wedding. In gratitude for freeing the slaves, the other tribes pledge their own resources to Auda. Aicha offers to lead Auda back to her own tribe in order to enlist their aid. However Auda is mistakenly shot by one of the tribe and seems to have been killed, only to revive in the middle of his own funeral rites. Ali realizes that Auda's condition had in fact been a medical phenomenon known as mors putativa in which a head trauma induces all the symptoms of death. However the tribals believe that Auda has been revived from death in the manner of the Islamic prophet Muhammad and that he is the leader foretold in the legends of the Zamiri tribe. Ali, despite knowing the true nature of Auda's injuries, does nothing to dissuade this notion. The Zamiri tribe now rallies around Auda as their leader. One of Nesib's planes manages to track them down and opens fire. The tribals manage to shoot down the plane, but not before it fatally wounds Ali. Auda administers his last rites as Ali lies dying in his arms. With his last breath Ali makes Auda promise to 'overturn the chessboard' in effect asking him to depose both Amar and Nessib, thereby putting an end to the conflict. \n\nAuda rides with his army to the gates of Nessib's city. Amar arrives and meets Auda, who reveals that he has united all the other tribes and intends to keep the Yellow Belt for them. Amar reveals that Nesib has agreed to Amar's conditions and demands that Auda turn over his army to Amar. During the discussion, Amar is shot dead by the sheikh of the Beni Sirri tribe, who had in fact been aiming for Auda. Auda's army is outraged, believing that Nessib had double crossed them and tried to kill Auda during the negotiations. Nessib's army commander realizes that with Amar dead, there is nothing to stop Auda from attacking. As the shells rain down, Auda gives up any hope for talks and leads his army to sweep over Nessib's defenses. Though Nessib has superior weaponry, the combined might of all the tribes in Auda's army overwhelms them by sheer numbers. \n\nAuda is knocked off his horse by a stray shell and beset upon by the Beni Sirri sheikh. The sheikh easily defeats him at close quarters combat and mocks him, asking whether Auda learned to fence 'in a library'. Just before the sheikh can land the killing blow, Aicha stabs him in the back and saves Auda. Learning about the developments, Nesib abdicates the throne in favor of his only remaining child, Leyla. Auda, through his marriage to Leyla is now ruler of both cities. In the city, Auda's forces find Hassan in the dungeons, indicating that he had never betrayed Amar, but had instead been held prisoner by Nesib. Auda walks into the library where he finds Nesib, who compliments him on his achievements and asks what Auda intends to do. Auda replies that unlike his father, he does not dislike foreigners and that he believes that they have much to offer each other. On being questioned by Auda about what to do with Nessib, he admits that if he were in Auda's place he would have had him killed, quickly and painlessly. Auda instead opts to send him to Houston to sit on the Board of Directors of the oil companies, where he can protect the interests of their people. Auda offers him a backhanded compliment saying that 'he can't think of anyone more cunning' than Nesib to fill the role and that the oil company people 'deserve him'. The film ends with Auda holding a meeting with several foreigners presumably representatives of the oil companies.\n\nWhile there was praise for its ambitious scope, production values and action, the film was overall poorly received, criticized for being tedious and slow, as well as the ethnicity of its lead actors (namely Banderas and Strong). \"Touted as the Arab breakthrough into the international cinema arena, \"Black Gold\" pits Mark Strong and Antonio Banderas against each other as warring emirs torn between the traditional ways and modern temptations. But despite its honourable intentions, \"Black Gold\" hits the ground with a terrible clunking thud, its broken-English dialogue squeezing the life out of it practically from the off,\" \"The Guardian's\" Andrew Pulver wrote in his analysis.\n\n"}
{"id": "6458575", "url": "https://en.wikipedia.org/wiki?curid=6458575", "title": "Buenos Aires tetra", "text": "Buenos Aires tetra\n\nThe Buenos Aires tetra (\"Hyphessobrycon anisitsi\") is a tropical fish from South America. It was first observed in the wild in 1907, by Carl H. Eigenmann.\n\nAn aquarium subject for over 60 years, the Buenos Aires tetra is a well loved aquarium fish, often admired for its flashy red color.\n\nThe Buenos Aires tetra originates from its namesake, South America. They are found in South America in the La Plata region of Argentina, Paraguay, and southeastern Brazil. In the wild, they are commonly found in rivers, ponds, lakes, and streams, namely, in the La Plata region. They are a freshwater fish, and do not do well in conditions that may offer too salty or too filthy of water. They swim in schools and are a social fish, always together with their own kind.\n\nThe tetra is a tropical, silver metallic-colored fish, with red-tipped fins and a black marking on the dorsal fin. The tetra is a hardy community fish for beginners, and is optimal for the beginning fish hobbyist. It is also relatively large for a tetra, growing up to 7 cm (2.8 inches). They can live up to 5 or 6 years. Its silver color picks up flashy neon highlights depending on how the light hits the fish. The top and bottom of the tail fin is generally red, along with the pelvic and anal fins. The dorsal fin may also have a hint of red at the very tip. Its most distinguishing characteristic is the caudal peduncle, or the connecting area between the tail and the rest of the body, which features a bold, black 'cross' shape.\n\nThe Buenos Aires tetra are very durable, and do not have any special needs or requirements. A maintained freshwater tank and food is all that they really need. However, they are hearty feeders that must be well-fed or they may begin to nip at their long-finned tank-mates. Keeping the tetras in a school of 5 or more definitely decreases aggression. This species can occasionally nip at and eat aquarium plants.\n\nThe Buenos Aires Tetras are an omnivorous species. In the wild they feed on worms, crustaceans, insects, and plants, but in the aquarium they will generally eat all kinds of live, fresh, and flake foods. To keep a good balance, give them a high quality flake food every day. To keep these tetras at their best and most colorful, offer regular meals of live and frozen foods, such as bloodworms, daphnia, and brine shrimp. Vegetables should also be added to their diet. Feed these tetras several times a day and only what they can consume in 3 minutes or less at each feeding.\n\nBuenos Aires tetras breed occasionally in an aquarium setting. They breed by scattering their eggs into their environment, hoping that they will be fertilized and lead to young. When breeding does occur, a hospital or \"breeding tank\" will be necessary. For optimal breeding conditions, use slightly acidic water. Much like most other species of fish, once the eggs begin to hatch, removing the parents will reduce the number of lost fry, as the parents will soon begin to eat their young. Once the eggs are laid, the fry will begin to hatch in about 24 hours. For the next three or four days, they will eat their egg sac and then become free swimming fry.\n\n"}
{"id": "36688188", "url": "https://en.wikipedia.org/wiki?curid=36688188", "title": "Cadec-online.com", "text": "Cadec-online.com\n\ncadec-online.com is a multilingual web application that performs analysis of composite materials and is used primarily for teaching, especially within the disciplines of aerospace engineering, materials science, naval engineering, mechanical engineering, and civil engineering. Users navigate the application through a tree view which structures the component chapters. cadec-online is an engineering cloud application. It uses the LaTeX library to render equations and symbols, then Sprites to optimize the delivery of images to the page.\n\ncadec-online.com implements micromechanics for composites reinforced with unidirectional fibers, and random fibers, as well as plain weave, twill, and satin textiles. It predicts lamina elastic moduli, strength values, coefficient of thermal expansion (CTE), moisture expansion, and other micromechanical properties. The application conducts this analysis through several theoretical models, including:\n\n\ncadec-online.com can calculate the three-dimensional (3D) stiffness and compliance matrices, the two-dimensional (2D) \"reduced\" stiffness and compliance matrices, in lamina coordinate system (cs). It is also capable of transforming the composite laminates matrices to any other coordinate system. Lamina types supported by the software include:\n\n\nThe application can carry out laminate analyses including calculation of laminate stiffness, stress, strain, and failure. The software supports intact and damaged laminates (see damage mechanics). For each category, cadec-online.com can calculate the laminate thermal stresses, laminate coefficient of thermal expansion, laminate stiffness and compliance matrices for composite laminates. Also, the application can predict the laminate moduli, which are orthotropic material equivalents for the stiffness of the laminate in both bending and membrane modes of deformation.\n\ncadec-online.com predicts failures such as first ply failure (FPF) and last ply failure (LPF) under mechanical, thermal, and moisture loads, as well as in situ effects, using several failure criteria (FC) including:\n\n\nUses discrete damage mechanics (DDM) to predict crack density vs. strain for any symmetric laminate subjected to any membrane state of strain. The results can be exported to Excel for plotting. The state variable describing the damage state of the material is the crack density in each ply. The thermodynamic force is the midsurface strain applied to the laminate. The relevant material properties are the fracture toughnesses in modes I (opening) and II (shear) of the ply.\n\nThe application can predict the stiffness and strength of composite materials reinforced with plain weave, twill, and satin textile, also called fabric. The textile lamina is idealized as a transversely isotropic material. The calculated textile lamina can be used as any other lamina in the rest of the application. The calculated properties include:\n\n\nThe application is able to analyze laminated composite thin walled beams with general cross sections. Beams can be asymmetric and loaded by general combinations of forces in three planes (axial, vertical and horizontal) as well as by three moments (torque and two bending moments). cadec-online.com computes section properties such as the shear center.\n\ncadec-online.com defines four different types of loads:\n\n\ncadec-online.com features an API that allows users to access virtually all of the capabilities present in the web version of the software from other software environments such as Abaqus, Ansys, Matlab, Python, .NET Framework, Mathematica, etc.\n\n\n"}
{"id": "41281419", "url": "https://en.wikipedia.org/wiki?curid=41281419", "title": "Cajanus scarabaeoides", "text": "Cajanus scarabaeoides\n\nCajanus scarabaeoides is a flowering plant in the genus \"Cajanus\" Of the 32 different species within the genus \"Cajanus\", only one, \"C. cajan\" (pigeonpea), is cultivated. \"Cajanus scarabaeoides\" is the closest wild relative to \"C. cajan\", and is one of the easiest wild species to cross with pigeonpea cultivars. \"C. scarabaeoides\" is found naturally in both temperate and tropical zones around the globe. This species has higher levels of drought tolerance, is found to have greater protein content, and has higher levels of resistance to insect pests compared to cultivated types. These genetic traits can be crossed with \"C. cajan\" to improve the crop’s productivity. For subsistence farmers, this can reduce economic losses and drastically improve overall crop yield.\n\n\"Cajanus scarabaeoides\" is a very close wild relative species of \"Cajanus cajan\" (common name, pigeonpea). It is a dicot angiosperm belonging to the Fabaceae family. \"C. scarabaeoidis\" may be an annual or a perennial, making it a flexible crop for subsistence farmers.\n\nThe branches of \"C. scarabaeoides\" can be straight or winding and up to 135 cm in length. \"C. scarabaeoides\" has pinnate leaves, typically arranged in a trifoliate manner with flowers that are yellow with red veins. The pods of \"C. scarabaeoides\" are oblong in shape, typically 11–34 mm in length and 6–10 mm in width. The seedpods are densely covered in a combination of short and long hairs and are typically a dark purple colour, containing anywhere from 1-7 seeds. The seeds of \"C. scarabaeoides\" range from 2.4–4 mm long, 1.8–3 mm wide, and 1–2 mm thick and are either black in colour or speckled. Compared to the pigeonpea cultivars, \"C. scarabaeoides\" has a higher pod seed percentage, 74% compared to 20%, and has more multiseed pods, on average 6.04 seeds compared to 3.0 seeds\n\n\"C. scarabaeoides\" is the most widely distributed wild species of \"C. cajan\" and is native to many countries in both temperate and tropical zones. It is native to Madagascar in Africa. In temperate Asia it is native to China, Japan and Taiwan. In tropical Asia it is native to Bangladesh, Bhutan, India, Nepal, Pakistan, Sri Lanka, Myanmar, Thailand, Vietnam, Indonesia, Malaysia, Papua New Guinea and the Philippines. In Oceania it is native to Australia and Fiji.\n\nIn Asia, \"C. scarabaeoides\" is the most commonly disbursed wild species and can be found in abundance in the Chinese provinces of Yunnan, Guizhou, Guangxim Guangdong, Hainan, Fuijan and Taiwan. In China there are several other names for this species. In Mandarin Chinese it is called “Man Cao Chong Duo”. In Guangdong dialect it is called “Shui Kom Ts’o”. In Yunnan dialect it is called “Jia Yan Pi Guo”.\n\n\"C. scarabaeoides\" occurs naturally in the wild, and can be found in open grassland and dry vegetation areas and in deciduous forests. It is often found along the ridges of cultivated fields, along roads or footpaths, or on hill slopes. It is typically found where there is a decent amount of sunlight, and populations tend to dwindle in dark bush areas or dense forests. This crop is known as a “creeper-climber” that supports itself on surrounding grass and small shrubs. In the Tiandong county in the Guangxi province of China, it can be found growing in wastelands at elevations of 180 m. In the Yi Oun Yang mountains, it can be found growing wildly in the dry hills and beside rivers. The vast areas in which this crop has the ability to grow provides advantages for farmers in both rural and peri-urban areas, as it can be supported by a variety of environments.\n\nWhile pigeonpea is already a particularly good crop in terms of resistance to drought, \"C. scarabaeoides\" has even greater drought tolerance properties and is therefore capable of thriving with very little annual rainfall.\n\nIn many accessions studied of \"C. scarabaeoides\", many have been found to flower early compared to pigeonpea cultivars. One ICRISAT study reports flowering in some \"C. scarabaeoides\" accessions as early as 34 days compared to 60 days and another ICRISAT study reports \"C. scarabaeoides\" accessions flowering within 70 days compared to 126 days. If \"C. scarabaeoides\" can be crossed with pigeonpea cultivars for this desirable trait, farmers can reduce harvest time and increase overall yield. With the world’s changing climate, this trait is useful to improve the long-term sustainability of the pigeonpea crop\n\nIn China, \"C. scarabaeoides\" is sometimes used as fodder, and has shown to be effective in reducing diarrhea in cattle. In addition, the leaves of the plant species have been used to improve indigestion in traditional medicines as well as limit the excessive production of urine.\n\nWithin the \"Cajanus\" species, the pod borer, \"Helicoverpa armigera\", is a major constraint that limits crop productivity. This insectivorous pest attacks the pods during the developmental stage, which reduces the total grain yield of the plant. This pest is very difficult to manage, largely due to its extensive host range and migratory capabilities. Additionally, \"H. armigera\" has become more resistant to certain insecticides in recent years, increasing the degree of difficulty to which management of this pest is possible.\n\nWild relatives of pigeonpea, specifically \"C. scarabaeoides\", have high levels of resistance to this destructive insect pest. The larval survival rate of \"H. armigera\" on \"C. scarabaeoides\" is only 21%, where it is 78% on pigeonpea. There is significant evidence showing that these species have different mechanisms than that of the cultivated types, all of which limit the ability for \"H. armigera\" to thrive on the plant. For scientists, breeders, and subsistence farmers, identifying these mechanisms can improve host plant resistance in cultivated types and reduce economic losses.\n\nResearch has been done on the types of plant trichomes that different \"Cajanus\" species possess. Typically, there are 5 types of trichomes found, where types A, B and E are glandular and types C and D are non glandular. \"C. scarabaeoides\" was found to have a greater proportion of type C, short non-glandular, and type B, short glandular, trichomes. It lacks the type A, long glandular, trichomes that cultivated pigeonpea possess. The high density of short, nonglandular and glandular trichomes on \"C. scarabaeoides\" act as a barrier against the young larvae of \"H. armigera\". This barrier prevents larvae from feeding on the pods, causing mortality due to starvation before they are able to reach maturity. \"H. armigera\" lays 80% of its eggs on the pod surface of \"Cajanus\" species, so possessing type C and B trichomes is extremely beneficial in contributing to larval mortality.\n\nOther research has been done on the chemical components extracted from the pod surface of \"C. scarabaeoides\" and compared with that of cultivated pigeonpea. The β-carophyllene and guaiene that is emitted from cultivated pigeonpea, which attracts \"H. armigera\", was found to be absent in \"C. scarabaeoides\". Acetone that was extracted from the pod surface of pigeonpea was found to stimulate larval feeding, where in \"C. scarabaeoides\" the extracts did not possess this characteristic. The water extracted from the pods of both pigeonpea and \"C. scarabaeoides\" showed greater antifeedant activity in that of \"C. scarabaeoides\". For scientists and breeders, being able to identify feeding stimulants associated with host plants and related insect pests allows for the selection of less susceptible genotypes during hybridization\n\n\"H. armigera\" is the most damaging insect pest to pigeonpea cultivars, causing annual yield losses of more than $300 million globally. A study in India in 1992-93 and 1997-98 showed an average yield loss from \"H. armigera\" as high as 90-100%. For what is typically a low value and easy to manage crop, identifying resistant cultivars and their genotypic traits can bring significant economic benefits for poor, subsistence farmers [10].\n\nSimilar to the cultivated types, \"C. scarabaeoides\" is rich in protein and essential amino acids. The seed protein content can range from 17.8-27%, typically being in the upper portion of the range, where cultivated types typically only have around 20% protein content. \"C. scarabaeoides\" leaves are also rich in protein, around 13%. This allows farmers to make greater use of the whole crop, as both the seeds and leaves can be eaten for their protein. Additionally, \"C. scarabaeoides\" is rich in the amino acids methionine and cysteine, around 3% of protein compared to only 2% in cultivated pigeonpea. These sulfur-based amino acids play an essential role in building the protein structures within this crop.\n\nThe sugar content in the pods of \"C. scarabaeoides\" was found to be much lower than that of cultivated species. Furthermore, the pods of \"C. scarabaeoides\" were also shown to have higher levels of condensed tannins. Studies suggest that these two mechanisms could be possible factors that limit the larval feeding and the growing ability of \"H.armigera\", respectively. These beneficial traits can be used by breeders to cross into cultivated types to improve insect pest resilience on pigeonpea.\n\nICRISAT currently maintains 213 accessions of 19 \"Cajanus\" species that represent a total of 9 countries. \"C. scarabaeoides\" comprises one of the largest collections at the gene bank, with a total of 102 accessions. By selecting for specific genetic traits in the wild species and incorporating these genes into the cultivated relatives, improved characteristics such as pest resistance and drought tolerance can improve the overall productivity and production of the crop. Improving the diversity of traits in \"Cajanus\" species can be favourable to farmers who face challenges of drought and have crops susceptible to pests\n"}
{"id": "947387", "url": "https://en.wikipedia.org/wiki?curid=947387", "title": "Chevrolet Tahoe", "text": "Chevrolet Tahoe\n\nThe Chevrolet Tahoe (and its rebadged version the GMC Yukon) is a full-size SUV from General Motors. Chevrolet and GMC sold two different-sized SUVs under their Blazer/Jimmy model names through the early 1990s. This situation changed when GMC rebadged the full-size Jimmy as the Yukon in 1991. Chevrolet waited until 1994 to rebadge the redesigned mid-size S-10 Blazer as the Blazer, renaming the full-size Blazer as the Tahoe. The name Tahoe refers to the rugged and scenic area surrounding Lake Tahoe in the western United States. The name Yukon refers to the Yukon territory of northern Canada. For the 1995 model year, the Tahoe and Yukon gained a new 4-door model slotting in size between the 2-door models and the longer wheelbase and higher passenger capacity to up to nine passengers like the Chevrolet Suburban and newly named Yukon XL.\n\nThe Tahoe is sold in North America, Central America, the Middle East (excluding Israel), Chile, Ecuador, Myanmar, Cambodia, Laos, Angola and Russia as a left-hand drive vehicle.\n\nThe Chevrolet Tahoe and GMC Yukon currently serve as a part of General Motors' full-size SUV family. Lengthened wheelbase models are available for both as the Suburban for Chevrolet and Yukon XL for GMC. A luxury Denali model joined the Yukon lineup in 1998. As of 2002, a Denali version of the Yukon XL has also been available as the Yukon XL Denali. The Cadillac Escalade is closely related to the Denali models of the Yukon. As of February 2014, the 2014 Tahoe was the top-ranked Affordable Large SUV in \"U.S. News & World Report\"s rankings.\n\nThe Tahoe has regularly been the best selling full-size SUV in the United States, often times outselling its competition by 2 to 1. \n\nThe new GMC Yukon was introduced in 1991 for the 1992 model year, while Chevrolet continued to use the K5 Blazer name through 1994. All were 2-door models through 1994. The Chevrolet K5 Blazer was discontinued after the 1994 model year. The Chevrolet Tahoe was introduced in 1995 for the 1995 model year with the addition of an all-new 4-door version. The Tahoe was \"Motor Trend\" magazine's Truck of the Year for 1996. It is named for Lake Tahoe on the California-Nevada border in the United States.\n\nThe Tahoe/Yukon were shorter than the Suburban on which they were based, but shared that vehicle's \"GMT400\" platform. This was a true truck chassis, and was based on that used in the Chevrolet Silverado full-size pickup truck. Both two-door and four-door models were produced in rear- and four-wheel drive.\nThe two-door weighs roughly while the four-door weighs approximately .\n\"AutoTrac\" full-time all-wheel drive and a programmable Homelink transmitter were added for 1998. The upscale Denali trim line to the Yukon was introduced in 1999 as these vehicles became popular with wealthy families.\n\nThe standard engine was Chevrolet's LO5 small-block V8, while a turbocharged Detroit Diesel V8 was available beginning in 1994.\n\nIn Mexico, the Tahoe 2-door was released in 1995, called the Chevrolet Silverado, and in 1998 the 4-door was released as the Silverado 4-door, and both were available in Base, LS, and luxury LT trim lines.\nIn Venezuela, the Tahoe 2-door was released in 1993 (4WD only), called the Chevrolet Grand Blazer, and in 1996 the 4-door was released as the Grand Blazer 4-door (2WD). In 1996 the 2-door was discontinued. In 1996 only Grand Blazer 4-door 4WD was available. \nIn Bolivia, 1995 the 4-door was released as the Tahoe 4-door (4WD).\n\nBeginning in 1994, GM began making numerous annual changes to the Blazer/Yukon, including:\n\nWhen the GMT800 platform based Chevrolet Tahoe/GMC Yukon was released for the 2000 model year, a 2WD Tahoe Limited and 4WD Tahoe Z71 remained in production on the GMT400 platform as special edition vehicles. These special edition vehicles were produced in the Arlington, Texas assembly plant for the 2000 model year only.\n\nThe Chevrolet Tahoe Limited was based on the Chevrolet Tahoe SS concept vehicle introduced in 1996 that never made it to production. It is reported that GM disliked the idea of an SS badged vehicle at a time when insurance companies were already demanding higher premiums for SUVs. The Tahoe SS prototype vehicles made in 1996 were painted either an unspecified metallic green or metallic blue, but the Tahoe Limited was produced only in \"Onyx Black\". The Tahoe Limited had a distinctive exterior appearance that included a factory equipped ground effects, a monochromatic theme with bumpers and grille painted in the same high gloss black as the body, removal of the roof rack, and fog lamps integrated into the front bumper. Other notable features of the Tahoe Limited included the Z60 high-performance chassis package (commonly known as the police package) which places the body of the vehicle lower than the 4WD Tahoe, two-tone gray and charcoal leather interior seating surfaces, a gauge cluster, Bilstein shock absorbers, a 3.42 or optional 3.73 rear gear ratio, a limited slip rear differential, an engine oil cooling system, and distinctive 16 inch Ronal R36 five-spoke aluminum wheels.\nThe Chevrolet Tahoe Z71 also exhibited a monochromatic appearance similar to the Tahoe Limited, but instead of \"Onyx Black\" the Tahoe Z71 was offered in either \"Light Pewter Metallic\", \"Victory Red\", \"Emerald Green Metallic\", or \"Indigo Blue Metallic\". Features of the Tahoe Z71 were similar to those of the Tahoe Limited with a few key differences to distinguish the 2WD Tahoe Limited from the 4WD Tahoe Z71. Features that differ from those previously mentioned on the Tahoe Limited included the Z71 off-road chassis package ( Bilstein shock absorbers, a standard 3.73 rear gear ratio G80 locking differential), color-keyed wheel flares, trim, grille and bumpers, underbody skid plates, inset driving lights built into the center of the front bumper, oversized two row radiator, two-tone leather seating surfaces in either a gray or neutral theme, distinctive taillamp lens protectors, black tubular side assist step bars, black brush guard, and distinctive 16 inch Alcoa five spoke 6 bolt polished aluminum wheels.\n\nThe L31 5.7 L Vortec V8 powerplant and 4L60E four-speed automatic transmission shared with other GMT400 Chevrolet Tahoe vehicles were not modified in these special edition vehicles, and as such these special editions were mostly appearance packages, albeit with unique-handling suspension options.\n\nOther than the Tahoe Limited Edition, the vehicle was redesigned and launched in March 2000 for the 2000 model year on the new \"GMT800\" platform, still shared with the full-sized pickups and SUV's. Two new engines replaced the old 5.7 L (350 cu in) small-block V8, and while both were smaller, both produced more horsepower but less torque. In Mexico, the GMT800 Chevy Tahoe is called a Chevrolet Sonora and used the front end from the 2003-2007 Silverado pickup. A 2-door GMT800 Tahoe prototype was made but never entered production.\n\nBoth vehicles received significant updates with only the grille, headlights and body-side moldings distinguishing one from the other. Both vehicles now featured softer lines as part of a more aerodynamic design. The interior was also updated with new seats, dashboard, and door panels.\nAll-new Tahoe and Yukon are launched. Side-impact airbags are standard for driver and front passenger, OnStar communications system optional, automatic headlamp control standard, Power-operated sunroof optional for first time, new uplevel 9-speaker audio system with rear mounted subwoofer, Driver Message Center, new PassLock II theft-deterrent system, Autoride Suspension system optional on Tahoe LT and Yukon SLT, all-new independent SLA front suspension with torsion bars and all-new five-link rear suspension with coil springs.\n\nNew colors \"Forest Green Metallic\" and \"Redfire Metallic\" are introduced while \"Dark Copper Metallic\" and \"Dark Carmine Red Metallic\" are canceled. Two-tone paint is discontinued, optional Z71 package available on LS 4x4 includes: tube side steps, unique lower molding and wheel flares with extensions, color-keyed grille, bumpers, door handles and mirrors, unique front foglamps, unique luggage carrier with rear roller (roof rack), specifically tuned shock absorbers, 17-inch wheels, P265/70R17 all-terrain tires and OnStar. 40/20/40 bench seat gets revised cupholder design, Tan/Neutral replaces Light Oak/Medium Oak interior trim. Onstar is now optional on Tahoe LS.\n\nTahoe LS receives a plethora of new standard features, including: six-way power adjustable seats, heated exterior mirrors with drivers-side auto-dimming feature, electric rear-window defogger and HomeLink Universal Transmitter. Base trim is discontinued. Vortec 5300 V8 is now flex-fuel E85 capable. Tahoe LT models ordered in \"Redfire Metallic\" now come with body-color front bumper cap, bodyside moldings and wheel flares. Premium ride suspension is made standard on all models (excluding Z71).\n\nGM's full-size SUVs saw a major refresh for the 2003 model year. New features include: StabiliTrak stability enhancement system, dual-level airbags, passenger-sensing system, adjustable brake and accelerator pedals with memory. New radio systems with Radio Data System (RDS), custom designed Bose audio system available on models with front bucket seats, XM Satellite Radio, and Panasonic rear-seat DVD entertainment system. Tri-Zone climate control with manual controls standard on LS and Z71, automatic controls standard on LT. Second-row bucket seats optional on models with leather seating surfaces. LT models feature power-adjustable, body-color exterior mirrors with power folding, memory and integrated turn signal indicators. \n\n2003 Tahoe's provided early compliance to the 2005 LATCH (Lower Anchors and Tethers for Children) federal safety standards. A redesigned instrument cluster featured a new Driver Information Center, which can monitor and report on up to 34 different vehicle system functions including: Service StabiliTrak, Ice Possible and Door Ajar. The interior was refreshed, including a new eight-button steering wheel that allows the driver to safely access new infotainment features as well as a redesigned center console. \n\nThere were also new improvements to the powertrain and electrical system for 2003. These included a new Electronic Throttle Control (ETC) system to improve throttle feel and new oxygen sensors to improve durability and reduce emissions during engine warm-up. Models sold in California received a more robust catalytic-converter to meet Ultra Low Emissions Vehicle (ULEV) standards. A new battery-rundown protection system automatically turns off the headlamps, parking lamps, and interior lighting if left on for more than 10 minutes with the key removed from the ignition. \n\nNew colors for 2003 include \"Sandalwood Metallic\" and \"Dark Spiral Gray Metallic\".\n\nThe Tahoe received minor updates for 2004, most notably newly designed 16 and 17-inch wheel choices and a tire pressure monitoring system. Hydroboost brakes and a front passenger seat belt reminder were also added as was a 7-to-4 pin trailer brake wiring adapter. 2004 was the final year Tahoe could be ordered with rear barn doors. \n\nThe 5.3L Vortec 5300 V8 is now rated at and .\n\nNew colors for 2004 were \"Dark Blue Metallic\", \"Silver Birch Metallic\", and \"Sport Red Metallic\".\n\nNew features for 2005 include, StabiliTrak standard on all models, Z71 package now available on 2WD models, upgraded tire pressure monitoring system, new OnStar system featuring Gen 6 hardware with analog/digital coverage and upgraded hands-free capabilities. OnStar is now standard on all trims. Touch-screen navigation system now optional. 160-amp alternator, new interior trim, and redesigned aerodynamic side-sill assist steps to improve efficiency. All Tahoes come standard with a rear liftgate and liftglass. \n\nA new all-electric cooling system helps with quiet operation as well as fuel efficiency. New aerodynamic changes, including a new front air deflector, help the Tahoe improve fuel efficiency by one MPG. \n\nThe 4.8L Vortec 4800 V8 is now rated at and .\n\nNew colors include \"Sandstone Metallic\" and \"Bermuda Blue Metallic\".\n\n2006 marked the final year of the Tahoe and Yukon on the GMT800 platform and because of this only minor changes were made. These changes included combining the OnStar and XM Satellite Radio antennas into one single unit, removal of the Chevrolet badging on the liftgate, relocating the catalytic converter closer to the engine to improve emissions performance and a new manual parking brake adjuster. \n\nThe flex-fuel capable Vortec 5300 V8 is now available on all retail packages. \n\nGeneral Motors redesigned the Tahoe and Yukon on the new \"GMT900\" platform in late 2005 as a 2007 model. A hybrid version of the Tahoe/Yukon, which uses the shared GM/Chrysler Advanced Hybrid System 2, followed with the 2008 models. The GMT900 based Tahoe and Yukon exceeded initial sales expectations and continued to sell well despite a weakening market for large SUVs. The short-wheelbase Tahoe and its police counterpart began production at Arlington Assembly on December 1, 2005. SWB Yukon production began in early 2006, with Janesville Assembly coming online as well. For the first time, GM used the Tahoe name in Mexico.\n\nFor 2007, the GMC Yukon and Chevrolet Tahoe received different front fascias, hood, and tail lights. The GMC Yukon boasted a monolithic grille and headlights, while the Chevrolet Tahoe grille was divided by a body-colored bar similar to the chrome bar found on the GMT800 Tahoe. Both the redesigned Yukon and Tahoe featured a more angular design that gave the vehicles a more upscale appearance than their predecessors. The interior was significantly redesigned as well, features a faux wood trim dashboard with chrome-accented instrument controls. New door panels, as well as new seats, were also added to the interior it still retains its nine-passenger seating availability on LS and SLE models only like the Chevy Suburban and GMC Yukon XL.\n\nHighway mileage was improved from to with the addition of Active Fuel Management cylinder deactivation. For 2009, the 6.2 L engine in the Yukon Denali got a power increase to , while a 6.2 L was added as an option for the Tahoe LTZ. A 6-speed 6L80 automatic transmission replaced the 4-speed on all trucks except 2WD models with the 4.8 L engine. The 2011 model has a NHTSA rollover risk of 24.6%, and a lateral roadholding of 0.79g.\n\n2009 LTZ Models had the option of the 6.2L Vortec 6200. This was only an option in 2009 and only in the LTZ trim.\n\n2010 models underwent a mild mid-cycle refresh including a slightly raised bumper, removal of the GM \"Mark of Excellence\" door badge, revised interior door trim, improved side structure, side torso airbags and optional side blind zone alert.\n\nFor 2012, the GMC Yukon offered a special Heritage Edition package, featuring unique Heritage Edition exterior badging, embroidered color-keyed carpeted floor mats, embroidered headrests, SLT-2 Equipment Group standard features (such as 10-way memory leather seats and power liftgate), and optional 20-inch wheels. The Heritage Edition added $1,970 to the price of the Yukon and Yukon XL and was offered in three colors, \"Heritage Blue\", \"White Diamond Tricoat\" and \"Onyx Black\".\n\nIn 2006, the 2007 Tahoe was featured on and promoted through Donald Trump's TV series, \"The Apprentice\", where the two teams put together a show for the top General Motors employees to learn about the new Tahoe. Also, The Apprentice sponsored a controversial online contest in which anyone could create a 30-second commercial for the new Tahoe by entering text captions into the provided video clips; the winner's ad would air on national television. An early example of User generated marketing, headed by ad agency Campbell Ewald, the campaign began to backfire when hundreds of environmentally conscious parodies flooded YouTube and Chevy's website critiquing the vehicle for its low gas mileage. Over 400 negative ads were created in total, however over 20,000 positive ads were created making the campaign, according to Chevrolet, a success, despite the negative media attention.\n\nThe Chevrolet Tahoe, Chevrolet Suburban, GMC Yukon, GMC Yukon XL, Cadillac Escalade, and Cadillac Escalade ESV had a shortened 2014 model year starting in June 2013, and was replaced with a new version in February 2014 as a 2015 model. The 2015 Tahoe/Yukon was supposed to be based on the next-generation Chevrolet Silverado, which was introduced in May 2013. Rumor was that the vehicles could go to monocoque crossovers; those were shelved, as General Motors plans to add to the traditional body-on-frame truck-based SUV, as the 2012 Chevrolet Colorado-based Trailblazer debuted in 2013 for international markets, the Colorado debuting in 2014 for North America (as a 2015 model), and truck owners wanting a smaller truck-based SUV, instead of the monocoque crossover. GM's citing of healthy sales for their full-size SUVs in 2012 was another reason to retain the BOF platform.\n\nThe Tahoe and Yukon are now built on the GMT K2XX platform and assigned as K2UC (for Chevrolet Tahoe) and K2UG (for GMC Yukon). Production on the Tahoe and Yukon began in December 2013 with the first completed SUVs being used for testing purposes, and started officially shipping the vehicles to dealerships on February 5, 2014. The 2015 Tahoe was later named the fastest-selling vehicle for the month of February 2014, averaging seven days of sales upon its release. \nOn September 12, 2013, GM released photos and press release of the fourth-generation Tahoe and Yukon. Like its larger siblings Suburban and Yukon XL, the front fascias of the Tahoe and Yukon are distinct, but from the base of the A-pillars back, they share most of the same styling cues. This now includes inlaid doors that tuck into the door sills, instead of over them, improving aerodynamics and fuel economy, and lessens interior noise. The hoods and liftgate panels now are made of aluminum in an effort to reduce vehicle weight. A more-efficient, direct-injected EcoTec3 powertrain coupled with improved aerodynamics help increase fuel economy for the SUVs. Both fourth-generation SUVs do not share a single piece of sheetmetal or lighting element with the brands' full-size pick-up trucks, and the front grilles of both vehicles are slightly altered to give them their own identity. The front headlights features projector-beam headlamps that flanks the Chevrolet-signature dual-port grille – chrome on all models, sweeping into the front fenders, while the LTZ trims feature high-intensity discharge headlamps and light-emitting diode daytime running lamps.\n\nAlso new are the addition of fold-flat second- and third-row seats, now a standard feature but can be equipped with an optional power-folding feature for the upgraded trims, and an additional two inches of leg room for second-row passengers. The third-row fold-flat feature is accomplished using a raised platform that reduces available cargo space behind the third-row seats. Standard third-row seats and raised \"fold-flat\" platform significantly reduce available cargo space compared with previous Tahoe models. Multiple USB ports and power outlets are now spread throughout their interiors, including one 110-volt, three-prong outlet, with the Tahoe adding an available eight-inch color touch-screen radio with next-generation MyLink connectivity along with an available rear-seat entertainment system (but will not feature a Blu-ray option that is exclusive to the Suburban for 2015), while the Yukon added a standard eight-inch-diagonal color touch screen radio with enhanced IntelliLink and available navigation.\n\nThe Yukon interior has more features including seats stuffed with dual-firmness foam, a standard Bose sound system and SD card slots, and laminated glass for the windshield and front windows, decreasing interior noise. GM's third-generation magnetic ride control suspension is optional on the Tahoe LTZ models, whose upgraded features include third-generation magnetic ride control, a real-time damping system that delivers more precise body motion control by \"reading\" the road every millisecond, and changing damping in just five milliseconds.\n\nThe new platform is based on the 2014 Chevrolet Silverado 1500 and GMC Sierra 1500. Both SUVs feature sound deadening material to improve cabin quietness. A new Cadillac Escalade and Cadillac Escalade ESV arrived in dealerships in April 2014. Models continued for the Tahoe and Suburban as LS, LT, and LTZ, and the Yukon and Yukon XL as SLE, SLT, and Denali. The Tahoe and Yukon went on sale in February 2014 as an early 2015 model year vehicle. Prices for the all-new GM SUVs are expected to stay the same as the current-generation GMT900 SUVs. The Tahoe Hybrid, Yukon Hybrid, and Yukon Denali Hybrid models will be dropped along with their powertrains, as well as the Cadillac Escalade Hybrid and Cadillac Escalade Hybrid Platinum. The Yukon Denali AWD is also being dropped as well. And despite reports that it is under consideration at this point, GM is mulling plans to bring the SS and RS package back for the Tahoe.\n\nOn August 29, 2014, Chevrolet announced that the Z71 package would return with a launch by leaking a photo that displayed the Z71 badge on one of its large SUVs. On September 26, 2014, Chevrolet debuted the updated Z71 Tahoe at the State Fair of Texas, along with the debut of the Texas Edition Tahoe, the latter due to Texas having the largest units of Tahoes sold in the United States (as of August 2014, sales of the Chevrolet SUVs in Texas were up 37 percent) and to celebrate the 60th anniversary of GM's Arlington Assembly plant; production began in October 2014. As with the previous Z71 Tahoe, this version will continue to be offered in a 4WD LT trim only, featuring a front skid plate, off-road tires mounted on 18-inch wheels, a unique grille, running boards and \"Z71\" identification inside and out. Fog lamps, front tow hooks and front parking assist are also included. The Texas Edition will be available in both LT and LTZ trims, featuring a maximum trailering package, twenty-inch polished aluminum wheels (on LT models), twenty-two-inch premium painted aluminum wheels (on LTZ models), and an exclusive \"Texas Edition\" badge. The Texas Edition Tahoe package will be part of a lineup that will also use the Texas Edition package, alongside the Suburban and Silverado.\n\nThe demand and interest in the redesigned Tahoe has also translated into a 108 percent sales spike since it went on sale in February 2014, with most dealers reporting the units being sold within 17 days after they arrive on the dealership lots, with most customers opting for the fully optioned LTZ model, making it one of Chevrolet's fastest-selling vehicles in 2014 along with Suburban, who posted higher sales and quicker inventory turnovers.\n\nOn March 23, 2014, a 2015 GMC Yukon caught fire and went up in flames in Anaheim, California, after a couple, who along with the sales representative who were giving them a test drive, noticed the vehicle stopping and that smoke was starting to make its way into the cabin, prompting the three individuals to park the car and escape before it was destroyed within 15 minutes. The cause of the fire was traced to an oil leak and an engine malfunction. Despite being an isolated incident, the 2015 Tahoe and Yukon are not believed to be tied to GM's announced recall of its vehicles that was made on March 17, 2014. But five days later on March 28, 2014, GM announced a recall on the 2015 Tahoe and Yukon in order to fix a “transmission oil cooler line that is not securely seated in its fitting”, causing the vehicle to stop and rupture the oil cooling line, resulting in the engine to malfunction and catching fire immediately. On June 6, 2014, GM issued another recall on the 2015 Tahoe and Yukon because their radio control modules may not work, and thus prevent certain audible safety warnings.\n\nThe Tahoe received 4G LTE, WiFi, and Siri capability, a new color palette, Brownstone Metallic, and added a hands-free power liftgate feature that is standard on LTZ, but included on LT with the optional Luxury Package. The MyLink with Navigation feature became standard on the LTZ trim, while E85 capability is removed from retail orders\n\nThe Yukon, in addition to receiving the aforementioned features, sees the 6.2-liter EcoTec3 V8 engine being updated with the new 8L90E eight-speed automatic transmission for the interim model year, allowing it to improve fuel economy.\n\nA body-colored painted Shark Fin antenna was added with the 2015 mid-year refresh for all models.\n\nFor the 2016 model year, the Chevrolet Tahoe received more upgraded changes and new features, similar to the ones added to the Suburban. The changes included power-adjustable pedals, forward collision alert, IntelliBeam headlamps, lane keep assist and a safety alert seat as part of the newly introduced enhanced driver alert package as an available option on the LS trim. The inside floor console with storage area-SD card reader was removed and a new infotainment system was introduced, officially ending the CD player era for the Tahoe; the 8-inch MyLink feature was expanded to the LS trim and became standard (replacing the 4-inch display), although the navigation feature remains as an option on LT and standard on LTZ. A new liftgate shield was added to the Theft Protection Package, along with the new lane keep assist which replaced the lane departure warning. The capless fuel fill tanks became standard on all trims. \"Siren Red Tintcoat\" and \"Iridescent Pearl Tricoat\" became the new color trims, replacing \"Crystal Red Tintcoat\" and \"White Diamond Tricoat\". The instrument cluster was re-configured with a new multi-color enhancement and a heads-up display was introduced as a standard only on the LTZ trim.\n\nThe 2016 GMC Yukon also sees similar changes, with liftgate, power, hands-free now packaged on SLT trims, a free-flow feature that replaced the Premium package, and two new premium colors (\"Crimson Red Tintcoat\" and \"White Frost Tricoat\") replacing \"Crystal Red Tintcoat\" and \"White Diamond Tricoat\" respectively.\n\nOn May 27, 2015, Chevrolet announced that the 2016 Tahoe will come equipped with both Apple CarPlay and Android Auto Capability features. However, only one of their phone brands at any one time can be used. The 2014 and 2015 LTZ and Denali packages come equipped with a \"wireless\" charging accessory. while the Android Auto option will only be available on LT and LTZ trims featuring 8 inch screens.\n\nOn July 10, 2015, GM announced that the Russian-built Chevrolet Tahoe will receive the 6.2L V8 L86 EcoTec engine, which will be the only engine offered in the region as an exclusive to the Russian and CIS markets only, as GM has no plans to make it available to North America for the time being.\n\nFor the 2017 model year, Chevrolet made upgraded changes to the Tahoe. New additions include the teen driver feature, the App Store feature, a rear seat reminder, and low speed forward automatic braking. Two new colors, \"Blue Velvet\" and \"Pepperdust Metallic\", were added to replace four colors. The 22-inch wheels option was expanded to three, and the rear seat entertainment system was overhauled. Chevrolet also made changes in the level trims, with the c-pillar badge now removed from LS models and the LTZ trim is renamed to Premier, which will continue to be the top of the line model.\n\nThe 2017 model year Yukon also received similar changes, but with a few exceptions. Two new colors, \"Dark Blue Sapphire Metallic\" and \"Mineral Metallic\" were introduced, the latter exclusive to the Denali, which is also adding a new 22-inch ultra bright aluminum wheels with midnight silver premium paint and a head-up display to its features. The interior backlights changed from red to blue. The heated and vented driver and front passenger seats are now standard on the SLT and Denali trims.\n\nFor the 2018 model year, the 6.2-liter EcoTec3 V8 engine will be available in the Tahoe RST (when bought with the Performance Package). All M.Y. 2018 Tahoe and Suburban trim levels will now be standard with LED Daytime Running Lights which will replace the high beam projector DRLs on LS and LT trim levels. The 2018 Tahoe will have the Custom Edition package added as an option on the base LS trim level. The Chevy Tahoe LS trim level will feature a chrome grille, 18\" painted aluminum wheels, 18\" all season blackwall tires, and third row seat delete (which reduces passenger capacity to 5 seats from 8 seats) if ordered with the Custom Edition package. The GMC Yukon received a 10 speed automatic transmission on the Denali trim level and the GMC Yukon is facelifted for 2018 with a refreshed grille similar to the one from the 2017 GMC Acadia and 2018 GMC Terrain. \nThe 2019 model year Tahoe/Yukon will only see minor changes. One exterior color, \"Shadow Gray Metallic\", will replace both \"Havana Metallic\" and \"Tungsten Metallic\", and the Premier model receives a newly redesigned badge on the liftgate.\n\nThe GMC Denali nameplate started as the luxury version of the Yukon for the 1999 model year. The Denali is available on both standard and XL versions and is the top-of-the-line trim in the GMC Yukon lineup.\n\nIn 1998, at the time of its introduction, the Yukon Denali was GM's answer to the Lincoln Navigator, but then GM introduced a clone to the Yukon Denali and rebadged it \"Escalade\" under the Cadillac nameplate. The Yukon Denali's exterior was shared with the Cadillac Escalade, with the entire front clip and lower side body panels differing from the standard Yukon. In the interior, in addition, the Denali features luxury options not available in the Yukon. These included an upgraded leather interior, power seats, heated seats front and rear, Bose stereo system, and some woodgrain on the dashboard. The 1999 Yukon Denali and Escalade also saw the first application of GM's OnStar communications system in a full-size SUV. \n\nEven though the Yukon was redesigned alongside the Chevrolet Suburban and Tahoe in 2000, the Denali and Escalade remained on the GMT400 platform. It was not until 2001 that the Denali and Escalade were redesigned. While the Escalade departed from its Yukon based exterior design scheme in attempt to hide its roots, the Yukon Denali's exterior is almost the same as that of a post-2000 model year GMT800 Yukon. Embossed side body panels and slightly reworked headlights with projector-beam lenses along with 17\" polished wheels and a unique grille and front bumper differentiate the Yukon Denali from the regular Yukon. 2001 also saw the introduction of the \"punch\" grille which now has become the hallmark of the Denali nameplate.\n\nThe Yukon Denali was redesigned for the 2007 model year alongside the regular mainstream Yukon. The biggest change was in the styling, such as the flattened tailgate reminiscent of the new Tahoe, and particularly the grill and headlight shapes, which made the Yukon look less aggressive than previous models. The only exterior difference between the Yukon Denali and the standard Yukon are the chrome grille and extensive use of chrome accents; and of course the insignias, embeveled rockerpanels, chromed headlights and the Vortec 6200 engine which it shares with the Cadillac Escalade.\n\nThe Yukon Denali was redesigned and updated along with other GM SUVs in September 2013 and went to dealerships on February 5, 2014 as a 2015 model. The Yukon Denali continues with the top-of-the-line features (using the similar features found in the Tahoe LTZ trim) and the front grille honeycomb design, equipped with a new active noise-cancellation technology and GM's third-generation magnetic ride control suspension as a standard feature. In November 2014 The Yukon Denali saw its MSRP bumped up by $1,300 in part due to the loaded features added to its 2015 mid-year updates.\n\nThe Tahoe made its hybrid electric debut in late 2007. In January 2008, starting price was US$50,540. The starting price of the 2009 model was increased to US$51,405.\n\nThe Chevrolet Tahoe Hybrid uses a combination of its dual displacement 3.0/6.0 L V8 engine and two (continuous) electric motors that charge a 300-volt nickel-metal hydride battery. The vehicle can run on either gasoline, electricity or a mixture of the two using automatic Two-Mode Hybrid system that monitors the vehicle's torque, and state of the battery to choose the most efficient source of power. The battery is charged either directly by generating electricity through driving one or both electric motors using the gasoline engine (while the vehicle is coasting or being driven by the gasoline engine), or by the wheels driving one or both electric motors through what is called \"Regenerative braking\" when the vehicle is decelerating, thus regaining some of the energy invested in forward momentum. The Tahoe is considered a strong or full hybrid, in that it can run entirely on the battery (for a limited range) at low speeds. In city driving, the EPA rating of fuel consumption for the 2WD version of the hybrid is . In comparison, non-hybrid varieties of the Tahoe are rated no higher than in city driving. In highway driving, the EPA rating is .\n\nThe Tahoe and Yukon Hybrid models were discontinued after the 2013 model year as GM moved to make their SUVs more fuel efficient with the introduction of the EcoTec engine that they would later install in the 2015 Tahoe and Yukon.\n\nIn Brazil the Tahoe GMT400 was sold under the name of \"Grand Blazer\", sourced from Argentina with rear-wheel drive only, regionally-sourced 6-cylinder engines (the late-type 4.1 M.P.F.I. Chevrolet straight-6 as the only gasoline-powered option and the 4.2L MWM Sprint 6.07T for those who preferred Diesel) and manual transmission. The GMT400 is used by Brazilian elite police units, such as the BOPE (Batalhão de Operações Policiais Especiais) in Rio de Janeiro and the ROTA (Rondas Ostensivas Tobias de Aguiar) and GATE (Grupo de Ações Taticas Especiais) in São Paulo. It is also used by the São Paulo State Police and the Rio Grande do Sul State Police. But as the cars grew old, the great majority of them were substituted, mainly by the smaller Chevrolet Blazer midsize SUV, the locally imported Ford Crown Victoria and Ford Police Interceptor Sedan. The Tahoe name was not used in Brazil because it would be unfamiliar to most Brazilians since it refers to the Lake Tahoe on the border of Nevada and California.\nAnother South American country, Chile, has incorporated this vehicle as transport unit for special operations unit (GOPE) of the Chilean Police (Carabineros de Chile) to carry communication equipment.\n\nThe Tahoe was also assembled in Venezuela from CKD kits for 3 generations, being phased out in 2014 along most of the local Chevrolet range due to unfavorable economic conditions and political circumstances. The 4th generation of the Tahoe has not been available in Venezuela.\n\nIn Ecuador, due to fiscal benefits to hybrid vehicles, the 3rd generation of the Tahoe had been offered only in the Hybrid versions until 2011. A review of the benefit to exclude hybrid vehicles with a displacement greater than 1.8L rendered the Tahoe Hybrid less cost-effective than the non-hybrid in Ecuador.\n\nIn North America, the Tahoe is used by many law enforcement agencies, fire departments, and EMS agencies. Prior to the announcement of the Z56 police package model the civilian base & LS models based on the GMT400 were used in police service. During the 1997 model year, the Tahoe was offered with the Z56 police option using suspension components from the discontinued 454SS truck – the first Tahoe Z56s were available only in 2WD until the GMT400 was phased out. The original prototype had rear disc brakes based on the B-body 9C1s whereas the production Z56s came with rear drum brakes. Plans for outfitting the Tahoe with the Z56 police package originated around the 1994 model year when GM broke news about phasing out its B-platform sedans (Caprice, Impala SS, Roadmaster) at the end of the 1996 model year where a replacement was imminent since GM ended production of its body-on-frame passenger sedans due to SUV sales. Since the introduction of the GMT900, Chevrolet currently offers two versions of the police package Tahoe; a four-wheel-drive version and a two-wheel-drive version. Just like the GMT400 based models civilian GMT800 Tahoe base & LS models were also used for police use until the Z56 police package option was reintroduced in 2004 late in the GMT800's life cycle by some agencies.\n\nChevrolet refers to the four-wheel drive (4WD) version as \"Special Service Vehicle\" (SSV) which has the 5W4 code. This version of the Tahoe can be used for all purposes except pursuits and high speed responses due to its high center of gravity just below the front window (height, not location), thus having a higher probability of rolling-over at high speeds. This version is preferred by agencies where snow, ice, flooding, rough terrain, and ground clearance are common issues.[34]\n\nChevrolet refers to the two-wheel drive (2WD) version—also known as the rear-wheel drive (RWD) version—as \"Police Pursuit Vehicle\" (PPV). This version be used for all purposes including pursuits and high speed responses. The center of gravity in this vehicle is lower than that of the four-wheel-drive version and the ground clearance is about 1 in (25 mm) less, with a standard rear bumper replacing the tow hitch on civilian Tahoes. Highway patrol agencies prefer the two-wheel-drive version, where pursuits and long distance responses are more common. The two-wheel-drive Chevrolet Tahoe is the only pursuit-rated SUV on the market today, and as of the 2012 model year, the last body-on-frame, rear-wheel drive police vehicle manufactured for the United States market since Ford phased out its aging Panther platform while the Volvo XC90-based Ford Police Interceptor (known as the Ford Explorer in civilian trim). Other SUVs like the Ford Expedition are used by many law enforcement and EMS agencies, but are not pursuit rated.[35]\n\nOn November 11, 2013, Chevrolet announced that they will build a 2015 Chevrolet Tahoe Police Patrol Vehicle, that was previewed as a concept at the 2013 SEMA show in Las Vegas. The vehicle became available in 2WD and 4WD drivetrains with orders coming in the first quarter of 2014 for special service models, followed by the orders for the police pursuit models afterwards.\n\nWhen production of the CUCV II ended in 2000, GM redesigned it to coincide with civilian truck offerings. The CUCV nomenclature was changed to Light Service Support Vehicle in 2001. In 2005, LSSV production switched to AM General, a unit of MacAndrews and Forbes Holdings. The LSSV is a GM-built Chevrolet Silverado 1500, Chevrolet Silverado 2500 HD, Chevrolet Tahoe, or Chevrolet Suburban that is powered by a Duramax 6.6 liter turbo diesel engine. As GM has periodically redesigned its civilian trucks and SUVs from 2001 to the present, LSSVs have also been updated cosmetically.\n\nThe militarization of standard GM trucks/SUVs to become LSSVs includes exterior changes such as CARC paint (Forest Green, Desert Sand, or 3-color Camouflage), blackout lights, military bumpers, a brush guard, a NATO slave receptacle/NATO trailer receptacle, a pintle hook, tow shackles and a 24/12 volt electrical system. The dashboard has additional controls and dataplates. The truck also can be equipped with weapon supports in the cab, cargo tie down hooks, folding troop seats, pioneer tools, winches, and other military accessories. In the Canadian Army these vehicles are nicknamed \"Milverado\".\n\nThe Enhanced Mobility Package (EMP) option adds an uprated suspension, 4-wheel anti-lock brakes, a locking rear differential, beadlock tires, a tire pressure monitoring system and other upgrades. About 2,000 LSSV units have been sold to U.S. and international military and law enforcement organizations.\n\n\nChevrolet TAHOE RST 2018\n\n"}
{"id": "53538665", "url": "https://en.wikipedia.org/wiki?curid=53538665", "title": "Coal mining in Bangladesh", "text": "Coal mining in Bangladesh\n\nCoal mining in Bangladesh started with the discovery of the first coal mine in Jamalganj in 1962. Mining in Bangladesh consists mainly of coal , granite, natural gas, and petroleum mining.\n\nIn Pre-partition era of British India, coal from Garo hills of Meghalaya was transported and traded through East Bengal. Mining offices opened in Dhaka and trade was aided by British Rail network in India. In 1961 UN-Pak Mineral Survey Project started surveys in then East Pakistan (today Bangladesh) by Geological Survey of Pakistan. In 1962 the survey found 1.05 million ton of coal in Jamalganj, Sunamganj District. The next big discovery came in 1985 when Geological Survey of Bangladesh discovered coal in Dinajpur. In 1989 a coalfield was discovered in khalashpir, Rangpur District and another in 1995 in Dighipara by the Geological Survey of Bangladesh. Multinational BHP Billiton discovered Phulbari coalfield in 1997. Global Coal Management Plc is waiting approval for the Phulbari coalfield. Bangladesh has an estimated 2 billion tones of coal in underground reserves in the Northwest region of the country. Bangladeshi coal reserves are underexplored because of concerns related to method, technology, and social consequences. Coal and limestone mined in Meghalaya was transported to Bangladesh, and helped the establishment of cement factories there. Coal accounts for two percent of the energy generation in Bangladesh. The government of Bangladesh has not established a unified coal policy. The Government of Bangladesh plans to increase power generation from coal to 50 percent from the present 2 percent by 2021.\n\nBarapukuria coal mine is run by the Barapukuria Coal Mining Company Limited. Barapukuria Coal Mining Company Limited is a subsidiary of the state owned Petrobangla. The mine is located in Dinajpur, this is the only active mine in Bangladesh. The locals near the mine have reported damages to their houses. The water bodies near the mine have dried up. In response the government has acquired lands near the mine.\n\nIn July 2018, it was revealed that the around 142 hundred thousand tons of coal, worth over BDT 227 billion, has been misappropriated by the officials of the Barapukuria Coal Mining Company Limited. The incident became popular as Barapukuria coal scam.\n\n"}
{"id": "12684798", "url": "https://en.wikipedia.org/wiki?curid=12684798", "title": "Cotton paper", "text": "Cotton paper\n\nCotton paper, also known as rag paper, is made using cotton linters or cotton from used cloth (rags) as the primary material. Important documents are often printed on cotton paper, because it is known to last many years without deterioration. Cotton paper is superior in both strength and durability to wood pulp-based paper, which may contain high concentrations of acids, and also absorbs ink or toner better. Different grades of cotton paper can be produced.\n\nHigh-quality cotton fibre paper is known to last hundreds of years without appreciable fading, discoloration, or deterioration, so it is often used for important documents, such as the archival copies of dissertations or theses. As a rule of thumb, for each percentage point of cotton fibre, a user may expect one year of resisting deterioration by use (the handling to which paper may be subjected). Legal document paper typically contains 25% cotton. Cotton paper will produce a better printout than copy paper because it can more readily absorb ink or toner.\n\nCotton paper is typically graded as 25%, 50%, or 100% cotton. Usually it can be checked by holding the cotton paper up to the light and looking just below the watermark for a number. 100% cotton paper may contain small amounts of acids, and should be tested or certified before use for archival documents.\n\nSecond-cut cotton linters have a normal average fibre length of 1.45 µm, and have similar properties as a short softwood pulp.\n\nCotton bond paper can be found at most stores that sell stationery and other office products. Some cotton paper contains a watermark. It is used for banknotes in a number of countries. These banknotes are typically made from 100% cotton paper, but can also be made with a mixture of 75% or less flax. Other materials may also be used and still be known as \"currency paper\".\n\nHigher quality art papers are often made from cotton.\n\nIt has found extensive use as a printed circuit board substrate when mixed with epoxy resins and classified into CEM 1, CEM 2 etc.\n\nCotton was first used with a mixture of silk to make paper called Carta Bombycina. In the 1800s, fiber crops such as flax fibres or cotton from used cloths (rags) were the primary material source. By the turn of the 20th century, most paper was made from wood pulp, but cotton is still used in specialty papers. As cotton rags now often contain synthetic fibres, papermakers have turned to second-cut cotton linters as raw material sources for making pulp for cotton papers.\n\n"}
{"id": "35047495", "url": "https://en.wikipedia.org/wiki?curid=35047495", "title": "E-Cycle Washington", "text": "E-Cycle Washington\n\nE-Cycle Washington is an electronics recycling program managed by the US state of Washington. It allows consumers and businesses with <50 employees to recycle electronics free of charge.\n\nThe Washington State legislature passed a law in 2006 which requires manufacturers of certain electronic products to be responsible for recycling their products at their end of life. The manufacturers were allowed to determine themselves how to pay for the program. 212 manufacturers created an industry association for this purpose which charges manufacturers based on their market share and the amount of items being recycled. As of 2014, over 400 manufacturers participate in the program.\n\nThe law required at least one collection point in each county plus one in each city whose population exceeds 10,000. Most collection points are recycling businesses and thrift stores. Currently there are over 340 collection sites and services.\n\nThe state began collecting TVs, computers and monitors for free recycling in 2009 and has since added tablet computers, e-readers and portable DVD players. In its first year of operation, 38 million pounds of electronics were collected. This greatly exceeded the forecast of 26 million pounds, and the total collected does not include working items that were resold by thrift stores. In the first 6 years of operation the program collected over 253 million pounds for recycling. See ecyclewashington.org\n\nhttp://www.quicklaptopcash.com/blog/the-impacts-of-lithium-ion-batteries-on-our-environment"}
{"id": "15049153", "url": "https://en.wikipedia.org/wiki?curid=15049153", "title": "Edge banding", "text": "Edge banding\n\nEdge Banding, or edgebanding, is the name of both a process and an associated narrow strip of material used to create durable and aesthetically pleasing trim edges during finish carpentry.\n\nEdge banding is used to cover the exposed sides of materials such as plywood, particle board or MDF, increasing durability and giving the appearance of a solid or more valuable material. Common substitutes for edgebanding include face frames or molding. Edge banding can be made of different materials including PVC, ABS, acrylic, melamine, wood or wood veneer.\n\nTraditional edge banding was a manual process requiring ordinary carpentry tools and materials. In modern applications, particularly for high-volume, repetitive manufacturing steps such as cabinet doors, edge banding is applied to the substrate by an automated process using a hot-melt adhesive. Hot melt adhesives can be water or solvent based and may consist of various raw materials including EVA, PUR, PA, APOA, and PO. A substrate primer may also be used as a bonding agent between the adhesive and the substrate. Thicker edge bandings typically require a slight concavity to provide a tight glue line. The thickness can vary from .018\" to 5mm or even more. The machine that applies the edge banding is called edgebander. An edgebander bonds the edge banding to the substrate, trims the leading and trailing edges, trims top and bottom flush with the substrate, scraps any surplus, and buffs the finished edge.\n\n"}
{"id": "55002318", "url": "https://en.wikipedia.org/wiki?curid=55002318", "title": "Einride", "text": "Einride\n\nEinride is a Swedish transport company based in Stockholm, Sweden. The company was founded in 2016 by Robert Falck, Filip Lilja and Linnéa Kornehed. and specializes in Self-driving vehicles. They are known for creating the T-pod and the T-log, vehicles designed from the ground up for fully autonomous and remote operation.\n\nThe company was established in 2016 and is one of the automotive companies that are pursuing Self-driving vehicles. The company got its name from the Nordic god Thor, meaning the lone rider. In the Spring of 2017, the company introduced a new type of transport vehicle, the T-pod, which is unique due to the vehicle not having a cabin and for being fully electric truck. The first full-scale prototype of the T-pod was revealed on July 4, 2017 at Almedalen Week in Visby, Sweden. Einride has announced their partnerships with Lidl in 2017 and DB Schenker in 2018. On July 12th, 2018 at the Goodwood Festival of Speed, Einride launched the T-log, an autonomous and all-electric logging truck. \n\nEinride uses self-driving technology as well as remote operation for the T-pod which allows drivers to monitor multiple vehicles and remotely control the vehicle in difficult traffic situations. On March 28, 2018, Einride announced their partnership with NVIDIA and announced that they will be using Drive PX-series technology in the T-pod. The T-pod can travel 124 miles on a fully charged battery, and Einride is developing charging station solutions to power the vehicles for longer journeys. Einride's goal is to use this technology to create a sustainable transport system free from carbon dioxide. The company plans to have 200 T-pods operate between Gothenburg and Helsingborg by 2020.\n\nAs of August 2018, Einride offers two vehicle models: the T-pod and the T-log.\nThe ‘T-pod,’ an electric self-driving vehicle that is remotely controlled by drivers, is significantly smaller than today’s heavy trucks and works alongside its innovative charging stations and pioneering infrastructure. The T-pod is about 23 feet long and has an operating weight of 20 tons. It is controlled remotely by an operator, which provides the advantage of human flexibility and decision-making, but it also has the ability to take advantage of a self-driving system. \n\nThe T-log is an autonomous, all-electric logging truck and is more powerful than the T-pod. The T-log incorporates some off-road capabilities and is designed to navigate forest roads. the T-log is an environmentally and health friendly alternative to diesel powered trucks. Equipped with cameras, lidars and radars, it has 360-degree awareness of its surroundings - no blind spots, no dead angles. \n"}
{"id": "32012319", "url": "https://en.wikipedia.org/wiki?curid=32012319", "title": "El Quimbo Dam", "text": "El Quimbo Dam\n\nThe El Quimbo Dam is a concrete faced rock-fill dam (CFRD) and hydroelectric power project under development in the Huila Department of southwestern-central Colombia, approximately south of the city of Neiva, on the Magdalena River. It is located about upstream from the confluence of the Páez River with the Magdalena River. Its works were officially opened on February 25, 2011 in the presence of President Juan Manuel Santos. It is one of the largest infrastructure projects in the country. The project is planned to be completed over a period of 4 years, in 2015.\n\nThe project will have a powerhouse near the base of the dam with an installed capacity of 400 MW, which is expected to achieve an average energy generation of /year, with a dam that will have a live storage of and an inundated area . The objective is to enhance the energy security and stability of the Colombian electricity supply, meeting about 8% of energy demand in Colombia with energy prospects of to 2034.\n\nEnvironmental license to implement the project was granted by Colombia's Environment Ministry, which was announced by Alvaro Uribe, the then President of Colombia, in May 2009. It is the first private sector hydro project to be built in Colombia under a new government policy. Apart from implementing the project, environmental issues that are to be addressed by the Emgesa, the project developer, are compensatory afforestation, compensation to project-affected people (no indigenous people are affected) and the protection of water supplies.\n\nEmgesa will receive, under a 20-year power contract authorized by Colombia's Comision de Regulacion de Energia y Gas (CREG), a price of US$14 per megawatt-hour for the power sold to the department. It is also intended to sell the electricity to neighbouring countries, thus boosting the national economy. The project is estimated to cost around $837 million, invested by Spanish utility Endesa, through its Colombian subsidiary Emgesa.\n\nThe project is located in Colombia, in the Magdalena River basin formed by the central and eastern mountain ranges, to the south of department of Huila. It is bounded between the coordinates and . The administrative jurisdiction of the project covers the municipalities of Garzón, Gigante, El Agrado, Paicol, Tesalia and Altamira. However, the dam and the project's powerhouse are located within the municipality of Gigante. The existing Betania Dam is about downstream. Bogotá, the capital of Colombia is to the north. It is to the south of Neiva, from Gigante and from Garzón.\n\nThe project area lies in the narrow gorge section of the Magdalena River. The rock formation is of friable sandstones of tertiary age. It is upstream from the confluence of the Magdalena and Páez Rivers. The contractor will provide about 48,000 million pesos to build the perimeter road that will boost tourism and commercial development southwest of the Huila Department.\n\nThe proposed hydroelectric project, a run-of-river reservoir scheme, consists of a high concrete-faced rock fill dam (CFRD) on the Magdalena River. The length of the dam is . There is also an auxiliary dam (an auxiliary dike) of height and crest length. The full reservoir level in the dam is above sea level (asl) and foundation level is asl. The reservoir created by the dam has an area of and stretches over a length of , with an average width of . Other features of the project include a diversion tunnel to facilitate construction of the dam (to divert river flows away from the working area of the dam) which is long, a spillway structure to route the design flood discharge, two intake structures (spaced at ) to divert water through two penstock lines ( length) to the powerhouse located on the downstream.\n\nEmgesa has committed to buying of land for 17,000 million pesos to connect two forest reserves in the area of influence (Reserva Forestal de la Amazonía y la Reserva Forestal Central). There are also plans to build a viaduct over the reservoir that will connect the municipalities of Garzón and El Agrado. The decision to build this major dam as a concrete faced rockfill dam was decided after studying several types of dams for the prevailing site conditions. The site conditions, which dictated the choice of the dam, are the narrow gorge of the river and the geological formation of highly friable sandstones at the dam site. The project area is subject to earthquakes and seismic factors have been accounted for in the design of the dam and appurtenant works. Nine earthquakes were experienced in Colombia between 1762 and 1994 with the earthquake event of 2 February 1736 with magnitude of 6.3 on Richter scale to last one on 6 June 1994 of magnitude 6.6 with epicentre of 73 km distance of the project site, which caused severe damage and fatalities. Twenty-three active unstable slope areas, with two potentially unstable land slides (mud slides) are also noted in the project area.\n\nConstruction materials involved in building the project complex include: concrete-; surface excavation–; underground excavation–; rockfill embankment–; and steel reinforcement–15,000 tonnes.\n\nThe powerhouse located at the toe of the dam has two vertical axis Francis turbines each of 200MW capacity. Each is designed for a discharge of . The generators are of the synchronous type with a generation capacity of 225 MVA. The generation voltage is 13.8 kV. Single-phase transformers of 75 MVA capacity with a voltage relation of 13.8–230 kV are proposed. A switch yard is located adjoining the powerhouse. Power is evacuated through 230 kV transmission line of length from the switch yard to Betania-San Bernardino transmission system.\n\nThe ecological and social impacts have been examined in great details and remedial actions have been planned for all the identified impacts. The major impacts are submergence of land (both agricultural and forest areas), displacement of people coming under submergence, submergence of a bridge over the Yaguilga River, submergence of the church of San José de Belén, the submergence of cocoa orchards of Río Loro, 78 archaeological sites between two archaeological regions, effect on fisheries, terrestrial fauna due to submergence and submergence of infrastructure works.\n\nThe project will require evicting 467 families and flooding of prime land in this region. The developer has to ensure that the project affected people have the same or better quality of life in the new areas where they would be resettled with \"all public services, health coverage and education, restoration of economic activity and level income equal or greater than the current conditions\". Irrigation facilities shall be provided to the resettled families for 5200 ha with facilities for organic production as a measure for the conversion of soil and natural animal management; reforestation in a new land area acquired by the developer shall cover 11079.6 ha to compensate for the loss of 3034ha due to submergence, which shall also ensure the restoration of tropical dry forest for 5 years; shift the church of San José de Belén and rebuild at the new location; removal all vegetation in the submergence area before reservoir filling to prevent decomposition of vegetation and growth of aquatic macrophytes; create an information base on the archaeological sites coming under submergence with due research; repopulate migratory species of fish in the upstream areas of the river and also establish two limnigraph stations; create suitable dumping areas to dispose excavated material which are not used on the project; establish alternate roads, bridges, water supply works and other infrastructure facilities that would be submerged; and rehabilitate of fauna coming under submergence areas by measures such as chasing, capturing and relocation of the fauna (small, medium and large mammals, serpents and birds) and also establish two veterinary units to treat wounded animals.\n\nA ferry and six landings are planned to tap the tourism potential of the new dam; 25,000 million pesos have been set aside for this. The reservoir will also benefit the development of fish farming.\n\nOn February 21, 2012, \"Revista Semana\" published a report that makes reference to a video produced by Bladimir Sánchez, a Communicator from the Huila Department. The video documents events that took place between February 14 and 15, 2012 during the eviction of protesters such as local fishermen and miners by the Colombian Police Force. Several were injured by the excessive use of force demonstrated by the Police during these events.\n"}
{"id": "5242205", "url": "https://en.wikipedia.org/wiki?curid=5242205", "title": "Energy Security Act", "text": "Energy Security Act\n\nThe Energy Security Act was signed into law by U.S. President Jimmy Carter on June 30, 1980.\n\nIt consisted of six major acts:\n\n\n"}
{"id": "3010589", "url": "https://en.wikipedia.org/wiki?curid=3010589", "title": "Flory–Huggins solution theory", "text": "Flory–Huggins solution theory\n\nFlory–Huggins solution theory is a mathematical model of the thermodynamics of polymer solutions which takes account of the great dissimilarity in molecular sizes in adapting the usual expression for the entropy of mixing. The result is an equation for the Gibbs free energy change formula_1 for mixing a polymer with a solvent. Although it makes simplifying assumptions, it generates useful results for interpreting experiments.\n\nThe thermodynamic equation for the Gibbs energy change accompanying mixing at constant temperature and (external) pressure is\n\nA change, denoted by formula_3, is the value of a variable for a solution or mixture minus the values for the pure components considered separately. The objective is to find explicit formulas for formula_4 and formula_5, the enthalpy and entropy increments associated with the mixing process.\n\nThe result obtained by Flory and Huggins is\n\nThe right-hand side is a function of the number of moles formula_7 and volume fraction formula_8 of solvent (component formula_9), the number of moles formula_10 and volume fraction formula_11 of polymer (component formula_12), with the introduction of a parameter formula_13 to take account of the energy of interdispersing polymer and solvent molecules. formula_14 is the gas constant and formula_15 is the absolute temperature. The volume fraction is analogous to the mole fraction, but is weighted to take account of the relative sizes of the molecules. For a small solute, the mole fractions would appear instead, and this modification is the innovation due to Flory and Huggins. In the most general case the mixing parameter, formula_13, is a free energy parameter, thus including an entropic component.\n\nWe first calculate the \"entropy\" of mixing, the increase in the uncertainty about the locations of the molecules when they are interspersed. In the pure condensed phases — solvent and polymer — everywhere we look we find a molecule. Of course, any notion of \"finding\" a molecule in a given location is a thought experiment since we can't actually examine spatial locations the size of molecules. The expression for the entropy of mixing of small molecules in terms of mole fractions is no longer reasonable when the solute is a macromolecular chain. We take account of this dissymmetry in molecular sizes by assuming that individual polymer segments and individual solvent molecules occupy sites on a . Each site is occupied by exactly one molecule of the solvent or by one monomer of the polymer chain, so the total number of sites is\n\nformula_18 is the number of solvent molecules and formula_19 is the number of polymer molecules, each of which has formula_20 segments.\n\nFrom statistical mechanics we can calculate the entropy change, the increase in spatial uncertainty, as a result of mixing solute and solvent.\n\nwhere formula_22 is Boltzmann's constant. Define the lattice \"volume fractions\" formula_8 and formula_24\n\nThese are also the probabilities that a given lattice site, chosen at random, is occupied by a solvent molecule or a polymer segment, respectively. Thus\n\nFor a small solute whose molecules occupy just one lattice site, formula_20 equals one, the volume fractions reduce to molecular or mole fractions, and we recover the usual equation from ideal mixing theory.\n\nIn addition to the entropic effect, we can expect an \"enthalpy\" change. There are three molecular interactions to consider: solvent-solvent formula_29, monomer-monomer formula_30 (not the covalent bonding, but between different chain sections), and monomer-solvent formula_31. Each of the last occurs at the expense of the average of the other two, so the energy increment per monomer-solvent contact is\n\nThe total number of such contacts is\n\nwhere formula_34 is the coordination number, the number of nearest neighbors for a lattice site, each one occupied either by one chain segment or a solvent molecule. That is, formula_35 is the total number of polymer segments (monomers) in the solution, so formula_36 is the number of nearest-neighbor sites to \"all\" the polymer segments. Multiplying by the probability formula_8 that any such site is occupied by a solvent molecule, we obtain the total number of polymer-solvent molecular interactions. An approximation following mean field theory is made by following this procedure, thereby reducing the complex problem of many interactions to a simpler problem of one interaction.\n\nThe enthalpy change is equal to the energy change per polymer monomer-solvent interaction multiplied by the number of such interactions\n\nThe polymer-solvent interaction parameter \"chi\" is defined as\n\nIt depends on the nature of both the solvent and the solute, and is the only \"material-specific\" parameter in the model. The enthalpy change becomes\n\nAssembling terms, the total free energy change is\n\nwhere we have converted the expression from molecules formula_18 and formula_19 to moles formula_7 and formula_45 by transferring Avogadro's number formula_46 to the gas constant formula_47.\n\nThe value of the interaction parameter can be estimated from the Hildebrand solubility parameters formula_48 and formula_49\n\nwhere formula_51 is the actual volume of a polymer segment.\n\nThis treatment does not attempt to calculate the conformational entropy of folding for polymer chains. (See the random coil discussion.) The conformations of even amorphous polymers will change when they go into solution, and most thermoplastic polymers also have lamellar crystalline regions which do not persist in solution as the chains separate. These events are accompanied by additional entropy and energy changes.\n\nIn the most general case the interaction formula_52 and the ensuing mixing parameter, formula_13, is a free energy parameter, thus including an entropic component. This means that aside to the regular mixing entropy there is another entropic contribution from the interaction between solvent and monomer. This contribution is sometimes very important in order to make quantitative predictions of thermodynamic properties.\n\nMore advanced solution theories exist, such as the Flory-Krigbaum theory.\n\n\n"}
{"id": "47569830", "url": "https://en.wikipedia.org/wiki?curid=47569830", "title": "Interactive Energy AG", "text": "Interactive Energy AG\n\nInteractive Energy AG is an energy and commodities company. Physical trading, logistics and distribution are at the core of the business, but these are complemented by refining, shipping, terminals exploration and production, power generation, and mining business solutions.\n\nThe company trades in and distributes physical commodities sourced from third party producers. They also provide, processing, storage, logistics and other services to commodity producers and consumers. Their customer base is highly diversified, with a high proportion of long-term commercial relationships. The business model of the company covers a wide range of products, activities and locations.\n\nFurthermore, they offer their customers the option of registering and trading with an Interactive Energy application, which can be used by clients on their mobile or on tablets. The application also provides real-time pricing, market data and news, along with a full history of their transactions, P&L information and an integrated tracking system allowing the user to monitor the shipment of the physical commodity.\n\nFounded in Switzerland in 2015, the company has offices in Lucerne, London, and Hong Kong.\n\nInteractive Energy AG is a joint stock corporation, which is completely financed by Ruben Katsobashvili, who has been active in different commodity sectors, notably in the petroleum industry in Russia.\n\nKatsobashvili is a Russian investor who lives in Moscow. He made his fortune with the Ben brothers in Telecoms, one of the strongest wholesalers in Europe.\n\nThe Director of Interactive Energy AG is Jose Luis Fernandez, who was born on 15 March 1961. He is the current Group Director, a position he has held since May 2015.\n\nThe company is a trusted organization that follows the legal responsibilities of EU Law and is compliant with the relevant national and EU legislative texts for different resources like Electricity, Natural Gas and Energy Efficiency.\n\nThe relevant EU legislative texts are laws at both EU and federal level, including acts, orders, ordinances, directives or decisions by the European Commission.\n\nInteractive Energy AG has earned the Energy Efficiency Certificate for being CO2 Neutral and the company also has a certificate for achieving balance by engaging in CO2 reducing projects.\n\nAs of 1 June 2015, the company structure and its services have been categorized into different major Sectors and five Selling and Market Organizations (SMOs)\n\n\nCorporate Headquarters\n\nLucerne\n\nSwitzerland\n\nOthers\n\nLondon, UK\n\nEurope\n\nNorth and South America\n\nHong Kong\n\nFar East\n\nRussia\n\nColombia\n\nUSA\n\nSouth Africa\n\nIndonesia\n\nAustralia\n\nFuture Functional Areas\n\nBeijing\n\nSingapore\n\nMumbai\n\nEU\n\nThe board of directors of Interactive Energy AG includes Ruben Katsobashvili and Jose Luis Aneas Fernandez and consists of the Chairman, the CEO and a number of non-Executive directors.\n\nThe board manages the sales, logistics, trading, origination and operation activities for all kinds of trading and financial projects from all over the world majorly in the European countries.\n\nThe business has major activities which include physical trading, marketing and logistics as well as risk advisory services.\n\nThe company displays regular and updated online data entries for trading to help investors make decisions according to the market environment.\n\n"}
{"id": "22715390", "url": "https://en.wikipedia.org/wiki?curid=22715390", "title": "International Hydropower Association", "text": "International Hydropower Association\n\nThe International Hydropower Association (IHA) is a non-profit, international organisation and membership association representing the global hydropower sector. IHA has members in more than 80 countries, including over 100 corporate and affiliate members working across sectors such as electricity generation, water management, construction, engineering and related industries. IHA also partners with international organisations, research institutions, governments and civil society. The association's mission is \"to advance sustainable hydropower by building and sharing knowledge on its role in renewable energy systems, freshwater management and climate change solutions\".\n\nThe International Hydropower Association (IHA) was formed under the auspices of UNESCO in 1995 as a forum to promote and disseminate good practices and further knowledge about hydropower. IHA employed its first full-time member of staff in 2001. It now consists of five departments, a central office (London), a regional office (South America) and a national office (China).\n\nFollowing the publication of the World Commission on Dams final report in 2000, IHA participated in the United Nations Environmental Programme’s Dams and Development Project.\n\nIn 2004, IHA became a founding member of the International Renewable Energy Alliance (REN Alliance) along with partner organisations representing the bioenergy, geothermal, solar and wind industries. The REN Alliance was established to advance the role of renewable energy systems, and continues to build influence today.\n\nIHA published its first sustainability guidelines for hydropower projects in 2004, followed by the IHA Sustainability Protocol in 2006. This work formed the basis for the Hydropower Sustainability Assessment Forum, a multi-stakeholder body consisting of representatives from government, commercial and development banks, social and environmental NGOs, and the hydropower sector.\n\nInitiated in partnership with the World Wildlife Fund (WWF) and The Nature Conservancy (TNC), the forum was convened over three years to develop a new tool to measure and guide sustainability performance in the hydropower sector: the Hydropower Sustainability Assessment Protocol.\n\nIHA has managed the roll-out and implementation of the protocol since its launch in 2011, and works in partnership with 15 leading companies around the world to promote a better understanding of how this tool can be used in different local and regional contexts.\n\nIn 2007, IHA hosted its first world congress in Turkey, bringing together hundreds of the world’s leading hydropower decision-makers, policymakers and thought leaders.\n\nThe World Hydropower Congress is now a biennial event and continues to serve as a reference point for the sector. It has been held in Turkey (2007) Iceland (2009), Brazil (2011), Malaysia (2013) and China (2015).\n\nIHA aims to advance sustainable hydropower’s role in meeting the world’s water and energy needs. The association has the following strategic objectives: \n\nIHA currently has over 100 corporate and affiliate members, including many of the world’s leading hydropower, electricity and construction companies.\n\nIHA has five different membership categories: \n\nIHA organises the biennial World Hydropower Congress, regarded as the key reference for decision-makers and thought leaders in the hydropower sector. The first congress was held in Antalya, Turkey in 2007. The event has since been held in Reykjavík, Iceland on 23–26 June 2009, Iguassu, Brazil on 14–17 June 2011, Kuching, Sarawak, Malaysia on 21–24 May 2013 and Beijing, China on 19–21 May 2015.\n\nIHA is governed by a board that comprises an international group of experts, bringing together high-level experience and different international perspectives of hydropower. IHA Board members are elected by IHA's membership.\n\nThe IHA Board is currently led by President Mr Ken Adams (Canada), six Vice Presidents (from Australia, Brazil, Canada, China, Germany and Malaysia), a further twelve board members (from Australia, Austria, Brazil, France, Norway, India, Iceland, Russia and South Africa), and the Chief Executive (a non-voting board member position), Richard Taylor.\n\nThe Board aims for a balanced geographic distribution of representation in its composition and conducts its affairs, including two-yearly elections, according to a formal written constitution and by-laws.\n\nThe Board is supported by a Central Office, the administrative arm of the IHA. There are also a number of committees and groups working on strategic and topical issues formed in accordance with Board resolutions.\n\nIHA's central office is situated in Sutton, Greater London, United Kingdom. IHA also has regional offices in China and Brazil.\n\nIHA acts as a voice for hydropower in international governmental and sector water, energy, and climate change forums and in the media - promoting a fact-based, balanced, consensus-building approach. The organisation is drawn on as a source for statistics on hydropower for authoritative world energy and renewable energy publications such as the REN21 Global Status Reports and IIASA Global Energy Assessment (GEA). IHA maintains a database of the world's hydropower stations and companies, built in collaboration with regulators, ministries,electricity associations, utilities, and station owners and operators.\n\nIHA has consultative and/or observer status with all United Nations agencies addressing water, energy and climate change and cooperates and collaborates with international organisations with interests in renewable energy such as IEA, WEC, and the World Bank. It is an active participant in the International Renewable Energy Agency (IRENA), founded in Bonn, Germany, on 26 January 2009.\n\nIHA is a founding member of International Renewable Energy Alliance (REN Alliance), which was formed on 4 June 2004, in Bonn, Germany, by the International Geothermal Association (IGA), the International Solar Energy Society (ISES), and the World Wind Energy Association (WWEA). The World Bioenergy Association (WBA) subsequently joined the REN Alliance in June 2009.\n\nThe Hydropower Sustainability Assessment Protocol, is a comprehensive tool to assess the sustainability of hydropower projects.\n\nIt was launched in June 2011 at the International Hydropower Association (IHA) World Congress.\n\nIt provides a thorough, evidence-based assessment of between 19-23 relevant sustainability topics, depending on the development stage of the project.\n\nThe Protocol is the product of a rigorous multi-stakeholder development process between 2008 and 2010, involving representatives from social and environmental NGOs (Oxfam, Nature Conservancy, Transparency International, WWF); governments (China, Germany, Iceland, Norway, Zambia); commercial and development banks (including banks that are signatory to the Equator Principles, and the World Bank); and the hydropower sector, represented by IHA.\n\nThe Protocol development process included field trials in 16 countries, across six continents, and stakeholder engagement with 1,933 individuals in 28 countries.\n\nThe topics cover the three pillars of sustainability: social, economic, and environmental, and include issues such as downstream flow regimes, indigenous peoples, biodiversity, infrastructure safety, resettlement, water quality, and erosion and sedimentation.\n\nThe assessment tools are used as a framework to produce a sustainability profile for a hydropower project. In so doing, multiple stakeholders can become better informed on the sustainability profile of a project, and develop strategies to address any weaknesses.\n\nThe Protocol can be used during all stages of hydropower project development: early stage, preparation, implementation and operation. This new approach to promote continuous improvement in hydropower sustainability has been designed so that the sustainability of hydropower projects can be assessed anywhere in the world, covering a broad range of possible case scenarios.\n\nThe UNESCO / IHA GHG Status of Freshwater Reservoirs Research Project is hosted by IHA, in collaboration with the International Hydrological Programme (IHP) of UNESCO.\n\nThe Project is a global initiative to improve understanding of the impact of reservoirs on natural greenhouse gas (GHG) emissions in a river basin. To date, the Project has involved some 160 researchers, scientists and professionals, from more than 100 institutions. The overall objective of the project is the evaluation of changes in GHG emissions due to the impoundment of freshwater reservoirs. The project deliverables include:\n\n\nA key milestone in the project was the publication, in 2010, of the GHG Measurement Guidelines for Freshwater Reservoirs, a comprehensive tool to assess the GHG status of freshwater reservoirs, describing standardised procedures for field measurements and calculation methods to estimate the impact of the creation of a reservoir on a river basin’s overall GHG emissions. The application of these Guidelines to a set of representative reservoirs worldwide allows the building of a reliable, standardised results database, in order to develop the basis for predictive modelling capability.\n\nSince then, IHA has been involved through this project in the development of a risk screening tool, the GHG Reservoir Screening Tool, that will allow for rapid and low-cost assessment of likely GHG emissions from reservoirs. This will allow developers and operators to assess whether a reservoir is likely to generate emissions, and therefore require further and more detailed examination and modelling.\n\nA revised version of the tool, G-Res, will allocate emissions to the particular services provided by the reservoir. A prototype of the revised tool was launched at the World Hydropower Congress in Beijing, in May 2015.\n\nThe tool is intended to inform decision makers if there is likely to be any significant GHG footprint associated with the purposes for which the reservoir is being developed. The tool will be applicable for both existing and planned reservoirs. If the tool identifies a reservoir that is likely to cause a significant impact, the recommended action will include the possibility of detailed modelling.\n\nThe GHG Screening Tool provides an estimate of the likely level of total (gross) GHG emissions from a freshwater reservoir. It has been developed as an empirical model, making use of existing published data of gross GHG emissions from previous assessments on 169 reservoirs around the world. The tool output provides an indication of the need for further assessment of GHG emissions.\n\nLaunched in 2015, the Mosonyi Award for Excellence in Hydropower recognises individuals within IHA's membership for outstanding contributions to the sector. Individual contributions related to the award may include:\nProposed candidates are reviewed by a panel convened by the IHA Board. The panel nominates selected candidates, with the IHA Board making the final decision on the selection of recipients. Up to three individuals will receive an award in 2015.\n\nThe award is named after Professor Emil Mosonyi, the founding president of IHA. Mosonyi, who died in 2009 aged 98, made major contributions during his long career in hydropower. A special award was presented by the IHA Board to Prof. Dr. Emil Mosonyi, IHA Founder and Honorary President, on 20 October 2004 at the closing ceremony of Hydro 2004 in Porto, Portugal.\n\nRecipients of this award are:\n\nIn collaboration with UNESCO, IHA previously awarded the IHA Blue Planet Prize every two years. The prize recognizes outstanding performance in sustainable management of hydropower schemes. The previous recipients of the award are:\nThe IHA Blue Planet Prize was not awarded during the development phase of the Hydropower Sustainability Assessment Protocol. It was officially relaunched during the 2015 World Hydropower Congress in Beijing, and will be awarded in 2017.\n\nIt will be awarded to a project that has either demonstrated excellence in sustainability, or has significantly improved the manner in which projects are developed in the country or region. Only projects from Least Developed Countries will be eligible for consideration under the second alternative.\n\nThe prize will be assessed on its performance in respect of the Hydropower Sustainability Assessment Protocol – a tool that measure the sustainability of hydropower projects across a range of environmental, social, economic and technical considerations.\n\n\n\n"}
{"id": "230711", "url": "https://en.wikipedia.org/wiki?curid=230711", "title": "Johannes Diderik van der Waals", "text": "Johannes Diderik van der Waals\n\nJohannes Diderik van der Waals (; 23 November 1837 – 8 March 1923) was a Dutch theoretical physicist and thermodynamicist famous for his work on an equation of state for gases and liquids.\n\nHis name is primarily associated with the van der Waals equation of state that describes the behavior of gases and their condensation to the liquid phase. His name is also associated with van der Waals forces (forces between stable molecules), with van der Waals molecules (small molecular clusters bound by van der Waals forces), and with van der Waals radii (sizes of molecules). As James Clerk Maxwell said about Van der Waals, \"there can be no doubt that the name of Van der Waals will soon be among the foremost in molecular science.\"\n\nIn his 1873 thesis, van der Waals noted the non-ideality of real gases and attributed it to the existence of intermolecular interactions. He introduced the first equation of state derived by the assumption of a finite volume occupied by the constituent molecules. Spearheaded by Ernst Mach and Wilhelm Ostwald, a strong philosophical current that denied the existence of molecules arose towards the end of the 19th century. The molecular existence was considered unproven and the molecular hypothesis unnecessary. At the time van der Waals' thesis was written (1873), the molecular structure of fluids had not been accepted by most physicists, and liquid and vapor were often considered as chemically distinct. But van der Waals's work affirmed the reality of molecules and allowed an assessment of their size and attractive strength. His new formula revolutionized the study of equations of state. By comparing his equation of state with experimental data, Van der Waals was able to obtain estimates for the actual size of molecules and the strength of their mutual attraction. The effect of Van der Waals's work on molecular physics in the 20th century was direct and fundamental. By introducing parameters characterizing molecular size and attraction in constructing his equation of state, Van der Waals set the tone for modern molecular science. That molecular aspects such as size, shape, attraction, and multipolar interactions should form the basis for mathematical formulations of the thermodynamic and transport properties of fluids is presently considered an axiom. With the help of the van der Waals's equation of state, the critical-point parameters of gases could be accurately predicted from thermodynamic measurements made at much higher temperatures. Nitrogen, oxygen, hydrogen, and helium subsequently succumbed to liquefaction. Heike Kamerlingh Onnes was significantly influenced by the pioneer work of van der Waals. In 1908, Onnes became the first to make liquid helium; this led directly to his 1911 discovery of superconductivity.\n\nVan der Waals started his career as a school teacher. He became the first physics professor of the University of Amsterdam when in 1877 the old Athenaeum was upgraded to Municipal University. Van der Waals won the 1910 Nobel Prize in physics for his work on the equation of state for gases and liquids.\n\nJohannes Diderik van der Waals was born on 23 November 1837 in Leiden in the Netherlands. He was the eldest of ten children born to Jacobus van der Waals and Elisabeth van den Berg. His father was a carpenter in Leiden. As was usual for working-class children in the 19th century, he did not go to the kind of secondary school that would have given him the right to enter university. Instead he went to a school of “advanced primary education”, which he finished at the age of fifteen. He then became a teacher's apprentice in an elementary school. Between 1856 and 1861 he followed courses and gained the necessary qualifications to become a primary school teacher and head teacher.\n\nIn 1862, he began to attend lectures in mathematics, physics and astronomy at the University in his city of birth, although he was not qualified to be enrolled as a regular student in part because of his lack of education in classical languages. However, the University of Leiden had a provision that enabled outside students to take up to four courses a year. In 1863 the Dutch government started a new kind of secondary school (HBS, a school aiming at the children of the higher middle classes). Van der Waals—at that time head of an elementary school—wanted to become a HBS teacher in mathematics and physics and spent two years studying in his spare time for the required examinations.\n\nIn 1865, he was appointed as a physics teacher at the HBS in Deventer and in 1866, he received such a position in The Hague, which was close enough to Leiden to allow van der Waals to resume his courses at the University there. In September 1865, just before moving to Deventer, van der Waals married the eighteen-year-old Anna Magdalena Smit.\n\nVan der Waals still lacked the knowledge of the classical languages that would have given him the right to enter university as a regular student and to take examinations. However, it so happened that the law regulating the university entrance was changed and dispensation from the study of classical languages could be given by the minister of education. Van der Waals was given this dispensation and passed the qualification exams in physics and mathematics for doctoral studies.\n\nAt Leiden University, on June 14, 1873, he defended his doctoral thesis \"Over de Continuïteit van den Gas- en Vloeistoftoestand\" (on the continuity of the gaseous and liquid state) under Pieter Rijke. In the thesis, he introduced the concepts of molecular volume and molecular attraction.\n\nIn September 1877 van der Waals was appointed the first professor of physics at the newly founded Municipal University of Amsterdam. Two of his notable colleagues were the physical chemist Jacobus Henricus van 't Hoff and the biologist Hugo de Vries. Until his retirement at the age of 70 van der Waals remained at the Amsterdam University. He was succeeded by his son Johannes Diderik van der Waals, Jr., who also was a theoretical physicist. In 1910, at the age of 72, van der Waals was awarded the Nobel Prize in physics. He died at the age of 85 on March 8, 1923.\n\nThe main interest of van der Waals was in the field of thermodynamics. He was influenced by Rudolf Clausius' 1857 treatise entitled \"Über die Art der Bewegung, welche wir Wärme nennen\" (\"On the Kind of Motion which we Call Heat\"). Van der Waals was later greatly influenced by the writings of James Clerk Maxwell, Ludwig Boltzmann, and Willard Gibbs. Clausius' work led him to look for an explanation of Thomas Andrews' experiments that had revealed, in 1869, the existence of critical temperatures in fluids. He managed to give a semi-quantitative description of the phenomena of condensation and critical temperatures in his 1873 thesis, entitled \"Over de Continuïteit van den Gas- en Vloeistoftoestand\" (On the continuity of the gas and liquid state). This dissertation represented a hallmark in physics and was immediately recognized as such, e.g. by James Clerk Maxwell who reviewed it in Nature in a laudatory manner.\n\nIn this thesis he derived the equation of state bearing his name. This work gave a model in which the liquid and the gas phase of a substance merge into each other in a continuous manner. It shows that the two phases are of the same nature. In deriving his equation of state van der Waals assumed not only the existence of molecules (the existence of atoms was disputed at the time), but also that they are of finite size and attract each other. Since he was one of the first to postulate an intermolecular force, however rudimentary, such a force is now sometimes called a van der Waals force.\n\nA second great discovery was published in 1880, when he formulated the Law of Corresponding States. This showed that the van der Waals equation of state can be expressed as a simple function of the critical pressure, critical volume, and critical temperature. This general form is applicable to all substances (see van der Waals equation.) The compound-specific constants \"a\" and \"b\" in the original equation are replaced by universal (compound-independent) quantities. It was this law which served as a guide during experiments which ultimately led to the liquefaction of hydrogen by James Dewar in 1898 and of helium by Heike Kamerlingh Onnes in 1908.\n\nIn 1890, van der Waals published a treatise on the \"Theory of Binary Solutions\" in the Archives Néerlandaises. By relating his equation of state with the Second Law of Thermodynamics, in the form first proposed by Willard Gibbs, he was able to arrive at a graphical representation of his mathematical formulations in the form of a surface which he called Ψ (Psi) surface following Gibbs, who used the Greek letter Ψ for the free energy of a system with different phases in equilibrium.\n\nMention should also be made of van der Waals' theory of capillarity which in its basic form first appeared in 1893. In contrast to the mechanical perspective on the subject provided earlier by Pierre-Simon Laplace, van der Waals took a thermodynamic approach. This was controversial at the time, since the existence of molecules and their permanent, rapid motion were not universally accepted before Jean Baptiste Perrin's experimental verification of Albert Einstein's theoretical explanation of Brownian motion.\n\nHe married Anna Magdalena Smit in 1865, and the couple had three daughters (Anne Madeleine, , Johanna Diderica) and one son, the physicist Jacqueline was a poet of some note. Van der Waals' nephew Peter van der Waals was a cabinet maker and a leading figure in the Sapperton, Gloucestershire school of the Arts and Crafts movement. The wife of Johannes van der Waals died of tuberculosis at 34 years old in 1881. After becoming a widower Van der Waals never remarried and was so shaken by the death of his wife that he did not publish anything for about a decade. He died in Amsterdam on March 8, 1923, one year after his daughter Jacqueline had died.\n\nHis grandson, Christopher D. Vanderwal is a distinguished professor of Chemistry at the University of California, Irvine.\n\nVan der Waals received numerous honors and distinctions, besides winning the 1910 Nobel Prize in Physics. He was awarded an honorary doctorate of the University of Cambridge; was made honorary member of the Imperial Society of Naturalists of Moscow, the Royal Irish Academy and the American Philosophical Society; corresponding member of the Institut de France and the Royal Academy of Sciences of Berlin; associate member of the Royal Academy of Sciences of Belgium; and foreign member of the Chemical Society of London, the National Academy of Sciences of the U.S., and of the Accademia dei Lincei of Rome. Van der Waals was a member of the \"Koninklijke Nederlandse Akademie van Wetenschappen\" (Royal Netherlands Academy of Sciences) since 1875. From 1896 until 1912, he was secretary of this society.\n\n\n\n"}
{"id": "33711901", "url": "https://en.wikipedia.org/wiki?curid=33711901", "title": "List of Category 4 Pacific hurricanes", "text": "List of Category 4 Pacific hurricanes\n\nCategory 4, the second-highest classification on the Saffir–Simpson Hurricane Scale, is used for tropical cyclones that have winds of 130–156 mph (209–251 km/h; 113–136 kn). The division of the eastern and central Pacific basins occurs at 140° W; the eastern Pacific covers area east of 140° W, while the central Pacific extends between 140° W to 180° W. Both basins' division points are at 66° N as a northern point and the equator as the southern point. As of 2018, 126 hurricanes have attained Category 4 status in the northeastern Pacific basins. This list does not include storms that also attained Category 5 status on the scale.\n\nNumerous climatological factors influence the formation of hurricanes in the Pacific basins. The North Pacific High and Aleutian Low, usually present between January and April, cause strong wind shear and unfavorable conditions for the development of hurricanes. During its presence, El Niño results in increased numbers of powerful hurricanes through weaker wind shear, while La Niña reduces the number of such hurricanes through the opposite. Global warming may also influence the formation of tropical cyclones in the Pacific basin. During a thirty-year period with two sub-periods, the first between 1975 and 1989 and the second between 1990 and 2004, an increase of thirteen Category 4 or 5 storms was observed from the first sub-period.\n\nOn the Saffir–Simpson Hurricane Scale, \"Category 4\" is the second-most powerful classification, with winds ranging between 130–156 mph (209–251 km/h; 113–136 kn). When these hurricanes make landfall, impacts are usually severe but are not as destructive as Category 5 hurricanes that come ashore. The term \"maximum sustained wind\" refers to the average wind speed measured during the period of one minute at the height of above the ground. The windspeed is measured at that height to prevent disruption from obstructions. Wind gusts in tropical cyclones are usually approximately 30% stronger than the one-minute maximum sustained winds.\n\nThe northeastern Pacific hurricane basins are divided into two parts – eastern and central. The eastern Pacific basin extends from all areas of the Pacific north of the equator east of 140° W, while the central Pacific basin includes areas north of the equator between 140° W and 180° W. Both basins extend to the Arctic Circle at 66° N. When tropical cyclones cross from the Atlantic into the Pacific, the name of the previous storm is retained if the system continues to exhibit tropical characteristics; however, when hurricanes degenerate into a remnant low-pressure area, the system is designated with the next name on the rotating eastern Pacific hurricane naming list.\n\nSince 1900, 126 Category 4 hurricanes have been recorded in the eastern and central Pacific basins. Of these, fourteen have attained Category 4 status on more than one occasion, by weakening to a status on the Saffir–Simpson Hurricane Scale lower than Category 4 and later restrengthening into a Category 4. Such storms are demarcated by the dates they first attained and the final time they lost the intensity. Only three storms, Hurricane Fico in 1978, Hurricane Norbert in 1984, and Hector in 2018, reached Category 4 status three times or more.\n\nBetween 1970 and 1975, advisories for systems in the eastern Pacific basins were initiated by the Eastern Pacific Hurricane Center (EPHC) as part of the National Weather Service (NWS) office in San Francisco, California. At that time, the advisories released were written in cooperation with the United States Navy Fleet Weather Center in Alameda and the Air Force Hurricane Liaison Officer at the McClellan Air Force Base. Following the move of the hurricane center to Redwood City in 1976, track files were created and altered by Arthur Pike and were later re-modified following the release of a study in 1980. The National Hurricane Center (NHC) extended its authority to the EPHC in 1988, and subsequently began maintaining the tracks.\n\nA total of 126 Category 4 hurricanes have been recorded in the eastern and central Pacific basins since 1900. Only two Category 4 hurricanes have been recorded in May, in addition to 14 in June, 24 in July, 31 in August, 32 in September, 18 in October, and two in November. No Category 4 storms have developed during the off-season. It is theorized that global warming was responsible for an increase of 13 Category 4 and 5 storms that developed in the eastern Pacific, from 36 in the period of 1975–1989 to 49 in the period of 1990–2004. It was estimated that if sea-surface temperatures ascended by 2 to 2.5 degrees, the intensity of tropical cyclones would increase by 6–10% internationally. During years with the existence of an El Niño, sea-surface temperatures increase in the eastern Pacific, resulting in an increase in activity as vertical wind shear decreases in the Pacific; the opposite happens in the Atlantic basin during El Niño, when wind shear increases creating an unfavourable environment for tropical cyclone formation in the Atlantic. Contrary to El Niño, La Niña increases wind shear over the eastern Pacific and reduces it over the Atlantic.\n\nThe presence of a semi-permanent high-pressure area known as the North Pacific High in the eastern Pacific is a dominant factor against formation of tropical cyclones in the winter, as the Pacific High results in wind shear that causes environmental conditions for tropical cyclone formation to be unconducive. Its effects in the central Pacific basin are usually related to keeping cyclones away from the Hawaiian Islands. Due to westward trade winds, hurricanes in the Pacific nearly never head eastward, although several storms have defied the odds and headed eastward. A second factor preventing tropical cyclones from forming during the winter is the occupation of a semi-permanent low-pressure area designated the Aleutian Low between January and April. Its presence over western Canada and the northwestern United States contributes to the area's occurrences of precipitation in that duration. In addition, its effects in the central Pacific near 160° W causes tropical waves that form in the area to drift northward into the Gulf of Alaska and dissipate. Its retreat in late-April allows the warmth of the Pacific High to meander in, bringing its powerful clockwise wind circulation with it. The Intertropical Convergence Zone departs southward in mid-May permitting the formation of the earliest tropical waves, coinciding with the start of the eastern Pacific hurricane season on May 15.\n\nCooler waters near the Baja California peninsula are thought to prevent storms in the eastern Pacific from transitioning into an extratropical cyclone; as of 2009, only three storms listed in the database are known to have successfully completed an extratropical transition.\n\nOf the 126 Category 4 hurricanes that have formed in the eastern and central Pacific basins, 28 have made landfall. Of them, four made landfall at Category 4 intensity, three at Category 3, eleven at Categories 2 and 1, eight as tropical storms, and six as tropical depressions. Several of these storms weakened slightly after attaining Category 4 status as they approached land; this is usually a result of dry air, shallower water due to shelving, cooler waters, or interaction with land. Only in six years – 1976, 1983, 1992, 1997, 2014 and 2018 – more than one Category 4 hurricane made landfall, and only during one year – 2018 – did four Category 4 hurricanes made landfall.\n\n"}
{"id": "27005975", "url": "https://en.wikipedia.org/wiki?curid=27005975", "title": "Marun Dam", "text": "Marun Dam\n\nMarun Dam, also spelled Maroun, is a rock-fill embankment dam on the Marun River about north of Behbahan in Behbahan County, Khuzestan Province, Iran. The dam serves to provide water for irrigation and to generate hydroelectric power as well. Construction on the dam began in 1989 and it was completed in 1998. A smaller Marun-II regulator dam is planned downstream. The 150 MW power station was commissioned in 2004.\n\n"}
{"id": "44203051", "url": "https://en.wikipedia.org/wiki?curid=44203051", "title": "Moana Sands Conservation Park", "text": "Moana Sands Conservation Park\n\n\n"}
{"id": "22867206", "url": "https://en.wikipedia.org/wiki?curid=22867206", "title": "Moisture meter", "text": "Moisture meter\n\nMoisture meters are used to measure the percentage of water in a given substance. This information can be used to determine if the material is ready for use, unexpectedly wet or dry, or otherwise in need of further inspection. Wood and paper products are very sensitive to their moisture content. Physical properties are strongly affected by moisture content and high moisture content for a period of time may progressively degrade a material. \n\nNewly-cut logs can have a moisture content (MC) of 80% or more, depending on species. Since wood shrinks, and can also split, twist or otherwise change shape as it dries, most wood is dried before being used. This is most often done using a kiln, but may use the air drying method, which is much slower. In most parts of the United States, the minimum moisture content that can be generally obtained in air drying is about 12 to 15 percent. Most air-dried material is usually closer to 20 percent moisture content when used. \n\nIn-kiln drying is usually monitored by some type of moisture meter. Moisture meters are used to measure the amount of water in the wood so that the woodworker can determine if it is suitable for the intended purpose. Building inspectors and many more, carpenters, hobbyists, and other woodworkers often are required to have moisture meters. Wood flooring installers, for example, have to verify that the MC of the wood matches the relative humidity in the air of the building. If this step is skipped, a vast array of problems may present itself: cracking, cupping, crowning, buckling, sunken joints, and cracked finishes.\n\nThe problems caused by varying degrees of moisture content in wood go beyond simple shrinkage in the dimensions of wood parts. Problems with distortions in the shape of the wood, such as twisting, warping and cupping, occur because of the difference in the degree of dimensional change in wood cells tangentially (perpendicular to the grain and parallel to the growth rings) versus radially (perpendicular to the growth rings).\n\nA moisture meter gives a reading of the approximate moisture content of wood. The reading helps in determining whether the wood is suitably dry for its intended purpose. The moisture content reading can also assist in planning a project design that will accommodate future changes in dimension caused by changes in relative humidity.The amount of overall shrinkage lumber will undergo in the drying process varies from wood species to wood species. The difference between radial and tangential shrinkage also varies from species to species. Woods with a low ratio of tangential to radial shrinkage, such as teak and mahogany, are less prone to distortion due to changes in moisture content than woods with a high ratio, such as eastern white pine and certain species of oak. Species with both low overall shrinkage and a low tangential/radial shrinkage ratio are more stable and will react better to changes in moisture content.\n\nFor wood that is to be used in making furniture, for wood floors, in construction or for any building project, the ideal state is one of equilibrium moisture content (EMC). EMC means that the wood is in balance with the relative humidity it surrounding environment, and is therefore neither gaining or losing in moisture content. In reality, however, it is extremely rare for an environment to maintain a constant fixed relative humidity, and some degree of dimensional change along with seasonal changes in relative humidity is to be expected. \n\nFor typical woodworking operations, two basic types of moisture meters are available. Depending in the brand, pin-type meters measure the electrical resistance, termed resistivity, or its reciprocal, conductance, of the wood substrate. Water freely conducts electricity; consequently, increasing water content correlates to increased conductance. Pin-type meters incorporate two pin electrodes which are driven into the wood fibers and directly measure electrical resistance or conductivity. Each brand of moisture meter utilizes proprietary calculations and displays the final moisture content as a percentage. \n\nThe second type of moisture meter relies on the dielectric properties of wood. The meter incorporates two pads which serve as rubber electrodes that transmit and receive a signal when pressed into the wood substrate. The pad type moisture meter is non-invasive in nature and requires only surface contact with the wood to obtain a reading. The non-invasive meter creates a low-frequency electrical wave between the two pads and measures the electrical properties of the wood, similar to the invasive pin-type meter.\n\nMoisture meters may also be utilized by building-industry professionals to ascertain precise moisture content of a wide range of materials found in the built environment, such as gypsum board drywall or interior finish plaster. Locating high moisture within buildings is essential in locating leaks that may not be visible to the eye. Moisture content is measured in the same manner as wood and displayed in Wood Moisture Equivalent, or WME. WME is the theoretical moisture content that the substrate would contain if it were wood.\n\n\n"}
{"id": "27644912", "url": "https://en.wikipedia.org/wiki?curid=27644912", "title": "Nuclear Liability Act", "text": "Nuclear Liability Act\n\nThe Civil Liability for Nuclear Damage Act, 2010 or Nuclear Liability Act is a highly debated and controversial Act which was passed by both houses of Indian parliament. The Act aims to provide a civil liability for nuclear damage and prompt compensation to the victims of a nuclear incident through a nofault liability to the operator, appointment of Claims Commissioner, establishment of Nuclear Damage Claims Commission and for matters connected therewith or incidental thereto.\n\nThis is one of the last steps needed to activate the 2008 Indo-U.S. civilian nuclear agreement as the United state nuclear reactor manufacturing companies will require the liability bill to get insurance in their home state. \nThe government has encountered fierce opposition when trying to push this bill through parliament on several occasions. This is because it contains several controversial clauses that the opposition parties claim to be 'unconstitutional'. The opposition believes the bill is being pushed through due to US pressure though this is denied by the government.\n\nThe Act effectively caps the maximum amount of liability in case of each nuclear accident at to be paid by the operator of the nuclear plant, and if the cost of the damages exceeds this amount, special drawing rights up to 300 million will be paid by the Central Government.\n\nThe Act made amendments in the Atomic Energy Act 1962 allowing private investment in the Indian nuclear power program. The issue of an accident is sensitive in India, where a gas leak in a US company's Union Carbide factory in Bhopal city killed about 20,000 people in 1984 in one of the world's worst industrial disasters. The Act came into force from 11 November 2011.\n\nIndia has an ambitious goal to increase 5-fold the amount of electricity produced from nuclear power plants to 20,000 MWe by 2020. This will be further increased to 27,000 MWe by 2032. In this way, India will produce 25 percent of its electricity from nuclear power plants by 2050. India's present production of electricity through nuclear power is 5780 MW. To increase the share of nuclear power, foreign companies would need to be involved in the manufacture and supply of nuclear reactors.\n\nAlthough there is no international obligation for such a bill, in order to attract the US companies involved in nuclear commerce such as General Electric and Westinghouse, it is necessary to introduce a liability bill which would help these private companies in getting insurance cover in their home state. Thus, the bill will help in the realisation of the Indo-U.S. Nuclear deal.\n\nAnother motive for the bill is to legally and financially bind the operator and the government to provide relief to the affected population in the case of a nuclear accident. In consideration of the long-term costs related to clean-up and shut-down activities if a nuclear accident were to occur, prominent members of the civil society in India have called on the Government and political parties to hold nuclear suppliers responsible and liable for nuclear accidents.\n\nAdvances in nuclear technology have significantly reduced the probability of a nuclear catastrophe and is considered an environment friendly and sustainable source of energy. However, it is still necessary to keep in mind the negative aspects of the nuclear energy and measures must be taken for its peaceful use. However the Fukushima Daiichi nuclear disaster have created once again a debate in India (and the world over) over the destructive nature of nuclear energy.\n\nA major point of debate is the amount of financial assistance to be provided under such circumstances as it is considered insufficient and unsatisfactory. Other than this, the bill contain certain clauses which if implemented will let free the manufacturer and supplier legally and to a large extent financially as well.\n\nThe Atomic Energy Act, 1962 empowers the Government to produce, develop, use and dispose of atomic energy either by itself or through any authority or Corporation established by it or a Government company. In this regard, an indigenous sequential three-stage nuclear power programme based on optimum utilization of the country’s nuclear resources of modest uranium and abundant thorium is being pursued. Large capacity nuclear power reactors based on foreign cooperation are also being implemented as additionalities, for faster capacity addition.\n\nClause 6 defines the share of financial liability. It states that the liability of an operator for each nuclear incident shall be:\n\n(a) for nuclear reactors having power equal to 10 MW or above Rs. 1,500 crores (i.e. Rupees 15 billion) \n(b) in respect of spent fuel reprocessing plants, rupees three hundred crores;\n(c) in respect of the research reactors having thermal power below ten MW, fuel facilities other than spent fuel reprocessing plants and transportation of nuclear materials, Rupees one hundred crores (Rupees 1 billion).\n\nHowever, the Central government may review the operator's liability from time to time and specify a higher amount.\n\nand the remaining amount will be paid by the Indian government. If written into the contract, the operator can claim the liabilities from the manufacturer and supplier. But the maximum amount payable by the foreign companies will be limited to a meagre sum of Rs.1500 crore .\n\nThis is considered as a moot point as the operator will be the Nuclear Power Corporation of India Ltd. (NPCIL) which itself is a government owned facility. In other words, the government may have to foot the entire bill thereby exonerating the manufacturer/supplier.\n\nThis clause deals with the legal binding of the culpable groups in case of a nuclear accident. It allows only the operator (NPCIL) to sue the manufacturers and suppliers. Victims will not be able to sue anyone. In reality, no one will be considered legally liable because the recourse taken by the operator will yield only.\nRIGHT TO RECOURSE:\nAfter paying amounts to the victims operator has the right to recourse to the suppliers.\nSECTION 17(A):Right to recourse will apply in case it is already mentioned in the contract.\nSECTION 17(B):Right to recourse in case of a nuclear damage becausof the patent or latent defects in the materials or his employee. It also includes defects in sub-standard services.\nSECTION 17(C):If damag is by a particular act of an individual with an intention to cause damage.\n\nClause 18 of the nuclear liability bill limits the time to make a claim within 10 years. This is considered to be too short as there may be long term damage due to a nuclear accident.\n\nClause 35 extends the legal binding that the responsible groups may have to face. The operator or the responsible persons in case of a nuclear accident will undergo the trial under Nuclear Damage Claims Commissions and no civil court is given the authority. The country will be divided into zones with each zone having a Claims Commissioner. This is in contrast to the US counterpart – the Price Anderson Act, in which lawsuits and criminal proceedings proceed under the US courts.\n\nA Public Interest Litigation (PIL) has also been filed against the Act at the Supreme Court of India in 2011, examining the constitutionality of the Act regarding the Right to Life as enshrined in the Constitution of India.\n\nThe Bhopal Gas tragedy was another accident where an inherently dangerous substance was leaked and caused havoc. Despite this, low liability and compensation resulted, after several delays. Victims were not sufficiently or effectively compensated and rehabilitated. Additionally, the environmental impact of nuclear activity is far reaching. A nuclear accident is disastrous for the environment. \nA nuclear accident is equally, if not more, harmful. The Act does not properly address liability in the face of an accident or even day to day risks.\n\n\n"}
{"id": "34763202", "url": "https://en.wikipedia.org/wiki?curid=34763202", "title": "Ontario Sustainable Energy Association", "text": "Ontario Sustainable Energy Association\n\nThe Ontario Sustainable Energy Association (OSEA) is a non-profit organization supporting the growth of renewable energy and Community Power projects in the Canadian Province of Ontario. OSEA advocated an advanced renewable energy Feed-in Tariff program for Ontario, resulting in the creation of the Renewable Energy Standard Offer Program, a precursor to the Green Energy Act and, in 2007, the most progressive energy policy in North America in a decade.\nOSEA has approximately 130 community and industry members as well as individual members. The affairs of the Association are managed by a Board of Directors elected by the membership.\n\nOSEA was incorporated in 2001, sponsored by a number of community, environmental and industrial groups to be a focal point for consolidating activities promoting community power and integrated sustainable energy. Its Board is governed by twelve directors, four of which are elected by the membership annually. OSEA has been led and managed by Kristopher Stevens since May 2008 with Deborah Doncaster, Paul Gipe and Gwen Glover having served before him.\n\nThe concerns of the group included advocacy, outreach and capacity building. OSEA focuses on creating practical advice and guidelines such as community, municipal, First Nations and developer focused guidebooks and webinars that advance the collective interests of the sector. OSEA has been at the vanguard of sector transformation through various policy papers which have helped inform the creation of key policies, regulations and programs such as the Renewable Standard Offer Program (RESOP) and the Green Energy and Economy Act.\n\nOSEA has hosted and co-hosted five successful international exhibitions and conferences to advance the sustainable energy sector with new insights, information and business opportunities. These include\n\n- The 7th World Wind Energy Conference in Kingston, Ontario (2008). Previous conferences had been held in Beijing, Berlin, Buenos Aires, Melbourne, and New Delhi. It was at the Kingston conference that the Green Energy Act Alliance was launched with the intent of bringing together a single voice to make conservation and renewable energy the priority in Ontario.\n- Three consecutive Community Power Conference held in Toronto Ontario\n- All-Energy Canada Exhibition and Conference in Toronto, Ontario (2014)\n\nOSEA has approximately 130 community and industry members as well as individual members. The affairs of the Association are governed by a Board of Directors elected by the membership.\n\nThe Ontario Sustainable Energy Association encourages and enables the people of Ontario to improve the environment, the economy and their health by producing clean, sustainable energy in their homes, businesses and communities. The OSEA community - staff, interns, volunteers, members, friends and supporters - are actionists (as opposed to activists) looking for (and working on) solutions. They work pro-actively to build bridges between stakeholders and seek ways to improve Ontario's energy system collaboratively recognizing that community, industry and government should all play a role in shaping Ontario’s energy future.\n\nOSEA defines sustainable energy as including:\n\nOSEA supports the growth of Community Power in Ontario. Community Power is a class of community-based energy projects that are owned, developed and controlled in full or in part (50 per cent or more) by residents of the community in which the project is located. Community Power proponents include local residents, farmer collaboratives, co-operatives, First Nations, municipalities and other institutions working to develop local sustainable energy projects.\n\nA typical community spends 20 percent of its gross income buying energy, and 80 percent of those dollars leave town buying imported energy. Community-based sustainable energy developments provide an excellent opportunity to help keep energy dollars in the community, create economic development, empower residents, cut pollution and greenhouse gases and address energy security concerns. According to the Iowa Policy Project locally owned renewable energy project generates 5-10 times the local economic benefits than do conventional ownership models. From a solely economic perspective every dollar invested by local community members results in a 3 times multiplier within the community.\n\nThe Community Power Services Group of OSEA helps communities with the earlier stages of their projects. Examples include:\n\nOSEA engages government, regulators and energy stakeholders on an ongoing basis to ensure that communities are empowered to develop their local sustainable energy resources for a greener, healthier future. OSEA advocated an advanced renewable energy Feed-in Tariff program for Ontario, resulting in the creation of the Renewable Energy Standard Offer Program, a precursor to the Green Energy Act and, in 2007, the most progressive energy policy in North America in a decade.\n\n\nOSEA has produced a number of well researched resources to help inform the discussion about Ontario’s evolution towards a 100% sustainable energy system including:\n\n\nCentered on conservation and renewable energy this includes:\n\n"}
{"id": "27808402", "url": "https://en.wikipedia.org/wiki?curid=27808402", "title": "Organotrifluoroborate", "text": "Organotrifluoroborate\n\nOrganotrifluoroborates are organoboron compounds that contain an anion with the general formula [RBF]. They can be thought of as protected boronic acids, or as adducts of carbanions and boron trifluoride. Organotrifluoroborates are tolerant of air and moisture and are easy to handle and purify. They are often used in organic synthesis as alternatives to boronic acids (RB(OH)), boronate esters (RB(OR′)), and organoboranes (RB), particularly for Suzuki-Miyaura coupling.\n\nBoronic acids RB(OH) react with potassium bifluoride K[HF] to form trifluoroborate salts K[RBF].\n\nOrganotrifluoroborates are strong nucleophiles and react with electrophiles without transition-metal catalysts.\n\nThe mechanism of organotrifluoroborate-based Suzuki-Miyaura coupling reactions has recently been investigated in detail. The organotrifluoroborate hydrolyses to the corresponding boronic acid \"in situ\", so a boronic acid can be used in place of an organotrifluoroborate, as long as it is added slowly and carefully.\n"}
{"id": "37158447", "url": "https://en.wikipedia.org/wiki?curid=37158447", "title": "PK-3 Plus (ISS Experiment)", "text": "PK-3 Plus (ISS Experiment)\n\nThe PK-3 Plus or (Plasmakristall-3 Plus) laboratory is a joint Russian-German laboratory for the investigation of dusty/complex plasmas on board the International Space Station (ISS), with the principal investigators at the German Max Planck Institute for Extraterrestrial Physics and the Russian Institute for High Energy Densities. It is the successor to the PKE Nefedov experiment with improvements in hardware, diagnostics and software. The laboratory was launched in December 2005 and has been operated for the first time in January 2006. As of September 2012, it has been used in 18 missions.\n\nThe heart of the PK-3 Plus laboratory consists of a capacitively coupled plasma chamber. A plasma is generated by applying a radio frequency voltage signal to two electrodes. Microparticles are then injected into the plasma via dispensers that are mounted on the side of the electrodes. The microparticles are illuminated with a sheet of laser light from the side. They scatter the light, which is then recorded by up to four cameras mounted around the plasma chamber. The data from the cameras are recorded on hard drives that are physically brought back to Earth via Soyuz capsules for analysis.\n\nPK-3 Plus studies complex plasmas - plasmas that contain microparticles. The microparticles acquire high negative charges by collecting electrons from the surrounding plasma. They interact with each other and with the plasma particles, e.g., they experience a drag force from the ions that are streaming to the edges of the plasma.\n\nDepending on the experimental settings like the gas pressure, the system made up of the microparticles forms various phases - solid, liquid or gaseous. In this sense, the microparticles can be seen as analogous to atoms or molecules in ordinary physical systems. The experiments are performed by observing the movement of the microparticles and tracing them from camera frame to frame.\n\nThe PK-3 Plus experiment allows investigating a large variety of topics, for instance: the crystal structure, fluid-solid phase transitions, electrorheological fluids, wave propagation, the heartbeat instability, Mach cone formation and the speed of sound, and lane formation\n\n"}
{"id": "19581511", "url": "https://en.wikipedia.org/wiki?curid=19581511", "title": "Peptization", "text": "Peptization\n\nPeptization or Deflocculation is the process responsible for the formation of stable dispersion of colloidal particles in dispersion medium. In other words it may be defined as a process of converting a precipitate into colloidal sol by shaking it with dispersion medium in the presence of small amount of electrolyte. The electrolyte used in this process is called as peptizing agent. \n\nThis is particularly important in colloid chemistry or for precipitation reactions in an aqueous solution. When colloidal particles bear a same sign electric charge, they mutually repel each other and cannot aggregate together. Freshly precipitated aluminium or iron hydroxide is extremely difficult to filter because the very fine colloidal particles directly pass through a paper filter. To facilitate the filtration, the colloidal suspension must be first flocculated by adding a concentrated solution of salt to the system. Multivalent cations are more efficient flocculants than monovalent cations: AlCl > CaCl > NaCl. The electrical charges present at the surface of the particles are so \"neutralised\" and disappear. More correctly speaking, the electrical double layer existing at the surface of the particles is compressed by the added electrolyte and collapses at high ionic strength. The electrical repulsion no longer hinders the aggregation of particles and they can then coalesce to form a flocculent precipitate that is easy to filter. If the precipitate is washed with an excessive volume of deionised water, the electrical double layer present at the surface of the particles expands again and the electrical repulsion reappears: the precipitate peptizes and the particles pass again through the filter. \n\nPeptization is also used in nanoparticle synthesis to make a large grouping of particles split into many primary particles. This is done by changing the surface properties, applying a charge, or by adding a surfactant. \n\nIn the synthesis of titania (titanium dioxide) nanoparticles, peptization involves adsorption of quaternary ammonium cation on the titania surface. This causes the surface to become positively charged. Electrostatic repulsion of the primary particles in the agglomerated titania breaks up the agglomerate into primary particles.\n\n"}
{"id": "8068667", "url": "https://en.wikipedia.org/wiki?curid=8068667", "title": "Piassava", "text": "Piassava\n\nPiassava, which is also called piaçaba, piasaba, pissaba, piassaba, and piaçá, is a fibrous product of two Brazilian palms: \"Attalea funifera\" and \"Leopoldinia piassaba\". It is often used in making brooms, and for other purposes.\n\nWest African piassava palm\n\n<br>\n"}
{"id": "25916521", "url": "https://en.wikipedia.org/wiki?curid=25916521", "title": "Plasma (physics)", "text": "Plasma (physics)\n\nPlasma () is one of the four fundamental states of matter, and was first described by chemist Irving Langmuir in the 1920s. Plasma can be artificially generated by heating or subjecting a neutral gas to a strong electromagnetic field to the point where an ionised gaseous substance becomes increasingly electrically conductive, and long-range electromagnetic fields dominate the behaviour of the matter.\n\nPlasma and ionised gases have properties and display behaviours unlike those of the other states, and the transition between them is mostly a matter of nomenclature and subject to interpretation. Based on the surrounding environmental temperature and density, partially ionised or fully ionised forms of plasma may be produced. Neon signs and lightning are examples of partially ionised plasma. The Earth's ionosphere is a plasma and the magnetosphere contains plasma in the Earth's surrounding space environment. The interior of the Sun is an example of fully ionised plasma, along with the solar corona and stars.\n\nPositive charges in ions are achieved by stripping away electrons orbiting the atomic nuclei, where the total number of electrons removed is related to either increasing temperature or the local density of other ionised matter. This also can be accompanied by the dissociation of molecular bonds, though this process is distinctly different from chemical processes of ion interactions in liquids or the behaviour of shared ions in metals. The response of plasma to electromagnetic fields is used in many modern technological devices, such as plasma televisions or plasma etching.\n\nPlasma may be the most abundant form of ordinary matter in the universe, although this hypothesis is currently tentative based on the existence and unknown properties of dark matter. Plasma is mostly associated with stars, extending to the rarefied intracluster medium and possibly the intergalactic regions.\n\nThe word \"plasma\" comes or 'jelly', and describes the behaviour of the Ionised atomic nuclei and the electrons within the surrounding region of the plasma. Very simply, each of these nuclei are suspended in a movable sea of electrons. Plasma was first identified in a Crookes tube, and so described by Sir William Crookes in 1879 (he called it \"radiant matter\"). The nature of this \"cathode ray\" matter was subsequently identified by British physicist Sir J.J. Thomson in 1897. \n\nThe term \"plasma\" was coined by Irving Langmuir in 1928. Lewi Tonks and Harold Mott-Smith, both of whom worked with Irving Langmuir in the 1920s, recall that Langmuir first used the word \"plasma\" in analogy with blood. Mott-Smith recalls, in particular, that the transport of electrons from thermionic filaments reminded Langmuir of \"the way blood plasma carries red and white corpuscles and germs.\" \n\nLangmuir described the plasma he observed as follows:\n\nPlasma is a state of matter in which an ionised gaseous substance becomes highly electrically conductive to the point that long-range electric and magnetic fields dominate the behaviour of the matter. The plasma state can be contrasted with the other states: solid, liquid, and gas.\n\nPlasma is an electrically neutral medium of unbound positive and negative particles (i.e. the overall charge of a plasma is roughly zero). Although these particles are unbound, they are not \"free\" in the sense of not experiencing forces. Moving charged particles generate an electric current within a magnetic field, and any movement of a charged plasma particle affects and is affected by the fields created by the other charges. In turn this governs collective behaviour with many degrees of variation. Three factors define a plasma:\n\n\nPlasma temperature is commonly measured in kelvins or electronvolts and is, informally, a measure of the thermal kinetic energy per particle. High temperatures are usually needed to sustain ionisation, which is a defining feature of a plasma. The degree of plasma ionisation is determined by the electron temperature relative to the ionization energy (and more weakly by the density), in a relationship called the Saha equation. At low temperatures, ions and electrons tend to recombine into bound states—atoms—and the plasma will eventually become a gas.\n\nIn most cases the electrons are close enough to thermal equilibrium that their temperature is relatively well-defined, even when there is a significant deviation from a Maxwellian energy distribution function, for example, due to UV radiation, energetic particles, or strong electric fields. Because of the large difference in mass, the electrons come to thermodynamic equilibrium amongst themselves much faster than they come into equilibrium with the ions or neutral atoms. For this reason, the ion temperature may be very different from (usually lower than) the electron temperature. This is especially common in weakly ionised technological plasmas, where the ions are often near the ambient temperature.\n\nFor plasma to exist, ionisation is necessary. The term \"plasma density\" by itself usually refers to the \"electron density\", that is, the number of free electrons per unit volume. The degree of ionisation of a plasma is the proportion of atoms that have lost or gained electrons, and is controlled by the electron and ion temperatures and electron-ion vs electron-neutral collision frequencies. The degree of ionisation, formula_1, is defined as formula_2, where formula_3 is the number density of ions and formula_4 is the number density of neutral atoms. The \"electron density\" is related to this by the average charge state formula_5 of the ions through formula_6, where formula_7 is the number density of electrons.\n\nIn a plasma, the electron-ion collision frequency formula_8 is much greater than the electron-neutral collision frequency formula_9. Therefore, with a weak degree of ionization formula_1, the electron-ion collision frequency can equal the electron-neutral collision frequency: formula_11 is the limit separating a plasma from being partially or fully ionized.\n\n\n\nMost of \"technological\" (engineered) plasmas are weakly ionized gases.\n\nBased on the relative temperatures of the electrons, ions and neutrals, plasmas are classified as \"thermal\" or \"non-thermal\" (also referred to as \"cold plasmas\"). \n\n\n\nA particular and unusual case of \"inverse\" nonthermal plasma is the very high temperature plasma produced by the Z machine, where ions are much hotter than electrons.\n\nSince plasmas are very good electrical conductors, electric potentials play an important role. The average potential in the space between charged particles, independent of how it can be measured, is called the \"plasma potential\", or the \"space potential\". If an electrode is inserted into a plasma, its potential will generally lie considerably below the plasma potential due to what is termed a Debye sheath. The good electrical conductivity of plasmas makes their electric fields very small. This results in the important concept of \"quasineutrality\", which says the density of negative charges is approximately equal to the density of positive charges over large volumes of the plasma (formula_6), but on the scale of the Debye length there can be charge imbalance. In the special case that \"double layers\" are formed, the charge separation can extend some tens of Debye lengths.\n\nThe magnitude of the potentials and electric fields must be determined by means other than simply finding the net charge density. A common example is to assume that the electrons satisfy the Boltzmann relation:\n\nDifferentiating this relation provides a means to calculate the electric field from the density:\n\nIt is possible to produce a plasma that is not quasineutral. An electron beam, for example, has only negative charges. The density of a non-neutral plasma must generally be very low, or it must be very small, otherwise, it will be dissipated by the repulsive electrostatic force.\n\nIn astrophysical plasmas, Debye screening prevents electric fields from directly affecting the plasma over large distances, i.e., greater than the Debye length. However, the existence of charged particles causes the plasma to generate, and be affected by, magnetic fields. This can and does cause extremely complex behaviour, such as the generation of plasma double layers, an object that separates charge over a few tens of Debye lengths. The dynamics of plasmas interacting with external and self-generated magnetic fields are studied in the academic discipline of magnetohydrodynamics.\n\nPlasma with a magnetic field strong enough to influence the motion of the charged particles is said to be magnetized. A common quantitative criterion is that a particle on average completes at least one gyration around the magnetic field before making a collision, i.e., formula_18, where formula_19 is the \"electron gyrofrequency\" and formula_20 is the \"electron collision rate\". It is often the case that the electrons are magnetized while the ions are not. Magnetized plasmas are \"anisotropic\", meaning that their properties in the direction parallel to the magnetic field are different from those perpendicular to it. While electric fields in plasmas are usually small due to the high conductivity, the electric field associated with a plasma moving in a magnetic field is given by formula_21 (where formula_22 is the electric field, formula_23 is the velocity, and formula_24 is the magnetic field), and is not affected by Debye shielding.\n\nPlasma is often called the \"fourth state of matter\" after solid, liquids and gases, despite plasma typically being an ionised gas. It is distinct from these and other lower-energy states of matter. Although it is closely related to the gas phase in that it also has no definite form or volume, it differs in a number of ways, including the following:\n\nPlasmas are by far the most common phase of ordinary matter in the universe, both by mass and by volume.\n\nAbove the Earth's surface, the ionosphere is a plasma, and the magnetosphere contains plasma. Within our Solar System, interplanetary space is filled with the plasma expelled via the solar wind, extending from the Sun's surface out to the heliopause. Furthermore, all the distant stars, and much of interstellar space or intergalactic space is also likely filled with plasma, albeit at very low densities. Astrophysical plasmas are also observed in Accretion disks around stars or compact objects like white dwarfs, neutron stars, or black holes in close binary star systems. Plasma is associated with ejection of material in astrophysical jets, which have been observed with accreting black holes or in active galaxies like M87's jet that possibly extends out to 5,000 light-years.\n\nPlasmas can appear in nature in various forms and locations, which can be usefully broadly summarised in the following Table:\n\nAlthough the underlying equations governing plasmas are relatively simple, plasma behaviour is extraordinarily varied and subtle: the emergence of unexpected behaviour from a simple model is a typical feature of a complex system. Such systems lie in some sense on the boundary between ordered and disordered behaviour and cannot typically be described either by simple, smooth, mathematical functions, or by pure randomness. The spontaneous formation of interesting spatial features on a wide range of length scales is one manifestation of plasma complexity. The features are interesting, for example, because they are very sharp, spatially intermittent (the distance between features is much larger than the features themselves), or have a fractal form. Many of these features were first studied in the laboratory, and have subsequently been recognized throughout the universe. Examples of complexity and complex structures in plasmas include:\n\nStriations or string-like structures, also known as Birkeland currents, are seen in many plasmas, like the plasma ball, the aurora, lightning, electric arcs, solar flares, and supernova remnants. They are sometimes associated with larger current densities, and the interaction with the magnetic field can form a magnetic rope structure. High power microwave breakdown at atmospheric pressure also leads to the formation of filamentary structures. (See also Plasma pinch)\n\nFilamentation also refers to the self-focusing of a high power laser pulse. At high powers, the nonlinear part of the index of refraction becomes important and causes a higher index of refraction in the center of the laser beam, where the laser is brighter than at the edges, causing a feedback that focuses the laser even more. The tighter focused laser has a higher peak brightness (irradiance) that forms a plasma. The plasma has an index of refraction lower than one, and causes a defocusing of the laser beam. The interplay of the focusing index of refraction, and the defocusing plasma makes the formation of a long filament of plasma that can be micrometers to kilometers in length. One interesting aspect of the filamentation generated plasma is the relatively low ion density due to defocusing effects of the ionised electrons. (See also Filament propagation)\n\nThe strength and range of the electric force and the good conductivity of plasmas usually ensure that the densities of positive and negative charges in any sizeable region are equal (\"quasineutrality\"). A plasma with a significant excess of charge density, or, in the extreme case, is composed of a single species, is called a non-neutral plasma. In such a plasma, electric fields play a dominant role. Examples are charged particle beams, an electron cloud in a Penning trap and positron plasmas.\n\nA dusty plasma contains tiny charged particles of dust (typically found in space). The dust particles acquire high charges and interact with each other. A plasma that contains larger particles is called grain plasma. Under laboratory conditions, dusty plasmas are also called \"complex plasmas\".\n\nImpermeable plasma is a type of thermal plasma which acts like an impermeable solid with respect to gas or cold plasma and can be physically pushed. Interaction of cold gas and thermal plasma was briefly studied by a group led by Hannes Alfvén in 1960s and 1970s for its possible applications in insulation of fusion plasma from the reactor walls. However, later it was found that the external magnetic fields in this configuration could induce kink instabilities in the plasma and subsequently lead to an unexpectedly high heat loss to the walls.\nIn 2013, a group of materials scientists reported that they have successfully generated stable impermeable plasma with no magnetic confinement using only an ultrahigh-pressure blanket of cold gas. While spectroscopic data on the characteristics of plasma were claimed to be difficult to obtain due to the high pressure, the passive effect of plasma on synthesis of different nanostructures clearly suggested the effective confinement. They also showed that upon maintaining the impermeability for a few tens of seconds, screening of ions at the plasma-gas interface could give rise to a strong secondary mode of heating (known as viscous heating) leading to different kinetics of reactions and formation of complex nanomaterials.\n\nTo completely describe the state of a plasma, all of the\nparticle locations and velocities that describe the electromagnetic field in the plasma region would need to be written down.\nHowever, it is generally not practical or necessary to keep track of all the particles in a plasma.\nTherefore, plasma physicists commonly use less detailed descriptions, of which\nthere are two main types:\n\nFluid models describe plasmas in terms of smoothed quantities, like density and averaged velocity around each position (see Plasma parameters). One simple fluid model, magnetohydrodynamics, treats the plasma as a single fluid governed by a combination of Maxwell's equations and the Navier–Stokes equations. A more general description is the two-fluid plasma picture, where the ions and electrons are described separately. Fluid models are often accurate when collisionality is sufficiently high to keep the plasma velocity distribution close to a Maxwell–Boltzmann distribution. Because fluid models usually describe the plasma in terms of a single flow at a certain temperature at each spatial location, they can neither capture velocity space structures like beams or double layers, nor resolve wave-particle effects.\n\nKinetic models describe the particle velocity distribution function at each point in the plasma and therefore do not need to assume a Maxwell–Boltzmann distribution. A kinetic description is often necessary for collisionless plasmas. There are two common approaches to kinetic description of a plasma. One is based on representing the smoothed distribution function on a grid in velocity and position. The other, known as the particle-in-cell (PIC) technique, includes kinetic information by following the trajectories of a large number of individual particles. Kinetic models are generally more computationally intensive than fluid models. The Vlasov equation may be used to describe the dynamics of a system of charged particles interacting with an electromagnetic field.\nIn magnetized plasmas, a gyrokinetic approach can substantially reduce the computational expense of a fully kinetic simulation.\n\nMost artificial plasmas are generated by the application of electric and/or magnetic fields through a gas. Plasma generated in a laboratory setting and for industrial use can be generally categorized by:\n\nJust like the many uses of plasma, there are several means for its generation, however, one principle is common to all of them: there must be energy input to produce and sustain it. For this case, plasma is generated when an electric current is applied across a dielectric gas or fluid (an electrically non-conducting material) as can be seen in the adjacent image, which shows a discharge tube as a simple example (DC used for simplicity).\n\nThe potential difference and subsequent electric field pull the bound electrons (negative) toward the anode (positive electrode) while the cathode (negative electrode) pulls the nucleus. As the voltage increases, the current stresses the material (by electric polarization) beyond its dielectric limit (termed strength) into a stage of electrical breakdown, marked by an electric spark, where the material transforms from being an insulator into a conductor (as it becomes increasingly ionised). The underlying process is the Townsend avalanche, where collisions between electrons and neutral gas atoms create more ions and electrons (as can be seen in the figure on the right). The first impact of an electron on an atom results in one ion and two electrons. Therefore, the number of charged particles increases rapidly (in the millions) only \"after about 20 successive sets of collisions\", mainly due to a small mean free path (average distance travelled between collisions).\n\nWith ample current density and ionisation, this forms a luminous electric arc (a continuous electric discharge similar to lightning) between the electrodes. Electrical resistance along the continuous electric arc creates heat, which dissociates more gas molecules and ionises the resulting atoms (where degree of ionisation is determined by temperature), and as per the sequence: solid-liquid-gas-plasma, the gas is gradually turned into a thermal plasma. A thermal plasma is in thermal equilibrium, which is to say that the temperature is relatively homogeneous throughout the heavy particles (i.e. atoms, molecules and ions) and electrons. This is so because when thermal plasmas are generated, electrical energy is given to electrons, which, due to their great mobility and large numbers, are able to disperse it rapidly and by elastic collision (without energy loss) to the heavy particles.\n\nBecause of their sizable temperature and density ranges, plasmas find applications in many fields of research, technology and industry. For example, in: industrial and extractive metallurgy, surface treatments such as plasma spraying (coating), etching in microelectronics, metal cutting and welding; as well as in everyday vehicle exhaust cleanup and fluorescent/luminescent lamps, fuel ignition, while even playing a part in supersonic combustion engines for aerospace engineering.\n\n\n\nA world effort was triggered in the 1960s to study magnetohydrodynamic converters in order to bring MHD power conversion to market with commercial power plants of a new kind, converting the kinetic energy of a high velocity plasma into electricity with no moving parts at a high efficiency. Research was also conducted in the field of supersonic and hypersonic aerodynamics to study plasma interaction with magnetic fields to eventually achieve passive and even active flow control around vehicles or projectiles, in order to soften and mitigate shock waves, lower thermal transfer and reduce drag.\n\nSuch ionized gases used in \"plasma technology\" (\"technological\" or \"engineered\" plasmas) are usually \"weakly ionized gases\" in the sense that only a tiny fraction of the gas molecules are ionized. These kinds of weakly ionized gases are also nonthermal \"cold\" plasmas. In the presence of magnetics fields, the study of such magnetized nonthermal weakly ionized gases involves resistive magnetohydrodynamics with low magnetic Reynolds number, a challenging field of plasma physics where calculations require dyadic tensors in a 7-dimensional phase space. When used in combination with a high Hall parameter, a critical value triggers the problematic electrothermal instability which limited these technological developments.\n\nPlasmas are the object of study of the academic field of \"plasma science\" or \"plasma physics\", including sub-disciplines such as space plasma physics. It currently involves the following fields of active research and features across many journals, whose interest includes:\n\n\n\n\n"}
{"id": "26145195", "url": "https://en.wikipedia.org/wiki?curid=26145195", "title": "Plastic", "text": "Plastic\n\nPlastic is material consisting of any of a wide range of synthetic or semi-synthetic organic compounds that are malleable and so can be molded into solid objects.\n\nPlasticity is the general property of all materials which can deform irreversibly without breaking but, in the class of moldable polymers, this occurs to such a degree that their actual name derives from this specific ability.\n\nPlastics are typically organic polymers of high molecular mass and often contain other substances. They are usually synthetic, most commonly derived from petrochemicals, however, an array of variants are made from renewable materials such as polylactic acid from corn or cellulosics from cotton linters.\n\nDue to their low cost, ease of manufacture, versatility, and imperviousness to water, plastics are used in a multitude of products of different scale, including paper clips and spacecraft. They have prevailed over traditional materials, such as wood, stone, horn and bone, leather, metal, glass, and ceramic, in some products previously left to natural materials.\n\nIn developed economies, about a third of plastic is used in packaging and roughly the same in buildings in applications such as piping, plumbing or vinyl siding. Other uses include automobiles (up to 20% plastic), furniture, and toys. In the developing world, the applications of plastic may differ — 42% of India's consumption is used in packaging.\n\nPlastics have many uses in the medical field as well, with the introduction of polymer implants and other medical devices derived at least partially from plastic. The field of plastic surgery is not named for use of plastic materials, but rather the meaning of the word plasticity, with regard to the reshaping of flesh.\n\nThe world's first fully synthetic plastic was bakelite, invented in New York in 1907 by Leo Baekeland who coined the term 'plastics'. Many chemists have contributed to the materials science of plastics, including Nobel laureate Hermann Staudinger who has been called \"the father of polymer chemistry\" and Herman Mark, known as \"the father of polymer physics\".\n\nThe success and dominance of plastics starting in the early 20th century led to environmental concerns regarding its slow decomposition rate after being discarded as trash due to its composition of large molecules. Toward the end of the century, one approach to this problem was met with wide efforts toward recycling.\n\nThe word plastic derives from the Greek πλαστικός (\"plastikos\") meaning \"capable of being shaped or molded\" and, in turn, from πλαστός (\"plastos\") meaning \"molded\".\n\nThe plasticity, or malleability, of the material during manufacture allows it to be cast, pressed, or extruded into a variety of shapes, such as: films, fibers, plates, tubes, bottles, boxes, amongst many others.\n\nThe common noun plastic should not be confused with the technical adjective \"plastic\". The adjective is applicable to any material which undergoes a plastic deformation, or permanent change of shape, when strained beyond a certain point.\nFor example, aluminum which is stamped or forged exhibits plasticity in this sense, but is not plastic in the common sense. By contrast, some plastics will, in their finished forms, break before deforming and therefore are not \"plastic\" in the technical sense.\n\nMost plastics contain organic polymers. The vast majority of these polymers are formed from \"chains of carbon atoms\", 'pure' or with the addition of: oxygen, nitrogen, or sulfur. The chains comprise many repeat units, formed from monomers. Each polymer chain will have several thousand repeating units.\n\nThe backbone is the part of the chain that is on the \"main path\", linking together a large number of repeat units.\n\nTo customize the properties of a plastic, different molecular groups \"hang\" from this backbone. These \"pendant units\" are usually \"hung\" on the monomers, before the monomers themselves are linked together to form the polymer chain. It is the structure of these side chains that influences the properties of the polymer.\n\nThe molecular structure of the repeating unit can be fine tuned to influence specific properties in the polymer.\n\nPlastics are usually classified by: the chemical structure of the polymer's backbone and side chains; some important groups in these classifications are: the acrylics, polyesters, silicones, polyurethanes, and halogenated plastics.\n\nPlastics can also be classified by: the chemical process used in their synthesis, such as: condensation, polyaddition, and cross-linking.\n\nPlastics can also be classified by: their various physical properties, such as: hardness, density, tensile strength, resistance to heat and glass transition temperature, and by their chemical properties, such as the organic chemistry of the polymer and its resistance and reaction to various chemical products and processes, such as: organic solvents, oxidation, and ionizing radiation. In particular, most plastics will melt upon heating to a few hundred degrees celsius.\n\nOther classifications are based on qualities that are relevant for manufacturing or product design. Examples of such qualities and classes are: thermoplastics and thermosets, conductive polymers, biodegradable plastics and engineering plastics and other plastics with particular structures, such as elastomers.\n\nOne important classification of plastics is by the permanence or impermanence of their form, or whether they are: thermoplastics or thermosetting polymers.\nThermoplastics are the plastics that, when heated, do not undergo chemical change in their composition and so can be molded again and again. Examples include: polyethylene (PE), polypropylene (PP), polystyrene (PS) and polyvinyl chloride (PVC). Common thermoplastics range from 20,000 to 500,000 amu, while thermosets are assumed to have infinite molecular weight.\n\nThermosets, or thermosetting polymers, can melt and take shape only once: after they have solidified, they stay solid. In the thermosetting process, a chemical reaction occurs that is irreversible. The vulcanization of rubber is an example of a thermosetting process: before heating with sulfur, the polyisoprene is a tacky, slightly runny material; after vulcanization, the product is rigid and non-tacky.\n\nMany plastics are completely amorphous, such as: all thermosets; polystyrene and its copolymers; and polymethyl methacrylate.\n\nHowever, some plastics are partially crystalline and partially amorphous in molecular structure, giving them both a melting point, the temperature at which the attractive intermolecular forces are overcome, and also one or more glass transitions, the temperatures above which the extent of localized molecular flexibility is substantially increased. These so-called semi-crystalline plastics include: polyethylene, polypropylene, polyvinyl chloride, polyamides (nylons), polyesters and some polyurethanes.\n\nIntrinsically Conducting Polymers (ICP) are organic polymers that conduct electricity. While plastics can be made electrically conductive, with a conductivity of up to 80 kS/cm in stretch-oriented polyacetylene, they are still no match for most metals like copper which have a conductivity of several hundred kS/cm. Nevertheless, this is a developing field.\n\nBiodegradable plastics are plastics that degrade, or break down, upon exposure to: sunlight or ultra-violet radiation, water or dampness, bacteria, enzymes or wind abrasion. In some instances, rodent, pest, or insect attack can also be considered as forms of biodegradation or environmental degradation.\n\nSome modes of degradation require that the plastic be exposed at the surface (aerobic), whereas other modes will only be effective if certain conditions exist in landfill or composting systems (anaerobic).\n\nSome companies produce biodegradable additives, to enhance biodegradation. Plastic can have starch powder added as a filler to allow it to degrade more easily, but this still does not lead to the complete breaking down of the plastic.\n\nSome researchers have genetically engineered bacteria to synthesize completely biodegradable plastics, such as Biopol; however, these are expensive at present.\n\nWhile most plastics are produced from petrochemicals, bioplastics are made substantially from renewable plant materials such: as cellulose and starch. Due both to the finite limits of the petrochemical reserves and to the threat of global warming, the development of bioplastics is a growing field.\n\nHowever, bioplastic development begins from a very low base and, as yet, does not compare significantly with petrochemical production. Estimates of the global production capacity for bio-derived materials is put at 327,000 tonnes/year. In contrast, global production of polyethylene (PE) and polypropylene (PP), the world’s leading petrochemical derived polyolefins, was estimated at over 150 million tonnes in 2015.\n\nThis category includes both commodity plastics, or standard plastics, and engineering plastics.\n\n\nThe development of plastics has evolved from the use of natural plastic materials (e.g., chewing gum, shellac) to the use of chemically modified, natural materials (e.g., natural rubber, nitrocellulose, collagen, galalite) and finally to completely synthetic molecules (e.g., bakelite, epoxy, polyvinyl chloride). Early plastics were bio-derived materials such as egg and blood proteins, which are organic polymers. In around 1600 BC, Mesoamericans used natural rubber for balls, bands, and figurines. Treated cattle horns were used as windows for lanterns in the Middle Ages. Materials that mimicked the properties of horns were developed by treating milk-proteins (casein) with lye.\n\nIn the nineteenth century, as industrial chemistry developed during the Industrial Revolution, many materials were reported. The development of plastics also accelerated with Charles Goodyear's discovery of vulcanization to thermoset materials derived from natural rubber.\n\nParkesine (nitrocellulose) is considered the first man-made plastic. The plastic material was patented by Alexander Parkes, in Birmingham, England in 1856. It was unveiled at the 1862 Great International Exhibition in London. \"Parkesine\" won a bronze medal at the 1862 World's fair in London. Parkesine was made from cellulose (the major component of plant cell walls) treated with nitric acid as a solvent. The output of the process (commonly known as cellulose nitrate or pyroxilin) could be dissolved in alcohol and hardened into a transparent and elastic material that could be molded when heated. By incorporating pigments into the product, it could be made to resemble ivory.\n\nIn 1897, the Hanover, Germany mass printing press owner Wilhelm Krische was commissioned to develop an alternative to blackboards. The resultant horn-like plastic made from the milk protein casein was developed in cooperation with the Austrian chemist (Friedrich) Adolph Spitteler (1846–1940). The final result was unsuitable for the original purpose. In 1893, French chemist Auguste Trillat discovered the means to insolubilize casein by immersion in formaldehyde, producing material marketed as galalith.\n\nIn the early 1900s, Bakelite, the first fully synthetic thermoset, was reported by Belgian chemist Leo Baekeland by using phenol and formaldehyde.\n\nAfter World War I, improvements in chemical technology led to an explosion in new forms of plastics, with mass production beginning in the 1940s and 1950s (around World War II). Among the earliest examples in the wave of new polymers were polystyrene (PS), first produced by BASF in the 1930s, and polyvinyl chloride (PVC), first created in 1872 but commercially produced in the late 1920s. In 1923, Durite Plastics Inc. was the first manufacturer of phenol-furfural resins. In 1933, polyethylene was discovered by Imperial Chemical Industries (ICI) researchers Reginald Gibson and Eric Fawcett.\n\nIn 1954, polypropylene was discovered by Giulio Natta and began to be manufactured in 1957.\n\nIn 1954, expanded polystyrene (used for building insulation, packaging, and cups) was invented by Dow Chemical.\nPolyethylene terephthalate (PET)'s discovery is credited to employees of the Calico Printers' Association in the UK in 1941; it was licensed to DuPont for the US and ICI otherwise, and as one of the few plastics appropriate as a replacement for glass in many circumstances, resulting in widespread use for bottles in Europe.\n\nPlastics manufacturing is a major part of the chemical industry, and some of the world's largest chemical companies have been involved since the earliest days, such as the industry leaders BASF and Dow Chemical.\n\nIn 2014, sales of the top fifty companies amounted to . The firms came from some eighteen countries in total, with more than half of the companies on the list being headquartered in the US. Many of the top fifty plastics companies were concentrated in just three countries:\n\nBASF was the world's largest chemical producer for the ninth year in a row.\n\nTrade associations which represent the industry in the US include the American Chemistry Council.\n\nMany of the properties of plastics are determined by standards specified by ISO, such as:\n\nMany of the properties of plastics are determined by the UL Standards, tests specified by Underwriters Laboratories (UL), such as:\n\nBlended into most plastics are additional organic or inorganic compounds. The average content of additives is a few percent. Many of the controversies associated with plastics actually relate to the additives: organotin compounds are particularly toxic.\n\nTypical additives include:\n\nPolymer stabilizers prolong the lifetime of the polymer by suppressing degradation that results from UV-light, oxidation, and other phenomena. Typical stabilizers thus absorb UV light or function as antioxidants.\n\nMany plastics contain fillers, to improve performance or reduce production costs. Typically fillers are mineral in origin, e.g., chalk. Other fillers include: starch, cellulose, wood flour, ivory dust and zinc oxide.\n\nPlasticizers are, by mass, often the most abundant additives. These oily but nonvolatile compounds are blended in to plastics to improve rheology, as many organic polymers are otherwise too rigid for particular applications.\n\nColorants are another common additive, though their weight contribution is small.\n\nPure plastics have low toxicity due to their insolubility in water and because they are biochemically inert, due to a large molecular weight. Plastic products contain a variety of additives, some of which can be toxic. For example, plasticizers like adipates and phthalates are often added to brittle plastics like polyvinyl chloride to make them pliable enough for use in food packaging, toys, and many other items. Traces of these compounds can leach out of the product. Owing to concerns over the effects of such leachates, the European Union has restricted the use of DEHP (di-2-ethylhexyl phthalate) and other phthalates in some applications, and the United States has limited the use of DEHP, DPB, BBP, DINP, DIDP, and DnOP in children's toys and child care articles with the Consumer Product Safety Improvement Act. Some compounds leaching from polystyrene food containers have been proposed to interfere with hormone functions and are suspected human carcinogens. Other chemicals of potential concern include alkylphenols.\n\nWhereas the finished plastic may be non-toxic, the monomers used in the manufacture of the parent polymers may be toxic. In some cases, small amounts of those chemicals can remain trapped in the product unless suitable processing is employed. For example, the World Health Organization's International Agency for Research on Cancer (IARC) has recognized vinyl chloride, the precursor to PVC, as a human carcinogen.\n\nSome polymers may also decompose into the monomers or other toxic substances when heated. In 2011, it was reported that \"almost all plastic products\" sampled released chemicals with estrogenic activity, although the researchers identified plastics which did not leach chemicals with estrogenic activity.\n\nThe primary building block of polycarbonates, bisphenol A (BPA), is an estrogen-like endocrine disruptor that may leach into food. Research in Environmental Health Perspectives finds that BPA leached from the lining of tin cans, dental sealants and polycarbonate bottles can increase body weight of lab animals' offspring. A more recent animal study suggests that even low-level exposure to BPA results in insulin resistance, which can lead to inflammation and heart disease.\n\nAs of January 2010, the LA Times newspaper reports that the United States FDA is spending $30 million to investigate indications of BPA being linked to cancer.\n\nBis(2-ethylhexyl) adipate, present in plastic wrap based on PVC, is also of concern, as are the volatile organic compounds present in new car smell.\n\nThe European Union has a permanent ban on the use of phthalates in toys. In 2009, the United States government banned certain types of phthalates commonly used in plastic.\n\nMost plastics are durable and degrade very slowly, as their chemical structure renders them resistant to many natural processes of degradation. There are differing estimates of how much plastic waste has been produced in the last century. By one estimate, one billion tons of plastic waste have been discarded since the 1950s. Others estimate a cumulative human production of 8.3 billion tons of plastic of which 6.3 billion tons is waste, with a recycling rate of only 9%. Much of this material may persist for centuries or longer, given the demonstrated persistence of structurally similar natural materials such as amber.\n\nThe presence of plastics, particularly microplastics, within the food chain is increasing. In the 1960s microplastics were observed in the guts of seabirds, and since then have been found in increasing concentrations. The long-term effects of plastic in the food chain are poorly understood. In 2009, it was estimated that 10% of modern waste was plastic, although estimates vary according to region. Meanwhile, 50–80% of debris in marine areas is plastic.\n\nPrior to the Montreal Protocol, CFCs were commonly used in the manufacture of polystyrene, and as such the production of polystyrene contributed to the depletion of the ozone layer.\n\nThe effect of plastics on global warming is mixed. Plastics are generally made from petroleum. If the plastic is incinerated, it increases carbon emissions; if it is placed in a landfill, it becomes a carbon sink although biodegradable plastics have caused methane emissions.\n\nProduction of plastics from crude oil requires 62 to 108 MJ/Kg (taking into account the average efficiency of US utility stations of 35%). Producing silicon and semiconductors for modern electronic equipment is even more energy consuming: 230 to 235 MJ/Kg of silicon, and about 3,000 MJ/Kg of semiconductors. This is much higher than the energy needed to produce many other materials, e.g. iron (from iron ore) requires 20-25 MJ/Kg of energy, glass (from sand, etc.) 18–35 MJ/Kg, steel (from iron) 20–50 MJ/Kg, paper (from timber) 25–50 MJ/Kg.\n\nControlled high-temperature incineration, above 850 °C for two seconds, performed with selective additional heating, breaks down toxic dioxins and furans from burning plastic, and is widely used in municipal solid waste incineration. Municipal solid waste incinerators also normally include flue gas treatments to reduce pollutants further. This is needed because uncontrolled incineration of plastic produces polychlorinated dibenzo-p-dioxins, a carcinogen (cancer causing chemical). The problem occurs because the heat content of the waste stream varies. Open-air burning of plastic occurs at lower temperatures, and normally releases such toxic fumes.\n\nPlastics can be pyrolyzed into hydrocarbon fuels, since plastics include hydrogen and carbon. One kilogram of waste plastic produces roughly a liter of hydrocarbon.\n\nPlastics contribute to approximately 10% of discarded waste. Depending on their chemical composition, plastics and resins have varying properties related to contaminant absorption and adsorption. Polymer degradation takes much longer as a result of saline environments and the cooling effect of the sea. These factors contribute to the persistence of plastic debris in certain environments. Recent studies have shown that plastics in the ocean decompose faster than was once thought, due to exposure to sun, rain, and other environmental conditions, resulting in the release of toxic chemicals such as bisphenol A. However, due to the increased volume of plastics in the ocean, decomposition has slowed down. The Marine Conservancy has predicted the decomposition rates of several plastic products. It is estimated that a foam plastic cup will take 50 years, a plastic beverage holder will take 400 years, a disposable nappy will take 450 years, and fishing line will take 600 years to degrade.\n\nMicrobial species capable of degrading plastics are known to science, and some are potentially useful for the disposal of certain classes of plastic waste.\n\nThermoplastics can be remelted and reused, and thermoset plastics can be ground up and used as filler, although the purity of the material tends to degrade with each reuse cycle. There are methods by which plastics can be broken down to a feedstock state.\n\nThe greatest challenge to the recycling of plastics is the difficulty of automating the sorting of plastic wastes, making it labor-intensive. Typically, workers sort the plastic by looking at the resin identification code, although common containers like soda bottles can be sorted from memory. Typically, the caps for PETE bottles are made from a different kind of plastic which is not recyclable, which presents additional problems for the sorting process. Other recyclable materials such as metals are easier to process mechanically. However, new processes of mechanical sorting are being developed to increase the capacity and efficiency of plastic recycling.\n\nWhile containers are usually made from a single type and color of plastic, making them relatively easy to sort, a consumer product like a cellular phone may have many small parts consisting of over a dozen different types and colors of plastics. In such cases, the resources it would take to separate the plastics far exceed their value and the item is discarded. However, developments are taking place in the field of active disassembly, which may result in more product components being reused or recycled. Recycling certain types of plastics can be unprofitable as well. For example, polystyrene is rarely recycled because the process is usually not cost effective. These unrecycled wastes are typically disposed of in landfills, incinerated or used to produce electricity at waste-to-energy plants.\n\nAn early success in the recycling of plastics is Vinyloop, an industrial process to separate PVC from other materials through dissolution, filtration and separation of contaminants. A solvent is used in a closed loop to elute PVC from the waste. This makes it possible to recycle composite PVC waste, which is normally incinerated or put in a landfill. Vinyloop-based recycled PVC's primary energy demand is 46 percent lower than conventionally produced PVC. The global warming potential is 39 percent lower. This is why the use of recycled material leads to a significantly better ecological outcome. This process was used after the Olympic Games in London 2012. Parts of temporary Buildings like the Water Polo Arena and the Royal Artillery Barracks were recycled. In this way, the PVC Policy could be fulfilled, which says that no PVC waste should be left after the games had ended.\n\nIn 1988, to assist recycling of disposable items, the Plastic Bottle Institute of the U.S. Society of the Plastics Industry devised a now-familiar scheme to mark plastic bottles by plastic type. Under this scheme, a plastic container is marked with a triangle of three \"chasing arrows\", which encloses a number denoting the plastic type:\n\n\nThe first plastic based on a synthetic polymer was made from phenol and formaldehyde, with the first viable and cheap synthesis methods invented in 1907, by Leo Hendrik Baekeland, a Belgian-born American living in New York state. Baekeland was looking for an insulating shellac to coat wires in electric motors and generators. He found that combining phenol (CHOH) and formaldehyde (HCOH) formed a sticky mass and later found that the material could be mixed with wood flour, asbestos, or slate dust to create strong and fire resistant \"composite\" materials. The new material tended to foam during synthesis, requiring that Baekeland build pressure vessels to force out the bubbles and provide a smooth, uniform product, as he announced in 1909, in a meeting of the American Chemical Society. Bakelite was originally used for electrical and mechanical parts, coming into widespread use in consumer goods and jewelry in the 1920s. Bakelite was a purely synthetic material, not derived from living matter. It was also an early thermosetting plastic.\n\nUnplasticised polystyrene is a rigid, brittle, inexpensive plastic that has been used to make plastic model kits and similar knick-knacks. It also is the basis for some of the most popular \"foamed\" plastics, under the name \"styrene foam\" or \"Styrofoam\". Like most other foam plastics, foamed polystyrene can be manufactured in an \"open cell\" form, in which the foam bubbles are interconnected, as in an absorbent sponge, and \"closed cell\", in which all the bubbles are distinct, like tiny balloons, as in gas-filled foam insulation and flotation devices. In the late 1950s, \"high impact\" styrene was introduced, which was not brittle. It finds much current use as the substance of toy figurines and novelties.\n\nPolyvinyl chloride (PVC, commonly called \"vinyl\") incorporates chlorine atoms. The C-Cl bonds in the backbone are hydrophobic and resist oxidation (and burning). PVC is stiff, strong, heat and weather resistant, properties that recommend its use in devices for plumbing, gutters, house siding, enclosures for computers and other electronics gear. PVC can also be softened with chemical processing, and in this form it is now used for shrink-wrap, food packaging, and rain gear.\n\nAll PVC polymers are degraded by heat and light. When this happens, hydrogen chloride is released into the atmosphere and oxidation of the compound occurs. Because hydrogen chloride readily combines with water vapor in the air to form hydrochloric acid, polyvinyl chloride is not recommended for long-term archival storage of silver, photographic film or paper (mylar is preferable).\n\nThe plastics industry was revolutionized in the 1930s with the announcement of polyamide (PA), far better known by its trade name nylon. Nylon was the first purely synthetic fiber, introduced by DuPont Corporation at the 1939 World's Fair in New York City.\n\nIn 1927, DuPont had begun a secret development project designated Fiber66, under the direction of Harvard chemist Wallace Carothers and chemistry department director Elmer Keiser Bolton. Carothers had been hired to perform pure research, and he worked to understand the new materials' molecular structure and physical properties. He took some of the first steps in the molecular design of the materials.\n\nHis work led to the discovery of synthetic nylon fiber, which was very strong but also very flexible. The first application was for bristles for toothbrushes. However, Du Pont's real target was silk, particularly silk stockings. Carothers and his team synthesized a number of different polyamides including polyamide 6.6 and 4.6, as well as polyesters.\n\nIt took DuPont twelve years and US$27 million to refine nylon, and to synthesize and develop the industrial processes for bulk manufacture. With such a major investment, it was no surprise that Du Pont spared little expense to promote nylon after its introduction, creating a public sensation, or \"nylon mania\".\n\nNylon mania came to an abrupt stop at the end of 1941 when the US entered World War II. The production capacity that had been built up to produce nylon stockings, or just \"nylons\", for American women was taken over to manufacture vast numbers of parachutes for fliers and paratroopers. After the war ended, DuPont went back to selling nylon to the public, engaging in another promotional campaign in 1946 that resulted in an even bigger craze, triggering the so-called nylon riots.\n\nSubsequently, polyamides 6, 10, 11, and 12 have been developed based on monomers which are ring compounds; e.g. caprolactam. Nylon 66 is a material manufactured by condensation polymerization.\n\nNylons still remain important plastics, and not just for use in fabrics. In its bulk form it is very wear resistant, particularly if oil-impregnated, and so is used to build gears, plain bearings, valve seats, seals and because of good heat-resistance, increasingly for under-the-hood applications in cars, and other mechanical parts.\n\nPoly(methyl methacrylate) (PMMA), also known as acrylic or acrylic glass as well as by the trade names Plexiglas, Acrylite, Lucite, and Perspex among several others (see below), is a transparent thermoplastic often used in sheet form as a lightweight or shatter-resistant alternative to glass. The same material can be utilised as a casting resin, in inks and coatings, and has many other uses.\n\nNatural rubber is an elastomer (an elastic hydrocarbon polymer) that originally was derived from \"latex\", a milky colloidal suspension found in specialised vessels in some plants. It is useful directly in this form (indeed, the first appearance of rubber in Europe was cloth waterproofed with unvulcanized latex from Brazil). However, in 1839, Charles Goodyear invented vulcanized rubber; a form of natural rubber heated with sulfur (and a few other chemicals), forming cross-links between polymer chains (vulcanization), improving elasticity and durability.\n\nIn 1851, Nelson Goodyear added fillers to natural rubber materials to form ebonite.\n\nThe first fully synthetic rubber was synthesized by Sergei Lebedev in 1910. In World War II, supply blockades of natural rubber from South East Asia caused a boom in development of synthetic rubber, notably styrene-butadiene rubber. In 1941, annual production of synthetic rubber in the U.S. was only 231 tonnes which increased to 840,000 tonnes in 1945. In the space race and nuclear arms race, Caltech researchers experimented with using synthetic rubbers for solid fuel for rockets. Ultimately, all large military rockets and missiles would use synthetic rubber based solid fuels, and they would also play a significant part in the civilian space effort.\n\n\n"}
{"id": "28159230", "url": "https://en.wikipedia.org/wiki?curid=28159230", "title": "Prosperity Without Growth", "text": "Prosperity Without Growth\n\nProsperity Without Growth is a book by author and economist Tim Jackson. It was originally released as a report by the Sustainable Development Commission. The study rapidly became the most downloaded report in the Commission's nine-year history when it was published in 2009. The report was later that year reworked and published as a book by Earthscan. A revised and expanded edition (\"Prosperity Without Growth: Foundations for the Economy of Tomorrow\") was published in January 2017.\n\nBy arguing that \"prosperity – in any meaningful sense of the word – transcends material concerns\", the book summarizes the evidence showing that, beyond a certain point, growth does not increase human well-being. \"Prosperity without Growth\" analyses the complex relationships between economic growth, environmental crises and social recession. It proposes a route to a sustainable economy, and argues for a redefinition of \"prosperity\" in light of the evidence on what really contributes to people’s well-being. \n\nThe second edition expands on these ideas and sets out the framework for what he calls \"the economy of tomorrow\". By attending the nature of enterprise as a form of social organisation, the meaning of work as participation in society, the function of investment as a commitment to the future; and the role of money as a social good, he demonstrates how the economy may be transformed in ways that protect employment, promotes and facilitates social investment, reduce inequality and support both ecological and financial stability.\n\nThe first edition was described by \"Le Monde\" as \"one of the most outstanding pieces of environmental economics literature in recent years\". The sociologist Anthony Giddens referred to it as \"a must-read for anyone concerned with issues of climate change and sustainability - bold, original and comprehensive.\" The second edition received endorsements from Yanis Varoufakis, who referred to it as “essential reading for those refusing to succumb to a dystopic future”. Noam Chomsky called it a \"thoughtful and penetrating critique\". Herman Daly praised it with: \"It is hard to improve a classic, but Jackson has done it... a clearly written yet scholarly union of moral vision, with solid economics.\" Rowan Williams called it \"one of the most important essays of our generation: both visionary and realistic, rooted in careful research and setting out difficult but achievable goals, it gives what we so badly need - an alternative to passivity, short-term selfishness and cynicism\".\n\nThe second edition of \"Prosperity without growth: Foundations for the Economy of Tomorrow\" is organised in eleven chapters:\n\n\"Prosperity without Growth: Economics for a Finite Planet\" (2009) has been translated into seventeen languages including Swedish (\"Välfärd utan tillväxt: så skapar vi ett hållbart samhälle\", 2011), German (\"Wohlstand ohne Wachstum\", 2011), French (\"Prospérité sans croissance\", 2010), Greek (\"Ευημερία χωρίς ανάπτυξη\", 2012), Spanish (\"Prosperidad sin crecimiento\", 2011), Italian (\"Prosperità senza crescita\", 2011), Dutch (\"Welvaart zonder groei\", 2010) and Chinese (\"无增长的繁荣，2011\").\n\nThe second edition, \"Prosperity without Growth: Foundations for the Economy of Tomorrow\" (2017), has been translated into German, French and Italian.\n\n\n"}
{"id": "2558645", "url": "https://en.wikipedia.org/wiki?curid=2558645", "title": "RYAN", "text": "RYAN\n\nOperation RYAN (or RYaN, ) was a Cold War military intelligence program run by the Soviet Union during the early 1980s when they believed the United States was planning for an imminent first strike attack. The name is an acronym for Raketno-Yadernoe Napadenie (, \"Nuclear Missile Attack\"). The purpose of the operation was to collect intelligence on potential contingency plans of the Reagan administration to launch a nuclear first strike against the Soviet Union. The program was initiated in May 1981 by Yuri Andropov, then chairman of the KGB.\n\nAccording to the historian Christopher Andrew, Andropov suffered from a \"Hungarian complex\" from his personal experience of the Hungarian Revolution in 1956. He had, as the Soviet ambassador to Hungary, \"watched in horror from the windows of his embassy as officers of the hated Hungarian security service were strung up from lampposts\". Andropov remained haunted for the rest of his life by the speed with which an apparently all-powerful Communist one-party state had begun to topple. Leonid Brezhnev and Yuri Andropov, then Chairman of the KGB, justified the creation of Operation RYaN because, they claimed, the United States was \"actively preparing for nuclear war\" against the Soviet Union and its allies. According to a newly released Stasi report, the primary \"Chekist work\" discussed in the May 1981 meeting was the \"demand to allow for 'no surprise.'\"\n\nThe Soviet defector Oleg Gordievsky divulged a top-secret KGB telegram sent to the London KGB residency in February 1983. It stated: \"The objective of the assignment is to see that the Residency works systematically to uncover any plans in preparation by the main adversary [USA] for RYAN and to organize a continual watch to be kept for indications of a decision being taken to use nuclear weapons against the USSR or immediate preparations being made for a nuclear missile attack.\" An attachment listed seven \"immediate\" and thirteen \"prospective\" tasks for the agents to complete and report. These included: the collection of data on potential places of evacuation and shelter, an appraisal of the level of blood held in blood banks, observation of places where nuclear decisions were made and where nuclear weapons were stored, observation of key nuclear decision makers, observation of lines of communication, reconnaissance of the heads of churches and banks, and surveillance of security services and military installations.\n\nRYAN took on a new significance after the announcement of plans to deploy Pershing II nuclear-armed missiles to West Germany. These missiles were designed to be launched from road-mobile vehicles, making the launch sites very hard to find. The flight time from West Germany to European Russia was only four to six minutes (approximate flying time from six to eight minutes from West Germany to Moscow), giving the Soviets little or no warning.\n\nOn 23 March 1983 Ronald Reagan publicly announced development of the Strategic Defense Initiative. The Soviet government felt that the purpose of SDI technology was to render the US invulnerable to Soviet attack, thereby allowing the US to launch missiles against the USSR without fear of retaliation. This concern about a surprise attack prompted the sudden expansion of the RYAN program. The level of concern reached its peak after the Soviets shot down KAL 007 near Moneron Island on 1 September 1983, and during the North Atlantic Treaty Organisation exercise Able Archer 83. The Soviet Union believed that a United States first strike on the Soviet Union was imminent.\n\nAlthough Andropov died in February 1984, RYAN continued to be maintained and developed under the direction of Victor Chebrikov. Consultations held in August 1984 between the STASI's head of the Main Directorate of Reconnaissance, Markus Wolf and KGB experts discussed the early detection of potential war preparations in adversaries and indicated that the First Chief Directorate of the KGB was proposing to create a new division to deal exclusively with RYAN. 300 positions within the KGB were earmarked for RYAN of which 50 were reserved for the new division.\n\nOperation RYAN continued to be maintained until at least April 1989.\n\n\n"}
{"id": "18990381", "url": "https://en.wikipedia.org/wiki?curid=18990381", "title": "Renewable Energy Payments", "text": "Renewable Energy Payments\n\nRenewable Energy Payments are a competitive alternative to Renewable Energy Credits (REC's).\n\nAlthough the intent with both methods is the same, to stimulate growth in the alternative and renewable energy space, REP's have proven to offer benefits to local jobs, businesses and economies while making the growth fundable and lendable by financial institutions. \nRenewable Energy Payments are the mechanisms or instruments at the heart of specific state, provincial or national renewable energy policies. REPs are incentives for homeowners, farmers, businesses, etc., to become producers of renewable energy, or to increase their production of renewable energy. As such, they increase our overall production and use of renewable energy, and decrease our consumption and burning of fossil fuels.\n\nIn a broad stroke, Renewable Energy Payments, sometimes known as a Feed-in Tariff place obligations on utility companies to buy electricity from renewable energy sources, often small, local companies, for a fixed period of time. The underlying premise being that with fixed payments the once volatile renewable energy projects now become lendable and attractive for financing, thus stimulating growth and innovation.\nProponents of Renewable Energy Payments argue that this policy has proven to stimulate local economies, innovation and small business growth because in its truest form REP's put everyone, whether small business, individual, or farmers on an equal footing with large commercial titans of industry.\n\nRepresentative Jay Inslee of Washington says \"We can give homeowners, farmers and communities across America investment security that they can take to the bank. We know from experience in Germany, Spain and dozens of other countries around the world that this policy approach spurs unparalleled and affordable renewable-energy development.\" \n\nThe alternative to Renewable Energy Payments are what are called Renewable Energy Credits, which have been likened to the Alaskan Bridge to nowhere in a recent filing by the Florida Alliance for Renewable Energy. (FARE)\n\n1. Renewable Energy World\n2. Jay Inslee\n3. Florida Alliance for Renewable Energy\n3. FARE Filing\n\n\n"}
{"id": "294989", "url": "https://en.wikipedia.org/wiki?curid=294989", "title": "Spark gap", "text": "Spark gap\n\nA spark gap consists of an arrangement of two conducting electrodes separated by a gap usually filled with a gas such as air, designed to allow an electric spark to pass between the conductors. When the potential difference between the conductors exceeds the breakdown voltage of the gas within the gap, a spark forms, ionizing the gas and drastically reducing its electrical resistance. An electric current then flows until the path of ionized gas is broken or the current reduces below a minimum value called the \"holding current\". This usually happens when the voltage drops, but in some cases occurs when the heated gas rises, stretching out and then breaking the filament of ionized gas. Usually, the action of ionizing the gas is violent and disruptive, often leading to sound (ranging from a \"snap\" for a spark plug to thunder for a lightning discharge), light and heat.\n\nSpark gaps were used historically in early electrical equipment, such as spark gap radio transmitters, electrostatic machines, and X-ray machines. Their most widespread use today is in spark plugs to ignite the fuel in internal combustion engines, but they are also used in lightning arresters and other devices to protect electrical equipment from high-voltage transients.\n\nThe light emitted by a spark does not come from the current of electrons itself, but from the material medium fluorescing in response to collisions from the electrons. When electrons collide with molecules of air in the gap, they excite their orbital electrons to higher energy levels. When these excited electrons fall back to their original energy levels, they emit energy as light. It is impossible for a visible spark to form in a vacuum. Without intervening matter capable of electromagnetic transitions, the spark will be invisible (see vacuum arc).\n\nSpark gaps are essential to the functioning of a number of electronic devices.\n\nA spark plug uses a spark gap to initiate combustion. The heat of the ionization trail, but more importantly, UV radiation and hot free electrons (both cause the formation of reactive free radicals) ignite a fuel-air mixture inside an internal combustion engine, or a burner in a furnace, oven, or stove. The more UV radiation is produced and successfully spread into the combustion chamber, the further the combustion process proceeds.\n\nSpark gaps are frequently used to prevent voltage surges from damaging equipment. Spark gaps are used in high-voltage switches, large power transformers, in power plants and electrical substations. Such switches are constructed with a large, remote-operated switching blade with a hinge as one contact and two leaf springs holding the other end as second contact. If the blade is opened, a spark may keep the connection between blade and spring conducting. The spark ionizes the air, which becomes conductive and allows an arc to form, which sustains ionization and hence conduction. A Jacob's ladder on top of the switch will cause the arc to rise and eventually extinguish. One might also find small Jacob's ladders mounted on top of ceramic insulators of high-voltage pylons. These are sometimes called horn gaps. If a spark should ever manage to jump over the insulator and give rise to an arc, it will be extinguished.\n\nSmaller spark gaps are often used to protect sensitive electrical or electronic equipment from high-voltage surges. In sophisticated versions of these devices (called gas tube arresters), a small spark gap breaks down during an abnormal voltage surge, safely shunting the surge to ground and thereby protecting the equipment. These devices are commonly used for telephone lines as they enter a building; the spark gaps help protect the building and internal telephone circuits from the effects of lightning strikes. Less sophisticated (and much less expensive) spark gaps are made using modified ceramic capacitors; in these devices, the spark gap is simply an air gap sawn between the two lead wires that connect the capacitor to the circuit. A voltage surge causes a spark that jumps from lead wire to lead wire across the gap left by the sawing process. These low-cost devices are often used to prevent damaging arcs between the elements of the electron gun(s) within a cathode ray tube (CRT).\n\nSmall spark gaps are very common in telephone switchboards, as the long phone cables are very susceptible to induced surges from lightning strikes. Larger spark gaps are used to protect power lines.\n\nSpark gaps are commonly implemented on Printed Circuit Boards in mains power electronics products using two closely spaced exposed PCB traces. This is an effectively zero cost method of adding crude overload protection to electronics products.\n\nTransils and trisils are the solid-state alternatives to spark gaps for lower-power applications. Neon bulbs are also used for this purpose.\n\nA triggered spark gap in an air-gap flash is used to produce photographic light flashes in the sub-microsecond domain.\n\nA spark radiates energy throughout the electromagnetic spectrum. Nowadays, this is usually regarded as illegal radio frequency interference and is suppressed, but in the early days of radio communications (1880–1920), this was the means by which radio signals were transmitted, in the unmodulated spark-gap transmitter. Many radio spark gaps include cooling devices, such as the rotary gap and heat sinks, since the spark gap becomes quite hot under continuous use at high power.\n\nA calibrated spherical spark gap will break down at a highly repeatable voltage, when corrected for air pressure, humidity and temperature. A gap between two spheres can provide a voltage measurement without any electronics or voltage dividers, to an accuracy of about 3%. A spark gap can be used to measure high voltage AC, DC, or pulses, but for very short pulses, an ultraviolet light source or radioactive source may be put on one of the terminals to provide a source of electrons.\n\nSpecial purpose, high-energy triggerable spark gaps can be used to rapidly switch high voltages and very high currents for certain pulsed power applications, such as pulsed lasers, railguns, fusion, ultrastrong pulsed magnetic field research, and in the triggering of nuclear bombs. Commercially available devices can be divided into two classes: positive pressure and triggered vacuum gaps. Positive pressure triggered gaps have a limited operating voltage range (for instance, from 1/3 to 2/3 of the self breakdown voltage). Triggered vacuum gaps offer a wide operating voltage range (400 V to 90 kV is achievable). Both classes can switch higher energy levels than any thyristor, thyratron, krytron, or sprytron. Triggered gaps are popular for single-shot and low-repetition-rate applications. One such switch is known as a trigatron. The Ignitron and Crossatron could be considered triggered gaps. The latter is unique in that it can be turned back off by the control electrode after conduction begins. The xenon flash tube is another common triggered gap. Various schemes have also been devised to trigger open air gaps on command. A set of spark gaps are a key element of a Marx generator, used to generate high-voltage impulses; the spark gaps allow a chain of capacitors to be slowly charged in parallel and then rapidly discharged in series.\n\nA Jacob's ladder (more formally, a high voltage traveling arc) is a device for producing a continuous train of large sparks that rise upwards. The spark gap is formed by two wires, approximately vertical but gradually diverging from each other towards the top in a narrow \"V\" shape. It was named for the \"ladder to heaven\" described in the Bible.\n\nWhen high voltage is applied to the gap, a spark forms across the bottom of the wires where they are nearest each other, rapidly changing to an electric arc. Air breaks down at about 30 kV/cm, depending on humidity, temperature, etc. Apart from the anode and cathode voltage drops, the arc behaves almost as a short circuit, drawing as much current as the electrical power supply can deliver, and the heavy load dramatically reduces the voltage across the gap.\n\nThe heated ionized air rises, carrying the current path with it. As the trail of ionization gets longer, it becomes more and more unstable, finally breaking. The voltage across the electrodes then rises and the spark re-forms at the bottom of the device.\n\nThis cycle leads to an exotic-looking display of electric white, yellow, blue or purple arcs, which is often seen in films about mad scientists. The device was a staple in schools and science fairs of the 1950s and 1960s, typically constructed out of a Model T spark coil or any other source of high voltage in the 10,000–30,000-volt range, such as a neon sign transformer (5–15 kV) or a television picture tube circuit (flyback transformer) (10–28 kV), and two coat hangers or rods built into a \"V\" shape. For larger ladders, microwave oven transformers connected in series, voltage multipliers and utility pole transformers (pole pigs) run in reverse (step-up) are commonly used.\n\nExposure to an arc-producing device can pose health hazards. An arc formed in air will ionize oxygen and nitrogen, which then can re-form into reactive molecules such as ozone and nitric oxide. These products can be damaging to the mucous membranes. Plants are also susceptible to ozone poisoning. These hazards are greatest when the arc is continuous and in an enclosed space such as a room. An arc that occurs outside is less of a hazard because the heated ionized gases will rise up into the air and dissipate into the atmosphere. Spark gaps which only intermittently produce short spark bursts are also minimally hazardous because the volume of ions generated is very small.\n\nArcs can also produce a broad spectrum of wavelengths spanning the visible light and the invisible ultraviolet and infrared spectrum. Very intense arcs generated by means such as arc welding can produce significant amounts of ultraviolet which is damaging to the retina of the observer. These arcs should only be observed through special dark filters which reduce the arc intensity and shield the observer's eyes from the ultraviolet rays.\n\n\n"}
{"id": "2845520", "url": "https://en.wikipedia.org/wiki?curid=2845520", "title": "Square planar molecular geometry", "text": "Square planar molecular geometry\n\nThe square planar molecular geometry in chemistry describes the stereochemistry (spatial arrangement of atoms) that is adopted by certain chemical compounds. As the name suggests, molecules of this geometry have their atoms positioned at the corners of a square on the same plane about a central atom.\n\nSquare-planar coordination geometry violates the points-on-sphere geometries observed from most compounds (i.e. linear, trigonal, tetrahedral, trigonal bipyramidal, ...). The isolation of both \"trans\"- and \"cis\"-PtCl(NH) led Alfred Werner to propose square planar molecular geometry.\n\nThe addition of two ligands to linear compounds, ML, can afford square planar complexes. For example, XeF adds fluorine to give square planar XeF.\n\nIn principle, square planar geometry can be achieved by flattening a tetrahedron. As such, the interconversion of tetrahedral and square planar geometries provides an intramolecular pathway for the isomerization of tetrahedral compounds. This pathway does not operate readily for hydrocarbons, but tetrahedral nickel(II) complexes, \"e.g.\" NiBr(PPh), undergo this change reversibly.\n\nSquare planar geometry can also be achieved by the removal of a pair of ligands from the \"z\"-axis of an octahedron, leaving four ligands in the \"xy\" plane. For transition metal compounds, the crystal field splitting diagram for square planar geometry can thus be derived from the octahedral diagram. The removal of the two ligands stabilizes the d level, leaving the d level as the most destabilized. Consequently, the d remains unoccupied in complexes of metals with the d configuration. These compounds typically have 16 valence electrons (eight from ligands, eight from the metal).\n\nNumerous compounds adopt this geometry, examples being especially numerous for transition metal complexes. The noble gas compound XeF adopts this structure as predicted by VSEPR theory. The geometry is prevalent for transition metal complexes with d configuration, which includes Rh(I), Ir(I), Pd(II), Pt(II), and Au(III). Notable examples include the anticancer drugs cisplatin [PtCl(NH)] and carboplatin. Many homogeneous catalysts are square planar in their resting state, such as Wilkinson's catalyst and Crabtree's catalyst. Other examples include Vaska's complex and Zeise's salt. Certain ligands (such as porphyrins) stabilize this geometry.\n\nA general d-orbital splitting diagram for square planar (D) transition metal complexes can be derived from the general octahedral (O) splitting diagram, in which the d and the d orbitals are degenerate and higher in energy than the degenerate set of d, d and d orbitals. When the two axial ligands are removed to generate a square planar geometry, the d orbital is driven lower in energy as electron-electron repulsion with ligands on the z-axis is no longer present. However, for purely σ-donating ligands the d orbital is still higher in energy than the d, d and d orbitals because of the torus shaped lope of the d orbital. It bears electron density on the x- and y-axes and therefore interacts with the filled ligand orbitals. The d, d and d orbitals are generally presented as degenerate but they have to split into two different energy levels with respect to the irreducible representations of the point group D. Their relative ordering depends on the nature of the particular complex. Furthermore, the splitting of d-orbitals is perturbed by π-donating ligands in contrast to octahedral complexes. In the square planar case strongly π-donating ligands can cause the d and d orbitals to be higher in energy than the d\"\" orbital, whereas in the octahedral case π-donating ligands only affect the magnitude of the d-orbital splitting and the relative ordering of the orbitals is conserved.\n\n\n"}
{"id": "8841318", "url": "https://en.wikipedia.org/wiki?curid=8841318", "title": "Sylvinite", "text": "Sylvinite\n\nSylvinite is a sedimentary rock made of a mechanical mixture of the minerals sylvite (KCl, or potassium chloride) and halite (NaCl, or sodium chloride). Sylvinite is the most important source for the production of potash in North America, Russia and the UK. Most Canadian operations mine sylvinite with proportions of about 31% KCl and 66% NaCl with the balance being insoluble clays, anhydrite and in some locations carnallite. Other deposits of sylvinite are in Belarus, Brazil, France, Germany, Kazakhstan, Slovakia and Spain.\n"}
{"id": "50899162", "url": "https://en.wikipedia.org/wiki?curid=50899162", "title": "Tornadoes of 1967", "text": "Tornadoes of 1967\n\nThis page documents the tornadoes and tornado outbreaks of 1967, primarily in the United States. Most tornadoes form in the U.S., although some events may take place internationally. Tornado statistics for older years like this often appear significantly lower than modern years due to fewer reports or confirmed tornadoes.\n"}
{"id": "14772999", "url": "https://en.wikipedia.org/wiki?curid=14772999", "title": "UK Emissions Trading Scheme", "text": "UK Emissions Trading Scheme\n\nThe UK Emissions Trading Scheme was a voluntary emissions trading system created as a pilot prior to the mandatory European Union Emissions Trading Scheme which it now runs in parallel with. It ran from 2002 and it closed to new entrants in 2009. Management of the scheme transferred to the Department of Energy and Climate Change in 2008.\n\nAt the time, the scheme was a novel economic approach, being the first multi-industry carbon trading system in the world. (Denmark ran a pilot greenhouse gas trading scheme between 2001 and 2003 but this only involved eight electricity companies). It took note of the emerging international consensus on the benefits of carbon trading that were being proposed in the mandatory Kyoto Protocol, which had not been ratified at that time, and allowed government and corporate early movers and to gain experience in the auction process and the trading system that the later schemes have entailed. \nIt ran in parallel to a tax on energy use, the Climate Change Levy, introduced in April 2001, but companies could get a discount on the tax if they elected to make reductions through participation in the trading scheme.\n\nThe voluntary trading scheme recruited 34 participants from UK industries and organisations who promised to make reductions in their carbon emissions, this has since expanded to 54 sectors of the UK economy. In return they received a share of a £215 million \"incentive fund\" from the Department for Environment, Food and Rural Affairs (DEFRA). Each agreed to hold sufficient allowances to cover its actual emissions for that year, and participate in a cap and trade system, with an annually reducing cap. Each participant could then decide to take action to manage its emissions to exactly meet its target, or reduce its actual emissions below its target (thereby releasing allowances that it could sell on, or save for use in future years), or buy allowances from other participants to cover any excess.\n\nFrom March 2002, DEFRA ran an auction of emission allowances to perform allocations to participants, after the start of the mandatory EU scheme.\n\nThe UK's National Audit Office and DEFRA's consultants ran reviews of the system in order to establish its basis and drew lessons from it. \n\nThey concluded that the scheme did achieve some emission reductions from the participants, although more could have been achieved had targets been more demanding. \n"}
{"id": "5212064", "url": "https://en.wikipedia.org/wiki?curid=5212064", "title": "Vacuum coffee maker", "text": "Vacuum coffee maker\n\nA vacuum coffee maker brews coffee using two chambers where vapor pressure and vacuum produce coffee. This type of coffee maker is also known as \"vac pot\", \"siphon\" or \"syphon coffee maker,\" and was invented by Loeff of Berlin in the 1830s. These devices have since been used for more than a century in many parts of the world. Design and composition of the vacuum coffee maker varies. The chamber material is borosilicate glass, metal, or plastic, and the filter can be either a glass rod or a screen made of metal, cloth, paper, or nylon. The Napier Vacuum Machine, presented in 1840, was an early example of this technique. While vacuum coffee makers generally were excessively complex for everyday use, they were prized for producing a clear brew, and were quite popular until the middle of the twentieth century. The Bauhaus interpretation of this device can be seen in Gerhard Marcks' Sintrax coffee maker of 1925.\n\nA vacuum coffee maker operates as a siphon, where heating and cooling the lower vessel changes the vapor pressure of water in the lower, first pushing the water up into the upper vessel, then allowing the water to fall back down into the lower vessel. Concretely, the principle of a vacuum coffee maker is to heat water in the lower vessel of the brewer until expansion forces the contents through a narrow tube into an upper vessel containing coffee grounds (as water temperature increases, dense liquid water increasingly converts to less dense water vapor gas, which takes up more space and thus increases pressure); when the water reaches and exceeds the boiling point (so the vapor pressure equals and then exceeds atmospheric pressure), the (water vapor) pressure in the lower vessel exceeds the (atmospheric) pressure in the top vessel and water is pushed up the siphon tube into the upper vessel. During brewing, a small amount of water and sufficient water vapor remain in the lower vessel and are kept hot enough so the pressure will support the column of water in the siphon. When enough time has elapsed that the coffee has finished brewing, the heat is removed and the pressure in the bottom vessel drops, so the force of gravity (acting on the water) and atmospheric pressure (pressing on the liquid in the upper vessel) push the water down into the lower vessel, through a strainer and away from the grounds, ending brewing. The coffee can then be decanted from the lower chamber; the device must usually be taken apart to pour out the coffee.\n\nThe iconic Moka pot coffee maker functions on the same principle but the water is forced up from the bottom chamber through a third middle chamber containing the coffee grounds to the top chamber which has an air gap to prevent the brewed coffee from returning downwards. The prepared coffee is then poured off from the top.\n\nNote that siphons work by \"pushing\" (the water is under pressure – see hydrostatic pressure, not under tension), and it is the changing vapor pressure in the lower vessel, combined with the constant atmospheric pressure in the upper vessel that drive the siphon. When the water cools the pressure in the lower vessel drops as steam condenses into dense water, taking up less volume and hence dropping the pressure. This creates a partial vacuum, causing the atmospheric pressure outside the container (along with gravity) to force the liquid back into the lower vessel.\n\nAn early variation of this principle is called a \"balance siphon\". This implementation has the two chambers arranged side by side on a balance-like device, with a counterweight attached to the heated chamber. Once the vapor has forced the hot water out, the counterweight activates a spring-loaded snuffer which smothers the flame and allows the initial chamber to cool down thus lowering pressure (creating a vacuum) and causing the brewed coffee to seep in.\n\n\n"}
{"id": "14177102", "url": "https://en.wikipedia.org/wiki?curid=14177102", "title": "Yosepha Alomang", "text": "Yosepha Alomang\n\nYosepha Alomang (Mama Yosepha) is from the Indonesian province of Papua, one of the most biologically diverse places on the planet. \n\nShe was awarded the Goldman Environmental Prize in 2001, for her efforts on organizing her community to resist the mining company Freeport-McMoRan's mining practices over three decades that have destroyed rainforests, polluted rivers, and displaced communities.\n"}
