{"id": "45186651", "url": "https://en.wikipedia.org/wiki?curid=45186651", "title": "A Narmada Diary", "text": "A Narmada Diary\n\nA Narmada Diary is a 1995 documentary on the struggle of those adversely impacted by the Sardar Sarovar Dam project. It was directed by Anand Patwardhan and Simantini Dhuru and released in the year 1995. A record from about 1990-1993, of the measures adopted and hardship faced by the Narmada Bachao Andolan movement and the people inhabiting the place affected was presented. The documentary won the \"Filmfare Award for Best Documentary, 1996\" and the \"Grand Prize\" at Earth-Vision Film Festival, Tokyo, 1996. In spite of winning the central government's national awards, the movie was not allowed to be shown on National TV\n\n"}
{"id": "38390513", "url": "https://en.wikipedia.org/wiki?curid=38390513", "title": "Acoustoelastic effect", "text": "Acoustoelastic effect\n\nThe acoustoelastic effect is how the sound velocities (both longitudinal and shear wave velocities) of an elastic material change if subjected to an initial static stress field. This is a non-linear effect of the constitutive relation between mechanical stress and finite strain in a material of continuous mass. In classical linear elasticity theory small deformations of most elastic materials can be described by a linear relation between the applied stress and the resulting strain. This relationship is commonly known as the generalised Hooke's law. The linear elastic theory involves second order elastic constants (e.g. formula_1 and formula_2) and yields constant longitudinal and shear sound velocities in an elastic material, not affected by an applied stress. The acoustoelastic effect on the other hand include higher order expansion of the constitutive relation (non-linear elasticity theory) between the applied stress and resulting strain, which yields longitudinal and shear sound velocities dependent of the stress state of the material. In the limit of an unstressed material the sound velocities of the linear elastic theory are reproduced.\n\nThe acoustoelastic effect was investigated as early as 1925 by Brillouin. He found that the propagation velocity of acoustic waves would decrease proportional to an applied hydrostatic pressure. However, a consequence of his theory was that sound waves would stop propagating at a sufficiently large pressure. This paradoxial effect was later shown to be caused by the incorrect assumptions that the elastic parameters were not affected by the pressure.\nIn 1937 Murnaghan presented a mathematical theory extending the linear elastic theory to also include finite deformation in elastic isotropic materials. This theory included three third-order elastic constants formula_3, formula_4, and formula_5. In 1953 Huges and Kelly used the theory of Murnaghan in their experimental work to establish numerical values for higher order elastic constants for several elastic materials including Polystyrene, Armco iron, and Pyrex, subjected to hydrostatic pressure and uniaxial compression.\n\nThe acoustoelastic effect is an effect of finite deformation of non-linear elastic materials. A modern comprehensive account of this can be found in. This book treats the application of the non-linear elasticity theory and the analysis of the mechanical properties of solid materials capable of large elastic deformations. The special case of the acoustoelastic theory for a compressible isotropic hyperelastic material, like polycrystalline steel, is reproduced and shown in this text from the non-linear elasticity theory as presented by Ogden.\n\nA hyperelastic material is a special case of a Cauchy elastic material in which the stress at any point is objective and determined only by the current state of deformation with respect to an arbitrary reference configuration (for more details on deformation see also the pages Deformation (mechanics) and Finite strain). However, the work done by the stresses may depend on the path the deformation takes. Therefore, a Cauchy elastic material has a non-conservative structure, and the stress cannot be derived from a scalar elastic potential function. The special case of Cauchy elastic materials where the work done by the stresses is independent of the path of deformation is referred to as a Green elastic or hyperelastic material. Such materials are conservative and the stresses in the material can be derived by a scalar elastic potential, more commonly known as the Strain energy density function.\n\nThe constitutive relation between the stress and strain can be expressed in different forms based on the chosen stress and strain forms. Selecting the 1st Piola-Kirchhoff stress tensor formula_6 (which is the transpose of the nominal stress tensor formula_7), the constitutive equation for a compressible hyper elastic material can be expressed in terms of the Lagrangian Green strain (formula_8) as:\nwhere formula_10 is the deformation gradient tensor, and where the second expression uses the Einstein summation convention for index notation of tensors. formula_11 is the strain energy density function for a hyperelastic material and have been defined per unit volume rather than per unit mass since this avoids the need of multiplying the right hand side with the mass density formula_12 of the reference configuration.\n\nAssuming that the scalar strain energy density function formula_13 can be approximated by a Taylor series expansion in the current strain formula_8, it can be expressed (in index notation) as:\nImposing the restrictions that the strain energy function should be zero and have a minimum when the material is in the un-deformed state (i.e. formula_16) it is clear that there are no constant or linear term in the strain energy function, and thus:\nwhere formula_18 is a fourth-order tensor of second-order elastic moduli, while formula_19 is a sixth-order tensor of third-order elastic moduli.\nThe symmetry of formula_20 together with the scalar strain energy density function formula_11 implies that the second order moduli formula_18 have the following symmetry:\nwhich reduce the number of independent elastic constants from 81 to 36. In addition the power expansion implies that the second order moduli also have the major symmetry\nwhich further reduce the number of independent elastic constants to 21. The same arguments can be used for the third order elastic moduli formula_19. These symmetries also allows the elastic moduli to be expressed by the Voigt notation (i.e. formula_26 and formula_27).\n\nThe deformation gradient tensor can be expressed in component form as\nwhere formula_29 is the displacement of a material point formula_30 from coordinate formula_31 in the reference configuration to coordinate formula_32 in the deformed configuration (see Figure 2 in the finite strain theory page). Including the power expansion of strain energy function in the constitutive relation and replacing the Lagrangian strain tensor formula_33 with the expansion given on the finite strain tensor page yields (note that lower case formula_34 have been used in this section compared to the upper case on the finite strain page) the constitutive equation\nwhere\nand higher order terms have been neglected\n(see for detailed derivations).\nFor referenceM by neglecting higher order terms in formula_37 this expression reduce to\nformula_38\nwhich is a version of the generalised Hooke's law where formula_39 is a measure of stress while formula_37 is a measure of strain, and formula_18 is the linear relation between them.\n\nAssuming that a small dynamic (acoustic) deformation disturb an already statically stressed material the acoustoelastic effect can be regarded as the effect on a small deformation superposed on a larger finite deformation (also called the small-on-large theory). Let us define three states of a given material point. In the reference (un-stressed) state the point is defined by the coordinate vector formula_42 while the same point has the coordinate vector formula_43 in the static initially stressed state (i.e. under the influence of an applied pre-stress). Finally, assume that the material point under a small dynamic disturbance (acoustic stress field) have the coordinate vector formula_44. The total displacement of the material points (under influence of both a static pre-stress and an dynamic acoustic disturbance) can then be described by the displacement vectors\nwhere\ndescribes the static (Lagrangian) initial displacement due to the applied pre-stress, and the (Eulerian) displacement due to the acoustic disturbance, respectively. \nCauchy's first law of motion (or balance of linear momentum) for the additional Eulerian disturbance formula_47 can then be derived in terms of the intermediate Lagrangian deformation formula_48 assuming that the small-on-large assumption\nholds.\nUsing the Lagrangian form of Cauchy's first law of motion, where the effect of a constant body force (i.e. gravity) has been neglected, yields\n\nThe right hand side (the time dependent part) of the law of motion can be expressed as\nunder the assumption that both the unstressed state and the initial deformation state are static and thus formula_55.\n\nFor the left hand side (the space dependent part) the spatial Lagrangian partial derivatives with respect to formula_56 can be expanded in the Eulerian formula_57 by using the chain rule and changing the variables through the relation between the displacement vectors as \nwhere the short form formula_59 has been used. Thus\nAssuming further that the static initial deformation formula_48 (the pre-stressed state) is in equilibrium means that formula_62, and the law of motion can in combination with the constitutive equation given above be reduced to a linear relation (i.e. where higher order terms in formula_63) between the static initial deformation formula_48 and the additional dynamic disturbance formula_65 as (see for detailed derivations)\nwhere \nThis expression is recognised as the linear wave equation. Considering a plane wave of the form\nwhere formula_69 is a Lagrangian unit vector in the direction of propagation (i.e. parallel to the wave number formula_70normal to the wave front, ), formula_71 is a unit vector referred to as the polarization vector (describing the direction of particle motion), formula_72 is the phase wave speed, and formula_73 is a twice continuously differentiable function (e.g. a sinusodial function). Inserting this plane wave in to the linear wave equation derived above yields\n\nwhere formula_75 is introduced as the acoustic tensor, and depends on formula_69 as\nThis expression is called the propagation condition and determines for a given propagation direction formula_78 the velocity and polarization of possible waves corresponding to plane waves. The wave velocities can be determined by the characteristic equation\nwhere formula_80 is the determinant and formula_81 is the identity matrix.\n\nFor a hyperelastic material formula_75 is symmetric (but not in general), and the eigenvalues (formula_83) are thus real. For the wave velocities to also be real the eigenvalues need to be positive. If this is the case, three mutually orthogonal real plane waves exist for the given propagation direction formula_69. From the two expressions of the acoustic tensor it is clear that\nand the inequality formula_86 (also called the strong ellipticity condition) for all non-zero vectors formula_69 and formula_71 guarantee that the velocity of homogeneous plane waves are real. The polarization formula_89 corresponds to a longitudinal wave where the particle motion is parallel to the propagation direction (also referred to as a compressional wave). The two polarizations where formula_90 corresponds to transverse waves where the particle motion is orthogonal to the propagation direction (also referred to as shear waves).\n\nFor a second order isotropic tensor (i.e. a tensor having the same components in any coordinate system) like the Lagrangian strain tensor formula_8 have the invariants formula_92 where formula_93 is the trace operator, and formula_94. The strain energy function of an isotropic material can thus be expressed by formula_95, or a superposition there of, which can be rewritten as\nwhere formula_97 are constants. The constants formula_1 and formula_2 are the second order elastic moduli better known as the Lamé parameters, while formula_100 and formula_101 are the third order elastic moduli introduced by, which are alternative but equivalent to formula_102 and formula_5 introduced by Murnaghan.\nCombining this with the general expression for the strain energy function it is clear that\nwhere formula_105. Historically different selection of these third order elastic constants have been used, and some of the variations is shown in Table 1.\n\nTable 2 and 3 present the second and third order elastic constants for some steel types presented in literature\n\nA cuboidal sample of a compressible solid in an unstressed reference configuration can be expressed by the Cartesian coordinates formula_106, where the geometry is aligned with the Lagrangian coordinate system, and formula_107 is the length of the sides of the cuboid in the reference configuration. Subjecting the cuboid to a uniaxial tension in the formula_108-direction so that it deforms with a pure homogeneous strain such that the coordinates of the material points in the deformed configuration can be expressed by formula_109, which gives the\nelongations\nin the formula_32-direction. Here formula_112 signifies the current (deformed) length of the cuboid side formula_113 and where the ratio between the length of the sides in the current and reference configuration are denoted by\ncalled the principal stretches. For an isotropic material this corresponds to a deformation without any rotation (See polar decomposition of the deformation gradient tensor where formula_115 and the rotation formula_116). This can be described through spectral representation by the principal stretches formula_117 as eigenvalues, or equivalently by the elongations formula_118.\n\nFor a uniaxial tension in the formula_108-direction (formula_120 we assume that the formula_121 increase by some amount. If the lateral faces are free of traction (i.e. formula_122) the lateral elongations formula_123 and formula_124 are limited to the range formula_125. For isotropic symmetry the lateral elongations (or contractions) must also be equal (i.e. formula_126). The range corresponds to the range from total lateral contraction (formula_127, which is non-physical), and to no change in the lateral dimensions (formula_128). It is noted that theoretically the range could be expanded to values large than 0 corresponding to an increase in lateral dimensions as a result of increase in axial dimension. However, very few materials (called auxetic materials) exhibit this property.\n\nIf the strong ellipticity condition (formula_129) holds, three orthogonally polarization directions (formula_71 will give a non-zero and real sound velocity for a given propagation direction formula_69. The following will derive the sound velocities for óne selection of applied uniaxial tension, propagation direction, and an orthonormal set of polarization vectors. For a uniaxial tension applied in the formula_108-direction, and deriving the sound velocities for waves propagating orthogonally to the applied tension (e.g. in the formula_133-direction with propagation vector formula_134), one selection of orthonormal polarizations may be\nwhich gives the three sound velocities\nwhere the first index formula_113 of the sound velocities formula_138 indicate the propagation direction (here the formula_133-direction, while the second index formula_140 indicate the selected polarization direction (formula_141 corresponds to particle motion in the propagation direction formula_113 – i.e. longitudinal wave, and formula_143 corresponds to particle motion perpendicular to the propagation direction – i.e. shear wave).\n\nExpanding the relevant coefficients of the acoustic tensor, and substituting the second- and third-order elastic moduli formula_18 and formula_19 with their isotropic equivalents, formula_146 and formula_147 respectively, leads to the sound velocities expressed as\nwhere\nare the acoustoelastic coefficients related to effects from third order elastic constants.\n\nTo be able to measure the sound velocity, and more specifically the change in sound velocity, in a material subjected to some stress state, one can measure the velocity of an acoustic signal propagating through the material in question. There are several methods to do this but all of them use one of two physical relations of the sound velocity. The first relation is related to the time it takes a signal to propagate from one point to another (typically the distance between two acoustic transducers or two times the distance from one transducer to a reflective surface). This is often referred to as \"Time-of-flight\" (TOF) measurements, and use the relation\nwhere formula_153 is the distance the signal travels and formula_51 is the time it takes to travel this distance. The second relation is related to the inverse of the time, the frequency, of the signal. The relation here is\nwhere formula_73 is the frequency of the signal and formula_1 is the wave length. The measurements using the frequency as measurand use the pheonomenon of acoustic resonance where formula_5 number of wave lengths match the length over which the signal resonate. Both these methods are dependent on the distance over which it measure, either directly as in the Time-of-flight, or indirectly through the matching number of wavelengths over the physical extent of the specimen which resonate.\n\nIn general there are two ways to set up a transducer system to measure the sound velocity in a solid. One is a setup with two or more transducers where one is acting as a transmitter, while the other(s) is acting as a receiver. The sound velocity measurement can then be done by measuring the time between a signal is generated at the transmitter and when it is recorded at the receiver while assuming to know (or measure) the distance the acoustic signal have traveled between the transducers, or conversely to measure the resonance frequency knowing the thickness over which the wave resonate. The other type of setup is often called a \"pulse-echo\" system. Here one transducer is placed in the vicinity of the specimen acting both as transmitter and receiver. This requires a reflective interface where the generated signal can be reflected back toward the transducer which then act as a receiver recording the reflected signal. See ultrasonic testing for some measurement systems.\n\nAs explained above, a set of three orthonormal polarizations (formula_71) of the particle motion exist for a given propagation direction formula_69 in a solid. For measurement setups where the transducers can be fixated directly to the sample under investigation it is possible to create these three polarizations (one longitudinal, and two orthogonal transverse waves) by applying different types of transducers exciting the desired polarization (e.g. piezoelectric transducers with the needed oscillation mode). Thus it is possible to measure the sound velocity of waves with all three polarizations through either time dependent or frequency dependent measurement setups depending on the selection of transducer types. However, if the transducer can not be fixated to the test specimen a coupling medium is needed to transmit the acoustic energy from the transducer to the specimen. Water or gels are often used as this coupling medium. For measurement of the longitudinal sound velocity this is sufficient, however fluids do not carry shear waves, and thus to be able to generate and measure the velocity of shear waves in the test specimen the incident longitudinal wave must interact at an oblique angle at the fluid/solid surface to generate shear waves through mode conversion. Such shear waves are then converted back to longitudinal waves at the solid/fluid surface propagating back through the fluid to the recording transducer enabling the measurement of shear wave velocities as well through a coupling medium.\n\nAs the industry strives to reduce maintenance and repair costs, non-destructive testing of structures becomes increasingly valued both in production control and as a means to measure the utilization and condition of key infrastructure. There are several measurement techniques to measure stress in a material. However, techniques using optical measurements, magnetic measurements, X-ray diffraction, and neutron diffraction are all limited to measuring surface or near surface stress or strains. Acoustic waves propagate with ease through materials and provide thus a means to probe the interior of structures, where the stress and strain level is important for the overall structural integrity.\nSince the sound velocity of such non-linear elastic materials (including common construction materials like aluminium and steel) have a stress dependency, one application of the acoustoelastic effect may be measurement of the stress state in the interior of a loaded material utilizing different acoustic probes (e.g. ultrasonic testing) to measure the change in sound velocities.\n\nseismology study the propagation of elastic waves through the Earth and is used in e.g. earthquake studies and in mapping the Earth's interior. The interior of the Earth is subjected to different pressures, and thus the acoustic signals may pass through media in different stress states. The acoustoelastic theory may thus be of practical interest where nonlinear wave behaviour may be used to estimate geophysical properties.\n\nOther applications may be in medical sonography and elastography measuring the stress or pressure level in relevant elastic tissue types \n(e.g. \n\n"}
{"id": "2826835", "url": "https://en.wikipedia.org/wiki?curid=2826835", "title": "Altar Stone (Stonehenge)", "text": "Altar Stone (Stonehenge)\n\nThe Altar Stone is a recumbent central megalith at Stonehenge in England, dating to Stonehenge phase 3i, around 2600 BC. It is made of a purplish-green micaceous sandstone and is thought to have originated from outcrops of the Senni Beds formation of the Old Red Sandstone in Wales, though this is currently in debate. Stone 80 (Altar Stone) was most recently excavated in the 1950s, but no written records of the excavation survive, and there are no samples available for examination that are established as having come from the monolith. Stone 55 (a sarsen megalith) lies on top of Stone 80 perpendicularly, and is thought to have fallen across it. The Altar Stone weighs approximately six tons and (if it ever was upright) would have stood nearly two metres tall. Some believe that it always was recumbent It is sometimes classed as a bluestone, because it does not have a local provenance. \n\nIts name probably comes from a comment by Inigo Jones who wrote: \"...whether it might be an Altar or no I leave to the judgment of others’.\n"}
{"id": "41384413", "url": "https://en.wikipedia.org/wiki?curid=41384413", "title": "Aquatic-terrestrial subsidies", "text": "Aquatic-terrestrial subsidies\n\nEnergy and nutrients derived from aquatic ecosystems and transferred to terrestrial ecosystems are termed aquatic-terrestrial subsidies or, more simply, aquatic subsidies. The most common examples of aquatic subsidies involve organisms that move across habitat boundaries and deposit their nutrients as they decompose in terrestrial habitats or are consumed by terrestrial predators, such as spiders, lizards, birds, and bats. This phenomenon is exemplified by aquatic insects that develop within streams and lakes before emerging as winged adults and moving to terrestrial habitats. Fish removed from aquatic ecosystems by terrestrial predators are another important example. Conversely, the flow of energy and nutrients from terrestrial ecosystems to aquatic ecosystems are considered terrestrial subsidies; both aquatic subsidies and terrestrial subsidies are types of cross-boundary subsidies. \n\nWhile the magnitude of aquatic subsidies to terrestrial ecosystems is low compared to those moving in the reverse direction (from terrestrial to aquatic habitats), aquatic subsidies are generally of higher nutritional quality because they come from animal, rather than plant-based or detrital, sources. In addition to their nutritional value, however, aquatic subsidies are increasingly recognized as important sources of environmental contaminants to terrestrial food webs. In this way, aquatic animals can accumulate pollutants in their tissues and exoskeletons (such as metals and polychlorinated biphenyls) and move them to riparian and terrestrial systems as they emerge or are consumed by terrestrial predators.\n\n"}
{"id": "51274994", "url": "https://en.wikipedia.org/wiki?curid=51274994", "title": "Augusta-Priolo", "text": "Augusta-Priolo\n\nThe petrochemical complex of Augusta-Priolo (called \"Polo petrolchimico siracusano\" in Italian) is a vast industrialized coastal area in eastern Sicily including the territory of the municipalities of Augusta, Priolo Gargallo and Melilli. Main industrial activities are oil refining, processing of oil derivatives and energy production. The complex, whose beginnings date back to 1949, has produced significant environmental problems for the coastline and the entire area, which accordingly is sometimes dubbed \"Trinacria nera\" (black Sicily or black triangle) \n\nAmong the companies in the region are ExxonMobil, Sasol, ERG, Polimeri Europa and Syndial.\n\n"}
{"id": "435397", "url": "https://en.wikipedia.org/wiki?curid=435397", "title": "Automatic meter reading", "text": "Automatic meter reading\n\nAutomatic meter reading (AMR) is the technology of automatically collecting consumption, diagnostic, and status data from water meter or energy metering devices (gas, electric) and transferring that data to a central database for billing, troubleshooting, and analyzing.\nThis technology mainly saves utility providers the expense of periodic trips to each physical location to read a meter. Another advantage is that billing can be based on near real-time consumption rather than on estimates based on past or predicted consumption. This timely information coupled with analysis can help both utility providers and customers better control the use and production of electric energy, gas usage, or water consumption.\n\nAMR technologies include handheld, mobile and network technologies based on telephony platforms (wired and wireless), radio frequency (RF), or powerline transmission.\n\nWith touch-based AMR, a meter reader carries a handheld computer or data collection device with a wand or probe. The device automatically collects the readings from a meter by touching or placing the read probe in close proximity to a reading coil enclosed in the touchpad. When a button is pressed, the probe sends an interrogate signal to the touch module to collect the meter reading. The software in the device matches the serial number to one in the route database, and saves the meter reading for later download to a billing or data collection computer. Since the meter reader still has to go to the site of the meter, this is sometimes referred to as \"on-site\" AMR. Another form of contact reader uses a standardized infrared port to transmit data. Protocols are standardized between manufacturers by such documents as ANSI C12.18 or IEC 61107.\n\nAMR Hosting is a back-office solution which allows a user to track his/her electricity, water, or gas consumption over the Internet. All data is collected in near real-time, and is stored in a database by data acquisition software. The user can view the data via a web application, and can analyze the data using various online analysis tools such as charting load profiles, analyzing tariff components, and verify his/her utility bill.\n\nRadio frequency based AMR can take many forms. The more common ones are handheld, mobile, satellite and fixed network solutions. There are both two-way RF systems and one-way RF systems in use that use both licensed and unlicensed RF bands.\n\nIn a two-way or \"wake up\" system, a radio signal is normally sent to an AMR meter's unique serial number, instructing its transceiver to power-up and transmit its data. The meter transceiver and the reading transceiver both send and receive radio signals. In a one-way “bubble-up” or continuous broadcast type system, the meter transmits continuously and data is sent every few seconds. This means the reading device can be a receiver only, and the meter a transmitter only. Data travels only from the meter transmitter to the reading receiver. There are also hybrid systems that combine one-way and two-way techniques, using one-way communication for reading and two-way communication for programming functions.\n\nRF-based meter reading usually eliminates the need for the meter reader to enter the property or home, or to locate and open an underground meter pit. The utility saves money by increased speed of reading, has less liability from entering private property, and has fewer missed readings from being unable to access the meter.\n\nThe technology based on RF is not readily accepted everywhere. In several Asian countries, the technology faces a barrier of regulations in place pertaining to use of the radio frequency of any radiated power. For example, in India the radio frequency which is generally in ISM band is not free to use even for low power radio of 10 mW. The majority of manufacturers of electricity meters have radio frequency devices in the frequency band of 433/868 MHz for large scale deployment in European countries. The frequency band of 2.4 GHz can be now used in India for outdoor as well as indoor applications, but few manufacturers have shown products within this frequency band. Initiatives in radio frequency AMR in such countries are being taken up with regulators wherever the cost of licensing outweighs the benefits of AMR.\n\nIn handheld AMR, a meter reader carries a handheld computer with a built-in or attached receiver/transceiver (radio frequency or touch) to collect meter readings from an AMR capable meter. This is sometimes referred to as \"walk-by\" meter reading since the meter reader walks by the locations where meters are installed as they go through their meter reading route. Handheld computers may also be used to manually enter readings without the use of AMR technology as an alternate but this will not support exhaustive data which can be accurately read using the meter reading electronically.\n\nMobile or \"drive-by\" meter reading is where a reading device is installed in a vehicle. The meter reader drives the vehicle while the reading device automatically collects the meter readings. Often, for mobile meter reading, the reading equipment includes navigational and mapping features provided by GPS and mapping software. With mobile meter reading, the reader does not normally have to read the meters in any particular route order, but just drives the service area until all meters are read. Components often consist of a laptop or proprietary computer, software, RF receiver/transceiver, and external vehicle antennas.\n\nSatellite transmitters can be installed in the field next to existing meters. The satellite AMR devices communicates with the meter for readings, and then sends those readings over a fixed or mobile satellite network. This network requires a clear view to the sky for the satellite transmitter/receiver, but eliminates the need to install fixed towers or send out field technicians, thereby being particularly suited for areas with low geographic meter density.\n\n\nThere are also meters using AMR with RF technologies such as cellular phone data systems, ZigBee, Bluetooth, Wavenis and others. Some systems operate with U.S. Federal Communications Commission (FCC) licensed frequencies and others under FCC Part 15, which allows use of unlicensed radio frequencies.\n\nWiSmart is a versatile platform which can be used by a variety of electrical home appliances in order to provide wireless TCP/IP communication using the 802.11 b/g protocol.\n\nDevices such as the Smart Thermostat permit a utility to lower a home's power consumption to help manage power demand.\n\nThe city of Corpus Christi became one of the first cities in the United States to implement citywide Wi-Fi, which had been free until May 31, 2007, mainly to facilitate AMR after a meter reader was attacked by a dog. Today many meters are designed to transmit using Wi-Fi, even if a Wi-Fi network is not available, and they are read using a drive-by local Wi-Fi hand held receiver.\n\nThe meters installed in Corpus Christi are not directly Wi-Fi enabled, but rather transmit narrow-band burst telemetry on the 460 MHz band. This narrow-band signal has much greater range than Wi-Fi, so the number of receivers required for the project are far fewer. Special receiver stations then decode the narrow-band signals and resend the data via Wi-Fi.\n\nMost of the automated utility meters installed in the Corpus Christi area are battery powered. Wi-Fi technology is unsuitable for long-term battery-powered operation.\n\nPLC is a method where electronic data is transmitted over power lines back to the substation, then relayed to a central computer in the utility's main office. This would be considered a type of fixed network system—the network being the distribution network which the utility has built and maintains to deliver electric power. Such systems are primarily used for electric meter reading. Some providers have interfaced gas and water meters to feed into a PLC type system.\n\nIn 1972, Theodore George “Ted” Paraskevakos, while working with Boeing in Huntsville, Alabama, developed a sensor monitoring system which used digital transmission for security, fire and medical alarm systems as well as meter reading capabilities for all utilities. This technology was a spin-off of the automatic telephone line identification system, now known as Caller ID.\n\nIn 1974, Mr. Paraskevakos was awarded a U.S. patent for this technology. In 1977, he launched Metretek, Inc., which developed and produced the first fully automated, commercially available remote meter reading and load management system. Since this system was developed pre-Internet, Metretek utilized the IBM series 1 mini-computer. For this approach, Mr. Paraskevakos and Metretek were awarded multiple patents.\n\nThe primary driver for the automation of meter reading is not to reduce labor costs, but to obtain data that is difficult to obtain. As an example, many water meters are installed in locations that require the utility to schedule an appointment with the homeowner in order to obtain access to the meter. In many areas, consumers have demanded that their monthly water bill be based on an actual reading, instead of (for example) an estimated monthly usage based on just one actual meter reading made every 12 months. Early AMR systems often consisted of walk-by and drive-by AMR for residential customers, and telephone-based AMR for commercial or industrial customers. What was once a need for monthly data became a need for daily and even hourly readings of the meters. Consequently, the sales of drive-by and telephone AMR has declined in the US, while sales of fixed networks has increased. The US Energy Policy Act of 2005 asks that electric utility regulators consider the support for a \"...time-based rate schedule \"(to)\" enable the electric consumer to manage energy use and cost through advanced metering and communications technology.\" \nThe trend now is to consider the use of advanced meters as part of an Advanced Metering Infrastructure.\n\nOriginally AMR devices just collected meter readings electronically and matched them with accounts. As technology has advanced, additional data could then be captured, stored, and transmitted to the main computer, and often the metering devices could be controlled remotely. This can include events alarms such as tamper, leak detection, low battery, or reverse flow. Many AMR devices can also capture interval data, and log meter events. The logged data can be used to collect or control time of use or rate of use data that can be used for water or energy usage profiling, time of use billing, demand forecasting, demand response, rate of flow recording, leak detection, flow monitoring, water and energy conservation enforcement, remote shutoff, etc. Advanced metering infrastructure, or AMI is the new term coined to represent the networking technology of fixed network meter systems that go beyond AMR into remote utility management. The meters in an AMI system are often referred to as smart meters, since they often can use collected data based on programmed logic.\n\nThe Automatic Meter Reading Association (AMRA) endorses the National Association of Regulatory Utility Commissioners (NARUC) resolution to eliminate regulatory barriers to the broad implementation of advanced metering infrastructure (AMI). The resolution, passed in February 2007, acknowledged the role of AMI in supporting the implementation of dynamic pricing and the resulting benefits to consumers. The resolution further identified the value of AMI in achieving significant utility operational cost savings in the areas of outage management, revenue protection and asset management. The resolution also called for AMI business case analysis to identify cost-effective deployment strategies, endorsed timely cost recovery for prudently incurred AMI expenditures and made additional recommendations on rate making and tax treatment of such investments.\n\nAdvanced metering systems can provide benefits for utilities, retail providers and customers. Benefits will be recognized by the utilities with increased efficiencies, outage detection, tamper notification and reduced labor cost as a result of automating reads, connections and disconnects. Retail providers will be able to offer new innovative products in addition to customizing packages for their customers. In addition, with the meter data being readily available, more flexible billing cycles would be available to their customers instead of following the standard utility read cycles. With timely usage information available to the customer, benefits will be seen through opportunities to manage their energy consumption and change from one REP to another with actual meter data. Because of these benefits, many utilities are moving towards implementing some types of AMR solutions.\n\nIn many cases, smart metering is required by law (e.g. Pennsylvania's Act 129 (2008)).\n\nThe benefits of smart metering for the utility.\n\n\nThe benefits of smart metering for the customer.\n\n\nConstruction practices, weather, and the need for information drive utilities in different parts of the world towards AMR at different rates. In the US, there have been significant fixed network deployments of both RF based and PLC based technologies. Some countries have either deployed or plan to deploy AMR systems throughout the entire country.\n\nBy using a combination of AMR and energy analytics reports, SPAR were able to reduce energy consumption by 20%.\n\nAMI in Australia has grown from both government policy which sought to rectify observed market inefficiencies, and distribution businesses who looked to gain operational efficiencies. In July 2008, there was a mandated program being planned in Victoria for the deployment of 2.6 million meters over a 4-year period. The anticipated peak installation rate of AMI meters was 5,000 per day across Victoria. The program governance was provided by an industry steering committee.\n\nIn 2009 the Victorian Auditor General undertook a review of the program and found that there were \"significant inadequacies\" in advice to Government and that project governance \"has not been appropriate\". The Victorian government subsequently announced a moratorium of the program\n\n\n"}
{"id": "12308997", "url": "https://en.wikipedia.org/wiki?curid=12308997", "title": "Autonomous detection system", "text": "Autonomous detection system\n\nAutonomous Detection Systems (ADS), also called biohazard detection systems, or autonomous pathogen detection systems, are designed to monitor air in the environment and to detect the presence of airborne chemicals, toxins, pathogens, or other biological agents capable of causing human illness or death. Currently under development, these systems monitor the air continuously and send real-time alerts to appropriate authorities in the event of an act of bioterrorism or biological warfare.\n\nIn the United States, an ADS system (BDS) was developed for the U.S. Postal Service following the anthrax scare of 2001. The detection systems were installed in 2006.\n\n"}
{"id": "4205876", "url": "https://en.wikipedia.org/wiki?curid=4205876", "title": "Auxochrome", "text": "Auxochrome\n\nAn auxochrome (from Ancient Greek \"auxanō\" \"increase\" and \"chrōma\" \"colour\") is a group of atoms attached to a chromophore which modifies the ability of that chromophore to absorb light. They themselves fail to produce the colour; but when present along with the chromophores in an organic compound intensifies the colour of the chromogen. Examples include the hydroxyl group (−OH), the amino group (−NH), the aldehyde group (−CHO), and the methyl mercaptan group (−SCH).\n\nAn auxochrome is a functional group of atoms with one or more lone pairs of electrons when attached to a chromophore, alters both the wavelength and intensity of absorption. If these groups are in direct conjugation with the pi-system of the chromophore, they may increase the wavelength at which the light is absorbed and as a result intensify the absorption. A feature of these auxochromes is the presence of at least one lone pair of electrons which can be viewed as extending the conjugated system by resonance.\n\nIt increases the color of any organic compound. For example, benzene does not display color as it does not have a chromophore; but nitrobenzene is pale yellow color because of the presence of a nitro group (−NO) which acts as a chromophore. But \"p\"-hydroxynitrobenzene exhibits a deep yellow color, in which the −OH group acts as an auxochrome. Here the auxochrome (−OH) is conjugated with the chromophore −NO. Similar behavior is seen in azobenzene which has a red color, but \"p\"-hydroxyazobenzene is dark red in color.\n\nThe presence of an auxochrome in the chromogen molecule is essential to make a dye. However, if an auxochrome is present in the meta position to the chromophore, it does not affect the color.\n\nAn auxochrome is known as a compound that produces a bathochromic shift, also known as red shift because it increases the wavelength of absorption, therefore moving closer to infrared light. Woodward−Fieser rules estimate the shift in wavelength of maximum absorption for several auxochromes attached to a conjugated system in an organic molecule.\n\nAn auxochrome helps a dye to bind to the object that is to be colored. Electrolytic dissociation of the auxochrome group helps in binding and it is due to this reason a basic substance takes an acidic dye.\n\nA molecule exhibits colour because it absorbs colours only of certain frequencies and reflects or transmits others. They are capable of absorbing and emitting light of various frequencies. Light waves with frequency very close to their natural frequency are absorbed readily. This phenomenon, known as resonance, means that the molecule can absorb radiation of a particular frequency which is same as the frequency of electron movement within the molecule. The chromophore is the part of the molecule where the energy difference between two different molecular orbitals falls within the range of the visible spectrum and hence absorbs some particular colours from visible light. Hence the molecule appears coloured. When auxochromes are attached to the molecule, the natural frequency of the chromophore gets changed and thus the colour gets modified. Different auxochromes produce different effects in the chromophore which in turn causes absorption of light from other parts of the spectrum. Normally, auxochromes which intensify the colour are chosen.\n\nThere are mainly two types of auxochromes:\n\n"}
{"id": "23798333", "url": "https://en.wikipedia.org/wiki?curid=23798333", "title": "Blackstone Wind Farm", "text": "Blackstone Wind Farm\n\nThe Blackstone Wind Farm is an approved offshore wind power project in the Black Sea area of Romania. It will have 100 individual wind turbines with a nominal output of around 5 MW each which will deliver up to 500 MW of power, enough to power over 334,821 homes, with a capital investment required of approximately US$1.4 billion.\n"}
{"id": "2594620", "url": "https://en.wikipedia.org/wiki?curid=2594620", "title": "Bonny Light oil", "text": "Bonny Light oil\n\nBonny Light oil is a high grade of Nigerian crude oil with high API gravity (low specific gravity), produced in the Niger Delta basin and named after the prolific region around the city of Bonny.\n\nThe very low sulfur content of Bonny Light crude makes it a highly desired grade for its low corrosiveness to refinery infrastructure and the lower environmental impact of its byproducts in refinery effluent.\n\nOther grades of Nigerian crude oil are Qua Iboe crude oil, Brass River crude oil, and Forcados crude oil. The Cabinda crude oil is a common grade of crude oil produced in Angola. The Bonny Light is in high demand specifically by American and European refineries. It is therefore a major source of income for the oil-rich nation.\n\nThe national government of Nigeria runs and takes part in the petroleum industry of Nigeria through the NNPC corporation. Libya's NOC also owns a 36% stake in NNPC which is valued at $112 Billion.\n"}
{"id": "10066313", "url": "https://en.wikipedia.org/wiki?curid=10066313", "title": "Burgers vector", "text": "Burgers vector\n\nIn materials science, the Burgers vector, named after Dutch physicist Jan Burgers, is a vector, often denoted as b, that represents the magnitude and direction of the lattice distortion resulting from a dislocation in a crystal lattice.\n\nThe vector's magnitude and direction is best understood when the dislocation-bearing crystal structure is first visualized \"without\" the dislocation, that is, the \"perfect\" crystal structure. In this perfect crystal structure, a rectangle whose lengths and widths are integer multiples of \"a\" (the unit cell edge length) is drawn \"encompassing\" the site of the original dislocation's origin. Once this encompassing rectangle is drawn, the dislocation can be introduced. This dislocation will have the effect of deforming, not only the perfect crystal structure, but the rectangle as well. The said rectangle could have one of its sides disjoined from the perpendicular side, severing the connection of the length and width line segments of the rectangle at one of the rectangle's corners, and displacing each line segment from each other. What was once a rectangle before the dislocation was introduced is now an open geometric figure, whose opening defines the direction and magnitude of the Burgers vector. Specifically, the breadth of the opening defines the magnitude of the Burgers vector, and, when a set of fixed coordinates is introduced, an angle between the termini of the dislocated rectangle's length line segment and width line segment may be specified.\n\nWhen calculating the Burgers vector practically, one may draw a rectangular clockwise circuit from a starting point to enclose the dislocation (see the picture above). The Burgers vector will be the vector from the START to the END of the circuit. \n\nThe direction of the vector depends on the plane of dislocation, which is usually on one of the closest-packed crystallographic planes. \nThe magnitude is usually represented by the equation (For BCC and FCC lattices only):\nwhere \"a\" is the unit cell edge length of the crystal, ||b|| is the magnitude of Burgers vector and \"h\", \"k\", and \"l\" are the components of the Burgers vector, b = formula_2 , and the coefficient a/2 is owing to the fact that in BCC and FCC lattices, the shortest lattice vectors could be as expressed formula_2. Comparatively, for simple cubic lattices, b = formula_4 and hence the magnitude is represented by\n\nIn most metallic materials, the magnitude of the Burgers vector for a dislocation is of a magnitude equal to the interatomic spacing of the material, since a single dislocation will offset the crystal lattice by one close-packed crystallographic spacing unit.\n\nIn edge dislocations, the Burgers vector and dislocation line are perpendicular to one another. In screw dislocations, they are parallel.\n\nThe Burgers vector is significant in determining the yield strength of a material by affecting solute hardening, precipitation hardening and work hardening.\n\n"}
{"id": "6269281", "url": "https://en.wikipedia.org/wiki?curid=6269281", "title": "CIIC 500", "text": "CIIC 500\n\nCIIC 500 is a stone inscription discovered on the Isle of Man in 1909. The stone's material is 'clay slate' and is believed to have originated from hills six miles to the south of the site where it was found.\n\n\n"}
{"id": "3013918", "url": "https://en.wikipedia.org/wiki?curid=3013918", "title": "CIS Tower", "text": "CIS Tower\n\nThe CIS Tower is an office skyscraper on Miller Street in Manchester, England. It was completed in 1962 and rises to 387 feet (118 m) in height. The Grade II listed building, which houses the Co-operative Banking Group, is Manchester's second-tallest building and the tallest office building in the United Kingdom outside London. The tower remained as built for over 40 years until maintenance issues on the service tower required an extensive renovation which included covering its facade in photovoltaic panels.\n\nThe tower was designed as a prestige headquarters to showcase the Co-operative movement in Manchester. In 1958 the company proposed building an office tower block, construction began the following year and was completed in 1962.\nIt was designed by Gordon Tait of Burnett, Tait & Partners and Co-operative's own architect, G. S. Hay. In the 1990s, it was granted Grade II listed building status by English Heritage. The tower, described as \"the best of the Manchester 1960s office blocks\", was listed for its \"discipline and consistency\". It is part of a group with New Century House and its Conference Hall on Corporation Street. The tower's design was influenced by Skidmore, Owings & Merrill's Inland Steel Building in Chicago after a visit by the architects in 1958.\n\nIn 1962, at 387 feet, the CIS Tower overtook the Shell Centre as the tallest building in the United Kingdom, a title it retained for a year until it was replaced by the Millbank Tower in London. In 2006 the Beetham Tower became the tallest building in Manchester.\n\nThe office tower building rises above a five-storey podium block. It has a steel frame and glass curtain walls with metal window frames. Black vitreous enamel panels demarcate the floor levels. The building materials, glass, enamelled steel and aluminium, were chosen so that the building could remain clean in the polluted Manchester atmosphere. The tower's concrete service shaft, which rises above the office tower, has two bands of vents at the top and was clad in a mosaic made up of 14 million centimetre-square, grey tesserae designed to shimmer and sparkle. The projecting reinforced concrete service shaft houses lifts and emergency stairs.\n\nThe ground floor is set back behind six pillars. A green bronze-like, abstract mural sculpted by William George Mitchell made from fibreglass covers the entrance hall's rear wall. The building has 700,000 square feet of floor area with clear open spaces on the office floors. Interiors were designed by Misha Black of the Design Research Unit. The executive areas are delineated by the use of teak and cherry wood veneers.\n\nWithin six months of construction some of the mosaic tiles on the service tower became detached owing to cement failure and lack of expansion joints in the concrete. Although the tower was granted listed building status in 1995, falling tiles were an ongoing problem. English Heritage had to be consulted as alterations could change the building's appearance.\n\nIn 2004 CIS consulted Solarcentury with a view to replacing the deteriorating mosaic with 575.5 kW of blue building-integrated photovoltaic (PV) cells which would provide a permanent green energy solution, generating approximately 180,000 kWh (average of 20 kW) of electricity per year. The work was completed by Arup and at that time was the largest commercial solar façade in Europe. The PV cells made by Sharp Electronics began feeding electricity to the National Grid in November 2005. The project, which cost £5.5 million, was partly funded by the Northwest Regional Development Agency which granted £885,000 and the Energy Savings Trust at the Department of Trade and Industry (DTI) contributed £175,000. The solar power project was chosen by the DTI as one of the \"10 best green energy projects\" of 2005. Out of sight on the roof are 24 wind turbines generating 10% of the tower's electricity. \n"}
{"id": "18537533", "url": "https://en.wikipedia.org/wiki?curid=18537533", "title": "Clyde Wind Farm", "text": "Clyde Wind Farm\n\nThe Clyde Wind Farm is a 350 megawatt (MW) wind farm near Abington in South Lanarkshire, Scotland.\n\nThe 152-turbine project by Scottish and Southern Energy was approved by the Scottish Parliament in July 2008, and is capable of powering 200,000 homes. SSE was given planning permission to build a wind farm with turbines built on either side of the M74 motorway.\n\nConstruction of the wind farm, which was budgeted for £600 million, began in early 2009 and finished in 2012. Welcon Towers Ltd won the contract to supply the towers for all 152 turbines for the £600 million Clyde Wind Farm. Jesper Øhlenschlæger, chief executive officer of Welcon Towers parent company Skykon, said: ‘The Clyde project is a very important business win for our Campbeltown manufacturing. Scotland has become the most positive and the most interesting renewable wind power market in Europe. The Clyde Wind Farm project represents a landmark phase in Scotland’s renewable energy strategy.’\n\nThe farm was opened at a ceremonial ribbon cutting by First Minister of Scotland Alex Salmond in September 2012.\n\nIn July 2014 it was announced that Scottish ministers had approved an extension to the Clyde Wind Farm. The extension will see 54 extra turbines, capable of generating an additional 162MW. This will bring the total generating capacity of the wind farm to 512MW.\n\n\n"}
{"id": "20727759", "url": "https://en.wikipedia.org/wiki?curid=20727759", "title": "Cobalt sulfide", "text": "Cobalt sulfide\n\nCobalt sulfide is the name for chemical compounds with a formula CoS. Well-characterized species include minerals with the formula CoS, CoS, CoS, and CoS. In general, the sulfides of cobalt are black, semiconducting, insoluble in water, and nonstoichiometric.\n\nCobalt sulfides occur widely as minerals, comprising major sources of all cobalt compounds. Binary cobalt sulfide minerals include the cattierite (CoS) and linnaeite (CoS). CoS (see image in table) is isostructural with iron pyrite, featuring disulfide groups, i.e. CoS. Linnaeite, also rare, adopts the spinel motif. The CoS compound is known as a very rare cobaltpentlandite (the Co analogue of pentlandite). Mixed metal sulfide minerals include carrollite (CuCoS) and [[siegenite (CoNiS).\n\nCobalt sulfide minerals are converted to cobalt via roasting and extraction into aqueous acid. In some processes, the cobalt salts are further purified by precipitated when aqueous solutions of cobalt(II) ions are treated with [[hydrogen sulfide]]. Not only is this reaction useful in the purification of cobalt from its ores, but also in [[qualitative inorganic analysis]].\n\nIn combination with molybdenum, the sulfides of cobalt are used as catalysts for the industrial process called [[hydrodesulfurization]], which is implemented on a large scale in [[Oil refinery|refineries]]. Synthetic cobalt sulfides are widely investigated as electrocatalysts.\n\n\n\n[[Category:Cobalt compounds]]\n[[Category:Sulfides]]\n"}
{"id": "12754000", "url": "https://en.wikipedia.org/wiki?curid=12754000", "title": "Complex metal hydride", "text": "Complex metal hydride\n\nComplex metal hydrides are salts wherein the anions contain hydrides. In the older chemical literature as well as contemporary materials science textbooks, a \"metal hydride\" is assumed to be nonmolecular, i.e. three-dimensional lattices of atomic ions. In such systems, hydrides are often interstitial and nonstoichiometric, and the bonding between the metal and hydrogen atoms is significantly ionic. In contrast, complex metal hydrides typically contain more than one type of metal or metalloid and may be soluble but invariably react with water. They exhibit ionic bonding between a positive metal ion with molecular anions containing the hydride. In such materials the hydrogen is bonded with significant covalent character to the second metal or metalloid atoms.\n\nIn general, complex metal hydrides have the formula MM'H, where M is an alkali metal cation or cation complex and M' is a metal or metalloid. Well known examples feature group 13 elements, especially boron and aluminium including sodium aluminium hydride, NaAlH (also known as sodium alanate), lithium aluminum hydride, LiAlH, and lithium borohydride, (LiBH). Complex metal hydrides are often soluble in etherial solvents. Other complex metal hydrides are numerous. Illustrative examples include the salts [MgBr(THF)]FeH and KReH.\n\n"}
{"id": "4838787", "url": "https://en.wikipedia.org/wiki?curid=4838787", "title": "Delta-wye transformer", "text": "Delta-wye transformer\n\nA delta-wye transformer is a type of three-phase electric power transformer design that employs delta-connected windings on its primary and wye/star connected windings on its secondary. A neutral wire can be provided on wye output side. It can be a single three-phase transformer, or built from three independent single-phase units. An equivalent term is delta-star transformer. \n\nDelta-wye transformers are common in commercial, industrial, and high-density residential locations, to supply three-phase distribution systems.\n\nAn example would be a distribution transformer with a delta primary, running on three 11 kV phases with no neutral or earth required, and a star (or wye) secondary providing a 3-phase supply at 415 V, with the domestic voltage of 240 available between each phase and the earthed (grounded) neutral point.\n\nThe delta winding allows third-harmonic currents to circulate within the transformer, and prevents third-harmonic currents from flowing in the supply line. Delta connected windings are not common for higher transmission voltages (138 kV and above) owing to the higher cost of insulation compared with a wye connection. \n\nDelta-wye transformers introduce a 30, 150, 270 or 330 degree phase shift. Thus they cannot be paralleled with wye-wye (or delta-delta) transformers. However, they can be paralleled with identical configurations and some different configurations of other delta-wye (or wye-delta with some attention) transformers.\n\n\n"}
{"id": "36797134", "url": "https://en.wikipedia.org/wiki?curid=36797134", "title": "Deposit-refund system", "text": "Deposit-refund system\n\nA deposit-refund system (DRS), also known as deposit-return system, advance deposit fee or deposit-return scheme, is a surcharge on a product when purchased and a rebate when it is returned. A well-known example is when container deposit legislation mandates that a refund is given when reusable packaging is returned. Deposit-refund system are a market-based instrument to address externalities. As with Pigovian taxes a DRS aims to limit pollution of various types by creating an incentive to return a product. \n\nWhile most commonly used with beverage containers it can be used on other materials including liquid and gaseous wastes. Deposit-refund systems are used on products such as batteries, tyres, automotive oil, consumer electronics and shipping pallets.\n\nThere are three potential advantages of a DRS: it reduces illegal dumping by giving a financial incentive, it makes monitoring and enforcement easier, and evading the costs is difficult.\n\nDeposit-refund systems can be both voluntary or mandated by legislation.\n\n"}
{"id": "34469840", "url": "https://en.wikipedia.org/wiki?curid=34469840", "title": "Dielectric absorption", "text": "Dielectric absorption\n\nDielectric absorption is the name given to the effect by which a capacitor, that has been charged for a long time, discharges only incompletely when briefly discharged. Although an ideal capacitor would remain at zero volts after being discharged, real capacitors will develop a small voltage from time-delayed dipole discharging, a phenomenon that is also called dielectric relaxation, \"soakage\", or \"battery action\". For some dielectrics, such as many polymer films, the resulting voltage may be less than 1–2% of the original voltage, but it can be as much as 15% for electrolytic capacitors. The voltage at the terminals generated by the dielectric absorption may possibly cause problems in the function of an electronic circuit or can be a safety risk to personnel. In order to prevent shocks, most very large capacitors are shipped with shorting wires that need to be removed before they are used.\n\nCharging a capacitor (due to a voltage between the capacitor plates) causes an electric field to be applied to the dielectric between the electrodes. This field exerts a torque on the molecular dipoles, causing the directions of the dipole moments to align with the field direction. This change in the molecular dipoles is called oriented polarization and also causes heat to be generated, resulting in dielectric losses (see dissipation factor). The orientation of the dipoles does not follow the electric field synchronously, but is delayed by a time constant that depends on the material. This delay corresponds to a hysteresis response of the polarization to the external field.\n\nWhen the capacitor is discharging, the strength of the electric field is decreasing and the common orientation of the molecular dipoles is returning to an undirected state in a process of relaxation. Due to the hysteresis, at the zero point of the electric field, a material-dependent number of molecular dipoles are still polarized along the field direction without a measurable voltage appearing at the terminals of the capacitor. This is like an electrical remanence. The oriented dipoles will be discharged spontaneously over time and the voltage at the electrodes of the capacitor will decay exponentially. The complete discharge time of all dipoles can be days to weeks depending on the material. This \"reloaded\" voltage can be retained for months, even in electrolytic capacitors, caused by the high insulation resistance in common modern capacitor dielectrics. The discharge of a capacitor and the subsequent reloading can be repeated several times.\n\nDielectric absorption is a property which has been long known. Its value can be measured in accordance with the IEC/EN 60384-1 standard. The capacitor shall be charged at the DC voltage rating for 60 minutes. Then the capacitor shall be disconnected from the power source and shall be discharged for 10 s. The voltage regained on the capacitor terminals (recovery voltage) within 15 minutes is the dielectric absorption voltage. The size of the dielectric absorption voltage is specified in relation to the applied voltage in percent and depends on the dielectric material used. It is specified by many manufacturers in the data sheets.\n\nThe voltage at the terminals generated by the dielectric absorption may possibly cause problems in the function of an electronic circuit. For sensitive analog circuits such as sample and hold circuits, integrators, charge amplifiers or high-quality audio circuits, Class-1 ceramic or polypropylene capacitors instead of Class-2 ceramic capacitors, polyester film capacitors or electrolytic capacitors are used. For most electronic circuits, particularly filtering applications, the small dielectric absorption voltage has no influence on the proper electrical function of the circuit. For aluminum electrolytic capacitors with non-solid electrolyte which are not built into a circuit, the dielectric absorption voltage generated can be a personnel safety risk. The voltage can be quite substantial, for example 50 V for 400 V electrolytic capacitors, and can cause damages to semiconductor devices, or cause sparks during installation in the circuit. Larger aluminum electrolytic capacitors and high-voltage power capacitors are transported and delivered short-circuited to dissipate this unwanted and possibly dangerous energy.\n\nAnother effect of dielectric absorption is sometimes described as \"soakage\". This manifests as a component of leakage current and it contributes to the loss factor of the capacitor. This effect has been known of only recently: it is now a proportionately greater part of leakage current due to the significantly improved properties of modern capacitors.\n\nFor double-layer capacitors, there are no figures from manufacturers available.\n\n\n"}
{"id": "18356718", "url": "https://en.wikipedia.org/wiki?curid=18356718", "title": "Dwarf coconut", "text": "Dwarf coconut\n\nDwarf coconut is a range of varieties of coconut palm. The use of the word “dwarf” here does not refer to the tree’s size, as it can reach heights of 50-80 feet which is certainly not a dwarf. Instead, the dwarf designation refers to the size in which it will begin to produce the coveted or harvestable coconut.\n\nOther types of dwarfs are: (in alphabetical order)\n\nThe \"Malayan Dwarf\" is a variety of dwarf coconut. The palm is classified based on the nut color: ivory yellow nuts, apricot red nuts, and green nuts. The palm's resistance to the Lethal Yellowing disease is the characteristic that makes it to be one of the important dwarf types in the world.\n\nRecognition of the Malayan Dwarf's resistance to lethal yellowing was in 1956 at Round Hill, Jamaica, where the Malayan Dwarfs affected by the disease was only a very small percentage.\n\nThe Maypan coconut palm is a \"Dwarf\"x\"Tall\" hybrid between the Malayan Dwarf and \"Panama Tall\". The \"Dwarf\"x\"Tall\" hybrid between the Malayan Dwarf (yellow) and \"West African Tall\" is locally known as \"Mawa\", in Ivory Coast, Mawa is known as \"Port-Bouet-121\".\n\n"}
{"id": "19451464", "url": "https://en.wikipedia.org/wiki?curid=19451464", "title": "Enel Agighiol Wind Farm", "text": "Enel Agighiol Wind Farm\n\nThe Enel Agighiol Wind Farm is wind farm located in Tulcea County, Romania. It has 17 individual wind turbines with a nominal output of around 2 MW each and delivers up to 34 MW of power, enough to power over 35,000 homes, which required a capital investment of approximately US$50 million.\n"}
{"id": "170165", "url": "https://en.wikipedia.org/wiki?curid=170165", "title": "Fermi energy", "text": "Fermi energy\n\nThe Fermi energy is a concept in quantum mechanics usually referring to the energy difference between the highest and lowest occupied single-particle states in a quantum system of non-interacting fermions at absolute zero temperature.\nIn a Fermi gas, the lowest occupied state is taken to have zero kinetic energy, whereas in a metal, the lowest occupied state is typically taken to mean the bottom of the conduction band.\n\nConfusingly, the term \"Fermi energy\" is often being used for referring to a different yet closely related concept, the Fermi \"level\" (also called electrochemical potential).\nThere are a few key differences between the Fermi level and Fermi energy, at least as they are used in this article:\n\nSince the Fermi level in a metal at absolute zero is the energy of the highest occupied single particle state,\nthen the Fermi energy in a metal is the energy difference between the Fermi level and lowest occupied single-particle state, at zero-temperature.\n\nIn quantum mechanics, a group of particles known as fermions (for example, electrons, protons and neutrons) obey the Pauli exclusion principle. This states that two fermions cannot occupy the same quantum state. Since an idealized non-interacting Fermi gas can be analyzed in terms of single-particle stationary states, we can thus say that two fermions cannot occupy the same stationary state. These stationary states will typically be distinct in energy. To find the ground state of the whole system, we start with an empty system, and add particles one at a time, consecutively filling up the unoccupied stationary states with the lowest energy. When all the particles have been put in, the Fermi energy is the kinetic energy of the highest occupied state.\n\nAs a consequence, even if we have extracted all possible energy from a Fermi gas by cooling it to near absolute zero temperature, the fermions are still moving around at a high speed. The fastest ones are moving at a velocity corresponding to a kinetic energy equal to the Fermi energy. This speed is known as the Fermi velocity. Only when the temperature exceeds the related Fermi temperature, do the electrons begin to move significantly faster than at absolute zero.\n\nThe Fermi energy is an important concept in the solid state physics of metals and superconductors. It is also a very important quantity in the physics of quantum liquids like low temperature helium (both normal and superfluid He), and it is quite important to nuclear physics and to understanding the stability of white dwarf stars against gravitational collapse.\n\nThe Fermi energy for fermions spin-½ in a three-dimensional (non-relativistic) system is given by:\nwhere \"N\" is the number of particles, \"m\" the rest mass of each fermion, \"V\" the volume of the system and formula_2 the reduced Planck constant.\nUnder the free electron model, the electrons in a metal can be considered to form a Fermi gas. The number density formula_3 of conduction electrons in metals ranges between approximately 10 and 10 electrons/m, which is also the typical density of atoms in ordinary solid matter. This number density produces a Fermi energy of the order of 2 to 10 electronvolts.\n\nStars known as white dwarfs have mass comparable to our Sun, but have about a hundredth of its radius. The high densities mean that the electrons are no longer bound to single nuclei and instead form a degenerate electron gas. Their Fermi energy is about 0.3 MeV.\n\nAnother typical example is that of the nucleons in the nucleus of an atom. The radius of the nucleus admits deviations, so a typical value for the Fermi energy is usually given as 38 MeV.\n\nUsing this definition of above for the Fermi energy, various related quantities can be useful.\n\nThe Fermi temperature is defined as:\n\nwhere formula_5 is the Boltzmann constant and formula_6 the Fermi energy. The Fermi temperature can be thought of as the temperature at which thermal effects are comparable to quantum effects associated with Fermi statistics. The Fermi temperature for a metal is a couple of orders of magnitude above room temperature.\n\nOther quantities defined in this context are Fermi momentum\n\nand Fermi velocity\n\nThese quantities are the momentum and group velocity, respectively, of a fermion at the Fermi surface.\n\nThe Fermi momentum can also be described asformula_9, where formula_10 is the radius of the Fermi sphere and is called the Fermi wavevector.\n\nThese quantities are \"not\" well-defined in cases where the Fermi surface is non-spherical.\n\n"}
{"id": "18527330", "url": "https://en.wikipedia.org/wiki?curid=18527330", "title": "Foldy–Wouthuysen transformation", "text": "Foldy–Wouthuysen transformation\n\nThe Foldy–Wouthuysen transform is widely used in high energy physics. It was historically formulated by Leslie Lawrance Foldy and Siegfried Adolf Wouthuysen in 1949 to understand the nonrelativistic limit of the Dirac equation, the equation for spin- particles. A detailed general discussion of the Foldy–Wouthuysen-type transformations in particle interpretation of relativistic wave equations is in Acharya and Sudarshan (1960).\n\nThe FW transformation is a unitary transformation of the orthonormal basis in which both the Hamiltonian and the state are represented. The eigenvalues do not change under such a unitary transformation, that is, the physics does not change under such a unitary basis transformation. Therefore, such a unitary transformation can always be applied: in particular a unitary basis transformation may be picked which will put the Hamiltonian in a more pleasant form, at the expense of a change in the state function, which then represents something else. See for example the Bogoliubov transformation, which is an orthogonal basis transform for the same purpose. The suggestion that the FW transform is applicable to the state \"or\" the Hamiltonian is thus not correct.\n\nFoldy and Wouthuysen made use of a canonical transform that has now come to be known as the \"Foldy–Wouthuysen transformation\". A brief account of the history of the transformation is to be found in the obituaries of Foldy and Wouthuysen and the biographical memoir of Foldy. Before their work, there was some difficulty in understanding and gathering all the interaction terms of a given order, such as those for a Dirac particle immersed in an external field. With their procedure the physical interpretation of the terms was clear, and it became possible to apply their work in a systematic way to a number of problems that had previously defied solution. The Foldy–Wouthuysen transform was extended to the physically important cases of spin-0 and spin-1 particles, and even generalized to the case of arbitrary spins.\n\nThe Foldy–Wouthuysen (FW) transformation is a unitary transformation on a fermion wave function of the form:\n\nwhere the unitary operator is the 4 × 4 matrix:\n\nAbove,\n\nis the unit vector oriented in the direction of the fermion momentum. The above are related to the Dirac matrices by and , with . A straightforward series expansion applying the commutativity properties of the Dirac matrices demonstrates that above is true. The inverse\n\nso it is clear that , where is a 4 × 4 identity matrix.\n\nThis transformation is of particular interest when applied to the free-fermion Dirac Hamiltonian operator\n\nin biunitary fashion, in the form:\n\nUsing the commutativity properties of the Dirac matrices, this can be massaged over into the double-angle expression:\n\nThis factors out into:\n\nClearly, the FW transformation is a \"continuous\" transformation, that is, one may employ any value for which one chooses. Now comes the distinct question of choosing a particular value for , which amounts to choosing a particular transformed representation.\n\nOne particularly important representation, is that in which the transformed Hamiltonian operator is diagonalized. Clearly, a completely diagonalized representation can be obtained by choosing such that the term in is made to vanish. Such a representation is specified by defining:\n\nso that is reduced to the diagonalized (this presupposes that is taken in the Dirac–Pauli representation (after Paul Dirac and Wolfgang Pauli) in which it is a diagonal matrix):\n\nBy elementary trigonometry, also implies that:\n\nso that using in now leads following reduction to:\n\nPrior to Foldy and Wouthuysen publishing their transformation, it was already known that is the Hamiltonian in the Newton–Wigner (NW) representation (named after Theodore Duddell Newton and Eugene Wigner) of the Dirac equation. What therefore tells us, is that by applying a FW transformation to the Dirac–Pauli representation of Dirac's equation, and then selecting the continuous transformation parameter so as to diagonalize the Hamiltonian, one arrives at the NW representation of Dirac's equation, because NW itself already contains the Hamiltonian specified in (). See this link.\n\nIf one considers an on-shell mass—fermion or otherwise—given by , and employs a Minkowski metric tensor for which , it should be apparent that the expression\nis equivalent to the component of the energy-momentum vector , so that is alternatively specified rather simply by .\n\nNow let us consider a fermion at rest, which we may define in this context as a fermion for which . From or , this means that , so that and, from , that the unitary operator . Therefore, any operator in the Dirac-Pauli representation upon which we perform a biunitary transformation, will be given, for an at-rest fermion, by:\n\nContrasting the original Dirac–Pauli Hamiltonian operator\n\nwith the NW Hamiltonian , we do indeed find the \"at rest\" correspondence:\n\nNow, let us consider the velocity operator. To obtain this operator, we must commute the Hamiltonian operator with the canonical position operators , i.e., we must calculate\n\nOne good way to approach this calculation, is to start by writing the scalar rest mass as\n\nand then to mandate that the scalar rest mass commute with the . Thus, we may write:\n\nwhere we have made use of the Heisenberg canonical commutation relationship to reduce terms. Then, multiplying from the left by and rearranging terms, we arrive at:\n\nBecause the canonical relationship\n\nthe above provides the basis for computing an inherent, non-zero acceleration operator, which specifies the oscillatory motion known as zitterbewegung.\n\nIn the Newton–Wigner representation, we now wish to calculate\n\nIf we use the result at the very end of section 2 above, , then this can be written instead as:\n\nUsing the above, we need simply to calculate , then multiply by .\n\nThe canonical calculation proceeds similarly to the calculation in section 4 above, but because of the square root expression in , one additional step is required.\n\nFirst, to accommodate the square root, we will wish to require that the scalar square mass commute with the canonical coordinates , which we write as:\n\nwhere we again use the Heisenberg canonical relationship . Then, we need an expression for which will satisfy . It is straightforward to verify that:\n\nwill satisfy when again employing . Now, we simply return the factor via , to arrive at:\n\nThis is understood to be the velocity operator in the Newton–Wigner representation. Because:\n\nit is commonly thought that the zitterbewegung motion arising out of vanishes when a fermion is transformed into the Newton–Wigner representation.\n\nNow, let us compare equations and for a fermion at rest, defined earlier in section 3 as a fermion for which . Here, () remains:\n\nwhile becomes:\n\nIn equation we found that for a fermion at rest, for any operator. One would expect this to include:\n\nhowever, equations and for a fermion appear to contradict .\n\nThe powerful machinery of the Foldy–Wouthuysen transform originally developed for the Dirac equation has found applications in many situations such as acoustics, and optics.\n\nIt has found applications in very diverse areas such as atomic systems synchrotron radiation and derivation of the Bloch equation for polarized beams.\n\nThe application of the Foldy–Wouthuysen transformation in acoustics is very natural; comprehensive and mathematically rigorous accounts.\n\nIn the traditional scheme the purpose of expanding the optical Hamiltonian\n\nin a series using\n\nas the expansion parameter is to understand the propagation of the quasi-paraxial beam in terms of a series of approximations (paraxial plus nonparaxial). Similar is the situation in the case of charged-particle optics. Let us recall that in relativistic quantum mechanics too one has a similar problem of understanding the relativistic wave equations as the nonrelativistic approximation plus the relativistic correction terms in the quasi-relativistic regime. For the Dirac equation (which is first-order in time) this is done most conveniently using the Foldy–Wouthuysen transformation leading to an iterative diagonalization technique. The main framework of the newly developed formalisms of optics (both light optics and charged-particle optics) is based on the transformation technique of Foldy–Wouthuysen theory which casts the Dirac equation in a form displaying the different interaction terms between the Dirac particle and an applied electromagnetic field in a nonrelativistic and easily interpretable form.\n\nIn the Foldy–Wouthuysen theory the Dirac equation is decoupled through a canonical transformation into two two-component equations: one reduces to the Pauli equation in the nonrelativistic limit and the other describes the negative-energy states. It is possible to write a Dirac-like matrix representation of Maxwell's equations. In such a matrix form the Foldy–Wouthuysen can be applied.\n\nThere is a close algebraic analogy between the Helmholtz equation (governing scalar optics) and the Klein–Gordon equation; and between the matrix form of the Maxwell's equations (governing vector optics) and the Dirac equation. So it is natural to use the powerful machinery of standard quantum mechanics (particularly, the Foldy–Wouthuysen transform) in analyzing these systems.\n\nThe suggestion to employ the Foldy–Wouthuysen Transformation technique in the case of the Helmholtz equation was mentioned in the literature as a remark.\n\nIt was only in the recent works, that this idea was exploited to analyze the quasiparaxial approximations for specific beam optical system. The Foldy–Wouthuysen technique is ideally suited for the Lie algebraic approach to optics. With all these plus points, the powerful and ambiguity-free expansion, the Foldy–Wouthuysen Transformation is still little used in optics. The technique of the Foldy–Wouthuysen Transformation results in what is known as nontraditional prescriptions of Helmholtz optics and Maxwell optics respectively. The nontraditional approaches give rise to very interesting wavelength-dependent modifications of the paraxial and aberration behaviour. The nontraditional formalism of Maxwell optics provides a unified framework of light beam optics and polarization. The nontraditional prescriptions of light optics are closely analogous with the quantum theory of\ncharged-particle beam optics. In optics, it has enabled the deeper connections in the wavelength-dependent regime between light optics and charged-particle optics to be seen (see Electron optics).\n\n"}
{"id": "38575383", "url": "https://en.wikipedia.org/wiki?curid=38575383", "title": "Gansu Dunhuang Solar Park", "text": "Gansu Dunhuang Solar Park\n\nThe Gansu Dunhuang Solar Park is a 50-megawatt (MW) photovoltaic power station located in the Gansu Province, in China. All of the modules, which range from 230 to 240 watts, are mounted at a fixed tilt angle of 38°. It is located in the \"Photoelectricity Park of Dunhuang City\". China's first solar power plant, 10 MW, was built here and commissioned on 30 September 2009. An additional 95 MW is expected in 2013, and 5,000 MW by 2020.\n\n"}
{"id": "22171568", "url": "https://en.wikipedia.org/wiki?curid=22171568", "title": "Goldwind", "text": "Goldwind\n\nGoldwind (parent company Xinjiang Goldwind Science & Technology Co., Ltd.) is a Chinese wind turbine manufacturer headquartered in Beijing, China. Goldwind was the largest wind turbine manufacturer globally (by installations) in 2015. As of 2016, it was ranked 3rd for onshore and also 3rd for offshore turbine manufacturing by Bloomberg New Energy Finance.\n\nIn 2008, Goldwind purchased their PMDD technology from a German company called Vensys. Goldwind had 1,500 employees in 2009. It was the second-largest Chinese wind turbine manufacturer by 2011 market share and the third-largest in the world. In 2015, Goldwind became the largest turbine manufacturer in the world.\n\nWith offices and facilities throughout Asia, Europe and the Americas, Goldwind employs over 6,000 staff and combined, has an installed capacity base of 38 gigawatts (GW) across six continents. Goldwind is principally engaged in researching, developing, manufacturing and marketing large-sized wind turbine generator sets. The company's major products include 1.5MW, 2.0MW, 2.5MW, and 3MW(S) Permanent Magnet Direct-Drive (PMDD) wind turbine generators. The company also provides wind power technology services, investment and sale of wind power projects and technology transfer service.\n\nGoldwind Americas, a wholly owned subsidiary of Xinjiang Goldwind Science & Technology, Co., Ltd., is based in Chicago, IL and currently employs approximately 100 individuals. Goldwind Americas (Goldwind USA, Inc.) has installed approximately 500 MW of PMDD turbines. In 2016 and 2017, Goldwind acquired two Texas projects totaling a combined capacity of 300 MW. Once constructed, the Rattlesnake Wind Project and Heart of Texas Wind Project will be Goldwind's largest U.S. projects to date. In 2017, Goldwind obtained $140m in tax equity financing commitments from a unit of Berkshire Hathaway Energy and Citi for its 160MW Rattlesnake wind project in central Texas – easily the most financing of US origin yet secured by a Chinese turbine OEM in the U.S. market. Goldwind Americas also manages and operates the company's Panamanian team, which oversees the Penonome I and Penonome II wind farms totaling 270 MW, and the Chilean service operations team.\n\nFor the financial year ended 31 December 2015, revenue from operations for the Group was RMB29,846. 00 million, representing an increase of 69.84% compared with RMB17,572.60 million for the financial year ended 31 December 2014. Net profit attributable to owners of the Company was RMB2,849.50 million, representing an increase of 55.74% compared with RMB1,829.68 million for the financial year ended 31 December 2014. The Group reported basic earnings per share of RMB1.05.\n\nGoldwind turbines are exclusively PMDD machines.\n\nGoldwind's initial use of the PMDD fully converted design came through its partner and eventual subsidiary VENSYS, with the VENSYS 62 which has been in operation since 2004.\n\nPMDD Models above 1000 kW\n\nPrevious Generation Models below 1000 kW\n\n\n"}
{"id": "42662963", "url": "https://en.wikipedia.org/wiki?curid=42662963", "title": "Hatgyi Dam", "text": "Hatgyi Dam\n\nThe Hatgyi Dam () is a planned hydropower gravity dam to be constructed on the Salween River in southeastern Myanmar. The dam site is in Myanmar’s Karen State. The dam project is being funded by the Electricity Generating Authority of Thailand (EGAT), China's Sinohydro Corporation, and Myanmar's Ministry of Electric Power and International Group of Entrepreneurs (IGE). The dam is expected to produce 1,100 to 1,500 megawatts of power, the majority of which will be sent to Thailand.\n\nNumerous environmental groups and local villagers have decried the project’s anticipated ecological and social impact. Criticisms have targeted the project's alleged lack of transparency, damage to the environment, disruption of peace negotiations between the Karen National Union and the Myanmar Army, and impact on local communities, including displacement of local populations.\n\nThe dam site is in the Myaing Gyi Ngu area of the Hlaing Bwe Township in Myanmar's Karen State. It is 33 kilometers downstream of the confluence of the Moei and Salween Rivers, and about 47 kilometers from the Thai-Myanmar border. It is opposite Mae Hong Son Province in northern Thailand. The portion of the Salween at which the dam will be built features particularly strong rapids, which in the dry season become a waterfall. The dam site is also within the Kahilu Wildlife Sanctuary.\n\nThe areas surrounding the Hatgyi Dam site are ethnically diverse and ecologically rich. At least 13 different indigenous groups live along the Salween river, and over ten million people are supported by the Salween River basin. The Salween's waters are nutrient-rich and support local vegetation and farmlands.\n\nThe dam is being funded by the Electricity Generating Authority of Thailand (EGAT), a Thai government owned company which has built dams since 1964, China's Sinohydro Corporation, and Myanmar's Ministry of Electric Power and International Group of Entrepreneurs (IGE), with Sinohydro holding the majority of shares, EGAT being the second largest shareholder, and Myanmar holding the fewest shares. IGE is owned by Nay Aung, son of Aung Thaung, the central advisor for Myanmar's Union Solidarity and Development Party.\n\nThe majority of the generated energy will be sent to Thailand, which is looking to secure cheap energy supplies to replace its high reliance on natural gas. Currently, 70 percent of Thailand's energy comes from natural gas, the supply of which is expected to last only thirty years.\n\nThe project began in 1998 with a feasibility study titled, \"Preliminary Feasibility Study of Hutgyi Hydropower Project in the Union of Myanmar\". The study recommended the construction of a \"a low height, run-of-river dam having a capacity of 300 MW\". In November 2005, however, Thailand's energy minister cited a newer feasibility which stated that the dam could handle a capacity of up to 1,200 megawatts of power.\n\nIn 2004, survey work began on the project. However, when two EGAT employees were killed in May of that year, one from a grenade and the other from a landmine, the project was halted. In December 2005, in preparation for a series of dam investments along the Salween River, a memorandum of understanding (MOU) was signed between Myanmar's Hydro Electric Power Department and Thailand's EGAT. In June of the following year, another memorandum of understanding was signed, which this time included Sinohydro, in addition to Myanmar and EGAT. Shortly thereafter, work commenced on road repairs near the dam site. In these MOUs, Myanmar and Thailand agreed to keep all data and joint studies on the dam \"strictly confidential\".\n\nWork on the project once again stalled when an EGAT staff member was killed in 2007. Work resumed on the project in 2008. Looking at the concurrent developments in the region's ethnic conflicts, the Karen National Liberation Army Peace Council (KNLAPC), which was founded by the commander of the Karen National Liberation Army’s 7th Brigade, signed a peace agreement with Myanmar's State Peace and Development Council (SPDC) in February 2007.\n\nIn 2008, another MOU was signed, this time between Myanmar and Sinohydro, which established Sinohydro as the majority shareholder in the dam project. June 2009 saw another uptick in violence, as the DKBA and the army began an offensive against the KNU. Because of this offensive, around 4,000 Karen villagers were forced to flee their homes. This was the largest refugee influx from Karen to Thailand in over a decade.\n\nAcross the border during the same year, Thailand's Office of the Prime Minister formed a subcommittee to look into and monitor the potential human rights impacts of the Hatgyi project. In 2010, the subcommittee recommended more extensive studies on the local impacts of the dam project, focusing especially on the impact of the project on Thailand. The recommendation claimed the existing environmental impact assessment (EIA) did not focus enough on the implications of the project for Thailand. Nothing ever came of these recommendations, however.\n\nSince the 1990s, when it broke off from the Christian-dominated Karen National Union, the Democratic Karen Buddhist Army had been cooperating with the Myanmar Army. However, in 2010, Saw La Pwe formed an anti-government breakaway faction of the DKBA.\n\nPublic hearings regarding the construction of the dam were held in the Sop Moei District of northern Thailand in 2011. Attendance was high. In December of that year, the Karen National Union reluctantly agreed to allow further surveys to be conducted for the dam. In January of the following year, a temporary ceasefire was signed between the national army and the KNU. Only two months later, however, there was a noticeable build-up of army troops in the area around the dam site.\n\nAround this time, Jin Honggen, economic and commercial counselor at China's Myanmar embassy, related the concern of many Chinese investors at the lack of progress in a variety of investment projects, including the Hatgyi Dam. He stated that, \"some Chinese companies in Myanmar, especially those investing in resource fields, are worrying their interests can't be secured\".\n\nIn late-February 2013, Myanmar's deputy minister of electric power told parliament that the Hatgyi Dam was one of six dams whose construction had been approved. Less than two months later, in April, local villagers first reported visible evidence of the project getting underway, claiming that EGAT was setting up water measurement posts at the Hatgyi Dam site. At the same time, Myanmar's military presence in the area was as high as ever, as witnesses reported the presence of eight army battalions near the dam site. Meanwhile, the DKBA was forced to evacuate the dam area in May. As of September 2014, troop deployments to the dam site continued in order to increase security.\n\nConcerns about the impact of the dam on local populations have largely revolved around the displacement of numerous villages due to flooding, and the lack of involvement of local communities in the decision-making process.\n\nIn 2013, the Karen National Union explained to local villagers that nine Karen villages would be flooded and damaged as a result of the Hatgyi Dam. Karen Rivers Watch (KRW) pegs the number of displaced villages at 21. EJ Atlas estimates the potential affected population to range from 1,000 to 30,000 local residents, with listed impacts including displacement, loss of livelihood, and loss of culture. Saw Kyaw Phoe of Mae Par Village explained: \"If the dam is built, our village, the whole area, including our paddy farms and our gardens will be flooded. I, myself, will have no place to live.\"\n\nAn open letter written by Karen Rivers Watch in 2011 stated that the dam would only exacerbate problems associated with the displacement of war-torn communities, with very little benefit actually being received by the local community, as the majority of the power generated by the dam would be exported.\n\nPaul Sein Twa, director of the Karen Environmental and Social Action Network (KESAN) pointed out that, \"Local people do not want any dams on the Salween River, especially in Karen State, without the free, prior and informed consent of impacted communities.\" Along those lines, the aforementioned KRW open letter expressed that there had been \"little consultation with affected communities\" and that the majority of the local populations opposed the dam.\n\nThe higher military presence in the area associated with the construction and maintenance of the Hatgyi Dam has also been a source of concern and criticism. Naw Paw Gay, of the KRW, explained that, \"the presence of the Burmese troops will lead to more human rights abuses.\" Saw Kyaw Phoe, meanwhile, while commenting on the negative implications of increased troops in the area to protect the dam, related that the Myanmar army would, \"force villagers to carry their food ration and ammunition and to lead Burmese troops through land mined areas.\"\n\nInternational Rivers claims that the dam project has been obstructing peace negotiations between the Karen National Union and the Myanmar Army. Likewise, the Democratic Karen Buddhist Army's tactical operations commander Colonel Saw Maung Kyar explained that, \"problems will get worse if [the government] starts increasing the military forces by using the dam as an excuse.\" Meanwhile, the joint secretary of the Karen National Union, P'Doh Saw That Thi Bwe, argued that, \"the Hatgyi hydropower project is located in our land. Our stand is not to allow such kind of big project until we have a guarantee for discussing politics.\"\n\nEnvironmental researcher Steve Thompson of the Karen Environmental and Social Action Network (KESAN) commented on the Hatgyi Dam by explaining that the dam could, \"wipe out many or most riverine fish species, with serious socio-economic consequences for local villagers\". An open letter by the Karen Rivers Watch in 2011 described the Hatgyi Dam site as being in an \"ecologically sensitive area\", close to an active fault line. EJ Atlas listed environmental impacts as including flooding, biodiversity loss, soil, pollution, and deforestation.\n"}
{"id": "28397633", "url": "https://en.wikipedia.org/wiki?curid=28397633", "title": "Ishida (company)", "text": "Ishida (company)\n\nIshida began manufacturing weighing scales in 1893 in Kyoto under authorization of Japan’s first weights and measures license. In 1948 the company was founded under the name Ishida Scales Mfg. Co. Ltd. by Otokichi II Ishida. In 1972 it invented the multihead weigher and first exported it in 1978. In 1985, the first international subsidiary, Ishida Europe, was founded, followed by subsidiaries in Brazil, Malaysia, China, Thailand, Canada, Korea, Vietnam, Indonesia and India. In 1993 the Company was renamed Ishida. On 19 May 2010, the position of president saw its fourth handover to Takahide Ishida from his father Ryuichi Ishida.\n\n\nIshida today manufactures entire line solutions for food packing. With installations worldwide of 35,000 units, it remains the world brand leader in multihead weighers. Its range now includes screw feeder weighers and fresh food weighers for sticky and oily products. Ishida also manufactures cut-gate weighers, for granules and powders, machines for separating stacks of food trays (denesters), tray sealers, snack food bagmakers and other packaging line equipment such as conveyors and gantries. Inspection equipment includes checkweighers, X-ray systems, seal testers and vision systems. Ishida also produces software for packing line efficiency optimisation. It remains a manufacturer of scales for retail outlets.\n\nThe first subsidiary company was the UK based Ishida Europe in 1985, which currently operates as the European headquarters. The Americas are served by subsidiaries in the USA, Canada and Brazil and there are subsidiaries in the Middle East and South Africa. Ishida has subsidiaries throughout Asia, including India, China, Korea, Thailand and Vietnam. Manufacturing facilities are currently in the UK, Japan, China, Korea, India and Brazil. Ishida equipment is distributed in Australia primarily by Heat and Control, and in New Zealand by Gilbarco on the retail side and Heat and Control for the manufacturing equipment.\n"}
{"id": "633006", "url": "https://en.wikipedia.org/wiki?curid=633006", "title": "James Young (chemist)", "text": "James Young (chemist)\n\nJames Young (13 July 1811 – 13 May 1883) was a Scottish chemist best known for his method of distilling paraffin from coal and oil shales. He is often referred to as Paraffin Young.\n\nJames Young was born in the Drygate area of Glasgow, the son of John Young, a cabinetmaker and joiner. He became his father's apprentice at an early age and educated himself at night school, attending evening classes at the nearby Anderson's College (now Strathclyde University) from the age of 19. At Anderson's College he met Thomas Graham, who had just been appointed as a lecturer on chemistry. In 1831 Young was appointed as Graham's assistant and occasionally took some of his lectures. While at Anderson's College he also met and befriended the explorer David Livingstone; this friendship continued until Livingstone's death in Africa many years later. \n\nOn 21 August 1838 he married Mary Young of Paisley middle parish; in 1839 they moved to Lancashire.\n\nIn Young's first scientific paper, dated 4 January 1837, he described a modification of a voltaic battery invented by Michael Faraday. Later that same year he moved with Graham to University College, London where he helped him with experimental work.\n\nIn 1839 Young was appointed manager at James Muspratt's chemical works Newton-le-Willows, near St Helens, Merseyside, and in 1844 to Tennants, Clow & Co. at Manchester, for whom he devised a method of making sodium stannate directly from cassiterite.\n\nIn 1845 he served on a committee of the Manchester Literary and Philosophical Society for the investigation of potato blight, and suggested immersing the potatoes in dilute sulphuric acid as a means of combatting the disease; he was not elected a member of the Society until 19 October 1847. Finding the \"Manchester Guardian\" newspaper insufficiently liberal, he also began a movement for the establishment of the \"Manchester Examiner\" newspaper which was first published in 1846.\n\nIn 1847 Young had his attention called to a natural petroleum seepage in the Riddings colliery at Alfreton, Derbyshire from which he distilled a light thin oil suitable for use as lamp oil, at the same time obtaining a thicker oil suitable for lubricating machinery.\n\nIn 1848 Young left Tennants', and in partnership with his friend and assistant Edward Meldrum, set up a small business refining the crude oil. The new oils were successful, but the supply of oil from the coal mine soon began to fail (eventually being exhausted in 1851). Young, noticing that the oil was dripping from the sandstone roof of the coal mine, theorised that it somehow originated from the action of heat on the coal seam and from this thought that it might be produced artificially.\n\nFollowing up this idea, he tried many experiments and eventually succeeded in producing, by distilling cannel coal at a low heat, a fluid resembling petroleum, which when treated in the same way as the seep oil gave similar products. Young found that by slow distillation he could obtain a number of useful liquids from it, one of which he named \"paraffine oil\" because at low temperatures it congealed into a substance resembling paraffin wax.\n\nThe production of these oils and solid paraffin wax from coal formed the subject of his patent dated 17 October 1850. In 1850 Young & Meldrum and Edward William Binney entered into partnership under the title of E.W. Binney & Co. at Bathgate in West Lothian and E. Meldrum & Co. at Glasgow; their works at Bathgate were completed in 1851 and became the first truly commercial oil-works in the world, using oil extracted from locally mined torbanite, lamosite, and bituminous coal to manufacture naphtha and lubricating oils; paraffin for fuel use and solid paraffin were not sold till 1856.\n\nIn 1852 Young left Manchester to live in Scotland and that same year took out a US patent for the production of paraffin oil by distillation of coal. Both the US and UK patents were subsequently upheld in both countries in a series of lawsuits and other producers were obliged to pay him royalties.\n\nIn 1865 Young bought out his business partners and built second and larger works at Addiewell, near West Calder. It was a substantial industrial complex, in its time one of the largest chemical works in Scotland. In 1866 Young sold the concern to Young's Paraffin Light and Mineral Oil Company. Although Young remained in the company, he took no active part in it, instead withdrawing from business to occupy himself with yachting, travelling, scientific pursuits, and looking after the estates which he had purchased.\n\nThe company continued to grow and expanded its operations, selling paraffin oil and paraffin lamps all over the world and earning for its founder the nickname \"Paraffin\" Young. Addiewell remained the centre of operations for Young's Paraffin Light and Mineral Oil Co. Ltd., but as local supplies of shale became exhausted, activities were increasingly focussed on other shale-fields. The refinery closed around 1921. Other companies worked under license from Young's firm, and paraffin manufacture spread over the south of Scotland.\n\nWhen the reserves of torbanite eventually gave out the company moved on to pioneer the exploitation of West Lothian's oil shale (lamosite) deposits, not so rich in oil as cannel coal and torbanite. By the 1900s nearly 2 million tons of shale were being extracted annually, employing 4,000 men.\n\n\n\nYoung's wife died, and by 1871 he had moved with his children to Kelly House, near Wemyss Bay in the district of Inverkip. The 1881 census record shows him living with his son and daughter at this estate. Young died at the age of 71 in his home on 16 May 1883, in the presence of his son James. He was buried at Inverkip churchyard.\n\n\n\n"}
{"id": "39354806", "url": "https://en.wikipedia.org/wiki?curid=39354806", "title": "Laleli Dam", "text": "Laleli Dam\n\nThe Laleli Dam is an embankment dam, currently under construction near the town of Laleli on the Çoruh River in Erzurum Province, Turkey. The primary purpose of the dam is hydroelectric power production. After a prolonged legal battle, construction on the dam began in 2013 and it is expected to be complete in 2017. The dam, which will power a 99 MW power station, will also flood several villages to include Laleli. The dam's reservoir will stretch east into Bayburt Province.\n"}
{"id": "16156519", "url": "https://en.wikipedia.org/wiki?curid=16156519", "title": "Melex", "text": "Melex\n\nMelex is an electric vehicle produced by a company of the same name in Mielec, Poland. It has been in production since 1971.\n\nThe car, having full necessary equipment such as headlamps and seatbelts, is very quiet and does not pollute the environment, so it is usually used in places where standard cars are not allowed. It is also smaller than them, but slower (the maximum speed is about 30 km/h, depending on the version). Some versions require having a driver's license.\n\nAt first, the car was in use as a transport vehicle for golf players, but now it has many versions that include:\n\nIts typical use is to travel in quiet places, such as parks, or cemeteries, or in factories to transport people or things over short distances, where speed is not necessary\n\nThe standard Melex has no doors and can carry two people (a driver and a passenger). It can contain more seats for more passengers and/or baggage.\n\nIt is equipped with a battery and rectifier which allow easy recharging.\n\n"}
{"id": "28446647", "url": "https://en.wikipedia.org/wiki?curid=28446647", "title": "Metallurgical failure analysis", "text": "Metallurgical failure analysis\n\nMetallurgical failure analysis is the process by which a metallurgist determines the mechanism that has caused a metal component to fail. Typical failure modes involve various types of corrosion and mechanical damage. It has been estimated that the direct annual [cost of corrosion] alone in the United States was $276 billion, approximately 3.1% of GDP, in 1998. Corrosion costs have continued to skyrocket and total corrosion costs now are greater than $1 trillion annually in the United States as of 2012.\n\nMetal components fail as a result of the environmental conditions to which they are exposed to as well as the mechanical stresses that they experience. Often a combination of both environmental conditions and stress will cause failure.\n\nMetal components are designed to withstand the environment and stresses that they will be subjected to. The design of a metal component involves not only a specific elemental composition but also specific manufacturing process such as heat treatments, machining processes, etc.… The huge arrays of different metals that result all have unique physical properties.\n\nAnalysis of a failed part can be done using destructive testing or non-destructive testing. Destructive testing involves removing a metal component from service and sectioning the component for analysis. Destructive testing gives the failure analyst the ability to conduct the analysis in a laboratory setting and perform tests on the material that will ultimately destroy the component. Non destructive testing is a test method that allows certain physical properties of metal to be examined without taking the samples completely out of service. NDT is generally used to detect failures in components before the component fails catastrophically.\n\nThere is no standardized list of metallurgical failure modes and different metallurgists might use a different name for the same failure mode. The Failure Mode Terms listed below are those accepted by ASTM, ASM, and/or NACE as distinct metallurgical failure mechanisms.\n\n\n\n"}
{"id": "3498129", "url": "https://en.wikipedia.org/wiki?curid=3498129", "title": "Multipurpose tree", "text": "Multipurpose tree\n\nMultipurpose trees are trees that are deliberately grown and managed for more than one output. They may supply food in the form of fruit, nuts, or leaves that can be used as a vegetable; while at the same time supplying firewood, add nitrogen to the soil, or supply some other combination of multiple outputs. \"Multipurpose tree\" is a term common to agroforestry, particularly when speaking of tropical agroforestry where the tree owner is a subsistence farmer.\n\nWhile all trees can be said to serve several purposes, such as providing habitat, shade, or soil improvement; multipurpose trees have a greater impact on a farmer’s well being because they fulfill more than one basic human need. In most cases multipurpose trees have a primary role; such as being part of a living fence, or a windbreak, or used in an ally cropping system. In addition to this they will have one or more secondary roles, most often supplying a family with food or firewood, or both.\n\nWhen a multipurpose tree is planted, a number of needs and functions can be fulfilled at once. They may be used as a windbreak, while also supplying a staple food for the owner. They may be used as fencepost in a living fence, while also being the main source of firewood for the owner. They may be intercropped into existing fields, to supply nitrogen to the soil, and at the same time serve as a source of both food and firewood.\n\nCommon multipurpose trees of the tropics include:\n\nIdeally most trees found on tropical farms should be multipurpose, and provide more to the farmer than simply shade and firewood. In most cases they should be nitrogen fixing legumes, or trees that greatly increase the farmer's food security.\n"}
{"id": "2654679", "url": "https://en.wikipedia.org/wiki?curid=2654679", "title": "National Grid plc", "text": "National Grid plc\n\nNational Grid plc is a British multinational electricity and gas utility company headquartered in Warwick, United Kingdom. Its principal activities are in the United Kingdom and Northeastern United States. It has a primary listing on the London Stock Exchange, and is a constituent of the FTSE 100 Index. It has a secondary listing on the New York Stock Exchange.\n\nBefore 1990, both the generation and transmission activities in England and Wales were under the responsibility of Central Electricity Generating Board (CEGB). The present electricity market in the United Kingdom was built upon the breakup of the CEGB into four separate companies in the 1990s.\n\nIts generation (or upstream) activities were transferred to three generating companies—PowerGen, National Power, and Nuclear Electric (later British Energy, eventually EDF Energy)—and its transmission (or downstream) activities to the National Grid Company.\n\nIn 1990, the transmission activities of the CEGB were transferred to the National Grid Company plc, which was owned by the twelve regional electricity companies (RECs) through a holding company, National Grid Group plc. The company was first listed on the London Stock Exchange in 1995.\n\nWith the beginning of the new millennium, National Grid pursued some major mergers and acquisitions. In 2000, National Grid Group acquired New England Electric System and Eastern Utilities Associates. In January 2002, National Grid Group acquired Niagara Mohawk Power Corporation, a New York State utility.\n\nIn October 2002, National Grid Group merged with Lattice Group plc, owner of Transco—the gas distribution business (Lattice demerged from BG Group in 2000). National Grid Group changed its name to National Grid Transco plc. In August 2004, National Grid Transco agreed to sell four of its regional gas distribution networks. The total cash consideration was £5.8 billion. NGT kept ownership of four other distribution networks, which comprise almost half of Great Britain's gas distribution network. In July 2005, National Grid Transco was renamed National Grid plc. On 26 July 2005, National Grid Company was renamed National Grid Electricity Transmission plc, and on 10 October 2005, Transco was renamed National Grid Gas plc.\n\nIn February 2006, National Grid announced that it had agreed to buy KeySpan Corporation, a gas distributor and electricity producer in the United States, for $7.3bn (£4.1bn) in cash. Around the same time, National Grid also announced the acquisition of the Southern Union Company subsidiary, New England Gas Company in Rhode Island. The acquisitions of the two natural gas delivery companies materially doubled the size of National Grid's American subsidiary, creating the second largest utility in the United States with more than 8 million customers. The acquisition of KeySpan was completed on 24 August 2007 following government and regulatory approval and endorsement by the shareholders of the two companies.\n\nIn May 2007, National Grid formed a joint venture with the Dutch transmission operator TenneT for a 1,000 MW BritNed DC link between the Isle of Grain in Kent and Maasvlakte, near Rotterdam. The installation of the first section of cable link started on 11 September 2009, whilst the entire cable was completed in October 2010. The interconnection became operational on 1 April 2011, and by January 2012, electricity flow had mostly been from the Netherlands to the United Kingdom. The BritNed interconnection would serve as a vital link for the foreseeable European super grid project.\n\nIn spring 2011, National Grid sold off its services in New Hampshire, after their request to increase gas and electric rates was denied.\n\nIn November 2015, it was announced that Steve Holliday, the CEO for ten years, would leave in March 2016 and that John Pettigrew, its executive director who joined National Grid 25 years ago, would succeed him. In June 2016, the Energy Select Committee argued that the company faced too many conflicts of interest, particularly with regard to its ownership of international interconnectors. The committee proposed that the company should be split up.\n\nIn December 2016, National Grid agreed to sell a 61% stake in its gas distribution business, to a consortium of Macquarie Infrastructure and Real Assets, Allianz Capital Partners, Hermes Investment Management, CIC Capital Corporation, Qatar Investment Authority, Dalmore Capital and Amber Infrastructure Limited, with a further 14% stake under negotiation. The sale was completed on 31 March 2017, following clearance by the European Commission, and the resulting company was named Cadent Gas.\n\nIn November 2017, \"Daily Energy Insider\" reported that National Grid signed an agreement with the Department of Energy's Pacific Northwest National Laboratory (PNNL). The agreement has National Grid and PNNL partner with each other to modernize the electric grid and add resiliency to the system. The agreement also focuses efforts on cyber security, distributed energy resources, advanced transmission network controls and \"grid scale energy storage.\"\n\n\n\nContract negotiations with Massachusetts gas workers represented by the United Steelworkers broke down in June 2018 and the company locked out more than 1,000 employees, cutting off healthcare and pay.\n\nOn April 2, 1990, the natural gas lines serving homes in Danvers, Massachusetts were accidentally over-pressurized by a Boston Gas Co. worker. This resulted in fires and explosions along Lafayette St., Maple St., Venice St. and Beaver Park Av. Six people had to be treated for injuries.\n\n\n"}
{"id": "11123364", "url": "https://en.wikipedia.org/wiki?curid=11123364", "title": "Natural gas storage", "text": "Natural gas storage\n\nNatural gas, like many other commodities, can be stored for an indefinite period of time in natural gas storage facilities for later consumption.\n\nGas storage is principally used to meet load variations. Gas is injected into storage during periods of low demand and withdrawn from storage during periods of peak demand. It is also used for a variety of secondary purposes, including:\n\n\nCharacteristics of underground storage facilities need to be defined and measured. A number of volumetric measures have been put in place for that purpose:\n\nThe measurements above are not fixed for a given storage facility. For example, deliverability depends on several factors including the amount of gas in the reservoir and the pressure etc. Generally, a storage facility’s deliverability rate varies directly with the total amount of gas in the reservoir. It is at its highest when the reservoir is full and declines as gas is withdrawn. The injection capacity of a storage facility is also variable and depends on factors similar to those that affect deliverability. The injection rate varies inversely with the total amount of gas in storage. It is at its highest when the reservoir is nearly empty and declines as more gas is injected. The storage facility operator may also change operational parameters. This would allow, for example, the storage capacity maximum to be increased, the withdrawal of base gas during very high demand or reclassifying base gas to working gas if technological advances or engineering procedures allow.\n\nThe most important type of gas storage is in underground reservoirs. There are three principal types — depleted gas reservoirs, aquifer reservoirs and salt cavern reservoirs. Each of these types has distinct physical and economic characteristics which govern the suitability of a particular type of storage type for a given application.\n\nThese are the most prominent and common form of underground storage. They are the reservoir formations of natural gas fields that have produced all their economically recoverable gas. The depleted reservoir formation is readily capable of holding injected natural gas. Using such a facility is economically attractive because it allows the re-use, with suitable modification, of the extraction and distribution infrastructure remaining from the productive life of the gas field which reduces the start-up costs. Depleted reservoirs are also attractive because their geological and physical characteristics have already been studied by geologists and petroleum engineers and are usually well known. Consequently, depleted reservoirs are generally the cheapest and easiest to develop, operate, and maintain of the three types of underground storage.\n\nIn order to maintain working pressures in depleted reservoirs, about 50 percent of the natural gas in the formation must be kept as cushion gas. However, since depleted reservoirs were previously filled with natural gas and hydrocarbons, they do not require the injection of gas that will become physically unrecoverable as this is already present in the formation. This provides a further economic boost for this type of facility, particularly when the cost of gas is high.\nTypically, these facilities are operated on a single annual cycle; gas is injected during the off-peak summer months and withdrawn during the winter months of peak demand.\n\nA number of factors determine whether or not a depleted gas field will make an economically viable storage facility. Geographically, depleted reservoirs should be relatively close to gas markets and to transportation infrastructure (pipelines and distribution systems) which will connect them to that market. Since the fields were at one time productive and connected to infrastructure, distance from market is the dominant geographical factor. Geologically, it is preferred that depleted reservoir formations have high porosity and permeability. The porosity of the formation is one of the factors that determines the amount of natural gas the reservoir is able to hold. Permeability is a measure of the rate at which natural gas flows through the formation and ultimately determines the rate of injection and withdrawal of gas from storage.\n\nAquifers are underground, porous and permeable rock formations that act as natural water reservoirs. In some cases they can be used for natural gas storage. Usually these facilities are operated on a single annual cycle as with depleted reservoirs. The geological and physical characteristics of aquifer formation are not known ahead of time and a significant investment has to go into investigating these and evaluating the aquifer’s suitability for natural gas storage.\n\nIf the aquifer is suitable, all of the associated infrastructure must be developed from scratch, increasing the development costs compared to depleted reservoirs. This includes installation of wells, extraction equipment, pipelines, dehydration facilities, and possibly compression equipment. Since the aquifer initially contains water there is little or no naturally occurring gas in the formation and of the gas injected some will be physically unrecoverable. As a result, aquifer storage typically requires significantly more cushion gas than depleted reservoirs; up to 80% of the total gas volume. Most aquifer storage facilities were developed when the price of natural gas was low, meaning this cushion gas was inexpensive to sacrifice. With rising gas prices aquifer storage becomes more expensive to develop.\n\nA consequence of the above factors is that developing an aquifer storage facility is usually time consuming and expensive. Aquifers are generally the least desirable and most expensive type of natural gas storage facility.\n\nUnderground salt formations are well suited to natural gas storage. Salt caverns allow very little of the injected natural gas to escape from storage unless specifically extracted. The walls of a salt cavern are strong and impervious to gas over the lifespan of the storage facility.\n\nOnce a salt feature is discovered and found to be suitable for the development of a gas storage facility a cavern is created within the salt feature. This is done by the process of solution mining. Fresh water is pumped down a borehole into the salt. Some of the salt is dissolved leaving a void and the water, now saline, is pumped back to the surface. The process continues until the cavern is the desired size. Once created, a salt cavern offers an underground natural gas storage vessel with very high deliverability. Cushion gas requirements are low, typically about 33 percent of total gas capacity.\n\nSalt caverns are usually much smaller than depleted gas reservoir and aquifer storage facilities. A salt cavern facility may occupy only one one-hundredth of the area taken up by a depleted gas reservoir facility. Consequently, salt caverns cannot hold the large volumes of gas necessary to meet base load storage requirements. Deliverability from salt caverns is, however, much higher than for either aquifers or depleted reservoirs. This allows the gas stored in a salt cavern to be withdrawn and replenished more readily and quickly. This quick cycle-time is useful in emergency situations or during short periods of unexpected demand surges.\n\nAlthough construction is more costly than depleted field conversions when measured on the basis of dollars per thousand cubic feet of working gas, the ability to perform several withdrawal and injection cycles each year reduces the effective cost.\n\nThere are also other types of storage such as:\n\nLNG facilities provide delivery capacity during peak periods when market demand exceeds pipeline deliverability. LNG storage tanks possess a number of advantages over underground storage. As a liquid at approximately −163 °C (−260 °F), it occupies about 600 times less space than gas stored underground, and it provides high deliverability at very short notice because LNG storage facilities are generally located close to market and can be trucked to some customers avoiding pipeline tolls. There is no requirement for cushion gas and it allows access to a global supply. LNG facilities are, however, more expensive to build and maintain than developing new underground storage facilities.\nGas can be temporarily stored in the pipeline system itself, through a process called line packing. This is done by packing more gas into the pipeline by an increase in the pressure. During periods of high demand, greater quantities of gas can be withdrawn from the pipeline in the market area, than is injected at the production area. The process of line packing is usually performed during off peak times to meet the next day’s peaking demands. This method, however, only provides a temporary short-term substitute for traditional underground storage.\n\nGas can be stored above ground in a gasholder (or gasometer), largely for balancing, not long-term storage, and this has been done since Victorian times. These store gas at district pressure, meaning that they can provide extra gas very quickly at peak times. Gasholders are perhaps most used in the United Kingdom and Germany. \nThere are two kinds of gasholder — column-guided, which are guided up by a large frame that is always visible, regardless of the position of the holder; and spiral-guided, which have no frame and are guided up by concentric runners in the previous lift.\n\nPerhaps the most famous British gasholder is the large column-guided \"Oval gasholders\" that overlooks The Oval cricket ground in London. Gasholders were built in the United Kingdom from early Victorian times; many, such as Kings Cross in London and St. Marks Street in Kingston upon Hull are so old that they are entirely riveted, as their construction predates the use of welding in construction. The last to be built in the UK was in 1983.\n\nInterstate pipeline companies rely heavily on underground storage to perform load balancing and system supply management on their long-haul transmission lines. FERC regulations though demand that these companies open up the remainder of their capacity not used for that purpose to third parties. Twenty-five interstate companies currently operate 172 underground natural gas storage facilities. In 2005, their facilities accounted for about 43 percent of overall storage deliverability and 55 percent of working gas capacity in the US. These operators include the Columbia Gas Transmission Company, Dominion Gas Transmission Company, The National Fuel Gas Supply Company, Natural Gas Pipeline of America, Texas Gas Transmission Company, Southern Star Central Pipeline Company, TransCanada Corporation.\n\nIntrastate pipeline companies use storage facilities for operational balancing and system supply as well as to meet the energy demand of end-use customers. LDCs generally use gas from storage to serve customers directly. This group operates 148 underground storage sites and account for 40 percent of overall storage deliverability and 32 percent of working gas capacity in the US. These operators include Consumers Energy Company and the Northern Illinois Gas Company (Nicor), in the US and Enbridge and Union Gas in Canada.\n\nThe deregulation activity in the underground gas storage arena has attracted independent storage service providers to develop storage facilities. The capacity made available would then be leased to third-party customers such as marketers and electricity generators. It is expected that in the future, this group would take more market share, as more deregulation takes place. Currently in the US, this group accounts for 18 percent of overall storage deliverability and 13 percent of working gas capacity in the US.\n\nAs of January 2011, there are 124 underground storage facilities in Europe.\n\nThe United States is typically broken out into three main regions when it comes to gas consumption and production. These are the consuming East, the consuming West and the producing South.\nThe consuming east region, particularly the states in the northern part, heavily rely on stored gas to meet the peak demand during the cold winter months. Due to the prevailing cold winters, large population centers and developed infrastructure, it is not surprising that this region has the highest level of working gas storage capacity of the other regions and the largest number of storage sites, mainly in depleted reservoirs. In addition to underground storage, LNG is increasingly playing a crucial role in providing supplemental backup and/or peaking supply to LDCs on a short term basis. Although the total capacity for these LNG facilities does not match those of underground storage in scale, the short term high deliverability makes up for that.\n\nThe consuming west region has the smallest share of gas storage both in terms of the number of sites as well as gas capacity/deliverability. Storage in this area is mostly used to allow domestic and Albertan gas, coming from Canada, to flow at a rather constant rate.\n\nThe producing south's storage facilities are linked to the market centers and play a crucial role in the efficient export, transmission and distribution of natural gas produced to the consuming regions. These storage facilities allow the storage of gas that is not immediately marketable to be stored for later use.\nIn Canada, the maximum working gas stored was in 2006. Alberta storage accounts for 47.5 percent of the total working gas volume. It is followed by Ontario which accounts for 39.1 percent, British Columbia which accounts for 7.6 percent, Saskatchewan which accounts for 5.1 percent and finally Quebec which accounts for 0.9 percent.\n\nInterstate pipeline companies in the US are subject to the jurisdiction of the Federal Energy Regulatory Commission (FERC). Prior to 1992, these companies owned all the gas that flowed through their systems. This also included gas in their storage facility, over which they had complete control. Then FERC Order 636 was implemented. This required the companies to operate their facilities, including gas storage on an open access basis. For gas storage, this meant that these companies could only reserve the capacity needed to maintain system integrity. The rest of the capacity would be available for leasing to third parties on a nondiscriminatory basis. Open access has opened a wide variety of application for gas storage, particularly for marketers which can now exploit price arbitrage opportunities. Any storage capacity would be priced at cost-based pricing, unless the provider can demonstrate to FERC that it lacks market power, in which case it may be allowed to price at market-based rates to gain market share. FERC defines market power as \"..the ability of a seller profitably to maintain prices above competitive levels for a significant period of time.\". The underlying pricing structure for storage has discouraged development in the gas storage sector, which has not seen many new storage facilities constructed, besides current ones being expanded. In 2005, FERC announced a new Order 678 targeted particularly to gas storage. This rule is intended to stimulate the development of new gas storage facility in the ultimate goal of reducing natural gas price volatility. Commission Chairman Joseph T. Kelliher observed: \"Since 1988, natural gas demand in the United States has risen 24 percent. Over the same period, gas storage capacity has increased only 1.4 percent. While construction of storage capacity has lagged behind the demand for natural gas, we have seen record levels of price volatility. This suggests that current storage capacity is inadequate. Further, this year, what storage capacity exists may be full far earlier than in any previous year. According to some analysts, that raises the prospect that some domestic gas production may be shut-in. Our final rule should help reduce price volatility and expand storage capacity.\"\nThis ruling aims at opening up two approaches for developers of natural gas storage, to be able to charge market-based rates. The first one is the redefinition of the relevant product market for storage that includes alternatives for storage such as available pipeline capacity, local gas production and LNG terminals. The second approach aims at implementing section 312 of the Energy Policy Act. It would allow an applicant to request authority to charge \"market-based rates even if a lack of market power has not been demonstrated, in circumstances where market-based rates are in the public interest and necessary to encourage the construction of storage capacity in the area needing storage service and that customers are adequately protected,\" the\nCommission said. It is expected that this new order will entice developers, especially independent storage operators, to develop new facilities in the near future.\n\nThe Underground Gas storage Company, as a subsidiary of the National Iranian Gas company, was established in 2007 with the following objectives:\n\nSarajeh reservoir is depleted hydrocarbon reservoirs storage capacity of 3/3 billion cubic meters of natural gas, the company said. Storage capacity of the project to remove gas stored in the reservoir during the third phase follows considering are.\n- Phase 1: 10 million cubic meters of gas per day\n- Phase 2: 20 million cubic meters of gas per day\n- Phase 3: 30 million cubic meters of gas per day.\nThe project started at the end of 86 years from the date 10/17/85, 16% and 54% of the initial installation of the goods has grown. After 87 years at the end of the company's natural gas storage development project, the installation of any goods supplied to 84% is reached. Electric power supply operation by the end of the project has not improved in 86 years at the end of year 87 5/96% is reached. The gas injection project design, installation and commissioning of gas injection compressors 87 till about 3/9% has been developed.\n\nThe gas storage tank project in Shurijeh. Tank capacity approx eight quarters and the ability to produce 40 billion cubic meter.\nGas in there and execute the following phases of the project have been considered:\n- The first phase of 10 AD. CE. M.. The injection of 20 m. CE. CE.\n- Phase II 20 m. M.. M.. R. injection and 40 mm\n\nIn Alberta, gas storage rates are not regulated and providers negotiate rates with their customers on a contract-by-contract basis. However the Carbon facility which is owned by ATCO gas is regulated, since ATCO is a utility company. Therefore, ATCO Gas has to charge cost-based rates for its customers, and can market any additional capacities at market-based rates.\nIn Ontario, gas storage is regulated by the Ontario Energy Board. Currently all the available storage is owned by vertically integrated utilities. The utility companies have to price their storage capacity sold to their customers at cost-based rates, but can market any remaining capacity at market-based rates. Storage developed by independent storage developers can charge market-based rates.\nIn British Columbia, gas storage is not regulated. All available storage capacity is marketed at market-based rates.\n\nThe regulation of gas storage, transportation and sale is overseen by Ofgem (a government regulator). This has been the case since the gas industry was privatised in 1986. Most forms of gas storage were owned by Transco (now part of National Grid plc), however the national network has now largely been broken down into regional networks, owned by different companies, they are however all still answerable to Ofgem.\n\nAs with all infrastructural investments in the energy sector, developing storage facilities is capital intensive. Investors usually use the return on investment as a financial measure for the viability of such projects. It has been estimated that investors require a rate or return between 12 percent to 15 percent for regulated projects, and close to 20 percent for unregulated projects. The higher expected return from unregulated projects is due to the higher perceived market risk. In addition significant expenses are accumulated during the planning and location of potential storage sites to determine its suitability, which further increases the risk.\n\nThe capital expenditure to build the facility mostly depends on the physical characteristics of the reservoir. First of all, the development cost of a storage facility largely depends on the type of the storage field. As a general rule of thumb, salt caverns are the most expensive to develop on a Bcf of Working Gas Capacity Basis. However one should keep in mind that because the gas in such facilities can be cycled repeatedly, on a Deliverability basis, they may be less costly. A Salt Cavern facility might cost anywhere from $10 million to $25 million/Bcf of working gas capacity. The wide price range is because of region difference which dictates the geological requirements. These factors include the amount of compressive horsepower required, the type of surface and the quality of the geologic structure to name a few. A depleted reservoir costs between $5 million to $6 million/Bcf of Working Gas Capacity. Finally another major cost incurred when building new storage facilities is that of base gas. The amount of base gas in a reservoir could be as high as 80% for aquifers making them very unattractive to develop when gas prices are high. On the other hand, salt caverns require the least amount of base gas. The high cost of base gas is what drives the expansion of current sites vs the development of new ones. This is because expansions require little addition to base gas.\n\nThe expected cash flows from such projects depend on a number of factors. These include the services the facility provides as well as the regulatory regime under which it operates. Facilities that operate primarily to take advantage of commodity arbitrage opportunities are expected to have different cash flow benefits than ones primarily used to ensure seasonal supply reliability. Rules set by regulators can on one hand restrict the profit made by storage facility owners or on the other hand guarantee profit, depending on the market model.\n\nTo understand the economics of gas storage, it is crucial to be able to value it. Several approaches have been proposed. They include:\nThe different valuation modes co-exist in the real world and are not mutually exclusive. Buyers and sellers typically use a combination of the different prices to come up with the true value of storage. An example of the different valuations and the price they generate can be found in the table below.\nThis valuation mode is typically used to value regulated storage, for instance storage operated by interstate pipeline companies. These companies are regulated by FERC. This pricing method allows the developers to recover their cost and an agreed upon return on investment. The regulatory body requires that the rates and tariffs are maintained and publicly published. The services provided by these companies include firm and interruptible storage as well as no-notice storage services. Usually, cost of service pricing is used for depleted reservoir facilities. If it is used to price, say salt cavern formations, the cost would be very high, due to the high cost of development of such facilities.\n\nThis valuation mode is typically used by local distribution companies (LDCs). It is based on pricing storage, according to the savings resulting from not having to resort to other more expensive options. This pricing mode depends on the consumer and their respective load profile/shape.\n\nThe seasonal valuation of storage is also referred to as the \"intrinsic value\". It is evaluated as the difference between the two prices in a pair of forward prices. The idea being that one can lock-in a forward spread, either physically or financially. For developers seeking to study the feasibility of building a storage facility, they would typically look at the long-term price spreads.\n\nIn addition to possessing an intrinsic value, storage may also have extrinsic value. Intrinsic valuation of storage does not take the cycling ability of high-deliverability storage. The extrinsic valuation reflects the fact that in such facilities, say salt cavern formations, a proportion of the space can be used more than once, thus increasing value. Such high-deliverability storage facility allows its user to respond to variations in demand/price within a season or during a given day rather than just seasonal variations as was the case with single cycle facilities.\n\nIn general as we see in the graph below, high gas prices are typically associated to low storage periods. Usually when prices are high during the early months of the refill season (April–October), many users of storage adopt a wait and see attitude. They limit their gas intake in anticipation that the prices will drop before the heating season begins (November–March). However, when that decrease does not occur, they are forced to buy natural gas at high prices. This is particularly true for Local Distribution and other operators who rely on storage to meet the seasonal demand for their customers. On the other hand, other storage users, who use storage as a marketing tool (hedging or speculating) will hold off storing a lot of gas when the prices are high.\nResearch is being conducted on many fronts in the gas storage field to help identify new improved and more economical ways to store gas.\nResearch being conducted by the US Department of Energy is showing that salt formations can be chilled allowing for more gas to be stored. This will reduce the size of the formation needed to be treated, and have salt extracted from it. This will lead to cheaper development costs for salt formation storage facility type.\nAnother aspect being looked at, are other formations that may hold gas. These include hard rock formations such as granite, in areas where such formations exists and other types currently used for gas storage do not.\nIn Sweden a new type of storage facility has been built, called \"lined rock cavern\". This storage facility consists of installing a steel tank in a cavern in the rock of a hill and surrounding it with concrete. Although the development cost of such facility is quite expensive, its ability to cycle gas multiple times compensates for it, similar to salt formation facilities.\nFinally, another research project sponsored by the Department of Energy, is that of hydrates. Hydrates are compounds formed when natural gas is frozen in the presence of water. The advantage being that as much as 181 standard cubic feet of natural gas could be stored in a single cubic foot of hydrate.\n\n\n"}
{"id": "17314828", "url": "https://en.wikipedia.org/wiki?curid=17314828", "title": "Naturmobil", "text": "Naturmobil\n\nThe Naturmobil is a horse-powered vehicle for travel on paved roads. The vehicle is controlled by a driver in a similar way to a motor-driven vehicle, with the horse inside the vehicle on a treadmill. It weighs , or probably around with the horse. It cruises at about , with a top speed of about . The treadmill also charges batteries which will power the vehicle if the horse needs a break.\n\nThe vehicle was invented by Abdolhadi Mirhejazi, an Iranian engineer, who says \"the Naturemobil was developed with the safety and welfare of the animal in mind\". At least one veterinarian, Dr. Matt Pietrak, has concerns for the horse's wellbeing while powering the vehicle.\n\n"}
{"id": "13806732", "url": "https://en.wikipedia.org/wiki?curid=13806732", "title": "Non-Evaporable Getter", "text": "Non-Evaporable Getter\n\nNon evaporable getters (NEG), based on the principle of metallic surface sorption of gas molecules, are mostly porous alloys or powder mixtures of Al, Zr, Ti, V and Fe. They help to establish and maintain vacuums by soaking up or bonding to gas molecules that remain within a partial vacuum. This is done through the use of materials that readily form stable compounds with active gases. They are important tools for improving the performance of many vacuum systems. Sintered onto the inner surface of high vacuum vessels, the NEG coating can be applied even to spaces that are narrow and hard to pump out, which makes it very popular in particle accelerators where this is an issue. The main sorption parameters of the kind of NEGs, like pumping speed and sorption capacity, have low limits.\nA different type of NEG, which is not coated, is the Tubegetter. The activation of these getters is accomplished mechanically or at a temperature from 550 K. The temperature range is from 0 to 800 K under HV/UHV conditions.\n\nThe NEG acts as a getter or getter pump that is able to reduce the pressure to less than 10 Pa.\n\nVideo: Non-Evaporable Getter (NEG) Operation\nFolder: TubeGetter\n\nIon pump (physics)\n"}
{"id": "3194389", "url": "https://en.wikipedia.org/wiki?curid=3194389", "title": "Northern coastal scrub", "text": "Northern coastal scrub\n\nNorthern coastal scrub is a scrubland plant community of California and Oregon. It occurs along the Pacific Coast from Point Sur on the Central California coast in Monterey County, California, to southern Oregon. It frequently forms a landscape mosaic with coastal prairie. \n\nThe Northern coastal scrub's predominant plants are low evergreen shrubs and herbs. \n\nCharacteristic shrubs include coyote brush \"(Baccharis pilularis)\", California yerba santa \"(Eriodictyon californicum)\", coast silk-tassel \"(Garrya elliptica)\", salal \"(Gaultheria shallon)\", and yellow bush lupine \"(Lupinus arboreus)\". \n\nHerbaceous Northern coastal scrub species include western blue-eyed grass \"(Sisyrinchium bellum)\", Douglas iris \"(Iris douglasiana)\", and .\n\n"}
{"id": "32798877", "url": "https://en.wikipedia.org/wiki?curid=32798877", "title": "Optical overheating protection", "text": "Optical overheating protection\n\nWith all solar thermal collector systems there is a potential risk that the solar collector may reach an equilibrium or stagnation temperature higher than the maximum safe operating temperature. Various measures are taken for optical overheating protection.\n\nStagnation temperatures are encountered under conditions of high radiation while no heat transfer fluid is flowing through the collector, for example during power failures, component failures, servicing, energy storage capacity limitations, or periods when little hot water is extracted from the system. More generally, stagnation conditions can be considered to be any situation under which the solar collector cannot adequately dispatch the absorbed solar heat to the heat transfer fluid.\n\nBesides any damaging effects to the system, high stagnation temperatures also place constraints on collector materials. These materials must retain their important properties during and after exposure to the high stagnation temperatures. This implies that solar collectors are generally built from high temperature resistant materials. These materials are usually expensive, heavy, and have an overall high environmental impact.\n\nPolymeric materials offer a significant cost-reduction and environmental improvement potential for solar thermal collectors and may thus benefit a broader utilization of solar energy for various heating purposes. However, the long-term service temperature of plastics is limited. Thus, for potential applications of plastics in solar absorbers an appropriate design including overheating protection is essential. Feasible ways would be a reduction in optical gain (for example, using thermotropic layers, or electrochromic devices) or an increase in system losses, by dumping of the hot water excess.\n\nIn this article an alternative method to decrease the optical gain is presented. The method is based on the geometry of prisms and the phenomenon of Total Internal Reflection.\n\nAccording to Snell's law, light cannot escape from a medium when it strikes the medium boundary at an angle of incidence (θ) that is larger than the critical angle (θ), an optical phenomenon called Total Internal Reflection. The critical angle can be calculated using;\n\nformula_1 \n\nFor a polycarbonate medium, with a refraction index of n=1.59, placed in an atmosphere of air with a refraction index close to 1, Total Internal Reflection occurs when θ > θ=39°.\n\nConsider a polycarbonate prismatic structure with an apex angle α=45° placed in an atmosphere of air. A ray of light that strikes the medium boundary at normal incidence is total internal reflected, as θ=45°> θ=39°. In presence of water, θ=56.8° and θ=45°< θ, the incoming light is merely refracted and traverses the polycarbonate medium. As such, water acts as a switching fluid. In theory, water can be replaced by any other liquid, with an index of refraction close to that of the prismatic structure, to act as the switching fluid.\n\nThe optical switch consists of a self-regulating mechanism. In its \"passive\" state the switch is filled with liquid and light is allowed to pass through the switch and heat the system behind it. As the system heats up, the switching fluid evaporates out of the optical switch and the prismatic structure starts to behave as a reflective surface. No more light passes through the switch, limiting the maximum temperature of the system to the evaporation temperature of the liquid.\n\nResulting from its geometry, the optical switch is sensitive to the angle of the incident beam. Depending on the shape of the prisms, the transmittance of the switch in its reflective state during a typical day shows characteristic angular dependence. This dependence can be used to find specific transmission curves for different applications, where the geometry of the prisms serves as the input variable.\n\nThe main application for which the optical switch was developed is overheating protection for solar thermal collectors. The prismatic geometry can be integrated within the cover plate of the collectors to prevent them from overheating, either by self-regulation through evaporation, or by draining the water out of the switch at a specified maximum temperature. Temperature limitation would allow for the use of polymeric materials within solar collectors, dramatically reducing cost-price and increasing market penetration.\n\nAnother application of the switch is in windows for both housing and office buildings. The amount of sunlight entering the building can be controlled by the switching liquid. Preventing the amount of sunlight entering a building will reduce the temperature inside the building on sunny days.\n\nFinally, the switch can be used within roofs of greenhouses. The plants in the greenhouse can be protected from damage on sunny days by putting the switch in the reflective state. Currently greenhouses are covered with a chalk layer to protect the plants during summer from excessive sunlight. Applying the chalk layer is time consuming and bad for the environment. Once the chalk is applied, it also blocks sunlight during less sunny days. The optical switch could potentially resolve this issue using the switching mechanism described above.\n\nThe temperature inside the greenhouse can be regulated by switching a certain amount of roof sections in the reflective state. Also the switching fluid inside the roof can be circulated to extract heat from the greenhouse. These cooling methods allow that the (roof) windows remain closed and that the climate (relative humidity and raised CO2 levels) remain optimal and constant.\n\nThe Switching fluid in the greenhouse roof can be used as a filter for a certain part of the solar spectrum. Water allows so-called \"PAR\" light (Photosynthetically active radiation, the light that plants use to grow) to pass, while \"NIR\" (Near Infra Red) light is absorbed. The amount of NIR light to absorb can be tuned by solving micro-particles of Copper Sulphate or clay in the switching fluid. In that way optimum growth conditions can be selected.\n\nSome greenhouse products, like flowers, are grown by using artificial light during the night. This artificial light causes so-called light pollution in the environment of the greenhouse. When a greenhouse roof consists of a well designed optical switch the greenhouse roof becomes reflective during the night, which keeps the artificial light inside the greenhouse. As a side effect there are less number of lights needed since the roof acts as an efficient mirror.\n\n"}
{"id": "915549", "url": "https://en.wikipedia.org/wiki?curid=915549", "title": "Orano", "text": "Orano\n\nOrano (previously Areva) is a French multinational group specializing in nuclear power and renewable energy headquartered in Paris La Défense. Areva is majority owned by the French state, through French Alternative Energies and Atomic Energy Commission (54.37%), Banque publique d'investissement (3.32%), and Agence des participations de l'État (28.83%). Moreover, Électricité de France of which the French government has a majority ownership stake, owned 2.24%; Kuwait Investment Authority owned 4.82% as the second largest shareholder after the French state.\n\nIn 2017 the majority of its reactor business Framatome (previously Areva NP) was sold to Électricité de France, while Areva will quit the new company (\"New NP\") by 2020. Japan Nuclear Fuel Limited and Mitsubishi Heavy Industries will take a 5% stake each in another new company, which will run the nuclear fuel business of Areva, later named Orano (previously \"NewCo\").\n\nOn 23 January 2018 Areva was renamed Orano.\n\nAreva was created on 3 April 2001, by the merger of Framatome (later Areva NP, now sold off and renamed back to Framatome), Cogema (now Areva NC) and Technicatome (now Areva TA). \nIt was based on the structure of its precursor, CEA-Industrie, and Anne Lauvergeon was named CEO. \nIn 2009, Siemens sold its remaining shares of Areva NP.\n\nJean-Pierre Raffarin, the former Prime Minister of France, government announced the privatization of Areva in 2003, but it was postponed several times, the French government opting finally for the privatization of GDF and EDF. At the end of October 2005, French Prime Minister Dominique de Villepin announced that he had suspended the privatization process.\n\nIn 2006, Areva created its Renewable Energies Business Group.\nCreusot Forge and Creusot Mécanique merged into the Areva group, even though there were quality concerns over Creusot Forge's work.\n\nIn March 2010, Areva indicated work was being done on a new type of burner reactor type capable of breaking down actinides created as a product of nuclear fission.\n\nDue to financial difficulties in 2016, in 2017 Areva NP merged with and as of January 04, 2018, Areva NP is now known as Framatome.\n\nIn December 2011, Areva suspended building work at several sites in France, Africa and the United States, one day after forecasting a €1.6 billion ($2.1 billion) loss. \nAreva halted \"capacity extensions\" at its La Hague Reprocessing Plant, in northern France, at its Melox factory in the southwest, and at two sites attached to its Tricastin power plant in the south. \nWork has also stopped on extensions to uranium mines in Bakouma in the Central African Republic, Trekkopje in Namibia, and Ryst Kuil in South Africa, and caused a potential delay in construction until a capital solution is secured for the Eagle Rock Enrichment Facility in the United States.\n\nAreva wrote off most of the $2.5 billion purchase cost of Canadian uranium mining company Uramin, purchased in 2007, after concluding that its uranium ore deposits were of negligible value.\n\nIn September 2014 Standard & Poor's stated it might downgrade Areva’s debt following weak first-half results, leading to Areva indicating it would cut capital spending and dispose of some assets.\nIn October 2014, CEO Luc Oursel took a leave of absence for health reasons. \nHe died in December of the same year.\n\nOn 19 November 2014 Areva suspended its financial targets for 2015/2016 as a result of delays on its Finnish project (Olkiluoto Nuclear Power Plant Unit 3) and \"lacklustre\" nuclear market. \nAs a result, on 20 November 2014 Standard & Poor's downgraded Areva long-term debt to BB+ and short-term-debt to A-3. \nThis led to difficulties for Areva to raise money on the financial market, however according to S&P Areva has a \"high cash cushion\". \nSome analysts speculate that Areva needs a 1.5-2 billion euro capital increase – at the moment the French government (Second Valls Government) is unwilling to increase capital for the state-owned company.\n\nThe balance sheet 2014 shows a sales revenue of 8.336 bn Euro and a consolidated net income of -4.834 bn Euro (0,58 Euro loss per 1 Euro sales revenue).\n\nIn March 2015 Standard & Poor's further downgraded Areva's credit rating to BB- after Areva posted a €4.8 billion loss for 2014. \nAreva intends to cut costs to save about €1 billion annually by 2017, and to sell €450 million of assets by 2017.\n\nThe French government is considering a rescue plan which may include some form of bailout from Électricité de France (EDF). \nIn May, Areva announced that it expected to cut 5,000 to 6,000 jobs out of its 42,000 employees globally. In July 2015, EDF agreed to a majority stake in Areva NP. Bernard Fontana has been appointed CEO of Areva NP as of 1 September 2015. On October 31st 2015, it was announced that Mitsubishi Heavy Industries would be taking an equity stake in Areva NP (up to 34 percent), possibly eventually extending it to one in Areva as a whole. MHI decided to take this step because of Areva NP's vital role in their Atmea reactor joint venture.\n\nIn July 2015 EDF valued Areva NP at €2.7 billion ($2.9 billion) and agreed to take a majority stake in Areva NP, following a French government instruction they create a \"global strategic partnership\".\n\nIn December 2015, operations at Le Creusot Forge were stopped following the discovery of quality assurance problems.\n\nFollowing a discovery at the Flamanville Nuclear Power Plant, about 400 large steel forgings manufactured by Le Creusot Forge since 1965 have been found to have carbon-content irregularities that weakened the steel. A widespread programme of reactor checks was started involving a progressive programme of reactor shutdowns, likely to continue over the winter high electricity demand period into 2017. This caused power price increases in Europe as France increased electricity imports, especially from Germany, to augment supply. As of late October 2016, 20 of France's 58 reactors are offline. These steel quality concerns may prevent the regulator giving the life extensions from 40 to 50 years, that had been assumed by energy planners, for many reactors. In December 2016 the \"Wall Street Journal\" characterised the problem as a \"decades long coverup of manufacturing problems\", with Areva executives acknowledging that Le Creusot had been falsifying documents.\n\nIn December 2016 international inspectors at Le Creusot Forge found evidence of recent doctored paperwork, which had not been detected by Areva’s independent quality control checks. The team concluded that they \"were not confident that the improvement programmes and associated remedial actions ... were sufficiently resourced, prioritised and integrated in order to bring about sustained improvements in manufacturing performance and nuclear safety culture\".\n\nIn March 2017 Autorité de sûreté nucléaire (ASN) determined that the safety culture in the plant was not sufficient to produce nuclear components and that the plant did not have sufficiently good equipment for some of its nuclear production, which led to the manufacture of substandard parts and document falsification. In April 2017 ASN published the requirements for forging to resume at Le Creusot Forge. In August 2017 ASN published a draft decision requiring the examination of Le Creusot Forge manufacturing records of all components during scheduled reactor refuelling outages.\n\nOn 25 January 2018 ASN gave approval for the resumption of the manufacture of forgings at Le Creusot. The 2016 improvement plan was 90% complete and will complete during the first half of 2018. The forge had been out of operation since December 2015. \"Irregularities\" in records of about 400 of 3854 nuclear components produced since 1965 had been found, and analysis of these deviations were expected to continue until the end of 2018.\n\nIn June 2016 Areva's restructuring plans were made public, including the sale of the majority of its reactor business to EDF in 2017, excluding the Olkiluoto 3 EPR under construction in Finland which will remain with Areva SA.\n\nOn 1 March 2017 Areva published figures for 2016 (loss 665 million Euro) and announced some information about the planned restructuring.\n\nOn 21 March 2017 Japan Nuclear Fuel Limited (JNFL) and Mitsubishi Heavy Industries (MHI) agreed to each take a 5% stake in the restructured Areva nuclear reactor business for €250 million each, dependent on the Flamanville 3 reactor pressure vessel and other large steel forgings passing safety tests.\nSome days later, Toshiba's nuclear branch (Westinghouse Electric Company) filed for Chapter 11 bankruptcy protection.\n\nIn December 2017, Areva and EDF signed agreements on the transfer of Areva NP's nuclear reactor operations.\n\nOn 23 January 2018 Areva was renamed Orano. The name Areva shall continue to be used only relating to the holding company Areva SA, that is holding risks associated with Olkiluoto and the troubled Creusot foundry. The company's new circular yellow logo refers to yellowcake uranium concentrate and to the nuclear fuel cycle.\n\nAreva is an international company in nuclear and renewable energy. It is the only company with a presence in each industrial activity linked to nuclear energy: mining, chemistry, enrichment, fuel assembly, reprocessing, engineering, nuclear propulsion and reactors, treatment, recycling, stabilization and dismantling. Areva offers technological solutions for CO₂-free energy partly through Areva Renewables.\n\nAreva’s activities are divided into 5 Business Groups:\n\nAreva’s subsidiary for power transportation and distribution, Areva T&D, was sold to Schneider Electric and Alstom in June 2010.\n\nCurrent subsidiaries include Euriware and SAFRAN.\n\nThe Mining Business Group manages the exploration of uranium ore, its extraction, and processing, as well as the restoration of sites after mine closure. Areva’s Mining Business Group has staff on five continents and operates uranium production sites in Canada, Kazakhstan, and Niger. As of the end of 2013, the Mining Business Group employed 4,463 people.\n\nUranium production accounted for 19 percent of Areva's consolidated revenue in 2013.\n\nThe Front End Business Group secures access to fuel for Areva’s customers, and oversees operations to convert uranium into nuclear fuel, including the chemical conversion of ore into uranium hexafluoride, the enrichment of uranium, and the design and production of fuel for nuclear reactors. As of the end of 2013, 8,555 people worked for Areva’s Front End Business Group. The Business Group accounted for 24 percent of Areva’s consolidated revenue. In 2013, the Front End Business Group saw a 6.8 percent revenue increase to $3 billion.\n\nThe Front End Business Group is divided into two main business units:\n\n\nThe Reactors & Services Business Group designs and builds Pressurized Water Reactors (PWR) and Boiling Water Reactors (BWR). The Business Group also designs and builds naval propulsion and research reactors and offers products to service nuclear reactors.\n\nThe Business Group had 15,592 employees at the end of 2013 and generated 36 percent of Areva’s revenue in the same year. It is split into six business units:\n\n\nThe Back End Business Group develops recycling solutions for used fuel for reuse in reactors. It also offers storage and transportation solutions for radioactive material, and site rehabilitation at nuclear facilities. \nAt the end of 2013, the Business Group had 11,583 employees and made up for 19 percent of Areva’s revenue. \nBased in Europe and North America, the Back End Business Group is divided into five Business Units:\n\nIn 2017 the NUHOMS Matrix advanced used nuclear fuel storage overpack, a high-density system for storing multiple spent fuel rods in canisters, was launched.\n\nAreva created its global Renewable Energies Business Group in 2006 as an expansion of its clean energy portfolio with renewable energy. This group represents four business lines: concentrated solar power, offshore wind power, biomass power, and hydrogen power storage and distribution. The Renewable Energies Business Group has offices in the United States, Brazil, India and France.\n\nIn February 2010, Areva formed Areva Solar with its acquisition of US-based Ausra, a provider of concentrated solar power solutions using Compact Linear Fresnel Reflector (CLFR) technology. Headquartered in Mountain View, California, Areva Solar operates a 5MWe solar power plant in Bakersfield, California, at its Kimberlina Solar Thermal Energy Plant development facility. Areva Solar markets its solar boiler technology in three formats: as a standalone solar power plant generating 50MW+ of electricity, as a solar booster (20-50MW) adding solar steam power to an existing fossil-fuel power plant for carbon mitigation, and as an industrial steam provider for food, oil, desalination and other processes. Areva Solar’s boilers attain 750-degree F (400° C) superheated steam, and are the first and only solar boiler to receive S-Stamp certification by the American Society of Mechanical Engineers (ASME).\n\nIn April 2012, Areva announced that it would build a concentrated solar power (CSP) installation in Rajasthan, India. It will be Asia’s largest CSP installation and will be operated by India’s Reliance Power Limited.\n\nIn 2007, Areva purchased 51 percent of offshore wind turbine manufacturer Multibrid. In June 2010, Areva purchased the remaining 49 percent and formed Areva Wind.\n\nAt its Bremerhaven, Germany manufacturing facility, Areva Wind produces 5MW wind turbines specifically designed and sealed for offshore environments. The company designs, manufactures, assembles and commissions its wind turbines and blades for offshore wind projects, notably in the North Sea.\n\nIn the summer of 2009, AREVA Wind installed the first 6 M5000 turbines of the Alpha Ventus project in the North Sea. These are located about 45 kilometers from the German island of Borkum, at a depth of 30 meters. It is the first German wind energy park to be constructed out at sea under offshore conditions and in such deep water. The M5000 turbine has an output of 5MW at an average wind speed of around 12 meters per second. From the ocean’s surface to the top tip of the blade rotation, an Areva Wind turbine assembly stands taller (616 feet / 188 meters) than the Washington Monument (555 feet / 169 meters).\n\nAreva Wind designs, assembles, installs and commissions 5-Megawatt wind turbines for offshore wind farms, a growing international industry. Each turbine can produce enough electricity to power 4,000 to 5,000 homes. Areva also designs and manufactures rotor blades through its subsidiary Areva Blades.\n\nAreva Wind has two manufacturing sites in Bremerhaven and Stade, Germany.\n\nIn France, the Group is constructing two industrial sites for nacelles and blade production in Le Havre.\n\nIn January 2014, Areva and Spain’s wind turbine manufacturer Gamesa Corporación Tecnológica announced a preliminary deal to create a joint venture (Adwen) in the offshore wind power business. Adwen is a source of contention during negotiations between Gamesa and Siemens Wind Power regarding their merger, as Siemens was reluctant to fund factories and development of an 8 MW turbine in France.\n\nAREVA is a global leader in the production and operation of biomass power plants, with over 38 years of experience in bioenergy and more than 95 plants built worldwide, representing 2.5 GWe of installed capacity.\n\nThese plants are located in Brazil, Chile, Germany, the Netherlands, India and Thailand.\n\nAreva Renewables Brazil, a subsidiary of Areva, builds biomass power plants based on bagasse (organic waste from sugar cane). In the United States, Areva formed a partnership with Duke Energy in September 2008, named ADAGE, to build and operate 55MW biomass power plants based on clean wood waste collected during sustainable forestry operations. The first biomass power plant was progressing through permitting in Mason County, Washington, but was terminated in 2011.\n\nIn 2012, Areva acquired a bio-coal production technology called Thermya, which produces a biofuel issued from biomass torrefaction that can replace coal to generate thermal energy and electricity.\n\nAREVA is developing AdCub, a modular concept for compact power plants targeting small-scale biomass resources that addresses relatively untapped growth opportunities in Europe. The objective is to combine innovative solutions which optimize customer costs for small-scale power plants (between 3 and 6 MW) and reduce construction time. This new biomass plant concept also aims to improve upstream technologies for multi-fuel acceptance and higher availability.\n\nAREVA designs, manufactures and industrializes turnkey energy storage solutions and products to generate electricity with fuel cells and produce hydrogen by electrolysis. AREVA supports ongoing research in the hydrogen field through partnerships with a number of industrial companies and national research organizations including: Agence Nationale de la Recherche (French National Research Agency), the Horizon Hydrogen Energy (H2E) program, PAN-H (\"Plan d’Action National sur l’Hydrogène et les piles à combustible\") and Pôle Capenergies, an innovation cluster dedicated to the development of renewable forms of energy without greenhouse gas emissions.\n\nIn 2012, Areva inaugurated a hydrogen storage system called the MYRTE platform near Ajaccio, Corsica (MYRTE is the French acronym for \"Mission Hydrogène Renouvelable pour l’intégration au réseau électrique\"). The system aims to establish the feasibility of a storage solution for solar energy using hydrogen technologies, which would serve as a back-up system to stabilize Corsica’s power grid. In 2014, AREVA’s energy storage and management system, the Greenergy Box, was added to the existing installation, in operation since early 2013. This management system increases grid output from the energy stored in hydrogen to 150 kW.\n\nIn May 2014, AREVA SMART ENERGIES, through its subsidiary CETH2, and the ADEME (French Environment and Energy Management Agency), announced the creation of the AREVA H2-Gen joint venture. The joint venture aims to manufacture Proton Exchange Membrane (PEM) electrolysers, a technology that enables the production of hydrogen from water and electricity.\n\nAreva is involved in military technology, such as designing the nuclear reactor for the French Barracuda class submarine.\n\nOne of Areva's subsidiaries, Euriware (founded in 1991) specializes in consulting and IT services and employs 2,200 people. AREVA also owns 1.99 percent of Safran and 1.4 percent of Suez Environnement.\n\nCERCA, a subsidiary of Areva that produces fuel for research reactors, is also involved in TRIGA, a research reactor established in 1996 with the US firm General Atomics that is used for training, research, and the production of radioisotopes.\n\nAREVA is a former corporate member of the Bruegel think tank.\n\nAreva NC (Nuclear Cycle), formerly Cogema, is active in all stages of the nuclear fuel cycle.\n\nFramatome, formerly Areva NP (Nuclear Power), specializes in the design and construction of nuclear power plants, fuel supply, and maintenance.\n\nAreva TA, formerly Technicatome, specializes in nuclear propulsion reactors and nuclear research facilities.\n\nEuriware provides consulting and IT services in the energy and industry sectors.\nAreva Med focuses on the development of therapies to fight cancer.\nAreva Mines is active in mining activities, including exploration, extraction, and processing of uranium ore.\n\nAreva's main shareholder is the French public-sector company, the CEA (\"Commissariat à l'énergie atomique et aux énergies alternatives\"), which owns 68.88 percent. Areva is incorporated under French law as a \"société anonyme\" (SA: public corporation) and is also recognized as a public limited company in the United Kingdom and a corporation in United States jurisdictions. The French State (including the shares owned by the CEA) controls 87 percent.\n\nIn June 2011, the French government named Luc Oursel, formerly chief of marketing, as Chief Executive Officer and President of Areva, replacing Anne Lauvergeon, who served 10 years in the position. Upon Oursel's death in 2014, the board named Phillippe Knoche, company COO, as acting CEO while they conduct a search for a permanent replacement. The CEO's actions are subject to considerable oversight by the supervisory board.\nSpencer Abraham, the former U.S. Secretary of Energy, was a non-executive chairman of Areva Inc. from 2006 to 2012. Mike Rencheck has been the CEO of Areva Inc. in North America since March 2012.\n\nAs of December 2014, the members of the Executive Board are: \n\nAreva realized €9.104 billion in sales revenue in 2010 and €-423 million in operating income. Areva had €3.672 billion of net debt at the end of 2010. In June 2010, Standard & Poor's downgraded Areva’s debt rating to BBB+, due to weakened profitability following a further € 400 million provision for the Olkiluoto-3 over-running European Pressurized Reactor build. In July 2010, the French government authorised a 15 percent capital increase at Areva, in which Électricité de France (EDF) could raise its stake from 2.4 to 7 percent.\n\nAreva’s operating income dropped by -2.6 percent in 2011. The stock exchange of the share was interrupted on Monday 12 December 2011. On 12 December 2011 Areva gave a warning of €1.6 billion losses and 2.4 billion € write down in 2011 and informed of a saving program. In addition to Fukushima accident, the Olkiluoto Nuclear Power Plant was expected to bring losses for the company.\n\nAreva realized €9.342 billion in revenue in 2012 and €118 million in operating income. On 1 March 2012 Areva announced a business-deficit of €2.4 billion (US$3.2 billion). 1,500 employees in Germany were to be laid off, after the German decision to phase out nuclear power by 2022.\n\nAreva reported consolidated revenue of 9.240 billion euros in 2013, an increase of 4.0 percent on a reported basis and of 6.4 percent like for like compared with 2012.\n\nAreva's EBITDA for 2013 (earnings before interest, taxes, depreciation and amortisation) was 1.04 billion euros.\n\nThe consolidated backlog stood at 41.5 billion euros at December 31, 2013, down from 44.6 billion euros a year earlier.\n\nNuclear sales declined for Areva after the 2011 Japanese Fukushima disaster, and this was also the case for rival nuclear companies such as General Electric and Westinghouse Electric. Areva had a €4.8 billion loss in 2014 on sales of 8.3 billion euros.\n\nIn December 2003, Finland became the first country to order a new nuclear reactor in Western Europe in 15 years – it will become Finland's fifth reactor.\nAreva started construction of the first European Pressurized Reactor (EPR) in Olkiluoto in 2005. The reactor, which is one of the first of the new Generation III reactors, was initially expected to begin producing electricity in 2009, but the project has faced delays.\n\nIn 2003 Finland’s Olkiluoto contract was issued on a turnkey basis and in 2004 a contracted fixed price was established as €3200M. Over the years delays have accumulated and costs started to overrun substantially.\n\nIn its 2006 Annual Report, Areva recorded a write-down of €507M associated with the delay. the project is at least seven years behind schedule with a total cost estimate of €8.5 billion ($11.4 billion). According to the customer's estimate, the Olkiluoto plant will be finished by 2016. Owner TVO had originally contracted to start selling nuclear power at the end of April 2009.\n\nA second EPR is currently under construction at the Flamanville Nuclear Power Plant in France. As of 2014, this plant is behind schedule and over-budget. The reactor is set to generate its first power in 2017.\n\nIn November 2007, Areva agreed to a €8 billion deal with the China Guangdong Nuclear Power Group to supply them with two EPRs in Taishan, Guangdong, China. Under the terms of the agreement, Areva will also help operate the plant, including the reprocessing of spent fuel. The nuclear reactor in Taishan is projected to cost less and be built more quickly following lessons learned in Finland.\n\nThe Atmea I is a new evolutionary Generation III reactor design targeted towards both developed and developing economies. It is a joint venture between Areva and Mitsubishi Heavy Industries. The design is for a PWR with a power output of about 1,100 MWe. Plans called for the design to be ready for licensing applications by the end of 2009. The first is expected to be built in Turkey with construction set to begin by 2017 or after, and completed by 2023 or after.\n\nIn 2009, Areva announced that its 1,250 MWe Generation III+ boiling water reactor (BWR) design, provisionally known as SWR-1000, will henceforth be called Kerena. The Kerena design was developed from that of the Gundremmingen Nuclear Power Plant by Areva, with extensive German input and using operating experience from Generation II BWRs to simplify systems engineering.\n\nWorldwide, the Areva group has an industrial presence in 43 countries and its commercial network reaches more than 100 countries. As of 2013, it employed approximately 45,340 people. In 2006, \"Fortune Magazine\" reported that Areva was the \"Most Admired Global Energy Company.\"\n\nAreva has partnered with engineering contractors to aid in the reconstruction of Iraq by manufacturing equipment to construct electrical substations.\n\nIn 2007, AREVA purchased UraMin, which later became Areva Resources Southern Africa.\n\nAreva in the UK provides services for nuclear reactor design and construction, as well as related services. The Business Group also offers renewable technologies, including offshore wind and bioenergy. \n\nThe Hinkley plant will be the first nuclear power plant built in Britain since the Sizewell B in Suffolk, which started electricity production in 1995.\n\nThe Hinkley Point C plant is projected to cut CO2 emissions in the UK by 9 million tons a year and create 25,000 construction jobs and 900 operations jobs.\n\nAreva operates two mines through SOMAÏR and COMINAK situated in Arlit, in northern Niger, and is also developing the Imouraren project situated 80 km from Arlit. These mines and projects employ more than 7,000 people. Niger is the world's fourth largest uranium producer.\n\nUp until 2007, Areva was the only industrial company able to extract uranium in the country, but under a policy shift by then President Mamadou Tandja, the field was opened to competition and Sino-U, a Chinese nuclear energy company, was granted uranium concessions. On 25 July 2007, the CEO of Areva-Niger, Dominique Pin, was expelled from Niger (although he was in Paris at the time) on charges of supporting the Tuareg Rebellion. The \"Financial Times\" described the entry of Sino-U in 2007 as a \"battle for resources\" between China and France and illustrated a competition view held by some that saw \"China’s pursuit of Africa’s resources as a direct – and potentially destabilizing – threat to western interests\". The country manager for AREVA disagreed with this view, stating in the same article that there were many uranium blocks, sufficient to keep both companies busy.\n\nThe population of Niger was exposed to a serious famine in 2005. AREVA donated €130,500 from January to June 2005 to the food crisis coordination group of Niger, and €120,000 in July in the form of two planes loaded with food and organized by Bernard Kouchner's \"Réunir\" NGO.\nIn November 2009, Greenpeace released a report indicating that two villages near AREVA's mining operations in Niger had dangerously high levels of radiation. To restore the information about AREVA’s mining activities in Niger, AREVA published a report answering in complete transparency to all the accusations made by the NGO. \n\nIn May 2013, the firm’s SOMAÏR uranium mine was damaged and one civilian was killed in an Al-Qaeda-linked suicide bomb attack, targeting the firm on the grounds that it was a French-owned company. The mine reached full production again in August 2013.\n\nAreva is present in Namibia through its subsidiary AREVA Resources Namibia. The Group's uranium mining project in Namibia at Trekkopje was put on hold in 2012 due to a decrease in uranium prices following the 2011 Fukushima nuclear disaster.\n\nAreva is currently running a care and maintenance program at the site so that it can be reopened in the future. The program is run at a cost of $10 million per year.\n\nAt the end of 2013, Areva directly employed about 50 people in Namibia, as well as about 50 suppliers and contractors in the country.\n\nAreva Resources Namibia finished a 20 million cubic meter desalination plant in the semi-arid Erongo region of the country in 2010.\n\nIn June 2014, AREVA announced plans to sign a 10-year deal with state-run utility Namibia Water Corp to maintain water supplies essential to uranium mines in the country.\n\nAreva has had a presence in Gabon since the 1950s, although there is currently no uranium mining in the country.\n\nToday, Areva operates in the country under the subsidiary Areva Gabon, established in 2008. Areva Gabon is focused on exploring new mining possibilities at sites around the country. Gabon has received mineral exploration permits in four areas: Mopia, Andjogo, Lekabi, and N’Goutou.\n\nIn 2010, Areva launched the Mounana Health Observatory to treat former workers who became ill after working in the COMUF mine, which was shut down in 1999.\n\nOn 13 August 2007, the French newspaper \"Le Parisien\" alleged that the Franco-Libyan civil nuclear power agreement signed by President Nicolas Sarkozy did not concern desalinization of sea water, as claimed by the French government, but instead focused in particular on selling the EPR to Libya, a contract potentially worth $3 billion. \"Le Parisien\" cited Philippe Delaune, deputy to the deputy director of international affairs for the CEA atomic agency, which is the main share-holder in Areva. Following allegations that the deal had been related to the release of the Bulgarian nurses, the French Socialist Party, through the spokesperson Jean-Louis Bianco, declared that this deal was \"geopolitically irresponsible\". The German government also denounced the agreement.\n\nAreva Resources Canada Inc., created in 1993, is a uranium mining, milling, and exploration company based in Saskatoon, Saskatchewan. Areva Resources Canada has offices and operations in Saskatoon, La Ronge, McClean Lake, Cluff Lake, and in Baker Lake, Nunavut.\n\n\nAreva operates as Areva Inc. in the USA, and is present in more than 35 locations across 20 states, employing nearly 5,000 people. Areva Inc. supplies network products to two-thirds of all US utilities. Areva Inc.'s headquarters are located in Charlotte, North Carolina.\n\nIn February 2002, the U.S. Secretary of Energy Spencer Abraham announced the Nuclear Power 2010 Program, which included plans for two EPRs. This private/public partnership aimed to develop advanced reactor technologies and deploy new nuclear power plants by 2010, when the program was completed. On 15 September 2005, Areva and Constellation Energy of Baltimore announced a joint venture called UniStar Nuclear to market the commercial EPR in the US. The joint venture later became UniStar Nuclear Energy in 2007. In 2010, EDF acquired 100 percent of UniStar Nuclear Energy. However these plans failed to come to fruition, and in February 2015 Areva suspended the EPR Design Certification Application Review process at the U.S. Nuclear Regulatory Commission (NRC).\n\nAreva Inc., based in Charlotte, announced on May 6, 2008 that it would seek all necessary approval to build a uranium enrichment facility in Bonneville County, Idaho, about twenty miles west of Idaho Falls and near the Idaho National Laboratory, with the proposed name of Eagle Rock Enrichment Facility (EREF). Areva had officially announced plans for a US enrichment facility already since 2007. The Eagle Rock plant was expected to provide uranium enrichment services to US nuclear plant operators using advanced proven centrifuge technology developed by the Enrichment Technology Company Ltd. (ETC), an Areva/Urenco joint venture. In 2010, EDF acquired 100 percent of UniStar Nuclear Energy.\n\nIn December 2011, as part of an Areva investment freeze, the company announced a potential delay in construction until a capital solution was secured for the Eagle Rock Enrichment Facility. It is now budgeted at $3 billion, has a US Nuclear Regulatory Commission license and a $2 billion loan guarantee from the Energy Department. In May 2013, Areva announced that it no longer projects a date for building the Eagle Rock plant because of uncertainties regarding its financing.\n\nIn 2007, Areva signed a ten-year deal with the South Korean public company KHNP to enrich uranium at its Georges Besse II enrichment plant, which was inaugurated in 2010. The deal is worth over €1 billion.\n\nIn November 2013, Areva and South Korea electric utility KEPCO signed a memorandum of understanding to collaborate in the renewable energy sector, with a particular focus on biomass in Southeast Asia.\n\nIn China, Areva won an €8 billion ($11.9 billion) agreement in 2007 to build nuclear reactors.\n\nLauvergeon said the deal enables allows Areva to \"consolidate its presence in one of the most dynamic markets in the world with enormous potential.”\n\nThe third-generation pressurized water reactors boosted state-run China Guangdong Nuclear Power Corporation’s (CGNPC) output by 3,400 megawatts.\n\nThe contract also called on Areva to provide uranium to fuel the reactors through 2026. The EPR reactors are expected to begin commercial operation in 2015 in the city of Taishan in Guangdong province, an export manufacturing powerhouse with heavy demand for power and high levels of industrial pollution.\n\nThe China General Nuclear Power Corporation has enhanced the French 900 MWe three-cooling-loop design imported in the 1990s into the 1,000 MWe CPR-1000 design.\n\nSome intellectual property rights are retained by Areva, which limits CPR-1000 export potential for China. Areva is reported to be considering whether it should market the cheaper, less sophisticated, CPR-1000 alongside the EPR, for countries that are new to nuclear power.\n\nIn December 2013, Areva and China General Nuclear Power (CGN) agreed to a partnership in renewable energy, with a focus on offshore wind, biomass, concentrated solar power, and energy storage.\n\nOn 18 December 2008, Areva signed an agreement with the Nuclear Power Corporation of India Limited (NPCIL) for the supply of 300 tonnes of uranium to India for power generation, thus becoming the first-ever foreign supplier of Uranium to the country after the NSG waiver.\n\nOn 4 February 2009, Areva signed an MOU to supply two to six nuclear reactors to the Nuclear Power Corporation of India Limited, for the Jaitapur Nuclear Power Project in the Indian state of Maharashtra. The deal is thought to be worth around 8 billion Euro ($10.4 billion).\n\nIn June 2008, Areva reached an agreement with Kazatomprom. The collaboration lead to the creation of a joint-venture Katco which provides 4000 uranium's tones per year. Due to that contract Kazakhstan became one of the three most important partnerships for the group.\n\nIn January 2007, Areva was fined €53 million by the European Commission for rigging EU electricity markets through a cartel involving 11 companies, including ABB, Alstom, Fuji, Hitachi Japan, AE Power Systems, Mitsubishi Electric Corp, Schneider Electric, Siemens, Toshiba and VA Tech ELIN. According to the Commission, \"between 1988 and 2004, the companies rigged bids for procurement contracts, fixed prices, allocated projects to each other, shared markets and exchanged commercially important and confidential information.\" Siemens was given a fine of €396 million, more than half of the total, for its alleged leadership role in the cartel.\n\nAreva is not accused of any cartel involvement other than through the acquisition of an Alstom unit in January 2004. \"This subsidiary was acquired by the Areva group towards the end of the infringement, in January 2004. The parent entities of the Areva group share a joint liability with that subsidiary for the period after its acquisition.\" \"A few months before the cartel ended, Alstom sold the unit involved to Areva, which knew nothing of the cartel. It [Areva] and Alstom have joint liability for 53.6 million, which they must decide how to split.\"\n\nEU Competition Commissioner Neelie Kroes declared that \"The commission has put an end to a cartel which has cheated public utility companies and consumers for more than 16 years\".\n\nOn 24 June 2012, an armed group assaulted a uranium plant operated by Areva at Bakouma in the southeast of the Central African Republic. A statement by the military described that \"a violent clash on Sunday afternoon pitted\" Central African troops against \"an unidentified group of armed men attempting to launch an assault on the site of mining company Areva\".\n\nAccording to the report the army successfully repelled the attack, but \"the enemy did some material damage and pulled back while taking a sizeable quantity mainly of food with them.\" The Areva group issued no immediate statement regarding the attack. Central African military sources believe that the attack was organized by members of the Chadian rebel group Popular Front for Recovery (FPR) led by 'General' Baba Ladde, which has been active in the region since 2008. The army says it's conducting further operations to neutralize the remaining armed rebels in the region of Bakouma.\n\nIn January 2014, Al Jazeera produced \"Orphans of the Sahara\", three-part series on the Tuareg people of the Sahara desert, in which claims were brought to light that Areva mining and consequent radiations are causing diseases and deaths among their people.\n\nAl Jazeera also published Areva’s response, in which the company said it submits regular reports on its environmental monitoring of water, air and soil to the Nigerien Office of Environmental Assessments and Impact Studies (BEEEI) which indicate that there is no pollution around the sites in question.\n\nAreva has sponsored several sporting events over the years.\n\n\n\n"}
{"id": "231630", "url": "https://en.wikipedia.org/wiki?curid=231630", "title": "Plasmon", "text": "Plasmon\n\nIn physics, a plasmon is a quantum of plasma oscillation. Just as light (an optical oscillation) consists of photons, the plasma oscillation consists of plasmons. The plasmon can be considered as a quasiparticle since it arises from the quantization of plasma oscillations, just like phonons are quantizations of mechanical vibrations. Thus, plasmons are collective (a discrete number) oscillations of the free electron gas density. For example, at optical frequencies, plasmons can couple with a photon to create another quasiparticle called a plasmon polariton.\n\nThe plasmon was initially proposed in 1952 by David Pines and David Bohm and was shown to arise from a Hamiltonian for the long-range electron-electron correlations.\n\nSince plasmons are the quantization of classical plasma oscillations, most of their properties can be derived directly from Maxwell's equations.\n\nPlasmons can be described in the classical picture as an oscillation of electron density with respect to the fixed positive ions in a metal. To visualize a plasma oscillation, imagine a cube of metal placed in an external electric field pointing to the right. Electrons will move to the left side (uncovering positive ions on the right side) until they cancel the field inside the metal. If the electric field is removed, the electrons move to the right, repelled by each other and attracted to the positive ions left bare on the right side. They oscillate back and forth at the plasma frequency until the energy is lost in some kind of resistance or damping. Plasmons are a quantization of this kind of oscillation.\n\nPlasmons play a large role in the optical properties of metals and semiconductors. Light of frequencies below the plasma frequency is reflected by a material because the electrons in the material screen the electric field of the light. Light of frequencies above the plasma frequency is transmitted by a material because the electrons in the material cannot respond fast enough to screen it. In most metals, the plasma frequency is in the ultraviolet, making them shiny (reflective) in the visible range. Some metals, such as copper and gold, have electronic interband transitions in the visible range, whereby specific light energies (colors) are absorbed, yielding their distinct color. In semiconductors, the valence electron plasmon frequency is usually in the deep ultraviolet, while their electronic interband transitions are in the visible range, whereby specific light energies (colors) are absorbed, yielding their distinct color which is why they are reflective. It has been shown that the plasmon frequency may occur in the mid-infrared and near-infrared region when semiconductors are in the form of nanoparticles with heavy doping.\n\nThe plasmon energy can often be estimated in the free electron model as\n\nwhere formula_6 is the conduction electron density, formula_7 is the elementary charge, formula_8 is the electron mass, formula_9 the permittivity of free space, formula_4 the reduced Planck constant and formula_11 the plasmon frequency.\n\nSurface plasmons are those plasmons that are confined to surfaces and that interact strongly with light resulting in a polariton. They occur at the interface of a material exhibiting positive real part of their relative permittivity, i.e. dielectric constant, (e.g. vacuum, air, glass and other dielectrics) and a material whose real part of permittivity is negative at the given frequency of light, typically a metal or heavily doped semiconductors. In addition to opposite sign of the real part of the permittivity, the magnitude of the real part of the permittivity in the negative permittivity region should typically be larger than the magnitude of the permittivity in the positive permittivity region, otherwise the light is not bound to the surface (i.e. the surface plasmons do not exist) as shown in the famous book by Raether. At visible wavelengths of light, e.g. 632.8 nm wavelength provided by a He-Ne laser, interfaces supporting surface plasmons are often formed by metals like silver or gold (negative real part permittivity) in contact with dielectrics such as air or silicon dioxide. The particular choice of materials can have a drastic effect on the degree of light confinement and propagation distance due to losses. Surface plasmons can also exist on interfaces other than flat surfaces, such as particles, or rectangular strips, v-grooves, cylinders, and other structures. Many structures have been investigated due to the capability of surface plasmons to confine light below the diffraction limit of light.\n\nSurface plasmons can play a role in surface-enhanced Raman spectroscopy and in explaining anomalies in diffraction from metal gratings (Wood's anomaly), among other things. Surface plasmon resonance is used by biochemists to study the mechanisms and kinetics of ligands binding to receptors (i.e. a substrate binding to an enzyme). Multi-parametric surface plasmon resonance can be used not only to measure molecular interactions, but also nanolayer properties or structural changes in the adsorbed molecules, polymer layers or graphene, for instance.\n\nSurface plasmons may also be observed in the X-ray emission spectra of metals. A dispersion relation for surface plasmons in the X-ray emission spectra of metals has been derived (Harsh and Agarwal).\nMore recently surface plasmons have been used to control colors of materials. This is possible since controlling the particle's shape and size determines the types of surface plasmons that can couple to it and propagate across it. This in turn controls the interaction of light with the surface. These effects are illustrated by the historic stained glass which adorn medieval cathedrals. In this case, the color is given by metal nanoparticles of a fixed size which interact with the optical field to give the glass its vibrant color. In modern science, these effects have been engineered for both visible light and microwave radiation. Much research goes on first in the microwave range because at this wavelength material surfaces can be produced mechanically as the patterns tend to be of the order a few centimeters. To produce optical range surface plasmon effects involves producing surfaces which have features <400 nm. This is much more difficult and has only recently become possible to do in any reliable or available way.\n\nRecently, graphene has also been shown to accommodate surface plasmons, observed via near field infrared optical microscopy techniques and infrared spectroscopy. Potential applications of graphene plasmonics mainly addressed the terahertz to midinfrared frequencies, such as optical modulators, photodetectors, biosensors.\n\nThe position and intensity of plasmon absorption and emission peaks are affected by molecular adsorption, which can be used in molecular sensors. For example, a fully operational device detecting casein in milk has been prototyped, based on detecting a change in absorption of a gold layer. Localized surface plasmons of metal nanoparticles can be used for sensing different types of molecules, proteins, etc.\n\nPlasmons are being considered as a means of transmitting information on computer chips, since plasmons can support much higher frequencies (into the 100 THz range, whereas conventional wires become very lossy in the tens of GHz). However, for plasmon-based electronics to be practical, a plasmon-based amplifier analogous to the transistor, called a plasmonstor, needs to be created.\n\nPlasmons have also been proposed as a means of high-resolution lithography and microscopy due to their extremely small wavelengths; both of these applications have seen successful demonstrations in the lab environment.\n\nFinally, surface plasmons have the unique capacity to confine light to very small dimensions, which could enable many new applications.\n\nSurface plasmons are very sensitive to the properties of the materials on which they propagate. This has led to their use to measure the thickness of monolayers on colloid films, such as screening and quantifying protein binding events. Companies such as Biacore have commercialized instruments that operate on these principles. Optical surface plasmons are being investigated with a view to improve makeup by L'Oréal and others.\n\nIn 2009, a Korean research team found a way to greatly improve organic light-emitting diode efficiency with the use of plasmons.\n\nA group of European researchers led by IMEC has begun work to improve solar cell efficiencies and costs through incorporation of metallic nanostructures (using plasmonic effects) that can enhance absorption of light into different types of solar cells: crystalline silicon (c-Si), high-performance III-V, organic, and dye-sensitized.\nFull color holograms using \"plasmonics\" have been demonstrated.\n\n\n"}
{"id": "1574904", "url": "https://en.wikipedia.org/wiki?curid=1574904", "title": "Prince Rupert's drop", "text": "Prince Rupert's drop\n\nPrince Rupert's drops (also known as Dutch or Batavian tears) are toughened glass beads created by dripping molten glass into cold water, which causes it to solidify into a tadpole-shaped droplet with a long, thin tail. These droplets are characterized internally by very high residual stresses, which give rise to counter-intuitive properties, such as the ability to withstand a blow from a hammer or a bullet on the bulbous end without breaking, while exhibiting explosive disintegration if the tail end is even slightly damaged. In nature, similar structures are produced under certain conditions in volcanic lava and are known as Pele's tears.\n\nThe drops are named after Prince Rupert of the Rhine, who brought them to England in 1660, although they were reportedly being produced in the Netherlands earlier in the 17th century and had probably been known to glassmakers for much longer. They were studied as scientific curiosities by the Royal Society and the unravelling of the principles of their unusual properties probably led to the development of the process for the production of toughened glass, patented in 1874. Research carried out in the 20th and 21st centuries shed further light on the reasons for the drops' contradictory properties.\n\nPrince Rupert's drops are produced by dropping molten glass drops into cold water. The water rapidly cools and solidifies the glass from the outside inward. This thermal quenching may be described using a simplified model of a rapidly cooled sphere. Prince Rupert's drops have remained a scientific curiosity for nearly 400 years due to two unusual mechanical properties — when the tail is snipped, the drop disintegrates explosively into powder, whereas the bulbous head can withstand compressive forces of up to .\n\nThe explosive disintegration arises due to multiple crack bifurcation events when the tail is cut — a single crack is accelerated in the tensile residual stress field in the center of the tail and bifurcates after it reaches a critical velocity of . Given these high speeds, the disintegration process due to crack bifurcation can only be inferred by looking into the tail and employing high speed imaging techniques. This is perhaps why this curious property of the drops remained unexplained for centuries.\n\nThe second unusual property of the drops, namely the strength of the heads, is a direct consequence of large compressive residual stresses—up to —that exist in the vicinity of the head's outer surface. This stress distribution is measured by using glass's natural property of stress-induced birefringence and by employing techniques of 3D photoelasticity. The high fracture toughness due to residual compressive stresses makes Prince Rupert's drops one of the earliest examples of toughened glass.\n\nA scholarly account of the early history of Prince Rupert’s drops is given in the \"Notes and Records\" of the Royal Society of London. Most of the early scientific study of the drops was performed at the Royal Society.\n\nThe drops are reliably reported to have been made in Mecklenburg in North Germany, as early as 1625. However, it has been claimed that they were invented in the Netherlands (although it has been suggested that they had been known about by glassmakers since the time of the Roman Empire), hence common names for them in the 17th century were \"lacrymae Borussicae\" (Prussian tears) or \"lacrymae Batavicae\" (Dutch tears). The secret of how to make them remained in the Mecklenburg area for some time, although the drops were disseminated across Europe from there, for sale as toys or curiosities.\n\nThe Dutch scientist Constantijn Huygens asked Margaret Cavendish, Duchess of Newcastle to investigate the properties of the drops; her opinion after carrying out experiments was that a small amount of volatile liquid was trapped inside.\n\nAlthough Prince Rupert did not discover the drops, he played a role in their history by bringing them to Britain in 1660. He gave them to King Charles II, who in turn delivered them in 1661 to the Royal Society (which had been created the previous year) for scientific study. Several early publications from the Royal Society give accounts of the drops and describe experiments performed. Among these publications was \"Micrographia\" of 1665 by Robert Hooke, who later would discover Hooke’s Law. His publication laid out correctly most of what can be said about Prince Rupert’s drops without a fuller understanding than existed at the time, of elasticity (to which Hooke himself later contributed) and of the failure of brittle materials from the propagation of cracks. A fuller understanding of crack propagation had to wait until the work of A. A. Griffith in 1920.\nIn 1994, Srinivasan Chandrasekar, an engineering professor at Purdue University, and Munawar Chaudhri, head of the materials group at the University of Cambridge, used high-speed framing photography to observe the drop-shattering process and concluded that while the surface of the drops experiences highly compressive stresses, the inside experience high tension forces, creating a state of unequal equilibrium which can easily be disturbed by breaking the tail. However, this left the question of how the stresses are distributed throughout a Prince Rupert's drop.\n\nIn a further study published in 2017, the team collaborated with Hillar Aben, a professor at Tallinn University of Technology in Estonia using a transmission polariscope to measure the optical retardation of light from a red LED as it travelled through the glass drop, and used the data to construct the stress distribution throughout the drop. This showed that the heads of the drops have a much higher surface compressive stress than previously thought at up to , but that this surface compressive layer is also thin, only about 10% of the diameter of the head of a drop. This gives the surface a high fracture strength which means that it is necessary to create a crack that enters the interior tension zone in order to break the droplet. As cracks on the surface tend to grow parallel to the surface, they cannot enter the tension zone but a disturbance in the tail allows cracks to enter the tension zone.\n\nThe process for the production of toughened glass by quenching was probably inspired by the study of the drops, as it was patented in England by Parisian Francois Barthelemy Alfred Royer de la Bastie, in 1874, just one year after V. De Luynes had published accounts of his experiments with them.\nIt has been known since at least the 19th century that formations similar to Prince Rupert's drops are produced under certain conditions in volcanic lava. More recently researchers at the University of Bristol and the University of Iceland have studied the glass particles produced by explosive fragmentation of Prince Rupert's drops in the laboratory to better understand magma fragmentation and ash formation driven by stored thermal stresses in active volcanoes.\n\nBecause of their use as a party piece, Prince Rupert’s drops became widely known in the late 17th century—far more than today. It can be seen that educated people (or those in \"society\") were expected to be familiar with them, from their use in the literature of the day. Samuel Butler used them as a metaphor in his poem \"Hudibras\" in 1663, and Pepys refers to them in his diary.\n\nThe drops were immortalized in a verse of the anonymous \"\" (1663):\nDiarist George Templeton Strong wrote (volume 4, p.122) of a hazardous sudden breaking up of pedestrian-bearing ice in New York City's East River during the winter of 1867 that \"The ice flashed into fragments all at once like a Prince Rupert's drop.\"\n\nSigmund Freud, discussing the dissolution of military groups in \"Group Psychology and the Analysis of the Ego\" (1921), notes the panic that results from the loss of the leader: \"The group vanishes in dust, like a Prince Rupert's drop when its tail is broken off.\"\n\nIn the 1940 detective novel \"There Came Both Mist and Snow\" by Michael Innes (J.I.M. Stewart), a character incorrectly refers to them as \"Verona drops\"; the error is corrected towards the end of the novel by the detective Sir John Appleby.\n\nIn his 1943 novella \"Conjure Wife\", Fritz Leiber uses Prince Rupert drops as a metaphor for the volatility of several characters' personalities. These small-town college faculty people seem to be placid and impervious, but \"explode\" at a mere \"flick of the filament\".\n\nPeter Carey devotes a chapter to the drops in his 1988 novel \"Oscar and Lucinda\".\n\nAlfred Jarry's novel \"Supermale\" makes reference to the drops in an analogy for the molten glass drops falling from a failed device meant to pass eleven thousand volts of electricity through the supermale's body.\n\n\n"}
{"id": "23615266", "url": "https://en.wikipedia.org/wiki?curid=23615266", "title": "Salvage rider", "text": "Salvage rider\n\nThe 1995 salvage rider was a provision in the Emergency Supplemental Appropriations Act of 1995 (P.L. 104-19, Sec. 2001, July 27, 1995) to expand salvage timber sales from July 27, 1995, through December 31, 1996, by exempting them from public challenges under environmental laws. This was controversial because it reinstated numerous timber sales in Washington and Oregon that had been stopped to protect endangered and threatened species habitat.\n"}
{"id": "118575", "url": "https://en.wikipedia.org/wiki?curid=118575", "title": "Soft gamma repeater", "text": "Soft gamma repeater\n\nA soft gamma repeater (SGR) is an astronomical object which emits large bursts of gamma-rays and X-rays at irregular intervals. It is conjectured that they are a type of magnetar or, alternatively, neutron stars with fossil disks around them.\n\nOn March 5, 1979 a powerful gamma-ray burst was noted. As a number of receivers at different locations in the Solar System saw the burst at slightly different times, its direction could be determined, and it was shown to originate from near a supernova remnant in the Large Magellanic Cloud.\n\nOver time it became clear that this was not a normal gamma-ray burst. The photons were less energetic in the soft gamma-ray and hard X-ray range, and repeated bursts came from the same region.\n\nAstronomer Chryssa Kouveliotou of the Universities Space Research Association (USRA) at NASA's Marshall Space Flight Center decided to test the theory that soft gamma repeaters were magnetars. According to the theory, the bursts would cause the object to slow down its rotation. In 1998, she made careful comparisons of the periodicity of soft gamma repeater SGR 1806-20. The period had increased by 0.008 seconds since 1993, and she calculated that this would be explained by a magnetar with a magnetic-field strength of 8×10 teslas (8×10 gauss). This was enough to convince the international astronomical community that soft gamma repeaters are indeed magnetars.\n\nAn unusually spectacular soft gamma repeater burst was SGR 1900+14 observed on August 27, 1998. Despite the large distance to this SGR, estimated at 20,000 light years, the burst had large effects on the Earth's atmosphere. The atoms in the ionosphere, which are usually ionized by the Sun's radiation by day and recombine to neutral atoms by night, were ionized at nighttime at levels not much lower than the normal daytime level. The Rossi X-Ray Timing Explorer (RXTE), an X-ray satellite, received its strongest signal from this burst at this time, even though it was directed at a different part of the sky, and should normally have been shielded from the radiation.\n\nKnown soft gamma repeaters include:\nThe numbers give the position in the sky, for example, SGR 0525-66 has a right ascension of 5h25m and a declination of −66°. The date of discovery sometimes appears in a format such as 1979/1986 to refer to the year the object was discovered, in addition to the year soft gamma repeaters were recognized as a separate class of objects rather than \"normal\" gamma-ray bursts.\n\n\n"}
{"id": "22149570", "url": "https://en.wikipedia.org/wiki?curid=22149570", "title": "Solar micro-inverter", "text": "Solar micro-inverter\n\nA solar micro-inverter, or simply microinverter, is a plug-and-play device used in photovoltaics, that converts direct current (DC) generated by a single solar module to alternating current (AC). Microinverters contrast with conventional string and central solar inverters, in which a single inverter is connected to multiple solar panels. The output from several microinverters can be combined and often fed to the electrical grid.\n\nMicroinverters have several advantages over conventional inverters. The main advantage is that they electrically isolate the panels from each other, so small amounts of shading, debris or snow lines on any one solar module, or even a complete module failure, does not disproportionately reduce the output of the entire array. Each microinverter harvests optimum power by performing maximum power point tracking (MPPT) for its connected module. Simplicity in system design, lower amperage wires, simplified stock management, and added safety are other factors introduced with the microinverter solution.\n\nThe primary disadvantages of a microinverter include a higher initial equipment cost per peak watt than the equivalent power of a central inverter since each inverter needs to be installed adjacent to a panel (usually on a roof). This also makes them harder to maintain and more costly to remove and replace. Some manufacturers have addressed these issues with panels with built-in microinverters. A microinverter has often a longer lifespan than a central inverter, which will need replacement during the lifespan of the solar panels. Therefore the financial disadvantage at first may become an advantage on the long term.\n\nA type of technology similar to a microinverter is a power optimizer which also does panel-level maximum power point tracking, but does not convert to AC per module.\n\nSolar panels produce direct current at a voltage that depends on module design and lighting conditions. Modern modules using 6-inch cells typically contain 60 cells and produce a nominal 24-30 V. (so inverters are ready for 24-50 V).\n\nFor conversion into AC, panels may be connected in series to produce an array that is effectively a single large panel with a nominal rating of 300 to 600 VDC. The power then runs to an inverter, which converts it into standard AC voltage, typically 230 VAC / 50 Hz or 240 VAC / 60 Hz.\n\nThe main problem with the \"string inverter\" approach is the string of panels acts as if it were a single larger panel with a max current rating equivalent to the poorest performer in the string. For example, if one panel in a string has 5% higher resistance due to a minor manufacturing defect, the entire string suffers a 5% performance loss. This situation is dynamic. If a panel is shaded its output drops dramatically, affecting the output of the string, even if the other panels are not shaded. Even slight changes in orientation can cause output loss in this fashion. In the industry, this is known as the \"Christmas-lights effect\", referring to the way an entire string of series-strung Christmas tree lights will fail if a single bulb fails. However, this effect is not entirely accurate and ignores the complex interaction between modern string inverter maximum power point tracking and even module bypass diodes. Shade studies by major microinverter and DC optimizer companies show small yearly gains in light, medium and heavy shaded conditions- 2%, 5% and 8% respectively- over an older string inverter.\n\nAdditionally, the efficiency of a panel's output is strongly affected by the load the inverter places on it. To maximize production, inverters use a technique called maximum power point tracking to ensure optimal energy harvest by adjusting the applied load. However, the same issues that cause output to vary from panel to panel, affect the proper load that the MPPT system should apply. If a single panel operates at a different point, a string inverter can only see the overall change, and moves the MPPT point to match. This results in not just losses from the shadowed panel, but the other panels too. Shading of as little as 9% of the surface of an array can, in some circumstances, reduce system-wide power as much as 54%. However, as stated above, these yearly yield losses are relatively small and newer technologies allow some string inverters to significantly reduce the effects of partial shading.\n\nAnother issue, though minor, is that string inverters are available in a limited selection of power ratings. This means that a given array normally up-sizes the inverter to the next-largest model over the rating of the panel array. For instance, a 10-panel array of 2300 W might have to use a 2500 or even 3000 W inverter, paying for conversion capability it cannot use. This same issue makes it difficult to change array size over time, adding power when funds are available (modularity). If the customer originally purchased a 2500 W inverter for their 2300 W of panels, they cannot add even a single panel without over-driving the inverter. However, this over sizing is considered common practice in today's industry (sometimes as high as 20% over inverter nameplate rating) to account for module degradation, higher performance during winter months or to achieve higher sell back to the utility.\n\nOther challenges associated with centralized inverters include the space required to locate the device, as well as heat dissipation requirements. Large central inverters are typically actively cooled. Cooling fans make noise, so location of the inverter relative to offices and occupied areas must be considered. And because cooling fans have moving parts, dirt, dust, and moisture can negatively affect their performance over time. String inverters are quieter but might produce a humming noise in late afternoon when inverter power is low.\n\nMicroinverters are small inverters rated to handle the output of a single panel. Modern grid-tie panels are normally rated between 225 and 275W, but rarely produce this in practice, so microinverters are typically rated between 190 and 220 W (someones, 100W). Because it is operated at this lower power point, many design issues inherent to larger designs simply go away; the need for a large transformer is generally eliminated, large electrolytic capacitors can be replaced by more reliable thin-film capacitors, and cooling loads are reduced so no fans are needed. Mean time between failures (MTBF) are quoted in hundreds of years.\n\nMore importantly, a microinverter attached to a single panel allows it to isolate and tune the output of that panel. For example, in the same 10-panel array used as an example above, with microinverters any panel that is under-performing has no effect on panels around it. In that case, the array as a whole produces as much as 5% more power than it would with a string inverter. When shadowing is factored in, if present, these gains can become considerable, with manufacturers generally claiming 5% better output at a minimum, and up to 25% better in some cases. Furthermore, a single model can be used with a wide variety of panels, new panels can be added to an array at any time, and do not have to have the same rating as existing panels.\n\nSometimes, as much as two solar panels are attached to the same microinverters (duo microinverter). The power that inputs the microinverter is then ≥600W and 24 V (i.e. as said, two 12 V solar panel can be tied together). Microinverter then converts power provided by solar panel(s) into standard AC voltage, typically 230 VAC / 50 Hz or 240 VAC / 60 Hz. The typical size of this microinverter is: 22x16.4x5.2cm / 8.66x6.46x2.05\".\n\nAs said, microinverters produce grid-matching power directly at the back of the panel (i.e. 220V). Arrays of panels are connected in parallel to each other, and then to the grid. This has the major advantage that a single failing panel or inverter cannot take the entire string offline. Combined with the lower power and heat loads, and improved MTBF, some suggest that overall array reliability of a microinverter-based system is significantly greater than a string inverter-based one. This assertion is supported by longer warranties, typically 15 to 25 years, compared with 5 or 10 year warranties that are more typical for string inverters. Additionally, when faults occur, they are identifiable to a single point, as opposed to an entire string. This not only makes fault isolation easier, but unmasks minor problems that might not otherwise become visible – a single under-performing panel may not affect a long string's output enough to be noticed.\n\nThe main disadvantage of the microinverter concept has, until recently, been cost. Because each microinverter has to duplicate much of the complexity of a string inverter but spread that out over a smaller power rating, costs on a per-watt basis are greater. This offsets any advantage in terms of simplification of individual components. As of October 2010, a central inverter costs approximately $0.40 per watt, whereas a microinverter costs approximately $0.52 per watt. Like string inverters, economic considerations force manufacturers to limit the number of models they produce. Most produce a single model that may be over or undersize when matched with a specific panel.\n\nIn many cases the packaging can have a significant effect on price. With a central inverter you may have only one set of panel connections for dozens of panels, a single AC output, and one box. With microinverters, each one has to have its own set of inputs and outputs, in its own box. Because that box is on the roof, it has to be sealed and weatherproofed. This can represent a significant portion of the overall price-per-watt.\n\nTo further reduce costs, some models control two or three panels from a single box, reducing the packaging and associated costs. Some systems simply place two entire micros in a single box, while others duplicate only the MPPT section of the system and use a single DC-to-AC stage for further cost reductions. Some have suggested that this approach will make microinverters comparable in cost with those using string inverters. With steadily decreasing prices, the introduction of dual microinverters and the advent of wider model selections to match PV module output more closely, cost is less of an obstacle so microinverters may now spread more widely.\n\nMicroinverters have become common where array sizes are small and maximizing performance from every panel is a concern. In these cases, differential in price-per-watt is minimized due to the small number of panels, and has little effect on overall system cost. The improvement in energy harvest given a fixed size array can offset this difference in cost. For this reason, microinverters have been most successful in the residential market, where limited space for panels constrains array size, and shading from nearby trees or other objects is often an issue. Microinverter manufacturers list many installations, some as small as a single panel and the majority under 50.\n\nAn often overlooked disadvantage of micro inverters is the future operation and maintenance costs associated with them. While the technology has improved over the years the fact remains that the devices will eventually either fail or wear out. The installer must balance these replacement costs (around $400 per truck roll), increased safety risks to personnel, equipment and module racking against the profit margins for the installation. For homeowners, the eventual wear out or premature device failures will introduce potential damage to the roof tiles or shingles, property damage and other nuisances.\n\nWhile microinverters generally have a lower efficiency than string inverters, the overall efficiency is increased due to the fact that every inverter / panel unit acts independently. In a string configuration, when a panel on a string is shaded, the output of the entire string of panels is reduced to the output of the lowest producing panel. This is not the case with micro inverters.\n\nA further advantage is found in the panel output quality. The rated output of any two panels in the same production run can vary by as much as 10% or more. This is mitigated with a string configuration but not so in a microinverter configuration. The result is maximum power harvesting from a microinverter array.\n\nMonitoring and maintenance is also easier as many microinverter producers provide apps or websites to monitor the power output of their units. In many cases, these are proprietary; however this is not always the case. Following the demise of Enecsys, and the subsequent closure of their site; a number of private sites such as Enecsys-Monitoring sprung up to enable owners to continue to monitor their systems.\n\nEfficient conversion of DC power to AC requires the inverter to store energy from the panel while the grid's AC voltage is near zero, and then release it again when it rises. This requires considerable amounts of energy storage in a small package. The lowest-cost option for the required amount of storage is the electrolytic capacitor, but these have relatively short lifetimes normally measured in years, and those lifetimes are shorter when operated hot, like on a rooftop solar panel. This has led to considerable development effort on the part of microinverter developers, who have introduced a variety of conversion topologies with lowered storage requirements, some using the much less capable but far longer lived thin film capacitors where possible.\n\nThree-phase electric power represents another solution to the problem. In a three-phase circuit, the power does not vary between (say) +120 to -120 Volts between two lines, but instead varies between 60 and +120 or -60 and -120V, and the periods of variation are much shorter. Inverters designed to operate on three phase systems require much less storage. A three-phase micro using zero-voltage switching can also offer higher circuit density and lower cost components, while improving conversion efficiency to over 98%, better than the typical one-phase peak around 96%.\n\nThree-phase systems, however, are generally only seen in industrial and commercial settings. These markets normally install larger arrays, where price sensitivity is the highest. Uptake of three-phase micros, in spite of any theoretical advantages, appears to be very low.\n\nMicroinverters protection usually include: anti-islanding; short circuit; \nreverse polarity; low voltage; over voltage and over temperature.\n\nFoldable solar panel with AC microinverters can be used to recharge laptops and some electric vehicles.\n\nThe microinverter concept has been in the solar industry since its inception. However, flat costs in manufacturing, like the cost of the transformer or enclosure, scaled favorably with size, and meant that larger devices were inherently less expensive in terms of price per watt. Small inverters were available from companies like ExelTech and others, but these were simply small versions of larger designs with poor price performance, and were aimed at niche markets.\n\nIn 1991 the US company Ascension Technology started work on what was essentially a shrunken version of a traditional inverter, intended to be mounted on a panel to form an \"AC panel\". This design was based on the conventional linear regulator, which is not particularly efficient and dissipates considerable heat. In 1994 they sent an example to Sandia Labs for testing. In 1997, Ascension partnered with US panel company ASE Americas to introduce the 300 W SunSine panel.\n\nDesign of, what would today be recognized as a \"true\" microinverter, traces its history to late 1980s work by Werner Kleinkauf at the ISET (\"Institut für Solare Energieversorgungstechnik\"), now Fraunhofer Institute for Wind Energy and Energy System Technology. These designs were based on modern high-frequency switching power supply technology, which is much more efficient. His work on \"module integrated converters\" was highly influential, especially in Europe.\n\nIn 1993 Mastervolt introduced their first grid-tie inverter, the Sunmaster 130S, based on a collaborative effort between Shell Solar, Ecofys and ECN. The 130 was designed to mount directly to the back of the panel, connecting both AC and DC lines with compression fittings. In 2000, the 130 was replaced by the Soladin 120, a microinverter in the form of an AC adapter that allows panels to be connected simply by plugging them into any wall socket.\n\nIn 1995, OKE-Services designed a new high-frequency version with improved efficiency, which was introduced commercially as the OK4-100 in 1995 by NKF Kabel, and re-branded for US sales as the Trace Microsine. A new version, the OK4All, improved efficiency and had wider operating ranges.\n\nIn spite of this promising start, by 2003 most of these projects had ended. Ascension Technology was purchased by Applied Power Corporation, a large integrator. APC was in turn purchased by Schott in 2002, and SunSine production was canceled in favor of Schott's existing designs. NKF ended production of the OK4 series in 2003 when a subsidy program ended. Mastervolt has moved on to a line of \"mini-inverters\" combining the ease-of-use of the 120 in a system designed to support up to 600 W of panels.\n\nIn the aftermath of the 2001 Telecoms crash, Martin Fornage of Cerent Corporation was looking for new projects. When he saw the low performance of the string inverter for the solar array on his ranch, he found the project he was looking for. In 2006 he formed Enphase Energy (now integrated in Siemens) with another Cerent engineer, Raghu Belur, and they spent the next year applying their telecommunications design expertise to the inverter problem.\n\nReleased in 2008, the Enphase M175 model was the first commercially successful microinverter. A successor, the M190, was introduced in 2009, and the latest model, the M215, in 2011. Backed by $100 million in private equity, Enphase quickly grew to 13% marketshare by mid-2010, aiming for 20% by year-end. They shipped their 500,000th inverter in early 2011, and their 1,000,000th in September of the same year. In early 2011, they announced that re-branded versions of the new design will be sold by Siemens directly to electrical contractors for widespread distribution.\n\nEnphase has subscribed an agreement with EnergyAustralia, to market its micro-inverter technology.\n\nEnphase's success did not go unnoticed, and since 2010 a host of competitors have appeared. Many of these are identical to the M190 in specs, and even in the casing and mounting details. Some differentiate by competing head-to-head with Enphase in terms of price or performance, while others are attacking niche markets.\n\nLarger firms have also stepped into the field: SMA, Enecsys and iEnergy. \n\nOKE-Services updated OK4-All product was recently bought by SMA and released as the SunnyBoy 240 after an extended gestation period, while Power-One has introduced the AURORA 250 and 300. Other major players included Enecsys and SolarBridge, especially outside the North American market. The only USA made microinverter in production is from Chilicon Power. Since 2009, several companies from Europe to China, including major central inverter manufacturers, have launched microinverters—validating the microinverter as an established technology and one of the biggest technology shifts in the PV industry in recent years.\n\nAPsystems is marketing inverters for up to four solar modules a microinverters, including the three-phase YP1000 with an AC output of up to 900 Watt.\n\nIn 2018, there are 20 microinverter manufacturers around the world including: Apparent, Delta, Sparq, Kaco, ABB, Array Converter, Chilicon Power, GreenRay Solar, Azuray Technologies, Petra Solar, Direct Grid, Accurate Solar, OKE/SMA, Exeltech, National Semiconductor, Larankelo, Enphase, APsystems, Northern Electric & Power (Northernep / NEP), ReneSola (Micro Replus), SolarEpic, SWEA and Plug & Power. \n\nThere is a growing list of big name PV companies around the world who have partnered with microinverter companies to produce and sell AC solar panels, including Trina Solar, BenQ, LG, Canadian Solar, Suntech, SunPower, NESL, Hanwha SolarOne, Sharp and other ones that are just joining.\n\nThe period between 2009 and 2012 included unprecedented downward price movement in the PV market. At the beginning of this period, wholesale pricing for panels was generally around $2.00 to $2.50/W, and inverters around 50 to 65 cents/W. By the end of 2012, panels were widely available in wholesale at 65 to 70 cents, and string inverters around 30 to 35 cents/W. In comparison, microinverters have proven relatively immune to these same sorts of price declines, moving from about 65 cents/W to 50 to 55 once cabling is factored in. This could lead to widening losses as the suppliers attempt to remain competitive.\n\nNevertheless, in 2018 some DC 12 / 24 V to AC 110 / 220V inverters are sold by $0.06/W (ie 100W microinverter by $6.81).\n\n\n\n\n"}
{"id": "2046625", "url": "https://en.wikipedia.org/wiki?curid=2046625", "title": "Steam whistle", "text": "Steam whistle\n\nA steam whistle is a device used to produce sound with the aid of live steam, which acts as a vibrating system (compare to train horn).\n\nThe whistle consists of the following main parts, as seen on the drawing: the whistle bell (1), the steam orifice or aperture (2), and the valve (9).\n\nWhen the lever (10) is pulled (usually via a pull cord), the valve opens and lets the steam escape through the orifice. The steam will alternately compress and rarefy in the bell, creating the sound. The pitch, or tone, is dependent on the length of the bell; and also how far the operator has opened the valve. Some locomotive engineers invented their own style of whistling.\n\nSteam whistles were often used in factories, and similar places to signal the start or end of a shift, etc. Railway locomotives, traction engines, and steam ships have traditionally been fitted with a whistle for warning and communication purposes. Large diameter steam whistles were used on light houses, likely beginning in the 1850s.\n\nThe earliest use of steam whistles was as boiler low-water alarms in the 18th century and early 19th century. During the 1830s, whistles were adopted by railroads and steamship companies.\n\nSteam warning devices have been used on trains since 1833 when George Stephenson invented and patented a steam trumpet for use on the Leicester and Swannington Railway.\nPeriod literature makes a distinction between a steam trumpet and a steam whistle.\nA copy of the trumpet drawing signed May 1833 shows a device about eighteen inches high with an ever-widening trumpet shape with a six-inch diameter at its top or mouth. It is said that George Stephenson invented his trumpet after an accident on the Leicester and Swannington Railway where a train hit either a cart, or a herd of cows, on a level crossing and there were calls for a better way of giving a warning. Although no-one was injured, the accident was deemed serious enough to warrant Stephenson’s personal intervention. One account states that [driver] Weatherburn had `mouthblown his horn' at the crossing in an attempt to prevent the accident, but that no attention had been paid to this audible warning, perhaps because it had not been heard.\n\nStephenson subsequently called a meeting of directors and accepted the suggestion of the company manager, Ashlin Bagster, that a horn or whistle which could be activated by steam should be constructed and fixed to the locomotives. Stephenson later visited a musical instrument maker on Duke Street in Leicester, who on Stephenson's instructions constructed a ‘Steam Trumpet’ which was tried out in the presence of the board of Directors ten days later.\n\nStephenson mounted the trumpet on the top of the boiler's steam dome, which delivers dry steam to the cylinders. The company went on to mount the device on its other locomotives\n\nLocomotive steam trumpets were soon replaced by steam whistles. Air whistles were used on some Diesel and electric locomotives, but these mostly employ air horns.\n\nAn array of steam whistles arranged to play music is referred to as a calliope.\n\nIn York, Pennsylvania, a variable pitch steam whistle at the New York Wire Company has been played annually on Christmas Eve since 1925 (except in 1986 and 2005) in what has come to be known as \"York's Annual Steam Whistle Christmas Concert\". On windy nights, area residents report hearing the concert as far as 12 to 15 miles away. The whistle, which is in the Guinness Book of World Records, was powered by an air compressor during the 2010 concert due to the costs of maintaining and running the boiler.\n\n\n\nA whistle has a characteristic natural resonant frequency that can be detected by gently blowing human breath across the whistle rim, much as one might blow over the mouth of a bottle. The active sounding frequency (when the whistle is blown on steam) may differ from the natural frequency as discussed below. These comments apply to whistles with a mouth area at least equal to the cross-sectional area of the whistle.\n\n\n\n\n\n\nWhistle sound level varies with several factors:\n\n\n\n\nAcoustic length or effective length is the quarter wavelength generated by the whistle. It is calculated as one quarter the ratio of speed of sound to the whistle’s frequency. Acoustic length may differ from the whistle’s physical length, also termed geometric length. depending upon mouth configuration, etc. The end correction is the difference between the acoustic length and the physical length above the mouth. The end correction is a function of diameter whereas the ratio of acoustic length to physical length is a function of scale. These calculations are useful in whistle design to obtain a desired sounding frequency. Working length in early usage meant whistle acoustic length, i.e., the effective length of the \"working\" whistle, but recently has been used for physical length including the mouth.\n\nLoudness is a subjective perception that is influenced by sound pressure level, sound duration, and sound frequency. High sound pressure level potential has been claimed for the whistles of Vladimir Gavreau, who tested whistles as large as 1.5 meter (59-inch) diameter (37 Hz).\nA 20-inch diameter ring-shaped whistle (“Ultrawhistle”) patented and produced by Richard Weisenberger sounded 124 decibels at 100 feet. The variable pitch steam whistle at the New York Wire Company in York, Pennsylvania, was entered in the Guinness Book of World Records in 2002 as the loudest steam whistle on record at 124.1dBA from a set distance used by Guinness. The York whistle was also measured at 134.1 decibels from a distance of 23-feet.\n\nA fire-warning whistle supplied to a Canadian saw mill by the Eaton, Cole, and Burnham Company in 1882 measured 20 inches in diameter, four feet nine inches from bowl to ornament, and weighed 400 pounds. The spindle supporting the whistle bell measured 3.5 inches diameter and the whistle was supplied by a four-inch feed pipe.\nOther records of large whistles include an 1893 account of U.S. President Grover Cleveland activating the “largest steam whistle in the world,” said to be “five feet” at the Chicago World's Fair.\nThe sounding chamber of a whistle installed at the 1924 Long-Bell Lumber Company, Longview, Washington measured 16 inches diameter x 49 inches in length.\nThe whistle bells of multi-bell chimes used on ocean liners such as the \"RMS Titanic\" measured 9, 12, and 15 inches diameter.\nThe whistle bells of the Canadian Pacific steamships \"Assiniboia\" and \"Keewatin\" measured 12 inches in diameter and that of the Keewatin measured 60 inches in length.\nA multi-bell chime whistle installed at the Standard Sanitary Manufacturing Company in 1926 was composed of five separate whistle bells measuring 5 x15, 7 x 21, 8x 24, 10 x 30, and 12 x36 inches, all plumbed to a five-inch steam pipe.\nThe Union Water Meter Company of Worcester Massachusetts produced a gong whistle composed of three bells, 8 x 9-3/4, 12 x 15, and 12 x 25 inches. Twelve-inch diameter steam whistles were commonly used at light houses in the 19th century.\nIt has been claimed that the sound level of an Ultrawhistle would be significantly greater than that of a conventional whistle, but comparative tests of large whistles have not been undertaken. Tests of small Ultrawhistles have not shown higher sound levels compared to conventional whistles of the same diameter.\n\n"}
{"id": "9759634", "url": "https://en.wikipedia.org/wiki?curid=9759634", "title": "Survive To Fight", "text": "Survive To Fight\n\nSurvive To Fight is the title of a British Army publication which details the use of NBC protective equipment and other procedures to be carried in the event of an attack with nuclear, biological or chemical weapons. So far four editions have been published; the first three of which are in the form of a ring-bound manual with a plastic cover, and the latest, Edition 4, is a paperback booklet. Edition I covers the use of the S6 Respirator and Mk.III NBC suit and overboots, Edition II featuring the S10 Respirator and Mk.IV suit and overboots which were introduced since, and Edition III (1995) is a revised version of II, and features the addition of the Mk.V overboots. A concise aide-memoire, containing much of the same information as in the manuals is also issued, in the form of a tall card which is stored in the respirator haversack, with different versions being issued to the British Army and Navy in accordance with differing requirements for each service.\n\n"}
{"id": "49634756", "url": "https://en.wikipedia.org/wiki?curid=49634756", "title": "Upper Kaleköy Dam", "text": "Upper Kaleköy Dam\n\nThe Upper Kaleköy Dam, also known as the Yukarı Kaleköy Dam, is a gravity dam currently under construction on the Murat River near the town of Kale in Solhan district of Bingöl Province, eastern Turkey. Construction on the dam began in 2012 and it is expected to be completed in 2018. It is one of six major dams planned for the river. Its primary purpose is hydroelectric power generation and it will support a 636.6 MW hydroelectric power station. The tall dam will withhold a reservoir of . It is owned by Kalehan Energy Generation.\n\n"}
{"id": "1693910", "url": "https://en.wikipedia.org/wiki?curid=1693910", "title": "Vaneless ion wind generator", "text": "Vaneless ion wind generator\n\nA vaneless ion wind generator or power fence is a proposed wind power device that produces electrical energy directly by using the wind to pump electric charge from one electrode to another, with no moving parts.\n\nWind energy is usually extracted to make electricity by means of a wind turbine. The bird deaths, vibrational noise, and moving shadows associated with wind turbines would not occur with ion based power generation.\n\nOne design uses water sprayed from a nozzle facing a toroidal charged electrode. This induces an opposite charge in the water, and when the water flows out of the nozzle, each drop carries a small amount of charge. These water droplets are then blown by the wind, going through the center of the charged toroid without touching it. The droplets then hit a fine mesh, adding to its charge. \n\nThe other alternative is to use the Earth as the second electrode. The main advantage of this system is that it has no moving parts except the water droplets. The disadvantages are that it needs a constant supply of water, its wind profile can't be reduced, it requires many small parts, and it has to be well-crafted to reduce corona discharge losses. The device would produce direct current, which would need to be inverted to apply to a standard AC power grid.\n\nLord Kelvin created a similar device that used the energy of \"falling\" water droplets to generate high voltage, sometimes called \"Kelvin's Thunderstorm\".\n\n\n\n"}
{"id": "31232405", "url": "https://en.wikipedia.org/wiki?curid=31232405", "title": "Wooden toymaking in the Ore Mountains", "text": "Wooden toymaking in the Ore Mountains\n\nThe history of wooden toymaking in the Ore Mountains is closely bound to regional circumstances. The Ore Mountains are located in Central Europe on the border between Germany and the Czech Republic. For many centuries it was a countryside in which the local population eked out a hard existence from the land. Long and harsh winters restricted agriculture; in addition the region had very poor communications. With the onset of ore mining a new line of commerce developed, but the hard labour and high risks involved meant that it was only work for young and strong men. Many were injured or died as a result of accidents. The miners rose early in the morning whilst it was still dark in order to go to work and did not return home again until late in the evening after dark. From those times comes a custom that has survived to the present day, the practice of placing lights in the window. These lights were intended to show the miners the safe way back to the homes of their families.\n\nIt was not long before the winnings of silver ore fell in many parts of the mountains and numerous pits had to close. This hardship forced its inhabitants to look for other work and, thanks to abundance of timber in the region, the manufacture of wooden toys became an important secondary source of income. Entire families were engaged in wooden toymaking, especially in the harsh winters. Child labour in conditions of poor lighting and equipment was the rule rather than the exception. The children often had to work more than 12 hours a day. Families developed a high degree of specialisation. For example, an experienced turner would be working on turning animal shapes (\"Reifendrehen\"), another man carved the animal figures out of it and another family took over the painting and the manufacture of small boxes. Earnings from the individual steps in the process were very low. Production was usually bought by travelling merchants who used their position without mercy, to force down the prices. The majority of these products was taken to the toy markets of Nuremberg and redistributed from there. The formation of associations and cooperatives (such as Dregeno) was in order to ensure a minimum level of income.\n\nProduction is concentrated today in the toymaking village of Seiffen and its surrounding area in the middle of the so-called \"German Christmas Land\" as the Ore Mountains are called. A great variety of products has developed, but they are clearly associated with the Ore Mountains and go by the concept of Ore Mountain folk art. They include many typical wooden Ore Mountain products such as Christmas pyramids, wooden \"Räuchermann\" incense smokers, nutcrackers, wooden figures (Christmas Angels, miners, turned animals, etc.), Christmas mountain scenes (\"Weihnachtsberg\"e) and Ore Mountain candle arches.\n\n\n"}
