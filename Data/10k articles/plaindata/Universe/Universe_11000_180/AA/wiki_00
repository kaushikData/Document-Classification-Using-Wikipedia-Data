{"id": "48927277", "url": "https://en.wikipedia.org/wiki?curid=48927277", "title": "Aliso Canyon gas leak", "text": "Aliso Canyon gas leak\n\nThe Aliso Canyon gas leak (also called Porter Ranch gas leak and Porter Ranch gas blowout) was a massive natural gas leak that was discovered by SoCalGas employees on October 23, 2015. Gas was escaping from a well within the Aliso Canyon's underground storage facility in the Santa Susana Mountains near Porter Ranch, Los Angeles. This second-largest gas storage facility of its kind in the United States belongs to the Southern California Gas Company, a subsidiary of Sempra Energy. On January 6, 2016, Governor Jerry Brown issued a state of emergency. The Aliso gas leak's carbon footprint is said to be larger than the \"Deepwater Horizon\" leak in the Gulf of Mexico. On February 11, 2016, the gas company reported that it had the leak under control. On February 18, 2016, state officials announced that the leak was permanently plugged.\n\nAn estimated (0.000097 Gt) of methane and of ethane were released into the atmosphere, The initial effect of the release increased the estimated 5.3 Gt of methane in the Earth's atmosphere by about 0.002%, diminishing to half that in 6-8 years.\n\nIt was widely reported to have been the worst single natural gas leak in U.S. history in terms of its environmental impact. By comparison, the entire rest of the South Coast Air Basin combined emits approximately 413,000 tonnes of methane and 23,000 tonnes of ethane annually.\n\nAfter oil was discovered at Aliso Canyon in 1938, J. Paul Getty's Tidewater Associated Oil Company produced oil and gas until the Sesnon-Frew reservoir, the largest reservoir within the oil field, was depleted in the early 1970s. On December 18, 1968, a blowout and fire, destroying equipment but causing no injuries, occurred at one of the wells when an operator attempted to remove two gas-lift valves. Getty sold his portion of the field to Pacific Lighting Company, a gas company, which repurposed it to gas storage in 1972. As opposed to current practice, older wells were not sealed to the surrounding rock formation, including their often more than one mile of steel casing. Today, \"cement from the surface of the ground to the bottom of the well [...] makes the casings stronger and protects them from water.\" The Aliso Canyon natural gas storage facility contains 115 wells tapping a reservoir that \"hold[s] up to 86 billion cubic feet of natural gas for distribution to residences, businesses, and electric utilities in the L.A. basin\". The field is the second largest storage facility of its kind in the United States.\n\nSouthern California Gas Company said the leak in well SS-25 was discovered \"on October 23 during one of its twice-daily well observations.\" Residents of the nearby Porter Ranch reported what they thought was a home with a major leak on October 23, 2015. SoCal Gas \"went from home to home to home, giving everybody the A-OK and [...] didn't admit to having a gas leak until [...] probably around the 28th of October.\"\n\nLocal residents believed the gas blowout started before SoCalGas admitted to discovering it, and many residents reported having unexplained illnesses a week or so earlier. According to KPCC's website, some residents posted complaints of earlier symptoms on a Porter Ranch gas leak Facebook page.\n\nThis event has been called a blowout in several news reports. For instance, an February 2016 article in The Orange County Register called \"Gas facility that had blowout over Porter Ranch will have to play by new rules.\" stated \"in the case of the blown-out well, the casing is believed to have failed under high pressure, allowing the gas to escape.\" According to OSHA's website a blowout is \"an uncontrolled flow of gas, oil, or other well fluids\" from a well.\"\nAn article in The Huffington Post entitled \"Here’s How Enormous The Methane Blowout Is In California\" stated that as of February 1, 2016 the SS-25 well released an estimated 91,000 metric tons of methane gas.\"The article said that \"while this gas blowout has prompted comparisons to BP’s oil well failure in the Gulf of Mexico in 2010 and the Exxon Valdez oil spill in Alaska in 1989, experts say this leak will have further-reaching environmental consequences.\"\n\nThe source of the leak was a metal pipe in a breached casing of injection well \"Standard Sesnon 25\" (SS 25) that lies deep. SoCal Gas had hypothesized that the leak was no more than down in the column used to move gas in and out of the well. Well SS 25 was drilled in 1953 and initially had safety valves, which were removed in 1979 because the valves were old and leaking. Because the well was not considered \"critical, that is, one within 100 feet of a road or a park, or within 300 feet of a home\", the valves were merely removed and not replaced. Multiple safety valves had been removed from the Aliso Canyon/Porter Ranch 1950s-era pipes in 1979, and were never replaced, a fact that was also confirmed by a special investigation into the leak by Congressman Brad Sherman's office. The atmospheric scientist Steve Conley said the wellhead in Aliso Canyon was 61 years old and he was not \"shock[ed] that it failed\". One reason for the casing failure may have been gas flow not just through the tubing, but also through the casing \"in order to meet the demand of a customer\", as told by an injection well expert of Texas A&M University interviewed by NPR.\n\nOn December 7, 2015, an anonymous video was published that showed a cloud of methane gas hovering over the community of Porter Ranch. The image was captured with an infrared camera, showing the extent of the plume.\n\nNatural gas consists largely of methane, an invisible and odorless greenhouse gas with a global warming potential 86 times greater than carbon dioxide in a 20-year time frame, tailing off to about 29 times the effect of carbon dioxide in a 100-year time frame on a mass per mass basis. The leak initially released about 44,000 kilograms(kg) of methane per hour or 1,200 tons of methane every day, which in terms of greenhouse gas output per month compares with the equivalent effluvia from 200,000 cars in a year. On January 14, 2016, \"Time m\"agazine compared the 1.6 million lbs of methane released each day to the emissions of 6 coal fired powerplants, 2.2 million cows per day, or 4.5 million cars. As of January 2016, the latest methane measurement per the California Air Resources Board (Carb) was from December 22, 2015 and had decreased from a peak of 58,000±12,000  kg/h to 30,300 kg/h, the equivalent of \"more than 1,411,851 cars\" by a different calculation using EPA estimates. The Aliso gas leak carbon footprint is referred to as \"larger than the Deepwater Horizon leak in the Gulf of Mexico\". This single event had a 100-year global warming potential of about 1.5% of the entire annual SoCAB methane and carbon dioxide emissions.\n\nSteve Conley, an atmospheric scientist at UC Davis and owner of Scientific Aviation, independently measured the gas emissions. He sampled the air by flying over the site in his single-engine airplane equipped with an analyzer, which is one of only 3 or 4 of such planes in the country. He said the Aliso Canyon flights were \"the hardest [...] I've ever done\" because of the headache-inducing smell and sickening turbulence. He doubted the readings initially, and \"thought [the instruments] had stopped working because I'd never seen measurement that large before.\" He had coincidentally been contracted by the California Energy Commission prior to the gas leak, and said that if he hadn’t been contracted and ready to go, \"no one would have known how big [the leak] was.\"\n\nMethane emissions from the Aliso Canyon gas leak were detected by the TCCON site at Caltech within a day of the start of the leak, but researchers at Caltech did not have an explanation for what was causing something \"really weird\" over Pasadena until later. TCCON's monitoring data were used in analyses of the leak to estimate ethane emissions of 7,700 ± 1,700 tonnes using Steve Conley's methane estimate of 97,100 tonnes.\n\nBesides methane, the gas leak contained tert-butyl mercaptan, tetrahydrothiophene, and methyl mercaptan, which gives the gas a rotten-egg smell. These odorants have been used for decades and were considered harmless although they can cause nausea and vomiting. In addition, the gas contains some volatile organic compounds such as the carcinogen benzene. The pollutants may have long-term consequences far beyond the region.\n\nLocal residents have reported headaches, nausea, and severe nosebleeds. About 50 children per day saw school nurses for severe nosebleeds. There have been more than usual eye, ear and throat infections. By December 25, 2015, more than 2,200 families from the Porter Ranch neighborhood had been temporarily relocated, and more than 2,500 households were still being processed. As of January 7, 2016, 2,824 households or about 11,296 people had been temporarily relocated by SoCal Gas, while more than 6,500 families have filed for help. Two schools were relocated in January.\n\nThe community of Eight Mile, Alabama had a spill of the natural gas odorant within their community in 2008. Residents continue to experience these symptoms under their long term exposure to mercaptan. The residents say they got very little of the assistance provided to Porter Ranch residents. Methyl mercaptan is toxic if inhaled according to its MSDS sheet. However, it is considered fairly harmless by government and industry officials. Whether it is the source of the illnesses in Porter Ranch and Eight Mile has been a subject of debate.\n\nBy the end of November 2015 SoCal Gas had attempted six well kill procedures to stop the gas flow by pumping a mixture of mud and brine down the well, the last being on November 25. The attempts failed because of ice formation and a high upward pressure averaging 2,700 pounds per square inch.\n\nOn December 4, 2015 SoCal Gas started drilling a relief well to the caprock, 8,000 feet down, with the help of Boots & Coots, a subsidiary of Halliburton. The relief well is \"similar to the relief well BP’s engineers drilled to stop oil flowing into the Gulf of Mexico in 2010 after the Deepwater Horizon disaster\". The plan was to pump liquid and cement into the main well once the relief well could vent the gas safely. SoCal Gas estimated the first relief well would be completed by February 24, 2016. SoCal Gas planned to drill a secondary relief well, estimating the leak repair to take up until the end of March 2016.\n\nAfter the seventh effort to plug the leak with slurry starting December 22 had created a 25-feet-deep crater around the wellhead, the danger of a blow out increased. The well needed to be stabilized with tension cables and further attempts to plug the well from above were halted. State regulators became more concerned about the wellhead's stability.\n\nIn January SoCal Gas sought permission to capture the natural gas and possibly incinerate it, but state regulators voted against it, worried about safety.\n\nOn February 11, 2016, the relief well intercepted the base of the leaking well and the company began pumping heavy fluids to control the flow of gas out of the leaking well. On February 18, 2016, state officials announced that the leak was permanently plugged.\n\nSteve Conley, the atmospheric scientist that has been measuring the leak's gas emissions, has stated that there was no rapid response plan for this kind of event: \"We do not have anything in place to measure giant leaks like this, or to watch them to solve issues.\" He suggested that contracts needed to be ready so that, \"as soon as a leak is spotted you are given a go order and two hours later you're measuring a leak.\"\n\nOn January 11, 2016 Mitchell Englander, the Los Angeles City Councilman representing Porter Ranch, criticized SoCal Gas \"operating a facility of this magnitude, [...] feeding 20 million addresses\" for not having a backup plan, the delay in bringing necessary equipment on site \"from the Gulf states like they did in this particular situation\" and the delay in catching the brine, oil and chemical mist \"landing on people's homes and turning their cars black\".\n\nThe Center for Biological Diversity has criticized Governor Brown's slow response \"because state regulators' hands-off approach to underground injection helped set the stage for this catastrophe.\" The Center also said, \"The state has known for years that aging natural gas infrastructure was a disaster waiting to happen, but officials mostly ignored those risks.\"\n\nExperts in petroleum engineering criticized the delay for not drilling a relief well until six weeks after the leak was known, and for drilling only one relief well instead of two.\n\nIn January 2016 the Associated Press reported that SoCal Gas had been under-reporting the levels of benzene in the air surrounding the well.\n\nWithin two days of discovery, \"a dozen or more local and state agencies were involved\". The California Air Resources Board measured methane on the ground near the well, from towers, satellites and airplanes, communicating results to the California Energy Commission and SoCal Gas. The Board asked SoCal Gas 18 very detailed questions about the natural gas, basic leakage rate and leak dynamics. The Federal Aviation Administration established a temporary flight restriction over the leak site until March 2016. On December 7, 2016 Los Angeles City Attorney Mike Feuer sued SoCal Gas over its handling of the massive well failure. On December 15, 2015, the County of Los Angeles declared a state of emergency.\n\nFeuer's Office went to Court again on December 23, 2015 and secured a judicially enforceable agreement with SoCal Gas for the company to speed up relocation efforts of residents affected by the massive leak. \n\nLocal residents called upon Governor Jerry Brown to intercede. Kathleen Brown, his sister, is on the board of Sempra Energy, which owns Southern California Gas. The Governor visited the site and the neighborhood on January 4, 2016, and declared a state of emergency on January 6, 2016. He issued stepped-up inspections and safety measures for all natural gas storage facilities in California; further injections at Aliso Canyon had already been prohibited.\n\nLocal health officials indicated that long-term exposure to certain trace chemicals could lead to health problems.\n\nIn January 2016 two California senators asked the heads of the US Department of Justice and US Department of Transportation and the Environmental Protection Agency for a \"legal analysis of any federal authorities that could apply to this incident and storage fields in general\" and a \"technical analysis of whether Southern California Gas Company could more quickly reduce the gas stored in the facility.\" On January 11, four California State senators introduced bills to enact an immediate moratorium on any new injections of natural gas and the use of vintage wells (SB 875), to ensure that SoCal Gas will pay housing, relocation and emergency response costs and prohibit the California Public Utilities Commission (CPUC) from allocating those costs to ratepayers, and that it will pay the costs to mitigate the greenhouse gas emissions from its utility profits (SB 876). SB 887 would require all 14 underground natural gas storage facilities to be inspected within the next 12 months and at least annually thereafter; it would also require enhanced safety standards such as the installation of subsurface safety valves, using new leak-detection technology and development of rigorous response plans.\n\nThe unavailability of the Aliso Canyon gas storage facility has caused insufficient delivery of gas to power plants, leading to a strained electricity grid. The CPUC has ordered Southern California Edison to install a 20 MW (80 MWh) lithium ion battery storage capacity at the utility's Mira Loma substation near San Bernardino, California to mitigate power failures during winter.\n\nOn March 31, 2016 The New York Times published a magazine feature article about the leak called \"The Invisible Catastrophe,\" noting that in an election year a \"menacing disaster that causes mass vomiting and mass nosebleeds in a wealthy, vote-rich community ...is a candidate’s dream.\" While crediting Governor Jerry Brown as the \"only politician who has failed to use the gas leak for political gain,\" it noted freshman Congressman Steve Knight's acceptance of campaign contributions from SoCalGas' parent company Sempra Energy, and his continued defense of the company into December when he said they were \"working on this as diligently as they can.\" While Knight later called for congressional hearings on the matter, one of his opponents, Democrat Bryan Caforio, quickly seized on the environmental disaster as a central issue of the 25th Congressional race, which according to the Times article \"promises to be one of few closely contested races in the House.\" \n\n\"The Los Angeles Times\" wrote that no politician is \"pursuing the issue as hard as Caforio,\" who on January 6, 2016, spoke to a group of disgruntled Porter Ranch residents who attended a meeting co-hosted by environmental activist Erin Brockovich. Caforio \"aggressively attacked\" his opponent, Steve Knight, for his delayed response to the leak. While Knight states he does not want to politicize the disaster, fellow Republican Michael Antonovich, a Los Angeles County supervisor who has voted consistently against regulation efforts, has been outspoken about his determination to hold SoCalGas responsible.\n\nElected officials, including Congressman Brad Sherman and Senator Henry Stern, took the position that no decision about whether to reactivate the facility until the health study and an ongoing search for the leak’s cause are complete. SoCalGas had been repeatedly asking California regulators for permission to resume pumping pressurized natural gas into the ground in Porter Ranch, northeast of Los Angeles, at the depleted oil field, which has continued to be used for storage. Records show the utility has been negotiating this step with regulators for several weeks. Since the multiple investigations into the cause of the leak have not been completed, Congressman Brad Sherman insisted that the process of reopening the storage facility must proceed with extreme caution. Aliso Canyon is not deemed ready to resume normal operations by regulators or chief politicians that are involved with the oversight of the securing and cleanup of the leak. Congressman Brad Sherman urged that the gas company be restricted to minimal operations required to prevent power interruptions. The storage field is a primary supplier to gas-powered electric plants. The gas leak has been cited by experts and independent researchers as the largest in U.S. History.\n\nOn November 23, 2015, some residents who had been displaced by the leaking gases filed a lawsuit in Los Angeles Superior Court to require that SoCal Gas to disclose information related to the health risks of the gases released. The plaintiffs also had continued to seek damages.\n\nIn January, it was thought that residents might have filed as many as 1,000 lawsuits. \nOn February 2, 2016, Los Angeles County filed criminal charges against SoCalGas for its failure to report the leak immediately. The charges include three counts of failing to immediately report the leak following its detection on October 23, 2015, and one count of \"discharging air contaminants\" beginning the same day.\n\nOn September 13, 2016, \"The Los Angeles Times\" reported that SoCalGas reached a plea agreement with the Los Angeles County prosecutor's office. Under the agreement, SoCalGas plead no contest to one misdemeanor count of failing to immediately report the October 2015 leak and was ordered to pay a $4 million fine.\nThe resulting settlement between the local Los Angeles prosecutor's office and SoCalGas executives had ended in a mere misdemeanor charge, the lowest possible charge, which can be expunged. In addition to the negotiated plea deal, SoCalGas had paid a penalty fine of a relatively small $4 million U.S. dollars. $4 million USD is the equivalent of the amount of revenue that SoCalGas received in profits in less than most average 12 hour time periods during the fiscal year of 2015. The Porter Ranch Gas Leak has been cited often by experts and independent tests as the largest gas leak in U.S. History.\n\nIn 2018 SoCalGas agreed to payments of $119.5 million to several government entities over the incident.\n\n\n"}
{"id": "1405420", "url": "https://en.wikipedia.org/wiki?curid=1405420", "title": "Angra Nuclear Power Plant", "text": "Angra Nuclear Power Plant\n\nAngra Nuclear Power Plant is Brazil's only nuclear power plant. It is located at the Central Nuclear Almirante Álvaro Alberto (CNAAA) on the Itaorna Beach in Angra dos Reis, Rio de Janeiro, Brazil. It consists of two Pressurized water reactors, Angra I, with a net output of 609 MWe, first connected to the power grid in 1985 and Angra II, with a net output of 1,275 MWe, connected in 2000. Work on a third reactor, Angra III, with a projected output of 1,245 MWe, began in 1984 but was halted in 1986. Work started again on 1 June 2010 for entry into service in 2015 and later delayed to 2018.\n\nThe Central Nuclear Almirante Álvaro Alberto complex is administrated by Eletronuclear, a state company with the monopoly in nuclear power generation in Brazil. The complex employs some 3,000 people and generates another 10,000 indirect jobs in Rio de Janeiro state.\n\nAngra I was purchased from Westinghouse of the USA (its sister power plant is Krško Nuclear Power Plant in Slovenia). The balance of plant design was subcontracted to Gibbs and Hill (USA) in association with PROMON Engenharia S.A. and construction to Brasileira de Engenharia S.A. \n\nThe purchase did not include the transfer of sensitive reactor technology. As a result, Angra II was built with German technology, as part of a comprehensive nuclear agreement between Brazil and West Germany signed by President Ernesto Geisel in 1975. The complex was designed to have three PWR units with a total output of around 3,000 MWe and was to be the first of 4 nuclear plants that would be built up to 1990.\n\nThe plant has two pressurized water reactors, with a total net capacity of . Its units are rated as follows:\n\nThe development of Angra III began in 1984 as a Siemens/KWU pressurized water reactor but was halted in 1986. About 70% of the plant's equipment was purchased in 1985 but has been in storage ever since. In June 2007, restarting of work on was approved by the National Council for Energy Policy. President Luiz Inácio Lula da Silva greenlit the project in July 2007. On December 2008, Eletronuclear signed an industrial cooperation agreement with Areva. On 31 May 2010, the National Nuclear Energy Commission granted a licence for construction of the third reactor. Construction of the reactor, which has a capacity of 1,350 MWe, begun on 1 June 2010 and was predicted to be operational by 2018.\n\nAfter stopping construction in 2014, the Brazilian government have decided to auction off the incomplete power station to private investors in 2018. Based on that timetable and the volume of construction works to complete, the Energy deputy minister expects completion to be achieved by 2023.\n\n\n"}
{"id": "47259175", "url": "https://en.wikipedia.org/wiki?curid=47259175", "title": "Azura (wave power device)", "text": "Azura (wave power device)\n\nAzura is a wave power device currently being tested in Hawaii. It is connected to the municipal grid providing electricity to Hawaii. According to the United States Department of Energy, this is the first time that a wave power generator has been officially verified to be supplying energy to a power grid in North America. This has been verified by the University of Hawaii. The device can generate 20 kilowatts of power.\n\nThe device is at the Marine Corps Base Hawaii's Wave Energy Test Site (WETS) on the north shore of Kaneohe Bay, Oahu. It is situated on the surface of a 30-meter-deep berth where it is being monitored.\n\nThis prototype (TRL 5/6) was developed by Northwest Energy Innovations (NWEI) with the support of the U.S. Navy, the United States Department of Energy, and the University of Hawaii. It will be in operation for a 1-year period of assessment. During that time, the University of Hawaii will be responsible for the collection and analysis of data.\n\nAzura was originally named \"WET-NZ\" from \"Wave Energy Technology-New Zealand\".\n\nAzura floats on the surface of the sea and weighs 45 tons (41 tonnes). It has a unique floating mechanism that can rotate 360 degrees. This enables it to extract power from horizontal (surge) as well as vertical (heave) wave motion. It has reserve buoyancy that is very low, allowing it to partially submerge beneath large waves.\n\nAzura is a point absorber. This means that it uses a floating surface mechanism to absorb the energy of waves from different directions. This is the most common type of deepwater wave energy generator. The generator is driven with a high-pressure hydraulics system.\n\nThe initial phase of development used a smaller prototype that was tested in a wave tank.\n\nA second prototype was then installed in 2012 for a 6-week period at the Northwest National Marine Renewable Energy Center’s test site off the coast of Oregon in an open-sea area. During that test, the device was exposed to wave heights of up to 3.75-meters in a 12 to 14-second sea state.\n\nBoth tests were successful.\n\nNWEI will use information gathered during the current test to further develop the project. With the Department of Energy providing an additional 5 million dollars, NWEI plans to modify Azura to increase its efficiency and improve reliability. A new design will then be tested which will be full-scale and made to generate between 500 kilowatts and one megawatt of power.\n\nAt the end of 2017, Northwest Energy Innovations (NWEI) intends to install a full-scale model. The 500-kilowatt to 1-megawatt generator will be situated in a 60 to 80-meter-deep (100–150 feet) berth. One megawatt is sufficient to provide electricity to several hundred homes.\n\n\n"}
{"id": "57973487", "url": "https://en.wikipedia.org/wiki?curid=57973487", "title": "Bass Point Reserve", "text": "Bass Point Reserve\n\nBass Point Reserve is a heritage-listed former cedar timber industry, Aboriginal camping, meeting place, pastoral property and basalt mine and now nature conservation and passive recreation area located at Boollwarroo Parade, Shell Cove in the City of Shellharbour local government area of New South Wales, Australia. It is also known as Long Point. The property is owned by Department of the Environment and Shellharbour City Council. It was added to the New South Wales State Heritage Register on 18 January 2013.\n\nArchaeological evidence indicates that Aboriginal people occupied the Illawarra region for some 20,000 years prior to the arrival of European settlers. The Elouera people are traditional custodians of the land extending from Stanwell Park in the north, Shoalhaven River in the south, west to Picton and Moss Vale, and east to the ocean. The Elouera people are a group, subdivided from the larger Dharawal group, that occupies the land from Botany Bay to Jervis Bay.\n\nIn the Illawarra region, the Elouera were identified as fresh or salt water people due to their occupation of particular marine or estuarine landscapes and their use of the natural resources found in these environments. Essentially, the Elouera people lived a hunter/gatherer lifestyle, governed by the sustainable use of their surrounding environment and the available resources. Traditionally, the division of labour in such a society was determined by gender and age - men hunted by land and sea while women gathered food and resources. The Aboriginal people had developed their understanding of the region and environmental sustainability over thousands of years and there is archaeological evidence to suggest that the people travelled the land making use of seasonal resources when they were abundantly available and allowing those depleted to regenerate with time.\n\nThe diet of the Elouera people was varied and flexible - consisting of (among other things) fish, shellfish and seals from the ocean; and wallabies, possums, birds and plants from the land. Evidence of this changing diet has been found through archaeological investigation of the shell middens at both Bass Point and along the NSW coastline.\n\nThese shell middens are found in coastal environments throughout Australia - but particularly on the east coast. Those identified at Bass Point have been dated at 6000 years old, from the period when the sea levels stabilised and the coastal environment developed into what it is today. Analysis of the content of these middens has revealed shell and food remains that indicate the hunter/gatherer lifestyle of the traditional Aboriginal people in the Illawarra region. Excavation of shell middens at Bass Point have also revealed the changing tools and technology used by the Elouera people to exploit the available resources around them - in particular, the development and evolution of hunting practices as species of fish and animals changed with the seasons and over the years.\n\nMiddens are usually found in close proximity to both fresh water supplies and have often resulted from an established occupation of a place. Evidence at Bass Point indicates the longevity of its use by the Aboriginal people as an important camping and meeting place - a value supported by the oral tradition of the local people. The coastal plain is known to have been an abundant natural environment of food and fresh water resources and, with their in-depth understanding of the marine environment, Bass Point must have been regarded by the Aboriginal people as a resourceful place that could sustain long-term occupation.\n\nA harmonious and balanced relationship between the Aboriginal people and the environment existed for thousands of years and it was in this form when the Aboriginal people first had contact with European explorers. Although there may have been earlier contact with Portuguese, Spanish, Polynesian or Asian explorers, the first report of Bass Point and the local Aboriginal people comes from Captain James Cook who sailed by the region on the \"Endeavour\" in April 1770. Those on board noted in their journals of their observations - \"Sunday, 22nd April:...and were so near the shore as to distinguish several people upon the Sea Beach. They appeared to be of a very dark Colour...Thursday, 26th April: Saw several smokes along shore after dark, and 2 or 3 times a fire\". The journals from the Endeavour also make note of the \"numerous campfires, on the blackness of the natives, and of a luxuriant vegetation and varied landscape\". There are historical reports that Cook attempted to make a landing along the Illawarra coastline but abandoned these efforts due to uncertain and dangerous conditions. Had this landing been successful, it would have been the first on Australian soil - predating that at Botany Bay. Although the lives of the Aboriginal people continued relatively undisturbed following this visit, stories soon circulated between the groups about sightings of the \"White Swan\" (believed to be a reference to the sails of the Endeavour).\n\nThe geographic nature of the Illawarra ensured that the region remained isolated from the early colonial settlement following the First Fleet landing in 1788. The surrounding mountains provided a barrier to the north and west and, with the absence of a natural and accessible harbour, official settlement of the region did not occur until early in the nineteenth century. Stories of the arrival of Europeans surely spread to the region through communication between Aboriginal groups quickly and, as in other parts of Australia, it was first thought by the Aboriginal people that the settlers were their reborn ancestors. Their pale skin but similar features ensured inquisitive but mostly civil early interactions between the settlers and the Elouera people.\n\nThe first official exploration of the Illawarra region (then known as the 'Five Islands District') was recorded by George Bass and Matthew Flinders in 1796. Following a similar exploratory expedition up the Georges River, Bass and Flinders ventured to the Illawarra region to explore and examine the country, take specimens from the environment and to report their findings back to the colony with recommendations for possible future settlements. The local Aboriginal people may have had sporadic interactions with other Europeans after the Bass/Flinders visit (including the shipwreck survivors of the Sydney Cove in 1797 who were making the arduous and largely fatal trek from Cape Howe to Sydney) but little changed as a result of the First Fleet landing until the region was officially settled in 1803 - the effect of this settlement on the local Aboriginal people being felt almost immediately.\n\nThe colonial settlement in Sydney experienced a severe drought in 1803 that threatened the agricultural industries (particularly cattle grazing) that supported life in the colony. Finding fresh new pastures was essential to the survival of the industry and reports of the rich untouched land further south at Lake Illawarra soon circulated amongst the colonists. Free grazing rights were issued by the government - and a significant land parcel stretching from Lake Illawarra to the Minnamurra River (and incorporating Bass Point) was granted to free settler, James Badgery, for cattle running.\n\nAt this time, Red Cedar-cutting was also an important industry in the colony and there were lucrative supplies discovered in the virgin bush of the Illawarra region.\n\nBoth the cattlemen and the cedar-cutters were adventurous and were the first Europeans to traverse the unknown country - successfully doing so by following the traditional trails established by the Aboriginal people. From even this early period of settlement, European exploitation of the land and resources had begun. The easy abuse of land through clearing of vegetation and wildlife would have been an abhorrent disrespect to the Aboriginal people. The traditional lifestyle of the people had been sustained by this land and its immediate exploitation by the European settlers would no doubt have led to future conflicts.\n\nIn 1816, in an effort to regulate land ownership in an official system, Governor Lachlan Macquarie called for the division of the region (what is now Shellharbour Municipality) into land grants - 22 in total. The free grants were given to prominent colonial citizens and cattlemen - one such grant of 1650 acres, and later an additional 2000 acres (including Bass Point), was granted to D'Arcy Wentworth, a wealthy colonial official and medical practitioner.\n\nAlthough Wentworth was promised the land grants in 1817, he was not issued with the land until 1821 when he established \"Peterborough Estate\". Following the exit of Badgery, Wentworth was able to run his own cattle while, at the same time, acquiring surrounding grants to expand his land holding. There is a strong presumption that the expansion of Peterborough Estate was organised with the other land-holders from very early on. The land was rich in pasture and possibility but remained largely unsettled and unused by its early owners. By 1827, Wentworth had acquired the largest land holding in the region totalling 13,050 acres - including Bass Point and the private Peterborough township (that is now Shellharbour village).\n\nThe repossession and division of the richest lands in the region had a devastating effect on the Elouera people. The prime locations selected for land grants would have been those with easy accessibility to fresh water supplies - the areas most valued by the Aboriginal people for resources and as traditional camping places. Larger numbers of people occupying the land also reduced the capacity for the Aboriginal people to traverse the region as they had traditionally done.\n\nColonial grazing and farming of the land also had a detrimental and lasting effect on the native food supplies used by the Aboriginal people. Traditional plants had been destroyed by grazing cattle and replaced by introduced plant species.\n\nAdditionally, native animal species had been frightened away from their habitats. In no time, the scarcity of traditional resources saw a crisis arise in the lifestyle of the local Aboriginal people - their homeland was being ravaged and their very survival was at risk.\n\nThe rapid degradation of the sustainable environment left the traditional Aboriginal people with one option: appropriate the introduced crops and stock to survive. While traditional Aboriginal life had no concept of ownership (but rather a shared use of land and resources), the settlers viewed their actions as theft and crimes punishable by violent retaliation. At this point in relations, the Aboriginal people were struggling to maintain their traditional ways of life while contending with the force of the colonial settlers and it was soon realised that the Elouera people had little to match the sheer number of settlers with their introduced firearms and diseases (smallpox, influenza and tuberculosis).\n\nThe colonial government, under Governor Macquarie's direction, had declared unofficial war against the Aboriginal people. A detachment of the 46th Regiment was sent to Red Point (now Hill 60 at Port Kembla) to bring a show of force to the Aboriginal people. \"During April of 1816 Macquarie instructed his soldiers to seek out the Aborigines and \"strike them with terror ... drive them to a distance from the settlements of the White Men ... inflict terrible and exemplary punishments\" so as the NSW Government would not be seen to show cowardice in the face of perceived Aboriginal aggression\". It was the colonial intention that fear be instilled in the local Aboriginal people to reduce retaliation attempts on the settlement.\n\nAs a result of this action, the traditional Aboriginal population was quickly decimated. With forced removal to fringe camps, assimilation to European culture and the imposition of strict control measures - coupled with their delegation to the bottom of the social hierarchy - the Aboriginal people had no recourse against the invasion of the settlers throughout the nineteenth century. Governor Macquarie's plans to expand the colonial frontier had been a success and the rich land of the Illawarra region had been cleared of its traditional inhabitants and was free to be exploited by the settlers.\n\nBy the 1840s, the colony was experiencing an economic depression and the large landholdings in the region were soon subdivided into smaller tenant farms. Provided rent-free for periods of up to six years, the land was leased to families for the purpose of clearing native vegetation and cultivating crops. Wheat and maize were popular early crops but soon proved to be susceptible to rust and ultimately financially unprofitable for the farmers. By the second half of the nineteenth century however, the dairy industry had been established and was proving to be a successful business for the small landholders in the region.\n\nDuring this period, of Peterborough Estate (including Bass Point) had been sold by the Wentworth family to George Laurence Fuller who named the property \"Dunmore Estate\". By 1880, Fuller had negotiated a mining venture and established a basalt \"blue gold\" quarry to the south of Bass Point including the construction of a new jetty to ship the quarried metal. Although the enterprise collapsed within two years, Fuller resumed operations as the proprietor and manager and, by 1890, business was booming. To support the industry, Fuller soon improved and extended the jetty to and commissioned the construction of the SS \"Dunmore\" to transport the crushed basalt from Bass Point to the markets of Sydney.\n\nShipping was the favoured mode of transport of the product but the journey was often a hazardous one with a number of ships lost along the coast and in the waters surrounding Bass Point. The \"Bertha\", an wooden schooner, is thought to be one of the earliest wrecks from the basalt trade. It was reported that she was transporting bluestone from Kiama to Sydney and, on 9 September 1879, ran ashore on the north side of Bass Point and broke apart. It was reported in the media at the time that the local Aboriginal people camped at Bass Point had assisted with the rescue of the three crewmen and two passengers on board by conveying a line from the stricken vessel to the shore.\n\nOver the years, other ships experienced a similar fate to that of the \"Bertha\": in 1880, the wooden paddle steamer \"Our Own\" was wrecked off the beach at Bass Point with the loss of two lives; the \"Alexander Berry\", a wooden steamer, went down in 1901 with four of the five crewmen perishing in the accident; the \"Comboyne\" wrecked in 1920; and the \"Kiltobranks\", another blue metal carrier, in 1924.\n\nOne of the more well-known shipwrecks of Bass Point was the loss of the \"Cities Service Boston\" on 16 May 1943. The US oil tanker was transporting fuel supplies from the Middle East before running aground during a storm at what is now known as Boston Point. Australian soldiers from the 6th Australian Machine Gun Battalion were camped nearby and assisted in the rescue of the 62 crew on board. All lives from the Boston were saved but four Australian soldiers perished in the rescue. To commemorate the loss, plaques were unveiled at Bass Point in 1968 and a remembrance service is held at the site each year.\n\nSince Dunmore Estate was sold in the 1920s, the land in and around Bass Point was used by the Australian Military Forces for defensive, training and surveillance functions. By , Bass Point was purchased by Imperial Chemical Industries (ICI) and mining of its extensive basalt deposits was renewed. The original jetty had deteriorated beyond the needs of the mining leases and was ultimately demolished in 1958 before being replaced by a new jetty directly east of the original.\n\nIt was as a result of the increased mining of Bass Point in the 1960s, that its future as a natural reserve was first considered. Local conservationists had formed a society to promote the natural value of Bass Point and to balance the interests of both conservation and development. A reserve was officially declared in 1968 and, furthermore, Bushrangers Bay Marine Reserve was declared in 1982.\n\nSince the historic value of Bass Point has been recognised, there have been archaeological investigations of the area that have revealed significant information about its pre-contact history. Of the 12 shell midden sites identified on Bass Point, Dr Sandra Bowdler investigated six sites in 1970 as the basis for her thesis. Further analysis of the remaining six sites was undertaken by Dr PJ Hughes in 1974.\n\nMiddens of the NSW South Coast, including those at Bass Point, contain indicative remains of the food sources of the Aboriginal people. Upon analysis of the middens at Bass Point, Dr Bowdler and Dr Hughes discovered shells and bones of shellfish, fish, wallabies, bandicoots, possums, birds and seals.\n\nIt was also considered that the many middens along the northern shoreline may, in fact, represent a single continuous midden site.\n\nThese archaeological excavations revealed the environmental change and evolution of Bass Point over time and, further analysis of the midden sites has shown the development of techniques used by the Aboriginal people to hunt and gather available resources. As a result of these archaeological assessments, Bass Point is now considered to be one of the most significant Aboriginal archaeological sites to be excavated in NSW.\n\nLocated south of Wollongong, Bass Point Reserve is a 4 km-long natural promontory of rocky shorelines and sandy embayments that supports a diverse collection of natural and cultural elements.\n\nAs a cultural landscape, Bass Point Reserve includes sites of Aboriginal archaeological significance and others of European historical significance. Fourteen sites associated with the sustained Aboriginal occupation of the land have been identified - 13 of which are coastal shell middens and one meeting and camping place. Sites of European occupation include potential remnants of the original jetty (to the west of the existing jetty) and, on Boston Point, a memorial to the 1943 shipwreck of the Cities Service Boston and the four lives lost during the rescue operation.\n\nIn the waters surrounding Bass Point Reserve, six shipwrecks have been identified and the associated artefact scatter recorded. These wrecks include \"Bertha\" (1879), \"Our Own\" (1880), \"Alexander Berry\" (1901), \"Comboyne\" (1920), \"Kiltobranks\" (1924) and the \"Cities Service Boston\" (1943).\n\nThe Bass Point Marine Area surrounding the reserve is regarded as highly significant for its biodiversity and pristine condition. This relatively undisturbed environment supports a variety of common, rare and endangered fauna and flora species. The shallow and sheltered waters have seen the development of a significant but fragile Sea Grass Habitat that provides a suitable environment for many aquatic animals to shelter, feed and breed.\n\nBass Point Marine Area is also classified as critical habitat for the Grey Nurse Shark - an endangered species under NSW law.\n\nTo the eastern point of Bass Point Reserve is Bushrangers Bay Aquatic Reserve. Made up of beaches, intertidal rock pools, seagrasses and submarine cliffs the reserve provides important habitats for a variety of animals including fish, anemones, sponges, crabs, molluscs and urchins.\n\nBass Point Reserve also supports diverse headland vegetation and significant littoral rainforest - making it one of the most important and unique natural landscapes in southern NSW. Littoral rainforest is generally a closed forest with its structure and composition strongly influenced by its close proximity to the marine environment. Positioned on coastal headlands or beach sand dunes, littoral rainforest is considered an Endangered Ecological Community in NSW. The natural vegetation of Bass Point Reserve supports a variety of flora and fauna, both common and rare to the region.\n\nToday, the spectacular visual interplay of the bush vegetation, beaches, headlands, cliffs, ocean and sky makes Bass Point Reserve a scenic location for recreation visitors.\n\nThe natural environment of Bass Point Reserve has undergone periods of regeneration by Shellharbour Council's rangers and is in good condition. The ongoing maintenance of visitor areas has also ensured the good condition of facilities.\n\nSince the Bass Point Marine Area was identified, and specific areas classified as critical habitats and aquatic reserves, the marine environment surrounding Bass Point Reserve has remained in pristine condition.\n\nAlthough Bass Point Reserve has been subject to earlier archaeological investigations (in 1969/70 by Dr Sandra Bowdler and again in 1974 by Hughes & Sullivan), there is great potential for further archaeological discoveries - both terrestrial and maritime.\n\nThrough Dr Bowdler's investigation, she concluded that there is potential that the individual shell midden sites on the northern shoreline could represent one single continuous midden site. It is also highly probable that there may be unrecorded Aboriginal artefact scatters and burial sites on the reserve (either individually or in association with midden sites).\n\nIn regard to shipwrecks, the location of the Bertha remains undiscovered and it is also possible that there are further shipwrecks that have gone unrecorded in present documentation.\n\nBass Point Reserve is an evolving and naturally changing landscape but its designation as a nature reserve has ensured that the site will remain a natural environment into the future while maintaining its use for recreational purposes.\n\nSince pre-European contact, with archaeological evidence indicating sustained occupation of Bass Point by Aboriginal people (some 20,000 years), the following modifications have been made:\n\nAs at 28 March 2012, Bass Point Reserve is of state heritage significance for both its Aboriginal and European values; its pre- and post-contact history; and its natural and maritime heritage.\n\nArchaeological evidence indicates that Aboriginal people have occupied the Illawarra region and Bass Point Reserve for some 20,000 years prior to the arrival of European settlers. The traditional custodians of the land, the Elouera people lived in a hunter/gatherer lifestyle, governed by the sustainable use of the environment and the resources available. Bass Point was a place of established occupation for the Aboriginal people and is regarded as a traditionally important camping and meeting place.\n\nBass Point has been the focus of attention from archaeologists since the late 1960s as an area that has potential to reveal significant information about pre-contact history in NSW. Twelve midden sites and one camping/meeting place have been identified and archaeological excavations have revealed the environmental change and evolution of the area over time and the development of techniques used by the Aboriginal people to hunt and gather available resources.\n\nAlongside Burrill Lake rock shelter (which is of similar antiquity), Bass Point is considered to be one of the most significant Aboriginal archaeological sites to be excavated in NSW. It is considered to be a rare example of established occupation and continues to be of exceptionally high significance to the Aboriginal people of NSW.\n\nUpon the arrival of European settlers to the Illawarra region in 1803, the land of Bass Point was granted to D'Arcy Wentworth, a wealthy colonial official and the Principal Surgeon and Principal Superintendent of Police. A significant colonial figure, Wentworth developed a substantial estate (of some - including Bass Point) from 1821-1865 and was influential in the development of the Shellharbour area.\n\nBass Point also had a significant but brief association with Captain James Cook who first made note of the region and its Aboriginal occupants as the Endeavour sailed by the coastline in April 1770.\n\nBass Point has another brief association with the colonial explorers George Bass and Matthew Flinders who made the first recorded European visit to the region in 1796. The contemporary naming of Bass Point commemorates these early explorations.\n\nBass Point Reserve has significant natural features and habitats that contribute to its aesthetic value. A prominent headland in the region, Bass Point contains elements of bush, beach and ocean that create a visually spectacular environment of both land and sea.\n\nThis key coastal landscape is also regarded as highly significant for its biodiversity and pristine condition. The relatively undisturbed environment supports a variety of common, rare and endangered fauna and flora species - including littoral rainforest and habitats for the endangered grey nurse shark and sea grasses.\n\nThe maritime landscape around Bass Point Reserve also contains a number of shipwrecks and archaeological evidence, dating from 1879. The most significant and well known, the Cities Service Boston, was wrecked in May 1943 and a memorial was erected at Boston Point to commemorate the Australian lives lost in the rescue.\n\nBass Point Reserve was listed on the New South Wales State Heritage Register on 18 January 2013 having satisfied the following criteria.\n\nThe place is important in demonstrating the course, or pattern, of cultural or natural history in New South Wales.\n\nBass Point Reserve is of state heritage significance for both its Aboriginal and European values and its pre- and post-contact history.\n\nArchaeological evidence indicates that Aboriginal people occupied Bass Point Reserve for some 20,000 years prior to the arrival of European settlers. The traditional custodians of the land, these people lived in a hunter/gatherer lifestyle, governed by the sustainable use of the environment and the resources available.\n\nBass Point has been the focus of attention from archaeologists since the late 1960s as an area that has potential to reveal significant information about pre-contact history in NSW. Twelve midden sites and one camping/meeting place have been identified and archaeological excavations have revealed the environmental change and evolution of the area over time and the development of techniques used by the Aboriginal people to hunt and gather available resources.\n\nAlongside Burrill Lake rock shelter (which is of similar antiquity), Bass Point is considered to be one of the most significant Aboriginal archaeological sites to be excavated in NSW.\n\nOfficial European settlement in the Illawarra region and on Bass Point Reserve, started from 1817 with the division of land and the establishment of agriculture and industry. The development of basalt mining on the point saw the growth of shipping in the region but, due to the hazardous conditions of the new transport route, a number of ships were wrecked off the Bass Point coastline - the \"Bertha\" (1879); \"Our Own\" (1880); \"Alexander Berry\" (1901); \"Comboyne\" (1920); \"Kiltobranks\" (1924); and the \"Cities Service Boston\" (1943).\n\nThe place has a strong or special association with a person, or group of persons, of importance of cultural or natural history of New South Wales's history.\n\nBass Point Reserve is of state heritage significance for its association with a number of significant people and groups.\n\nTraditionally, and for some 20,000 years prior to European settlement, the land has been occupied by the Elouera people of the Dharawal group. The longevity of the use of Bass Point as a camping and meeting place indicates its significance to the Aboriginal people of this region. Documentary, archaeological and oral evidence indicate that Bass Point was, and is, considered to be an extremely important place by local Aboriginal people and that \"the general \"feeling\" about the place was that it was a good and happy place\". Although the significance of this site to the Elouera people stretches for thousands of years into the past, its importance to the contemporary Aboriginal community continues today.\n\nBass Point also had a significant, but brief, association with Captain James Cook and the Endeavour. As the Endeavour sailed by the Illawarra coastline in April 1770, journal notes were made about their observations of the landscape and the traditional Aboriginal occupants. There is evidence to suggest that Cook attempted a landing in the region but abandoned the effort due to dangerous conditions. Had this landing attempt succeeded, it would have been the first on Australian soil - predating that at Botany Bay.\n\nThe first recorded European visit to the region was by colonial explorers and naval men, George Bass and Matthew Flinders, in 1796. Exploring the unknown country to observe and report back to the colony, Bass and Flinders were most-likely the first contact the Elouera people had had with European settlers. The contemporary naming of Bass Point commemorates the initial explorations of these significant explorers.\n\nAfter the Illawarra region had been officially settled, the land was divided into free grants, and Bass Point was granted to D'Arcy Wentworth, a wealthy colonial official and medical practitioner. Wentworth had arrived in Australia as a free settler and Assistant Surgeon on a convict fleet in 1790. By 1811, Governor Lachlan Macquarie had appointed him Principal Surgeon and Principal Superintendent of Police. Also a founding member of the Bank of NSW, Wentworth is a significant colonial figure who developed a substantial estate (of some 13,050 acres, including Bass Point) from 1821-1865.\n\nThe place is important in demonstrating aesthetic characteristics and/or a high degree of creative or technical achievement in New South Wales.\n\nBass Point Reserve has significant natural features and habitats that contribute to its aesthetic value. A prominent headland in the region, Bass Point contains important elements of bush, beach and ocean that create a visually spectacular environment of both land and sea. This key coastal landscape has significant aesthetic value and its general visitation numbers reflects the public's ongoing admiration for the natural beauty of this site.\n\nThe place has a strong or special association with a particular community or cultural group in New South Wales for social, cultural or spiritual reasons.\n\nThe social significance of Bass Point Reserve remains strong in contemporary Aboriginal culture. There is a widespread understanding that the Elouera people (a subdivision of the larger Dharawal group) are the traditional occupants of the land of the Illawarra region. The coastal landscape at Bass Point supported the established camp sites of the Aboriginal people and provided a diverse and sustainable natural source of food and fresh water for some 20,000 years prior to European arrival to Australia. Following white settlement, the Aboriginal people continued to use the Bass Point area as a camp site and meeting place.\n\nThe designation of Bass Point Reserve has ensured that the natural value of the site is conserved and is still available for use by the Aboriginal people of the region today. The site is also widely used by visitors as a tourist and recreational resource.\n\nThe place has potential to yield information that will contribute to an understanding of the cultural or natural history of New South Wales.\n\nBass Point Reserve has state heritage significance for its potential to reveal further information through archaeological research. Although it has been subject to earlier archaeological investigations (in 1969/70 by Dr Sandra Bowdler and again in 1974 by Hughes & Sullivan), there is great potential for further archaeological discoveries, both terrestrial and maritime.\n\nThrough Dr Bowdler's investigation, it was concluded that there is potential that the individual shell midden sites on the northern shoreline could represent one single and continuous midden. It is also highly probable that there may be unrecorded Aboriginal artefact scatters and burial sites on the reserve (either individually or in association with midden sites).\n\nIn regard to shipwrecks, the location of the Bertha remains undiscovered and it is also possible that there are further shipwrecks that have gone unrecorded in present documentation.\n\nThe place possesses uncommon, rare or endangered aspects of the cultural or natural history of New South Wales.\n\nBass Point has been the focus of attention from archaeologists since the late 1960s as an area that has potential to reveal significant information about pre-contact history in NSW.\n\nTwelve midden sites and one camping/meeting place have been identified at Bass Point and archaeological excavations have revealed the environmental change and evolution of the area over time and the development of techniques used by the Aboriginal people to hunt and gather available resources.\n\nAlongside Burrill Lake rock shelter (which is of similar antiquity), Bass Point is considered to be one of the most significant Aboriginal archaeological sites to be excavated in NSW.\n\nThe place is important in demonstrating the principal characteristics of a class of cultural or natural places/environments in New South Wales.\n\nBass Point is representative of prominent headlands in the Illawarra region and contains the coastal vegetation found throughout the area. Bass Point is also representative of places that had established occupation by Aboriginal people due to the plentiful and sustained food resources occurring naturally in the environment.\n\n\n\n"}
{"id": "19767260", "url": "https://en.wikipedia.org/wiki?curid=19767260", "title": "Beacon Power", "text": "Beacon Power\n\nBeacon Power is an American limited liability company and wholly owned subsidiary of Rockland Capital LLC specializing in flywheel-based energy storage headquartered in Tyngsboro, Massachusetts. Beacon designs and develops products aimed at utility frequency regulation for power grid operations.\n\nThe storage systems are designed to help utilities match supply with varying demand by storing excess power in arrays of flywheels at off-peak times for use during peak demand.\n\nBeacon Power was founded in Woburn, Massachusetts in 1997 as a subsidiary of SatCon Technology Corporation, a maker of alternative energy management systems. The company went public in 2000.\n\nIn June 2008, Beacon Power opened new headquarters in Tyngsboro, with financing from Massachusetts state agencies. The new facility is intended to support an expansion of the company's operation.\n\nIn 2009 Beacon received a loan guarantee from the United States Department of Energy (DOE) for $43 million to build a 20-megawatt flywheel power plant in Stephentown, New York.\n\nOn 30 October 2011, the company filed for Chapter 11 bankruptcy protection under in the United States bankruptcy court in Delaware. As part of the bankruptcy court proceedings, Beacon Power agreed on November 18 to sell its Stephentown facility to repay the DOE loan.\n\nAs of 6 February 2012, Rockland Capital, a private equity firm, bought the plant and most of the company's other assets for $30.5 million. Rockland Capital intends to rehire most of staff and to provide the capital to build a second 20MW plant in Pennsylvania.\n\n\n\n"}
{"id": "4656507", "url": "https://en.wikipedia.org/wiki?curid=4656507", "title": "Bose–Hubbard model", "text": "Bose–Hubbard model\n\nThe Bose–Hubbard model gives a description of the physics of interacting spinless bosons on a lattice. It is closely related to the Hubbard model which originated in solid-state physics as an approximate description of superconducting systems and the motion of electrons between the atoms of a crystalline solid. The model was first introduced by Gersch and Knollman in 1963 in the context of granular superconductors. (The term 'Bose' in its name refers to the fact that the particles in the system are bosonic.) The model rose to prominence in the 1980s after it was found to capture the essence of the superfluid-insulator transition in a way that was much more mathematically tractable than fermionic metal-insulator models.\n\nThe Bose–Hubbard model can be used to describe physical systems such as bosonic atoms in an optical lattice, as well as certain magnetic insulators. Furthermore, it can also be generalized and applied to Bose–Fermi mixtures, in which case the corresponding Hamiltonian is called the Bose–Fermi–Hubbard Hamiltonian.\n\nThe physics of this model is given by the Bose–Hubbard Hamiltonian:\n\nformula_1.\n\nHere, formula_2 denotes summation over all neighboring lattice sites formula_3 and formula_4, while formula_5 and formula_6 are regular bosonic creation and annihilation operators such that formula_7 gives the number of particles on site formula_3. The model is parametrized by the hopping amplitude formula_9 describing the mobility of bosons in the lattice, the on-site interaction formula_10 which can be attractive (formula_11) or repulsive (formula_12), and the chemical potential formula_13, which essentially sets the total number of particles. If unspecified, typically the phrase 'Bose–Hubbard model' refers to the case where the on-site interaction is repulsive.\n\nThis Hamiltonian has a global formula_14 symmetry, which means that it is invariant (i.e. its physical properties are unchanged) by the transformation formula_15. In a superfluid phase, this symmetry is spontaneously broken.\n\nThe dimension of the Hilbert space of the Bose–Hubbard model is given by formula_16, where formula_17 is the total number of particles, while formula_18 denotes the total number of lattice sites. At fixed formula_19 or formula_20, the Hilbert space dimension formula_21 grows polynomially, but at a fixed density of formula_22 bosons per site, it grows exponentially as formula_23. Analogous Hamiltonians may be formulated to describe spinless fermions (the Fermi-Hubbard model) or mixtures of different atom species (Bose–Fermi mixtures, for example). In the case of a mixture, the Hilbert space is simply the tensor product of the Hilbert spaces of the individual species. Typically additional terms need to be included to model interaction between species.\n\nAt zero temperature, the Bose–Hubbard model (in the absence of disorder) is in either a Mott insulating state at small formula_24, or in a superfluid state at large formula_25. The Mott insulating phases are characterized by integer boson densities, by the existence of an energy gap for particle-hole excitations, and by zero compressibility. The superfluid is characterized by long-range phase coherence, a spontaneous breaking of the Hamiltonian's continuous formula_14 symmetry, a non-zero compressibility and superfluid susceptibility. At non-zero temperature, in certain parameter regimes there will also be a regular fluid phase which does not break the formula_14 symmetry and does not display phase coherence. Both of these phases have been experimentally observed in ultracold atomic gases.\n\nIn the presence of disorder, a third, ‘‘Bose glass\" phase exists. The Bose glass is a Griffiths phase, and can be thought of as a Mott insulator containing rare 'puddles' of superfluid. These superfluid pools are not connected to each other, so the system remains insulating, but their presence significantly changes the thermodynamics of the model. The Bose glass phase is characterized by a finite compressibility, the absence of a gap, and by an infinite superfluid susceptibility. It is insulating despite the absence of a gap, as low tunneling prevents the generation of excitations which, although close in energy, are spatially separated. The Bose glass has been shown to have a non-zero Edwards-Anderson order parameter and has been suggested to display replica symmetry breaking, however this has not been proven.\n\nThe phases of the clean Bose–Hubbard model can be described using a mean-field Hamiltonian:formula_28where formula_29 is the lattice co-ordination number. This can be obtained from the full Bose-Hubbard Hamiltonian by setting formula_30 where formula_31, neglecting terms quadratic in formula_32 (which we assume to be infinitesimal) and relabelling formula_33. Because this decoupling breaks the formula_14 symmetry of the initial Hamiltonian for all non-zero values of formula_35, this parameter acts as a superfluid order parameter. For simplicity, this decoupling assumes formula_35 to be the same on every site - this precludes exotic phases such as supersolids or other inhomogeneous phases. (Other decouplings are of course possible if one wishes to allow for such phases.)\n\nWe can obtain the phase diagram by calculating the energy of this mean-field Hamiltonian using second-order perturbation theory and finding the condition for which formula_37. To do this, we first write the Hamiltonian as a site-local piece plus a perturbation:formula_38where the bilinear terms formula_39 and its conjugate are treated as the perturbation, as we assume the order parameter formula_35 to be small near the phase transition. The local term is diagonal in the Fock basis, giving the zeroth-order energy contribution:formula_41where formula_42 is an integer that labels the filling of the Fock state. The perturbative piece can be treated with second-order perturbation theory, which leads to:formula_43We can then express the energy as a series expansion in even powers of the order parameter (also known as the Landau formalism):formula_44After doing so, the condition for the mean-field, second-order phase transition between the Mott insulator and the superfluid phase is given by:formula_45where the integer formula_42 describes the filling of the formula_47 Mott insulating lobe. Plotting the line formula_48 for different integer values of formula_42 will generate the boundary of the different Mott lobes, as shown in the phase diagram.\n\nUltracold atoms in optical lattices are considered a standard realization of the Bose–Hubbard model. The ability to tune parameters of the model using simple experimental techniques and the lack of the lattice dynamics which are present in solid-state electronic systems mean that ultracold atoms offer a very clean, controllable realisation of the Bose–Hubbard model. The biggest downside with optical lattice technology is the trap lifetime, with atoms typically only being trapped for a few tens of seconds.\n\nTo see why ultracold atoms offer such a convenient realisation of Bose-Hubbard physics, we can derive the Bose-Hubbard Hamiltonian starting from the second quantized Hamiltonian which describes a gas of ultracold atoms in the optical lattice potential. This Hamiltonian is given by the expression:\n\nformula_50,\n\nwhere formula_51 is the optical lattice potential, formula_52 is the (contact) interaction amplitude, and formula_13 is the chemical potential. The tight binding approximation results in the substitution formula_54 which leads to the Bose-Hubbard Hamiltonian if one restricts the physics to the lowest band (formula_55) and the interactions are local at the level of the discrete mode. Mathematically, this can be stated as the requirement that formula_56 except for case formula_57. Here, formula_58 is a Wannier function for a particle in an optical lattice potential localized around site formula_3 of the lattice and for the formula_60th Bloch band.\n\nThe tight-binding approximation significantly simplifies the second quantized Hamiltonian, though it introduces several limitations at the same time:\n\nQuantum phase transitions in the Bose–Hubbard model were experimentally observed by Greiner et al., and density dependent interaction parameters formula_62 were observed by I.Bloch's group. Single-atom resolution imaging of the Bose–Hubbard model has been possible since 2009 using quantum gas microscopes.\n\nThe Bose–Hubbard model is also of interest to those working in the field of quantum computation and quantum information. Entanglement of ultra-cold atoms can be studied using this model.\n\nIn the calculation of low energy states the term proportional to formula_63 means that large occupation of a single site is improbable, allowing for truncation of local Hilbert space to states containing at most formula_64 particles. Then the local Hilbert space dimension is formula_65 The dimension of the full Hilbert space grows exponentially with the number of sites in the lattice, therefore computer simulations are limited to the study of systems of 15-20 particles in 15-20 lattice sites. Experimental systems contain several millions lattice sites, with average filling above unity. For the numerical simulation of this model, an algorithm of exact diagonalization is presented in this paper.\n\nOne-dimensional lattices may be studied using density matrix renormalization group (DMRG) and related techniques such as time-evolving block decimation (TEBD). This includes to calculate the ground state of the Hamiltonian for systems of thousands of particles on thousands of lattice sites, and simulate its dynamics governed by the Time-dependent Schrödinger equation.\n\nHigher dimensions are significantly more difficult due to the quick growth of entanglement.\n\nAll dimensions may be treated by Quantum Monte Carlo algorithms, which provide a way to study properties of thermal states of the Hamiltonian, as well as the particular the ground state.\n\nBose-Hubbard-like Hamiltonians may be derived for different physical systems containing ultracold atom gas in the periodic potential. They include, but are not limited to:\n\n"}
{"id": "18907411", "url": "https://en.wikipedia.org/wiki?curid=18907411", "title": "Campbell's Kingdom", "text": "Campbell's Kingdom\n\nCampbell's Kingdom is a 1957 British adventure film directed by Ralph Thomas, based on the 1952 novel of the same name by Hammond Innes. The film stars Dirk Bogarde and Stanley Baker, with Michael Craig, Barbara Murray, James Robertson Justice and Sid James in support. The story is set in Alberta, Canada, and largely follows the principles of the Northwestern genre of film-making.\n\nRecently diagnosed with a terminal disease, Bruce Campbell (Dirk Bogarde) unexpectedly finds himself the owner of a small valley in the Canadian Rocky Mountains as the result of a bequest from his grandfather. After travelling from England, Bruce arrives at \"Campbell's Kingdom\" (as the locals disparagingly call it) to find its existence under threat from the construction of a new hydroelectricity dam. Convinced that his grandfather was right and that the Kingdom may be prospective for oil, the race is on to prove that there is oil under Campbell's Kingdom before the mining company building the dam can flood the valley. Standing in his way is corrupt construction contractor Owen Morgan (Stanley Baker), who resorts to dirty tricks in order to prevent Campbell from succeeding in his quest. However, Bruce is ably and enthusiastically assisted by love interest Jean Lucas (Barbara Murray), geologist Boy Bladen (Michael Craig) and drilling contractor James MacDonald (James Robertson Justice). Unfortunately for Campbell the residents of the nearby town of Come Lucky invested heavily in his grandfather's schemes, only to feel cheated when his projects came to nothing. Gradually Bruce manages to turn them around by exposing the fraud and lies of Morgan and the mining company.\n\nHammond Innes's novel was published in 1952 and was based on the Canadian oil boom of the late 1940s. Innes researched the novel extensively and it was a best seller.\n\nThe story was serialised as \"Nothing to Lose\". Film rights were purchased prior to the novel's publication by the producing-directing team of Betty Box and Ralph Thomas, who had just made \"The Venetian Bird\". Location scouting for the film began in August 1952, with Box and Thomas touring the Canadian Rockies.\n\n\"That serial title refers to the hero's special armour,\" said Ralph Thomas. \"He's a young Canadian whose friends help him strike oil on a piece of land willed by his grandfather to disprove the general belief that the old man was unhinged. And he happens to be fatally stricken. He's a dying man.\"\n\n\"There's quite a bit of room for suspense,\" said Betty Box. \"For one thing a big oil company is obstructing him. Our climax will have the gusher flooded by a break in a huge dam. With an ingenious twist.\"\n\nEric Ambler was assigned to write the script. In 1954 Jack Hawkins and Shelley Winters were mentioned as possible stars.\n\nHowever, because it was an expensive project the film took a number of years to be financed. Box and Thomas had a major box office hit in \"Doctor in the House\" (1954) which turned Dirk Bogarde into a star. Bogarde was ideal casting for \"Campbell's Kingdom\" and he became attached to the project. The film was almost made after \"Doctor at Sea\" when there was a financial crisis in the British film industry and Rank requested Box, Thomas and Bogarde make a third \"Doctor\" film, \"Doctor at Large\", instead. While this was done, Box, Ambler and Innes worked on the script to reduce the budget.\n\nThe tremendous success of \"Doctor at Large\" led to Rank financing \"Campbell's Kingdom\". \"We'd all earned a change of subject, we felt,\" wrote Box later.\n\nThe film would be one of a number of films made by the Rank Organisation in the 1950s to appeal to the international market. They tended to be adventure films shot on location overseas in colour based on some best selling novel. Other examples include \"Windom's Way\", \"Robbery Under Arms\", \"The Wind Cannot Read\", \"Nor the Moon by Night\" and \"Ferry to Hong Kong\".\n\nBox said Eric Ambler had a very different approach to writing to Hammond Innes. \"Innes was a writer who spent as much time researching his books as he did writing them,\" she wrote. \"Eric's books depend much more on characterisation, motivation and ideologies rather than realistic documentation. I remember him telling me that he hadn't even visited most of the countries he wrote about, and I sensed that he wasn't entirely happy working on \"Campbell's Kingdom\".\" Another writer, Robin Estridge, was hired to work on the script.\n\nRay Milland had tried to buy the rights to the novel. He offered to play the role of Campbell for no cash payment and a percentage of the profits, but Rank cast Bogarde.\n\nAlthough the story is set in Alberta, it was too expensive to film there. Box wrote, \"In those days dollars were very scarce, the Treasury didn't want to know and by shooting in Europe we would save a considerable amount of money that would have been spent in transporting people and equipment across the Atlantic and on to the foothills of the Rockies.\" The landscape exteriors were shot in the Italian Dolomites and the rest was shot at Pinewood Studios outside London.\n\nBetty Box says three of the cast brought their wives on location to Italy - Michael Craig, Stanley Baker and Sid James - and all three were pregnant.\n\nBox later wrote that the film \"was released during one of Britain's worst flu epidemics - Asian flu - which didn't help at the box office\" but that \"overseas the movie was successful. We even sold it to the Russians. It took me an hour or two to work that one out... The dam bursts because the wicked capitalists in Canada use cheap cement!\"\n\nDirk Bogarde won the \"Picturegoer\" Award for Best Actor for the film.\n\nThe film was released in the US in 1960.\n\nRichard Gregson, film producer, agent and brother of actor Michael Craig, later gave \"Campbell's Kingdom\" as an example of the output of Rank under Sir John Davis in the late 50s:\nThose awful middle class pictures that had no meaning for anyone, because somehow he couldn't come to terms with the fact that the vast majority of people in this country were lower middle class and working class people. So all those boring tales like \"Campbell's Kingdom\" had nothing to do with real life. If you are clever enough as Korda and Balcon were, you can get away with it, but Davis wasn't. It was very square entertainment.\nBFI Screenonline said \"Campbell's Kingdom\", along with later Bogarde-Box-Thomas adventure films such as \"The Wind Cannot Read\" and \"The High Bright Sun\" \"could have benefited from a more conventionally rugged leading man\" than Bogarde.\n\n\n"}
{"id": "12013342", "url": "https://en.wikipedia.org/wiki?curid=12013342", "title": "Chemical transport reaction", "text": "Chemical transport reaction\n\nIn chemistry, a chemical transport reaction describes a process for purification and crystallization of non-volatile solids. The process is also responsible for certain aspects of mineral growth from the effluent of volcanoes. The technique is distinct from chemical vapor deposition, which usually entails decomposition of molecular precursors (e.g. SiH → Si + 2H) and which gives conformal coatings.\n\nThe technique, which was popularized by Schäfer, entails the reversible conversion of nonvolatile elements and chemical compounds into volatile derivatives. The volatile derivative migrates throughout a sealed reactor, typically a sealed and evacuated glass tube heated in a tube furnace. Because the tube is under a temperature gradient, the volatile derivative reverts to the parent solid and the transport agent is released at the end opposite to which it originated (see next section). The transport agent is thus catalytic. The technique requires that the two ends of the tube (which contains the sample to be crystallized) be maintained at different temperatures. So-called two-zone tube furnaces are employed for this purpose. The method derives from the Van Arkel de Boer process which was used for the purification of titanium and vanadium and uses iodine as the transport agent.\nTransport reactions are classified according to the thermodynamics of the reaction between the solid and the transporting agent. When the reaction is exothermic, then the solid of interest is transported from the cooler end (which can be quite hot) of the reactor to a hot end, where the equilibrium constant is less favorable and the crystals grow. The reaction of molybdenum dioxide with the transporting agent iodine is an exothermic process, thus the MoO migrates from the cooler end (700 °C) to the hotter end (900 °C):\nUsing 10 milligrams of iodine for 4 grams of the solid, the process requires several days. \n\nAlternatively, when the reaction of the solid and the transport agent is endothermic, the solid is transported from a hot zone to a cooler one. For example:\nThe sample of iron(III) oxide is maintained at 1000 °C, and the product is grown at 750 °C. HCl is the transport agent. Crystals of hematite are reportedly observed at the mouths of volcanoes because of chemical transport reactions whereby volcanic hydrogen chloride volatilizes iron(III) oxides.\n\nA similar reaction like that of MoO is used in halogen lamps. The tungsten is evaporated from the tungsten filament and converted with traces of oxygen and iodine into the WOI, at the high temperatures near the filament the compound decomposes back to tungsten, oxygen and iodine. \n"}
{"id": "10507462", "url": "https://en.wikipedia.org/wiki?curid=10507462", "title": "Commercial sorghum", "text": "Commercial sorghum\n\nCommercial sorghum is the cultivation and commercial exploitation of species of grasses within the genus \"Sorghum\" (often \"S. bicolor\"). These plants are used for grain, fibre and fodder. The plants are cultivated in warmer climates worldwide. Commercial \"Sorghum\" species are native to tropical and subtropical regions of Africa and Asia.\n\nOther names include \"durra\", Egyptian millet, \"feterita\", Guinea corn, \"jwari\" ज्वारी (Marathi), \"jowar\", \"juwar\", milo, \"shallu\", Sudan grass, \"cholam\" (Tamil), \"jola\"/ಜೋಳ (Kannada), \"jonnalu\" (Telugu), \"gaoliang\" (), great millet, \"kafir\" corn, \"dura\", \"dari\", \"mtama\", and \"solam\".\n\nSorghum has been, for centuries, one of the most important staple foods for millions of poor rural people in the semiarid tropics of Asia and Africa. For some impoverished regions of the world, sorghum remains a principal source of energy, protein, vitamins and minerals. Sorghum grows in harsh environments where other crops do not grow well, just like other staple foods, such as cassava, that are common in impoverished regions of the world. It is usually grown without application of any fertilizers or other inputs by a multitude of small-holder farmers in many countries.\n\nGrain sorghum is the third most important cereal crop grown in the United States and the fifth most important cereal crop grown in the world. In 2010, Nigeria was the world's largest producer of grain sorghum, followed by the United States and India. In developed countries, and increasingly in developing countries such as India, the predominant use of sorghum is as fodder for poultry and cattle. Leading exporters in 2010 were the United States, Australia and Argentina; Mexico was the largest importer of sorghum.\n\nAn international effort is under way to improve sorghum farming. The International Crops Research Institute for the Semi-Arid Tropics (ICRISAT) has improved sorghum using traditional genetic improvement and integrated genetic and natural resources management practices. New varieties of sorghum from ICRISAT has now resulted in India producing 7 tons per hectare. Some 194 improved cultivars are now planted worldwide. In India, increases in sorghum productivity resulting from improved cultivars have freed up six million hectares of land, enabling farmers to diversify into high-income cash crops and boost their livelihoods. Sorghum is used primarily as poultry feed, and secondarily as cattle feed and in brewing applications.\n\nThe last wild relatives of commercial sorghum are currently confined to Africa south of the Sahara — although Zohary and Hopf add \"perhaps\" Yemen and Sudan — indicating its domestication took place there. However, note Zohary and Hopf, \"the archaeological exploration of sub-Saharan Africa is yet in its early stages, and we still lack critical information for determining where and when sorghum could have been taken into cultivation.\" Although rich finds of \"S. bicolor\" have been recovered from Qasr Ibrim in Egyptian Nubia, the wild examples have been dated to \"circa\" 800–600 BCE, and the domesticated ones no earlier than CE 100. The earliest archeological evidence comes from sites dated to the second millennium BC in India and Pakistan — where \"S. bicolor\" is not native. These incongruous finds have been interpreted, according again to Zohary and Hopf,\n\nMost cultivated varieties of sorghum can be traced back to Africa, where they grow on savanna lands. During the Muslim Agricultural Revolution, sorghum was planted extensively in parts of the Middle East, North Africa and Europe. The name \"sorghum\" comes from Italian \"sorgo\", in turn from Latin \"Syricum (granum)\" meaning \"grain of Syria\".\n\nDespite the antiquity of sorghum, it arrived late to the Near East. It was unknown in the Mediterranean area into Roman times. Tenth century records indicate it was widely grown in Iraq, and became the principal food of Kirman in Persia. In addition to the eastern parts of the Muslim world, the crop was also grown in Egypt and later in Islamic Spain. From Islamic Spain, it was introduced to Christian Spain and then France (by the 12th century). In the Muslim world, sorghum was grown usually in areas where the soil was poor or the weather too hot and dry to grow other crops.\n\nSorghum is well adapted to growth in hot, arid or semiarid areas. The many subspecies are divided into four groups — grain sorghums (such as milo), grass sorghums (for pasture and hay), sweet sorghums (formerly called \"Guinea corn\", used to produce sorghum syrups), and broom corn (for brooms and brushes). The name \"sweet sorghum\" is used to identify varieties of \"S. bicolor\" that are sweet and juicy.\n\nSorghum is used for food, fodder, and the production of alcoholic beverages. It is drought-tolerant and heat-tolerant, and is especially important in arid regions. It is an important food crop in Africa, Central America, and South Asia, and is the \"fifth most important cereal crop grown in the world\".\n\nThe FAO reports that 440,000 square kilometres were devoted worldwide to sorghum production in 2004. In the US, sorghum grain is used primarily as a maize (corn) substitute for livestock feed because their nutritional values are very similar. Some hybrids commonly grown for feed have been developed to deter birds, and therefore contain a high concentration of tannins and phenolic compounds, which causes the need for additional processing to allow the grain to be digested by cattle.\n\nFAO reported the United States of America was the top producer of sorghum in 2009, with a harvest of 9.7 million tonnes. The next four major producers of sorghum, in decreasing quantities, were India, Nigeria, Sudan and Ethiopia. The other major sorghum producing regions in the world, by harvested quantities, were: Australia, Brazil, China, Burkina Faso, Argentina, Mali, Cameroon, Egypt, Niger, Tanzania, Chad, Uganda, Mozambique, Venezuela, and Ghana.\n\nIn the future, use of sorghum may increase in Tanzania, as farmers replace maize with the drought-resistant crop in areas where rainfall declines due to climate change. Following lobbying by the ICRISAT-led Hope Project, the government recently included improved varieties of sorghum in its seed subsidy programme and agreed to provide a fertiliser subsidy programme for sorghum for the first time. This means that the government will buy seed from seed companies and sell it to farmers at almost half the market price. Tanzania's farmers have reported that improved sorghum varieties grow quickly, demand less labour and are more resistant to pests and diseases.\n\nThe world harvested 55.6 million tonnes of sorghum in 2010. The world average annual yield for the 2010 sorghum crop was 1.37 tonnes per hectare. The most productive farms of sorghum were in Jordan, where the national average annual yield was 12.7 tonnes per hectare. The national annual average yield in world's largest producing country, the USA, was 4.5 tonnes per hectare.\n\nThe allocation of farm area to sorghum crops has been dropping, while the yields per hectare have been increasing. The biggest sorghum crop the world produced in the last 40 years was in 1985, with 77.6 million tonnes harvested that year.\n\nIn arid, less developed regions of the world, sorghum is an important food crop, especially for subsistence farmers. It is used to make such foods as couscous, sorghum flour, porridge and molasses.\n\n\"Bhakri\" (\"jolada rotti\" in northern Karnataka), a variety of unleavened bread usually made from sorghum, is the staple diet in many parts of India, such as Maharashtra state and northern Karnataka state. In eastern Karnataka and the Rayalaseema area of Andhra Pradesh, \"roti\" (\"jonna rotte\") made with sorghum is the staple food.\n\nIn South Africa, sorghum meal is often eaten as a stiff porridge much like pap. It is called \"mabele\" in Northern Sotho and \"brown porridge\" in English. The porridge can be served with \"maswi\" (soured milk) or \"merogo\" (a mixture of boiled greens much like collard greens or spinach).\n\nIn Ethiopia, sorghum is fermented to make injera flatbread, and in Sudan it is fermented to make kisra. In India, dosa is sometimes made with a sorghum-grain mixture, but rice is more commonly used in place of sorghum.\n\nIn the cuisine of the Southern United States, sorghum syrup was used as a sweet condiment, much as maple syrup was used in the North, usually for biscuits, corn bread, pancakes, hot cereals or baked beans. It is uncommon today.\n\nIn Arab cuisine, the unmilled grain is often cooked to make couscous, porridges, soups, and cakes. Many poor use it, along with other flours or starches, to make bread. The seeds and stalks are fed to cattle and poultry. Some varieties have been used for thatch, fencing, baskets, brushes and brooms, and stalks have been used as fuel. Medieval Islamic texts list medical uses for the plant.\n\nSorghum seeds can be popped in the same manner as popcorn (i.e., with oil or hot air, etc.), although the popped kernels are smaller than popcorn (see photo on the right).\n\nSorghum sometimes is used for making tortillas (e.g., in Central America). In El Salvador, they sometimes use sorghum (\"maicillo\") to make tortillas when there is not enough corn.\n\nSince 2000, sorghum has come into increasing use in homemade and commercial breads and cereals made specifically for the gluten-free diet.\n\nIn China, sorghum is the most important ingredient for the production of distilled beverages, such as \"maotai\" and \"kaoliang wine\", as seen in the 1987 film \"Red Sorghum\".\n\nIn southern Africa, sorghum is used to produce beer, including the local version of Guinness. In recent years, sorghum has been used as a substitute for other grain in gluten-free beer. Although the African versions are not \"gluten-free\", as malt extract is also used, truly gluten-free beer using such substitutes as sorghum or buckwheat are now available. Sorghum is used in the same way as barley to produce a \"malt\" that can form the basis of a mash that will brew a beer without gliadin or hordein (together \"gluten\") and therefore can be suitable for coeliacs or others sensitive to certain glycoproteins.\n\nIn November 2006, Lakefront Brewery of Milwaukee, Wisconsin, launched its \"New Grist\" gluten-free beer, brewed with sorghum and rice. It is one of its most successful lines. It is aimed at those with coeliac disease, although its low-carb content also makes it popular with health-minded drinkers.\n\nIn December 2006, Anheuser-Busch of St. Louis, Missouri, introduced their new \"Redbridge\" beer. This beer is gluten-free and is produced with sorghum as the main ingredient. Redbridge was the first sorghum-based beer to be nationally distributed in the United States.\n\nAfrican sorghum beer is a brownish-pink beverage with a fruity, sour taste. Its alcohol content can vary between 1% and 8%. African sorghum beer is high in protein, which contributes to foam stability, giving it a milk-like head. Because this beer is not filtered, its appearance is cloudy and yeasty, and may also contain bits of grain. This beer is said to be very thirst-quenching, even if it is traditionally consumed at room temperature.\n\nAfrican sorghum beer is a popular drink primarily amongst the black community for historical reasons. African sorghum beer is said to be a traditional drink of the Zulu people of Southern Africa. It also became popular amongst the black community in South Africa, in part because the only exception to the prohibition, which was lifted in 1962 and only applied to black people, was sorghum beer.\n\nSorghum beer is also associated with the development of the segregationist \"Durban System\" in South Africa in the early 20th Century. The turn of the 20th Century saw growing segregationist tendencies amongst the white populations of South African towns. Fearful of the alleged diseases of black residents, the white populations of these towns sought to prevent black Africans from gaining permanent residence in urban areas, and separate them from the white communities. Within this context, two municipalities, Durban and Pietermaritzburg, devised a system by which black Africans in their locality would be housed in 'native locations' outside the main towns, with their segregated accommodation paid for from revenues from the municipal monopoly over sorghum beer. This solved the problem of white rate-payers having to foot the cost of segregation, and ensured the whole scheme paid for itself. After the passage of the 1923 Natives (Urban Areas) Act, all municipalities in South Africa were given the powers to enforce racial segregation, and the Durban System was extended throughout the union, ensuring that segregation was paid for from African rents and beerhall monopolies.\n\nSorghum beer is called \"bjala\" in northern Sotho and is traditionally made to mark the unveiling of a loved-one's tombstone. The task of making the beer falls traditionally to women. The process is begun several days before the party, when the women of the community gather together to bring the sorghum and water to a boil in huge cast iron pots over open fires. After the mix has fermented for several days, it is strained - a somewhat labor-intensive task. Sorghum beer is known by many different names in various countries across Africa, including Umqombothi (South Africa) \"burukuto\" (Nigeria), \"pombe\" (East Africa) and \"bil-bil\" (Cameroon). African sorghum beer brewed using grain sorghum undergoes lactic acid fermentation, as well as alcoholic fermentation.\n\nThe steps in brewing African sorghum beer are: malting, mashing, souring and alcoholic fermentation. All steps, with the exception of the souring, can be compared to traditional beer brewing.\n\nThe souring of African sorghum beer by lactic acid fermentation is responsible for the distinct sour taste. Souring may be initiated using yogurt, sour dough starter cultures, or by spontaneous fermentation. The natural microflora of the sorghum grain maybe also be the source of lactic acid bacteria; a handful of raw grain sorghum or malted sorghum may be mixed in with the wort to start the lactic acid fermentation. Although many lactic acid bacteria strains may be present, \"Lactobacillus\" spp. is responsible for the lactic acid fermentation in African sorghum beer.\n\nCommercial African sorghum beer is packaged in a microbiologically active state. The lactic acid fermentation and/or alcoholic fermentation may still be active. For this reason, special plastic or carton containers with vents are used to allow gas to escape. Spoilage is a big safety concern when it comes to African sorghum beer. Packaging does not occur in sterile conditions and many microorganisms may contaminate the beer. Also, using wild lactic acid bacteria increases the chances of spoilage organisms being present. However, the microbiologically active characteristic of the beer also increases the safety of the product by creating competition between organisms. Although aflatoxins from mould were found on sorghum grain, they were not found in industrially produced African sorghum beer.\n\nSorghum straw (stem fibres) can also be made into excellent wallboard for house building, as well as biodegradable packaging. Since it does not accumulate static electricity, it is also used in packaging materials for sensitive electronic equipment.\n\nLittle research has been done to improve sorghum cultivars because the vast majority of sorghum production is done by subsistence farmers. The crop is therefore mostly limited by insects, disease and weeds, rather than by the plant's inherent ability. To improve the plant's viability in sustaining populations in drought-prone areas, a larger capital investment would be necessary to control plant pests and ensure optimum planting and harvesting practices.\n\nIn November 2005, however, the US Congress passed a Renewable Fuels Standard as part of the Energy Policy Act of 2005, with the goal of producing 30 billion litres (8 billion gallons) of renewable fuel (ethanol) annually by 2012. Currently, 12% of grain sorghum production in the US is used to make ethanol.\n\nAn AP article claims that sorghum-sap-based ethanol has four times the energy yield as corn-based ethanol, but is on par with sugarcane. \n\nSorghum requires an average temperature of at least 25 °C to produce maximum grain yields in a given year. Maximum photosynthesis is achieved at daytime temperatures of at least 30 °C. Night time temperatures below 13 °C for more than a few days can severely reduce the plants' potential grain production. Sorghum cannot be planted until soil temperatures have reached 17 °C. The long growing season, usually 90–120 days, causes yields to be severely decreased if plants are not in the ground early enough.\n\nGrain sorghum is usually planted with a commercial corn seeder at a depth of 2–5 cm, depending on the density of the soil (shallower in heavier soil). The goal in planting, when working with fertile soil, is 50,000 to 300,000 plants per hectare. Therefore, with an average emergence rate of 75%, sorghum should be planted at a rate of 2–12 kg of seed per hectare.\n\nYields have been found to be boosted by 10–15% when optimum use of moisture and sunlight are available, by planting in 25 cm rows instead of the conventional 1-meter rows.\n\nSorghum, in general, is a very competitive crop, and does well in competition with weeds in narrow rows. Sorghum produces a chemical compound called sorgoleone, which the plant uses to combat weeds. The chemical is so effective in preventing the growth of weeds it sometime prohibits the growth of other crops harvested on the same field. To address this problem, researchers at the Agricultural Research Service found two gene sequences believed to be responsible for the enzymes that secrete the chemical compound sorogoleone. The discovery of these gene sequences will help researchers one day in developing sorghum varieties that cause less soil toxicity and potentially target gene sequences in other crops to increase their natural pesticide capabilities, as well.\n\nInsect and diseases are not prevalent in sorghum crops. Birds, however, are a major source of yield loss. Hybrids with higher tannin content and growing the crop in large field blocks are solutions used to combat the birds. The crop may also be attacked by corn earworms, aphids, and some Lepidoptera larvae, including turnip moths.\n\nIt is a very high nitrogen-feeding crop. An average hectare producing 6.3 tonnes of grain yield requires 110 kg of nitrogen, but relatively small amounts of phosphorus and potassium (15 kg of each).\n\nSorghum’s growth habit is similar to that of maize, but with more side shoots and a more extensively branched root system. The root system is very fibrous, and can extend to a depth of up to 1.2 m. The plant finds 75% of its water in the top metre of soil, and because of this, in dry areas, the plant’s production can be severely affected by the water holding capacity of the soil. The plants require up to 70–100 mm of moisture every 10 days in early stages of growth, and as sorghum progresses through growth stages and the roots penetrate more deeply into the soil to tap into hidden water reserves, the plant needs progressively less water. By the time the seed heads are filling, optimum water conditions are down to about 50 mm every 10 days. Compacted soil or shallow topsoil can limit the plant's ability to deal with drought by limiting its root system. Since these plants have evolved to grow in hot, dry areas, it is essential to keep the soil from compacting and to grow on land with ample cultivated topsoil.\n\nWild species of sorghum tend to grow to a height of 1.5–2 m; however, due to problems this height created when the grain was being harvested, in recent years, cultivars with genes for dwarfism have been selected, resulting in sorghum that grows to between 60 and 120 cm tall.\n\nSorghum's yields are not affected by short periods of drought as severely as other crops such as maize, because it develops its seed heads over longer periods of time, and short periods of water stress do not usually have the ability to prevent kernel development. Even in a long drought severe enough to hamper sorghum production, it will still usually produce some seed on smaller and fewer seed heads. Rarely will one find a kernelless season for sorghum, even under the most adverse water conditions. Sorghum's ability to thrive with less water than maize may be due to its ability to hold water in its foliage better than maize. Sorghum has a waxy coating on its leaves and stems which helps to keep water in the plant, even in intense heat.\n\nSorghum is about 70% starch, so is a good energy source. Its starch consists of 70 to 80% amylopectin, a branched-chain polymer of glucose, and 20 to 30% amylose, a straight-chain polymer.\n\nThe digestibility of the sorghum starch is relatively poor in its unprocessed form, varying between 33 and 48%. Processing of the grain by methods such as steaming, pressure cooking, flaking, puffing or micronization of the starch increases the digestibility of sorghum starch. This has been attributed to a release of starch granules from the protein matrix, rendering them more susceptible to enzymatic digestion.\n\nOn cooking, the gelatinized starch of sorghum tends to return from the soluble, dispersed and amorphous state to an insoluble crystalline state. This phenomenon is known as retrogradation; it is enhanced with low temperatures and high concentrations of starch. Amylose, the linear component of the starch, is more susceptible to retrogradation.\n\nCertain sorghum varieties contain antinutritional factors such as tannins. The presence of tannins is claimed to contribute to the poor digestibility of sorghum starch. Processing in humid thermal environments aids in lowering the antinutritional factors.\n\nSorghum starch does not contain gluten. This makes it a possible grain for those who are gluten sensitive.\n\nAfter starch, proteins are the main constituent of sorghum. The essential amino acid profile of sorghum protein is claimed to depend on the sorghum variety, soil and growing conditions. A wide variation has been reported. For example, lysine content in sorghum has been reported to vary from 71 to 212 mg per gram of nitrogen. Some studies on sorghum's amino acid composition suggest albumin and globulin fractions contained high amounts of lysine and tryptophan and in general were well-balanced in their essential amino acid composition. On the other hand, some studies claim sorghum's prolamin fraction was extremely poor in lysine, arginine, histidine and tryptophan and contained high amounts of proline, glutamic acid and leucine. The digestibility of sorghum protein has also been found to vary between different varieties and source of sorghum, ranging from 30 to 70%.\n\nA World Health Organization report suggests the inherent capacity of the existing sorghum varieties commonly consumed in poor countries was not adequate to meet the growth requirements of infants and young children. The report also claimed sorghum alone may not be able to meet the healthy maintenance requirements in adults. A balanced diet would supplement sorghum with other food staples.\n\nSorghum's nutritional profile includes several minerals. This mineral matter is unevenly distributed and is more concentrated in the germ and the seed coat. In milled sorghum flours, minerals such as phosphorus, iron, zinc and copper decreased with lower extraction rates. Similarly, pearling the grain to remove the fibrous seed coat resulted in considerable reductions in the mineral contents of sorghum. The presence of antinutrition factors such as tannins in sorghum reduces its mineral availability as food. It is important to process and prepare sorghum properly to improve its nutrition value.\n\nSorghum is a good source of B-complex vitamins. Some varieties of sorghum contain β-carotene which can be converted to vitamin A by the human body; given the photosensitive nature of carotenes and variability due to environmental factors, scientists claim sorghum is likely to be of little importance as a dietary source of vitamin A precursor. Some fat-soluble vitamins, namely D, E and K, have also been found in sorghum grain in detectable, but insufficient, quantities. Sorghum as it is generally consumed is not a source of vitamin C.\n\nThe following table shows the nutrient content of sorghum and compares it to major staple foods in a raw form. Raw forms of these staples, however, are not edible and cannot be digested. These must be prepared and cooked as appropriate for human consumption. In processed and cooked form, the relative nutritional and antinutritional contents of each of these grains is remarkably different from that of the raw forms reported in this table. The nutrition value for each staple food in cooked form depends on the cooking method (for example: boiling, baking, steaming, frying, etc.).\n\n\n"}
{"id": "55300439", "url": "https://en.wikipedia.org/wiki?curid=55300439", "title": "Corey-Pauling rules", "text": "Corey-Pauling rules\n\nIn biochemistry, the Corey-Pauling rules, not to be confused with Pauling's rules, are a set of three basic statements that govern the secondary nature of proteins, in particular, the CO-NH peptide link. They were originally proposed by Robert Corey and Linus Pauling.\n\nThe rules are as follows:\n"}
{"id": "38556915", "url": "https://en.wikipedia.org/wiki?curid=38556915", "title": "Crucey Solar Park", "text": "Crucey Solar Park\n\nThe Crucey Solar Park is a 60 MW solar farm in France. It was built by EDF Énergies Nouvelles in the communes of Maillebois, Crucey-Villages, and Louvilliers-lès-Perche. It has 741,150 thin-film photovoltaics panels made by First Solar.\n\n"}
{"id": "362222", "url": "https://en.wikipedia.org/wiki?curid=362222", "title": "Distributor", "text": "Distributor\n\nA distributor is an enclosed rotating shaft used in spark-ignition internal combustion engines that have mechanically-timed ignition. The distributor's main function is to route secondary, or high voltage, current from the ignition coil to the spark plugs in the correct firing order, and for the correct amount of time. Except in magneto systems, the distributor also houses a mechanical or inductive breaker switch to open and close the ignition coil's primary circuit.\n\nThe first reliable battery operated ignition was developed by Dayton Engineering Laboratories Co. (Delco) and introduced in the 1910 Cadillac. This ignition was developed by Charles Kettering and was considered a wonder in its day. Atwater Kent invented his Unisparker ignition system about this time in competition with the Delco system. By the end of the 20th century mechanical ignitions were disappearing from automotive applications in favor of inductive or capacitive electronic ignitions fully controlled by engine control units (ECU), rather than directly timed to the engine's crankshaft speed.\n\nA distributor consists of a rotating arm or rotor inside the distributor cap, on top of the distributor shaft, but insulated from it and the body of the vehicle (ground). The distributor shaft is driven by a gear on the camshaft on most overhead valve engines, and attached directly to a camshaft on most overhead cam engines. (The distributor shaft may also drive the oil pump.) The metal part of the rotor contacts the high voltage cable from the ignition coil via a spring-loaded carbon brush on the underside of the distributor cap. The metal part of the rotor arm passes close to (but does not touch) the output contacts which connect via high tension leads to the spark plug of each cylinder. As the rotor spins within the distributor, electric current is able to jump the small gaps created between the rotor arm and the contacts due to the high voltage created by the ignition coil.\n\nThe distributor shaft has a cam that operates the contact breaker (also called \"points\"). Opening the points causes a high induction voltage in the system's ignition coil.\n\nThe distributor also houses the centrifugal advance unit: a set of hinged weights attached to the distributor shaft, that cause the breaker points mounting plate to slightly rotate and advance the spark timing with higher engine revolutions per minute (rpm). In addition, the distributor has a vacuum advance unit that advances the timing even further as a function of the vacuum in the inlet manifold. Usually there is also a capacitor attached to the distributor. The capacitor is connected parallel to the breaker points, to suppress sparking to prevent excessive wear of the points.\n\nAround the 1970s the primary breaker points were largely replaced with a Hall effect sensor or optical sensor. As this is a non-contacting device and the ignition coil is controlled by solid state electronics, a great amount of maintenance in point adjustment and replacement was eliminated. This also eliminates any problem with breaker follower or cam wear, and by eliminating a side load it extends distributor shaft bearing life. The remaining secondary (high voltage) circuit stayed essentially the same, using an ignition coil and a rotary distributor.\n\nMost distributors used on electronically fuel injected engines lack vacuum and centrifugal advance units. On such distributors, the timing advance is controlled electronically by the engine computer. This allows more accurate control of ignition timing, as well as the ability to alter timing based on factors other than engine speed and manifold vacuum (such as engine temperature). Additionally, eliminating vacuum and centrifugal advance results in a simpler and more reliable distributor.\n\nThe distributor cap is the cover that protects the distributor's internal parts and holds the contacts between internal rotor and the spark plug wires.\n\nThe distributor cap has one post for each cylinder, and in points ignition systems there is a central post for the current from the ignition coil coming into the distributor. There are some exceptions however, as some engines (many Alfa Romeo cars, some 1980s Nissans) have \"two\" spark plugs per cylinder, so there are two leads coming out of the distributor per cylinder. Another implementation is the wasted spark system, where a single contact serves two leads, but in that case each lead connects one cylinder. In General Motors high energy ignition (HEI) systems there is no central post and the ignition coil sits on top of the distributor. Some Toyota and Honda engines also have their coil within the distributor cap. On the inside of the cap there is a terminal that corresponds to each post, and the plug terminals are arranged around the circumference of the cap according to the firing order in order to send the secondary voltage to the proper spark plug at the right time.\n\nThe rotor is attached to the top of the distributor shaft which is driven by the engine's camshaft and thus synchronized to it. Synchronization to the camshaft is required as the rotor must turn at exactly half the speed of the main crankshaft in the 4-stroke cycle. Often, the rotor and distributor are attached directly to the end of the one of (or the only) camshaft, at the opposite end to the timing drive belt. This rotor is pressed against a carbon brush on the center terminal of the distributor cap which connects to the ignition coil. The rotor is constructed such that the center tab is electrically connected to its outer edge so the current coming in to the center post travels through the carbon point to the outer edge of the rotor. As the camshaft rotates, the rotor spins and its outer edge passes each of the internal plug terminals to fire each spark plug in sequence.\n\nEngines that use a mechanical distributor may fail if they run into deep puddles because any water that gets onto the distributor can short out the electric current that should go through the spark plugs, rerouting it directly to the body of the vehicle. This in turn causes the engine to stop as the fuel is not ignited in the cylinders. This problem can be fixed by removing the distributor's cap and drying the cap, cam, rotor and the contacts by wiping with tissue paper or a clean rag, by blowing hot air on them, or using a moisture displacement spray e.g. WD-40 or similar. Oil, dirt or other contaminants can cause similar problems, so the distributor should be kept clean inside and outside to ensure reliable operation. Some engines include a rubber o-ring or gasket between the distributor base and cap to help prevent this problem. The gasket is made of a material like Viton or butyl for a tight seal in extreme temperatures and chemical environments. This gasket should not be discarded when replacing the cap. Most distributor caps have the position of the number 1 cylinder's terminal molded into the plastic. By referencing a firing order diagram and knowing the direction the rotor turns, (which can be seen by cranking the engine with the cap off) the spark plug wires can be correctly routed. Most distributor caps are designed so that they cannot be installed in the wrong position. Some older engine designs allow the cap to be installed in the wrong position by 180 degrees, however. The number 1 cylinder position on the cap should be noted before a cap is replaced.\n\nThe distributor cap is a prime example of a component that eventually succumbs to heat and vibration. It is a relatively easy and inexpensive part to replace if its bakelite housing does not break or crack first. Carbon deposit accumulation or erosion of its metal terminals may also cause distributor-cap failure.\n\nAs it is generally easy to remove and carry off, the distributor cap can be taken off as a means of theft prevention. Although not practical for everyday use, because it is essential for the starting and running of the engine, its removal thwarts any attempt at hot-wiring the vehicle.\n\nModern engine designs have abandoned the high-voltage distributor and coil, instead performing the distribution function in the primary circuit electronically and applying the primary (low-voltage) pulse to individual coils for each spark plug, or one coil for each pair of companion cylinders in an engine (two coils for a four-cylinder, three coils for a six-cylinder, four coils for an eight-cylinder, and so on).\n\nIn traditional remote distributorless systems, the coils are mounted together in a transformer oil filled 'coil pack', or separate coils for each cylinder, which are secured in a specified place in the engine compartment with wires to the spark plugs, similar to a distributor setup. General Motors, Ford, Chrysler, Hyundai, Subaru, Volkswagen and Toyota are among the automobile manufacturers known to have used coil packs. Coil packs by Delco for use with General Motors engines allow removal of the individual coils in case one should fail, but in most other remote distributorless coil pack setups, if a coil were to fail, replacement of the whole pack would be required to fix the problem.\n\nMore recent layouts utilize a coil located very near to (\"Coil-Near-Plugs\") or directly on top of each spark plug (Direct Ignition, \"DI\", \"coil-on-plug\", or \"COP\"). This design avoids the need to transmit very high voltages, which is often a source of trouble, especially in damp conditions.\n\nBoth direct and remote distributorless systems also allow finer levels of ignition control by the engine computer, which helps to increase power output, decrease fuel consumption and emissions, and implement features such as cylinder deactivation. Spark plug wires, which need routine replacement due to wear, are also eliminated when the individual coils are mounted directly on top of each plug, since the power is transported a very short distance from the coil to the plug.\n\nFour-stroke 2-cylinder engines can be built without a distributor, as in the Citroen 2CV of 1948 and BMW boxer twin motorcycles, and some Honda motorcycles from the 1960s (e.g. the CL160 Scrambler). Both spark plugs of the boxer twin are fired simultaneously, resulting in a wasted spark on the cylinder currently on its exhaust stroke.\n\nFour-stroke 4-cylinder engines can be built without a distributor, as in the Citroen ID19. Two coils are used with one coil firing two of the spark plugs simultaneously, resulting in a wasted spark on the cylinder currently on its exhaust stroke, and the other coil used for the other two cylinders. This system has been scaled up to engines with virtually an unlimited number of cylinders.\n\nFour-stroke one-cylinder engines can be built without a distributor, as in many lawn mowers. The spark plug is fired on every stroke, resulting in a wasted spark in the cylinder when on its exhaust stroke.\n\n"}
{"id": "2251965", "url": "https://en.wikipedia.org/wiki?curid=2251965", "title": "Earthing system", "text": "Earthing system\n\nIn an electrical installation, an earthing system or grounding system connects specific parts of that installation with the Earth's conductive surface for safety and functional purposes. The point of reference is the Earth's conductive surface. The choice of earthing system can affect the safety and electromagnetic compatibility of the installation. Regulations for earthing systems vary considerably among countries, though many follow the recommendations of the International Electrotechnical Commission. Regulations may identify special cases for earthing in mines, in patient care areas, or in hazardous areas of industrial plants. \n\nIn addition to electric power systems, other systems may require grounding for safety or function. Tall structures may have lightning rods as part of a system to protect them from lightning strikes. Telegraph lines may use the Earth as one conductor of a circuit, saving the cost of installation of a return wire over a long circuit. Radio antennas may require particular grounding for operation, as well as to control static electricity and provide lightning protection.\n\nAn earth ground connection of the exposed conductive parts of electrical equipment helps protect from electric shock by keeping the exposed conductive surface of connected devices close to earth potential, when a failure of electrical insulation occurs. When a fault occurs, current flows from the power system to earth. The current may be high enough to operate the over current protection fuse or circuit breaker, which will then interrupt the circuit. To ensure the voltage on exposed surfaces is not too high, the impedance (resistance) of the connection to earth must be kept low relative to the normal circuit impedance. \n\nAn alternative to protective earthing of exposed surfaces is a design with \"double insulation\" or other precautions, such that a single failure or highly probable combination of failures cannot result in contact between live circuits and the surface. For example, a hand-held power tool might have an extra system of electrical insulation between internal components and the case of the tool, so that even if the insulation for the motor or switch fails, the tool case is not energized.\n\nA \"functional earth\" connection serves a purpose other than electrical safety, and may carry current as part of normal operation. For example, in a single-wire earth return power distribution system, the earth forms one conductor of the circuit and carries all the load current. Other examples of devices that use functional earth connections include surge suppressors and electromagnetic interference filters.\n\nIn low-voltage networks, which distribute the electric power to the widest class of end users, the main concern for design of earthing systems is safety of consumers who use the electric appliances and their protection against electric shocks. The earthing system, in combination with protective devices such as fuses and residual current devices, must ultimately ensure that a person must not come into touch with a metallic object whose potential relative to the person's potential exceeds a \"safe\" threshold, typically set at about 50 V.\n\nOn electricity networks with a system voltage of 240 V to 1.1 kV, which are mostly used in industrial / mining equipment / machines rather than publicly accessible networks, the earthing system design is as equally important from safety point of view as for domestic users.\nIn most developed countries, 220 V, 230 V, or 240 V sockets with earthed contacts were introduced either just before or soon after World War II, though with considerable national variation in popularity. In the United States and Canada, 120 V power outlets installed before the mid-1960s generally did not include a ground (earth) pin. In the developing world, local wiring practice may not provide a connection to an earthing pin of an outlet. \nFor a time, US National Electrical Code allowed certain major appliances permanently connected to the supply to use the supply neutral wire as the equipment enclosure connection to ground. This was not permitted for plug-in equipment as the neutral and energized conductor could easily be accidentally exchanged, creating a severe hazard. If the neutral was interrupted, the equipment enclosure would no longer be connected to ground. Normal imbalances in a split phase distribution system could create objectionable neutral to ground voltages. Recent editions of the NEC no longer permit this practice. For these reasons, most countries have now mandated dedicated protective earth connections that are now almost universal.\n\nIf the fault path between accidentally energized objects and the supply connection has low impedance, the fault current will be so large that the circuit overcurrent protection device (fuse or circuit breaker) will open to clear the ground fault. Where the earthing system does not provide a low-impedance metallic conductor between equipment enclosures and supply return (such as in a TT separately earthed system), fault currents are smaller, and will not necessarily operate the overcurrent protection device. In such case a residual current detector is installed to detect the current leaking to ground and interrupt the circuit.\n\nInternational standard IEC 60364 distinguishes three families of earthing arrangements, using the two-letter codes TN, TT, and IT.\n\nThe first letter indicates the connection between earth and the power-supply equipment (generator or transformer):\n\nThe second letter indicates the connection between earth or network and the electrical device being supplied:\n\nIn a TN earthing system, one of the points in the generator or transformer is connected with earth, usually the star point in a three-phase system. The body of the electrical device is connected with earth via this earth connection at the transformer.\nThis arrangement is a current standard for residential and industrial electric systems particularly in Europe.\n\nThe conductor that connects the exposed metallic parts of the consumer's electrical installation is called\n\"protective earth\" (\"PE\"; see also: Ground). The conductor that connects to the star point in a three-phase system, or that carries the return current in a single-phase system, is called \"neutral\" (\"N\"). Three variants of TN systems are distinguished:\n\nIt is possible to have both TN-S and TN-C-S supplies taken from the same transformer. For example, the sheaths on some underground cables corrode and stop providing good earth connections, and so homes where high resistance \"bad earths\" are found may be converted to TN-C-S. This is only possible on a network when the neutral is suitably robust against failure, and conversion is not always possible. The PEN must be suitable reinforced against failure, as an open circuit PEN can impress full phase voltage on any exposed metal connected to the system earth downstream of the break. The alternative is to provide a local earth and convert to TT.\nThe main attraction of a TN network is the low impedance earth path allows easy automatic disconnection (ADS) on a high current circuit in the case of a line-to-PE short circuit as the same breaker or fuse will operate for either L-N or L-PE faults, and an RCD is not needed to detect earth faults.\n\nIn a TT (Terra-Terra) earthing system, the protective earth connection for the consumer is provided by a local earth electrode, (sometimes referred to as the Terra-Firma connection) and there is another independently installed at the generator. There is no 'earth wire' between the two.\nThe fault loop impedance is higher, and unless the electrode impedance is very low indeed, a TT installation should always have an RCD (GFCI) as its first isolator.\n\nThe big advantage of the TT earthing system is the reduced conducted interference from other users' connected equipment. TT has always been preferable for special applications like telecommunication sites that benefit from the interference-free earthing. Also, TT networks do not pose any serious risks in the case of a broken neutral. In addition, in locations where power is distributed overhead, earth conductors are not at risk of becoming live should any overhead distribution conductor be fractured by, say, a fallen tree or branch.\n\nIn pre-RCD era, the TT earthing system was unattractive for general use because of the difficulty of arranging reliable automatic disconnection (ADS) in the case of a line-to-PE short circuit (in comparison with TN systems, where the same breaker or fuse will operate for either L-N or L-PE faults). But as residual current devices mitigate this disadvantage, the TT earthing system has become much more attractive providing that all AC power circuits are RCD-protected. In some countries (such as the UK) is recommended for situations where a low impedance equipotential zone is impractical to maintain by bonding, where there is significant outdoor wiring, such as supplies to mobile homes and some agricultural settings, or where a high fault current could pose other dangers, such as at fuel depots or marinas.\n\nThe TT earthing system is used throughout Japan, with RCD units in most industrial settings. This can impose added requirements on variable frequency drives and switched-mode power supplies which often have substantial filters passing high frequency noise to the ground conductor.\n\nIn an IT network, the electrical distribution system has no connection to earth at all, or it has only a high impedance connection.\n\nWhile the national wiring regulations for buildings of many countries follow the IEC 60364 terminology, in North America (United States and Canada), the term \"equipment grounding conductor\" refers to equipment grounds and ground wires on branch circuits, and \"grounding electrode conductor\" is used for conductors bonding an earth ground rod (or similar) to a service panel. \"Grounded conductor\" is the system \"neutral\".\nAustralian and New Zealand standards use a modified PME earthing system called Multiple Earthed Neutral (MEN). The neutral is grounded (earthed) at each consumer service point thereby effectively bringing the neutral potential difference to zero along the whole length of LV lines.\nIn the UK and some Commonwealth countries, the term \"PNE\", meaning Phase-Neutral-Earth is used to indicate that three (or more for non-single-phase connections) conductors are used, i.e., PN-S.\n\nA resistance earth system is used for mining in India as per Central Electricity Authority Regulations. Instead of a solid connection of neutral to earth, a neutral grounding resistor (NGR) is used to limit the current to ground to less than a750 mA. Due to the fault current restriction it is more safe for gassy mines.\n\nThe neutral earthing resistor is monitored to detect an interrupted ground connection and to shut off power if a fault is detected. .\n\nTo avoid accidental shock, earth leakage relay/sensor are used at the source to isolate the power when leakage exceed certain limit. Earth leakage circuit breakers are used for the purpose. Current sensing breaker are called RCB/ RCCB. In industrial applications, earth leakage relays are used with separate core balanced current transformers. This protection works in the range of milli-Amps and can be set from 30 mA to 3000 mA.\n\nA separate pilot wire is run from distribution/ equipment supply system in addition to earth wire, to supervise the continuity of the wire. This is used in the trailing cables of mining machinery. If the earth wire is broken, the pilot wire allows a sensing device at the source end to interrupt power to the machine. This type of circuit is a must for portable heavy electric equipment (like LHD (Load, Haul, Dump machine)) being used in under ground mines.\n\n\n\n\n\n\nIn high-voltage networks (above 1 kV), which are far less accessible to the general public, the focus of earthing system design is less on safety and more on reliability of supply, reliability of protection, and impact on the equipment in presence of a short circuit. Only the magnitude of phase-to-ground short circuits, which are the most common, is significantly affected with the choice of earthing system, as the current path is mostly closed through the earth. Three-phase HV/MV power transformers, located in distribution substations, are the most common source of supply for distribution networks, and type of grounding of their neutral determines the earthing system.\n\nThere are five types of neutral earthing:\n\nIn \"solid\" or \"directly\" earthed neutral, transformer's star point is directly connected to the ground. In this solution, a low-impedance path is provided for the ground fault current to close and, as result, their magnitudes are comparable with three-phase fault currents. Since the neutral remains at the potential close to the ground, voltages in unaffected phases remain at levels similar to the pre-fault ones; for that reason, this system is regularly used in high-voltage transmission networks, where insulation costs are high.\n\nTo limit short circuit earth fault an additional neutral earthing resistor (NER) is added between the neutral of transformer's star point and earth.\n\nWith low resistance fault current limit is relatively high. In India it is restricted for 50 A for open cast mines as per Central Electricity Authority Regulations, CEAR, 2010, rule 100.\n\nHigh resistance grounding system grounds the neutral through a resistance which limits the ground fault current to a value equal to or slightly greater than the capacitive charging current of that system\n\nIn \"unearthed\", \"isolated\" or \"floating neutral\" system, as in the IT system, there is no direct connection of the star point (or any other point in the network) and the ground. As a result, ground fault currents have no path to be closed and thus have negligible magnitudes. However, in practice, the fault current will not be equal to zero: conductors in the circuit — particularly underground cables — have an inherent capacitance towards the earth, which provides a path of relatively high impedance.\n\nSystems with isolated neutral may continue operation and provide uninterrupted supply even in presence of a ground fault. However, while the fault is present, the potential of other two phases relative to the ground reaches formula_1 of the normal operating voltage, creating additional stress for the insulation; insulation failures may inflict additional ground faults in the system, now with much higher currents.\n\nPresence of uninterrupted ground fault may pose a significant safety risk: if the current exceeds 4 A – 5 A an electric arc develops, which may be sustained even after the fault is cleared. For that reason, they are chiefly limited to underground and submarine networks, and industrial applications, where the reliability need is high and probability of human contact relatively low. In urban distribution networks with multiple underground feeders, the capacitive current may reach several tens of amperes, posing significant risk for the equipment.\n\nThe benefit of low fault current and continued system operation thereafter is offset by inherent drawback that the fault location is hard to detect.\n\n\n"}
{"id": "3206764", "url": "https://en.wikipedia.org/wiki?curid=3206764", "title": "Electrolysis of water", "text": "Electrolysis of water\n\nElectrolysis of water is the decomposition of water into oxygen and hydrogen gas due to the passage of an electric current. The reaction has a standard potential of −1.23 V, meaning it ideally requires a potential difference of 1.23 volts to split water.\n\nThis technique can be used to make (hydrogen gas) and breathable oxygen. As hydrogen is an important industrial commodity, by far most industrial methods produce hydrogen from natural gas instead, in the Steam reforming process. \n\nJan Rudolph Deiman and Adriaan Paets van Troostwijk used, in 1789, an electrostatic machine to make electricity which was discharged on gold electrodes in a Leyden jar with water. In 1800 Alessandro Volta invented the voltaic pile, and a few weeks later William Nicholson and Anthony Carlisle used it for the electrolysis of water. When Zénobe Gramme invented the Gramme machine in 1869 electrolysis of water became a cheap method for the production of hydrogen. A method of industrial synthesis of hydrogen and oxygen through electrolysis was developed by Dmitry Lachinov in 1888.\n\nA DC electrical power source is connected to two electrodes, or two plates (typically made from some inert metal such as platinum, stainless steel or iridium) which are placed in the water. Hydrogen will appear at the cathode (where electrons enter the water), and oxygen will appear at the anode. Assuming ideal faradaic efficiency, the amount of hydrogen generated is twice the amount of oxygen, and both are proportional to the total electrical charge conducted by the solution. However, in many cells competing side reactions occur, resulting in different products and less than ideal faradaic efficiency.\n\nElectrolysis of \"pure\" water requires excess energy in the form of overpotential to overcome various activation barriers. Without the excess energy the electrolysis of \"pure\" water occurs very slowly or not at all. This is in part due to the limited self-ionization of water. Pure water has an electrical conductivity about one millionth that of seawater. Many electrolytic cells may also lack the requisite electrocatalysts. The efficiency of electrolysis is increased through the addition of an electrolyte (such as a salt, an acid or a base) and the use of electrocatalysts.\n\nCurrently the electrolytic process is rarely used in industrial applications since hydrogen can currently be produced more affordably from fossil fuels.\n\nIn pure water at the negatively charged cathode, a reduction reaction takes place, with electrons (e) from the cathode being given to hydrogen cations to form hydrogen gas. The half reaction, balanced with acid, is:\n\nAt the positively charged anode, an oxidation reaction occurs, generating oxygen gas and giving electrons to the anode to complete the circuit:\n\nThe same half reactions can also be balanced with base as listed below. Not all half reactions must be balanced with acid or base. Many do, like the oxidation or reduction of water listed here. To add half reactions they must both be balanced with either acid or base. The acid-balanced reactions predominate in acidic (low pH) solutions, while the base-balanced reactions predominate in basic (high pH) solutions.\nCombining either half reaction pair yields the same overall decomposition of water into oxygen and hydrogen:\n\nThe number of hydrogen molecules produced is thus twice the number of oxygen molecules. Assuming equal temperature and pressure for both gases, the produced hydrogen gas has therefore twice the volume of the produced oxygen gas. The number of electrons pushed through the water is twice the number of generated hydrogen molecules and four times the number of generated oxygen molecules.\n\nDecomposition of pure water into hydrogen and oxygen at standard temperature and pressure is not favorable in thermodynamic terms.\n\nThus, the standard potential of the water electrolysis cell (E = E − E) is -1.23 V at 25 °C at pH 0 ([H] = 1.0 M). At 25 °C with pH 7 ([H] = 1.0 M), the potential is unchanged based on the Nernst equation. The thermodynamic standard cell potential can be obtained from standard-state free energy calculations to find ΔG° and then using the equation: ΔG°= -nFE°(where E° is the cell potential). In practice when an electrochemical cell is \"driven\" toward completion by applying reasonable potential, it is kinetically controlled. Therefore, activation energy, ion mobility (diffusion) and concentration, wire resistance, surface hindrance including bubble formation (causes electrode area blockage), and entropy, require a greater applied potential to overcome these factors. The amount of increase in potential required is termed the overpotential.\n\nIf the above described processes occur in pure water, H cations will be consumed/reduced at the cathode and OH anions will consumed/oxidised at the anode. This can be verified by adding a pH indicator to the water: the water near the cathode is basic while the water near the anode is acidic. The negative hydroxide ions that approach the anode mostly combine with the positive hydronium ions (HO) to form water. The positive hydronium ions that approach the cathode mostly combine with negative hydroxide ions to form water. Relatively few hydronium/hydroxide ions reach the cathode/anode. This can cause a concentration overpotential at both electrodes.\n\nPure water is a fairly good insulator since it has a low autoionization, K = 1.0×10 at room temperature and thus pure water conducts current poorly, 0.055 µS·cm. Unless a very large potential is applied to cause an increase in the autoionization of water the electrolysis of pure water proceeds very slowly limited by the overall conductivity.\n\nIf a water-soluble electrolyte is added, the conductivity of the water rises considerably. The electrolyte disassociates into cations and anions; the anions rush towards the anode and neutralize the buildup of positively charged H there; similarly, the cations rush towards the cathode and neutralize the buildup of negatively charged OH there. This allows the continuous flow of electricity.\n\nCare must be taken in choosing an electrolyte, since an anion from the electrolyte is in competition with the hydroxide ions to give up an electron. An electrolyte anion with less standard electrode potential than hydroxide will be oxidized instead of the hydroxide, and no oxygen gas will be produced. A cation with a greater standard electrode potential than a hydrogen ion will be reduced instead, and no hydrogen gas will be produced.\n\nThe following cations have lower electrode potential than H and are therefore suitable for use as electrolyte cations: Li, Rb, K, Cs, Ba, Sr, Ca, Na, and Mg. Sodium and lithium are frequently used, as they form inexpensive, soluble salts.\n\nIf an acid is used as the electrolyte, the cation is H, and there is no competitor for the H created by disassociating water. The most commonly used anion is sulfate (), as it is very difficult to oxidize, with the standard potential for oxidation of this ion to the peroxydisulfate ion being +2.05 volts.\n\nStrong acids such as sulfuric acid (HSO), and strong bases such as potassium hydroxide (KOH), and sodium hydroxide (NaOH) are frequently used as electrolytes due to their strong conducting abilities.\n\nA solid polymer electrolyte can also be used such as Nafion and when applied with a special catalyst on each side of the membrane can efficiently split the water molecule with as little as 1.5 Volts. There are also a number of other solid electrolyte systems that have been trialed and developed with a number of electrolysis systems now available commercially that use solid electrolytes.\n\nElectrolyte-free pure water electrolysis has been achieved by using deep-sub-Debye-length nanogap electrochemical cells. When the gap distance between cathode and anode even smaller than Debye-length (1 micron in pure water, around 220 nm in distilled water), the double layer regions from two electrodes can overlap with each other, leading to uniformly high electric field distributed inside the entire gap. Such high electric field can significantly enhance the ion transport inside water (mainly due to migration), further enhancing self-ionization of water and keeping the whole reaction continuing, and showing small resistance between the two electrodes. In this case, the two half-reactions are coupled together and limited by electron-transfer steps (electrolysis current saturated when further reducing the electrode distance).\n\nTwo leads, running from the terminals of a battery, are placed in a cup of water with a quantity of electrolyte to establish conductivity in the solution. Using NaCl (table salt) in an electrolyte solution results in chlorine gas rather than oxygen due to a competing half-reaction. With the correct electrodes and correct electrolyte, such as baking soda (sodium bicarbonate), hydrogen and oxygen gases will stream from the oppositely charged electrodes. Oxygen will collect at the positively charged electrode (anode) and hydrogen will collect at the negatively charged electrode (cathode). Note that hydrogen is positively charged in the HO molecule, so it ends up at the negative electrode. (And vice versa for oxygen.)\n\nNote that an aqueous solution of water with chloride ions, when electrolysed, will result in either OH if the concentration of Cl is low, or in chlorine gas being preferentially discharged if the concentration of Cl is greater than 25% by mass in the solution.\n\nThe Hofmann voltameter is often used as a small-scale electrolytic cell. It consists of three joined upright cylinders. The inner cylinder is open at the top to allow the addition of water and the electrolyte. A platinum electrode is placed at the bottom of each of the two side cylinders, connected to the positive and negative terminals of a source of electricity. When current is run through the Hofmann voltameter, gaseous oxygen forms at the anode (positive) and gaseous hydrogen at the cathode (negative). Each gas displaces water and collects at the top of the two outer tubes, where it can be drawn off with a stopcock.\n\nMany industrial electrolysis cells are very similar to Hofmann voltameters, with complex platinum plates or honeycombs as electrodes. Generally the only time hydrogen is intentionally produced from electrolysis is for specific point of use application such as is the case with oxyhydrogen torches or when extremely high purity hydrogen or oxygen is desired. The vast majority of hydrogen is produced from hydrocarbons and as a result contains trace amounts of carbon monoxide among other impurities. The carbon monoxide impurity can be detrimental to various systems including many fuel cells.\n\nHigh-pressure electrolysis is the electrolysis of water with a compressed hydrogen output around 12-20 MPa (120–200 Bar, 1740–2900 psi). By pressurising the hydrogen in the electrolyser, the need for an external hydrogen compressor is eliminated; the average energy consumption for internal compression is around 3%.\n\nHigh-temperature electrolysis (also HTE or steam electrolysis) is a method currently being investigated for water electrolysis with a heat engine. High temperature electrolysis may be preferable to traditional room-temperature electrolysis because some of the energy is supplied as heat, which is cheaper than electricity, and because the electrolysis reaction is more efficient at higher temperatures.\n\nIn 2014, researchers announced an electrolysis system made of inexpensive, abundant nickel and iron rather than precious metal catalysts, such as platinum or iridium. The nickel-metal/nickel-oxide structure is more active than pure nickel metal or pure nickel oxide alone. The catalyst significantly lowers the required voltage. Also nickel–iron batteries are being investigated for use as combined batteries and electrolysis for hydrogen production. Those \"battolysers\" could be charged and discharged like conventional batteries, and would produce hydrogen when fully charged.\n\nIn 2017, researchers reported using nanogap electrochemical cells to achieve high-efficiency electrolyte-free pure water electrolysis at room temperature. In nanogap electrochemical cells, the two electrodes are so close to each other (even smaller than Debye-length in pure water) that the mass transport rate can be even higher than the electron-transfer rate, leading to two half-reactions coupled together and limited by electron-transfer step. Experiments shows that the electrical current density from pure water electrolysis can be even larger than that from 1 mol/L sodium hydroxide solution. The mechanism, \"Virtual Breakdown Mechanism\", is completely different from the well-established traditional electrochemical theory, due to such nanogap size effect.\n\nAbout five percent of hydrogen gas produced worldwide is created by electrolysis. The majority of this hydrogen produced through electrolysis is a side product in the production of chlorine and caustic soda. This is a prime example of a competing side reaction.\n\nThe electrolysis of brine, a water/sodium chloride mixture, is only half the electrolysis of water since the chloride ions are oxidized to chlorine rather than water being oxidized to oxygen. Thermodynamically, this would not be expected since the oxidation potential of the chloride ion is less than that of water, but the rate of the chloride reaction is much greater than that of water, causing it to predominate. The hydrogen produced from this process is either burned (converting it back to water), used for the production of specialty chemicals, or various other small-scale applications.\n\nWater electrolysis is also used to generate oxygen for the International Space Station.\n\nHydrogen may later be used in a fuel cell as a storage method of energy and water.\n\nEfficiency of modern hydrogen generators is measured by \"energy consumed per standard volume of hydrogen\" (MJ/m), assuming standard temperature and pressure of the H. The lower the energy used by a generator, the higher would be its efficiency; a 100%-efficient electrolyser would consume of hydrogen, . Practical electrolysis (using a rotating electrolyser at 15 bar pressure) may consume , and a further if the hydrogen is compressed for use in hydrogen cars.\n\nElectrolyser vendors provide efficiencies based on enthalpy. To assess the claimed efficiency of an electrolyser it is important to establish how it was defined by the vendor (i.e. what enthalpy value, what current density, etc.).\n\nThere are two main technologies available on the market, \"alkaline\" and \"proton exchange membrane\" (PEM) electrolysers.\nAlkaline electrolysers are cheaper in terms of investment (they generally use nickel catalysts), but less efficient; PEM electrolysers, conversely, are more expensive (they generally use expensive platinum-group metal catalysts) but are more efficient and can operate at higher current densities, and can therefore be possibly cheaper if the hydrogen production is large enough.\n\nConventional alkaline electrolysis has an efficiency of about 70%. Accounting for the accepted use of the higher heat value (because inefficiency via heat can be redirected back into the system to create the steam required by the catalyst), average working efficiencies for PEM electrolysis are around 80%. This is expected to increase to between 82-86% before 2030. Theoretical efficiency for PEM electrolysers are predicted up to 94%.\nConsidering the industrial production of hydrogen, and using current best processes for water electrolysis (PEM or alkaline electrolysis) which have an effective electrical efficiency of 70-80%, producing 1 kg of hydrogen (which has a specific energy of 143 MJ/kg or about 40 kWh/kg) requires 50–55 kWh of electricity. At an electricity cost of $0.06/kWh, as set out in the Department of Energy hydrogen production \ntargets for 2015, the hydrogen cost is $3/kg. With the range of natural gas prices from 2016 as shown in the graph (Hydrogen Production Tech Team Roadmap, November 2017) putting the cost of SMR hydrogen at between $1.20 and $1.50, the cost price of hydrogen via electrolysis is still over double 2015 DOE hydrogen target prices. The US DOE target price for hydrogen in 2020 is $2.30/kg, requiring an electricity cost of $0.037/kWh, which is achievable given recent PPA tenders for wind and solar in many regions. This puts the $4/gge H2 dispensed objective well within reach, and close to a slightly elevated natural gas production cost for SMR.\n\nIn other parts of the world, steam methane reforming is between $1–3/kg on average. This makes production of hydrogen via electrolysis cost competitive in many regions already, as outlined by Nel Hydrogen and others, including an article by the IEA examining the conditions which could lead to a competitive advantage for electrolysis.\n\nReal water electrolysers require higher voltages for the reaction to proceed. The part that exceeds 1.23 V is called overpotential or overvoltage, and represents any kind of loss and nonideality in the electrochemical process.\n\nFor a well designed cell the largest overpotential is the reaction overpotential for the four-electron oxidation of water to oxygen at the anode; electrocatalysts can facilitate this reaction, and platinum alloys are the state of the art for this oxidation. Developing a cheap, effective electrocatalyst for this reaction would be a great advance, and is a topic of current research; there are many approaches, among them a 30-year-old recipe for molybdenum sulfide, graphene quantum dots, carbon nanotubes, perovskite, and nickel/nickel-oxide. The simpler two-electron reaction to produce hydrogen at the cathode can be electrocatalyzed with almost no overpotential by platinum, or in theory a hydrogenase enzyme. If other, less effective, materials are used for the cathode (e.g. graphite), large overpotentials will appear.\n\nThe electrolysis of water in standard conditions requires a theoretical minimum of 237 kJ of electrical energy input to dissociate each mole of water, which is the standard Gibbs free energy of formation of water. It also requires energy to overcome the change in entropy of the reaction. Therefore, the process cannot proceed below 286 kJ per mol if no external heat/energy is added.\n\nSince each mole of water requires two moles of electrons, and given that the Faraday constant \"F\" represents the charge of a mole of electrons (96485 C/mol), it follows that the minimum voltage necessary for electrolysis is about 1.23 V. If electrolysis is carried out at high temperature, this voltage reduces. This effectively allows the electrolyser to operate at more than 100% electrical efficiency. In electrochemical systems this means that heat must be supplied to the reactor to sustain the reaction. In this way thermal energy can be used for part of the electrolysis energy requirement. In a similar way the required voltage can be reduced (below 1 V) if fuels (such as carbon, alcohol, biomass) are reacted with water (PEM based electrolyzer in low temperature) or oxygen ions (solid oxide electrolyte based electrolyzer in high temperature). This results in some of the fuel's energy being used to \"assist\" the electrolysis process and can reduce the overall cost of hydrogen produced.\n\nHowever, observing the entropy component (and other losses), voltages over 1.48 V are required for the reaction to proceed at practical current densities (the \"thermoneutral\" voltage).\n\nIn the case of water electrolysis, Gibbs free energy represents the minimum \"work\" necessary for the reaction to proceed, and the reaction enthalpy is the amount of energy (both work and heat) that has to be provided so the reaction products are at the same temperature as the reactant (i.e. standard temperature for the values given above). Potentially, an electrolyser operating at 1.48 V would be 100% efficient.\n\n"}
{"id": "50783862", "url": "https://en.wikipedia.org/wiki?curid=50783862", "title": "Energy Conversion and Management", "text": "Energy Conversion and Management\n\nEnergy Conversion and Management is a peer-reviewed scientific journal covering research on energy generation, utilization, conversion, storage, transmission, conservation, management, and sustainability that was established in 1979. It is published by Elsevier Ltd. and the editor-in-chief is Moh'd Ahmad Al-Nimr (Jordan University of Science and Technology).\n\nThe journal is abstracted and indexed in Current Contents/Engineering, Computing & Technology, Science Citation Index Expanded, and Scopus. According to the \"Journal Citation Reports\", the journal has a 2016 impact factor of 5.589.\n"}
{"id": "9787405", "url": "https://en.wikipedia.org/wiki?curid=9787405", "title": "Epiphone Valve Junior", "text": "Epiphone Valve Junior\n\nThe Epiphone Valve Junior is a small 5 watt class A electric guitar amplifier.\n\n\nThe Epiphone Valve Jr. version 1 combo was released in 2005 as a part of Epiphone's line of amplifiers. In January 2006 a head version was released and also a version 2 combo. In June 2007 an updated head version and a version 3 combo were released. The first Epiphone Valve Juniors (version 1) were only combo amplifiers and received only a fair reception, due to such problems as buzzing and hum caused by the AC filament voltage. Epiphone then released a head version, correcting these issues and changed the AC filament voltage to DC voltage. Version 2 combos also featured these modifications. The version 1 and version 2 combos feature an 8\" 4 ohm Ceramic speaker.\n\nEpiphone released Combo version 3 and a newer version of the head with higher retail prices and feature a variety of tube brands based on availability including Electro-Harmonix, JJ, and Sovtek. The newer versions of the head and version 3 combos feature updated transformers with 4, 8 and 16 ohm outputs and the version 3 combos feature an 8\" 16 ohm Eminence Lady Luck speaker. In 2007, Epiphone released a speaker cabinet that can be used with the Epiphone Valve Junior, featuring a 1x 12\" 16 ohm Eminence Lady Luck speaker rated at 70watts RMS.\n\n\nAs of summer 2012, these amps appear to have been dropped from the line. The Amps page no longer exists on the Epiphone website.\n\nThe Epiphone Valve Junior has become very popular to modify because of its basic design. There are many websites, kits, and forums dedicated to this purpose, including the Epiphone official amps and accessories forum. Some sites offer kits to match the gain characteristics of Marshall, Fender and Vox amplifiers. These kits often do not include a bridge rectifier to convert the version 1 AC filament supply voltage to DC, instead they suggest twisting the AC leads together to cancel the generated electromagnetic fields, a technique used in twisted-pair Ethernet cable to eliminate cross-talk.\n\nA somewhat limited edition of the Valve Junior head was produced by Epiphone, dubbed the Valve Junior Hot Rod. It was released as a response to the extensive number of modifications that were being applied to the regular Valve Junior. The circuitry was altered to include a second 12AX7 preamp tube, a gain control, and a spring reverb in addition to the single volume control, as well as a standby switch to preserve tube life. Thanks to a dual function jack on the amp's rear panel, the Hot Rod can also be used as a standalone reverb unit to be used in conjunction with another amplifier's input or effects loop. In this function, the gain knob becomes the unit's dwell control, the volume knob retains its level function, and the reverb knob becomes the mixture control. When used as a reverb unit, the Hot Rod does not need to have a speaker connected to its output jacks because an internal \"dummy load\" activates automatically when the Reverb output jack is used.\n\nThe first 4 digits of the serial number are the date code MMYY (MM=month, YY=year). Version 2 Valve Juniors have date codes starting at \"0106\" January 2006 up to June 2007 when version 3 was released.\n\n\n"}
{"id": "261967", "url": "https://en.wikipedia.org/wiki?curid=261967", "title": "Fracture", "text": "Fracture\n\nA fracture is the separation of an object or material into two or more pieces under the action of stress. The fracture of a solid usually occurs due to the development of certain displacement discontinuity surfaces within the solid. If a displacement develops perpendicular to the surface of displacement, it is called a normal tensile crack or simply a crack; if a displacement develops tangentially to the surface of displacement, it is called a shear crack, slip band, or dislocation.\n\nBrittle fractures occur with no apparent deformation before fracture; ductile fractures occur when visible deformation does occur before separation. Fracture strength or breaking strength is the stress when a specimen fails or fractures. A detailed understanding of how fracture occurs in materials may be assisted by the study of fracture mechanics.\n\nFracture strength, also known as breaking strength, is the stress at which a specimen fails via fracture. This is usually determined for a given specimen by a tensile test, which charts the stress–strain curve (see image). The final recorded point is the fracture strength.\n\nDuctile materials have a fracture strength lower than the ultimate tensile strength (UTS), whereas in brittle materials the fracture strength is equivalent to the UTS. If a ductile material reaches its ultimate tensile strength in a load-controlled situation, it will continue to deform, with no additional load application, until it ruptures. However, if the loading is displacement-controlled, the deformation of the material may relieve the load, preventing rupture.\n\nThere are two types of fractures :\n\nIn \"brittle fracture\", no apparent plastic deformation takes place before fracture. Brittle fracture typically involves little energy absorption and occurs at high speeds—up to 2133.6 m/s (7000 ft/s) in steel. In most cases brittle fracture will continue even when loading is discontinued.\n\nIn brittle crystalline materials, fracture can occur by \"cleavage\" as the result of tensile stress acting normal to crystallographic planes with low bonding (cleavage planes). In amorphous solids, by contrast, the lack of a crystalline structure results in a conchoidal fracture, with cracks proceeding normal to the applied tension.\n\nThe theoretical strength of a crystalline material is (roughly)\nwhere: –\n\nOn the other hand, a crack introduces a stress concentration modeled by\nwhere: –\n\nPutting these two equations together, we get\n\nLooking closely, we can see that sharp cracks (small formula_8) and large defects (large formula_7) both lower the fracture strength of the material.\n\nRecently, scientists have discovered supersonic fracture, the phenomenon of crack propagation faster than the speed of sound in a material. This phenomenon was recently also verified by experiment of fracture in rubber-like materials.\n\nThe basic sequence in a typical brittle fracture is: introduction of a flaw either before or after the material is put in service, slow and stable crack propagation under recurring loading, and sudden rapid failure when the crack reaches critical crack length based on the conditions defined by fracture mechanics. Brittle fracture may be avoided by controlling three primary factors: material fracture toughness (K), nominal stress level (σ), and introduced flaw size (a). Residual stresses, temperature, loading rate, and stress concentrations also contribute to brittle fracture by influencing the three primary factors.\n\nUnder certain conditions, ductile materials can exhibit brittle behavior. Rapid loading, low temperature, and triaxial stress constraint conditions may cause ductile materials to fail without prior deformation.\n\nIn \"ductile fracture\", extensive plastic deformation (necking) takes place before fracture. The terms \"rupture\" or \"ductile rupture\" describe the ultimate failure of ductile materials loaded in tension. Rather than cracking, the material \"pulls apart,\" generally leaving a rough surface. In this case there is slow propagation and an absorption of a large amount of energy before fracture.\n\nBecause ductile rupture involves a high degree of plastic deformation, the fracture behavior of a propagating crack as modelled above changes fundamentally. Some of the energy from stress concentrations at the crack tips is dissipated by plastic deformation ahead of the crack as it propagates.\n\nThe basic steps in ductile fracture are void formation, void coalescence (also known as crack formation), crack propagation, and failure, often resulting in a cup-and-cone shaped failure surface.\n\nThere are three standard conventions for defining relative displacements in elastic materials in order to analyze crack propagation as proposed by Irwin. In addition fracture can involve uniform strain or a combination of these modes.\n\n\nThe manner in which a crack propagates through a material gives insight into the mode of fracture. With ductile fracture a crack moves slowly and is accompanied by a large amount of plastic deformation around the crack tip. A ductile crack will usually not propagate unless an increased stress is applied and generally cease propagating when loading is removed. In a ductile material, a crack may progress to a section of the material where stresses are slightly lower and stop due to the blunting effect of plastic deformations at the crack tip. On the other hand, with brittle fracture, cracks spread very rapidly with little or no plastic deformation. The cracks that propagate in a brittle material will continue to grow once initiated.\n\nCrack propagation is also categorized by the crack characteristics at the microscopic level. A crack that passes through the grains within the material is undergoing transgranular fracture. A crack that propagates along the grain boundaries is termed an intergranular fracture. Typically, the bonds between material grains are stronger at room temperature than the material itself, so transgranular fracture is more likely to occur. When temperatures increase enough to weaken the grain bonds, intergranular fracture is the more common fracture mode.\n\nFailures caused by brittle fracture have not been limited to any particular category of engineered structure. Though brittle fracture is less common than other types of failure, the impacts to life and property can be more severe. The following notable historic failures were attributed to brittle fracture:\n\n\n\n"}
{"id": "26379200", "url": "https://en.wikipedia.org/wiki?curid=26379200", "title": "George Willis Pack", "text": "George Willis Pack\n\nGeorge Willis Pack (born 1831, Peterboro, New York; d. August 31, 1906, Southampton, Long Island, New York) was a second-generation timberman on Michigan's Lower Peninsula. Building on his father's legacy, over the course of several decades, Pack successfully developed his timber businesses, becoming one of Michigan's first millionaires. His son, Charles Lathrop Pack; and grandson, Randolph Greene Pack, carried on the family tradition.\n\nHis father, George Pack, Jr. had established two sawmills outside of Lexington, Michigan, in a place known as Pack's Mills.\n\nAfter years of working with the elder Pack, in 1860 George Willis Pack, together with John L. Woods, established Carrington, Pack & Company, in Sand Beach, Michigan.\n\nIn 1864, with Woods and Jeremiah Jenks, George Willis Pack established a second sawmill, Pack, Jenks & Company, also near Sand Beach.\n\nA third firm, Woods & Company, in Port Crescent, Michigan, was formed in 1870.\n\nIn 1876, Pack, Woods & Company was formed in Oscoda, Michigan, on the Au Sable River.\n\nGeorge Willis Pack grew to be a wealthy man. He \"would be remembered as one of the few millionaires who had lived in Huron County.\n\n\n\n\n"}
{"id": "20964386", "url": "https://en.wikipedia.org/wiki?curid=20964386", "title": "Gloria (heating system)", "text": "Gloria (heating system)\n\nGloria (meaning \"glory\" in Spanish) was a central heating system used in Castile beginning in the Middle Ages. It was a direct descendant of the Roman hypocaust, and due to its slow rate of combustion, it allowed people to use smaller fuels such as hay instead of wood.\n\nThe Gloria consisted of a firebox, generally located outside (in a courtyard, for example), which burned hay, and one or more ducts that ran under the floors of the rooms to be heated. The warm exhaust gases from the combustion would pass through these ducts and then be released outside through a vertical flue.\n\nThe system is more efficient than a fireplace, because the rate of combustion (and therefore the heat output) can be regulated by restricting the airflow into the firebox. Moreover, the air required for combustion does not have to pass through the interior of the building, which reduces cold drafts. Finally, because the firebox is not open to the interior, there is no risk of filling the interior with smoke.\n\nDespite these benefits, it is not advisable to use this system today, as modern furnaces are far more efficient. The modern equivalent of the Gloria would be underfloor heating, which uses piped hot water under the floor to heat rooms, and like the Gloria, has the benefit of reducing the temperature gradients from floor to ceiling.\n\n"}
{"id": "25282014", "url": "https://en.wikipedia.org/wiki?curid=25282014", "title": "Greymouth Petroleum", "text": "Greymouth Petroleum\n\nGreymouth Petroleum is an energy company in New Zealand, established 2000. It owns gas and oil fields.\n\n"}
{"id": "45237559", "url": "https://en.wikipedia.org/wiki?curid=45237559", "title": "H is for Hawk", "text": "H is for Hawk\n\nH is for Hawk is a memoir by British author Helen Macdonald. It won the Samuel Johnson Prize and Costa Book of the Year award among other honours.\n\n\"H is for Hawk\" tells Macdonald's story of the year she spent training a northern goshawk in the wake of her father's death. Her father, Alisdair Macdonald, was a respected photojournalist who died suddenly of a heart attack in 2007. Having been a falconer for many years, she purchased a young goshawk to help her through the grieving process.\n\nThe book reached \"The Sunday Times\" best-seller list within two weeks of being published in July 2014.\n\nIn an interview with \"The Guardian\", Macdonald said, \"While the backbone of the book is a memoir about that year when I lost my father and trained a hawk, there are also other things tangled up in that story which are not memoir. There is the shadow biography of TH White, and a lot of nature-writing, too. I was trying to let these different genres speak to each other.\"\n\nJudges of the Samuel Johnson Prize specifically highlighted that marriage of genres as one of the reasons for selecting \"H is for Hawk\" as the winner.\n\nIn \"H is for Hawk: A New Chapter\", part of BBC's \"Natural World\" series in 2017, she trained a new goshawk chick.\n\n"}
{"id": "683942", "url": "https://en.wikipedia.org/wiki?curid=683942", "title": "Horsehead Nebula", "text": "Horsehead Nebula\n\nThe Horsehead Nebula (also known as Barnard 33) is a dark nebula in the constellation Orion. The nebula is located just to the south of Alnitak, the easternmost star of Orion's Belt, and is part of the much larger Orion Molecular Cloud Complex. The nebula was first recorded in 1888 by Scottish astronomer Williamina Fleming on a photographic plate taken at the Harvard College Observatory. The Horsehead Nebula is approximately 1500 light years from Earth. It is one of the most identifiable nebulae because its resemblance to a horse's head.\n\nThe dark cloud of dust and gas is a region in the Orion Molecular Cloud Complex where star formation is taking place. It is located in the constellation of Orion, which is prominent in the winter evening sky in the Northern Hemisphere and the summer evening sky in the Southern Hemisphere. This stellar nursery contains over 100 known kinds of organic and inorganic gases as well as dust; some of the latter is made up of large and complex organic molecules.\n\nThe red colour originates from hydrogen gas predominantly behind the nebula, ionized by the nearby bright star Sigma Orionis. Magnetic fields channel the gases leaving the nebula into streams, shown as streaks in the background glow. A glowing strip of hydrogen gas marks the edge of the massive cloud, and the densities of nearby stars are noticeably different on either side.\n\nThe heavy concentrations of dust in the Horsehead Nebula region and neighbouring Orion Nebula are localized, resulting in alternating sections of nearly complete opacity and transparency. The darkness of the Horsehead is caused mostly by thick dust blocking the light of stars behind it. The lower part of the Horsehead's neck casts a shadow to the left. The visible dark nebula emerging from the gaseous complex is an active site of the formation of \"low-mass\" stars. Bright spots in the Horsehead Nebula's base are young stars just in the process of forming.\n\n\n"}
{"id": "5593024", "url": "https://en.wikipedia.org/wiki?curid=5593024", "title": "Indiglo", "text": "Indiglo\n\nIndiglo is a product feature on watches marketed by Timex, incorporating an electroluminescent panel as a backlight for even illumination of the watch dial.\n\nThe brand is owned by Indiglo Corporation, which is in turn solely owned by Timex, and the name derives from the word indigo, as the original watches featuring the technology emitted a green-blue light. The Indiglo name was originally developed by Austin Innovations Inc.\n\nTimex introduced the Indiglo technology in 1992 in their \"Ironman\" watch line and subsequently expanded its use to 70% of their watch line, including men's and women's watches, sport watches and chronographs. Casio introduced their version of electroluminescent backlight technology, Illuminator, in 1995.\n\nIndiglo backlights typically emit a distinct greenish-blue color and evenly light the entire display or dial. Certain Indiglo models, e.g., Timex Datalink USB, use a negative liquid-crystal display so that only the digits are illuminated, rather than the entire display.\n\nHunter Fann company has also incorporated Indiglo technology in lighting the displays on many of its thermostats.\n\nFrom 2006-2011, the Timex Group marketed a line of high-end quartz watches under the TX Watch Company brand, using a proprietary six-hand, four-motor, micro-processor controlled movement. To separate the brand from Timex, the movements had luxury features associated with a higher-end brand, e.g., sapphire crystals and stainless steel or titanium casework — and used hands treated with super-luminova luminescent pigment for low-light legibility — rather than indiglo technology. \n\nWhen the Timex Group migrated the microprocessor-controlled, multi-motor, multi-hand technology to its Timex brand in 2012, it created a sub-collection marketed as Intelligent Quartz (IQ). The line employed the same movements and capabilities from the TX brand, at a much lower price-point -- incorporating indiglo technology rather than the super-luminova pigments.\n\n"}
{"id": "35583049", "url": "https://en.wikipedia.org/wiki?curid=35583049", "title": "Isothiouronium", "text": "Isothiouronium\n\nIn organic chemistry, isothiouronium is a functional group with the formula [RSC(NH)] (R = alkyl, aryl) and is the acid salt of isothiourea. The H centres can also be replaced by alkyl and aryl. Structurally, these cations resemble guanidinium cations. The CNS core is planar and the C-N bonds are short.\n\nSalts comprising these anions are typically prepared by alkylation of thiourea:\n\nHydrolysis of isothiouronium salts gives thiols.\n\nIsothiouronium salts in which the sulfur has been alkylated, such as \"S\"-methylisothiourea hemisulfate (CAS No: 867-44-7), will convert amines into guanidinium groups. This approach is sometimes called the Rathke synthesis after Bernhard Rathke who first reported it in 1881.\n\nChelating resins with isothiouronium groups are used to recover mercury and noble metals including platinum from solutions.\n"}
{"id": "12011713", "url": "https://en.wikipedia.org/wiki?curid=12011713", "title": "Japan Atomic Power Company", "text": "Japan Atomic Power Company\n\nThe is a company initially formed to jump start the commercial use of nuclear power in Japan, and currently operates two different sites. According to the official web site, JAPC is \"the only power company in Japan solely engaged in nuclear energy\".\n\nJAPC owns both units at the Tōkai Nuclear Power Plant and the Tsuruga Nuclear Power Plant with plans to expand at Tsuruga.\n\nThe company is jointly owned by Japan's major electric utilities: The Tokyo Electric Power Company (28.23%), Kansai Electric Power (18.54%), Chubu Electric Power (15.12%), Hokuriku Electric Power Company (13.05%), Tohoku Electric Power (6.12%), and Electric Power Development Company (J-Power) (5.37%).\n\nOn 11 March 2011 several nuclear reactors in Japan were badly damaged by the 2011 Tōhoku earthquake and tsunami.\nThe Tōkai Nuclear Power Plant lost external electric power, experienced the failure of one of its two cooling pumps, and two of its three emergency power generators. External electric power could only be restored two days after the earthquake.\n\nIn February 2013 in an attempt to raise money to be able to pay back loans due in April 2013, Japan Atomic Power did sell part of its uranium-stock. Streamlining and selling the uranium would be needed to pay back 40 billion yen. After April 2013 the major shareholders were expected to guarantee the payments for some 100 billion yen in loans. Japan Atomic Power refused to disclose the buyer.\n\nJapan's first nuclear activity in a previously non-nuclear country will be four 1000 MW reactors at Ninh Thuận 2 Nuclear Power Plant. The feasibility study to be carried out by Japan Atomic Power Company. Japan Atomic Power Company will also consult the project. The plant will be built by a consortium, International Nuclear Energy Development of Japan Co, which comprises 13 Japanese companies. The plant will be owned and operated by state-owned electricity company EVN.\n\nUnit 1 is expected to be commissioned in 2021, unit 2 in 2022, unit 3 in 2024 and unit 4 in 2025.\n\n \n"}
{"id": "55559057", "url": "https://en.wikipedia.org/wiki?curid=55559057", "title": "John C. Lincoln", "text": "John C. Lincoln\n\nJohn C. Lincoln (July 17, 1866 - May 24, 1959) was an American inventor, entrepreneur, philanthropist and in 1924, the Vice-Presidential candidate under the Commonwealth Land Party ticket. He held 55 patents on several electrical devices, founded the Lincoln Electric Co., invested in the construction of the Camelback Inn, presided over he Bagdad Mine and funded two hospitals in Phoenix, one which bears his name.\n\nLincoln (birth name: John Cromwell Lincoln) was born in Painesville, Ohio to William Edward Lincoln, (1831 - 1920), an abolitionist minister and Frances Louise Marshall Lincoln (1839 - 1918), a physician. There he received his primary and secondary education. In 1888, he graduated from Ohio State University with a degree in Electrical Engineering and soon after became a construction superintendent. He trained under Charles F. Brush, who invented the arc light and engineered America’s first electric street car. In 1891,Lincoln patented his first invention, an electric brake for street-railway cars.\n\nIn 1889, Henry George, visited Cleveland to speak about his ideals. George was an American political economist and journalist whose writing was immensely popular in the 19th century, and sparked several reform movements of the Progressive Era. His writings also inspired the economic philosophy known as Georgism, based on the belief that people should own the value they produce themselves, but that the economic value derived from land (including natural resources) should belong equally to all members of society. Lincoln was among those in the crowd who attended the political meeting. He was impressed with George and read George's book \"Progress and Poverty\" three times. His enthusiasm was such that he became a member of the Single Tax Georgism movement.\n\nIn 1895, he founded the Lincoln Electric Company in Cleveland, Ohio with a capital investment of $200.00. Lincoln used the facilities of his company to do research and to experiment with the electrical devices which he invented. The main product of the company was electric motors of his own design. He was directly responsible for 55 patents which gained him national fame as an industrialist. Lincoln helped finance the education of his brother James F. Lincoln who joined him upon his graduation in 1907, as the sales director of the company. The product line of the company expanded and included battery chargers for electric automobiles. In 1914, James F. Lincoln headed and supervised the company as general manager. This in turn gave Lincoln the time that he needed to continue with his research, experiments and inventions. The first variable voltage, single operator, portable welding machine in the world was introduced by the Lincoln Electric Co. in 1911. Lincoln created and patented a flux. The flux made a weld as flexible as steel. The shielded arc welding process developed by Lincoln played an important role in the manufacture of merchant ships in World War II.\nHe developed an employee bonus system in the company which made them the highest paid in the industry. This system was successful he would eventually apply it to his future ventures. In 1933, Lincoln published \"The Procedure Handbook of Arc Welding Design\".\n\nThe Single Tax Party, a political party, was renamed in 1924, and became the Commonwealth Land Party. The party's logo included an image of the earth with the phrase, “The Birthright of All Mankind”. New York City served as the Party headquarters with state branches in Ohio, Pennsylvania, and New Jersey.\n\nThe party platform was the following:\n\nLincoln became a member of the Commonwealth Land Party and in 1924, was nominated by the party for Vice-President with William J. Wallace, from New Jersey, running for President in the Presidential Elections of 1924. The Wallace and Lincoln team received 2,778 of the Popular vote and 0 Electoral votes in comparison with the Republican Party's candidates Calvin Coolidge and Charles G. Dawes who received 15,725,016 of the Popular votes and 382 Electoral votes thus, winning the Presidential election.\n\nLincoln's first wife was Myrtle H Humphrey Lincoln. On April 4, 1892, he had a daughter with Myrtle whom they named Louise Lincoln. Myrtle died in 1913. He later met and married Helen Colville. Helen was a college graduate and former educator who taught math and science. On June 15, 1922, she gave birth to a child, whom they named Joseph Colville Lincoln. Eventually, they had two more daughters and three sons.\n\nIn 1931, Lincoln's wife Helen was diagnosed with tuberculosis. He learned about the Desert Mission which was founded in 1927, by Marguerite Colley and Elizabeth Beatty in Sunnyslope, Arizona. This was a facility – a comprehensive, faith-based community center — that provided for the medical, social, and religious needs of the people living in the Sunnyslope Community. He was also told that the dry air and climate in Arizona was good for those who suffered from the disease. He then decided to move, with Helen and their three children, from Cleveland to Sunnyslope for the sake of his wife's health. Within two years, Helen was again healthy and the Lincoln's started to work with the Desert Mission, the haven for the sick and poor of Sunnyslope. He promoted Phoenix as a haven for health seekers.\n\nIn 1933, the Lincolns’ made an initial donation of $2,000 which helped buy 20 acres for the Mission’s expansion. This initial donation was the first of many contributions which he made to the organization which was renamed the John C. Lincoln Health Network in 1954. Helen fought and worked for the burgeoning network. Lincoln became the director of the Desert Mission of the Young Men's Christian Association and the Good Samaritan Hospital in Arizona.\n\nIn 1936, there were approximately 600 residents in Sunny Slope. There was still much vacant land, covered with vegetation and cacti. As the Sunnyslope neighborhood grew, the medical functions of the Desert Mission became a separate entity by the 1950s, later known as the John C. Lincoln Health Network, and now known as \"HonorHealth\" after a 2013 merger with Scottsdale Healthcare. The Desert Mission remains in operation as a subsidiary of this healthcare group.\n\nLincoln also joined in the construction of the Camelback Inn in Scottsdale, Arizona. Lincoln had made many investments in real estate and owned large tracts of land between the slopes of Mummy and Camelback Mountains. The property was a remote desert scrub land located outside Phoenix and had no water, electricity or telephone access.\n\nJack Stewart, a sportswriter and publicist, approached Lincoln with an idea. He wanted to build a pueblo-style hotel which could reflect Southwestern and Native American culture rather than the more commonplace dude ranch-style resort. Lincoln became convinced that the project had potential and invested $200,000 plus, the land that he owned between the Mummy and Camelback Mountains.\n\nThe Camelback Inn opened on December 15, 1936, under the management of Stewart, with the slogan \"Where Time Stands Still\". There were accommodations for 77 guests. Lincoln served as president of the Inn from 1936 to 1959. The resort became very popular among Hollywood celebrities and political leaders. Among the early celebrities who frequented the Inn were Mrs. Dwight D. Eisenhower, Clark Gable, Jimmy Stewart, Bette Davis, and J.W. Marriott, Sr., who shared the Stewart's love for the Camelback Inn.\n\nIn 1968, the Camelback Inn was purchased by Bill Marriott of Marriott International and in March 2003, the hotel was officially renamed by its parent corporation as \"Camelback Inn, a JW Marriott Resort & Spa\". It is located at 5402 East Lincoln Drive.\n\nHe also had a financial interest in the Universal Wire Spring Company. Among the many products he patented in the company's' name are:\n\nThe Lincoln family began to acquire Bagdad Mining stock in the latter part of World War II. By September 1944, Lincoln acquired 50% in the Bagdad Mine stock and had become the president of the Bagdad Copper Company. The Bagdad Mine represents one of the largest copper reserves in the United States and in the world, having estimated reserves of 873.6 million tonnes of ore grading 0.36% copper. It is located in Yavapai County, Arizona, just west of the unincorporated community of Bagdad. Lincoln established a laboratory in the mining area dedicated to the research of the oxide ores. He had a small pilot plant built to test the fluosolids roasting of the sulfide concentrates. Ernest Russell Dickie, who was familiar with the Vulture Mine, became Lincolns' associate. Dickie supervised its activities throughout the years of vast expansion and development.\n\nIn 1946, Lincoln established the Lincoln Foundation in Phoenix. He was inspired by Henry George's ideals which in turn served as an inspiration to create the Foundation. In 1966, the University of Hartford in Connecticut accepted the establishment of the foundation's John C. Lincoln Institute within its campus. Eventually, the Lincoln Institute of Land Policy established an independent school and as such became the primary grant recipient of the foundation. The Lincoln Institute of Land Policy school is located just outside of Harvard Square at 113 Brattle Street, Cambridge, Massachusetts.\n\nIn 1947, Lincoln became the third president of the Board of Trustees of the Henry George School of Social Science in New York upon the death of Anna George de Mille, daughter of Henry George. He served in this position until 1958. As stated in the beginning of this article Lincoln as a young man had read \"Progress and Poverty\" by Henry George and became a believer of George's ideas.\n\nLincoln made monetary contributions to the school as an individual. However, he received complaints from different factions about various upsets at the school. He then decided to use the Lincoln Foundation, which he created, in 1946 to channel his contributions. For a number of years, the Lincoln Foundation supported the school.\n\nIn July, 1957, on the occasion of the 25th anniversary banquet of the Henry George School the school's vice president, Ezra Cohen, paid tribute to Lincoln and saluted him:\n\nLincoln donated the money to begin construction of the John C. Lincoln Medical Center in Sunnyslope section of Phoenix and the John C. Lincoln Deer Valley Medical Center which now goes by the name of the Deer Valley Medical Center. Upon his death in 1959, Helen and his friends almost immediately began fundraising for an official hospital to commemorate his deeds. John C. Lincoln Hospital opened its doors in 1965.\n\nJohn C. Lincoln Medical Center is currently a 266-bed, full-service hospital. It's a leader in robotic and scarless surgery and has extensive cardiology and heart care services with a radiation-free, 3D heart mapping system. The hospital is an American College of Surgeons-verified Level I Trauma Center, a Primary Stroke Center, and an accredited Chest Pain Center. General surgery residents are trained there.\n\nOther services include a specialty surgery unit to care for orthopedic, urology, neurology and other surgery patients, inpatient and outpatient rehabilitation services, critical care unit, interventional radiology, and inpatient and outpatient medical imaging. The facility has earned Magnet designation, the highest national designation for excellence in nursing care, three times. The John C. Lincoln Medical Center is located at 250 E. Dunlap Ave. in the Sunnyslope section of Phoenix.\n\nThe John C Lincoln Deer Valley Hospital (now the Deer Valley Medical Center) is currently a 204-bed, full-service hospital offering extensive inpatient and outpatient general surgery and cardiac surgery and care. There is a Breast Health and Research Center on the campus which offers the latest technology, including 3D mammograms and an MRI on site. The facility has earned Magnet designation, the highest national designation for excellence in nursing care. The Deer Valley Medical Center is located at 19829 N. 27th Ave. in Phoenix.\n\nLincolns' descendants have all been members of the network’s board of directors of the John C. Lincoln Health Network.\n\nLincoln was experimenting with high speed crushing rolls when he died on May 24, 1959, at the age of 92. He is buried in Phoenix's Greenwood/Memory Lawn Mortuary & Cemetery. At the time of his death he was survived by Mrs. Lincoln, two daughters and three sons. Lincolns' 55th patent was awarded posthumously for a spring cushion that’s still used in cars today. Lincoln Drive in Phoenix is named after him. In 1998, Lincoln was posthumously inducted into the American Mining Hall of Fame.\n\nLincolns' second wife, Helen lived to be 102, she died November 12, 1994, beating her prognosis by six decades. She is buried alongside her husband in Greenwood/Memory Lawn Mortuary & Cemetery.\n\nThe American Welding Society established the AWS Foundation which awards the John C. Lincoln Memorial Scholarship. The purpose of the scholarship is to provide financial assistance to those individuals interested in pursuing a career in welding engineering. The John C. Lincoln Memorial Scholarship is awarded to an undergraduate student pursuing a minimum four-year bachelor's degree in welding engineering (WE) or welding engineering technology (WET); however, priority is given to welding engineering (WE) students.\n\nHis daughter Louise Lincoln Kerr, became an accomplished musician, composer, and philanthropist. She co-founded the National Society of Arts and Letters in 1944 and the Phoenix Symphony in 1947. She was also a benefactor to the School of Music at Arizona State University. She was inducted into the Arizona Women's Hall of Fame in 2004. Louise died on December 10, 1977. Her former home and studio in Scottsdale was recognized with a listing on the National Register of Historic Places on April 14, 2010.\n\nAmong Lincoln's written works are the following:\n\n"}
{"id": "20762724", "url": "https://en.wikipedia.org/wiki?curid=20762724", "title": "Journal of Electroanalytical Chemistry", "text": "Journal of Electroanalytical Chemistry\n\nThe Journal of Electroanalytical Chemistry is a peer-reviewed scientific journal on electroanalytical chemistry, published by Elsevier twice per month. It was originally established in 1959 under the current name, but was known as the Journal of Electroanalytical Chemistry and Interfacial Electrochemistry from 1967 to 1991. It is currently edited by J. M. Feliu (Universidad de Alicante). The journal is associated with the International Society of Electrochemistry. While the journal is now published exclusively in English, earlier volumes sometimes published articles in French and German.\n\nThe journal, which \"The New York Times\" described as \"a specialty publication not widely circulated\" in 1990, became more broadly known in 1989 when Martin Fleischmann and Stanley Pons published a description of their controversial cold fusion research in it, withdrawing their work from publication in \"Nature\" after questions were raised during peer review there.\n\nAccording to the \"Journal Citation Reports\", the journal has a 2015 impact factor of 3.012. It is abstracted and indexed in the following bibliographic databases\n"}
{"id": "57193334", "url": "https://en.wikipedia.org/wiki?curid=57193334", "title": "Kholombidzo Hydroelectric Power Station", "text": "Kholombidzo Hydroelectric Power Station\n\nKholombidzo Dam, also Kholombidzo Hydroelectric Power Sation is a planned hydroelectric dam in Malawi.\n\nThe power station would be located across the Shire River, in the village of \"Kholombidzo\", Blantyre District, in the Southern Region of Malawi. Kholombidzo, is the location of \"Kholombidzo Falls\" (formerly Murchison Falls), adjacent to the village of \"Chipironje\". This is about , by road, north-west of Blantyre, the financial capital and largest city of Malawi.\n\nIn 2013, the government of Malawi advertised for bids to carry out feasibility studies for a power station with capacity of 160–370 megawatts, using US$2 million granted to Malawi by the African Development Bank (AfDB).\n\nThe contract for the feasibility study and project design was awarded to COBA, a Portuguese engineering firm. It is anticipated that the plant will be build under the public-private-partnership (PPP) model, with the Electricity Generation Company Malawi Limited (Egenco), representing the government.\n\nThe power generated is expected to be evacuated via the nearby 400kV substation at \"Phombeya\", approximately , by road, to the north of Kholombidzo Falls.\n\nThe feasibility study put the cost of construction at US$435 million. The scope of work involves construction of (a) a dam (b) a powerhouse (c) a substation (d) service roads. It also includes the installation of (e) four turbines (f) transformers (g) generators and () the laying of transmission lines. Construction is expected to start in 2018 and conclude in 2021.\n\n\n"}
{"id": "3053898", "url": "https://en.wikipedia.org/wiki?curid=3053898", "title": "Korean Mountain Preservation League", "text": "Korean Mountain Preservation League\n\nThe Korean Mountain Preservation League, formerly the Korean Mountaineering League, is a non-profit non-governmental organization that focuses on the conservation of South Korea's mountain environments. It was founded in 2005 by Shawn James Morrissey, a mountaineer and author, who currently acts as the KMPL's president. The group is made up of Koreans and expatriates.\n\nThe KMPL's work includes: cleaning of littered trails and campsites; trail assessment; education of environmentally safe mountaineering practices; and campaigns against government legislation that will affect the mountain environments of Korea.\n\n"}
{"id": "53041357", "url": "https://en.wikipedia.org/wiki?curid=53041357", "title": "Lake Seyfe", "text": "Lake Seyfe\n\nLake Seyfa () is a lake in Kırşehir Province, central Turkey. It is a Ramsar site.\n\nThe lake is located in Mucur district northeast of Kırşehir.\n"}
{"id": "36834064", "url": "https://en.wikipedia.org/wiki?curid=36834064", "title": "Land grant to Ḫasardu kudurru", "text": "Land grant to Ḫasardu kudurru\n\nThe land grant to Ḫasardu kudurru, is a four-sided limestone \"narû\", or memorial stele, from the late 2nd millennium BC Mesopotamia recording the gift of 144 hectares of land on the bank of the Royal Canal in the Bīt-Pir’i-Amurru region of the Diyala valley by Kassite monarch Meli-Šipak (ca. 1186–1172 BC) to an official or \"sukkal mu’irri\", by the name of Ḫa-\"-du\" (reading uncertain). It is titled, “O Adad, the hero, bestow irrigation ditches of abundance here!\" and is notable for the light it sheds on middle Babylonian officialdom.\n\nThe object was excavated by Hormuzd Rassam during his 1881–82 excavations in Sippar on behalf of the British Museum. It was recovered, along with two other entitlement stelae, from a room in the temple of Šamaš and given the museum reference BM 90829.\n\nThirteen gods are invoked by name together with \"all the gods whose names are portrayed on this \"narû\".\" These are represented by eighteen icons arranged around the conical top.\n\nThe god Marduk is pictured twice, once by a \"kusarikku\" holding a spade, and once with a \"marru\" or tasseled spade in front of the \"kusarikku\". Ea may be represented both by the south wind and a ram-headed crook. Šuqamuna and Šumalia, the Kassite deities associated with the investiture of kings are portrayed by a bird on a perch. Several of the symbols are widely attested icons of their gods such as the lunar disc for Sîn, solar disc for Šamaš, the lightning-fork for Adad, the lamp for Nusku, the leaping dog for Gula, the mace with twin lion-heads for Nergal, the eagle-headed mace for Ninurta, the eight-pointed star for Ištar, and the coiled snake for Ištaran.\n\nThe principal parties to the transaction were the king and a Kassite military official:\n\n\nThe title \"sukkal mu'erru\" suggests his rȏle is as a representative or liaison officer at the royal court on behalf of the military commander, or \"mu'erru\".\n\nThe officials conducting the transfer:\n\n\nThe witnesses to the transaction:\n\n\nThe term \"ša rēši\" designated a royal eunuch in the Assyrian court but there is no evidence of a similar fate for a court official in middle Babylonia. Furthermore, Kidin-Marduk (not the witness on this kudurru), an official with this title is pictured bearded having inherited the position from his father, and later bequeathing it to his son, on a cylinder seal of the reign of Burna-Buriaš II. It is significant that both the military positions are occupied by Kassites.\n\n"}
{"id": "4643304", "url": "https://en.wikipedia.org/wiki?curid=4643304", "title": "Lightest Supersymmetric Particle", "text": "Lightest Supersymmetric Particle\n\nIn particle physics, the lightest supersymmetric particle (LSP) is the generic name given to the lightest of the additional hypothetical particles found in supersymmetric models. In models with R-parity conservation, the LSP is stable; in other words, the LSP cannot decay into any Standard Model particle, since all SM particles have the opposite R-parity. There is extensive observational evidence for an additional component of the matter density in the Universe that goes under the name dark matter. The LSP of supersymmetric models is a dark matter candidate and is a weakly interacting massive particle (WIMP).\n\nThe LSP is unlikely to be a charged wino, charged higgsino, slepton,\nsneutrino, gluino, squark, or gravitino but is most likely a mixture of neutral higgsinos, the bino and the neutral winos, i.e. a neutralino. In particular, if the LSP were charged (and is abundant in our galaxy) such particles would have been captured by the Earth's magnetic field and form heavy hydrogen-like atoms. Searches for anomalous hydrogen in natural water however have been without any evidence for such particles and thus put severe constraints on the existence of a charged LSP.\n\nDark matter particles must be electrically neutral; otherwise they would scatter light and thus not be \"dark\". They must also almost certainly be non-colored. \nWith these constraints, the LSP could be the lightest neutralino, the gravitino, or the lightest sneutrino.\n\n\nIn extra-dimensional theories, there are analogous particles called \"LKP\"s or \"Lightest Kaluza-Klein Particle\". These are the stable particles of extra-dimensional theories.\n\n"}
{"id": "34365874", "url": "https://en.wikipedia.org/wiki?curid=34365874", "title": "List of protected areas of Cambodia", "text": "List of protected areas of Cambodia\n\nThis is a list of protected areas of Cambodia.\n\n"}
{"id": "18069", "url": "https://en.wikipedia.org/wiki?curid=18069", "title": "Lubricant", "text": "Lubricant\n\nA lubricant is a substance, usually organic, introduced to reduce friction between surfaces in mutual contact, which ultimately reduces the heat generated when the surfaces move. It may also have the function of transmitting forces, transporting foreign particles, or heating or cooling the surfaces. The property of reducing friction is known as lubricity.\n\nIn addition to industrial applications, lubricants are used for many other purposes. Other uses include cooking (oils and fats in use in frying pans, in baking to prevent food sticking), bioapplications on humans (e.g. lubricants for artificial joints), ultrasound examination, medical examination.\n\nLubicants have been in some use for thousands of years. Calcium soaps have been identified on the axles of chariots dated to 1400 BC. Building stones were slid on oil-impregrated lumber in the time of the pyramids. In the Roman era, lubricants were based on olive oil and rapeseed oil, as well as animal fats. The growth of lubrication accelerated in the Industrial Revolution with the accompanying use of metal-based machinery. Relying initially on natural oils, needs for such machinery shifted toward petroleum-based materials early in the 1900s. A breakthrough came with the development of vacuum distillation of petroleum, as described by the Vacuum Oil Company. This technology allowed the purification of very nonvolatile substances, which are common in many lubricants.\n\nA good lubricant generally possesses the following characteristics:\n\nTypically lubricants contain 90% base oil (most often petroleum fractions, called mineral oils) and less than 10% additives. Vegetable oils or synthetic liquids such as hydrogenated polyolefins, esters, silicones, fluorocarbons and many others are sometimes used as base oils. Additives deliver reduced friction and wear, increased viscosity, improved viscosity index, resistance to corrosion and oxidation, aging or contamination, etc.\n\nNon-liquid lubricants include powders (dry graphite, PTFE, molybdenum disulphide, tungsten disulphide, etc.), PTFE tape used in plumbing, air cushion and others. Dry lubricants such as graphite, molybdenum disulphide and tungsten disulphide also offer lubrication at temperatures (up to 350 °C) higher than liquid and oil-based lubricants are able to operate. Limited interest has been shown in low friction properties of compacted oxide glaze layers formed at several hundred degrees Celsius in metallic sliding systems, however, practical use is still many years away due to their physically unstable nature.\n\nA large number of additives are used to impart performance characteristics to the lubricants. Modern automotive lubricants contain as many as ten additives, comprising up to 20% of the lubricant, the main families of additives are:\n\nIn 1999, an estimated 37,300,000 tons of lubricants were consumed worldwide. Automotive applications dominate, but other industrial, marine, and metal working applications are also big consumers of lubricants. Although air and other gas-based lubricants are known (e.g., in fluid bearings), liquid lubricants dominate the market, followed by solid lubricants.\n\nLubricants are generally composed of a majority of base oil plus a variety of additives to impart desirable characteristics. Although generally lubricants are based on one type of base oil, mixtures of the base oils also are used to meet performance requirements.\n\nThe term \"mineral oil\" is used to refer to lubricating base oils derived from crude oil. The American Petroleum Institute (API) designates several types of lubricant base oil:\n\n\nThe lubricant industry commonly extends this group terminology to include:\n\nCan also be classified into three categories depending on the prevailing compositions:\n\nPetroleum-derived lubricant can also be produced using synthetic hydrocarbons (derived ultimately from petroleum). These include:\n\nPTFE: polytetrafluoroethylene (PTFE) is typically used as a coating layer on, for example, cooking utensils to provide a non-stick surface. Its usable temperature range up to 350 °C and chemical inertness make it a useful additive in special greases. Under extreme pressures, PTFE powder or solids is of little value as it is soft and flows away from the area of contact. Ceramic or metal or alloy lubricants must be used then.\n\nInorganic solids: Graphite, hexagonal boron nitride, molybdenum disulfide and tungsten disulfide are examples of solid lubricants. Some retain their lubricity to very high temperatures. The use of some such materials is sometimes restricted by their poor resistance to oxidation (e.g., molybdenum disulfide degrades above 350 °C in air, but 1100 °C in reducing environments.\n\nMetal/alloy: Metal alloys, composites and pure metals can be used as grease additives or the sole constituents of sliding surfaces and bearings. Cadmium and gold are used for plating surfaces which gives them good corrosion resistance and sliding properties, Lead, tin, zinc alloys and various bronze alloys are used as sliding bearings, or their powder can be used to lubricate sliding surfaces alone.\n\nAqueous lubrication is of interest in a number of technological applications. Strongly hydrated brush polymers such as PEG can serve as lubricants at liquid solid interfaces. By continuous rapid exchange of bound water with other free water molecules, these polymer films keep the surfaces separated while maintaining a high fluidity at the brush–brush interface at high compressions, thus leading to a very low coefficient of friction.\n\nBiolubricants are derived from vegetable oils and other renewable sources. They usually are triglyceride esters (fats obtained from plants and animals. For lubricant base oil use, the vegetable derived materials are preferred. Common ones include high oleic canola oil, castor oil, palm oil, sunflower seed oil and rapeseed oil from vegetable, and tall oil from tree sources. Many vegetable oils are often hydrolyzed to yield the acids which are subsequently combined selectively to form specialist synthetic esters. Other naturally derived lubricants include lanolin (wool grease, a natural water repellent).\n\nWhale oil was a historically important lubricant, with some uses up to the latter part of the 20th century as a friction modifier additive for automatic transmission fluid.\n\nIn 2008, the biolubricant market was around 1% of UK lubricant sales in a total lubricant market of 840,000 tonnes/year.\n\nOne of the single largest applications for lubricants, in the form of motor oil, is protecting the internal combustion engines in motor vehicles and powered equipment.\n\nAnti-tack or anti-stick coatings are designed to reduce the adhesive condition (stickiness) of a given material. The rubber, hose, and wire and cable industries are the largest consumers of anti-tack products but virtually every industry uses some form of anti-sticking agent. Anti-sticking agents differ from \"lubricants\" in that they are designed to reduce the inherently adhesive qualities of a given compound while lubricants are designed to reduce friction between any two surfaces.\n\nLubricants are typically used to separate moving parts in a system. This separation has the benefit of reducing friction, wear and surface fatigue, together with reduced heat generation, operating noise and vibrations. Lubricants achieve this in several ways. The most common is by forming a physical barrier i.e., a thin layer of lubricant separates the moving parts. This is analogous to hydroplaning, the loss of friction observed when a car tire is separated from the road surface by moving through standing water. This is termed hydrodynamic lubrication. In cases of high surface pressures or temperatures, the fluid film is much thinner and some of the forces are transmitted between the surfaces through the lubricant..\n\nTypically the lubricant-to-surface friction is much less than surface-to-surface friction in a system without any lubrication. Thus use of a lubricant reduces the overall system friction. Reduced friction has the benefit of reducing heat generation and reduced formation of wear particles as well as improved efficiency. Lubricants may contain additives known as friction modifiers that chemically bind to metal surfaces to reduce surface friction even when there is insufficient bulk lubricant present for hydrodynamic lubrication, e.g. protecting the valve train in a car engine at startup.\n\nBoth gas and liquid lubricants can transfer heat. However, liquid lubricants are much more effective on account of their high specific heat capacity. Typically the liquid lubricant is constantly circulated to and from a cooler part of the system, although lubricants may be used to warm as well as to cool when a regulated temperature is required. This circulating flow also determines the amount of heat that is carried away in any given unit of time. High flow systems can carry away a lot of heat and have the additional benefit of reducing the thermal stress on the lubricant. Thus lower cost liquid lubricants may be used. The primary drawback is that high flows typically require larger sumps and bigger cooling units. A secondary drawback is that a high flow system that relies on the flow rate to protect the lubricant from thermal stress is susceptible to catastrophic failure during sudden system shut downs. An automotive oil-cooled turbocharger is a typical example. Turbochargers get red hot during operation and the oil that is cooling them only survives as its residence time in the system is very short (i.e. high flow rate). If the system is shut down suddenly (pulling into a service area after a high-speed drive and stopping the engine) the oil that is in the turbo charger immediately oxidizes and will clog the oil ways with deposits. Over time these deposits can completely block the oil ways, reducing the cooling with the result that the turbo charger experiences total failure, typically with seized bearings. Non-flowing lubricants such as greases and pastes are not effective at heat transfer although they do contribute by reducing the generation of heat in the first place.\n\nLubricant circulation systems have the benefit of carrying away internally generated debris and external contaminants that get introduced into the system to a filter where they can be removed. Lubricants for machines that regularly generate debris or contaminants such as automotive engines typically contain detergent and dispersant additives to assist in debris and contaminant transport to the filter and removal. Over time the filter will get clogged and require cleaning or replacement, hence the recommendation to change a car's oil filter at the same time as changing the oil. In closed systems such as gear boxes the filter may be supplemented by a magnet to attract any iron fines that get created.\n\nIt is apparent that in a circulatory system the oil will only be as clean as the filter can make it, thus it is unfortunate that there are no industry standards by which consumers can readily assess the filtering ability of various automotive filters. Poor automotive filters \nsignificantly reduces the life of the machine (engine) as well as making the system inefficient.\n\nLubricants known as hydraulic fluid are used as the working fluid in hydrostatic power transmission. Hydraulic fluids comprise a large portion of all lubricants produced in the world. The automatic transmission's torque converter is another important application for power transmission with lubricants.\n\nLubricants prevent wear by keeping the moving parts apart. Lubricants may also contain anti-wear or extreme pressure additives to boost their performance against wear and fatigue.\n\nMany lubricants are formulated with additives that form chemical bonds with surfaces or that exclude moisture, to prevent corrosion and rust. It reduces corrosion between two metallic surface and avoids contact between these surfaces to avoid immersed corrosion.\n\nLubricants will occupy the clearance between moving parts through the capillary force, thus sealing the clearance. This effect can be used to seal pistons and shafts.\n\nA further phenomenon that has undergone investigation in relation to high temperature wear prevention and lubrication, is that of a compacted oxide layer glaze formation. Such glazes are generated by sintering a compacted oxide layer. Such glazes are crystalline, in contrast to the amorphous glazes seen in pottery. The required high temperatures arise from metallic surfaces sliding against each other (or a metallic surface against a ceramic surface). Due to the elimination of metallic contact and adhesion by the generation of oxide, friction and wear is reduced. Effectively, such a surface is self-lubricating.\n\nAs the \"glaze\" is already an oxide, it can survive to very high temperatures in air or oxidising environments. However, it is disadvantaged by it being necessary for the base metal (or ceramic) having to undergo some wear first to generate sufficient oxide debris.\n\nIt is estimated that 40% of all lubricants are released into the environment. Common disposal methods include recycling, burning, landfill and discharge into water, though typically disposal in landfill and discharge into water are strictly regulated in most countries, as even small amount of lubricant can contaminate a large amount of water. Most regulations permit a threshold level of lubricant that may be present in waste streams and companies spend hundreds of millions of dollars annually in treating their waste waters to get to acceptable levels.\n\nBurning the lubricant as fuel, typically to generate electricity, is also governed by regulations mainly on account of the relatively high level of additives present. Burning generates both airborne pollutants and ash rich in toxic materials, mainly heavy metal compounds. Thus lubricant burning takes place in specialized facilities that have incorporated special scrubbers to remove airborne pollutants and have access to landfill sites with permits to handle the toxic ash.\n\nUnfortunately, most lubricant that ends up directly in the environment is due to general public discharging it onto the ground, into drains and directly into landfills as trash. Other direct contamination sources include runoff from roadways, accidental spillages, natural or man-made disasters and pipeline leakages.\n\nImprovement in filtration technologies and processes has now made recycling a viable option (with rising price of base stock and crude oil). Typically various filtration systems remove particulates, additives and oxidation products and recover the base oil. The oil may get refined during the process. This base oil is then treated much the same as virgin base oil however there is considerable reluctance to use recycled oils as they are generally considered inferior. Basestock fractionally vacuum distilled from used lubricants has superior properties to all natural oils, but cost effectiveness depends on many factors. Used lubricant may also be used as refinery feedstock to become part of crude oil. Again, there is considerable reluctance to this use as the additives, soot and wear metals will seriously poison/deactivate the critical catalysts in the process. Cost prohibits carrying out both filtration (soot, additives removal) and re-refining (distilling, isomerisation, hydrocrack, etc.) however the primary hindrance to recycling still remains the collection of fluids as refineries need continuous supply in amounts measured in cisterns, rail tanks.\n\nOccasionally, unused lubricant requires disposal. The best course of action in such situations is to return it to the manufacturer where it can be processed as a part of fresh batches.\n\n\"Environment:\"\nLubricants both fresh and used can cause considerable damage to the environment mainly due to their high potential of serious water pollution. Further the additives typically contained in lubricant can be toxic to flora and fauna. In used fluids the oxidation products can be toxic as well. Lubricant persistence in the environment largely depends upon the base fluid, however if very toxic additives are used they may negatively affect the persistence. Lanolin lubricants are non-toxic making them the environmental alternative which is safe for both users and the environment.\n\n\n\n\n\n"}
{"id": "10974917", "url": "https://en.wikipedia.org/wiki?curid=10974917", "title": "National Petroleum Staff Association", "text": "National Petroleum Staff Association\n\nThe National Petroleum Staff Association is a trade union in Trinidad and Tobago. It organises senior staff in the state owned National Petroleum company.\n\n"}
{"id": "303076", "url": "https://en.wikipedia.org/wiki?curid=303076", "title": "Neon sign", "text": "Neon sign\n\nIn the signage industry, neon signs are electric signs lighted by long luminous gas-discharge tubes that contain rarefied neon or other gases. They are the most common use for neon lighting, which was first demonstrated in a modern form in December 1910 by Georges Claude at the Paris Motor Show. While they are used worldwide, neon signs were popular in the United States from about 1920–1960. The installations in Times Square, many originally designed by Douglas Leigh, were famed, and there were nearly 2,000 small shops producing neon signs by 1940. In addition to signage, neon lighting is used frequently by artists and architects, and (in a modified form) in plasma display panels and televisions. The signage industry has declined in the past several decades, and cities are now concerned with preserving and restoring their antique neon signs.\n\nThe neon sign is an evolution of the earlier Geissler tube, which is a broken glass tube containing a \"rarefied\" gas (the gas pressure in the tube is well below atmospheric pressure). When a voltage is applied to electrodes inserted through the glass, an electrical glow discharge results. Geissler tubes were quite popular in the late 19th century, and the different colors they emitted were characteristics of the gases within. They were, however, unsuitable for general lighting; the pressure of the gas inside typically declined in use. The direct predecessor of neon tube lighting was the Moore tube, which used nitrogen or carbon dioxide as the luminous gas and a patented mechanism for maintaining pressure; Moore tubes were sold for commercial lighting for a number of years in the early 1900s. \n\nThe discovery of neon in 1898 by the British scientists William Ramsay and Morris W. Travers included the observation of a brilliant red glow in Geissler tubes. Travers wrote, \"the blaze of crimson light from the tube told its own story and was a sight to dwell upon and never forget.\" Following neon's discovery, neon tubes were used as scientific instruments and novelties. A sign created by Perley G. Nutting and displaying the word \"neon\" may have been shown at the Louisiana Purchase Exposition of 1904, although this claim has been disputed; in any event, the scarcity of neon would have precluded the development of a lighting product. However, after 1902, Georges Claude's company in France, Air Liquide, began producing industrial quantities of neon, essentially as a byproduct of their air liquefaction business. From December 3–18, 1910, Claude demonstrated two long bright red neon tubes at the Paris Motor Show. This demonstration lit a peristyle of the \"Grand Palais\" (a large exhibition hall). Claude's associate, Jacques Fonsèque, realized the possibilities for a business based on signage and advertising. By 1913 a large sign for the vermouth Cinzano illuminated the night sky in Paris, and by 1919 the entrance to the Paris Opera was adorned with neon tube lighting. Over the next several years, patents were granted to Claude for two innovations still used today: a \"bombardment\" technique to remove impurities from the working gas of a sealed sign, and a design for the internal electrodes of the sign that prevented their degradation by sputtering.\n\nIn 1923, Georges Claude and his French company Claude Neon introduced neon gas signs to the United States by selling two to a Packard car dealership in Los Angeles. Earle C. Anthony purchased the two signs reading \"Packard\" for $1,250 apiece. Neon lighting quickly became a popular fixture in outdoor advertising. Visible even in daylight, people would stop and stare at the first neon signs for hours, dubbed \"liquid fire.\"\n\nWhat may be the oldest surviving neon sign in the United States, still in use for its original purpose, is the sign “Theatre” (1929) at the Lake Worth Playhouse in Lake Worth, Florida.\n\nThe next major technological innovation in neon lighting and signs was the development of fluorescent tube coatings. Jacques Risler received a French patent in 1926 for these. Neon signs that use an argon/mercury gas mixture emit a good deal of ultraviolet light. When this light is absorbed by a fluorescent coating, preferably inside the tube, the coating (called a \"phosphor\") glows with its own color. While only a few colors were initially available to sign designers, after the Second World War, phosphor materials were researched intensively for use in color televisions. About two dozen colors were available to neon sign designers by the 1960s, and today there are nearly 100 available colors.\n\nNeon tube signs\nare produced by the craft of bending glass tubing into shapes. A worker skilled in this craft is known as a glass bender, neon bender or tube bender. The neon tube is made out of 4 or 5-foot long straight sticks of hollow glass sold by sign suppliers to neon shops worldwide, where they are manually assembled into individual custom designed and fabricated lamps.\n\nTubing in external diameters ranging from about 8–15 mm with a 1 mm wall thickness is most commonly used, although 6 mm tubing is now commercially available in colored glass tubes. The tube is heated in sections using several types of burners that are selected according to the amount of glass to be heated for each bend. These burners include ribbon, cannon, or crossfires, as well as a variety of gas torches. Ribbon burners are strips of fire that make the gradual bends, while crossfires are used to make sharp bends.\n\nThe interior of the tubes may be coated with a thin phosphorescent powder coating, affixed to the interior wall of the tube by a binding material. The tube is filled with a purified gas mixture, and the gas ionized by a high voltage applied between the ends of the sealed tube through cold cathodes welded onto the ends. The color of the light emitted by the tube may be just that coming from the gas, or the light from the phosphor layer. Different phosphor-coated tubing sections may be butt welded together using glass working torches to form a single tube of varying colors, for effects such as a sign where each letter displays a different color letter within a single word.\n\n\"Neon\" is used to denote the general type of lamp, but neon gas is only one of the types of tube gases principally used in commercial application. Pure neon gas is used to produce only about one-third of the colors (mostly shades of red and orange, and some warmer or more intense shades of pink). The greatest number of colors (including all shades of blue, yellow, green, violet, and white, as well as some cooler or softer shades of pink) produced by filling with another inert gas, argon, and a drop of mercury (Hg) which is added to the tube immediately after purification. When the tube is ionized by electrification, the mercury evaporates into mercury vapor, which fills the tube and produces strong ultraviolet light. The ultraviolet light thus produced excites the various phosphor coatings designed to produce different colors. Even though this class of neon tubes use no neon at all, they are still denoted as \"neon.\" Mercury-bearing lamps are a type of cold-cathode fluorescent lamps.\n\nEach type of neon tubing produces two different possible colors, one with neon gas and the other with argon/mercury. Some \"neon\" tubes are made without phosphor coatings for some of the colors. Clear tubing filled with neon gas produces the ubiquitous yellowish orange color with the interior plasma column clearly visible, and is the cheapest and simplest tube to make. Traditional neon glasses in America over 20 years old are lead glass that are easy to soften in gas fires, but recent environmental and health concerns of the workers has prompted manufacturers to seek more environmentally safe special soft glass formulas. One of the vexing problems avoided this way is lead glass' tendency to burn into a black spot emitting lead fumes in a bending flame too rich in the fuel/oxygen mixture. Another traditional line of glasses was colored soda lime glasses coming in a myriad of glass color choices, which produce the highest quality, most hypnotically vibrant and saturated hues. Still more color choices are afforded in either coating, or not coating, these colored glasses with the various available exotic phosphors.\n\nIt is the wide range of colors and the ability to make a tube that can last for years or decades without replacement, that makes this an art. Since these tubes require so much custom labor, they would have very little economic viability if they did not have such a long lifetime when well processed. The intensity of neon light produced increases as the tube diameter grows smaller, that is, the intensity varies inversely with the square root of the interior diameter of the tubing, and the resistance of the tube increases as the tubing diameter decreases accordingly, because tube ionization is greatest at the center of the tube, and the ions migrate to and are recaptured and neutralized at the tube walls. The greatest cause of neon tube failure is the gradual absorption of neon gas by high voltage ion implantation into the interior glass walls of the tubes which depletes the gas, and eventually causes the tube resistance to rise to a level that it can no longer light at the rated voltage, but this may take well over 50 years if the tube is properly processed during bombardment and gas back-filling.\n\nThis long lifetime has created a practical market for neon use for interior architectural cove lighting in a wide variety of uses including homes, where the tube can be bent to any shape, fitted in a small space, and can do so without requiring tube replacement for a decade or more.\n\nA section of the glass is heated until it is malleable; then it is bent into shape and aligned to a neon sign pattern paper containing the graphics or lettering to which the final product will conform. A tube bender corks off the hollow tube before heating and holds a latex rubber blow hose at the other end, through which he gently presses a small amount of air to keep the tube diameter constant as it is bending. The trick of bending is to bend one small section or bend at a time, heating one part of the tubing so that it is soft, without heating some other part of the tube as well, which would make the bend uncontrollable. A bend, once the glass is heated, must be brought to the pattern and fitted rapidly before the glass hardens again, because it is difficult to reheat once completely cooled without risking breakage. It is frequently necessary to skip one or more bends and come back to it later, by measuring carefully along the length of the tube. One tube letter may contain 7–10 small bends, and mistakes are not easily corrected without going back and starting all over again. If more tubing is required, another piece is welded onto it, or the parts can be all welded onto each other at the final step. The finished tube must be vacuum tight and clean inside in order to operate. Once the tube is filled with mercury, if any mistake is made after that, the entire tube should be started over anew, because breathing heated mercury-impregnated glass and phosphor causes long term heavy metal poisoning in neon workers. Sticks of tubing are joined until the tube reaches an impractical size, and several tubes are joined in series with the high voltage neon transformer. Extreme ends of the electrical circuit must be isolated from each other to prevent tube puncture and buzzing from corona effect.\n\nA cold cathode electrode is melted (or welded) to each end of the tube as it is finished. The hollow electrodes are also traditionally lead glass and contain a small metal shell with two wires protruding through the glass to which the sign wiring will later be attached. All welds and seals must be leak-proof at high vacuum before proceeding further.\n\nThe tube is attached to a manifold which is then attached to a high-quality vacuum pump. The tube is then evacuated of air until it reaches a vacuum level of a few torr. The evacuation is paused, and a high current is forced through the low-pressure air in the tube via the electrodes (in a process known as \"bombarding\"). This current and voltage is far above the level that occurs in final operation of the tube. The current depends on the specific electrodes used and the diameter of the tube but is typically in the 150 mA to 1,500 mA range, starting low and increasing towards the end of the process to ensure that the electrodes are adequately heated without melting the glass tube. The bombarding current is provided by a large transformer with an open-circuit voltage of roughly 15,000VAC to 23,000VAC. The bombarding transformer acts as an adjustable constant current source, and the actual voltage during operation depends on the length and pressure of the tube. Typically the operator will maintain the pressure as high as the bombarder will allow to ensure maximum power dissipation and heating. Bombarding transformers may be specially made for this use, or may be repurposed electrical utility distribution transformers (the type seen mounted on utility poles) operated backwards to produce a high voltage output.\n\nThis very high power dissipation in the tube heats the glass walls to a temperature of several hundred degrees Celsius, and any dirt and impurities within are drawn off in the gasified form by the vacuum pump. The greatest impurities that are driven off this way are the gases that coat the inside wall of the tubing by adsorption, mainly oxygen, carbon dioxide, and especially water vapor. The current also heats the electrode metal to over 600C, producing a bright orange incandescent color. The cathodes are prefabricated hollow metal shells with a small opening (sometimes a ceramic donut aperture) which contains in the interior surface of the shell a light dusting of a cold cathode low work function powder (usually a powder ceramic molar eutectic point mixture including BaCO), combined with other alkaline earth oxides, which reduces to BaO when heated to about 500 degrees F, and reduces the work function of the electrode for cathodic emission. Barium Oxide has a work function of roughly 2 eV whereas tungsten at room temperature has a work function 100 times more, or 4.0 eV. This represents the cathode drop or electron energy required to remove electrons from the surface of the cathode. This avoids the necessity of using a hot wire thermoelectric cathode such as is used in conventional fluorescent lamps. And for that reason, neon tubes are extremely long lived when properly processed, in contrast to fluorescent tubing, because there is no wire filament as there is in a fluorsecent tube to burn out like a common light bulb. The principal purpose of doing this is to purify the interior of the tube \"before\" the tube is sealed off so that when it is operated, these gases and impurities are not driven off and released by the plasma and the heat generated into the sealed tube, which would quickly burn the metal cathodes and mercury droplets (if pumped with argon/mercury) and oxidize the interior gases and cause immediate tube failure. The more thorough the purification of the tube is, the longer lasting and stable the tube will be in actual operation. Once these gases and impurities are liberated under pre-filling bombardment into the tube interior they are quickly evacuated by the pump.\n\nWhile still attached to the manifold, the tube is allowed to cool while pumping down to the lowest pressure the system can achieve. It is then filled to a low pressure of a few torrs (millimeters of mercury) with one of the noble gases, or a mixture of them, and sometimes a small amount of mercury. This gas fill pressure represents roughly 1/100th of the pressure of the atmosphere. The required pressure depends on the gas used and the diameter of the tube, with optimal values ranging from (for a long 20 mm tube filled with argon/mercury) to (for a short 8 mm diameter tube filled with pure neon). Neon or argon are the most common gases used; krypton, xenon, and helium are used by artists for special purposes but are not used alone in normal signs. A premixed combination of argon and helium is often used in lieu of pure argon when a tube is to be installed in a cold climate, since the helium increases voltage drop (and thus power dissipation), warming the tube to operating temperature faster. Neon glows bright red or reddish orange when lit. When argon or argon/helium is used, a tiny droplet of mercury is added. Argon by itself is very dim pale lavender when lit, but the droplet of mercury fills the tube with mercury vapor when sealed, which then emits ultraviolet light upon electrification. This ultraviolet emission allows finished argon/mercury tubes to glow with a variety of bright colors when the tube has been coated on the interior with ultraviolet-sensitive phosphors after being bent into shape.\n\nAn alternative way of processing finished neon tubes has also been used. Because the only purpose of bombardment by electrical means is to purify the interior of tubes, it is also possible to produce a tube by heating the tube externally either with a torch or with an oven, while heating the electrode with a radio frequency induction heating (RFIH) coil. While this is less productive, it creates a cleaner custom tube with significantly less cathode damage, longer life and brilliance, and can produce tubes of very small sizes and diameters, down to 6mm OD. The tube is heated thoroughly under high vacuum without external electrical application, until the outgassed gases can be seen to have been totally depleted and the pressure drops to a high vacuum again. Then the tube is filled, sealed and the mercury dropped and shaken.\n\nThe finished glass pieces are illuminated by either a neon sign transformer or a switched-mode power supply, usually running at voltages ranging between 2–15 kV and currents between 18 and 30 mA (higher currents available on special order.) These power supplies operate as constant-current sources (a high voltage supply with a very high internal impedance), since the tube has a negative characteristic electrical impedance. Standard tube tables established in the early days of neon are still used that specify the gas fill pressures, in either Ne or Hg/Ar, as a function of tube length in feet, tube diameter and transformer voltage.\n\nThe standard traditional neon transformer, a magnetic shunt transformer, is a special non-linear type designed to keep the voltage across the tube raised to whatever level is necessary to produce the fixed current needed. The voltage drop of a tube is proportional to length and so the maximum voltage and length of tubing fed from a given transformer is limited. Generally, the loaded voltage drops to about 800 VAC at full current. The short-circuit current is about the same.\n\nCompact high frequency inverter-converter transformers developed in the early 1990s are used, especially when low Radio Frequency Interference (RFI) is needed, such as in locations near high-fidelity sound equipment. At the typical frequency of these solid state transformers, the plasma electron-ion recombination time is too long to extinguish and reignite the plasma at each cycle, unlike the case at power line frequency. The plasma does not broadcast high frequency switching noise and remains ionized continually, becoming radio noise free.\n\nThe most common current rating is 30 mA for general use, with 60 mA used for high-brightness applications like channel letters or architectural lighting. 120 mA sources are occasionally seen in illuminating applications, but are uncommon since special electrodes are required to withstand the current, and an accidental shock from a 120 mA transformer is much more likely to be fatal than from the lower current supplies.\n\nThe efficiency of neon lighting ranges between that of ordinary incandescent lights and that of fluorescent lamps, depending on color. On a per-watt basis, incandescents produce 10 to 20 lumens, while fluorescents produce 50 to 100 lumens. Neon light efficiency ranges from 10 lumens per watt for red, up to 60 lumens for green and blue when these colors result from internal phosphor coatings.\n\nA trick of the eye is used to produce visually distinct neon display segments by blocking out parts of the tube with an opaque coating. One complete assembly may be composed of contiguous tube elements joined by glass welding to one another so that the same current passes through, for example, several letters joined end to end from cathode to cathode. To the untrained eye, this looks like separate tubes, but the electrical splice is the plasma inside the crossover glass itself. The entire tube lights up, but the segments that the viewer is not supposed to see are covered with highly opaque special black or gray glass paint. This heat-resistant coating is either painted on or dipped. Without blockout paint, the unintended visual connections would make the display appear confusing.\n\nIn most mass-produced low-priced signs today, clear glass tubing is coated with translucent paint to produce colored light. In this way, several different colors can be produced inexpensively from a single glowing tube. Over time, elevated temperatures, thermal cycling, or exposure to weather may cause the colored coating to flake off the glass or change its hue. A more expensive alternative is to use high-quality colored glass tubing, which retains a more stable appearance as it ages.\n\nLight-emitting tubes form colored lines with which a text can be written or a picture drawn, including various decorations, especially in advertising and commercial signage. By programming sequences of switching parts on and off, there are many possibilities for dynamic light patterns that form animated images.\n\nIn some applications, neon tubes are increasingly being replaced with LEDs, given the steady advance in LED luminosity and decreasing cost of high-intensity LEDs. However, proponents of neon technology maintain that they still have significant advantages over LEDs.\n\nNeon illumination is valuable to invoke 1940s or 1950s nostalgia in marketing and in historic restoration of architectural landmarks from the neon era. Architecture in the streamline moderne era often deployed neon to accent structural pigmented glass built into the façade of a 1930s or 1940s structure; many of these buildings now qualify for inclusion on historic registers such as the U.S. National Register of Historic Places if their historic integrity is faithfully maintained.\n\n\n\n"}
{"id": "18336364", "url": "https://en.wikipedia.org/wiki?curid=18336364", "title": "Novovoronezh Nuclear Power Plant II", "text": "Novovoronezh Nuclear Power Plant II\n\nNovovoronezh Nuclear Power Plant II (NvNPPII; []) is a Russian nuclear power plant located in Voronezh Oblast. \nUnit 1 started operation in 2017 and was the first Generation III+ nuclear reactor in the world.\nUnit 2 is under construction and two more units are planned.\nIt is being built on the same site as the present Novovoronezh Nuclear Power Plant.\n\nIn 2006, the Russian government legislated a nuclear expansion plan for 2007–2015. The plan aimed to put two new nuclear reactors into operation each year from 2012. This decision provided impetus for the construction of NNPPII, which had been originally been mooted in 1999. \nOn 20 June 2007 preparations began at the construction site.\nConstruction starting ceremony was held on 12 July 2009.\n\nIn January 2017 the plant took delivery of a safety instrumentation and control system from Areva NP for installation in its Unit 1.\n\nThe power station will comprise two to four VVER-1200/392M reactors of the AES-2006 type. These reactors are the first of their kind. Unit 1 was planned to be added to the grid in 2012, with the second unit to be added a year later. Cost of the project is between 110 and 130 billion Rubles. \nThe city of Novovoronezh is to provide housing for incoming NvNPP II construction workers. In early 2008 the first two apartment blocks were complete and ready to use.\n\nConstruction of the nuclear power plant is important because the existing Novovoronezh nuclear power plant is a focal point of the energy supply system. The power plant complex provides energy not only to Voronezh Oblast but to Belgorod, Lipetsk and Tambov territories as well.\n\nThe Novovoronezh Nuclear Power Plant II will have four units:\n\nService life of the VVER-1200 is 60 years.\n\n"}
{"id": "857361", "url": "https://en.wikipedia.org/wiki?curid=857361", "title": "Nuclear Energy Board", "text": "Nuclear Energy Board\n\nThe Nuclear Energy Board (NEB) was an Irish agency charged with developing nuclear power in Ireland. It was established in Ireland on November 30, 1973 by the Nuclear Energy (An Bord Fuinnimh Núicléigh) Act 1971.\n\nThe board was responsible in the 1970s for pursuing the policy of developing a nuclear power station, which was to be located at Carnsore Point, County Wexford. This policy ultimately failed and the board gradually faded from public attention, eventually concentrating on nuclear-related environmental reports. It was not a large organisation, with the Electricity Supply Board doing most operational work.\n\nIn 1968, Ireland's economic development required more energy production and the Electricity Supply Board began evaluating ways of diversifying its electricity generation. The Turlough Hill project had just commenced and this was one of the most prestigious engineering projects since the foundation of the state and the Shannon hydroelectric scheme. In the 1970s the need for new energy sources became more urgent, especially after the 1973 energy crisis. In 1975 Bord Gáis was established in order to develop Kinsale gas field, slowing the nuclear energy project as it was hoped it may be an alternative. However, in 1974 planning permission was sought for four reactors with County Wexford County Council, with one to be built immediately, most likely of pressurized water reactor design.\n\nThe economic slowdown of 1974 and 1975 saw the project temporarily put on hold. When Desmond O'Malley became the new Minister for Industry, Commerce and Energy in 1977 the project once again became a priority of government policy. This time the government wanted to build a 650 MW plant at Carnsore at a cost of £350 million (Punt) at then-prices. In 1979 the project was again postponed, following a change in government when George Colley became the new minister in charge of the project and the incident at Three Mile Island in the United States. Friends of the Earth and other groups lobbied against the plan and in 1981 the Electricity Supply Board and the government announced it was no longer national policy.\n\nUltimately the board was remembered for the plans put on and off hold, and resulting immense controversy. Also there was criticism that the government overestimated the need of energy in Ireland in the future; at one point it was estimated that industry would consume 57% of energy by 1990 - internationally this is rather large, as 40% is a typical value. Nevertheless, Ireland in the 1970s was regarded as being in a dangerous position on energy, as 75% of needs were met by oil, and European Economic Community policy was to reduce this below 50% by 1985, after two energy crises.\n\nAfter 1981 the Nuclear Energy Board was not immediately abolished, instead rather than becoming nuclear advocate, with the board became redefined in a new role as an environmentalist. The board sponsored a number of reports, in particular on the Sellafield plant which has long been a source of dispute between Ireland and the United Kingdom.\n\nOn 1 April 1992 the successor to the board was established, the Radiological Protection Institute of Ireland. The production of electricity for supply to the national grid, by nuclear fission, is currently prohibited under the Electricity Regulation Act 1999 (Section 18).\n\n\"Nuclear Energy Board Final Report 1973-1992, Dublin 1992.\"\n"}
{"id": "14456919", "url": "https://en.wikipedia.org/wiki?curid=14456919", "title": "Pleuston", "text": "Pleuston\n\nPleuston are the organisms that live in the thin surface layer existing at the air-water interface of a body of water as their habitat. Examples include some cyanobacteria, some gastropods, the ferns \"Azolla\" and \"Salvinia\" and the seed plants \"Lemna\", \"Wolffia\", \"Pistia\", \"Eichhornia crassipes\" and \"Hydrocharis\". Some fungi and fungi-like protists may be also found.\n\nThe term \"Neuston\" is used either:\n\nNeustons, broadly defined, are made up of some species of fish (see flying fish), beetles (see whirligig beetle), protozoans, bacteria and spiders (see fishing spider and diving bell spider). Springtails in the genera \"Podura\" and \"Sminthurides\" are almost exclusively neustonic, while \"Hypogastrura\" species often aggregate on pond surfaces. Water striders such as \"Gerris\" are common examples of insects that supports their weight on water's surface tension. By extension, the term may also refer to non-organismal floating aggregations (see, \"e.g.\", Great Pacific Garbage Patch).\n\nPlankton (organisms that float or drift within the water) are distinguished from nekton (organisms that swim, powerfully, in the water), and benthos (organisms on the bottom of a body of water).\n"}
{"id": "38986449", "url": "https://en.wikipedia.org/wiki?curid=38986449", "title": "Pyrophanite", "text": "Pyrophanite\n\nPyrophanite is a manganese titanium oxide mineral with formula: MnTiO. It is a member of the ilmenite group. It is a deep red to greenish black mineral which crystallizes in the trigonal system.\n\nIt was first described in 1890 from an occurrence in the Harstigen Mine, Filipstad, Värmland, Sweden. Its name was derived from the Greek \"πΰρ,\" fire, and \"φαίνεσθαι,\" to appear, because of the deep red color of the mineral.\n\nIts main occurrence is in manganese deposits that have undegone metamorphism. It also occurs in granite, amphibolite and serpentinite as an uncommon accessory \nmineral. Associated minerals include ilmenite, geikielite, hematite, spinel, gahnite, chromite, magnetite, ganophyllite, manganophyllite, hendricksite, garnet and calcite.\n"}
{"id": "30080636", "url": "https://en.wikipedia.org/wiki?curid=30080636", "title": "Queen of the Sun", "text": "Queen of the Sun\n\nQueen of the Sun: What Are the Bees Telling Us? is a 2010 documentary film directed by Taggart Siegel. The film investigates multiple angles of the recent bee epidemic colony collapse disorder. It also explores the historical and contemporary relationship between bees and humans. Featuring interviews from Michael Pollan, Gunther Hauk, Vandana Shiva, Hugh Wilson, Michael Thiele (former bee keeper at Green Gulch Farm), May Berenbaum, Carlo Petrini and Raj Patel.\n\nThe film was an official selection of the 2010 International Documentary Film Festival Amsterdam for the Green Screen Competition. It received the Documentary Audience Award from both the Maui Film Festival and Indie Memphis Film Festival. It was 2010 Pare Lorentz Honorable Mention for the International Documentary Association.\n\n"}
{"id": "14248647", "url": "https://en.wikipedia.org/wiki?curid=14248647", "title": "Quillagua", "text": "Quillagua\n\nQuillagua is an oasis in the Tocopilla Province, in the Antofagasta Region of northern Chile. It is a part of the commune of María Elena. The Loa River is crossed by the Pan-American Highway in this area.\nAccording to the \"Dirección Meteorológica de Chile\", Quillagua would be drier than Arica and thus it would be the driest place on Earth. This is also recognized by the \"Guinness Book of Records\".\n\n"}
{"id": "13973140", "url": "https://en.wikipedia.org/wiki?curid=13973140", "title": "Skid cone", "text": "Skid cone\n\nIn forestry, a skid cone is a hollow steel or plastic cone placed over the sawn end of a log. When skidding (dragging) logs end-wise, it presents a pointed end that deflects itself past obstacles.\n\nSkid cones are most popularly used when skidding single logs behind ATVs or light tractors, particularly when a single operator is too occupied with driving to keep a continuous watch behind. Heavy tractors and large logs smash through obstacles. Horse loggers work from behind the horse and have a better view of the moving log. Skid cones are also useful when winching by rope, where it's more difficult to steer a log.\n\nSkid cones prevent logs from being stopped by roots, stumps or residual trees while being harvested. It also protects standing valuable trees along the path from being damaged. Operator safety is enhanced, as the cone prevents sudden stops caused by logs getting caught in an obstacle.\n"}
{"id": "13837337", "url": "https://en.wikipedia.org/wiki?curid=13837337", "title": "Solar challenge", "text": "Solar challenge\n\nSolar challenge refers to races between solar powered vehicles, such as:\n"}
{"id": "436208", "url": "https://en.wikipedia.org/wiki?curid=436208", "title": "Solar thermal rocket", "text": "Solar thermal rocket\n\nA solar thermal rocket is a theoretical spacecraft propulsion system that would make use of solar power to directly heat reaction mass, and therefore would not require an electrical generator, like most other forms of solar-powered propulsion do. The rocket would only have to carry the means of capturing solar energy, such as concentrators and mirrors. The heated propellant would be fed through a conventional rocket nozzle to produce thrust. Its engine thrust would be directly related to the surface area of the solar collector and to the local intensity of the solar radiation.\n\nIn the shorter term, solar thermal propulsion has been proposed both for longer-life, lower-cost and more-flexible cryogenic upper stage launch vehicles and for on-orbit propellant depots. Solar thermal propulsion is also a good candidate for use in reusable inter-orbital tugs, as it is a high-efficiency low-thrust system that can be refuelled with relative ease.\n\nThere are two solar thermal propulsion concepts, differing primarily in the method by which they use solar power to heat up the propellant:\n\nDue to limitations in the temperature that heat exchanger materials can withstand (approximately 2800 K), the indirect absorption designs cannot achieve specific impulses beyond 900 seconds (9 kN·s/kg = 9 km/s) (or up to 1000 seconds, see below). The direct absorption designs allow higher propellant temperatures and therefore higher specific impulses, approaching 1200 seconds. Even the lower specific impulse represents a significant increase over that of conventional chemical rockets, however, an increase that can provide substantial payload gains (45 percent for a LEO-to-GEO mission) at the expense of increased trip time (14 days compared to 10 hours).\n\nSmall-scale hardware has been designed and fabricated for the Air Force Rocket Propulsion Laboratory (AFRPL) for ground test evaluation. Systems with 10 to 100 N of thrust have been investigated by SART.\n\nReusable Orbital Transfer Vehicles (OTV), sometimes called (inter-orbital) space tugs, propelled by solar thermal rockets have been proposed. The concentrators on solar thermal tugs are less susceptible to radiation in the Van Allen belts than the solar arrays of solar electric OTV.\n\nMost proposed designs for solar thermal rockets use hydrogen as their propellant due to its low molecular weight which gives excellent specific impulse of up to 1000 seconds (10 kN·s/kg) using heat exchangers made of rhenium. \n\nConventional thought has been that hydrogen—although it gives excellent specific impulse—is not space storable. Recent design work has developed an approach to substantially reduce hydrogen boiloff, and to economically utilize the small remaining boiloff product for requisite in-space tasks, essentially achieving zero boil off (ZBO) from a practical point of view.\n\nOther substances could also be used. Water gives quite poor performance of 190 seconds (1.9 kN·s/kg), but requires only simple equipment to purify and handle, and is space storable and this has very seriously been proposed for interplanetary use, using in-situ resources.\n\nAmmonia has been proposed as a propellant. It offers higher specific impulse than water, but is easily storable, with a freezing point of −77 degrees Celsius and a boiling point of −33.34 °C. The exhaust dissociates into hydrogen and nitrogen, leading to a lower average molecular weight, and thus a higher Isp (65% of hydrogen).\n\nA solar-thermal propulsion architecture outperforms architectures involving electrolysis and liquification of hydrogen from water by more than an order of magnitude, since electrolysis requires heavy power generators, whereas distillation only requires a simple and compact heat source (either nuclear or solar); so the propellant production rate is correspondingly far higher for any given initial mass of equipment. However its use does rely on having clear ideas of the location of water ice in the solar system, particularly on lunar and asteroidal bodies, and such information is not known, other than that the bodies with the asteroid belt and further from the Sun are expected to be rich in water ice.\n\nSolar thermal rockets have been proposed as a system for launching a small personal spacecraft into orbit. The design is based on a high altitude airship which uses its envelope to focus sunlight onto a tube. The propellant, which would likely be ammonia, is then fed through to produce thrust. Possible design flaws include whether the engine could produce enough thrust to overcome drag, and whether the skin of the airship wouldn't fail at hypersonic velocities. This has many similarities to the orbital airship proposed by JP Aerospace.\n\n, two proposals for utilizing solar-thermal propulsion on in-space post-launch spacecraft systems have been made.\n\nA concept to provide low Earth orbit (LEO) propellant depots that could be used as way-stations for other spacecraft to stop and refuel on the way to beyond-LEO missions has proposed that waste gaseous hydrogen—an inevitable byproduct of long-term liquid hydrogen storage in the radiative heat environment of space—would be usable as a monopropellant in a solar-thermal propulsion system. The waste hydrogen would be productively utilized for both orbital stationkeeping and attitude control, as well as providing limited propellant and thrust to use for orbital maneuvers to better rendezvous with other spacecraft that would be inbound to receive fuel from the depot.\n\nSolar-thermal monoprop hydrogen thrusters are also integral to the design of the next-generation cryogenic upper stage rocket proposed by U.S. company United Launch Alliance (ULA). The Advanced Common Evolved Stage (ACES) is intended as a lower-cost, more-capable and more-flexible upper stage that would supplement, and perhaps replace, the existing ULA Centaur and ULA Delta Cryogenic Second Stage (DCSS) upper stage vehicles. The ACES Integrated Vehicle Fluids option eliminates all hydrazine monopropellant and all helium pressurant from the space vehicle—normally used for attitude control and station keeping—and depends instead on solar-thermal monoprop thrusters using waste hydrogen.\n\nThe viability of various trips using Solar Thermal propulsion was investigated by Gordon Woodcock and Dave Byers in 2003.\n\n\n"}
{"id": "29476970", "url": "https://en.wikipedia.org/wiki?curid=29476970", "title": "Transposition tower", "text": "Transposition tower\n\nIn electrical power transmission, a transposition tower is a transmission tower that changes the relative physical positions of the conductors of a transmission line. A transposition tower allows these sections to be connected together, while maintaining adequate clearance for the conductors. This is important since it distributes electrical impedances between phases of a circuit over time, reducing the problem of one conductor carrying more current than others.\n\nDouble-circuit lines are usually set up with conductors of the same phase placed opposite each other.\nThis reduces the reactance due to mutual inductance; the reactance of both circuits together is less than half that of one circuit. For example, a section of a line may be (top-to-bottom) phases A-B-C on the left, also phases C'-B'-A' on the right. The next section may be B-C-A on the left, also A'-C'-B' on the right. Therefore, the rotation on each side of the tower will be opposite.\n\n"}
{"id": "8342716", "url": "https://en.wikipedia.org/wiki?curid=8342716", "title": "Tropical cyclone observation", "text": "Tropical cyclone observation\n\nTropical cyclone observation has been carried out over the past couple of centuries in various ways. The passage of typhoons, hurricanes, as well as other tropical cyclones have been detected by word of mouth from sailors recently coming to port or by radio transmissions from ships at sea, from sediment deposits in near shore estuaries, to the wiping out of cities near the coastline. Since World War II, advances in technology have included using planes to survey the ocean basins, satellites to monitor the world's oceans from outer space using a variety of methods, radars to monitor their progress near the coastline, and recently the introduction of unmanned aerial vehicles to penetrate storms. Recent studies have concentrated on studying hurricane impacts lying within rocks or near shore lake sediments, which are branches of a new field known as paleotempestology. This article details the various methods employed in the creation of the hurricane database, as well as reconstructions necessary for reanalysis of past storms used in projects such as the Atlantic hurricane reanalysis.\n\nRecent studies of the O and C isotopes found in stalagmites in Belize show that tropical cyclone events can leave markers that can be separated out on a week-by-week basis. The error rate of this type of microanalysis was 1 error in 1,200 sampling points.\n\nRocks contain certain isotopes of elements, known as natural tracers, which describe the conditions under which they formed. By studying the calcium carbonate in coral rock, past sea surface temperature and hurricane information can be revealed. Lighter oxygen isotopes (O) are left behind in coral during periods of very heavy rainfall. Since hurricanes are the main source of extreme rainfall in the tropical oceans, past hurricane events can be dated to the days of their impact on the coral by looking at the increased O concentration within the coral.\n\nKam Biu-Liu, a professor at Louisiana State University, has been studying sediment lying at the bottom of coastal lakes and marshes in order to study the frequency and intensity of hurricanes over the past 5,000 years. Since storm surges sweep coastal sands with them as they progress inland, a layer of sand is left behind in coastal lakes and marshes. Radiocarbon dating is then used to date the layers.\n\nBefore the invention of the telegraph in the early to mid-19th century, news was as fast as the quickest horse, stage, or ship. Normally, there was no advance warning of a tropical cyclone impact. However, the situation changed in the 19th century as seafaring people and land-based researchers, such as Father Viñes in Cuba, came up with systematic methods of reading the sky's appearance or the sea state, which could foretell a tropical cyclone's approach up to a couple days in advance.\n\nIn China, the abundance of historical documentary records in the form of \"\" (semiofficial local gazettes) offers an extraordinary opportunity for providing a high-resolution historical dataset for the frequency of typhoon strikes. Kam-biu Liu \"et al.\" (2001) reconstructed a 1,000-year time series of typhoon landfalls in the Guangdong Province of southern China since AD 975 and found that on a decadal timescale, the twenty-year interval from AD 1660 to 1680 is the most active period on record, with twenty-eight to thirty-seven typhoon landfalls per decade. The variability in typhoon landfalls in Guangdong mimics that observed in other paleoclimatic proxies (e.g., tree rings, ice cores) from China and the northern hemisphere. Remarkably, the two periods of most frequent typhoon strikes in Guangdong (AD 1660-1680, 1850–1880) coincide with two of the coldest and driest periods in northern and central China during the Little Ice Age.\n\nFor centuries, people have sailed the world's oceans and seas, and for just as long, they have encountered storms. The worst of the cyclones over the open seas likely took those that observed them into the depths of the oceans. However, some did survive to report harrowing tales. Before the invention of the wireless telegraph in 1905, reports about storms at sea either coincided with their arrival at the coast as ships scrambled into port, or came weeks and months afterwards from remote ports of call. Ship and buoy reports, available since the 1970s, are used in real-time not only for their temperature, pressure, and wind measurements, but also for their sea surface temperature and wave height measurements.\n\nWind reports from ships at sea have become increasingly based on anemometers, and less so on the Beaufort Scale. This is important to note as the Beaufort Scale underestimates winds at higher wind speeds, indicating ship wind observations taken for older storms are likely to underrepresent their true value.\n\nAs Christopher Landsea \"et al.\" point out, many tropical cyclones that formed on the open sea and did not affect any coast usually went undetected prior to satellite observation since the 1970s. They estimated an undercount bias of zero to six tropical cyclones per year between 1851 and 1885 and zero to four per year between 1886 and 1910. These undercounts roughly take into account the typical size of tropical cyclones, the density of shipping tracks over the Atlantic basin, and the amount of populated coastline.\n\nIn the early 20th century, forecasting the track of cyclones was still confined to areas of the greatest surface pressure falls, based upon surface weather observations, and climatology. These methods proved to be the cutting edge of tropical cyclone forecasting through the mid 20th century. Land-based surface observations remain invaluable as a source of real-time information at locations near the coastline and inland. Combined with ship observations and newspapers, they formed the total information network for hurricane detection until radiosondes were introduced in 1941 and reconnaissance aircraft began in 1944. Land-based observations of pressure and wind can show how quickly a tropical cyclone is decaying as it moves inland. Their rainfall reports show where significant rainfall is occurring, and can be an alert for possible flooding. With the establishment of the ASOS network in the United States during the 1990s, more locations are reporting around the clock than ever before.\n\nSince the 1990s, academic researchers have begun to deploy mobile weather stations fortified to withstand hurricane-force winds. The two largest programs are the Florida Coastal Monitoring Program and the Wind Engineering Mobile Instrumented Tower Experiment. During landfall, the NOAA Hurricane Research Division compares and verifies data from reconnaissance aircraft, including wind speed data taken at flight level and from GPS dropwindsondes and stepped-frequency microwave radiometers, to wind speed data transmitted in real time from weather stations erected near or at the coast. The National Hurricane Center uses the data to evaluate conditions at landfall and to verify forecasts.\n\nThe idea of aircraft reconnaissance of tropical cyclones first was put forth by Captain W. L. Farnsworth of the Galveston Commercial Association in the early 1930s. Supported by the United States Weather Bureau, it passed both the United States Senate and United States House of Representatives in 1936. Since 1944, aircraft have been flying out to sea to find tropical cyclones. Before regular satellite coverage, this was a hit-or-miss affair. Thereafter, aircraft flights into tropical systems became more targeted and precise. Nowadays, a C-130 is used as a hurricane hunter by the Air Force, while the P-3 Orion is used by the National Oceanic and Atmospheric Administration for research projects used to better understand tropical cyclones and improve hurricane forecasts.\nThe implementation of synoptic observation missions by a Gulfstream jet, where dropwindsondes are used to investigate a tropical cyclone's environment, has led to a 15-20 percent reduction in track forecast errors where such missions were present.\n\nHistorical aircraft used for weather and hurricane tracking include:\n\n\nIn Canada, the Convair 580 is used by National Research Council to track hurricanes.\n\nThe era of the aerosonde began in 1998, when the Australian Bureau of Meteorology flew an aerosonde into Tropical Cyclone Tiffany. In 2005, Hurricane Ophelia became the first Atlantic tropical cyclone where an unmanned aerial vehicle, known as an aerosonde, mission was used for a tropical cyclone. The first typhoon was penetrated by an aerosonde in 2005 as well. Unlike normal reconnaissance flights, the aerosonde stayed near the surface after a 10-hour flight within the tropical cyclone.\n\nDuring World War II, radar technology was developed to detect aircraft. It soon became apparent that large areas became obscured when significant weather was in the area. In 1957, the National Weather Service established the United States' first radar network to cover the coastline and act as first warning of an impending tropical cyclone. Upgraded in the 1990s to use doppler technology, radar can provide rainfall estimates, wind estimates, possible locations of tornadoes within a system's spiral bands, as well as the center location of a tropical cyclone. The United States operates with a network of 158 Doppler Radars across the country.\n\nBeginning with the launching of TIROS-I in April 1960, satellites have been used to look for tropical cyclones. The Dvorak technique was developed from early satellite images of tropical cyclones to determine real-time a tropical cyclone's strength from characteristics seen on satellite imagery. In most tropical cyclone basins, use of the satellite-based Dvorak technique is the primary method used to determine a tropical cyclone's maximum sustained winds. The extent of banding and difference in temperature between the eye and eyewall is used within the technique to assign a maximum sustained wind and pressure. Since the mid-1990s, microwave imagery has been able to determine the center of rotation when that center is obscured by mid to high level cloudiness. Cloud top temperatures are used in real-time to estimate rainfall rates within the cyclone.\n\n"}
{"id": "37667209", "url": "https://en.wikipedia.org/wiki?curid=37667209", "title": "Trung Sơn Dam", "text": "Trung Sơn Dam\n\nThe Trung Sơn Dam (\"Dự Án Nhà máy Thủy điện Trung Sơn\") is a hydroelectric power station under construction on the Ma River in northwestern Vietnam. Located in the Trung Sơn commune, Quan Hóa District, Thanh Hóa Province, it is approximately southwest of Hòa Bình city, and northwest of Thanh Hóa city. The dam will create a reservoir which covers a large area of the Mường Lát and Quan Hóa Districts in the Thanh Hoa province as well as part of the Mộc Châu District in Sơn La Province. It is approximately from the Vietnam–Laos border.\n\nThe Trung Sơn hydropower project is owned by Trung Sơn Hydropower Company Limited (TSHPCo), the entity established by Vietnam Electricity (EVN) in Decision No. 106/QD-ENV dated March 17, 2011. TSHPCo is responsible for the management, construction and operation of the Trung Sơn hydropower project.\n\nThe project will provide both power generation and flood control. The powerhouse is designed to contain four generating units with a total capacity of 260 MW and an annual output of 1,018.61 GWh, a significant addition to the national grid. The flood control storage of will help prevent floods downstream.\n\nThe project will cost a total of VND 7,775,146 million (equivalent to US $410.68 million). This includes US $330 million from a World Bank loan, which was signed by the Socialist Republic of Vietnam and the World Bank on June 28, 2011. Additionally, the project was given slightly over US $80 million from the counterpart fund of (EVN).\n\nThe project has created social, environmental and community relation programs to mitigate anticipated and unanticipated issues with populations either directly or indirectly impacted, the Resettlement, Livelihoods and Ethnic Minorities Development Program (RLDP). These populations consist of approximately 10,600 people (2,327 households), of which 7,012 (1,516 households) will be directly impacted in the main project area, resulting in a total of 533 households having to be resettled. The RLDP includes a Resettlement Plan (RP), a Community Livelihoods Improvement Plan (CLIP) and an Ethnic Minorities Development Plan (EMDP).\n\nIn addition, management has prepared a Supplementary Environmental and Social Impact Assessment (SESIA) with Environmental Management Plan (EMP) for the project. This plan includes principles, approaches, procedures and methods to be used to control and minimize environmental impacts of all project-related construction and operation activities. Compared with fossil-fuel based energy plants of the same size, the dam produces far less greenhouse gas emissions (GHGs). TSHPCo maintains a website at www.trungsonhp.vn where public information is routinely updated.\n\n\nThe project is composed of four components:\n\n\n\n"}
{"id": "4417124", "url": "https://en.wikipedia.org/wiki?curid=4417124", "title": "Uranium market", "text": "Uranium market\n\nThe uranium market, like all commodity markets, has a history of volatility, moving not only with the standard forces of supply and demand, but also to whims of geopolitics. It has also evolved particularities of its own in response to the unique nature and use of this material.\n\nHistorically, uranium has been mined in countries willing to export, including Australia and Canada. However, countries now responsible for more than 30% of the world’s uranium production include Kazakhstan, Namibia, Niger, and Uzbekistan.\n\nUranium from mining is used almost entirely as fuel for nuclear power plants. Following the 2011 Fukushima nuclear disaster, the global uranium market remains depressed, with the uranium price falling more than 50%, declining share values, and reduced profitability of uranium producers since March 2011. As a result, uranium companies worldwide are reducing capacity, closing operations and deferring new production.\n\nBefore uranium is ready for use as nuclear fuel in reactors, it must undergo a number of intermediary processing steps which are identified as the front end of the nuclear fuel cycle: mining it (either underground or in open pit mines), milling it into yellowcake, enriching it and finally fuel fabrication to produce fuel assemblies or bundles. This technologically complicated and challenging process is simple in comparison to the complexity of the market that has evolved to provide these three services.\n\nThe world's top uranium producers in 2012 with 64% of production were Kazakhstan (36.5% of world production), Canada (15.4%) and Australia (12.0%). Other major producers included Niger, Namibia and Russia Purification facilities are almost always located at the mining sites. The facilities for enrichment, on the other hand, are found in those countries that produce significant amounts of electricity from nuclear power. Large commercial enrichment plants are in operation in France, Germany, Netherlands, UK, United States, and Russia, with smaller plants elsewhere. These nations form the core of the uranium market and influence considerable control over all buyers. The uranium market is a classic seller's market. The uranium cartel, as it became known, was the alliance of the major uranium producing nations. Representatives of these five countries met in Paris, France in February, 1972 to discuss the \"orderly marketing\" of uranium. Although sounding innocuous, they had, amongst themselves, a monopoly in the uranium market and were deciding to exercise it.\n\nGlobal demand for uranium rose steadily from the end of World War II, largely driven by nuclear weapons procurement programs. This trend lasted until the early 1980s, when changing geopolitical circumstances as well as environmental, safety, economic concerns over nuclear power plants reduced demand somewhat. The production of a series of large hydro-electric power stations has also helped to depress the global market since the early 1970s. This phenomenon can be traced back to the construction of the vast Aswan Dam in Egypt, and to a certain extent with the ambitious Three Gorges Dam in China. During this time, large uranium inventories accumulated. In fact, until 1985 the Western uranium industry was producing material much faster than nuclear power plants and military programs were consuming it. Uranium prices slid throughout the decade with few respites, leaving the price below $10 per pound for yellowcake by year-end 1989.\nAs uranium prices fell, producers began curtailing operations or exiting the business entirely, leaving only a few actively involved in uranium mining and causing uranium inventories to shrink significantly. Since 1990 uranium requirements have outstripped uranium production. World uranium requirements have increased steadily to 171 million pounds of yellowcake in 2014.\n\nHowever several factors are pushing both industrialized and developing nations towards alternative energy sources. The increasing rate of consumption of fossil fuel is a concern for nations lacking in reserves, especially non-OPEC nations. The other issue is the level of pollution produced by coal-burning plants, and despite their vastness, an absence of economical methods for tapping into solar, wind-driven, or tidal reserves. Uranium suppliers hope that this will mean an increase in market share and an increase in volume over the long term.\n\nUranium prices reached an all-time low in 2001, costing US$7/lb. This was followed by a period of gradual rise, followed by a bubble culminating in mid-2007, which caused the price to peak at around US$137/lb. This was the highest price (adjusted for inflation) in 25 years. The higher price during the bubble has spurred new prospecting and reopening of old mines. In 2012 Kazatomprom and Areva were the top two producing companies (with 15% of the production each), followed by Cameco (14%), ARMZ Uranium Holding (13%) and Rio Tinto (9%).\n\nFollowing the shutdown of many nuclear power plants after the Fukushima Daiichi nuclear disaster in 2011, demand had fallen to about per year in 2015 with future forecasts uncertain.\n\nBecause of the improvements in gas centrifuge technology in the 2000s, replacing former gaseous diffusion plants, cheaper separative work units have enabled the economic production of more enriched uranium from a given amount of natural uranium, by re-enriching tails ultimately leaving a depleted uranium tail of lower enrichment. This has somewhat lowered the demand for natural uranium.\n\nUnlike other metals such as copper or nickel, uranium is not traded on an organized commodity exchange such as the London Metal Exchange. Instead it is traded in most cases through contracts negotiated directly between a buyer and a seller. Recently, however, the New York Mercantile Exchange announced a 10-year agreement to provide for the trade of on and off exchange uranium futures contracts.\n\nThe structure of uranium supply contracts varies widely. Pricing can be as simple as a single fixed price, or based on various reference prices with economic corrections built in. Contracts traditionally specify a base price, such as the uranium spot price, and rules for escalation. In base-escalated contracts, the buyer and seller agree on a base price that escalates over time on the basis of an agreed-upon formula, which may take economic indices, such as GDP or inflation factors, into consideration.\n\nA spot market contract usually consists of just one delivery and is typically priced at or near the published spot market price at the time of purchase. However 85% of all uranium has been sold under long-term, multi-year contracts with deliveries starting one to three years after the contract is made. Long-term contract terms range from two to 10 years, but typically run three to five years, with the first delivery occurring within 24 months of contract award. They may also include a clause that allows the buyer to vary the size of each delivery within prescribed limits. For example, delivery quantities may vary from the prescribed annual volume by plus or minus 15%.\n\nOne of the peculiarities of the nuclear fuel cycle is the way in which utilities with nuclear power plants buy their fuel. Instead of buying fuel bundles from the fabricator, the usual approach is to purchase uranium in all of these intermediate forms. Typically, a fuel buyer from power utilities will contract separately with suppliers at each step of the process. Sometimes, the fuel buyer may purchase enriched uranium product, the end product of the first three stages, and contract separately for fabrication, the fourth step to eventually obtain the fuel in a form that can be loaded into the reactor. The utilities believe—rightly or wrongly—that these options offers them the best price and service. They will typically retain two or three suppliers for each stage of the fuel cycle, who compete for their business by tender. Sellers consist of suppliers in each of the four stages as well as brokers and traders. There are fewer than 100 companies that buy and sell uranium in the western world.\n\nIn addition to being sold in different forms, uranium markets are differentiated by geography. The global trading of uranium has evolved into two distinct marketplaces shaped by historical and political forces. The first, the western world marketplace comprises the Americas, Western Europe and Australia. A separate marketplace comprises countries within the former Soviet Union, or the Commonwealth of Independent States (CIS), Eastern Europe and China. Most of the fuel requirements for nuclear power plants in the CIS are supplied from the CIS's own stockpiles. Often producers within the CIS also supply uranium and fuel products to the western world, increasing competition.\n\nAs of 2015, total identified uranium resources were sufficient for more than a century of supply based on current requirements.\n\nIn 1983, physicist Bernard Cohen proposed that the world supply of uranium is effectively inexhaustible, and could therefore be considered a form of renewable energy. He claims that fast breeder reactors, fueled by naturally-replenished uranium extracted from seawater, could supply energy at least as long as the sun's expected remaining lifespan of five billion years. These reactors use uranium-238, which is more common than the uranium-235 required by conventional reactors.\n\n\n"}
