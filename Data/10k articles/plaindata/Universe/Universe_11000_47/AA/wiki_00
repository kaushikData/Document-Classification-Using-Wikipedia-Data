{"id": "38402309", "url": "https://en.wikipedia.org/wiki?curid=38402309", "title": "Alphabet Energy", "text": "Alphabet Energy\n\nAlphabet Energy is a startup company founded in 2009 at the University of California, Berkeley by thermoelectrics expert Matthew L. Scullin and Peidong Yang. The company uses nanotechnology and materials science applications to create thermoelectric generators that are more cost effective than previous bismuth telluride-based devices. The company is based in Hayward, California. It started with a license to use silicon nanowire developed at Lawrence Berkeley National Laboratory. They moved from UC Berkeley to offices in San Francisco in 2011, and later to Hayward.\n\nAlphabet has a number of patents related to the capture of waste heat for purposes of electricity generation. The company is working with tetrahedrite, a common mineral with thermoelectric properties.\n\n2011's \"The Lean Startup: How Today's Entrepreneurs Use Continuous Innovation to Create Radically Successful Businesses\" describes Alphabet Energy's approach to product development as an example of the successful practice of the book's principles. Author Eric Ries is on Alphabet's advisory board.\n\nAlphabet has raised over $35 million in venture capital funding from Claremont Creek, TPG Capital, Encana and the California Clean Energy Fund. They were chosen as a 2014 World Economic Forum Technology Pioneer and as a 2015 IHS CERAWeek Energy Innovation Pioneer.\n\nThe company’s name, based on the word alpha, comes from its use as a term for a Seebeck coefficient, and has no relation to the Google holding company, Alphabet Inc.\n\nIn 2014, Alphabet Energy introduced the world’s first industrial-scale thermoelectric generator, the E1. The E1 takes exhaust heat from large industrial engines and turns it into electricity. The result is an engine that needs less fuel to deliver the same power. The E1 is optimized for engines up to 1,400 kW, and works on any engine or exhaust source, currently generating up to 25 kWe on a standard 1,000 kW engine. The E1's modules are interchangeable but currently come with a low-cost proprietary thermoelectric material and the device is rated for a 10-year life span. As advances in thermoelectric materials are made, new modules can be swapped in for old ones, to continually improve fuel efficiency to as much as 10%.\n\nIn 2017 Alphabet Energy, with a $2-million grant from the California Energy Commission (CEC), has been partnering with Berkley Lab \"to create a cost-effective thermoelectric waste heat recovery system to reduce both energy use in the industrial sector and electricity-related carbon emissions.\" The goal is a prototype with 10+ percent efficiency, operating temperatures beyond the 400 degree Celsius limit up to 800 degrees, possible remote electricity generation for areas off the grid, and \"modularization for a broad scale of..applications\" unique at various locations. \n\n"}
{"id": "423387", "url": "https://en.wikipedia.org/wiki?curid=423387", "title": "Antifreeze protein", "text": "Antifreeze protein\n\nAntifreeze proteins (AFPs) or ice structuring proteins (ISPs) refer to a class of polypeptides produced by certain animals, plants, fungi and bacteria that permit their survival in subzero environments. AFPs bind to small ice crystals to inhibit growth and recrystallization of ice that would otherwise be fatal. There is also increasing evidence that AFPs interact with mammalian cell membranes to protect them from cold damage. This work suggests the involvement of AFPs in cold acclimatization.\n\nUnlike the widely used automotive antifreeze, ethylene glycol, AFPs do not lower freezing point in proportion to concentration. Rather, they work in a noncolligative manner. This phenomenon allows them to act as an antifreeze at concentrations 1/300th to 1/500th of those of other dissolved solutes. Their low concentration minimizes their effect on osmotic pressure. The unusual properties of AFPs are attributed to their selective affinity for specific crystalline ice forms and the resulting blockade of the ice-nucleation process.\n\nAFPs create a difference between the melting point and freezing point (busting temperature of AFP bound ice crystal) known as thermal hysteresis. The addition of AFPs at the interface between solid ice and liquid water inhibits the thermodynamically favored growth of the ice crystal. Ice growth is kinetically inhibited by the AFPs covering the water-accessible surfaces of ice.\n\nThermal hysteresis is easily measured in the lab with a nanolitre osmometer. Organisms differ in their values of thermal hysteresis. The maximum level of thermal hysteresis shown by fish AFP is approximately -3.5 °C (Sheikh Mahatabuddin et al, SciRep)(29.3 °F). However, insect antifreeze proteins are 10–30 times more active than fish proteins. This difference probably reflects the lower temperatures encountered by insects on land. In contrast, aquatic organisms are exposed only to -1 to -2 °C below freezing. During the extreme winter months, the spruce budworm resists freezing at temperatures approaching -30 °C. The Alaskan beetle \"Upis ceramboides\" can survive in a temperature of -60 °C by using antifreeze agents that are not proteins.\n\nThe rate of cooling can influence the thermal hysteresis value of AFPs. Rapid cooling can substantially decrease the nonequilibrium freezing point, and hence the thermal hysteresis value. Consequently, organisms cannot necessarily adapt to their subzero environment if the temperature drops abruptly.\n\nSpecies containing AFPs may be classified as\n\nFreeze avoidant: These species are able to prevent their body fluids from freezing altogether. Generally, the AFP function may be overcome at extremely cold temperatures, leading to rapid ice growth and death.\n\nFreeze tolerant: These species are able to survive body fluid freezing. Some freeze tolerant species are thought to use AFPs as cryoprotectants to prevent the damage of freezing, but not freezing altogether. The exact mechanism is still unknown. However, it is thought AFPs may inhibit recrystallization and stabilize cell membranes to prevent damage by ice. They may work in conjunction with protein ice nucleators (PINs) to control the rate of ice propagation following freezing.\n\nThere are many known nonhomologous types of AFPs.\n\nAntifreeze glycoproteins or AFGPs are found in Antarctic notothenioids and northern cod. They are 2.6-3.3 kD. AFGPs evolved separately in notothenioids and northern cod. In notothenioids, the AFGP gene arose from an ancestral trypsinogen-like serine protease gene.\n\nType I AFP is found in winter flounder, longhorn sculpin and shorthorn sculpin. It is the best documented AFP because it was the first to have its three-dimensional structure determined. Type I AFP consists of a single, long, amphipathic alpha helix, about 3.3-4.5 kD in size. There are three faces to the 3D structure: the hydrophobic, hydrophilic, and Thr-Asx face.\n\nType I-hyp AFP (where hyp stands for hyperactive) are found in several righteye flounders. It is approximately 32 kD (two 17 kD dimeric molecules). The protein was isolated from the blood plasma of winter flounder. It is considerably better at depressing freezing temperature than most fish AFPs.\n\nType II AFPs are found in sea raven, smelt and herring. They are cysteine-rich globular proteins containing five disulfide bonds. Type II AFPs likely evolved from calcium dependent (c-type) lectins. Sea ravens, smelt, and herring are quite divergent lineages of teleost. If the AFP gene were present in the most recent common ancestor of these lineages, it's peculiar that the gene is scattered throughout those lineages, present in some orders and absent in others. It has been suggested that lateral gene transfer could be attributed to this discrepancy, such that the smelt acquired the type II AFP gene from the herring.\n\nType III AFPs are found in Antarctic eelpout. They exhibit similar overall hydrophobicity at ice binding surfaces to type I AFPs. They are approximately 6kD in size. Type III AFPs likely evolved from a sialic acid synthase gene present in Antarctic eelpout. Through a gene duplication event, this gene—which has been shown to exhibit some ice-binding activity of its own—evolved into an effective AFP gene.\n\nType IV AFPs are found in longhorn sculpins. They are alpha helical proteins rich in glutamate and glutamine. This protein is approximately 12KDa in size and consists of a 4-helix bundle. Its only posttranslational modification is a pyroglutamate residue, a cyclized glutamine residue at its N-terminus.\n\nThe classification of AFPs became more complicated when antifreeze proteins from plants were discovered. Plant AFPs are rather different from the other AFPs in the following aspects:\n\n\n\"See also dehydrin\"\n\nThere are two types of insect antifreeze proteins, \"Tenebrio\" and \"Dendroides\" AFPs which are both in different insect families. They are similar to one another, both being hyperactive (i.e. greater thermal hysteresis value) and consist of varying numbers of 12- or 13-mer repeats of approximately 8.3 to 12.5 kD. Throughout the length of the protein, at least every sixth residue is a cysteine. It has recently been discovered that even highly truncated insect AFP (e.g. peptide consisting of just two or three coils) may exhibit ice structuring and thermal hysteresis activities.\n\n\"Tenebrio\" or Type V AFPs are found in beetles, whereas \"Dendroides\" or \"Choristoneura fumiferana\" AFPs are found in some Lepidoptera.\n\nAFPs were also found in microorganisms living in sea ice. The diatoms \"Fragilariopsis cylindrus\" and \"F. curta\" play a key role in polar sea ice communities, dominating the assemblages of both platelet layer and within pack ice. AFPs are widespread in these species, and the presence of AFP genes as a multigene family indicates the importance of this group for the genus \"Fragilariopsis\". AFPs identified in \"F. cylindrus\" belong to an AFP family which is represented in different taxa and can be found in other organisms related to sea ice (\"Colwellia\" spp., \"Navicula glaciei\", \"Chaetoceros neogracile\" and \"Stephos longipes and Leucosporidium antarcticum\") and Antarctic inland ice bacteria (Flavobacteriaceae), as well as in cold-tolerant fungi (\"Typhula ishikariensis\", \"Lentinula edodes\" and \"Flammulina populicola\".)\n\nThe remarkable diversity and distribution of AFPs suggest the different types evolved recently in response to sea level glaciation occurring 1-2 million years ago in the Northern hemisphere and 10-30 million years ago in Antarctica. This independent development of similar adaptations is referred to as convergent evolution. There are two reasons why many types of AFPs are able to carry out the same function despite their diversity:\n\nAFPs are thought to inhibit growth by an adsorption–inhibition mechanism. They adsorb to nonbasal planes of ice, inhibiting thermodynamically favored ice growth. The presence of a flat, rigid surface in some AFPs seems to facilitate its interaction with ice via Van der Waals force surface complementarity.\n\nNormally, ice crystals grown in solution only exhibit the basal (0001) and prism faces (1010), and appear as round and flat discs. However, it appears the presence of AFPs exposes other faces. It now appears the ice surface 2021 is the preferred binding surface, at least for AFP type I. Through studies on type I AFP, ice and AFP were initially thought to interact through hydrogen bonding (Raymond and DeVries, 1977). However, when parts of the protein thought to facilitate this hydrogen bonding were mutated, the hypothesized decrease in antifreeze activity was not observed. Recent data suggest hydrophobic interactions could be the main contributor. It is difficult to discern the exact mechanism of binding because of the complex water-ice interface. Currently, attempts to uncover the precise mechanism are being made through use of molecular modelling programs (molecular dynamics or the Monte Carlo method).\n\nAccording to the structure and function study on the antifreeze protein from \"Pseudopleuronectes americanus\", the antifreeze mechanism of the type-I AFP molecule was shown to be due to the binding to an ice nucleation structure in a zipper-like fashion through hydrogen bonding of the hydroxyl groups of its four Thr residues to the oxygens along the formula_1 direction in ice lattice, subsequently stopping or retarding the growth of ice pyramidal planes so as to depress the freeze point.\n\nThe above mechanism can be used to elucidate the structure-function relationship of other antifreeze proteins with the following two common features:\n\nIn the 1950s, Norwegian scientist Scholander set out to explain how Arctic fish can survive in water colder than the freezing point of their blood. His experiments led him to believe there was “antifreeze” in the blood of Arctic fish. Then in the late 1960s, animal biologist Arthur DeVries was able to isolate the antifreeze protein through his investigation of Antarctic fish. These proteins were later called antifreeze glycoproteins (AFGPs) or antifreeze glycopeptides to distinguish them from newly discovered nonglycoprotein biological antifreeze agents (AFPs). DeVries worked with Robert Feeney (1970) to characterize the chemical and physical properties of antifreeze proteins. In 1992, Griffith \"et al.\" documented their discovery of AFP in winter rye leaves. Around the same time, Urrutia, Duman and Knight (1992) documented thermal hysteresis protein in angiosperms. The next year, Duman and Olsen noted AFPs had also been discovered in over 23 species of angiosperms, including ones eaten by humans. They reported their presence in fungi and bacteria as well.\n\nRecent attempts have been made to relabel antifreeze proteins as ice structuring proteins to more accurately represent their function and to dispose of any assumed negative relation between AFPs and automotive antifreeze, ethylene glycol. These two things are completely separate entities, and show loose similarity only in their function.\n\nNumerous fields would be able to benefit from the protection of tissue damage by freezing. Businesses are currently investigating the use of these proteins in:\n\nUnilever has obtained UK approval to use a genetically modified yeast to produce antifreeze proteins from fish, for use in ice cream production.\n\nOne recent, successful business endeavor has been the introduction of AFPs into ice cream and yogurt products. This ingredient, labelled ice-structuring protein, has been approved by the Food and Drug Administration. The proteins are isolated from fish and replicated, on a larger scale, in genetically modified yeast.\n\nThere is concern from organizations opposed to genetically modified organisms (GMOs), who belive that antifreeze proteins may cause inflammation. Intake of AFPs in diet is likely substantial in most northerly and temperate regions already. Given the known historic consumption of AFPs, it is safe to conclude their functional properties do not impart any toxicologic or allergenic effects in humans.\n\nAs well, the transgenic process of ISP production is widely used in society already. Insulin and rennet are produced using this technology. The process does not impact the product; it merely makes production more efficient and prevents the death of fish which would otherwise be killed to extract the protein.\n\nCurrently, Unilever incorporates AFPs into some of its American products, including some Popsicle ice pops and a new line of Breyers Light Double Churned ice cream bars. In ice cream, AFPs allow the production of very creamy, dense, reduced fat ice cream with fewer additives. They control ice crystal growth brought on by thawing on the loading dock or kitchen table which drastically reduces texture quality.\n\nIn November 2009, the Proceedings of the National Academy of Sciences published the discovery of a molecule in an Alaskan beetle that behaves like AFPs, but is composed of saccharides and fatty acids.\n\nA 2010 study demonstrated the stability of superheated water ice crystals in an AFP solution, showing while the proteins can inhibit freezing, they can also inhibit melting.\n\n"}
{"id": "37994124", "url": "https://en.wikipedia.org/wiki?curid=37994124", "title": "Association (ecology)", "text": "Association (ecology)\n\nIn phytosociology and community ecology an association is a type of ecological community with a predictable species composition, consistent physiognomy (structural appearance) which occurs in a particular habitat type. The term was first coined by Alexander von Humboldt and formalised by the International Botanical Congress in 1910.\n\nAn association can be viewed as a real, integrated entity shaped either by species interactions or by similar habitat requirements, or it can be viewed as merely a common point along a continuum. The former view was championed by American ecologist Frederic Clements, who viewed the association as a whole that was more than the sum of its parts, and by Josias Braun-Blanquet, a Swiss-born phytosociologist. On the other end of the argument was American ecologist Henry Gleason, who saw these groupings of plant species as a coincidence produced by the \"fluctuation and fortuitous immigration of plants, and an equally fluctuating and variable environment\".\n"}
{"id": "4667218", "url": "https://en.wikipedia.org/wiki?curid=4667218", "title": "Atomic Energy Organization of Iran", "text": "Atomic Energy Organization of Iran\n\nThe Atomic Energy Organization of Iran (AEOI) is the main official body responsible for implementing regulations and operating nuclear energy installations in Iran.\n\nIt is headquartered in northern Amir Abad district in Tehran, but has facilities throughout the country. The organization is currently headed by Ali Akbar Salehi, who was named to replace Fereydoon Abbasi 15 August 2013.\n\n\n\n"}
{"id": "12131888", "url": "https://en.wikipedia.org/wiki?curid=12131888", "title": "Bloch oscillation", "text": "Bloch oscillation\n\nBloch oscillation is a phenomenon from solid state physics. It describes the oscillation of a particle (e.g. an electron) confined in a periodic potential when a constant force is acting on it.\nIt was first pointed out by Bloch and Zener while studying the electrical properties of crystals. In particular, they predicted that the motion of electrons in a perfect crystal under the action of a constant electric field would be oscillatory instead of uniform. While in natural crystals this phenomenon is extremely hard to observe due to the scattering of electrons by lattice defects, it has been observed in semiconductor superlattices and in different physical systems such as cold atoms in an optical potential and ultrasmall Josephson junctions.\n\nThe one-dimensional equation of motion for an electron in a constant electric field E is:\n\nformula_1 , \n\nas was pointed out by wave-particle duality and formula_2 in solid state physics (rather than formula_3 in general).\n\nformula_4\n\nwhich has the solution\n\nformula_5 .\n\nThe velocity v of the electron is given by\n\nformula_6 ,\n\nwhere formula_7 denotes the dispersion relation for the given energy band.\nSuppose that the latter has the (tight-binding) form\n\nformula_8,\n\nwhere a is the lattice parameter and A is a constant. Then v(k) is given by\n\nformula_9 ,\n\nand the electron position formula_10 by\n\nformula_11 .\n\nThis shows that the electron oscillates in real space. The angular frequency of the oscillations is given by formula_12.\n\nBloch oscillations were predicted by Nobel laureate Leo Esaki in 1970. However, they were not experimentally observed for a long time, because in natural solid-state bodies, formula_13 is (even with very high electric field strengths) not large enough to allow for full oscillations of the charge carriers within the diffraction and tunneling times, due to relatively small lattice periods. The development in semiconductor technology has recently led to the fabrication of structures with super lattice periods that are now sufficiently large, based on artificial semiconductors. The oscillation period in those structures is smaller than the diffraction time of the electrons, hence more oscillations can be observed in a time window below the diffraction time. For the first time the experimental observation of Bloch oscillations in such super lattices at very low temperatures was shown by Jochen Feldmann and Karl Leo in 1992. Other realizations were \n\n"}
{"id": "7829373", "url": "https://en.wikipedia.org/wiki?curid=7829373", "title": "Bob (physics)", "text": "Bob (physics)\n\nA bob is the weight on the end of a pendulum found most commonly, but not exclusively, in pendulum clocks.\n\nAlthough a pendulum can theoretically be any shape, any rigid object swinging on a pivot, clock pendulums are usually made of a weight or \"bob\" attached to the bottom end of a rod, with the top attached to a pivot so it can swing. The advantage of this construction is that it positions the centre of mass close to the physical end of the pendulum, farthest from the pivot. This maximizes the moment of inertia, and minimises the length of pendulum required for a given period. Shorter pendulums allow the clock case to be made smaller, and also minimize the pendulum's air resistance. Since most of the energy loss in clocks is due to air friction of the pendulum, this allows clocks to run longer on a given power source.\n\nTraditionally, a pendulum bob is a round flat disk, lens-shaped in section, but bobs in older clocks often have decorative carving and shapes characteristic of the type of clock. They are usually made of a dense metal such as iron or brass. Lead is denser, but is usually avoided because of its softness, which would result in the bob being dented during its inevitable collisions with the inside of the clock case when the clock is moved.\n\nIn most pendulum clocks the rate is adjusted by moving the bob up or down on the pendulum rod. Moving it up shortens the pendulum, making it beat more quickly, and causing the clock to gain time. In the most common arrangement, the bob is attached to the pendulum with an adjustment nut at the bottom, on the threaded end of the pendulum rod. Turning the nut adjusts the height of the bob. But some bobs have levers or dials to adjust the height. In some precision clocks there is a smaller auxiliary weight on a threaded shaft to allow more fine adjustment. Tower clocks sometimes have a tray mounted on the pendulum rod, to which small weights can be added or removed, to adjust the rate without stopping the clock.\n\nThe weight of the bob itself has little effect on the period of the pendulum. However, a heavier bob helps to keep the pendulum moving smoothly until it receives its next push from the clock's escapement mechanism. That increases the pendulum's Q factor, making the motion of the pendulum more independent of the escapement and the errors it introduces, leading to increased accuracy. On the other hand, the heavier the bob is the more energy must be supplied by the clock's power source and more friction and wear occurs in the clock's movement. Pendulum bobs in quality clocks are usually made as heavy as the clock's movement can drive. A common weight for the bob of a one second pendulum, widely used in grandfather clocks and many others, is around 2 kilograms.\n\n"}
{"id": "2145763", "url": "https://en.wikipedia.org/wiki?curid=2145763", "title": "Burgers material", "text": "Burgers material\n\nA Burgers material is a viscoelastic material which consists of a Maxwell material and a Kelvin material in series. It is named after the Dutch physicist Johannes Martinus Burgers.\n\nGiven that the Kelvin material has an elasticity formula_1 and viscosity formula_2, and the Maxwell material has an elasticity formula_3 and viscosity formula_4, the Burgers model has the constitutive equation\nwhere formula_6 is the stress and formula_7 is the strain.\n\nThis model incorporates viscous flow into the standard linear solid model, giving a linearly increasing asymptote for strain under fixed loading conditions.\n\n\n"}
{"id": "40806252", "url": "https://en.wikipedia.org/wiki?curid=40806252", "title": "Choo Choo Track &amp; Toy Co", "text": "Choo Choo Track &amp; Toy Co\n\nChoo Choo Track & Toy Co is a small, family-owned and operated toy company manufacturing in the United States.\n\nIn business since 1999, Choo Choo Track & Toy Co was originally a source of track for Whittle Shortline.\n\nWooden track, accessories, train display cases, specialty transition connectors, and train tables for children are manufactured near St. Louis, Mo and available at local train shows or online. Bridges are constructed using CNC-milled wooden sides instead of plastic and track is made from reforested American Beech wood. Wooden track and accessories are compatible with Thomas the Tank Engine and BRIO wooden track systems.\n"}
{"id": "17751264", "url": "https://en.wikipedia.org/wiki?curid=17751264", "title": "Coalite", "text": "Coalite\n\nCoalite is a brand of low-temperature coke used as a smokeless fuel. The title refers to the residue left behind when coal is carbonised at 640 degrees Celsius. It was invented by Thomas Parker in 1904. In 1936 the Smoke Abatement Society awarded its inventor a posthumous gold medal.\n\nCoalite is darker and more friable than high temperature coke. It is easier to ignite, burns with an attractive flame, and is lighter than coal, making it an ideal fuel for open domestic firegrates. Drawbacks are its tendencies to produce an excessive residual ash, to burn quickly and give off sulphurous fumes.\nCoal delivered by rail, first from the nearby Bolsover colliery, and later from other sources, was heated in 8 large air sealed ovens called \"batteries\". \nVolatile constituents were driven off and condensed into coal oil and a watery fraction called ammoniacal liquor. Coal gas was used to heat the ovens and also burned in the works boilers and furnaces. Any excess was flared off. The coal oil and liquor were piped over the road to the chemical works section where they were processed into various fractions and industrial chemicals. The residual Coalite solid fuel was cooled, then sorted into various grades based on size and stockpiled for distribution by road transport.\n\nTwo years after Thomas Parker died in 1915, the forerunner of the Coalite company was formed with the building of a production unit at Barugh near Barnsley. In the 20s, two more plants at Askern near Doncaster and at East Greenwich in London were opened, the latter being operated under license by the South Metropolitan Gas Company.\n\nIn April 1937, the main manufacturing plant at Buttermilk Lane, Bolsover was opened by Prince George, Duke of Kent. At the time it was the largest one of its type in the world.\n\nIn 1939, another works was opened at Wern Tarw in South Wales. In the 1950s older plants were closed and production was concentrated at the expanded Bolsover and Askern plants.\nPlants were also opened later in Rossington, near Doncaster, and at Grimethorpe in South Yorkshire. The coal oil and liquor from all these plants was processed at the central refinery at the Bolsover plant. The ovens continued producing Coalite until the Bolsover works closed down in 2004.\n\nBy 1939, the company was producing a low octane petrol called \"Coalene\" as well as diesel and other fuel oils. It continued to do so through the Second World War and into the early 1960s. It supplied fuel to the Royal Air Force and Royal Navy during the war, reputedly keeping 12 squadrons flying and 2 battleships sailing.\n\nIn 1948, the company changed its name to the Coalite Chemical Company to reflect the diversified nature of the business. In 1952 the registered Head office address was moved from London to Bolsover. In 1956, after the introduction of the Clean Air Act 1956 Coalite was licensed as an \"authorised fuel\". Demand increased and the company expanded accordingly.\n\nThe Group consisted of several companies in the 1950–1960s, with a Francis L. Waring, F.Inst.F. as Managing Director of this group of companies; \n\nIn 1978, the company merged with the Charringtons Coke distribution company and with this they acquired the Falkland Islands Company. From 1984 to 1989 Eric Varley, the ex-Labour cabinet minister was company chairman.\n\nIn 1986, the group acquired Hargreaves Fuel distribution services. This marked the zenith of the company's fortunes. The group was now at its most diverse and widespread, owning subsidiaries that were totally unrelated to its core business. These included sheep farming, Dormobile camper vans, Builders merchants, Pyrometer Instrument manufacturers, vehicle sales and many others as well as industrial land across the country. From 1987 to 1991, the company sponsored Chesterfield F.C.\n\nIn 1989, the company was taken over by Anglo United, From 1990 to 1992, the company sponsored world class snooker tournaments\nIn 1997, Falkland Islands Holdings subsidiary was floated off and became an independent company again. \n\nThe solid fuel side of the business began to shrink in the 1980s. There was falling demand for the product as cheaper natural gas gradually took over the domestic heating market. The Askern works closed in 1986 and Grimethorpe in 1994. After 1982, there were several waves of redundancies.\n\nAs well as the reduced demand for solid fuel the slow demise of the company was accelerated after the Anglo takeover. During the 1990s, there were financial difficulties because the relatively small Anglo United had borrowed heavily from HSBC to buy the much larger Coalite group and had intended to service this debt by asset stripping Coalite's many subsidiaries, selling them off whilst retaining the core business of solid fuel production. The sell-off did not realise as much cash as was required and even a raid on the company pension fund did little to reduce the debt. Weighed down by this debt and with little money for investment the company declined as it faced outside competition in a downturned agrochemical market. It was making losses of £2 million per annum in the late 90s. At the same time, it was embroiled in legal actions concerning land and river pollution and the resulting adverse publicity affected the sales of its products.\n\nIn 2002, Anglo was bought by a consortium of local businessmen. Any viable assets were covertly (but legally) moved into separate subsidiaries under the Anglo holding company. The remaining debts were left with the much diminished Coalite Chemicals Ltd which went into administration and then receivership, and finding no buyers, closed down finally in 2004, leaving a considerable number of redundant employees with much reduced pensions.\n\nThe company gained attention as a manufacturer of the chemical, 245 trichlorophenol, a precursor of Agent Orange which was a defoliant used in the Malayan Emergency and the Vietnam War. An explosion in a pilot plant in 1968 had killed a chemist and spread dioxins over the debris. 79 employees contracted chloracne whilst cleaning up and even some of their families were affected by traces brought home on clothing. There was more controversy when the company appeared reluctant to reveal the location of sites where the contaminated plant debris was buried and when the case notes of an independent investigator were mysteriously stolen from her home.\n\nA specialised 245 unit was built soon after, with stringent controls and safety measures to prevent any future accidents but when the Seveso disaster in 1976 reawakened public concern over this issue, Ted Needham, the company chairman, deemed it expedient to close down this now uneconomical plant. The reluctance of his employees to work in it may also have influenced his decision.\n\nProblems continued with dioxin emission from the works incinerator which burned residues from the chlorination plant. This caused further public protest and instigated official investigations that culminated in prosecution and a fine of £150,000 for contamination of the river Doe Lea and surrounding farmland in 1996. In 1994, the river gained the dubious distinction of being the most polluted in the world with regard to dioxins, 27 times higher than the second worst waterway, a Norwegian Fjord near a Magnesium processing plant. The pollution was believed to extend 13 miles downstream into the River Rother and the River Don, near Rotherham in South Yorkshire.\n\nThe site has been derelict since 2004. From 2005, the Bolsover works were gradually demolished. In November 2016, decontamination of the area began and the Environment Agency was made aware of an increase in odours from the site after complaints from local residents.\n\nObtained from coal by pyrolysis\n\nObtained from Coal oil by distillation.\n\nObtained from Middle oil by solvent extraction and distillation.\n\nObtained from Ammoniacal liquor by solvent extraction, distillation and crystallisation.\n\nSaleable products from further processing\n\nObtained from used tyres by pyrolysis\nThis final process might have saved the company. Successful trials with existing modified plant showed that large scale reclamation of useful products from used tyres was possible. However it came too late to prevent financial collapse. This was attributed by the company to the delayed response of the regulating authorities in granting licenses for the sale of the tyre oil as fuel.\n\n"}
{"id": "55666518", "url": "https://en.wikipedia.org/wiki?curid=55666518", "title": "Cyclone Herwart", "text": "Cyclone Herwart\n\nCyclone Herwart was a European windstorm that affected Southern Denmark, Germany, Poland, Austria, Hungary and the Czech Republic on 28–29 October 2017. Named by the Free University of Berlin Meteorology Department, the storm was an extratropical cyclone formed as a secondary low to a more northerly centre of low pressure named \"Grischa\" coming southward from the Svalbard Islands region, the latter splitting in two low-pressure areas late on 28 October. The center of Herwart started rotating counterclockwise around the main low pressure area, passing over Norway, Sweden, Latvia and then losing power while moving over western Russia.\n\nIn Denmark, which was hit on 28 October, the storm was named \"Ingolf\". In Hungary, the storm was named \"Nárcisz\" (\"Narcissus\"), a Hungarian female name whose name day is on 29 October.\n\nThe storm and rainfall caused death and destruction. Train connections were closed in Northern Germany, and major bridges in Denmark were closed for traffic.\nHerwart caused disruption to the German energy market as the strong winds caused an over-supply of energy onto the German power grid. Prices for energy slipped into negative values as low as minus €83.06 euros per megawatt hour, with an average low of minus €52.11 (normal energy prices are €37 per megawatt hour). These values were the lowest since Christmas 2012. This can lead to a situation where Austria and Switzerland find it attractive to use the energy in their pumped storage reservoirs, where it can later be sold back to Germany.\n\nHerwart was not as strong as Xavier which struck Germany earlier in October 2017. Herwart is considered one of the strongest storms of the past 10 years in Germany, ranked in 8th place in strongest storms there in the last decade, but not considered memorable when compared to the strongest storms such as Kyrill (2007) and Lothar (1999). Herwart was responsible for the worst insurance losses in the Czech Republic since the storms Kyrill and Emma (2008).\n"}
{"id": "25396282", "url": "https://en.wikipedia.org/wiki?curid=25396282", "title": "December 2003 nor'easter", "text": "December 2003 nor'easter\n\nThe December 2003 New England snowstorm was a severe nor'easter that impacted the Eastern United States during the first week of the month. It produced heavy snowfall throughout the New England and Mid-Atlantic regions, exceeding in northern New England. The cyclone had complex origins, involving several individual weather disturbances. An area of low pressure primarily associated with the southern branch of the jet stream spread light precipitation across portions of the Midwest and Southeast. The low reached the coast on December 5 and continued to produce snow throughout the Mid-Atlantic. Another system involving the northern branch of the jet stream merged with the initial storm, causing another coastal storm to develop. This storm soon became the primary feature as it intensified and moved northeastward. It reached Cape Cod on December 6, but became nearly stationary through the morning of December 7. It had finally dissipated by December 8.\n\nConditions surrounding the storm allowed for several bands of heavy snowfall to set up over New York State and New England, including a small area of per hour snowfall rates in the Hudson Valley. As a result of extremely cold temperatures over the region, snowfall accumulations were generally significant and broke several daily records. At Albany, New York, of snow fell in just one day. Locations affected by the storm commonly picked up , with totals occasionally exceeding .\n\nThe event led to widespread travel delays from Washington, D.C. to Boston, and around 13 people lost their lives because of the storm. 35.6 of snow inches fell just 14 miles north of Boston in the city of Peabody, Massachusetts. The nor'easter was among the largest early-season winter storms on record to affect the major East Coast cities. Many areas reported blizzard-like conditions.\n\n\n"}
{"id": "6243127", "url": "https://en.wikipedia.org/wiki?curid=6243127", "title": "District cooling", "text": "District cooling\n\nDistrict cooling is the cooling equivalent of district heating. Working on broadly similar principles to district heating, district cooling delivers chilled water to buildings like offices and factories needing cooling. In winter, the source for the cooling can often be sea water, so it is a cheaper resource than using electricity to run compressors for cooling.\nAlternatively, District Cooling can be provided by a Heat Sharing Network which enables each building on the circuit to use a heat pump to reject heat to an ambient ground temperature circuit. \n\nThe Helsinki district cooling system uses otherwise wasted heat from summer time CHP power generation units to run absorption refrigerators for cooling during summer time, greatly reducing electricity usage. In winter time, cooling is achieved more directly using sea water. The adoption of district cooling is estimated to reduce the consumption of electricity for cooling purposes by as much as 90 per cent and an exponential growth in usage is forecast. The idea is now being adopted in other Finnish cities.\n\nThe use of district cooling grows also rapidly in Sweden in a similar way.\n\nCornell University's Lake Source Cooling System uses Cayuga Lake as a heat sink to operate the central chilled water system for its campus and to also provide cooling to the Ithaca City School District. The system has operated since the summer of 2000 and was built at a cost of $55–60 million. It cools a 14,500 tons (50 MW) load.\n\nWorking since 1985, the system of the École Polytechnique Fédérale de Lausanne combines, depending on the needs, cooling and heat extraction. This allows for a higher overall energy efficiency of the 19 MW system.\n\nIn 2009, a district cooling system was installed in the United Nations area of Geneva, drawing water from Lake Geneva. The system is in the process of being expanded to other areas of Geneva.\n\nIn August 2004, Enwave Energy Corporation, a district energy company based in Toronto, Ontario, Canada, started operating a system that uses water from Lake Ontario to cool downtown buildings, including office towers, the Metro Toronto Convention Centre, a small brewery and a telecommunications centre. The process has become known as Deep Lake Water Cooling (DLWC). It will provide for over 40,000 tons (140 MW) of cooling—a significantly larger system than has been installed elsewhere. Another feature of the Enwave system is that it is integrated with Toronto’s drinking water supply. The Toronto drinking water supply required a new intake location that would be further from shore and deeper in the lake. This posed two problems for the utility that managed the city's drinking water supply: 1. the capital cost of moving the water intake location and additionally, the new location would supply water that was so cold it would require heating before it could be distributed. The cooperation of the district cooling agency, Enwave, solved both problems: Enwave paid for the cost of moving the water intake and also supplied the heat to warm the drinking water supply to acceptable levels by effectively extracting the heat from the buildings it served. Contact between drinking water and the Enwave cooling system is restricted to thermal contact in a heat exchanger. Drinking water does not circulate through the Enwave cooling systems. \n\nTabreed currently delivers over 1 million refrigeration tons of cooling, across 72 plants located throughout the region, cooling iconic infrastructure projects such as Sheikh Zayed Grand Mosque, Cleveland Clinic, Ferrari World, Yas Mall, Aldar HQ, Etihad Towers, Marina Mall, World Trade Center in Abu Dhabi featuring the Burj Mohammed Bin Rashid, Dubai Metro, Dubai Parks & Resorts, and the Jabal Omar Development in the Holy City of Mecca, alongside several other hotels, hospitals, residential and commercial towers.\n\nIn January 2006, PAL technology is one of the emerging project management companies in UAE involved in the diversified business of desalination, sewage treatment and district cooling system. More than 400,000 Tons (1400 MW) of district cooling projects are planned. The Palm Jumeirah utilises district cooling supplied by Palm Utilities LLC to provide air conditioning for buildings on the trunk and crescent of the Palm. The Dubai Metro system, inaugurated in 2009, is the first mass transit network in the world to use district cooling to lower temperatures in stations and trains.\n\nIn 2006, a district cooling system came online in Amsterdam's Zuidas, drawing water from the Nieuwe Meer\n\nGandhinagar in India is also planning to use the district cooling system for its Gujarat International Finance Tec-City in Gujarat.\n\nOn November 9, 2010, The world's largest district cooling plant opened at The Pearl-Qatar. This plant is owned and operated by Qatar District Cooling Company (known as Qatar Cool). It is capable of cooling a load of 130,000 tons (450 MW). The Plant was Built by C.A.T. group, a Lebanese-International general contractor with vast experience in District Cooling.\n\nThe Lusail City district cooling system will supply chilled water to end users through an integrated network with a connected cooling of 500,000 Tons of Refrigeration by utilizing multiple chiller plants which are Marina, Wadi, West and North. This will be one of the largest district cooling systems in the world.\n\nA project started in 2012 in Kuwait for the Sabah Al-Salem University City with district cooling. It is capable of cooling a load of 72000 TR and it has two central utility plants having 36 chillers, 36 cooling towers and 2 TES (Thermal Energy Storage) tank. The entire university is having total 8 million square meters. In the utility tunnel total 18 km Pre-Insulated BS pipes (including 11 km 52”) with supporting Steel Structure (1150 Tons), Complete Fire Suppression System - 28 km Galvanized BS pipes, Water Supply and Drainage System – 9 km Polyethylene Pipes, Complete CCTV system, 104 km Fiber Optics BMS cables, Lighting & Power Systems, Central battery System, 175,000 m2 epoxy coating, 61 Nos. Fire rated Remote control Steel Doors. The total project duration is 1810 days.\n\nIn Germany, amongst , Munich established a rapidly growing system in 2011 with its core below the Karlsplatz (Stachus), drawing water from the underground Stadtgrabenbach. There's a 24km network, currently supplying 16 larger organizations. \n\nIf the other renewable alternatives are too warm during the summer or too expensive, cold storage can be investigated. In large scale applications underground and snow storage are the most likely alternatives. In an underground storage the winter cold is heat exchanged from the air and loaded into the bedrock or an aquifer by one or more bore holes. In a snow storage frozen water (snow and/or ice) is saved in some kind of storage (pile, pit, cavern etc.). The cold is utilized by pumping melt water to the cooling object, directly in a district cooling system or indirect by a heat exchanger. The lukewarm melt water is then pumped back to the snow where it gets cooled and mixed with new melt water. Snow cooling works as a single cold source but can also be used for peak cooling since there is no relevant cooling limit. In Sweden there is one snow cooling plant in Sundsvall, built and owned by the county. The cooling load in Sundsvall is about 2000 kW (570 tons of refrigeration) and 1500 MWh/year.\n\nEspecially in subtropical regions not only cooling, but dehumidifying of the air becomes important. Liquid desiccant cooling allows to generate remotely and efficiently a moisture absorbing liquid. This liquid can be pumped or transported long distances without energy loss.\n"}
{"id": "43838525", "url": "https://en.wikipedia.org/wiki?curid=43838525", "title": "EGTS", "text": "EGTS\n\nEGTS (Electric Green Taxiing System) is an electric taxiing system which allows aircraft to taxi and pushback without requiring the use of aircraft engines, and is designed to reduce fuel volumes used by aircraft and reduce greenhouse gas emissions during ground operations.\nEGTS technology enables aircraft to avoid using their main engines during taxiing and instead taxi autonomously under their own electrical power, using the Auxiliary Power Unit (APU) generator. The system is designed for single-aisle aircraft, such as the Airbus A320 and the Boeing 737.\n\nEGTS is an electric taxiing system that is used prior to takeoff and can help airlines reduce cost by eliminating the need to use jet engines which are not efficient on the ground. It can also reduce foreign object damage and is environmentally friendly as it reduce carbon and other emissions created during the taxiing phase. The main landing gear is equipped with an electric motor powered by the auxiliary power unit which allows the aircraft to push back from the gate and taxi without a tug or its jet engines.\n\nThe system weighs 300 kg and is permanently installed on the aircraft.\nThe system can accelerate the aircraft to 20 knots.\n\nWith electric motors located on each of the main landing gear driving inboard wheels and powered by the APU generator, the EGTS system allows aircraft to push back from the gate without a tug tractor, and taxi without the use of the main engines.\n\nThe Pilot Interface Unit enables the pilot to switch on the EGTS and select the desired taxi speed (forward) or push back speed (backwards). The EGTS Controller receives and converts actions into orders to power the electronics.\n\nThe wheel actuator applies the required torque and speed proportional to the wheel as per the instructions received from the Wheel Actuator Controller Unit (WACU). The WACU converts current into instructions to the electrical motor proportional to the pilot’s command as delivered by the EGTS Controller.\n\nElectric taxiing was invented by Delos Aerospace in 2003, and patented in 2006.\n\nHoneywell and SAFRAN announced their joint venture EGTS International at the Paris Air Show in 2011.\n\nIn 2012, easyJet, in collaboration with Honeywell and Safran, announced that it would be the first airline to support the development and trial of the electric green taxiing system (EGTS).\n\nFollowing the initial phase of ground testing and first move in April 2013, the system was first demonstrated at the Paris Air Show 2013.\n\nIn June 2013, Air France announced its support of the development of the EGTS taxiing system.\n\nIn December 2013, Airbus signed a memorandum of understanding (MoU) with EGTS International to further develop and evaluate EGTS for the A320 Family.\n\nIn March 2014, Honeywell and Safran signed a Memorandum of Understanding with GoAir to support the advancement of the EGTS taxiing system.\nGoAir will test the system and supply operational data to have an accurate projection of fuel saving to potential customers.\n\nIn April 2014, Mexican airline Interjet became the first North American airline to announce its support for the advancement of the EGTS taxiing system,\n\nAt Farnborough Air Show in July 2016, while the development was successful and customers were interested, Honeywell terminated its joint venture with Safran with low oil prices sapping its market but Safran will continue to work on taxiing systems as system design was completed while its A320 demonstrator had been decommissioned since 2013.\n\nIn July 2017, to promote it in China, Safran signed a Memorandum of Understanding with a subsidiary of China Aviation Supplies.\n\nCompetitor Taxibot is the only certified and operational alternative taxiing system in the market; it is a semi robotic tractor which meets the aircraft for taxi-in and taxi-out, once connected it is controlled by the pilot. Competing products in development by WheelTug are different as they are installed on the nose gear.\n\nIn 2011, L-3 Communications trialled a similar main landing gear electric taxiing system on an A320 non-flyable demonstrator with Lufthansa Technik and others, and proposed to develop the eTaxi system with Crane Aerospace but abandoned its plans in 2013 due to high investment costs.\nWheelTug, which could park parallel to airport buildings and use two jet bridges on regular widebodies terminals, aims to enter service in 2018.\n"}
{"id": "24511932", "url": "https://en.wikipedia.org/wiki?curid=24511932", "title": "FFKM", "text": "FFKM\n\nFFKMs (by ASTM 1418 standard) (equivalent to FFPMs by ISO/DIN 1629 standard) are perfluoroelastomeric compounds containing an even higher amount of fluorine than FKM fluoroelastomers.\n\nFFKMs (by ASTM 1418 standard) (equivalent to FFPMs by ISO/DIN 1629 standard) are perfluoroelastomeric compounds containing an even higher amount of fluorine than FKM. They have further improved resistance to high temperatures and chemicals and even withstand environments where Oxygen-Plasma are present for many hours. Certain grades have a maximum continuous service temperature of . They are commonly used to make O-rings and gaskets that are used in applications that involve contact with hydrocarbons or highly corrosive fluids, or when a wide range of temperatures is encountered.\n\nFor vacuum applications, demanding very low contamination (out-gassing and particle emission) as well as high temperature operation (200 - 300 °C) for prolonged out-baking or processing times and where a copper or metal sealing is not possible or very inconvenient/expensive, a custom-made, clean-room manufactured, sealing such as Kalrez® 9100, SCVBR or Perlast can be used. After manufacturing, they are O-plasma vacuum cleaned (and/or vacuum baked) to reach out-gassing performance similar to Teflon while reaching vacuum leak tightness (permeability rates) similar to FKM (Viton) compounds. This combination of properties allows FFKM seals to reach well into UHV pressures without the use of metal sealing. However, they are significantly more expensive than standard FKM O-rings.\n"}
{"id": "1082841", "url": "https://en.wikipedia.org/wiki?curid=1082841", "title": "Fictitious force", "text": "Fictitious force\n\nA fictitious force (also called a pseudo force, d'Alembert force, or inertial force) is an apparent force that acts on all masses whose motion is described using a non-inertial frame of reference, such as a rotating reference frame. Examples are the forces that act on passengers in an accelerating or braking automobile, and the force that pushes objects toward the rim of a centrifuge.\n\nThe fictitious force F is due to an object's inertia when the reference frame does not move inertially, and thus begins to accelerate relative to the free object. The fictitious force thus does not arise from any physical interaction between two objects (that is, it is not a \"contact force\"), but rather from the acceleration a of the non-inertial reference frame itself, which from the viewpoint of the frame now appears to be an acceleration of the object instead, requiring a \"force\" to make this happen. As stated by Iro:\n\nAssuming Newton's second law in the form F = \"m\"a, fictitious forces are always proportional to the mass \"m\".\n\nThe fictitious force on an object arises as an imaginary influence, when the frame of reference used to describe the object's motion is accelerating compared to a non-accelerating frame. The fictitious force \"explains,\" using Newton's mechanics, why an object does not follow Newton's laws and \"floats freely\" as if weightless. As a frame can accelerate in any arbitrary way, so can fictitious forces be as arbitrary (but only in direct response to the acceleration of the frame). However, four fictitious forces are defined for frames accelerated in commonly occurring ways: one caused by any relative acceleration of the origin in a straight line (rectilinear acceleration); two involving rotation: centrifugal force and Coriolis force; and a fourth, called the Euler force, caused by a variable rate of rotation, should that occur. \n\nGravitational force would also be a fictitious force based upon a field model in which particles distort spacetime due to their mass, such as General Relativity.\n\nThe role of fictitious forces in Newtonian mechanics is described by Tonnelat:\nFictitious forces arise in classical mechanics and special relativity in all non-inertial frames. Inertial frames are privileged over non-inertial frames because they do not have physics whose causes are outside of the system, while non-inertial frames do. Fictitious forces, or physics whose cause is outside of the system, are no longer necessary in general relativity, since these physics are explained with the geodesics of spacetime.\n\nThe surface of the Earth is a rotating reference frame. To solve classical mechanics problems exactly in an Earth-bound reference frame, three fictitious forces must be introduced: the Coriolis force, the centrifugal force (described below) and the Euler force. The Euler force is typically ignored because the variations in the angular velocity of the rotating Earth surface are usually insignificant. Both of the other fictitious forces are weak compared to most typical forces in everyday life, but they can be detected under careful conditions. For example, Léon Foucault used his Foucault pendulum to show that a Coriolis force results from the Earth's rotation. If the Earth were to rotate twenty times faster (making each day only ~72 minutes long), people could easily get the impression that such fictitious forces were pulling on them, as on a spinning carousel; people in temperate and tropical latitudes would, in fact, need to hold on in order to avoid being launched into orbit by the centrifugal force.\n\nObservers inside a closed box that is moving with a constant velocity cannot detect their own motion; however, observers within an accelerating reference frame can detect that they are in a non-inertial reference frame from the fictitious forces that arise. For example, for straight-line acceleration Vladimir Arnold presents the following theorem:\n\nOther accelerations also give rise to fictitious forces, as described mathematically below. The physical explanation of motions in an inertial frame is the simplest possible, requiring no fictitious forces: fictitious forces are zero, providing a means to distinguish inertial frames from others.\n\nAn example of the detection of a non-inertial, rotating reference frame is the precession of a Foucault pendulum. In the non-inertial frame of the Earth, the fictitious Coriolis force is necessary to explain observations. In an inertial frame outside the Earth, no such fictitious force is necessary.\n\nFigure 1 (top) shows an accelerating car. When a car accelerates, a passenger feels like they're being pushed back into the seat. In an inertial frame of reference attached to the road, there is no physical force moving the rider backward. However, in the rider's non-inertial reference frame attached to the accelerating car, there \"is\" a backward fictitious force. We mention two possible reasons for the force to clarify its (the force's) existence:\n\nHow can the accelerating frame be discovered to be non-inertial? In the accelerating frame, everything appears to be subject to zero net force, and nothing moves. Nonetheless, compression of the seat is observed and is explained in the accelerating frame (and in an inertial frame) by the force of acceleration on the seat from the car on one side, and the opposing force of reaction to acceleration by the passenger on the other. Identification of the accelerating frame as non-inertial cannot be based simply on the compression of the seat, which all observers can explain; rather it is based on the \"simplicity\" of the physical explanation for this compression.\n\nThe explanation of the seat compression in the accelerating frame requires not only the thrust from the axle of the car, but additional (fictitious) forces. In an inertial frame, only the thrust from the axle is necessary. Therefore, the inertial frame has a \"simpler\" physical explanation (not necessarily a simpler mathematical formulation, however), indicating the accelerating frame is a non-inertial frame of reference. In other words, in the inertial frame, fictitious forces are zero. See inertial frame for more detail.\n\nThis example illustrates how fictitious forces arise from switching from an inertial to a non-inertial reference frame. Calculations of physical quantities (compression of the seat, required force from the axle) made in any frame give the same answers, but in some cases calculations are easier to make in a non-inertial frame. (In this simple example, the calculations are equally complex for the two frames described.)\n\nA similar effect occurs in circular motion, circular from the standpoint of an inertial frame of reference attached to the road. When seen from a non-inertial frame of reference attached to the car, the fictitious force called the centrifugal force appears. If the car is moving at constant speed around a circular section of road, the occupants will feel pushed outside by this centrifugal force, away from the center of the turn. Again the situation can be viewed from inertial or non-inertial frames:\n\n\nA classic example of fictitious force in circular motion is the experiment of rotating spheres tied by a cord and spinning around their center of mass. In this case, as with the linearly accelerating car example, the identification of a rotating, non-inertial frame of reference can be based upon the vanishing of fictitious forces. In an inertial frame, fictitious forces are not necessary to explain the tension in the string joining the spheres. In a rotating frame, Coriolis and centrifugal forces must be introduced to predict the observed tension.\n\nTo consider another example, where a rotating reference frame is very natural to us, namely the surface of the rotating Earth, centrifugal force reduces the apparent force of gravity by about one part in a thousand, depending on latitude. This reduction is zero at the poles, maximum at the equator.\n\nThe fictitious Coriolis force, which is observed in rotational frames, is ordinarily visible only in very large-scale motion like the projectile motion of long-range guns or the circulation of the Earth's atmosphere (see Rossby number). Neglecting air resistance, an object dropped from a 50-meter-high tower at the equator will fall 7.7 millimeters eastward of the spot below where it is dropped because of the Coriolis force.\n\nIn the case of distant objects and a rotating reference frame, what must be taken into account is the resultant force of centrifugal and Coriolis force. Consider a distant star observed from a rotating spacecraft. In the reference frame co-rotating with the spacecraft, the distant star appears to move along a circular trajectory around the spacecraft. The apparent motion of the star is an apparent centripetal acceleration. Just like in the example above of the car in circular motion, the centrifugal force has the same magnitude as the fictitious centripetal force, but is directed in the opposite, centrifugal direction. In this case the Coriolis force is twice the magnitude of the centrifugal force, and it points in centripetal direction. The vector sum of the centrifugal force and the Coriolis force is the total fictitious force, which in this case points in centripetal direction.\n\nFictitious forces can be considered to do work, provided that they move an object on a trajectory that changes its energy from potential to kinetic. For example, consider a person in a rotating chair holding a weight in their outstretched hand. If they pull their hand inward toward their body, from the perspective of the rotating reference frame, they have done work against the centrifugal force. When the weight is let go, it spontaneously flies outward relative to the rotating reference frame, because the centrifugal force does work on the object, converting its potential energy into kinetic. From an inertial viewpoint, of course, the object flies away from them because it is suddenly allowed to move in a straight line. This illustrates that the work done, like the total potential and kinetic energy of an object, can be different in a non-inertial frame than an inertial one.\n\nThe notion of \"fictitious force\" comes up in Einstein's general theory of relativity. All fictitious forces are proportional to the mass of the object upon which they act, which is also true for gravity. This led Albert Einstein to wonder whether gravity was a fictitious force as well. He noted that a freefalling observer in a closed box would not be able to detect the force of gravity; hence, freefalling reference frames are equivalent to an inertial reference frame (the equivalence principle). Following up on this insight, Einstein was able to formulate a theory with gravity as a fictitious force; attributing the apparent acceleration of gravity to the curvature of spacetime. This idea underlies Einstein's theory of general relativity. See Eötvös experiment.\n\nMany problems require use of noninertial reference frames, for example, those involving satellites and particle accelerators. Figure 2 shows a particle with mass \"m\" and position vector x(\"t\") in a particular inertial frame A. Consider a non-inertial frame B whose origin relative to the inertial one is given by X(\"t\"). Let the position of the particle in frame B be x(\"t\"). What is the force on the particle as expressed in the coordinate system of frame B? \n\nTo answer this question, let the coordinate axis in B be represented by unit vectors u with \"j\" any of { 1, 2, 3 } for the three coordinate axes. Then\n\nThe interpretation of this equation is that x is the vector displacement of the particle as expressed in terms of the coordinates in frame B at time \"t\". From frame A the particle is located at:\n\nAs an aside, the unit vectors { u } cannot change magnitude, so derivatives of these vectors express only rotation of the coordinate system B. On the other hand, vector X simply locates the origin of frame B relative to frame A, and so cannot include rotation of frame B.\n\nTaking a time derivative, the velocity of the particle is:\n\nThe second term summation is the velocity of the particle, say v as measured in frame B. That is:\n\nThe interpretation of this equation is that the velocity of the particle seen by observers in frame A consists of what observers in frame B call the velocity, namely v, plus two extra terms related to the rate of change of the frame-B coordinate axes. One of these is simply the velocity of the moving origin v. The other is a contribution to velocity due to the fact that different locations in the non-inertial frame have different apparent velocities due to rotation of the frame; a point seen from a rotating frame has a rotational component of velocity that is greater the further the point is from the origin.\n\nTo find the acceleration, another time differentiation provides:\n\nUsing the same formula already used for the time derivative of x, the velocity derivative on the right is:\n\nConsequently,\n\nThe interpretation of this equation is as follows: the acceleration of the particle in frame A consists of what observers in frame B call the particle acceleration a, but in addition there are three acceleration terms related to the movement of the frame-B coordinate axes: one term related to the acceleration of the origin of frame B, namely a, and two terms related to rotation of frame B. Consequently, observers in B will see the particle motion as possessing \"extra\" acceleration, which they will attribute to \"forces\" acting on the particle, but which observers in A say are \"fictitious\" forces arising simply because observers in B do not recognize the non-inertial nature of frame B.\n\nThe factor of two in the Coriolis force arises from two equal contributions: (i) the apparent change of an inertially constant velocity with time because rotation makes the direction of the velocity seem to change (a \"d\"v/d\"t\" term) and (ii) an apparent change in the velocity of an object when its position changes, putting it nearer to or further from the axis of rotation (the change in formula_7 due to change in \"x \" ).\n\nTo put matters in terms of forces, the accelerations are multiplied by the particle mass:\n\nThe force observed in frame B, F = \"m\"a is related to the actual force on the particle, F, by\n\nwhere:\n\nThus, we can solve problems in frame B by assuming that Newton's second law holds (with respect to quantities in that frame) and treating F as an additional force.\n\nBelow are a number of examples applying this result for fictitious forces. More examples can be found in the article on centrifugal force.\n\nA common situation in which noninertial reference frames are useful is when the reference frame is rotating. Because such rotational motion is non-inertial, due to the acceleration present in any rotational motion, a fictitious force can always be invoked by using a rotational frame of reference. Despite this complication, the use of fictitious forces often simplifies the calculations involved.\n\nTo derive expressions for the fictitious forces, derivatives are needed for the apparent time rate of change of vectors that take into account time-variation of the coordinate axes. If the rotation of frame 'B' is represented by a vector Ω pointed along the axis of rotation with orientation given by the right-hand rule, and with magnitude given by\n\nthen the time derivative of any of the three unit vectors describing frame B is\n\nand\n\nas is verified using the properties of the vector cross product. These derivative formulas now are applied to the relationship between acceleration in an inertial frame, and that in a coordinate frame rotating with time-varying angular velocity ω(\"t\"). From the previous section, where subscript A refers to the inertial frame and B to the rotating frame, setting a = 0 to remove any translational acceleration, and focusing on only rotational properties (see Eq. 1):\n\nCollecting terms, the result is the so-called \"acceleration transformation formula\":\n\nThe physical acceleration a due to what observers in the inertial frame A call \"real external forces\" on the object is, therefore, not simply the acceleration a seen by observers in the rotational frame B, but has several additional geometric acceleration terms associated with the rotation of B. As seen in the rotational frame, the acceleration a of the particle is given by rearrangement of the above equation as:\n\nThe net force upon the object according to observers in the rotating frame is F = \"m\"a. If their observations are to result in the correct force on the object when using Newton's laws, they must consider that the additional force F is present, so the end result is F = F + F. Thus, the fictitious force used by observers in B to get the correct behavior of the object from Newton's laws equals:\n\nHere, the first term is the \"Coriolis force\", the second term is the \"centrifugal force\", and the third term is the \"Euler force\".\n\nAs a related example, suppose the moving coordinate system \"B\" rotates in a circle of radius \"R\" about the fixed origin of inertial frame \"A\", but maintains its coordinate axes fixed in orientation, as in Figure 3. The acceleration of an observed body is now (see Eq. 1):\nwhere the summations are zero inasmuch as the unit vectors have no time dependence. The origin of system \"B\" is located according to frame \"A\" at:\nleading to a velocity of the origin of frame \"B\" as:\n\nleading to an acceleration of the origin of \"B\" given by:\nBecause the first term, which is\nis of the same form as the normal centrifugal force expression:\nit is a natural extension of standard terminology (although there is no standard terminology for this case) to call this term a \"centrifugal force\". Whatever terminology is adopted, the observers in frame \"B\" must introduce a fictitious force, this time due to the acceleration from the orbital motion of their entire coordinate frame, that is radially outward away from the center of rotation of the origin of their coordinate system:\nand of magnitude:\n\nNotice that this \"centrifugal force\" has differences from the case of a rotating frame. In the rotating frame the centrifugal force is related to the distance of the object from the origin of frame \"B\", while in the case of an orbiting frame, the centrifugal force is independent of the distance of the object from the origin of frame \"B\", but instead depends upon the distance of the origin of frame \"B\" from \"its\" center of rotation, resulting in the \"same\" centrifugal fictitious force for \"all\" objects observed in frame \"B\".\n\nAs a combination example, Figure 4 shows a coordinate system \"B\" that orbits inertial frame \"A\" as in Figure 3, but the coordinate axes in frame \"B\" turn so unit vector u always points toward the center of rotation. This example might apply to a test tube in a centrifuge, where vector u points along the axis of the tube toward its opening at its top. It also resembles the Earth-Moon system, where the Moon always presents the same face to the Earth. In this example, unit vector u retains a fixed orientation, while vectors u, u rotate at the same rate as the origin of coordinates. That is,\nHence, the acceleration of a moving object is expressed as (see Eq. 1):\nwhere the angular acceleration term is zero for constant rate of rotation.\nBecause the first term, which is\nis of the same form as the normal centrifugal force expression:\nit is a natural extension of standard terminology (although there is no standard terminology for this case) to call this term the \"centrifugal force\". Applying this terminology to the example of a tube in a centrifuge, if the tube is far enough from the center of rotation, |X| = \"R\" ≫ |x|, all the matter in the test tube sees the same acceleration (the same centrifugal force). Thus, in this case, the fictitious force is primarily a uniform centrifugal force along the axis of the tube, away from the center of rotation, with a value |F| = ω \"R\", where \"R\" is the distance of the matter in the tube from the center of the centrifuge. It is standard specification of a centrifuge to use the \"effective\" radius of the centrifuge to estimate its ability to provide centrifugal force. Thus, a first estimate of centrifugal force in a centrifuge can be based upon the distance of the tubes from the center of rotation, and corrections applied if needed.\n\nAlso, the test tube confines motion to the direction down the length of the tube, so v is opposite to u and the Coriolis force is opposite to u, that is, against the wall of the tube. If the tube is spun for a long enough time, the velocity v drops to zero as the matter comes to an equilibrium distribution. For more details, see the articles on sedimentation and the Lamm equation.\n\nA related problem is that of centrifugal forces for the Earth-Moon-Sun system, where three rotations appear: the daily rotation of the Earth about its axis, the lunar-month rotation of the Earth-Moon system about their center of mass, and the annual revolution of the Earth-Moon system about the Sun. These three motions influence the tides.\n\nFigure 5 shows another example comparing the observations of an inertial observer with those of an observer on a rotating carousel. The carousel rotates at a constant angular velocity represented by the vector Ω with magnitude ω, pointing upward according to the right-hand rule. A rider on the carousel walks radially across it at constant speed, in what appears to the walker to be the straight line path inclined at 45° in Figure 5. To the stationary observer, however, the walker travels a spiral path. The points identified on both paths in Figure 5 correspond to the same times spaced at equal time intervals. We ask how two observers, one on the carousel and one in an inertial frame, formulate what they see using Newton's laws.\n\nThe observer at rest describes the path followed by the walker as a spiral. Adopting the coordinate system shown in Figure 5, the trajectory is described by r(\"t\"):\nwhere the added π/4 sets the path angle at 45° to start with (just an arbitrary choice of direction), u is a unit vector in the radial direction pointing from the center of the carousel to the walker at time \"t\". The radial distance \"R\"(\"t\") increases steadily with time according to:\nwith \"s\" the speed of walking. According to simple kinematics, the velocity is then the first derivative of the trajectory:\nwith u a unit vector perpendicular to u at time \"t\" (as can be verified by noticing that the vector dot product with the radial vector is zero) and pointing in the direction of travel.\nThe acceleration is the first derivative of the velocity:\nThe last term in the acceleration is radially inward of magnitude ω \"R\", which is therefore the instantaneous centripetal acceleration of circular motion. The first term is perpendicular to the radial direction, and pointing in the direction of travel. Its magnitude is 2\"s\"ω, and it represents the acceleration of the walker as the edge of the carousel is neared, and the arc of circle traveled in a fixed time increases, as can be seen by the increased spacing between points for equal time steps on the spiral in Figure 5 as the outer edge of the carousel is approached.\n\nApplying Newton's laws, multiplying the acceleration by the mass of the walker, the inertial observer concludes that the walker is subject to two forces: the inward, radially directed centripetal force, and another force perpendicular to the radial direction that is proportional to the speed of the walker.\n\nThe rotating observer sees the walker travel a straight line from the center of the carousel to the periphery, as shown in Figure 5. Moreover, the rotating observer sees that the walker moves at a constant speed in the same direction, so applying Newton's law of inertia, there is \"zero\" force upon the walker. These conclusions do not agree with the inertial observer. To obtain agreement, the rotating observer has to introduce fictitious forces that appear to exist in the rotating world, even though there is no apparent reason for them, no apparent gravitational mass, electric charge or what have you, that could account for these fictitious forces.\n\nTo agree with the inertial observer, the forces applied to the walker must be exactly those found above. They can be related to the general formulas already derived, namely:\nIn this example, the velocity seen in the rotating frame is:\nwith u a unit vector in the radial direction. The position of the walker as seen on the carousel is:\nand the time derivative of Ω is zero for uniform angular rotation. Noticing that\nand\nwe find:\nTo obtain a straight-line motion in the rotating world, a force exactly opposite in sign to the fictitious force must be applied to reduce the net force on the walker to zero, so Newton's law of inertia will predict a straight line motion, in agreement with what the rotating observer sees. The fictitious forces that must be combated are the Coriolis force (first term) and the centrifugal force (second term). (These terms are approximate.) By applying forces to counter these two fictitious forces, the rotating observer ends up applying exactly the same forces upon the walker that the inertial observer predicted were needed.\n\nBecause they differ only by the constant walking velocity, the walker and the rotational observer see the same accelerations. From the walker's perspective, the fictitious force is experienced as real, and combating this force is necessary to stay on a straight line radial path holding constant speed. It's like battling a crosswind while being thrown to the edge of the carousel.\n\nNotice that this kinematical discussion does not delve into the mechanism by which the required forces are generated. That is the subject of kinetics. In the case of the carousel, the kinetic discussion would involve perhaps a study of the walker's shoes and the friction they need to generate against the floor of the carousel, or perhaps the dynamics of skateboarding, if the walker switched to travel by skateboard. Whatever the means of travel across the carousel, the forces calculated above must be realized. A very rough analogy is heating your house: you must have a certain temperature to be comfortable, but whether you heat by burning gas or by burning coal is another problem. Kinematics sets the thermostat, kinetics fires the furnace.\n\n\n\n\n"}
{"id": "47003735", "url": "https://en.wikipedia.org/wiki?curid=47003735", "title": "Glossary of industrial scales and weighing", "text": "Glossary of industrial scales and weighing\n\nWhile almost every weighing scale uses the same basic principle (the gravitational force exerted by an object divided by the area the force is applied on), industrial weighing scales are designed to do a lot more. They handle heavier loads, often in different conditions, both environmental and physical.\n\nThe terminology applied to different industrial weighing systems is widely ranged, with some terms adapted specifically for them, which are included in this glossary. Modern scales are electronic, and often computer-networked; consequently, the field has imported a fair amount of terminology from electrical engineering and computerized data transfer.\n\n"}
{"id": "1123615", "url": "https://en.wikipedia.org/wiki?curid=1123615", "title": "Herbig–Haro object", "text": "Herbig–Haro object\n\nHerbig–Haro (HH) objects are turbulent looking patches of nebulosity associated with newborn stars. They are formed when narrow jets of partially ionized gas ejected by said stars collide with nearby clouds of gas and dust at speeds of several hundred kilometres per second. Herbig–Haro objects are ubiquitous in star-forming regions, and several are often seen around a single star, aligned with its rotational axis. Most of them lie within about one parsec of the source, although some have been observed several parsecs away. HH objects are transient phenomena that last around a few tens of thousand years. They can change visibly over quite short timescales of a few years as they move rapidly away from their parent star into the gas clouds of interstellar space (the interstellar medium or ISM). Hubble Space Telescope observations have revealed the complex evolution of HH objects over the period of a few years, as parts of the nebula fade while others brighten as they collide with the clumpy material of the interstellar medium.\n\nFirst observed in the late 19th century by Sherburne Wesley Burnham, Herbig–Haro objects were not recognised as being a distinct type of emission nebula until the 1940s. The first astronomers to study them in detail were George Herbig and Guillermo Haro, after whom they have been named. Herbig and Haro were working independently on studies of star formation when they first analysed the objects, and recognised that they were a by-product of the star formation process.\n\nAlthough HH objects are a visible wavelength phenomena, many remain invisible at these wavelengths due to dust and gas envelope and are only visible at infrared wavelengths. Such objects, when observed in near infrared, are called MHOs.\n\nThe first HH object was observed in the late 19th century by Sherburne Wesley Burnham, when he observed the star T Tauri with the refracting telescope at Lick Observatory and noted a small patch of nebulosity nearby. However, it was catalogued merely as an emission nebula, later becoming known as Burnham's Nebula, and was not recognised as a distinct class of object. T Tauri was found to be a very young and variable star, and is the prototype of the class of similar objects known as T Tauri stars which have yet to reach a state of hydrostatic equilibrium between gravitational collapse and energy generation through nuclear fusion at their centres.\n\nFifty years after Burnham's discovery, several similar nebulae were discovered which were so small as to be almost star-like in appearance. Both Haro and Herbig made independent observations of several of these objects in the Orion Nebula during the 1940s. Herbig also looked at Burnham's Nebula and found it displayed an unusual electromagnetic spectrum, with prominent emission lines of hydrogen, sulfur and oxygen. Haro found that all the objects of this type were invisible in infrared light.\n\nFollowing their independent discoveries, Herbig and Haro met at an astronomy conference in Tucson, Arizona in December 1949. Herbig had initially paid little attention to the objects he had discovered, being primarily concerned with the nearby stars, but on hearing Haro's findings he carried out more detailed studies of them. The Soviet astronomer Viktor Ambartsumian gave the objects their name (Herbig–Haro objects, normally shortened to HH objects), and based on their occurrence near young stars (a few hundred thousand years old), suggested they might represent an early stage in the formation of T Tauri stars.\n\nStudies of the HH objects showed they were highly ionized, and early theorists speculated that they were reflection nebulae containing low-luminosity hot stars deep inside. However, the absence of infrared radiation from the nebulae meant there could not be stars within them, as these would have emitted abundant infrared light. In 1975 American astronomer R. D. Schwartz theorized that winds from T Tauri stars produce shocks in ambient medium on encounter, resulting in generation of visible light.\n\nWith discovery of collimated jet in HH 46/47, it became clear that HH objects are indeed shock induced phenomenon with shocks being driven by collimated jet from protostars.\n\nStars form by gravitational collapse of interstellar gas clouds. As the collapse increases the density, radiative energy loss decreases due to increased opacity. This raises the temperature of the cloud which prevents further collapse, and a hydrostatic equilibrium is established. Gas continues to fall towards the core in a rotating disk. This is called a protostar. Some of the accreting material is ejected out along the star's axis of rotation in two jets of partially-ionized gas (plasma).\nThe mechanism for producing these collimated bipolar jets is not entirely understood, but it is believed that interaction between the accretion disk and the stellar magnetic field accelerates some of the accreting material from within a few astronomical units of the star away from the disk plane. At these distances the outflow is divergent, with fanning out at an angle in the range of 10−30°, but it becomes increasingly collimated at distances of tens to hundreds of astronomical units from the source, as its expansion is constrained. The jets also carry away the excess angular momentum resulting from accretion of material onto the star, which would otherwise cause the star to rotate rapidly and disintegrate. When these jets collide with the interstellar medium, they give rise to the small patches of bright emission which comprise HH objects.\n\nElectromagnetic emission from HH objects is caused when shock waves collide with the interstellar medium, creating what is called the \"terminal working surfaces\". Spectroscopic observations of their doppler shifts indicate velocities of several hundred kilometres per second, but the emission lines in those spectra are weaker than what would be expected from such high speed collisions. This suggests that some of the material they are colliding with is also moving along the beam, although at a lower speed. Spectroscopic observations of HH objects show they are moving away from the source stars at speeds of several hundred km/s. In recent years, the high optical resolution of Hubble Space Telescope has revealed the proper motion of many HH objects in observations spaced several years apart. As they move away from the parent star, HH objects evolve significantly, varying in brightness on timescales of a few years. Individual knots within an object may brighten and fade or disappear entirely, while new knots have been seen to appear. This is because of the precession of the jets and their pulsating, rather than steady, eruption from the parent stars. Faster jets catch up with earlier slower jets, creating the so-called \"internal working surfaces\", where streams of gas collide and generate shock waves and consequently emissions.\nThe total mass being ejected to form typical HH objects is estimated to be of the order of 10–10 per year, a very small amount of material compared to the mass of the stars themselves but amounts to about 1–10% of the total mass accreted in a year. Mass loss tends to decrease with increasing age of the source. The temperatures observed in HH objects are typically about 8000–12,000 K, similar to those found in other ionized nebulae such as H II regions and planetary nebulae. Densities, on the other hand, are higher than in other nebulae, ranging from a few thousand to a few tens of thousands of particles per cm, compared to generally less than 1000/cm in H II regions and planetary nebulae.\n\nDensities also decrease as the source evolves over time. HH objects consist mostly of hydrogen and helium, which account for about 75% and 24% of their mass respectively. Around 1% of the mass of HH objects is made up of heavier chemical elements, including oxygen, sulfur, nitrogen, iron, calcium and magnesium. Abundances of these elements, determined from emission lines of respective ions, are generally similar to their cosmic abundances. Many chemical compounds found in surrounding interstellar medium, but not present in the source material, such as metal hydrides, are believed to have been produced by shock induced chemical reactions.\n\nNear to the source star, about 20–30% of the gas in HH objects is ionized, but this proportion decreases at increasing distances. This implies the material is ionized in the polar jet, and recombines as it moves away from the star, rather than being ionized by later collisions. Shocking at the end of the jet can re-ionize some material, however, giving rise to bright \"caps\" at the ends of the jets.\n\nHH objects are named in order of their discovery; HH1 and HH2 being the earliest such objects to be identified. About 500 individual objects are now known. They are ubiquitous in star-forming H II regions, and are often found in large groups. They are typically observed near Bok globules (dark nebulae which contain very young stars) and often emanate from them. Several HH objects have been seen near a single energy source, forming a string of objects along the line of the polar axis of the parent star.\n\nThe number of known HH objects has increased rapidly over the last few years, but that is a very small proportion of the estimated up to 150,000 in the Milky Way, the vast majority of which are too far away to be resolved. Most HH objects lie within about one parsec of their parent star. Many, however, are seen several parsecs away.\n\nHH46/47 is located about 450 parsecs away and is powered by class I protostar binary. Bipolar jet is slamming into the surrounding medium at a velocity of 300 km/s, producing two emission caps about 2.6 parsecs apart. Jet outflow is accompanied by a 0.3 parsec long molecular gas outflow which is swept up by the jet itself. Infrared studies by Spitzer Space Telescope have revealed a variety of chemical compounds in the molecular outflow, including water (ice), methanol, methane, carbon monoxide, carbon dioxide (dry ice) and various silicates.\n\nLocated around 460 parsecs away in Orion nebula, HH34 is produced by a highly collimated bipolar jet powered by class I protostar. Matter in the jet is moving at about 220 km/s. Two bright bow shocks, separated by about 0.44 parsec, are present on the opposite sides of the source, followed by series of fainter ones at larger distances, making the whole complex about 3 parsecs long. Jet is surrounded by 0.3 parsec long weak molecular outflow near the source.\n\nThe stars from which HH jets are emitted are all very young stars, few tens of thousands to about a million years old. Youngest of these are still protostars in the process of collecting from their surrounding gases. Astronomers divide these stars into classes 0, I, II and III, according to how much infrared radiation the stars emit. A greater amount of infrared radiation implies a larger amount of cooler material surrounding the star, which indicates it is still coalescing. The numbering of the classes arises because class 0 objects (the youngest) were not discovered until classes I, II and III had already been defined.\n\nClass 0 objects are only a few thousand years old, so young that they are not yet undergoing nuclear fusion reactions at their centres. Instead, they are powered only by the gravitational potential energy released as material falls onto them. Nuclear fusion has begun in the cores of Class I objects, but gas and dust are still falling onto their surfaces from the surrounding nebula, and most of luminosity is accounted for by gravitational energy. They are generally still shrouded in dense clouds of dust and gas, which obscure all their visible light and as a result can only be observed at infrared and radio wavelengths. The in-fall of gas and dust has largely finished in Class II objects (Classical T Tauri stars), but they are still surrounded by disks of dust and gas, while class III objects (Weak-line T Tauri stars) have only trace remnants of their original accretion disk.\n\nAbout 80% of the stars giving rise to HH objects are in fact binary or multiple systems (two or more stars orbiting each other), which is a much higher proportion than that found for low mass stars on the main sequence. This may indicate that binary systems are more likely to generate the jets which give rise to HH objects, and evidence suggests the largest HH outflows might be formed when multiple star systems disintegrate. It is thought that most stars originate from multiple star systems, but that a sizable fraction are disrupted before they reach the main sequence by gravitational interactions with nearby stars and dense clouds of gas.\n\nHH objects associated with very young stars or very massive protostars are often hidden from view at optical wavelengths by the cloud of gas and dust from which they form. The intervening material can diminish the visual magnitude by factors of tens or even hundreds at optical wavelengths. Such deeply embedded objects can only be observed at infrared or radio wavelengths, usually in the frequencies of hot molecular hydrogen or warm carbon monoxide emission.\n\nIn recent years, infrared images have revealed dozens of examples of \"infrared HH objects\". Most look like bow waves (similar to the waves at the head of a ship), and so are usually referred to as molecular \"bow shocks\". The physics of infrared bow shocks can be understood in much the same way as that of HH objects, since these objects are essentially the same – supersonic shocks driven by collimated jets from the opposite poles of a protostar. It is only the conditions in the jet and surrounding cloud that are different, causing infrared emission from molecules rather than optical emission from atoms and ions.\n\nIn 2009 the acronym \"MHO\", for Molecular Hydrogen emission-line Object, was approved for such objects, detected in near infrared, by the International Astronomical Union Working Group on Designations, and has been entered into their on-line Reference Dictionary of Nomenclature of Celestial Objects. The MHO catalogue (see external links below) contains over 2000 objects.\n\n\n"}
{"id": "2840108", "url": "https://en.wikipedia.org/wiki?curid=2840108", "title": "Hurricane preparedness", "text": "Hurricane preparedness\n\nCyclone mitigation encompasses the actions and planning taken before a tropical cyclone strikes to mitigate damage and injury from the storm. Knowledge of tropical cyclone impacts on an area help plan for future possibilities. Preparedness may involve preparations made by individuals as well as centralized efforts by governments or other organizations. Tracking storms during the tropical cyclone season helps individuals know current threats. Regional Specialized Meteorological Centers and Tropical Cyclone Warning Centers provide current information and forecasts to help individuals make the best decision possible.\n\nTropical cyclones out at sea cause large waves, heavy rain, flood and high winds, disrupting international shipping and, at times, causing shipwrecks. On land, strong winds can damage or destroy vehicles, buildings, bridges, and other outside objects, turning loose debris into deadly flying projectiles. The storm surge, or the increase in sea level due to the cyclone, is typically the worst effect from landfalling tropical cyclones, historically resulting in 90% of tropical cyclone deaths.\nThe broad rotation of a landfalling tropical cyclone, and vertical wind shear at its periphery, spawns tornadoes. Tornadoes can also be spawned as a result of eyewall mesovortices, which persist until landfall.\n\nOver the past two centuries, tropical cyclones have been responsible for the deaths of about 1.9 million people worldwide. Large areas of standing water caused by flooding lead to infection, as well as contributing to mosquito-borne illnesses. Crowded evacuees in shelters increase the risk of disease propagation. Tropical cyclones significantly interrupt infrastructure, leading to power outages, bridge destruction, and the hampering of reconstruction efforts. On average, the Gulf and east coasts of the United States suffer approximately US $5 billion (1995 US $) in cyclone damage every year. The majority (83%) of tropical cyclone damage is caused by severe hurricanes, category 3 or greater. However, category 3 or greater hurricanes only account for about one-fifth of cyclones that make landfall every year.\n\nAlthough cyclones take an enormous toll in lives and personal property, they may be important factors in the precipitation regimes of places they impact, as they may bring much-needed precipitation to otherwise dry regions. Tropical cyclones also help maintain the global heat balance by moving warm, moist tropical air to the middle latitudes and polar regions, and by regulating the thermohaline circulation through upwelling. The storm surge and winds of hurricanes may be destructive to human-made structures, but they also stir up the waters of coastal estuaries, which are typically important fish breeding locales. Tropical cyclone destruction spurs redevelopment, greatly increasing local property values.\n\nWhen hurricanes surge upon shore from the ocean, salt is introduced to many freshwater areas and raises the salinity levels too high for some habitats to withstand. Some are able to cope with the salt and recycle it back into the ocean, but others can not release the extra surface water quickly enough or do not have a large enough freshwater source to replace it. Because of this, some species of plants and vegetation die due to the excess salt. In addition, hurricanes can carry toxins and acids onto shore when they make landfall. The flood water can pick up the toxins from different spills and contaminate the land that it passes over. The toxins are very harmful to the people and animals in the area, as well as the environment around them. The flooding water can also spark many dangerous oil spills.\n\nRecent windstorm activity, in the United States in particular, has focused interest in measures that can be used to lower the likelihood of damage to structures. Billions of dollars of damages have resulted from such strong winds and the manner in which structures have been built in the affected areas in the United States. Some building codes address mitigation measures. For example, the Florida Building Code, specifies the type of nail used to secure roof sheathing. The specification was determined by scientific research conducted by Florida International University's International Hurricane Research Center.\n\nThere have been many lessons learned about individual preparedness since Hurricane Katrina. The biggest responsibility was helping the children of New Orleans. Supplies were sufficient until there was more damage than the hospitals were prepared for. People were afraid that their safety was in danger due to lack of security and support at hospitals.\n\nAn important decision in individual preparedness is determining if and when to evacuate an area that will be affected by a tropical cyclone. Tropical cyclone tracking charts allow people to track ongoing systems to form their own opinions regarding where the storms are going and whether or not they need to prepare for the system being tracked, including possible evacuation. This continues to be encouraged by the National Oceanic and Atmospheric Administration and National Hurricane Center. Some agencies provide track storms in their immediate vicinity, while others cover entire ocean basins. One can choose to track one storm per map, use the map until the table is filled, or use one map per season. Some tracking charts have important contact information in case of an emergency or to locate nearby hurricane shelters. Tracking charts allow tropical cyclones to be better understood by the end user. If evacuation is not possible or necessary, other preparedness actions include storing supplies, securing a home against extreme winds and rain, and making plans with others prior to the storm's landfall.\n\nHurricane preparedness kits usually include drinkable water, sealed pre-prepared meals MRE, first-aid kits, prescription medications in sealed containers, waterproof battery-powered or hand-crank-powered flashlights and radios, a whistle or other sound-signaling device, a multi-tool with a knife, identification and medical cards, any necessary medical records, waterproof bags or portable waterproof containers, and other supplies helpful to a survival situation. If your pets will be with you make sure you include canned or dry food for them as well(any dry food included should be rotated every 2 months). You should also include veterinarian records and proof of vaccinations for all of your pets.\n\nPreparedness also may include having discussed evacuation plans and routes, and informing others of those plans before a disaster occurs.\n\nEvacuation to hurricane shelters is an option of last resort. Shelter space is first-come, first-served and only intended preserve human life. Buildings designated as shelters in Florida are required to only have been constructed to meet minimum code requirements applicable at the time of design. Some shelters are expected to protect occupants from wind and water but are not expected to provide food, water, sanitation, or bedding.\n\nBased on preparedness of the people in a region, the preparedness level was greatly affected by the salary, race, age, etc. Many people are not prepared for the worst-case scenario, but they should be. Being prepared for a huge disaster is what will ultimately save your life if worse comes to worst.\n\nHospitals are needed to be prepared in advance for huge natural disasters. Nurses are in high demand when people are in trouble. They need to be prepared for ready for any situation that enters the hospital. After hurricanes such as Katrina and Rita, nurses are needed to be fully capable of any possible scenario.\n\nAn important decision a homeowner should make is to locate the building outside of range from the coast that is exposed to storm surge. Regardless of protection from the effects of wind, a building can be flooded or destroyed by storm surge waters.\n\nTo mitigate the effects of high winds and associated debris impact, the home can be examined out by an experienced hurricane mitigation professional. Many hurricane protection companies offer free consultations as part of their marketing strategy. The State of Florida has taken steps to help its homeowners in the area of hurricane preparedness through its My Safe Florida Home grant program. Qualified homeowners receive a free in-home assessment of their home's hurricane readiness. The state matches funds spent by the homeowner, up to a $5,000 limit.\n\nWhen making these types of home improvements, there are two major areas of focus: the roof and the openings.\n\nHomes may be retrofitted to withstand the extreme conditions of a tropical cyclone. Common modifications include reinforcing gabled roofs, applying additional adhesives to roof shingles, installing hurricane straps and clips to ensure the roof stays in place despite high winds. Hurricane resistant shutters, as well as impact resistant glass may help keep windows closed from driving rain, despite flying debris.\n\nExternal patio and pool screen enclosures are especially vulnerable during a hurricane. A homeowner who anticipates sustained winds greater than 100 mph may use a razor knife taped to a telescopic pole to cut down the screens from the enclosure's skeleton structure. This will prevent the screens from acting as a sail and pulling down the entire enclosure structure.\n\nReinforcing garage doors and entry doors is also common practice for hurricane preparing a home. Garage doors may be protected by fabric screens, made of a strong woven fabric, to keep out projectiles and to reinforce the door.\n\nThe goal of these mitigation measures and products is to decrease the likelihood of severe damage to a home. There is no guarantee that these measures will safeguard any home against any kind of storm and the projectiles that may come with a storm, but a well-protected home is far more likely to come through a hurricane in better shape than a home that has little or no protection.\n\nHurricane mitigation uses policies to make buildings and other infrastructure more resistant to the effects of tropical cyclones. In addition to facilities themselves being at risk, the internal contents of the structures can be damaged as result of exposure to water if the building envelope is breached, usually as a result of the strong winds associated with hurricanes and tropical storms. Although the negative pressure caused by high velocity wind flowing over a building roof can cause the roof to fail with the building envelope intact, broken windows allow the air pressure to rise inside a building, creating an even greater pressure difference, and increasing the likelihood of roof failure. This pressure increase results after an opening, that is, a window or door, is breached after being struck and broken by wind blown debris.\n\nSee also\n\nFew codes make the use of hurricane mitigation products mandatory. As the field is rather new to construction, few standards exist to address uniform testing or product certification of product installations.\n\nThere is no single test standard or facility capable of qualifying the protection of an entire building. The Cyclone Testing Station in Australia can test building systems (e.g., roofs, exterior cladding), and indirectly test entire small buildings with structural loading to simulate wind pressures. In the United States, the test facilities at the FM Global Research Campus includes both direct wind testing of roof components to speeds up to , and roof uplift simulation using suction tables per industry standards.\n\nUniversity of Florida's team led by Forrest Masters developed \"the world's most powerful portable hurricane simulator, a giant machine capable of reproducing winds in excess of and recreating rain.\" It consists of eight large fans driven by four diesel engines, with a 5,000-gallon (19,000 litre) water tank to keep the engines cooled. The university is currently designing water-resistant windows, wind-proof tiles and altogether stronger structures.\n\nThis machine was used in the \"MythBusters\" 2009 season to test the myth that it's better to leave the windows open during a hurricane than closed (the myth was declared Busted).\n\n"}
{"id": "29766098", "url": "https://en.wikipedia.org/wiki?curid=29766098", "title": "Juan Pablo Orrego", "text": "Juan Pablo Orrego\n\nJuan Pablo Orrego is a Chilean ecologist, musician and environmentalist. He is the current president of the Ecosistemas (NGO). He is one of the most influential environmental voices in Chile, and Latin America as consequence of his important participation in campaigns against damming projects in Chile that threatened local communities and valuable eecosystems. During the 90's he was president of the activist group \"Grupo de Acción por el Biobío\", which represents the indigenous Pehuenche people in the Biobío Region. He was awarded the Goldman Environmental Prize in 1997, for his organizing of protests against the ecological damages from a series of dam building projects involving the Biobío River, including the Pangue and Ralco Hydroelectric Plants.\n\n"}
{"id": "22603189", "url": "https://en.wikipedia.org/wiki?curid=22603189", "title": "Jugyeom", "text": "Jugyeom\n\nTo make Jugyeom, sea salt is packed into bamboo canisters and sealed with yellow clay. The mixture is baked in an iron oven and roasted in a pine fire.\n\nA bamboo stem is filled with bay salt produced from the west coast, sealed with red clay, and baked in a kiln with pine tree firewood. The baked salt lumps, hardening after baking. It is taken out, crushed, and repacked in the bamboo stem for the next cycle. During baking the salt absorbs the bamboo constituents that bring a distinctive sweetness, which is called Gamrojung flavor. Baking darkens the salt. The ninth baking process uses the highest temperature, over 1,000℃. Afterwrards the bamboo salt contains blue, yellow, red, white and black.\n\nWell-baked bamboo salt, with a temperature above 1,500℃, is called “Purple bamboo salt” because of its unique purple color, which indicates the best quality. While the quality of bamboo salt cannot be solely determined by color, its crystal structure and hardiness is definitive.\n\nIn Korean folk medicine, trace elements in the yellow clay and bamboo are thought to make this form of salt more healthy. Historically, has been used as a digestive aid, styptic, disinfectant or dentifrice.\n\nStudies have reported \"in vitro\" and \"in vivo\" anti-cancer effects.\n\nA study published in \"Experimental and Therapeutic Medicine\" suggests that Purple Bamboo Salt may prevent the growth of oral cancers in mice.\n\nAccording to \"The Universe and God's Medicine\" by Il-hoon (In-san) Kim in 1981., can be used to treat:\n\n\n\nIn the 2012 film \"Masquerade\", bamboo salt caused a silver spoon in a bowl of soup to turn black, but before this explanation was discovered, the event caused the King to believe people were trying to poison him.\n\n"}
{"id": "20646679", "url": "https://en.wikipedia.org/wiki?curid=20646679", "title": "Lightning rod", "text": "Lightning rod\n\nA lightning rod (US, AUS) or lightning conductor (UK) is a metal rod mounted on a structure and intended to protect the structure from a lightning strike. If lightning hits the structure, it will preferentially strike the rod and be conducted to ground through a wire, instead of passing through the structure, where it could start a fire or cause electrocution. Lightning rods are also called finials, air terminals or strike termination devices.\n\nIn a lightning protection system, a lightning rod is a single component of the system. The lightning rod requires a connection to earth to perform its protective function. Lightning rods come in many different forms, including hollow, solid, pointed, rounded, flat strips or even bristle brush-like. The main attribute common to all lightning rods is that they are all made of conductive materials, such as copper and aluminum. Copper and its alloys are the most common materials used in lightning protection.\n\nThe principle of the lightning rod was first detailed by Benjamin Franklin in Pennsylvania in 1749, who in the subsequent years developed his invention for household application (published in 1753) and further improvements towards a reliable system around 1760.\n\nAs buildings become taller, lightning becomes more of a threat. Lightning can damage structures made of most materials, such as masonry, wood, concrete and steel, because the huge currents and voltages involved can heat materials to high temperature, causing a potential for fire.\n\nA lightning conductor may have been intentionally used in the Leaning Tower of Nevyansk. The spire of the tower is crowned with a metallic rod in the shape of a gilded sphere with spikes. This lightning rod is grounded through the rebar carcass, which pierces the entire building.\n\nThe Nevyansk Tower was built between 1721 and 1745, on the orders of industrialist Akinfiy Demidov. The Nevyansk Tower was built 28 years before Benjamin Franklin's experiment and scientific explanation. However, the true intent behind the metal rooftop and rebars remains unknown.\n\nThe church tower of many European cities, which was usually the highest structure in the city, was likely to be hit by lightning. Early on, Christian churches tried to prevent the occurrence of the damaging effects of lightning by prayers. Peter Ahlwardts (\"Reasonable and Theological Considerations about Thunder and Lightning\", 1745) advised individuals seeking cover from lightning to go anywhere except in or around a church.\n\nThere is an ongoing debate over whether a \"metereological machine\", invented by Premonstratensian priest Prokop Diviš and erected in Přímětice near Znojmo, Moravia (now Czech Republic) in June 1754, does count as an individual invention of the lightning rod. Diviš's apparatus was, according to his private theories, aimed towards preventing thunderstorms altogether by constantly depriving the air of its superfluous electricity. The apparatus was, however, mounted on a free-standing pole and probably better grounded than Franklin's lightning rods at that time, so it served the purpose of a lightning rod. After local protests, Diviš had to cease his weather experiments around 1760.\n\nIn what later became the United States, the pointed lightning rod conductor, also called a lightning attractor or Franklin rod, was invented by Benjamin Franklin in 1749 as part of his groundbreaking exploration of electricity. Although not the first to suggest a correlation between electricity and lightning, Franklin was the first to propose a workable system for testing his hypothesis. Franklin speculated that, with an iron rod sharpened to a point,\n\nFranklin speculated about lightning rods for several years before his reported kite experiment. This experiment, it is said, took place because he was tired of waiting for Christ Church in Philadelphia to be completed so he could place a lightning rod on top of it.\n\nIn the 19th century, the lightning rod became a decorative motif. Lightning rods were embellished with ornamental glass balls (now prized by collectors). The ornamental appeal of these glass balls has been used in weather vanes. The main purpose of these balls, however, is to provide evidence of a lightning strike by shattering or falling off. If after a storm a ball is discovered missing or broken, the property owner should then check the building, rod, and grounding wire for damage.\n\nBalls of solid glass occasionally were used in a method purported to prevent lightning strikes to ships and other objects. The idea was that glass objects, being non-conductors, are seldom struck by lightning. Therefore, goes the theory, there must be something about glass that repels lightning. Hence the best method for preventing a lightning strike to a wooden ship was to bury a small solid glass ball in the tip of the highest mast. The random behavior of lightning combined with observers' confirmation bias ensured that the method gained a good bit of credence even after the development of the marine lightning rod soon after Franklin's initial work.\n\nThe first lightning conductors on ships were supposed to be hoisted when lightning was anticipated, and had a low success rate. In 1820 William Snow Harris invented a successful system for fitting lightning protection to the wooden sailing ships of the day, but despite successful trials which began in 1830, the British Royal Navy did not adopt the system until 1842, by which time the Imperial Russian Navy had already adopted the system.\n\nIn the 1990s, the 'lightning points' were replaced as originally constructed when the Statue of Freedom atop the United States Capitol building in Washington, D.C. was restored. The statue was designed with multiple devices that are tipped with platinum. The Washington Monument also was equipped with multiple lightning points, and the Statue of Liberty in New York Harbor gets hit by lightning, which is shunted to ground.\n\nA \"lightning protection system\" is designed to protect a structure from damage due to lightning strikes by intercepting such strikes and safely passing their extremely high currents to ground. A lightning protection system includes a network of air terminals, bonding conductors, and ground electrodes designed to provide a low impedance path to ground for potential strikes.\n\nLightning protection systems are used to prevent or lessen lightning strike damage to structures. Lightning protection systems mitigate the fire hazard which lightning strikes pose to structures. A lightning protection system provides a low-impedance path for the lightning current to lessen the heating effect of current flowing through flammable structural materials. If lightning travels through porous and water-saturated materials, these materials may literally explode if their water content is flashed to steam by heat produced from the high current. This is why trees are often shattered by lightning strikes.\n\nBecause of the high energy and current levels associated with lightning (currents can be in excess of 150,000 amps), and the very rapid rise time of a lightning strike, no protection system can guarantee absolute safety from lightning. Lightning current will divide to follow every conductive path to ground, and even the divided current can cause damage. Secondary \"side-flashes\" can be enough to ignite a fire, blow apart brick, stone, or concrete, or injure occupants within a structure or building. However, the benefits of basic lightning protection systems have been evident for well over a century.\n\nLaboratory-scale measurements of the effects of [any lightning investigation research] do not scale to applications involving natural lightning. Field applications have mainly been derived from trial and error based on the best intended laboratory research of a highly complex and variable phenomenon.\n\nThe parts of a lightning protection system are air terminals (lightning rods or strike termination devices), bonding conductors, ground terminals (ground or \"earthing\" rods, plates, or mesh), and all of the connectors and supports to complete the system. The air terminals are typically arranged at or along the upper points of a roof structure, and are electrically bonded together by bonding conductors (called \"down conductors\" or \"downleads\"), which are connected by the most direct route to one or more grounding or earthing terminals. Connections to the earth electrodes must not only have low resistance, but must have low self-inductance.\n\nAn example of a structure vulnerable to lightning is a wooden barn. When lightning strikes the barn, the wooden structure and its contents may be ignited by the heat generated by lightning current conducted through parts of the structure. A basic lightning protection system would provide a conductive path between an air terminal and earth, so that most of the lightning's current will follow the path of the lightning protection system, with substantially less current traveling through flammable materials.\n\nA controversy over the assortment of operation theories dates back to the 18th century, when Benjamin Franklin himself stated that his lightning protectors protected buildings by dissipating electric charge. He later retracted the statement, stating that the device's exact mode of operation was something of a mystery at that point.\n\nOriginally, scientists believed that such a lightning protection system of air terminals and \"downleads\" directed the current of the lightning down into the earth to be \"dissipated\". However, high speed photography has clearly demonstrated that lightning is actually composed of both a cloud component and an oppositely charged ground component. During \"cloud-to-ground\" lightning, these oppositely charged components usually \"meet\" somewhere in the atmosphere well above the earth to equalize previously unbalanced charges. The heat generated as this electric current flows through flammable materials is the hazard which lightning protection systems attempt to mitigate by providing a low-resistance path for the lightning circuit. No lightning protection system can be relied upon to \"contain\" or \"control\" lightning completely (nor thus far, to prevent lightning strikes entirely), but they do seem to help immensely on most occasions of lightning strikes.\n\nSteel framed structures can bond the structural members to earth to provide lightning protection. A metal flagpole with its foundation in the earth is its own extremely simple lightning protection system. However, the flag(s) flying from the pole during a lightning strike may be completely incinerated.\n\nThe majority of lightning protection systems in use today are of the traditional Franklin design. The fundamental principle used in Franklin-type lightning protections systems is to provide a sufficiently low impedance path for the lightning to travel through to reach ground without damaging the building. This is accomplished by surrounding the building in a kind of Faraday cage. A system of lightning protection conductors and lightning rods are installed on the roof of the building to intercept any lightning before it strikes the building.\n\nIn telegraphy and telephony, a lightning arrester is a device placed where wires enter a structure, in order to prevent damage to electronic instruments within and ensuring the safety of individuals near the structures. Lightning arresters, also called surge protectors, are devices that are connected between each electrical conductor in a power or communications system, and the ground. They help prevent the flow of the normal power or signal currents to ground, but provide a path over which high-voltage lightning current flows, bypassing the connected equipment. Arresters are used to limit the rise in voltage when a communications or power line is struck by lightning or is near to a lightning strike.\n\nIn overhead electric transmission (high-tension) systems, one or two lighter gauge conductors may be mounted to the top of the pylons, poles, or towers not specifically used to send electricity through the grid. These conductors, often referred to \"static\", \"pilot\" or \"shield\" wires are designed to be the point of lightning termination instead of the high-voltage lines themselves. These conductors are intended to protect the primary power conductors from lightning strikes.\n\nThese conductors are bonded to earth either through the metal structure of a pole or tower, or by additional ground electrodes installed at regular intervals along the line. As a general rule, overhead power lines with voltages below 50 kV do not have a \"static\" conductor, but most lines carrying more than 50 kV do. The ground conductor cable may also support fibre optic cables for data transmission.\n\nIn some instances, these conductors are insulated from direct bonding with earth and may be used as low voltage communication lines. If the voltage exceeds a certain threshold, such as during a lightning termination to the conductor, it \"jumps\" the insulators and passes to earth.\n\nProtection of electrical substations is as varied as lightning rods themselves, and is often proprietary to the electric company.\n\nRadio mast radiators may be insulated from the ground by a gap at the base. When lightning hits the mast, it jumps this gap. A small inductivity in the feed line between the mast and the tuning unit (usually one winding) limits the voltage increase, protecting the transmitter from dangerously high voltages.\nThe transmitter must be equipped with a device to monitor the antenna's electrical properties. This is very important, as a charge could remain after a lightning strike, damaging the gap or the insulators.\nThe monitoring device switches off the transmitter when the antenna shows incorrect behavior, e.g. as a result of undesired electrical charge. When the transmitter is switched off, these charges dissipate. The monitoring device makes several attempts to switch back on. If after several attempts the antenna continues to show improper behavior, possibly as result of structural damage, the transmitter remains switched off.\n\nIdeally, the underground part of the assembly should reside in an area of high ground conductivity. If the underground cable is able to resist corrosion well, it can be covered in salt to improve its electrical connection with the ground. While the electrical resistance of the lightning conductor between the air terminal and the Earth is of significant concern, the inductive reactance of the conductor could be more important. For this reason, the down conductor route is kept short, and any curves have a large radius. If these measures are not taken, lightning current may arc over a resistive or reactive obstruction that it encounters in the conductor. At the very least, the arc current will damage the lightning conductor and can easily find another conductive path, such as building wiring or plumbing, and cause fires or other disasters. Grounding systems without low resistivity to the ground can still be effective in protecting a structure from lightning damage. When ground soil has poor conductivity, is very shallow, or non-existent, a grounding system can be augmented by adding ground rods, counterpoise (ground ring) conductor, cable radials projecting away from the building, or a concrete building's reinforcing bars can be used for a ground conductor (Ufer ground). These additions, while still not reducing the resistance of the system in some instances, will allow the [dispersion] of the lightning into the earth without damage to the structure.\n\nAdditional precautions must be taken to prevent side-flashes between conductive objects on or in the structure and the lightning protection system. The surge of lightning current through a lightning protection conductor will create a voltage difference between it and any conductive objects that are near it. This voltage difference can be large enough to cause a dangerous side-flash (spark) between the two that can cause significant damage, especially on structures housing flammable or explosive materials. The most effective way to prevent this potential damage is to ensure the electrical continuity between the lightning protection system and any objects susceptible to a side-flash. Effective bonding will allow the voltage potential of the two objects to rise and fall simultaneously, thereby eliminating any risk of a side-flash.\n\nConsiderable material is used to make up lightning protection systems, so it is prudent to consider carefully where an air terminal will provide the greatest protection. Historical understanding of lightning, from statements made by Ben Franklin, assumed that each \"lightning rod\" protected a cone of 45 degrees. This has been found to be unsatisfactory for protecting taller structures, as it is possible for lightning to strike the side of a building.\n\nA modeling system based on a better understanding of the termination targeting of lightning, called the Rolling Sphere Method, was developed by Dr Tibor Horváth. It has become the standard by which traditional Franklin Rod systems are installed. To understand this requires knowledge of how lightning 'moves'. As the step leader of a lightning bolt jumps toward the ground, it steps toward the grounded objects nearest its path. The maximum distance that each step may travel is called the \"critical distance\" and is proportional to the electric current. Objects are likely to be struck if they are nearer to the leader than this critical distance. It is standard practice to approximate the sphere's radius as 46 m near the ground.\n\nAn object outside the critical distance is unlikely to be struck by the leader if there is a solidly grounded object within the critical distance. Locations that are considered safe from lightning can be determined by imagining a leader's potential paths as a sphere that travels from the cloud to the ground. For lightning protection, it suffices to consider all possible spheres as they touch potential strike points. To determine strike points, consider a sphere rolling over the terrain. At each point, a potential leader position is simulated. Lightning is most likely to strike where the sphere touches the ground. Points that the sphere cannot roll across and touch are safest from lightning. Lightning protectors should be placed where they will prevent the sphere from touching a structure. A weak point in most lightning diversion systems is in transporting the captured discharge from the lightning rod to the ground, though. Lightning rods are typically installed around the perimeter of flat roofs, or along the peaks of sloped roofs at intervals of 6.1 m or 7.6 m, depending on the height of the rod. When a flat roof has dimensions greater than 15 m by 15 m, additional air terminals will be installed in the middle of the roof at intervals of 15 m or less in a rectangular grid pattern.\n\nThe optimal shape for the tip of a lightning rod has been controversial since the 18th century. During the period of political confrontation between Britain and its American colonies, British scientists maintained that a lightning rod should have a ball on its end, while American scientists maintained that there should be a point. , the controversy had not been completely resolved.\nIt is difficult to resolve the controversy because proper controlled experiments are nearly impossible, but work performed by Charles B. Moore, et al., in 2000 has shed some light on the issue, finding that moderately rounded or blunt-tipped lightning rods act as marginally better strike receptors. As a result, round-tipped rods are installed on most new systems in the United States, though most existing systems still have pointed rods. According to the study,\n\nIn addition, the height of the lightning protector relative to the structure to be protected and the Earth itself will have an effect.\n\nThe charge transfer theory states that a lightning strike to a protected structure can be prevented by reducing the electrical potential between the protected structure and the thundercloud. This is done by transferring electric charge (such as from the nearby Earth to the sky or vice versa). Transferring electric charge from the Earth to the sky is done by installing engineered products composed of many points above the structure. It is noted that pointed objects will indeed transfer charge to the surrounding atmosphere and that a considerable electric current can be measured through the conductors as ionization occurs at the point when an electric field is present, such as happens when thunderclouds are overhead.\n\nIn the United States, the National Fire Protection Association (NFPA) does not currently endorse a device that can prevent or reduce lightning strikes. The NFPA Standards Council, following a request for a project to address Dissipation Array[tm] Systems and Charge Transfer Systems, denied the request to begin forming standards on such technology (though the Council did not foreclose on future standards development after reliable sources demonstrating the validity of the basic technology and science were submitted).\n\nThe theory of early streamer emission proposes that if a lightning rod has a mechanism producing ionization near its tip, then its lightning capture area is greatly increased. At first, small quantities of radioactive isotopes (radium-226 or americium-241) were used as sources of ionization between 1930 and 1980, later replaced with various electrical and electronic devices. According to an early patent, since most lightning protectors' ground potentials are elevated, the path distance from the source to the elevated ground point will be shorter, creating a stronger field (measured in volts per unit distance) and that structure will be more prone to ionization and breakdown.\n\nAFNOR, the French national standardization body, issued a standard, NF C 17-102, covering this technology. The NFPA also investigated the subject and there was a proposal to issue a similar standard in the USA. Initially, an NFPA independent third party panel stated that \"the [Early Streamer Emission] lightning protection technology appears to be technically sound\" and that there was an \"adequate theoretical basis for the [Early Streamer Emission] air terminal concept and design from a physical viewpoint\".) The same panel also concluded that \"the recommended [NFPA 781 standard] lightning protection system has never been scientifically or technically validated and the Franklin rod air terminals have not been validated in field tests under thunderstorm conditions.\"\n\nIn response, the American Geophysical Union concluded that \"[t]he Bryan Panel reviewed essentially none of the studies and literature on the effectiveness and scientific basis of traditional lightning protection systems and was erroneous in its conclusion that there was no basis for the Standard.\" AGU did not attempt to assess the effectiveness of any proposed modifications to traditional systems in its report. The NFPA withdrew its proposed draft edition of standard 781 due to a lack of evidence of increased effectiveness of Early Streamer Emission-based protection systems over conventional air terminals.\n\nMembers of the Scientific Committee of the International Conference on Lightning Protection (ICLP) have issued a joint statement stating their opposition to Early Streamer Emission technology. ICLP maintains a web page with information related to ESE and related technologies. Still, the number of buildings and structures equipped with ESE lightning protection systems is growing as well as the number of manufacturers of ESE air terminals from Europe, Americas, Middle East, Russia, China, South Korea, ASEAN countries, and Australia.\n\nLightning strikes to a metallic structure can vary from leaving no evidence—except, perhaps, a small pit in the metal—to the complete destruction of the structure. When there is no evidence, analyzing the strikes is difficult. This means that a strike on an uninstrumented structure must be visually confirmed, and the random behavior of lightning renders such observations difficult. There are also inventors working on this problem, such as through a lightning rocket. While controlled experiments may be off in the future, very good data is being obtained through techniques which use radio receivers that watch for the characteristic electrical 'signature' of lightning strikes using fixed directional antennas. Through accurate timing and triangulation techniques, lightning strikes can be located with great precision, so strikes on specific objects often can be confirmed with confidence.\n\nThe energy in a lightning strike is typically in the range of 1 to 10 billion joules. This energy is released usually in a small number of separate strokes, each with duration of a few tens of microseconds (typically 30 to 50 microseconds), over a period of about one fifth of a second. The great majority of the energy is dissipated as heat, light and sound in the atmosphere.\n\nLightning protection for aircraft is provided by mounting devices on the aircraft structure. The protectors are provided with extensions through the structure of the aircraft's outer surface and within a static discharger. Protection systems for use in aircraft must protect critical and non-critical electronic equipment. Aircraft lightning protection provides an electrical path having a plurality of conductive segments, continuous or discontinuous, that upon exposure to a high voltage field form an ionization channel due to the system's breakdown voltage. Various lightning protection systems must reject the surge currents associated with the lightning strikes. Lightning protection means for aircraft include components which are dielectrics and metallic layers applied to the ordinarily lightning-accessible surfaces of composite structures. Various ground connection means to the layers comprises a section of wire mesh fusing the various layers to an attachment connecting the structure to an adjacent ground structure. Composite-to-metal or composite-to-composite structural joints are protected by making the interface areas conductive for transfer of lightning current.\n\nSome aircraft lightning protection systems use a shielded cable system. These systems consist of one or more conductors enclosed by a conductive shield. The cable has both conductors of one end connected to a grounding element. This is intended to provide protection from electromagnetic interference. Such systems reduce the electromagnetically induced voltage in a shielded conductor. This is intended to provide protection against induced electromagnetic interference from lightning. This network provides a normally-high impedance which breaks down to a very low impedance in response to a momentary voltage surge electromagnetically induced in the shield. This establishes a conductive path between the shield and ground. Any surge voltage from lightning creates a current through the cable. This results in an electromagnetic field of the opposite direction, which cancels or reduces the magnitude of the electromagnetic field within the shielded cable.\n\nA lightning protection installation on a watercraft comprises a \"lightning protector\" mounted on the top of a mast or superstructure, and a \"grounding conductor\" in contact with the water. Electrical conductors attach to the protector and run down to the conductor. For a vessel with a conducting (iron or steel) hull, the grounding conductor is the hull. For a vessel with a non-conducting hull, the grounding conductor may be retractable, attached to the hull, or attached to a centerboard.\n\nSome structures are inherently more or less at risk of being struck by lightning. The risk for a structure is a function of the size (area) of a structure, the height, and the number of lightning strikes per year per mi² for the region. For example, a small building will be less likely to be struck than a large one, and a building in an area with a high density of lightning strikes will be more likely to be struck than one in an area with a low density of lightning strikes. The National Fire Protection Association provides a risk assessment worksheet in their lightning protection standard.\n\nThe International Electrotechnical Commission (IEC) lightning risk-assessment comprises four parts: loss of living beings, loss of service to public, loss of cultural heritage, and loss of economic value. Loss of living beings is rated as the most important and is the only loss taken into consideration for many nonessential industrial and commercial applications.\n\nThe introduction of lightning protection systems into standards allowed various manufactures to develop protector systems to a multitude of specifications. There are multiple international, national, corporate and military lightning protection standards.\n\n\n"}
{"id": "40141556", "url": "https://en.wikipedia.org/wiki?curid=40141556", "title": "List of films about renewable energy", "text": "List of films about renewable energy\n\nThis is a list of films about renewable and alternative energy.\n\n\n\nStarkiller base in Star Wars VII uses solar power"}
{"id": "35408416", "url": "https://en.wikipedia.org/wiki?curid=35408416", "title": "Lorentz force velocimetry", "text": "Lorentz force velocimetry\n\nLorentz force velocimetry (LFV) is a noncontact electromagnetic flow measurement technique. LFV is particularly suited for the measurement of velocities in liquid metals like steel or aluminium and is currently under development for metallurgical applications.The measurement of flow velocities in hot and aggressive liquids such as liquid aluminium and molten glass constitutes one of the grand challenges of industrial fluid mechanics. Apart from liquids, LFV can also be used to measure the velocity of solid materials as well as for detection of micro-defects in their structures.\n\nA Lorentz force velocimetry system is called Lorentz force flowmeter (LFF). A LFF measures the integrated or bulk Lorentz force resulting from the interaction between a liquid metal in motion and an applied magnetic field. In this case the characteristic length of the magnetic field is of the same order of magnitude as the dimensions of the channel. It must be addressed that in the case where localized magnetic fields are used, it is possible to perform local velocity measurements and thus the term Lorentz force velocimeter is used.\n\nThe use of magnetic fields in flow measurement date back to 19th century, when in 1832 Michael Faraday attempted to determine the velocity of the River Thames. Faraday applied a method in which a flow (the river flow) is exposed to a magnetic field (earth magnetic field) and the induced voltage is measured using two electrodes across the same flow. This method is the basis of the one of the most successful commercial applications in flow metering known as the inductive flowmeter. The theory of such devices has been developed and comprehensively summarized by Prof. J. A. Shercliff in early 1950s. While inductive flowmeters are widely used for flow measurement in fluids at room temperatures such as beverages, chemicals and waste water, they are not suited for flow measurement of media such as hot, aggressive or for local measurements where surrounding obstacles limit access to the channel or pipe. Since they require electrodes to be inserted into the fluid, their use is limited to applications at temperatures far below the melting points of practically relevant metals.\n\nThe Lorentz force velocimetry was invented by the A. Shercliff. However, it did not find practical application in these early years up until recent technical advances; in manufacturing of rare earth and non rare-earth strong permanent magnets, accurate force measurement techniques, multiphysical process simulation software for magnetohydrodynamic (MHD) problems that this principle could be turned into a feasible working flow measurement technique. LFV is currently being developed for applications in metallurgy as well as in other areas.\n\nBased on theory introduced by Shercliff there have been several attempts to develop flow measurement methods which do not require any mechanical contact with the fluid. Among them is the eddy current flowmeter which measures flow-induced changes in the electric impedance of coils interacting with the flow. More recently, a non-contact method was proposed in which a magnetic field is applied to the flow and the velocity is determined from measurements of flow-induced deformations of the applied magnetic field.\n\nThe principle of Lorentz force velocimetry is based on measurements of the Lorentz force that occurs due to the flow of a conductive fluid under the influence of a variable magnetic field. According to Faraday's law, when a metal or conductive fluid moves through a magnetic field, eddy currents generate there by electromotive force in zones of maximal magnetic field gradient (in the present case in the inlet and outlet zones). Eddy current in its turn creates induced magnetic field according to Ampère's law. The interaction between eddy currents and total magnetic field gives rise to Lorentz force that breaks the flow. By virtue of Newton's third law \"actio=reactio\" a force with the same magnitude but opposite direction acts upon its source - permanent magnet. Direct measurement of the magnet's reaction force allows to determine fluid's velocity, since this force is proportional to flow rate. The Lorentz force used in LFV has nothing to do with magnetic attraction or repulsion. It is only due to the eddy currents whose strength depends on the electrical conductivity, the relative velocity between the liquid and the permanent magnet as well as the magnitude of the magnetic field.\n\nSo, when a liquid metal moves across magnetic field lines, the interaction of the magnetic field (which are either produced by a current-carrying coil or by a permanent magnet) with the induced eddy currents leads to a Lorentz force (with density formula_1) which brakes the flow. The Lorentz force density is roughly\n\nwhere formula_3 is the electrical conductivity of the fluid, formula_4 its velocity, and formula_5 the magnitude of the magnetic field. This fact is well known and has found a variety of applications. This force is proportional to the velocity and conductivity of the fluid, and its measurement is the key idea of LFV. With the recent advent of powerful rare earth permanent magnets (like NdFeB, SmCo and other kind of magnets) and tools for designing sophisticated systems by permanent magnet the practical realization of this principle has now become possible.\n\nThe primary magnetic field formula_6 can be produced by a permanent magnet or a primary current formula_7 (see Fig. 1). The motion of the fluid under the action of the primary field induces eddy currents which are sketched in figure 3. They will be denoted by formula_8 and are called secondary currents. The interaction of the secondary current with the primary magnetic field is responsible for the Lorentz force within the fluid\nwhich breaks the flow.\n\nThe secondary currents create a magnetic field formula_10, the secondary magnetic field. The interaction of the primary electric current with the secondary magnetic field gives rise to the Lorentz force on the magnet system\nformula_11\nThe reciprocity principle for the Lorentz force velocimetry states that the electromagnetic forces on the fluid and on the magnet system have the same magnitude and act in opposite direction, namely\nThe general scaling law that relates the measured force to the unknown velocity can be derived with reference to the simplified situation shown in Fig. 2. Here a small permanent magnet with dipole moment formula_13 is located at a distance formula_14 above a semi-infinite fluid moving with uniform velocity formula_4 parallel to its free surface.\nThe analysis that leads to the scaling relation can be made quantitative by assuming that the magnet is a point dipole with dipole moment formula_16 whose magnetic field is given by\nwhere formula_18 and formula_19. Assuming a velocity field formula_20 for formula_21, the eddy currents can be computed from Ohm's law for a moving electrically conducting fluid\nsubject to the boundary conditions formula_23 at formula_24 and formula_25 as formula_26. First, the scalar electric potential is obtained as\nfrom which the electric current density is readily calculated. They are indeed horizontal. Once they are known, the Biot–Savart law can be used to compute the secondary magnetic field formula_28. Finally, the force is given by\nwhere the gradient of formula_30 has to be evaluated at the location of the dipole. For the problem at hand all these steps can be carried out analytically without any approximation leading to the result\nThis provides us with the estimate\n\nLorentz force flowmeters are usually classified in several main conceptual setups. Some of them designed as static flowmeters where the magnet system is at rest and one measures the force acting on it. Alternatively, they can be designed as rotary flowmeters where the magnets are arranged on a rotating wheel and the spinning velocity is a measure of the flow velocity. Obviously, the force acting on a Lorentz force flowmeter depends both on the velocity distribution and on the shape of the magnet system. This classification depends on the relative direction of the magnetic field that is being applied respect to the direction of the flow. In Figure 3 one can distinguish diagrams of the longitudinal and the transverse Lorentz force flowmeters.\n\nIt is important to mention that even that in figures only a coil or a magnet are sketched, the principle holds for both.\n\nRotary LFF consists of a freely rotating permanent magnet (or an array of magnets mounted on a flywheel as shown in figure 4), which is magnetized perpendicularly to the axle it is mounted on. When such a system is placed close to a duct carrying an electrically conducting fluid flow, it rotates so that the driving torque due to the eddy currents induced by the flow is balanced by the braking torque induced by the rotation itself. The equilibrium rotation rate varies directly with the flow velocity and inversely with the distance between the magnet and the duct. In this case it is possible to measure either the torque on the magnet system or the angular velocity at which the wheel spins.\n\nLFV is sought to be extended to all fluid or solid materials, providing that they are electrically conductors. As shown before, the Lorentz force generated by the flow depend linearly on the conductivity of the fluid. Typically, the electrical conductivity of molten metals is of the order of formula_33 so the Lorentz force is in the range of some \"mN\". However, equally important liquids as glass melts and electrolytic solutions have a conductivity of formula_34 giving rise to a Lorentz force of the order of micronewtons or even smaller.\n\nAmong different possibilities to measure the effect on the magnet system, it has been successfully applied those based on the measurement of the deflection of a parallel spring under an applied force. Firstly using a strain gauge and then recording the deflection of a quartz spring with an interferometer, in whose case the deformation is detected to within 0.1 nm.\n\nRecent advance in LFV made it possible for metering flow velocity of media which has very low electroconductivity, particularly by varying parameters as well as using some state-of-art force measurement devices enable to measure flow velocity of electrolyte solutions with conductivity that is 10 times smaller than that for the liquid metals. There are variety of industrial and scientific applications where noncontact flow measurement through opaque walls or in opaque liquids is desirable. Such applications include flow metering of chemicals, food, beverages, blood, aqueous solutions in the pharmaceutical industry, molten salts in solar thermal power plants, and high temperature reactors as well as glass melts for high-precision optics.\n\nA noncontact flowmeter is a device that is neither in mechanical contact with the liquid nor with the wall of the pipe in which the liquid flows. Noncontact flowmeters are equally useful when walls are contaminated like in the processing of radioactive materials, when pipes are strongly vibrating or in cases when portable flowmeters are to be developed. If the liquid and the wall of the pipe are transparent and the liquid contains tracer particles, optical measurement techniques, are effective enough tool to perform noncontact measurements. However, if either the wall or the liquid are opaque as is often the case in food production, chemical engineering, glass making, and metallurgy, very few possibilities for noncontact flow measurement exist.\n\nThe force measurement system is an important part of the Lorentz force velocimetry. With high resolution force measurement system makes the measurement of even lower conductivity possible.Up to date has the force measurement system continually being developed. At first the pendulum-like setups was used (Figure 5). One of the experimental facilities consists of two high power (410 mT) magnets made of NdFeB suspended by thin wires on both side of channel thereby creating magnetic field perpendicular to the fluid flow, here deflection is measured by interferometer system. The second setup consists of state-of-art weighting balance system (Figure 6) from which is being hanged optimized magnets on the base of Halbach array system. While the total mass of both magnet systems are equal (1 kg), this system induces 3 times higher system response due to arrangement of individual elements in the array and it's interaction with predefined fluid profile. Here use of very sensitive force measuring devices is desirable, since flow velocity is being converted from the very tiny detected Lorentz Force. This force in combination with unavoidable dead weight formula_35 of the magnet (formula_36) is around formula_37. After that, the method of differential force measurement was developed. With this method two balance were used, one with magnet and the other is with same-weight-dummy. In this way the influence of environment would be reduced. Recently, it have been reported that the flow measurements by this method is possible for saltwater flows whose electrical conductivity is as small as 0.06 S/m (range of electrical conductivity of the regular water from tap).\n\nLorentz force sigmometry (LOFOS) is a contactless method for measuring the thermophysical properties of materials, no matter whether it is a fluid or a solid body. The precise measurements of electrical value, density, viscosity, thermal conductivity and surface tension of molten metals are in great importance in industry applications. One of the major problems in the experimental measurements of the thermophysical properties at high temperature (>1000 K) in the liquid state is the problem of chemical reaction between the hot fluid and the electrical probes.\nThe basic equation for calculating the electrical conductivity is derived from the equation that links the mass flow rate formula_38 and Lorentz force formula_39 generated by magnetic field in flow:\n\nwhere formula_41 is the specific electrical conductivity equals to the ratio of the electrical conductivity formula_3 and the mass density of fluid formula_43. formula_44 is a calibration factor that depends on the geometry of the LOFOS system.\n\nFrom equation above the cumulative mass during operating time is determined as\n\nwhere formula_46 is the integral of Lorentz force within the time process. From this equation and considering the specific electrical conductivity formula, one can derive the final equation to compute the electrical conductivity for the fluid, in the form\n\nTime-of-flight Lorentz force velocimetry, is intended for contactless determination of flow rate in conductive fluids. It can be successfully used even in case when such material properties as electrical conductivity or density are not precisely known under specific outer conditions. The last reason makes time-of-flight LFV especially important for industry application. According to time-of-flight LFV (Fig. 9) two coherent measurement systems are mounted on a channel one by one. The measurement is based on getting of cross-correlating function of signals, which are registered by two magnetic measurement's system. Every system consists of permanent magnet and force sensor, so inducing of Lorentz force and measurement of the reaction force are made simultaneously. Any cross-correlation function is useful only in case of qualitative difference between signals and for creating the difference in this case turbulent fluctuations are used. Before reaching of measurement zone of a channel liquid passes artificial vortex generator that induces strong disturbances in it. And when such fluctuation-vortex reaches magnetic field of measurement system we can observe a peak on its force-time characteristic while second system still measures stable flow. Then according to the time between peaks and the distance between measurement system observer can estimate mean velocity and, hence, flow rate of the liquid by equation:\n\nwhere formula_49 is the distance between magnet system, formula_50 the time delay between recorded peaks, and formula_51 is obtained experimentally for every specific liquid, as shown in figure 9.\n\nA different, albeit physically closely related challenge is the detection of deeply lying flaws and inhomogeneities in electrically conducting solid materials.\n\nIn the traditional version of eddy current testing an alternating (AC) magnetic field is used to induce eddy currents inside the material to be investigated. If the material contains a crack or flaw which make the spatial distribution of the electrical conductivity nonuniform, the path of the eddy currents is perturbed and the impedance of the coil which generates the AC magnetic field is modified. By measuring the impedance of this coil, a crack can hence be detected. Since the eddy currents are generated by an AC magnetic field, their penetration into the subsurface region of the material is limited by the skin effect. The applicability of the traditional version of eddy current testing is therefore limited to the analysis of the immediate vicinity of the surface of a material, usually of the order of one millimeter. Attempts to overcome this fundamental limitation using low frequency coils and superconducting magnetic field sensors have not led to widespread applications.\n\nA recent technique, referred to as Lorentz force eddy current testing (LET), exploits the advantages of applying DC magnetic fields and relative motion providing deep and relatively fast testing of electrically conducting materials. In principle, LET represents a modification of the traditional eddy current testing from which it differs in two aspects, namely (i) how eddy currents are induced and (ii) how their perturbation is detected. In LET eddy currents are generated by providing the relative motion between the conductor under test and a permanent magnet (see figure 10). If the magnet is passing by a defect, the Lorentz force acting on it shows a distortion whose detection is the key for the LET working principle. If the object is free of defects, the resulting Lorentz force remains constant.\n\nThe advantages of LFV are\n\nThe limitations of the LFV are\n\n\n"}
{"id": "2203151", "url": "https://en.wikipedia.org/wiki?curid=2203151", "title": "Madison Symmetric Torus", "text": "Madison Symmetric Torus\n\nThe Madison Symmetric Torus (MST) is a reversed field pinch (RFP) physics experiment with applications to both fusion energy research and astrophysical plasmas located at University of Wisconsin-Madison. RFPs are significantly different from tokamaks (the most popular magnetic confinement scheme) in that they tend to have a higher power density and better confinement characteristics for a given average magnetic field. RFPs also tend to be dominated by non-ideal phenomena and turbulent effects. MST is one of the sites in the Center for Magnetic Self Organization (CMSO).\n\nAs in most such experiments the MST plasma is a toroidal pinch, which means the plasma is shaped like a donut and confined by a magnetic field generated by a large current flowing through it. MST falls into an unconventional class of machine called a reversed field pinch (RFP.) The RFP is so named because the toroidal magnetic field that permeates the plasma spontaneously reverses direction near the edge.\n\nA reversed field pinch is formed similarly to other toroidal pinch devices, by driving current through the plasma from an associated capacitor bank or other high-current power source. In a tokamak the toroidal field is much stronger than the poloidal field, but in an RFP it's just the opposite. In fact, in an RFP the externally applied toroidal field is switched off shortly after startup. The plasma in an RFP is also much closer to the wall than in a Tokamak. This permits a peculiar arrangement of the magnetic field lines, which will 'relax' into a new state such that the total magnetic energy in the plasma is minimized and the total magnetic helicity is conserved. The relaxed state, called a Taylor state, is marked by a peculiar arrangement of magnetic field lines where the toroidal magnetic field at the edge spontaneously reverses direction.\n\nLike most toroidal confinement schemes, the RFP relies on a transient burst of current to create the plasma and the magnetic fields that confine it. But for the RFP to be a viable fusion energy candidate the plasma must be sustained by a steady state current source. OFCD is a scheme for driving a steady current in a relaxed plasma by adding sizable oscillating perturbations to the toroidal and poloidal fields injecting both power and helicity into the plasma.\n\nA nonlinear reaction in the plasma combines the two oscillations in such a way that, on average, a steady\ncurrent is maintained.\n\nOne of the challenges facing the RFP is fueling the hot core of the plasma directly, rather than relying on the deuterium gas\nto seep in slowly from the edge. The Pellet Injector fires a frozen pellet of deuterium into the plasma using a blast of gas or\na mechanical punch. The pellet is vaporized and ionized as it travels into the core of the plasma.\n\nEvery gradient is a source of free energy, especially if it's across a magnetic field. In MST the current is stronger\nin the core than at the edge. This peaked current profile serves as a source of free energy for magnetic fluctuations\nculminating in violent events in the plasma called sawteeth.\n\nPPCD alleviates this effect by driving a current at the edge of the plasma, flattening the current profile. Small pulses\nare added to the power supply currents that drive the toroidal field. The resultant pulsed toroidal magnetic field,\nwith the aid of Faraday's law, creates a poloidal electric field and hence a poloidal current. A great deal of research on MST is devoted to the study of this effect and its application for enhanced confinement.\n\nIn order to initiate a sustained fusion reaction, it is usually necessary to use many methods to heat the plasma. Neutral Beam Injection (NBI) involves injecting a high energy beam of neutral atoms, typically hydrogen or deuterium, into the core of the plasma. These energetic atoms transfer their energy to the plasma, raising the overall temperature. The neutral atoms injected don't remain neutral. As the beam passes through the plasma, the atoms are ionized as they bounce off the ions in the plasma. Because the magnetic field inside the torus is bent into a circle, the fast ions are hoped to be confined in the background plasma. The confined fast ions are slowed down by the background plasma, the same way air resistance slows down a baseball. The energy transfer from the fast ions to the plasma increases the plasma temperature.\nThe actual injector can be seen from the observation window. It looks like a long silver cylinder laying on its side but tilted slightly downward against the torus near the back of the machine. When the injector is pulsed, 20,000 volts accelerates the beam to about 30 amperes of current for about 1.5 milliseconds.\n\nProblems would occur if the fast ions aren't confined within the plasma long enough for them to deposit their energy. Magnetic fluctuations bedevil plasma confinement in this type of device by scrambling what we hoped were well behaved magnetic fields. If the fast ions are susceptible to this type of behavior, they can escape very quickly. However, there is evidence to suggest that they aren't.\n\nEBW is an acronym for Electron Bernstein Wave and is named after the plasma physicist, Ira Bernstein.\n\nBernstein Wave Mode relates to a method of injecting ion or electron energy (IBW or EBW) into a plasma to increase its temperature in an attempt to reach fusion conditions. A plasma is a phase of matter which occurs naturally during lightning and electrical discharges and which is created artificially in fusion reactors to produce extremely high temperatures.\n\nA definition may be found in the Laurence Livermore Plasma dictionary.\n\nThis is an experiment on the MST to heat the plasma and to drive electric current inside the plasma.\n\nThere is a large electric current in the plasma inside this machine; it is responsible for creating the necessary magnetic fields to make the reversed field pinch configuration. It also heats the plasma very quickly — the same way wires inside your toaster get hot. Your toaster probably uses about 10 ampere of current, while the plasma in MST is heated by up to 600,000 amperes. But even though the plasma reaches over 10,000,000 degrees Fahrenheit, it is not hot enough for practical fusion energy and we need to find other ways to deposit energy into the plasma. The EBW is a way to inject microwave power to further heat the plasma. The standard microwave oven produces around 1 kW of power at a frequency of 2.45 GHz; the EBW experiment is currently producing 150 kW at 3.6 GHz, and it is a goal of the team to upgrade to over 2 MW. To generate this type of power (on a low budget), decommissioned military radar equipment and home-made voltage power supplies are used.\n\nThe second (and perhaps more scientifically important) goal of the EBW experiment is to drive electric current in a\nprescribed place within the plasma. The main plasma current distributes itself naturally, and the plasma tends to concentrate current\ninto the center, leaving less current near the edge. This can lead to instability of the plasma. It has been shown (both\ntheoretically and by experiments in the Madison Symmetric Torus) that driving current in the edge makes the plasma more stable to fluctuations in the magnetic field, resulting in better confinement of the hot plasma and leading to much higher temperature. Using the EBW to drive this\nstabilizing current would be a very important scientific result. The ability to deposit very specifically the auxiliary current\ngives us the opportunity to optimize our current drive schemes. The heating is also very localized, allowing us to study how hot (at least locally) the plasma can become within this magnetic confinement scheme — in plasma physics terms, this is called finding the beta limit. This is an unanswered question for the RFP and will give insight on whether or not this type of machine could be scaled up to a cost effective, efficient fusion reactor.\n\nThe Heavy Ion Beam Probe (HIBP) fires potassium ions into the plasma. By measuring their trajectory we get a profile of\nseveral key properties inside the plasma.\n\nThis versatile diagnostics tool has been used in magnetic confinement fusion experiments to\ndetermine the electric potential, electron density, electron temperature, and magnetic vector potential of the plasma.\n\nA stream of sodium ions (the primary beam) is injected from the ion gun across the magnetic field\ninto the plasma. As the singly charged particles pass through the plasma, they are further ionized creating\nthe doubly charged secondary beam.\n\nThe secondaries are then detected and analyzed outside the plasma. By curving the trajectories, the magnetic field separates secondary\nions from primary ions. Because of this, only secondaries ionized at a given plasma position reach a given detector location.\nThis allows the HIBP to make measurements localized to the ionization position.\nThe secondary current is related to local electron density and the ionization cross-section of the primary ions, which is itself\na function of the electron temperature. The electric potential can be obtained from the energy difference between primary and secondary\nion beams. The energy of the secondary beam can be determined from the angle at which it enters the energy analyzer.\n\nThe MST-HIBP system consists of:\n\n\nFIR, or Far Infrared, refers to light with wavelengths between 1 and 10 mm. The FIR system in MST is based on the FIR lasers enclosed in the beige-colored laser safety room to the right of the picture shown, in the second floor hallway.\nThere are four FIR lasers in the system. One is a CO laser which produces a continuous power of about 120 W. This beam is then split in three. Each beam optically pumps a Formic Acid vapor laser operating at a wavelength of 432.6 mm, and a power of about 20 mW. The FIR system has 2 modes of operation: interferometry and polarimetry.\n\nWhat does FIR diagnostic system measure?\n\nThe electron density, plasma current density, and magnetic field are three important plasma parameters of MST.\nThe FIR system is used to measure their spatial and temporal distributions.\n\nHow does FIR interferometry work?\n\nLike glass, a plasma has a refractive index different from that of vacuum (or air) that depends on plasma electron density.\nWe send one laser beam through the plasma (the probe beam), one through the air (the reference beam), and measure the phase difference\nbetween them. This experimental configuration is called a Mach-Zehnder interferometer. The measured phase\nis proportional to the average plasma electron density along the beam path.\n\nIn MST, we send multiple probe beams (blue lines in the figure) through the plasma at different radii. We then apply the\nso-called Abel inversion technique to obtain a profile of the plasma electron density.\n\nHow does FIR polarimetry work?\n\nA plasma is also an optically active media, meaning when a linearly polarized electromagnetic wave is propagating parallel (or anti-parallel)\nto the magnetic field, the polarization of the wave exiting the plasma will rotate a small angle.\nThis is called Faraday rotation, and the angle is called the Faraday rotation angle. The FIR\nsystem measures the Faraday rotation, which is proportional to the line average of the electron density times the magnetic\nfield component parallel to the beam path.\n\nThe reason for Faraday rotation is as follows: When a linearly polarized wave is propagating along a magnetic field line,\nit is de-composed into left-hand and right-hand circularly polarized components. The phase difference between them as they\nexit the plasma causes the recombined linearly polarized wave to rotate its polarization direction. In MST, we launch two co-propagating, counter-rotating waves to probe the plasma. We then measure the phase difference between these two beams, which will be twice the Faraday rotation angle.\n\nIn the figure, each of the 11 blue probe beams is a combination of two counter-rotating, circularly polarized beams, measuring the Faraday rotation angles along the same chords as the interferometer does. The combined interferometer phases and Faraday rotation angles can then be combined to determine the poloidal magnetic field distribution. Using Ampere's law, the toroidal plasma current can be determined as well.\n\nHow well does the FIR diagnostic system work?\n\nThe FIR system for MST is very precise. The Faraday rotation angle for MST plasmas is typically\nwithin 5 degrees. To measure such small signal, we have achieved an accuracy of 0.06 degree. The temporal resolution is less than 1 microsecond.\n\nWhat are some of the research topics related to FIR?\n\nFIR is an essential tool for most of the research topics in MST since it provides information about the basic plasma parameters. The system measures electron density, toroidal current, poloidal magnetic field, and the spatial profiles of each.\n\nCurrently, we are exploring the possibility of measuring toroidal magnetic field and poloidal plasma current by using the\nplasma bi-refringence effect, or the Cotton-Mouton effect. When a linearly polarized EM wave is propagating perpendicular to\nthe magnetic field, the refractive index depends on whether the wave polarization is parallel or perpendicular to the magnetic\nfield direction.\n\nWhy choose FIR lasers?\n\nFor plasma polarimetry-interferometry, the wavelength we chose is sufficiently long to provide measurable plasma induced phase changes, but sufficiently short to avoid\ncomplicated plasma-wave interactions, including the bending of the beam. There are many high power molecular laser lines available in this wavelength range, and many commercially available detectors.\n\nWhat is Thomson Scattering?\n\nThomson scattering is the result of a collision between a photon (an electromagnetic wave) and a charged particle, such as an electron. When an electron and photon \"collide\" the electron feels a Lorentz force from the oscillating electric and magnetic fields of the photon and is accelerated. This acceleration causes the electron to emit a different photon in a different direction. This emitted photon has a wavelength shifted from that of the incident photon by an amount dependent on the electron energy. Another way of looking at this is that the electron absorbs the energy of the photon and re emits the energy in the form of a different electromagnetic wave. This scattering of a photon by an electron is called Thomson Scattering.\n\nHow is Thomson Scattering useful to plasma physicists?\n\nSince the wavelength of the scattered photon depends on the energy of the scattering electron, Thomson scattering is good way to measure the energy of an electron. This is done by creating a photon of known wavelength and measuring the wavelength of the scattered photon. The Thomson Scattering configuration at MST uses a 1064 nm Nd:YAG Laser System, which produces the best time-resolution electron temperature readings in the world. We create our photons with high power lasers that we shine into a window on the top of the MST, and collect scattered photons with a large collection lens on the side of the MST.\n\nThe wavelength distribution of the scattered photons tells us the energy distribution of the electrons in the plasma, giving us a direct unobtrusive way of getting the temperature of the electrons. The amount of photons we actually collect can also tell us something about the density of the electrons in the plasma.\n\nFusion plasmas are typically generated from ionization of a neutral gas. In most cases, an isotope of hydrogen — called deuterium — is used as the plasma fuel. These plasmas are therefore primarily made up of deuterium ions (plus electrons), and it is necessary to diagnose the behavior of these ions if the relevant plasma physics is to be understood. However, in any fusion device, other types of ions (\"impurities\") are also present. These exist naturally due to the inability to achieve a perfect vacuum in a fusion reactor before fueling. Thus, materials such as water vapor, nitrogen, and carbon will be found in small amounts in typical plasma discharges. Impurities may also be generated during plasma discharges due to plasma-wall interactions. These interactions primarily cause material from the wall to be ejected into the plasma through sputtering. In the Madison Symmetric Torus (MST), properties of the impurity ions (e.g. carbon, oxygen, etc.) are closely linked to properties of the deuterium ions as a result of strong interaction between the ion species. Thus, impurity ion measurements can, in principle, provide direct information about the deuterium ions. Measurements of the impurity ion temperature (\"T\") and flow velocity (\"v\") are obtained on MST using Charge Exchange Recombination Spectroscopy, or CHERS.\n\nThe CHERS process can be broken down into two separate steps: Charge Exchange and Radiative Decay. In the first stage, an electron is transferred from a neutral atom (e.g. deuterium) to an impurity ion that has no electrons (e.g. C).\nDuring this transfer, the electron typically winds up in an excited state (high energy level) of the impurity ion. As the electron decays down to the ground state (minimum energy level), energy conservation requires radiation to be emitted by the impurity ion. This emission has discrete values of energy, or wavelength, which correspond to the energy differences between the initial and final atomic levels of a particular electron transition. For example, consider charge exchange between a deuterium atom and a C ion: if the electron is transferred to the \"n\"=7 energy level of the carbon ion, then the ion will emit radiation at discrete energies given by the difference in energy between the \"n\"=7 and \"n\"=6 levels, the \"n\"=6 and \"n\"=5 levels, the \"n\"=5 and \"n\"=4 levels, and so on (down to \"n=1\"). This line emission is Doppler-broadened as a result of ion thermal motion, and Doppler-shifted as a result of ion flow. The Doppler shift causes the emission to be blue-shifted (towards shorter wavelength/higher frequency) if the ions are moving towards the point of observation, or red-shifted (towards longer wavelength/lower frequency) if the flow is away from the point of observation. Measurements of the carbon emission line shape are therefore used to extract values for the impurity ion temperature and velocity.\n\n\"Charge Exchange\": H + C →\n\nH + C (\"n=7\", \"l=6\")\n\n\"Radiative decay\": C (\"n=7\", \"l=6\") →\n\nC (\"n=6\", \"l=5\") + h (photon)\n\nIn a typical fusion device the neutral atom density is small. Therefore, the amount of radiated emission that results from charge exchange between impurity ions and neutrals is also small. On MST, the neutral density is enhanced by injection of fast hydrogen atoms via a diagnostic neutral beam (DNB). As a result, the radiated emission is greatly increased, though primarily along the beam injection path (the DNB is located below the deck, and cannot be seen from here; the injection path is from right to left across the plasma).\nPerpendicular to the beam path, there exist a number of optical ports for viewing the plasma at different radial positions. For a given plasma discharge, a fiber bundle system is placed on one of these ports, and is used to collect emission along its line-of-sight (black tubes on top of the machine contain light collection optics; fibers are placed in the long, curved white tube when not in use). This emission is sent to a spectrometer (located in big purple box), where it is dispersed over a finite wavelength range — which is centered on the emission line of interest — by a pair of optical gratings. However, because the collected emission is dominated by radiation from along the beam path, the measurements are effectively localized to the intersection volume between the fiber view and the beam. On MST, this intersection volume is small (~ 2 cm) compared to the plasma volume, allowing spatially resolved measurements of \"T\" and \"v\" to be obtained. Data collected from a number of plasma discharges — for which the location of the fiber bundle system is varied — are used to construct radial profiles of the impurity ion temperature and velocity, providing important information for understanding the physics of plasmas in MST. Typical ion temperatures measured by CHERS on MST are in the range of 100 to 800 eV (2 million to 17 million degrees Fahrenheit), depending on position in the plasma and type of discharge. Likewise, measured equilibrium ion velocities are on the order of 1,000 to 10,000 meters per second.\n\n\n"}
{"id": "19848", "url": "https://en.wikipedia.org/wiki?curid=19848", "title": "Max Planck", "text": "Max Planck\n\nMax Karl Ernst Ludwig Planck, FRS (; 23 April 1858 – 4 October 1947) was a German theoretical physicist whose discovery of energy quanta won him the Nobel Prize in Physics in 1918.\n\nPlanck made many contributions to theoretical physics, but his fame as a physicist rests primarily on his role as the originator of quantum theory, which revolutionized human understanding of atomic and subatomic processes. In 1948 the German scientific institution the Kaiser Wilhelm Society (of which Planck was twice president) was renamed the Max Planck Society (MPS). The MPS now includes 83 institutions representing a wide range of scientific directions.\n\nPlanck came from a traditional, intellectual family. His paternal great-grandfather and grandfather were both theology professors in Göttingen; his father was a law professor in Kiel and Munich.\nPlanck was born in Kiel, Holstein, to Johann Julius Wilhelm Planck and his second wife, Emma Patzig. He was baptized with the name of \"Karl Ernst Ludwig Marx Planck\"; of his given names, \"Marx\" (a now obsolete variant of \"Markus\" or maybe simply an error for \"Max\", which is actually short for \"Maximilian\") was indicated as the \"appellation name\". However, by the age of ten he signed with the name \"Max\" and used this for the rest of his life.\n\nHe was the 6th child in the family, though two of his siblings were from his father's first marriage. Among his earliest memories was the marching of Prussian and Austrian troops into Kiel during the Second Schleswig War in 1864. In 1867 the family moved to Munich, and Planck enrolled in the Maximilians gymnasium school, where he came under the tutelage of Hermann Müller, a mathematician who took an interest in the youth, and taught him astronomy and mechanics as well as mathematics. It was from Müller that Planck first learned the principle of conservation of energy. Planck graduated early, at age 17. This is how Planck first came in contact with the field of physics.\n\nPlanck was gifted when it came to music. He took singing lessons and played piano, organ and cello, and composed songs and operas. However, instead of music he chose to study physics.\nThe Munich physics professor Philipp von Jolly advised Planck against going into physics, saying, \"in this field, almost everything is already discovered, and all that remains is to fill a few holes.\" Planck replied that he did not wish to discover new things, but only to understand the known fundamentals of the field, and so began his studies in 1874 at the University of Munich. Under Jolly's supervision, Planck performed the only experiments of his scientific career, studying the diffusion of hydrogen through heated platinum, but transferred to theoretical physics.\n\nIn 1877 he went to the Friedrich Wilhelms University in Berlin for a year of study with physicists Hermann von Helmholtz and Gustav Kirchhoff and mathematician Karl Weierstrass. He wrote that Helmholtz was never quite prepared, spoke slowly, miscalculated endlessly, and bored his listeners, while Kirchhoff spoke in carefully prepared lectures which were dry and monotonous. He soon became close friends with Helmholtz. While there he undertook a program of mostly self-study of Clausius's writings, which led him to choose thermodynamics as his field.\n\nIn October 1878 Planck passed his qualifying exams and in February 1879 defended his dissertation, \"Über den zweiten Hauptsatz der mechanischen Wärmetheorie\" (\"On the second law of thermodynamics\"). He briefly taught mathematics and physics at his former school in Munich.\n\nIn June 1880, he presented his habilitation thesis, \"Gleichgewichtszustände isotroper Körper in verschiedenen Temperaturen\" (\"Equilibrium states of isotropic bodies at different temperatures\").\n\nWith the completion of his habilitation thesis, Planck became an unpaid Privatdozent (German academic rank comparable to lecturer/assistant professor) in Munich, waiting until he was offered an academic position. Although he was initially ignored by the academic community, he furthered his work on the field of heat theory and discovered one after another the same thermodynamical formalism as Gibbs without realizing it. Clausius's ideas on entropy occupied a central role in his work.\n\nIn April 1885 the University of Kiel appointed Planck as associate professor of theoretical physics. Further work on entropy and its treatment, especially as applied in physical chemistry, followed. He published his \"Treatise on Thermodynamics\" in 1897. He proposed a thermodynamic basis for Svante Arrhenius's theory of electrolytic dissociation.\n\nIn 1889 he was named the successor to Kirchhoff's position at the Friedrich-Wilhelms-Universität in Berlin – presumably thanks to Helmholtz's intercession – and by 1892 became a full professor. In 1907 Planck was offered Boltzmann's position in Vienna, but turned it down to stay in Berlin. During 1909, as a University of Berlin professor, he was invited to become the Ernest Kempton Adams Lecturer in Theoretical Physics at Columbia University in New York City. A series of his lectures were translated and co-published by Columbia University professor A. P. Wills. He retired from Berlin on 10 January 1926, and was succeeded by Erwin Schrödinger.\n\nIn March 1887 Planck married Marie Merck (1861–1909), sister of a school fellow, and moved with her into a sublet apartment in Kiel. They had four children: Karl (1888–1916), the twins Emma (1889–1919) and Grete (1889–1917), and Erwin (1893–1945).\n\nAfter the apartment in Berlin, the Planck family lived in a villa in Berlin-Grunewald, Wangenheimstrasse 21. Several other professors from University of Berlin lived nearby, among them theologian Adolf von Harnack, who became a close friend of Planck. Soon the Planck home became a social and cultural center. Numerous well-known scientists, such as Albert Einstein, Otto Hahn and Lise Meitner were frequent visitors. The tradition of jointly performing music had already been established in the home of Helmholtz.\n\nAfter several happy years, in July 1909 Marie Planck died, possibly from tuberculosis. In March 1911 Planck married his second wife, Marga von Hoesslin (1882–1948); in December his fifth child Hermann was born.\n\nDuring the First World War Planck's second son Erwin was taken prisoner by the French in 1914, while his oldest son Karl was killed in action at Verdun. Grete died in 1917 while giving birth to her first child. Her sister died the same way two years later, after having married Grete's widower. Both granddaughters survived and were named after their mothers. Planck endured these losses stoically.\n\nIn January 1945, Erwin, to whom he had been particularly close, was sentenced to death by the Nazi Volksgerichtshof because of his participation in the failed attempt to assassinate Hitler in July 1944. Erwin was executed on 23 January 1945.\n\nAs a professor at the Friedrich-Wilhelms-Universität in Berlin, Planck joined the local Physical Society. He later wrote about this time: \"In those days I was essentially the only theoretical physicist there, whence things were not so easy for me, because I started mentioning entropy, but this was not quite fashionable, since it was regarded as a mathematical spook\". Thanks to his initiative, the various local Physical Societies of Germany merged in 1898 to form the German Physical Society (Deutsche Physikalische Gesellschaft, DPG); from 1905 to 1909 Planck was the president.\n\nPlanck started a six-semester course of lectures on theoretical physics, \"dry, somewhat impersonal\" according to Lise Meitner, \"using no notes, never making mistakes, never faltering; the best lecturer I ever heard\" according to an English participant, James R. Partington, who continues: \"There were always many standing around the room. As the lecture-room was well heated and rather close, some of the listeners would from time to time drop to the floor, but this did not disturb the lecture\". Planck did not establish an actual \"school\"; the number of his graduate students was only about 20, among them:\n\nIn 1894 Planck turned his attention to the problem of black-body radiation. He had been commissioned by electric companies to create maximum light from lightbulbs with minimum energy. The problem had been stated by Kirchhoff in 1859: \"how does the intensity of the electromagnetic radiation emitted by a black body (a perfect absorber, also known as a cavity radiator) depend on the frequency of the radiation (i.e., the color of the light) and the temperature of the body?\". The question had been explored experimentally, but no theoretical treatment agreed with experimental values. Wilhelm Wien proposed Wien's law, which correctly predicted the behaviour at high frequencies, but failed at low frequencies. The Rayleigh–Jeans law, another approach to the problem, created what was later known as the \"ultraviolet catastrophe\", but contrary to many textbooks this was not a motivation for Planck.\n\nPlanck's first proposed solution to the problem in 1899 followed from what Planck called the \"principle of elementary disorder\", which allowed him to derive Wien's law from a number of assumptions about the entropy of an ideal oscillator, creating what was referred-to as the Wien–Planck law. Soon it was found that experimental evidence did not confirm the new law at all, to Planck's frustration. Planck revised his approach, deriving the first version of the famous Planck black-body radiation law, which described the experimentally observed black-body spectrum well. It was first proposed in a meeting of the DPG on 19 October 1900 and published in 1901. This first derivation did not include energy quantisation, and did not use statistical mechanics, to which he held an aversion. In November 1900, Planck revised this first approach, relying on Boltzmann's statistical interpretation of the second law of thermodynamics as a way of gaining a more fundamental understanding of the principles behind his radiation law. As Planck was deeply suspicious of the philosophical and physical implications of such an interpretation of Boltzmann's approach, his recourse to them was, as he later put it, \"an act of despair ... I was ready to sacrifice any of my previous convictions about physics.\"\n\nThe central assumption behind his new derivation, presented to the DPG on 14 December 1900, was the supposition, now known as the Planck postulate, that electromagnetic energy could be emitted only in quantized form, in other words, the energy could only be a multiple of an elementary unit:\n\nwhere is Planck's constant, also known as Planck's action quantum (introduced already in 1899), and is the frequency of the radiation. Note that the elementary units of energy discussed here are represented by and not simply by . Physicists now call these quanta photons, and a photon of frequency will have its own specific and unique energy. The total energy at that frequency is then equal to multiplied by the number of photons at that frequency.\nAt first Planck considered that quantisation was only \"a purely formal assumption ... actually I did not think much about it...\"; nowadays this assumption, incompatible with classical physics, is regarded as the birth of quantum physics and the greatest intellectual accomplishment of Planck's career (Ludwig Boltzmann had been discussing in a theoretical paper in 1877 the possibility that the energy states of a physical system could be discrete). The discovery of Planck's constant enabled him to define a new universal set of physical units (such as the Planck length and the Planck mass), all based on fundamental physical constants upon which much of quantum theory is based. In recognition of Planck's fundamental contribution to a new branch of physics, he was awarded the Nobel Prize in Physics for 1918 (he actually received the award in 1919).\n\nSubsequently, Planck tried to grasp the meaning of energy quanta, but to no avail. \"My unavailing attempts to somehow reintegrate the action quantum into classical theory extended over several years and caused me much trouble.\" Even several years later, other physicists like Rayleigh, Jeans, and Lorentz set Planck's constant to zero in order to align with classical physics, but Planck knew well that this constant had a precise nonzero value. \"I am unable to understand Jeans' stubbornness – he is an example of a theoretician as should never be existing, the same as Hegel was for philosophy. So much the worse for the facts if they don't fit.\"\n\nMax Born wrote about Planck: \"He was, by nature, a conservative mind; he had nothing of the revolutionary and was thoroughly skeptical about speculations. Yet his belief in the compelling force of logical reasoning from facts was so strong that he did not flinch from announcing the most revolutionary idea which ever has shaken physics.\"\n\nIn 1905, the three epochal papers by Albert Einstein were published in the journal \"Annalen der Physik\". Planck was among the few who immediately recognized the significance of the special theory of relativity. Thanks to his influence, this theory was soon widely accepted in Germany. Planck also contributed considerably to extend the special theory of relativity. For example, he recast the theory in terms of classical action.\n\nEinstein's hypothesis of light \"quanta\" (photons), based on Heinrich Hertz's 1887 discovery (and further investigation by Philipp Lenard) of the photoelectric effect, was initially rejected by Planck. He was unwilling to discard completely Maxwell's theory of electrodynamics. \"The theory of light would be thrown back not by decades, but by centuries, into the age when Christiaan Huygens dared to fight against the mighty emission theory of Isaac Newton ...\"\n\nIn 1910, Einstein pointed out the anomalous behavior of specific heat at low temperatures as another example of a phenomenon which defies explanation by classical physics. Planck and Nernst, seeking to clarify the increasing number of contradictions, organized the First Solvay Conference (Brussels 1911). At this meeting Einstein was able to convince Planck.\n\nMeanwhile, Planck had been appointed dean of Berlin University, whereby it was possible for him to call Einstein to Berlin and establish a new professorship for him (1914). Soon the two scientists became close friends and met frequently to play music together.\n\nAt the onset of the First World War Planck endorsed the general excitement of the public, writing that, \"Besides much that is horrible, there is also much that is unexpectedly great and beautiful: the smooth solution of the most difficult domestic political problems by the unification of all parties (and) ... the extolling of everything good and noble.\"\n\nNonetheless, Planck refrained from the extremes of nationalism. In 1915, at a time when Italy was about to join the Allied Powers, he voted successfully for a scientific paper from Italy, which received a prize from the Prussian Academy of Sciences, where Planck was one of four permanent presidents.\n\nPlanck also signed the infamous \"Manifesto of the 93 intellectuals\", a pamphlet of polemic war propaganda (while Einstein retained a strictly pacifistic attitude which almost led to his imprisonment, being spared by his Swiss citizenship). But in 1915 Planck, after several meetings with Dutch physicist Lorentz, revoked parts of the Manifesto. Then in 1916 he signed a declaration against German annexationism.\n\nIn the turbulent post-war years, Planck, now the highest authority of German physics, issued the slogan \"persevere and continue working\" to his colleagues.\n\nIn October 1920 he and Fritz Haber established the \"Notgemeinschaft der Deutschen Wissenschaft\" (Emergency Organization of German Science), aimed at providing financial support for scientific research. A considerable portion of the money the organization would distribute was raised abroad.\n\nPlanck also held leading positions at Berlin University, the Prussian Academy of Sciences, the German Physical Society and the Kaiser Wilhelm Society (which in 1948 became the Max Planck Society). During this time economic conditions in Germany were such that he was hardly able to conduct research. In 1926 Planck became a foreign member of the Royal Netherlands Academy of Arts and Sciences.\n\nDuring the interwar period, Planck became a member of the Deutsche Volks-Partei (German People's Party), the party of Nobel Peace Prize laureate Gustav Stresemann, which aspired to liberal aims for domestic policy and rather revisionistic aims for politics around the world.\n\nPlanck disagreed with the introduction of universal suffrage and later expressed the view that the Nazi dictatorship resulted from \"the ascent of the rule of the crowds\".\n\nAt the end of the 1920s Bohr, Heisenberg and Pauli had worked out the Copenhagen interpretation of quantum mechanics, but it was rejected by Planck, and by Schrödinger, Laue, and Einstein as well. Planck expected that wave mechanics would soon render quantum theory—his own child—unnecessary. This was not to be the case, however. Further work only cemented quantum theory, even against his and Einstein's philosophical revulsions. Planck experienced the truth of his own earlier observation from his struggle with the older views in his younger years: \"A new scientific truth does not triumph by convincing its opponents and making them see the light, but rather because its opponents eventually die, and a new generation grows up that is familiar with it.\"\n\nWhen the Nazis came to power in 1933, Planck was 74. He witnessed many Jewish friends and colleagues expelled from their positions and humiliated, and hundreds of scientists emigrated from Nazi Germany. Again he tried to \"persevere and continue working\" and asked scientists who were considering emigration to remain in Germany. Nevertheless, he did help his nephew, the economist Hermann Kranold to emigrate to London after his arrest. He hoped the crisis would abate soon and the political situation would improve.\n\nOtto Hahn asked Planck to gather well-known German professors in order to issue a public proclamation against the treatment of Jewish professors, but Planck replied, \"If you are able to gather today 30 such gentlemen, then tomorrow 150 others will come and speak against it, because they are eager to take over the positions of the others.\" Under Planck's leadership, the Kaiser Wilhelm Society (KWG) avoided open conflict with the Nazi regime, except concerning Fritz Haber. Planck tried to discuss the issue with Adolf Hitler but was unsuccessful. In the following year, 1934, Haber died in exile.\n\nOne year later, Planck, having been the president of the KWG since 1930, organized in a somewhat provocative style an official commemorative meeting for Haber. He also succeeded in secretly enabling a number of Jewish scientists to continue working in institutes of the KWG for several years. In 1936, his term as president of the KWG ended, and the Nazi government pressured him to refrain from seeking another term.\n\nAs the political climate in Germany gradually became more hostile, Johannes Stark, prominent exponent of Deutsche Physik (\"German Physics\", also called \"Aryan Physics\") attacked Planck, Sommerfeld and Heisenberg for continuing to teach the theories of Einstein, calling them \"white Jews\". The \"Hauptamt Wissenschaft\" (Nazi government office for science) started an investigation of Planck's ancestry, claiming that he was \"1/16 Jewish\", but Planck himself denied it.\nIn 1938, Planck celebrated his 80th birthday. The DPG held a celebration, during which the Max-Planck medal (founded as the highest medal by the DPG in 1928) was awarded to French physicist Louis de Broglie. At the end of 1938, the Prussian Academy lost its remaining independence and was taken over by Nazis (\"Gleichschaltung\"). Planck protested by resigning his presidency. He continued to travel frequently, giving numerous public talks, such as his talk on Religion and Science, and five years later he was sufficiently fit to climb 3,000-metre peaks in the Alps.\n\nDuring the Second World War the increasing number of Allied bombing missions against Berlin forced Planck and his wife to temporarily leave the city and live in the countryside. In 1942 he wrote: \"In me an ardent desire has grown to persevere this crisis and live long enough to be able to witness the turning point, the beginning of a new rise.\" In February 1944 his home in Berlin was completely destroyed by an air raid, annihilating all his scientific records and correspondence. His rural retreat was threatened by the rapid advance of the Allied armies from both sides.\n\nIn 1944, Planck's son Erwin was arrested following the attempted assassination of Hitler in the 20 July plot. Erwin consequently died at the hands of the Gestapo; his death destroyed much of Max Planck's will to live. After the end of the war Planck, his second wife, and his son by her were brought to a relative in Göttingen, where Planck died on 4 October 1947. His grave is situated in the old Stadtfriedhof (City Cemetery) in Göttingen.\n\nPlanck was a member of the Lutheran Church in Germany. However, Planck was very tolerant towards alternative views and religions.\nIn a lecture in 1937 entitled \"Religion und Naturwissenschaft\" he suggested the importance of these symbols and rituals related directly with a believer's ability to worship God, but that one must be mindful that the symbols provide an imperfect illustration of divinity. He criticized atheism for being focused on the derision of such symbols, while at the same time warned of the over-estimation of the importance of such symbols by believers.\n\nHe was favorable to all religions, but he himself chose Christianity. He did, however, regret the Church's demands for unquestioning belief, which served to repel questioners. For example, he believed \"the faith in miracles must yield, step by step, before the steady and firm advance of the facts of science, and its total defeat is undoubtedly a matter of time.\" \n\nIn his 1937 lecture \"Religion and Naturwissenschaft,\" Planck expressed the view that God is everywhere present, and held that \"the holiness of the unintelligible Godhead is conveyed by the holiness of symbols.\" Atheists, he thought, attach too much importance to what are merely symbols. Planck was a churchwarden from 1920 until his death, and believed in an almighty, all-knowing, beneficent God (though not necessarily a personal one). Both science and religion wage a \"tireless battle against skepticism and dogmatism, against unbelief and superstition\" with the goal \"toward God!\"\n\nMax Planck said in 1944, \"As a man who has devoted his whole life to the most clear headed science, to the study of matter, I can tell you as a result of my research about atoms this much: There is no matter as such. All matter originates and exists only by virtue of a force which brings the particle of an atom to vibration and holds this most minute solar system of the atom together. We must assume behind this force the existence of a conscious and intelligent spirit (orig. geist). This spirit is the matrix of all matter.\"\n\nPlanck regarded the scientist as a man of imagination and Christian faith. He said: \"Both religion and science require a belief in God. For believers, God is in the beginning, and for physicists He is at the end of all considerations… To the former He is the foundation, to the latter, the crown of the edifice of every generalized world view\".\n\nOn the other hand, Planck wrote, \"...'to believe' means 'to recognize as a truth,' and the knowledge of nature, continually advancing on incontestably safe tracks, has made it utterly impossible for a person possessing some training in natural science to recognize as founded on truth the many reports of extraordinary occurrences contradicting the laws of nature, of miracles which are still commonly regarded as essential supports and confirmations of religious doctrines, and which formerly used to be accepted as facts pure and simple, without doubt or criticism. The belief in miracles must retreat step by step before relentlessly and reliably progressing science and we cannot doubt that sooner or later it must vanish completely.\"\n\nLater in life, Planck's views on God were that of a deist. For example, six months before his death a rumour started that Planck had converted to Catholicism, but when questioned what had brought him to make this step, he declared that, although he had always been deeply religious, he did not believe \"in a personal God, let alone a Christian God.\"\n\n\n\n\n"}
{"id": "3833746", "url": "https://en.wikipedia.org/wiki?curid=3833746", "title": "Myrmekite", "text": "Myrmekite\n\nMyrmekite describes a vermicular, or wormy, intergrowth of quartz in plagioclase. The intergrowths are microscopic in scale, typically with maximum dimensions less than 1 millimeter. The plagioclase is sodium-rich, usually albite or oligoclase. These quartz-plagioclase intergrowths are associated with and commonly in contact with potassium feldspar. Myrmekite is formed under metasomatic conditions, usually in conjunction with tectonic deformations. It has to be clearly separated from micrographic and granophyric intergrowths, which are magmatic.\n\nThe word myrmekite is derived from the Ancient Greek μὑρμηχἰα (wart) or μὑρμηξ (ant) and was used by Jakob Sederholm in 1899 for the first time to describe these structures.\n\nDuring K-metasomatism of plagioclase several different types of myrmekite can appear:\n\nThis is the initial stage of K-metasomatism in cataclastically-deformed magmatic plutonic rocks. The breakage happens primarily along grain boundary seals and the K-metasomatism may locally replace rims of zoned plagioclase crystals to form interstitial alkali feldspar and \"rim myrmekite\" (see illustration).\n\nWhen tectonic strains increase and the cataclasis becomes more intense interior breakage in the crystals ensues and albite-twinned plagioclase crystals are bent. The K-metasomatism therefore can reach deeper into the crystals and increase its effects. Nearly complete to complete replacement of plagioclase takes place and leads to the formation of \"wartlike myrmekite\" in places where the replacement was incomplete. The illustration shows tartan-twinned microcline having completely replaced plagioclase. The places with incomplete replacement are taken up by wartlike myrmekite.\n\nGradations occur from rocks containing exclusively rim myrmekite to those containing both rim myrmekite and wartlike myrmekite and finally to those containing exclusively wartlike myrmekite.\n\nA very important observation is that the maximum coarseness (tubular diameter) of the quartz vermicules shows a strong correlation with the Ca content of the plagioclase in the original, unreplaced, non-myrmekite-bearing magmatic rock. The coarsest vermicules occur in the \nmetasomatized rock where the original plagioclase was the most calcic.\n\nAn example for the formation of wartlike myrmekite can be found at the Twentynine Palms, California quartz monzonite which issued from an older, yet undated diorite.\n\nThis is the third type of quartz-feldspar intergrowth in metasomatic granitoids. Again this process depends on tectonically deformed crystals. In this particular case an irregular subtraction of Ca, Na and Al from deformed plagioclase happens which causes an imbalance in the relative amounts of residual Al and Si. More Si remains than can fit into the lattice structure of the alkali feldspar that replaces the plagioclase. The result is \"ghost myrmekite\" – either as tiny quartz ovoids in remnant albite islands in the alkali feldspar or as tiny quartz ovoids as clusters without albite hosts in the alkali feldspar (see illustration).\n\nExamples for this structure are found in California in the Mount Rubidoux leucogranite and in granodiorites in the Sierra Nevada.\n\nDuring Ca- metasomatism myrmekite can be formed under different circumstances:\n\nHere Ca-bearing fluids enter primary alkali feldspar through cracks and react with the alkali feldspar. Through this reaction cracks are filled with quartz and myrmekite. The replacement reactions can affect large portions (> 60%) of the primary alkali feldspar. An important distinctive feature of this type of myrmekite formation is the constant thickness of the vermicules, whereas in the K-metasomatism their thickness changes as a function of the Ca-content of the plagioclase and they also taper towards the alkali feldspar.\n\nAn example for this type of Ca-metasomatism is found in a megacrystal granite near Alastaro in Finland.\n\nThe process stays the same, the only difference being the country rocks the Ca-bearing fluids act upon. Charnockites distinguish themselves from ordinary granitoids by the appearance of orthopyroxene (hypersthene) and can also be of metamorphic origin.\n\nAn example for this type of Ca-metasomatism is found in Sri Lanka.\n\nIn this type of Ca-metasomatism instead of the alkali feldspar it is the ubiquitous plagioclase that gets attacked by the Ca-bearing fluids. The resulting myrmekite also shows vermicules with constant thickness but unlike in the first case the vermicules formed in anorthosites can taper locally to the primary, non-quartz-bearing plagioclase. This behaviour can be explained by the incorporation of Na demanding more silica in the feldspar lattice.\n\nExamples are found in layered igneous complexes.\n\nA first variety of this type of metasomatism affects only enclaves within a granitoid. Here the influx of Na-rich fluids in the temperature range 450 °C to 650 °C from the host leads to the replacement of alkali feldspar by myrmekite within the enclaves. During this process a reequilibration with the Na-poorer feldspars (plagioclase) in the enclaves takes place. As a consequence Ca gets released in plagioclase which in turn can now react upon K-feldspar to form myrmekite. Basically this process is very similar to the Ca-metasomatism on K-feldspar described above except for the Na-fluids acting as a trigger.\n\nAn example is the Velay granite in the northeastern Massif Central in France.\n\nIn the second variety Na- and Ca-bearing fluids truly act together. This leads via the replacement of primary K-feldspar (perthitic and non-perthitic microcline) to the formation of plagioclase (albite or oligoclase) and in certain places also to the formation of myrmekite. The myrmekite does not show wartlike tapering vermicules, but vermicules that are nearly constant in size because the host plagioclase containing the quartz vermicules has nearly a constant Na/Ca composition. These vermicules are confined and scattered entirely within the interior of the plagioclase forming irregular spindles, arcuate patterns and ovals.\n\nFor this process to operate it is important that Ca is sufficiently present so that a fairly calcic plagioclase can be formed which in turn releases enough silica for the myrmekite vermicules. If only Na is present then no myrmekite will form.\n\nAn example can be found in the Lyon Mountain granite gneiss north of Ausable Forks in New York.\n\nDuring progressive deformation in mylonitic, ductile shear zones myrmekite is commonly concentrated in shortening quarters in the rim of sigmoidal K-feldspar crystals. Simpson and Wintsch (1989) explain the asymmetric distribution of myrmekite by a preferential proceeding of the K-feldspar breakdown reaction at sites of high differential stress (stress-concentration sites) during retrograde metamorphism. Internally the arrangement of the quartz vermicules in the myrmekites also shows a monoclinic symmetry, which independently can serve as an internal shear sense indicator. Asymmetric myrmekite is therefore a quarter structure.\n\nYet Lorence G. Collins does not agree with the assumption of the K-feldspar being primary magmatic and the myrmekite being formed due to deformation-induced Na-Ca-metasomatism. His sampling beyond the shear zone revealed an undeformed, felsic biotite diorite whose primary plagioclase was being replaced from the inside out by K-feldspar due to K-metasomatism. The deformations were therefore more or less continuous and had not only affected the shear zone but also the older plutonic country rocks, thence bringing about a metasomatic change in mineralogy.\n\nMyrmekite can appear in many different rock types and different geologic settings. Typically it occurs in granites and similar igneous rocks (granitoids, diorites, gabbros) and in metamorphic gneisses similar to granite in composition. It can also occur in mylonites, in anorthosites and the orthopyroxene-bearing charnockites.\n\nThese characteristic intergrowths have been explained in a variety of ways:\n\n\n"}
{"id": "31676899", "url": "https://en.wikipedia.org/wiki?curid=31676899", "title": "Natural Currents Energy Services", "text": "Natural Currents Energy Services\n\nNatural Currents Energy Services, LLC is a developer and installer of hydro-electric systems. They produce scalable underwater turbines designed to capture energy from the tides of rivers and oceans. They hold Federal Energy Regulatory Commission (\"FERC\") energy development permits at 10 sites in the US, roughly 25% of the total tidal zone permits granted by FERC. The company has designed two models of turbine, the Red Hawk Tidal Power Generator and the Sea Dragon Tidal Turbine.\n\nNatural Currents has partnered with the City College of New York to assess sites with potential for tidal power development. The 18-month study is funded by grants from the New Jersey Department of Transportation and the University Transportation Research Center.\n\nOn Mar. 5, 2012, Natural currents released results from a study of tidal currents off the coast of New Jersey for 2008 to 2009. The findings indicate a conservative estimate of 417 MW of potential tidal power in New Jersey, compared to the 357 MW measured by a Georgia Tech Report in 2011.\n\n\n"}
{"id": "790284", "url": "https://en.wikipedia.org/wiki?curid=790284", "title": "Nitrous oxide engine", "text": "Nitrous oxide engine\n\nA nitrous oxide engine is an engine which requires oxygen for burning the fuel in order to create power (mainly stems from the decomposition of nitrous oxide (NO) rather than air). The system increases the internal combustion engine's power output by allowing fuel to be burned at a higher-than-normal rate, because of the higher partial pressure of oxygen injected with the fuel mixture. Nitrous injection systems may be \"dry\", where the nitrous oxide is injected separately from fuel, or \"wet\" in which additional fuel is carried into the engine along with the nitrous. Nitrous oxide systems may not be permitted for street or highway use, depending on local regulations. Nitrous oxide use is permitted in certain classes of auto racing. Reliable operation of an engine with nitrous injection requires careful attention to the strength of engine components and to the accuracy of the mixing systems, otherwise destructive detonations or exceeding engineered component maximums may occur. Nitrous oxide injection systems were applied as early as World War II for certain aircraft engines.\n\nIn the context of racing, nitrous oxide is often termed nitrous or NOS. The term NOS is derived from the initials of the company name Nitrous Oxide Systems, one of the pioneering companies in the development of nitrous oxide injection systems for automotive performance use.\n\nWhen a mole of nitrous oxide decomposes, it releases half a mole of O molecules (oxygen gas), and one mole of N molecules (nitrogen gas). This decomposition allows an oxygen concentration of 36.36% to be reached. Nitrogen gas is non-combustible and does not support combustion. Air—which contains only 21% oxygen, the rest being nitrogen and other equally non-combustible and non-combustion-supporting gasses—permits a 12-percent-lower maximum-oxygen level than that of nitrous oxide. This oxygen supports combustion; it combines with fuels such as gasoline, alcohol, diesel fuel, propane, or CNG to produce carbon dioxide and water vapor, along with heat, which causes the former two products of combustion to expand and exert pressure on pistons, driving the engine.\n\nNitrous oxide is stored as a liquid in tanks, but is a gas under atmospheric conditions. When injected as a liquid into an inlet manifold, the vaporization and expansion causes a reduction in air/fuel charge temperature with an associated increase in density, thereby increasing the cylinder's volumetric efficiency.\n\nAs the decomposition of NO into oxygen and nitrogen gas is exothermic and thus contributes to a higher temperature in the combustion engine, the decomposition increases engine efficiency and performance, which is directly related to the difference in temperature between the unburned fuel mixture and the hot combustion gasses produced in the cylinders. \n\nAll systems are based on a single stage kit, but these kits can be used in multiples (called two-, three-, or even four-stage kits). The most advanced systems are controlled by an electronic progressive delivery unit that allows a single kit to perform better than multiple kits can. Most Pro Mod and some Pro Street drag race cars use three stages for additional power, but more and more are switching to pulsed progressive technology. Progressive systems have the advantage of utilizing a larger amount of nitrous (and fuel) to produce even greater power increases as the additional power and torque are gradually introduced (as opposed to being applied to the engine and transmission immediately), reducing the risk of mechanical stress and, consequently, damage.\n\nCars with nitrous-equipped engines may be identified by the \"purge\" of the delivery system that most drivers perform prior to reaching the starting line. A separate electrically operated valve is used to release air and gaseous nitrous oxide trapped in the delivery system. This brings liquid nitrous oxide all the way up through the plumbing from the storage tank to the solenoid valve or valves that will release it into the engine's intake tract. When the purge system is activated, one or more plumes of nitrous oxide will be visible for a moment as the liquid flashes to vapor as it is released. The purpose of a nitrous purge is to ensure that the correct amount of nitrous oxide is delivered the moment the system is activated as nitrous and fuel jets are sized to produce correct air / fuel ratios, and as liquid nitrous is denser than gaseous nitrous, any nitrous vapor in the lines will cause the car to \"bog\" for an instant (as the ratio of nitrous / fuel will be too rich reducing engine power) until liquid nitrous oxide reaches the injection nozzle.\n\nThere are two categories of nitrous systems: \"dry\" & \"wet\" with four main delivery methods of nitrous systems: \"single nozzle\", \"direct port\", \"plate\", and \"bar\" used to discharge nitrous into the plenums of the intake manifold. Nearly all nitrous systems use specific orifice inserts, called jets, along with pressure calculations to meter the nitrous, or nitrous and fuel in wet applications, delivered to create a proper air-fuel ratio (AFR) for the additional horsepower desired.\n\nIn a \"dry\" nitrous system the nitrous delivery method provides nitrous only. The extra fuel required is introduced through the fuel injectors, keeping the manifold dry of fuel. This property is what gives the dry system its name. Fuel flow can be increased either by increasing the pressure or by increasing the time the fuel injectors remain open. \n\nDry nitrous systems typically rely on a single nozzle delivery method, but all of the four main delivery methods can be used in dry applications. Dry systems are not typically used in carbureted applications due to the nature of a carburetor's function and inability to provide large amounts of on-demand fuel. Dry nitrous systems on fuel injected engines will use increased fuel pressure or injector pulsewidth upon system activation as a means of providing the correct ratio of fuel for the nitrous. \n\nIn a \"wet\" nitrous system the nitrous delivery method provides nitrous and fuel together resulting in the intake manifold being \"wet\" with fuel, giving the category its name. Wet nitrous systems can be used in all four main delivery methods.\n\nIn wet systems on fuel/direct injected engines care must be taken to avoid backfires caused by fuel pooling in the intake tract or manifold and/or uneven distribution of the nitrous/fuel mixture. Port and direct fuel injection engines have intake systems engineered for the delivery of air only, not air and fuel. Since most fuels are heavier than air and not in a gaseous state when used with nitrous systems it does not behave in the same way as air alone; thus the possibility of the fuel being unevenly distributed to the combustion chambers of the engine causing lean conditions/detonation and/or pooling in parts of the intake tract/manifold presenting a dangerous situation in which the fuel may be ignited uncontrollably causing catastrophic failure to components. Carbureted and single point/throttle body injected engines use a wet manifold design that is engineered to evenly distribute fuel and air mixtures to all combustion chambers, making this mostly a non-issue for these applications.\n\nA \"single nozzle\" nitrous system introduces the nitrous or fuel/nitrous mixture via a single injection point. The nozzle is typically placed in the intake pipe/tract after the air filter, prior to the intake manifold and/or throttle body in fuel injected applications, and after the throttle body in carbureted applications. In wet systems the high pressures of the nitrous injected causes the aerosolization of the fuel injected in tandem via the nozzle, allowing for more thorough and even distribution of the nitrous/fuel mixture.\n\nA \"direct port\" nitrous system introduces the nitrous or fuel/nitrous mixture as close to the intake ports of the engine as is feasible via individual nozzles directly in each intake runner. Direct port nitrous systems will use the same or similar nozzles as those in single nozzle systems, just in numbers equal to or in multiples of the number of intake ports of the engine. Being that direct port systems do not have to rely on intake tract/manifold design to evenly distribute the nitrous or fuel/nitrous mixture they are inherently more precise than other delivery methods. The greater number of nozzles also allows a greater total amount of nitrous to be delivered than other systems. Multiple \"stages\" of nitrous can be accomplished by using multiple sets of nozzles at each intake port to further increase the power potential. Direct port nitrous systems are the most common delivery method in racing applications.\n\nA \"plate\" nitrous system uses a spacer placed somewhere between the throttle body and intake ports with holes drilled along its interior surfaces, or in a tube that is suspended from the plate, for the nitrous or fuel/nitrous mixture to be distributed through. Plate systems provide a drill-less solution compared to other delivery methods as the plates are generally application specific and fit between existing components such as the throttle body-to-intake-manifold or upper-intake-manifold-to-lower-intake-manifold junctions. Requiring little more than longer fasteners, plate systems are the most easily reversed systems as they need little to no permanent changes to the intake tract. Dependent on application, plate systems can provide precise nitrous or fuel/nitrous mixture distribution similar to that of direct port systems.\n\nA \"bar\" nitrous system utilizes a hollow tube, with a number of holes drilled along its length, placed inside the intake plenum to deliver nitrous. Bar nitrous delivery methods are almost exclusively dry nitrous systems due to the non-optimal fuel distribution possibilities of the bar. Bar nitrous systems are popular with racers that prefer their nitrous use to be hidden, as the nitrous distribution method is not immediately apparent and most associated components of the nitrous system can be obscured from view.\n\nNitrous systems can be used with a gaseous fuel such as propane or compressed natural gas. This has the advantage of being technically a \"dry\" system as the fuel is not in a liquid state when introduced to the intake tract. \n\nThe use of nitrous oxide carries with it concerns about the reliability and longevity of an engine present with all power adders. Due to the greatly increased cylinder pressures, the engine as a whole is placed under greater stress, primarily those components associated with the engine's rotating assembly. An engine with components unable to cope with the increased stress imposed by the use of nitrous systems can experience major engine damage, such as cracked or destroyed pistons, connecting rods, crankshafts, and/or blocks. Proper strengthening of engine components in addition to accurate and adequate fuel delivery are key to nitrous system use without catastrophic failure.\n\nNitrous oxide injection systems for automobiles are illegal for road use in some countries. For example, in New South Wales, Australia, the Roads and Traffic Authority Code of Practice for Light Vehicle Modifications (in use since 1994) states in clause 3.1.5.7.3 that \"The use or fitment of nitrous oxide injection systems is not permitted.\"\n\nIn Great Britain, there are no restrictions on use of , but the modification must be declared to the insurance company, which is likely to result in a higher premium for Motor Vehicle insurance or refusal to insure.\n\nIn Germany, despite its strict TÜV rules, a nitrous system could be installed and used legally in a street driven car. The requirements for the technical standard of the system are similar to those of aftermarket natural gas conversions.\n\nSeveral sanctioning bodies in drag racing allow or disallow the use of nitrous oxide in certain classes or have nitrous oxide specific classes. Nitrous is allowed in Formula Drift competition.\n\nA similar basic technique was used during World War II by Luftwaffe aircraft with the GM-1 system to maintain the power output of aircraft engines when at high altitude where the air density is lower. Accordingly, it was only used by specialized planes like high-altitude reconnaissance aircraft, high-speed bombers and high-altitude interceptors. It was sometimes used with the Luftwaffe's form of methanol-water injection, designated MW 50 (both meant as \"Notleistung\" short-term power boosting measures), to produce substantial increases in performance for fighter aircraft over short periods of time, as with their combined use on the Focke-Wulf Ta 152H fighter prototypes. \n\nBritish World War II usage of nitrous oxide injector systems were modifications of Merlin engines carried out by the Heston Aircraft Company for use in certain night fighter variants of the de Havilland Mosquito and PR versions of the Supermarine Spitfire.\n\n\n"}
{"id": "31479217", "url": "https://en.wikipedia.org/wiki?curid=31479217", "title": "Non-B database", "text": "Non-B database\n\nNon-B DB is a database integrating annotations and analysis of non-B DNA-forming sequence motifs. The database provides alternative DNA structure predictions including Z-DNA motifs, quadruplex-forming motifs, inverted repeats, mirror repeats and direct repeats and their associated subsets of cruciforms, triplex and slipped structures, respectively.\n\n\n"}
{"id": "28297350", "url": "https://en.wikipedia.org/wiki?curid=28297350", "title": "Nuclear energy policy by country", "text": "Nuclear energy policy by country\n\nNational nuclear energy policy is a national policy concerning some or all aspects of nuclear energy, such as mining for nuclear fuel, extraction and processing of nuclear fuel from the ore, generating electricity by nuclear power, enriching and storing spent nuclear fuel and nuclear fuel reprocessing. Nuclear energy policies often include the regulation of energy use and standards relating to the nuclear fuel cycle. \nNuclear power stations operate in 31 countries. China has 32 new reactors under construction, and there are also a considerable number of new reactors being built in South Korea, India, and Russia. At the same time, at least 100 older and smaller reactors will \"most probably be closed over the next 10-15 years\". So the expanding nuclear programs in Asia are balanced by retirements of aging plants and nuclear reactor phase-outs. Global nuclear electricity generation in 2012 was at its lowest level since 1999.\n\nFollowing the March 2011 Fukushima nuclear disaster in Japan, Germany has permanently shut down eight of its reactors and pledged to close the rest by 2022. The Italians have voted overwhelmingly to keep their country non-nuclear. Switzerland and Spain have banned the construction of new reactors. Japan's prime minister has called for a dramatic reduction in Japan's reliance on nuclear power. Taiwan's president did the same. Mexico has sidelined construction of 10 reactors in favor of developing natural-gas-fired plants. Belgium plans to phase out its nuclear plants by 2025.\n\nAs of 2012, countries such as Australia, Austria, Denmark, Greece, Ireland, Italy, Latvia, Liechtenstein, Luxembourg, Malta, Portugal, Israel, Serbia, Malaysia, and Norway have no nuclear power reactors and remain opposed to nuclear power.\n\nIn November 2015 and March 2017 Egypt signed preliminary agreements with Russian nuclear company Rosatom for a first VVER-1200 unit at El Dabaa to start in 2024. Discussions continue for final approval.\n\nGhana has research reactors, but no power plants. Ghana hopes to have one by 2018.\n\nKenya aims to build a 1,000 MWe nuclear power plant by 2022\n\nSouth Africa is the only country in Africa with a commercial nuclear power plant and it currently has an expansion policy.\n\nBangladesh considered building a nuclear power plant for the first time in 1961. Since then, several feasibility studies have been carried out, affirming the feasibility of the project. In 1963 the Rooppur site was selected. More recently, in 2001 Bangladesh adopted a national Nuclear Power Action Plan. On 24 June 2007, Bangladesh's government announced it will build a nuclear power plant to meet electricity shortages. The first nuclear power plant with a generation capacity between 700 and 1,000 MW will be installed by 2015 at Rooppur in Pabna district.\n\nAs of March 2014, China has 20 operating reactors, and 28 reactors under construction. Additional reactors are planned, providing 58 GWe of capacity by 2020.\n\nSix member states of the Gulf Cooperation Council (Kuwait, Saudi Arabia, Bahrain, the United Arab Emirates, Qatar and Oman) have announced that the Council is commissioning a study on the peaceful use of nuclear energy. In February 2007 they agreed with the IAEA to cooperate on a feasibility study for a regional nuclear power and desalination program.\n\nThe United Arab Emirates adopted a national policy on nuclear energy in July 2008 and a national nuclear energy law on 4 October 2009. According to the law and the policy document, the Emirates Nuclear Energy Corporation was established. Memorandums of understanding on cooperation in peaceful uses of nuclear energy are signed with France, the United States and the United Kingdom. In December 2009, the UAE decided to build a nuclear power plant with four APR1400 reactors. The first reactor to be developed by the Korea Electric Power is to come on line in 2017. The plant will be located at Barakah, from Ruwais.\n\nOn 29 March 2008, Bahrain signed a memorandum of understanding on nuclear energy with the United States.\n\nIn 2010 the Kuwait National Nuclear Energy Committee and the Russian company Rosatom signed a memorandum of understanding on the use of nuclear energy.\n\nIndia has 20 reactors operating and 6 reactors under construction.\n\nIndia has encountered effective local anti-nuclear opposition, growing national wariness about foreign nuclear reactors, and a nuclear liability controversy that threatens to prevent new reactor imports. There have been mass protests against the French-backed 9900 MW Jaitapur Nuclear Power Project in Maharashtra and the 2000 MW Koodankulam Nuclear Power Plant in Tamil Nadu. The state government of West Bengal state has also refused permission to a proposed 6000 MW facility near the town of Haripur that intended to host six Russian reactors.\n\nIn the mid-1990s, Indonesia conducted a feasibility study into constructing 12 nuclear power plants. The plan was postponed due to criticism from environmentalists and the Asian regional economic crisis in 1997. In 2006, Indonesian Government announced a plan to build its first major nuclear power plant on Muria peninsula, Jepara district, Central Java by 2015. However, this decision is not final yet. This plan is heavily criticized by environmental organisations.\n\nIn June 2007 was announced that in Gorontalo will be set up 70 MW floating nuclear power plant of Russian origin.\n\nIn the mid-1970s, Iran started construction of two PWR units at [Bushehr], but the project was suspended in 1979. In 1994, Russia agreed to complete unit 1 of Bushehr nuclear power plant and it was expected to be completed late in 2007. Also second reactor is planned at Bushehr. It also announced that a new nuclear power plant is to be built at Darkhovin in Khūzestān Province, where two plants were about to be constructed in the 1970s. Currently, Iran has reported that a power plant at Bushehr is operational.\n\nIran plans to build at least 19 more reactors with cumulative capacity of 20,000 MW by 2025 out of which at least six reactors are expected to be operational by 2020\n\nIsrael has no nuclear power plants. However, in January 2007, Israeli Infrastructure Minister Binyamin Ben-Eliezer said his country should consider producing nuclear power for civilian purposes. As a result of the nuclear emergencies at Japan's Fukushima I Nuclear Power Plant, on March 17, 2011, Prime Minister Benjamin Netanyahu indicated that Israel would not develop nuclear power.\n\nNuclear energy was a national strategic priority in Japan, but there has been concern about the ability of Japan's nuclear plants to withstand seismic activity. The Kashiwazaki-Kariwa Nuclear Power Plant was completely shut down for 21 months following an earthquake in 2007.\n\nFollowing an earthquake, tsunami, and the failure of cooling systems at the Fukushima I Nuclear Power Plant on March 11, 2011, a nuclear emergency was declared. This was the first time a nuclear emergency had been declared in Japan, and 140,000 residents within 20 km of the plant were evacuated. The total amount of radioactive material released is unclear, as the crisis is ongoing.\n\nOn 6 May 2011, Prime Minister Naoto Kan ordered the Hamaoka Nuclear Power Plant be shut down as an earthquake of magnitude 8.0 or higher is likely to hit the area within the next 30 years. Kan wanted to avoid a possible repeat of the Fukushima disaster. On 9 May 2011, Chubu Electric decided to comply with the government request. Kan later called for a new energy policy with less reliance on nuclear power.\n\nProblems in stabilizing the Fukushima I nuclear plant have hardened attitudes to nuclear power. As of June 2011, \"more than 80 percent of Japanese now say they are anti-nuclear and distrust government information on radiation\". Post-Fukushima polls suggest that somewhere \"between 41 and 54 percent of Japanese support scrapping, or reducing the numbers of, nuclear power plants\". Tens of thousands of people marched in central Tokyo in September 2011, chanting \"Sayonara nuclear power\" and waving banners, to call on Japan's government to abandon atomic energy. As of October 2011, only 11 nuclear power plants are operating. There have been electricity shortages, but Japan survived the summer without the extensive blackouts that had been predicted. An energy white paper, approved by the Japanese Cabinet in October 2011, says \"public confidence in safety of nuclear power was greatly damaged\" by the Fukushima disaster, and calls for a reduction in the nation's reliance on nuclear power.\n\nJordan has signed memorandums of understanding with the United States, United Kingdom, Canada, France, Japan, China, and South Korea. In December 2009, Jordan Atomic Energy Commission (JAEC) in cooperation with a consortium headed by the Korean Atomic Energy Research Institute signed an agreement with Daewoo Heavy Industries to build a its first 5 MW research reactor by 2014 at the Jordan University of Science and Technology. The research reactor will become a focal point for a Nuclear Technology Centre, which will train upcoming generations of nuclear engineers and scientists in the Kingdom in addition to provide irradiation services for the industrial, agricultural and medical sectors.\n\nJordan plans to start building its first nuclear power plant by 2013 at the site about from Aqaba. It will be used for electricity generation and desalination. The studies are carried out by Tractebel Engineering.\n\nJordan has also granted Areva exclusive mining rights for uranium in central Jordan.\n\nKazakhstan shut down its only NPP in 1999. In 2003, the Minister of Energy and Mines announced plans for the construction of a new NPP within the next 15 years. The two–three unit NPP is to be established on the shores of Lake Balkhash in the Karaganda region of central Kazakhstan.\n\nNorth Korea has no nuclear power program currently. Earlier the building of nuclear plant near Sinpo was started by USSR but construction was cancelled due to lack of funding. Under the Agreed Framework, North Korea agreed to end its graphite-moderated nuclear reactor program in exchange for construction of two PWRs at Kumho, but construction was suspended in November 2003. Under the Six-Party Talks, 19 September 2005 North Korea pledged to end all its nuclear programs and return to the Nuclear Non-Proliferation Treaty, in exchange for international inspections in return for benefits including energy aid and normalization of relations with Japan and the United States.\n\nSouth Korea has 18 operational nuclear power reactors, with two more under construction and scheduled to go online by 2004.\n\nAlthough Malaysia has established Nuclear Agency and been actively involved in the periodic review of the nuclear option, currently there is no nuclear power generation plant, and plans for a future nuclear plant are exploring the feasibility of such a plant.\n\nOn 15 May 2007, Myanmar and Russia signed an agreement to construct a nuclear research center in Myanmar. The center will comprise a 10 MWt light water reactor working on 20%-enriched U-235, an activation analysis laboratory, a medical isotope production laboratory, silicon doping system, nuclear waste treatment and burial facilities. Groups such as Greenpeace are concerned that such technology may pose possible security threats.\n\nPakistan operates five reactors generating 1430 MW, is building three new (3x1150). The current total nuclear generating capacity is 1430 MWe. Pakistan plans on constructing 32 reactors by 2050.\n\nSaudi Arabia hopes to build 16 reactors but has none. Saudi Arabia has commissioned Westinghouse Electric Company to build the AP1000 reactors.\n\nSyria abandoned its plans to build a VVER-440 reactor after the Chernobyl accident. The plans of nuclear program were revived at the beginning of the 2000s when Syria negotiated with Russia to build a nuclear facility that would include a nuclear power plant and a seawater atomic desalination plant.\n\nIn Taiwan nuclear energy policy is a contentious issue. On World Environment Day in June 2011, environmental groups protested against the nation's three operating nuclear power plants and the construction of a fourth plant.\n\nThe government elected in 2016 has policies that include a move toward a nuclear-free society, and is considering legislating to phase out nuclear power generation within nine years.\n\nAccording to the energy minister of Thailand, the state owned Electricity Generating Authority of Thailand will build its first two nuclear power plants by 2021. This decision was criticized by Greenpeace, which suggested to focus on alternative power supplies from hydropower and smaller biofuel plants before risking nuclear.\n\nIn the 1980s Vietnam undertook two preliminary nuclear power studies, which concluded that there was a need to introduce nuclear energy in order to meet the expected growth in electricity demand. A national energy plan stated that the nuclear power program was to be commenced by 2010. In February 2006, the government announced the first nuclear power plant would be commissioned by 2017. In June 2010, Vietnam announced that it plans to build fourteen reactors at eight locations by 2030, providing 10% of the nation's electricity. In October 2010, it signed an agreement with Russia for the construction of the country's first nuclear power plant, Ninh Thuan 1, due to begin in 2014.\n\nHowever, in November 2016 Vietnam decided to abandon nuclear power plans as they were \"not economically viable because of other cheaper sources of power.\"\n\nYemen has called for establishing The Arab Atomic Energy Agency for nuclear researches and using them for peaceful means, especially generating electricity.\n\nAlbania presently has no nuclear power plants, but in 2007 the government discussed constructing a nuclear power plant at Durrës. In addition to meeting the domestic energy demands, the plan foresaw electricity export to neighboring Balkan countries and Italy via an underwater cable, which would link the Italian and Albanian electricity networks. In April 2009, Albania and Croatia announced a plan to jointly construct a 1,500 MWe nuclear power plant on the shores of Lake Scutari (Lake Shkodër), near Albania's border with Montenegro.\n\nThe Belgian government in 2003 passed legislation banning the construction of new reactors for power generation. The existing ones were to be phased out after a 40-year lifespan. This implies that by 2025 all seven operating reactors would be shut down. The first three reactors were originally to be closed by 2015, however the lifespan of the Tihange-1 reactor was expanded through to 2025. The phase-out was confirmed in 2018.\n\nThe Bulgarian government has favored nuclear energy to generate electricity since 1956 and its first commercial nuclear reactor began to operate in 1974. Currently, two reactors are operational providing approximately 35% of the country's electricity. The government had plans to build two new units at the Belene Nuclear Power Plant. A contract with Russian Atomstroyexport has been signed for two AES-92 VVER-1000 reactors. It is expected this plant will not be built, the necessary investment is not there.\n\nThe Czechoslovak government completed its first nuclear power plant – a gas-cooled heavy water reactor – in 1972 in Bohunice. The country's first commercial nuclear power plant began operating in 1985, and the government is still committed to nuclear energy today. The Czech Republic currently has six nuclear reactors with a net MWe of 3,472 and plans to build two more 1,500 MWe reactors by 2020. According to data from 1990 to 2005, the Czech Republic posted the largest increase in nuclear energy capacity (114%) and energy production (96%) of any EU country. Furthermore, the Czech Republic exported 24,985 GWh in 2005.\n\nFinland has four commercial reactors, which provide 27% of the country's electricity. Two VVER-440 pressurized water reactors built by Atomenergoeksport and commissioned in 1977 and 1980, are located in Loviisa Nuclear Power Plant. They are operated by Fortum. Two boiling water reactors built by Asea-Atom (nowadays Westinghouse Electric Company) and commissioned in 1978 and 1980, are located at the Olkiluoto plant in Eurajoki, near Rauma. They are owned and operated by Teollisuuden Voima, a subsidiary of Pohjolan Voima. In 2002, the cabinet's decision to allow the construction of fifth reactor (the third at Olkiluoto) was accepted in the parliament. The reactor under construction is the European Pressurized Reactor, built by French company Areva. It is scheduled to start electricity production no earlier than 2015, a schedule slippage of at least six years.\n\nOn 21 April 2010, the cabinet decided to grant permits for construction of the sixth and seventh commercial reactors to Teollisuuden Voima and Fennovoima, a subsidiary of E.ON. The former will build the reactor at the Olkiluoto plant, and the latter at a new site in Pyhäjoki. The cabinet rejected the third application, by Fortum, to build a new reactor at Loviisa.\n\nThe nuclear power companies are responsible for storage and disposal of nuclear waste. Prior to 1994, Finnish companies exported their nuclear waste to the Soviet Union. However, a Finnish law passed in 1994 prohibited the transport of nuclear waste abroad. With this law, Finland became the first country that decided to encapsulate spent nuclear fuel into deep geological repositories. The construction of the first such repository, Onkalo, is set to begin in 2012, with an estimated completion date of 2020. Once in operation, the disposal process will involve putting twelve fuel assemblies into a boron steel canister and enclosing it into a copper capsule. Each capsule will then be placed in its own hole in the repository and packed with bentonite clay. The estimated cost of this project is about €818 million, which includes construction, encapsulation, and operating costs. The State Nuclear Waste Management Fund has saved approximately €1.4 billion from charges for generated electricity.\n\nAccording to a TNS Gallup survey conducted in January 2010, 48% of Finns had a positive view of nuclear power, and 17% were negative.\n\nAfter the oil crisis of the early 1970s, the French government decided in 1974 to move towards self-sufficiency in electricity production, primarily through the construction of nuclear power stations. France today produces around 78.1% of its electricity through nuclear power. Because France produces an overall electricity surplus, it exports nuclear-produced energy. The Board of Electricité de France (EDF) has approved construction of a 1630 MWe European Pressurized Reactor at Flamanville, Normandy. Construction is expected to begin in late 2007, with completion in 2012.\n\nFrance established a law in 2005 requiring that nuclear power be central to energy policy and security. Under this law, France would build a third-generation nuclear reactors, by 2015, of which it may decide to build forty more. Each EPR reactor would produce 1,600 MW of electricity versus the 900 MW that current reactors produce. The EPR reactor was also recognized as safer and more efficient. In August 2005, EDF announced that it wanted to replace all of its reactors with EPR reactors.\n\nEDF reprocesses approximately 850 of the 1,200 tons of used fuel each year. The reprocessed spent fuel is made into plutonium. The plutonium is then converted into fresh mixed oxide (MOX) fuel, which is used in over 35 reactors across Europe. These reactors can load approximately 20-50% of their cores with the MOX fuel.\n\nFollowing the 2011 Fukushima I nuclear accidents, the head of France's nuclear safety agency has said that France needs to upgrade the protection of vital functions in all its nuclear reactors to avoid a disaster in the event of a natural calamity, adding there was no need to close any plants.\n\nMany problems brought Fukushima I nuclear accidents to the nuclear power in France however. The initial budget for building Flamanville-3 of €3.3 billions jumped to €8.5 billions at the moment and the power price might reach 100EUR/MWh. EDF could not go below a price of 100GBP/MWh for a 30-year contract when they had to make an offer to UK for replacement of some old nuclear reactors. That is around 115EUR/MWh, when an average price of only 60EUR/MWh for producing power from any source could be considered profitable in Western Europe. Francois Hollande sticks to his campaign pledge to reduce the share of nuclear energy in the power supply in France to 50% from 75% by 2025. Areva, which was the symbol of the nuclear power in Europe for long time, lost over 80% of its shares and had to start investing in wind power. According to a poll conducted by BBC, the opposition to building new reactors in pro-nuclear France has risen from 66% in 2005 to 83% in 2011\n\nNuclear power in Germany accounted for 23% of national electricity consumption, before the permanent shutdown of 8 plants in March 2011. German nuclear power began with research reactors in the 1950s and 1960s with the first commercial plant coming online in 1969. It has been high on the political agenda in recent decades, with continuing debates about when the technology should be phased out. The topic received renewed attention at the start of 2007 due to the political impact of the Russia-Belarus energy dispute and in 2011 after the Fukushima I nuclear accidents.\n\nOn 30 May 2011, Germany formally announced plans to abandon nuclear energy completely within 11 years. The plan includes the immediate permanent closure of six nuclear power plants that had been temporarily shut down for testing in March 2011, and two more that have been offline a few years with technical problems. The remaining nine plants will be shut down between now and 2022. The announcement was first made by Norbert Röttgen, head of the Federal Ministry for Environment, Nature Conservation and Nuclear Safety, after late-night talks.\n\nChancellor Angela Merkel said the phase-out of plants, previously scheduled to go offline as late as 2036, would give Germany a competitive advantage in the renewable energy era, stating, \"As the first big industrialized nation, we can achieve such a transformation toward efficient and renewable energies, with all the opportunities that brings for exports, developing new technologies and jobs\". Merkel also pointed to Japan's \"helplessness\" – despite being an industrialized, technologically advanced nation – in the face of its nuclear disaster. Some German manufacturers and energy companies have criticized the plans, warning that Germany could face blackouts. The Energiewende (Energy U-turn) policies have suffered from inadequate investment in power infrastructure to bring power to markets. The reduced dependence on nuclear power has resulted in higher consumption of fossil fuels and therefore of greenhouse gas production.\n\nAfter the 1986 Chernobyl disaster Italy held a referendum, which supported shutting down Italy's four nuclear power plants. The construction of new reactors was halted and the last operating reactor was closed in 1990. In 1987, a moratorium on the construction of new nuclear plants was passed. Originally in effect until 1993, it had been extended until 2009. In 2004, a new energy law allowed joint ventures with foreign companies in relation to nuclear power plants and importing electricity from them. Following Silvio Berlusconi's victory in the 2008 election, Italy's industry minister announced that the government scheduled the construction to start the first new Italian nuclear-powered plant by 2013. On 24 February 2009, an agreement between France and Italy was signed according to which a study about the feasibility of building 4 new nuclear power plants in Italy to be conducted. On 9 July 2009 the Italian parliament passed a law on the establishment of a nuclear safety agency to be established by July 2010, and giving the government a task to select sites for new nuclear power plants. According to the 2010 Eurobarometer report only 20% of Italians support increase of the nuclear energy in the country's energy mix while 62% think that the share should be either maintained or reduced.\n\nThere was a uranium enrichment facility in Bosco Marengo that is now no longer operational and is being decommissioned by SOGIN.\n\nRomania's 1,400 MW Cernavodă Nuclear Power Plant produces around 18% of the nation's electrical power. It is based on Canadian technology and uses heavy water produced at Drobeta-Turnu Severin as its neutron moderator and water from the Danube for cooling. Two reactors are fully operational and another three are partially finished. When fully functional the plant will produce around 40% of Romania's total electricity needs.\n\nCurrently, nuclear waste is stored at the reactors for up to ten years. Then the waste is transported to dry storage. The government has conducted studies into a permanent geological repository.\n\nThere are plans to construct a second nuclear power plant in Transylvania after 2020.\n\nRussia operates 31 reactors, is building 3, and has plans for another 27. Russia has also begun building floating nuclear power plants. The £100 million ($204.9 million, 2 billion Rubles) vessel, the \"Lomonosov\", to be completed in 2010, is the first of seven plants that Moscow says will bring vital energy resources to remote Russian regions. While producing only a small fraction of the power of a standard Russian land-based plant, it can supply power to a city of 200,000, or function as a desalination plant. The Rosatom, the state-owned nuclear energy company said that at least 12 countries were also interested in buying floating nuclear plants.\n\nSerbia presently doesn't have any nuclear power plants. Previously, Vinča Nuclear Institute operated two research reactors; RA and RB. The research reactors were supplied by the USSR. The larger of the two reactors was rated at 6.5 MW and used Soviet-supplied 80% enriched uranium fuel.\n\nOn October 15, 1958, there was a criticality accident at one of the research reactors. Six workers received large doses of radiation; one died shortly afterwards. The other five received the first ever bone marrow transplants in Europe.\n\nThe nuclear research programme ended in 1968, while the reactors were switched off in 1984.\n\nAfter the 1986 Chernobyl disaster a nationwide campaign against usage of nuclear energy delivered a moratorium in a form of Law in 1989, which not only prohibited the construction of nuclear power plants, nuclear fuel fabrication and reprocessing facilities for used nuclear fuel but also criminalized any activity related to the aforementioned making it a criminal offense punishable by 5 years of prison.\n\nAfter the break up of the former Yugoslavia a new law was adopted on March 10, 1995, which confirmed the previous moratorium, although it removed the controversial article related to the nuclear criminal offense.\n\nEven though majority of people and politicians still support moratorium on usage of nuclear energy, past couple of years are marked with an increasing public campaign pointed towards repeal of the 1995 Law, or, at least, its limitation to 10 years, after which it would be possible to build nuclear reactors in Serbia.\n\nIn 1964, Spain began construction on its first of three nuclear reactors and completed construction in 1968.\nCurrently, Spain has eight nuclear reactors producing 20% of the country's electricity or 7,442 net MWe.\n\nSweden began research into nuclear energy in 1947 with the establishment of the atomic energy research organization. In 1964, the country built its first small heavy water reactor. The country decided to use hydropower and supplement it with nuclear energy to avoid the volatility in oil prices. Six reactors began commercial service in both the 1970s and 1980s, with one unit closed in 1999 and another in 2005. Currently, Sweden has 10 nuclear power reactors which provide over 40% of its electricity. On 17 June 2010, the Swedish Parliament adopted a decision allowing starting from 1 January 2011 a replacement of the existing reactors with new nuclear reactors.\n\nSwitzerland has five nuclear reactors, and around 40% of its electricity is generated by nuclear power. The country has had several referenda on the nuclear energy, beginning in 1979 with a citizens' initiative for nuclear safety, which was rejected. In 1984, there was a vote on an initiative \"for a future without further nuclear power stations\" with the result being a 55% to 45% vote against. On 23 September 1990, the people passed a motion to halt the construction of nuclear power plants (for a moratorium period of ten years) but rejected a motion to initiate a phase-out. On 18 May 2003 a motion calling for an extension to this moratorium (for another ten years) and another asking again on the question of a phase-out, were both rejected. In May 2011 the government decided it will phase out all nuclear power plants in the next twenty years\n\nUkraine operates 15 reactors, which supply 47.5% of Ukraine's electricity production of 195 billion kWh (2007). Ukraine's power sector is the twelfth-largest in the world in terms of installed capacity, with 54 gigawatts (GW). In 2006, the government planned to build 11 new reactors by the year 2030, in effect, almost doubling the current amount of nuclear power capacity.\n\nThe first full-scale nuclear reactor in Europe opened in Calder Hall, located in Cumberland, United Kingdom on October 17, 1956. Calder Hall was the world's first nuclear power reactor producing power for a national grid, though its primary purpose was military plutonium production. At its peak, Calder Hall produced 240 MWe of electricity. Over the next ten years, nine more nuclear reactors were built across the United Kingdom. The UK has decommissioned nearly all of its first generation Magnox reactors. Recently, the UK privatized its nuclear energy industry but government oversight remains. As of 2010, the United Kingdom has 19 reactors generating 18% of the country's electricity. By current accounting lifetimes all but one of them will be decommissioned by 2023, though many are likely to be life-extended. The government is encouraging the building of new generation plants as replacements. Currently, the reactors have a net capacity of 10,962 MWe.\n\nCanada operates 18 reactors accounting for about 15% of electrical generation, all in the province of Ontario except for one in New Brunswick. Increasing demands for electricity and environmental considerations have led Ontario to announce that it will maintain existing nuclear capacity by replacing older reactors with new ones. Canada has never had any serious accidents related to nuclear power, CANDU reactors are a particularly safe design. Canada is planning new reactors.\n\nMexico has one nuclear power plant, which consists of two boiling water reactors. In February 2007, contracts with Iberdrola and Alstom were signed to update the reactors by 2010. A committee has been established to recommend on new nuclear plants and the most recent proposal is for one unit to come on line by 2015 with seven more to follow it by 2025.\n\nAfter a time was thought to stop the use of nuclear energy then Mexico finally decided in 2007 by repowering the Laguna Verde Nuclear Power Station, which has two reactors of 683MW each, after the upgrading, reactors are increase their production to 817MW each, accounting 4.6% of the electricity production. On May 14, 2010 Energy Secretary Georgina Kessel announced that Mexico provides full development of nuclear energy in its energy mix as an alternative to discourage the use of fossil fuels and increase the generation of clean electricity.\n\nIn November 2011, Mexico abandoned plans to build as many as 10 new nuclear reactors and will focus instead on natural gas-fired electricity plants after boosting discoveries of the fuel.\n\nIn 2007, there were 104 (69 pressurized water reactors, 35 boiling water reactors) commercial nuclear generating units licensed to operate in the United States, producing approximately 20% of the country's electrical energy needs. In absolute terms, the United States is the world's largest supplier of commercial nuclear power. However, the development of nuclear power in the United States has been stymied ever since the Three Mile Island nuclear accident in 1979. Future development of nuclear power in the U.S. was to be enabled by the Energy Policy Act of 2005 and co-ordinated by the Nuclear Power 2010 Program, but many license applications filed with the Nuclear Regulatory Commission for proposed new reactors have been suspended or cancelled.\n\nAs of October 2011, plans for about 30 new reactors in the United States have been \"whittled down to just four, despite the promise of large subsidies and President Barack Obama's support of nuclear power, which he reaffirmed after Fukushima\". The only reactor currently under construction in America, at Watts Bar, Tennessee, was begun in 1973 and may be completed in 2012. Matthew Wald from the \"New York Times\" has reported that \"the nuclear renaissance is looking small and slow\".\n\nIn 2008, the Energy Information Administration projected almost 17 gigawatts of new nuclear power reactors by 2030, but in its 2011 projections, it \"scaled back the 2030 projection to just five\". A survey conducted in April 2011 found that 64 percent of Americans opposed the construction of new nuclear reactors. A survey sponsored by the Nuclear Energy Institute, conducted in September 2011, found that \"62 percent of respondents said they favor the use of nuclear energy as one of the ways to provide electricity in the United States, with 35 percent opposed\".\n\nAustralia has no nuclear power plants. However, Australia has up to 40% of the world's uranium deposits and is the world's second largest producer of uranium after Canada. At the same time Australia's extensive, low-cost coal and natural gas reserves have historically been used as strong arguments for avoiding nuclear power.\n\nIn 2005, the Australian government threatened to use its constitutional powers to take control of the approval process for new mines from the anti-nuclear Northern Territory government. They are also negotiating with China to weaken safeguard terms so as to allow uranium exports there. States controlled by the Australian Labor Party are blocking the development of new mines in their jurisdictions under the ALP's \"No New Mines policy.\"\n\nJohn Howard went to the November 2007 federal election with a pro-nuclear power platform but his government was defeated by Labor, which opposes nuclear power for Australia.\n\nNew Zealand has no nuclear power plants. It enacted the \"New Zealand Nuclear Free Zone, Disarmament, and Arms Control Act 1987\" which prohibits the stationing of nuclear weapons on the territory of New Zealand and the entry into New Zealand waters of nuclear armed or propelled ships. This Act of Parliament, however, does not prevent the construction of nuclear power plants. A 2008 survey shows that relatively few New Zealanders favour nuclear power as the best energy source. However a 2005 survey of business leaders showed that nearly two-thirds supported investigation of nuclear power. \nA research reactor was operated by the University of Canterbury until 1981. A nuclear reactor provided electricity for McMurdo Station, in the New Zealand Antarctic Territory from 1962-1972.\nFrom the mid-1960s until the early 1980s official energy policy was that nuclear energy would be required. The Royal Commission on Nuclear Power Generation in New Zealand in its 1978 report said that there was no immediate need for New Zealand to embark upon a nuclear power program, but suggested that early in the 21st Century \"a significant nuclear programme should be economically possible.\"\n\n\n"}
{"id": "25480490", "url": "https://en.wikipedia.org/wiki?curid=25480490", "title": "Nye Lubricants", "text": "Nye Lubricants\n\nNye Lubricants, Inc. traces its origins back to 1844 in New Bedford, Massachusetts. As the whaling industry's center, New Bedford proved a hot spot to start an oil and lubricant company. The company was founded by William Foster Nye.\n\nWilliam Foster Nye's company is still in business today. Nye Lubricants, Inc is now owned by its third set of owners and can be found in William Nye's hometown of Fairhaven, Massachusetts.\n\nNye's products are based on a range of synthetic chemistries, including polyalphaolefins, esters, glycols, polyphenylethers, silicones, alkylated naphthalenes, and all commercially available perfluoropolyethers(PFPE). Nye is also the exclusive global reseller of Pennzane multiplyalkylated cyclopentane fluids, oils and greases.\n\nNye Lubricants, Inc. formulates, manufactures, and sells synthetic lubricants, thermal coupling compounds, index-matching optical gels and fluids. Its headquarters are in Fairhaven, MA.\nNye supplies different industries, including automotive, computer printer, disc drive, mobile appliance, aerospace, defense and HB-LED OEM markets. The company also manufactures industrial maintenance lubricants for incidental food contact, high temperature and other extreme environments.\nNye is a corporate member of the National Lubricating Grease Institute(NLGI), the Society of Tribologists and Lubrication Engineers (STLE) and the European Lubricating Grease Institute (ELGI).\n\n\n"}
{"id": "24747485", "url": "https://en.wikipedia.org/wiki?curid=24747485", "title": "Oil field engine", "text": "Oil field engine\n\nThe term \"Oil Field Engine\" can refer to any sort of internal combustion engine used as a power source in the production of crude petroleum, but most commonly refers to a class of reciprocating engines built in the mid to late 19th and early 20th centuries, which is the focus of this article.\n\nWith the drilling of the first commercially successful oil well by Edwin Drake in 1859, a new industry was born- one that would rapidly demand new technologies for extraction. Initially, the steam engine used for drilling the well was kept in-place to lift the oil to the surface. As the well's production dropped off, the economic feasibility of firing a boiler and maintaining a steam operation at each individual well fell far short of the income from the well's production (often only a fraction of a barrel per day.) The ideal solution was to install a new gas engine at each well, thus eliminating hours of preparatory work and large quantities of fuel required to steam a boiler for only a few hours of production. High initial cost prevented this in most cases, so it became much more feasible to convert an abundance of existing steam engines to gas engines. The idea of a converted engine is most commonly credited to Dr. Edwin J. Fithian, a Portersville, PA physician with a great interest in mechanics. His 1897 prototype for a 10-horsepower conversion cylinder was turned down by the Oil Well Supply Company of Oil City, PA, so in 1898 Dr. Fithian partnered with John Carruthers and formed the Carruthers-Fithian Clutch Company, with headquarters in Grove City, PA. The \"half-breed\" concept (as these engines- being half steam engine, half gas engine, were often referred to) was an immediate success, with an oil producer being able to convert a steam engine with a 10HP gas cylinder and clutch for $120.00. Soon after, multiple companies capitalized on the market for conversion cylinders, most producing a simple two-stroke gas cylinder to bolt onto a steam bedplate, thus avoiding patent infringement from Carruthers-Fithian (which by 1899 had formed the Bessemer Gas Engine Company in Grove City.) Other manufacturers produced engines that were purely of an internal-combustion design, and examples of both exist to this day.\n\nWhether two or four-stroke in design, all oilfield engines share some common parts. A heavy cast iron bedplate secures the engine to its base, usually of concrete. The cylinder is attached to one end of the bedplate, the crankshaft bearings are at the other. The crankshaft rests in these bearings, with either one or two flywheels and a clutch fastened to it. A great number of oil field engines used a crosshead to connect the piston rod to the connecting rod; this slides back and forth between the bedplate and crosshead guides. Ignition in the combustion chamber is either by hot tube or spark plug.\n\nThere were a number of builders of oilfield engines, most located in the region producing Pennsylvania-Grade crude petroleum. Some of the best known include:\n\n"}
{"id": "31998337", "url": "https://en.wikipedia.org/wiki?curid=31998337", "title": "Oswego-Guardian–Texanita collision", "text": "Oswego-Guardian–Texanita collision\n\nThe \"Oswego-Guardian\"–\"Texanita\" collision was a maritime accident between two supertankers near Stilbaai, South Africa on 21 August 1972. The \"Texanita\" exploded and sank with the loss of 47 men, while a further life was lost on the \"Oswego Guardian\". The accident was a catalyst for change to marine traffic separation procedures as well as oil tanker inerting.\n\nThe \"Oswego-Guardian\" was rounding the southern tip of Africa, fully loaded with crude oil from the Middle East, while the \"Texanita\" was in ballast, and headed in the opposite direction from Trinidad to Ras Tanura. Both ships were approximately 100,000 tons deadweight and registered in Liberia. The ships collided in dense fog off Stilbaai, near Cape Agulhas. The oil vapours in the \"Texanita\"'s two empty tanks ignited, creating a massive explosion that tore the ship apart, causing it to sink in four minutes with the loss of 47 of its 50 crew; the explosion was heard away, inland from the coast.\n\nA United Nations resolution that would have made the use of inert gas safety systems mandatory for oil tankers, was still in draft in 1972. Experts believe that were the \"Texanita\" inerted, her fate might have been less tragic, possibly like that of the \"Venoil\".\n\nThe subsequent investigation determined that the masters of both ships failed to maintain an adequate lookout or to plot the course of the opposite ship; both ships also failed to reduce speed, despite observing one another on radar. At the time, the accident was the biggest tanker collision on record. One of the \"Texanita\"'s lifeboats was found at Bunbury, Western Australia two years later.\n\nThe accident contributed to an overhaul of the international maritime traffic separation system that was in force at the time.\n\n\n"}
{"id": "39359065", "url": "https://en.wikipedia.org/wiki?curid=39359065", "title": "Phase Transitions and Critical Phenomena", "text": "Phase Transitions and Critical Phenomena\n\nPhase Transitions and Critical Phenomena is a 20-volume series of books, comprising review articles on phase transitions and critical phenomena, published during 1972-2001. It is \"considered the most authoritative series on the topic\".\n\nVolumes 1-6 were edited by Cyril Domb and Melville S. Green, and after Green's death, volumes 7-20 were edited by Domb and Joel Lebowitz.\n\nVolume 4 was never published. Volume 5 was published in two volumes, as 5A and 5B.\n\n"}
{"id": "43304226", "url": "https://en.wikipedia.org/wiki?curid=43304226", "title": "Point Davenport Conservation Park", "text": "Point Davenport Conservation Park\n\nPoint Davenport Conservation Park is a protected area occupying Point Davenpoint, a headland between Foul Bay and Sturt Bay on the south coast of Yorke Peninsula in South Australia about south of Warooka. The park was proclaimed in 1987. The park is considered to be ‘an area of high biodiversity with a range of habitats including beaches and foredunes, and an estuary that is listed as a nationally important wetland.’ It is classified as an IUCN Category III protected area.\n\n"}
{"id": "536313", "url": "https://en.wikipedia.org/wiki?curid=536313", "title": "Polycarbonate", "text": "Polycarbonate\n\nPolycarbonates (PC) are a group of thermoplastic polymers containing carbonate groups in their chemical structures. Polycarbonates used in engineering are strong, tough materials, and some grades are optically transparent. They are easily worked, molded, and thermoformed. Because of these properties, polycarbonates find many applications. Polycarbonates do not have a unique resin identification code (RIC) and are identified as \"Other\", 7 on the RIC list. Products made from polycarbonate can contain trace quantities of the precursor monomer bisphenol A (BPA).\n\nPolycarbonates received their name because they are polymers containing carbonate groups (−O−(C=O)−O−). A balance of useful features, including temperature resistance, impact resistance and optical properties, positions polycarbonates between commodity plastics and engineering plastics.\n\nThe main polycarbonate material is produced by the reaction of bisphenol A (BPA) and phosgene . The overall reaction can be written as follows:\n\nThe first step of the synthesis involves treatment of bisphenol A with sodium hydroxide, which deprotonates the hydroxyl groups of the bisphenol A.\n\nThe diphenoxide (Na(OCH)CMe) reacts with phosgene to give a chloroformate, which subsequently is attacked by another phenoxide. The net reaction from the diphenoxide is:\n\nIn this way, approximately of polycarbonate is produced annually. Many other diols have been tested in place of bisphenol A (e.g., 1,1-bis(4-hydroxyphenyl)cyclohexane and dihydroxybenzophenone). The cyclohexane is used as a comonomer to suppress crystallisation tendency of the BPA-derived product. Tetrabromobisphenol A is used to enhance fire resistance. Tetramethylcyclobutanediol has been developed as a replacement for BPA.\n\nAn alternative route to polycarbonates entails transesterification from BPA and diphenyl carbonate:\nThe diphenyl carbonate was derived in part from carbon monoxide, this route being greener than the phosgene method.\n\nThe ring-opening polymerization of cyclic carbonates has been investigated.\n\nPolycarbonate is a durable material. Although it has high impact-resistance, it has low scratch-resistance. Therefore, a hard coating is applied to polycarbonate eyewear lenses and polycarbonate exterior automotive components. The characteristics of polycarbonate compare to those of polymethyl methacrylate (PMMA, acrylic), but polycarbonate is stronger and will hold up longer to extreme temperature. Polycarbonate is highly transparent to visible light, with better light transmission than many kinds of glass.\n\nPolycarbonate has a glass transition temperature of about , so it softens gradually above this point and flows above about . Tools must be held at high temperatures, generally above to make strain-free and stress-free products. Low molecular mass grades are easier to mold than higher grades, but their strength is lower as a result. The toughest grades have the highest molecular mass, but are much more difficult to process.\n\nUnlike most thermoplastics, polycarbonate can undergo large plastic deformations without cracking or breaking. As a result, it can be processed and formed at room temperature using sheet metal techniques, such as bending on a brake. Even for sharp angle bends with a tight radius, heating may not be necessary. This makes it valuable in prototyping applications where transparent or electrically non-conductive parts are needed, which cannot be made from sheet metal. PMMA/Acrylic, which is similar in appearance to polycarbonate, is brittle and cannot be bent at room temperature.\n\nMain transformation techniques for polycarbonate resins:\n\nPolycarbonate may become brittle when exposed to ionizing radiation above \n\nPolycarbonate is mainly used for electronic applications that capitalize on its collective safety features. Being a good electrical insulator and having heat-resistant and flame-retardant properties, it is used in various products associated with electrical and telecommunications hardware. It can also serve as a dielectric in high-stability capacitors. However, commercial manufacture of polycarbonate capacitors mostly stopped after sole manufacturer Bayer AG stopped making capacitor-grade polycarbonate film at the end of year 2000.\n\nThe second largest consumer of polycarbonates is the construction industry, e.g. for domelights, flat or curved glazing, and sound walls, which all use extruded flat solid or multiwall sheet, or corrugated sheet.\n\nA major application of polycarbonate is the production of Compact Discs, DVDs, and Blu-ray Discs. These discs are produced by injection molding polycarbonate into a mold cavity that has on one side a metal stamper containing a negative image of the disc data, while the other mold side is a mirrored surface.\n\nIn the automotive industry, injection-molded polycarbonate can produce very smooth surfaces that make it well-suited for sputter deposition or evaporation deposition of aluminium without the need for a base-coat. Decorative bezels and optical reflectors are commonly made of polycarbonate. Due to its low weight and high impact resistance, polycarbonate is the dominant material for making automotive headlamp lenses. However, automotive headlamps require outer surface coatings because of its low scratch resistance and susceptibility to ultraviolet degradation (yellowing). The use of polycarbonate in automotive applications is limited to low stress applications. Stress from fasteners, plastic welding and molding render polycarbonate susceptible to stress corrosion cracking when it comes in contact with certain accelerants such as salt water and plastisol. It can be laminated to make bullet-proof \"glass\", although \"bullet-resistant\" is more accurate for the thinner windows, such as are used in bullet-resistant windows in automobiles. The thicker barriers of transparent plastic used in teller's windows and barriers in banks are also polycarbonate.\nThe third generation Mazda MX-5 was offered with a Power Retractable Hard Top (PRHT) variant that uses a folding hardtop made of polycarbonate, which added only to the weight of a comparably equipped soft top without diminishing trunk space when retracted.\n\nSo-called \"theft-proof\" large plastic packaging for smaller items, which cannot be opened by hand, is uniformly made from polycarbonate.\nThe cockpit canopy of the Lockheed Martin F-22 Raptor jet fighter is made from a piece of high optical quality polycarbonate, and is the largest piece of its type formed in the world.\n\nKereta Api Indonesia, the major railway operator in Indonesia, uses polycarbonate solid sheet for their engine and passenger cars fleet since 2016 due to high train stone throwing frequency.\n\nPolycarbonate, being a versatile material with attractive processing and physical properties, has attracted myriad smaller applications. The use of injection molded drinking bottles, glasses and food containers is common, but the use of BPA in the manufacture of polycarbonate has stirred serious controversy (see Potential hazards in food contact applications), leading to development and use of \"BPA-free\" plastics in various formulations.\n\nPolycarbonate is commonly used in eye protection, as well as in other projectile-resistant viewing and lighting applications that would normally indicate the use of glass, but require much higher impact-resistance. Polycarbonate lenses also protect the eye from UV light. Many kinds of lenses are manufactured from polycarbonate, including automotive headlamp lenses, lighting lenses, sunglass/eyeglass lenses, swimming goggles and SCUBA masks, and safety glasses/goggles/visors including visors in sporting helmets/masks and police riot gear (helmet visors, riot shields, etc.). Windscreens in small motorized vehicles are commonly made of polycarbonate, such as for motorcycles, ATVs, golf carts, and small planes and helicopters.\n\nTypical products of sheet/film production include applications in advertisement (signs, displays, poster protection). But also applications as automotive safety glazing (ECE R 43). \n\nThe light weight of polycarbonate as opposed to glass has led to development of electronic display screens that replace glass with polycarbonate, for use in mobile and portable devices. Such displays include newer e-ink and some LCD screens, though CRT, plasma screen and other LCD technologies generally still require glass for its higher melting temperature and its ability to be etched in finer detail.\n\nAs more and more governments are restricting the use of glass in pubs and clubs due to the increased incidence of glassings, polycarbonate glasses are becoming popular for serving alcohol because of their strength, durability, and glass-like feel.\n\nOther miscellaneous items include durable, lightweight luggage, MP3/digital audio player cases, ocarinas, computer cases, fountain pens, riot shields, instrument panels, tealight candle containers and blender jars. Many toys and hobby items are made from polycarbonate parts, like fins, gyro mounts, and flybar locks in radio-controlled helicopters, and transparent LEGO (ABS is used for opaque pieces).\n\nStandard polycarbonate resins are not suitable for long term exposure to UV radiation. To overcome this the primary resin can have UV stabilisers added. These grades are sold as UV stabilized polycarbonate to injection moulding and extrusion companies. Other applications including polycarbonate sheet may have the anti-UV layer added as a special coating or a coextrusion for enhanced weathering resistance.\n\nPolycarbonate is also used as a printing substrate for nameplates and other forms of industrial grade under printed products. The polycarbonate provides a barrier to wear, elements, and fading.\n\nMany polycarbonate grades are used in medical applications and comply with both ISO 10993-1 and USP Class VI standards (occasionally referred to as PC-ISO). Class VI is the most stringent of the six USP ratings. These grades can be sterilized using steam at 120 °C, gamma radiation, or by the ethylene oxide (EtO) method. However, scientific research indicates possible problems with biocompatibility. Dow Chemical strictly limits all its plastics with regard to medical applications. More recently, scientists at the IBM Almaden Research Center have developed aliphatic polycarbonates with improved biocompatibility and degradability for nanomedicine applications.\n\nSome major smartphone manufacturers use polycarbonate. Nokia has used polycarbonate in their phones starting with the N9's unibody case in 2011. This practice continues with various phones in the Lumia series. Samsung has started using polycarbonate with Galaxy S III's battery cover in 2012. This practice continues with various phones in the Galaxy series. Apple started using polycarbonate with the iPhone 5C's unibody case in 2013.\n\nPolycarbonates were first discovered in 1898 by Alfred Einhorn, a German scientist working at the University of Munich. However, after 30 years of laboratory research, this class of materials was abandoned without commercialization. Research resumed in 1953, when Hermann Schnell at Bayer in Uerdingen, Germany patented the first linear polycarbonate. The brand name \"Merlon\" was registered in 1955, Later changed to Makrolon in the 1980s.\n\nAlso in 1953, and one week after the patent was submitted by Bayer, Daniel Fox at General Electric in Schenectady, New York, independently submitted a closely related patent on a branched polycarbonate. Both companies filed for U.S. patents in 1955, and agreed that the company lacking priority would be granted a license to the technology. Once patent priority was resolved, in Bayer's favor, Bayer began commercial production under the trade name Merlon in 1958 and GE began production under the name Lexan in 1960. The production of Lexan was taken over by SABIC in 2007. There are many applications for the LEXAN polycarbonate. One of them is patented production of the motorcycle helmets that meet the worldwide safety standards and requirements for this type of safety equipment. Their patented innovative Vision Protection System (VPS) is specific treatment of polycarbonate that makes it fog- and scratch-resistant.\n\nAfter 1970, the brownish original polycarbonate tint was improved to \"glass-clear.\"\n\nThe use of polycarbonate containers for the purpose of food storage is controversial. The basis of this controversy is their hydrolysis (degradation by water, often referred to as leaching) occurring at high temperature, releases bisphenol A:\n\nMore than 100 studies have explored the bioactivity of bisphenol A derived from polycarbonates. Bisphenol A appeared to be released from polycarbonate animal cages into water at room temperature and it may have been responsible for enlargement of the reproductive organs of female mice. However, the animal cages used in the research were fabricated from industrial grade polycarbonate, rather than FDA food grade polycarbonate.\n\nAn analysis of the literature on bisphenol A leachate low-dose effects by vom Saal and Hughes published in August 2005 seems to have found a suggestive correlation between the source of funding and the conclusion drawn. Industry-funded studies tend to find no significant effects whereas government-funded studies tend to find significant effects.\nSodium hypochlorite bleach and other alkali cleaners catalyze the release of the bisphenol A from polycarbonate containers. Alcohol is one recommended organic solvent for cleaning grease and oils from polycarbonate.\n\nStudies have shown that at high temperatures between 70 and 80 °C and high humidity, polycarbonates hydrolyzes to bisphenol A (BPA). This condition is similar to that observed in most incinerators. After about 30 days under such conditions, surface crystals were formed. Measurements indicated that about 70% by mass of the surface crystals were bisphenol A (BPA). BPA is a compound that is currently on the list of potential environmental hazardous chemicals. It is on the watch list of many countries, such as United States and Germany.\n\nThe leaching of BPA from polycarbonate can also occur at environmental temperature and normal pH (in landfills). The amount of leaching increases as the material becomes older. A study found that the decomposition of BPA in landfills (under anaerobic conditions) does not occur. It will therefore be persistent in landfills. Eventually, it will find its way into water bodies and contribute to aquatic pollution.\n\nIn the presence of UV light, oxidation of this polymer yields compounds such as ketones, phenols, o-phenoxybenzoic acid, benzyl alcohol and other unsaturated compounds. This has been suggested through kinetic and spectral studies. The yellow color formed after long exposure to sun can also be related to further oxidation of phenolic end group\n\nThis product can be further oxidized to form smaller unsaturated compounds. This can proceed by two different pathways, the products formed depends on which mechanism takes place.\n\n\n\nPhoto-oxidation reaction.\n\nPhoto-aging is another degradation route for polycarbonates. Polycarbonate molecules (such as the aromatic ring) absorb UV radiation. This absorbed energy causes cleavage of covalent bonds, which initiates the photo-aging process. The reaction can be propagated by side chain oxidation, ring oxidation or photo-fries rearrangement. Products formed include phenyl salicylate, dihydroxybenzophenone groups, and hydroxydiphenyl ether groups.\n\nPolycarbonate phenyl salicylate 2,2-dihydroxybenzophenone\n\nWaste polycarbonate will degrade at high temperatures to form solid, liquid and gaseous pollutants. A study showed that the products were, by mass, about 40–50% liquid, 14–16% gases, while 34–43% remained as solid residue. Liquid products contained mainly phenol derivatives (∼75%) and bisphenol (∼10%) also present. Therefore, burning of these discs is also not a viable method of disposal.\n\nPhenol derivatives are environmental pollutants, classified as volatile organic compounds (VOC). Studies show that they are likely to facilitate ground-level ozone formation and increase photo-chemical smog. In aquatic bodies, they can potentially accumulate in organisms. They are persistent in landfills, do not readily evaporate and would remain in the atmosphere.\n\nIn 2001, a species of fungus, \"Geotrichum candidum\", was found to consume the polycarbonate found in compact discs (CD). This has prospects for bioremediation.\n\n"}
{"id": "41550308", "url": "https://en.wikipedia.org/wiki?curid=41550308", "title": "Pony Express Pipeline", "text": "Pony Express Pipeline\n\nThe Pony Express Pipeline (PXP) is a pipeline connecting Guernsey, Wyoming with the oil hub of Cushing, Oklahoma.\n\nIn 2013 agreement was reached to convert the natural gas pipeline to carry crude oil from the Bakken formation shale plays in North Dakota and Montana. It is supposed to connect with the planned Double H Pipeline. The project is being developed by Tallgrass Pony Express Pipeline, LLC (Tallgrass Energy Partners). Construction was completed in October 2014. The Pony Express Pipeline will have a capacity of and be expandable to more than .\n\n\n"}
{"id": "35474809", "url": "https://en.wikipedia.org/wiki?curid=35474809", "title": "Roles of chemical elements", "text": "Roles of chemical elements\n\nThis table is designed to show the role(s) performed by each chemical element, in nature and in technology.\n\nZ = Atomic numberSym. = SymbolPer. = PeriodGr. = Group\n\n\n"}
{"id": "16075188", "url": "https://en.wikipedia.org/wiki?curid=16075188", "title": "Saab 9-X Biohybrid", "text": "Saab 9-X Biohybrid\n\nThe Saab 9-X Biohybrid concept car was developed by Saab in 2008, and first shown on the Geneva Motor Show. It is based on the GM Delta platform, and was thought to give an idea of the cancelled Saab 9-1X.\n\nIt has keyless entry and small cameras instead of rear-view mirrors. On the roof, there is a solar panel, like in the Saab EV-1, for charging the battery, both when driving and when parked. The headlights are automatic. A front mounted camera detects if you meet another car.\n\nThe front of the car follows the design of the Saab Aero-X while the rear has a cut off look, similar to the Saab 9-X. It is powered by a direct injected, E85 flex fuel capable turbocharged inline four cylinder engine of 1.4 liters giving at 5000 rpm and at 1750-5000 rpm. Connected to the engine by a belt is GM's BAS+ hybrid system which can provide a boost up to 19 hp. \n\nThe engine is connected to a manual six speed gearbox with automatic clutch. 0-100 km/h (62 mph) is achieved in 7.9 seconds and it has a top speed of . It has four seats although the headroom in the rear is somewhat restricted. As the name suggests, it runs both regular gasoline and E85. \n\nThe fuel consumption (running on E85) is and the tank to wheel CO emissions are 105 g/km. If driven faster than , the rear spoiler is extended and an underbody diffuser is deployed from the bottom of the rear bumper. When braking at speeds above , the spoiler acts like an air brake and also is used to increase the downforce of the car.\n\n"}
{"id": "52579969", "url": "https://en.wikipedia.org/wiki?curid=52579969", "title": "Sahara and Sahel Observatory", "text": "Sahara and Sahel Observatory\n\nThe Sahara and Sahel Observatory (, OSS) is an African intergovernmental organisation established in 1992 and based in Tunis, Tunisia. Its aim is to protect the environment in Sahara and Sahel, supervise the usage of natural resources in the region, and lobby for environmental accords, especially those pertaining desertification and climate change. The membership of the organisation comprises 22 African countries, five countries outside Africa (Canada, France, Germany, Italy, Switzerland), ten international organisations (including five sub-regional representatives from West, East and North Africa) and one non-governmental organisation. The organisation raised €17 million between 2006 and 2011. In April 2016, the Kingdom of Morocco was elected to a four-year term presiding the organisation.\n\nSome of OSS projects include the Long Term Ecological Observatory Network (, ROSELT), which consists of 25 observatories in 12 countries; a drought early warning system in Maghreb; and cross-border groundwater management in Algeria, Libya and Tunisia.\n"}
{"id": "791863", "url": "https://en.wikipedia.org/wiki?curid=791863", "title": "Seebeck coefficient", "text": "Seebeck coefficient\n\nThe Seebeck coefficient (also known as thermopower, thermoelectric power, and thermoelectric sensitivity) of a material is a measure of the magnitude of an induced thermoelectric voltage in response to a temperature difference across that material, as induced by the Seebeck effect. The SI unit of the Seebeck coefficient is volts per kelvin (V/K), although it is more often given in microvolts per kelvin (μV/K).\n\nThe use of materials with a high Seebeck coefficient is one of many important factors for the efficient behaviour of thermoelectric generators and thermoelectric coolers. More information about high-performance thermoelectric materials can be found in the Thermoelectric materials article. In thermocouples the Seebeck effect is used to measure temperatures, and for accuracy it is desirable to use materials with a Seebeck coefficient that is stable over time.\n\nPhysically, the magnitude and sign of the Seebeck coefficient can be approximately understood as being given by the entropy per unit charge carried by electrical currents in the material. It may be positive or negative. In conductors that can be understood in terms of independently moving, nearly-free charge carriers, the Seebeck coefficient is negative for negatively charged carriers (such as electrons), and positive for positively charged carriers (such as electron holes).\n\nOne way to define the Seebeck coefficient is the voltage built up when a small temperature gradient is applied to a material, and when the material has come to a steady state where the current density is zero everywhere. If the temperature difference Δ\"T\" between the two ends of a material is small, then the Seebeck coefficient of a material is defined as:\nwhere Δ\"V\" is the thermoelectric voltage seen at the terminals. (See below for more on the signs of Δ\"V\" and Δ\"T\".)\n\nNote that the voltage shift expressed by the Seebeck effect cannot be measured directly, since the measured voltage (by attaching a voltmeter) contains an additional voltage contribution, due to the temperature gradient and Seebeck effect in the measurement leads. The voltmeter voltage is always dependent on \"relative\" Seebeck coefficients among the various materials involved.\n\nMost generally and technically, the Seebeck coefficient is defined in terms of the portion of electric current driven by temperature gradients, as in the vector differential equation\nwhere formula_3 is the current density, formula_4 is the electrical conductivity, formula_5 is the voltage gradient, and formula_6 is the temperature gradient. The zero-current, steady state special case described above has formula_7, which implies that the two current density terms have cancelled out and so formula_8.\n\nThe sign is made explicit in the following expression:\nThus, if \"S\" is positive, the end with the higher temperature has the lower voltage, and vice versa. The voltage gradient in the material will point against the temperature gradient.\n\nThe Seebeck effect is generally dominated by the contribution from charge carrier diffusion (see below) which tends to push charge carriers towards the cold side of the material until a compensating voltage has built up. As a result, in p-type semiconductors (which have only positive mobile charges, electron holes), \"S\" is positive. Likewise, in n-type semiconductors (which have only negative mobile charges, electrons), \"S\" is negative. \nIn most conductors, however, the charge carriers exhibit both hole-like and electron-like behaviour and the sign of \"S\" usually depends on which of them predominates.\n\nAccording to the second Thomson relation (which holds for all non-magnetic materials in the absence of an externally applied magnetic field), the Seebeck coefficient is related to the Peltier coefficient formula_10 by the exact relation\nwhere formula_12 is the thermodynamic temperature.\n\nAccording to the first Thomson relation and under the same assumptions about magnetism, the Seebeck coefficient is related to the Thomson coefficient formula_13 by\nThe constant of integration is such that formula_15 at absolute zero, as required by Nernst's theorem.\n\nIn practice the absolute Seebeck coefficient is difficult to measure directly, since the voltage output of a thermoelectric circuit, as measured by a voltmeter, only depends on \"differences\" of Seebeck coefficients. This is because electrodes attached to a voltmeter must be placed onto the material in order to measure the thermoelectric voltage. The temperature gradient then also typically induces a thermoelectric voltage across one leg of the measurement electrodes. Therefore, the measured Seebeck coefficient is a contribution from the Seebeck coefficient of the material of interest and the material of the measurement electrodes. This arrangement of two materials is usually called a thermocouple.\n\nThe measured Seebeck coefficient is then a contribution from both and can be written as:\n\nAlthough only relative Seebeck coefficients are important for externally measured voltages, the absolute Seebeck coefficient can be important for other effects where voltage is measured indirectly. Determination of the absolute Seebeck coefficient therefore requires more complicated techniques and is more difficult, but such measurements have been performed on standard materials. These measurements only had to be performed once for all time, and for all materials; for any other material, the absolute Seebeck coefficient can be obtained by performing a relative Seebeck coefficient measurement against a standard material.\n\nA measurement of the Thomson coefficient formula_17, which expresses the strength of the Thomson effect, can be used to yield the absolute Seebeck coefficient through the relation: formula_18, provided that formula_17 is measured down to absolute zero. The reason this works is that formula_20 is expected to decrease to zero as the temperature is brought to zero—a consequence of Nernst's theorem. Such a measurement based on the integration of formula_21 was published in 1932, though it relied on the interpolation of the Thomson coefficient in certain regions of temperature.\n\nSuperconductors have zero Seebeck coefficient, as mentioned below. By making one of the wires in a thermocouple superconducting, it is possible to get a direct measurement of the absolute Seebeck coefficient of the other wire, since it alone determines the measured voltage from the entire thermocouple. A publication in 1958 used this technique to measure the absolute Seebeck coefficient of lead between 7.2 K and 18 K, thereby filling in an important gap in the previous 1932 experiment mentioned above.\n\nThe combination of the superconductor-thermocouple technique up to 18 K, with the Thomson-coefficient-integration technique above 18 K, allowed determination of the absolute Seebeck coefficient of lead up to room temperature. By proxy, these measurements led to the determination of absolute Seebeck coefficients for \"all materials\", even up to higher temperatures, by a combination of Thomson coefficient integrations and thermocouple circuits.\n\nThe difficulty of these measurements, and the rarity of reproducing experiments, lends some degree of uncertainty to the absolute thermoelectric scale thus obtained. In particular, the 1932 measurements may have incorrectly measured the Thomson coefficient over the range 20 K to 50 K. Since nearly all subsequent publications relied on those measurements, this would mean that all of the commonly used values of absolute Seebeck coefficient (including those shown in the figures) are too low by about 0.3 μV/K, for all temperatures above 50 K.\n\nIn the table below are Seebeck coefficients at room temperature for some common, nonexotic materials, measured relative to platinum.\nThe Seebeck coefficient of platinum itself is approximately −5 μV/K at room temperature, and so the values listed below should be compensated accordingly. For example, the Seebeck coefficients of Cu, Ag, Au are 1.5 μV/K, and of Al −1.5 μV/K. The Seebeck coefficient of semiconductors very much depends on doping, with generally positive values for p doped materials and negative values for n doping.\n\nA material's temperature, crystal structure, and impurities influence the value of thermoelectric coefficients. The Seebeck effect can be attributed to two things: charge-carrier diffusion and phonon drag.\n\nOn a fundamental level, an applied voltage difference refers to a difference in the thermodynamic chemical potential of charge carriers, and the direction of the current under a voltage difference is determined by the universal thermodynamic process in which (given equal temperatures) particles flow from high chemical potential to low chemical potential. In other words, the direction of the current in Ohm's law is determined via the thermodynamic arrow of time (the difference in chemical potential could be exploited to produce work, but is instead dissipated as heat which increases entropy). On the other hand, for the Seebeck effect not even the sign of the current can be predicted from thermodynamics, and so to understand the origin of the Seebeck coefficient it is necessary to understand the \"microscopic\" physics.\n\nCharge carriers (such as thermally excited electrons) constantly diffuse around inside a conductive material. Due to thermal fluctuations, some of these charge carriers travel with a higher energy than average, and some with a lower energy. When no voltage differences or temperature differences are applied, the carrier diffusion perfectly balances out and so on average one sees no current: formula_22. A net current can be generated by applying a voltage difference (Ohm's law), or by applying a temperature difference (Seebeck effect). To understand the microscopic origin of the thermoelectric effect, it is useful to first describe the microscopic mechanism of the normal Ohm's law electrical conductance—to describe what determines the formula_4 in formula_24. Microscopically, what is happening in Ohm's law is that higher energy levels have a higher concentration of carriers per state, on the side with higher chemical potential. For each interval of energy, the carriers tend to diffuse and spread into the area of device where there are fewer carriers per state of that energy. As they move, however, they occasionally scatter dissipatively, which re-randomizes their energy according to the local temperature and chemical potential. This dissipation empties out the carriers from these higher energy states, allowing more to diffuse in. The combination of diffusion and dissipation favours an overall drift of the charge carriers towards the side of the material where they have a lower chemical potential.\n\nFor the thermoelectric effect, now, consider the case of uniform voltage (uniform chemical potential) with a temperature gradient. In this case, at the hotter side of the material there is more variation in the energies of the charge carriers, compared to the colder side. This means that high energy levels have a higher carrier occupation per state on the hotter side, but also the hotter side has a \"lower\" occupation per state at lower energy levels. As before, the high-energy carriers diffuse away from the hot end, and produce entropy by drifting towards the cold end of the device. However, there is a competing process: at the same time low-energy carriers are drawn back towards the hot end of the device. Though these processes both generate entropy, they work against each other in terms of charge current, and so a net current only occurs if one of these drifts is stronger than the other. The net current is given by formula_25, where (as shown below) the thermoelectric coefficient formula_26 depends literally on how conductive high-energy carriers are, compared to low-energy carriers. The distinction may be due to a difference in rate of scattering, a difference in speeds, a difference in density of states, or a combination of these effects.\n\nThe processes described above apply in materials where each charge carrier sees an essentially static environment so that its motion can be described independently from other carriers, and independent of other dynamics (such as phonons). In particular, in electronic materials with weak electron-electron interactions, weak electron-phonon interactions, etc. it can be shown in general that the linear response conductance is\nand the linear response thermoelectric coefficient is\nwhere formula_29 is the energy-dependent conductivity, and formula_30 is the Fermi–Dirac distribution function. These equations are known as the Mott relations, of Sir Nevill Francis Mott. The derivativeformula_31 is a function peaked around the chemical potential (Fermi level) formula_32 with a width of approximately formula_33. The energy-dependent conductivity (a quantity that cannot actually be directly measured — one only measures formula_34) is calculated as formula_35 where formula_36 is the electron diffusion constant and formula_37 is the electronic density of states (in general, both are functions of energy).\n\nIn materials with strong interactions, none of the above equations can be used since it is not possible to consider each charge carrier as a separate entity. The Wiedemann–Franz law can also be exactly derived using the non-interacting electron picture, and so in materials where the Wiedemann–Franz law fails (such as superconductors), the Mott relations also generally tend to fail.\n\nThe formulae above can be simplified in a couple of important limiting cases:\n\nIn semimetals and metals, where transport only occurs near the Fermi level and formula_29 changes slowly in the range formula_39, one can perform a Sommerfeld expansion formula_40, which leads to\nThis expression is sometimes called \"the Mott formula\", however it is much less general than Mott's original formula expressed above.\n\nIn the free electron model with scattering, the value of formula_42 is of order formula_43, where formula_44 is the Fermi temperature, and so a typical value of the Seebeck coefficient in the Fermi gas is formula_45 (the prefactor varies somewhat depending on details such as dimensionality and scattering). In highly conductive metals the Fermi temperatures are typically around 10 – 10 K, and so it is understandable why their absolute Seebeck coefficients are only of order 1 – 10 μV/K at room temperature. Note that whereas the free electron model predicts a negative Seebeck coefficient, real metals actually have complicated band structures and may exhibit positive Seebeck coefficients (examples: Cu, Ag, Au).\n\nThe fraction formula_46 in semimetals is sometimes calculated from the measured derivative of formula_47 with respect to some energy shift induced by field effect. This is not necessarily correct and the estimate of formula_42 can be incorrect (by a factor of two or more), since the disorder potential depends on screening which also changes with field effect.\n\nIn semiconductors at low levels of doping, transport only occurs far away from the Fermi level. At low doping in the conduction band (where formula_49, where formula_50 is the minimum energy of the conduction band edge), one has formula_51. Approximating the conduction band levels' conductivity function as formula_52 for some constants formula_53 and formula_54,\nwhereas in the valence band when formula_56 and formula_57,\nThe values of formula_54 and formula_60 depend on material details; in bulk semiconductor these constants range between 1 and 3, the extremes corresponding to acoustic-mode lattice scattering and ionized-impurity scattering.\n\nIn extrinsic (doped) semiconductors either the conduction or valence band will dominate transport, and so one of the numbers above will give the measured values. In general however the semiconductor may also be intrinsic in which case the bands conduct in parallel, and so the measured values will be\nThe highest Seebeck coefficient is obtained when the semiconductor is lightly doped, however a high Seebeck coefficient is not necessarily useful. For thermoelectric power devices (coolers, generators) it is more important to maximize the thermoelectric power factor formula_62, or the thermoelectric figure of merit, and the optimum generally occurs at high doping levels.\n\nPhonons are not always in local thermal equilibrium; they move against the thermal gradient. They lose momentum by interacting with electrons (or other carriers) and imperfections in the crystal. If the phonon-electron interaction is predominant, the phonons will tend to push the electrons to one end of the material, hence losing momentum and contributing to the thermoelectric field. This contribution is most important in the temperature region where phonon-electron scattering is predominant. This happens for\n\nwhere formula_64 is the Debye temperature. At lower temperatures there are fewer phonons available for drag, and at higher temperatures they tend to lose momentum in phonon-phonon scattering instead of phonon-electron scattering. This region of the thermopower-versus-temperature function is highly variable under a magnetic field.\n\nThe Seebeck coefficient of a material corresponds thermodynamically to the amount of entropy \"dragged along\" by the flow of charge inside a material; it is in some sense the entropy per unit charge in the material.\n\nSuperconductors have zero Seebeck coefficient, because the current-carrying charge carriers (Cooper pairs) have no entropy; hence, the transport of charge carriers (the supercurrent) has zero contribution from any temperature gradient that might exist to drive it.\n"}
{"id": "20646704", "url": "https://en.wikipedia.org/wiki?curid=20646704", "title": "Sewage", "text": "Sewage\n\nSewage (or domestic wastewater or municipal wastewater) is a type of wastewater that is produced by a community of people. It is characterized by volume or rate of flow, physical condition, chemical and toxic constituents, and its bacteriologic status (which organisms it contains and in what quantities). It consists mostly of greywater (from sinks, tubs, showers, dishwashers, and clothes washers), blackwater (the water used to flush toilets, combined with the human waste that it flushes away); soaps and detergents; and toilet paper (less so in regions where bidets are widely used instead of paper). \n\nSewage usually travels from a building's plumbing either into a sewer, which will carry it elsewhere, or into an onsite sewage facility (of which there are many kinds). Whether it is combined with surface runoff in the sewer depends on the sewer design (sanitary sewer or combined sewer). The reality is, however, that most wastewater produced globally remains untreated causing widespread water pollution, especially in low-income countries: A global estimate by UNDP and UN-Habitat is that 90% of all wastewater generated is released into the environment untreated. In many developing countries the bulk of domestic and industrial wastewater is discharged without any treatment or after primary treatment only.\n\nThe term sewage is nowadays regarded as an older term and is being more and more replaced by \"wastewater\". In general American English usage, the terms \"sewage\" and \"sewerage\" mean the same thing. In common British usage, and in American technical and professional English usage, \"sewerage\" refers to the infrastructure that conveys sewage.\n\nBefore the 20th century, sewers usually discharged into a body of water such as a stream, river, lake, bay, or ocean. There was no treatment, so the breakdown of the human waste was left to the ecosystem. Today, the goal is that sewers route their contents to a wastewater treatment plant rather than directly to a body of water. In many countries, this is the norm; in many developing countries, it may be a yet-unrealized goal. \n\nCurrent approaches to sewage management may include handling surface runoff separately from sewage, handling greywater separately from blackwater (flush toilets), and coping better with abnormal events (such as peaks stormwater volumes from extreme weather).\n\nProper collection and safe, nuisance-free disposal of the liquid wastes of a community are legally recognized as a necessity in an urbanized, industrialized society. \n\n\nSewage is a complex mixture of chemicals, with many distinctive chemical characteristics. These include high concentrations of ammonium, nitrate, nitrogen, phosphorus, high conductivity (due to high dissolved solids), high alkalinity, with pH typically ranging between 7 and 8. The organic matter of sewage is measured by determining its biological oxygen demand (BOD) or the chemical oxygen demand (COD).\n\nSewage contains human feces, and therefore often contains pathogens of one of the four types: \nSewage can be monitored for both disease-causing and benign organisms with a variety of techniques. Traditional techniques involve filtering, staining, and examining samples under a microscope. Much more sensitive and specific testing can be accomplished with DNA sequencing, such as when looking for rare organisms, attempting eradication, testing specifically for drug-resistant strains, or discovering new species. Sequencing DNA from an environmental sample is known as metagenomics.\n\nSewage also contains environmental persistent pharmaceutical pollutants. Trihalomethanes can also be present as a result of past disinfection.\n\nSewage has also been analyzed to determine relative rates of use of prescription and illegal drugs among municipal populations.\n\nAll categories of sewage are likely to carry pathogenic organisms that can transmit disease to humans and animals. Sewage also contains organic matter that can cause odor and attract flies.\n\nSewage contains nutrients that may cause eutrophication of receiving water bodies; and can lead to ecotoxicity.\n\nA system of sewer pipes (sewers) collects sewage and takes it for treatment or disposal. The system of sewers is called \"sewerage\" or \"sewerage system\" (see London sewerage system) in British English and \"sewage system\" in American English. Where a main sewerage system has not been provided, sewage may be collected from homes by pipes into septic tanks or cesspits, where it may be treated or collected in vehicles and taken for treatment or disposal. Properly functioning septic tanks require emptying every 2–5 years depending on the load of the system.\n\nSewage treatment is the process of removing the contaminants from sewage to produce liquid and solid (sludge) suitable for discharge to the environment or for reuse. It is a form of waste management. A septic tank or other on-site wastewater treatment system such as biofilters or constructed wetlands can be used to treat sewage close to where it is created.\n\nSewage treatment results in sewage sludge which requires sewage sludge treatment before safe disposal or reuse. Under certain circumstances, the treated sewage sludge might be termed \"biosolids\" and can be used as a fertilizer.\n\nIn developed countries sewage collection and treatment is typically subject to local and national regulations and standards.\n\nRaw sewage is also disposed of to rivers, streams, and the sea in many parts of the world. Doing so can lead to serious pollution of the receiving water. This is common in developing countries and may still occur in some developed countries, for various reasons – usually related to costs.\n\nShips at sea are forbidden from discharging their sewage overboard unless three miles or more from shore.\n\nIncreasingly, agriculture is using untreated wastewater for irrigation. Cities provide lucrative markets for fresh produce, so are attractive to farmers. However, because agriculture has to compete for increasingly scarce water resources with industry and municipal users, there is often no alternative for farmers but to use water polluted with urban waste, including sewage, directly to water their crops. There can be significant health hazards related to using water loaded with pathogens in this way, especially if people eat raw vegetables that have been irrigated with the polluted water.\n\nThe International Water Management Institute has worked in India, Pakistan, Vietnam, Ghana, Ethiopia, Mexico and other countries on various projects aimed at assessing and reducing risks of wastewater irrigation. They advocate a ‘multiple-barrier’ approach to wastewater use, where farmers are encouraged to adopt various risk-reducing behaviours. These include ceasing irrigation a few days before harvesting to allow pathogens to die off in the sunlight, applying water carefully so it does not contaminate leaves likely to be eaten raw, cleaning vegetables with disinfectant or allowing fecal sludge used in farming to dry before being used as a human manure. The World Health Organization has developed guidelines for safe water use.\n\nCouncil Directive 91/271/EEC on Urban Wastewater Treatment was adopted on 21 May 1991, amended by the Commission Directive 98/15/EC.\nCommission Decision 93/481/EEC defines the information that Member States should provide the Commission on the state of implementation of the Directive.\n\nThe words \"sewage\" and \"sewer\" came from Old French \"essouier\" \"to drain\", which came from Latin \"exaquāre\". Their formal Latin antecedents are \"exaquāticum\" and \"exaquārium\". \n\nBoth words are descended from Old French \"assewer\", derived from the Latin \"exaquare\", \"to drain out (water)\".\n\n"}
{"id": "3656002", "url": "https://en.wikipedia.org/wiki?curid=3656002", "title": "Solid-state electronics", "text": "Solid-state electronics\n\nSolid-state electronics means semiconductor electronics; electronic equipment using semiconductor devices such as semiconductor diodes, transistors, and integrated circuits (ICs). The term is also used for devices in which semiconductor electronics which have no moving parts replace devices with moving parts, such as the solid-state relay in which transistor switches are used in place of a moving-arm electromechanical relay, or the solid state disk (SSD) a type of semiconductor memory used in computers to replace hard disk drives, which store data on a rotating disk. \n\nThe term \"solid state\" became popular in the beginning of the semiconductor era in the 1960s to distinguish this new technology based on the transistor, in which the electronic action of devices occurred in a solid state, from previous electronic equipment that used vacuum tubes, in which the electronic action occurred in a gaseous state. A semiconductor device works by controlling an electric current consisting of electrons or holes moving within a solid crystalline piece of semiconducting material such as silicon, while the thermionic vacuum tubes it replaced worked by controlling current conducted by a gas of particles, electrons or ions, moving in a vacuum within a sealed tube. Although the first solid state electronic device was the cat's whisker detector, a crude semiconductor diode invented around 1904, solid state electronics really started with the invention of the transistor in 1947. Before that, all electronic equipment used vacuum tubes, because vacuum tubes were the only electronic components that could \"amplify\", an essential capability in all electronics. The replacement of bulky, fragile, energy-wasting vacuum tubes by transistors in the 1960s and 1970s created a revolution not just in technology but in people's habits, making possible the first truly portable consumer electronics such as the transistor radio, cassette tape player, walkie-talkie and quartz watch, as well as the first practical computers and mobile phones. \n\nToday, almost all electronics are solid-state except in some applications such as radio transmitters, in which vacuum tubes are still used, and some power industrial control circuits which use electromechanical devices such as relays. Additional examples of solid state electronic devices are the microprocessor chip, LED lamp, solar cell, charge coupled device (CCD) image sensor used in cameras, and semiconductor laser.\n\n"}
{"id": "29080624", "url": "https://en.wikipedia.org/wiki?curid=29080624", "title": "Stage wagon", "text": "Stage wagon\n\nStage wagons are light horse-drawn or mule-drawn public passenger vehicles often referred to as stagecoaches. Like stagecoaches they made long scheduled trips using stage stations or posts where the horses would be replaced by fresh horses. Stage wagons were intended for use in particularly difficult conditions where standard stagecoaches would be too big and too heavy. \n\nThis style of vehicle was often named a mud-coach or mud-wagon. More like wagons than coaches the sides of the vehicle gave passengers little protection from the dirt of the road. Abbot, Downing named their's an overland wagon. A brand-name, \"Celerity\", later became popular in place of mud (wagon).\nThey were employed wherever the poor state of the roads and or demand for services did not warrant the expense of a stagecoach. Most stagecoach routes in the United States' West were opened with them and often operators continued to use these vehicles as stagecoaches.\n\nThey were not unlike a freight wagon with a high driver's seat, bench seats on the tray and posts holding up canvas to shelter passengers from the weather. \n\nThose stage wagons with throroughbraces had an undercarriage like those used by a Concord coach but the thoroughbraces were much shorter and mounted to make sure there was much less motion of the body. The thoroughbraces were brought over a bar at each end and attached to another bar above the outer side of each axle.\n\nStage wagon wheels and their iron tires were as much as fifty per cent wider than those of conventional stagecoaches\n\nWagons carrying freight had been taking passengers in Europe since 1500. This particular stage wagon type was first recorded near the end of the 18th century in use in eastern North America, US and Upper and Lower Canada. It was an unsprung wagon with the driver's bench seat providing room for two more passengers beside him. It might also carry more passenger seats on the tray behind. These extra seats were reached by climbing over the driver's seat. About this time, the Postmaster General Joseph Habersham required the driver's seat to be moved from the tray onto a front wall to improve the driver's vision and by dropping the tray improve the wagon's stability. This created the characteristic stagecoach-like profile of the stage wagon.\n\nTheir relatively simple design and construction allowed them to be sold by Abbot, Downing at around half the price of full-size Concord stagecoaches. Their suspension employed thoroughbraces that were much shorter than those used on Concord stagecoaches.\n"}
{"id": "24109772", "url": "https://en.wikipedia.org/wiki?curid=24109772", "title": "Toronto Solar Neighbourhoods Initiative", "text": "Toronto Solar Neighbourhoods Initiative\n\nToronto Solar Neighbourhoods Initiative (“Solar Neighbourhoods”) is a joint project of the Toronto Atmospheric Fund and Toronto Energy Efficiency Office, with support from the Toronto Environment Office and Toronto Hydro. Its goal is to encourage the installation of solar water heating systems in Toronto homes through financial incentives and other support to both homeowners and the local solar industry. Phase I (2008-2009) of the pilot project is targeting homes in Toronto’s Ward 30. The program was developed based on a directive in Toronto’s “Climate Change, Clean Air and Sustainable Energy Action Plan”, which was approved by Toronto City Council in June 2007.\n\nOfficial named the Toronto Solar Neighbourhoods Initiative, the pilot program is referred to as \"Solar Neighbourhoods\" for short.\n\nSolar Neighbourhoods was made possible by a provincially mandated grant of $400,000 from the Portlands Energy Centre, a natural gas-based power plant located within Ward 30, to go toward local air quality initiatives.\n\nThe Solar Neighbourhoods program offers incentives of up to $1,000 towards the purchase of solar water heating systems for eligible residents. Those who participate can choose to receive $1,000 up-front towards the cost of their system or a $500 up-front rebate along with zero-interest financing to cover the cost of the system (up to $10,000). In order to qualify for these savings homeowners must live within Ward 30, and have an ecoENERGY home assessment with the selected auditing partner, Windfall Ecology Centre. This ecoENERGY assessment is necessary to access federal and provincial government solar hot water incentives, as well as incentives for a number of other home energy improvements through the ecoENERGY-Retrofit Homes program. The installation must be done by a Solar Neighbourhoods eligible contractor who has met basic requirements, including Canadian Solar Industry Association (CanSIA) membership, using CanSIA certified installers, providing basic labour and equipment warranties, and maintaining adequate WSIB and commercial insurance. This incentive can be received in conjunction with other government incentives such as the ecoENERGY incentive, Ontario Home Energy Savings Program and 2009 Home Renovation Tax Credit.\n\nSolar Neighbourhoods works in conjunction with a local solar buying club called the Riverdale Initiative for Solar Energy (RISE Again), which is bringing together neighbours in Riverdale to purchase both solar water heating systems and solar photovoltaic systems. A number of community solar buying groups exist across Toronto and southern Ontario, and are working together under the OurPower.ca umbrella.\n"}
{"id": "1713860", "url": "https://en.wikipedia.org/wiki?curid=1713860", "title": "Traction powerstation", "text": "Traction powerstation\n\nA traction power station is a power station that produces only traction current, that is, electric current used for railways, trams, trolleybuses or other conveyances. Pure traction power stations are rare and there are many more power stations that generate current for other purposes, such as standard three-phase alternating current (AC), in addition to traction current.\n\n\n\n\n\n"}
{"id": "237744", "url": "https://en.wikipedia.org/wiki?curid=237744", "title": "Vortex generator", "text": "Vortex generator\n\nA vortex generator (VG) is an aerodynamic device, consisting of a small vane usually attached to a lifting surface (or airfoil, such as an aircraft wing) or a rotor blade of a wind turbine. VGs may also be attached to some part of an aerodynamic vehicle such as an aircraft fuselage or a car. When the airfoil or the body is in motion relative to the air, the VG creates a vortex, which, by removing some part of the slow-moving boundary layer in contact with the airfoil surface, delays local flow separation and aerodynamic stalling, thereby improving the effectiveness of wings and control surfaces, such as flaps, elevators, ailerons, and rudders.\n\nVortex generators are most often used to delay flow separation. To accomplish this they are often placed on the external surfaces of vehicles and wind turbine blades. On both aircraft and wind turbine blades they are usually installed quite close to the leading edge of the aerofoil in order to maintain steady airflow over the control surfaces at the trailing edge. VGs are typically rectangular or triangular, about as tall as the local boundary layer, and run in spanwise lines usually near the thickest part of the wing. They can be seen on the wings and vertical tails of many airliners.\n\nVortex generators are positioned obliquely so that they have an angle of attack with respect to the local airflow in order to create a tip vortex which draws energetic, rapidly moving outside air into the slow-moving boundary layer in contact with the surface. A turbulent boundary layer is less likely to separate than a laminar one, and is therefore desirable to ensure effectiveness of trailing-edge control surfaces. Vortex generators are used to trigger this transition. Other devices such as vortilons, leading-edge extensions, leading edge cuffs, also delay flow separation at high angles of attack by re-energizing the boundary layer.\n\nExamples of aircraft which use VGs include the Embraer 170 and Symphony SA-160. For swept-wing transonic designs, VGs alleviate potential shock-stall problems (e.g., Harrier, Blackburn Buccaneer, Gloster Javelin).\n\nMany aircraft carry vane vortex generators from time of manufacture, but there are also aftermarket suppliers who sell VG kits to improve the STOL performance of some light aircraft. Aftermarket suppliers claim (i) that VGs lower stall speed and reduce take-off and landing speeds, and (ii) that VGs increase the effectiveness of ailerons, elevators and rudders, thereby improving controllability and safety at low speeds. For home-built and experimental kitplanes, VGs are cheap, cost-effective and can be installed quickly; but for certified aircraft installations, certification costs can be high, making the modification a relatively expensive process.\n\nOwners fit aftermarket VGs primarily to gain benefits at low speeds, but a downside is that such VGs may reduce cruise speed slightly. In tests performed on a Cessna 182 and a Piper PA-28-235 Cherokee, independent reviewers have documented a loss of cruise speed of . However, these losses are relatively minor, since an aircraft wing at high speed has a small angle of attack, thereby reducing VG drag to a minimum.\n\nOwners have reported that on the ground, it can be harder to clear snow and ice from wing surfaces with VGs than from a smooth wing, but VGs are not generally prone to inflight icing as they reside within the boundary layer of airflow. VGs may also have sharp edges which can tear the fabric of airframe covers and may thus require special covers to be made.\n\nFor twin-engined aircraft, manufacturers claim that VGs reduce single-engine control speed (Vmca), increase zero fuel and gross weight, improve the effectiveness of ailerons and rudder, provide a smoother ride in turbulence and make the aircraft a more stable instrument platform\n\nSome VG kits available for light twin-engine airplanes may allow an increase in maximum takeoff weight. The maximum takeoff weight of a twin-engine airplane is determined by structural requirements and single-engine climb performance requirements (which are lower for a lower stall speed). For many light twin-engine airplanes, the single-engine climb performance requirements determine a lower maximum weight rather than the structural requirements. Consequently, anything that can be done to improve the single-engine-inoperative climb performance will bring about an increase in maximum takeoff weight.\n\nIn the USA from 1945 until 1991,\nthe one-engine-inoperative climb requirement for multi-engine airplanes with a maximum takeoff weight of or less was as follows:\n\nwhere formula_1 is the stalling speed in the landing configuration in miles per hour.\n\nInstallation of vortex generators can usually bring about a slight reduction in stalling speed of an airplane and therefore reduce the required one-engine-inoperative climb performance. The reduced requirement for climb performance allows an increase in maximum takeoff weight, at least up to the maximum weight allowed by structural requirements. An increase in maximum weight allowed by structural requirements can usually be achieved by specifying a maximum zero fuel weight or, if a maximum zero fuel weight is already specified as one of the airplane's limitations, by specifying a new higher maximum zero fuel weight. For these reasons, vortex generator kits for many light twin-engine airplanes are accompanied by a reduction in maximum zero fuel weight and an increase in maximum takeoff weight.\n\nThe one-engine-inoperative rate-of-climb requirement does not apply to single-engine airplanes, so gains in the maximum takeoff weight (based on stall speed or structural considerations) are less significant compared to those for 1945–1991 twins.\n\nAfter 1991, the airworthiness certification requirements in the USA specify the one-engine-inoperative climb requirement as a gradient independent of stalling speed, so there is less opportunity for vortex generators to increase the maximum takeoff weight of multi-engine airplanes whose certification basis is FAR 23 at amendment 23-42 or later.\n\nBecause the landing weights of most light aircraft are determined by structural considerations and not by stall speed, most VG kits increase only the takeoff weight and not the landing weight. Any increase in landing weight would require either structural modifications or re-testing the aircraft at the higher landing weight to demonstrate that the certification requirements are still met. However, after a lengthy flight, sufficient fuel may have been used, thereby bringing the aircraft back below the permitted maximum landing weight.\n\nVortex generators have been used on the wing underside of Airbus A320 family aircraft to reduce noise generated by airflow over circular pressure equalisation vents for the fuel tanks. Lufthansa claims a noise reduction of up to 2 dB can thus be achieved.\n\n\n"}
{"id": "28100751", "url": "https://en.wikipedia.org/wiki?curid=28100751", "title": "Xingang Port oil spill", "text": "Xingang Port oil spill\n\nThe Xingang Port oil spill is a spill that occurred in July 2010 caused by a rupture and subsequent explosion of two crude oil pipelines that run to an oil storage depot of the China National Petroleum Corporation in Xingang Harbour, Dalian, Liaoning Province, China. The 1,500 tonnes of oil spilled from the pipes created an slick in the Yellow Sea that grew to within a week. By July 21, the spill had spread to , and stretched as far as along the coast.\n\nThe spill occurred after a process to desulfurize oil in a pipeline at the port began, triggering a fire which subsequently burned for 15 hours. The fire burned from oil that was released from a filled storage tank with a 90,000 ton capacity that collapsed as a result of the fire. Oil from other nearby tanks was, according to a Greenpeace report released several weeks after the incident, intentionally released to prevent the fire from expanding towards a tank containing dimethylbenzene, a flammable chemical.\n\nAlthough Chinese government officials reported that as little as 1,500 tonnes of oil spilled, a former University of Alaska marine conservation specialist, Rick Steiner, estimates the spill to have a much higher total, with somewhere in the range of 60,000 to 90,000 tons (18.47 to 27.70 million gallons) of oil spilled into the Yellow Sea. He said that \"[i]t's enormous. That's at least as large as the official estimate of the Exxon Valdez disaster.\" However, a government spokesperson for Dalian refuted this estimate, and referred to a panel of experts assessing the spill's size and environmental effect saying that \"[w]e will know w[h]ether it's smaller or bigger than 60,000 tons based on the conclusion made by the panel\".\n\nAccording to leaks on Weibo, the accident may have begun as early as June, 10. However, because the government never formally acknowledged the accident took place, as well as threatened informed citizens to delete their posts, the true nature of the accident was not clear to the public. No assessment on the effect on wildlife was ever carried out.\n\nThe ensuing fire burned for 15 hours and one firefighter drowned after being swept overboard into the water. There was extensive damage to local wildlife areas, aquaculture, and beaches were closed following the spill and conflagration which appeared to have been caused by the injection of a highly oxidizing sulfur reducing agent into a pipeline.\n\nOn 26 July 2010, the Chinese local government announced the spill had been contained and that the spill had failed to reach open waters. Cleanup efforts continued two weeks after the original explosion began, with operations to remove the oil performed at night when the oil is most viscous.\n\nThe cleanup effort largely used low-tech methods, including containment booms and straw mats to absorb the oil, as well as a fleet of fishermen manually scooping oil out of the water and transferring it to barrels for storage and eventual disposal. Additionally, limited amounts of chemical dispersants were used, as well as a material to increase biodegradation of oil.\n\nAfter the incident the Chinese government announced that safety standards would be tightened up at ports.\n\nThe oil spill affected several aspects of the area's economy. Tourism was affected after oil began washing ashore on beaches, some of which were closed after the spill; according to Greenpeace, additional oil washing ashore was a possibility that remained for the rest of the summer. The oil severely affected fishing industry near Dalian, especially offshore shellfish farms, many of which were contaminated by the oil, either killing the fish or rendering them unfit for consumption. The economic loss was estimated to be as high as tens of million US dollars. Environmental damage was also serious as a result of the spill; the majority of wildlife inhabiting the area was exposed to oil, which led to the deaths of some, while others were expected to display longer-term effects.\n\nLu Guang's pictures of the oil spill and the subsequent funeral of firefighter Zhang Liang were awarded a third prize in the category \"Spot News\" in the 2011 World Press Photo contest.\n\n\n"}
{"id": "154750", "url": "https://en.wikipedia.org/wiki?curid=154750", "title": "Zirconium dioxide", "text": "Zirconium dioxide\n\nZirconium dioxide (), sometimes known as zirconia (not to be confused with zircon), is a white crystalline oxide of zirconium. Its most naturally occurring form, with a monoclinic crystalline structure, is the mineral baddeleyite. A dopant stabilized cubic structured zirconia, cubic zirconia, is synthesized in various colours for use as a gemstone and a diamond simulant.\n\nZirconia is produced by calcining zirconium compounds, exploiting its high thermal stability.\n\nThree phases are known: monoclinic below 1170 °C, tetragonal between 1170 °C and 2370 °C, and cubic above 2370 °C. The trend is for higher symmetry at higher temperatures, as is usually the case. A small percentage of the oxides of calcium or yttrium stabilize in the cubic phase. The very rare mineral tazheranite (Zr,Ti,Ca)O is cubic. Unlike TiO, which features six-coordinate Ti in all phases, monoclinic zirconia consists of seven-coordinate zirconium centres. This difference is attributed to the larger size of Zr atom relative to the Ti atom.\n\nZirconia is chemically unreactive. It is slowly attacked by concentrated hydrofluoric acid and sulfuric acid. When heated with carbon, it converts to zirconium carbide. When heated with carbon in the presence of chlorine, it converts to zirconium tetrachloride. This conversion is the basis for the purification of zirconium metal and is analogous to the Kroll process.\n\nZirconium dioxide is one of the most studied ceramic materials. ZrO adopts a monoclinic crystal structure at room temperature and transitions to tetragonal and cubic at higher temperatures. The change of volume caused by the structure transitions from tetragonal to monoclinic to cubic induces large stresses, causing it to crack upon cooling from high temperatures. When the zirconia is blended with some other oxides, the tetragonal and/or cubic phases are stabilized. Effective dopants include magnesium oxide (MgO), yttrium oxide (YO, yttria), calcium oxide (CaO), and cerium(III) oxide (CeO).\n\nZirconia is often more useful in its phase 'stabilized' state. Upon heating, zirconia undergoes disruptive phase changes. By adding small percentages of yttria, these phase changes are eliminated, and the resulting material has superior thermal, mechanical, and electrical properties. In some cases, the tetragonal phase can be metastable. If sufficient quantities of the metastable tetragonal phase is present, then an applied stress, magnified by the stress concentration at a crack tip, can cause the tetragonal phase to convert to monoclinic, with the associated volume expansion. This phase transformation can then put the crack into compression, retarding its growth, and enhancing the fracture toughness. This mechanism is known as transformation toughening, and significantly extends the reliability and lifetime of products made with stabilized zirconia.\n\nThe ZrO band gap is dependent on the phase (cubic, tetragonal, monoclinic, or amorphous) and preparation methods, with typical estimates from 5–7 eV.\n\nA special case of zirconia is that of tetragonal zirconia polycrystal, or TZP, which is indicative of polycrystalline zirconia composed of only the metastable tetragonal phase.\n\nThe main use of zirconia is in the production of hard ceramics, such as in dentistry (see below), with other uses including as a protective coating on particles of titanium dioxide pigments, as a refractory material, in insulation, abrasives and enamels. Stabilized zirconia is used in oxygen sensors and fuel cell membranes because it has the ability to allow oxygen ions to move freely through the crystal structure at high temperatures. This high ionic conductivity (and a low electronic conductivity) makes it one of the most useful electroceramics. Zirconium dioxide is also used as the solid electrolyte in electrochromic devices.\n\nZirconia is a precursor to the electroceramic lead zirconate titanate (\"PZT\"), which is a high-K dielectric, which is found in myriad components.\n\nThe very low thermal conductivity of cubic phase of zirconia also has led to its use as a thermal barrier coating, or TBC, in jet and diesel engines to allow operation at higher temperatures. Thermodynamically, the higher the operation temperature of an engine, the greater the possible efficiency. Another low thermal conductivity use is a ceramic fiber insulation for crystal growth furnaces, fuel cell stack insulation and infrared heating systems.\n\nThis material is also used in dentistry in the manufacture of 1) subframes for the construction of dental restorations such as crowns and bridges, which are then veneered with a conventional feldspathic porcelain for aesthetic reasons, or of 2) strong, extremely durable dental prostheses constructed entirely from monolithic zirconia, with limited but constantly improving aesthetics. Zirconia stabilized with yttria (yttrium oxide), known as yttria-stabilized zirconia, can be used as a strong base material in some full ceramic crown restorations. \nTransformation toughened zirconia is used to make ceramic knives. Because of the hardness, ceramic-edged cutlery stays sharp longer than steel edged products.\n\nDue to its infusibility and brilliant luminosity when incandescent, it was used as an ingredient of sticks for limelight.\n\nZirconia has been proposed to electrolyze carbon monoxide and oxygen from the atmosphere of Mars to provide both fuel and oxidizer that could be used as a store of chemical energy for use with surface transportation on Mars. Carbon monoxide/oxygen engines have been suggested for early surface transportation use as both carbon monoxide and oxygen can be straightforwardly produced by zirconia electrolysis without requiring use of any of the Martian water resources to obtain hydrogen, which would be needed for the production of methane or any hydrogen-based fuels.\n\nZirconia is also a potential high-k dielectric material with potential applications as an insulator in transistors.\n\nZirconia is also employed in the deposition of optical coatings; it is a high-index material usable from the near-UV to the mid-IR, due to its low absorption in this spectral region. In such applications, it is typically deposited by PVD.\n\nIn jewelry making, some watch cases are advertised as being \"black zirconium oxide\". In 2015 Omega released a fully ZrO watch named \"The Dark Side of The Moon\" with ceramic case, bezel, pushers and clasp, advertising it as four times harder than stainless steel and therefore much more resistant to scratches during everyday use.\n\nSingle crystals of the cubic phase of zirconia are commonly used as diamond simulant in jewellery. Like diamond, cubic zirconia has a cubic crystal structure and a high index of refraction. Visually discerning a good quality cubic zirconia gem from a diamond is difficult, and most jewellers will have a thermal conductivity tester to identify cubic zirconia by its low thermal conductivity (diamond is a very good thermal conductor). This state of zirconia is commonly called \"cubic zirconia\", \"CZ\", or \"zircon\" by jewellers, but the last name is not chemically accurate. Zircon is actually the mineral name for naturally occurring zirconium silicate (ZrSiO).\n\n\n\n"}
