{"id": "21385735", "url": "https://en.wikipedia.org/wiki?curid=21385735", "title": "1950 Sverdlovsk plane crash", "text": "1950 Sverdlovsk plane crash\n\nThe Sverdlovsk plane crash of 5 January 1950 killed all 19 persons on board, including almost the entire ice hockey team (VVS Moscow) of the Soviet Air Forces – 11 players, as well as a team doctor and a masseur. The team was on board a twin-engined Lisunov Li-2 transport aircraft, a licensed Soviet-built version of the DC-3, heading to a match against the Dzerzhinets (Chelyabinsk) (Traktor Chelyabinsk) hockey club. Due to poor weather at Chelyabinsk, the flight diverted to Sverdlovsk. The crew attempted four approaches but during the fifth approach to Koltsovo Airport at Sverdlovsk in the Soviet Unions Russian Soviet Federated Socialist Republic, the aircraft crashed near the airport in a heavy snowstorm with strong winds.\n\n\nTeam players Viktor Shuvalov and Vsevolod Bobrov were not on the flight; Bobrov overslept and took the train instead, and Shuvalov was injured.\n\nIn memory of the crash victims, a memorial was erected near the common grave in Koltsovo.\n\nThe crash occurred two and a half weeks after the 70th birthday of Joseph Stalin. According to a recent news article, Stalin's son Vasily Dzhugashvili, an Air Force commander of the Moscow Military District and the patron of the ice hockey team, was afraid of his father's possible reaction and of the crash investigation; he decided to recruit a new Air Force team in less than a day, except for three original players (including the later IIHF Hall of Fame member Vsevolod Bobrov), who were not on the crashed plane.\n\n"}
{"id": "24367861", "url": "https://en.wikipedia.org/wiki?curid=24367861", "title": "Australian Energy Regulator", "text": "Australian Energy Regulator\n\nThe Australian Energy Regulator (AER) is the regulator of the wholesale electricity and gas markets in Australia. It is part of the Australian Competition and Consumer Commission and enforces the rules established by the Australian Energy Market Commission.\n\nThe AER was established in July 2005. The next year all 13 bodies previously responsible for energy regulation had transferred responsibility to the AER. Decisions made by the regulator are subject to appeal.\n\nThe AER’s current functions are focused on regulating the natural monopoly transmission and distribution sectors of the national electricity market, monitoring the wholesale electricity market and enforcing electricity market rules. The AER's regulatory functions and powers are conferred upon it by the national electricity law and the national electricity rules. \n\nUnder the national electricity law and national electricity rules, the AER’s key responsibilities at the present time include:\n\nThe AER board currently has three members who are statutory appointments, including a full-time chair. Ms Paula Conboy is the Chair of the AER. Part IIIAA of the Trade Practices Act provides that one of the members of the AER must be a commissioner of the ACCC, with Ms Cristina Cifuentes appointed in this capacity. Mr Jim Cox is the AER member selected by the States and Territories.\n\n\n"}
{"id": "33057602", "url": "https://en.wikipedia.org/wiki?curid=33057602", "title": "Autolib'", "text": "Autolib'\n\nAutolib' was an electric car sharing service which was inaugurated in Paris, France, in December 2011. It closed on July 31st 2018. It was operated by the Bolloré industrial group, and complemented the city's bike sharing scheme, Velib', which was set up in 2007. The Autolib' scheme maintained a fleet of all-electric Bolloré Bluecars for public use on a paid subscription basis, employing a citywide network of parking and charging stations. , 3,980 Bluecars had been registered for the service, and the scheme had more than 126,900 registered subscribers; Autolib' furthermore offered 1,084 electric car stations in Paris agglomeration with 5,935 charging points.\n\nSince beginning operations in Paris, Autolib' expanded its business to the cities of Lyon and Bordeaux. Bolloré also signed deals to begin operating offshoots of Autolib' in London and Indianapolis in 2015 and Singapore in 2017.\n\nThe Autolib' system was a follow-up to Paris' successful Velib' bike sharing scheme, which began operations in 2007. The system's electric cars were supplied by the Bolloré industrial group, as the result of a collaboration with the Italian automotive firm Pininfarina. There were also plans to integrate payment for the bicycle and car hire schemes with the ticketing systems for traditional modes of public transport. In May 2012, Vincent Bolloré, the head of the Bolloré group, stated that he expected Autolib' to become profitable by 2018. \n\nConstruction of the Autolib' stations began in mid-2011, and 66 of the scheme's Bolloré Bluecars were deployed for a two-month preliminary trial period between October and December 2011. The system entered service on 5 December 2011, with an initial fleet of 250 Bluecars and 250 Autolib’ rental stations serving the city of Paris and 18 surrounding communities (96 in 2016), grouped into the syndicate of associated collectivities \"Autolib' Métropole\".\n\nAt the scheme's inception, car availability was a problematic issue, as more Parisians than expected subscribed to the service. Moreover, by early January 2012, up to 40 of the 250 cars in the initial fleet had been temporarily withdrawn from service to repair vandalism or malfunctions.\n\nBy July 2012, 650 parking and charging stations had been deployed around Paris and the 46 communes participating in the scheme, and by February 2013 there were 4,000 charging points. The program's user base grew from 6,000 subscribers at the end of December 2011 to 27,000 in July 2012, and reached 37,000 by early October 2012, of which 13,000 had an annual subscription.\n\nDuring 2012, sales of electric cars in France were led by the Bolloré Bluecar deployed for Autolib', with 1,543 units registered, representing 28% of total all-electric cars registered in the country that year. Over 2,000 Bolloré Bluecars had been registered for Autolib' by September 2013. By late September 2012, Autolib's fleet reached the milestone of 500,000 rentals since its launch, and the scheme's vehicles had been driven a cumulative total of by February 2013. By mid-October 2013, the service had provided over 3 million rentals, with an average of 10,000 rentals per day. By July 2014, Autolib' had 2,500 operational vehicles and over 150,000 subscribers, and its cars had covered a cumulative mileage of over since the scheme's introduction. In July 2018 the scheme ceased to operate. \n\nIn December 2009, the car-rental company Europcar brought the City of Paris to court, arguing that the Autolib' name was a plagiarism and unfair competition by the city. Europcar is the trademark owner of the rental car subscription service \"Autoliberté\", which has been in operation since 2001. The case was dismissed in March 2011 by the High Court of Paris, and Europcar decided to appeal. On 30 June 2012, the Paris Court of Appeal set aside the judgment of the High Court, and ruled that within a month Autolib' had to change its name because it breached trademark laws. The ruling implied that the name had to be changed on all 1,800 Autolib' cars, docking stations and subscriber cards, and also required that all of the scheme's advertising to be rewritten.\n\nAfter the ruling, the City of Paris and Europcar began negotiating an agreement to solve the brand name conflict. In November 2012, an agreement was reached to keep the Autolib' brand name. Europcar agreed to waive the enforcement of the court decision on the condition that the City of Paris would be the owner of the Autolib' brand, and in exchange for free advertising for Europcar's Autoliberté service. The agreement was signed for three years.\n\nOn 14 October 2013, a Bluecar was destroyed by fire while charging at an Autolib' kiosk in Paris; the fire then spread to a second Bluecar. A police investigation was subsequently conducted to ascertain the cause of the fire. According to Bolloré, the real-time telemetric monitoring system did not register a thermal runaway problem with the car's battery when the fire started. The company is attributing the origin of the fire to an external cause, probably vandalism. Bolloré reported that a total of 25 Autolib' Bluecars have burnt out since the scheme's inception, with most of the incidents certified as attributed to vandals.\n\nIn early 2013, the Bolloré Group announced plans to launch a similar car sharing service in Lyon and Bordeaux, but under a different brand name and with no cost to the cities. Bolloré's proposal was to fund the entire infrastructure, install and provide the vehicles, and cover the costs of maintenance and repairs. Bolloré furthermore expressed interest in launching a car-sharing service in Asia. In October 2013, proposals were advanced for Scootlib', an electric scooter sharing scheme intended to complement Velib' and Autolib'.\n\nBolloré's Lyon scheme, dubbed BlueLy, entered service in October 2013 with an initial fleet of 130 vehicles and 51 parking and charging stations. Its Bordeaux service, BlueCub, began operating in January 2014 with an initial fleet of 90 vehicles, 40 parking stations and 180 charging stations.\n\nIn June 2013, Autolib' formed an agreement with the American city of Indianapolis, Indiana, to develop an electric car sharing service there; the scheme, named BlueIndy, opened to the public in September 2015. A similar scheme based in London was announced in 2014, and will be inaugurated in March 2015, utilising London's existing network of electric car charging points. The London scheme is intended to encompass 3,000 electric cars by 2018.\n\nIn June 2016, Bolloré signed an agreement with Singapore's Land Transport Authority (LTA) and Economic Development Board to develop an electric car-sharing programme, known as BlueSG. Construction of the charging stations began at the end of September, 2017 and in December, 2017, the service was officially opened to the public with 30 charging stations and 80 cars located throughout the country, with plans to expand the service to offer 2,000 charging points in 500 charging locations, with 400 charging points open for public use and 1,000 electric cars deployed by 2020.\n\nThe Autolib’ Bluecar is available to anyone aged 18 or older with a valid French driving license, or a valid foreign license plus the international driving license, who takes out a paid subscription. Users can choose between 2 rental packages, with 30-minute fees varying from to depending on the rental plan. An available car can be collected for use from any rental station and returned to any other rental station. Each car has on-board GPS capabilities and can be tracked by the system's operations center.\n\nIn addition to the subscription fees, Autolib' charges a variable rate for each half an hour of use, but billing for each rental is calculated on a \"pro rata\" basis, which takes into account the actual duration of use rounded up to the nearest minute (except for the first 20 minutes, for which there is a minimum charge). The table below summarizes the subscription types available, the subscription fees and the corresponding 30-minute rates:\n\nIn addition to charging its own vehicles, the Autolib' scheme offers charging services for private owners of electric cars and motorcycles. Customers have to sign up for the \"recharge\" fee option. The subscription cost for cars and motorcycles is per year only the first year. Recharge customers have designated parking spaces at Autolib’ stations, marked with a blue square. Each hour costs for cars and motorcycles. Use of the charging infrastructure for private cars is limited to two times per day per subscriber.\n\nIn early 2014, Renault offered free Autolib' charging subscriptions to owners of Renault EVs. Autolib' charging stations are compatible with the following plug-in electric vehicles: BMW i3, Nissan Leaf, Mitsubishi i-MiEV, Opel Ampera, Renault Fluence Z.E., Renault Kangoo Z.E., Renault Twizy, Renault Zoe, Toyota Prius Plug-in Hybrid and all Smart ED, Peugeot iOn and Citroen C-Zero manufactured before May 2013.\n\nBolloré began leasing the Bluecar to individual and corporate customers in October 2012 at a price of () per month. The pricing includes insurance, parking and charging at Autolib' stations. According to Bolloré, each existing station already provides one recharging space for private electric cars outside the Autolib' system. The Bluecar retail version has a blue exterior color, unlike the Autolib' version's silver unpainted aluminum exterior. In February 2013 Bolleré announced the start of retail sales of the Bluecar, and for an optional monthly fee of , owners can have access to the Autolib' network of charging stations around Paris.\n\nAtos and the Bolloré Group launched the MyCar corporate car sharing pilot program in December 2012. The service is dedicated to Atos employees for their business travel needs at the company's headquarters in the French town of Bezons. A fleet of ten Atos-branded Bolloré Bluecars were initially deployed. In addition to charging vehicles at the solar-powered Bezons site, MyCar users can also access the Autolib' network of charging stations in Paris and its surrounding area.\n\nIn January 2013, a similar leasing deal was reached with Crédit Agricole, the largest retail banking group in France. Five Bluecars were leased for 20 months for business travel use by the bank's employees.\n\n\n"}
{"id": "15666979", "url": "https://en.wikipedia.org/wiki?curid=15666979", "title": "Baihetan Dam", "text": "Baihetan Dam\n\nThe Baihetan Dam () is a large hydroelectric dam under construction on the Jinsha River, a tributary of the Yangtze River in Sichuan and Yunnan provinces, in the southwest of China. \nThe dam is a 277 m tall double-curvature arch dam with a crest elevation of 827 m. \nIts width will be 72 m at the base and 13 m at the crest.\n\nThe facility will generate power by utilizing 16 turbines, each with a generating capacity of , taking the generating capacity to . In terms of generating capacity, it will be the second largest hydroelectric power plant in the world, after the Three Gorges Dam. When finished, it will be the third largest dam in China and the fourth in the world, in terms of dam volume. Construction on the dam began in 2008 and is expected to be complete in 2021.\n\n"}
{"id": "50252437", "url": "https://en.wikipedia.org/wiki?curid=50252437", "title": "Bawanur Dam", "text": "Bawanur Dam\n\nThe Bawanur Dam is an earth-fill dam currently being constructed on the Diyala River just upstream of the town of Bawanur in Sulaymaniyah Governorate, Iraq. The tall dam will support a 32 MW run-of-the-river hydroelectric power station. It will also serve to control floods and provide water for irrigation. In August 2013, the Kurdistan Regional Government signed a US$200 million contract with the Romanian firm, Hidroconstrucția, to build the dam and power station. The project is expected to be complete in 2018.\n\n"}
{"id": "1251355", "url": "https://en.wikipedia.org/wiki?curid=1251355", "title": "Broadcast seeding", "text": "Broadcast seeding\n\nIn agriculture, gardening, and forestry, broadcast seeding is a method of seeding that involves scattering seed, by hand or mechanically, over a relatively large area. This is in contrast to:\n\nBroadcast seeding is of particular use in establishing dense plant spacing, as for cover crops and lawns. In comparison to traditional drill planting, broadcast seeding will require 10–20% more seed. It's simpler, faster, and easier than traditional row sowing. Broadcast seeding works best for plants that do not require singular spacing or that are more easily thinned later. After broadcasting, seed is often lightly buried with some type of raking action, often done using vertical tillage tools. Utilizing these tools increases the success rate of germination by increasing seed-to-soil contact. \n\nSeeds sown in this manner are distributed unevenly, which may result in overcrowding. This method may not ensure that all seeds are sown at the correct depth. Incorrect depth, if too deep, would result in germination that would not allow the young plant to break the surface of the soil and prevent sprouting. If they are not sown evenly then there would be a lack of various nutrients from sunlight, oxygen etc in many crops or plants. \n\nIn addition, it is worth noting that not all seeds are good candidates for broadcast seeding. Often, only smaller seeds will sprout and continue to grow successfully when planted by way of broadcasting. In general, the larger the seed, the deeper it can be planted.\n\n"}
{"id": "2152618", "url": "https://en.wikipedia.org/wiki?curid=2152618", "title": "COSILAB", "text": "COSILAB\n\nCOSILAB is a software tool for solving complex chemical kinetics problems. It is used worldwide in research and industry, in particular in automotive, combustion, and chemical processing applications.\n\nProblems to be solved by COSILAB may involve thousands of reactions amongst hundreds of species for practically any mixture composition, pressure and temperature. Its computational capabilities allow for a complex chemical reaction to be studied in detail, including intermediate compounds, trace compounds and pollutants.\n\nWhilst complex chemistry is accounted for, chemical reactor or combustion geometries that can be handled by COSILAB are relatively simple. For the purpose of ``real-life\" simulations this limitation can be overcome, however, by using a library of pre-compiled subroutines and functions, that one can link to his or her own code written in Fortran, the C programming language or C++. In this way, it is possible to develop fully two-dimensional or three-dimensional CFD or computational fluid dynamics codes that are able to capture fairly realistic geometries.\n\nThe development of codes like COSILAB is motivated by a worldwide attempt to keep the environment clean and to save—or at least make best use of—the continuously diminishing fossil fuel resources.\n\n"}
{"id": "5386635", "url": "https://en.wikipedia.org/wiki?curid=5386635", "title": "Ceroma", "text": "Ceroma\n\nCeroma () was a word which first appeared in the works of the two Roman poets Juvenal and Martial and has come to be defined as a mixture of oil, wax and earth; or, a cloth with which ancient wrestlers rubbed themselves, not only to make their limbs more sleek and less capable of gripping, but more pliable and fit for exercise. However, scholars point out that this definition is a misunderstanding of satire and its correct meaning is a \"layer of mud or clay forming the floor of the wrestling ring in the times of the Empire\".\n\n"}
{"id": "38528850", "url": "https://en.wikipedia.org/wiki?curid=38528850", "title": "Chelyabinsk meteor", "text": "Chelyabinsk meteor\n\nThe Chelyabinsk meteor was a superbolide caused by an approximately near-Earth asteroid that entered Earth's atmosphere over Russia on 15 February 2013 at about 09:20 YEKT (03:20 UTC), with a speed of 19.16 ± 0.15 kilometres per second (60,000–69,000 km/h or 40,000–42,900 mph). It quickly became a brilliant superbolide meteor over the southern Ural region. The light from the meteor was brighter than the Sun, visible up to away. It was observed over a wide area of the region and in neighbouring republics. Some eyewitnesses also felt intense heat from the fireball.\n\nDue to its high velocity and shallow angle of atmospheric entry, the object exploded in an air burst over Chelyabinsk Oblast, at a height of around . The explosion generated a bright flash, producing a hot cloud of dust and gas that penetrated to , and many surviving small fragmentary meteorites, as well as a large shock wave. The bulk of the object's energy was absorbed by the atmosphere, with a total kinetic energy before atmospheric impact estimated from infrasound and seismic measurements to be equivalent to the blast yield of 400–500 kilotons of TNT (about 1.4–1.8 PJ) range – 26 to 33 times as much energy as that released from the atomic bomb detonated at Hiroshima.\n\nThe object was undetected before its atmospheric entry, in part because its radiant was close to the Sun. Its explosion created panic among local residents, and about 1,500 people were injured seriously enough to seek medical treatment. All of the injuries were due to indirect effects rather than the meteor itself, mainly from broken glass from windows that were blown in when the shock wave arrived, minutes after the superbolide's flash. Some 7,200 buildings in six cities across the region were damaged by the explosion's shock wave, and authorities scrambled to help repair the structures in sub-freezing temperatures.\n\nWith an estimated initial mass of about 12,000–13,000 metric tons (13,000–14,000 short tons, heavier than the Eiffel Tower), and measuring about in diameter, it is the largest known natural object to have entered Earth's atmosphere since the 1908 Tunguska event, which destroyed a wide, remote, forested, and very sparsely populated area of Siberia. The Chelyabinsk meteor is also the only meteor confirmed to have resulted in a large number of injuries. No deaths were reported.\n\nThe earlier-predicted and well-publicized close approach of a larger asteroid on the same day, the roughly 367943 Duende, occurred about 16 hours later; the very different orbits of the two objects showed they were unrelated to each other.\n\nLocal residents witnessed extremely bright burning objects in the sky in Chelyabinsk, Kurgan, Sverdlovsk, Tyumen, and Orenburg Oblasts, the Republic of Bashkortostan, and in neighbouring regions in Kazakhstan, when the asteroid entered the Earth's atmosphere over Russia. Amateur videos showed a fireball streaking across the sky and a loud boom several minutes afterwards. Some eyewitnesses also felt intense heat from the fireball.\n\nThe event began at 09:20:21 Yekaterinburg time, several minutes after sunrise in Chelyabinsk, and minutes before sunrise in Yekaterinburg. According to eyewitnesses, the bolide appeared brighter than the sun, as was later confirmed by NASA. An image of the object was also taken shortly after it entered the atmosphere by the weather satellite Meteosat 9. Witnesses in Chelyabinsk said that the air of the city smelled like \"gunpowder\", \"sulfur\" and \"burning odors\" starting about 1 hour after the fireball and lasting all day.\n\nThe visible phenomenon due to the passage of an asteroid or meteoroid through the atmosphere is called a meteor. If the object reaches the ground, then it is called a meteorite. During the Chelyabinsk meteoroid's traversal, there was a bright object trailing smoke, then an air burst (explosion) that caused a powerful blast wave. The latter was the only cause of the damage to thousands of buildings in Chelyabinsk and its neighbouring towns. The fragments then entered dark flight (without the emission of light) and created a strewn field of numerous meteorites on the snow-covered ground (officially named Chelyabinsk meteorites).\n\nThe last time a similar phenomenon was observed in the Chelyabinsk region was the Kunashak meteor shower of 1949, after which scientists recovered about weighing over in total. The Chelyabinsk meteor is thought to be the biggest natural space object to enter Earth's atmosphere since the 1908 Tunguska event, and the only one confirmed to have resulted in a large number of injuries, although a small number of panic-related injuries occurred during the Great Madrid Meteor Event of 10 February 1896.\n\nPreliminary estimates released by the Russian Federal Space Agency indicated the object was an asteroid moving at about in a \"low trajectory\" when it entered Earth's atmosphere. According to the Russian Academy of Sciences, the meteor then pushed through the atmosphere at a velocity of The radiant (the apparent position of origin of the meteor in the sky) appears from video recordings to have been above and to the left of the rising Sun.\n\nEarly analysis of CCTV and dashcam video posted online indicated that the meteor approached from east by south, and exploded about 40 km south of central Chelyabinsk above Korkino at a height of 23.3 km (14.5 miles, 76,000 feet), with fragments continuing in the direction of Lake Chebarkul. On 1 March 2013 NASA published a detailed synopsis of the event, stating that at peak brightness (at 09:20:33 local time), the meteor was 23.3 km (14.5 miles, 76,000 feet) high, located at 54.8°N, 61.1°E. At that time it was travelling at about 18.6 km/s (11.6 mi/s), (about 67,000 km/h, or about 41,750 mph) —almost 60 times the speed of sound. In November 2013, results were published based on a more careful calibration of dashcam videos in the field weeks after the event during a Russian Academy of Sciences field study, which put the point of peak brightness at 29.7 km altitude and the final disruption of the thermal debris cloud at 27.0 km, settling to 26.2 km, all with a possible systematic uncertainty of ±0.7 km.\n\nThe United States space agency NASA estimated the diameter of the bolide at about 17–20 m and has revised the mass several times from an initial , until reaching a final estimate of 10,000 tonnes (11,000 short tons, greater than the total weight of the Eiffel Tower). The air burst's blast wave, when it hit the ground, produced a seismic wave which registered on seismographs at magnitude 2.7.\n\nThe Russian Geographical Society said the passing of the meteor over Chelyabinsk caused three blasts of different energy. The first explosion was the most powerful, and was preceded by a bright flash, which lasted about five seconds. Initial newspaper altitude estimates ranged from with an explosive equivalent, according to NASA, of roughly , although there is some debate on this yield (500 kt is exactly the same energy released by the Ivy King nuclear explosion in 1952). According to a paper in 2013, all these ~500 kiloton yield estimates for the meteor airburst are \"uncertain by a factor of two because of a lack of calibration data at those high energies and altitudes.\"\nThe hypocentre of the explosion was to the south of Chelyabinsk, in Yemanzhelinsk and Yuzhnouralsk. Due to the height of the air burst, the atmosphere absorbed most of the explosion's energy. The explosion's blast wave first reached Chelyabinsk and environs between less than 2 minutes 23 seconds and 2 minutes 57 seconds later. The object did not release all of its kinetic energy in the form of a blast wave as some 90 kilotons of TNT of the total energy of the main airburst's fireball was emitted as visible light according to NASA's Jet Propulsion Laboratory, and two main fragments survived the primary airburst disruption at ; they flared around , with\none falling apart at and the other remaining luminous down to\n, with part of the meteoroid continuing on its general trajectory to punch a hole in the frozen Lake Chebarkul, an impact that was fortuitously captured on camera and released in November 2013.\n\nThe infrasound waves given off by the explosions were detected by 20 monitoring stations designed to detect nuclear weapons testing run by the Comprehensive Test Ban Treaty Organization (CTBTO) Preparatory Commission, including the distant Antarctic station, some away. The blast of the explosion was large enough to generate infrasound returns, after circling the globe, at distances up to about . Multiple arrivals involving waves that travelled twice around the globe have been identified. The meteor explosion produced the largest infrasounds ever to be recorded by the CTBTO infrasound monitoring system, which began recording in 2001, so great that they reverberated around the world several times, taking over a day to dissipate. Additional scientific analysis of US military infrasound data was aided by an agreement reached with US authorities to allow its use by civilian scientists, implemented only about a month before the Chelyabinsk meteor event.\nA preliminary estimate of the explosive energy by Astronomer Boris Shustov, director of the Russian Academy of Sciences Institute of Astronomy, was , another using empirical period-yield scaling relations and the infrasound records, by Peter Brown of the University of Western Ontario gave a value of and represents a best estimate for the yield of this airburst; there remains a potential \"uncertainty [in the order of] a factor of two in this yield value\". Brown and his colleagues also went on to publish a paper in November 2013 which stated that the \"widely referenced technique of estimating airburst damage does not reproduce the [Chelyabinsk] observations, and that the mathematical relations found in the book \"The Effects of Nuclear Weapons\" which are based on the effects of nuclear weapons—[which is] almost always used with this technique—overestimate blast damage [when applied to meteor airbursts]\". A similar overestimate of the explosive yield of the Tunguska airburst also exists; as incoming celestial objects have rapid directional motion, the object causes stronger blast wave and thermal radiation pulses at the ground surface than would be predicted by a stationary object exploding, limited to the height at which the blast was initiated-where the object's \"momentum is ignored\". Thus a meteor airburst of a given energy is \"much more damaging than an equivalent [energy] nuclear explosion at the same altitude.\"\nThe seismic wave produced when the primary airburst's blast struck the ground yields a rather uncertain \"best estimate\" of 430 kilotons (momentum ignored), corresponding to the seismic wave which registered on seismographs at magnitude 2.7.\n\nBrown also states that the double smoke plume formation, as seen in photographs, is believed to have coincided near the primary airburst section of the dust trail (as also pictured following the Tagish Lake fireball), and it likely indicates where rising air quickly flowed into the center of the trail, essentially in the same manner as a moving 3D version of a mushroom cloud. Photographs of this smoke trail portion, before it split into two plumes, show this cigar-shaped region glowing incandescently for a few seconds. This region is the area in which the maximum of material ablation occurred, with the double plume persisting for a time and then appearing to rejoin or close up.\n\nThe blast created by the meteor's air burst produced extensive ground damage over an irregular elliptical area around a hundred kilometres wide, and a few tens of kilometres long, with the secondary effects of the blast being the main cause of the considerable number of injuries. Russian authorities stated that 1,491 people sought medical attention in Chelyabinsk Oblast within the first few days. Health officials said 112 people had been hospitalised, with two in serious condition. A woman with a broken spine was flown to Moscow for treatment. Most of the injured were hurt by the secondary blast effects of shattered, falling or blown-in glass. The intense light from the meteor, momentarily 30 times brighter than the Sun, also produced injuries, leading to over 180 cases of eye pain, and 70 people subsequently reported temporary flash blindness. Twenty people reported ultraviolet burns similar to sunburn, possibly intensified by the presence of snow on the ground. Vladimir Petrov, when meeting with scientists to assess the damage, reported that he sustained so much sunburn from the meteor that the skin flaked only days later.\n\nA fourth-grade teacher in Chelyabinsk, Yulia Karbysheva, was hailed as a hero after saving 44 children from imploding window glass cuts. Despite not knowing the origin of the intense flash of light, Karbysheva thought it prudent to take precautionary measures by ordering her students to stay away from the room's windows and to perform a duck and cover maneuver and then to leave a building. Karbysheva, who remained standing, was seriously lacerated when the blast arrived and window glass severed a tendon in one of her arms and left thigh; none of her students, whom she ordered to hide under their desks, suffered cuts. The teacher was taken to a hospital which received 112 people that day. The majority of the patients were suffering from cuts.\nAfter the air blast, car alarms went off and mobile phone networks were overloaded with calls. Office buildings in Chelyabinsk were evacuated. Classes for all Chelyabinsk schools were cancelled, mainly due to broken windows. At least 20 children were injured when the windows of a school and kindergarten were blown in at 09:22. Following the event, government officials in Chelyabinsk asked parents to take their children home from schools.\n\nApproximately of a roof at a zinc factory collapsed during the incident. Residents in Chelyabinsk whose windows were smashed quickly sought to cover the openings with anything available, to protect themselves against temperatures of . Approximately 100,000 home-owners were affected, according to Chelyabinsk Oblast Governor Mikhail Yurevich. He also said that preserving the water pipes of the city's district heating was the primary goal of the authorities as they scrambled to contain further post-explosion damage.\n\nBy 5 March 2013 the number of damaged buildings was tallied at over 7,200, which included some 6,040 apartment blocks, 293 medical facilities, 718 schools and universities, 100 cultural organizations, and 43 sport facilities, of which only about one and a half percent had not yet been repaired. The oblast governor estimated the damage to buildings at more than 1 billion rubles (approximately Chelyabinsk authorities said that broken windows of apartment homes, but not the glazing of enclosed balconies, would be replaced at the state's expense. One of the buildings damaged in the blast was the Traktor Sport Palace, home arena of Traktor Chelyabinsk of the Kontinental Hockey League (KHL). The arena was closed for inspection, affecting various scheduled events, and possibly the postseason of the KHL.\n\nThe irregular elliptical disk shape/\"spread-eagled butterfly\" ground blast damage area, produced by the airburst, is a phenomenon first noticed upon studying the other larger airburst event: Tunguska.\nThe Chelyabinsk meteor struck without warning. Dmitry Medvedev, the Prime Minister of Russia, confirmed a meteor had struck Russia and said it proved that the \"entire planet\" is vulnerable to meteors and a spaceguard system is needed to protect the planet from similar objects in the future. Dmitry Rogozin, the deputy prime minister, proposed that there should be an international program that would alert countries to \"objects of an extraterrestrial origin\", also called potentially hazardous objects.\n\nColonel General Nikolay Bogdanov, commander of the Central Military District, created task forces that were directed to the probable impact areas to search for fragments of the asteroid and to monitor the situation. Meteorites (fragments) measuring were found from Chebarkul in the Chelyabinsk region.\n\nOn the day of the impact, Bloomberg News reported that the United Nations Office for Outer Space Affairs had suggested the investigation of creating an \"Action Team on Near-Earth Objects\", a proposed global asteroid warning network system, in face of 's approach. As a result of the impact, two scientists in California proposed directed-energy weapon technology development as a possible means to protect Earth from asteroids.\n\nIt is estimated that the frequency of airbursts from objects 20 metres across is about once in every 60 years. There have been three incidents in the previous century involving a comparable energy yield or higher: the 1908 Tunguska event, the 1930 Curuçá River event, and in 1963 off the coast of Prince Edward Islands in the Indian Ocean. Two of those were over unpopulated areas.\n\nCenturies before, the 1490 Ch'ing-yang event, of an unknown magnitude, apparently caused 10,000 deaths. While modern researchers are skeptical about that 10,000 deaths figure, the Tunguska event would have been devastating over a highly populous district.\n\nBased on its entry direction and speed of 19 kilometres/second, the Chelyabinsk meteor apparently originated in the asteroid belt between Mars and Jupiter. It was probably a fragmented asteroid. The meteorite has veins of black material which had experienced high-pressure shock and were once partly melted, due to a previous collision. The metamorphism in the chondrules in the meteorite samples indicates the rock making up the meteor had a history of collisions and was once several kilometres below the surface of a much larger LL-chondrite asteroid. The Chelyabinsk asteroid probably entered an orbital resonance with Jupiter (a common way for material to be ejected from the asteroid belt) which increased its orbital eccentricity until its perihelion was reduced enough for it to be able to collide with the Earth.\n\nIn the aftermath of the air burst of the body, a large number of small meteorites fell on areas west of Chelyabinsk, generally at terminal velocity, about the speed of a piece of gravel dropped from a skyscraper. Analysis of the meteor showed that all resulted from the main breakup at 27–34 km altitude. Local residents and schoolchildren located and picked up some of the meteorites, many located in snowdrifts, by following a visible hole that had been left in the outer surface of the snow. Speculators were active in the informal market that emerged for meteorite fragments.\n\nIn the hours following the visual meteor sighting, a wide hole was discovered on Lake Chebarkul's frozen surface. It was not immediately clear whether this was the result of an impact; scientists from the Ural Federal University collected 53 samples from around the hole the same day it was discovered. The early specimens recovered were all under in size and initial laboratory analysis confirmed their meteoric origin. They are ordinary chondrite meteorites and contain 10% iron. The fall is officially designated as the Chelyabinsk meteorite. The Chelyabinsk meteor was later determined to come from the LL chondrite group. The meteorites were LL5 chondrites having a shock stage of S4, and had a variable appearance between light and dark types. Petrographic changes during the fall allowed estimates that the body was heated between 65 and 135 degrees during its atmospheric entry.\n\nIn June 2013, Russian scientists reported that further investigation by magnetic imaging below the location of the ice hole in Lake Chebarkul had identified a -size meteorite buried in the mud at the bottom of the lake. Before recovery began, the chunk was estimated to weigh roughly .\n\nFollowing an operation lasting a number of weeks, it was raised from the bottom of the Chebarkul lake on 16 October 2013. With a total mass of , this is the largest found fragment of the Chelyabinsk meteorite. Initially, it tipped and broke the scales used to weigh it, splitting into three pieces.\n\nIn November 2013, a video from a security camera was released showing the impact of the fragment at the Chebarkul lake. This is the first recorded impact of a meteorite on video. From the measured time difference between the shadow generating meteor to the moment of impact, scientists calculated that this meteorite hit the ice at about 225 metres per second, 64 percent of the speed of sound.\n\nThe Russian government put out a brief statement within an hour of the event. The news was first reported by the hockey site Russian Machine Never Breaks before heavy coverage by the international media and the Associated Press with the Russian government's confirmation less than two hours afterwards. Less than 15 hours after the meteor impact, videos of the meteor and its aftermath had been viewed millions of times.\n\nThe number of injuries caused by the asteroid led the Internet-search giant Google to remove a Google Doodle from their website, created for the predicted pending arrival of another asteroid, . New York City planetarium director Neil deGrasse Tyson stated the Chelyabinsk meteor was unpredicted because no attempt had been made to find and catalogue every near-Earth object. Doing so would be very difficult, and current efforts only aim at a complete inventory of near-Earth objects. The Asteroid Terrestrial-impact Last Alert System, on the other hand, could now predict some Chelyabinsk-like events a day or so in advance, when their radiant is not close to the Sun.\n\nOn 27 March 2013, a broadcast episode of the NOVA science television series titled \"Meteor Strike\" documented the Chelyabinsk meteor, including the large amounts of meteoritic science revealed by the numerous videos of the airburst posted online by ordinary citizens. The NOVA program called the video documentation and the related scientific discoveries of the airburst \"unprecedented\". The documentary also discussed the much greater tragedy \"that could have been\" had the asteroid entered the Earth's atmosphere more steeply.\n\nMultiple videos of the Chelyabinsk superbolide, particularly from dashboard cameras and traffic cameras which are ubiquitous in Russia, helped to establish the meteor's provenance as an Apollo asteroid. Sophisticated analysis techniques included the subsequent superposition of nighttime starfield views over recorded daytime images, as well as the plotting of the daytime shadow vectors shown in several online videos.\n\nThe radiant of the impacting asteroid was located in the constellation Pegasus in the Northern hemisphere. The radiant was close to the Eastern horizon where the Sun was starting to rise.\n\nThe asteroid belonged to the Apollo group of near-Earth asteroids, and was roughly 40 days past perihelion (closest approach to the Sun) and had aphelion (furthest distance from the Sun) in the asteroid belt. Several groups independently derived similar orbits for the object, but with sufficient variance to point to different potential parent bodies of this meteoroid. The Apollo asteroid is one of the candidates proposed for the role of the \"parent body\" of the Chelyabinsk superbolide. Other published orbits are similar to the 2-kilometre-diameter asteroid to suggest they had once been part of the same object; they may not be able to reproduce the timing of the impact.\n\nPreliminary calculations rapidly showed that the object was unrelated to the long-predicted close approach of the asteroid 367943 Duende, that flew by Earth 16 hours later at a distance of 27,700 km. The Sodankylä Geophysical Observatory, Russian sources, the European Space Agency, NASA and the Royal Astronomical Society all indicated the two objects could not have been related because the two asteroids had widely different trajectories.\n\n\n\n"}
{"id": "22548759", "url": "https://en.wikipedia.org/wiki?curid=22548759", "title": "Chronomètre of Loulié", "text": "Chronomètre of Loulié\n\nThe chronomètre is a precursor of the metronome. It was invented circa 1694 by Étienne Loulié to record the preferred tempo of pieces of music. \n\nCirca 1694 Étienne Loulié, a musician who had recently collaborated with mathematician Joseph Sauveur on the education of Philippe, Duke of Chartres, was asked by Chartres to work with Sauveur on a scientific study of acoustics sponsored by the Royal Academy of Science.\n\nTo measure scientifically the number of beats per second caused by different dissonances, they used the \"seconds pendulum\" invented by Galileo earlier in the century. It doubtlessly was these experiments, on top of his lessons to Chartres, that gave Loulié the idea for his chronomètre, a precursor of the metronome.\n\nIn his \"Éléments\" (Paris: Ballard, 1696) — which resumes the lessons Loulié had given to Chartres and is dedicated to the prince — Loulié described this invention, complete with an engraving of the device. (A translation of Loulié's description is provided below.) \n\nThe device is basically a Galilean seconds pendulum disguised as a classical column. It consists of a six-foot-tall vertical \"ruler\" marked off in inches, with a little peg-hole at every inch. From the right-angle bar that protrudes at the capital of the Ionic capital, hangs a string with a plumb bob at the end. The length of the string — and therefore the speed of the pendulum swings — can be adjusted by moving the peg at the other end of the string up and down the vertical board and inserting it in one peg-hole or another. The shorter the string, the more rapid the swings; the longer the string, the slower the swings.\n\nTo specify the tempo of a piece, the composer could henceforth test the tempo at a variety of peg holes and, having determined the right tempo, could mark at the top of a piece the note value that represented the musical beat, plus the number of the hole into which the peg had been inserted.\n\nSauveur subsequently criticized the device because it was measured in inches, which did not conform with any known relation to the duration of a second. His \"échomètre\" tried to remedy this shortcoming, by marking the vertical ruler with the small units that the Sauveur was creating for his \"Nouveau système\". When Loulié died in 1702, Louis Léon Pajot, comte d'Onsenbray, acquired Loulié's model and presented his own variant (the \"métromètre\") to the Academy of Sciences in 1732. It was measured in seconds and made the swings of the pendulum audible. The size of all three devices rendered them too cumbersome for widespread use.\n\nOn pages 85–86 of his \"Éléments\", Loulié emphasized the usefulness of his device:\n\nLoulié's allusions to the airs of Jean Baptiste de Lully are particularly meaningful within the broader context of Loulié's business activities. Loulié was directing a workshop that copied Lully's airs (usually in trio form), probably for sale by his friend Henri Foucault, the music-paper dealer and disseminator of Lully's works in both manuscript and print.\n\nLoulié's own description of the invention follows, translated from pp. 83–86 of his \"Éléments\":\n\n"}
{"id": "10795528", "url": "https://en.wikipedia.org/wiki?curid=10795528", "title": "Cylindrospermopsin", "text": "Cylindrospermopsin\n\nCylindrospermopsin (abbreviated to CYN, or CYL) is a cyanotoxin produced by a variety of freshwater cyanobacteria. CYN is a polycyclic uracil derivative containing guanidino and sulfate groups. It is also zwitterionic, making it highly water soluble. CYN is toxic to liver and kidney tissue and is thought to inhibit protein synthesis and to covalently modify DNA and/or RNA. It is not known whether cylindrospermopsin is a carcinogen, but it appears to have no tumour initiating activity in mice.\n\nCYN was first discovered after an outbreak of a mystery disease on Palm Island, Queensland, Australia. The outbreak was traced back to a bloom of \"Cylindrospermopsis raciborskii\" in the local drinking water supply, and the toxin was subsequently identified. Analysis of the toxin led to a proposed chemical structure in 1992, which was revised after synthesis was achieved in 2000. Several analogues of CYN, both toxic and non-toxic, have been isolated or synthesised.\n\n\"C. raciborskii\" has been observed mainly in tropical areas, however has also recently been discovered in temperate regions of Australia, North, South America, New Zealand and Europe. However, CYN-producing strain of C. raciborskii has not been identified in Europe, several other cyanobacteria species occurring across the continent are able to synthesize it.\n\nIn 1979, 138 inhabitants of Palm Island, Queensland, Australia, were admitted to hospital, suffering various symptoms of gastroenteritis. All of these were children; in addition, 10 adults were affected but not hospitalised. Initial symptoms, including abdominal pain and vomiting, resembled those of hepatitis; later symptoms included kidney failure and bloody diarrhoea. Urine analysis revealed high levels of proteins, ketones and sugar in many patients, along with blood and urobilinogen in lesser numbers. The urine analysis, along with faecal microscopy and poison screening, could not provide a statistical link to the symptoms. All patients recovered within 4 to 26 days, and at the time there was no apparent cause for the outbreak. Initial thoughts on the cause included poor water quality and diet, however none were conclusive, and the illness was coined the “Palm Island Mystery Disease”.\n\nAt the time, it was noticed that this outbreak coincided with a severe algal bloom in the local drinking water supply, and soon after the focus turned to the dam in question. An epidemiological study of this “mystery disease” later confirmed that the Solomon Dam was implicated, as those that became ill had used water from the dam. It became apparent that a recent treatment of the algal bloom with copper sulfate caused lysis of the algal cells, releasing a toxin into the water. \nA study of the dam revealed that periodic blooms of algae were caused predominantly by three strains of cyanobacteria: two of the genus \"Anabaena\", and \"Cylindrospermopsis raciborskii\", previously unknown in Australian waters. A mouse bioassay of the three demonstrated that although the two \"Anabaena\" strains were non-toxic, \"C. raciborskii\" was highly toxic. Later isolation of the compound responsible led to the identification of the toxin cylindrospermopsin.\n\nA later report alternatively proposed that the excess copper in the water was the cause of the disease. The excessive dosing was following the use of least-cost contractors to control the algae, who were unqualified in the field.\n\nIsolation of the toxin using cyanobacteria cultured from the original Palm Island strain was achieved by gel filtration of an aqueous extract, followed by reverse-phase HPLC. Structure elucidation was achieved via mass spectrometry (MS) and nuclear magnetic resonance (NMR) experiments, and a structure (later proven slightly incorrect) was proposed (Figure 1).\n\nThis almost-correct molecule possesses a tricyclic guanidine group (rings A, B & C), along with a uracil ring (D). The zwitterionic nature of the molecule makes this highly water-soluble, as the presence of charged areas within the molecule creates a dipole effect, suiting the polar solvent. Sensitivity of key signals in the NMR spectrum to small changes in pH suggested that the uracil ring exists in a keto/enol tautomeric relationship, where a hydrogen transfer results in two distinct structures (Figure 2). It was originally proposed that a hydrogen bond between the uracil and guanidine groups in the enol tautomer would make this the dominant form.\n\nA second metabolite of \"C. raciborskii\" was identified from extracts of the cyanobacteria after the observation of a frequently occurring peak accompanying that of CYN during UV and MS experiments. Analysis by MS and NMR methods concluded that this new compound was missing the oxygen adjacent to the uracil ring, and was named deoxycylindrospermopsin (Figure 3).\n\nIn 1999, an epimer of CYN, named 7-epicyclindrospermopsin (epiCYN), was also identified as a minor metabolite from \"Aphanizomenon ovalisporum\". This occurred whilst isolating CYN from cyanobacteria taken from Lake Kinneret in Israel. The proposed structure of this molecule differed from CYN only in the orientation of the hydroxyl group adjacent to the uracil ring (Figure 4).\n\nSynthetic approaches to CYN started with the piperidine ring (A), and progressed to annulation of rings B and C. The first total synthesis of CYN was reported in 2000 through a 20-step process.\n\nImprovements to synthetic methods led to a revision of the stereochemistry of CYN in 2001. A synthetic process controlling each of the six stereogenic centres of epiCYN established that the original assignments of both CYN and epiCYN were in fact a reversal of the correct structures. An alternative approach by White and Hansen supported these absolute configurations (Figure 5). At the time of this correct assignment, it was suggested that the enol form was not dominant.\n\nOne of the key factors associated with the toxicity of CYN is its stability. Although the toxin has been found to degrade rapidly in an algal extract when exposed to sunlight, it is resistant to degradation by changes in pH and temperature, and shows no degradation in either the pure solid form or in pure water. As a result, in turbid and unmoving water the toxin can persist for long periods, and although boiling water will kill the cyanobacteria, it may not remove the toxin.\n\nHawkins \"et al.\". demonstrated the toxic effects of CYN by mouse bioassay, using an extract of the original Palm Island strain. Acutely poisoned mice displayed anorexia, diarrhoea and gasping respiration. Autopsy results revealed haemorrhages in the lungs, livers, kidneys, small intestines and adrenal glands. Histopathology revealed dose-related necrosis of hepatocytes, lipid accumulation, and fibrin thrombi formation in blood vessels of the liver and lungs, along with varying epithelial cell necrosis in areas of the kidneys.\n\nA more recent mouse bioassay of the effects of cylindrospermopsin revealed an increase in liver weight, with both lethal and non-lethal doses; in addition the livers appeared dark-coloured. Extensive necrosis of hepatocytes was visible in mice administered a lethal dose, and some localised damage was also observed in mice administered a non-lethal dose.\n\nAn initial estimate of the toxicity of CYN in 1985 was that an at 24 hours was 64±5 mg of freeze-dried culture/kg of mouse body weight on intraperitoneal injection. A further experiment in 1997 measured the LD as 52 mg/kg at 24 hours and 32 mg/kg at 7 days, however the data suggested that another toxic compound was present in the isolate of sonicated cells used; predictions made by Ohtani \"et al.\" about the 24‑hour toxicity were considerably higher, and it was proposed that another metabolite was present to account for the relatively low 24‑hour toxicity level measured.\n\nBecause the most likely human route of uptake of CYN is ingestion, oral toxicity experiments were conducted on mice. The oral LD was found to be 4.4-6.9 mg CYN/kg, and in addition to some ulceration of the oesophageal gastric mucosa, symptoms were consistent with that of intraperitoneal dosing. Stomach contents included culture material, which indicated that these LD figures might be overestimated.\n\nPathological changes associated with CYN poisoning were reported to be in four distinct stages: inhibition of protein synthesis, proliferation of membranes, lipid accumulation within cells, and finally cell death. Examination of mice livers removed at autopsy showed that on intraperitoneal injection of CYN, after 16 hours ribosomes from the rough endoplasmic reticulum (rER) had detached, and at 24 hours, marked proliferation of the membrane systems of the smooth ER and Golgi apparatus had occurred. At 48 hours, small lipid droplets had accumulated in the cell bodies, and at 100 hours, hepatocytes in the hepatic lobules were destroyed beyond function.\n\nThe process of protein synthesis inhibition has been shown to be irreversible, however is not conclusively the method of cytotoxicity of the compound. Froscio \"et al.\". proposed that CYN has at least two separate modes of action: the previously reported protein synthesis inhibition, and an as-yet unclear method of causing cell death. It has been shown that cells can survive for long periods (up to 20 hours) with 90% inhibition of protein synthesis, and still maintain viability. Since CYN is cytotoxic within 16–18 hours it has been suggested that other mechanisms are the cause of cell death.\n\nCytochrome P450 has been implicated in the toxicity of CYN, as blocking the action of P450 reduces the toxicity of CYN. It has been proposed that an activated P450-derived metabolite (or metabolites) of CYN is the main cause of toxicity. Shaw \"et al.\". demonstrated that the toxin could be metabolised \"in vivo\", resulting in bound metabolites in the liver tissue, and that damage was more prevalent in rat hepatocytes than other cell types.\n\nDue to the structure of CYN, which includes sulfate, guanidine and uracil groups, it has been suggested that CYN acts on DNA or RNA. Shaw \"et al.\". reported covalent binding of CYN or its metabolites to DNA in mice, and DNA strand breakage has also been observed. Humpage \"et al.\" also supported this, and in addition postulated that CYN (or a metabolite) acts on either the spindle or centromeres during cell division, inducing loss of whole chromosomes.\n\nThe uracil group of CYN has been identified as a pharmacophore of the toxin. In two experiments, the vinylic hydrogen atom on the uracil ring was replaced with a chlorine atom to form 5-chlorocylindrospermopsin, and the uracil group was truncated to a carboxylic acid, to form cylindrospermic acid (Figure 6). Both products were assessed as being non-toxic, even at 50 times the LD of CYN. In the previous determination of the structure of deoxycylindrospermopsin, a toxicity assessment of the compound was carried out. Mice injected intraperitoneally with four times the 5-day median lethal dose of CYN showed no toxic effects. As this compound was shown to be relatively abundant, it was concluded that this analogue was comparatively non-toxic. Given that both CYN and epiCYN are toxic, the hydroxyl group on the uracil bridge can be considered necessary for toxicity. As yet, the relative toxicities of CYN and epiCYN have not been compared.\n\nSince the Palm Island outbreak, several other species of cyanobacteria have been identified as producing CYN: \"Anabaena bergii\", \"Anabaena lapponica \", \"Aphanizomenon ovalisporum\", \"Umezakia natans\", \"Raphidiopsis curvata\". and \"Aphanizomenon issatschenkoi\". In Australia, three main toxic cyanobacteria exist: \"Anabaena circinalis\", \"Microcystis\" species and \"C. raciborskii\". Of these the latter, which produces CYN, has attracted considerable attention, not only due to the Palm Island outbreak, but also as the species is spreading to more temperate areas. Previously, the algae was classed as only tropical, however it has recently been discovered in temperate regions of Australia, Europe, North and South America, and also New Zealand.\n\nIn August 1997, three cows and ten calves died from cylindrospermopsin poisoning on a farm in northwest Queensland. A nearby dam containing an algal bloom was tested, and \"C. raciborskii\" was identified. Analysis by HPLC/mass spectrometry revealed the presence of CYN in a sample of the biomass. An autopsy of one of the calves reported a swollen liver and gall bladder, along with haemorrhages of the heart and small intestine. Histological examination of the hepatic tissue was consistent with that reported in CYN-affected mice. This was the first report of \"C. raciborskii\" causing mortality in animals in Australia.\n\nThe effect of a bloom of \"C. raciborskii\" on an aquaculture pond in Townsville, Australia was assessed in 1997. The pond contained Redclaw crayfish, along with a population of Lake Eacham Rainbowfish to control the excess food. Analysis revealed that the water contained both extracellular and intracellular CYN, and that the crayfish had accumulated this primarily in the liver but also in the muscle tissue. Examination of the gut contents revealed cyanobacterial cells, indicating that the crayfish had ingested intracellular toxin. An experiment using an extract of the bloom showed that it was also possible to uptake extracellular toxin directly into the tissues. Such bioaccumulation, particularly in the aquaculture industry, was of concern, especially when humans were the end users of the product.\n\nThe impact of cyanobacterial blooms has been assessed in economic terms. In December 1991, the world’s largest algal bloom occurred in Australia, where 1000 km of the Darling-Barwon River was affected. One million people-days of drinking water were lost, and the direct costs incurred totalled more than A$1.3 million. Moreover, 2000 site-days of recreation were also lost, and the economic cost was estimated at A$10 million, after taking into account indirectly affected industries such as tourism, accommodation and transport.\n\nCurrent methods include liquid chromatography coupled to mass spectrometry (LC-MS), mouse bioassay, protein synthesis inhibition assay, and reverse-phase HPLC-PDA (Photo Diode Array) analysis. A cell free protein synthesis assay has been developed which appears to be comparable to HPLC-MS.\n\n"}
{"id": "4553647", "url": "https://en.wikipedia.org/wiki?curid=4553647", "title": "Disulfur decafluoride", "text": "Disulfur decafluoride\n\nDisulfur decafluoride (SF) is a chemical compound discovered in 1934 by Denbigh and Whytlaw-Gray. Each sulfur atom of the SF molecule is octahedral, and surrounded by five fluorine atoms. SF is highly toxic, with toxicity four times that of phosgene. It was considered a potential chemical warfare pulmonary agent in World War II because it does not produce lacrimation or skin irritation, thus providing little warning of exposure. It is produced by the electrical decomposition of sulfur hexafluoride (SF)—an essentially inert insulator used in high voltage systems such as transmission lines, substations and switchgear. SF is also made during the production of SF, but is distilled out. It is a colorless liquid with a burnt match smell similar to sulfur dioxide. \n\nDisulfur decafluoride is produced primarily by the decomposition of sulfur hexafluoride:\n\nThis compound contains two sulfur atoms that have zero polarization between them and five polarized connections with fluorine atoms.\n\nAt temperatures above 150 °C, decomposes slowly (disproportionation) to and :\n\nIn the presence of excess chlorine gas, reacts to form sulfur chloride pentafluoride ():\n\nThe analogous reaction with bromine is reversible and yields . The reversibility of this reaction can be used to synthesize from .\n\nAmmonia is oxidised by into .\n\nDisulfur decafluoride is a colorless gas or liquid with a SO-like odor. It is about 4 times as poisonous as phosgene. Its toxicity is thought to be caused by its disproportionation in the lungs into , which is inert, and , which reacts with moisture to form sulfurous acid and hydrofluoric acid. Disulfur decafluoride itself is not toxic due to hydrolysis products, since it is hardly hydrolysed by water and most aqueous solutions.\n"}
{"id": "586610", "url": "https://en.wikipedia.org/wiki?curid=586610", "title": "Downburst", "text": "Downburst\n\nA downburst is a strong ground-level wind system that emanates from a point source above and blows radially, that is, in straight lines in all directions from the point of contact at ground level. Often producing damaging winds, it may be confused with a tornado, where high-velocity winds circle a central area, and air moves inward and upward; by contrast, in a downburst, winds are directed downward and then outward from the surface landing point.\n\nDownbursts are created by an area of significantly rain-cooled air that, after reaching ground level, spreads out in all directions producing strong winds. Dry downbursts are associated with thunderstorms with very little rain, while wet downbursts are created by thunderstorms with high amounts of rainfall. Microbursts and macrobursts are downbursts at very small and larger scales, respectively. Another variety, the heat burst, is created by vertical currents on the backside of old outflow boundaries and squall lines where rainfall is lacking. Heat bursts generate significantly higher temperatures due to the lack of rain-cooled air in their formation. Downbursts create vertical wind shear or microbursts, which is dangerous to aviation.\n\nA downburst is created by a column of sinking air that after hitting ground level, spreads out in all directions and is capable of producing damaging straight-line winds of over , often producing damage similar to, but distinguishable from, that caused by tornadoes. This is because the physical properties of a downburst are completely different from those of a tornado. Downburst damage will radiate from a central point as the descending column spreads out when hitting the surface, whereas tornado damage tends towards convergent damage consistent with rotating winds. To differentiate between tornado damage and damage from a downburst, the term straight-line winds is applied to damage from microbursts.\n\nDownbursts are particularly strong downdrafts from thunderstorms. Downbursts in air that is precipitation free or contains virga are known as dry downbursts; those accompanied with precipitation are known as wet downbursts. Most downbursts are less than in extent: these are called microbursts. Downbursts larger than in extent are sometimes called macrobursts. Downbursts can occur over large areas. In the extreme case, a derecho can cover a huge area more than wide and over long, lasting up to 12 hours or more, and is associated with some of the most intense straight-line winds, but the generative process is somewhat different from that of most downbursts.\n\nStraight-line winds (also known as plough winds, thundergusts and hurricanes of the prairie) are very strong winds that can produce damage, demonstrating a lack of the rotational damage pattern associated with tornadoes. Straight-line winds are common with the gust front of a thunderstorm or originate with a downburst from a thunderstorm. These events can cause considerable damage, even in the absence of a tornado. The winds can gust to and winds of or more can last for more than twenty minutes. In the United States, such straight-line wind events are most common during the spring when instability is highest and weather fronts routinely cross the country. Straight-line wind events in the form of derechos can take place throughout the eastern half of the U.S.\n\nStraight-line winds may be damaging to marine interests. Small ships, cutters and sailboats are at risk from this meteorological phenomenon.\n\nThe formation of a downburst starts with hail or large raindrops falling through drier air. Hailstones melt and raindrops evaporate, pulling latent heat from surrounding air and cooling it considerably. Cooler air has a higher density than the warmer air around it, so it sinks to the ground. As the cold air hits the ground it spreads out and a mesoscale front can be observed as a gust front. Areas under and immediately adjacent to the downburst are the areas which receive the highest winds and rainfall, if any is present. Also, because the rain-cooled air is descending from the middle troposphere, a significant drop in temperatures is noticed. Due to interaction with the ground, the downburst quickly loses strength as it fans out and forms the distinctive \"curl shape\" that is commonly seen at the periphery of the microburst (see image). Downbursts usually last only a few minutes and then dissipate, except in the case of squall lines and derecho events. However, despite their short lifespan, microbursts are a serious hazard to aviation and property and can result in substantial damage to the area.\n\nA special, and much rarer, kind of downburst is a heat burst, which results from precipitation-evaporated air compressionally heating as it descends from very high altitude, usually on the backside of a dying squall line or outflow boundary. Heat bursts are chiefly a nocturnal occurrence, can produce winds over , are characterized by exceptionally dry air, can suddenly raise the surface temperature to or more, and sometimes persist for several hours.\n\nDownbursts, particularly microbursts, are exceedingly dangerous to aircraft which are taking off or landing due to the strong vertical wind shear caused by these events. A number of fatal crashes have been attributed to downbursts.\n\n\n\n"}
{"id": "33545678", "url": "https://en.wikipedia.org/wiki?curid=33545678", "title": "EGP-6", "text": "EGP-6\n\nEGP-6 is a small type of nuclear reactor, a scaled down version of the RBMK reactor design. Notably, these reactors along with the RBMK designs are some of the few active reactors which still use ordinary (light) water cooled graphite as a neutron moderator.\n\nThe only operating four reactors of this type exist at the Bilibino Nuclear Power Plant, commissioned in 1974. The plant design was developed by the Ural Division of Teploelektroproekt together with Izhorskiye Zavody and FEI in Obninsk. It produces 12 MWe.\n\nIt is the world's smallest commercial nuclear reactor.\n"}
{"id": "6011500", "url": "https://en.wikipedia.org/wiki?curid=6011500", "title": "Energising India", "text": "Energising India\n\nEnergising India (2004) is a short documentary on how India can have energy without polluting the atmosphere. Although about India, this is a German film (since the producers are German, GTZ) and directed by the National Award winning Indian director Jyoti Sarup. This film was premiered in the World Energy Conservation Conference, Germany, in 2004.\n\nThe film showcases methods of industrialisation through the case study of three of the top companies of India- INDAL Aluminium (Birla Group), Reliance Petro-Chemical (Reliance Group) and Raymond (Raymond Group).\n\nThe film highlights the point that these companies have pollution below 1% and then shows the methods/techniques adopted by them.\n\nThe film started off by showcasing only one company. But when the film was shown to Mr. Sahi, the Secretary, dept. of Energy Conservation, Govt. of India he appreciated the film and efforts and the final result and asked the producers to give the film a wider perspective by adding to more companies of different sectors in the film.\n\n"}
{"id": "39068703", "url": "https://en.wikipedia.org/wiki?curid=39068703", "title": "Filetto marble", "text": "Filetto marble\n\nFiletto marble is a type of beige marble popular for use in sculpture and building decor. It is quarried at the city of North Sinai in the Arish.\n"}
{"id": "36751489", "url": "https://en.wikipedia.org/wiki?curid=36751489", "title": "Flame supervision device", "text": "Flame supervision device\n\nFor gas appliances, a flame supervision device (FSD) – alternative name: flame failure device (FFD) – is a general term for any device designed to stop flammable gas going to the burner of a gas appliance if the flame is extinguished. This is to prevent a dangerous buildup of gas within the appliance, its chimney or the room. Causes of flame failure include chimney downdraught, temporary interruption of the gas supply, gas under-pressure, liquid overspill on cooker hotplates or the draught from an oven door being opened and closed.\n\nFSDs may utilize one of several technologies: thermoelectric valves, flame conductance, flame rectification, ultraviolet sensing devices and liquid expansion valves. \n\nFSD usage in consumer products differs among political units – in the U.S., FSDs are not required by law or regulation for gas range (or gas stove) top burners and consequently are not present on ranges in the U.S. Cooker hotplates may not have a FSD on each burner. If a hotplate is to be used in a multi-occupancy building every burner must have its own FSD.\n\nWhen the FSD activates it should stop (or reduce to safe levels) gas flow to the burner until it is reset manually.\n\nOlder devices, such as bimetallic strips, were used in conjunction with pilot lights. The pilot light is no longer used in new devices, but may still be encountered on old appliances still in service. Pilot lights were withdrawn because their continual small flame represented a waste of fuel. Pilot lights required their own FSD, typically a thermocouple which held the valve open. Regular testing of FSD is a part of routine maintenance for gas appliances.\n\nOther safety devices may be fitted in addition to an FSD. One type of these are the Vitiation Sensing Devices, that detect an adequate supply of oxygen for efficient combustion, thus avoiding the production of poisonous carbon monoxide. As well as detecting a blocked supply of oxygen, these must also detect a blocked exhaust or reversed flow in the exhaust flue owing to wind conditions. Two methods are used to provide these, an Oxygen Depletion System (ODS) that measures the availability of oxygen for combustion or an Atmospheric Sensing Device (ASD) measures excess heat rise in the exhaust.\n\n\n"}
{"id": "275935", "url": "https://en.wikipedia.org/wiki?curid=275935", "title": "Flexible AC transmission system", "text": "Flexible AC transmission system\n\nA flexible alternating current transmission system (FACTS) is a system composed of static equipment used for the AC transmission of electrical energy. It is meant to enhance controllability and increase power transfer capability of the network. It is generally a power electronics-based system.\n\nFACTS is defined by the IEEE as \"a power electronic based system and other static equipment that provide control of one or more AC transmission system parameters to enhance controllability and increase power transfer capability.\"\n\nAccording to Siemens \"FACTS Increase the reliability of AC grids and reduce power delivery costs. They improve transmission quality and efficiency of power transmission by supplying inductive or reactive power to the grid.\n\nIn shunt compensation, power system is connected in shunt (parallel) with the FACTS. It works as a controllable current source. Shunt compensation is of two types:\n\n\n\nformula_1\n\nformula_2 power angle\n\nIn the case of a no-loss line, voltage magnitude at the receiving end is the same as voltage magnitude at the sending end: V = V=V.\nTransmission results in a phase lag formula_3 that depends on line reactance X.\n\nformula_4\n\nAs it is a no-loss line, active power P is the same at any point of the line:\n\nformula_5\n\nReactive power at sending end is the opposite of reactive power at receiving end:\n\nformula_6\nAs formula_7 is very small, active power mainly depends on formula_3 whereas reactive power mainly depends on voltage magnitude.\n\nFACTS for series compensation modify line impedance: X is decreased so as to increase the transmittable active power. However, more reactive power must be provided.\nformula_9\n\nReactive current is injected into the line to maintain voltage magnitude. Transmittable active power is increased but more reactive power is to be provided.\nformula_10\n\n\n\n\n\n"}
{"id": "14397548", "url": "https://en.wikipedia.org/wiki?curid=14397548", "title": "Forests Now Declaration", "text": "Forests Now Declaration\n\nThe Forests Now Declaration is a declaration that advocates using carbon credits to protect tropical forests. The Declaration was created by the Global Canopy Programme, and has been signed by over 200 NGOs, business leaders, scientists and conservationists. The Declaration was created as carbon credits from land use, land-use change and forestry were omitted from the Clean Development Mechanism for the First Commitment Period of the Kyoto Protocol despite contributing 18-25% of all emissions.\n\nDeforestation in the next five years will release more carbon dioxide than all aircraft since the Wright Brothers until at least 2025; however, credits from reduced deforestation were omitted from the Clean Development Mechanism for the first commitment period of the Kyoto Protocol, so there is little incentive for forested countries to reduce their deforestation rates. The Forests NOW declaration seeks to establish new market based mechanisms to protect the ecosystem services that forests provide in biodiversity conservation, carbon sequestration and global and local hydrological and mineral cycles.\n\nThe Declaration prescribes six changes to the existing carbon market frameworks:\n\nOver 200 individuals and organisations have signed the Declaration including:\n\nA full list of signatories can be found here \n\n"}
{"id": "1783020", "url": "https://en.wikipedia.org/wiki?curid=1783020", "title": "Gas generator", "text": "Gas generator\n\nA gas generator is a device for generating gas. A gas generator may create gas by a chemical reaction or from a solid or liquid source, when storing a pressurized gas is undesirable or impractical.\n\nThe term often refers to a device that uses a rocket propellant to generate large quantities of gas. The gas is typically used to drive a turbine rather than to provide thrust as in a rocket engine. Gas generators of this type are used to power turbopumps in rocket engines, and are used by some auxiliary power units to power electrical generators and hydraulic pumps. \n\nAnother common use of the term is in the Industrial gases industry, where gas generators are used to produce gaseous chemicals for sale.\nFor example, the chemical oxygen generator, which delivers breathable oxygen at a controlled rate over a prolonged period. During World War II, portable gas generators that converted coke to producer gas were used to power vehicles as a way of alleviating petrol shortages.\n\nOther types include the gas generator in an automobile airbag, which is designed to rapidly produce a specific quantity of inert gas.\n\nThe V-2 rocket used hydrogen peroxide decomposed by a liquid sodium permanganate catalyst solution as a gas generator. This was used to drive a turbopump to pressurize the main LOX-ethanol propellants. In the Saturn V F-1 and Space Shuttle main engine, some of the main propellant was burned to drive the turbopump (see gas-generator cycle and staged combustion cycle). The gas generator in these designs uses a highly fuel-rich mix to keep flame temperatures relatively low.\n\nThe Space Shuttle auxiliary power unit and the F-16 emergency power unit (EPU) use hydrazine as a fuel. The gas drives a turbine which drives hydraulic pumps. In the F-16 EPU it also drives an electrical generator.\n\nGas generators have also been used to power torpedoes. For example, the US Navy Mark 16 torpedo was powered by hydrogen peroxide.\nA concentrated solution of hydrogen peroxide is known as high test peroxide and decomposes to produce oxygen and water (steam).\n\nHydrazine decomposes to nitrogen and hydrogen. The reaction is strongly exothermic and produces high volume of hot gas from small volume of liquid.\n\nMany solid rocket propellant compositions can be used as gas generators.\n\nMany automobile airbags use sodium azide for inflation (). A small pyrotechnic charge triggers its decomposition, producing nitrogen gas, which inflates the airbag in around 30 milliseconds. A typical airbag in the US might contain 130 grams of sodium azide.\n\nSimilar gas generators are used for fire suppression.\n\nSodium azide decomposes exothermically to sodium and nitrogen.\nThe resulting sodium is hazardous, so other materials are added, e.g. potassium nitrate and silica, to convert it to a silicate glass.\nA chemical oxygen generator delivers breathable oxygen at a controlled rate over a prolonged period. \nSodium, potassium, and lithium chlorates and perchlorates are used.\n\nA device that converts coke or other carbonaceous material into producer gas may be used as a source of fuel gas for industrial use. Portable gas generators of this type were used during World War II to power vehicles as a way of alleviating petrol shortages.\n\n"}
{"id": "2785586", "url": "https://en.wikipedia.org/wiki?curid=2785586", "title": "Girder", "text": "Girder\n\nA girder is a support beam used in construction. It is the main horizontal support of a structure which supports smaller beams. Girders often have an I-beam cross section composed of two load-bearing \"flanges\" separated by a stabilizing \"web\", but may also have a box shape, Z shape and other forms. A girder is commonly used to build bridges.\n\nIn traditional timber framing a girder is called a girt.\n\nSmall steel girders are rolled into shape. Larger girders (1 m/3 feet deep or more) are made as plate girders, welded or bolted together from separate pieces of steel plate.\n\nThe Warren type girder replaces the solid web with an open latticework between the flanges. This truss arrangement combines strength with economy of materials and can therefore be relatively light. Patented in 1848 by its designers James Warren and Willoughby Theobald Monzani, its structure consists of longitudinal members joined only by angled cross-members, forming alternately inverted equilateral triangle-shaped spaces along its length, ensuring that no individual strut, beam, or tie is subject to bending or torsional straining forces, but only to tension or compression. It is an improvement over the Neville truss which uses a spacing configuration of isosceles triangles.\n\n"}
{"id": "146839", "url": "https://en.wikipedia.org/wiki?curid=146839", "title": "Gram", "text": "Gram\n\nThe gram (alternative spelling: gramme; SI unit symbol: g) (Latin \"\", from Greek , \"grámma\") is a metric system unit of mass.\n\nOriginally defined as \"the absolute weight of a volume of pure water equal to the cube of the hundredth part of a metre [1 cm], and at the temperature of melting ice\" (later at 4 °C, the temperature of maximum density of water). However, in a reversal of reference and defined units, a gram is now defined as one thousandth of the SI base unit, the kilogram, or 1×10 kg, which itself is \"now defined\" by the International Bureau of Weights and Measures, not in terms of grams, but by \"the amount of electricity needed to counteract its force\"\n\nThe only unit symbol for gram that is recognised by the International System of Units (SI) is \"g\" following the numeric value with a space, as in \"640 g\" to stand for \"640 grams\" in the English language. The SI does not support the use of abbreviations such as \"gr\" (which is the symbol for grains), \"gm\" (\"g⋅m\" is the SI symbol for gram-metre) or \"Gm\" (the SI symbol for gigametre).\n\nThe word \"gramme\" was adopted by the French National Convention in its 1795 decree revising the metric system as replacing the \"gravet\" introduced in 1793. Its definition remained that of the weight (\"poids\") of a cubic centimetre of water.\nFrench \"gramme\" was taken from the Late Latin term \"\". This word—ultimately from Greek (\"grámma\"), \"letter\"—had adopted a specialised meaning in Late Antiquity of \"one twenty-fourth part of an ounce\" (two oboli), corresponding to about 1.14 modern grams. This use of the term is found in the \"carmen de ponderibus et mensuris\" (\"poem about weights and measures\") composed around 400 AD.\nThere is also evidence that the Greek was used in the same sense at around the same time, in the 4th century, and survived in this sense into Medieval Greek, while the Latin term did not remain current in Medieval Latin and was recovered in Renaissance scholarship.\n\nThe gram was the fundamental unit of mass in the 19th-century centimetre–gram–second system of units (CGS). The CGS system co-existed with the MKS system of units, first proposed in 1901, during much of the 20th century, but the gram has been displaced by the kilogram as the fundamental unit for mass when the MKS system was chosen for the SI base units in 1960.\n\nThe gram is today the most widely used unit of measurement for non-liquid ingredients in cooking and grocery shopping worldwide.\n\nMost standards and legal requirements for nutrition labels on food products require relative contents to be stated per 100 g of the product, such that the resulting figure can also be read as a percentage by weight.\n\n\n\n"}
{"id": "1471036", "url": "https://en.wikipedia.org/wiki?curid=1471036", "title": "Graphical timeline of the universe", "text": "Graphical timeline of the universe\n\nThis more than 20-billion-year timeline of our universe shows the best estimates of major events from the universe's beginning to anticipated future events. Zero on the scale is the present day. A large step on the scale is one billion years; a small step, one hundred million years. The past is denoted by a minus sign: e.g., the oldest rock on Earth was formed about four billion years ago and this is marked at -4e+09 years, where 4e+09 represents 4 times 10 to the power of 9. The \"Big Bang\" event most likely happened 13.8 billion years ago; see age of the universe.\n\n"}
{"id": "23699646", "url": "https://en.wikipedia.org/wiki?curid=23699646", "title": "High tension leads", "text": "High tension leads\n\nHigh tension leads or high tension cables or spark plug wires or spark plug cables are the wires that connect a distributor, ignition coil, or magneto to each of the spark plugs in some types of internal combustion engine. \"High tension lead\" or \"cable\" is also used for any electrical cable carrying a high voltage in any context. \"Tension\" in this instance is a synonym for voltage. High tension leads, like many engine components, wear out over time. Each lead contains only one wire, as the current does not return through the same lead, but through the earthed/grounded engine which is connected to the opposite battery terminal (negative terminal on modern engines) high tension may also be referred to as HT. \nSpark plug wires have an outer insulation several times thicker than the conductor, made of a very flexible and heat-resistant material such as silicone or EPDM rubber. The thick insulation prevents arcing from the cable to an earthed engine component. A rubber \"boot\" covers each terminal. Dielectric grease can be used to improve insulation; a small amount can be applied in the inside of the rubber boot at each end of each wire to help seal out moisture. Printing on spark plug wires may include a brand name, insulation thickness (in millimeters), insulation material type, cylinder number, and conductor type (suppressor or solid wire). \n\nThe wire from each spark plug is just long enough to reach the distributor, without excess. Each end of a spark plug wire has a metal terminal that clips onto the spark plug and distributor, coil, or magneto. There are dedicated spark plug wire pliers, tools designed for removing the terminal from a spark plug without damaging it. \n\nTo reduce radio frequency interference (RFI) produced by the spark being radiated by the wires, which may cause malfunction of sensitive electronic systems in modern vehicles or interfere with the car radio, various means in the spark plug and associated lead have been used over time to reduce the nuisance:\n\nDuring the production of the spark plug cable, the presence of residual dielectric is often found. This deposit affects the conductivity characteristics of the wire, creating a more or less important resistance that does not allow the wire to pass the routine tests. The patent of the company Brecav allowed to pursue the improvement of the quality standards of production of the spark plug wire and a certification of the working hours of the spark plug cable.\n\nPlacing spark plug wires back into their separators or holders during replacement helps to keep them in place despite engine vibration, extending their life. \nA common problem with spark plug wires is corrosion of the metal end terminals. Better-quality spark plug wires usually have brass terminals, which are more resistant to corrosion than other metals used.\n\nOlder engines also have a wire connecting the ignition coil to the distributor, known as a coil wire. A coil wire is of the same construction as a spark plug wire, but generally shorter and with different terminals. Some distributors have an ignition coil built inside them, eliminating the need for a separate coil wire, e.g. GM High energy ignition system and some Toyotas and Hondas.\n\nMany modern car engines have multiple ignition coils (one for each pair of cylinders) built into a coil pack, eliminating the need for a distributor and coil wire. Some car engines use a small ignition coil mounted on top of each spark plug, eliminating the need for spark plug wires entirely.\n\nThe average life span of a candle cable is about 50000 km (in the case of a mixed path). The wear of the spark plug wire leads first to an increase in fuel consumption and a loss in performance. The increased risk of a worn spark plug wire is the possibility of damaging the system downstream of the carburation.\n\n"}
{"id": "2851833", "url": "https://en.wikipedia.org/wiki?curid=2851833", "title": "Hypermiling", "text": "Hypermiling\n\nHypermiling is driving a vehicle with techniques that minimize fuel consumption. Those who use these techniques are called \"hypermilers\".\n\nHypermiling can be practiced in any vehicle regardless of fuel consumption. It gained popularity due to the rise in gasoline prices in the 2000s. Some hypermiling techniques are illegal in some countries because they are dangerous.\n\nIn 2008, the New Oxford American Dictionary voted Hypermiling the best new word of the year.\n\nHypermiling has come under fire from several sides because some hypermilers show dangerous or illegal behaviour, such as tailgating larger vehicles on motorways to save fuel, cycling between accelerating and coasting in neutral, and even turning the engine off when its power is not needed. For this reason, the Hypermiling Safety Foundation was established in August 2008 to promote a safety and education program that promotes legal fuel-saving techniques.\n\nThe range of electric cars is limited. To get maximum out of the battery, drivers sometimes use hypermiling.\n\n"}
{"id": "55940397", "url": "https://en.wikipedia.org/wiki?curid=55940397", "title": "International Energy Agency Energy in Buildings and Communities Programme", "text": "International Energy Agency Energy in Buildings and Communities Programme\n\nThe International Energy Agency Energy in Buildings and Communities (IEA EBC) Programme, formerly known as the Energy in Buildings and Community Systems Programme (ECBCS), is one of the International Energy Agency’s Technology Collaboration Programmes (TCPs). The Programme \"carries out research and development activities toward near-zero energy and carbon emissions in the built environment\".\nThe programme was formally launched in 1977, following the oil crisis which drove research into alternative sources of energy and technologies to improve energy efficiency. Since then, IEA EBC’s main aim has been to provide an international focus for energy efficiency research in the building sector, with its current mission being to \"“develop and facilitate the integration of technologies and processes for energy efficiency and conservation into healthy, low emission and sustainable buildings and communities, through innovation and research”\".\n\n\nEvery five years, the IEA Committee on Energy Research and Technology (CERT) renews the Programme’s Strategic Plan. The latest EBC Strategic Plan was developed in 2013 and is effective for the period March 2014 to February 2019. It was developed in consultation with the IEA’s Working Party on Energy End-Use Technologies (EUWP) and CERT. The broad scope encompassed by the EBC Programme is reflected by five main focus areas as defined in the Strategic Plan: \n\nCountries currently participating in the EBC are: Australia, Austria, Belgium, Canada, P.R. China, Czech Republic, Denmark, Finland, France, Germany, Ireland, Italy, Japan, Republic of Korea, Netherlands, New Zealand, Norway, Portugal, Singapore, Spain, Sweden, Switzerland, United Kingdom and the United States of America.\n\nThe EBC carries out research and development (R&D) projects known as Annexes, with a typical duration of 3 to 4 years forming the Programme’s basis. “The outcomes of the Annexes address the determining factors for energy use in three domains: technological aspects, policy measures, and occupant behaviour”. Below is a list with completed and current Annexes.\n\n\n\nThe EBC Programme produces a series of scientific publications. \nOutcomes and summary reports (for policy and decision makers) of the various running and completed projects are published when available. \nThe EBC newsletter “EBC News” is published twice per year, including feedback from running and forthcoming Annexes as well as other articles in the field of energy use for buildings and communities.\nThe EBC Annual Report outlines the Programme’s yearly progress, including among others separate sections summarizing the status and available deliverables for each Annex.\n"}
{"id": "31370251", "url": "https://en.wikipedia.org/wiki?curid=31370251", "title": "Iroko", "text": "Iroko\n\nIroko is a large hardwood tree from the west coast of tropical Africa that can live up to 500 years. The tree is known to the Yoruba as ìrókò, logo or loko and is believed to have supernatural properties. Iroko is known to the Igbo people as oji wood. It is one of the woods sometimes referred to as African teak, although it is unrelated to the teak family. The wood colour is initially yellow but darkens to a richer copper brown over time.\nIt is yielded mostly (probably) by \"Milicia excelsa\". In much of the literature on this timber the names of the trees that yields it are given as \"Chlorophora excelsa\" and \"Chlorophora regia\".\n\nThe tree is feared in some cultures where it originates and hence is shunned or revered with offerings. Yoruba people believe that the tree is inhabited by a spirit, and anybody who sees the Iroko-man face to face becomes insane and speedily dies. According to the Yoruba, any man who cuts down any Iroko tree causes devastating misfortune on himself and all of his family, although if they need to cut down the tree they can make a prayer afterwards to protect themselves. \n\nThey also claim that the spirit of the Iroko can be heard in houses which use Iroko wood, as the spirit of the Iroko is trapped in the wood. In Nigeria the iroko wood is of much lower quality due to soil conditions as well as root-rot. Some Westerners refer to the wood as \"poor man's teak\".\n\nThe wood is used for a variety of purposes including boat-building, domestic flooring, furniture and outdoor gates. From the late 1990s, it was used as part of the txalaparta, a Basque musical instrument constructed of wooden boards, due to its lively sound. Iroko is one of the traditional djembe woods. Iroko wood was the wood chosen for the pews in the Our Lady of Peace Basilica.\n\nIt is a very durable wood; iroko does not require regular treatment with oil or varnish when used outdoors, although it is very difficult to work with tools as it tends to splinter easily, and blunts tools very quickly.\n\nIn the UK there are no trade restrictions on the machining of this timber. The only reported adverse effects known to be caused by the dust from Iroko are asthma, dermatitis and nettle rash.\n"}
{"id": "25652402", "url": "https://en.wikipedia.org/wiki?curid=25652402", "title": "Kislaya Guba Tidal Power Station", "text": "Kislaya Guba Tidal Power Station\n\nThe Kislaya Guba Tidal Power Station is an experimental project in Kislaya Guba, Russia.\n\nThe station is the world's 5th largest tidal power plant with the output capacity of . Station began operating in 1968, but was later shut down for 10 years until December 2004, when funding resumed. The old French-built generation unit was dismantled. In 2004 was installed first new generation unit, and in 2007 – second, . The site was originally chosen because the long and deep fjord had a fairly narrow outlet to the sea which could easily be dammed for the project. There are plans for two larger scale projects based on this design near Mezen, on the White Sea and Tugur on the Sea of Okhotsk.\n\n"}
{"id": "44582850", "url": "https://en.wikipedia.org/wiki?curid=44582850", "title": "Klaus Siebold", "text": "Klaus Siebold\n\nKlaus Siebold (born Laubusch (Lauta) 12 September 1930: died Spremberg 23 June 1995) was a German politician.\n\nHe was Minister for Coal and Energy in the German Democratic Republic (East Germany) in the 1970s. He hit the headlines in 1979 in connection with his perceived role in exacerbating the consequences of the country's during the winter of 1978/79.\n\nIn 1930 Klaus Siebold was born in , a small new town in eastern Saxony. Laubusch, originally named \"Kolonie Erika\" had been built ten years earlier to a plan by the architect/town planner Ewald Kleffel: it had taken on the name \"Laubusch\" from a nearby settlement that had been removed to make way for a lignite mine. Kleffel, who designed the town, had been much influenced by the English town planner Ebenezer Howard, but whereas Howard designed pleasant dormitory towns for London commuters, Kleffel's towns were built to support the rapidly expanding Lignite (brown coal) mining industry, and Laubusch was built to accommodate workers of the mining company. Klaus Siebold's education and later life were in several respects shaped by mining. He trained as a miner. He then studied at a technical college dedicated to mining engineering, where he became a qualified Mining Engineer. Siebel was also selected for higher education at the prestigious Karl Marx Academy just outside Berlin, and here he gained a degree in Social Sciences.\n\nIn 1952 Siebold took a job as a full-time official of East Germany's ruling Socialist Unity Party (SED/ \"Sozialistische Einheitspartei Deutschlands\"). Between 1957 and 1959 he served as director of a lignite-fueled power plant. He then became head of the Coal Industry section, both at the People's Economic Council (VWR / \"Volkswirtschaftsrat\"), and with the . Between 1963 and 1965 he was Deputy Chairman with responsibility for Coal and Energy with the VWR.\n\nHe entered government in December 1965 as Minister for Primary sector industry and was then switched in November 1971 to become , a position he retained till 1979.\n\nIn April 1978 he traveled to Maputo where he signed an agreement to send more East German mining experts to Mozambique. Since January 1978 East Germans had been working in the Bituminous coal mine at Moatize, following a firedamp explosion that had taken the lives of more than 100 Mozambican miners. As head of the East German side in the joint East German/Mozambique Economic Committee, Siebold visited Mozambique two more times in 1978, and was on each occasion received by President Samora Machel.\n\nKlaus Siebold lost his ministerial position because of the that followed the exceptional winter weather in 1978/79. He had sanctioned maintenance shut downs of power plant in January 1979 at a time when sustained heavy snow falls imperiled East German power supplies. The pumps serving numerous district heating systems failed and families had to be evacuated from their frozen apartments. In cold barns young cattle were frozen to death, as were hens in their battery cages. Shortly before the snow storms, on 13/14 December 1978, at the 9th conference of the SED Central Committee, Siebold was on the receiving end of the conventional lavish praise, here from a candidate for Politburo membership called . In contrast, at the party's tenth conference, on 27 April 1979, criticism came from the party leader, Erich Honecker, who asked that for a decisive improvement in leadership from the Ministry of Coal and Energy. \"Sudden power outages should on no account become necessary in the first place.\" (\"„Plötzliche Abschaltungen dürfen gar nicht erst notwendig werden.“\") On 28 June 1979 Siebold was sacked by the Chairman of the Council of State, Willi Stoph. His dismissal came without official plaudits, in eloquent contrast to the treatment afforded to , the other minister dismissed at the same time: Fichter was fulsomely thanked for his work.\n\nAfter this Siebold was sent back to work in industry. Among other things, he worked as Director of lignite-fueled power plant.\n\nIn the first, and as it turned out last, free national election held in East Germany, in March 1990, Klaus Siebold stood for election in the Cottbus electoral district, but he was not elected.\n\n"}
{"id": "33552749", "url": "https://en.wikipedia.org/wiki?curid=33552749", "title": "Korea Institute of Nuclear Safety", "text": "Korea Institute of Nuclear Safety\n\nThe Korea Institute of Nuclear Safety (KINS) in Daejeon, South Korea was independently established as the nuclear safety regulatory expert organization in February 1990 with a specific mission to develop and implement nuclear safety regulation. The ultimate goal of nuclear safety regulation is to protect the public and to preserve the environment from the radiation hazards that might be accompanied with the production or utilization of nuclear energy. Accomplishing its precious mission, KINS will make every effort to realize VISION 2020‘ Heart of Global Nuclear Safety’.\n\nThe vision of KINS is Heart of Global Nuclear Safety that means KINS becomes the \"heart of global nuclear safety\" trusted by the people and the world, by being committed to nuclear safety with the best expertise and leadership.\n\n"}
{"id": "8359450", "url": "https://en.wikipedia.org/wiki?curid=8359450", "title": "Laid paper", "text": "Laid paper\n\nLaid paper is a type of paper having a ribbed texture imparted by the manufacturing process. In the pre-mechanical period of European papermaking (from the 12th century into the 19th century), laid paper was the predominant kind of paper produced. Its use, however, diminished in the 19th century, when it was largely supplanted by wove paper. Laid paper is still commonly used by artists as a support for charcoal drawings.\n\nBefore the mechanization of papermaking, the laid pattern was produced by the wire sieve in the rectangular mold used to produce single sheets of paper. A worker would dip the mold into a vat containing diluted linen pulp, then lift it out, tilt it to spread the pulp evenly over the sieve, and, as the water drained out between the wires, shake the mold to lock the fibers together. In the process, the pattern of the wires in the sieve was imparted to the sheet of paper.\nModern papermaking techniques use a dandy roll to create the laid pattern during the early stages of manufacture, in the same way as applying a paper watermark. While in the wet state, the paper stock (a dilute dispersion of the cellulose fibers in water) is drained on a wire mesh to de-water the stock. During this process, a dandy roll with a laid mesh pattern is pressed into the wet stock, displacing the cellulose fiber. This pattern has to be applied at a particular stock consistency; otherwise the pattern will be lost as the fiber flows back while the stock moves past the dandy (too wet), or fiber will pick out of the stock (too dry), causing surface disruption. As the fiber is displaced, localized areas of higher and lower density are produced in a laid pattern, and the pattern is also created on the paper's surface. The pattern is therefore apparent both as one looks through the sheet and as one views its surface. Applying the laid pattern as a mechanical emboss would not create the ribbed pattern seen on looking through the sheet, as this is achieved only by watermarking techniques.\n\nThe traditional laid pattern consists of a series of wide-spaced lines (chain lines) parallel to the shorter sides of the sheet—or, in machine-made paper, running in the machine direction—and more narrowly spaced lines (laid lines) which are at 90 degrees to the chain lines.\n"}
{"id": "1423102", "url": "https://en.wikipedia.org/wiki?curid=1423102", "title": "Leo Yaffe", "text": "Leo Yaffe\n\nLeo Yaffe, (July 6, 1916 – May 14, 1997) was a Canadian nuclear chemistry scientist and a proponent of the peaceful uses of nuclear power.\n\nBorn in Devils Lake, North Dakota, his family moved to Winnipeg in 1920. He studied at the University of Manitoba receiving a B.Sc.(Hons) in 1940, a M.Sc in 1941, and was awarded an honorary D.Sc in 1982. He received a Ph.D in 1943 from McGill University.\n\nIn 1943, he was recruited by Atomic Energy of Canada Limited to work at the Manhattan Project's Montreal Laboratory, moving to the Chalk River Laboratories, on the banks of the Ottawa River, in Ontario, at the end of the war. He remained with the AECL until 1952.\n\nIn 1952, he moved to Montreal, where the J.S. Foster cyclotron had just been built at McGill University. In 1958 he became the Macdonald Professor of Chemistry.\n\nFrom 1963 to 1965 he was director of research at the International Atomic Energy Agency in Vienna. Returning to McGill he was appointed head of the Department of Chemistry until 1972. In 1974 he was appointed Vice-Principal (administration) which he held until he retired in 1981. From 1981 to 1982, he was the president of the Chemical Institute of Canada.\n\nHe married Betty Workman and has two children: Carla Krasnick, and Mark Yaffe. Yaffe died in Montreal in 1997. The McGill University Archives holds a collection of his personal papers and photographs.\n\n\n\n"}
{"id": "27883107", "url": "https://en.wikipedia.org/wiki?curid=27883107", "title": "List of nuclear power accidents by country", "text": "List of nuclear power accidents by country\n\nWorldwide, many nuclear accidents have occurred since the Chernobyl disaster in 1986. Two thirds of these mishaps occurred in the US. The French Atomic Energy Commission (CEA) has concluded that technical innovation cannot eliminate the risk of human errors in nuclear plant operation.\n\nAn interdisciplinary team from MIT has estimated that given the expected growth of nuclear power from 2005–2055, at least four serious nuclear power accidents would be expected in that period.\n\nThe nuclear power industry has improved the safety and performance of reactors, and has proposed new safer (but generally untested) reactor designs but there is no guarantee that the reactors will be designed, built and operated correctly. Mistakes do occur and the designers of reactors at Fukushima in Japan did not anticipate that a tsunami generated by an earthquake would disable the backup systems that were supposed to stabilize the reactor after the earthquake. According to UBS AG, the Fukushima I nuclear accidents have cast doubt on whether even an advanced economy like Japan can master nuclear safety. Catastrophic scenarios involving terrorist attacks are also conceivable. An interdisciplinary team from MIT has estimated that given the expected growth of nuclear power from 2005–2055, at least four serious nuclear accidents would be expected in that period.\n\nGlobally, there have been at least 99 (civilian and military) recorded nuclear power plant accidents from 1952 to 2009 (defined as incidents that either resulted in the loss of human life or more than US$50,000 of property damage, the amount the US federal government uses to define nuclear energy accidents that must be reported), totaling US$20.5 billion in property damages. Property damage costs include destruction of property, emergency response, environmental remediation, evacuation, lost product, fines, and court claims. Because nuclear power plants are large and complex, accidents on site tend to be relatively expensive.\n\nThe 1979 Three Mile Island accident in Pennsylvania was caused by a series of failures in secondary systems at the reactor, which allowed radioactive steam to escape and resulted in the partial core meltdown of one of two reactors at the site, making it the most significant accident in U.S. history.\n\nThe world's worst nuclear accident has been the 1986 Chernobyl disaster in the Soviet Union, one of two accidents that has been rated as a level 7 (the highest) event on the International Nuclear Event Scale. Note that the Chernobyl disaster may have scored an 8 or 9, if the scale continued. The accident occurred at the Chernobyl Nuclear Power Plant after an unsafe systems test led to a rupture of the reactor vessel and a series of steam explosions that destroyed reactor number four. The radioactivity plume spread to the surrounding city of Pripyat and covered extensive portions of Europe with traces of radioactivity, leaving reindeer in Northern Europe and sheep in portions of England unfit for human consumption. A \"Zone of alienation\" has been formed around the reactor.\n\nAt least 57 accidents have occurred since the Chernobyl disaster, and over 56 nuclear accidents have occurred in the USA. Relatively few accidents have involved fatalities.\n\nNote that not all ratings are final as Cancer and Uncounted/Hidden results may have/will occur.\n\nThis list is incomplete. See also Laka Foundation's list of recent nuclear and radiological incidents in Belgium.\n\n\n"}
{"id": "57701741", "url": "https://en.wikipedia.org/wiki?curid=57701741", "title": "Local Flexibility Markets (electricity)", "text": "Local Flexibility Markets (electricity)\n\nStill in the stage of development, Local Flexibility Markets will enable distributed energy resources (short DER, e.g. storage operators, actors, , , (renewable) power plants) to provide their flexibility in electricity demand or production/feed-in for the system operator or another counterparty at a local level. As there are different purposes for the use of this flexibility (market oriented, system oriented, grid oriented, see \"flexibility triangle\"), there exist a variety of different market designs, comprising different actors and role models. Several local market models aim to efficiently tackle the widespread issue of grid congestion. This category of local markets is described further in this article.\n\nThe already rapid expansion of accelerated in recent years. This is particularly the case in Germany, and even more so in its Northern regions. Nearly 50GW of installed wind capacity generated over a third of Germanys electricity demand in 2017. As an example, the Land of Schleswig-Holstein was able to cover 95% of its electricity demand by wind-generated energy ().\n\nSlow grid expansion causes congestions\n\nFor transportation to the consumer via the electricity grid, these strong amounts of energy require accordingly developed grid capacities. But while the expansion of wind energy happened very fast, mainly due to the EEG-incentives, the expansion of the grid happened much slower, since the regulation behind grid expansion requires extensive bureaucratic efforts. This fact causes a sophisticated problem: in times of strong wind energy generation, the amount of electricity is too high to get properly feed in and distributed through the grid. The limited grid capacities are simply not constructed to transport such high amounts of energy at once, the result is congestions:\n\nSolving congestions through feed-in management\n\nToday, system operators are given only one possible tool to encounter this problem and to secure grid stability: in times of strong wind, certain wind turbines are shut down. This is called Feed-In Management. But stopping wind turbines when their energy output is at its highest, comes at very high cost: both ecologically and economically:\n\nEcologically, because for every curtailed kWh of wind energy, another power plant must be activated to offset the now missing volumes, since they have already been traded in the market. Because the supplementing power plant must be ramped up rather quickly and precise, the first and only choice are ). This practice of balancing energy generation by activating certain power plants on the one hand, and shutting down certain generation capacities on the other is called . \n\nFeed-in management comes at very high economic costs for two reasons: first, the redispatched CCGT must be remunerated. Second, the wind turbine operator or owner must also be remunerated (by ) for every kWh he would otherwise have produced. These costs are not paid directly by the system operator. The system operator is entitled to pass on the costs to the end consumer, meaning that at the end, society pays. Annual costs for feed-in management in Germany were €373m in 2016, €550m in 2017, and are likely to increase up to €5bn until 2025. \n\nNew solution: Local Flexibility Markets \n\nLocal Flexibility Markets provide system operators with a new possibility instead of feed-in management. This new kind of market makes it possible to use local flexibility potential (provided by distributed energy resources (short DER) which can adapt their electricity demand or production, e.g. storage providers, electric vehicles, demand response and sector coupling facilities), meaning an increase/decrease in consumption/production at specific points in the grid. This means that higher electricity volumes can be used (or less volumes produced) locally, hence the amount of energy that needs to be transported over long distances via the grid is reduced. Regional trading of electricity enables the system operator to optimize local electricity flows in the grid. The condition for this is an examination of each DER, where the market operator assesses the physical impact of the DER on the grid, based on the location and the provided flexibility capacity (for example, the impact on the grid of a factory located directly next to a wind park is higher compared to a similar factory with a distance of several kilometers to the park). Regional pricing of electricity creates a financial incentive for flexibility providers to adapt their electricity demand or consumption to the current situation in the grid, which alleviates congestions in a market based manner.\n\nIn recent years, a variety of concepts regarding the roles and actors in local markets were developed. This article highlights the following concept, which refers to a flexibility market used by Transmission and Distribution System Operators ( and ) mainly for the purpose of alleviating grid congestions in a market based manner. It was developed within the EU H2020 project Smartnet. It is operated by an independent and neutral third party.\n\n\n\n\n\nOver the past years, different approaches towards the design of local markets occurred. Their main objective (trade energy locally) is always common, yet there are many different secondary objectives and ways of filling the roles. The following table classifies these different approaches by distinguishing criteria.\n\nThe benefit of a Local Flexibility Market from a system operators point of view is mainly financial. As stated above, the system operator does not come up for the feed-in management costs as he passes them over to the consumers. This situation is secured by actual German law. However, within the next five years, European law is going to change this situation by passing the so called “” Package, a central bill of law (see \"Regulatory Framework)). In the modified, new regulatory framework, system operators will be incentivized to use flexibility and shall avoid measures like feed-in management. Hence, using a Local Flexibility Market to solve congestions will be financially fortunate for a System Operator.\n\nWithin current European electricity markets, very limited options to market flexibility are given to flexibility providers. Furthermore, these options (e.g. the German balancing market “Regelenergiemarkt”) only reward the adoption of consumption/generation in time (for example ramping up a power plant during peak demand times), regardless of geographic location of the provider. Hence, there is no existing possibility for flexibility providers to financially profit from their location, even though the location could often be very advantageous regarding local congestions (for example: if a large factory with flexible energy demand is located near a , it potentially has a positive impact on the grid situation). By enabling flexibility providers to trade flexibility locally, Local Flexibility Markets enable them to participate in alleviating grid congestions and avoid expensive grid extension measures. For adapting their demand or production, they can be remunerated by the system operator in to ways: first, with either a surplus for selling or a discount when buying more energy, second by getting a higher price for their electricity compared to the spot market price.\n\nAs stated above, the annual societal costs for feed-in management measures are likely to increase up to €5bn until 2025. These costs could partly be mitigated by local markets. But Local markets also bring another advantage: by enabling the use of flexibility, they decrease or at least delay the occurrence of additional costs relating to grid extension. Grid extension amounts for €50bn until 2030 only on transmission level (Germany). By mitigating the required grid extension measures, local markets can extensively contribute to decrease the societal costs of the German .\n\nFurthermore, local markets support more accurate prices of electricity: within current market structures, the electricity prices in a country (zone) at a given time are even. The geographic location of a customer do not at all affect the price he pays for its electricity and significant price differences in between a price zone are not possible. This fact completely neglects the reality: due to local grid extension and transport of electricity, there are severe price differences within a country (for example, electricity in Northern Germany is way cheaper (theoretically) than in Southern Germany because it does not have to be transported as far). These price differences are not represented by current markets. local markets can determine and adjust prices according to real occurring costs at each node or location in the grid via transparent, market based procedures. This leads to prices that are more accurate than the current ones. More accurate prices maximize the social and therefore display a macro-economic benefit.\n\nWithin Europe and certain other countries, we face a variety of different regional circumstances regarding facts like installed renewable energy capacities, and flexibility potential, that strongly impact the suitability of local markets. Different regional circumstances require different Local Market Designs, hence there is no unified “one-size-fits -all” approach possible. This creates a need for the development and operation of several market designs, resulting in higher costs for the general public.\n\nThe advantageousness of the point \"macro-economic benefits\" remains subject to further discussion: politically, it makes indeed sense to unify the price for electricity within one country in order to avoid economic discrimination of certain regions. Yet, local markets would do exactly this: by pricing electricity according to local conditions, energy costs more in some regions than it does in others. Regional pricing of energy refers to the so called (currently reality in the US) and is a step away of the current zonal pricing model. This may be seen as a disadvantage of local markets. Nowadays, developments in the direction of a nodal market can be observed in the EU (see Poland).\n\nYet, it can be argued that Local Markets are a way to avoid going nodal, as they combine the advantages of both approaches: when used only for the purpose of alleviating congestions, the volumes that are traded in Local Markets are relatively small (compared to the national consumption), hence an effect for end-customers will not occur. Even though, regional price differences are taken into account and may affect more economically viable decisions: for example, a storage provider can place its system at the best possible location for providing flexibility, given that he can access the necessary information.\n\nAs of now, regulation regarding Local Electricity Markets, or electricity markets in general, is split over a variety of different acts, rather than being centralized. Therefore, to analyze the regulatory framework, different acts must be taken into account. The key role in most Local Flexibility Market concepts plays the system operator. Therefore, the following outlook into the regulatory framework strongly refers to a system operator point of view, as all important regulations derive from rules for system operators. The system operator of an electricity grid is always a monopolist, as there is only one electricity grid in each area. Hence, to ensure proper business activities, system operators are amongst the most strictly regulated companies around the world. This also means that new business activities (like trading in Local Markets would be one), remain subject to legislative changes. As changes in regulation always take their time, system operators are not incentivized currently to use Local Flexibility Markets as a measure for congestion management by now. Current regulation therefore hampers the use of flexibility, as stated in the graphic below:\n\nEntering into force 2020, the (CEP 4)is the most important legislative act regarding the future role of system operators and the whole energy market, as it includes nearly all regulations regarding the EU energy sector. Among many other objectives, CEP pushes certainly in the direction of flexibility use rather than traditional curtailment measures. Due to its high-level nature, it only points out and administrate the general direction of this development, rather than formulating specific detailed regulations. Specific law amendments are later on subject to federal administration. CEP 4 reflects the current developments in the system operators business very well and fosters these even further, especially regarding the employment of smart technology, digitalization, active grids and flexibility.\n\nThe main impacts of CEP 4 on system operator and market side are:\n\n\nOn a national level, system operators in Germany face a variety of different law acts regulating their business activities. The selection below lists the most important ones including the key points of each:\n\n\n\n\n\n\n\nThe conclusion of the complex regulatory environment is the following: currently, System Operators are not incentivized to use Local Markets as a mean of congestion management (this regulatory analysis focused on Germany, but results would be similar in other European countries). Therefore, changes in regulation are required to support this development. Entering into force by 2020, the european Clean Energy Package will bring these required changes to EU countries by obliging its members to adopt regulation in a way that incentivizes flexibility use and penalizes curtailment measures. It will take several more years, but the regulatory framework for flexibility markets is on its way.\n\nProjects funded by the Horizon 2020 initiative:\n\n\nThe most important German public funding initiative in the field of new solutions in the energy sector is the \"SINTEG program\". Under this program, 5 different projects in 5 German regions receive public funding:\n\n\n\n\n\n (electric power)\n"}
{"id": "4888799", "url": "https://en.wikipedia.org/wiki?curid=4888799", "title": "MIT Plasma Science and Fusion Center", "text": "MIT Plasma Science and Fusion Center\n\nThe Plasma Science and Fusion Center (PSFC) at the Massachusetts Institute of Technology (MIT) is known internationally as a leading university research center for the study of plasma and fusion science and technology.\n\nThe PSFC is an interdisciplinary research center, approaching the study of fusion by including the majority of the engineering and science disciplines found at MIT: physics, nuclear science and engineering, mechanical engineering, chemistry, astrophysics, materials science, and more. The PSFC's mission is to identify and understand how cutting-edge advances in science and technology can provide fusion energy “smaller and sooner”. The PSFC hosts a wide variety of relatively small experimental facilities at the Albany Street corridor on the campus of MIT, including plasma devices, powerful superconductor magnets and high-energy accelerators. In parallel, novel measurements are developed for the challenging fusion environment, which are then compared to leading-edge theory and simulation. This research mission is integrated with training and mentoring a new generation of multidisciplinary fusion scientists and engineers.\nThe PSFC is one of the largest producers of plasma physics PhDs in the world. However, the Center is not a degree granting body and instead draws students from MIT academic departments. Since plasma physics and fusion are by nature interdisciplinary subjects, students who perform research at the PSFC satisfy their educational goals by residing at any of several departments associated with the PSFC. Each department has its own admission procedure, requirements, and offers a number of plasma-related courses, many of which are taught at the PSFC by PSFC-affiliated professors.\n\nOriginally founded in 1976 as the Plasma Fusion Center (PFC), the Center consolidated research carried out in MIT's academic departments, the Francis Bitter Magnet Laboratory (FBML), and the Research Lab for Electronics (RLE). Since its founding, the PSFC has provided a home for work on a wide range of related topics covering fusion energy, plasma physics, plasma applications and magnet technology. In 2014, the magnet lab, which specializes in magnetic resonance spectroscopy, formally became part of the PSFC.\n\nThe synergy between ongoing research in plasma physics and the magnet technology developed at the FBML led to construction of the Alcator experiment in the early 1970s – the highest field magnetic confinement device in the world at that time. Success in that program and a growing national program in Fusion Energy provided the initial impetus for the Center’s formation. Founded at the request and with the collaboration of the U.S. Department of Energy, the original grant was for construction and operation of a tokamak reactor Alcator A, the first in a series of small, high-field tokamaks, followed by Alcator C (1978) and Alcator C-Mod (1993). \n\nMIT's most recent tokamak, Alcator C-Mod, ran from 1993 to 2016. In 2016 the project pressure reached 2.05 atmospheres – a 15 percent jump over the previous record of 1.77 atmospheres with a plasma temperature of 35 million degrees C, sustaining fusion for 2 seconds, yielding 600 trillion fusion reactions. The run involved a 5.7 tesla magnetic field. It reached this milestone on its final day of operation.\n\nResearch activities are in four interrelated areas.\n\nMagnetic Fusion Energy\n\nPlasma Science\n\nTechnology and Engineering\n\nMagnetic Resonance\n\n1976 - 1978: Albert Hall (Interim Director)\n\n1978 - 1988: Ronald C. Davidson\n\n1988 - 1995: Ronald R. Parker (also Deputy Director of ITER, beginning 1992)\n\n1992 - 1994: Dieter J. Sigmar (Acting Director)\n\n1994 - 1995: D. Bruce Montgomery (Acting Director)\n\n1995 - 2014: Miklos Porkolab\n\n2014 - present: Dennis G. Whyte (also Head, Department of Nuclear Science and Engineering)\n\nThe PSFC collaborates on projects with institutions around the US and the world.\n\n"}
{"id": "11561532", "url": "https://en.wikipedia.org/wiki?curid=11561532", "title": "Marduk-zakir-šumi I kudurru", "text": "Marduk-zakir-šumi I kudurru\n\nThe Marduk-zakir-shumi I kudurru is a boundary stone (kudurru) of Marduk-zakir-šumi I, a king in the 10th dynasty of Babylon from 855 - 819 BC.\n\nThe kudurru of Marduk-zakir-shumi is a stele-like kudurru, with a front face of cuneiform, and a semi-circular top, (a register area), with the king, people, and iconographic representations of gods.\n\n\n"}
{"id": "2802690", "url": "https://en.wikipedia.org/wiki?curid=2802690", "title": "National Research Universal reactor", "text": "National Research Universal reactor\n\nThe National Research Universal (NRU) reactor was a 135 MW nuclear research reactor built in the Chalk River Laboratories, Ontario, one of Canada’s national science facilities. It was a multipurpose science facility that served three main roles. It generated isotopes used to treat or diagnose over 20 million people in 80 countries every year (and, to a lesser extent, other isotopes used for non-medical purposes). It was the neutron source for the NRC Canadian Neutron Beam Centre: a materials research centre that grew from the Nobel Prize-winning work of Bertram Brockhouse. It was the test bed for Atomic Energy of Canada Limited to develop fuels and materials for the CANDU reactor. At the time of its retirement on March 31, 2018, it was the world's oldest operating nuclear reactor.\n\nThe NRU reactor design was started in 1949. It is fundamentally a Canadian design, significantly advanced from NRX. It was built as the successor to the NRX reactor at the Atomic Energy Project of the National Research Council of Canada at Chalk River Laboratories. The NRX reactor was the world's most intense source of neutrons when it started operation in 1947. It was not known how long a research reactor could be expected to operate, so the management of Chalk River Laboratories began planning the NRU reactor to ensure continuity of the research programs.\n\nNRU started self-sustained operation (or went \"critical\") on November 3, 1957, a decade after the NRX, and was ten times more powerful. It was initially designed as a 200 MW reactor, fueled with natural uranium. NRU was converted to 60 MW with highly-enriched uranium (HEU) fuel in 1964 and converted a third time in 1991 to 135 MW running on low-enriched uranium (LEU) fuel.\n\nOn Saturday, 24 May 1958 the NRU suffered a major accident. A damaged uranium fuel rod caught fire and was torn in two as it was being removed from the core. The fire was extinguished, but a sizeable quantity of radioactive combustion products had contaminated the interior of the reactor building and, to a lesser degree, an area of the surrounding laboratory site. The clean-up and repair took three months. NRU was operating again in August 1958. Care was taken to ensure no one was exposed to dangerous levels of radiation and staff involved in clean-up were monitored over the following decades. A Corporal named Bjarnie Hannibal Paulson who was at the clean up developed unusual skin cancers and received a disability pension.\n\nNRU's calandria, the vessel which contains its nuclear reactions, is made of aluminum, and was replaced in 1971 because of corrosion. The calandria has not been replaced since, although a second replacement is likely needed. An advantage of NRU's design is that it can be taken apart to allow for upgrade and repair.\n\nIn October 1986, the NRU reactor was recognized as a nuclear historic landmark by the American Nuclear Society. Since NRX was decommissioned in 1992, after 45 years of service, there has been no backup for NRU.\n\nIn 1994, Bertram Brockhouse was awarded the Nobel Prize in Physics, for his pioneering work carried out in the NRX and NRU reactors in the 1950s. He gave birth to a scientific technique which is now used around the world.\n\nIn 1996, AECL informed the Canadian Nuclear Safety Commission (then known as the Atomic Energy Control Board) that operation of the NRU reactor would not continue beyond December 31, 2005. It was expected that a replacement facility would be built inside that time. However, no replacement was built and in 2003, AECL advised the CNSC that they intended to continue operation of the NRU reactor beyond December 2005. The operating licence was initially extended to July 31, 2006, and a 63-month licence renewal was obtained in July 2006, allowing operation of the NRU until October 31, 2011.\n\nIn May 2007, the NRU set a new record for the production of medical isotopes.\n\nIn June 2007, a new neutron scattering instrument was opened in NRU. The D3 Neutron Reflectometer is designed for examining surfaces, thin films and interfaces. The technique of Neutron Reflectometry is capable of providing unique information on materials in the nanometre length scale.\n\nOn November 18, 2007, the NRU reactor was shut down for routine maintenance. This shutdown was voluntarily extended when AECL decided to install seismically-qualified emergency power systems (EPS) to two of the reactor's cooling pumps (in addition to the AC and DC backup power systems already in place), as required as part of its August 2006 operating license extension by the Canadian Nuclear Safety Commission (CNSC). This resulted in a worldwide shortage of radioisotopes for medical treatments because AECL had not pre-arranged for an alternate supply. On December 11, 2007, the House of Commons of Canada, acting on what the government described as \"independent expert\" advice, passed emergency legislation authorizing the restarting of the NRU reactor with one of the two seismic connections complete (one pump being sufficient to cool the core), and authorizing the reactor's operation for 120 days without CNSC approval. The legislation, C-38, was passed by the Senate and received Royal Assent on December 12. Prime Minister Stephen Harper accused the \"Liberal-appointed\" CNSC for this shutdown which \"jeopardized the health and safety of tens of thousands of Canadians\". Others viewed the actions and priorities of the Prime Minister and government in terms of protecting the eventual sale of AECL to private investors. The government later announced plans to sell part of AECL in May 2009.\n\nThe NRU reactor was restarted on December 16, 2007.\n\nOn January 29, 2008, the former President of the CNSC, Linda Keen, testified before a Parliamentary Committee that the risk of fuel failure in the NRU reactor was \"1 in 1000 years\", and claimed this to be a thousand times greater risk than the \"international standard\". These claims were refuted by AECL.\n\nOn February 2, 2008 the second seismic connection was complete. This timing was well within the above 120-day window afforded by Bill C-38.\n\nIn mid-May 2009 a heavy water leak at the base of the reactor vessel was detected, prompting a temporary shutdown of the reactor. The leak was estimated to be 5 kg (<5 litres) per hour, a result of corrosion. This was the second heavy water leak since late 2008. The reactor was defuelled and drained of all of its heavy water moderator. No administrative levels of radioactivity were exceeded, during the leak or defuelling, and all leaked water was contained and treated on site.\n\nThe reactor remained shut down until August 2010. The lengthy shutdown was necessary to defuel the reactor, ascertain the full extent of the corrosion to the vessel, and finally to effect the repairs — all with remote and restricted access from a minimum distance of 8 metres due to the residual radioactive fields in the reactor vessel. The 2009 shutdown occurred at a time when only one of the other four worldwide regular medical isotope sourcing reactors was producing, resulting in a worldwide shortage.\n\nOn March 31, 2018, following government direction to shut down operations, NRU was decommissioned.\n\nWith the construction of the earlier NRX reactor, it was possible for the first time to commercially manufacture isotopes that were not commonly found in nature. In the core of an operating reactor there are billions of free neutrons. By inserting a piece of target material into the core, atoms in the target can capture some of those neutrons and become heavier isotopes. Manufacturing medical isotopes was a Canadian medical innovation in the early 1950s.\n\nThe NRU reactor continued the legacy of NRX and until it was decommissioned March 31, 2018 produced more medical isotopes than any facility in the world.\n\nThe core of the NRU reactor was about 3 m wide and 3 m high, which is unusually large for a research reactor. That large volume enabled the bulk production of isotopes. Other research reactors in the world produce isotopes for medical and industrial uses, for example the European High Flux Reactor at Petten in the Netherlands, Maria Reactor in Poland, and the OPAL reactor in Australia which began operation in April 2007.\n\nNRU was originally scheduled to shut down in October 2016. With no stable isotope manufacturer ready to step in until 2018, the Canadian Government allowed the NRU to produce Isotopes until March 2018.\n\nThe NRU reactor is home to Canada's national facility for neutron scattering: the NRC Canadian Neutron Beam Centre. Neutron scattering is a technique where a beam of neutrons shines through a sample of material, and depending on how the neutrons scatter from the atoms inside, scientists can determine many details about the crystal structure and movements of the atoms within the sample.\n\nAn early pioneer of the technique was Bertram Brockhouse who built some of the early neutron spectrometers in the NRX and NRU reactors and was awarded the 1994 Nobel Prize in physics for the development of neutron spectroscopy.\n\nThe NRC Canadian Neutron Beam Centre continues that field of science today, operating as an open-access user facility allowing scientists from across Canada and around the world to use neutrons in their research programs.\n\nIt is common for a developed country to support a national facility for neutron scattering and one for X-ray scattering. The two types of facility provide complementary information about materials.\n\nAn unusual feature of the NRU reactor as Canada's national neutron source is its multipurpose design: able to manufacture isotopes, and support nuclear R&D at the same time as it supplies neutrons to the suite of neutron scattering instruments.\n\nThe NRU reactor is sometimes (incorrectly) characterized as simply a \"nuclear\" research facility. Neutron scattering however is not nuclear research, it is \"materials research\". Neutrons are an ideal probe of materials including metals, alloys, biomaterials, ceramics, magnetic materials, minerals, polymers, composites, glasses, nano-materials and many others. The neutron scattering instruments at the NRC Canadian Neutron Beam Centre are used by universities and industries from across Canada every year because knowledge of materials is important for innovation in many sectors.\n\nThe NRU reactor has test facilities built into its core that can replicate conditions inside a large electricity-producing reactor. NRU itself does not generate steam (or electricity); its cooling water heats up to approximately 55 degrees Celsius. Inside the test facilities though, high temperatures and pressures can be produced. It is essential to test out different materials before they are used in the construction of a nuclear generating station.\n\nThe fundamental knowledge gained from NRU enabled the development of the CANDU reactor, and is the foundation for the Canadian nuclear industry.\n\n\n"}
{"id": "26949901", "url": "https://en.wikipedia.org/wiki?curid=26949901", "title": "PNNM", "text": "PNNM\n\nPNNM is a similar explosive to ANNM, it consists of potassium nitrate and nitromethane in the ratio of 60:40 w/w respectively. It can be detonated with a grade 8 cap and often has aluminium powder added.\n"}
{"id": "33779179", "url": "https://en.wikipedia.org/wiki?curid=33779179", "title": "Paimpol–Bréhat tidal farm", "text": "Paimpol–Bréhat tidal farm\n\nThe Paimpol–Bréhat tidal farm is an 8 MW tidal turbine demonstration farm off Île-de-Bréhat near Paimpol, France. It is developed by Électricité de France (EdF). The project was initiated in 2004 and work began in 2008.\n\nThe tidal farm will consist of four turbines, 2 MW each. Turbines are assembled by DCNS and installed by OpenHydro. The first turbine was installed in August 2011. According to EdF, when completed it will be the world's largest tidal array and the world's first grid-connected tidal energy farm.\n"}
{"id": "20991984", "url": "https://en.wikipedia.org/wiki?curid=20991984", "title": "Remote racking system", "text": "Remote racking system\n\nA remote racking system is a system that allows an operator to operate a racking system from a remote location. It offers a safe alternative to manually racking circuit breakers, which reduces the requirement for service personnel to wear a full-body arc flash hazard suit for protection.\n\nA circuit breaker is an automatically operated electrical switch designed to protect an electrical circuit from damage caused by overload or short circuit.\n\nAn arc flash is a type of electrical explosion that results from a low impedance connection to ground or another voltage phase in an electrical system. By permitting the automatic racking of the circuit breaker from a remote location, the remote racking systems move service personnel outside the arc flash protection boundary, thus reducing the need for a full-body arc flash hazard suit. \n\nA remote switch operator is used to remotely operate various types and styles of circuit breakers and controls. When the remote racking system is used in conjunction with a remote switch operator, the user can also operate, trip, and release the circuit breaker from a safe distance.\n\nThere are several designs of remote racking systems on the market and most include either a wired or wireless remote control. The distance at which these can be used varies by which product is chosen. Also the style and size will also be a factor in choosing a remote racking system as some are larger than others.\n\n"}
{"id": "2645622", "url": "https://en.wikipedia.org/wiki?curid=2645622", "title": "Service drop", "text": "Service drop\n\nIn electric power distribution, a service drop is an overhead electrical line running from a utility pole, to a customer's building or other premises. It is the point where electric utilities provide power to their customers. The customer connection to an underground distribution system is usually called a \"service lateral\". Conductors of a service drop or lateral are usually owned and maintained by the utility company, but some industrial drops are installed and owned by the customer. \n\nAt the customer's premises, the wires usually enter the building through a weatherhead that protects against entry of rain and snow, and drop down through conduit to an electric meter which measures and records the power used for billing purposes, then enters the main service panel. The utility's portion of the system ends, and the customer's wiring begins, at the output socket of the electric meter. The service panel will contain a \"main\" fuse or circuit breaker, which controls all of the electric current entering the building at once, and a number of smaller fuses/breakers, which protect individual branch circuits. There is always provision for all power to be cut off by operating either a single switch or small number of switches (maximum of six in the United States, for example); when circuit breakers are used this is provided by the main circuit breaker.\n\nIn North America , the 120/240 V split phase system is used for residential service drops. A pole-mounted distribution transformer usually provides power for one or two residences. The service drop is made up of two 120 V lines and a neutral line. When these lines are insulated and twisted together, they are referred to as a triplex cable which may contain a supporting messenger cable in the middle of the neutral conductor to provide strength for long spans. The neutral line from the pole is connected to a ground near the service panel; often a conductive rod driven into the earth. The service drop provides the building with two 120 V lines of opposite phase, so 240 V can be obtained by connecting a load between the two 120 V conductors, while 120 V loads are connected between either of the two 120 V lines and the neutral line. 240 V circuits are used for high-demand devices, such as air conditioners, clothes dryers, ovens and boilers, while 120 V circuits are used for lighter loads such as lighting and ordinary small appliance outlets.\n\nIn many European countries and other countries that use European systems, three-phase service drops are often used for domestic residences. The use of three-phase power allows longer service drops to serve multiple residences, which is economical with the higher density of housing in Europe. The service drop consists of three phase wires and a neutral wire which is grounded. Each phase wire provides around 230 V to loads connected between it and the neutral. Each of the phase wires carries 50 Hz alternating current which is 120° out of phase with the other two. Several slightly different voltage standards have been used in the past as well: 220Y380, 230Y400 and 240Y415, with plans for future \"harmonization\" towards 230Y400. In this notation, the first number is the voltage between a phase wire and neutral, and the second number, after the \"Y\", is the line voltage (between any two-phase wires).\n\nOther countries, such as the UK and Republic of Ireland, generally provide a single phase and neutral per house, with every third house on the same phase. In this case the available current is higher. Single-phase supplies with a supplier's fuse of up to 100 A are common, while the 3-phase domestic supplies are generally of lower current rating (often less than 50 A) per phase.\n\nCommercial and industrial service drops can be much bigger, and are usually three phase. In the USA, common services are 120Y/208 (three 120 V circuits 120 degrees out of phase, with 208V line-to-line), 240 V three-phase, and 480 V three-phase. 600 V three-phase is common in Canada, and 380-415 V or 690 V three-phase is found in many other countries. Generally, higher voltages are used for heavy industrial loads, and lower voltages for commercial applications.\n\nIn North America where single-phase distribution transformers for service drops are the norm, three-phase service drops are often constructed using three single-phase transformers, wired in a Y configuration. This is called a \"transformer bank\".\n\nService conductors for a customer may be run underground, from a padmount transformer to a customer's meter. \n"}
{"id": "55324403", "url": "https://en.wikipedia.org/wiki?curid=55324403", "title": "Shedrack Chukwu", "text": "Shedrack Chukwu\n\nShedrack Chukwu Ogbogu is a Riverian politician from Ogba–Egbema–Ndoni. In 2015, Chukwu was appointed as Commissioner of the Ministry of Energy and Natural Resources, a position he held until 2017 when reappointed as Commissioner of the Ministry of Power.\n"}
{"id": "413092", "url": "https://en.wikipedia.org/wiki?curid=413092", "title": "Solar thermal energy", "text": "Solar thermal energy\n\nSolar thermal energy (STE) is a form of energy and a technology for harnessing solar energy to generate thermal energy or electrical energy for use in industry, and in the residential and commercial sectors.\n\nSolar thermal collectors are classified by the United States Energy Information Administration as low-, medium-, or high-temperature collectors. Low-temperature collectors are generally unglazed and used to heat swimming pools or to heat ventilation air. Medium-temperature collectors are also usually flat plates but are used for heating water or air for residential and commercial use. High-temperature collectors concentrate sunlight using mirrors or lenses and are generally used for fulfilling heat requirements up to 300 deg C / 20 bar pressure in industries, and for electric power production. Two categories include Concentrated Solar Thermal (CST) for fulfilling heat requirements in industries, and Concentrated Solar Power (CSP) when the heat collected is used for power generation. CST and CSP are not replaceable in terms of application. The largest facilities are located in the American Mojave Desert of California and Nevada. These plants employ a variety of different technologies. The largest examples include, Ivanpah Solar Power Facility (377 MW), Solar Energy Generating Systems installation (354 MW), and Crescent Dunes (110 MW). Spain is the other major developer of solar thermal power plant. The largest examples include, Solnova Solar Power Station (150 MW), the Andasol solar power station (150 MW), and Extresol Solar Power Station (100 MW).\n\nAugustin Mouchot demonstrated a solar collector with a cooling engine making ice cream at the 1878 Universal Exhibition in Paris. The first installation of solar thermal energy equipment occurred in the Sahara approximately in 1910 by Frank Shuman when a steam engine was run on steam produced by sunlight. Because liquid fuel engines were developed and found more convenient, the Sahara project was abandoned, only to be revisited several decades later.\n\nSystems for utilizing low-temperature solar thermal energy include means for heat collection; usually heat storage, either short-term or interseasonal; and distribution within a structure or a district heating network. In some cases more than one of these functions is inherent to a single feature of the system (e.g. some kinds of solar collectors also store heat). Some systems are passive, others are active (requiring other external energy to function).\n\nHeating is the most obvious application, but solar cooling can be achieved for a building or district cooling network by using a heat-driven absorption or adsorption chiller (heat pump). There is a productive coincidence that the greater the driving heat from insolation, the greater the cooling output. In 1878, Auguste Mouchout pioneered solar cooling by making ice using a solar steam engine attached to a refrigeration device.\n\nIn the United States, heating, ventilation, and air conditioning (HVAC) systems account for over 25% (4.75 EJ) of the energy used in commercial buildings (50% in northern cities) and nearly half (10.1 EJ) of the energy used in residential buildings. Solar heating, cooling, and ventilation technologies can be used to offset a portion of this energy. The most popular solar heating technology for heating buildings is the building integrated transpired solar air collection system which connects to the building's HVAC equipment. According to Solar Energy Industries Association over 500,000 m (5,000,000 square feet) of these panels are in operation in North America as of 2015.\n\nIn Europe, since the mid-1990s about 125 large solar-thermal district heating\nplants have been constructed, each with over 500 m (5400 ft) of solar\ncollectors. The largest are about 10,000 m, with capacities of 7\nMW-thermal and solar heat costs around 4 Eurocents/kWh without subsidies.\n40 of them have nominal capacities of 1 MW-thermal or more. The Solar District Heating program (SDH) has participation from 14 European Nations and the European Commission, and is working toward technical and market development, and holds annual conferences.\n\nGlazed solar collectors are designed primarily for space heating. They recirculate building air through a solar air panel where the air is heated and then directed back into the building. These solar space heating systems require at least two penetrations into the building and only perform when the air in the solar collector is warmer than the building room temperature. Most glazed collectors are used in the residential sector.\nUnglazed solar collectors are primarily used to pre-heat make-up ventilation air in commercial, industrial and institutional buildings with a high ventilation load. They turn building walls or sections of walls into low cost, high performance, unglazed solar collectors. Also called, \"transpired solar panels\" or \"solar wall\", they employ a painted perforated metal solar heat absorber that also serves as the exterior wall surface of the building. Heat transfer to the air takes place on the surface of the absorber, through the metal absorber and behind the absorber. The boundary layer of solar heated air is drawn into a nearby perforation before the heat can escape by convection to the outside air. The heated air is then drawn from behind the absorber plate into the building's ventilation system. \nA Trombe wall is a passive solar heating and ventilation system consisting of an air channel sandwiched between a window and a sun-facing thermal mass. During the ventilation cycle, sunlight stores heat in the thermal mass and warms the air channel causing circulation through vents at the top and bottom of the wall. During the heating cycle the Trombe wall radiates stored heat.\n\nSolar roof ponds are unique solar heating and cooling systems developed by Harold Hay in the 1960s. A basic system consists of a roof-mounted water bladder with a movable insulating cover. This system can control heat exchange between interior and exterior environments by covering and uncovering the bladder between night and day. When heating is a concern the bladder is uncovered during the day allowing sunlight to warm the water bladder and store heat for evening use. When cooling is a concern the covered bladder draws heat from the building's interior during the day and is uncovered at night to radiate heat to the cooler atmosphere. The Skytherm house in Atascadero, California uses a prototype roof pond for heating and cooling.\n\nSolar space heating with solar air heat collectors is more popular in the USA and Canada than heating with solar liquid collectors since most buildings already have a ventilation system for heating and cooling. The two main types of solar air panels are glazed and unglazed.\n\nOf the of solar thermal collectors produced in the United States in 2007, were of the low-temperature variety. Low-temperature collectors are generally installed to heat swimming pools, although they can also be used for space heating. Collectors can use air or water as the medium to transfer the heat to their destination.\n\nInterseasonal storage. Solar heat (or heat from other sources) can be effectively stored between opposing seasons in aquifers, underground geological strata, large specially constructed pits, and large tanks that are insulated and covered with earth.\n\nShort-term storage. Thermal mass materials store solar energy during the day and release this energy during cooler periods. Common thermal mass materials include stone, concrete, and water. The proportion and placement of thermal mass should consider several factors such as climate, daylighting, and shading conditions. When properly incorporated, thermal mass can passively maintain comfortable temperatures while reducing energy consumption.\n\nWorldwide, by 2011 there were about 750 cooling systems with solar-driven heat pumps, and annual market growth was 40 to 70% over the prior seven years. It is a niche market because the economics are challenging, with the annual number of cooling hours a limiting factor. Respectively, the annual cooling hours are roughly 1000 in the Mediterranean, 2500 in Southeast Asia, and only 50 to 200 in Central Europe. However, system construction costs dropped about 50% between 2007 and 2011. The International Energy Agency (IEA) Solar Heating and Cooling program (IEA-SHC) task groups working on further development of the technologies involved.\n\nA solar chimney (or thermal chimney) is a passive solar ventilation system composed of a hollow thermal mass connecting the interior and exterior of a building. As the chimney warms, the air inside is heated causing an updraft that pulls air through the building. These systems have been in use since Roman times and remain common in the Middle East.\n\nSolar process heating systems are designed to provide large quantities of hot water or space heating for nonresidential buildings.\n\nEvaporation ponds are shallow ponds that concentrate dissolved solids through evaporation. The use of evaporation ponds to obtain salt from sea water is one of the oldest applications of solar energy. Modern uses include concentrating brine solutions used in leach mining and removing dissolved solids from waste streams. Altogether, evaporation ponds represent one of the largest commercial applications of solar energy in use today.\n\nUnglazed transpired collectors are perforated sun-facing walls used for preheating ventilation air. Transpired collectors can also be roof mounted for year-round use and can raise the incoming air temperature up to 22 °C and deliver outlet temperatures of 45-60 °C. The short payback period of transpired collectors (3 to 12 years) make them a more cost-effective alternative to glazed collection systems. As of 2015, over 4000 systems with a combined collector area of 500,000 m had been installed worldwide. Representatives include an 860 m collector in Costa Rica used for drying coffee beans and a 1300 m collector in Coimbatore, India used for drying marigolds.\n\nA food processing facility in Modesto, California uses parabolic troughs to produce steam used in the manufacturing process. The 5,000 m collector area is expected to provide 15 TJ per year.\nThese collectors could be used to produce approximately 50% and more of the hot water needed for residential and commercial use in the United States. In the United States, a typical system costs $4000–$6000 retail ($1400 to $2200 wholesale for the materials) and 30% of the system qualifies for a federal tax credit + additional state credit exists in about half of the states. Labor for a simple open loop system in southern climates can take 3–5 hours for the installation and 4–6 hours in Northern areas. Northern system require more collector area and more complex plumbing to protect the collector from freezing. With this incentive, the payback time for a typical household is four to nine years, depending on the state. Similar subsidies exist in parts of Europe. A crew of one solar plumber and two assistants with minimal training can install a system per day. Thermosiphon installation have negligible maintenance costs (costs rise if antifreeze and mains power are used for circulation) and in the US reduces a households' operating costs by $6 per person per month. Solar water heating can reduce CO emissions of a family of four by 1 ton/year (if replacing natural gas) or 3 ton/year (if replacing electricity). Medium-temperature installations can use any of several designs: common designs are pressurized glycol, drain back, batch systems and newer low pressure freeze tolerant systems using polymer pipes containing water with photovoltaic pumping. European and International standards are being reviewed to accommodate innovations in design and operation of medium temperature collectors. Operational innovations include \"permanently wetted collector\" operation. This innovation reduces or even eliminates the occurrence of no-flow high temperature stresses called stagnation which would otherwise reduce the life expectancy of collectors.\n\nSolar thermal energy can be useful for drying wood for construction and wood fuels such as wood chips for combustion. Solar is also used for food products such as fruits, grains, and fish. Crop drying by solar means is environmentally friendly as well as cost effective while improving the quality. The less money it takes to make a product, the less it can be sold for, pleasing both the buyers and the sellers. Technologies in solar drying include ultra low cost pumped transpired plate air collectors based on black fabrics. Solar thermal energy is helpful in the process of drying products such as wood chips and other forms of biomass by raising the temperature while allowing air to pass through and get rid of the moisture.\n\nSolar cookers use sunlight for cooking, drying and pasteurization. Solar cooking offsets fuel costs, reduces demand for fuel or firewood, and improves air quality by reducing or removing a source of smoke.\n\nThe simplest type of solar cooker is the box cooker first built by Horace de Saussure in 1767. A basic box cooker consists of an insulated container with a transparent lid. These cookers can be used effectively with partially overcast skies and will typically reach temperatures of 50–100 °C.\n\nConcentrating solar cookers use reflectors to concentrate solar energy onto a cooking container. The most common reflector geometries are flat plate, disc and parabolic trough type. These designs cook faster and at higher temperatures (up to 350 °C) but require direct light to function properly.\n\nThe Solar Kitchen in Auroville, India uses a unique concentrating technology known as the solar bowl. Contrary to conventional tracking reflector/fixed receiver systems, the solar bowl uses a fixed spherical reflector with a receiver which tracks the focus of light as the Sun moves across the sky. The solar bowl's receiver reaches temperature of 150 °C that is used to produce steam that helps cook 2,000 daily meals.\n\nMany other solar kitchens in India use another unique concentrating technology known as the Scheffler reflector. This technology was first developed by Wolfgang Scheffler in 1986. A Scheffler reflector is a parabolic dish that uses single axis tracking to follow the Sun's daily course. These reflectors have a flexible reflective surface that is able to change its curvature to adjust to seasonal variations in the incident angle of sunlight. Scheffler reflectors have the advantage of having a fixed focal point which improves the ease of cooking and are able to reach temperatures of 450-650 °C. Built in 1999 by the Brahma Kumaris, the world's largest Scheffler reflector system in Abu Road, Rajasthan India is capable of cooking up to 35,000 meals a day. By early 2008, over 2000 large cookers of the Scheffler design had been built worldwide.\n\nSolar stills can be used to make drinking water in areas where clean water is not common. Solar distillation is necessary in these situations to provide people with purified water. Solar energy heats up the water in the still. The water then evaporates and condenses on the bottom of the covering glass.\n\nWhere temperatures below about 95 °C are sufficient, as for space heating, flat-plate collectors of the nonconcentrating type are generally used. Because of the relatively high heat losses through the glazing, flat plate collectors will not reach temperatures much above 200 °C even when the heat transfer fluid is stagnant. Such temperatures are too low for efficient conversion to electricity.\n\nThe efficiency of heat engines increases with the temperature of the heat source. To achieve this in solar thermal energy plants, solar radiation is concentrated by mirrors or lenses to obtain higher temperatures – a technique called Concentrated Solar Power (CSP). The practical effect of high efficiencies is to reduce the plant's collector size and total land use per unit power generated, reducing the environmental impacts of a power plant as well as its expense.\n\nAs the temperature increases, different forms of conversion become practical. Up to 600 °C, steam turbines, standard technology, have an efficiency up to 41%. Above 600 °C, gas turbines can be more efficient. Higher temperatures are problematic because different materials and techniques are needed. One proposal for very high temperatures is to use liquid fluoride salts operating between 700 °C to 800 °C, using multi-stage turbine systems to achieve 50% or more thermal efficiencies. The higher operating temperatures permit the plant to use higher-temperature dry heat exchangers for its thermal exhaust, reducing the plant's water use – critical in the deserts where large solar plants are practical. High temperatures also make heat storage more efficient, because more watt-hours are stored per unit of fluid.\n\nCommercial concentrating solar thermal power (CSP) plants were first developed in the 1980s. The world’s largest solar thermal power plants are now the 370 MW Ivanpah Solar Power Facility, commissioned in 2014, and the 354 MW SEGS CSP installation, both located in the Mojave Desert of California, where several other solar projects have been realized as well.\nWith the exception of the Shams solar power station, built in 2013 near Abu Dhabi, the United Arab Emirates, all other 100 MW or larger CSP plants are either located in the United States or in Spain.\n\nThe principal advantage of CSP is the ability to efficiently add thermal storage, allowing the dispatching of electricity over up to a 24-hour period. Since peak electricity demand typically occurs between about 4 and 8 pm, many CSP power plants use 3 to 5 hours of thermal storage. With current technology, storage of heat is much cheaper and more efficient than storage of electricity. In this way, the CSP plant can produce electricity day and night. If the CSP site has predictable solar radiation, then the CSP plant becomes a reliable power plant. Reliability can further be improved by installing a back-up combustion system. The back-up system can use most of the CSP plant, which decreases the cost of the back-up system.\n\nCSP facilities utilize high electrical conductivity materials, such as copper, in field power cables, grounding networks, and motors for tracking and pumping fluids, as well as in the main generator and high voltage transformers. \"(See: Copper in concentrating solar thermal power facilities.)\"\n\nWith reliability, unused desert, no pollution, and no fuel costs, the obstacles for large deployment for CSP are cost, aesthetics, land use and similar factors for the necessary connecting high tension lines. Although only a small percentage of the desert is necessary to meet global electricity demand, still a large area must be covered with mirrors or lenses to obtain a significant amount of energy. An important way to decrease cost is the use of a simple design.\n\nWhen considering land use impacts associated with the exploration and extraction through to transportation and conversion of fossil fuels, which are used for most of our electrical power, utility-scale solar power compares as one of the most land-efficient energy resources available:\nThe federal government has dedicated nearly 2,000 times more acreage to oil and gas leases than to solar development. In 2010 the Bureau of Land Management approved nine large-scale solar projects, with a total generating capacity of 3,682 megawatts, representing approximately 40,000 acres. In contrast, in 2010, the Bureau of Land Management processed more than 5,200 applications gas and oil leases, and issued 1,308 leases, for a total of 3.2 million acres. Currently, 38.2 million acres of onshore public lands and an additional 36.9 million acres of offshore exploration in the Gulf of Mexico are under lease for oil and gas development, exploration and production.\nDuring the day the sun has different positions. For low concentration systems (and low temperatures) tracking can be avoided (or limited to a few positions per year) if nonimaging optics are used. For higher concentrations, however, if the mirrors or lenses do not move, then the focus of the mirrors or lenses changes (but also in these cases nonimaging optics provides the widest acceptance angles for a given concentration). Therefore, it seems unavoidable that there needs to be a tracking system that follows the position of the sun (for solar photovoltaic a solar tracker is only optional). The tracking system increases the cost and complexity. With this in mind, different designs can be distinguished in how they concentrate the light and track the position of the sun.\n\nParabolic trough power plants use a curved, mirrored trough which reflects the direct solar radiation onto a glass tube containing a fluid (also called a receiver, absorber or collector) running the length of the trough, positioned at the focal point of the reflectors. The trough is parabolic along one axis and linear in the orthogonal axis. For change of the daily position of the sun perpendicular to the receiver, the trough tilts east to west so that the direct radiation remains focused on the receiver. However, seasonal changes in the angle of sunlight parallel to the trough does not require adjustment of the mirrors, since the light is simply concentrated elsewhere on the receiver. Thus the trough design does not require tracking on a second axis. The receiver may be enclosed in a glass vacuum chamber. The vacuum significantly reduces convective heat loss.\n\nA fluid (also called heat transfer fluid) passes through the receiver and becomes very hot. Common fluids are synthetic oil, molten salt and pressurized steam. The fluid containing the heat is transported to a heat engine where about a third of the heat is converted to electricity.\n\nFull-scale parabolic trough systems consist of many such troughs laid out in parallel over a large area of land. Since 1985 a solar thermal system using this principle has been in full operation in California in the United States. It is called the Solar Energy Generating Systems (SEGS) system. Other CSP designs lack this kind of long experience and therefore it can currently be said that the parabolic trough design is the most thoroughly proven CSP technology.\n\nThe SEGS is a collection of nine plants with a total capacity of 354 MW and has been the world's largest solar power plant, both thermal and non-thermal, for many years. A newer plant is Nevada Solar One plant with a capacity of 64 MW. The 150 MW Andasol solar power stations are in Spain with each site having a capacity of 50 MW. Note however, that those plants have heat storage which requires a larger field of solar collectors relative to the size of the steam turbine-generator to store heat and send heat to the steam turbine at the same time. Heat storage enables better utilization of the steam turbine. With day and some nighttime operation of the steam-turbine Andasol 1 at 50 MW peak capacity produces more energy than Nevada Solar One at 64 MW peak capacity, due to the former plant's thermal energy storage system and larger solar field. The 280MW Solana Generating Station came online in Arizona in 2013 with 6 hours of power storage. Hassi R'Mel integrated solar combined cycle power station in Algeria and Martin Next Generation Solar Energy Center both use parabolic troughs in a combined cycle with natural gas.\n\nThe enclosed trough architecture encapsulates the solar thermal system within a greenhouse-like glasshouse. The glasshouse creates a protected environment to withstand the elements that can negatively impact reliability and efficiency of the solar thermal system.\n\nLightweight curved solar-reflecting mirrors are suspended within the glasshouse structure. A single-axis tracking system positions the mirrors to track the sun and focus its light onto a network of stationary steel pipes, also suspended from the glasshouse structure. Steam is generated directly, using oil field-quality water, as water flows from the inlet throughout the length of the pipes, without heat exchangers or intermediate working fluids.\n\nThe steam produced is then fed directly to the field’s existing steam distribution network, where the steam is continuously injected deep into the oil reservoir. Sheltering the mirrors from the wind allows them to achieve higher temperature rates and prevents dust from building up as a result from exposure to humidity. GlassPoint Solar, the company that created the Enclosed Trough design, states its technology can produce heat for EOR for about $5 per million British thermal units in sunny regions, compared to between $10 and $12 for other conventional solar thermal technologies.\n\nGlassPoint’s enclosed trough system has been utilized at the Miraah facility in Oman, and a new project has recently been announced for the company to bring its enclosed trough technology to the South Belridge Oil Field, near Bakersfield, California.\n\nPower towers (also known as 'central tower' power plants or 'heliostat' power plants) capture and focus the sun's thermal energy with thousands of tracking mirrors (called heliostats) in roughly a two square mile field. A tower resides in the center of the heliostat field. The heliostats focus concentrated sunlight on a receiver which sits on top of the tower. Within the receiver the concentrated sunlight heats molten salt to over . The heated molten salt then flows into a thermal storage tank where it is stored, maintaining 98% thermal efficiency, and eventually pumped to a steam generator. The steam drives a standard turbine to generate electricity. This process, also known as the \"Rankine cycle\" is similar to a standard coal-fired power plant, except it is fueled by clean and free solar energy.\n\nThe advantage of this design above the parabolic trough design is the higher temperature. Thermal energy at higher temperatures can be converted to electricity more efficiently and can be more cheaply stored for later use. Furthermore, there is less need to flatten the ground area. In principle a power tower can be built on the side of a hill. Mirrors can be flat and plumbing is concentrated in the tower. The disadvantage is that each mirror must have its own dual-axis control, while in the parabolic trough design single axis tracking can be shared for a large array of mirrors.\n\nA cost/performance comparison between power tower and parabolic trough concentrators was made by the NREL which estimated that by 2020 electricity could be produced from power towers for 5.47 ¢/kWh and for 6.21 ¢/kWh from parabolic troughs. The capacity factor for power towers was estimated to be 72.9% and 56.2% for parabolic troughs. There is some hope that the development of cheap, durable, mass producible heliostat power plant components could bring this cost down.\n\nThe first commercial tower power plant was PS10 in Spain with a capacity of 11 MW, completed in 2007. Since then a number of plants have been proposed, several have been built in a number of countries (Spain, Germany, U.S., Turkey, China, India) but several proposed plants were cancelled as photovoltaic solar prices plummeted. A solar power tower is expected to come online in South Africa in 2014. Ivanpah Solar Power Facility in California generates 392 MW of electricity from three towers, making it the largest solar power tower plant when it came online in late 2013.\n\nCSP-Stirling is known to have the highest efficiency of all solar technologies (around 30%, compared to solar photovoltaic's approximately 15%), and is predicted to be able to produce the cheapest energy among all renewable energy sources in high-scale production and hot areas, semi-deserts, etc. A dish Stirling system uses a large, reflective, parabolic dish (similar in shape to a satellite television dish). It focuses all the sunlight that strikes the dish up onto a single point above the dish, where a receiver captures the heat and transforms it into a useful form. Typically the dish is coupled with a Stirling engine in a Dish-Stirling System, but also sometimes a steam engine is used. These create rotational kinetic energy that can be converted to electricity using an electric generator.\n\nIn 2005 Southern California Edison announced an agreement to purchase solar powered Stirling engines from Stirling Energy Systems over a twenty-year period and in quantities (20,000 units) sufficient to generate 500 megawatts of electricity. In January 2010, Stirling Energy Systems and Tessera Solar commissioned the first demonstration 1.5-megawatt power plant (\"Maricopa Solar\") using Stirling technology in Peoria, Arizona. At the beginning of 2011 Stirling Energy's development arm, Tessera Solar, sold off its two large projects, the 709 MW Imperial project and the 850 MW Calico project to AES Solar and K.Road, respectively. In 2012 the Maricopa plant was bought and dismantled by United Sun Systems. United Sun Systems released a new generation system, based on a V-shaped Stirling engine and a peak production of 33 kW. The new CSP-Stirling technology brings down LCOE to USD 0.02 in utility scale.\n\nAccording to its developer, Rispasso Energy, a Swedish firm, in 2015 its Dish Sterling system being tested in the Kalahari Desert in South Africa showed 34% efficiency.\n\nA linear Fresnel reflector power plant uses a series of long, narrow, shallow-curvature (or even flat) mirrors to focus light onto one or more linear receivers positioned above the mirrors. On top of the receiver a small parabolic mirror can be attached for further focusing the light. These systems aim to offer lower overall costs by sharing a receiver between several mirrors (as compared with trough and dish concepts), while still using the simple line-focus geometry with one axis for tracking. This is similar to the trough design (and different from central towers and dishes with dual-axis). The receiver is stationary and so fluid couplings are not required (as in troughs and dishes). The mirrors also do not need to support the receiver, so they are structurally simpler. When suitable aiming strategies are used (mirrors aimed at different receivers at different times of day), this can allow a denser packing of mirrors on available land area.\n\nRival single axis tracking technologies include the relatively new linear Fresnel reflector (LFR) and compact-LFR (CLFR) technologies. The LFR differs from that of the parabolic trough in that the absorber is fixed in space above the mirror field. Also, the reflector is composed of many low row segments, which focus collectively on an elevated long tower receiver running parallel to the reflector rotational axis.\n\nPrototypes of Fresnel lens concentrators have been produced for the collection of thermal energy by International Automated Systems. No full-scale thermal systems using Fresnel lenses are known to be in operation, although products incorporating Fresnel lenses in conjunction with photovoltaic cells are already available.\n\nMicroCSP is used for community-sized power plants (1 MW to 50 MW), for industrial, agricultural and manufacturing 'process heat' applications, and when large amounts of hot water are needed, such as resort swimming pools, water parks, large laundry facilities, sterilization, distillation and other such uses.\n\nThe enclosed parabolic trough solar thermal system encapsulates the components within an off-the-shelf greenhouse type of glasshouse. The glasshouse protects the components from the elements that can negatively impact system reliability and efficiency. This protection importantly includes nightly glass-roof washing with optimized water-efficient off-the-shelf automated washing systems. Lightweight curved solar-reflecting mirrors are suspended from the ceiling of the glasshouse by wires. A single-axis tracking system positions the mirrors to retrieve the optimal amount of sunlight. The mirrors concentrate the sunlight and focus it on a network of stationary steel pipes, also suspended from the glasshouse structure. Water is pumped through the pipes and boiled to generate steam when intense sun radiation is applied. The steam is available for process heat. Sheltering the mirrors from the wind allows them to achieve higher temperature rates and prevents dust from building up on the mirrors as a result from exposure to humidity.\n\nMore energy is contained in higher frequency light based upon the formula of formula_1, where h is the Planck constant and formula_2 is frequency. Metal collectors down convert higher frequency light by producing a series of Compton shifts into an abundance of lower frequency light. Glass or ceramic coatings with high transmission in the visible and UV and effective absorption in the IR (heat blocking) trap metal absorbed low frequency light from radiation loss. Convection insulation prevents mechanical losses transferred through gas. Once collected as heat, thermos containment efficiency improves significantly with increased size. Unlike Photovoltaic technologies that often degrade under concentrated light, Solar Thermal depends upon light concentration that requires a clear sky to reach suitable temperatures.\n\nHeat in a solar thermal system is guided by five basic principles: heat gain; heat transfer; heat storage; heat transport; and heat insulation. Here, heat is the measure of the amount of thermal energy an object contains and is determined by the temperature, mass and specific heat of the object. Solar thermal power plants use heat exchangers that are designed for constant working conditions, to provide heat exchange. Copper heat exchangers are important in solar thermal heating and cooling systems because of copper’s high thermal conductivity, resistance to atmospheric and water corrosion, sealing and joining by soldering, and mechanical strength. Copper is used both in receivers and in primary circuits (pipes and heat exchangers for water tanks) of solar thermal water systems.\n\nHeat gain is the heat accumulated from the sun in the system. Solar thermal heat is trapped using the greenhouse effect; the greenhouse effect in this case is the ability of a reflective surface to transmit short wave radiation and reflect long wave radiation. Heat and infrared radiation (IR) are produced when short wave radiation light hits the absorber plate, which is then trapped inside the collector. Fluid, usually water, in the absorber tubes collect the trapped heat and transfer it to a heat storage vault.\n\nHeat is transferred either by conduction or convection. When water is heated, kinetic energy is transferred by conduction to water molecules throughout the medium. These molecules spread their thermal energy by conduction and occupy more space than the cold slow moving molecules above them. The distribution of energy from the rising hot water to the sinking cold water contributes to the convection process. Heat is transferred from the absorber plates of the collector in the fluid by conduction. The collector fluid is circulated through the carrier pipes to the heat transfer vault. Inside the vault, heat is transferred throughout the medium through convection.\n\nHeat storage enables solar thermal plants to produce electricity during hours without sunlight. Heat is transferred to a thermal storage medium in an insulated reservoir during hours with sunlight, and is withdrawn for power generation during hours lacking sunlight. Thermal storage mediums will be discussed in a heat storage section. Rate of heat transfer is related to the conductive and convection medium as well as the temperature differences. Bodies with large temperature differences transfer heat faster than bodies with lower temperature differences.\n\nHeat transport refers to the activity in which heat from a solar collector is transported to the heat storage vault. Heat insulation is vital in both heat transport tubing as well as the storage vault. It prevents heat loss, which in turn relates to energy loss, or decrease in the efficiency of the system.\n\nA collection of mature technologies called seasonal thermal energy storage (STES) is capable of storing heat for months at a time, so solar heat collected primarily in Summer can be used for all-year heating. Solar-supplied STES technology has been advanced primarily in Denmark, Germany, and Canada, and applications include individual buildings and district heating networks. Drake Landing Solar Community in Alberta, Canada has a small district system and in 2012 achieved a world record of providing 97% of the community's all-year space heating needs from the sun. STES thermal storage mediums include deep aquifers; native rock surrounding clusters of small-diameter, heat exchanger equipped boreholes; large, shallow, lined pits that are filled with gravel and top-insulated; and large, insulated and buried surface water tanks.\n\nHeat storage allows a solar thermal plant to produce electricity at night and on overcast days. This allows the use of solar power for baseload generation as well as peak power generation, with the potential of displacing both coal- and natural gas-fired power plants. Additionally, the utilization of the generator is higher which reduces cost. Even short term storage can help by smoothing out the \"duck curve\" of rapid change in generation requirements at sunset when a grid includes large amounts of solar capacity.\n\nHeat is transferred to a thermal storage medium in an insulated reservoir during the day, and withdrawn for power generation at night. Thermal storage media include pressurized steam, concrete, a variety of phase change materials, and molten salts such as calcium, sodium and potassium nitrate.\n\nThe PS10 solar power tower stores heat in tanks as pressurized steam at 50 bar and 285 °C. The steam condenses and flashes back to steam, when pressure is lowered. Storage is for one hour. It is suggested that longer storage is possible, but that has not been proven in an existing power plant.\n\nA variety of fluids have been tested to transport the sun's heat, including water, air, oil, and sodium, but Rockwell International selected molten salt as best. Molten salt is used in solar power tower systems because it is liquid at atmospheric pressure, provides a low-cost medium to store thermal energy, its operating temperatures are compatible with today's steam turbines, and it is non-flammable and nontoxic. Molten salt is used in the chemical and metals industries to transport heat, so industry has experience with it.\n\nThe first commercial molten salt mixture was a common form of saltpeter, 60% sodium nitrate and 40% potassium nitrate. Saltpeter melts at 220 °C (430 °F) and is kept liquid at 290 °C (550 °F) in an insulated storage tank. Calcium nitrate can reduce the melting point to 131 °C, permitting more energy to be extracted before the salt freezes. There are now several technical calcium nitrate grades stable at more than 500 °C.\n\nThis solar power system can generate power in cloudy weather or at night using the heat in the tank of hot salt. The tanks are insulated, able to store heat for a week. Tanks that power a 100-megawatt turbine for four hours would be about 9 m (30 ft) tall and 24 m (80 ft) in diameter.\n\nThe Andasol power plant in Spain is the first commercial solar thermal power plant using molten salt for heat storage and nighttime generation. It came on line March 2009. On July 4, 2011, a company in Spain celebrated an historic moment for the solar industry: Torresol’s 19.9 MW concentrating solar power plant became the first ever to generate uninterrupted electricity for 24 hours straight, using a molten salt heat storage.\n\nIn 2016 SolarReserve proposed a 2 GW, $5 billion concentrated solar plant with storage in Nevada.\n\nPhase Change Material (PCMs) offer an alternative solution in energy storage. Using a similar heat transfer infrastructure, PCMs have the potential of providing a more efficient means of storage. PCMs can be either organic or inorganic materials. Advantages of organic PCMs include no corrosives, low or no undercooling, and chemical and thermal stability. Disadvantages include low phase-change enthalpy, low thermal conductivity, and flammability. Inorganics are advantageous with greater phase-change enthalpy, but exhibit disadvantages with undercooling, corrosion, phase separation, and lack of thermal stability. The greater phase-change enthalpy in inorganic PCMs make hydrate salts a strong candidate in the solar energy storage field.\n\nA design which requires water for condensation or cooling may conflict with location of solar thermal plants in desert areas with good solar radiation but limited water resources. The conflict is illustrated by plans of Solar Millennium, a German company, to build a plant in the Amargosa Valley of Nevada which would require 20% of the water available in the area. Some other projected plants by the same and other companies in the Mojave Desert of California may also be affected by difficulty in obtaining adequate and appropriate water rights. California water law currently prohibits use of potable water for cooling.\n\nOther designs require less water. The Ivanpah Solar Power Facility in south-eastern California conserves scarce desert water by using air-cooling to convert the steam back into water. Compared to conventional wet-cooling, this results in a 90% reduction in water usage at the cost of some loss of efficiency. The water is then returned to the boiler in a closed process which is environmentally friendly.\n\nOf all of these technologies the solar dish/Stirling engine has the highest energy efficiency. A single solar dish-Stirling engine installed at Sandia National Laboratories National Solar Thermal Test Facility (NSTTF) produces as much as 25 kW of electricity, with a conversion efficiency of 31.25%.\n\nSolar parabolic trough plants have been built with efficiencies of about 20%. Fresnel reflectors have an efficiency that is slightly lower (but this is compensated by the denser packing).\n\nThe gross conversion efficiencies (taking into account that the solar dishes or troughs occupy only a fraction of the total area of the power plant) are determined by net generating capacity over the solar energy that falls on the total area of the solar plant. The 500-megawatt (MW) SCE/SES plant would extract about 2.75% of the radiation (1 kW/m²; see Solar power for a discussion) that falls on its 4,500 acres (18.2 km²). For the 50 MW AndaSol Power Plant that is being built in Spain (total area of 1,300×1,500 m = 1.95 km²) gross conversion efficiency comes out at 2.6%.\n\nFurthermore, efficiency does not directly relate to cost: on calculating total cost, both efficiency and the cost of construction and maintenance should be taken into account.\n\n\n"}
{"id": "19774226", "url": "https://en.wikipedia.org/wiki?curid=19774226", "title": "Taylor column", "text": "Taylor column\n\nA Taylor column is a fluid dynamics phenomenon that occurs as a result of the Coriolis effect. It was named after Geoffrey Ingram Taylor. Rotating fluids that are perturbed by a solid body tend to form columns parallel to the axis of rotation called Taylor columns.\n\nAn object moving parallel to the axis of rotation in a rotating fluid experiences more drag force than what it would experience in a non rotating fluid. For example, a strongly buoyant ball (such as a pingpong ball) will rise to the surface slower than it would in a non rotating fluid. This is because fluid in the path of the ball that is pushed out of the way tends to circulate back to the point it is shifted away from, due to the Coriolis effect. The faster the rotation rate, the smaller the radius of the inertial circle traveled by the fluid.\n\nIn a non-rotating fluid the fluid parts above the rising ball and closes in underneath it, offering relatively little resistance to the ball. In a rotating fluid, the ball needs to push up a whole column of fluid above it, and it needs to drag a whole column of fluid along beneath it in order to rise to the surface.\n\nA rotating fluid thus displays some degree of rigidity.\n\nTaylor columns were first observed by William Thomson, Lord Kelvin, in 1868. Taylor columns were featured in lecture demonstrations by Kelvin in 1881 and by John Perry in 1890. The phenomenon is explained via the Taylor–Proudman theorem, and it has been investigated by Taylor, Grace, Stewartson, and Maxworthy—among others.\n\nTaylor columns have been rigorously studied. For \"Re\"«1, \"Ek\"«1, \"Ro\"«1, the drag equation for a cylinder of radius, \"a\", the following relation has been found.\n\nformula_1\n\nTo derive this, Moore and Saffman solved the linearised Navier–Stokes equation along in cylindrical coordinates, where some of the vertical and radial components of the viscous term are taken to be small relative to the Coriolis term:\n\nformula_2\n\nformula_3\n\nformula_4\n\nTo solve these equations, we incorporate the volume conservation condition as well:\n\nformula_5\n\nWe use the Ekman compatibility relation for this geometry to restrict the form of the velocity at the disk surface:\n\nformula_6\n\nThe resultant velocity fields can be solved in terms of Bessel functions.\n\nformula_7\n\nformula_8\n\nformula_9\n\nwhereby for \"Ek\"«1 the function \"A(k)\" is given by,\n\nformula_10\n\nIntegrating the equation for the \"v\", we can find the pressure and thus the drag force given by the first equation.\n\n"}
{"id": "28341744", "url": "https://en.wikipedia.org/wiki?curid=28341744", "title": "The Fourth Revolution: Energy", "text": "The Fourth Revolution: Energy\n\nThe Fourth Revolution: Energy, also known as \"Die 4. Revolution – Energy Autonomy\", is a German documentary film about renewable energy by Carl-A. Fechner, released in 2010. It depicts a vision for a global society that obtains 100 % of its energy from renewable sources and the complete reconstruction of the economy that this will require.\n\nProduction took four years and was financed by individuals. Parts of the film were made in 10 different countries, showing existing pioneer projects in different cultures, from those funded by Nobel Peace Prize laureate Muhammed Yunus through micro-credit, to the vision of the Right Livelihood Award laureate Hermann Scheer of Eurosolar, to modern businesses working in the renewable energy sector. The film launched in cinemas in Germany March 18, 2010 and had its U.S. premiere at the San Francisco Green Film Festival in March 2011.\n\nIn the German trailer to the film, the revolution in capitalist ownership of energy resources is stressed; Hermann Scheer says that \"instead of a few owners we will have hundreds of thousands...\" and \"energy supply will be democratized\". The film embodies the philosophy of Hermann Scheer, who died in 2010.\n\n"}
{"id": "289710", "url": "https://en.wikipedia.org/wiki?curid=289710", "title": "Turbidity", "text": "Turbidity\n\nTurbidity is the cloudiness or haziness of a fluid caused by large numbers of individual particles that are generally invisible to the naked eye, similar to smoke in air. The measurement of turbidity is a key test of water quality.\n\nFluids can contain suspended solid matter consisting of particles of many different sizes. While some suspended material will be large enough and heavy enough to settle rapidly to the bottom of the container if a liquid sample is left to stand (the settable solids), very small particles will settle only very slowly or not at all if the sample is regularly agitated or the particles are colloidal. These small solid particles cause the liquid to appear turbid.\n\nTurbidity (or haze) is also applied to transparent solids such as glass or plastic. In plastic production, haze is defined as the percentage of light that is deflected more than 2.5° from the incoming light direction.\n\nTurbidity in open water may be caused by growth of phytoplankton. Human activities that disturb land, such as construction, mining and agriculture, can lead to high sediment levels entering water bodies during rain storms due to storm water runoff. Areas prone to high bank erosion rates as well as urbanized areas also contribute large amounts of turbidity to nearby waters, through stormwater pollution from paved surfaces such as roads, bridges and parking lots. Some industries such as quarrying, mining and coal recovery can generate very high levels of turbidity from colloidal rock particles.\n\nIn drinking water, the higher the turbidity level, the higher the risk that people may develop gastrointestinal diseases. This is especially problematic for immunocompromised people, because contaminants like viruses or bacteria can become attached to the suspended solids. The suspended solids interfere with water disinfection with chlorine because the particles act as shields for the virus and bacteria. Similarly, suspended solids can protect bacteria from ultraviolet (UV) sterilization of water.\n\nIn water bodies such as lakes, rivers and reservoirs, high turbidity levels can reduce the amount of light reaching lower depths, which can inhibit growth of submerged aquatic plants and consequently affect species which are dependent on them, such as fish and shellfish. High turbidity levels can also affect the ability of fish gills to absorb dissolved oxygen. This phenomenon has been regularly observed throughout the Chesapeake Bay in the eastern United States.\n\nFor many mangrove areas, high turbidity is needed in order to support certain species, such as to protect juvenile fish from predators. For most mangroves along the eastern coast of Australia, in particular Moreton Bay, turbidity levels as high as 600 Nephelometric Turbidity Units (NTU) are needed for proper ecosystem health.\n\nThe most widely used measurement unit for turbidity is the Formazin Turbidity Unit (FTU). ISO refers to its units as FNU (Formazin Nephelometric Units). ISO 7027 provides the method in water quality for the determination of turbidity. It is used to determine the concentration of suspended particles in a sample of water by measuring the incident light scattered at right angles from the sample. The scattered light is captured by a photodiode, which produces an electronic signal that is converted to a turbidity. Open source hardware has been developed following the ISO 7027 method to measure turbidity reliably using an Arduino microcontroller and inexpensive LEDs.\n\nThere are several practical ways of checking water quality, the most direct being some measure of attenuation (that is, reduction in strength) of light as it passes through a sample column of water. The alternatively used Jackson Candle method (units: Jackson Turbidity Unit or JTU) is essentially the inverse measure of the length of a column of water needed to completely obscure a candle flame viewed through it. The more water needed (the longer the water column), the clearer the water. Of course water alone produces some attenuation, and any substances dissolved in the water that produce color can attenuate some wavelengths. Modern instruments do not use candles, but this approach of attenuation of a light beam through a column of water should be calibrated and reported in JTUs.\n\nThe propensity of particles to scatter a light beam focused on them is now considered a more meaningful measure of turbidity in water. Turbidity measured this way uses an instrument called a nephelometer with the detector set up to the side of the light beam. More light reaches the detector if there are lots of small particles scattering the source beam than if there are few. The units of turbidity from a calibrated nephelometer are called Nephelometric Turbidity Units (NTU). To some extent, how much light reflects for a given amount of particulates is dependent upon properties of the particles like their shape, color, and reflectivity. For this reason (and the reason that heavier particles settle quickly and do not contribute to a turbidity reading), a correlation between turbidity and total suspended solids (TSS) is somewhat unusual for each location or situation.\n\nTurbidity in lakes, reservoirs, channels, and the ocean can be measured using a Secchi disk. This black and white disk is lowered into the water until it can no longer be seen; the depth (Secchi depth) is then recorded as a measure of the transparency of the water (inversely related to turbidity). The Secchi disk has the advantages of integrating turbidity over depth (where variable turbidity layers are present), being quick and easy to use, and inexpensive. It can provide a rough indication of the depth of the euphotic zone with a 3-fold division of the \"Secchi depth\", however this cannot be used in shallow waters where the disk can still be seen on the bottom.\n\nAn additional device, which may help measuring turbidity in shallow waters is the turbidity tube. The turbidity tube condenses water in a graded tube which allows determination of turbidity based on a contrast disk in its bottom, being analogous to the Secchi disk.\n\nTurbidity in air, which causes solar attenuation, is used as a measure of pollution. To model the attenuation of beam irradiance, several turbidity parameters have been introduced, including the Linke turbidity factor (T).\n\nGovernments have set standards on the allowable turbidity in drinking water. In the United States, systems that use conventional or direct filtration methods turbidity cannot be higher than 1.0 nephelometric turbidity units (NTU) at the plant outlet and all samples for turbidity must be less than or equal to 0.3 NTU for at least 95 percent of the samples in any month. Systems that use filtration other than the conventional or direct filtration must follow state limits, which must include turbidity at no time exceeding 5 NTU. Many drinking water utilities strive to achieve levels as low as 0.1 NTU. The European standards for turbidity state that it must be no more than 4 NTU. The World Health Organization, establishes that the turbidity of drinking water should not be more than 5 NTU, and should ideally be below 1 NTU.\n\nThe U.S. Environmental Protection Agency (EPA) has published water quality criteria for turbidity. These criteria are scientific assessments of the effects of turbidity, which are used by states to develop water quality standards for water bodies. (States may also publish their own criteria.) Some states have promulgated water quality standards for turbidity, including:\n\nPublished analytical test methods for turbidity include:\n\nTurbidity is commonly treated using either a settling or filtration process. Depending on the application, chemical reagents will be dosed into the wastewater stream to increase the effectiveness of the settling or filtration process. Potable water treatment and municipal wastewater plants often remove turbidity with a combination of sand filtration, settling tanks, and clarifiers.\n\nIn-situ water treatment or direct dosing for the treatment of turbidity is common when the affected water bodies are dispersed (i.e. there are numerous water bodies spread out over a geographical area, such as small drinking water reservoirs), when the problem is not consistent (i.e. when there is turbidity in a water body only during and after the wet season) or when a low cost solution is required. In-situ treatment of turbidity involves the addition of a reagent, generally a flocculant, evenly dispensed over the surface of the body of water. The flocs then settle at the bottom of the water body where they remain or are removed when the water body is drained. This method is commonly used at coal mines and coal loading facilities where stormwater collection ponds have seasonal issues with turbidity. A number of companies offer portable treatment systems for in-situ water treatment or direct dosing of reagents.\n\nThere are a number of chemical reagents that are available for treating turbidity. Reagents available for treating turbidity include aluminium sulfate or alum (Al(SO)·nHO), ferric chloride (FeCl), gypsum (CaSO·2HO), poly-aluminium chloride, long chain acrylamide-based polymers and numerous proprietary reagents. The water chemistry must be carefully considered when chemical dosing as some reagents, such as alum, will alter the pH of the water.\n\nThe dosing process must also be considered when using reagents as the flocs may be broken apart by excessive mixing.\n\n\n"}
{"id": "16283950", "url": "https://en.wikipedia.org/wiki?curid=16283950", "title": "Volute spring", "text": "Volute spring\n\nA volute spring is a compression spring in the form of a cone (a volute). Under compression, the coils slide past each other, thus enabling the spring to be compressed to a very short length in comparison with what would be possible with a more conventional helix spring.\n\nThe shape of the initial spring steel (or other material) is a \"V\", with the ends of the V at either end of the wound spring (which forms a distorted cylinder, of wider diameter at the centre), and the bottom point of the V at the centre.\n\nSuch springs can frequently be found as a component of garden secateurs. Short posts anchored in each side of the handles, and inserted into each narrow end of the spring, keep the spring in position.\n"}
{"id": "47300509", "url": "https://en.wikipedia.org/wiki?curid=47300509", "title": "Μ(I) rheology", "text": "Μ(I) rheology\n\nIn granular mechanics, the \"μ(I)\" rheology is one model of the rheology of a granular flow. \n\nThe inertial number of a granular flow is a dimensionless quantity defined as\nformula_1\nwhere the eponymous \"μ\"(\"I\") is a dimensionless function of \"I\". As with Newtonian fluids, the first term -\"pδ\" represents the effect of pressure. The second term represents a shear stress: it acts in the direction of the shear, and its magnitude is equal to the pressure multiplied by a coefficient of friction \"μ\"(\"I\"). This is therefore a generalisation of the standard Coulomb friction model.\n\nOne deficiency of the \"μ(I)\" rheology is that it does not capture the hysteretic properties of a granular material.\n\nThe \"μ(I)\" rheology was developed by Jop \"et al.\" in 2006.\n"}
