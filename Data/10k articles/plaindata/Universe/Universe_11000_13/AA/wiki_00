{"id": "1862275", "url": "https://en.wikipedia.org/wiki?curid=1862275", "title": "Ambipolar diffusion", "text": "Ambipolar diffusion\n\nAmbipolar diffusion is diffusion of positive and negative species with opposite electrical charge due to their interaction via an electric field. In the case of ionic crystals, the fluxes of the diffusing species are coupled, while in a plasma the various species diffuse at the same rate.\n\nIn plasma physics, ambipolar diffusion is closely related to the concept of quasineutrality. In most plasmas, the forces acting on the ions are different from those acting on the electrons, so naively one would expect one species to be transported faster than the other, whether by diffusion or convection or some other process. If such differential transport has a divergence, then it results in a change of the charge density. The latter will in turn create an electric field that can alter the transport of one or both species in such a way that they become equal.\n\nThe simplest example is a plasma localized in an unmagnetized vacuum. (See Inertial confinement fusion.) Both electrons and ions will stream outward with their respective thermal velocity. If the ions are relatively cold, their thermal velocity will be small. The thermal velocity of the electrons will be fast due to their high temperature and low mass: formula_1. As the electrons leave the initial volume, they will leave behind a positive charge density of ions, which will result in an outwardly-directed electric field. This field will act on the electrons to slow them down and on the ions to speed them up. The net result is that both ions and electrons stream outward at the speed of sound, formula_2, which is much smaller than the electron thermal velocity, but usually much larger than the ion thermal velocity.\n\nIn astrophysics, \"ambipolar diffusion\" refers specifically to the decoupling of neutral particles from plasma, for example in the initial stage of star formation. The neutral particles in this case are mostly hydrogen molecules in a cloud that would undergo gravitational collapse if they were not collisionally coupled to the plasma. The plasma is composed of ions (mostly protons) and electrons, which are tied to the interstellar magnetic field and therefore resist collapse. In a molecular cloud where the fractional ionization is very low (one part per million or less), neutral particles only rarely encounter charged particles, and so are not entirely hindered in their collapse (note that now is dynamical collapse, not free fall) into a star.\n\n"}
{"id": "11204315", "url": "https://en.wikipedia.org/wiki?curid=11204315", "title": "Belo Monte Dam", "text": "Belo Monte Dam\n\nThe Belo Monte Dam (\"formerly known as\" Kararaô) is a hydroelectric dam complex currently under construction on the northern part of the Xingu River in the state of Pará, Brazil. The planned installed capacity of the dam complex would be 11,233 megawatts (MW), which would make it the second largest hydroelectric dam complex in Brazil and fourth largest in the world by installed capacity, behind the Three Gorges Dam and the Xiluodu Dam in China and the Brazilian-Paraguayan Itaipu Dam. \nConsidering the oscillations of flow river, guaranteed minimum capacity generation from the Belo Monte Dam would measure 4,571 MW, 39% of its maximum capacity.\n\nBrazil's rapid economic growth over the last decade has provoked a huge demand for new and stable sources of energy, especially to supply its growing industries. In Brazil, hydroelectric power plants produce over 85% of the electrical energy. The Government has decided to construct new hydroelectric dams to guarantee national energy security. \nHowever, there is opposition both within Brazil and among the international community to the project's potential construction regarding its economic viability, the generation efficiency of the dams and in particular its impacts on the region's people and environment. In addition, critics worry that construction of the Belo Monte Dam could make the construction of other dams upstream- which could have greater impacts- more viable.\nPlans for the dam began in 1975 but were soon shelved due to controversy; they were later revitalized in the late 1990s. \nIn the 2000s, the dam was redesigned, but faced renewed controversy and controversial impact assessments were carried out. \nOn 26 August 2010, a contract was signed with Norte Energia to construct the dam once the Brazilian Institute of Environment and Renewable Natural Resources (IBAMA) had issued an installation license. \nA partial installation license was granted on 26 January 2011 and a full license to construct the dam was issued on 1 June 2011. \nThe licensing process and the dam's construction have been mired in federal court battles; the current ruling is that construction is allowed, because the license is based on five different environmental technical reports and in accordance with the RIMA (Environmental Impact Report, EIA-RIMA) study for Belo Monte. \n\nThe first turbines went online on 5 May 2016. \nThe power station is planned to be completed by 2019.\n\nPlans for what would eventually be called the Belo Monte Dam Complex began in 1975 during Brazil's military dictatorship, when Eletronorte contracted the Consórcio Nacional de Engenheiros Consultores (CNEC) to realize a hydrographic study to locate potential sites for a hydroelectric project on the Xingu River. CNEC completed its study in 1979 and identified the possibility of constructing five dams on the Xingu River and one dam on the Iriri River.\n\nOriginal plans for the project based on the 1979 study included two dams close to Belo Monte. These were: Kararaô (called Belo Monte after 1989), Babaquara (called Altamira after 1998) which was the next upstream. Four other dams were planned upstream as well and they include the Ipixuna, Kakraimoro, Iriri and Jarina. The project was part of Eletrobras' \"2010 Plan\" which included 297 dams that were to be constructed in Brazil by 2010. The plan was leaked early and officially released in December 1987 to an antagonistic public. The plan had Belo Monte to be constructed by 2000 and Altamira by 2005. Such a speedy timetable was due to the belief that Brazil's relatively new environmental regulations could not stop large projects. The government offered little transparency to the people who would be affected regarding its plans for the hydroelectric project, provoking indigenous tribes of the region to organize what they called the \"I Encontro das Nações Indígenas do Xingu\" (First Encounter of the Indigenous Nations of the Xingu) or the \"Altamira Gathering\", in 1989. The encounter, symbolized by the indigenous woman leader Tuíra holding her machete against the face of then-engineer José Antonio Muniz Lopes sparked enormous repercussions both in Brazil and internationally over the plans for the six dams. As a result, the five dams above Belo Monte were removed from planning and Kararaô was renamed to Belo Monte at the request of the people of that tribe. Eletronorte also stated they would \"resurvey the fall\", meaning resurvey the dams on the river.\n\nBetween 1989 and 2002, the Belo Monte project was redesigned. The reservoir's surface area was reduced from to by moving the dam further upstream. The main rationale for this was to reduce flooding of the Bacajá Indigenous Area. In 1998, the Babaquara Dam was again placed into planning but under a new name, the Altamira Dam. This surprised local leaders as they felt plans for the dams above Belo Monte were cancelled. Some officials in Brazil were determined to build a dam on a river with an average flow of and at a site that offers a drop. One engineer said of the dam: \"God only makes a place like Belo Monte once in a while. This place was made for a dam.\" President of Eletronorte, José Muniz Lopes, in an interview with the newspaper O Liberal (Belo Monte entusiasma an Eletronorte por Sônia Zaghetto, 15 July 2001), affirmed: \"Within the electric sector's planning for the period 2010/2020, we’re looking at three dams – Marabá (Tocantins river), Altamira (previously called Babaquara, Xingu River) and Itaituba (São Luís do Tapajós). Some journalists say that we are not talking about these dams because we’re trying to hide them. It’s just that their time has not yet come. We’re now asking for authorization to intensify our studies for these dams. Brazil would be greatly benefited if we could follow Belo Monte with Marabá, then Altamira and Itaituba.\"\n\nIn 2002, Eletronorte presented a new environmental impact assessment for the Belo Monte Dam Complex, which presented three alternatives. Alternative A included the six original dams planned in 1975. Alternative B included a reduction to four dams, dropping Jarina and Iriri. Alternative C included a reduction to Belo Monte only. The new environmental impact assessment contained reductions in reservoir size and the introduction of a run-of-the-river model, in contrast to the large reservoirs characteristic of the 1975 plans.\n\nAlso in 2002, Workers' Party leader Luiz Inácio Lula da Silva earned a victory in his campaign for president, having originally run unsuccessful campaigns in 1989, 1994 and 1998. Lula soon brokered political deals with the center and right-wing sectors in 2003, especially with ex-president José Sarney of the state of Maranhão of the PMDB, which would set the precedent that eventually characterized the two Lula administrations: cooperation between the market and the state, a combination of a free market economy with larger social spending and welfare. This economic model provided the rationale and financial support for new efforts to construct Belo Monte. In 2007, at the beginning of Lula's second term in office, a new national investment program was introduced: the Programa de Aceleração do Crescimento (Program to Accelerate Growth). The Belo Monte Dam Complex figured as an anchor project of the new investment plan.\n\nIn 2008, another new environmental impact assessment was written, this time by Eletrobras with the participation of Odebrecht, Camargo Corrêa, and Andrade Gutierrez, which formally accepted Alternative C or the construction only of the Belo Monte dam itself. The assessment also presented further design changes; in order to avoid inundating indigenous territory, which is not permitted by the Brazilian Constitution, the new design included two canals to divert the water away from indigenous territories and into a reservoir called the \"Reservatorio dos Canais\" (Canals Reservoir). An additional reservoir would be created called the \"Reservatorio da Calha do Xingu\" (Xingu Riverbed Reservoir), and electricity would be generated from the two reservoirs using three dams: a complementary powerhouse called Pimental (233 MW), a complementary spillway called Bela Vista, and the main powerhouse called Belo Monte (11,000 MW). The \"Reservatorio dos Canais\" would be retained by over a dozen large dikes, and water from the reservoirs would be channeled towards the main powerhouse.\n\nHowever, transparency of the government's plans once again became an issue, sparking indigenous tribes of the region to organize another large meeting, called the \"Segundo Encontro dos Povos do Xingu\" (the Second Encounter of the Peoples of the Xingu) in the city of Altamira, Pará on 20 May 2008.\n\nIn February 2010, Brazilian environmental agency IBAMA granted a provisional environmental license, one of three licenses required by Brazilian legislation for development projects. The provisional license approved the 2008 environmental impact assessment and permitted the project auction to take place in April 2010.\n\nIn April 2010, Odebrecht, Camargo Corrêa, and CPFL Energia dropped out of the project tender, arguing that the artificially low price of the auction (R$83/US$47) set by the government was not viable for economic returns on investment. On 20 April 2010, the Norte Energia consortium won the project auction by bidding at R$77.97/MWh, almost 6% below the price ceiling of R$83/MWh. After the auction, local leaders around the project site warned of imminent violence. Kayapó leader Raoni Metuktire stated: \"There will be a war so the white man cannot interfere in our lands again.\" U.S. film director James Cameron also visited the site prior to the auction and stated he would produce an anti-Belo Monte Dam film called \"Message From Pandora\" which was later released in November.\nIn April 2010 the Brazilian Federal Attorney General's Office suspended the project tender and annulled the provisional environmental license on claims of unconstitutionality. Specifically, Article 176 of the Federal Constitution states that federal law must determine the conditions of mineral and hydroelectric extraction when these activities take place in indigenous peoples' territories, as is the case for the \"Big Bend\" (Volta Grande) region. As a result, the electric utility ANEEL canceled the project auction. The same day, the appellate court for Region 1 disenfranchised the Attorney General's suspension, reinstating the project auction at ANEEL.\n\nOn 26 August 2010, President Luiz Inacio Lula da Silva signed the contract with the Norte Energia at a ceremony in Brasília. Construction was not permitted to begin on the Belo Monte Dam Complex until IBAMA granted the second of the federally required environmental licenses, called the Installation License. The Installation License was only to be granted once Norte Energia shows indisputable proof that it has met 40 socio-environmental mitigation conditions upon which the first provisional environmental license was conditioned. According to an October 2010 IBAMA report, at least 23 conditions had not been met. Reports indicate that on 14 January 2011, a report from staff members of FUNAI, Fundação Nacional do Índio, (National Indian Foundation) had sent a report to IBAMA expressing concerns about the location of the project, its impact on reservation land, and the lack of attention to needs of the indigenous people, especially the Paquiçamba and recommending that FUNAI oppose any license to operate. Despite this report, FUNAI senior management sent IBAMA a letter on 21 January 2011 stating that it did not oppose the issuance of a limited construction license.\n\nOn 26 January 2011, a partial installation license was granted by IBAMA, authorizing Norte Energia to begin initial construction activities only including forest clearing, the construction of easement areas, and improvement of existing roads for the transport of equipment and machinery. In February 2011, Norte Energía signed contracts with multiple suppliers for the design, production, installation and commissioning of generation and associated equipment. On 1 June 2011, IBAMA granted the full license to construct the dam after studies were carried out and the consortium agreed to pay $1.9 billion in costs to address social and environmental problems. The only remaining license is one to operate the dam's power plant.\n\nOn 25 February 2011, the Federal Public Prosecutor filed its 11th lawsuit against Belo Monte Dam, suspending IBAMA's partial installation license, on the grounds that the Brazilian Constitution does not allow for the granting of partial project licenses. The Federal Public Prosecutor also argued that the 40 social and environmental conditions tied to IBAMA's provisional license of February 2010 had yet to be fulfilled, a prerequisite to the granting of a full installation license. On 25 February 2011, Brazilian federal judge Ronaldo Destêrro blocked the project citing environmental concerns. It was Brazil's biggest public hearing ever. The ruling was described by \"The Guardian\" as \"a serious setback\". President of a federal regional court Olindo Menezes overturned the decision on 3 March 2011 saying there was no need for all conditions to be met in order for preliminary work to begin. Construction site preparation began with a week after the decision. On 28 September though, due to concerns for local fishers, a federal judge prohibited Norte Energia from \"building a port, using explosives, installing dikes, building canals and any other infrastructure work that would interfere with the natural flow of the Xingu river, thereby affecting local fish stocks\". On 9 November, construction was allowed to recommence after federal judge Maria do Carmo Cardoso ruled that indigenous people did not have to be consulted by law before work approval. The ruling is expected to be appealed in the Supreme Federal Court.\n\nOn 25 April 2012, a regional judge ruled an upcoming employee strike to be illegal. Workers were seeking improved payments and additional time-off. However, according to the court the contractor company did not violate its terms and as such, its employees will be fined for R$200,000 (US$106,000) per day if they do not attend.\n\nOn 14 August 2012, work on the dam was halted by order of the Brazilian Federal Court, when federal judge Souza Prudente, halted construction on the controversial Belo Monte dam in the Amazon, saying that the indigenous peoples had not been consulted. The Supreme Federal Court overturned the decision on 28 August and ordered construction to recommence.\n\nA federal court overturned a lower court ruling that had suspended the dam's operating license in January 2016. The lower court suspension was due to allegations of human rights violations.\n\nThe Belo Monte Dam (AHE Belo Monte) is a complex of three dams, numerous dykes and a series of canals in order to supply two different power stations with water. The Pimental Dam () on the Xingu would be tall and long and have a structural volume of . It would create the Calha Do Xingu Reservoir which would have a normal capacity of and surface area of . The dam would support a power station and its spillway would serve as the complex's principal spillway with 17 floodgates and a maximum discharge. The dam's reservoir would also divert water into two long canals. These canals would supply water to the Dos Canais Reservoir, which is created within the \"Big Bend\" by the main dam, Belo Monte (), a series of 28 dykes around the reservoir's perimeter and the Bela Vista Dam () which lies on the Dos Canais Reservoir's eastern perimeter. The Belo Monte Dam would support the main power station in the complex. The power station would contain twenty vertical Francis turbines listed at 550 MW (max 560 MW). Supplying each turbine with water is a , diameter penstock, affording an average of of hydraulic head. The Pimental Dam's power station would contain seven Kaplan bulb turbines, each rated at 25.9 MW and with of hydraulic head.\n\nThe Belo Monte Dam would be tall and long and have a structural volume (embankments included) of while the Bella Vista would be high and long and have a structural volume of . The Dos Canais Reservoir would have a normal capacity of , a normal surface area of and a normal elevation area of above sea level. The Bela Vista Dam which serves as the complex's secondary spillway would have a maximum discharge capacity of .\n\nThe planned capacity of Belo Monte is listed at 11,233 MW. It is composed of the main Belo Monte Dam, and its turbine house with an installed capacity of 11,000 MW. The Pimental Dam which also includes a turbine house would have an installed capacity of 233.1 MW, containing 25.9 MW bulb turbines. The generation facility is planned to have twenty Francis turbines with a capacity of 550–611 MW each.\n\nIn February 2011, Norte Energía signed contracts with:\n\nWalter Coronado Antunes, the former Secretary of the Environment of the state of São Paulo, and ex-President of the state water and sanitation utility Sabesp, has said that the Belo Monte Dam Complex would be one of the least efficient hydro-power projects in the history of Brazil, producing only 10% of its 11,233 MW nameplate capacity between July and October (1,123 MW, and an average of only 4,419 MW throughout the year, or a 39% capacity factor). According to the president of Brazil's Energy Research Company (EPE), 39% is \"just a little below\" Brazil's average of 55%. Normally, the capacity factor of hydroelectric power plants is between 30% and 80%, while wind power is typically between 20% and 40%. According to a study by Eletrobras, even when at reduced capacities, Belo Monte would still have the capacity to supply the entire state of Para with electricity.\n\nCritics claim that the project would only make financial sense if the Brazilian government builds additional dam reservoirs upstream to guarantee a year-round flow of water, thus increasing the availability of generation. Supporters of the project point out that the seasonal minimum flow of the Xingu river occurs at a time when other Brazilian hydro plants are well supplied, so that no additional dams would have to be built. Reportedly, Brazil's National Council for Power Policies approved a resolution, previously sanctioned by then president Lula, that only one hydroelectric dam would be built on the Xingu. With one dam, critics don't see an advantage regarding the dam's cost-to-benefit ratio and question why the government would just want to construct one.\n\nAdditional upstream dams would directly and indirectly affect 25,000 indigenous peoples in the entire Xingu basin. Of particular note is the Altamira (Babaquara) Dam, which would flood an additional of reservoir, according to its original design.\n\nThe project is developed by Norte Energia. The consortium is controlled by the state-owned power company Eletrobras, which directly (15%) and through its subsidiaries Eletronorte (19.98%) and CHESF (15%) controls a 49.98% stake in the consortium.\n\nIn July 2010, the federal holding company Eletrobras stated that there were 18 partners and reported their adjusted share in the project:\nThe Norte Energia consortium construction companies were reported to have originally held a 40% share.\n\nIn April 2012 it was announced that a $146 million contract was signed between Norte Energia S.A. and a consortium consisting of ARCADIS logos (a subsidiary of ARCADIS) holding a 35% share, and Themag, Concremat, and ENGECORPS, who will provide their engineering services to the project.\n\nThe dam complex is expected to cost upwards of $16 billion and the transmission lines $2.5 billion. The project is being developed by the state-owned power company Eletronorte, and would be funded largely by the Brazilian Development Bank (BNDES). The project will also include substantial amounts of funding from Brazilian pension funds Petros, Previ, and Funcef. Private investors interested in the project include mining giants Alcoa and Vale, construction conglomerates Andrade Gutierrez, Votorantim, Grupo OAS, Queiroz Galvão, Odebrecht and Camargo Corrêa, and energy companies GDF Suez and Neoenergia.\n\nIn 2006, Conservation Strategy Fund (CSF) analyzed different cost-benefit scenarios for Belo Monte as an energy project, excluding environmental costs. Initial benefits appeared marginal. When simulating energy benefits using a modeling system it became obvious that Belo Monte would require additional upstream dams to provide water storage for dry season generation. CSF concluded that Belo Monte would not be sustainable without the proposed Altamira (Babaquara) dam which would have a reservoir more than 10 times the size of Belo Monte's, flood 30 times the area submerged by Belo Monte, indigenous territories of the Araweté/Igarapé Ipixuna, Koatinemo, Arara, Kararaô, and Cachoeira Seca do Irirí natives.\n\nDue to the project's lack of economic viability and lack of interest from private investors, the government has had to rely on pension funds and lines of credit from BNDES that draw from the , oriented towards paying the public debt, to finance the project; up to one-third of the project's official cost would be financed by incentives using public money.\n\nWWF-Brazil released a report in 2007 stating that Brazil could cut its expected demand for electricity by 40% by 2020 by investing in energy efficiency. The power saved would be equivalent to 14 Belo Monte hydroelectric plants and would result in national electricity savings of up to R$33 billion (US$19 billion).\n\nEx-director of ANEEL Afonso Henriques Moreira Santos stated that large dams such as Belo Monte were not necessary to meet the government's goal of 6% growth per year. Rather, he argued that Brazil could grow through increasing its installed capacity in wind power, currently only at 400 MW.\n\nHowever, a study by the Federal University of Rio de Janeiro, published in June 2011, criticised some of these alternative suggestions and defended the Belo Monte dam project. They state that compared to the estimated costs of alternative energies, the Belo Monte dam is cheaper both in economic and in socio-environmental costs.\n\nThe project is strongly criticized by indigenous people and numerous environmental organizations in Brazil plus organizations and individuals around the world.\n\nBelo Monte's of reservoir will flood of forest, about 0.01% of the Amazon forest. Though argued to be a relatively small area for a dam's energy output, this output cannot be fully obtained without the construction of other dams planned within the dam complex. The expected area of reservoir for the Belo Monte dam and the necessary Altamira dam together will exceed 6500 km2 of rainforest. \n\nThe environmental impact assessment written by Eletrobras, Odebrecht, Camargo Corrêa, and Andrade Gutierrez listed the following possible adverse effects\n\n\nIn February 2010, Brazilian environmental agency IBAMA granted an environmental license for the construction of the dam despite uproar from within the agency about incomplete information in the Environmental Impact Assessment (EIA) written by Eletrobras, Odebrecht, Camargo Corrêa, and Andrade Gutierrez. Previously in October 2009, a panel composed of independent experts and specialists from Brazilian universities and research institutes issued a report on the EIA, finding \"various omissions and methodological inconsistencies in the EIA...\" Among the problems cited within the EIA were the project's uncertain cost, deforestation, generation capacity, greenhouse gas emissions and in particular the omission of consideration for those affected by the river being mostly diverted in the long \"Big Bend\" (Volta Grande).\n\nTwo senior officials at IBAMA, Leozildo Tabajara da Silva Benjamin and Sebastião Custódio Pires, resigned their posts in 2009 citing high-level political pressure to approve the project. In January 2011, IBAMA president Abelardo Azevedo also resigned his post. The previous president Roberto Messias had also stepped down, citing in April 2010 that it was because of pressure from both the government and environmental organizations.\n\n140 organizations and movements from Brazil and across the globe decried the decision-making process in granting the environmental license for the dams in a letter to Brazilian President Luiz Inacio Lula da Silva in 2010.\n\nThe fish fauna of the Xingu river is extremely rich with an estimated 600 fish species and with a high degree of endemism. The area either dried out or drowned by the dam spans the entire known world distribution of a number of species, e.g. the zebra pleco (\"Hypancistrus zebra\"), the sunshine pleco (\"Scobinancistrus aureatus\"), the slender dwarf pike cichlid (\"Teleocichla centisquama\"), the killifish \"Anablepsoides xinguensis\" and \"Spectrolebias reticulatus\", and the Xingu dart-poison frog (\"Allobates crombiei\"). An independent expert review of the costs of the dam concluded that the proposed flow through the Volta Grande meant the river \"will not be capable of maintaining species diversity\", risking \"extinction of hundreds of species\".\n\nThe National Amazon Research Institute (INPA) calculated that during its first 10 years, the Belo Monte-Babaquara dam complex would emit 11.2 million metric tons of Carbon dioxide equivalent, and an additional 0.783 million metric tons of equivalent would be generated during construction and connection to the national energy grid. This independent study estimates greenhouse gas emissions of an amount that would require 41 years of optimal energy production from the Belo Monte Dam complex (including the now aborted Altamira Dam) in order to reach environmental sustainability over fossil fuel energy.\n\nDams in Brazil emit high amounts of methane, due to the lush jungle covered by waters each year as the basin fills. Carbon is trapped by foliage, which then decays anaerobically with help from methanogens, converting the carbon to methane, which is a more potent greenhouse gas than carbon dioxide. As a result, carbon emissions are emitted from the dam each year it is in operation. A 1990 study of the Curuá-Una Dam, also in Brazil, found that it pollutes 3.5 times more in carbon dioxide equivalent than an oil power plant generating an equal amount of electricity would; not in the form of the CO2 atmospheric pollution associated with fossil fuel burning, but as the more dangerous methane emissions. Furthermore, the forest will be cleared before flooding of the area, so the CO2 and methane emissions calculated for the flooding of the forested area will be significantly undercut. In addition, a study on the Brazilian Tucuruí dam showed that the actual greenhouse gas emissions were a factor ten higher than its official calculations showed, and this dam is no exception; it is feared that the Belo Monte Dam calculations are also deliberately undercutting reality and that the flooding of its reservoir will create a similar situation. If the dam builders cleared the forest beforehand, they would remove the organic matter from the reservoir floor and the dam would produce less of the greenhouse gas methane. However, in the case of Tucuruí, only the economically necessary forest was cut (10%, near the opening to the spillway) and the rest was left intact to be flooded by the reservoir. The contractors had sold the logging rights to the flooded area, but found the plot unviable in the short amount of time they had allocated before the area was set to be flooded. This forest has been decaying under the water through Methanogenesis and producing large amounts of greenhouse gases.\n\nOn the other hand, the energy generated by the dam for the next 50 years, at an average of 4419 MW, is 1.14 bboe (billion barrels of oil equivalent). This is approximately 9% of the proven oil reserves of Brazil (12.6 bbl), or 2% of the total oil reserves of Russia (60 bbl), or 5.5% of the proven oil reserves of the U.S. (21 bbl).\n\nAlthough strongly criticized by indigenous leaders, the president of Brazil's EPE claims they have popular support for the dam. On 20 April 2010 \"Folha de Sao Paulo\" poll showed 52% in favor of the dam. The dam will directly displace over 20,000 people, mainly from the municipalities of Altamira and Vitória do Xingu. Two river diversion canals wide by long will be excavated. The canals would divert water from the main dam to the power plant. Belo Monte will flood a total area of . Of the total, of flooded area will be forested land. The river diversion canals will reduce river flow by 80% in the area known as the Volta Grande (\"Big Bend\"), where the territories of the indigenous Juruna and Arara people, as well as those of sixteen other ethnic groups are located. While these tribes will not be directly impacted by reservoir flooding, and therefore will not be relocated, they may suffer involuntary displacement, as the river diversion negatively affects their fisheries, groundwater, ability to transport on the river and stagnant pools of water offer an environment for water-borne diseases, an issue that is criticized for not being addressed in the Environmental Impact Assessment.\n\nAmong the 20,000 to be directly displaced by reservoir flooding, resettlement programs have been identified by the government as necessary for mitigation. Norte Energia have failed to obtain free, prior, and informed consent from the Juruna and Arara indigenous tribes to be impacted by Belo Monte. The project would also attract an estimated 100,000 migrants to the area. An estimated 18,700 direct jobs would be created, with an additional 25,000 indirect jobs to accommodate the surge in population. However, only a fraction of the direct jobs will stay available after the project's completion, which critics have argued to spell economic disaster rather than economic prosperity.\n\nThe influx of immigrants and construction workers has also led to increased social tension between groups. Indigenous Groups report attacks and harassment, and in several occasions the destruction of property and the death of indigenous persons as a result from constructing and (illegal) logging activities. External researchers indicate that the majority of the Belo Monte dam's energy output will be relegated towards the aluminium industry, and will not benefit the people living in the area. However, Norte Energia released a clarification note stating their concern with the socioeconomic development of the area, including the promise to invest R$3.700 billion (1,300 million GBP) into various issues.\n\nThe IBAMA's environmental impact assessment has listed the following possible impacts:\n\n\nHowever, a clarification was released by the Brazilian authorities, in which it was deemed that the assured social and economic benefits, considered for the environmental redesign and the region's infrastructural developments, would outweigh the expected environmental damage. Since the beginning of the project many environmental and human rights organizations have been protesting against the construction of the Belo Monte Dam. On the 14th of August 2012 the Brazil Federal Court halted the construction of the Belo Monte Dam on the basis that the government's authorization of the dam was unconstitutional. The government didn't hold constitutionally required meetings with indigenous communities affected by the dam before granting permission in 2005 to start with the construction. This is against the Brazilian law and international human rights. However, Norte Energía, the company assigned with the construction of the Belo Monte Dam, has the possibility of an appeal to the Supreme Court.\n\nThe attitude and treatment of the Brazilian government towards the affected indigenous groups is strongly criticised internationally. The UN Human Rights Council has published statements denouncing Brazil's careless construction methods, and the International Labour Organization (ILO) likewise pointed out that the Brazilian state was in violation of ILO conventions (particularly the Indigenous and Tribal Peoples Convention, 1989 No 169) – although mechanisms for international enforcement are lacking, and it would require a Brazilian court to apply the binding principles of the convention, which Brazil has ratified. Indigenous groups have questioned the government's actions over these events, but their situation remains ignored by the authorities, as shown with the May 2011 Xingu Mission report of the CDDPH (Conselho de Defesa dos Direitos da Pessoa Humana), of which several sections regarding accusations of human right violations were excluded by the Special Secretary for Human Rights, Maria do Rosário Nunes.\n\n\n\n"}
{"id": "24507147", "url": "https://en.wikipedia.org/wiki?curid=24507147", "title": "Blowing Wild", "text": "Blowing Wild\n\nBlowing Wild is a 1953 American drama film directed by Hugo Fregonese and starring Gary Cooper, Barbara Stanwyck and Anthony Quinn. It was written by Philip Yordan. The story revolves around a love triangle set in Mexico's oilfields, where bandits are still active. Ruth Roman also stars and adds to the romantic entanglements.\n\nFrankie Laine sang the title song, \"Blowing Wild (The Ballad of Black Gold)\", which was written by Dmitri Tiomkin, with lyrics by Paul Francis Webster.\n\nAfter the bandit El Gavilan and his men blow up their South American oil rig, broke wildcatters Jeff Dawson and \"Dutch\" Peterson head back to town, looking for work. Sal Donnelly, an American down on her luck, tries to use her charms to get Jeff to buy her a ticket to get home; Jeff offers his oil lease as payment, but the ticket taker shows him a fistful of leases he already has.\n\nJeff accepts a very dangerous job delivering unstable nitroglycerin the next day for $800, despite Dutch's protests. That night, Dutch tries to mug a man for enough money to buy a meal. The man turns out to be \"Paco\" Conway, an old friend and former partner of Jeff and Dutch, who has struck it rich. He offers them work, but his marriage to Jeff's old flame Marina makes Jeff turn it down. The next day, Jeff and Dutch (and the nitroglycerin) are ambushed by El Gavilan. They get away, though Dutch is shot in the leg.\n\nWhen Jeff goes to collect their pay, Jackson claims he does not that much on him. Sal, whom Jackson is romancing, tells Jeff that Jackson has $2500 in his wallet. Jeff gets his money, after a brawl, and gives $200 to Sal for her ticket. However, a policeman confiscates Jeff's $600, as Jackson has other creditors, though he is gracious enough to leave Sal her money. With Dutch in the hospital, Jeff reluctantly goes to work for Paco, drilling a new oil well.\n\nMarina makes romantic overtures to Jeff, but he avoids her as best he can. He reminds her that he loved her once, but could not trust her. She admits it, but says she realized she loved him too after he had left. Paco remains oblivious to what is going on. To Jeff's initial annoyance, Sal gets a job as a blackjack dealer and sticks around. Later though, he starts going into town to see her.\n\nWhen El Gavilan threatens to blow up Paco's oil wells unless he pays $50,000 extortion money, Paco considers paying, much to Jeff's disgust. Marina sides with Jeff, calling her husband a coward. A drunken Paco later laments publicly that his wife loves another man. He finally realizes the other man is Jeff. When Paco tells her that he loves her regardless, Marina pushes him into a well, where the machinery kills him. Marina claims that Paco fell in by accident. When she lets slip to Jeff that she killed Paco so they could be together, he nearly strangles her, then regains control of himself and leaves the house. Just then, the bandits attack. The local police and Jeff fight them. Marina is irresistibly drawn to the fatal oil well during the battle, and dies when it is blown up. Jeff kills El Gavilan, then leaves with Dutch and Sal.\n\n\n"}
{"id": "57385471", "url": "https://en.wikipedia.org/wiki?curid=57385471", "title": "Camlin Fine Sciences", "text": "Camlin Fine Sciences\n\nCamlin Fine Sciences Ltd., formerly known as Camlin Fine Chemicals Ltd., is a provider of shelf life solutions, aroma ingredients, and performance chemicals. Led by Mr. Ashish Dandekar, Managing Director, CFS has emerged as the largest producer of food antioxidants like TBHQ tert-Butylhydroquinone and BHA Butylated hydroxyanisole and is also one of the world's leading Vanillin producer. \n\nCFS has offices and/or manufacturing facilities in several countries such as India, China, Italy, Brazil, Mexico and the United States. Having established global leadership in antioxidants, CFS forward integrated in blending and has a widened its application to food and beverage industry, petfood, animal feed, fishmeal, aquaculture, biodiesel etc. \n\nCamlin Fine Sciences holds patent for improved process in synthesis of BHA Butylated hydroxyanisole from TBHQ tert-Butylhydroquinone\n\nIn 1984, Camlin Fine Chemicals Division was started by Camlin with the setting up of a new ultra modern plant at Tarapur, India. \n\nIn 2006, Camlin Fine Chemicals demerged from the parent company and an entirely new entity formed called 'Camlin Fine Sciences Ltd.\n\nIn 2007, CFS was listed on the Bombay Stock Exchange \n\nIn 2011, CFS got listed on the National Stock Exchange (NSE).\n\nIn 2011, CFS took the first overseas with the acquisition of Borregaard Italia SpA, a Hydroquinone and Catechol manufacturing facility at Ravenna, Italy through its wholly owned subsidiary in Mauritius.\n\nIn 2014, CFS began operations in Brasil through its 100% subsidiary company CFS do Brasil Ltd, where it commenced production of antioxidant blends for food applications.\n\nIn 2016, CFS acquired 65 % stake in Dresen Quimica S.A.P.I.de C.V., Mexico along with its group companies and subsidiaries.\n\nIn July 2017, CFS became the 3rd largest producer of vanillin in the world with the acquisition of 51% stake in Ningbo Wanglong Flavors & Fragrances Company Ltd., China. \nIn November 2017, CFS entered into a preferred supply agreement with Lockheed Martin Advanced Energy Storage for manufacturing and supply of a specialty chemical.\nIn April 2018, CFS entered into an Animal Nutrition joint venture with Pahang Pharma (S) Pte. Ltd., Singapore.\n\nHaving established global leadership in antioxidants, CFS moved forward in blending by setting up a unit in Tarapur, India and in Brazil in 2014. Today, CFS has its blending facilities across the world (India, China, Mexico, USA and Brazil). It has a portfolio of more than 100 traditional and natural shelf life solution products under the brand names, Xtendra and NaSure. The Company has widened its applications for different food industries such as snacks, bakery, meat & poultry, spices & seasonings etc. and has also tapped potential shelf life solutions in fried snacks, pet food, fishmeal, biodiesel, confectionery, animal feed.\n\nCFS acquired 65% stake in Dresen Quimica S.A.P.I. de C.V., Mexico, to cover Central America and Andean States. Dresen is in the Mexican food and feed blends business. With this acquisition, the portfolio expanded to geographies in Mexico and Central America with products like antioxidant blends (traditional and natural),feed additives etc.\n\nCFS produces vanillin and ethyl vanillin using fully traceable vertically integrated production. CFS uses an environmentally friendly method for producing vanillin and ethyl vanillin (sold under the brand name Vanesse and Evanil respectively) from Catechol. The company makes all key ingredients in-house and has gained acceptance as a quality global vanillin player. The brands Vanesse (vanillin) and Evanil (ethyl vanillin) have grown their customer base not only from food and flavor industry, but also manufacturers of fragrances, incense sticks and pharmaceuticals industries and perfumeries.\n\nThe company has also launched Floral booster, which is specially developed for incense stick industry to enhance sweet note, sustain burning and aroma-spreading properties along with Intense Green-a fragrance and flavour chemical, and Vetigreen-an aromatic chemical for home cleaning solutions, personal care products, cosmetics and incense sticks.\nThe company entered into performance chemicals segment with the acquisition of Borregaard Italia Spa ltd in March 2011 and became one of the largest producers of di‐phenols (Hydroquinone and Catechol). To cater the demand from the market CFS has doubled its guaiacol capacity to 4000 Tonne per annum(tpa) and raised veratrole capacity by 67% to 1000tpa in FY16 and this will lead to 17% sales CAGR over FY15‐18 to Rs 2.04bn. CFS has a distribution hub in different parts of the world to cater to customer demands.\n"}
{"id": "2408886", "url": "https://en.wikipedia.org/wiki?curid=2408886", "title": "Chaco National Park", "text": "Chaco National Park\n\nThe Chaco National Park () is a national park of Argentina, located in the province of Chaco. It has an area of 150 km. It was created in 1954 in order to protect a sample of the Eastern Chaco, composed mainly of warm lowlands, with an annual summer rainfall between 750 and 1,300 mm.\n\nThis park is a protected area for the quebracho trees. Forests of \"quebracho colorado chaqueño\" (\"Schinopsis balansae\") were once located in the north of Santa Fe and the western half of Chaco, and had entered the northeast region of the province of Corrientes. Its strong wood and its abundant tannin caused it to be over-exploited for a century.\n\nThe area harbors several environments: scrubland, savanna, swamps, and small lakes. The scrubland is the habitat of the red quebracho (\"Schinopsis lorentzii\"), white quebracho (\"Aspidosperma quebracho-blanco\"), algarrobo (\"Prosopis alba\"), and lapacho (\"Tabebuia\" spp.), all of which are commercially valuable species. The fauna includes large predators such as cougars. In the lakes one finds yacare caimans and capybaras. Elsewhere there are armadillos, South American tapirs and plains viscachas, as well as birds (more than 340 species). The fauna also includes the black howler monkey (\"Alouatta caraya\").\n\nIndigenous communities of the Mocoví and Toba peoples are found in the protected area.\n\n\n"}
{"id": "5397510", "url": "https://en.wikipedia.org/wiki?curid=5397510", "title": "Chambranle", "text": "Chambranle\n\nIn architecture and joinery, the chambranle is the border, frame, or ornament, made of stone or wood, that is a component of the three sides round chamber doors, large windows, and chimneys.\n\nWhen a chambranle is plain and without mouldings, it is called a \"band\", \"case\", or \"frame\". The chambranle consists of three parts; the two sides, called \"montants\", or \"ports\", and the top, called the \"traverse\" or \"supercilium\". The chambranle of an ordinary door is frequently called a \"door-case\"; of a window, \"window-frame\"; and of a chimney, \"mantle-tree\".\n\nIn ancient architecture, \"antepagmenta\" were garnishings in posts or doors, wrought in stone or timber, or lintels of a window. The word comes from Latin and has been borrowed in English to be used for the entire chambranle, i.e. the door case, or window frame.\n\n\n"}
{"id": "1274206", "url": "https://en.wikipedia.org/wiki?curid=1274206", "title": "Cold hardening", "text": "Cold hardening\n\nCold hardening is the physiological and biochemical process by which an organism prepares for cold weather.\n\nPlants in temperate and polar regions adapt to winter and sub zero temperatures by relocating nutrients from leaves and shoots to storage organs. Freezing temperatures induce dehydrative stress on plants, as water absorption in the root and water transport in the plant decreases. Water in and between cells in the plant freezes and expands, causing tissue damage. Cold hardening is a process in which a plant undergoes physiological changes to avoid, or mitigate cellular injuries caused by sub-zero temperatures. Non-acclimatized individuals can survive −5 °C, while an acclimatized individual in the same species can survive −30°. Plants that originated in the tropics, like tomato or maize, don't go through cold hardening and are unable to survive freezing temperatures. The plant starts the adaptation by exposure to cold yet still not freezing temperatures. The process can be divided into three steps. First the plant perceives low temperature, then converts the signal to activate or repress expression of appropriate genes. Finally, it uses these genes to combat the stress, caused by sub-zero temperatures, affecting its living cells. Many of the genes and responses to low temperature stress are shared with other abiotic stresses, like drought or salinity.When temperature drops, the membrane fluidity, RNA and DNA stability, and enzyme activity change. These, in turn, affect transcription, translation, intermediate metabolism, and photosynthesis, leading to an energy imbalance. This energy imbalance is thought to be one of the ways the plant detects low temperature. Experiments on \"arabidopsis\" show that the plant detects the change in temperature, rather than the absolute temperature. The rate of temperature drop is directly connected to the magnitude of calcium influx, from the space between cells, into the cell. Calcium channels in the cell membrane detect the temperature drop, and promotes expression of low temperature responsible genes in \"alfalfa\" and \"arabidopsis\". The response to the change in calcium elevation depends on the cell type and stress history. Shoot tissue will respond more than a root cells, and a cell that already is adapted to cold stress will respond more than one that has not been through cold hardening before. Light doesn't control the onset of cold hardening directly, but shortening of daylight is associated with fall, and starts production of reactive oxygen species and excitation of photosystem 2, which influences low-temp signal transduction mechanisms. Plants with compromised perception of day length have compromised cold acclimation.\n\nCold increases cell membrane permeability and makes the cell shrink, as water is drawn out when ice is formed in the extracellular matrix between cells. To retain the surface area of the cell membrane so it will be able to regain its former volume when temperature rises again, the plant forms more and stronger Hechtian strands. These are tubelike structures that connect the protoplast with the cell wall. When the intracellular water freezes, the cell will expand, and without cold hardening the cell would rupture. To protect the cell membrane from expansion induced damage, the plant cell changes the proportions of almost all lipids in the cell membrane, and increases the amount of total soluble protein and other cryoprotecting molecules, like sugar and proline.\n\nChilling injury occurs at 0–10 degrees Celsius, as a result of membrane damage, metabolic changes, and toxic buildup. Symptoms include wilting, water soaking, necrosis, chlorosis, ion leakage, and decreased growth. Freezing injury may occur at temperatures below 0 degrees Celsius. Symptoms of extracellular freezing include structural damage, dehydration, and necrosis. If intracellular freezing occurs, it will lead to death. Freezing injury is a result of lost permeability, plasmolysis, and post-thaw cell bursting.\n\nWhen spring comes, or during a mild spell in winter, plants de-harden, and if the temperature is warm for long enough – their growth resumes.\n\nCold hardening has also been observed in insects such as the fruit fly and diamondback moth. The insects use rapid cold hardening to protect against cold shock during overwintering periods. Overwintering insects stay awake and active through the winter while non-overwintering insects migrate or die. Rapid cold hardening can be experienced during short periods of undesirable temperatures, such as cold shock in environment temperature, as well as the common cold months. The buildup of cryoprotective compounds is the reason that insects can experience cold hardening. Glycerol is a cryoprotective substance found within these insects capable of overwintering. Through testing, glycerol requires interactions with other cell components within the insect in order to decrease the body's permeability to the cold. When an insect is exposed to these cold temperatures, glycerol rapidly accumulates. Glycerol is known as a non-ionic kosmotrope forming powerful hydrogen bonds with water molecules. The hydrogen bonds in the glycerol compound compete with the weaker bonds between the water molecules causing an interruption in the makeup of ice formation. This chemistry found within the glycerol compound and reaction between water has been used as an antifreeze in the past, and can be seen here when concerning cold hardening. Proteins also play a large role in the cryoprotective compounds that increase ability to survive the cold hardening process and environmental change. Glycogen phosphorylase (GlyP) has been a key protein found during testing to increase in comparison to a controlled group not experiencing the cold hardening. Once warmer temperatures are observed the process of acclimation begins, and the increased glycerol along with other cryoprotective compounds and proteins are also reversed. There is a rapid cold hardening capacity found within certain insects that suggests not all insects can survive a long period of overwintering. Non-diapausing insects can sustain brief temperature shocks but often have a limit to which they can handle before the body can no longer produce enough cryoprotective components.\n\nInclusive to the cold hardening process being beneficial for insects survival during cold temperatures, it also helps improve the organisms performance. Rapid cold hardening (RCH) is one of the fastest cold temperature responses recorded. This process allows an insect to instantly adapt to the severe weather change without compromising function. The \"Drosophila melanogaster\" (common fruit fly) is a frequently experimented insect involving cold hardening. A proven example of RCH enhancing organisms performance comes from courting and mating within the fruit fly. It has been tested that the fruit fly mated more frequently once RCH has commenced in relation to a controlled insect group not experiencing RCH. Most insects experiencing extended cold periods are observed to modify the membrane lipids within the body. Desaturation of fatty acids are the most commonly seen modification to the membrane. When the fruit fly was observed under the stressful climate the survival rate increased in comparison to the fly prior to cold hardening.\n\nIn addition to testing on the common fruit fly, \"Plutella xylostella\" (diamondback moth) also has been widely studied for its significance in cold hardening. While this insect also shows an increase in glycerol and similar cryoprotective compounds, it also shows an increase in polyols. These compounds are specifically linked to cryoprotective compounds designed to withstand cold hardening. The polyol compound is freeze-susceptible and freeze tolerant. Polyols simply act as a barrier within the insect body by preventing intracellular freezing by restricting the extracellular freezing likely to happen in overwintering periods. During the larvae stage of the diamondback moth, the significance of glycerol was tested again for validity. The lab injected the larvae with added glycerol and in turn proved that glycerol is a major factor in survival rate when cold hardening. The cold tolerance is directly proportional to the buildup of glycerol during cold hardening.\n\nCold hardening of insects improves the survival rate of the species and improves function. Once environmental temperature begins to warm up above freezing, the cold hardening process is reversed and the glycerol and cryprotective compounds decrease within the body. This also reverts the function of the insect to pre-cold hardening activity.\n\n"}
{"id": "39410847", "url": "https://en.wikipedia.org/wiki?curid=39410847", "title": "Daryan Dam", "text": "Daryan Dam\n\nThe Daryan Dam, also spelled Darian, is an embankment dam constructed on the Sirvan River just north of Daryan in Paveh County, Kermanshah Province, Iran. The primary purpose of the dam is to supply up to of water annually to the long Nosoud Water Conveyance Tunnel where it will irrigate areas of Southwestern Iran. The dam will also have a 210 MW hydroelectric power station. Construction on the dam began in 2009 and the dam began to fill its reservoir in late November 2015. The Darian Dam Archaeological Salvage Program (DDASP) was planned by Iranian Center for Archaeological Research before flooding the reservoir. As a result a number of important archaeological sites were discovered and some were excavated. The power station is expected to be operational at the end of 2016. All works are expected to be complete by 2018. The dam's diversion tunnel was completed in June 2011. The dam was designed by Stucky of France and consultation was provided by Mahab Ghodss, International Consulting Engineering Co. In August 2010 Farab Co. won the contract to build the power station. In 2011, workers on the project held a protest against unpaid wages. The dam is also the subject of protest due to the forced relocations and ecological/cultural impact its reservoir will have.\n\n"}
{"id": "4303674", "url": "https://en.wikipedia.org/wiki?curid=4303674", "title": "Deposition (phase transition)", "text": "Deposition (phase transition)\n\nDeposition is a thermodynamic process, a phase transition in which gas transforms into solid without passing through the liquid phase. The reverse of deposition is sublimation and hence sometimes deposition is called desublimation.\n\nOne example of deposition is the process by which, in sub-freezing air, water vapor changes directly to ice without first becoming a liquid. This is how snow forms in clouds, as well as how frost and hoar frost form on the ground or other surfaces. Another example is when frost forms on a leaf. For deposition to occur, thermal energy must be removed from a gas. When the air becomes cold enough, water vapor in the air surrounding the leaf loses enough thermal energy to change into a solid. Even though the air temperature may be below the dew point, the water vapor may not be able to condense spontaneously if there is no way to remove the latent heat. When the leaf is introduced, the supercooled water vapor immediately begins to condense, but by this point is already past the freezing point. This causes the water vapor to change directly into a solid.\n\nAnother example is the soot that is deposited on the walls of chimneys. Soot molecules rise from the fire in a hot and gaseous state. When they come into contact with the walls they cool, and change to the solid state, without formation of the liquid state. The process is made use of industrially in combustion chemical vapor deposition.\n\nThere is an industrial coatings process, known as evaporative deposition, whereby a solid material is heated to the gaseous state in a low-pressure chamber, the gas molecules travel across the chamber space and then condense to the solid state on a target surface, forming a smooth and thin layer on the target surface. Again, the molecules do not go through an intermediate liquid state when going from the gas to the solid. See also physical vapor deposition, which is a class of processes used to deposit thin films of various materials onto various surfaces.\n\nDeposition releases energy and is an exothermic phase change.\n\n"}
{"id": "1465844", "url": "https://en.wikipedia.org/wiki?curid=1465844", "title": "Deutschlandsender Herzberg/Elster", "text": "Deutschlandsender Herzberg/Elster\n\nThe Deutschlandsender III was a 500 kilowatt longwave transmitter, erected in 1938/39 near Herzberg, Brandenburg in Germany. Used for the \"Deutschlandsender\" radio broadcasts, the guyed mast reaching a height of was the tallest construction in Europe and the second tallest in the world.\n\nThe Deutschlandsender III used a tall guyed steel lattice mast of triangular cross section. This was used as a mast radiator and was therefore mounted on a high steatite insulator. At the top of the mast there was a lens-like electrical lengthening structure with a diameter of and a height of .\n\nBecause the mast was under high voltage during transmission, the aircraft warning lighting was realized in a very unconventional manner. On small poles near the mast multiple rotating searchlights were mounted which illuminated the lens-like structure on the top.\n\nIt was planned to expand the facility to a circle group antenna. Therefore, ten tall masts should be built on a circle with a diameter of around the central mast. In 1944 construction of a backup antenna in form of a triangle antenna, carried by three tall masts, forming a triangle with sidelength, started on the location of the planned mast No. 9. This antenna could not be completed as a result of the war.\n\nOn 21 April 1945 the transmitter was severely damaged by Allied bombing. The mast remained unimpaired, but, it dismantled by the Soviet occupation troops, a task that lasted from July 1946 to 23 December 1947. The other parts of the facility were dismantled in 1959, when waterworks were built on the former station area. Nevertheless, there are still some remnants of the base visible at the location.\n\nIt is unknown what happened to the mast after it was dismantled. It is sometimes claimed that it was rebuilt in the Ukraine, as \"Kiev\" was scrawled on the containers the components were transported in.\n\n\n"}
{"id": "21233922", "url": "https://en.wikipedia.org/wiki?curid=21233922", "title": "Energy rationing", "text": "Energy rationing\n\nEnergy rationing primarily involves measures that are designed to force energy conservation as an alternative to price mechanisms in energy markets. Because of its economic consequences energy rationing is used as method of last resort, often at times of emergency such as during an energy crisis.\n\nExamples of energy rationing include fuel rationing and the use of ration books or ration stamps to restrict personal consumption. Energy rationing may include penalties such as surcharges and disconnection from electrical supply for those who choose not to reduce their demand voluntarily. \n\nLoad shedding is a common form of energy rationing used when electricity markets cannot keep up to demand, particularly peak demand. Limited electrical supply from power stations at times of drought or after infrastructure is damaged, can lead authorities to implement rationing. Brazil was forced to implement energy rationing due to drought in 2001. Reducing demand in this way aims to avoid forced power outages which are more disruptive than rationing.\n\nTradable Energy Quotas is an energy rationing system designed to enable nations to reduce their emissions of greenhouse gases along with their use of oil, gas and coal, and to ensure fair access to energy for all.\n\nOne issue with energy rationing is the cost of setting up a rationing schemes. Another criticism is that energy rationing schemes are unworkable and face many practical problems including consumer lawsuits. Due to the general disdain for restrictions that interfere with existing personal freedoms, such as ecotaxes and carbon rationing, energy rationing is not favoured by policy makers to mitigate global warming.\n\nAs oil becomes more scarce due to oil depletion countries that have reserve currencies will prefer to buy oil rather than ration it. The Oil Depletion Protocol is form of energy rationing that was developed by Richard Heinberg to ensure that price rationing does not price out poorer countries.\n\n"}
{"id": "42523481", "url": "https://en.wikipedia.org/wiki?curid=42523481", "title": "Excisive triad", "text": "Excisive triad\n\nIn topology, a branch of mathematics, an excisive triad is a triple formula_1 of topological spaces such that \"A\", \"B\" are subspaces of \"X\" and \"X\" is the union of the interior of \"A\" and the interior of \"B\". Note \"B\" is not required to be a subspace of \"A\".\n\n\n"}
{"id": "26065600", "url": "https://en.wikipedia.org/wiki?curid=26065600", "title": "February 1952 nor'easter", "text": "February 1952 nor'easter\n\nThe February 1952 nor'easter was a significant winter storm that impacted the New England region of the United States. The storm ranked as Category 1, or \"notable\", on the Northeast Snowfall Impact Scale. Its rapid intensification resulted in heavy snowfall between February 17 and 18, accumulating to . High winds also affected central and northern New England. The nor'easter is estimated to have caused 42 fatalities. In Maine, over 1,000 travelers became stranded on roadways. Two ships cracked in two offshore New England during the storm.\n\nThe development of this extratropical cyclone was associated with a pronounced weakening of the usual zonal, or west-to-east, flow which dominates the Mid-Latitudes during the winter, with the jet stream dipping from the latitude of Boston to the latitude of Charleston, South Carolina. The initial low-pressure area originated in the Gulf of Mexico, with a downstream redevelopment offshore Cape Hatteras on February 16. The cyclone deepened rapidly, 25 millibars in 24 hours between February 17 and February 18, while moving just offshore Long Island and southern New England, and dropped heavy precipitation across the central and northern Mid-Atlantic states. The snowfall associated with the storm across eastern New York and southern New England produced the heaviest snows of the winter.\n\nThere was significant disruption to shipping due to this storm. On February 18, 1952, while en route from New Orleans to Boston, the T2 tanker SS Pendleton' broke in two in a nor'easter south of Cape Cod, Massachusetts. A United States Coast Guard Consolidated PBY Catalina aircraft was diverted from searching for another T2 tanker to search for \"Pendleton\", and located both sections. At this point, the Coast Guard realized that they were dealing with two ships that had broken in two. The \"Coast Guard Motor Lifeboat CG 36500\" was dispatched from Chatham, Massachusetts. She had four crew on board as the rest of her crew had made themselves scarce on hearing that the \"CG-36500\" was to be sent out to \"Pendletons\" aid. Nine of \"Pendletons\" 41 crew were lost, eight were on the bow section and the ship's cook from the stern section, who had selflessly assisted the rest of the crew off the vessel before him. He was lost when, as the ship started to slide off a sand bar (that she landed on) by the strong gale-force winds, he jumped from the Jacob's ladder, fell into the ocean, and was struck by the lifeboat as it was hit by a wave, killing him instantly. With the survivors on board, a row developed over how they should be dealt with. Bernard C. Webber, of the \"CG-36500\" decided not to transfer them to and headed for the beach. The survivors were safely landed at Chatham.\n\nThe rescue of the survivors of the shipwrecked Pendleton is considered one of the most daring rescues of the United States Coast Guard. All four crew of \"CG-36500\" were awarded the Coast Guard's Gold Lifesaving Medal. At the time of her loss, \"Pendleton\" was insured for $1,690,000.\n\nThe stern ultimately grounded off Monomoy Island, south of Chatham, and her forepart grounded on Pollock Rip Shoal, at coordinates , The bow section was sold in 1953 to North American Smelting Co. for recycling at Bordentown, New Jersey. However, it was stranded on June 4, 1953 in the Delaware River and dismantled there c.1978 by the United States Army Corps of Engineers.\n\nThe movie The Finest Hours (2016 film) was based on a shipping mishap that occurred during this cyclone.\n\n"}
{"id": "7064416", "url": "https://en.wikipedia.org/wiki?curid=7064416", "title": "Fuel Price Escalator", "text": "Fuel Price Escalator\n\nThe Fuel Price Escalator (later Fuel Duty Stabiliser), a fuel duty policy in the United Kingdom ahead of inflation, was introduced in March 1993 as a measure to stem the increase in pollution from road transport and cut the need for new road building at a time of major road protests, at Twyford Down and other locations. Set initially at 3% above inflation it was increased in two stages to 6% before being suspended and then, in 2011, replaced by a 'fuel duty stabiliser' (also known as the 'fuel price stabiliser' and 'fair fuel stabiliser') following further increases the price of oil.\n\nAt a time of rapidly rising concerns about the effect of road transport on the environment, and in particular from the program of road building which had resulted in major road protests, at Twyford Down and other locations, the Conservative's under John Major introduced a 'Fuel Price Escalator' in March 1993 set initially at 3% ahead of inflation per year, increased to 5% later in the same year, and then increased again to 6% in 1997 by the Blair ministry after Labour won power.\n\nThe last rise due to the escalator took place following the budget on 9 March 1999 at a time of rapidly increasing oil prices. In 2000 at a time of rising protests at the cost of fuel Gordon Brown announced that the prices would only be increased by inflation due to the high price of oil.\n\nIncreases were deferred for a number of budgets and then in 2011, at a time of rapidly increasing oil prices, George Osborne cut 1p from the tax, increased the Petroleum Revenue Tax to raise at additional £2bn from North Sea oil firms, and announced that the escalator would be replaced with a 'fuel price stabiliser'. but would rise if oil prices fell below $75.\n\nIn 2011 budget the Chancellor had also announced a rise of 1p in January 2012 and then 5p in August 2013, but later cancelled the 1p rise and reduced the 5p August rise to 3p in November 2012. In the budget of 2012 Osborne confirmed the 3p August rise, before first postponing it and then cancelling it in December 2012. A further proposed inflation-based increase in fuel duty was cancelled by the chancellor in March 2013.\n\nIn March 2016, with oil prices at about $40 a barrel, and following widespread speculation that the duty would be increased at a time of record low oil prices, the chancellor froze fuel duty for the sixth year running, and reduced the tax on North Sea oil firms.\n\n\n"}
{"id": "40048943", "url": "https://en.wikipedia.org/wiki?curid=40048943", "title": "Geoffrey G Parker", "text": "Geoffrey G Parker\n\nGeoffrey G Parker is a scholar whose work focuses on distributed innovation, energy markets, and the economics of information. He co-developed the theory of two-sided markets with Marshall Van Alstyne.\n\nHis current research includes studies of platform business strategy, data governance, and technical/economic systems to integrate distributed energy resources.\n\nParker is Professor of Engineering and Director, Master of Engineering Management, (MEM) Thayer School of Engineering at Dartmouth College, the first national research university to graduate a class of engineers with more women than men. He has set the Thayer School of Engineering apart with the introduction of Data Analytics and Platform Design classes, emphasizing the business aspects of engineering and giving engineers the background they need to be business innovators and entrepreneurs. Parker is part of a unique culture that is breaking gender barriers.\n\nParker is also a Faculty Fellow at MIT and the MIT Center for Digital Business. Parker is co-author of the book \"Platform Revolution\", which was included among the 16 must-read business books for 2016 by Forbes.\n\nGeoffrey Parker was born in Dayton, Ohio. He received a BS in Electrical Engineering and Computer Science from Princeton University in 1986. He then completed the General Electric Company Financial Management Training Program and held multiple positions in engineering and finance at General Electric in North Carolina and Wisconsin. He obtained an MS in Electrical Engineering (Technology and Policy Program) in 1993 and a PhD in Management Science in 1998, both at the Massachusetts Institute of Technology.\n\nParker is Professor of Engineering and Director, Master of Engineering Management, Thayer School of Engineering, Dartmouth College. In addition, he is a Fellow at MIT’s Initiative on the Digital Economy where he leads platform industry research studies and co-chairs the annual MIT Platform Strategy Summit. Parker is a visiting scholar at the MIT Sloan School. His teaching includes platform strategy courses that provide managers the tools they need to understand the digital economy and technical courses that give students the skills they need to transform large data sets into actionable knowledge. He was formerly Professor of Management Science at Tulane University where he served as Director of the Tulane Energy Institute. Parker has taught undergraduate and full-time MBA courses as well as professional MBA and executive MBA programs.\n\nParker served as a National Science Foundation panelist from 2009 to 2011. He is a senior editor for the journal \"Production and Operations Management\", an associate editor for the journal \"Management Science\" and President of the Industry Studies Association. Parker is a member of General Electric’s Learning Advisory Board, consisting of academics drawn from across Africa, the United States of America and the United Kingdom, that assists in development and broadening of skills across Africa.\n\nParker co-organizes and co-chairs the annual MIT Platform Strategy Summit, an executive meeting on platform-centered economics and management, where he stressed the growth of platforms, their welfare implications and their takeover of government functions. At the same time, he co-chairs an academic meeting, the Platform Strategy Research Symposium. Parker served as chair of the U.S.-Israel Energy Summit in 2014.\n\nParker has made significant contributions to the field of network economics and strategy as co-developer of the theory of two-sided markets with Marshall Van Alstyne.\n\nParker and Van Alstyne observed that, unlike traditional value chains with cost and revenue on different sides, two-sided networks have cost and revenue on both sides, because the “platform” has a distinct group of users on each side. Their approach has been described as the “chicken and egg” problem of how to build a platform. They concluded that the problem must be solved by platform owners, typically by cross-subsidizing between groups or even giving away products or services for free. Two-sided network effects can cause markets to concentrate in the hands of a few firms. These properties inform the strategies and antitrust law approaches at all firms involved in the network.\n\nHis research includes studies of distributed innovation, business platform strategy, and platforms to integrate intermittent energy.\n\nParker is a frequent keynote speaker and advises senior leaders on their organizations’ platform strategies. Before attending MIT, he held positions in engineering and finance at GE.\n\nParker’s research has appeared in journals such as \"Harvard Business Review\", \"MIT Sloan Management Review\", \"Energy Economics\", \"Information Systems Research\"\", Journal of Economics and Management Strategy,\" \"Management Science\", \"\", and \"Strategic Management Journal\". His work has also been featured on business news publications such as “MarketWatch” and \"Wired\".\n\nHe is the co-author of \"Platform Revolution: How Networked Markets Are Transforming the Economy and How to Make Them Work for You\". The book describes the information technologies, standards, and rules that make up platforms, and are used and developed by the biggest and most innovative global companies. Forbes included it among 16 must-read business books for 2016, describing it as \"a practical guide to the new business model that is transforming the way we work and live.\"\n\nParker also co-wrote \"Operations Management For Dummies\" within the \"For Dummies\" franchise.\n\nFortune’s Excerpt of Platform Revolution: How Networked Markets Are Transforming the Economy and How to Make Them Work for You\n\nMIT Sloan Management Review: The Intersection of Management and Technology\n\nParker won the Wick Skinner Early Career Research Accomplishments Award in 2003. He was given the Dean’s Excellence in Teaching Award for Graduate Education at Freeman School of Business in 2014.\n\n\n\"Harvard Business Review: Pipelines, Platforms and the New Rules of Strategy\"\n\n\"Harvard Business Review \"\n\n\n\n\n\n\n\n"}
{"id": "33797433", "url": "https://en.wikipedia.org/wiki?curid=33797433", "title": "George H. Miley", "text": "George H. Miley\n\nGeorge H. Miley (born 1933) is a professor emeritus of physics from the University of Illinois at Urbana–Champaign. Miley is a Guggenheim Fellow and Fellow of the American Nuclear Society, the American Physical Society and the Institute of Electrical and Electronic Engineers. He was Senior NATO Fellow from 1994 to 1995, received the \"Edward Teller Medal\" in 1995, the \"IEEE Nuclear and Plasma Science Award in Fusion Technology\" in 2003 and the \"Radiation Science and Technology Award\" in 2004. He holds several patents.\n\nIn 1955, Miley received his B.S. in Chemical Engineering/Physics from Carnegie Mellon University. He obtained his M.Sc. (1956) and his Ph.D. (1959), both in Nuclear/Chemical Engineering, from the University of Michigan. \n\nIn 1961, he became assistant professor at the University of Illinois at Urbana–Champaign, and advanced to associate professor in 1964, and to professor of nuclear and electrical engineering in 1967. He has directed the Fusion Studies Lab since 1975 and has chaired the Nuclear Engineering Program from 1983 to 1995. Since August 2010 he is professor emeritus. \n\nHe was editor-in-chief of the American Nuclear Society's journal \"Fusion Science and Technology\" until his retirement in 2000. He was also the editor-in-chief of the \"Journal of Plasma Physics\" and \"Laser and Particle Beams\".\n\nHis work \"Direct Conversion of Nuclear Radiation Energy\" was written in 1970 and sponsored by the Atomic Energy Commission.\n\nMiley has investigated nuclear transmutations in thin films of metals based on earlier work on the Patterson Power Cell, supported by Clean Energy Technologies, Incorporated (CETI). Miley is also active in research on low-energy nuclear reactions (LENR) in thin metal films.\n\nAfter his retirement he participated in the 2011 World Green Energy Symposium, which was held in October 2011 at the Pennsylvania Convention Center in Philadelphia, Pennsylvania. During a Symposium session titled \"Cold Fusion – A Discussion\", Miley reported that he has constructed a LENR device that continuously produces several hundred watts of energy.\n\n\n"}
{"id": "15666839", "url": "https://en.wikipedia.org/wiki?curid=15666839", "title": "Goupitan Dam", "text": "Goupitan Dam\n\nThe Goupitan Dam () is an arch dam on the Wu River, a tributary of the Yangtze River in Guizhou Province, southwest of China. The dam's hydroelectric facility will operate on five turbines, each with a hydroelectric generating capacity of , for a total of . Constructions began on in 2003 and the first generator was operational in June 2009. All works were completed in 2011.\n\nThe dam is supplemented by the Goupitan shiplift, said to be the tallest shiplift in the world. \n\n"}
{"id": "715691", "url": "https://en.wikipedia.org/wiki?curid=715691", "title": "Higgsino", "text": "Higgsino\n\nIn particle physics, for models with N=1 supersymmetry a Higgsino, symbol , is the superpartner of the Higgs field. A Higgsino is a Dirac fermionic field with spin 1/2 and it refers to a weak isodoublet with hypercharge half under the Standard Model gauge symmetries. After electroweak symmetry breaking higgsino fields linearly mix with U(1) and SU(2) gauginos leading to four neutralinos and two charginos that refer to physical particles. While the two charginos are charged Dirac fermions (plus and minus each), the neutralinos are electrically neutral Majorana fermions. In an R-parity conserving version of the Minimal Supersymmetric Standard Model, the lightest neutralino typically becomes the lightest supersymmetric particle (LSP). The LSP is a particle physics candidate for the dark matter of the universe since it cannot decay to particles with lighter mass. A neutralino LSP, depending on its composition can be bino, wino or higgsino dominated in nature and can have different zones of mass values in order to satisfy the estimated dark matter relic density. Commonly, a higgsino dominated LSP is often referred as a higgsino, in spite of the fact that a higgsino is not a physical state in true sense.\n\nIn natural scenarios of SUSY, top squarks, bottom squarks, gluinos, and higgsino-enriched neutralinos and charginos are expected to be relatively light, enhancing their production cross sections. Higgsino searches have been performed by both the ATLAS and CMS experiments at the Large Hadron Collider at CERN, where physicists have searched for the direct electroweak pair production of Higgsinos. As of 2017, no experimental evidence for Higgsinos has been reported.\n\nIf dark matter is composed only of Higgsinos, then the Higgsino mass is 1.1 TeV. On the other hand, if dark matter has multiple components, then the Higgsino mass depends on the relevant multiverse distribution functions, making the mass of the Higgsino lighter.\n"}
{"id": "48459845", "url": "https://en.wikipedia.org/wiki?curid=48459845", "title": "High-frequency impulse-measurement", "text": "High-frequency impulse-measurement\n\nHFIM, acronym for high-frequency-impulse-measurement, is a type of measurement technique in acoustics, where structure-borne sound signals are detected and processed with certain emphasis on short-lived signals as they are indicative for crack formation in a solid body, mostly steel. The basic idea is to use mathematical signal processing methods such as Fourier analysis in combination with suitable computer hardware to allow for real-time measurements of acoustic signal amplitudes as well as their distribution in frequency space. The main benefit of this technique is the enhanced signal-to-noise ratio when it comes to the separation of acoustic emission from a certain source and other, unwanted contamination by any kinds of noise. The technique is therefore mostly applied in industrial production processes, e.g. cold forming or machining, where a 100 percent quality control is required or in condition monitoring for e.g. quantifying tool wear.\n\nHigh-frequency-impulse measurement is an algorithm for obtaining frequency information of any structure- or air-borne sound source on the basis of discrete signal transformations. This is mostly done using [Fourier series] to quantify the distribution of the energy content of a sound signal in frequency space. On the software side, the tool used for this is the fast Fourier transform (FFT) implementation of this mathematical transformation. This allows, in combination with specific hardware, to directly obtain frequency information so that this is accessible in-line, e.g. during a production process. Contrary to classical, off-line frequency analysis methods, the signal is not unfolded before transformation but is directly fed into the FFT computation. Single events, such as cracks, are hence depicted as extremely short-lived signals covering the entire frequency range (the Fourier transform of a single impulse is a signal covering the entire observed frequency space). Therefore, such single events are easily separable from other noises, even if they are much more energetic.\n\nBecause of its in-line capabilities, HFIM is mostly applied in industrial production processes when it comes to high quality standards e.g. for auto parts that are relevant for crash behavior of a car:\n\nThere are also several applications of HFIM devices in materials science laboratories where the exact timing of crack formation is relevant, for instance when determining the plasticity of a new kind of steel.\n\n\n"}
{"id": "19679428", "url": "https://en.wikipedia.org/wiki?curid=19679428", "title": "Inertance", "text": "Inertance\n\nInertance is a measure of the pressure difference in a fluid required to cause a unit change in the rate of change of volumetric flow-rate with time. The base SI units of inertance are kg m or Pa m s and the usual symbol is I.\n\nThe inertance of a tube is given by:\n\nwhere\n\nThe pressure difference is related to the change in flow-rate by the equation:\n\nwhere\nThis equation assumes constant density, that the acceleration is uniform, and that the flow is fully developed \"plug flow\". This precludes sharp bends, water hammer, and so on.\n\nTo some, it may appear counterintuitive that an increase in cross-sectional area of a tube reduces the inertance of the tube. However, for the same mass flow-rate, a lower cross-sectional area implies a higher fluid velocity and therefore a higher pressure difference to accelerate the fluid.\n\nIn respiratory physiology, inertance (of air) is measured in cmHO L s.\n\n1 cmHO L s ≈ 98100 Pa m s.\n\nUsing small-signal analysis, an inertance can be represented as a fluid reactance (c.f. electrical reactance)) through the relation:\n\nwhere\n"}
{"id": "113469", "url": "https://en.wikipedia.org/wiki?curid=113469", "title": "Joule–Thomson effect", "text": "Joule–Thomson effect\n\nIn thermodynamics, the Joule–Thomson effect (also known as the Joule–Kelvin effect, Kelvin–Joule effect) describes the temperature change of a \"real\" gas or liquid (as differentiated from an ideal gas) when it is forced through a valve or porous plug while keeping them insulated so that no heat is exchanged with the environment. This procedure is called a \"throttling process\" or \"Joule–Thomson process\". At room temperature, all gases except hydrogen, helium and neon cool upon expansion by the Joule–Thomson process when being throttled through an orifice; these three gases experience the same effect but only at lower temperatures. Some liquids such as hydraulic oils will be warmed by the Joule-Thomson throttling process.\n\nThe gas-cooling throttling process is commonly exploited in refrigeration processes such as air conditioners, heat pumps, and liquefiers. In hydraulics, the warming effect from Joule-Thomson throttling can be used to find internally leaking valves as these will produce heat which can be detected by thermocouple or thermal-imaging camera. Throttling is a fundamentally irreversible process. The throttling due to the flow resistance in supply lines, heat exchangers, regenerators, and other components of (thermal) machines is a source of losses that limits the performance.\n\nThe effect is named after James Prescott Joule and William Thomson, 1st Baron Kelvin, who discovered it in 1852. It followed upon earlier work by Joule on Joule expansion, in which a gas undergoes free expansion in a vacuum and the temperature is unchanged, if the gas is ideal.\n\nThe \"adiabatic\" (no heat exchanged) expansion of a gas may be carried out in a number of ways. The change in temperature experienced by the gas during expansion depends not only on the initial and final pressure, but also on the manner in which the expansion is carried out.\nThe temperature change produced during a Joule–Thomson expansion is quantified by the Joule–Thomson coefficient, formula_1. This coefficient may be either positive (corresponding to cooling) or negative (heating); the regions where each occurs for molecular nitrogen, N, are shown in the figure. Note that most conditions in the figure correspond to N being a supercritical fluid, where it has some properties of a gas and some of a liquid, but can not be really described as being either. The coefficient is negative at both very high and very low temperatures; at very high pressure it is negative at all temperatures. The maximum inversion temperature (621 K for N) occurs as zero pressure is approached. For N gas at low pressures, formula_1 is negative at high temperatures and positive at low temperatures. At temperatures below the gas-liquid coexistence curve, N condenses to form a liquid and the coefficient again becomes negative. Thus, for N gas below 621 K, a Joule–Thomson expansion can be used to cool the gas until liquid N forms.\n\nThere are two factors that can change the temperature of a fluid during an adiabatic expansion: a change in internal energy or the conversion between potential and kinetic internal energy. Temperature is the measure of thermal kinetic energy (energy associated with molecular motion); so a change in temperature indicates a change in thermal kinetic energy. The internal energy is the sum of thermal kinetic energy and thermal potential energy. Thus, even if the internal energy does not change, the temperature can change due to conversion between kinetic and potential energy; this is what happens in a free expansion and typically produces a decrease in temperature as the fluid expands. If work is done on or by the fluid as it expands, then the total internal energy changes. This is what happens in a Joule–Thomson expansion and can produce larger heating or cooling than observed in a free expansion.\n\nIn a Joule–Thomson expansion the enthalpy remains constant. The enthalpy, formula_3, is defined as\nwhere formula_5 is internal energy, formula_6 is pressure, and formula_7 is volume. Under the conditions of a Joule–Thomson expansion, the change in formula_8 represents the work done by the fluid (see the proof below). If formula_8 increases, with formula_3 constant, then formula_5 must decrease as a result of the fluid doing work on its surroundings. This produces a decrease in temperature and results in a positive Joule–Thomson coefficient. Conversely, a decrease in formula_8 means that work is done on the fluid and the internal energy increases. If the increase in internal energy exceeds the increase in potential energy, there will be an increase in the temperature of the fluid and the Joule–Thomson coefficient will be negative.\n\nFor an ideal gas, formula_8 does not change during a Joule–Thomson expansion. As a result, there is no change in internal energy; since there is also no change in thermal potential energy, there can be no change in thermal kinetic energy and, therefore, no change in temperature. In real gases, formula_8 does change.\n\nThe ratio of the value of formula_8 to that expected for an ideal gas at the same temperature is called the compressibility factor, formula_16. For a gas, this is typically less than unity at low temperature and greater than unity at high temperature (see the discussion in compressibility factor). At low pressure, the value of formula_16 always moves towards unity as a gas expands. Thus at low temperature, formula_16 and formula_8 will increase as the gas expands, resulting in a positive Joule–Thomson coefficient. At high temperature, formula_16 and formula_8 decrease as the gas expands; if the decrease is large enough, the Joule–Thomson coefficient will be negative.\n\nFor liquids, and for supercritical fluids under high pressure, formula_8 increases as pressure increases. This is due to molecules being forced together, so that the volume can barely decrease due to higher pressure. Under such conditions, the Joule–Thomson coefficient is negative, as seen in the figure above.\n\nThe physical mechanism associated with the Joule–Thomson effect is closely related to that of a shock wave, although a shock wave differs in that the change in bulk kinetic energy of the gas flow is not negligible.\n\nThe rate of change of temperature formula_23 with respect to pressure formula_6 in a Joule–Thomson process (that is, at constant enthalpy formula_3) is the \"Joule–Thomson (Kelvin) coefficient\" formula_1. This coefficient can be expressed in terms of the gas's volume formula_7, its heat capacity at constant pressure formula_28, and its coefficient of thermal expansion formula_29 as:\n\nSee the Derivation of the Joule–Thomson (Kelvin) coefficient below for the proof of this relation. The value of formula_1 is typically expressed in °C/bar (SI units: K/Pa) and depends on the type of gas and on the temperature and pressure of the gas before expansion. Its pressure dependence is usually only a few percent for pressures up to 100 bar.\n\nAll real gases have an \"inversion point\" at which the value of formula_1 changes sign. The temperature of this point, the \"Joule–Thomson inversion temperature\", depends on the pressure of the gas before expansion.\n\nIn a gas expansion the pressure decreases, so the sign of formula_33 is negative by definition. With that in mind, the following table explains when the Joule–Thomson effect cools or warms a real gas:\n\nHelium and hydrogen are two gases whose Joule–Thomson inversion temperatures at a pressure of one atmosphere are very low (e.g., about 45 K (−228 °C) for helium). Thus, helium and hydrogen warm when expanded at constant enthalpy at typical room temperatures. On the other hand, nitrogen and oxygen, the two most abundant gases in air, have inversion temperatures of 621 K (348 °C) and 764 K (491 °C) respectively: these gases can be cooled from room temperature by the Joule–Thomson effect.\n\nFor an ideal gas, formula_1 is always equal to zero: ideal gases neither warm nor cool upon being expanded at constant enthalpy.\n\nIn practice, the Joule–Thomson effect is achieved by allowing the gas to expand through a throttling device (usually a valve) which must be very well insulated to prevent any heat transfer to or from the gas. No external work is extracted from the gas during the expansion (the gas must not be expanded through a turbine, for example).\n\nThe cooling produced in the Joule–Thomson expansion makes it a valuable tool in refrigeration. The effect is applied in the Linde technique as a standard process in the petrochemical industry, where the cooling effect is used to liquefy gases, and also in many cryogenic applications (e.g. for the production of liquid oxygen, nitrogen, and argon). A gas must be below its inversion temperature to be liquefied by the Linde cycle. For this reason, simple Linde cycle liquefiers, starting from ambient temperature, cannot be used to liquefy helium, hydrogen, or neon. However, the Joule–Thomson effect can be used to liquefy even helium, provided that the helium gas is first cooled below its inversion temperature of 40 K.\n\nIn thermodynamics so-called \"specific\" quantities are quantities per unit mass (kg) and are denoted by lower-case characters. So \"h\", \"u\", and \"v\" are the specific enthalpy, specific internal energy, and specific volume (volume per unit mass, or reciprocal density), respectively. In a Joule–Thomson process the specific enthalpy \"h\" remains constant. To prove this, the first step is to compute the net work done when a mass \"m\" of the gas moves through the plug. This amount of gas has a volume of \"V\" = \"m\" \"v\" in the region at pressure \"P\" (region 1) and a volume \"V\" = \"m\" \"v\" when in the region at pressure \"P\" (region 2). Then in region 1, the \"flow work\" done \"on\" the amount of gas by the rest of the gas is: W = \"m\" \"P\"\"v\". In region 2, the work done \"by\" the amount of gas on the rest of the gas is: W = \"m\" \"P\"\"v\". So, the total work done \"on\" the mass \"m\" of gas is\n\nThe change in internal energy minus the total work done \"on\" the amount of gas is, by the first law of thermodynamics, the total heat supplied to the amount of gas. \nIn the Joule–Thomson process the gas is insulated, so no heat is absorbed. This means that\n\nwhere \"u\" and \"u\" denote the specific internal energies of the gas in regions 1 and 2, respectively. Using the definition of the specific enthalpy \"h = u + Pv\", the above equation implies that\n\nwhere h and \"h\" denote the specific enthalpies of the amount of gas in regions 1 and 2, respectively.\n\nA very convenient way to get a quantitative understanding of the throttling process is by using diagrams. There are many types of diagrams (\"h\"-\"T\" diagram, \"h\"-\"P\" diagram, etc.) Commonly used are the so-called \"T\"-\"s\" diagrams. Figure 2 shows the \"T\"-\"s\" diagram of nitrogen as an example. Various points are indicated as follows:\n\nAs shown before, throttling keeps \"h\" constant. E.g. throttling from 200 bar and 300 K (point a in fig. 2) follows the isenthalp (line of constant specific enthalpy) of 430 kJ/kg. At 1 bar it results in point b which has a temperature of 270 K. So throttling from 200 bar to 1 bar gives a cooling from room temperature to below the freezing point of water. Throttling from 200 bar and an initial temperature of 133 K (point c in fig. 2) to 1 bar results in point d, which is in the two-phase region of nitrogen at a temperature of 77.2 K. Since the enthalpy is an extensive parameter the enthalpy in d (\"h\") is equal to the enthalpy in e (\"h\") multiplied with the mass fraction of the liquid in d (\"x\") plus the enthalpy in f (\"h\") multiplied with the mass fraction of the gas in d (1 − \"x\"). So\n\nWith numbers: 150 = \"x\" 28 + (1 − \"x\") 230 so \"x\" is about 0.40. This means that the mass fraction of the liquid in the liquid–gas mixture leaving the throttling valve is 40%.\n\nIt is difficult to think physically about what the Joule–Thomson coefficient, formula_1, represents. Also, modern determinations of formula_1 do not use the original method used by Joule and Thomson, but instead measure a different, closely related quantity. Thus, it is useful to derive relationships between formula_1 and other, more convenient quantities. That is the purpose of this section.\n\nThe first step in obtaining these results is to note that the Joule–Thomson coefficient involves the three variables \"T\", \"P\", and \"H\". A useful result is immediately obtained by applying the cyclic rule; in terms of these three variables that rule may be written\n\nEach of the three partial derivatives in this expression has a specific meaning. The first is formula_1, the second is the constant pressure heat capacity, formula_28, defined by\n\nand the third is the inverse of the \"isothermal Joule–Thomson coefficient\", formula_49, defined by\n\nThis last quantity is more easily measured than formula_1 . Thus, the expression from the cyclic rule becomes\n\nThis equation can be used to obtain Joule–Thomson coefficients from the more easily measured isothermal Joule–Thomson coefficient. It is used in the following to obtain a mathematical expression for the Joule–Thomson coefficient in terms of the volumetric properties of a fluid.\n\nTo proceed further, the starting point is the fundamental equation of thermodynamics in terms of enthalpy; this is\n\nNow \"dividing through\" by d\"P\", while holding temperature constant, yields\n\nThe partial derivative on the left is the isothermal Joule–Thomson coefficient, formula_49, and the one on the right can be expressed in terms of the coefficient of thermal expansion via a Maxwell relation. The appropriate relation is \n\nwhere \"α\" is the cubic coefficient of thermal expansion. Replacing these two partial derivatives yields\n\nThis expression can now replace formula_49 in the earlier equation for formula_1 to obtain\n\nThis provides an expression for the Joule–Thomson coefficient in terms of the commonly available properties heat capacity, molar volume, and thermal expansion coefficient. It shows that the Joule–Thomson inversion temperature, at which formula_1 is zero, occurs when the coefficient of thermal expansion is equal to the inverse of the temperature. Since this is true at all temperatures for ideal gases (see expansion in gases), the Joule–Thomson coefficient of an ideal gas is zero at all temperatures.\n\nIt is easy to verify that for an ideal gas defined by suitable microscopic postulates that \"αT\" = 1, so the temperature change of such an ideal gas at a Joule–Thomson expansion is zero.\nFor such an ideal gas, this theoretical result implies that:\n\nThis rule was originally found by Joule experimentally for real gases and is known as Joule's second law. More refined experiments of course found important deviations from it.\n\n\n\n"}
{"id": "18761742", "url": "https://en.wikipedia.org/wiki?curid=18761742", "title": "List of glaciers of Chile", "text": "List of glaciers of Chile\n\nThe glaciers of Chile cover 2.7% (20,188 km) of the land area of the country, excluding Antártica Chilena, and have a considerable impact on its landscape and water supply. By surface 80% of South America’s glaciers lie in Chile. Glaciers develop in the Andes of Chile from 27˚S southwards and in a very few places north of 18°30'S in the extreme north of the country: in between they are absent because of extreme aridity, though rock glaciers formed from permafrost are common. The largest glaciers of Chile are the Northern and Southern Patagonian Ice Fields. From a latitude of 47° S and south some glaciers reach sea level.\n\nApart from height and latitude, the settings of Chilean glaciers depend on precipitation patterns; in this sense two different regions exist: the Dry Andes and the Wet Andes.\n\nThis is a list of the ice fields of Chile.\n\n\n\n\n"}
{"id": "11958207", "url": "https://en.wikipedia.org/wiki?curid=11958207", "title": "Lorna Salzman", "text": "Lorna Salzman\n\nLorna Salzman (Lorna Jackson) has been an American environmental activist, writer, lecturer, and organizer since the mid-1960s. She was a candidate for the 2004 presidential nomination of the Green Party of the United States.\n\nSalzman is a graduate of Cornell University. She is a member of the New York Academy of Sciences, and in 2000 she received the international Earth Day Award from the Earth Society Foundation for her committed environmental work.\n\nShe was married to Eric Salzman until his death in 2017. Their two daughters are poet Eva Salzman and composer/songwriter Stephanie Salzman.\n\n"}
{"id": "4215325", "url": "https://en.wikipedia.org/wiki?curid=4215325", "title": "Malia altar stone", "text": "Malia altar stone\n\nThe Malia altar stone is a stone slab bearing an inscription in Cretan hieroglyphs. It was found by a farmer near Malia, Crete. Chapouthier describes the find from an archeologist point of view. \nOlivier and Godard (1996) present several photographs of the Malia altar stone, which they list as item 328 in their inventory of Cretan hieroglyphs inscriptions. \n\nThe stone has a cuplike cavity and is thought to be a Minoan altar stone. The side of the Malia altar stone contains an inscription with sixteen glyphs. The inscription is the only known instance of Cretan hieroglyphs on stone and is significant as one of the longest Cretan hieroglyphs inscriptions. Of the sixteen glyphs of the inscription, three occur twice each. Some of the glyphs show similarities with those of the Arkalochori Axe and the Phaistos Disc. Revesz gives a translation of the text and discusses earlier translation attempts. \n\n"}
{"id": "2164334", "url": "https://en.wikipedia.org/wiki?curid=2164334", "title": "Margaret Meagher", "text": "Margaret Meagher\n\nBlanche Margaret Meagher, (January 27, 1911 – February 25, 1999) was a Canadian diplomat and, in 1958, was appointed Canada's first woman ambassador.\n\nBorn in Halifax, Nova Scotia, she was the Canadian ambassador to Israel (1958 to 1961), Austria (1962 to 1966) and Sweden (1969 to 1973). She also served in Mexico and England. She was also high commissioner to Cyprus, Kenya and Uganda, and chaired the International Atomic Energy Agency Board of Governors.\n\nIn 1974, she was made an Officer of the Order of Canada.\n\n"}
{"id": "17095683", "url": "https://en.wikipedia.org/wiki?curid=17095683", "title": "Mărișelu Wind Farm", "text": "Mărișelu Wind Farm\n\nThe Mărișelu Wind Farm is a proposed wind power project in Cluj County, Romania. It will have 100 individual wind turbines, with a nominal output of around 3 MW which will deliver up to 300 MW of power, enough to power over 180,000 homes, with a capital investment required of approximately US$230 million.\n"}
{"id": "14668818", "url": "https://en.wikipedia.org/wiki?curid=14668818", "title": "Oil megaprojects (2016)", "text": "Oil megaprojects (2016)\n\nThis page summarizes projects that propose to bring more than of new liquid fuel capacity to market with the first production of fuel beginning in 2016. This is part of the Wikipedia summary of Oil Megaprojects.\n\nTerminology\n"}
{"id": "40908918", "url": "https://en.wikipedia.org/wiki?curid=40908918", "title": "Oiticica Dam", "text": "Oiticica Dam\n\nThe Oiticica Dam is an earth fill embankment dam between the rivers Seridó and Piranhas-Açu in the southwest Brazilian state of Rio Grande do Norte. Its construction began in 2013 and is scheduled for completion in August 2015. It is projected to be the third largest man-made dam in Rio Grande do Norte.\n"}
{"id": "10800577", "url": "https://en.wikipedia.org/wiki?curid=10800577", "title": "Pack station", "text": "Pack station\n\nA pack station is the base of operations for transporting freight via pack animals in areas that do not allow for other forms transportation, either due to difficult access or use restrictions as defined in Wilderness Act. The station facilitates the transition from mechanized transportation to pack animals, and necessarily includes a corral for the animals and sometimes a stock loading ramp. In some places there may also be a barn or other structure to house feed and tack, and a loading dock or shelter for the items to be transported. In locations on private land, there may be a business office on site. \n\nThe term \"pack station\" is most often used in California in the Sierra Nevada mountains. In other parts of the USA, outfitters may simply refer to a permanent or semi-permanent trailhead or wilderness camp as a \"station\" or \"outfitter camp.\"\n\nOne wrangler on horseback can usually handle up to five pack mules, who are tethered together in a line called a pack string.\n\nMany commercial outfitters today support recreation activities such as camping and fishing trips and hunting expeditions. Government agencies such as the U.S. Forest Service and the National Park Service, as well as a few commercial outfitters have pack operations to transport construction materials, trail tools, and fire fighting equipment into wilderness areas.\n\nIn either case, the process and techniques used are very similar to those developed prior to the era of motor vehicles. Customers arrange for a meeting time or a delivery time and drop their goods and supplies at the pack station. Packing services are charged by the pound or by the animal, typically with a minimum price depending on the distance from the pack station. Everything is weighed, sorted for each animal, then split 50/50 to get a balanced load on each side of the animal; an unbalanced load will cause the saddle to slide to the heavy side, causing discomfort to the animal, and potentially inviting disaster. The average mule can carry as much as . A mammoth donkey can carry up to and standard donkey limit is . However, most pack station mule loads are limited to \n\nOutfitters generally operate within the boundaries of public land and are required to maintain an outfitters permit. USDA Forest Service calls this a special-use permit. A separate permit is required if the pack station or wilderness caches are located on public land.\n\n\n"}
{"id": "6275558", "url": "https://en.wikipedia.org/wiki?curid=6275558", "title": "Paper density", "text": "Paper density\n\nPaper density is its mass per unit volume. \"ISO 534:2011, Paper and board — Determination of thickness, density and specific volume\", indicates it is expressed in grams per cubic centimeter.\n\nThe density can be calculated by dividing the grammage of paper by its caliper.\n\n\n"}
{"id": "14626877", "url": "https://en.wikipedia.org/wiki?curid=14626877", "title": "Paracrystalline", "text": "Paracrystalline\n\nParacrystalline materials are defined as having short- and medium-range ordering in their lattice (similar to the liquid crystal phases) but lacking crystal-like long-range ordering at least in one direction.\n\nOrdering is the regularity in which atoms appear in a predictable lattice, as measured from one point. In a highly ordered, perfectly crystalline material, or single crystal, the location of every atom in the structure can be described exactly measuring out from a single origin. Conversely, in a disordered structure such as a liquid or amorphous solid, the location of the nearest and, perhaps, second-nearest neighbors can be described from an origin (with some degree of uncertainty) and the ability to predict locations decreases rapidly from there out. The distance at which atom locations can be predicted is referred to as the correlation length formula_1. A paracrystalline material exhibits correlation somewhere between the fully amorphous and fully crystalline.\n\nThe primary, most accessible source of crystallinity information is X-ray diffraction and cryo-electron microscopy, although other techniques may be needed to observe the complex structure of paracrystalline materials, such as fluctuation electron microscopy in combination with density of states modeling of electronic and vibrational states. Scanning transmission electron microscopy can provide real-space and reciprocal space characterization of paracrystallinity in nanoscale material, such as quantum dot solids.\n\nThe scattering of X-rays, neutrons and electrons on paracrystals is quantitatively described by the theories of the ideal and real paracrystal.\n\nRolf Hosemann’s definition of an ideal paracrystal is: \"The electron density distribution of any material is equivalent to that of a paracrystal when there is for every building block one ideal point so that the distance statistics to other ideal points is identical for all of these points. The electron configuration of each building block around its ideal point is statistically independent of its counterpart in neighboring building blocks. A building block corresponds then to the material content of a cell of this \"blurred\" space lattice, which is to be considered a paracrystal.\"\n\nNumerical differences in analyses of diffraction experiments on the basis of either of these two theories of paracrystallinity can often be neglected.\n\nJust like ideal crystals, ideal paracrystals extend theoretically to infinity. Real paracrystals, on the other hand, follow the empirical α*-law, which restricts their size. That size is also indirectly proportional to the components of the tensor of the paracrystalline distortion. Larger solid state aggregates are then composed of micro-paracrystals.\n\nThe words \"paracrystallinity\" and \"paracrystal\" were coined by the late Friedrich Rinne in the year 1933. Their German equivalents, e.g. \"Parakristall\", appeared in print one year earlier.\n\nThe paracrystalline model is a revision of the Continuous Random Network model first proposed by W. H. Zachariasen in 1932. One type of paracrystal model contains highly strained, microcrystalline crystallites surrounded by fully amorphous material. This is a higher energy state than the continuous random network model. The important distinction between this model and the microcrystalline phases is the lack of defined grain boundaries and highly strained lattice parameters, which makes calculations of molecular and lattice dynamics difficult. A general theory of paracrystals has been formulated in a basic textbook, and then further developed/refined by various authors.\n\nThe paracrystal model has been useful, for example, in describing the state of partially amorphous semiconductor materials after deposition. It has also been successfully applied to synthetic polymers, liquid crystals, biopolymers, quantum dot solids, and biomembranes.\n\n"}
{"id": "31175065", "url": "https://en.wikipedia.org/wiki?curid=31175065", "title": "Parchment craft", "text": "Parchment craft\n\nParchment craft, also known as Pergamano, is the art of embellishing and decorating parchment paper (or vellum paper) through the use of techniques such as embossing, perforating, stippling, cutting and coloring.\n\nParchment Craft has been predominantly used in the making of cards (religious devotional cards, greeting cards and gift cards) but the techniques are being applied to related items such as bookmarks and picture frames as well as 3-dimensional sculptural paper projects such as ornaments and boxes.\n\nHistorians believe that parchment craft originated as an art form in Europe during the 15th or 16th century. Parchment craft at that time occurred principally in Catholic communities, where crafts persons created lace-like items such as devotional pictures and communion cards. The craft developed over time, with new techniques and refinements being added. Until the 16th century, parchment craft was a European art form. However, missionaries and other settlers relocated to South America, taking parchment craft with them. As before, the craft appeared largely among the Catholic communities. Often, young girls receiving their First Communion received gifts of handmade parchment crafts.\n\nAlthough the invention of the printing press led to a reduced interest in hand made cards and items, by the 18th century, people were regaining interest in detailed handwork. Parchment cards became larger in size and crafters began adding wavy borders and perforations. In the 19th century, influenced by French romanticism, parchment crafters began adding floral themes and cherubs and hand embossing.\n\nThere are many tools involved in parchment craft; each has its specific purpose and use.\n\nDesigns are traced onto parchment using a \"mapping pen\" and acrylic ink (usually white, but various colors are used to accent purposes). A mapping pen uses a nib which is dipped in ink.\n\nDesigns can also be traced onto the parchment using a white pencil. This technique is usually used for very fine lines that will be directly traced over during the embossing process. The white pencil is used in this case because if one was to trace directly over an inked line, it would become black.\n\nEmbossing tools come in many sizes. The larger the tool the softer the embossing and the \"greyer\" the color of the embossed shape; the smaller the tool the \"whiter\" and more \"satiny\" the color of the embossed shape. The tips of these tools also are made with different materials, some plastic – for lighter embossing – and some steel – for the brighter whites. The tools range from \"large ball\" to \"extra fine ball\" and a \"stylus\" (for very fine lines and intricate details).\n\nNeedle tools are sometimes used for embossing details, but mostly are used for perforating to make decorative, lace-like patterns. Needle tools have either single or multiple points and are used for different purposes. A single needle tool is used for embossing or stippling. A 2 needle tool is used for even perforations for marking areas which are to be cut with scissors. A 4 needle tool is square-shaped and a necessity for classic lace patterns. 3,5,7 and half-circle needle tools are used for decorative additions to lace patterns. A scissors is used in combination with needle tools to cut crosses and slots into the patterning.\n\nThese pads are to be used as support whenever pressure is placed upon the parchment. For embossing a soft pad is required to provide even support. For perforating a felt pad is used.\n\nTo color parchment craft work many media can be used such as: \n\nThis is the first step in any Parchment Craft project. Tracing creates the guidelines for the areas of the design that are to be embossed. Tracing is done using the mapping pen and ink. Parchment has two sides-one with a smooth surface and one with a rough surface; tracing is done on the side with the rough surface because the ink more easily adheres to this type of surface. When tracing the mapping pen should be allowed to glide easily over the parchment, no pressure should be used as this increases the amount of ink that is dispersed from the nib and may also create an unintentional embossment.\n\nEmbossing is the process of creating raised relief within one's design. Embossing is used to create both concave and convex shapes by alternating the side on which the embossing is performed. Using the embossing tools and appropriate embossing pad, the parchment is rubbed backwards and forwards or side to side in parallel movements with increasing downward pressure so that the parchment can be evenly stretched. A finished embossment will be satin-white in color and will contrast with the translucency of the parchment paper. Color can be varied by moderating the level to which the shape is embossed. Fine lines and hatching can be obtained by embossing with the stylus tool.\n\nThe single needle tool is used for stippling. Stippling is a technique used for generating a matte white surface to areas or for fine detail work such as the centers of flowers. To create a matte white surface the parchment is first lightly embossed and then small holes are perforated on the parchment in very close proximity. The perforation is done using a cardboard pad rather than a felt pad so that the needle will not pierce through the parchment, but rather create an embossed dot.\n\nPerforating is used to create decorative and special lace patterns in Parchment Craft. Minimal tracing is required when perforating; designs are usually created by taping the parchment over a pattern and perforating directly over it. Various needle tools are used in the perforating technique. Perforating is also used to create edging and borders or to denote guidelines for cutting.\n\nCutting is used in combination with perforating. Cutting is used with the 2 needle tool to remove shapes from the design; it is used with the 4 needle tool to create crosses and strips within a lace pattern.\n\nColoring is a technique that gained popularity in Parchment Craft in the 20th century; before this, Parchment Craft was originally only white work. There are many methods for coloring Parchment Craft work. One of the most popular is \"dorsing\". Dorsing creates a soft background color for embossed shapes or the areas around them. Color is applied in the dorsing process using oil pastels or Dorso crayons which are rubbed on and then blended using paper toweling and odorless mineral spirits or an oil based medium such as linseed or lavender oil. For coloring that is applied to the parchment in entirety, the coloring is done before any tracing or embossing. If color is to be added to small areas, dorsing would be applied in a similar manner to specific areas before the embossing process. Dorsing can also be done using other coloring media such as felt-tip pens/markers, watercolor pencils, acrylic paints or inks.\n\n"}
{"id": "37669541", "url": "https://en.wikipedia.org/wiki?curid=37669541", "title": "Passive heave compensation", "text": "Passive heave compensation\n\nPassive heave compensation is a technique used to reduce the influence of waves upon lifting and drilling operations. A simple passive heave compensator (PHC) is a soft spring which utilizes spring isolation to reduce transmissibility to less than 1. PHC differs from AHC by not consuming external power.\n\nThe main principle in PHC is to store the energy from the external forces (waves) influencing the system and dissipate them or reapply them later. Shock absorbers or drill string compensators are simple forms of PHC, so simple that they are normally named heave compensators, while \"passive\" is used about more sophisticated hydraulic or mechanical systems.\n\nA typical PHC device consists of a hydraulic cylinder and a gas accumulator. When the piston rod extends it will reduce the total gas volume and hence compress the gas that in turn increases the pressure acting upon the piston. The compression ratio is low to ensure low stiffness. A well designed PHC device can achieve efficiencies above 80 percent.\n\nPHC is often used on offshore equipment that is at or linked to the seabed. Not requiring external energy, PHC may be designed as a fail-safe system reducing the wave impact on sub-sea operations. PHC may be used along with active heave compensation to form a semi-active system.\n\nThe PHC device is in this calculation connected to the crane hook. Newton`s second law is used to describe the acceleration of the payload:\nformula_1\n\nWhere\nformula_2 - is the mass of the load underneath the PHC device\nformula_3 - is the added mass of the load underneath the PHC device\nformula_4 - is the acceleration of the mass of the load underneath the PHC device\nformula_5 - is the stiffness of the PHC device\nformula_6 - is the vertical position of the mass underneath the PHC device\nformula_7 - is the wave amplitude\nformula_8 - is the angular wave frequency\nformula_9 - is time\n\nIf we ignore the transient solution we will find that the ratio between the amplitude of the load and the wave amplitude is:\nformula_10\n\nTo simplify the expression it is common to introduce formula_11 as the systems natural frequency, defined as:\nformula_12\n\nWe then get the following expression for the ratio:\nformula_13\n\nThe transmissibility formula_14 is defined as:\nformula_15\n\nFinally the efficiency is defined as:\nformula_16\n\nThe stiffness of a PHC device is given by:\nformula_17\n\nWhere\nformula_18 - is the gas pressure at equilibrium stroke\nformula_19 - is the piston area\nformula_20 - is the stroke length\nformula_21 - is the compression ratio\nformula_22 - is the adiabatic coefficient\n\nThe product formula_23 corresponds to the submerged weight of the payload. As can be seen from the expression it is clear that low compression ratios as well as long stroke length gives low stiffness.\n\n"}
{"id": "12327015", "url": "https://en.wikipedia.org/wiki?curid=12327015", "title": "Persoz pendulum", "text": "Persoz pendulum\n\nA Persoz pendulum is a device used for measuring hardness of materials. The instrument consists of a pendulum which is free to swing on two balls resting on a coated test panel. The pendulum hardness test is based on the principle that the amplitude of the pendulum's oscillation will decrease more quickly when supported on a softer surface. The hardness of any given coating is given by the number of oscillations made by the pendulum within the specified limits of amplitude determined by accurately positioned photo sensors. An electronic counter records the number of swings made by the pendulum\n\nThe pendulum consists of balls which rest on the coating under test and form the fulcrum. The Persoz pendulum is very similar to the Konig pendulum. Both employ the same principle, that is the softer the coating the more the pendulum oscillations are damped and the shorter the time needed for the amplitude of oscillation to be reduced by a specified amount. The two pendulums differ in shape, mass and oscillation time, and there is no general relationship between the results obtained using the two pieces of equipment. In either case, the test simply involves noting the time in seconds for the amplitude of swing to decrease from either 6 to 3 degrees (Konig pendulum) or 12 to 4 degrees (Persoz pendulum).\n"}
{"id": "31808123", "url": "https://en.wikipedia.org/wiki?curid=31808123", "title": "Piero Giorgio Bordoni", "text": "Piero Giorgio Bordoni\n\nPiero Giorgio Bordoni (Rome, 18 July 1915 – 19 September 2009) was an Italian physicist, who first measured the anelastic relaxation effect (dissipation of elastic energy and softening of the elastic modulus), named after him, due to the stress induced motion of dislocations in metals. That experiment, together with a similar one by T.S. Kê, opened the way to the study of the dynamics of dislocations in solids. Those were also the first experiments of anelasticity in solids, a branch of physics studying defects, excitations and phase transitions in condensed matter, first systematised by Clarence Zener.\n\nHe received the Degree in Electrotechnical Engineering at the Sapienza University of Rome in 1937, following the steps of his father, Ugo Bordoni, Engineering Professor in the same Faculty (after whom the Fondazione Ugo Bordoni was named in Italy, established in 1952 for supporting research and applications of telecommunications).\n\nHis interest in acoustics was first aroused, still a student in 1936, by the invitation to join the newly founded Institute of Ultracoustics (later of Electroacoustics and then of Acoustics) of Consiglio Nazionale delle Ricerche (CNR), then directed by Orso Mario Corbino.\n\nDuring the Second World War, he was initially in the Aeronautics and head of the Laboratory of Electroacoustics in Guidonia, where he worked at acoustically triggered torpedoes for the Navy.\n\nIn 1944 he became researcher of the Institute of Ultracoustics.\n\nIn 1947/8 he obtained a fellowship from CNR to stay 8 months at the Massachusetts Institute of Technology of Boston for studying the acoustic properties of lead down to liquid helium temperatures in the laboratory of John C. Slater, and discovered the anelastic effect due to dislocations, later named \"Bordoni relaxation\" or \"Bordoni peak\".\n\nIn 1949 he became assistant professor of Rational Mechanics at the Engineering Faculty of the Sapienza University of Rome and left the Institute of Acoustics. In 1954 became Professor of Mathematical Physics at the University of Pisa, where he remained until 1962, when he passed to the Faculty of Engineering of the University of Rome. He taught in Rome until 1985, but also carried out experimental scientific activity on the anelasticity of metals containing dislocations and interstitial hydrogen at the Institute of Acoustics \"O.M. Corbino\" of CNR, until 1998.\n\nFor his scientific achievements, he received an honorary degree in physics from the University of Perugia in 1988 and the Zener Prize in 1993. He had many interests besides physics, like ancient history and languages, and wrote witty sonnets in the style of the ‘Romanesco’ poet Giuseppe Gioachino Belli, inspired by episodes at the university and in the life.\n\nAfter an initial activity on electroacoustic transducers, he started measuring the complex dynamic elastic modulus of metals at low temperature, so carrying out the earliest researches of anelasticity in solids. During a stay in 1948 in the laboratory of John C. Slater at the Massachusetts Institute of Technology of Boston, where liquid helium for reaching very low temperatures was available, he discovered an anelastic relaxation process in Pb immediately recognized as due to the stress induced motion of dislocations. That type of relaxation, due to the motion of dislocations without the participation of mobile point defects, is known as \"Bordoni relaxation\"’ or \"Bordoni peak\", since it appears as a peak in the elastic energy dissipation versus temperature. Bordoni pursued this type of investigations, extending it to other metals with face-centered cubic structure, like copper, silver, gold, palladium, platinum but also body-centered cubic structure, like niobium, and with anisotropic hexagonal structure, where the simple change of temperature causes plastic phenomena. Models of increasing sophistication have been developed to explain the complex phenomenology of the Bordoni relaxation, but there is not yet general consensus on the detailed microscopic mechanisms, and this is still now a field of active research.\n\nBordoni also contributed to developing the instrumentation for anelastic experiments on resonanting samples, and to other aspects of anelasticity. In the 1980s he turned to the study of the interaction of dislocations with interstitial hydrogen, acting as a mobile point defect, and hence on reliable methods for electrolytically introducing H in the samples, controlling the occurrence of hydride precipitation.\n\n"}
{"id": "57554421", "url": "https://en.wikipedia.org/wiki?curid=57554421", "title": "Pomeranchuk instability", "text": "Pomeranchuk instability\n\nThe Pomeranchuk Instability is an instability in the shape of the Fermi surface of a material with interacting fermions, causing Landau’s Fermi liquid theory to break down. It occurs when a Landau parameter in Fermi liquid theory has a sufficiently negative value, causing deformations of the Fermi surface to be energetically favourable. It is named after the Soviet physicist Isaak Pomeranchuk.\n\nIn a Fermi liquid, renormalized single electron propagators (ignoring spin) are\nwhere capital momentum letters denote four vectors formula_2 and the Fermi surface has zero energy. The poles of formula_3 determine the quasiparticle energy-momentum dispersion relation. One can define the four-point vertex function formula_4 as the diagram with two incoming electrons of momentum formula_5 and two outgoing electrons of momentum formula_6 and amputated external lines: formula_7formula_8formula_9.The 2-particle-irreducible formula_10 is the sum of diagrams contributing to formula_11 that cannot be disconnected after cutting two electron propagators. When formula_12 is very small (the regime of interest here), the \"T\"-channel becomes dominant over the \"S\" and \"U\" channels, so the Dyson equation gives \n\nformula_13\n\nThen, matrix manipulations (treating these quantities like infinite matrices with indices labelled by pairs formula_14 and formula_15) showformula_16 is non-singular and satisfies matrix equation formula_17, where formula_18.The normalized Landau parameter is defined as formula_19, where formula_20 is Fermi surface density of states. Energy is approximated by the functional \n\nformula_21\n\nwhere formula_22 for momenta formula_23 near the Fermi momentum formula_24.\n\nIn a 3D isotropic Fermi liquid, consider small density fluctuations formula_25 where formula_26 and the infinitesimal function formula_27 parametrises the fluctuation (formula_28 denote the spherical harmonics). Plugging this into the energy functional, and assuming formula_29 is much smaller than formula_24,\ngives\nwhere formula_34 and formula_35 is the formula_36-th Legendre polynomial.Positive definite energy functional requires the Pomeranchuk stability criterion, formula_37; otherwise the Fermi surface distortion formula_29 will grow unbounded until the model breaks down in what is called the Pomeranchuk instability.\n\nIn 2D, a similar analysis, with circular wave fluctuations formula_39 instead of spherical harmonics and Chebyshev polynomials instead of Legendre polynomials, shows the Pomeranchuk constraint to be formula_40. In non-isotropic materials, the same qualitative result is true - for sufficiently negative Landau parameters, the Fermi surface is spontaneously destroyed with unstable fluctuations.\n\nThe point at which formula_41 is of much theoretical interest as it indicates a quantum phase transition from a Fermi liquid to a different state of matter, and above zero temperature a quantum critical state exists.\n\nMany physical quantities in Fermi liquid theory are simple expressions of components of Landau parameters. A few standard ones are listed here; they diverge or become unphysical beyond the quantum critical point.\n\nIsothermal compressibility: formula_42\n\nEffective mass: formula_43\n\nSpeed of first sound: formula_44\n\nZero sound describes how the localized fluctuations of the momentum density function formula_45 propagate through space and time. Just as the quasiparticle dispersion is given by the pole of the one-particle propagator, the zero sound dispersion relation is given by the pole of the T-channel of the vertex function formula_46 near small formula_47. Physically, this describes the propagation of an electron hole pair, which is responsible for the fluctuations in formula_45. \nFrom the relation formula_17 and ignoring the contributions of formula_50 for formula_51, the zero sound spectrum is given by the four-vectors formula_52 satisfying formula_53, or\n\nformula_54\n\nwhere formula_55. \n\nWhen formula_56, for each real formula_57 there is a real solution for formula_58, corresponding to a real zero sound dispersion relation of oscillatory waves. \nWhen formula_59, for each real formula_57 there is a pure imaginary solution for formula_58, corresponding to an exponential change in zero sound amplitude over time. For formula_62, formula_63 at all real formula_57, so amplitude is damped. But for formula_65, formula_66 for sufficiently small formula_57, implying exponential explosion of any low-momentum zero sound fluctuation. This is a manifestation of the Pomeranchuk instability.\n\nPomeranchuk instabilities at formula_68 are shown to not exist in non-relativistic systems by . However, instabilities at formula_69 have interesting solid state applications. From the form of spherical harmonics formula_70 (or formula_71 in 2d), the Fermi surface is distorted into an ellipsoid (or ellipse).Specifically, in 2d, the quadrupole moment order parameter formula_72has nonzero vacuum expectation value in the formula_69 Pomeranchuk instability. The Fermi surface has eccentricity formula_74 and spontaneous major axis orientation formula_75. Gradual spatial variation in formula_76 forms gapless Goldstone modes, forming a nematic liquid statistically analogous to a liquid crystal. Oganesyan et al.'s analysis of a model interaction between quadrupole moments predicts damped zero sound fluctuations of the quadrupole moment condensate for waves oblique to the ellipse axes. \n\nThe 2d square tight-binding Hubbard Hamiltonian with next-to-nearest neighbour interaction has been found by Halboth and Metzner to display instability in susceptibility of \"d\"-wave fluctuations under renormalization group flow. Thus, the Pomeranchuk instability is suspected to explain the experimentally measured anisotropy in cuprate superconductors such as LSCO and YBCO.\n\n"}
{"id": "1016489", "url": "https://en.wikipedia.org/wiki?curid=1016489", "title": "Raja Ramanna", "text": "Raja Ramanna\n\nRaja Ramanna (28 January 1928 – 24 September 2004) was an Indian physicist who is best known for his role in India's nuclear program during its early stages.\n\nHaving joined the nuclear program in 1964, Ramanna worked under Homi Jehangir Bhabha, and later became the director of this program in 1967. Ramanna expanded and supervised scientific research on nuclear weapons and was the first directing officer of the small team of scientists that supervised and carried out the test of the nuclear device, under the codename \"Smiling Buddha\", in 1974.\n\nRamanna was associated with and directed India's nuclear program for more than four decades, and also initiated industrial defence programmes for the Indian Armed Forces. He was a recipient of Padma Vibhushan, India's second highest civilian decoration, in honour of his services to build India's nuclear programme. Ramanna died in Mumbai in 2004 at the age of 79.\n\nRaja Ramanna was born in beginning of 1925 to Rukmini and Ramanna in Tumkur, in the princely state of Mysore. The parents having recognised his talent for music early in life were instrumental in introducing him to classical European music. Beginning his studies at Bishop Cotton Boys' School, Bangalore, where he mostly studied literature and classical music, he later attended Madras Christian College and resided at St.Thomas's Hall where he continued his interests in arts and literature but soon shifted back to physics. At Madras Christian College, Ramanna obtained a BSc in Physics and gained a Ba degree in Classical music in 1947.\n\nIn 1947 Ramanna went on to attend Bombay University where he gained his MSc in Physics, followed by M.Mus. in Music theory. Ramanna was awarded a Commonwealth Scholarship, and travelled to Great Britain in 1952 to complete his doctorate. Ramanna attended London University's King's College and enrolled in a doctoral programme there. In 1954, Raja Ramanna obtained a PhD in Nuclear Physics and also a LRSM from King's College London. In the United Kingdom, Ramanna was invited to do his research at the Atomic Energy Research Establishment (AERE) where he gained expertise in nuclear fuel cycles and reactor designing. While in the UK, Ramanna continued his interest in European music and Western philosophy, attending Opera and Orchestra performances every week.\n\nEuropean music and philosophy remained a lifelong passion for Ramanna, and after returning to India, Ramanna accomplished himself by performing classical European music at many public concerts in India and abroad. Ramannad also had a keen ear for Indian classical music. His musical talents also received wide appreciation in neighbouring Pakistan. In 1956, Ramanna was invited by the National College of Arts and National Academy of Performing Arts to perform and lecture on the classical piano with a live ensemble and received jubilant praise and honour for his performance.\n\nRamanna was one of the secretive personalities surrounding the Indian nuclear programme, a programme started and envisioned by Jawaharlal Nehru in 1947, and being directed by Homi J. Bhabha. After his doctorate in physics, Ramanna returned in 1954 to India, where he joined the senior technical staff of Bhabha Atomic Research Centre (BARC), where he worked under Homi J. Bhabha in classified nuclear weapons projects.\n\nWhile Bhaba dedicated to develop this programme, Ramanna inducted by to choose the preferable nuclear test site to carry out the weapon-testing experiments. The exact dates are unknown, but Ramanna chose and began the underground construction of nuclear test site at an Indian Army base, the Pokhran Test Range (PTR). After the disastrous death of Homi Bhaba, Ramanna was immediately elevated to become the directing officer of this programme. Ramanna, serving as the CDO of BARC, began to take initiate to develop the first nuclear weapon. At BARC, the initial designing of nuclear weapon was completed under his guidance and the necessary nuclear weapons' explosive material for this weapon was completed under Ramanna by 1970. As the first nuclear device was completed and developed under his guidance, Ramanna went to Indian Prime Minister's Office, where he had notified Indian premier Indira Gandhi about the successful development of the nuclear device.\n\nIn 1974, Ramanna and other officials of the BARC verbally notified Indira Gandhi that India was ready to conduct the test of its small miniature nuclear device. Indira Gandhi verbally gave permission to Ramanna to carry out the test, and preparation was taken under Ramanna. Ramanna immediately travelled to Pokhran to pay a visit to the nuclear site that was constructed under his guidance. Preparations were completed under extreme secrecy and the first nuclear device was flown from Trombay to Pokhran Test Range with Ramanna. Ramanna and his team installed the nuclear device in the nuclear test site and necessary preparations were done before Indira Gandhi's visit to his site. In the morning in May 1974, Ramanna conducted the first test of a small nuclear device under codename \"Smiling Buddha\". Pictures of Indira Gandhi inspecting the aftermath of the explosion site were flashed on front pages of newspapers in India and the world over with Ramanna and Dr. Homi Sethna, India's top nuclear scientist duo, by her side. Following this achievement, Ramanna gained international fame and was also honoured with India's second highest civilian award in 1975 by the Indira Gandhi's administration.\n\nIn 1978, Saddam Hussein approached Ramanna for help to build an Iraqi nuclear bomb. The offer came while Ramanna was in Baghdad for a week as Saddam's personal guest. He was given a tour of the capital and Iraq's main nuclear facility at Tuwaitha. At the end of the trip, Saddam invited the scientist to his office and told him: \"You have done enough for your country; don't go back. Stay here and take over our nuclear programme. I will pay you whatever you want.\" Ramanna was shocked and scared by the Iraqi proposal. He reportedly could not sleep that night, worried that he might never see his homeland again. He took the next flight out.\n\nLater in his career, Ramanna advocated for the strict policies to prevent nuclear proliferation. Ramanna also travelled to Pakistan, where he attended the annual International Physics Conference to deliver a lecture on nuclear physics, notably lectures on nuclear force. Ramanna began lobbying for peace process between India and Pakistan, and was a leading force to prevent nuclear escalation in the region. Later in the 1980s and 1990s, Ramanna served as Director General of the Defence Research and Development Organisation (DRDO) and as scientific adviser to the Defence Minister of India in 2000. He was a member of the Defence Research & Development Service (DRDS). Ramanna also joined the International Atomic Energy Agency in 1984 where he served as the President of the 30th General Conference of the IAEA.\n\nA multi-faceted personality, Ramanna was a gifted musician, and could play the piano as dextrously as he could speak about atomic energy. Music was close to his heart, and one of the two books he wrote was The Structure of Music in Raga And Western Systems (1993). The other was his autobiography, entitled Years of Pilgrimage (1991).\n\nIn 1990, Ramanna was made Union minister of State for defence by V.P. Singh administration. He was a nominated member of the Rajya Sabha from 1997 to 2003. Dr. Ramanna was closely associated with the I.I.T. Bombay, having been chairman of the board of Governors for three consecutive terms from 1975 to 1984. In 2000, Ramanna was also the first director of National Institute of Advanced Studies, Bangalore.\n\n\n\n\n\n"}
{"id": "43948", "url": "https://en.wikipedia.org/wiki?curid=43948", "title": "Star formation", "text": "Star formation\n\nStar formation is the process by which dense regions within molecular clouds in The \"medium\" is present further soon.-->interstellar space, sometimes referred to as \"stellar nurseries\" or \"star-forming regions\", collapse and form stars. As a branch of astronomy, star formation includes the study of the interstellar medium (ISM) and giant molecular clouds (GMC) as precursors to the star formation process, and the study of protostars and young stellar objects as its immediate products. It is closely related to planet formation, another branch of astronomy. Star formation theory, as well as accounting for the formation of a single star, must also account for the statistics of binary stars and the initial mass function.\n\nA spiral galaxy like the Milky Way contains stars, stellar remnants, and a diffuse interstellar medium (ISM) of gas and dust. The interstellar medium consists of 10 to 10 particles per cm and is typically composed of roughly 70% hydrogen by mass, with most of the remaining gas consisting of helium. This medium has been chemically enriched by trace amounts of heavier elements that were ejected from stars as they passed beyond the end of their main sequence lifetime. Higher density regions of the interstellar medium form clouds, or \"diffuse nebulae\", where star formation takes place. In contrast to spirals, an elliptical galaxy loses the cold component of its interstellar medium within roughly a billion years, which hinders the galaxy from forming diffuse nebulae except through mergers with other galaxies.\n\nIn the dense nebulae where stars are produced, much of the hydrogen is in the molecular (H) form, so these nebulae are called molecular clouds. Observations indicate that the coldest clouds tend to form low-mass stars, observed first in the infrared inside the clouds, then in visible light at their surface when the clouds dissipate, while giant molecular clouds, which are generally warmer, produce stars of all masses. These giant molecular clouds have typical densities of 100 particles per cm, diameters of , masses of up to 6 million solar masses (), and an average interior temperature of 10 K. About half the total mass of the galactic ISM is found in molecular clouds and in the Milky Way there are an estimated 6,000 molecular clouds, each with more than . The nearest nebula to the Sun where massive stars are being formed is the Orion nebula, away. However, lower mass star formation is occurring about 400–450 light years distant in the ρ Ophiuchi cloud complex.\n\nA more compact site of star formation is the opaque clouds of dense gas and dust known as Bok globules; so named after the astronomer Bart Bok. These can form in association with collapsing molecular clouds or possibly independently. The Bok globules are typically up to a light year across and contain a few solar masses. They can be observed as dark clouds silhouetted against bright emission nebulae or background stars. Over half the known Bok globules have been found to contain newly forming stars.\nAn interstellar cloud of gas will remain in hydrostatic equilibrium as long as the kinetic energy of the gas pressure is in balance with the potential energy of the internal gravitational force. Mathematically this is expressed using the virial theorem, which states that, to maintain equilibrium, the gravitational potential energy must equal twice the internal thermal energy. If a cloud is massive enough that the gas pressure is insufficient to support it, the cloud will undergo gravitational collapse. The mass above which a cloud will undergo such collapse is called the Jeans mass. The Jeans mass depends on the temperature and density of the cloud, but is typically thousands to tens of thousands of solar masses. This coincides with the typical mass of an open cluster of stars, which is the end product of a collapsing cloud.\n\nIn \"triggered star formation\", one of several events might occur to compress a molecular cloud and initiate its gravitational collapse. Molecular clouds may collide with each other, or a nearby supernova explosion can be a trigger, sending shocked matter into the cloud at very high speeds. (The resulting new stars may themselves soon produce supernovae, producing self-propagating star formation.) Alternatively, galactic collisions can trigger massive starbursts of star formation as the gas clouds in each galaxy are compressed and agitated by tidal forces. The latter mechanism may be responsible for the formation of globular clusters.\n\nA supermassive black hole at the core of a galaxy may serve to regulate the rate of star formation in a galactic nucleus. A black hole that is accreting infalling matter can become active, emitting a strong wind through a collimated relativistic jet. This can limit further star formation. Massive black holes ejecting radio-frequency-emitting particles at near-light speed can also block the formation of new stars in aging galaxies. However, the radio emissions around the jets may also trigger star formation. Likewise, a weaker jet may trigger star formation when it collides with a cloud.\n\nAs it collapses, a molecular cloud breaks into smaller and smaller pieces in a hierarchical manner, until the fragments reach stellar mass. In each of these fragments, the collapsing gas radiates away the energy gained by the release of gravitational potential energy. As the density increases, the fragments become opaque and are thus less efficient at radiating away their energy. This raises the temperature of the cloud and inhibits further fragmentation. The fragments now condense into rotating spheres of gas that serve as stellar embryos.\n\nComplicating this picture of a collapsing cloud are the effects of turbulence, macroscopic flows, rotation, magnetic fields and the cloud geometry. Both rotation and magnetic fields can hinder the collapse of a cloud. Turbulence is instrumental in causing fragmentation of the cloud, and on the smallest scales it promotes collapse.\n\nA protostellar cloud will continue to collapse as long as the gravitational binding energy can be eliminated. This excess energy is primarily lost through radiation. However, the collapsing cloud will eventually become opaque to its own radiation, and the energy must be removed through some other means. The dust within the cloud becomes heated to temperatures of , and these particles radiate at wavelengths in the far infrared where the cloud is transparent. Thus the dust mediates the further collapse of the cloud.\n\nDuring the collapse, the density of the cloud increases towards the center and thus the middle region becomes optically opaque first. This occurs when the density is about . A core region, called the First Hydrostatic Core, forms where the collapse is essentially halted. It continues to increase in temperature as determined by the virial theorem. The gas falling toward this opaque region collides with it and creates shock waves that further heat the core.\n\nWhen the core temperature reaches about , the thermal energy dissociates the H molecules. This is followed by the ionization of the hydrogen and helium atoms. These processes absorb the energy of the contraction, allowing it to continue on timescales comparable to the period of collapse at free fall velocities. After the density of infalling material has reached about 10 g / cm, that material is sufficiently transparent to allow energy radiated by the protostar to escape. The combination of convection within the protostar and radiation from its exterior allow the star to contract further. This continues until the gas is hot enough for the internal pressure to support the protostar against further gravitational collapse—a state called hydrostatic equilibrium. When this accretion phase is nearly complete, the resulting object is known as a protostar.\n\nAccretion of material onto the protostar continues partially from the newly formed circumstellar disc. When the density and temperature are high enough, deuterium fusion begins, and the outward pressure of the resultant radiation slows (but does not stop) the collapse. Material comprising the cloud continues to \"rain\" onto the protostar. In this stage bipolar jets are produced called Herbig-Haro objects. This is probably the means by which excess angular momentum of the infalling material is expelled, allowing the star to continue to form.\n\nWhen the surrounding gas and dust envelope disperses and accretion process stops, the star is considered a pre–main sequence star (PMS star). The energy source of these objects is gravitational contraction, as opposed to hydrogen burning in main sequence stars. The PMS star follows a Hayashi track on the Hertzsprung–Russell (H–R) diagram. The contraction will proceed until the Hayashi limit is reached, and thereafter contraction will continue on a Kelvin–Helmholtz timescale with the temperature remaining stable. Stars with less than thereafter join the main sequence. For more massive PMS stars, at the end of the Hayashi track they will slowly collapse in near hydrostatic equilibrium, following the Henyey track.\n\nFinally, hydrogen begins to fuse in the core of the star, and the rest of the enveloping material is cleared away. This ends the protostellar phase and begins the star's main sequence phase on the H–R diagram.\n\nThe stages of the process are well defined in stars with masses around or less. In high mass stars, the length of the star formation process is comparable to the other timescales of their evolution, much shorter, and the process is not so well defined. The later evolution of stars are studied in stellar evolution.\n\nKey elements of star formation are only available by observing in wavelengths other than the optical. The protostellar stage of stellar existence is almost invariably hidden away deep inside dense clouds of gas and dust left over from the GMC. Often, these star-forming cocoons known as Bok globules, can be seen in silhouette against bright emission from surrounding gas. Early stages of a star's life can be seen in infrared light, which penetrates the dust more easily than visible light. \nObservations from the Wide-field Infrared Survey Explorer (WISE) have thus been especially important for unveiling numerous Galactic protostars and their parent star clusters. Examples of such embedded star clusters are FSR 1184, FSR 1190, Camargo 14, Camargo 74, Majaess 64, and Majaess 98.\n\nThe structure of the molecular cloud and the effects of the protostar can be observed in near-IR extinction maps (where the number of stars are counted per unit area and compared to a nearby zero extinction area of sky), continuum dust emission and rotational transitions of CO and other molecules; these last two are observed in the millimeter and submillimeter range. The radiation from the protostar and early star has to be observed in infrared astronomy wavelengths, as the extinction caused by the rest of the cloud in which the star is forming is usually too big to allow us to observe it in the visual part of the spectrum. This presents considerable difficulties as the Earth's atmosphere is almost entirely opaque from 20μm to 850μm, with narrow windows at 200μm and 450μm. Even outside this range, atmospheric subtraction techniques must be used.\nX-ray observations have proven useful for studying young stars, since X-ray emission from these objects is about 100–100,000 times stronger than X-ray emission from main-sequence stars. The earliest detections of X-rays from T Tauri stars were made by the Einstein X-ray Observatory. For low-mass stars X-rays are generated by the heating of the stellar corona through magnetic reconnection, while for high-mass O and early B-type stars X-rays are generated through supersonic shocks in the stellar winds. Photons in the soft X-ray energy range covered by the Chandra X-ray Observatory and XMM Newton may penetrate the interstellar medium with only moderate absorption due to gas, making the X-ray a useful wavelength for seeing the stellar populations within molecular clouds. X-ray emission as evidence of stellar youth makes this band particularly useful for performing censuses of stars in star-forming regions, given that not all young stars have infrared excesses. X-ray observations have provided near-complete censuses of all stellar-mass objects in the Orion Nebula Cluster and Taurus Molecular Cloud.\n\nThe formation of individual stars can only be directly observed in the Milky Way Galaxy, but in distant galaxies star formation has been detected through its unique spectral signature.\n\nInitial research indicates star-forming clumps start as giant, dense areas in turbulent gas-rich matter in young galaxies, live about 500 million years, and may migrate to the center of a galaxy, creating the central bulge of a galaxy.\n\nOn February 21, 2014, NASA announced a greatly upgraded database for tracking polycyclic aromatic hydrocarbons (PAHs) in the universe. According to scientists, more than 20% of the carbon in the universe may be associated with PAHs, possible starting materials for the formation of life. PAHs seem to have been formed shortly after the Big Bang, are widespread throughout the universe, and are associated with new stars and exoplanets.\n\nIn February 2018, astronomers reported, for the first time, a signal of the reionization epoch, an indirect detection of light from the earliest stars formed - about 180 million years after the Big Bang.\n\n\nStars of different masses are thought to form by slightly different mechanisms. The theory of low-mass star formation, which is well-supported by a plethora of observations, suggests that low-mass stars form by the gravitational collapse of rotating density enhancements within molecular clouds. As described above, the collapse of a rotating cloud of gas and dust leads to the formation of an accretion disk through which matter is channeled onto a central protostar. For stars with masses higher than about , however, the mechanism of star formation is not well understood.\n\nMassive stars emit copious quantities of radiation which pushes against infalling material. In the past, it was thought that this radiation pressure might be substantial enough to halt accretion onto the massive protostar and prevent the formation of stars with masses more than a few tens of solar masses. Recent theoretical work has shown that the production of a jet and outflow clears a cavity through which much of the radiation from a massive protostar can escape without hindering accretion through the disk and onto the protostar. Present thinking is that massive stars may therefore be able to form by a mechanism similar to that by which low mass stars form.\n\nThere is mounting evidence that at least some massive protostars are indeed surrounded by accretion disks. Several other theories of massive star formation remain to be tested observationally. Of these, perhaps the most prominent is the theory of competitive accretion, which suggests that massive protostars are \"seeded\" by low-mass protostars which compete with other protostars to draw in matter from the entire parent molecular cloud, instead of simply from a small local region.\n\nAnother theory of massive star formation suggests that massive stars may form by the coalescence of two or more stars of lower mass.\n\n"}
{"id": "2086689", "url": "https://en.wikipedia.org/wiki?curid=2086689", "title": "Sulfur–iodine cycle", "text": "Sulfur–iodine cycle\n\nThe sulfur–iodine cycle (S–I cycle) is a three-step thermochemical cycle used to produce hydrogen.\n\nThe S–I cycle consists of three chemical reactions whose net reactant is water and whose net products are hydrogen and oxygen. All other chemicals are recycled. The S–I process requires an efficient source of heat.\n\nThe three reactions that produce hydrogen are as follows:\n\nThe sulfur and iodine compounds are recovered and reused, hence the consideration of the process as a cycle. This S–I process is a chemical heat engine. Heat enters the cycle in high-temperature endothermic chemical reactions 2 and 3, and heat exits the cycle in the low-temperature exothermic reaction 1. The difference between the heat entering and leaving the cycle exits the cycle in the form of the heat of combustion of the hydrogen produced.\n\nThe characteristics of the S–I process can be described as follows:\n\nThe S–I cycle was invented at General Atomics in the 1970s.\nThe Japan Atomic Energy Agency (JAEA) has conducted successful experiments with the S–I cycle in the Helium cooled High Temperature Test Reactor, a reactor which reached first criticality in 1998, JAEA have the aspiration of using further nuclear high-temperature generation IV reactors to produce industrial scale quantities of hydrogen. (The Japanese refer to the cycle as the IS cycle.) Plans have been made to test larger-scale automated systems for hydrogen production. Under an International Nuclear Energy Research Initiative (INERI) agreement, the French CEA, General Atomics and Sandia National Laboratories are jointly developing the sulfur-iodine process. Additional research is taking place at the Idaho National Laboratory, in Canada, Korea and Italy.\n\nThe S–I cycle involves operations with corrosive chemicals at temperatures up to about . The selection of materials with sufficient corrosion resistance under the process conditions is of key importance to the economic viability of this process. The materials suggested include the following classes: refractory metals, reactive metals, superalloys, ceramics, polymers, and coatings.\nSome materials suggested include tantalum alloys, niobium alloys, noble metals, high-silicon steels, several nickel-based superalloys, mullite, silicon carbide (SiC), glass, silicon nitride (SiN), and others. Recent research on scaled prototyping suggests that new tantalum surface technologies may be a technically and economically feasible way to make larger scale installations.\n\nThe sulfur-iodine cycle has been proposed as a way to supply hydrogen for a hydrogen-based economy. It does not require hydrocarbons like current methods of steam reforming but requires heat from combustion, nuclear reactions, or solar heat concentrators.\n\n\n\n"}
{"id": "23165204", "url": "https://en.wikipedia.org/wiki?curid=23165204", "title": "Teardrop trailer (truck)", "text": "Teardrop trailer (truck)\n\nA teardrop trailer is an aerodynamically shaped semi-trailer with a curved-roof that differs in shape from the traditional rectangularly shaped trailer. The trailer is meant to be paired with a compatibly designed tractor unit leading to greater fuel efficiency of the tractor-trailer combination. As such, the \"teardrop\" phrase refers to the entire combination, not just the trailer. The curved shape of the roof decreases aerodynamic drag by smoothing airflow over the top, thus improving fuel efficiency.\n\nOne operator has reported the trailers save 2% in fuel in service that included significant low speed delivery operation and 5% in higher speed long-haul operation.\n\nAnother factor improving airflow is the gap between the tractor and trailer is minimized to lessen the amount of turbulence created by air flowing into the gap.\n\nTeardrop trailers are only available in the United Kingdom and manufactured by one company, Don-Bur Trailers & Bodies Ltd which has patented the design. The manufacturer claims they can save 11.3% in fuel.\n\nThe design echos streamlined shapes used in submarines, automobiles and camping trailers. The first streamlined car was the Tropfenwagen introduced in 1921, named after its teardrop shape (the literal translation from German is \"drop car\").\n\nA study of trucking fleets in North American found that improving trailer aerodynamics was the least used method for improving fuel efficiency. Improving tractor aerodynamics and using low rolling resistance tires were adopted at much higher rates.\n\nThe trailers have been used in the United Kingdom for several years by various operators. Deutsche Post DHL operates over 1,100 in the UK and was the first operator in continental Europe; it announced in 2014 that it would utilize the trailers in France and Germany.\n\n\n"}
{"id": "1147005", "url": "https://en.wikipedia.org/wiki?curid=1147005", "title": "Ternary compound", "text": "Ternary compound\n\nIn inorganic chemistry, a ternary compound is a compound containing three different elements. An example is sodium phosphate, NaPO. The sodium ion has a charge of 1+ and the phosphate ion has a charge of 3-. Therefore, three sodium ions are needed to balance the charge of one phosphate ion. Another example of a ternary compound is calcium carbonate (CaCO). In naming and writing the formulae for ternary compounds, rules are similar to binary compounds.\n\nAccording to Rustum Roy and Olaf Müller, \"the chemistry of the entire mineral world informs us that \"chemical\" complexity can easily be accommodated within structural simplicity.\" The example of zircon is cited, where various metal atoms are replaced in the same crystal structure. \"The structural entity ... remains ternary in character and is able to accommodate an enormous range of chemical elements.\" The great variety of ternary compounds is therefore reduced to relatively few structures: \"By dealing with approximately ten ternary structural groupings we can cover the most important structures of science and technology specific to the non-metallics world. It is a remarkable instance of nature's simplexity.\"\n\nLetting A and B represent cations and X an anion, these ternary groupings are organized by stoichiometric types ABX, ABX, and ABX. Brackets ([ ]) are used to identify a structure by an enclosed typical ternary compound.\n\nA ternary compound of type ABX may be in the class of olivine, the spinel group, phenakite, [KNi F], [β-K SO], or [Ca F O].\n\nOne of type ABX may be of the class of zircon, scheelite, barite or an ordered silicon dioxide derivative.\n\nIn the ABX class of ternary compounds, there are the structures of perovskite (structure), calcium carbonate, pyroxenes, corundum and hexagonal ABX types.\n\nOther ternary compounds are described as crystals of types formula_1\n\nIn organic chemistry, the carbohydrates and carboxylic acids are ternary compounds with carbon, oxygen, and hydrogen. Other organic ternary compounds replace oxygen with another atom to form functional groups.\n"}
{"id": "41813667", "url": "https://en.wikipedia.org/wiki?curid=41813667", "title": "Tetraperchloratoaluminate", "text": "Tetraperchloratoaluminate\n\nTetraperchloratoaluminates are salts of the tetraperchloratoaluminate anion, [Al(ClO)]. The anion contains aluminium tetrahedrally surrounded by four perchlorate groups. The perchlorate is covalently bonded to the aluminium, but perchlorate is much more well known as an ion. The covalent bond to aluminium distorts the perchlorate and renders it unstable.\n\nRelated chemicals are the haloperchloroatoaluminates, where there is one perchloro group attached to aluminium, and three halogens such as chlorine (chloroperchloroatoaluminates) or bromine (bromoperchloroatoaluminates).\nNitronium tetraperchloratoaluminate is made from exact amounts of nitronium perchlorate and anhydrous aluminium chloride combined in liquid sulfur dioxide.\n\nAmmonium tetraperchloratoaluminate can be formed by three moles of nitronium perchlorate, one mole of anhydrous aluminium chloride, and one mole of ammonium perchlorate combined in liquid sulfur dioxide.\n\nThe tetraperchloratoaluminates are yellowish crystalline solids. They are stable up to 50 °C.\n\nAbove this temperature they decompose to hexaperchloratoaluminates which are more temperature stable.\n"}
{"id": "47304063", "url": "https://en.wikipedia.org/wiki?curid=47304063", "title": "Tony Bramley", "text": "Tony Bramley\n\nTony Bramley is a South Australian scuba diver and environmentalist who has campaigned for the protection of the giant Australian cuttlefish aggregation of northern Spencer Gulf since it was heavily fished in the late 1990s.\n\nSince 1998 Tony Bramley has promoted marine eco-tourism in the upper Spencer Gulf region, particularly on the rocky inshore reefs of the Point Lowly peninsula, where the cuttlefish gather to breed between May and August each year. As the proprietor of Whyalla Diving Services, Bramley manages a crew of commercial divers and also provisions visiting tourists and marine scientists who visit the region. Bramley has lived in Whyalla since 1979. He grew up in Edithburgh on Yorke Peninsula, and has spent most of his life on and in the waters of South Australia's two gulfs. Occasionally, Bramley has been involved in animal rescue operations, and once freed a whale which had become entangled in netting.\n"}
{"id": "8293530", "url": "https://en.wikipedia.org/wiki?curid=8293530", "title": "Tornado intensity", "text": "Tornado intensity\n\nTornado intensity can be measured by in situ or remote sensing measurements, but since these are impractical for wide scale use, intensity is usually inferred via proxies, such as damage. The Fujita scale and the Enhanced Fujita scale rate tornadoes by the damage caused. The Enhanced Fujita Scale was an upgrade to the older Fujita scale, with engineered (by expert elicitation) wind estimates and better damage descriptions, but was designed so that a tornado rated on the Fujita scale would receive the same numerical rating. An EF0 tornado will probably damage trees and peel some shingles off the roof. an EF5 tornado can rip homes off their foundations and leaving them bare and can even deform large skyscrapers. The similar TORRO scale ranges from a T0 for extremely weak tornadoes to T11 for the most powerful known tornadoes. Doppler radar data, photogrammetry, and ground swirl patterns (cycloidal marks) may also be analyzed to determine intensity and award a rating.\n\nTornadoes vary in intensity regardless of shape, size, and location, though strong tornadoes are typically larger than weak tornadoes. The association with track length and duration also varies, although longer track (and longer lived) tornadoes tend to be stronger.<ref name=\"width/length intensity relationship\"></ref> In the case of violent tornadoes, only a small portion of the path area is of violent intensity; most of the higher intensity is from subvortices. In the United States, 80% of tornadoes are EF0 and EF1 (T0 through T3) tornadoes. The rate of occurrence drops off quickly with increasing strength—less than 1% are violent tornadoes (EF4, T8 or stronger).\n\nFor many years, before the advent of Doppler radar, scientists had nothing more than educated guesses as to the speed of the winds in a tornado. The only evidence indicating the wind speeds found in the tornado was the damage left behind by tornadoes which struck populated areas. Some believed they reach ; others thought they might exceed , and perhaps even be supersonic. One can still find these incorrect guesses in some old (until the 1960s) literature, such as the original Fujita Intensity Scale developed by Dr. Tetsuya Theodore Fujita in the early '70s. However, one can find accounts (e.g. ; be sure to scroll down) of some remarkable work done in this field by a U.S. Army soldier, Sergeant John Park Finley.\n\nIn 1971, Dr. Tetsuya Theodore Fujita introduced the idea for a scale of tornado winds. With the help of colleague Allen Pearson, he created and introduced what came to be called the Fujita scale in 1973. This is what the F stands for in F1, F2, etc. The scale was based on a relationship between the Beaufort scale and the Mach number scale; the low end of F1 on his scale corresponds to the low end of B12 on the Beaufort scale, and the low end of F12 corresponds to the speed of sound at sea level, or Mach 1. In practice, tornadoes are only assigned categories F0 through F5.\n\nThe TORRO scale, created by the Tornado and Storm Research Organisation (TORRO), was developed in 1974 and published a year later. The TORRO scale has 12 levels, which cover a broader range with tighter graduations. It ranges from a T0 for extremely weak tornadoes to T11 for the most powerful known tornadoes. T0-T1 roughly correspond to F0, T2-T3 to F1, and so on. While T10+ would be approximately an F5, the highest tornado rated to date on the TORRO scale was a T8. There is some debate as to the usefulness of the TORRO scale over the Fujita scale—while it may be helpful for statistical purposes to have more levels of tornado strength, often the damage caused could be created by a large range of winds, rendering it hard to narrow the tornado down to a single TORRO scale category.\n\nResearch conducted in the late 1980s and 1990s suggested that, even with the implication of the Fujita scale, tornado winds were notoriously overestimated, especially in significant and violent tornadoes. Because of this, in 2006, the American Meteorological Society introduced the Enhanced Fujita scale, to help assign realistic wind speeds to tornado damage. The scientists specifically designed the scale so that a tornado assessed on the Fujita scale and the Enhanced Fujita scale would receive the same ranking. The EF-scale is more specific in detailing the degrees of damage on different types of structures for a given wind speed. While the F-scale goes from F0 to F12 in theory, the EF-scale is capped at EF5, which is defined as \"winds ≥\". In the United States, the Enhanced Fujita scale went into effect on February 2, 2007 for tornado damage assessments and the Fujita scale is no longer used.\n\nThe first observation which confirmed that F5 winds could occur happened on April 26, 1991. A tornado near Red Rock, Oklahoma was monitored by scientists using a portable Doppler radar, an experimental radar device that measures wind speed. Near the tornado's peak intensity, they recorded a wind speed of . Though the portable radar had uncertainty of ±, this reading was probably within the F5 range, confirming that tornadoes were capable of violent winds found nowhere else on earth.\n\nEight years later, during the 1999 Oklahoma tornado outbreak of May 3, 1999, another scientific team was monitoring an exceptionally violent tornado (one which would eventually kill 36 people in the Oklahoma City metropolitan area). At about 7:00 pm, they recorded one measurement of , faster than the previous record. Though this reading is just short of the theoretical F6 rating, the measurement was taken more than in the air, where winds are typically stronger than at the surface. In rating tornadoes, only surface wind speeds, or the wind speeds indicated by the damage resulting from the tornado, are taken into account. Also, in practice, the F6 rating is not used.\n\nWhile scientists have long theorized that extremely low pressures might occur in the center of tornadoes, there were no measurements to confirm it. A few home barometers had survived close passes by tornadoes, recording values as low as , but these measurements were highly uncertain. However, on June 24, 2003, a group of researchers successfully dropped devices called \"turtles\" into an F4 tornado near Manchester, South Dakota, one of which measured a pressure drop of more than as the tornado passed directly overhead. Still, tornadoes are widely varied, so meteorologists are still conducting research to determine if these values are typical or not.\n\nIn the United States, F0 and F1 (T0 through T3) tornadoes account for 80% of all tornadoes. The rate of occurrence drops off quickly with increasing strength—violent tornadoes (stronger than F4, T8), account for less than 1% of all tornado reports. Worldwide, strong tornadoes account for an even smaller percentage of total tornadoes. Violent tornadoes are extremely rare outside of the United States, Canada and Bangladesh.\n\nF5 and EF5 tornadoes are rare, occurring on average once every few years. An F5 tornado was reported in Elie, Manitoba in Canada, on June 22, 2007. Before that, the last confirmed F5 was the 1999 Bridge Creek–Moore tornado, which killed 36 people on May 3, 1999. Nine EF5 tornadoes have occurred in the United States, in Greensburg, Kansas on May 4, 2007; Parkersburg, Iowa on May 25, 2008; Smithville, Mississippi, Philadelphia, Mississippi, Hackleburg, Alabama and Rainsville, Alabama (four separate tornadoes) on April 27, 2011; Joplin, Missouri on May 22, 2011 and El Reno, Oklahoma on May 24, 2011. On May 20, 2013 a confirmed EF5 tornado again struck Moore, Oklahoma.\n\nA typical tornado has winds of or less, is approximately across, and travels a mile (1.6 km) or so before dissipating. However, tornadic behavior is extremely variable; these figures represent only statistical probability.\n\nTwo tornadoes that look almost exactly the same can produce drastically different effects. Also, two tornadoes which look very different can produce similar damage. This is due to the fact that tornadoes form by several different mechanisms, and also that they follow a life cycle which causes the same tornado to change in appearance over time. People in the path of a tornado should never attempt to determine its strength as it approaches. Between 1950 and 2014 in the United States, 222 people have been killed by EF1 tornadoes, and 21 have been killed by EF0 tornadoes.\n\nThe vast majority of tornadoes are designated EF1 or EF0, also known as \"weak\" tornadoes. However, weak is a relative term for tornadoes, as even these can cause significant damage. F0 and F1 tornadoes are typically short-lived—since 1980 almost 75% of tornadoes rated weak stayed on the ground for or less. However, in this time, they can cause both damage and fatalities.\n\nEF0 (T0-T1) damage is characterized by superficial damage to structures and vegetation. Well-built structures are typically unscathed, sometimes sustaining broken windows, with minor damage to roofs and chimneys. Billboards and large signs can be knocked down. Trees may have large branches broken off, and can be uprooted if they have shallow roots. Any tornado that is confirmed but causes no damage (i.e. remains in open fields) is always rated EF0 as well.\n\nEF1 (T2-T3) damage has caused significantly more fatalities than that caused by EF0 tornadoes. At this level, damage to mobile homes and other temporary structures becomes significant, and cars and other vehicles can be pushed off the road or flipped. Permanent structures can suffer major damage to their roofs.\n\nEF2 (T4-T5) tornadoes are the lower end of \"significant\", and yet are stronger than most tropical cyclones (though tropical cyclones affect a much larger area and their winds take place for a much longer duration). Well-built structures can suffer serious damage, including roof loss, and collapse of some exterior walls may occur at poorly built structures. Mobile homes, however, are totally destroyed. Vehicles can be lifted off the ground, and lighter objects can become small missiles, causing damage outside of the tornado's main path. Wooded areas will have a large percentage of their trees snapped or uprooted.\n\nEF3 (T6-T7) damage is a serious risk to life and limb and the point at which a tornado statistically becomes significantly more destructive and deadly. Few parts of affected buildings are left standing; well-built structures lose all outer and some inner walls. Unanchored homes are swept away, and homes with poor anchoring may collapse entirely. Small vehicles and similarly sized objects are lifted off the ground and tossed as projectiles. Wooded areas will suffer almost total loss of vegetation, and some tree debarking may occur. Statistically speaking, EF3 is the maximum level that allows for reasonably effective residential sheltering in place in a first floor interior room closest to the center of the house (the most widespread tornado sheltering procedure in America for those with no basement or underground storm shelter).\n\nWhile isolated examples exist of people surviving E/F5 impacts in their homes—one survivor of the Jarrell F5 sheltered in a bathtub and was miraculously blown to safety as her house disintegrated—surviving an E/F5 impact outside of a robust and properly constructed underground storm shelter is statistically unlikely.\n\nEF4 (T8-T9) damage typically results in a total loss of the affected structure. Well-built homes are reduced to a short pile of medium-sized debris on the foundation. Homes with poor or no anchoring will be swept completely away. Large, heavy vehicles, including airplanes, trains, and large trucks, can be pushed over, flipped repeatedly or picked up and thrown. Large, healthy trees are entirely debarked and snapped off close to the ground or uprooted altogether and turned into flying projectiles. Passenger cars and similarly sized objects can be picked up and flung for considerable distances. EF4 damage can be expected to level even the most robustly built homes, making the common practice of sheltering in an interior room on the ground floor of a residence insufficient to ensure survival. A storm shelter, reinforced basement or other subterranean shelter is considered necessary to provide any reasonable expectation of safety against EF4 damage.\n\nEF5 (T10-T11) damage represents the upper limit of tornado power, and destruction is almost always total. An EF5 tornado pulls well-built, well-anchored homes off their foundations and into the air before obliterating them, flinging the wreckage for miles and sweeping the foundation clean. Large, steel reinforced structures such as schools are completely leveled. Tornadoes of this intensity tend to shred and scour low-lying grass and vegetation from the ground. Very little recognizable structural debris is generated by EF5 damage, with most materials reduced to a coarse mix of small, granular particles and dispersed evenly across the tornado's damage path. Large, multi-ton steel frame vehicles and farm equipment are often mangled beyond recognition and deposited miles away or reduced entirely to unrecognizable component parts. The official description of this damage highlights the extreme nature of the destruction, noting that \"incredible phenomena will occur\"; historically, this has included such awesome displays of power as twisting skyscrapers, leveling entire communities, and stripping asphalt from roadbeds. Despite their relative rarity, the damage caused by EF5 tornadoes represents a disproportionately extreme hazard to life and limb— since 1950 in the United States, only 58 tornadoes (0.1% of all reports) have been designated F5 or EF5, and yet these have been responsible for more than 1300 deaths and 14,000 injuries (21.5% and 13.6%, respectively).\n\n\n"}
{"id": "38050893", "url": "https://en.wikipedia.org/wiki?curid=38050893", "title": "Ultra-high-temperature ceramics", "text": "Ultra-high-temperature ceramics\n\nUltra-high-temperature ceramics (UHTCs) are a class of refractory ceramics that offer excellent stability at temperatures exceeding 2000 °C being investigated as possible thermal protection system (TPS) materials, coatings for materials subjected to high temperatures, and bulk materials for heating elements. Broadly speaking, UHTCs are borides, carbides, nitrides, and oxides of early transition metals. Current efforts have focused on heavy, early transition metal borides such as hafnium diboride (HfB) and zirconium diboride (ZrB); additional UHTCs under investigation for TPS applications include hafnium nitride (HfN), zirconium nitride (ZrN), titanium carbide (TiC), titanium nitride (TiN), thorium dioxide (ThO), tantalum carbide (TaC) and their associated composites.\n\nBeginning in the early 1960s, demand for high-temperature materials by the nascent aerospace industry prompted the Air Force Materials Laboratory to begin funding the development of a new class of materials that could withstand the environment of proposed hypersonic vehicles such as Dyna-soar and the Space Shuttle at Manlabs Incorporated. Through a systematic investigation of the refractory properties of binary ceramics, they discovered that the early transition metal borides, carbides, and nitrides had surprisingly high thermal conductivity, resistance to oxidation, and reasonable mechanical strength when small grain sizes were used. Of these, ZrB and HfB in composites containing approximately 20% volume SiC were found to be the best performing.\n\nUHTC research was largely abandoned after the pioneering mid-century Manlabs work due to the completion of the Space Shuttle missions and the elimination of the Air force spaceplane development. Three decades later, however, research interest was rekindled by a string of 1990s era NASA programs aimed at developing a fully reusable hypersonic spaceplane such as the National Aerospace Plane, Venturestar/X-33, Boeing X-37, and the Air Force's Blackstar program. New research in UHTCs was led by NASA Ames, with research at the center continuing to the present through funding from the NASA Fundamental Aeronautics Program. UHTCs also saw expanded use in varied environments, from nuclear engineering to aluminum production.\nIn order to test real world performance of UHTC materials in reentry environments, NASA Ames conducted two flight experiments in 1997 and 2000. The slender Hypersonic Aero-thermodynamic Research Probes (SHARP B1 and B2) briefly exposed the UHTC materials to actual reentry environments by mounting them on modified nuclear ordnance Mk12A reentry vehicles and launching them on Minuteman III ICBMs. Sharp B-1 had a HfB2/SiC nosecone with a tip radius of 3.5 mm which experienced temperatures well above 2815 °C during reentry, ablating away at an airspeed of 6.9 km/s as predicted; however, it was not recovered and its axially-symmetric cone shape did not provide flexural strength data needed to evaluate the performance of UHTCs in linear leading edges. To improve the characterization of UHTC mechanical strength and better study their performance, SHARP-B2, was recovered and included four retractable, sharp wedge-like protrusions called \"strakes\" which each contained three different UHTC compositions which were extended into the reentry flow at different altitudes.\n\nThe SHARP-B2 test that followed permitted recovery of four segmented strakes which had three sections, each consisting of a different HfB or ZrB composite as shown in Figure 1. The vehicle was successfully recovered, despite the fact that it impacted the sea at three times the predicted velocity. The four rear strake segments (HfB) fractured between 14 and 19 seconds into reentry, two mid segments (ZrB/SiC) fractured, and no fore strake segments (ZrB/SiC/C) failed. The actual heat flux was 60% less than expected, actual temperatures were much lower than expected, and heat flux on the rear strakes was much higher than expected. The material failures were found to result from very large grain sizes in the composites and pure ceramics, with cracks following macroscopic crystal grain boundaries. Since this test, NASA Ames has continued refining production techniques for UHTC synthesis and performing basic research on UHTCs.\n\nMost research conducted in the last two decades has focused on improving the performance of the two most promising compounds developed by Manlabs, ZrB and HfB, though significant work has continued in characterizing the nitrides, oxides, and carbides of the group four and five elements. In comparison to carbides and nitrides, the diborides tend to have higher thermal conductivity but lower melting points, a tradeoff which gives them good thermal shock resistance and makes them ideal for many high-temperature thermal applications. The melting points of many UHTCs are shown in Table 1. Despite the high melting points of pure UHTCs, they are unsuitable for many refractory applications because of their high susceptibility to oxidation at elevated temperatures.\n\nTable 1. Crystal structures, densities, and melting points of selected UHTCs.\n\nUHTCs all exhibit strong covalent bonding which gives them structural stability at high temperatures. Metal carbides are brittle due to the strong bonds that exist between carbon atoms. The largest class of carbides, including Hf, Zr, Ti and Ta carbides have high melting points due to covalent carbon networks although carbon vacancies often exist in these materials; indeed, HfC has one of the highest melting points of any material. Nitrides such as ZrN and HfN have similarly strong covalent bonds but their refractory nature makes them especially difficult to synthesize and process. The stoichiometric nitrogen content can be varied in these complexes based on the synthetic technique utilized; different nitrogen content will give different properties to the material, such as how if x exceeds 1.2 in ZrNx, a new optically transparent and electrically insulating phase appears to form. Ceramic borides such as HfB and ZrB benefit from very strong bonding between boron atoms as well as strong metal to boron bonds; the hexagonal close-packed structure with alternating two-dimensional boron and metal sheets give these materials high but anisotropic strength as single crystals. Borides exhibit high thermal conductivity (on the order of 75 – 105 W/mK) and low coefficients of thermal expansion (5 – 7.8 x 10 K-1) and improved oxidation resistance in comparison to other classes of UHTCs. Thermal expansion, thermal conductivity and other data are shown in Table 2. The crystal structures, lattice parameters, densities, and melting points of different UHTCs are shown in Table 1.\n\nTable 2. Thermal expansion coefficients across selected temperature ranges and thermal conductivity at a fixed temperature for selected UHTCs.\n\nIn comparison with carbide and nitride-based ceramics, diboride-based UHTCs exhibit higher thermal conductivity (refer to Table 2, where we can see that hafnium diboride has thermal conductivity of 105, 75, 70 W/m*K at different temperature while hafnium carbide and nitride have values only around 20W/m*K). Thermal shock resistance of HfB and ZrB was investigated by ManLabs and it was found that these materials did not fail at thermal gradients sufficient for the failure of SiC; indeed, it was found that hollow cylinders could not be cracked by an applied radial thermal gradient without first being notched on the inner surface. UHTCs generally exhibit thermal expansion coefficients in the range of 5.9–8.3 × 10 K.The structural and thermal stability of ZrB and HfB UHTCs results from the occupancy of bonding and antibonding levels in hexagonal MB structures with alternating hexagonal sheets of metal and boride atoms. In such structures, the principal frontier electronic states are bonding and antibonding orbitals resulting from bonding between boron 2p orbitals and metal d orbitals; before group (IV), the number of available electrons in a unit cell is insufficient to fill all bonding orbitals, and beyond it they begin to fill the antibonding orbitals. Both effects reduce the overall bonding strength in the unit cell and therefore the enthalpy of formation and melting point. Experimental evidence shows that as one moves across the transition metal series in a given period, the enthalpy of formation of MB ceramics increases and peaks at Ti, Zr, and Hf before decaying as the metal gets heavier. As a result, the enthalpies of formation of several important UHTCs are as follows: HfB > TiB > ZrB > TaB > NbB > VB.\n\nTable 3 lists UHTC carbides and borides mechanical properties. It is extremely important that UHTCs are able to retain high bending strength and hardness at high temperatures (above 2000 °C). UHTCs generally exhibit hardness above 20 GPa due to the strong covalent bonds present in these materials. However, the different methods of processing UHTCs can lead to great variation in hardness values. UHTCs exhibit high flexural strengths of > 200 MPa at 1800 °C, and UHTCs with fine-grained particles exhibit higher flexural strengths than UHTCs with coarse grains. It has been shown that diboride ceramics synthesized as a composite with silicon carbide (SiC) exhibit increased fracture toughness (increase of 20% to 4.33 MPam) relative to the pure diborides. This is due to material densification and a reduction in grain size upon processing.\n\nTable. 3 Flexural strength, hardness, and Young's Modulus at given temperatures for selected UHTCs.\n\nWhile UHTCs have desirable thermal and mechanical properties, they are susceptible to oxidation at their elevated operating temperatures. The metal component oxidizes to a gas such as CO or NO, which is rapidly lost at the elevated temperatures UHTCs are most useful at; boron, for example, readily oxidizes to BO which becomes a liquid at 490 °C and vaporizes very rapidly above 1100 °C; in addition, their brittleness makes them poor engineering materials. Current research targets increasing their toughness and oxidation resistance by exploring composites with silicon carbide, the incorporation of fibers, and the addition of rare-earth hexaborides such as lanthanum hexaboride (LaB). It has been found that the oxidative resistance of HfB and ZrB are greatly enhanced through the inclusion of 30% weight silicon carbide due to the formation of a protective glassy surface layer upon the application of temperatures in excess of 1000 °C composed of SiO. To determine the effect of SiC content on diboride oxidation, ManLabs conducted a series of furnace oxidation experiments, in which the oxidation scale thickness as a function of temperature for pure HfB, SiC and HfB 20v% SiC were compared. At temperatures greater than 2100 K the oxide scale thickness on pure HfB is thinner than that on pure SiC, and HfB/20% SiC has the best oxidation resistance. Extreme heat treatment leads to greater oxidation resistance as well as improved mechanical properties such as fracture resistance.\n\nUHTCs possess simple empirical formulas and thus can be prepared by a wide variety of synthetic methods. UHTCs such as ZrB can be synthesized by stoichiometric reaction between constituent elements, in this case Zr and B. This reaction provides for precise stoichiometric control of the materials. At 2000 K, the formation of ZrB via stoichiometric reaction is thermodynamically favorable (ΔG=−279.6 kJ mol) and therefore, this route can be used to produce ZrB by self-propagating high-temperature synthesis (SHS). This technique takes advantage of the high exothermic energy of the reaction to cause high temperature, fast combustion reactions. Advantages of SHS include higher purity of ceramic products, increased sinterability, and shorter processing times. However, the extremely rapid heating rates can result in incomplete reactions between Zr and B, the formation of stable oxides of Zr, and the retention of porosity. Stoichiometric reactions have also been carried out by reaction of attrition milled (wearing materials by grinding) Zr and B powder (and then hot pressing at 600 °C for 6 h), and nanoscale particles have been obtained by reacting attrition milled Zr and B precursor crystallites (10 nm in size). Unfortunately, all of the stoichiometric reaction methods for synthesizing UHTCs employ expensive charge materials, and therefore these methods are not useful for large-scale or industrial applications.\n\nReduction of ZrO and HfO to their respective diborides can also be achieved via metallothermic reduction. Inexpensive precursor materials are used and reacted according to the reaction below:\n\nZrO + BO + 5Mg → ZrB + 5MgO\n\nMg is used as a reactant in order to allow for acid leaching of unwanted oxide products. Stoichiometric excesses of Mg and BO are often required during metallothermic reductions in order to consume all available ZrO. These reactions are exothermic and can be used to produce the diborides by SHS. Production of ZrB from ZrO via SHS often leads to incomplete conversion of reactants, and therefore double SHS (DSHS) has been employed by some researchers. A second SHS reaction with Mg and HBO as reactants along with the ZrB/ZrO mixture yields increased conversion to the diboride, and particle sizes of 25–40 nm at 800 °C. After metallothermic reduction and DSHS reactions, MgO can be separated from ZrB by mild acid leaching.\n\nSynthesis of UHTCs by boron carbide reduction is one of the most popular methods for UHTC synthesis. The precursor materials for this reaction (ZrO/TiO/HfO and BC) are less expensive than those required by the stoichiometric and borothermic reactions. ZrB is prepared at greater than 1600 °C for at least 1 hour by the following reaction:\n\n2ZrO + BC + 3C → 2ZrB + 4CO\n\nThis method requires a slight excess of boron, as some boron is oxidized during boron carbide reduction. ZrC has also been observed as a product from the reaction, but if the reaction is carried out with 20–25% excess BC, the ZrC phase disappears, and only ZrB remains. Lower synthesis temperatures (~1600 °C) produce UHTCs that exhibit finer grain sizes and better sinterability. Boron carbide must be subjected to grinding prior to the boron carbide reduction in order to promote oxide reduction and diffusion processes.\n\nBoron carbide reductions can also be carried out via reactive plasma spraying if a UHTC coating is desired. Precursor or powder particles react with plasma at high temperatures (6000–15000 °C) which greatly reduces the reaction time. ZrB and ZrO phases have been formed using a plasma voltage and current of 50 V and 500 A, respectively. These coating materials exhibit uniform distribution of fine particles and porous microstructures, which increased hydrogen flow rates.\n\nAnother method for the synthesis of UHTCs is the borothermic reduction of ZrO, TiO, or HfO with B. At temperatures higher than 1600 °C, pure diborides can be obtained from this method. Due to the loss of some boron as boron oxide, excess boron is needed during borothermic reduction. Mechanical milling can lower the reaction temperature required during borothermic reduction. This is due to the increased particle mixing and lattice defects that result from decreased particle sizes of ZnO and B after milling. This method is also not very useful for industrial applications due to the loss of expensive boron as boron oxide during the reaction.\n\nNanocrystals of group IV and V metal diborides such as TiB, ZrB, HfB, NbB, TaB were successfully synthesized by Zoli's Reaction, reduction of TiO, ZrO, HfO, NbBO, TaO with NaBH using a molar ratio M:B of 1:4 at 700 °C for 30 min under argon flow.\n\nMO + 3NaBH → MB + 2Na(g,l) + NaBO + 6H(g) (M=Ti, Zr, Hf)\n\nMO + 6.5NaBH → 2MB + 4Na(g,l) + 2.5NaBO+ 13H(g) (M=Nb,Ta)\n\nUHTCs can be prepared from solution-based synthesis methods as well, although few substantial studies have been conducted. Solution-based methods allow for low temperature synthesis of ultrafine UHTC powders. Yan et al. have synthesized ZrB powders using the inorganic-organic precursors ZrOC•8HO, boric acid and phenolic resin at 1500 °C. The synthesized powders exhibit 200 nm crystallite size and low oxygen content (~ 1.0 wt%). UHTC preparation from polymeric precursors has also been recently investigated. ZrO and HfO can be dispersed in boron carbide polymeric precursors prior to reaction. Heating the reaction mixture to 1500 °C results in the in situ generation of boron carbide and carbon, and the reduction of ZrO to ZrB soon follows. The polymer must be stable, processable, and contain boron and carbon in order to be useful for the reaction. Dinitrile polymers formed from the condensation of dinitrile with decaborane satisfy these criteria.\n\nChemical vapor deposition (CVD) of titanium and zirconium diborides is another method for preparing coatings of UHTCs. These techniques rely on metal halide and boron halide precursors (such as TiCl and BCl) in the gaseous phase and use H2 as a reducing agent. This synthesis route can be employed at low temperatures and produces thin films for coating on metal (and other material) surfaces. Mojima et al. have used CVD to prepare coatings of ZrB on Cu at 700–900 °C (Figure 2). Plasma enhanced CVD (PECVD) has also been used to prepare UHTC diborides. After plasma of the reacting gases is created (by radio frequency or direct current discharge between two electrodes) the reaction takes place, followed by deposition. The deposition takes place at lower temperatures compared to traditional CVD because only the plasma needs to be heated to provide sufficient energy for the reaction. ZrB has been prepared via PECVD at temperatures lower than 600 °C as a coating on zircalloy. Zirconium borohydride can also be used as a precursor in PECVD. Thermal decomposition of Zr(BH) to ZrB can occur at temperatures in the range of 150–400 °C in order to prepare amorphous, conductive films.\n\nDiboride-based UHTCs often require high-temperature and -pressure processing to produce dense, durable materials. The high melting points and strong covalent interactions present in UHTCs make it difficult to achieve uniform densification in these materials. Densification is only achieved at temperatures above 1800 °C once grain boundary diffusion mechanisms become active. Unfortunately, processing of UHTCs at these temperatures results in materials with larger grain sizes and poor mechanical properties including reduced toughness and hardness. To achieve densification at lower temperatures, several techniques can be employed: additives such as SiC can be used in order to form a liquid phase at the sintering temperature, the surface oxide layer can be removed, or the defect concentration can be increased. SiC can react with the surface oxide layer in order to provide diboride surfaces with higher energy: adding 5–30 vol% SiC has demonstrated improved densification and oxidation resistance of UHTCs. SiC can be added as a powder or a polymer to diboride UHTCs. The addition of SiC as a polymer has several advantages over the more traditional addition of SiC as a powder because SiC forms along the grain boundaries when added as a polymer, which increases measures of fracture toughness (by ~24%). In addition to improved mechanical properties, less SiC needs to be added when using this method, which limits the pathways for oxygen to diffuse into the material and react. Although addition of additives such as SiC can improve densification of UHTC materials, it should be noted that these additives lower the maximum temperature at which UHTCs can operate due to the formation of eutectic liquids. The addition of SiC to ZrB lowers the operating temperature of ZrB from 3245 °C to 2270 °C.\n\nHot pressing is a popular method for obtaining densified UHTC materials that relies upon both high temperatures and pressures to produce densified materials. Powder compacts are heated externally and pressure is applied hydraulitically. In order to improve densification during hot pressing, diboride powders can undergo milling by attrition to obtain powders of <2μm. Milling also allows for more uniform dispersion of the additive SiC. Hot pressing temperature, pressure, heating rate, reaction atmosphere, and holding times are all factors that affect the density and microstructure of UHTC pellets obtained from this method. In order to achieve >99% densification from hot pressing, temperatures of 1800–2000 °C and pressures of 30 MPa or greater are required. UHTC materials with 20vol.% SiC and toughened with 5% carbon black as additives exhibit increased densification above 1500 °C, but these materials still require temperatures of 1900 °C and a pressure of 30 MPa in order to obtain near theoretical densities. Other additives such as AlO and YO have also been used during the hot pressing of ZrB-SiC composites at 1800 °C. These additives react with impurities to form a transient liquid phase and promote sintering of the diboride composites. The addition of rare earth oxides such as YO, YbO, LaO and NdO can lower densification temperatures and can react with surface oxides to promote densification. Hot pressing may result in improved densities for UHTCs, but it is an expensive technique that relies on high temperatures and pressures to provide useful materials.\n\nPressureless sintering is another method for processing and densifying UHTCs. Pressureless sintering involves heating powdered materials in a mold in order to promote atomic diffusion and create a solid material. Compacts are prepared by uniaxial die compaction, and then the compacts are fired at chosen temperatures in a controlled atmosphere. Exaggerated grain growth that hinders densification occurs during sintering due to the low-intrinsic sinterability and the strong covalent bonds of Ti, Zr, and Hf diborides. Full densification of ZrB by pressureless sintering is very difficult to obtain; Chamberlain et al. have only been able to obtain ~98% densification by heating at 2150 °C for 9 h (Figure 3). Efforts to control grain size and improve densification have focused on adding third phases to the UHTCs, some examples of these phases include the addition of boron and iridium. Addition of Ir in particular has shown an increase in the toughness of HfB/20vol.% SiC by 25%. Sintered density has also been shown to increase with the addition of Fe (up to 10% w/w) and Ni (up to 50% w/w) to achieve densifications of up to 88% at 1600 °C. More advances in pressureless sintering must be made before it can be considered a viable method for UHTC processing.\n\nSpark plasma sintering is another method for the processing of UHTC materials. Spark plasma sintering often relies on slightly lower temperatures and significantly reduced processing times compared to hot pressing. During spark plasma sintering, a pulsed direct current passes through graphite punch rods and dies with uniaxial pressure exerted on the sample material. Grain growth is suppressed by rapid heating over the range 1500–1900 °C; this minimizes the time the material has to coarsen. Higher densities, cleaner grain boundaries, and elimination of surface impurities can all be achieved with spark plasma sintering. Spark plasma sintering also uses a pulsed current to generate an electrical discharge that cleans surface oxides off of the powder. This enhances grain boundary diffusion and migration as well as densification of the material. The UHTC composite ZrB/20vol%SiC can be prepared with 99% density at 2000 °C in 5 min via spark plasma sintering. ZrB2-SiC composites have also been prepared by spark plasma sintering at 1400 °C over a period of 9 min. Spark plasma sintering has proven to be a useful technique for the synthesis of UHTCs, especially for preparation of UHTCs with smaller grain sizes.\n\nUHTCs, specifically Hf and Zr based diboride, are being developed to handle the forces and temperatures experienced by leading vehicle edges in atmospheric reentry and sustained hypersonic flight. The surfaces of hypersonic vehicles experience extreme temperatures in excess of 2500 °C while also being exposed to high-temperature, high-flow-rate oxidizing plasma. The material design challenges associated with developing such surfaces have so far limited the design of orbital re-entry bodies and hypersonic air-breathing vehicles such as scramjets and DARPA's HTV because the bow shock in front of a blunt body protects the underlying surface from the full thermal force of the onrushing plasma with a thick layer of relatively dense and cool plasma.\n\nSharp edges dramatically reduce drag, but the current generation of thermal protection system materials are unable to withstand the considerably higher forces and temperatures experienced by sharp leading edges in reentry conditions. The relation between radius of curvature and temperature in a leading edge is inversely proportional, e.g. as radius decreases temperature increases during hypersonic flight. Vehicles with \"sharp\" leading edges have significantly higher lift to drag ratios, enhancing the fuel efficiency of sustained flight vehicles such as DARPA's HTV-3 and the landing cross-range and operational flexibility of reusable orbital spaceplane concepts being developed such as the Reaction Engines Skylon and Boeing X-33.\n\nZirconium diboride is used in many boiling water reactor fuel assemblies due to its refractory nature, corrosion resistance, high-neutron-absorption cross-section of 759 barns, and stoichiometric boron content. Boron acts as a \"burnable\" neutron absorber because its two isotopes, 10B and 11B, both transmute into stable nuclear reaction products upon neutron absorption (4He + 7Li and 12C, respectively) and therefore act as sacrificial materials which protect other components which become more radioactive with exposure to thermal neutrons. However, the boron in ZrB2|ZrB must be enriched in 11B because the gaseous helium evolved by 10B strains the fuel pellet of UO creates a gap between coating and fuel, and increases the fuel's centerline temperature; such cladding materials have been used on the uranium oxide fuel pellets in Westinghouse AP-1000 nuclear reactors. The high thermal neutron absorbance of boron also has the secondary effect of biasing the neutron spectrum to higher energies, so the fuel pellet retains more radioactive 239Pu at the end of a fuel cycle. In addition to this deleterious effect of integrating a neutron absorber on the surface of a fuel pellet, boron coatings have the effect of creating a power density bulge in the middle of a nuclear reactor fuel cycle through the superposition of 235U depletion and faster burning of 11B. To help level out this bulge, ZrB/Gd cermets are being studied which would extend fuel lifetime by superimposing three simultaneous degradation curves.\n\nDue to the combination of refractory properties, high thermal conductivity, and the advantages of large stoichiometric boron content outlined in the above discussion of integral neutron absorbing fuel pellet cladding, refractory diborides have been used as control rod materials and have been studied for use in space nuclear power applications. While boron carbide is the most popular material for fast breeder reactors due to its lack of expense, extreme hardness comparable to diamond, and high cross-section, it completely disintegrates after a 5% burnup and is reactive when in contact with refractory metals. Hafnium diboride also suffers from high susceptibility to material degradation with boron transmutation, but its high melting point of 3380 °C and the large thermal neutron capture cross section of hafnium of 113 barns and low reactivity with refractory metals such as tungsten makes it an attractive control rod material when clad with a refractory metal.\n\nTitanium diboride is a popular material for handling molten aluminum due to its electrical conductivity, refractory properties, and its ability to wet with molten aluminum providing a superior electrical interface while not contaminating the aluminum with boron or titanium. TiB has been used as a drained cathode in the electroreduction of molten Al(III). In drained-cathode processes, aluminum can be produced with an electrode gap of only 0.25m with an accompanying reduction in required voltage. However, implementation of such technology still faces hurdles: with a reduction in voltage, there is a concomitant reduction in heat generation and better insulation at the top of the reactor is required. In addition to improved insulation, the technology requires better bonding methods between TiB and the bulk graphite electrode substrate. Bonding tiles of TiB or applying composite coatings each present their own unique challenges, with the high cost and large TiB capital cost of the former and the design difficulty of the latter. Composite materials must have each component degrade at the same rate, or the wettability and thermal conductivity of the surface will be lost with active material still remaining deeper within the electrode plate.\n\nZrB/60%SiC composites have been used as novel conducting ceramic heaters which display high oxidation resistance and melting points, and do not display the negative temperature coefficient resistance property of pure silicon carbide. The metal-like conductance of ZrB allows for its conductivity to decrease with increasing temperature, preventing uncontrollable electrical discharge while maintaining high operational upper bounds for operation. It was also found that through incorporation of 40% ZrB flexural strength was reduced from 500 MPa and 359 MPa in SiC and ZrB single crystals to 212.96 MPa, with flexural strength highly correlated to the size of grains in the annealed ceramic material. Conductivity at 500 °C was found to be 0.005 Ω cm for the 40% SiC composite, versus 0.16 Ω cm in pure SiC.\n"}
{"id": "1778123", "url": "https://en.wikipedia.org/wiki?curid=1778123", "title": "Vapour density", "text": "Vapour density\n\nVapour density is the density of a vapour in relation to that of hydrogen. It may be defined as mass of a certain volume of a substance divided by mass of same volume of hydrogen.\n\nvapour density = mass of \"n\" molecules of gas / mass of \"n\" molecules of hydrogen.\nThus:\n\nvapour density = molar mass of gas / molar mass of H\n\nvapour density = molar mass of gas / 2.016\n\nvapour density = ~½ × molar mass\n\nFor example, vapour density of mixture of NO and NO is 38. 3.Vapour density is a unitless quantity.\n\nIn many web sources, particularly in relation to safety considerations at commercial and industrial facilities in the U.S., vapour density is defined with respect to air, not hydrogen. Air is given a vapour density of one. For this use, air has a molecular weight of 28.97 atomic mass units, and all other gas and vapour molecular weights are divided by this number to derive their vapour density. For example, acetone has a vapour density of 2 in relation to air. That means acetone vapour is twice as heavy as air. This can be seen by dividing the molecular weight of Acetone, 58.1 by that of air, 28.97, which equals 2.\n\nWith this definition, the vapour density would indicate whether a gas is denser (greater than one) or less dense (less than one) than air. The density has implications for container storage and personnel safety—if a container can release a dense gas, its vapour could sink and, if flammable, collect until it is at a concentration sufficient for ignition. Even if not flammable, it could collect in the lower floor or level of a confined space and displace air, possibly presenting an asphyxiation hazard to individuals entering the lower part of that space.\n\n"}
{"id": "38749386", "url": "https://en.wikipedia.org/wiki?curid=38749386", "title": "Vishapakar", "text": "Vishapakar\n\nA vishapakar () also known as vishap stones, vishap stellas, \"serpent-stones\", \"dragon stones\", or simply as vishaps, are characteristic menhirs found in large quantities in the Armenian Highlands, in natural and artificial ponds, and other sources of water. They are commonly carved from one piece of stone, into cigar-like shapes with fish heads or serpents. Supposedly they are images of vishaps, mystical creatures.\n\nThe words etymology is disputed, either meaning a poisonous water-living creature or a creature of prodigious size. There are currently 150 vishaps in existence, of which 90 are found in Armenia.\n\nCertain studies believe Vishap is foremost worshipped as water, rain and a rich-giving soul, whose tail is capable of creating canals and paths when it hits the Earth. Vishap is also seen as an eerie monster who is a water source and guards treasure. Almost all the mythology explains divinity or god's hand as the cause of Vishap's death, which absorbed the water, the treasures that were guarded, and released the sacrificial virgins. Thus, old Egyptian myth regards Vishap as a power of darkness, who is defeated by the sun goddess Rán. As for Armenian myth, Vahagn, the dragon slayer, fights and wins the battle against Vishap. The Vishap battle myths have spread across the Armenian population as an old folk tale (for example, Dikran and Aztahag, Daredevils of Sassoun). They have also had an influence in Christian literature. According to legend, Vishap's death and virgin sacrifice saves Saint George. Other legend says Vishap is presented as a Sun, who is a bad and destructive force, that the angels fight with (thunder as a symbol of the fight, the lightning as Archangel Gabriel's flashing sword, the sparks as a fiery arrow, and the rainbow as the bow).\n\nAccording to Manuk Abeghian, Vishaps have been donated to Pantheon's beloved goddess, Astghik. As for Grigor Ghapantsyan, they symbolize the dying and resurrecting god, Ara the Beautiful.\n\nFound in Armenia's Gegham mountains, Lake Sevan's north-east coast, Mount Aragats's slopes, Garni, the valley of Çoruh River, as well as other places, where they used to worship Vishap stones in ancient times. They are obvious with \"Vishap\" names. They were carved from massive stones (the biggest being 5.06m high), in a fish form, with a snake, bull, ram, stork, etc, as well as bird sculptures, usually placed in fountains, canals, reservoirs, and artificial lakes nearby. It can be assumed that these slabs were supporting agriculture and irrigation, by worshipping personal water deities.\n\nVishaps were introduced by the Armenian writer Atrpet in 1880. His work was published in 1926. In 1909, when Nicholas Marr and Yakov Smirnov's visited Armenia's Temple of Garni for a paleontological excavation, the local residents heard stories about the Vishaps that lived in the tall mountains. Scientists organized the expedition, climbed the Gegham mountains, to confirm the existence Vishaps and whether they have any scientific significance or not. The findings in the Gegham mountains were published in 1931.\n\nThe scientists in the mountains discovered megalithic stone sculptures, which the Armenians called \"Vishap\" and the Kurds \"giant Yurt\", and were mostly in the form of a fish. The biggest Vishap measured 4.75m high, and 55cm wide. In 1909, all the Vishaps were destroyed, and a part of them were buried in the soil.\n\nSoon, other expeditions that were organized on the Gegham mountains found more Vishaps. In 1910, Nicholas Marr and Yakov Smirnov had already found 27 similar megalithic sculptures. There were similar Vishaps discovered in Armenia's Lake Sevan, southern Georgia, and eastern Turkey.\n\nDetermining how old Vishaps are is particularly difficult. The monuments are placed away from neighborhoods, whose radiocarbon analysis of the organic residues would enable them to determine the approximate age. Suddenly, on the giant Yurt's premiere Vishap that was found, there was images of the cross and Armenian letters dated from the 13th Century. The position of the cross and the writings show that in the 13th Century, the Vishap was still in the upright position. In 1963, in the Garni area a Vishap was excavated, which had King Argishti I of Urartu's cuneiform inscription. This Vishap is dated 2 BC to 1 BC. \n\nAll the findings are carved on one stone, that is within 3-5m high. Most of the Vishaps are in a fish form that resembles a catfish. Basically, the carved details represent the fish eyes, mouth, tail, and gills. Another portion of the Vishaps are pictured as a hoofed animal such as a bull or ram and may represent a sacrifice, with various cases only pictured as stakes on the stretched animal skin. On other Vishaps, there are waves symbolizing water, which often come out of the mouth of the bull, long-legged birds, and rare snakes.\n\n\nMost of the Vishap stones are found fallen down in a horizontal position, lying down. However, the three forms listed above are designed and carved on all sides. The tails of the fish forms of the Vishap stones suggests that they were also once in a standing position.\n\nThe Vishaps are monuments used to worship water, which are believe to have a close tie to water distribution. Almost all the Vishaps are found in places related to mountain springs or canals. Similarly, there are irrigation systems found at Artanish Bay, Mount Aragats,the Tokhmakagan backwaters of the Gegham mountains, and near Gemerzek settlements. Although it is impossible to precisely date the irrigation systems, scientists have linked the Vishaps to ancient fertility and water worship.\n\nScientists have noticed similarities between Vishaps and menhirs in North Caucasus and Europe. There are also similarities with monuments found in northern Mongolia. However, compared to the other structures listed above, Vishaps have a completely other purpose and have been clearly established by other tribes.\n\n\n\n"}
{"id": "57749408", "url": "https://en.wikipedia.org/wiki?curid=57749408", "title": "Water Resources Department, Madhya Pradesh", "text": "Water Resources Department, Madhya Pradesh\n\nThe Water Resources Department is the government department in the state of Madhya Pradesh responsible for irrigation and flood control.\n"}
{"id": "52266363", "url": "https://en.wikipedia.org/wiki?curid=52266363", "title": "Wave Energy Scotland", "text": "Wave Energy Scotland\n\nWave Energy Scotland (WES) is a technology development body set up by the Scottish Government to facilitate the development of wave energy in Scotland. It was set up in 2015 and is a subsidiary of Highlands and Islands Enterprise (HIE) based in Inverness. WES manages a number of projects resulting from its pre-commercial procurement funding calls.\n\nThe Scottish Government took positive action to support the ailing wave energy sector in Scotland, following the demise of one of the leading developers Pelamis Wave Power. The Energy Minister Fergus Ewing announced an initial budget for the body of £14.3 million over 13 months at the RenewableUK conference in February 2015\n\nThe original objectives for WES were set out by the Scottish Government as:\n\n\nTo date, WES has held four funding calls, .\n\nA total of 42 applications were made for this £7m call, with contracts awarded to nine consortia. \nEight projects were funded for the first stage of the Novel Wave Energy Converter call, out of 37 applications. \nTen projects were funded from this third Innovation Call\nThirteen concepts were funded from this fourth Innovation Call.\n\nWES has acquired intellectual property developed by the now defunct companies Pelamis Wave Power and Aquamarine Power. The former as part of the inception of Wave Energy Scotland, whilst the latter was completed in September 2016.\n\nThe first Wave Energy Scotland annual conference was held on 2 December 2016 at Pollock Halls in Edinburgh This provided an update of ongoing and future calls, plus quick-fire updates from participants ongoing PTO and NWEC calls.\n\nA second annual conference was held on 28 November 2017 .\n\nThe third annual conference will be held on 6 December 2018 at the Edinburgh International Conference Centre\n\n\n"}
