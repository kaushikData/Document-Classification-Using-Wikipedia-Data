{"id": "8521492", "url": "https://en.wikipedia.org/wiki?curid=8521492", "title": "AEP meter label format", "text": "AEP meter label format\n\nThe California Direct Access Standards setting process in 1997 identified the need to standardize the electric meter label identifier so as to create a unique identifier for every electric meter in the United States. The AEP meter label format is a recommended solution for this need.\n\nAEP meter label format follows ANSI C12.10 requirements for format of the electric meter labels. (See below) The format for the barcode label is:\n\nwhere\n\nThe meter test code is used by calibration equipment manufacturers to automatically set up their equipment to calibrate a given meter. The user scans in the AEP barcode on the meter, and the calibrator will use the correct voltage and test amps. The last five characters are user specified, and many electric utilities use these characters for an inventory code or date of manufacture.\n"}
{"id": "10018517", "url": "https://en.wikipedia.org/wiki?curid=10018517", "title": "Adam's Task", "text": "Adam's Task\n\nAdam’s Task: Calling Animals by Name by philosopher, poet, and animal trainer Vicki Hearne describes an innovative and metaphysical approach to training animals. Based on studies of literary criticism, philosophy, and extensive hands-on experience in training, Hearne asserts that animals (specifically those that commonly cohabit or interact with humans) are far more intelligent than most people assume. In fact, they are capable of developing an understanding of “the good,” a moral code that influences their motives and actions. In response to her studies and experiments, Hearne developed an entirely new system of animal training that contradicts modern animal behavioral research and yet, she insists through many examples, is astonishingly effective.\n\n\"Adam's Task\" was Hearne's first—and probably best-known—book, though she authored several other successful titles, including \"Animal Happiness\" and \"Bandit\". The focus of \"Adam's Task\" is on the complexities of human-animal communication and the importance of seeing domestic animals as they truly are—not as anthropomorphized creatures whom we see however we wish to see them. Some of the training methods described in the book have drawn criticism from various factions of animal activists, but Hearne stresses that she is striving for intellectual and moral integrity in all of her studies, and many agree that her obvious love for animals speaks in her defense.\n\nThe \"Boston Globe\" calls \"Adam’s Task\" “A beautiful, wonderful book of the sort that permanently refreshes thought and feeling.” The \"New York Times\" says “When Ms. Hearne relates a dog or horse story, the animals become full-fledged characters, as brightly delineated as people created by Dickens or Twain.”\n\n\"Adam's Task\" was first published by Knopf in 1986. It is now available from Skyhorse Publishing with a new introduction by Donald McCaig.\n"}
{"id": "58005", "url": "https://en.wikipedia.org/wiki?curid=58005", "title": "Airship", "text": "Airship\n\nAn airship or dirigible balloon is a type of aerostat or lighter-than-air aircraft that can navigate through the air under its own power. Aerostats gain their lift from large gasbags filled with a lifting gas that is less dense than the surrounding air.\nIn early dirigibles, the lifting gas used was hydrogen, due to its high lifting capacity and ready availability. Helium gas has almost the same lifting capacity and is not flammable, unlike hydrogen, but is rare and relatively expensive. Significant amounts were first discovered in the United States and for a while helium was only used for airships in that country. Most airships built since the 1960s have used helium, though some have used hot air.\n\nThe envelope of an airship may form a single gasbag, or may contain a number of internal gas-filled cells. An airship also has engines, crew, and optionally also payload accommodation, typically housed in one or more \"gondolas\" suspended below the envelope.\n\nThe main types of airship are non-rigid, semi-rigid, and rigid. Non-rigid airships, often called \"blimps\", rely on internal pressure to maintain their shape. Semi-rigid airships maintain the envelope shape by internal pressure, but have some form of supporting structure, such as a fixed keel, attached to it. Rigid airships have an outer structural framework that maintains the shape and carries all structural loads, while the lifting gas is contained in one or more internal gasbags or cells. Rigid airships were first flown by Count Zeppelin and the vast majority of rigid airships built were manufactured by the firm he founded. As a result, rigid airships are often called zeppelins.\n\nAirships were the first aircraft capable of controlled powered flight, and were most commonly used before the 1940s; their use decreased over time as their capabilities were surpassed by those of aeroplanes. Their decline was accelerated by a series of high-profile accidents, including the 1930 crash and burning of British \"R101\" in France, the 1933 and 1935 storm-related crashes of the twin airborne aircraft carrier U.S. Navy helium-filled rigids, the and USS \"Macon\" respectively, and the 1937 burning of the hydrogen-filled \"Hindenburg\". From the 1960s, helium airships have been used in applications where the ability to hover in one place for an extended period outweighs the need for speed and manoeuvrability, such as advertising, tourism, camera platforms, geological surveys, and aerial observation.\n\nDuring the pioneer years of aeronautics, terms such as \"airship\", \"air-ship\", \"air ship\" and \"ship of the air\" meant any kind of navigable or dirigible flying machine. In 1919 Frederick Handley Page was reported as referring to \"ships of the air,\" with smaller passenger types as \"air yachts.\" In the 1930s, large intercontinental flying boats were also sometimes referred to as \"ships of the air\" or \"flying-ships\". Nowadays the term \"airship\" is used only for powered, dirigible balloons, with sub-types being classified as rigid, semi-rigid or non-rigid. Semi-rigid architecture is the more recent, following advances in deformable structures and the exigency of reducing weight and volume of the airships. They have a minimal structure that keeps the shape jointly with overpressure of the gas envelope.\n\nAn \"aerostat\" is an aircraft that remains aloft using buoyancy or static lift, as opposed to the aerodyne, which obtains lift by moving through the air. Airships are a type of aerostat. The term \"aerostat\" has also been used to indicate a tethered or moored balloon as opposed to a free-floating balloon. Aerostats today are capable of lifting a payload of 3,000 pounds to an altitude of more than 4.5 kilometers above sea level. They can also stay in the air for extended periods of time, particularly when powered by an on-board generator or if the tether contains electrical conductors. Due to this capability, aerostats can be used as platforms for telecommunication services. For instance, Platform Wireless International Corporation announced in 2001 that it would use a tethered 1,250-pound airborne payload to deliver cellular phone service to a 140-mile region in Brazil. The European Union's ABSOLUTE project was also reportedly exploring the use of tethered aerostat stations to provide telecommunications during disaster response.\n\nAirships were originally called \"dirigible balloons\", from the French \"ballon dirigeable\" or shortly \"dirigeable\" (meaning \"steerable\", from the French \"diriger\" – to direct, guide or steer). This was the name that inventor Henri Giffard gave to his machine that made its first flight on 24 September 1852.\n\nA blimp is a non-rigid aerostat. In American usage it refers specifically to a non-rigid type of dirigible balloon or airship. In British usage it refers to any non-rigid aerostat, including barrage balloons and other kite balloons, having a streamlined shape and stabilising tail fins.\n\nThe term zeppelin is a genericized trademark that originally referred to airships manufactured by the German Zeppelin Company, which built and operated the first rigid airships in the early years of the twentieth century. The initials LZ, for (German for \"Zeppelin airship\"), usually prefixed their craft's serial identifiers.\n\nStreamlined rigid (or semi-rigid) airships are usually referred to as \"Zeppelin\", because of the fame that this company has acquired due to the number of airships it produced.\n\nHybrid airships fly with a positive aerostatic contribution, usually equal to the empty weight of the system, and the variable payload is sustained by propulsion or aerodynamic contribution.\n\nAirships are classified according to their method of construction into rigid, semi-rigid and non-rigid types.\n\nA rigid airship has a rigid framework covered by an outer skin or envelope. The interior contains one or more gasbags, cells or balloons to provide lift. Rigid airships are typically unpressurised and can be made to virtually any size. Most, but not all, of the German Zeppelin airships have been of this type.\n\nA semi-rigid airship has some kind of supporting structure but the main envelope is held in shape by the internal pressure of the lifting gas. Typically the airship has an extended, usually articulated keel running along the bottom of the envelope to stop it kinking in the middle by distributing suspension loads into the envelope, while also allowing lower envelope pressures.\n\nNon-rigid airships are often called \"blimps\". Most, but not all, of the American Goodyear airships have been blimps.\n\nA non-rigid airship relies entirely on internal gas pressure to retain its shape during flight. Unlike the rigid design, the non-rigid airship's gas envelope has no compartments. It typically has smaller internal bags or \"ballonets\" containing air, however. At sea level, these are filled with air. As altitude is increased, the lifting gas expands and air from the ballonets is expelled through valves to maintain the hull's shape. To return to sea level, the process is reversed: air is forced back into the ballonets by both scooping air from the engine exhaust and using auxiliary blowers.\n\nThe two main parts of an airship are its gas-containing envelope and a gondola or similar structure slung beneath and containing crew and other equipment. The engines may be mounted in the gondola or elsewhere off the envelope.\n\nThe basic structure of an airship may be rigid, semi-rigid or non-rigid, as described.\n\nThe envelope itself is the structure, including textiles that contains the buoyant gas. Internally two ballonets placed in the front part and in the rear part of the hull contains air. surrounding one or more gas-bags or ballonets within it.\n\nThe problem of the exact determination of the pressure on an airship envelope is still problematic and has fascinated major scientists such as Theodor Von Karman over history.\n\nFins at the rear of the envelope stabilize the airship, allowing it to fly straight. On some smaller designs these fins are themselves part of a gas bag and gain their shape only when inflated.\n\nA few airships have been metal-clad, with rigid and nonrigid examples made. Each kind used a thin gastight metal envelope, rather than the usual rubber-coated fabric envelope. Only four metal-clad ships are known to have been built, and only two actually flew: Schwarz's first aluminum rigid airship of 1893 collapsed, while his second flew; the nonrigid ZMC-2 built for the U.S. Navy flew from 1929 to 1941 when it was scrapped as too small for operational use on anti-submarine patrols; while the 1929 nonrigid Slate Aircraft Corporation \"City of Glendale\" collapsed on its first flight attempt. Both nonrigid ships nevertheless had strong metal monocoque envelopes that, while they maintained their shape uninflated, required an overpressure during flight.\n\nEarly airships used hydrogen as their lifting gas, which is the lightest available. Typically, hydrogen was generated during the filling process, by reacting dilute sulphuric acid with metal filings. The first hydrogen balloon in 1783 used iron filings, while the British Nulli Secundus of 1907 used zinc.\n\nLater, the United States began to use helium because it is non-flammable and has 92.7% of the buoyancy (lifting power) of hydrogen. Following a series of airship disasters in the 1930s, and especially the Hindenburg disaster where the airship burst into flames, hydrogen fell into disuse.\n\nThermal airships use a heated lifting gas, usually air, in a fashion similar to hot air balloons. The first to do so was flown in 1973 by the British company Cameron Balloons.\n\nThe term \"gondola\" is used to describe a crew car of an airship, slung beneath the centre of the envelope. These may be short, for cockpit and landing gear alone, or longer to provide passenger space. Early gondolas were open structures slung beneath the envelope, while later ones were enclosed and hung directly from the internal framing. A nonrigid blimp carries all of its passengers within a gondola. Rigid airships may have further passenger or cargo space inside the envelope. The large airship \"Graf Zeppelin\" was noted for its distinctively short passenger gondola, mounted far forward so as to improve ground clearance. The majority of crew accommodation and cargo holds were placed inside the envelope.\n\nSmall airships carry their engine(s) in their gondola. Where there were multiple engines on larger airships, these were placed in separate nacelles, termed \"power cars\" or \"engine cars\". To allow asymmetric thrust to be applied for maneuvering, these power cars were mounted towards the sides of the envelope, away from the centre line gondola. This also raised them above the ground, reducing the risk of a propeller strike when landing. Widely spaced power cars were also termed \"wing cars\", from the use of \"wing\" to mean being on the side of something, as in a theater, rather than the aerodynamic device. These engine cars carried a crew during flight who maintained the engines as needed, but who also worked the engine controls, throttle etc., mounted directly on the engine. Instructions were relayed to them from the pilot's station by a telegraph system, as on a ship.\n\nWhile elevators and swivelling propellers provide fine control of altitude, larger changes of height used to be controlled by either venting gas to lose altitude or dropping ballast to gain altitude. Large airships typically carried several water tanks fore and aft, allowing them to adjust longitudinal trim as well as height. Some modern designs instead pump lifting gas between the gas bags and storage cylinders.\n\nThe main advantage of airships with respect to any other vehicle is of environmental nature. They require less energy to remain in flight, if compared to any other air vehicle.\n\nIn 1670 the Jesuit Father Francesco Lana de Terzi, sometimes referred to as the \"Father of Aeronautics\", published a description of an \"Aerial Ship\" supported by four copper spheres from which the air was evacuated. Although the basic principle is sound, such a craft was unrealizable then and remains so to the present day, since external air pressure would cause the spheres to collapse unless their thickness was such as to make them too heavy to be buoyant. A hypothetical craft constructed using this principle is known as a \"Vacuum airship\".\n\nA more practical dirigible airship was described by Lieutenant Jean Baptiste Marie Meusnier in a paper entitled \"Mémoire sur l’équilibre des machines aérostatiques\" (Memorandum on the equilibrium of aerostatic machines) presented to the French Academy on 3 December 1783. The 16 water-color drawings published the following year depict a streamlined envelope with internal ballonnets that could be used for regulating lift: this was attached to a long carriage that could be used as a boat if the vehicle was forced to land in water. The airship was designed to be driven by three propellers and steered with a sail-like aft rudder. In 1784 Jean-Pierre Blanchard fitted a hand-powered propeller to a balloon, the first recorded means of propulsion carried aloft. In 1785 he crossed the English Channel in a balloon equipped with flapping wings for propulsion and a birdlike tail for steering.\n\nThe 19th century saw continued attempts to add methods of propulsion to balloons. The Australian William Bland sent designs for his \"Atmotic Airship\" to the Great Exhibition held in London in 1851, where a model was displayed. This was an elongated balloon with a steam engine driving twin propellers suspended underneath. The lift of the balloon was estimated as 5 tons and the car with the fuel as weighing 3.5 tons, giving a payload of 1.5 tons. Bland believed that the machine could be driven at and could fly from Sydney to London in less than a week.\n\nIn 1852 Henri Giffard became the first person to make an engine-powered flight when he flew in a steam-powered airship. Airships would develop considerably over the next two decades. In 1863 Solomon Andrews flew his aereon design, an unpowered, controllable dirigible in Perth Amboy, New Jersey and offered the device to the U.S. Military during the Civil War. He flew a later design in 1866 around New York City and as far as Oyster Bay, New York. This concept used changes in lift to provide propulsive force, and did not need a powerplant. In 1872, the French naval architect Dupuy de Lome launched a large navigable balloon, which was driven by a large propeller turned by eight men. It was developed during the Franco-Prussian war and was intended as an improvement to the balloons used for communications between Paris and the countryside during the siege of Paris, but was completed only after the end of the war.\n\nIn 1872 Paul Haenlein flew an airship with an internal combustion engine running on the coal gas used to inflate the envelope, the first use of such an engine to power an aircraft. Charles F. Ritchel made a public demonstration flight in 1878 of his hand-powered one-man rigid airship, and went on to build and sell five of his aircraft.\nIn 1874 Micajah Clark Dyer filed U.S. Patent 154,654 'Apparatus for Navigating the Air\". It is believed successful trial flights were made between 1872-1874, but detailed dates are not available. The apparatus used a combination of wings and paddle wheels for navigation and propulsion. “In operating the machinery the wings receive an upward and downward motion, in the manner of the wings of a bird, the outer ends yielding as they are raised, but opening out and then remaining rigid while being depressed. The wings, if desired, may be set at an angle so as to propel forward as well as to raise the machine in the air. The paddle-wheels are intended to be used for propelling the machine, in the same way that a vessel is propelled in water. An instrument answering to a rudder is attached for guiding the machine. A balloon is to be used for elevating the flying ship, after which it is to be guided and controlled at the pleasure of its occupants.”. More details can be found in the book about his life.\n\nIn 1883 the first electric-powered flight was made by Gaston Tissandier, who fitted a Siemens electric motor to an airship.\n\nThe first fully controllable free flight was made in 1884 by Charles Renard and Arthur Constantin Krebs in the French Army airship \"La France\". La France made the first flight of an airship that landed where it took off; the long, airship covered in 23 minutes with the aid of an electric motor, and a battery. It made seven flights in 1884 and 1885.\n\nIn 1888 the design of the Campbell Air Ship, designed by Professor Peter C. Campbell, was submitted to aeronautic engineer Carl Edgar Myers for examination. After his approval it was built by the Novelty Air Ship Company. It was lost at sea in 1889 while being flown by Professor Hogan during an exhibition flight.\n\nIn 1888–97 Frederich Wölfert built three airships powered by Daimler Motoren Gesellschaft-built petrol engines, the last of which caught fire in flight and killed both occupants in 1897. The 1888 version used a 2 hp single cylinder Daimler engine and flew from Canstatt to Kornwestheim.\nIn 1897, an airship with an aluminum envelope was built by the Hungarian-Croatian engineer David Schwarz. It made its first flight at Tempelhof field in Berlin after Schwarz had died. His widow, Melanie Schwarz, was paid 15,000 marks by Count Ferdinand von Zeppelin to release the industrialist Carl Berg from his exclusive contract to supply Schwartz with aluminium.\n\nIn 1897—99 Konstantin Danilewsky, medical doctor and inventor from Kharkiv (now Ukraine, then Russian Empire), built four muscle-powered airships, of gas volume 150—180 cub.m. About 200 ascents were made within a framework of experimental flight program, at two locations, with no significant incidents \n\nIn July 1900, the Luftschiff Zeppelin LZ1 made its first flight. This led to the most successful airships of all time: the Zeppelins, named after Count von Zeppelin who began working on rigid airship designs in the 1890s, leading to the flawed LZ1 in 1900 and the more successful LZ2 in 1906. The Zeppelin airships had a framework composed of triangular lattice girders covered with fabric that contained separate gas cells. At first multiplane tail surfaces were used for control and stability: later designs had simpler cruciform tail surfaces. The engines and crew were accommodated in \"gondolas\" hung beneath the hull driving propellers attached to the sides of the frame by means of long drive shafts. Additionally, there was a passenger compartment (later a bomb bay) located halfway between the two engine compartments.\n\nAlberto Santos-Dumont was a wealthy Brazilian who lived in France and had a passion for flying. He designed 18 balloons and dirigibles before turning his attention to fixed-winged aircraft.\nOn 19 October 1901 he flew his airship \"Number 6\", a small semi-rigid with a detached keel, from the Parc Saint Cloud to and around the Eiffel Tower and back in under thirty minutes. This feat earned him the Deutsch de la Meurthe prize of 100,000 francs. Many inventors were inspired by Santos-Dumont's small airships and a veritable airship craze began worldwide. Many airship pioneers, such as the American Thomas Scott Baldwin, financed their activities through passenger flights and public demonstration flights. Stanley Spencer built the first British airship with funds from advertising baby food on the sides of the envelope. Others, such as Walter Wellman and Melvin Vaniman, set their sights on loftier goals, attempting two polar flights in 1907 and 1909, and two trans-Atlantic flights in 1910 and 1912.\n\nIn 1902, the Spanish engineer Leonardo Torres Quevedo published details of an innovative airship design in Spain and France. With a non-rigid body and internal bracing wires, it overcame the flaws of these types of aircraft as regards both rigid structure (zeppelin type) and flexibility, providing the airships with more stability during flight, and the capability of using heavier engines and a greater passenger load. In 1905, helped by Captain A. Kindelán, he built the airship \"España\" at the Guadalajara military base. Next year he patented his design without attracting official interest. In 1909 he patented an improved design that he offered to the French Astra company, who started mass-producing it in 1911 as the Astra-Torres airship. The distinctive three-lobed design was widely used during the Great War by the Entente powers.\n\nOther airship builders were also active before the war: from 1902 the French company Lebaudy Frères specialized in semirigid airships such as the \"Patrie\" and the \"République\", designed by their engineer Henri Julliot, who later worked for the American company Goodrich; the German firm Schütte-Lanz built the wooden-framed SL series from 1911, introducing important technical innovations; another German firm Luft-Fahrzeug-Gesellschaft built the \"Parseval-Luftschiff\" (PL) series from 1909, and Italian Enrico Forlanini's firm had built and flown the first two Forlanini airships.\n\nOn May 12, 1902, the inventor and Brazilian aeronaut Augusto Severo de Albuquerque Maranhao and his French mechanic, Georges Saché, died when they were flying over Paris in the airship called Pax. A marble plaque at number 81 of the Avenue du Maine in Paris, celebrates the location of Augusto Severo accident. The Catastrophe of the Balloon \"Le Pax\" is a 1902 short silent film recreation of the catastrophe, directed by Georges Méliès.\n\nIn Britain, the Army built their first dirigible, the \"Nulli Secundus\", in 1907. The Navy ordered the construction of an experimental rigid in 1908. Officially known as His Majesty's Airship No. 1 and nicknamed the \"Mayfly\", it broke its back in 1911 before making a single flight. Work on a successor did not start until 1913.\n\nIn 1910 Walter Wellman unsuccessfully attempted an aerial crossing of the Atlantic Ocean in the airship \"America\".\n\nThe prospect of airships as bombers had been recognized in Europe well before the airships were up to the task. H. G. Wells' \"The War in the Air\" (1908) described the obliteration of entire fleets and cities by airship attack. The Italian forces became the first to use dirigibles for a military purpose during the Italo–Turkish War, the first bombing mission being flown on 10 March 1912. It was World War I, however, that marked the airship's real debut as a weapon. The Germans, French and Italians all used airships for scouting and tactical bombing roles early in the war, and all learned that the airship was too vulnerable for operations over the front. The decision to end operations in direct support of armies was made by all in 1917.\n\nMany in the German military believed they had found the ideal weapon with which to counteract British naval superiority and strike at Britain itself, while more realistic airship advocates believed the zeppelin's value was as a long range scout/attack craft for naval operations. Raids on England began in January 1915 and peaked in 1916: following losses to the British defenses only a few raids were made in 1917–18, the last in August 1918. Zeppelins proved to be terrifying but inaccurate weapons. Navigation, target selection and bomb-aiming proved to be difficult under the best of conditions, and the cloud cover that was frequently encountered by the airships reduced accuracy even further. The physical damage done by airships over the course of the war was insignificant, and the deaths that they caused amounted to a few hundred. Nevertheless, the raid caused a significant diversion of British resources to defense efforts. The airships were initially immune to attack by aircraft and anti-aircraft guns: as the pressure in their envelopes was only just higher than ambient air, holes had little effect. But following the introduction of a combination of incendiary and explosive ammunition in 1916, their flammable hydrogen lifting gas made them vulnerable to the defending aeroplanes. Several were shot down in flames by British defenders, and many others destroyed in accidents. New designs capable of reaching greater altitude were developed, but although this made them immune from attack it made their bombing accuracy even worse.\n\nCountermeasures by the British included sound detection equipment, searchlights and anti-aircraft artillery, followed by night fighters in 1915. One tactic used early in the war, when their limited range meant the airships had to fly from forward bases and the only zeppelin production facilities were in Friedrichshafen, was the bombing of airship sheds by the British Royal Naval Air Service. Later in the war, the development of the aircraft carrier led to the first successful carrier-based air strike in history: on the morning of 19 July 1918, seven Sopwith 2F.1 Camels were launched from and struck the airship base at Tondern, destroying zeppelins L 54 and L 60.\n\nThe British Army had abandoned airship development in favour of aeroplanes before the start of the war, but the Royal Navy had recognized the need for small airships to counteract the submarine and mine threat in coastal waters. Beginning in February 1915, they began to develop the SS (Sea Scout) class of blimp. These had a small envelope of and at first used aircraft fuselages without the wing and tail surfaces as control cars. Later, more advanced blimps with purpose-built gondolas were used. The NS class (North Sea) were the largest and most effective non-rigid airships in British service, with a gas capacity of , a crew of 10 and an endurance of 24 hours. Six bombs were carried, as well as three to five machine guns. British blimps were used for scouting, mine clearance, and convoy patrol duties. During the war, the British operated over 200 non-rigid airships. Several were sold to Russia, France, the United States, and Italy. The large number of trained crews, low attrition rate and constant experimentation in handling techniques meant that at the war's end Britain was the world leader in non-rigid airship technology.\n\nThe Royal Navy continued development of rigid airships until the end of the war. Eight rigid airships had been completed by the armistice, (No. 9r, four 23 Class, two R23X Class and one R31 Class), although several more were in an advanced state of completion by the war's end. Both France and Italy continued to use airships throughout the war. France preferred the non-rigid type, whereas Italy flew 49 semi-rigid airships in both the scouting and bombing roles.\n\nAeroplanes had essentially replaced airships as bombers by the end of the war, and Germany's remaining zeppelins were destroyed by their crews, scrapped or handed over to the Allied powers as war reparations. The British rigid airship program, which had mainly been a reaction to the potential threat of the German airships, was wound down.\n\nA number of nations operated airships between the two world wars. Britain, the United States and Germany were the only constructors of rigid airships, with Italy and France making limited use of Zeppelins handed over as war reparations. Italy, the Soviet Union, the United States and Japan mainly operated semi-rigid airships.\n\nUnder the terms of the Treaty of Versailles, Germany was not allowed to build airships of greater capacity than a million cubic feet. Two small passenger airships, LZ 120 \"Bodensee\" and its sister ship LZ 121 \"Nordstern\", were built immediately after the war but were confiscated following the sabotage of the wartime Zeppelins that were to have been handed over as war reparations: \"Bodensee\" was given to Italy and \"Nordstern\" to France. On May 12, 1926, the Italian semi-rigid airship \"Norge\" was the first aircraft to fly over the North Pole.\n\nThe British R33 and R34 were near-identical copies of the German L 33, which had come down almost intact in Yorkshire on 24 September 1916. Despite being almost three years out of date by the time they were launched in 1919, they became two of the most successful airships in British service. The creation of the Royal Air Force (RAF) in early 1918 created a hybrid British airship program. The RAF was not interested in airships while the Admiralty was, so a deal was made where the Admiralty would design any future military airships and the RAF would handle manpower, facilities and operations. On 2 July 1919, R34 began the first double crossing of the Atlantic by an aircraft. It landed at Mineola, Long Island on 6 July after 108 hours in the air; the return crossing began on 8 July and took 75 hours. This feat failed to generate enthusiasm for continued airship development, and the British airship program was rapidly wound down.\n\nDuring World War One, the U.S. Navy acquired its first airship, the DH-1, but it was destroyed while being inflated shortly after delivery to the Navy. After the war, the U.S. Navy contracted to buy the R 38, which was being built in Britain, but before it was handed over it was destroyed because of a structural failure during a test flight.\nAmerica then started constructing the , designed by the Bureau of Aeronautics and based on the Zeppelin L 49. Assembled in Hangar No. 1 and first flown on 4 September 1923 at Lakehurst, New Jersey, it was the first airship to be inflated with the noble gas helium, which was then so scarce that the \"Shenandoah\" contained most of the world's supply. A second airship, , was built by the Zeppelin company as compensation for the airships that should have been handed over as war reparations according to the terms of the Versailles Treaty but had been sabotaged by their crews. This construction order saved the Zeppelin works from the threat of closure. The success of the \"Los Angeles\", which was flown successfully for eight years, encouraged the U.S. Navy to invest in its own, larger airships. When the \"Los Angeles\" was delivered, the two airships had to share the limited supply of helium, and thus alternated operating and overhauls.\n\nIn 1922 Sir Dennistoun Burney suggested a plan for a subsidised air service throughout the British Empire using airships (the Burney Scheme). Following the coming to power of Ramsay MacDonald's Labour government in 1924, the scheme was transformed into the Imperial Airship Scheme, under which two airships were built, one by a private company and the other by the Royal Airship Works under Air Ministry control. The two designs were radically different. The \"capitalist\" ship, the \"R100\", was more conventional, while the \"socialist\" ship, the R101, had many innovative design features. Construction of both took longer than expected, and the airships did not fly until 1929. Neither airship was capable of the service intended, though the R100 did complete a proving flight to Canada and back in 1930. On 5 October 1930, however, the R101, which had not been thoroughly tested after major modifications, crashed on its maiden voyage at Beauvais in France killing 48 of the 54 people aboard. Among the dead were the craft's chief designer and the Secretary of State for Air. The disaster put an end to further British airship development.\n\nThe Locarno Treaties of 1925 lifted the restrictions on German airship construction, and the Zeppelin company started construction of the \"Graf Zeppelin\" (LZ 127), the largest airship that could be built in the company's existing shed, and intended to stimulate interest in passenger airships. The \"Graf Zeppelin\" burned \"blau gas\", similar to propane, stored in large gas bags below the hydrogen cells, as fuel. Since its density was similar to that of air, it avoided the weight change as fuel was used, and thus the need to valve hydrogen. The \"Graf Zeppelin\" was a great success and had an impressive safety record, flying over (including the first circumnavigation of the globe by air) without a single passenger injury.\nThe U.S. Navy experimented with the use of airships as airborne aircraft carriers, developing an idea pioneered by the British. The USS \"Los Angeles\" was used for initial experiments, and the and , the world's largest at the time, were used to test the principle in naval operations. Each carried four F9C Sparrowhawk fighters in its hangar, and could carry a fifth on the trapeze. The idea had mixed results. By the time the Navy started to develop a sound doctrine for using the ZRS-type airships, the last of the two built, USS \"Macon\", had been lost. The seaplane had become more capable, and was considered a better investment.\n\nEventually, the U.S. Navy lost all three U.S.-built rigid airships to accidents. USS \"Shenandoah\" flew into a severe thunderstorm over Noble County, Ohio while on a poorly planned publicity flight on 3 September 1925. It broke into pieces, killing 14 of its crew. USS \"Akron\" was caught in a severe storm and flown into the surface of the sea off the shore of New Jersey on 3 April 1933. It carried no life boats and few life vests, so 73 of its crew of 76 died from drowning or hypothermia. USS \"Macon\" was lost after suffering a structural failure offshore near Point Sur Lighthouse on 12 February 1935. The failure caused a loss of gas, which was made much worse when the aircraft was driven over pressure height causing it to lose too much helium to maintain flight. Only two of its crew of 83 died in the crash thanks to the inclusion of life jackets and inflatable rafts after the \"Akron\" disaster.\n\nThe Empire State Building was completed in 1931 with a dirigible mast, in anticipation of passenger airship service. Various entrepreneurs experimented with commuting and shipping freight via airship.\n\nIn the 1930s the German Zeppelins successfully competed with other means of transport. They could carry significantly more passengers than other contemporary aircraft while providing amenities similar to those on ocean liners, such as private cabins, observation decks, and dining rooms. Less importantly, the technology was potentially more energy-efficient than heavier-than-air designs. Zeppelins were also faster than ocean liners. On the other hand, operating airships was quite involved. Often the crew would outnumber passengers, and on the ground large teams were necessary to assist mooring and very large hangars were required at airports.\nBy the mid-1930s only Germany still pursued airship development. The Zeppelin company continued to operate the \"Graf Zeppelin\" on passenger service between Frankfurt and Recife in Brazil, taking 68 hours. Even with the small \"Graf Zeppelin\", the operation was almost profitable. In the mid-1930s work started to build an airship designed specifically to operate a passenger service across the Atlantic. The \"Hindenburg\" (LZ 129) completed a successful 1936 season, carrying passengers between Lakehurst, New Jersey and Germany. 1937 started with the most spectacular and widely remembered airship accident. Approaching the Lakehurst mooring mast minutes before landing on 6 May 1937, the \"Hindenburg\" burst into flames and crashed. Of the 97 people aboard, 36 died: 13 passengers, 22 aircrew, and one American ground-crewman. The disaster happened before a large crowd, was filmed and a radio news reporter was recording the arrival. This was a disaster that theater goers could see and hear in newsreels. The \"Hindenburg\" disaster shattered public confidence in airships, and brought a definitive end to their \"golden age\". The day after the \"Hindenburg\" crashed, the \"Graf Zeppelin\" landed at the end of its flight from Brazil. This was the last international passenger airship flight.\n\n\"Hindenburg\"s sister ship, the \"Graf Zeppelin II\" (LZ 130), could not carry commercial passengers without helium, which the United States refused to sell to Germany during wartime. The \"Graf Zeppelin\" made several test flights and conducted some electronic espionage until 1939 when it was grounded due to the beginning of the war. The two \"Graf Zeppelins\" were scrapped in the spring of 1940.\n\nDevelopment of airships continued only in the United States, and to a smaller extent, the Soviet Union. The Soviet Union had several semi-rigid and non-rigid airships. The semi-rigid dirigible SSSR-V6 OSOAVIAKhIM was among the largest of these craft, and it set the longest endurance flight at the time of over 130 hours. It crashed into a mountain in 1938, however, killing 13 of the 19 people on board. While this was a severe blow to the Soviet airship program, they continued to operate non-rigid airships until 1950.\n\nWhile Germany determined that airships were obsolete for military purposes in the coming war and concentrated on the development of aeroplanes, the United States pursued a program of military airship construction even though it had not developed a clear military doctrine for airship use. When the Japanese attacked Pearl Harbor on 7 December 1941, bringing the United States into World War II, the U.S. Navy had 10 nonrigid airships:\nOnly \"K\"- and \"TC\"-class airships were suitable for combat and they were quickly pressed into service against Japanese and German submarines, which were then sinking American shipping within visual range of the American coast. U.S. Navy command, remembering airship's anti-submarine success in World War I, immediately requested new modern antisubmarine airships and on 2 January 1942 formed the ZP-12 patrol unit based in Lakehurst from the four \"K\" airships. The ZP-32 patrol unit was formed from two \"TC\" and two \"L\" airships a month later, based at NAS Moffett Field in Sunnyvale, California. An airship training base was created there as well. The status of submarine-hunting Goodyear airships in the early days of World War II has created significant confusion. Although various accounts refer to airships \"Resolute\" and \"Volunteer\" as operating as \"privateers\" under a Letter of Marque, Congress never authorized a commission, nor did the President sign one.\nIn the years 1942–44, approximately 1,400 airship pilots and 3,000 support crew members were trained in the military airship crew training program and the airship military personnel grew from 430 to 12,400. The U.S. airships were produced by the Goodyear factory in Akron, Ohio. From 1942 till 1945, 154 airships were built for the U.S. Navy (133 \"K\"-class, 10 \"L\"-class, seven \"G\"-class, four \"M\"-class) and five \"L\"-class for civilian customers (serial numbers \"L-4\" to \"L-8\").\n\nThe primary airship tasks were patrol and convoy escort near the American coastline. They also served as an organization centre for the convoys to direct ship movements, and were used in naval search and rescue operations. Rarer duties of the airships included aerophoto reconnaissance, naval mine-laying and mine-sweeping, parachute unit transport and deployment, cargo and personnel transportation. They were deemed quite successful in their duties with the highest combat readiness factor in the entire U.S. air force (87%).\n\nDuring the war, some 532 ships without airship escort were sunk near the U.S. coast by enemy submarines. Only one ship, the tanker \"Persephone\", of the 89,000 or so in convoys escorted by blimps was sunk by the enemy. Airships engaged submarines with depth charges and, less frequently, with other on-board weapons. They were excellent at driving submarines down, where their limited speed and range prevented them from attacking convoys. The weapons available to airships were so limited that until the advent of the homing torpedo they had little chance of sinking a submarine.\n\nOnly one airship was ever destroyed by U-boat: on the night of 18/19 July 1943, the \"K-74\" from ZP-21 division was patrolling the coastline near Florida. Using radar, the airship located a surfaced German submarine. The \"K-74\" made her attack run but the U-boat opened fire first. \"K-74\"s depth charges did not release as she crossed the U-boat and the \"K-74\" received serious damage, losing gas pressure and an engine but landing in the water without loss of life. The crew was rescued by patrol boats in the morning, but one crewman, Aviation Machinist's Mate Second Class Isadore Stessel, died from a shark attack. The U-Boat, , was slightly damaged and the next day or so was attacked by aircraft, sustaining damage that forced it to return to base. It was finally sunk on 24 August 1943 by a British Vickers Wellington near Vigo, Spain.\n\nFleet Airship Wing One operated from Lakehurst, New Jersey, Glynco, Georgia, Weeksville, North Carolina, South Weymouth NAS Massachusetts, Brunswick NAS and Bar Harbor Maine, Yarmouth, Nova Scotia, and Argentia, Newfoundland.\nSome Navy blimps saw action in the European war theater. In 1944-45, the U.S. Navy moved an entire squadron of eight Goodyear K class blimps (K-89, K-101, K-109, K-112, K-114, K-123, K-130, & K-134) with flight and maintenance crews from Weeksville Naval Air Station in North Carolina to Naval Air Station Port Lyautey, French Morocco. Their mission was to locate and destroy German U-boats in the relatively shallow waters around the Strait of Gibraltar where magnetic anomaly detection (MAD) was viable. PBY aircraft had been searching these waters but MAD required low altitude flying that was dangerous at night for these aircraft. The blimps were considered a perfect solution to establish a 24/7 MAD barrier (fence) at the Straits of Gibraltar with the PBYs flying the day shift and the blimps flying the night shift. The first two blimps (K-123 & K-130) left South Weymouth NAS on 28 May 1944 and flew to Argentia, Newfoundland, the Azores, and finally to Port Lyautey where they completed the first transatlantic crossing by nonrigid airships on 1 June 1944. The blimps of USN Blimp Squadron ZP-14 (Blimpron 14, aka \"The Africa Squadron\") also conducted mine-spotting and mine-sweeping operations in key Mediterranean ports and various escorts including the convoy carrying United States President Franklin D. Roosevelt and British Prime Minister Winston Churchill to the Yalta Conference in 1945. Airships from the ZP-12 unit took part in the sinking of the last U-Boat before German capitulation, sinking the \"U-881\" on 6 May 1945 together with destroyers Atherton and Mobery.\n\nOther airships patrolled the Caribbean, Fleet Airship Wing Two, Headquartered at NAS Richmond, Florida, covered the Gulf of Mexico from Richmond and Key West, Florida, Houma, Louisiana, as well as Hitchcock and Brownsville, Texas. FAW 2 also patrolled the northern Caribbean from San Julian, the Isle of Pines (now called Isla de la Juventud) and Guantanamo Bay, Cuba as well as Vernam Field, Jamaica.\n\nNavy blimps of Fleet Airship Wing Five, (ZP-51) operated from bases in Trinidad, British Guiana and Paramaribo, Suriname. Fleet Airship Wing Four operated along the coast of Brazil. Two squadrons, VP-41 and VP-42 flew from bases at Amapá, Igarapé-Açu, São Luís Fortaleza, Fernando de Noronha, Recife, Maceió, Ipitanga (near Salvador, Bahia), Caravelas, Vitória and the hangar built for the \"Graf Zeppelin\" at Santa Cruz, Rio de Janeiro.\n\nFleet Airship Wing Three operated squadrons, ZP-32 from Moffett Field, ZP-31 at NAS Santa Ana, and ZP-33 at NAS Tillamook, Oregon. Auxiliary fields were at Del Mar, Lompoc, Watsonville and Eureka, California, North Bend and Astoria, Oregon, as well as Shelton and Quillayute in Washington.\n\nFrom 2 January 1942 until the end of war airship operations in the Atlantic, the blimps of the Atlantic fleet made 37,554 flights and flew 378,237 hours. Of the over 70,000 ships in convoys protected by blimps, only one was sunk by a submarine while under blimp escort.\n\nThe Soviet Union flew a single airship during the war. The \"W-12\", built in 1939, entered service in 1942 for paratrooper training and equipment transport. It made 1432 flights with 300 metric tons of cargo until 1945. On 1 February 1945, the Soviets constructed a second airship, a \"Pobeda\"-class (\"Victory\"-class) unit (used for mine-sweeping and wreckage clearing in the Black Sea) that crashed on 21 January 1947. Another \"W\"-class - W-12bis \"Patriot\" - was commissioned in 1947 and was mostly used until the mid 1950s for crew training, parades and propaganda.\n\nAlthough airships are no longer used for major cargo and passenger transport, they are still used for other purposes such as advertising, sightseeing, surveillance, research and advocacy.\n\nIn the 1980s, Per Lindstrand and his team introduced the \"GA-42\" airship, the first airship to use fly-by-wire flight control, which considerably reduced the pilot's workload.\n\nThe world's largest thermal airship () was constructed by the Per Lindstrand company for French botanists in 1993. The \"AS-300\" carried an underslung raft, which was positioned by the airship on top of tree canopies in the rain forest, allowing the botanists to carry out their treetop research without significant damage to the rainforest. When research was finished at a given location, the airship returned to pick up and relocate the raft.\n\nIn the spring of 2004, Lindstrand Technologies supplied the world's first fully functional unmanned airship to the Ministry of Defense in Spain. This airship carried a classified payload and its surveillance mission was also classified. Four years later, this airship, which is designated \"GA-22\", still flies on an almost daily basis.\n\nIn June 1987, the U.S. Navy awarded a US$168.9 million contract to Westinghouse Electric and Airship Industries of the UK to find out whether an airship could be used as an airborne platform to detect the threat of sea-skimming missiles, such as the Exocet. At 2.5 million cubic feet, the Westinghouse/Airship Industries Sentinel 5000 (Redesignated YEZ-2A by the U. S. Navy) prototype design was to have been the largest blimp ever constructed. Additional funding for the Naval Airship Program was killed in 1995, however, and development was discontinued.\n\nThe \"CA-80\" airship, which was produced in 2000 by Shanghai Vantage Airship Manufacture Co., Ltd., had a successful trial flight in September 2001. This was designed for the purpose of advertisement and propagation, air-photo, scientific test, tour and surveillance duties. It was certified as a grade-A Hi-Tech introduction program (№ 20000186) in Shanghai. The CAAC authority granted a type design approval and certificate of airworthiness for the airship.\n\nIn the 1990s the Zeppelin company returned to the airship business. Their new model, designated the Zeppelin NT, made its maiden flight on 18 September 1997. there were four NT aircraft flying, a fifth was completed in March 2009 and an expanded NT-14 (14,000 cubic meters of helium, capable of carrying 19 passengers) was under construction. One was sold to a Japanese company, and was planned to be flown to Japan in the summer of 2004. Due to delays getting permission from the Russian government, the company decided to transport the airship to Japan by sea. One of the four NT craft is in South Africa carrying diamond detection equipment from De Beers, an application at which the very stable low vibration NT platform excels. The project included design adaptations for high temperature operation and desert climate, as well as a separate mooring mast and a very heavy mooring truck. NT-4 belonged to Airship Ventures of Moffett Field, Mountain View in the San Francisco Bay Area, and provided sight-seeing tours.\n\nBlimps are used for advertising and as TV camera platforms at major sporting events. The most iconic of these are the Goodyear Blimps. Goodyear operates three blimps in the United States, and The Lightship Group, now The AirSign Airship Group, operates up to 19 advertising blimps around the world. Airship Management Services owns and operates three Skyship 600 blimps. Two operate as advertising and security ships in North America and the Caribbean. Airship Ventures operated a Zeppelin NT for advertising, passenger service and special mission projects. They were the only airship operator in the U.S. authorized to fly commercial passengers, until closing their doors in 2012.\n\nSkycruise Switzerland AG owns and operates two Skyship 600 blimps. One operates regularly over Switzerland used on sightseeing tours.\n\nThe Switzerland-based Skyship 600 has also played other roles over the years. For example, it was flown over Athens during the 2004 Summer Olympics as a security measure. In November 2006, it carried advertising calling it \"The Spirit of Dubai\" as it began a publicity tour from London to Dubai, UAE on behalf of The Palm Islands, the world's largest man-made islands created as a residential complex.\n\nLos Angeles-based Worldwide Aeros Corp. produces FAA Type Certified Aeros 40D Sky Dragon airships.\n\nIn May 2006, the U.S. Navy began to fly airships again after a hiatus of nearly 44 years. The program uses a single American Blimp Company A-170 nonrigid airship, with designation MZ-3A. Operations focus on crew training and research, and the platform integrator is Northrop Grumman. The program is directed by the Naval Air Systems Command and is being carried out at NAES Lakehurst, the original centre of U.S. Navy lighter-than-air operations in previous decades.\n\nIn November 2006 the U.S. Army bought an A380+ airship from American Blimp Corporation through a Systems level contract with Northrop Grumman and Booz Allen Hamilton. The airship started flight tests in late 2007, with a primary goal of carrying of payload to an altitude of under remote control and autonomous waypoint navigation. The program will also demonstrate carrying of payload to The platform could be used for Multi-Intelligence collections. In 2008, the \"CA-150\" airship was launched by Vantage Airship. This is an improved modification of model \"CA-120\" and completed manufacturing in 2008. With larger volume and increased passenger capacity, it is the largest manned nonrigid airship in China at present.\n\nAn airship was prominently featured in the James Bond film \"A View to a Kill\", released in 1985. The Skyship 500 had the livery of Zorin Industries.\n\nIn late June 2014 the Electronic Frontier Foundation flew the GEFA-FLUG AS 105 GD/4 blimp AE Bates (owned by, and in conjunction with, Greenpeace) over the NSA's Bluffdale Utah Data Center in protest.\n\nHybrid designs such as the Heli-Stat airship/helicopter, the Aereon aerostatic/aerodynamic craft, and the CycloCrane (a hybrid aerostatic/rotorcraft), struggled to take flight. The Cyclocrane was also interesting in that the airship's envelope rotated along its longitudinal axis.\n\nIn 2005, a short-lived project of the U.S. Defense Advanced Research Projects Agency (DARPA) was Walrus HULA, which explored the potential for using airships as long-distance, heavy lift craft. The primary goal of the research program was to determine the feasibility of building an airship capable of carrying of payload a distance of and land on an unimproved location without the use of external ballast or ground equipment (such as masts). In 2005, two contractors, Lockheed Martin and US Aeros Airships were each awarded approximately $3 million to do feasibility studies of designs for WALRUS. Congress removed funding for Walrus HULA in 2006.\n\nIn 2010, the U.S. Army awarded a $517 million (£350.6 million) contract to Northrop Grumman and partner Hybrid Air Vehicles to develop a Long Endurance Multi-Intelligence Vehicle (LEMV) system, in the form of three HAV 304s. The project was cancelled in February 2012 due to it being behind schedule and over budget; also the forthcoming U.S. withdrawal from Afghanistan where it was intended to be deployed. Following this the Hybrid Air Vehicles HAV 304 Airlander 10 was repurchased by Hybrid Air Vehicles then modified and reassembled in Bedford, UK, and renamed the Airlander 10. It is currently being tested in readiness for its UK flight test programme.\n\n, a French company, manufactures and operates airships and aerostats. For 2 years, A-NSE has been testing its airships for the French Army. Airships and aerostats are operated to provide intelligence, surveillance, and reconnaissance (ISR) support. Their airships include many innovative features such as water ballast take-off and landing systems, variable geometry envelopes and thrust–vectoring systems.\n\nThe U..S government has funded two major projects in the high altitude arena. The Composite Hull High Altitude Powered Platform (CHHAPP) is sponsored by U.S. Army Space and Missile Defense Command. This aircraft is also sometimes called \"HiSentinel High-Altitude Airship\". This prototype ship made a five-hour test flight in September 2005. The second project, the high-altitude airship (HAA), is sponsored by DARPA. In 2005, DARPA awarded a contract for nearly $150 million to Lockheed Martin for prototype development. First flight of the HAA was planned for 2008 but suffered programmatic and funding delays. The HAA project evolved into the High Altitude Long Endurance-Demonstrator (HALE-D). The U.S. Army and Lockheed Martin launched the first-of-its kind HALE-D on July 27, 2011. After attaining an altitude of , due to an anomaly, the company decided to abort the mission. The airship made a controlled descent in an unpopulated area of southwest Pennsylvania.\n\nOn 31 January 2006 Lockheed Martin made the first flight of their secretly built hybrid airship designated the P-791. The design is very similar to the SkyCat, unsuccessfully promoted for many years by the British company Advanced Technologies Group (ATG). Although Lockheed Martin is developing a design for the DARPA WALRUS HULA project, it claimed that the P-791 is unrelated to WALRUS. Nonetheless, the design represents an approach that may well be applicable to WALRUS. Some believe that Lockheed Martin had used the secret P-791 program as a way to get a head start on the other WALRUS competitor, US Aeros Airships.\n\nIn the 1990s, the successor of the original Zeppelin company in Friedrichshafen, the \"Zeppelin Luftschifftechnik GmbH\", reengaged in airship construction. The first experimental craft (later christened \"Friedrichshafen\") of the type \"Zeppelin NT\" flew in September 1997. Though larger than common blimps, the \"Neue Technologie\" (New Technology) zeppelins are much smaller than their giant ancestors and not actually Zeppelin-types in the classical sense. They are sophisticated semirigids. Apart from the greater payload, their main advantages compared to blimps are higher speed and excellent maneuverability. Meanwhile, several \"Zeppelin NT\" have been produced and operated profitably in joyrides, research flights and similar applications.\n\nIn June 2004, a Zeppelin NT was sold for the first time to a Japanese company, Nippon Airship Corporation, for tourism and advertising mainly around Tokyo. It was also given a role at the 2005 Expo in Aichi. The aircraft began a flight from Friedrichshafen to Japan, stopping at Geneva, Paris, Rotterdam, Munich, Berlin, Stockholm and other European cities to carry passengers on short legs of the flight. Russian authorities denied overflight permission, however, so the airship had to be dismantled and shipped to Japan rather than following the historic \"Graf Zeppelin\" flight from Germany to Japan.\n\nIn 2008, Airship Ventures Inc. began operations from Moffett Federal Airfield near Mountain View, California and until November 2012 offered tours of the San Francisco Bay Area for up to 12 passengers.\n\nIn November 2005, De Beers, a diamond mining company, launched an airship exploration program over the remote Kalahari desert. A Zeppelin NT, equipped with a Bell Geospace gravity gradiometer, was used to find potential diamond mines by scanning the local geography for low-density rock formations, known as kimberlite pipes. On 21 September 2007, the airship was severely damaged by a whirlwind while in Botswana. One crew member, who was on watch aboard the moored craft, was slightly injured but released after overnight observation in hospital.\nSeveral companies, such as Cameron Balloons in Bristol, United Kingdom, build hot-air airships. These combine the structures of both hot-air balloons and small airships. The envelope is the normal cigar shape, complete with tail fins, but is inflated with hot air instead of helium to provide the lifting force. A small gondola, carrying the pilot and passengers, a small engine, and the burners to provide the hot air are suspended below the envelope, beneath an opening through which the burners protrude.\n\nHot-air airships typically cost less to buy and maintain than modern helium-based blimps, and can be quickly deflated after flights. This makes them easy to carry in trailers or trucks and inexpensive to store. They are usually very slow moving, with a typical top speed of 25–30 km/h (15–20 mph, 6.7–8.9 m/s). They are mainly used for advertising, but at least one has been used in rainforests for wildlife observation, as they can be easily transported to remote areas.\n\nRemote-controlled (RC) airships, a type of unmanned aerial system (UAS), are sometimes used for commercial purposes such as advertising and aerial video and photography as well as recreational purposes. They are particularly common as an advertising mechanism at indoor stadiums. While RC airships are sometimes flown outdoors, doing so for commercial purposes is illegal in the US. Commercial use of an unmanned airship must be certified under part 121.\n\nToday, with large, fast, and more cost-efficient fixed-wing aircraft and helicopters, it is unknown whether huge airships can operate profitably in regular passenger transport though, as energy costs rise, attention is once again returning to these lighter-than-air vessels as a possible alternative. At the very least, the idea of comparatively slow, \"majestic\" cruising at relatively low altitudes and in comfortable atmosphere certainly has retained some appeal. There have been some niches for airships in and after World War II, such as long-duration observations, antisubmarine patrol, platforms for TV camera crews, and advertising; these, however, generally require only small and flexible craft, and have thus generally been better fitted for cheaper (non-passenger) blimps.\n\nIt has periodically been suggested that airships could be employed for cargo transport, especially delivering extremely heavy loads to areas with poor infrastructure over great distances. This has also been called roadless trucking. Also, airships could be used for heavy lifting over short distances (e.g. on construction sites); this is described as heavy-lift, short-haul. In both cases, the airships are heavy haulers. One recent enterprise of this sort was the \"Cargolifter\" project, in which a hybrid (thus not entirely Zeppelin-type) airship even larger than \"Hindenburg\" was projected. Around 2000, CargoLifter AG built the world's largest self-supporting hall, measuring long, wide and high about south of Berlin. In May 2002, the project was stopped for financial reasons; the company had to file bankruptcy. The enormous CargoLifter hangar was later converted to house the Tropical Islands Resort. Although no rigid airships are currently used for heavy lifting, hybrid airships are being developed for such purposes. AEREON 26, tested in 1971, was described in John McPhee's \"The Deltoid Pumpkin Seed\".\n\nAn impediment to the large-scale development of airships as heavy haulers has been figuring out how they can be used in a cost-efficient way. In order to have a significant economic advantage over ocean transport, cargo airships must be able to deliver their payload faster than ocean carriers but more cheaply than airplanes. William Crowder, a fellow at the Logistics Management Institute, has calculated that cargo airships are only economical when they can transport 500 to 1,000 tons, approximately the same as a super-jumbo aircraft. The large initial investment required to build such a large airship has been a hindrance to production, especially given the risk inherent in a new technology. The chief commercial officer of the company hoping to sell the LMH-1, a cargo airship currently being developed by Lockheed Martin, believes that airships can be economical in hard-to-reach locations such as mining operations in northern Canada that currently require ice roads.\n\nA metal-clad airship has a very thin metal envelope, rather than the usual fabric. The shell may be either internally braced or monocoque as in the ZMC-2, which flew many times in the 1920s, the only example ever to do so. The shell may be gas-tight as in a non-rigid blimp, or the design may employ internal gas bags as in a rigid airship. Compared to a fabric envelope the metal cladding is expected to be more durable.\n\nA hybrid airship is a general term for an aircraft that combines characteristics of heavier-than-air (aeroplane or helicopter) and lighter-than-air technology. Examples include helicopter/airship hybrids intended for heavy lift applications and dynamic lift airships intended for long-range cruising. It should be noted that most airships, when fully loaded with cargo and fuel, are usually ballasted to be heavier than air, and thus must use their propulsion system and shape to create aerodynamic lift, necessary to stay aloft. All airships can be operated to be slightly heavier than air at periods during flight (descent). Accordingly, the term \"hybrid airship\" refers to craft that obtain a significant portion of their lift from aerodynamic lift or other kinetic means.\n\nFor example, the Aeroscraft is a buoyancy assisted air vehicle that generates lift through a combination of aerodynamics, thrust vectoring and gas buoyancy generation and management, and for much of the time will fly heavier than air. Aeroscraft is Worldwide Aeros Corporation's continuation of DARPA's now cancelled Walrus HULA (Hybrid Ultra Large Aircraft) project.\n\nThe Patroller P3 hybrid airship developed by Advanced Hybrid Aircraft Ltd, BC, Canada, is a relatively small (85,000 feet3 = 2,400 m3) buoyant craft, manned by the crew of 5 and with the endurance of up to 72 hours. The flight-tests with the 40% RC scale model proved that such a craft can be launched and landed without a large team of strong ground-handlers. Design features a special “winglet” for aerodynamic lift control.\n\nAirships have been proposed as a potential cheap alternative to surface rocket launches for achieving Earth orbit. JP Aerospace have proposed the Airship to Orbit project, which intends to float a multi-stage airship up to mesospheric altitudes of 55 km (180,000 ft) and then use ion propulsion to accelerate to orbital speed. At these heights, air resistance would not be a significant problem for achieving such speeds. The company has not yet built any of the three stages.\n\nNASA have proposed the High Altitude Venus Operational Concept, which comprises a series of five missions including manned missions to the atmosphere of Venus in airships. Pressures on the surface of the planet are too high for human habitation, but at a specific altitude the pressure is equal to that found on Earth and this makes Venus a potential target for human colonization.\n\nThe advantage of airships over aeroplanes is that static lift sufficient for flight is generated by the lifting gas and requires no engine power. This was an immense advantage before the middle of World War I and remained an advantage for long-distance or long-duration operations until World War II. Modern concepts for high-altitude airships include photovoltaic cells to reduce the need to land to refuel, thus they can remain in the air until consumables expire.\n\nThe disadvantages are that an airship has a very large reference area and comparatively large drag coefficient, thus a larger drag force compared to that of aeroplanes and even helicopters. Given the large frontal area and wetted surface of an airship, a practical limit is reached around . Thus airships are used where speed is not critical.\n\nThe lift capability of an airship is equal to the buoyant force minus the weight of the airship. This assumes standard air-temperature and pressure conditions. Corrections are usually made for water vapor and impurity of lifting gas, as well as percentage of inflation of the gas cells at liftoff. Based on specific lift (lifting force per unit volume of gas), the greatest static lift is provided by hydrogen (11.15 N/m or 71 lb/1000 cu ft) with helium (10.37 N/m or 66 lb/1000 cu ft) a close second. At 6.13 N/m (39 lb/1000 cu ft), steam is a distant third. Other cheap gases, such as methane, carbon monoxide, ammonia and natural gas have even less lifting capacity and are flammable, toxic, corrosive, or all three (neon is even more costly than helium, with less lifting capacity). Operational considerations such as whether the lift gas can be economically vented and produced in flight for control of buoyancy (as with hydrogen) or even produced as a byproduct of propulsion (as with steam) affect the practical choice of lift gas in airship designs.\n\nIn addition to static lift, an airship can obtain a certain amount of dynamic lift from its engines. Dynamic lift in past airships has been about 10% of the static lift. Dynamic lift allows an airship to \"take off heavy\" from a runway similar to fixed-wing and rotary-wing aircraft. This requires additional weight in engines, fuel, and landing gear, however, negating some of the static lift capacity.\n\nThe altitude at which an airship can fly largely depends on how much lifting gas it can lose due to expansion before stasis is reached. The ultimate altitude record for a rigid airship was set in 1917 by the L-55 under the command of Hans-Kurt Flemming when he forced the airship to attempting to cross France after the \"Silent Raid\" on London. The L-55 lost lift during the descent to lower altitudes over Germany and crashed due to loss of lift. While such waste of gas was necessary for the survival of airships in the later years of World War I, it was impractical for commercial operations, or operations of helium-filled military airships. The highest flight made by a hydrogen-filled passenger airship was on the \"Graf Zeppelin's\" around-the-world flight. The practical limit for rigid airships was about , and for pressure airships around .\n\nModern airships use dynamic helium volume. At sea-level altitude, helium takes up only a small part of the hull, while the rest is filled with air. As the airship ascends, the helium inflates with reduced outer pressure, and air is pushed out and released from the downward valve. This allows an airship to reach any altitude with balanced inner and outer pressure if the buoyancy is enough. Some civil aerostats could reach without explosion due to overloaded inner pressure.\n\nThe greatest disadvantage of the airship is size, which is essential to increasing performance. As size increases, the problems of ground handling increase geometrically. As the German Navy changed from the P class of 1915 with a volume of over to the larger Q class of 1916, the R class of 1917, and finally the W class of 1918, at almost ground handling problems reduced the number of days the Zeppelins were able to make patrol flights. This availability declined from 34% in 1915, to 24.3% in 1916 and finally 17.5% in 1918.\n\nSo long as the power-to-weight ratios of aircraft engines remained low and specific fuel consumption high, the airship had an edge for long-range or -duration operations. As those figures changed, the balance shifted rapidly in the aeroplane's favour. By mid-1917, the airship could no longer survive in a combat situation where the threat was aeroplanes. By the late 1930s, the airship barely had an advantage over the aeroplane on intercontinental over-water flights, and that advantage had vanished by the end of World War II.\n\nThis is in face-to-face tactical situations. Currently, a High-altitude airship project is planned to survey hundreds of kilometres as their operation radius, often much farther than the normal engagement range of a military aeroplane. For example, a radar mounted on a vessel platform high has radio horizon at range, while a radar at altitude has radio horizon at range. This is significantly important for detecting low-flying cruise missiles or fighter-bombers.\n\nThe most commonly used lifting gas, helium, is inert and therefore presents no fire risk. A series of vulnerability tests were done by the UK Defence Evaluation and Research Agency DERA on a Skyship 600. Since the internal gas pressure was maintained at only 1–2% above the surrounding air pressure, the vehicle proved highly tolerant to physical damage or to attack by small-arms fire or missiles. Several hundred high-velocity bullets were fired through the hull, and even two hours later the vehicle would have been able to return to base. Ordnance passed through the envelope without causing critical helium loss. In all instances of light armament fire evaluated under both test and live conditions, the airship was able to complete its mission and return to base.\n\n"}
{"id": "30866498", "url": "https://en.wikipedia.org/wiki?curid=30866498", "title": "Architonnerre", "text": "Architonnerre\n\nThe Architonnerre ( Architronito) was a steam-powered cannon, a description of which is found in the papers of Leonardo da Vinci dating to the late 15th century, although he attributes its invention to Archimedes in the 3rd century BC.\n\nLeonardo's description was hidden amongst his papers until it was rediscovered by Étienne-Jean Delécluze of the French Institute in 1838 and published in the magazine \"L'Artiste\" in 1841, well after the modern high pressure steam engine had been independently invented.\n\nThe following is the most likely means of operation from the description given.\n\nAccount from the \"Nelson Examiner and New Zealand Chronicle\", 1842:\n\nThe Steam-Engine. — M. Delecluze has lately made a discovery among the manuscripts of Leonardo da Vinci, carrying back a knowledge of the steam-engine to at least as far back as the 15th century. He has published in the \"Artiste\" a notice on the life of Leonardo da Vinci, to which he adds a fac-simile of a page from one of his manuscripts, and on which are five sketches with the pen, representing the details of the apparatus of a steam-gun, with an explanatory note upon what he designates under the name of the \" \"Architonnerre\",\" and of which note the following is a translation : — \" Invention of Archimedes. — The \"Architonnerre\" is a machine of fine copper, which throws balls with a loud report and great force. It is used in the following manner : — One third of this instrument contains a large quantity of charcoal fire. When the water is well heated, a screw at the top of the vessel which contains the water must be made quite tight. On closing the screw above, all the water will escape below, will descend into the heated portion of the instrument, and be immediately converted into a vapour so abundant and powerful, that it is wonderful to see its fury and hear the noise it produces. This machine will carry a ball of a talent in, weight.\" It is worthy of remark that Leonardo da Vinci- far from claiming the merit of this invention for himself or the men of his time- attributes it to Archimedes.\n\nThe weight of the cannonball is described as one talent. A Roman talent was , although the amount varied across the ancient world by a few kilos.\n\n"}
{"id": "50252619", "url": "https://en.wikipedia.org/wiki?curid=50252619", "title": "Bastora Dam", "text": "Bastora Dam\n\nThe Bastora Dam is a gravity dam currently being constructed on the Bastora River near Gomespan in Erbil Governorate, Iraq. It is located about northeast of Erbil. The primary purpose of the dam is water supply for irrigation but it will support a small 2.4 MW hydroelectric power station. It is expected to irrigate . Construction on the tall roller-compacted concrete dam began in 2013 and it is expected to be complete in 2018. \n"}
{"id": "5173509", "url": "https://en.wikipedia.org/wiki?curid=5173509", "title": "Beta-alumina solid electrolyte", "text": "Beta-alumina solid electrolyte\n\nβ<nowiki>\"</nowiki>-Alumina (\"beta prime-prime alumina\") is an isomorphic form of aluminium oxide (AlO), a hard polycrystalline ceramic, which, when prepared as an electrolyte, is complexed with a mobile ion, such as Na, K, Li, Ag, H, Pb, Sr or Ba depending on the application. Beta-alumina is a good conductor of its mobile ion yet allows no non-ionic (i.e., electronic) conductivity.\n\nSodium beta alumina is a non-stoichiometric sodium aluminate known for its rapid transport of Na ions. This material selectively passes sodium ions while blocking other species, including liquid sodium and liquid sulfur. It is a ceramic which can be formed and sintered by commercially available techniques and its conductivity at operating temperatures – 250 to 300 degrees Celsius – compares favorably with electrolytes used in conventional battery systems such as sulfuric acid and potassium hydroxide. The crystal structure of the Na-AlO provides an essential rigid framework with channels along which the ionic species of the solid can migrate. Ion transport involves hopping from site to site along these channels.\n\nBASE was first developed by researchers at the Ford Motor Company, in the search for a storage device for electric vehicles while developing the sodium-sulfur battery. The NaS battery consists of sulfur at the positive electrode and sodium at the negative electrode as active materials, and sodium-conducting beta alumina ceramic as the electrolyte separating both electrodes.\nThis sealed battery is kept at approximately 300 degrees Celsius and is operated under the condition that the active materials at both electrodes are liquid and its electrolyte is solid. It is only at temperatures around 300 degrees Celsius or more, that it is possible for the negative sodium electrode to completely coat, or \"wet,\" the ceramic electrolyte. At lower temperatures, molten sodium has issues in covering the surface of the beta alumina ceramic. This causes sodium to curl up akin to a drop of oil in water, reducing surface area contact and making the battery less efficient. At such high temperatures, since both active materials have high surface area contact and internal resistance becomes low enough, the NaS battery shows excellent performance. As a secondary battery, which allows reversible charging and discharging, the NaS battery can be continuously used. Several commercial installations use this type of battery for load leveling.\n\nThe sodium sulfur battery was a topic of intense worldwide interest during the 1970s and 1980s, but interest in the technology for vehicle use diminished for a variety of technical and economic reasons. In contrast, its \"successor\", the sodium nickel chloride battery, is now entering the commercialization phase. The sodium nickel chloride battery (or ZEBRA battery, so-called for the Zeolite Battery Research Africa Project) has been under development for almost 20 years.\n\nWhen BASE is used in a sodium nickel chloride (ZEBRA) cell, several requirements must be met. It must have a low resistivity, typically 4 Ωcm at 350 °C, and a strength in excess of 200 MPa. It must be produced in the form of a thin-walled (1.25 mm), convoluted tube by low-cost production methods, and it must maintain a stable cell resistance for up to 10 years. These requirements have mostly been met by a variation of the sol-gel process.\n\nBASE is also used in alkali-metal thermal to electric converters (AMTECs). An AMTEC is a high efficiency device for directly converting heat to electricity. An AMTEC operates as a thermally regenerative electrochemical cell by expanding sodium through the pressure differential across the (BASE) membrane. BASE electrolytes have been used in some molten-carbonate fuel cells, as well as other liquid electrode/solid electrolyte fuel cell designs.\n\n"}
{"id": "11032325", "url": "https://en.wikipedia.org/wiki?curid=11032325", "title": "Bird tracks", "text": "Bird tracks\n\nBird tracking provides a way to assess the habitat range and behavior of birds without ever seeing the bird. Bird tracking falls under the category of tracking and is related to animal tracking. A guide to bird tracking has been published. Bird tracking is a tool used by naturalists to assess what birds are present in an ecosystem even if the bird is rarely seen.\n\nIn the Pacific Northwest of the United States, a program called NatureMapping collects data by educating the public and having them pool their data in a citizen science application. Data can be collected in the field using a handheld palm pilot and GPS system that streamlines the collection process. This free program is called CyberTracker. In order to make sure that data is reliable, a tracker evaluation system has been put in place through the CyberTracker organization.\n\n"}
{"id": "31249953", "url": "https://en.wikipedia.org/wiki?curid=31249953", "title": "Black Gold (2011 Nigerian film)", "text": "Black Gold (2011 Nigerian film)\n\nNot to be confused with \"Black Gold (2011 Qatari film)\"\n\nBlack Gold is a 2011 drama film co-produced and directed by Jeta Amata. One local Niger Delta community's struggle against their own government and a multi-national oil corporation who has plundered their land and destroyed the environment. The film was reissued in 2012 with the title \"Black November\", with 60% of the scenes reshot and additional scenes included to make the film \"more current\".\n\nThey hope to tell the story from the perspective of people who have lived through it. The people who have seen their land and rivers polluted by oil, the people that are struggling.\n\n"}
{"id": "204466", "url": "https://en.wikipedia.org/wiki?curid=204466", "title": "Bubble chamber", "text": "Bubble chamber\n\nA bubble chamber is a vessel filled with a superheated transparent liquid (most often liquid hydrogen) used to detect electrically charged particles moving through it. It was invented in 1952 by Donald A. Glaser, for which he was awarded the 1960 Nobel Prize in Physics. Supposedly, Glaser was inspired by the bubbles in a glass of beer; however, in a 2006 talk, he refuted this story, although saying that while beer was not the inspiration for the bubble chamber, he did experiments using beer to fill early prototypes.\n\nWhile bubble chambers were extensively used in the past, they have now mostly been supplanted by wire chambers and spark chambers. Notable bubble chambers include the Big European Bubble Chamber (BEBC) and Gargamelle.\nThe bubble chamber is similar to a cloud chamber, both in application and in basic principle. It is normally made by filling a large cylinder with a liquid heated to just below its boiling point. As particles enter the chamber, a piston suddenly decreases its pressure, and the liquid enters into a superheated, metastable phase. Charged particles create an ionization track, around which the liquid vaporizes, forming microscopic bubbles. Bubble density around a track is proportional to a particle's energy loss.\n\nBubbles grow in size as the chamber expands, until they are large enough to be seen or photographed. Several cameras are mounted around it, allowing a three-dimensional image of an event to be captured. Bubble chambers with resolutions down to a few micrometers (μm) have been operated.\n\nThe entire chamber is subject to a constant magnetic field, which causes charged particles to travel in helical paths whose radius is determined by their charge-to-mass ratios and their velocities. Since the magnitude of the charge of all known charged, long-lived subatomic particles is the same as that of an electron, their radius of curvature must be proportional to their momentum. Thus, by measuring their radius of curvature, their momentum can be determined.\n\nNotable discoveries made by bubble chamber include the discovery of weak neutral currents at Gargamelle in 1973, which established the soundness of the electroweak theory and led to the discovery of the W and Z bosons in 1983 (at the UA1 and UA2 experiments). Recently, bubble chambers have been used in research on Weakly interacting massive particles (WIMP)s, at SIMPLE, COUPP, PICASSO and more recently, PICO.\n\nAlthough bubble chambers were very successful in the past, they are of limited use in modern very-high-energy experiments for a variety of reasons:\n\n\nDue to these issues, bubble chambers have largely been replaced by wire chambers, which allow particle energies to be measured at the same time. Another alternative technique is the spark chamber.\n\n"}
{"id": "54321473", "url": "https://en.wikipedia.org/wiki?curid=54321473", "title": "Chalconatronite", "text": "Chalconatronite\n\nChalconatronite is a carbonate mineral and rare secondary copper mineral that contains copper, sodium, carbon, oxygen, and hydrogen, its chemical formula is NaCu(CO)•3(HO). Chalconatronite is partially soluble in water, and only decomposes, although chalconatronite is soluble while cold, in dilute acids. The name comes from the mineral's compounds, copper (\"chalcos\" in Greek) and natron, naturally forming sodium carbonate. The mineral is thought to be formed by water carrying alkali carbonates (possibly from soil) reacting with bronze. Similar minerals include malachite, azurite, and other copper carbonates. Chalconatronite has also been found and recorded in Australia, Germany, and Colorado.\n\nMost chalconatronite formed on bronze and silver that have been treated with either sodium sesquicarbonate or sodium cyanide to prevent corrosion and bronze disease. The mineral has also been proven to form on the surface of copper artifacts after being treated with aqueous sodium carbonate. This formation by using sodium sesquicarbonate is undesirable by many antique collectors, as the mineral changes the patinas of copper artifacts. When the mineral forms, it can replace copper salts within the patina, and turn the color from a rich green to a blue-green or even black.\n\nThe mineral was recorded in 1955 on three bronze artifacts from ancient Egypt, which were being held in the Fogg Art Museum at Harvard. Chalconatronite was found inside of two bronze figures (one depicting a seated Sekhmet, and another one depicting a group of cats and kittens) from around the late Nubian Dynasty or early Saite Period. Another chalconatronite specimen was found under a bronze censer from the late Coptic Period. The chalconatronite found on the censer formed over cuprite and some atacamite crystals, which are associated minerals.\n\nChalconatronite was also found on iron and copper Roman armor in 1982 at a site in Chester, England. Some of the mineral was found on a copper pin in St. Mark's Basilica, Venice and in two different Mayan paintings. Along with pseudomalachite, chalconatronite was found on an illuminated manuscript from the sixteenth century. Synthetic chalconatronite could have possibly been made in ancient China as a form of pigment, named \"synthetic malachite\". It was made by taking copper oxide and boiling it with white alum in a \"sufficient amount of water\". After the result is cooled, a natron solution would be added to precipitate a synthetic form of chalconatronite, as sodium copper carbonate.\n\n"}
{"id": "428161", "url": "https://en.wikipedia.org/wiki?curid=428161", "title": "Citizens for Alternatives to Chemical Contamination", "text": "Citizens for Alternatives to Chemical Contamination\n\nCitizens for Alternatives to Chemical Contamination (CACC) is a Michigan-based grassroots environmental organization centered in the Great Lakes and committed to fostering awareness of environmental issues through a network of citizens and organizations. CACC has been working to protect our environment since 1978.\n\nCACC sponsors the annual Backyard Eco Conference to give citizens the opportunity to learn from environmental movers and doers and to network with other activists from around the Great Lakes Basin.\n\nCACC is a member of Great Lakes United, the Michigan Environmental Council, Michigan Energy Reform Coalition, the Environmental Fund for Michigan, and the National Citizens' Network on Oil and Gas Wastes.\n\n<br>\n"}
{"id": "33106331", "url": "https://en.wikipedia.org/wiki?curid=33106331", "title": "Conspicuous conservation", "text": "Conspicuous conservation\n\nConspicuous conservation is an idea that grew out of conspicuous consumption. It refers to the relatively recent phenomenon of engaging in activities that are environmentally friendly in order to obtain or signal a higher social status.\nAccording to a 1978 article by Ronald D. White, the concept of \"conspicuous conservation\" was first used by economist Seymour Sacks. Jeff Mikulina, then-executive director of the Hawaii Chapter of the Sierra Club, mentioned the term in 2005, and articles in 2007 also referred the concept. The term was invoked separately in 2010 by Berkeley researchers Sexton and Sexton.\n\nVladas Griskevicius, Joshua M. Tybur, and Bram Van den Bergh (2010) argue that buying \"green products\" can be construed as altruistic. Because altruistic behavior might function as a costly signal of social status, conspicuous conservation can be interpreted as a signal of high status. Experiments showed that activating status motives led people to choose green products over more luxurious non-green products. The status motive increases the willingness to buy green products in public (but not in private) settings and in settings where green products cost more than non-green products. According to the authors, status competition can thus be used to promote pro-environmental behavior.\n"}
{"id": "28363118", "url": "https://en.wikipedia.org/wiki?curid=28363118", "title": "Electric skateboard", "text": "Electric skateboard\n\nAn electric skateboard is a personal transporter based on a skateboard. The speed is controlled by a hand-held throttle or weight-shifting and the direction of travel is adjusted by tilting the board to one side or the other.\n\nThe MotoBoard, which was gasoline-powered was released in the summer of 1975, but were banned in California due to their noise and pollution in 1997.\n\nLouie Finkle of Seal Beach, California is often cited as an originator of the modern electric skateboard, offering his first wireless electric skateboard in 1997 and a patent filed in April 1999, however it wasn't until the 2004-6 that electric motors and batteries were available with sufficient torque and efficiency to power boards effectively.\n\nIn 2012 ZBoard raised nearly 30 times their target for a balance controlled electric skateboard on kickstarter, which was well received at the Consumer Electronics Show in Las Vegas in January 2013. Their 2015 campaign on Indiegogo was 86 time over-subscribed, raising $1m.\n\nIt was originally designed for local transport, but now offer a more serious \"Off Road\" model as a new thrill sport. The Off Road style boards are able to traverse grass, gravel, dirt and hard sand with ease and are often seen at low tide on the beach.\n\nThe typical range for an 800-watt unit with a LiFePO battery is between 16 and 20 km (9–12 miles). 600 W units achieve 20% less range than their 800 W counterparts, and units using the older sealed lead acid batteries achieve 30% less range than those using the LiFePO batteries. The maximum speed of a typical electric skateboard is about .\n\nMany manufacturers now offer 12 Ah LiFePO packs as an optional upgrade over the more standard SLA battery packs, which reduces the weight of the boards by 10 kg (LiFePO packs weigh in at 5 kg compared to the 15 kg of the standard SLA pack). This results in a lighter, more agile board. Additionally, discharge chemistry of a LiFePO battery allows the motor to run at top speed constantly until the battery is exhausted, compared to the initial high current from an SLA battery which quickly tapers off as it discharges. Furthermore, a high quality 12 Ah-rated LiFePO pack can realistically deliver 9–10 Ah, compared to a 12 Ah-rated SLA pack which realistically delivers 7–8 Ah due to the high energy demands of an electric skateboard's motor (typically 25–35 amps when riding at high speed, over rough or sloping terrain).\n\nBy increasing the battery capacity to 20 Ah (using small-factor LiFePO pouch cells), ranges of 30 kilometers or more can be achieved even when riding at constant high speed.\nIn 2016, four companies started to offer electric skateboards with the ability to run over 30 km with an 8Ah battery unlike their predecessors who offer up to 25 km and cost almost double.\n\n"}
{"id": "4315911", "url": "https://en.wikipedia.org/wiki?curid=4315911", "title": "Eletropaulo", "text": "Eletropaulo\n\nEletropaulo () is a major Brazilian power distributor in the state of São Paulo, created in the breakup of the old state-owned power distribution company Eletropaulo that monopolized electricity distribution in São Paulo from 1981 to 1999. The similarity of the names makes most old customers call it simply Eletropaulo.\n\nThe company has around 5.8 million customers - nearly 20 million people - in an area of 4,526 kilometers ², in 24 municipalities in the metropolitan area of São Paulo, including the city itself;\n\nOwnership of \"Eletropaulo\" is shared between AES (which owns 17% of the company) and the Brazilian Development Bank (which owns 19%). Its stock is traded on B3, and was part of the Ibovespa index.\n\nElectrical power infrastructure in São Paulo was originally developed in the first half of the 20th century by the São Paulo Tramway, Light and Power Company, a company financed with foreign capital and legally domiciled in Toronto, Ontario, Canada. After 1912, this company was part of the holding company Brazilian Traction, Light and Power Company (usually known in Brazil simply as \"Light\"), which eventually changed its name to Brascan (for \"Brasil\" + \"Canada\"). At the end of the 1970s, the company's Brazilian assets transferred to Brazilian ownership, first to private hands and then (in 1981) to the government of the state of São Paulo. The Canadian company, which had in the meantime diversified to other areas, still exists under the name Brookfield Asset Management.\n\n"}
{"id": "7271480", "url": "https://en.wikipedia.org/wiki?curid=7271480", "title": "Frameless construction", "text": "Frameless construction\n\nA common construction method for frameless cabinets originated in Europe after World War II and is known as the \"32-mm system\" or \"European system\". This nomenclature is derived from the 32-millimetre spacing between the system holes used for construction and installation of hardware typically used for doors, drawers and shelves. There are numerous 32mm based cabinet systems, one such system is Hettich's System 32. In North America it is also often referred to as \"European Cabinetry\" popular due to its simplicity of construction, clean lines and low cost.\n\nWith frameless (AKA full access) cabinets, thicker sides (boxes) keep the cabinet much more stable and avoids the use of a front frame, such as found in face-frame cabinets. By eliminating the front frame, you can have more room to get large objects inside with more usable space.\n\n"}
{"id": "10890", "url": "https://en.wikipedia.org/wiki?curid=10890", "title": "Fundamental interaction", "text": "Fundamental interaction\n\nIn physics, the fundamental interactions, also known as fundamental forces, are the interactions that do not appear to be reducible to more basic interactions. There are four fundamental interactions known to exist: the gravitational and electromagnetic interactions, which produce significant long-range forces whose effects can be seen directly in everyday life, and the strong and weak interactions, which produce forces at minuscule, subatomic distances and govern nuclear interactions. Some scientists speculate that a fifth force might exist, but this is not widely accepted nor proven.\n\nEach of the known fundamental interactions can be described mathematically as a \"field\". The gravitational force is attributed to the curvature of spacetime, described by Einstein's general theory of relativity. The other three are discrete quantum fields, and their interactions are mediated by elementary particles described by the Standard Model of particle physics.\n\nWithin the Standard Model, the strong interaction is carried by a particle called the gluon, and is responsible for quarks binding together to form hadrons, such as protons and neutrons. As a residual effect, it creates the nuclear force that binds the latter particles to form atomic nuclei. The weak interaction is carried by particles called W and Z bosons, and also acts on the nucleus of atoms, mediating radioactive decay. The electromagnetic force, carried by the photon, creates electric and magnetic fields, which are responsible for the attraction between orbital electrons and atomic nuclei which holds atoms together, as well as chemical bonding and electromagnetic waves, including visible light, and forms the basis for electrical technology. Although the electromagnetic force is far stronger than gravity, it tends to cancel itself out within large objects, so over the largest distances (on the scale of planets and galaxies), gravity tends to be the dominant force.\n\nAll four fundamental forces are believed to be related, and to unite into a single force at high energies on a minuscule scale, the Planck scale, but particle accelerators cannot produce the enormous energies required to experimentally probe this. Efforts to devise a common theoretical framework that would explain the relation between the forces are perhaps the greatest goal of theoretical physicists today. The weak and electromagnetic forces have already been unified with the electroweak theory of Sheldon Glashow, Abdus Salam, and Steven Weinberg for which they received the 1979 Nobel Prize in physics. Progress is currently being made in uniting the electroweak and strong fields within a Grand Unified Theory (GUT). A bigger challenge is to find a way to quantize the gravitational field, resulting in a theory of quantum gravity (QG) which would unite gravity in a common theoretical framework with the other three forces. Some theories, notably string theory, seek both QG and GUT within one framework, unifying all four fundamental interactions along with mass generation within a theory of everything (ToE).\n\nIn his 1687 theory, Isaac Newton postulated space as an infinite and unalterable physical structure existing before, within, and around all objects while their states and relations unfold at a constant pace everywhere, thus absolute space and time. Inferring that all objects bearing mass approach at a constant rate, but collide by impact proportional to their masses, Newton inferred that matter exhibits an attractive force. His law of universal gravitation mathematically stated it to span the entire universe instantly (despite absolute time), or, if not actually a force, to be instant interaction among all objects (despite absolute space.) As conventionally interpreted, Newton's theory of motion modelled a \"central force\" without a communicating medium. Thus Newton's theory violated the first principle of mechanical philosophy, as stated by Descartes, \"No action at a distance\". Conversely, during the 1820s, when explaining magnetism, Michael Faraday inferred a \"field\" filling space and transmitting that force. Faraday conjectured that ultimately, all forces unified into one.\n\nIn 1873, James Clerk Maxwell unified electricity and magnetism as effects of an electromagnetic field whose third consequence was light, travelling at constant speed in a vacuum. The electromagnetic field theory contradicted predictions of Newton's theory of motion, unless physical states of the luminiferous aether—presumed to fill all space whether within matter or in a vacuum and to manifest the electromagnetic field—aligned all phenomena and thereby held valid the Newtonian principle relativity or invariance.\n\nThe Standard Model of particle physics was developed throughout the latter half of the 20th century. In the Standard Model, the electromagnetic, strong, and weak interactions associate with elementary particles, whose behaviours are modelled in quantum mechanics (QM). For predictive success with QM's probabilistic outcomes, particle physics conventionally models QM events across a field set to special relativity, altogether relativistic quantum field theory (QFT). Force particles, called gauge bosons—\"force carriers\" or \"messenger particles\" of underlying fields—interact with matter particles, called fermions. Everyday matter is atoms, composed of three fermion types: up-quarks and down-quarks constituting, as well as electrons orbiting, the atom's nucleus. Atoms interact, form molecules, and manifest further properties through electromagnetic interactions among their electrons absorbing and emitting photons, the electromagnetic field's force carrier, which if unimpeded traverse potentially infinite distance. Electromagnetism's QFT is quantum electrodynamics (QED).\n\nThe electromagnetic interaction was modelled with the weak interaction, whose force carriers are W and Z bosons, traversing the minuscule distance, in electroweak theory (EWT). Electroweak interaction would operate at such high temperatures as soon after the presumed Big Bang, but, as the early universe cooled, split into electromagnetic and weak interactions. The strong interaction, whose force carrier is the gluon, traversing minuscule distance among quarks, is modeled in quantum chromodynamics (QCD). EWT, QCD, and the Higgs mechanism, whereby the Higgs field manifests Higgs bosons that interact with some quantum particles and thereby endow those particles with mass comprise particle physics' Standard Model (SM). Predictions are usually made using calculational approximation methods, although such perturbation theory is inadequate to model some experimental observations (for instance bound states and solitons.) Still, physicists widely accept the Standard Model as science's most experimentally confirmed theory.\n\nBeyond the Standard Model, some theorists work to unite the electroweak and strong interactions within a Grand Unified Theory (GUT). Some attempts at GUTs hypothesize \"shadow\" particles, such that every known matter particle associates with an undiscovered force particle, and vice versa, altogether supersymmetry (SUSY). Other theorists seek to quantize the gravitational field by the modelling behaviour of its hypothetical force carrier, the graviton and achieve quantum gravity (QG). One approach to QG is loop quantum gravity (LQG). Still other theorists seek both QG and GUT within one framework, reducing all four fundamental interactions to a Theory of Everything (ToE). The most prevalent aim at a ToE is string theory, although to model matter particles, it added SUSY to force particles—and so, strictly speaking, became superstring theory. Multiple, seemingly disparate superstring theories were unified on a backbone, M-theory. Theories beyond the Standard Model remain highly speculative, lacking great experimental support.\n\nIn the conceptual model of fundamental interactions, matter consists of fermions, which carry properties called charges and spin ± (intrinsic angular momentum ±, where ħ is the reduced Planck constant). They attract or repel each other by exchanging bosons.\n\nThe interaction of any pair of fermions in perturbation theory can then be modelled thus:\n\nThe exchange of bosons always carries energy and momentum between the fermions, thereby changing their speed and direction. The exchange may also transport a charge between the fermions, changing the charges of the fermions in the process (e.g., turn them from one type of fermion to another). Since bosons carry one unit of angular momentum, the fermion's spin direction will flip from + to − (or vice versa) during such an exchange (in units of the reduced Planck's constant).\n\nBecause an interaction results in fermions attracting and repelling each other, an older term for \"interaction\" is force.\n\nAccording to the present understanding, there are four fundamental interactions or forces: gravitation, electromagnetism, the weak interaction, and the strong interaction. Their magnitude and behaviour vary greatly, as described in the table below. Modern physics attempts to explain every observed physical phenomenon by these fundamental interactions. Moreover, reducing the number of different interaction types is seen as desirable. Two cases in point are the unification of:\n\nBoth magnitude (\"relative strength\") and \"range\", as given in the table, are meaningful only within a rather complex theoretical framework. It should also be noted that the table below lists properties of a conceptual scheme that is still the subject of ongoing research.\n\nThe modern (perturbative) quantum mechanical view of the fundamental forces other than gravity is that particles of matter (fermions) do not directly interact with each other, but rather carry a charge, and exchange virtual particles (gauge bosons), which are the interaction carriers or force mediators. For example, photons mediate the interaction of electric charges, and gluons mediate the interaction of color charges.\n\n\"Gravitation\" is by far the weakest of the four interactions at the atomic scale, where electromagnetic interactions dominate. But the idea that the weakness of gravity can easily be demonstrated by suspending a pin using a simple magnet (such as a refrigerator magnet) is fundamentally flawed. The only reason the magnet is able to hold the pin against the gravitational pull of the entire Earth is due to its relative proximity. There is clearly a short distance of separation between magnet and pin where a breaking point is reached, and due to the large mass of Earth this distance is disappointingly small.\n\nThus gravitation is very important for macroscopic objects and over macroscopic distances for the following reasons. Gravitation:\n\nEven though electromagnetism is far stronger than gravitation, electrostatic attraction is not relevant for large celestial bodies, such as planets, stars, and galaxies, simply because such bodies contain equal numbers of protons and electrons and so have a net electric charge of zero. Nothing \"cancels\" gravity, since it is only attractive, unlike electric forces which can be attractive or repulsive. On the other hand, all objects having mass are subject to the gravitational force, which only attracts. Therefore, only gravitation matters on the large-scale structure of the universe.\n\nThe long range of gravitation makes it responsible for such large-scale phenomena as the structure of galaxies and black holes and it retards the expansion of the universe. Gravitation also explains astronomical phenomena on more modest scales, such as planetary orbits, as well as everyday experience: objects fall; heavy objects act as if they were glued to the ground, and animals can only jump so high.\n\nGravitation was the first interaction to be described mathematically. In ancient times, Aristotle hypothesized that objects of different masses fall at different rates. During the Scientific Revolution, Galileo Galilei experimentally determined that this hypothesis was wrong under certain circumstances — neglecting the friction due to air resistance, and buoyancy forces if an atmosphere is present (e.g. the case of a dropped air-filled balloon vs a water-filled balloon) all objects accelerate toward the Earth at the same rate. Isaac Newton's law of Universal Gravitation (1687) was a good approximation of the behaviour of gravitation. Our present-day understanding of gravitation stems from Einstein's General Theory of Relativity of 1915, a more accurate (especially for cosmological masses and distances) description of gravitation in terms of the geometry of spacetime.\n\nMerging general relativity and quantum mechanics (or quantum field theory) into a more general theory of quantum gravity is an area of active research. It is hypothesized that gravitation is mediated by a massless spin-2 particle called the graviton.\n\nAlthough general relativity has been experimentally confirmed (at least for weak fields ) on all but the smallest scales, there are rival theories of gravitation. Those taken seriously by the physics community all reduce to general relativity in some limit, and the focus of observational work is to establish limitations on what deviations from general relativity are possible.\n\nProposed extra dimensions could explain why the gravity force is so weak.\n\nElectromagnetism and weak interaction appear to be very different at everyday low energies. They can be modelled using two different theories. However, above unification energy, on the order of 100 GeV, they would merge into a single electroweak force.\n\nElectroweak theory is very important for modern cosmology, particularly on how the universe evolved. This is because shortly after the Big Bang, the temperature was approximately above 10 K, the electromagnetic force and the weak force were merged into a combined electroweak force.\n\nFor contributions to the unification of the weak and electromagnetic interaction between elementary particles, Abdus Salam, Sheldon Glashow and Steven Weinberg were awarded the Nobel Prize in Physics in 1979.\n\nElectromagnetism is the force that acts between electrically charged particles. This phenomenon includes the electrostatic force acting between charged particles at rest, and the combined effect of electric and magnetic forces acting between charged particles moving relative to each other.\n\nElectromagnetism has infinite range like gravity, but is vastly stronger, and therefore describes a number of macroscopic phenomena of everyday experience such as friction, rainbows, lightning, and all human-made devices using electric current, such as television, lasers, and computers. Electromagnetism fundamentally determines all macroscopic, and many atomic levels, properties of the chemical elements, including all chemical bonding.\n\nIn a four kilogram (~1 gallon) jug of water there are\n\nformula_1\n\nof total electron charge. Thus, IF we could separate ALL the positive(+) charges into 1 gallon jug and separate ALL the negative (-)\ncharges into the other gallon jug and IF we place two such jugs a meter apart, the electrons in one of the jugs will attract those in the other jug with a force of\n\nformula_2\n\nThis force is larger than the planet Earth would weigh if weighed on another Earth. The atomic nuclei in one jug also repel those in the other with the same force. However, these repulsive forces are canceled by the attraction of the electrons in jug A with the nuclei in jug B and the attraction of the nuclei in jug A with the electrons in jug B, resulting in no net force. Electromagnetic forces are tremendously stronger than gravity but cancel out so that for large bodies gravity dominates.\n\nElectrical and magnetic phenomena have been observed since ancient times, but it was only in the 19th century James Clerk Maxwell discovered that electricity and magnetism are two aspects of the same fundamental interaction. By 1864, Maxwell's equations had rigorously quantified this unified interaction. Maxwell's theory, restated using vector calculus, is the classical theory of electromagnetism, suitable for most technological purposes.\n\nThe constant speed of light in a vacuum (customarily described with a lowercase letter \"c\") can be derived from Maxwell's equations, which are consistent with the theory of special relativity. Einstein's 1905 theory of special relativity, however, which flows from the observation that the speed of light is constant no matter how fast the observer is moving, showed that the theoretical result implied by Maxwell's equations has profound implications far beyond electromagnetism on the very nature of time and space.\n\nIn another work that departed from classical electro-magnetism, Einstein also explained the photoelectric effect by utilizing Max Planck's discovery that light was transmitted in 'quanta' of specific energy content based on the frequency, which we now call photons. Starting around 1927, Paul Dirac combined quantum mechanics with the relativistic theory of electromagnetism. Further work in the 1940s, by Richard Feynman, Freeman Dyson, Julian Schwinger, and Sin-Itiro Tomonaga, completed this theory, which is now called quantum electrodynamics, the revised theory of electromagnetism. Quantum electrodynamics and quantum mechanics provide a theoretical basis for electromagnetic behavior such as quantum tunneling, in which a certain percentage of electrically charged particles move in ways that would be impossible under the classical electromagnetic theory, that is necessary for everyday electronic devices such as transistors to function.\n\nThe \"weak interaction\" or \"weak nuclear force\" is responsible for some nuclear phenomena such as beta decay. Electromagnetism and the weak force are now understood to be two aspects of a unified electroweak interaction — this discovery was the first step toward the unified theory known as the Standard Model. In the theory of the electroweak interaction, the carriers of the weak force are the massive gauge bosons called the W and Z bosons. The weak interaction is the only known interaction which does not conserve parity; it is left-right asymmetric. The weak interaction even violates CP symmetry but does conserve CPT.\n\nThe \"strong interaction\", or \"strong nuclear force\", is the most complicated interaction, mainly because of the way it varies with distance. At distances greater than 10 femtometers, the strong force is practically unobservable. Moreover, it holds only inside the atomic nucleus.\n\nAfter the nucleus was discovered in 1908, it was clear that a new force, today known as the nuclear force, was needed to overcome the electrostatic repulsion, a manifestation of electromagnetism, of the positively charged protons. Otherwise, the nucleus could not exist. Moreover, the force had to be strong enough to squeeze the protons into a volume whose diameter is about 10 m, much smaller than that of the entire atom. From the short range of this force, Hideki Yukawa predicted that it was associated with a massive particle, whose mass is approximately 100 MeV.\n\nThe 1947 discovery of the pion ushered in the modern era of particle physics. Hundreds of hadrons were discovered from the 1940s to 1960s, and an extremely complicated theory of hadrons as strongly interacting particles was developed. Most notably:\n\nWhile each of these approaches offered deep insights, no approach led directly to a fundamental theory.\n\nMurray Gell-Mann along with George Zweig first proposed fractionally charged quarks in 1961. Throughout the 1960s, different authors considered theories similar to the modern fundamental theory of quantum chromodynamics (QCD) as simple models for the interactions of quarks. The first to hypothesize the gluons of QCD were Moo-Young Han and Yoichiro Nambu, who introduced the quark color charge and hypothesized that it might be associated with a force-carrying field. At that time, however, it was difficult to see how such a model could permanently confine quarks. Han and Nambu also assigned each quark color an integer electrical charge, so that the quarks were fractionally charged only on average, and they did not expect the quarks in their model to be permanently confined.\n\nIn 1971, Murray Gell-Mann and Harald Fritzsch proposed that the Han/Nambu color gauge field was the correct theory of the short-distance interactions of fractionally charged quarks. A little later, David Gross, Frank Wilczek, and David Politzer discovered that this theory had the property of asymptotic freedom, allowing them to make contact with experimental evidence. They concluded that QCD was the complete theory of the strong interactions, correct at all distance scales. The discovery of asymptotic freedom led most physicists to accept QCD since it became clear that even the long-distance properties of the strong interactions could be consistent with experiment if the quarks are permanently confined.\n\nAssuming that quarks are confined, Mikhail Shifman, Arkady Vainshtein and Valentine Zakharov were able to compute the properties of many low-lying hadrons directly from QCD, with only a few extra parameters to describe the vacuum. In 1980, Kenneth G. Wilson published computer calculations based on the first principles of QCD, establishing, to a level of confidence tantamount to certainty, that QCD will confine quarks. Since then, QCD has been the established theory of the strong interactions.\n\nQCD is a theory of fractionally charged quarks interacting by means of 8 bosonic particles called gluons. The gluons interact with each other, not just with the quarks, and at long distances the lines of force collimate into strings. In this way, the mathematical theory of QCD not only explains how quarks interact over short distances but also the string-like behavior, discovered by Chew and Frautschi, which they manifest over longer distances.\n\nNumerous theoretical efforts have been made to systematize the existing four fundamental interactions on the model of electroweak unification.\n\nGrand Unified Theories (GUTs) are proposals to show that the three fundamental interactions described by the Standard Model are all different manifestations of a single interaction with symmetries that break down and create separate interactions below some extremely high level of energy. GUTs are also expected to predict some of the relationships between constants of nature that the Standard Model treats as unrelated, as well as predicting gauge coupling unification for the relative strengths of the electromagnetic, weak, and strong forces (this was, for example, verified at the Large Electron–Positron Collider in 1991 for supersymmetric theories).\n\nTheories of everything, which integrate GUTs with a quantum gravity theory face a greater barrier, because no quantum gravity theories, which include string theory, loop quantum gravity, and twistor theory, have secured wide acceptance. Some theories look for a graviton to complete the Standard Model list of force-carrying particles, while others, like loop quantum gravity, emphasize the possibility that time-space itself may have a quantum aspect to it.\n\nSome theories beyond the Standard Model include a hypothetical fifth force, and the search for such a force is an ongoing line of experimental research in physics. In supersymmetric theories, there are particles that acquire their masses only through supersymmetry breaking effects and these particles, known as moduli can mediate new forces. Another reason to look for new forces is the discovery that the expansion of the universe is accelerating (also known as dark energy), giving rise to a need to explain a nonzero cosmological constant, and possibly to other modifications of general relativity. Fifth forces have also been suggested to explain phenomena such as CP violations, dark matter, and dark flow.\n\nIn December 2015, two observations in the ATLAS and CMS detectors at the Large Hadron Collider hinted at the existence of a new particle six times heavier than the Higgs boson. However, after obtaining more experimental data, the anomaly appeared to not be significant.\n\n\n\n"}
{"id": "21213525", "url": "https://en.wikipedia.org/wiki?curid=21213525", "title": "Glow Energy", "text": "Glow Energy\n\nGlow Energy PCL is a power and utility company in Thailand established in 1993. Glow Energy is one of the largest private electricity generators in Thailand. Its core business is to produce and supply electricity to EGAT, and to produce and deliver electricity, steam, and processed water to industrial customers. EGAT power purchase\nagreements are central to Glow's business, accounting for 59.7 percent of revenues in 2015. Glow Energy is a subsidiary of Engie. The French energy company has a 69.11 percent stake in Glow.\n\nThe company has four major production facilities in industrial areas in Rayong and Chonburi Provinces. , the company had a total generating capacity of 3,207 MW of electricity and 1,206 tons per hour of steam. Electricity sales accounted for over 85.4 percent of total revenues in 2015. The sale of steam accounted for 12.1 percent. In addition to Thailand, Glow Energy operates a hydroelectric plant in Lao PDR's Attapeu Province.\n\nGlow generates electricity using natural gas (62.6 percent of total cost of sales in 2015), coal (16.8 percent of total cost of sales in 2015), and, increasingly, solar.\n\nGlow in 2015 reported revenues of 65,369 million baht, total assets of 117,169 million baht, and a net profit of 8,355 million baht, all down from FY2014 results. The company employed over 800 persons.\n\n"}
{"id": "34462951", "url": "https://en.wikipedia.org/wiki?curid=34462951", "title": "Greedy Lying Bastards", "text": "Greedy Lying Bastards\n\nGreedy Lying Bastards is a 2012 American documentary film directed by Craig Rosebraugh. The film explores the phenomenon of climate change denial.\n\n\"Greedy Lying Bastards\" investigates the climate change misinformation campaign waged by the petroleum industry and its funded think tanks. The film exposes how a small number of well-paid spokespeople have worked to confuse the public and lawmakers on the issue. Both ExxonMobil and Koch Industries are identified in the film as two of the worst culprits funding the denial campaign. In addition to exposing the denial campaign, \"Greedy Lying Bastards\" tells the stories of those currently impacted by changing climate. These include residents of Kivalina, Alaska who are faced with relocation due to erosion of shorelines caused from rising temperatures and those in Tuvalu facing sea level rise. The film also covers the 2012 Colorado wildfires, drought in Kansas (part of the wider 2010–13 Southern United States and Mexico drought) and Hurricane Sandy.\n\n\"Greedy Lying Bastards\" was directed, produced and narrated by Craig Rosebraugh. He co-wrote the film with two-time Emmy Award winning editor Patrick Gambuti, Jr., who also served as editor. Daryl Hannah was an executive producer and Michael Brook, who wrote the score for An Inconvenient Truth, was the composer.\n\nIn making the film Rosebraugh sought to \"undertake a project that would uncover the hidden agenda of the oil industry and provide answers as to why we as a nation fail to implement clean energy policies and take effective action on important problems such as climate change.\"\n\nThe film began production in 2009 and finished late in 2012.\n\n\"Greedy Lying Bastards\" was released theatrically in North America on March 8, 2013, where it grossed $45,000. Disinformation released it on home video January 14, 2014, and it grossed another $73,537 in North American sales.\n\nRotten Tomatoes, a review aggregator, reports that 69% of 35 surveyed critics gave the film a positive review; the average rating is 6.2/10. The site's consensus reads: \"It's not particularly subtle, but \"Greedy Lying Bastards\" is effective in questioning the motives of climate change deniers.\" Metacritic rated it 56/100 based on 14 reviews.\n\n\n\n"}
{"id": "18207973", "url": "https://en.wikipedia.org/wiki?curid=18207973", "title": "Grid parity", "text": "Grid parity\n\nGrid parity (or socket parity) occurs when an alternative energy source can generate power at a levelized cost of electricity (LCOE) that is less than or equal to the price of purchasing power from the electricity grid. The term is most commonly used when discussing renewable energy sources, notably solar power and wind power. Grid parity depends upon whether you are calculating from the point of view of a utility or of a retail consumer.\n\nReaching grid parity is considered to be the point at which an energy source becomes a contender for widespread development without subsidies or government support. It is widely believed that a wholesale shift in generation to these forms of energy will take place when they reach grid parity.\n\nGermany was one of the first countries to reach parity for solar PV in 2011 and 2012 for utility-scale solar and rooftop solar PV, respectively. By January 2014, grid parity for solar PV systems had already been reached in at least nineteen countries.\n\nWind power reached grid parity in some places in Europe in the mid 2000s, and has continued to reduce in price.\n\nThe price of electricity from the grid is complex. Most power sources in the developed world are generated in industrial scale plants developed by private or public consortia. The company providing the power and the company delivering that power to the customers are often separate entities who enter into a Power Purchase Agreement that sets a fixed rate for all of the power delivered by the plant. On the other end of the wire, the local distribution company (LDC) charges rates that will cover their power purchases from the variety of producers they use.\n\nThis relationship is not straightforward; for instance, an LDC may buy large amounts of base load power from a nuclear plant at a low fixed cost and then buy peaking power only as required from natural gas peakers at a much higher cost, perhaps five to six times. Depending on their billing policy, this might be billed to the customer at a flat rate combining the two rates the LDC pays, or alternately based on a time-based pricing policy that tries to more closely match input costs with customer prices.\n\nAs a result of these policies, the exact definition of \"grid parity\" varies not only from location to location, but customer to customer and even hour to hour.\n\nFor instance, wind power connects to the grid on the distribution side (as opposed to the customer side). This means it competes with other large forms of industrial-scale power like hydro, nuclear or coal-fired plants, which are generally inexpensive forms of power. Additionally, the generator will be charged by the distribution operator to carry the power to the markets, adding to their levelized costs.\n\nSolar has the advantage of scaling easily from systems as small as a single solar panel placed on the customer's roof. In this case the system has to compete with the post-delivery \"retail\" price, which is generally much higher than the wholesale price at the same time.\n\nIt is also important to consider changes in grid pricing when determining whether or not a source is at parity. For instance, the introduction of time-of-use pricing and a general increase in power prices in Mexico during 2010 and 2011 has suddenly made many forms of renewable energy reach grid parity. A drop in power prices, as has happened in some locations due to the late-2000s recession, can likewise render systems formerly at parity no longer interesting.\n\nIn general terms, fuel prices continue to increase, while renewable energy sources continue to reduce in up-front costs. As a result, widespread grid parity for wind and solar are generally predicted for the time between 2015 and 2020.\n\nGrid parity is most commonly used in the field of solar power, and most specifically when referring to solar photovoltaics (PV). As PV systems do not use fuel and are largely maintenance-free, the levelized cost of electricity (LCOE) is dominated almost entirely by the capital cost of the system. With the assumption that the discount rate will be similar to the inflation rate of grid power, the levelized cost can be calculated by dividing the original capital cost by the total amount of electricity produced over the system's lifetime.\n\nAs the LCOE of solar PV is dominated by the capital costs, and the capital costs by the panels, the wholesale prices of PV modules are the main consideration when tracking grid parity. A 2015 study shows price/kWh dropping by 10% per year since 1980, and predicts that solar could contribute 20% of total electricity consumption by 2030, whereas the International Energy Agency predicts 16% by 2050.\n\nThe price of electricity from these sources dropped about 25 times between 1990 and 2010. This rate of price reduction accelerated between late-2009 and mid-2011 due to oversupply; the wholesale cost of solar modules dropped approximately 70%. These pressures have demanded efficiencies throughout the construction chain, so total installed cost has also been strongly lowered. Adjusting for inflation, it cost $96 per watt for a solar module in the mid-1970s. Process improvements and a very large boost in production have brought that figure down 99 percent, to 68¢ per watt in February 2016, according to data from Bloomberg New Energy Finance. The downward move in pricing continues. Palo Alto California signed a wholesale purchase agreement in 2016 that secured solar power for 3.7 cents per kilowatt-hour. And in sunny Dubai large-scale solar generated electricity sold in 2016 for just $0.0299 per kWh -- \"competitive with any form of fossil-based electricity — and cheaper than most.\"\n\nThe average retail price of solar cells as monitored by the Solarbuzz group fell from $3.50/watt to $2.43/watt over the course of 2011, and a decline to prices below $2.00/watt seems inevitable. Solarbuzz tracks retail prices, which includes a large mark-up over wholesale prices, and systems are commonly installed by firms buying at the wholesale price. For this reason, total installation costs are commonly similar to the retail price of the panels alone. Recent total-systems installation costs are around $2500/kW in Germany or $3,250 in the UK. As of 2011, the capital cost of PV has fallen well below that of nuclear power and is set to fall further.\n\nAll that remains to calculate the LCOE is the expected production. Modules are generally warranted for 25 years and suffer only minor degradation during that time, so all that is needed to predict the generation is the local insolation. According to PVWatts a one-kilowatt system in Matsumoto, Nagano will produce 1187 kilowatt-hour (kWh) of electricity a year. Over a 25-year lifetime, the system will produce about 29,675 kWh (not accounting for the small effects of system degradation, about 0.25% a year). If this system costs $5,000 to install ($5 per watt), very conservative compared to worldwide prices, the LCOE = 5,000/29,675 ~= 17 cents per kWh. This is lower than the average Japanese residential rate of ~19.5 cents, which means that, in this simple case which skips the necessary time value of money calculation, PV has reached grid parity for residential users in Japan.\n\nDeciding whether or not PV is at grid parity is more complex than other sources, due to a side-effect of one of its main advantages. Compared to most sources, like wind turbines or hydro dams, PV can scale successfully to systems as small as one panel or as large as millions. In the case of small systems, they can be installed at the customer's location. In this case the LCOE competes against the \"retail\" price of grid power, which includes all upstream additions like transmission fees, taxes, etc. In the example above, grid parity has been reached in Nagano. However, retail prices are generally higher than wholesale prices, so grid parity may not have been reached for the very same system installed on the supply-side of the grid.\n\nIn order to encompass all of these possibilities, Japan's NEDO defines the grid parity in three phases:\nThese categories are ranked in terms of the price of power they displace; residential power is more expensive than commercial wholesale. Thus, it is expected that the 1st phase would be reached earlier than the 3rd phase.\n\nPredictions from the 2006 time-frame expected \"retail\" grid parity for solar in the 2016 to 2020 era, but due to rapid downward pricing changes, more recent calculations have forced dramatic reductions in time scale, and the suggestion that solar has already reached grid parity in a wide variety of locations. The European Photovoltaic Industry Association (EPIA) calculated that PV would reach parity in many of the European countries by 2020, with costs declining to about half of those of 2010. However, this report was based on the prediction that prices would fall 36 to 51% between 2010 and 2020, a decrease that actually took place during the year the report was authored. The parity line was claimed to have been crossed in Australia in September 2011, and module prices have continued to fall since then.\n\nStanwell Corporation, an electricity generator owned by the Queensland government made a loss in 2013 from its 4,000 MW of coal and gas fired generation. The company attributed this loss to the expansion of rooftop solar generation which reducing the price of electricity during the day, on some days the price per MWh (usually $40–$50 Australian dollars) was almost zero. The Australian Government and Bloomberg New Energy Finance forecast the production of energy by rooftop solar to rise sixfold between 2014 and 2024.\n\nPhotovoltaics are now starting to compete in some places without subsidies. Shi Zhengrong has said that, as of 2012, unsubsidised solar power is already competitive with fossil fuels in India, Hawaii, Italy and Spain. As PV system prices decline it is inevitable that subsidies will end. \"Solar power will be able to compete without subsidies against conventional power sources in half the world by 2015\". In fact, recent evidence suggest that photovoltaic grid parity has already been reached in countries of the Mediterranean basin (Cyprus).\n\nPredictions that a power source becomes self-supporting when parity is reached appear to be coming true. According to many measures, PV is the fastest growing source of power in the world:\n\nFor large-scale installations, prices below $1.00/watt are now common. In some locations, PV has reached grid parity, the cost at which it is competitive with coal or gas-fired generation. More generally, it is now evident that, given a carbon price of $50/ton, which would raise the price of coal-fired power by 5c/kWh, solar PV will be cost-competitive in most locations. The declining price of PV has been reflected in rapidly growing installations, totalling about 23 GW in 2011. Although some consolidation is likely in 2012, as firms try to restore profitability, strong growth seems likely to continue for the rest of the decade. Already, by one estimate, total investment in renewables for 2011 exceeded investment in carbon-based electricity generation.\nThe dramatic price reductions in the PV industry have been causing a number of other power sources to become less interesting. Nevertheless, there remains the widespread belief that concentrating solar power (CSP) will be even less expensive than PV, although this is suitable for industrial-scale projects only, and thus has to compete at wholesale pricing. One company stated in 2011 that CSP costs $0.12/kWh to produce in Australia, and expects this to drop to $0.06/kWh by 2015 due to improvements in technology and reductions in equipment manufacturing costs. Greentech Media predicts that LCOE of CSP and PV power will lower to $0.07–$0.12/kWh by 2020 in California.\n\nGrid parity also applies to wind power where it varies according to wind quality and existing distribution infrastructure. ExxonMobil predicts wind power real cost will approach parity with natural gas and coal without carbon sequestration and be cheaper than natural gas and coal with carbon sequestration by 2025.\n\nWind turbines reached grid parity in some areas of Europe in the mid-2000s, and in the US around the same time. Falling prices continue to drive the levelized cost down and it has been suggested that it has reached general grid parity in Europe in 2010, and will reach the same point in the US around 2016 due to an expected reduction in capital costs of about 12%. Nevertheless, a significant amount of the wind power resource in North America remains above grid parity due to the long transmission distances involved. (see also OpenEI Database for cost of electricity by source).\n\n\n"}
{"id": "53577480", "url": "https://en.wikipedia.org/wiki?curid=53577480", "title": "Haugaland Kraft", "text": "Haugaland Kraft\n\nHaugaland Kraft is a Norwegian power company formed in 1998 as a merger between Haugesund Energi and Karmsund Kraftlag.\n\nHaugaland Kraft is owned by the municipalities of Karmøy, Haugesund, Tysvær, Vindafjord, Bokn, Sveio and Utsira.\n\nThe company produces and distributes electric power (mainly hydroelectricity) to customers. In recent years, the company has been exploring possibilities of building wind power stations, but it has proven to be difficult to get concession from local authorities for such plans. The company also offers broadband services.\n\nThe head office is located in Haugesund.\n"}
{"id": "6693502", "url": "https://en.wikipedia.org/wiki?curid=6693502", "title": "Institute for Sustainable Energy", "text": "Institute for Sustainable Energy\n\nThe Institute for Sustainable Energy at Eastern Connecticut State University was established in 2001 to identify, develop, and implement the means for achieving a sustainable energy future. The Institute focuses on matters relating to energy education, energy policy, efficiency conservation and load management, renewable energy, distributed generation, protection of environmental resources, and the dissemination of useful information on energy alternatives and sustainability to users and providers of energy. The Institute adds an unbiased focus on practical applications and dissemination of information about how to improve the energy profile and sustainability of Connecticut and the region.\n\nThe Institute is funded and supported by the Connecticut Energy Efficiency Fund through the Energy Conservation Management Board and the Department of Public Utility Control. The Institute also receives grants, conference sponsorships, donations, contracts, and payments for services from organizations, including the US EPA, US DOE, CT Office of Policy and Management, CT Department of Environmental Protection, the CT Clean Energy Fund, and the CT Green Building Council. The current director of the Institute is William Leahy.\n\nAs of January 1, 2009, the Connecticut State Building Code shall require that any (1) building, except a residential building with no more than four units, constructed after January 1, 2009, that is projected to cost not less than five million dollars, and (2) renovation to any building, except a residential building with no more than four units, started after January 1, 2010, that is projected to cost not less than two million dollars must receive a Leadership in Energy and Environmental Design (LEED) Silver rating or higher. The Institute for Sustainable Energy will be the sole entity responsible for providing exemptions to this new CT Building Code requirement.\n\n"}
{"id": "55543636", "url": "https://en.wikipedia.org/wiki?curid=55543636", "title": "Jiaxing Power Station", "text": "Jiaxing Power Station\n\nThe Jiaxing Power Station is a state-owned 5000 megawatt coal fired power station owned by Zhejiang Energy Group Co Ltd in Zhejiang. The $15.78 billion plant is situated in Jiaxing.\n\nThe plant consists of two 300MW sub-critical units, four 600MW super-critical units and two 1000MW ultra-supercritical units, which combined form the 5000MW plant.\n"}
{"id": "18401", "url": "https://en.wikipedia.org/wiki?curid=18401", "title": "Laminar flow", "text": "Laminar flow\n\nIn fluid dynamics, laminar flow (or streamline flow) occurs when a fluid flows in parallel layers, with no disruption between the layers. At low velocities, the fluid tends to flow without lateral mixing, and adjacent layers slide past one another like playing cards. There are no cross-currents perpendicular to the direction of flow, nor eddies or swirls of fluids. In laminar flow, the motion of the particles of the fluid is very orderly with particles close to a solid surface moving in straight lines parallel to that surface.\nLaminar flow is a flow regime characterized by high momentum diffusion and low momentum convection.\n\nWhen a fluid is flowing through a closed channel such as a pipe or between two flat plates, either of two types of flow may occur depending on the velocity and viscosity of the fluid: laminar flow or turbulent flow. Laminar flow tends to occur at lower velocities, below a threshold at which it becomes turbulent. Turbulent flow is a less orderly flow regime that is characterised by eddies or small packets of fluid particles, which result in lateral mixing. In non-scientific terms, laminar flow is \"smooth\", while turbulent flow is \"rough\".\n\nThe type of flow occurring in a fluid in a channel is important in fluid-dynamics problems and subsequently affects heat and mass transfer in fluid systems. The dimensionless Reynolds number is an important parameter in the equations that describe whether fully developed flow conditions lead to laminar or turbulent flow. The Reynolds number is the ratio of the inertial force to the shearing force of the fluid: how fast the fluid is moving relative to how viscous it is, irrespective of the scale of the fluid system. Laminar flow generally occurs when the fluid is moving slowly or the fluid is very viscous. As the Reynolds number increases, such as by increasing the flow rate of the fluid, the flow will transition from laminar to turbulent flow at a specific range of Reynolds numbers, the laminar–turbulent transition range depending on small disturbance levels in the fluid or imperfections in the flow system. If the Reynolds number is very small, much less than 1, then the fluid will exhibit Stokes, or creeping, flow, where the viscous forces of the fluid dominate the inertial forces.\n\nThe specific calculation of the Reynolds number, and the values where laminar flow occurs, will depend on the geometry of the flow system and flow pattern. The common example is flow through a pipe, where the Reynolds number is defined as\n\nwhere:\n\nFor such systems, laminar flow occurs when the Reynolds number is below a critical value of approximately 2,040, though the transition range is typically between 1,800 and 2,100.\n\nFor fluid systems occurring on external surfaces, such as flow past objects suspended in the fluid, other definitions for Reynolds numbers can be used to predict the type of flow around the object. The particle Reynolds number Re would be used for particle suspended in flowing fluids, for example. As with flow in pipes, laminar flow typically occurs with lower Reynolds numbers, while turbulent flow and related phenomena, such as vortex shedding, occur with higher Reynolds numbers.\n\nA common application of laminar flow is in the smooth flow of a viscous liquid through a tube or pipe. In that case, the velocity of flow varies from zero at the walls to a maximum along the cross-sectional centre of the vessel. The flow profile of laminar flow in a tube can be calculated by dividing the flow into thin cylindrical elements and applying the viscous force to them.\n\nAnother example is the flow of air over an aircraft wing. The boundary layer is a very thin sheet of air lying over the surface of the wing (and all other surfaces of the aircraft). Because air has viscosity, this layer of air tends to adhere to the wing. As the wing moves forward through the air, the boundary layer at first flows smoothly over the streamlined shape of the airfoil. Here, the flow is laminar and the boundary layer is a laminar layer. Prandtl applied the concept of the laminar boundary layer to airfoils in 1904.\n\nLaminar airflow is used to separate volumes of air, or prevent airborne contaminants from entering an area. Laminar flow hoods are used to exclude contaminants from sensitive processes in science, electronics and medicine. Air curtains are frequently used in commercial settings to keep heated or refrigerated air from passing through doorways. A laminar flow reactor (LFR) is a reactor that uses laminar flow to study chemical reactions and process mechanisms.\n\n\n"}
{"id": "22158946", "url": "https://en.wikipedia.org/wiki?curid=22158946", "title": "Lamination paper", "text": "Lamination paper\n\nLamination paper is a paper used for laminates. Normally on particle or fiberboards giving a good-looking and resistant surface for use as furniture, decoration panels and flooring.\n\nA laminate consists of a single or multiple layers, each having its own distinct function. The base is most often particle- or fiberboards, then some layers of absorbent kraft paper. The last layers are a \"decor paper\" covered with an \"overlay\". The lamination papers are covered with an inert resin, often melamine, which is cured to form a hard composite with the structure of paper. The laminates may also have a lining on the back side of \"laminating kraft\" to compensate for the tension created by the top side lamination.\n\nCheaper particle boards may have only a lining of laminating kraft to give surface washability and resistance to wear.\n\nThe \"decor paper\" can also be processed under heat and low/high pressure to create a melamine laminated sheet, that has several applications.\n\nThe absorbent kraft paper is a normal kraft paper with controlled absorbency, which means a high degree of porosity. It is made of clean low kappa hardwood kraft with good uniformity. The grammage is 80 - 120 g/m and normally 2-4 plies are used.\n\nThe decor paper is the most critical of the lamination papers as it gives the visual appearance of the laminate. The impregnation resin and cellulose have about the same refraction index which means that the cellulose fibers of the paper appear as a shade and only the dyestuffs and pigments are visible. Due to this the decor paper demands extreme cleanness and is produced only on small paper machines with grammage 50 - 150 g/m.\n\nThe overlay paper have grammage of 18 – 50 m and is made of pure cellulose, thus it must be made of well delignified pulp. It becomes transparent after impregnation letting the appearance of the decor paper come through.\n\nThe laminating kraft have a grammage of 70 - 150 g/m and is a smooth dense kraft paper.\n"}
{"id": "8165166", "url": "https://en.wikipedia.org/wiki?curid=8165166", "title": "Lists of environmental topics", "text": "Lists of environmental topics\n\nThe natural environment commonly referred to simply as the environment, is all living and non-living things that occur naturally on Earth or some part of it (e.g. the natural environment in a country). This includes complete ecological units that function as natural systems without massive human intervention, including all vegetation, animals, microorganisms, rocks, atmosphere and natural phenomena that occur within their boundaries. And it includes universal natural resources and physical phenomena that lack clear-cut boundaries, such as air, water, and climate, as well as energy, radiation, electric charge, and magnetism, not originating from human activity.\n\n\n\n\n\n"}
{"id": "3124562", "url": "https://en.wikipedia.org/wiki?curid=3124562", "title": "Litter (vehicle)", "text": "Litter (vehicle)\n\nThe litter is a class of wheelless vehicles, a type of human-powered transport, for the transport of persons. Examples of litter vehicles include palki or पालकी (India), পালকি (Bengal), lectica (ancient Rome), kiệu (Vietnam, 轎), sedan chair (Britain), litera (Spain), palanquin (France, India), jiao (China, 轎), liteira (Portugal), wo (วอ, Chinese style known as kiao เกี้ยว) (Thailand), gama (Korea), koshi, ren, Norimono, and kago, (Japan, 駕籠), tahtırevan (Turkey) and sankayan (Philippines). \n\nSmaller litters may take the form of open chairs or beds carried by two or more carriers, some being enclosed for protection from the elements. Larger litters, for example those of the Chinese emperors, may resemble small rooms upon a platform borne upon the shoulders of a dozen or more people. To most efficiently carry a litter, porters either place the carrying poles directly upon their shoulders or use a yoke to transfer the load from the carrying poles to the shoulders.\n\nA simple litter, often called a king carrier, consists of a sling attached along its length to poles or stretched inside a frame. The poles or frame are carried by porters in front and behind. Such simple litters are common on battlefields and emergency situations, where terrain prohibits wheeled vehicles from carrying away the dead and wounded.\n\nLitters can also be created by the expedient of the lashing of poles to a chair. Such litters, consisting of a simple cane chair with maybe an umbrella to ward off the elements and two stout bamboo poles, may still be found in Chinese mountain resorts such as the Huangshan Mountains to carry tourists along scenic paths and to viewing positions inaccessible by other means of transport.\n\nA more luxurious version consists of a bed or couch, sometimes enclosed by curtains, for the passenger or passengers to lie on. These are carried by at least two porters in equal numbers in front and behind, using wooden rails that pass through brackets on the sides of the couch. The largest and heaviest types would be carried by draught animals.\nAnother form, commonly called a sedan chair, consists of a chair or windowed cabin suitable for a single occupant, also carried by at least two porters in front and behind, using wooden rails that pass through brackets on the sides of the chair. These porters were known in London as \"chairmen\". These have been very rare since the 19th century, but such enclosed portable litters have been used as an elite form of transport for centuries, especially in cultures where women are kept secluded.\n\nSedan chairs, in use until the 19th century, were accompanied at night by link-boys who carried torches. Where possible, the link boys escorted the fares to the chairmen, the passengers then being delivered to the door of their lodgings. Several houses in Bath, Somerset, England still have the link extinguishers on the exteriors, shaped like outsized candle snuffers (). In the 1970s, entrepreneur and Bathwick resident, John Cuningham, revived the sedan chair service business for a brief amount of time.\n\nIn pharaonic Egypt and many oriental realms such as India and China, the ruler and divinities (in the form of an idol like lord Krishna) were often transported in a litter in public, frequently in procession, as during state ceremonial or religious festivals.\n\nThe instructions for how to construct the Ark of the Covenant in the Book of Exodus resembles a litter.\n\nIn Ancient Rome, a litter called \"lectica\" or \"sella\" often carried members of the imperial family, as well as other dignitaries and other members of the rich elite, when not mounted on horseback.\n\nThe habit must have proven quite persistent, for the Third Council of Braga in 675 AD saw the need to order that bishops, when carrying the relics of martyrs in procession, must walk to the church, and not be carried in a chair, or litter, by deacons clothed in white.\n\nIn the Catholic Church, Popes were carried the same way in Sedia gestatoria, which was replaced later by the Popemobile.\n\nA palanquin is a covered litter, usually for one passenger. It is carried by an even number of bearers (between two and eight, but most commonly four) on their shoulders, by means of a pole projecting fore and aft.\n\nThe word is derived from the Sanskrit \"palyanka\", meaning bed or couch. The Malay and Javanese form is \"palangki\", in Hindi and Bengali, \"palki\". The Portuguese apparently added a nasal termination to these to make \"palanquim\". English adopted it from Portuguese as \"palanquin\".\n\nPalanquins vary in size and grandeur. The smallest and simplest, a cot or frame suspended by the four corners from a bamboo pole and borne by two bearers, is called a doli. Larger palanquins are rectangular wooden boxes eight feet long, four feet wide, and four feet high, with openings on either side screened by curtains or shutters. Interiors are furnished with bedding and pillows. Ornamentation reflects the social status of the traveller. The most ornate palanquins have lacquer paintwork and cast bronze finials at the ends of the poles. Designs include foliage, animals, and geometric patterns.\n\nIbn Batutta describes them as being \"carried by eight men in two lots of four, who rest and carry in turn. In the town there are always a number of these men standing in the bazaars and at the sultan's gate and at the gates of other persons for hire.\" Those for \"women are covered with silk curtains.\" \n\nPalanquins are mentioned in literature as early as the \"Ramayana\" (c. 250BC). Indian women of rank always travelled by palanquin. The conveyance proved popular with European residents in India, and was used extensively by them. Pietro Della Valle, a 17th-century Italian traveller, wrote:\n\nBeing transported by palanquin was pleasant. Owning one and keeping the staff to power it was a luxury affordable even to low-paid clerks of the East India Company. Concerned that this indulgence led to neglect of business in favor of \"rambling\", in 1758 the Court of Directors of the company prohibited its junior clerks from purchasing and maintaining palanquins. Also in the time of the British in India, \"dolis\" served as military ambulances, used to carry the wounded from the battlefield.\n\nIn the early 19th century, the most prevalent mode of long distance transport for the affluent was by palanquin. The post office could arrange, with a few days notice, relays of bearers to convey a traveller's palanquin between \"stages\" or stations. The distance between these in the government's \"dak\" (Hindi: \"mail\") system averaged about , and could be covered in three hours. A relay's usual complement consisted of two torch-bearers, two luggage-porters, and eight palanquin-bearers who worked in gangs of four, although all eight might pitch in at steep sections. A passenger could travel straight through or break their journey at dak bungalows located at certain stations.\n\nUntil the mid-19th century, \"most people in Calcutta kept a plankin and a set of bearers\", but they fell out of favor for long journeys as steamers, railways, and roads suitable for wheeled transport were developed. By the beginning of the 20th century they were nearly \"obsolete among the better class of Europeans\". Rickshaws, introduced in the 1930s, supplanted them for trips around town.\n\nModern use of the palanquin is limited to ceremonial occasions. A \"doli\" carries the bride in a traditional wedding, and they may be used to carry religious images in Hindu processions.\n\nIn Han China the elite travelled in light bamboo seats supported on a carrier's back like a backpack. In the Northern Wei Dynasty and the Northern and Southern Song Dynasties, wooden carriages on poles appear in painted landscape scrolls.\n\nA commoner used a wooden or bamboo \"civil litter\" (), while the mandarin class used an \"official litter\" () enclosed in silk curtains.\n\nThe chair with perhaps the greatest importance was the bridal chair. A traditional bride is carried to her wedding ceremony by a \"shoulder carriage\" (), usually hired. These were lacquered in an auspicious shade of red, richly ornamented and gilded, and were equipped with red silk curtains to screen the bride from onlookers.\n\nSedan chairs were once the only public conveyance in Hong Kong, filling the role of cabs. Chair stands were found at all hotels, wharves, and major crossroads. Public chairs were licensed, and charged according to tariffs which would be displayed inside. Private chairs were an important marker of a person's status. Civil officers' status was denoted by the number of bearers attached to his chair. Before Hong Kong's Peak Tram went into service in 1888, wealthy residents of The Peak were carried on sedan chairs by coolies up the steep paths to their residence including Sir Richard MacDonnell's (former Governor of Hong Kong) summer home, where they could take advantage of the cooler climate. Since 1975 an annual sedan chair race has been held to benefit the Matilda International Hospital and commemorate the practice of earlier days.\n\nIn Korea, royalty and aristocrats were carried in wooden litters called gama. Gamas were primarily used by royalty and government officials. There were six types of gama, each assigned to different government official rankings. In traditional weddings, the bride and groom are carried to the ceremony in separate gamas. Because of the difficulties posed by the mountainous terrain of the Korean peninsula and the lack of paved roads, gamas were preferred over wheeled vehicles.\n\nAs the population of Japan increased, less and less land was available as grazing for the upkeep of horses. With the availability of horses restricted to martial uses, human powered transport became more important and prevalent.\n\nKago (Kanji: 駕籠, Hiragana: かご) were often used in Japan to transport the non-samurai citizen. were used by the warrior class and nobility, most famously during the Tokugawa period when regional samurai were required to spend a part of the year in Edo (Tokyo) with their families, resulting in yearly migrations of the rich and powerful (Sankin-kōtai) to and from the capital along the central backbone road of Japan.\n\nSomewhat similar in appearance to kago are the portable shrines that are used to carry the \"god-body\" (goshintai), the central totemic core normally found in the most sacred area of Shinto Shrines, on a tour to and from a shrine during some religious festivals.\n\nTraditional Vietnam employed two distinct types of litters, the \"cáng\" and the \"kiệu\". The \"cáng\" is a basic bamboo pole with the rider reclining in a hammock. More elaborate cáng had an adjustable woven bamboo shade to shelter the occupant. Dignitaries would have an entourage to carry parasols.\n\nThe \"kiệu\" resemble more of the sedan chair, enclosed with a fixed elaborately carved roof and doors. While the cáng has become obsolete, the \"kiệu\" is retained in certain traditional rituals a part of a temple devotional procession.\n\nIn Thailand, the royalty were also carried in wooden litters called wo (\"พระวอ\" Phra Wo, literally, \"Royal Sedan\") for large ceremonies. Wos were elaborately decorated litters that were delicately carved and colored by gold leaf. Stained glass is also used to decorate the litters. Presently, Royal Wos and carriages are only used for royal ceremonies in Thailand. They are exhibited in the Bangkok National Museum.\n\nIn traditional Javanese society, the generic palanquin or \"joli\" was a wicker chair with a canopy, attached to two poles, and borne on men's shoulders, and was available for hire to any paying customer. As a status marker, gilded throne-like palanquins, or \"jempana\", were originally reserved solely for royalty, and later co-opted by the Dutch, as a status marker: the more elaborate the palanquin, the higher the status of the owner. The \"joli\" was transported either by hired help, by nobles' peasants, or by slaves.\nHistorically, the palanquin of a Javanese king (\"raja\"), prince (\"pangeran\"), lord (\"raden mas\") or other noble (\"bangsawan\") was known as a \"jempana\"; a more throne-like version was called a \"pangkem\". It was always part of a large military procession, with a yellow (the Javanese colour for royalty) square canopy. The ceremonial parasol (\"payung\") was held above the palanquin, which was carried by a bearer behind and flanked by the most loyal bodyguards, usually about 12 men, with pikes, sabres, lances, muskets, keris and a variety of disguised blades. In contrast, the canopy of the Sumatran palanquin was oval-shaped and draped in white cloth; this was reflective of greater cultural permeation by Islam. Occasionally, a weapon or heirloom, such as an important keris or tombak, was given its own palanquin. In Hindu culture in Bali today, the tradition of using palanquins for auspicious statues, weapons or heirlooms continues, for funerals especially; in more elaborate rituals, a palanquin is used to bear the body, and is subsequently cremated along with the departed.\n\nIn pre-colonial Philippines, litters were a way of transportation for the elite; Datus or sovereign princes and their wives use a Sankayan or Sakayan, a wooden or bamboo throne called with elaborate and intricate carvings carried by their servants. Also among their retinue were umbrella-bearers, to shade the royalty and nobility from the intense heat.\n\nPrinces or princesses who were sequestered from the world were called Binukot or Binocot (“set apart”). A special type of royal, these individuals were forbidden to walk on the ground or be exposed to the general populace. When they needed to go anywhere, they were veiled and carried in a hammock or a basket-like litter similar to bird’s nests carried by their slaves. Longer journeys required that they be borne inside larger, covered palanquins with silk covers, with some taking the form of a miniature hut.\n\nIn Southern Ghana the Akan and the Ga-Dangme carry their chiefs and kings in palanquins when they appear in their state durbars. When used in such occasions these palanquins may be seen as a substitutes of a state coach in Europe or a horse used in Northern Ghana. The chiefs of the Ga (\"mantsemei\") in the Greater Accra Region (Ghana) use also figurative palanquins which are built after a chief's family symbol or totem. But these day the figurative palanquins are very seldom used. They are related with the figurative coffins which have become very popular among the Ga in the last 50 years. Since these figurative coffins were shown 1989 in the exhibition \"Les magicians de la terre\" in the Centre Pompidou in Paris they were shown in many art museums around the world.\n\nFrom at least the 15th century until the 19th century, litters of varying types known as \"tipoye\" were used in the Kingdom of Kongo as a mode of transportation for the elites. Seat-style litters with a single pole along the back of the chair carried by two men (usually slaves) were topped with an umbrella. Lounge-style litters in the shape of a bed were used to move one to two people with a porter at each corner. Due to the tropical climate, horses were not native to the area nor could they survive very long once introduced by the Portuguese. Human portage was the only mode of transportation in the region and became highly adept with missionary accounts claiming the litter transporters could move at speeds 'as fast as post horses at the gallop'.\n\nPortuguese and Spanish navigators and colonisers encountered litters of various sorts in India, Mexico, and Peru. Such novelties, imported into Spain, spread into France and then to Britain. All the European names for these devices ultimately derive from the root \"sed-\", as in Latin \"sedere\", \"to sit,\" which gave rise to \"seda\" (\"seat\") and its diminutive \"sedula\" (\"little seat\"), the latter of which was contracted to \"sella\", the traditional Classical Latin name for a chair, including a carried chair.\nIn Europe this mode of transportation met with instant success. Henry VIII of England (reigned 1509-1547) was carried around in a sedan chair—it took four strong chairmen to carry him towards the end of his life—but the expression \"sedan chair\" did not appear in print until 1615. It does not seem to take its name from the city of Sedan. Trevor Fawcett notes (see link) that British travellers Fynes Moryson (in 1594) and John Evelyn (in 1644–5) remarked on the \"seggioli\" of Naples and Genoa, which were chairs for public hire slung from poles and carried on the shoulders of two porters.\n\nFrom the mid-17th century, visitors taking the waters at Bath would be conveyed in a chair enclosed in baize curtains, especially if they had taken a heated bath and were going straight to bed to sweat. The curtains kept off a possibly fatal draft. These were not the proper sedan chairs \"to carry the better sort of people in visits, or if sick or infirmed\" (Celia Fiennes). In the 17th and 18th centuries, the chairs stood in the main hall of a well-appointed city residence, where a lady could enter and be carried to her destination without setting foot in a filthy street. The neoclassical sedan chair made for Queen Charlotte (Queen Consort from 1761 to 1818) remains at Buckingham Palace.\nBy the mid-17th century, sedans for hire had become a common mode of transportation. London had \"chairs\" available for hire in 1634, each assigned a number and the chairmen licensed because the operation was a monopoly of a courtier of King Charles I. Sedan chairs could pass in streets too narrow for a carriage and were meant to alleviate the crush of coaches in London streets, an early instance of traffic congestion. A similar system later operated in Scotland. In 1738 a fare system was established for Scottish sedans, and the regulations covering chairmen in Bath are reminiscent of the modern Taxi Commission's rules. A trip within a city cost six pence and a day's rental was four shillings. A sedan was even used as an ambulance in Scotland's Royal Infirmary.\n\nChairmen moved at a good clip. In Bath they had the right-of-way: pedestrians hearing \"By your leave\" behind them knew to flatten themselves against walls or railings as the chairmen hustled through. There were often disastrous accidents, upset chairs, and broken glass-paned windows.\n\nThe wealthy also used sedan chairs in the cities of colonial America. And an ailing 81-year-old Benjamin Franklin travelled to meetings of the United States Constitutional Convention in 1787 in a sedan chair.\n\nIn various colonies, litters of various types were maintained under native traditions, but often adopted by the white colonials as a new ruling and/or socio-economic elite, either for practical reasons (often comfortable modern transport was unavailable, e.g. for lack of decent roads) and/or as a status symbol. During the 17-18th centuries, palanquins (see above) were very popular among European traders in Bengal, so much so that in 1758 an order was issued prohibiting their purchase by certain lower-ranking employees.\n\nIn Great Britain, in the early 19th century, the public sedan chair began to fall out of use, perhaps because streets were better paved or perhaps because of the rise of the more comfortable, companionable and affordable hackney carriage. In Glasgow, the decline of the sedan chair is illustrated by licensing records which show twenty-seven sedan chairs in 1800, eighteen in 1817, and ten in 1828. During that same period the number of registered hackney carriages in Glasgow rose to one hundred and fifty.\n\nA similar but simpler palanquin was used by the elite in parts of 18th- and 19th-century Latin America. Often simply called a \"silla\" (Spanish for seat or chair), it consisted of a simple wooden chair with an attached tumpline. The occupant sat in the chair, which was then affixed to the back of a single porter, with the tumpline supported by his head. The occupant thus faced backwards during travel. This style of palanquin was probably due to the steep terrain and rough or narrow roads unsuitable to European-style sedan chairs. Travellers by \"silla\" usually employed a number of porters, who would alternate carrying the occupant. The porters were known as silleros, cargueros or silleteros (sometimes translated as \"saddle-men\").\n\nA chair borne on the back of a porter, almost identical to the \"silla\", is used in the mountains of China for ferrying older tourists and visitors up and down the mountain paths. One of these mountains where the \"silla\" is still used is the Huangshan Mountains of Anhui province in Eastern China.\n\n\n\n"}
{"id": "15262271", "url": "https://en.wikipedia.org/wiki?curid=15262271", "title": "Living stump", "text": "Living stump\n\nA living stump is created when a live tree is cut, burned, eaten, or infected, causing its cambium to die above the root system. Stumps are generally characterized as having a thin outer layer of living cells that surround a hollow central cavity. Living stumps can survive for several years by root grafting to the root system of living trees and using excess carbon reserves. Root grafting allows for carbon transfer from living trees to living stumps resulting in incremental cambium growth in the stump. Stumps can grow a callus tissue over its cross section which prolongs longevity of the stump by protecting it from infection and insect damage. A living stump which is capable of producing sprouts or cuttings is known as a \"stool\", and is used in the coppicing method of woodland management. \n\n\"Pinus strobus\" (white pine)\n\n\"Castanea dentata\" (American chestnut)\n\n\"Tsuga spp.\" (hemlock)\n\n\"Pseudotsuga menziesii\" (Douglas fir)\n\n\"Cedrus spp.\" (cedar)\n"}
{"id": "3539154", "url": "https://en.wikipedia.org/wiki?curid=3539154", "title": "Low rolling resistance tire", "text": "Low rolling resistance tire\n\nLow rolling resistance tires are designed to reduce the energy loss as a tire rolls, decreasing the required rolling effort — and in the case of automotive applications, improving vehicle fuel efficiency. Approximately 5–15% of the fuel consumed by a typical car may be used to overcome rolling resistance. A 2003 California Energy Commission (CEC) preliminary study estimated that adoption of low-rolling resistance tires could save 1.5–4.5% of all gasoline consumption, but that current data were also insufficient to compare safety and other characteristics.\n\nA United States National Highway Traffic Safety Administration (NHTSA) study in 2009 found that if 2% of the replacement tires would reduce their rolling resistance by 5%, there would be 7.9 million gallons fuel and 76,000 metric tons of CO2 saved annually. \n\nRolling resistance can be expressed by the rolling resistance coefficient (RRC or C), which is the value of the rolling resistance force divided by the wheel load. A lower coefficient means the tires will use less energy to travel a certain distance. The coefficient is mostly considered as independent of speed, but for precise calculations it is tabled at several speeds or an additional speed-dependent part is used. The Society of Automotive Engineers (SAE) has developed test practices to measure the RRC of tires. These tests (SAE J1269 and SAE J2452) are usually performed on new tires.\n\nWhen measured by using these standard test practices, most new passenger tires have reported RRCs ranging from 0.007 to 0.014. In the case of bicycle tires, values of 0.0025 to 0.005 are achieved. These coefficients are measured on rollers, with power meters on road surfaces, or with coast-down tests. In the latter two cases, the effect of air resistance must be subtracted or the tests performed at very low speeds.\nIn 2009 The CEC used a rating called Rolling Resistance Force RRF. RRF and RRC, rolling resistance coefficient are very similar. Difference is taking the RRF and dividing it by the load(weight) to get RRC. So a Michelin Harmony tire rated at 9.45 RRF at 1000 pounds load would be .0095 RRC.\n\nIn Canada, Transport Canada tests will be conducted on a number of different tires mounted on 15 and 16-inch rims – the most common tire sizes in Canada – to determine how rolling resistance is influenced by vehicle size, tire width and profile. Results will be used to inform Canadians about the types of low rolling resistance tires available in Canada, and whether they can help reduce fuel consumption and pollutants from passenger vehicles.\nSAE J2452 is a standard defined by the Society of Automotive Engineers to define the rolling resistance of tires.\n\nThe rolling resistance coefficient (RRC) indicates the amount of force required to overcome the hysteresis of the material as the tire rolls. Tire pressure, vehicle weight and velocity all play a role in how much force is lost to rolling resistance.\n\nThe basic model equation for SAE J2452 is:\n\nwhere:\n\nThe units of the coefficients are matched to the units used in the model, i.e. ( metric / Imperial)\n\nThis model is newer than SAE J1269 and provides more accuracy over a range of different vehicle loads (weight), tire pressures and vehicle speeds.\n\nBecause fuel efficiency is an important selling point for most hybrid vehicles, they are often equipped with low-rolling resistance tires. Electric vehicles are also often equipped with low-rolling resistance tires to maximize their range.\n\nAuto manufacturers in the United States typically equip new vehicles with tires that have lower rolling resistance than their average after-market replacements, in order to meet Corporate Average Fuel Economy (CAFE) standards.\n\nThese include Conti Contact, Michelin Energy, Bridgestone Ecopia, and Goodyear Eagle LS tires. For Indian roads, Madras Rubber Factory(MRF) offers the MRF ZSLK range of eco-friendly car tires with low rolling resistance.\n\nSome tires available in 2003 ranked by coefficient from lowest (least wasteful), according to the United States National Academy of Sciences Transportation Research Board Special Report 286 and the March 2003 Green Seal report on the topic.\n\n\nHere is a list of Consumer Report's tires that achieved their best rolling resistance rating. The tires at the top of the list are rated higher overall.\n\nBelow are the light duty tires (as reported by Consumer reports) achieving their best rolling resistance rating. Again, higher overall rated tires are closer to the top of the list.\nFollowed by:\n\nNew models by 2009:\n\nDepending on the specific technique and materials used by the manufacturers, tire life may be as good as conventional tires, and traction may also be as good.\n\nA Union of Concerned Scientists newsletter says \"LRR tires also meet the same federal standards for treadwear, traction, and temperature resistance as regular tires.\" \n\nOlder Low-rolling resistance tires may reduce ability to grip, especially when taking corners, and may also wear out more rapidly.\n\nCalifornia has rolling resistance requirements that went into effect in July 2008. The law was passed in 2003 and the standards and reporting requirements were finalized in 2007.\n\nUnited Nations require a regulation too: \"Tire Rolling Sound Emissions, Adhesion on Wet Surfaces, and Rolling Resistance | UN Regulation No. 117\".\n\nPassenger tires are tested for rolling resistance in order to obtain the German Blue Angel eco-label \n\n\n"}
{"id": "19916", "url": "https://en.wikipedia.org/wiki?curid=19916", "title": "Meitnerium", "text": "Meitnerium\n\nMeitnerium is a synthetic chemical element with symbol Mt and atomic number 109. It is an extremely radioactive synthetic element (an element not found in nature, but can be created in a laboratory). The most stable known isotope, meitnerium-278, has a half-life of 7.6 seconds, although the unconfirmed meitnerium-282 may have a longer half-life of 67 seconds. The GSI Helmholtz Centre for Heavy Ion Research near Darmstadt, Germany, first created this element in 1982. It is named after Lise Meitner.\n\nIn the periodic table, meitnerium is a d-block transactinide element. It is a member of the 7th period and is placed in the group 9 elements, although no chemical experiments have yet been carried out to confirm that it behaves as the heavier homologue to iridium in group 9 as the seventh member of the 6d series of transition metals. Meitnerium is calculated to have similar properties to its lighter homologues, cobalt, rhodium, and iridium.\n\nMeitnerium was first synthesized on August 29, 1982 by a German research team led by Peter Armbruster and Gottfried Münzenberg at the Institute for Heavy Ion Research (Gesellschaft für Schwerionenforschung) in Darmstadt. The team bombarded a target of bismuth-209 with accelerated nuclei of iron-58 and detected a single atom of the isotope meitnerium-266:\n\nThis work was confirmed three years later at the Joint Institute for Nuclear Research at Dubna (then in the Soviet Union).\n\nUsing Mendeleev's nomenclature for unnamed and undiscovered elements, meitnerium should be known as \"eka-iridium\". In 1979, during the Transfermium Wars (but before the synthesis of meitnerium), IUPAC published recommendations according to which the element was to be called \"unnilennium\" (with the corresponding symbol of \"Une\"), a systematic element name as a placeholder, until the element was discovered (and the discovery then confirmed) and a permanent name was decided on. Although widely used in the chemical community on all levels, from chemistry classrooms to advanced textbooks, the recommendations were mostly ignored among scientists in the field, who either called it \"element 109\", with the symbol of \"E109\", \"(109)\" or even simply \"109\", or used the proposed name \"meitnerium\".\n\nThe naming of meitnerium was discussed in the element naming controversy regarding the names of elements 104 to 109, but \"meitnerium\" was the only proposal and thus was never disputed. The name \"meitnerium\" (Mt) was suggested by the GSI team in September 1992 in honor of the Austrian physicist Lise Meitner, a co-discoverer of protactinium (with Otto Hahn), and one of the discoverers of nuclear fission. In 1994 the name was recommended by IUPAC, and was officially adopted in 1997. It is thus the only element named specifically after a non-mythological woman (curium being named for both Pierre and Marie Curie).\n\nMeitnerium has no stable or naturally occurring isotopes. Several radioactive isotopes have been synthesized in the laboratory, either by fusing two atoms or by observing the decay of heavier elements. Eight different isotopes of meitnerium have been reported with atomic masses 266, 268, 270, and 274–278, two of which, meitnerium-268 and meitnerium-270, have known but unconfirmed metastable states. A ninth isotope with atomic mass 282 is unconfirmed. Most of these decay predominantly through alpha decay, although some undergo spontaneous fission.\n\nAll meitnerium isotopes are extremely unstable and radioactive; in general, heavier isotopes are more stable than the lighter. The most stable known meitnerium isotope, Mt, is also the heaviest known; it has a half-life of 7.6 seconds. (The unconfirmed Mt is yet heavier and appears to have an even longer half-life of 67 seconds.) A metastable nuclear isomer, Mt, has been reported to also have a half-life of over a second. The isotopes Mt and Mt have half-lives of 0.72 and 0.44 seconds respectively. The remaining four isotopes have half-lives between 1 and 20 milliseconds. The undiscovered isotope Mt has been predicted to be the most stable towards beta decay; no known meitnerium isotope has been observed to undergo beta decay. Some unknown isotopes, such as Mt, Mt, and Mt, are predicted to have half-lives longer than the known isotopes. Before their discovery, Mt and Mt were predicted to have half-lives of 20 seconds and 1 minute respectively, but they were later found to have half-lives of only 0.44 seconds and 5 milliseconds respectively.\n\nMeitnerium is the seventh member of the 6d series of transition metals. Since element 112 (copernicium) has been shown to be a group 12 metal, it is expected that all the elements from 104 to 111 would continue a fourth transition metal series, with meitnerium as part of the platinum group metals. Calculations on its ionization potentials and atomic and ionic radii are similar to that of its lighter homologue iridium, thus implying that meitnerium's basic properties will resemble those of the other group 9 elements, cobalt, rhodium, and iridium.\n\nPrediction of the probable chemical properties of meitnerium has not received much attention recently. Meitnerium is expected to be a noble metal. Based on the most stable oxidation states of the lighter group 9 elements, the most stable oxidation states of meitnerium are predicted to be the +6, +3, and +1 states, with the +3 state being the most stable in aqueous solutions. In comparison, rhodium and iridium show a maximum oxidation state of +6, while the most stable states are +4 and +3 for iridium and +3 for rhodium. The oxidation state +9, represented only by iridium in [IrO], might be possible for its congener meitnerium in the nonafluoride (MtF) and the [MtO] cation, although [IrO] is expected to be more stable than these meitnerium compounds. The tetrahalides of meitnerium have also been predicted to have similar stabilities to those of iridium, thus also allowing a stable +4 state. It is further expected that the maximum oxidation states of elements from bohrium (element 107) to darmstadtium (element 110) may be stable in the gas phase but not in aqueous solution.\n\nMeitnerium is expected to be a solid under normal conditions and assume a face-centered cubic crystal structure, similarly to its lighter congener iridium. It should be a very heavy metal with a density of around 37.4 g/cm, which would be the second-highest of any of the 118 known elements, second only to that predicted for its neighbor hassium (41 g/cm). In comparison, the densest known element that has had its density measured, osmium, has a density of only 22.61 g/cm. This results from meitnerium's high atomic weight, the lanthanide and actinide contractions, and relativistic effects, although production of enough meitnerium to measure this quantity would be impractical, and the sample would quickly decay. Meitnerium is also predicted to be paramagnetic.\n\nTheoreticians have predicted the covalent radius of meitnerium to be 6 to 10 pm larger than that of iridium. The atomic radius of meitnerium is expected to be around 128 pm.\n\nMeitnerium is the first element on the periodic table whose chemistry has not yet been investigated. Unambiguous determination of the chemical characteristics of meitnerium has yet to have been established due to the short half-lives of meitnerium isotopes and a limited number of likely volatile compounds that could be studied on a very small scale. One of the few meitnerium compounds that are likely to be sufficiently volatile is meitnerium hexafluoride (), as its lighter homologue iridium hexafluoride () is volatile above 60 °C and therefore the analogous compound of meitnerium might also be sufficiently volatile; a volatile octafluoride () might also be possible. For chemical studies to be carried out on a transactinide, at least four atoms must be produced, the half-life of the isotope used must be at least 1 second, and the rate of production must be at least one atom per week. Even though the half-life of Mt, the most stable known meitnerium isotope, is 7.6 seconds, long enough to perform chemical studies, another obstacle is the need to increase the rate of production of meitnerium isotopes and allow experiments to carry on for weeks or months so that statistically significant results can be obtained. Separation and detection must be carried out continuously to separate out the meitnerium isotopes and have automated systems experiment on the gas-phase and solution chemistry of meitnerium, as the yields for heavier elements are predicted to be smaller than those for lighter elements; some of the separation techniques used for bohrium and hassium could be reused. However, the experimental chemistry of meitnerium has not received as much attention as that of the heavier elements from copernicium to livermorium.\n\nThe Lawrence Berkeley National Laboratory attempted to synthesize the isotope Mt in 2002–2003 for a possible chemical investigation of meitnerium because it was expected that it might be more stable than the isotopes around it as it has 162 neutrons, a magic number for deformed nuclei; its half-life was predicted to be a few seconds, long enough for a chemical investigation. However, no atoms of Mt were detected, and this isotope of meitnerium is currently unknown.\n\nAn experiment determining the chemical properties of a transactinide would need to compare a compound of that transactinide with analogous compounds of some of its lighter homologues: for example, in the chemical characterization of hassium, hassium tetroxide (HsO) was compared with the analogous osmium compound, osmium tetroxide (OsO). In a preliminary step towards determining the chemical properties of meitnerium, the GSI attempted sublimation of the rhodium compounds rhodium(III) oxide (RhO) and rhodium(III) chloride (RhCl). However, macroscopic amounts of the oxide would not sublimate until 1000 °C and the chloride would not until 780 °C, and then only in the presence of carbon aerosol particles: these temperatures are far too high for such procedures to be used on meitnerium, as most of the current methods used for the investigation of the chemistry of superheavy elements do not work above 500 °C.\n\nFollowing the 2014 successful synthesis of seaborgium hexacarbonyl, Sg(CO), studies were conducted with the stable transition metals of groups 7 through 9, suggesting that carbonyl formation could be extended to further probe the chemistries of the early 6d transition metals from rutherfordium to meitnerium inclusive. Nevertheless, the challenges of low half-lives and difficult production reactions make meitnerium difficult to access for radiochemists, though the isotopes Mt and Mt are long-lived enough for chemical research and may be produced in the decay chains of Ts and Mc respectively. Mt is likely more suitable, since producing tennessine requires a rare and rather short-lived berkelium target.\n\n"}
{"id": "6134329", "url": "https://en.wikipedia.org/wiki?curid=6134329", "title": "Merle Randall", "text": "Merle Randall\n\nMerle Randall (January 29, 1888 – March 17, 1950) was an American physical chemist famous for his work with Gilbert N. Lewis, over a period of 25 years, in measuring reaction heat of chemical compounds and determining their corresponding free energy. Together, their 1923 textbook \"Thermodynamics and the Free Energy of Chemical Substances\" became a classic work in the field of chemical thermodynamics.\n\nIn 1932, Merle Randall authored two scientific papers with Mikkel Frandsen: \"“The Standard Electrode Potential of Iron and the Activity Coefficient of Ferrous Chloride,”\" and \"“Determination of the Free Energy of Ferrous Hydroxide from Measurements of Electromotive Force.”\"\n\nRandall completed his Ph.D. at the Massachusetts Institute of Technology in 1912 with a dissertation on \"“Studies in Free Energy”\".\n\nBased on work by J. Willard Gibbs, it was known that chemical reactions proceeded to an equilibrium determined by the free energy of the substances taking part. Using this theory, Gilbert Lewis spent 25 years determining free energies of various substances. In 1923, he and Randall published the results of this study and formalizing chemical thermodynamics.\n\nAccording to the Belgian thermodynamicist Ilya Prigogine, their influential 1923 textbook led to the replacement of the term “affinity” by the term “free energy” in much of the English-speaking world.\n\n"}
{"id": "54507368", "url": "https://en.wikipedia.org/wiki?curid=54507368", "title": "Meta-Diethynylbenzene dianion", "text": "Meta-Diethynylbenzene dianion\n\nThe \"meta\"-diethynylbenzene dianion is a structural isomer of the \"ortho\"-diethynylbenzene dianion. The \"meta\"-diethynylbenzene dianion is not as strong as \"ortho\"-diethynylbenzene dianion, but it is still the second-strongest superbase. Like the \"ortho\"-diethynylbenzene dianion, the \"meta\"-diethynylbenzene dianion exists in the gas state, contrary to the other bases and superbases which exist in the solution state. In the \"meta\"-diethynylbenzene dianion, the two negatively charged ethynyl (−) groups are bonded to the first and third carbon atoms of the benzene ring. But in the \"ortho\"-diethynylbenzene dianion, the ethynyl groups are bonded to the first and second carbon atoms of the benzene ring. Like Ortho-diethynylbenzene dianion, Meta-diethynylbenzene dianion has no known use.\n\n\n\n\n\n\nThe same process can generate either \"ortho\"-diethynylbenzene dianion and Meta-diethynylbenzene dianion depending on which isomer of the original compound is used.\n\n\n"}
{"id": "19161122", "url": "https://en.wikipedia.org/wiki?curid=19161122", "title": "Máire Mullarney", "text": "Máire Mullarney\n\nMáire Mullarney (1 September 1921 – 18 August 2008) was an Irish environmentalist, educationalist and Esperanto advocate. She was one of the founding members of the Irish Green Party in 1981. She stood for the party in three Dáil elections in the 1980s and was elected to Dublin County Council in 1991, a position she kept until 1999.\n\nMaire Mullarney also published a book called \"Anything school can do you can do better\" it was about her raising her children as well as home educating.\n\n"}
{"id": "14668793", "url": "https://en.wikipedia.org/wiki?curid=14668793", "title": "Oil megaprojects (2015)", "text": "Oil megaprojects (2015)\n\nThis page summarizes projects that propose to bring more than of new liquid fuel capacity to market with the first production of fuel beginning in 2015. This is part of the Wikipedia summary of Oil Megaprojects.\n\nTerminology\n"}
{"id": "3449036", "url": "https://en.wikipedia.org/wiki?curid=3449036", "title": "Osaka Gas", "text": "Osaka Gas\n\n, commonly written as 大阪ガス, is a Japanese gas company based in Osaka, Japan. It supplies gas to the Kansai region, especially the Keihanshin area.\n\nOsaka Gas is also engaged in upstream, midstream and downstream energy projects throughout the world, including LNG terminals, pipelines and independent power projects, particularly in Southeast Asia, Australia and North America.\n\nOsaka Gas began operations in 1897 in Nishi-ku, Osaka, on a site now occupied by the Dome City Gas Building near the Kyocera Dome. It expanded to Wakayama in 1911. Following the end of World War II, in October 1945, Osaka Gas merged with 14 other gas companies in the Kansai region, expanding its footprint to cover Kobe and Kyoto.\n\nOsaka Gas entered its first overseas upstream LNG project in Brunei in 1972, followed by investments in Indonesia in 1977 and Australia in 1989.\n\n\nOmni Escapes Magazine Escapes Magazine Summer 2018\n\n"}
{"id": "27613780", "url": "https://en.wikipedia.org/wiki?curid=27613780", "title": "Pompey's Pillar (column)", "text": "Pompey's Pillar (column)\n\nPompey's Pillar () is a Roman triumphal column in Alexandria, Egypt, the largest of its type constructed outside the imperial capitals of Rome and Constantinople, located at the Serapeum of Alexandria. The only known free-standing column in Roman Egypt which was not composed of drums, it is one of the largest ancient monoliths and one of the largest monolithic columns ever erected.\n\nThe monolithic column shaft measures 20.46 m in height with a diameter of 2.71 m at its base. The weight of the single piece of red Aswan granite is estimated at 285 tonnes. The column is 26.85 m high including its base and capital. Other authors give slightly deviating dimensions.\n\nErroneously dated to the time of Pompey, the Corinthian column was actually built in 297 AD, commemorating the victory of Roman emperor Diocletian over an Alexandrian revolt.\n\nMuslim traveller Ibn Battuta visited Alexandria in 1326 AD. He describes the pillar and recounts the tale of an archer who shot an arrow tied to a string over the column. This enabled him to pull a rope tied to the string over the pillar and secure it on the other side in order to climb over to the top of the pillar. \n\nIn early 1803, British commander John Shortland of HMS \"Pandour\" flew a kite over Pompey's Pillar. This enabled him to get ropes over it, and then a rope ladder. On February 2, he and John White, \"Pandour\"s Master, climbed it. When they got to the top they displayed the Union Jack, drank a toast to King George III, and gave three cheers. Four days later they climbed the pillar again, erected a staff, fixed a weather vane, ate a beef steak, and again toasted the king.\n\n\n"}
{"id": "41721510", "url": "https://en.wikipedia.org/wiki?curid=41721510", "title": "Pseudocapacitance", "text": "Pseudocapacitance\n\nPseudocapacitance is the electrochemical storage of electricity in an electrochemical capacitor (Pseudocapacitor). This faradaic charge transfer originates by a very fast sequence of reversible faradaic redox, electrosorption or intercalation processes on the surface of suitable electrodes. Pseudocapacitance is accompanied by an electron charge-transfer between electrolyte and electrode coming from a de-solvated and adsorbed ion. One electron per charge unit is involved. The adsorbed ion has no chemical reaction with the atoms of the electrode (no chemical bonds arise) since only a charge-transfer takes place.\n\nFaradaic pseudocapacitance only occurs together with static double-layer capacitance. Pseudocapacitance and double-layer capacitance both contribute inseparable to the total capacitance value.\n\nThe amount of pseudocapacitance depends on the surface area, material and structure of the electrodes. Pseudocapacitance may contribute more capacitance than double-layer capacitance for the same surface area by 100x.\n\nThe amount of electric charge stored in a pseudocapacitance is linearly proportional to the applied voltage. The unit of pseudocapacitance is farad.\n\n\nRedox reactions in batteries with faradaic charge-transfer between an electrolyte and the surface of an electrode were characterized decades ago. These chemical processes are associated with chemical reactions of the electrode materials usually with attendant phase changes. Although these chemical processes are relatively reversible, battery charge/discharge cycles often irreversibly produce unreversed chemical reaction products of the reagents. Accordingly, the cycle-life of rechargeable batteries is usually limited. Further, the reaction products lower power density. Additionally, the chemical processes are relatively slow, extending charge/discharge times.\n\nA fundamental difference between redox reactions in batteries and in electrochemical capacitors (supercapacitors) is that in the latter, the reactions are a very fast sequence of reversible processes with electron transfer without any phase changes of the electrode molecules. They do not involve making or breaking chemical bonds. The de-solvated atoms or ions contributing the pseudocapacitance simply cling to the atomic structure of the electrode and charges are distributed on surfaces by physical adsorption processes. Compared with batteries, supercapacitor faradaic processes are much faster and more stable over time, because they leave only traces of reaction products. Despite the reduced amount of these products, they cause capacitance degradation. This behavior is the essence of pseudocapacitance.\n\nPseudocapacitive processes lead to a charge-dependent, linear capacitive behavior, as well as the accomplishment of non-faradaic double-layer capacitance in contrast to batteries, which have a nearly charge-independent behavior. The amount of pseudocapacitance depends on the surface area, material and structure of the electrodes. The pseudocapacitance may exceed the value of double-layer capacitance for the same surface area by 100x.\n\nApplying a voltage at the capacitor terminals moves the polarized ions or charged atoms in the electrolyte to the opposite polarized electrode. Between the surfaces of the electrodes and the adjacent electrolyte an electric double-layer forms. One layer of ions on the electrode surface and the second layer of adjacent polarized and solvated ions in the electrolyte move to the opposite polarized electrode. The two ion layers are separated by a single layer of electrolyte molecules. Between the two layers, a static electric field forms that results in double-layer capacitance. Accompanied by the electric double-layer, some de-solvated electrolyte ions pervade the separating solvent layer and are adsorbed by the electrode's surface atoms. They are specifically adsorbed and deliver their charge to the electrode. In other words, the ions in the electrolyte within the Helmholtz double-layer also act as electron donors and transfer electrons to the electrode atoms, resulting in a faradaic current. This faradaic charge transfer, originated by a fast sequence of reversible redox reactions, electrosorptions or intercalation processes between electrolyte and the electrode surface is called pseudocapacitance.\n\nDepending on the electrode's structure or surface material, pseudocapacitance can originate when specifically adsorbed ions pervade the double-layer, proceeding in several one-electron stages. The electrons involved in the faradaic processes are transferred to or from the electrode's valence-electron states (orbitals) and flow through the external circuit to the opposite electrode where a second double-layer with an equal number of opposite-charged ions forms. The electrons remain in the strongly ionized and electrode surface's \"electron hungry\" transition-metal ions and are not transferred to the adsorbed ions. This kind of pseudocapacitance has a linear function within narrow limits and is determined by the potential-dependent degree of surface coverage of the adsorbed anions. The storage capacity of the pseudocapacitance is limited by the finite quantity of reagent or of available surface.\n\nSystems that give rise to pseudocapacitance:\n\nAll three types of electrochemical processes have appeared in supercapacitors.\n\nWhen discharging pseudocapacitance, the charge transfer is reversed and the ions or atoms leave the double-layer and spread throughout the electrolyte.\n\nElectrodes' ability to produce pseudocapacitance strongly depends on the electrode materials' chemical affinity to the ions adsorbed on the electrode surface as well as on the electrode pore structure and dimension. Materials exhibiting redox behavior for use as pseudocapacitor electrodes are transition-metal oxides inserted by doping in the conductive electrode material such as active carbon, as well as conducting polymers such as polyaniline or derivatives of polythiophene covering the electrode material.\n\nThese materials provide high pseudocapacitance and were thoroughly studied by Conway. Many oxides of transition metals like ruthenium (), iridium (), iron (), manganese () or sulfides such as titanium sulfide () or their combinations generate faradaic electron–transferring reactions with low conducting resistance.\n\nRuthenium dioxide () in combination with sulfuric acid () electrolyte provides one of the best examples of pseudocapacitance, with a charge/discharge over a window of about 1.2 V per electrode. Furthermore, the reversibility on these transition metal electrodes is excellent, with a cycle life of more than several hundred-thousand cycles. Pseudocapacitance originates from a coupled, reversible redox reaction with several oxidation steps with overlapping potential. The electrons mostly come from the electrode's valence orbitals. The electron transfer reaction is very fast and can be accompanied with high currents.\n\nThe electron transfer reaction takes place according to:\n\nDuring charge and discharge, (protons) are incorporated into or removed from the crystal lattice, which generates storage of electrical energy without chemical transformation. The OH groups are deposited as a molecular layer on the electrode surface and remain in the region of the Helmholtz layer. Since the measurable voltage from the redox reaction is proportional to the charged state, the reaction behaves like a capacitor rather than a battery, whose voltage is largely independent of the state of charge.\n\nAnother type of material with a high amount of pseudocapacitance is electron-conducting polymers. Conductive polymer such as polyaniline, polythiophene, polypyrrole and polyacetylene have a lower reversibility of the redox processes involving faradaic charge transfer than transition metal oxides, and suffer from a limited stability during cycling. Such electrodes employ electrochemical doping or dedoping of the polymers with anions and cations. Highest capacitance and power density are achieved with a n/p-type polymer configuration, with one negatively charged (n-doped) and one positively charged (p-doped) electrode.\n\nPseudocapacitance may originate from the electrode structure, especially from the material pore size. The use of carbide-derived carbons (CDCs) or carbon nanotubes (CNTs) as electrodes provides a network of small pores formed by nanotube entanglement. These nanoporous materials have diameters in the range of <2 nm that can be referred to as intercalated pores. Solvated ions in the electrolyte are unable to enter these small pores, but de-solvated ions that have reduced their ion dimensions are able to enter, resulting in larger ionic packing density and increased charge storage. The tailored sizes of pores in nano-structured carbon electrodes can maximize ion confinement, increasing specific capacitance by faradaic adsorption treatment. Occupation of these pores by de-solvated ions from the electrolyte solution occurs according to (faradaic) intercalation.\n\nPseudocapacitance properties can be expressed in a cyclic voltammogram. For an ideal double-layer capacitor, the current flow is reversed immediately upon reversing the potential yielding a rectangular-shaped voltammogram, with a current independent of the electrode potential. For double-layer capacitors with resistive losses, the shape changes to a parallelogram. In faradaic electrodes the electrical charge stored in the capacitor is strongly dependent on the potential, therefore, the voltammetry characteristics deviate from the parallelogram due to a delay while reversing the potential, ultimately coming from kinetic charging processes.\n\nPseudocapacitance is an important property in supercapacitors.\n\n"}
{"id": "4438779", "url": "https://en.wikipedia.org/wiki?curid=4438779", "title": "Renewable Fuels Association", "text": "Renewable Fuels Association\n\nThe Renewable Fuels Association (RFA) represents the ethanol industry promoting policies, regulations, and research and development initiatives that will lead to the increased production and use of ethanol fuel. First organized in 1981, RFA serves as a voice of advocacy for the ethanol industry, providing research data and industry analysis to its members, to the public via the media, to the United States Congress, as well as to related federal and state agencies.\n\nRFA's chairman is Neil Koehler of Pacific Ethanol, Inc. and the vice-chairman is Jeanne McCaherty of Guardian Energy, LLC. The RFA has offices in both St. Louis, Missouri and Washington, the District of Columbia.\n\nGeoff Cooper is RFA’s President and CEO, a position he’s held since October 2018. Previously he served as RFA Executive Vice President.  In addition to overseeing market analysis and policy research, he provides regulatory support and strategic planning for the association and its members. Geoff also focuses on issues related to lifecycle analysis, sustainability and ethanol co-products. Prior to joining RFA, Geoff served as Director of Ethanol Programs for the National Corn Growers Association. In this role, he led research and promotional efforts to increase the production and use of corn-based ethanol. Previously, Geoff served as a Captain in the U.S. Army, specializing in bulk petroleum supply and logistics. A Wyoming native, Geoff graduated from Drake University in Des Moines, Iowa. He earned his master’s degree at Webster University in St. Louis.\n\nThe RFA argues that the Environmental Protection Agency (EPA) abused its waiver authority by setting RVOs lower than the statutory minimums. They say Congress clearly intended for the law to apply according to supply that could be available rather than demand. They contend that the EPA has conflated the two. Under the Energy Independence and Security Act of 2007 (EISA), the statutory standard for 2017 is 24 billion gallons. The EPA only set an RVO of 18.8 billion gallons of biofuel for 2017. This was up from 18.4 billion gallons in 2016. Ethanol supporters and oil companies alike criticized this target.\n\nThe RFA considers employment in the ethanol industry to be a primary example of \"green jobs.\" The 2010 US Ethanol Industry Salary Survey determined that employees about 500,000 people with 75% of such workers making more than $50,000 per year and 99% receiving health benefits. The RFA points out that these jobs, heavily concentrated in rural areas, provide a much-needed economic boost to otherwise depressed places.\n\n, almost half of new vehicles produced by Chrysler, Ford, and General Motors are flex-fuel, meaning roughly one-quarter of all new vehicles sold by 2015 are capable of using up to E85. However, obstacles to widespread use of E85 fuel remain. A 2014 analysis by the Renewable Fuels Association found that oil companies prevent or discourage affiliated retailers from selling E85 through rigid franchise and branding agreements, restrictive supply contracts, and other tactics. The report showed independent retailers are five times more likely to offer E85 than retailers carrying an oil company brand.\n\nThe RFA convenes four concern-specific committees.\n\n\n"}
{"id": "343225", "url": "https://en.wikipedia.org/wiki?curid=343225", "title": "Sea ice", "text": "Sea ice\n\nSea ice arises as seawater freezes. Because ice is less dense than water, it floats on the ocean's surface (as does fresh water ice, which has an even lower density). Sea ice covers about 7% of the Earth's surface and about 12% of the world's oceans. Much of the world's sea ice is enclosed within the polar ice packs in the Earth's polar regions: the Arctic ice pack of the Arctic Ocean and the Antarctic ice pack of the Southern Ocean. Polar packs undergo a significant yearly cycling in surface extent, a natural process upon which depends the Arctic ecology, including the ocean's ecosystems. Due to the action of winds, currents and temperature fluctuations, sea ice is very dynamic, leading to a wide variety of ice types and features. Sea ice may be contrasted with icebergs, which are chunks of ice shelves or glaciers that calve into the ocean. Depending on location, sea ice expanses may also incorporate icebergs.\n\nSea ice does not simply grow and melt. During its lifespan, it is very dynamic. Due to the combined action of winds, currents, water temperature, and air temperature fluctuations, sea ice expanses typically undergo a significant amount of deformation. Sea ice is classified according to whether or not it is able to drift, and according to its age.\n\nSea ice can be classified according to whether or not it is attached (or frozen) to the shoreline (or between shoals or to grounded icebergs). If attached, it is called landfast ice, or more often, fast ice (from \"fastened\"). Alternatively, and unlike fast ice, drift ice occurs further offshore in very wide areas, and encompasses ice that is free to move with currents and winds. The physical boundary between fast ice and drift ice is the \"fast ice boundary\". The drift ice zone may be further divided into a \"shear zone\", a \"marginal ice zone\" and a \"central pack\". Drift ice consists of \"floes\", individual pieces of sea ice or more across. There are names for various floe sizes: \"small\" – ; \"medium\" – ; \"big\" – ; \"vast\" – ; and \"giant\" – more than . The term \"pack ice\" is used either as a synonym to \"drift ice\", or to designate drift ice zone in which the floes are densely packed. The overall sea ice cover is termed the \"ice canopy\" from the perspective of submarine navigation.\n\nAnother classification used by scientists to describe sea ice is based on age, that is, on its development stages. These stages are: \"new ice\", \"nilas\", \"young ice\", \"first-year\" and \"old\".\n\n\"New ice\" is a general term used for recently frozen sea water that does not yet make up solid ice. It may consist of frazil ice (plates or spicules of ice suspended in water), slush (water saturated snow), or \"shuga\" (spongy white ice lumps a few centimeters across). Other terms, such as grease ice and pancake ice, are used for ice crystal accumulations under the action of wind and waves.\n\n\"Nilas\" designates a sea ice crust up to in thickness. It bends without breaking around waves and swells. Nilas can be further subdivided into \"dark nilas\" – up to in thickness and very dark, and \"light nilas\" – over in thickness and lighter in color.\n\n\"Young ice\" is a transition stage between nilas and first-year ice, and ranges in thickness from to , Young ice can be further subdivided into \"grey ice\" – to in thickness, and \"grey-white ice\" – to in thickness. Young ice is not as flexible as nilas, but tends to break under wave action. In a compression regime, it will either raft (at the grey ice stage) or ridge (at the grey-white ice stage).\n\n\"First-year sea ice\" is ice that is thicker than \"young ice\" but has no more than one year growth. In other words, it is ice that grows in the fall and winter (after it has gone through the \"new ice — nilas — young ice\" stages and grows further) but does not survive the spring and summer months (it melts away). The thickness of this ice typically ranges from to . First-year ice may be further divided into \"thin\" ( to ), \"medium\" ( to ) and \"thick\" (>).\n\n\"Old sea ice\" is sea ice that has survived at least one melting season (\"i.e.\" one summer). For this reason, this ice is generally thicker than first-year sea ice. Old ice is commonly divided into two types: \"second-year ice\", which has survived one melting season, and \"multiyear ice\", which has survived more than one. (In some sources, \"old ice\" is more than 2-years old.) Multi-year ice is much more common in the Arctic than it is in the Antarctic. The reason for this is that sea ice in the south drifts into warmer waters where it melts. In the Arctic, much of the sea ice is land-locked.\n\nWhile fast ice is relatively stable (because it is attached to the shoreline or the seabed), drift (or pack) ice undergoes relatively complex deformation processes that ultimately give rise to sea ice's typically wide variety of landscapes. Wind is thought to be the main driving force along with ocean currents. The Coriolis force and sea ice surface tilt have also been invoked. These driving forces induce a state of stress within the drift ice zone. An ice floe converging toward another and pushing against it will generate a state of \"compression\" at the boundary between both. The ice cover may also undergo a state of \"tension\", resulting in divergence and fissure opening. If two floes drift sideways past each other while remaining in contact, this will create a state of \"shear\".\n\nSea ice deformation results from the interaction between ice floes, as they are driven against each other. The end result may be of three types of features: 1) Rafted ice, when one piece is overriding another; 2) Pressure ridges, a line of broken ice forced downward (to make up the \"keel\") and upward (to make the \"sail\"); and 3) \"Hummock\", an hillock of broken ice that forms an uneven surface. A \"shear ridge\" is a pressure ridge that formed under shear – it tends to be more linear than a ridge induced only by compression. A \"new ridge\" is a recent feature — it is sharp-crested, with its side sloping at an angle exceeding 40 degrees. In contrast, a \"weathered ridge\" is one with a rounded crest and with sides sloping at less than 40 degrees. Stamukhi are yet another type of pile-up but these are grounded and are therefore relatively stationary. They result from the interaction between fast ice and the drifting pack ice.\n\n\"Level ice\" is sea ice that has not been affected by deformation, and is therefore relatively flat.\n\nLeads and polynyas are areas of open water that occur within sea ice expanses even though air temperatures are below freezing, and provide a direct interaction between the ocean and the atmosphere, which is important for the wildlife. Leads are narrow and linear – they vary in width from meter to km scale. During the winter, the water in leads quickly freezes up. They are also used for navigation purposes – even when refrozen, the ice in leads is thinner, allowing icebreakers access to an easier sail path, and submarines to surface more easily. Polynyas are more uniform in size than leads and are also larger – two types are recognized: 1) \"Sensible-heat polynyas\", caused by the upwelling of warmer water and 2) \"Latent-heat polynyas\", resulting from persistent winds from the coastline.\n\nOnly the top layer of water needs to cool to the freezing point. Convection of the surface layer involves the top , down to the pycnocline of increased density.\n\nIn calm water, the first sea ice to form on the surface is a skim of separate crystals which initially are in the form of tiny discs, floating flat on the surface and of diameter less than . Each disc has its c-axis vertical and grows outwards laterally. At a certain point such a disc shape becomes unstable, and the growing isolated crystals take on a hexagonal, stellar form, with long fragile arms stretching out over the surface. These crystals also have their c-axis vertical. The dendritic arms are very fragile, and soon break off, leaving a mixture of discs and arm fragments. With any kind of turbulence in the water, these fragments break up further into random-shaped small crystals which form a suspension of increasing density in the surface water, an ice type called frazil or grease ice. In quiet conditions the frazil crystals soon freeze together to form a continuous thin sheet of young ice; in its early stages, when it is still transparent — that is the ice called \"nilas\". Once nilas has formed, a quite different growth process occurs, in which water freezes on to the bottom of the existing ice sheet, a process called \"congelation\" growth. This growth process yields first-year ice.\n\nIn rough water, fresh sea ice is formed by the cooling of the ocean as heat is lost into the atmosphere. The uppermost layer of the ocean is supercooled to slightly below the freezing point, at which time tiny ice platelets (frazil ice) form. With time, this process leads to a mushy surface layer, known as grease ice. Frazil ice formation may also be started by snowfall, rather than supercooling. Waves and wind then act to compress these ice particles into larger plates, of several meters in diameter, called pancake ice. These float on the ocean surface, and collide with one another, forming upturned edges. In time, the pancake ice plates may themselves be rafted over one another or frozen together into a more solid ice cover, known as consolidated pancake ice. Such ice has a very rough appearance on top and bottom.\n\nIf sufficient snow falls on sea ice to depress the freeboard below sea level, sea water will flow in and a layer of ice will form of mixed snow/sea water. This is particularly common around Antarctica.\n\nRussian scientist Vladimir Vize (1886–1954) devoted his life to study the Arctic ice pack and developed the \"Scientific Prediction of Ice Conditions Theory\", for which he was widely acclaimed in academic circles. He applied this theory in the field in the Kara Sea, which led to the discovery of Vize Island.\n\nThe annual freeze and melt cycle is set by the annual cycle of solar insolation and of ocean and atmospheric temperature, and of variability in this annual cycle.\n\nIn the Arctic, the area of ocean covered by sea ice increases over winter from a minimum in September to a maximum in March or sometimes February, before melting over the summer. In the Antarctic, where the seasons are reversed, the annual minimum is typically in February and the annual maximum in September or October, and the presence of sea ice abutting the calving fronts of ice shelves has been shown to influence glacier flow and potentially the stability of the Antarctic Ice Sheet. \n\nThe growth and melt rate are also affected by the state of the ice itself. During growth, the ice thickening due to freezing (as opposed to dynamics) is itself dependent on the thickness, so that the ice growth slows as the ice thickens. Likewise, during melt, thinner sea ice melts faster. This leads to different behaviour between multiyear and first year ice. In addition, melt ponds on the ice surface during the melt season lower the albedo such that more solar radiation is absorbed, leading to a feedback where melt is accelerated. The presence of melt ponds is affected by the permeability of the sea ice- i.e. whether meltwater can drain- and the topography of the sea ice surface, i.e. the presence of natural basins for the melt ponds to form in. First year ice is flatter than multiyear ice due to the lack of dynamic ridging, so ponds tend to have greater area. They also have lower albedo since they are on thinner ice, which blocks less of the solar radiation from reaching the dark ocean below. \n\nChanges in sea ice conditions are best demonstrated by the rate of melting over time. A composite record of Arctic ice demonstrates that the floes' retreat began around 1900, experiencing more rapid melting beginning within the past 50 years . Satellite study of sea ice began in 1979, and became a much more reliable measure of long-term changes in sea ice. In comparison to the extended record, the sea-ice extent in the polar region by September 2007 was only half the recorded mass that had been estimated to exist within the 1950–1970 period.\n\nArctic sea ice extent ice hit an all-time low in September 2012, when the ice was determined to cover only 24% of the Arctic Ocean, offsetting the previous low of 29% in 2007. Predictions of when the first \"ice free\" Arctic summer might occur vary.\n\nAntarctic sea ice extent gradually increased in the period of satellite observations, which began in 1979, until a rapid decline in southern hemisphere spring of 2016.\n\nSea ice provides an ecosystem for various polar species, particularly the polar bear, whose environment is being threatened as global warming causes the ice to melt more as the Earth's temperature gets warmer. Furthermore, the sea ice itself functions to help keep polar climates cool, since the ice exists in expansive enough amounts to maintain a cold environment. At this, sea ice's relationship with global warming is cyclical; the ice helps to maintain cool climates, but as the global temperature increases, the ice melts, and is less effective in keeping those climates cold. The bright, shiny surface (albedo) of the ice also serves a role in maintaining cooler polar temperatures by reflecting much of the sunlight that hits it back into space. As the sea ice melts, its surface area shrinks, diminishing the size of the reflective surface and therefore causing the earth to absorb more of the sun's heat. As the ice melts it lowers the albedo thus causing more heat to be absorbed by the Earth and further increase the amount of melting ice. Though the size of the ice floes is affected by the seasons, even a small change in global temperature can greatly affect the amount of sea ice, and due to the shrinking reflective surface that keeps the ocean cool, this sparks a cycle of ice shrinking and temperatures warming. As a result, the polar regions are the most susceptible places to climate change on the planet.\n\nFurthermore, sea ice affects the movement of ocean waters. In the freezing process, much of the salt in ocean water is squeezed out of the frozen crystal formations, though some remains frozen in the ice. This salt becomes trapped beneath the sea ice, creating a higher concentration of salt in the water beneath ice floes. This concentration of salt contributes to the salinated water's density, and this cold, denser water sinks to the bottom of the ocean. This cold water moves along the ocean floor towards the equator, while warmer water on the ocean surface moves in the direction of the poles. This is referred to as \"conveyor belt motion\", and is a regularly occurring process.\n\nIn order to gain a better understanding about the variability, numerical sea ice models are used to perform sensitivity studies. The two main ingredients are the ice dynamics and the thermodynamical properties (see Sea ice emissivity modelling, Sea ice growth processes and Sea ice thickness).\n\nMany global climate models (GCMs) have sea ice implemented in their numerical simulation scheme in order to capture the Ice-albedo feedback correctly. Examples include:\n\nThe Coupled model intercomparison project offers a standard protocol for studying the output of coupled atmosphere-ocean general circulation models. The coupling takes place at the atmosphere-ocean interface where the sea ice may occur.\n\nIn addition to global modeling, various regional models deal with sea ice. Regional models are employed for seasonal forecasting experiments and for process studies.\n\nSea ice is part of the Earth's biosphere. When sea water freezes, the ice is riddled with brine-filled channels which sustain sympagic organisms such as bacteria, algae, copepods and annelids, which in turn provide food for animals such as krill and specialised fish like the Bald notothen, fed upon in turn by larger animals such as Emperor penguins and Minke whales.\n\nA decline of seasonal sea ice puts the survival of Arctic species such as ringed seals and polar bears at risk.\n\n\n"}
{"id": "11131252", "url": "https://en.wikipedia.org/wiki?curid=11131252", "title": "Secretariat of Environment and Natural Resources (Mexico)", "text": "Secretariat of Environment and Natural Resources (Mexico)\n\nThe Secretariat of Environment and Natural Resources (in Spanish: Secretaría del Medio Ambiente y Recursos Naturales, SEMARNAT) is Mexico's environment ministry. Its head, the Secretary of the Environment, is a member of the federal executive cabinet and is appointed by the President of the Republic. the appointed Secretary of the Environment in the cabinet of Enrique Peña Nieto is Rafael Pacchiano Alamán who was preceded by Juan José Guerra Abud, who was in the position from 1 December 2012 to 27 August 2015.\n\nThe Secretariat is charged with the mission of protecting, restoring, and conserving the ecosystems, natural resources, assets and environmental services of Mexico with the goal of fostering sustainable development.\n\nThe Secretari of Environment and Natural Resources of México is the Secretary of State to which, according to Law of Federal Public Administration in its Article 32a, corresponds to the release of the following functions:\n\nTo carry out these functions, the Secretariat of Environment and Natural Resources has the following Undersecretaries:\n\nThese Undersecretaries are then charged with the operation of the following units of the SEMARNAT:\n\n\n"}
{"id": "12543398", "url": "https://en.wikipedia.org/wiki?curid=12543398", "title": "South African Solar Challenge", "text": "South African Solar Challenge\n\nThe Sasol Solar Challenge is a South African endurance challenge for solar-powered vehicles, with classes for hybrid vehicles, electric vehicles, and biofuel-powered vehicles as well. Recognised by the International Solarcar Federation, the first challenge was run in 2008, and every two years thereafter. The event covers roughly 2,500 km, but has set loops for teams to repeat, with the potential to do 5,000 km. The current record, held by Dutch team Nuon, is 4,716 km, set in 2016 in their car Nuna. The challenge route may change from year to year, and it is planned to run from Pretoria to Stellenbosch in 2018.\n\nThe primary objective is to design, manage, build and race solar powered-vehicles across South Africa. The challenge sees a collaboration between pupils, students, private individuals and various industry and government partners, to work together to have a safe, technology-rich event. Moreover, the challenge is seen as an educational tool to focus on and communicate about science and technology to the broad public.\n\n\n"}
{"id": "4945362", "url": "https://en.wikipedia.org/wiki?curid=4945362", "title": "Switched capacitor", "text": "Switched capacitor\n\nA switched capacitor is an electronic circuit element used for discrete-time signal processing. It works by moving charges into and out of capacitors when switches are opened and closed. Usually, non-overlapping signals are used to control the switches, so that not all switches are closed simultaneously. Filters implemented with these elements are termed \"switched-capacitor filters\", and depend only on the ratios between capacitances. This makes them much more suitable for use within integrated circuits, where accurately specified resistors and capacitors are not economical to construct.\n\nThe simplest switched-capacitor (SC) circuit is the switched-capacitor resistor, made of one capacitor C and two switches S and S which connect the capacitor with a given frequency alternately to the input and output of the SC. Each switching cycle transfers a charge formula_1 from the input to the output at the switching frequency formula_2. The charge \"q\" on a capacitor \"C\" with a voltage \"V\" between the plates is given by:\n\nwhere \"V\" is the voltage across the capacitor. Therefore, when S is closed while S is open, the charge stored in the capacitor C is:\n\nWhen S is closed, some of that charge is transferred out of the capacitor, after which the charge that remains in capacitor C is:\n\nThus, the charge moved out of the capacitor to the output is:\n\nBecause this charge \"q\" is transferred at a rate \"f\", the rate of transfer of charge per unit time is:\n\nSubstituting for \"q\" in the above, we have:\n\nLet \"V\" be the voltage across the SC from input to output. So:\n\nSo the equivalent resistance \"R\" (i.e., the voltage–current relationship) is:\n\nThus, the SC behaves like a resistor whose value depends on capacitance \"C\" and switching frequency \"f\".\n\nThe SC resistor is used as a replacement for simple resistors in integrated circuits because it is easier to fabricate reliably with a wide range of values. It also has the benefit that its value can be adjusted by changing the switching frequency (i.e., it is a programmable resistance). See also: operational amplifier applications.\n\nThis same circuit can be used in discrete time systems (such as analog to digital converters) as a track and hold circuit. During the appropriate clock phase, the capacitor samples the analog voltage through switch one and in the second phase presents this held sampled value to an electronic circuit for processing.\n\nOften switched-capacitor circuits are used to provide accurate voltage gain and integration by switching a sampled capacitor onto an op-amp with a capacitor formula_11 in feedback. One of the earliest of these circuits is the parasitic-sensitive integrator developed by the Czech engineer Bedrich Hosticka. Here is an analysis. Denote by formula_12 the switching period. In capacitors,\n\nThen, when S1 opens and S2 closes (they are never both closed at the same time), we have the following:\n\n1) Because formula_14 has just charged:\n\n2) Because the feedback cap, formula_11, is suddenly charged with that much charge (by the op amp, which seeks a virtual short circuit between its inputs):\n\nNow dividing 2) by formula_11:\n\nAnd inserting 1):\n\nThis last equation represents what is going on in formula_11 - it increases (or decreases) its voltage each cycle according to the charge that is being \"pumped\" from formula_14 (due to the op-amp).\n\nHowever, there is a more elegant way to formulate this fact if formula_23 is very short. Let us introduce formula_24 and formula_25 and rewrite the last equation divided by dt:\n\nTherefore, the op-amp output voltage takes the form:\n\nThis is an inverting integrator with an \"equivalent resistance\" formula_28. This allows its \"on-line\" or \"runtime\" adjustment (if we manage to make the switches oscillate according to some signal given by e.g. a microcontroller).\n\nThe delaying parasitic insensitive integrator has a wide use in discrete time electronic circuits such as biquad filters, anti-alias structures, and delta-sigma data converters. This circuit implements the following z-domain function:\n\nOne useful characteristic of switched-capacitor circuits is that they can be used to perform many circuit tasks at the same time, which is difficult with non-discrete time components. The multiplying digital to analog converter (MDAC) is an example as it can take an analog input, add a digital value formula_30 to it, and multiply this by some factor based on the capacitor ratios. The output of the MDAC is given by the following:\n\nThe MDAC is a common component in modern pipeline analog to digital converters as well as other precision analog electronics and was first created in the form above by Stephen Lewis and others at Bell Laboratories.\n\nSwitched-capacitor circuits are analysed by writing down charge conservation equations, as in this article, and solving them with a computer algebra tool. For hand analysis and for getting more insight into the circuits, it is also possible to do a Signal-flow graph analysis, with a method that is very similar for switched-capacitor and continuous-time circuits .\n\n\n"}
{"id": "30040", "url": "https://en.wikipedia.org/wiki?curid=30040", "title": "Titanium", "text": "Titanium\n\nTitanium is a chemical element with symbol Ti and atomic number 22. It is a lustrous transition metal with a silver color, low density, and high strength. Titanium is resistant to corrosion in sea water, aqua regia, and chlorine.\n\nTitanium was discovered in Cornwall, Great Britain, by William Gregor in 1791, and was named by Martin Heinrich Klaproth after the Titans of Greek mythology. The element occurs within a number of mineral deposits, principally rutile and ilmenite, which are widely distributed in the Earth's crust and lithosphere, and it is found in almost all living things, water bodies, rocks, and soils. The metal is extracted from its principal mineral ores by the Kroll and Hunter processes. The most common compound, titanium dioxide, is a popular photocatalyst and is used in the manufacture of white pigments. Other compounds include titanium tetrachloride (TiCl), a component of smoke screens and catalysts; and titanium trichloride (TiCl), which is used as a catalyst in the production of polypropylene.\n\nTitanium can be alloyed with iron, aluminium, vanadium, and molybdenum, among other elements, to produce strong, lightweight alloys for aerospace (jet engines, missiles, and spacecraft), military, industrial processes (chemicals and petrochemicals, desalination plants, pulp, and paper), automotive, agri-food, medical prostheses, orthopedic implants, dental and endodontic instruments and files, dental implants, sporting goods, jewelry, mobile phones, and other applications.\n\nThe two most useful properties of the metal are corrosion resistance and strength-to-density ratio, the highest of any metallic element. In its unalloyed condition, titanium is as strong as some steels, but less dense. There are two allotropic forms and five naturally occurring isotopes of this element, Ti through Ti, with Ti being the most abundant (73.8%). Although they have the same number of valence electrons and are in the same group in the periodic table, titanium and zirconium differ in many chemical and physical properties.\n\nAs a metal, titanium is recognized for its high strength-to-weight ratio. It is a strong metal with low density that is quite ductile (especially in an oxygen-free environment), lustrous, and metallic-white in color. The relatively high melting point (more than 1,650 °C or 3,000 °F) makes it useful as a refractory metal. It is paramagnetic and has fairly low electrical and thermal conductivity.\n\nCommercially pure (99.2% pure) grades of titanium have ultimate tensile strength of about 434 MPa (63,000 psi), equal to that of common, low-grade steel alloys, but are less dense. Titanium is 60% denser than aluminium, but more than twice as strong as the most commonly used 6061-T6 aluminium alloy. Certain titanium alloys (e.g., Beta C) achieve tensile strengths of over 1,400 MPa (200,000 psi). However, titanium loses strength when heated above .\n\nTitanium is not as hard as some grades of heat-treated steel; it is non-magnetic and a poor conductor of heat and electricity. Machining requires precautions, because the material can gall unless sharp tools and proper cooling methods are used. Like steel structures, those made from titanium have a fatigue limit that guarantees longevity in some applications.\n\nThe metal is a dimorphic allotrope of an hexagonal α form that changes into a body-centered cubic (lattice) β form at . The specific heat of the α form increases dramatically as it is heated to this transition temperature but then falls and remains fairly constant for the β form regardless of temperature.\n\nLike aluminium and magnesium, titanium metal and its alloys oxidize immediately upon exposure to air. Titanium readily reacts with oxygen at in air, and at in pure oxygen, forming titanium dioxide. It is, however, slow to react with water and air at ambient temperatures because it forms a passive oxide coating that protects the bulk metal from further oxidation. When it first forms, this protective layer is only 1–2 nm thick but continues to grow slowly; reaching a thickness of 25 nm in four years.\n\nAtmospheric passivation gives titanium excellent resistance to corrosion, almost equivalent to platinum. Titanium is capable of withstanding attack by dilute sulfuric and hydrochloric acids, chloride solutions, and most organic acids. However, titanium is corroded by concentrated acids. As indicated by its negative redox potential, titanium is thermodynamically a very reactive metal that burns in normal atmosphere at lower temperatures than the melting point. Melting is possible only in an inert atmosphere or in a vacuum. At , it combines with chlorine. It also reacts with the other halogens and absorbs hydrogen.\n\nTitanium is one of the few elements that burns in pure nitrogen gas, reacting at to form titanium nitride, which causes embrittlement. Because of its high reactivity with oxygen, nitrogen, and some other gases, titanium filaments are applied in titanium sublimation pumps as scavengers for these gases. Such pumps inexpensively and reliably produce extremely low pressures in ultra-high vacuum systems.\n\nTitanium is the ninth-most abundant element in Earth's crust (0.63% by mass) and the seventh-most abundant metal. It is present as oxides in most igneous rocks, in sediments derived from them, in living things, and natural bodies of water. Of the 801 types of igneous rocks analyzed by the United States Geological Survey, 784 contained titanium. Its proportion in soils is approximately 0.5 to 1.5%.\n\nCommon titanium-containing minerals are anatase, brookite, ilmenite, perovskite, rutile, and titanite (sphene). Akaogiite is an extremely rare mineral consisting of titanium dioxide. Of these minerals, only rutile and ilmenite have economic importance, yet even they are difficult to find in high concentrations. About 6.0 and 0.7 million tonnes of those minerals were mined in 2011, respectively. Significant titanium-bearing ilmenite deposits exist in western Australia, Canada, China, India, Mozambique, New Zealand, Norway, Sierra Leone, South Africa, and Ukraine. About 186,000 tonnes of titanium metal sponge were produced in 2011, mostly in China (60,000 t), Japan (56,000 t), Russia (40,000 t), United States (32,000 t) and Kazakhstan (20,700 t). Total reserves of titanium are estimated to exceed 600 million tonnes.\n\nThe concentration of titanium is about 4 picomolar in the ocean. At 100 °C, the concentration of titanium in water is estimated to be less than 10 M at pH 7. The identity of titanium species in aqueous solution remains unknown because of its low solubility and the lack of sensitive spectroscopic methods, although only the 4+ oxidation state is stable in air. No evidence exists for a biological role, although rare organisms are known to accumulate high concentrations of titanium.\n\nTitanium is contained in meteorites, and it has been detected in the Sun and in M-type stars (the coolest type) with a surface temperature of . Rocks brought back from the Moon during the Apollo 17 mission are composed of 12.1% TiO. It is also found in coal ash, plants, and even the human body. Native titanium (pure metallic) is very rare.\n\nNaturally occurring titanium is composed of 5 stable isotopes: Ti, Ti, Ti, Ti, and Ti, with Ti being the most abundant (73.8% natural abundance). Eleven radioisotopes have been characterized, the most stable being Ti with a half-life of 63 years; Ti, 184.8 minutes; Ti, 5.76 minutes; and Ti, 1.7 minutes. All the other radioactive isotopes have half-lives less than 33 seconds and the majority, less than half a second.\n\nThe isotopes of titanium range in atomic weight from 39.99 u (Ti) to 57.966 u (Ti). The primary decay mode before the most abundant stable isotope, Ti, is electron capture and the primary mode after is beta emission. The primary decay products before Ti are element 21 (scandium) isotopes and the primary products after are element 23 (vanadium) isotopes.\n\nTitanium becomes radioactive upon bombardment with deuterons, emitting mainly positrons and hard gamma rays.\n\nThe +4 oxidation state dominates titanium chemistry, but compounds in the +3 oxidation state are also common. Commonly, titanium adopts an octahedral coordination geometry in its complexes, but tetrahedral TiCl is a notable exception. Because of its high oxidation state, titanium(IV) compounds exhibit a high degree of covalent bonding. Unlike most other transition metals, simple aquo Ti(IV) complexes are unknown.\n\nThe most important oxide is TiO, which exists in three important polymorphs; anatase, brookite, and rutile. All of these are white diamagnetic solids, although mineral samples can appear dark (see rutile). They adopt polymeric structures in which Ti is surrounded by six oxide ligands that link to other Ti centers.\n\nThe term \"titanates\" usually refers to titanium(IV) compounds, as represented by barium titanate (BaTiO). With a perovskite structure, this material exhibits piezoelectric properties and is used as a transducer in the interconversion of sound and electricity. Many minerals are titanates, e.g. ilmenite (FeTiO). Star sapphires and rubies get their asterism (star-forming shine) from the presence of titanium dioxide impurities.\n\nA variety of reduced oxides (suboxides) of titanium are known, mainly reduced stoichiometries of titanium dioxide obtained by atmospheric plasma spraying.TiO, described as a Ti(IV)-Ti(III) species, is a purple semiconductor produced by reduction of TiO with hydrogen at high temperatures, and is used industrially when surfaces need to be vapour-coated with titanium dioxide: it evaporates as pure TiO, whereas TiO evaporates as a mixture of oxides and deposits coatings with variable refractive index. Also known is TiO, with the corundum structure, and TiO, with the rock salt structure, although often nonstoichiometric.\n\nThe alkoxides of titanium(IV), prepared by reacting TiCl with alcohols, are colourless compounds that convert to the dioxide on reaction with water. They are industrially useful for depositing solid TiO via the sol-gel process. Titanium isopropoxide is used in the synthesis of chiral organic compounds via the Sharpless epoxidation.\n\nTitanium forms a variety of sulfides, but only TiS has attracted significant interest. It adopts a layered structure and was used as a cathode in the development of lithium batteries. Because Ti(IV) is a \"hard cation\", the sulfides of titanium are unstable and tend to hydrolyze to the oxide with release of hydrogen sulfide.\n\nTitanium nitride (TiN) is a member of a family of refractory transition metal nitrides and exhibits properties similar to both covalent compounds including; thermodynamic stability, extreme hardness, thermal/electrical conductivity, and a high melting point. TiN has a hardness equivalent to sapphire and carborundum (9.0 on the Mohs Scale), and is often used to coat cutting tools, such as drill bits. It is also used as a gold-colored decorative finish and as a barrier metal in semiconductor fabrication. Titanium carbide, which is also very hard, is found in cutting tools and coatings.\n\nTitanium tetrachloride (titanium(IV) chloride, TiCl) is a colorless volatile liquid (commercial samples are yellowish) that, in air, hydrolyzes with spectacular emission of white clouds. Via the Kroll process, TiCl is produced in the conversion of titanium ores to titanium dioxide, e.g., for use in white paint. It is widely used in organic chemistry as a Lewis acid, for example in the Mukaiyama aldol condensation. In the van Arkel process, titanium tetraiodide (TiI) is generated in the production of high purity titanium metal.\n\nTitanium(III) and titanium(II) also form stable chlorides. A notable example is titanium(III) chloride (TiCl), which is used as a catalyst for production of polyolefins (see Ziegler-Natta catalyst) and a reducing agent in organic chemistry.\n\nOwing to the important role of titanium compounds as polymerization catalyst, compounds with Ti-C bonds have been intensively studied. The most common organotitanium complex is titanocene dichloride ((CH)TiCl). Related compounds include Tebbe's reagent and Petasis reagent. Titanium forms carbonyl complexes, e.g. (CH)Ti(CO).\n\nFollowing the success of platinum-based chemotherapy, titanium(IV) complexes were among the first non-platinum compounds to be tested for cancer treatment. The advantage of titanium compounds lies in their high efficacy and low toxicity. In biological environments, hydrolysis leads to the safe and inert titanium dioxide. Despite these advantages the first candidate compounds failed clinical trials. Further development resulted in the creation of potentially effective, selective, and stable titanium-based drugs. Their mode of action is not yet well understood.\n\nTitanium was discovered in 1791 by the clergyman and amateur geologist, William Gregor, as an inclusion of a mineral in Cornwall, Great Britain. Gregor recognized the presence of a new element in ilmenite when he found black sand by a stream and noticed the sand was attracted by a magnet. Analyzing the sand, he determined the presence of two metal oxides: iron oxide (explaining the attraction to the magnet) and 45.25% of a white metallic oxide he could not identify. Realizing that the unidentified oxide contained a metal that did not match any known element, Gregor reported his findings to the Royal Geological Society of Cornwall and in the German science journal \"Crell's Annalen\".\n\nAround the same time, Franz-Joseph Müller von Reichenstein produced a similar substance, but could not identify it. The oxide was independently rediscovered in 1795 by Prussian chemist Martin Heinrich Klaproth in rutile from Boinik (German name Bajmócska), a village in Hungary (now Bojničky in Slovakia). Klaproth found that it contained a new element and named it for the Titans of Greek mythology. After hearing about Gregor's earlier discovery, he obtained a sample of manaccanite and confirmed that it contained titanium.\n\nThe currently known processes for extracting titanium from its various ores are laborious and costly; it is not possible to reduce the ore by heating with carbon (as in iron smelting) because titanium combines with the carbon to produce titanium carbide. Pure metallic titanium (99.9%) was first prepared in 1910 by Matthew A. Hunter at Rensselaer Polytechnic Institute by heating TiCl with sodium at 700–800 °C under great pressure in a batch process known as the Hunter process. Titanium metal was not used outside the laboratory until 1932 when William Justin Kroll proved that it can be produced by reducing titanium tetrachloride (TiCl) with calcium. Eight years later he refined this process with magnesium and even sodium in what became known as the Kroll process. Although research continues into more efficient and cheaper processes (e.g., FFC Cambridge, Armstrong), the Kroll process is still used for commercial production.\n\nTitanium of very high purity was made in small quantities when Anton Eduard van Arkel and Jan Hendrik de Boer discovered the iodide, or crystal bar, process in 1925, by reacting with iodine and decomposing the formed vapours over a hot filament to pure metal.\n\nIn the 1950s and 1960s, the Soviet Union pioneered the use of titanium in military and submarine applications (Alfa class and Mike class) as part of programs related to the Cold War. Starting in the early 1950s, titanium came into use extensively in military aviation, particularly in high-performance jets, starting with aircraft such as the F-100 Super Sabre and Lockheed A-12 and SR-71.\n\nRecognizing the strategic importance of titanium, the U.S. Department of Defense supported early efforts of commercialization.\n\nThroughout the period of the Cold War, titanium was considered a strategic material by the U.S. government, and a large stockpile of titanium sponge was maintained by the Defense National Stockpile Center, which was finally depleted in the 2000s. According to 2006 data, the world's largest producer, Russian-based VSMPO-AVISMA, was estimated to account for about 29% of the world market share. As of 2015, titanium sponge metal was produced in six countries: China, Japan, Russia, Kazakhstan, the US, Ukraine, and India. (in order of output).\n\nIn 2006, the U.S. Defense Advanced Research Projects Agency (DARPA) awarded $5.7 million to a two-company consortium to develop a new process for making titanium metal powder. Under heat and pressure, the powder can be used to create strong, lightweight items ranging from armour plating to components for the aerospace, transport, and chemical processing industries.\n\nThe processing of titanium metal occurs in four major steps: reduction of titanium ore into \"sponge\", a porous form; melting of sponge, or sponge plus a master alloy to form an ingot; primary fabrication, where an ingot is converted into general mill products such as billet, bar, plate, sheet, strip, and tube; and secondary fabrication of finished shapes from mill products.\nBecause it cannot be readily produced by reduction of its dioxide, titanium metal is obtained by reduction of TiCl with magnesium metal in the Kroll process. The complexity of this batch production in the Kroll process explains the relatively high market value of titanium, despite the Kroll process being less expensive than the Hunter process. To produce the TiCl required by the Kroll process, the dioxide is subjected to carbothermic reduction in the presence of chlorine. In this process, the chlorine gas is passed over a red-hot mixture of rutile or ilmenite in the presence of carbon. After extensive purification by fractional distillation, the TiCl is reduced with 800 °C molten magnesium in an argon atmosphere. Titanium metal can be further purified by the van Arkel–de Boer process, which involves thermal decomposition of titanium tetraiodide.\nA more recently developed batch production method, the FFC Cambridge process, consumes titanium dioxide powder (a refined form of rutile) as feedstock and produces titanium metal, either powder or sponge. The process involves fewer steps than the Kroll process and takes less time. If mixed oxide powders are used, the product is an alloy.\n\nCommon titanium alloys are made by reduction. For example, cuprotitanium (rutile with copper added is reduced), ferrocarbon titanium (ilmenite reduced with coke in an electric furnace), and manganotitanium (rutile with manganese or manganese oxides) are reduced.\n\nAbout fifty grades of titanium and titanium alloys are designed and currently used, although only a couple of dozen are readily available commercially. The ASTM International recognizes 31 grades of titanium metal and alloys, of which grades one through four are commercially pure (unalloyed). Those four vary in tensile strength as a function of oxygen content, with grade 1 being the most ductile (lowest tensile strength with an oxygen content of 0.18%), and grade 4 the least ductile (highest tensile strength with an oxygen content of 0.40%). The remaining grades are alloys, each designed for specific properties of ductility, strength, hardness, electrical resistivity, creep resistance, specific corrosion resistance, and combinations thereof.\n\nIn addition to the ASTM specifications, titanium alloys are also produced to meet aerospace and military specifications (SAE-AMS, MIL-T), ISO standards, and country-specific specifications, as well as proprietary end-user specifications for aerospace, military, medical, and industrial applications.\n\nTitanium powder is manufactured using a flow production process known as the Armstrong process that is similar to the batch production Hunter process. A stream of titanium tetrachloride gas is added to a stream of molten sodium metal; the products (sodium chloride salt and titanium particles) is filtered from the extra sodium. Titanium is then separated from the salt by water washing. Both sodium and chlorine are recycled to produce and process more titanium tetrachloride.\n\nAll welding of titanium must be done in an inert atmosphere of argon or helium to shield it from contamination with atmospheric gases (oxygen, nitrogen, and hydrogen). Contamination causes a variety of conditions, such as embrittlement, which reduce the integrity of the assembly welds and lead to joint failure.\n\nCommercially pure flat product (sheet, plate) can be formed readily, but processing must take into account the fact that the metal has a \"memory\" and tends to spring back. This is especially true of certain high-strength alloys. Titanium cannot be soldered without first pre-plating it in a metal that is solderable. The metal can be machined with the same equipment and the same processes as stainless steel.\n\nTitanium is used in steel as an alloying element (ferro-titanium) to reduce grain size and as a deoxidizer, and in stainless steel to reduce carbon content. Titanium is often alloyed with aluminium (to refine grain size), vanadium, copper (to harden), iron, manganese, molybdenum, and other metals. Titanium mill products (sheet, plate, bar, wire, forgings, castings) find application in industrial, aerospace, recreational, and emerging markets. Powdered titanium is used in pyrotechnics as a source of bright-burning particles.\n\nAbout 95% of all titanium ore is destined for refinement into titanium dioxide (), an intensely white permanent pigment used in paints, paper, toothpaste, and plastics. It is also used in cement, in gemstones, as an optical opacifier in paper, and a strengthening agent in graphite composite fishing rods and golf clubs.\n\nBecause titanium alloys have high tensile strength to density ratio, high corrosion resistance, fatigue resistance, high crack resistance, and ability to withstand moderately high temperatures without creeping, they are used in aircraft, armour plating, naval ships, spacecraft, and missiles. For these applications, titanium is alloyed with aluminium, zirconium, nickel, vanadium, and other elements to manufacture a variety of components including critical structural parts, fire walls, landing gear, exhaust ducts (helicopters), and hydraulic systems. In fact, about two thirds of all titanium metal produced is used in aircraft engines and frames. The titanium 6AL-4V alloy accounts for almost 50% of all alloys used in aircraft applications.\n\nThe Lockheed A-12 and its development the SR-71 \"Blackbird\" were two of the first aircraft frames where titanium was used, paving the way for much wider use in modern military and commercial aircraft. An estimated 59 metric tons (130,000 pounds) are used in the Boeing 777, 45 in the Boeing 747, 18 in the Boeing 737, 32 in the Airbus A340, 18 in the Airbus A330, and 12 in the Airbus A320. The Airbus A380 may use 77 metric tons, including about 11 tons in the engines. In aero engine applications, titanium is used for rotors, compressor blades, hydraulic system components, and nacelles. An early use in jet engines was for the Orenda Iroquois in the 1950s.\n\nBecause titanium is resistant to corrosion by sea water, it is used to make propeller shafts, rigging, and heat exchangers in desalination plants; heater-chillers for salt water aquariums, fishing line and leader, and divers' knives. Titanium is used in the housings and components of ocean-deployed surveillance and monitoring devices for science and the military. The former Soviet Union developed techniques for making submarines with hulls of titanium alloys forging titanium in huge vacuum tubes.\n\nTitanium is used in the walls of the Juno spacecraft's vault to shield on-board electronics.\n\nWelded titanium pipe and process equipment (heat exchangers, tanks, process vessels, valves) are used in the chemical and petrochemical industries primarily for corrosion resistance. Specific alloys are used in oil and gas downhole applications and nickel hydrometallurgy for their high strength (e. g.: titanium beta C alloy), corrosion resistance, or both. The pulp and paper industry uses titanium in process equipment exposed to corrosive media, such as sodium hypochlorite or wet chlorine gas (in the bleachery). Other applications include ultrasonic welding, wave soldering, and sputtering targets.\n\nTitanium tetrachloride (TiCl), a colorless liquid, is important as an intermediate in the process of making TiO and is also used to produce the Ziegler–Natta catalyst. Titanium tetrachloride is also used to iridize glass and, because it fumes strongly in moist air, it is used to make smoke screens.\n\nTitanium metal is used in automotive applications, particularly in automobile and motorcycle racing where low weight and high strength and rigidity are critical. The metal is generally too expensive for the general consumer market, though some late model Corvettes have been manufactured with titanium exhausts, and a Corvette Z06's LT4 supercharged engine uses lightweight, solid titanium intake valves for greater strength and resistance to heat.\n\nTitanium is used in many sporting goods: tennis rackets, golf clubs, lacrosse stick shafts; cricket, hockey, lacrosse, and football helmet grills, and bicycle frames and components. Although not a mainstream material for bicycle production, titanium bikes have been used by racing teams and adventure cyclists.\n\nTitanium alloys are used in spectacle frames that are rather expensive but highly durable, long lasting, light weight, and cause no skin allergies. Many backpackers use titanium equipment, including cookware, eating utensils, lanterns, and tent stakes. Though slightly more expensive than traditional steel or aluminium alternatives, titanium products can be significantly lighter without compromising strength. Titanium horseshoes are preferred to steel by farriers because they are lighter and more durable.\nTitanium has occasionally been used in architecture. The Monument to Yuri Gagarin, the first man to travel in space (), as well as the Monument to the Conquerors of Space on top of the Cosmonaut Museum in Moscow are made of titanium for the metal's attractive colour and association with rocketry. The Guggenheim Museum Bilbao and the Cerritos Millennium Library were the first buildings in Europe and North America, respectively, to be sheathed in titanium panels. Titanium sheathing was used in the Frederic C. Hamilton Building in Denver, Colorado.\n\nBecause of titanium's superior strength and light weight relative to other metals (steel, stainless steel, and aluminium), and because of recent advances in metalworking techniques, its use has become more widespread in the manufacture of firearms. Primary uses include pistol frames and revolver cylinders. For the same reasons, it is used in the body of laptop computers (for example, in Apple's PowerBook line).\n\nSome upmarket lightweight and corrosion-resistant tools, such as shovels and flashlights, are made of titanium or titanium alloys.\n\nBecause of its durability, titanium has become more popular for designer jewelry (particularly, titanium rings). Its inertness makes it a good choice for those with allergies or those who will be wearing the jewelry in environments such as swimming pools. Titanium is also alloyed with gold to produce an alloy that can be marketed as 24-carat gold because the 1% of alloyed Ti is insufficient to require a lesser mark. The resulting alloy is roughly the hardness of 14-carat gold and is more durable than pure 24-carat gold.\n\nTitanium's durability, light weight, and dent and corrosion resistance make it useful for watch cases. Some artists work with titanium to produce sculptures, decorative objects and furniture.\n\nTitanium may be anodized to vary the thickness of the surface oxide layer, causing optical interference fringes and a variety of bright colors. With this coloration and chemical inertness, titanium is a popular metal for body piercing.\n\nTitanium has a minor use in dedicated non-circulating coins and medals. In 1999, Gibraltar released world's first titanium coin for the millennium celebration. The Gold Coast Titans, an Australian rugby league team, award a medal of pure titanium to their player of the year.\n\nBecause titanium is biocompatible (non-toxic and not rejected by the body), it has many medical uses, including surgical implements and implants, such as hip balls and sockets (joint replacement) and dental implants that can stay in place for up to 20 years. The titanium is often alloyed with about 4% aluminium or 6% Al and 4% vanadium.\n\nTitanium has the inherent ability to osseointegrate, enabling use in dental implants that can last for over 30 years. This property is also useful for orthopedic implant applications. These benefit from titanium's lower modulus of elasticity (Young's modulus) to more closely match that of the bone that such devices are intended to repair. As a result, skeletal loads are more evenly shared between bone and implant, leading to a lower incidence of bone degradation due to stress shielding and periprosthetic bone fractures, which occur at the boundaries of orthopedic implants. However, titanium alloys' stiffness is still more than twice that of bone, so adjacent bone bears a greatly reduced load and may deteriorate.\n\nBecause titanium is non-ferromagnetic, patients with titanium implants can be safely examined with magnetic resonance imaging (convenient for long-term implants). Preparing titanium for implantation in the body involves subjecting it to a high-temperature plasma arc which removes the surface atoms, exposing fresh titanium that is instantly oxidized.\n\nTitanium is used for the surgical instruments used in image-guided surgery, as well as wheelchairs, crutches, and any other products where high strength and low weight are desirable.\n\nTitanium dioxide nanoparticles are widely used in electronics and the delivery of pharmaceuticals and cosmetics.\n\nBecause of it is corrosion resistance, containers made of titanium have been studied for the long-term storage of nuclear waste. Containers lasting more than 100,000 years are thought possible with manufacturing conditions that minimize material defects. A titanium \"drip shield\" could also be installed over containers of other types to enhance their longevity.\n\nThe fungal species \"Marasmius oreades\" and \"Hypholoma capnoides\" can bioconvert titanium in titanium polluted soils.\n\nTitanium is non-toxic even in large doses and does not play any natural role inside the human body. An estimated quantity of 0.8 milligrams of titanium is ingested by humans each day, but most passes through without being absorbed in the tissues. It does, however, sometimes bio-accumulate in tissues that contain silica. One study indicates a possible connection between titanium and yellow nail syndrome. An unknown mechanism in plants may use titanium to stimulate the production of carbohydrates and encourage growth. This may explain why most plants contain about 1 part per million (ppm) of titanium, food plants have about 2 ppm, and horsetail and nettle contain up to 80 ppm.\n\nAs a powder or in the form of metal shavings, titanium metal poses a significant fire hazard and, when heated in air, an explosion hazard. Water and carbon dioxide are ineffective for extinguishing a titanium fire; Class D dry powder agents must be used instead.\n\nWhen used in the production or handling of chlorine, titanium should not be exposed to dry chlorine gas because it may result in a titanium–chlorine fire. Even wet chlorine presents a fire hazard when extreme weather conditions cause unexpected drying.\n\nTitanium can catch fire when a fresh, non-oxidized surface comes in contact with liquid oxygen. Fresh metal may be exposed when the oxidized surface is struck or scratched with a hard object, or when mechanical strain causes a crack. This poses a limitation to its use in liquid oxygen systems, such as those in the aerospace industry. Because titanium tubing impurities can cause fires when exposed to oxygen, titanium is prohibited in gaseous oxygen respiration systems. Steel tubing is used for high pressure systems (3,000 p.s.i.) and aluminium tubing for low pressure systems.\n\n\n"}
{"id": "30906", "url": "https://en.wikipedia.org/wiki?curid=30906", "title": "Transformer", "text": "Transformer\n\nA transformer is a static electrical device that transfers electrical energy between two or more circuits. A varying current in one coil of the transformer produces a varying magnetic flux, which, in turn, induces a varying electromotive force (emf) or \"voltage\" across a second coil wound around the same core.\nElectric power can be transferred between the two coils, without a metallic connection between the two circuits. Faraday's law of induction discovered in 1831 described the induced voltage effect in any secondary coil due to changing magnetic flux cutting it.\n\nTransformers are used to increase or decrease the alternating voltages in electric power applications.\n\nSince the invention of the first constant-potential transformer in 1885, transformers have become essential for the transmission, distribution, and utilization of alternating current electrical energy. A wide range of transformer designs is encountered in electronic and electric power applications. Transformers range in size from RF transformers less than a cubic centimeter in volume to units interconnecting the power grid weighing hundreds of tons.\nAn ideal transformer is a theoretical, linear transformer that is lossless and perfectly coupled. Perfect coupling implies infinitely high core magnetic permeability and winding inductances and zero net magnetomotive force.\n\nA varying current in the transformer's primary winding attempts to create a varying magnetic flux in the transformer core and therefore a varying magnetic flux cutting the secondary winding. This varying flux at the secondary winding induces a varying electromotive force (EMF) or voltage in the secondary winding due to electromagnetic induction and the secondary current so produced creates a flux equal and opposite to that produced by the primary winding, in accordance with Lenz's law. The result is that there is zero net flux in the core.\nThe primary and secondary windings are wrapped around a core of infinitely high magnetic permeability so that all of the magnetic flux passes through both the primary and secondary windings. With a voltage source connected to the primary winding and load impedance connected to the secondary winding, the transformer currents flow in the indicated directions and the core flux cancels to zero. (See also Polarity.)\n\nAccording to Faraday's law, since the same magnetic flux passes through both the primary and secondary windings in an ideal transformer, a voltage is induced in each winding proportional to its number of windings. Thus, referring to the equations shown in the sidebox at right, according to Faraday's law, we have primary and secondary winding voltages defined by eq. 1 & eq. 2, respectively. The primary EMF is sometimes termed counter EMF. This is in accordance with Lenz's law, which states that induction of EMF always opposes development of any such change in magnetic field.\n\nThe transformer winding voltage ratio is thus shown to be directly proportional to the winding turns ratio according to eq. 3. However, some sources use the inverse definition.\n\nAccording to the law of conservation of energy, any load impedance connected to the ideal transformer's secondary winding results in conservation of apparent, real and reactive power consistent with eq. 4.\n\nThe ideal transformer identity shown in eq. 5 is a reasonable approximation for the typical commercial transformer, with voltage ratio and winding turns ratio both being inversely proportional to the corresponding current ratio.\n\nBy Ohm's law and the ideal transformer identity:\n\nThe ideal transformer model neglects the following basic linear aspects in real transformers:\n\n(a) Core losses, collectively called magnetizing current losses, consisting of\n\n(b) Unlike the ideal model, the windings in a real transformer have non-zero resistances and inductances associated with:\n\n(c) similar to an inductor, parasitic capacitance and self-resonance phenomenon due to the electric field distribution. Three kinds of parasitic capacitance are usually considered and the closed-loop equations are provided \n\nThe transformer model with capacitance is quite complicated, and is rarely attempted; even the does not include the parasitic capacitance. However, the capacitance can be measured by comparing open-circuit inductance to a short-circuit inductance..\nThe ideal transformer model assumes that all flux generated by the primary winding links all the turns of every winding, including itself. In practice, some flux traverses paths that take it outside the windings. Such flux is termed \"leakage flux\", and results in leakage inductance in series with the mutually coupled transformer windings. Leakage flux results in energy being alternately stored in and discharged from the magnetic fields with each cycle of the power supply. It is not directly a power loss, but results in inferior voltage regulation, causing the secondary voltage not to be directly proportional to the primary voltage, particularly under heavy load. Transformers are therefore normally designed to have very low leakage inductance.\n\nIn some applications increased leakage is desired, and long magnetic paths, air gaps, or magnetic bypass shunts may deliberately be introduced in a transformer design to limit the short-circuit current it will supply. Leaky transformers may be used to supply loads that exhibit negative resistance, such as electric arcs, mercury- and sodium- vapor lamps and neon signs or for safely handling loads that become periodically short-circuited such as electric arc welders.\n\nAir gaps are also used to keep a transformer from saturating, especially audio-frequency transformers in circuits that have a DC component flowing in the windings. A saturable reactor exploits saturation of the core to control alternating current.\n\nKnowledge of leakage inductance is also useful when transformers are operated in parallel. It can be shown that if the percent impedance and associated winding leakage reactance-to-resistance (\"X\"/\"R\") ratio of two transformers were hypothetically exactly the same, the transformers would share power in proportion to their respective volt-ampere ratings (e.g. 500 kVA unit in parallel with 1,000 kVA unit, the larger unit would carry twice the current). However, the impedance tolerances of commercial transformers are significant. Also, the Z impedance and X/R ratio of different capacity transformers tends to vary, corresponding 1,000 kVA and 500 kVA units' values being, to illustrate, respectively, \"Z\" ≈ 5.75%, \"X\"/\"R\" ≈ 3.75 and \"Z\" ≈ 5%, \"X\"/\"R\" ≈ 4.75.\nReferring to the diagram, a practical transformer's physical behavior may be represented by an equivalent circuit model, which can incorporate an ideal transformer.\n\nWinding joule losses and leakage reactances are represented by the following series loop impedances of the model:\nIn normal course of circuit equivalence transformation, \"R\" and \"X\" are in practice usually referred to the primary side by multiplying these impedances by the turns ratio squared, (\"N\"/\"N\") = a.\nCore loss and reactance is represented by the following shunt leg impedances of the model:\n\"R\" and \"X\" are collectively termed the \"magnetizing branch\" of the model.\n\nCore losses are caused mostly by hysteresis and eddy current effects in the core and are proportional to the square of the core flux for operation at a given frequency. The finite permeability core requires a magnetizing current \"I\" to maintain mutual flux in the core. Magnetizing current is in phase with the flux, the relationship between the two being non-linear due to saturation effects. However, all impedances of the equivalent circuit shown are by definition linear and such non-linearity effects are not typically reflected in transformer equivalent circuits. With sinusoidal supply, core flux lags the induced EMF by 90°. With open-circuited secondary winding, magnetizing branch current \"I\" equals transformer no-load current.\n\nThe resulting model, though sometimes termed 'exact' equivalent circuit based on linearity assumptions, retains a number of approximations. Analysis may be simplified by assuming that magnetizing branch impedance is relatively high and relocating the branch to the left of the primary impedances. This introduces error but allows combination of primary and referred secondary resistances and reactances by simple summation as two series impedances.\n\nTransformer equivalent circuit impedance and transformer ratio parameters can be derived from the following tests: open-circuit test, short-circuit test, winding resistance test, and transformer ratio test.\n\nIf the flux in the core is purely sinusoidal, the relationship for either winding between its rms voltage \"E\" of the winding, and the supply frequency \"f\", number of turns \"N\", core cross-sectional area \"a\" in m and peak magnetic flux density \"B\" in Wb/m or T (tesla) is given by the universal EMF equation:\n\nformula_1\n\nIf the flux does not contain even harmonics the following equation can be used for half-cycle average voltage \"E\" of any waveshape:\n\nformula_2\n\nA dot convention is often used in transformer circuit diagrams, nameplates or terminal markings to define the relative polarity of transformer windings. Positively increasing instantaneous current entering the primary winding's ‘dot’ end induces positive polarity voltage exiting the secondary winding's ‘dot’ end.\n\nThree-phase transformers used in electric power systems will have a nameplate that indicate the phase relationships between their terminals. This may be in the form of a phasor diagram, or using an alpha-numeric code to show the type of internal connection (wye or delta) for each winding.\n\nThe EMF of a transformer at a given flux increases with frequency. By operating at higher frequencies, transformers can be physically more compact because a given core is able to transfer more power without reaching saturation and fewer turns are needed to achieve the same impedance. However, properties such as core loss and conductor skin effect also increase with frequency. Aircraft and military equipment employ 400 Hz power supplies which reduce core and winding weight. Conversely, frequencies used for some railway electrification systems were much lower (e.g. 16.7 Hz and 25 Hz) than normal utility frequencies (50–60 Hz) for historical reasons concerned mainly with the limitations of early electric traction motors. Consequently, the transformers used to step-down the high overhead line voltages (e.g. 15 kV) were much larger and heavier for the same power rating than those required for the higher frequencies.\nOperation of a transformer at its designed voltage but at a higher frequency than intended will lead to reduced magnetizing current. At a lower frequency, the magnetizing current will increase. Operation of a large transformer at other than its design frequency may require assessment of voltages, losses, and cooling to establish if safe operation is practical. For example, transformers may need to be equipped with 'volts per hertz' over-excitation, ANSI function 24, relays to protect the transformer from overvoltage at higher than rated frequency.\n\nOne example is in traction transformers used for electric multiple unit and high-speed train service operating across regions with different electrical standards. The converter equipment and traction transformers have to accommodate different input frequencies and voltage (ranging from as high as 50 Hz down to 16.7 Hz and rated up to 25 kV) while being suitable for multiple AC asynchronous motor and DC converters and motors with varying harmonics mitigation filtering requirements.\n\nAt much higher frequencies the transformer core size required drops dramatically: a physically small and cheap transformer can handle power levels that would require a massive iron core at mains frequency. The development of switching power semiconductor devices and complex integrated circuits made switch-mode power supplies viable, to generate a high frequency from a much lower one (or DC), change the voltage level with a small transformer, and, if necessary, rectify the changed voltage.\n\nLarge power transformers are vulnerable to insulation failure due to transient voltages with high-frequency components, such as caused in switching or by lightning.\n\nTransformer energy losses are dominated by winding and core losses. Transformers' efficiency tends to improve with increasing transformer capacity. The efficiency of typical distribution transformers is between about 98 and 99 percent.\n\nAs transformer losses vary with load, it is often useful to tabulate no-load loss, full-load loss, half-load loss, and so on. Hysteresis and eddy current losses are constant at all load levels and dominate overwhelmingly without load, while variable winding joule losses dominating increasingly as load increases. The no-load loss can be significant, so that even an idle transformer constitutes a drain on the electrical supply. Designing energy efficient transformers for lower loss requires a larger core, good-quality silicon steel, or even amorphous steel for the core and thicker wire, increasing initial cost. The choice of construction represents a trade-off between initial cost and operating cost.\n\nTransformer losses arise from:\nThere are also radiative losses due to the oscillating magnetic field but these are usually small.\n\nClosed-core transformers are constructed in 'core form' or 'shell form'. When windings surround the core, the transformer is core form; when windings are surrounded by the core, the transformer is shell form. Shell form design may be more prevalent than core form design for distribution transformer applications due to the relative ease in stacking the core around winding coils. Core form design tends to, as a general rule, be more economical, and therefore more prevalent, than shell form design for high voltage power transformer applications at the lower end of their voltage and power rating ranges (less than or equal to, nominally, 230 kV or 75 MVA). At higher voltage and power ratings, shell form transformers tend to be more prevalent. Shell form design tends to be preferred for extra-high voltage and higher MVA applications because, though more labor-intensive to manufacture, shell form transformers are characterized as having inherently better kVA-to-weight ratio, better short-circuit strength characteristics and higher immunity to transit damage.\n\nTransformers for use at power or audio frequencies typically have cores made of high permeability silicon steel. The steel has a permeability many times that of free space and the core thus serves to greatly reduce the magnetizing current and confine the flux to a path which closely couples the windings. Early transformer developers soon realized that cores constructed from solid iron resulted in prohibitive eddy current losses, and their designs mitigated this effect with cores consisting of bundles of insulated iron wires. Later designs constructed the core by stacking layers of thin steel laminations, a principle that has remained in use. Each lamination is insulated from its neighbors by a thin non-conducting layer of insulation. The transformer universal EMF equation implies an acceptably large core cross-sectional area to avoid saturation.\n\nThe effect of laminations is to confine eddy currents to highly elliptical paths that enclose little flux, and so reduce their magnitude. Thinner laminations reduce losses, but are more laborious and expensive to construct. Thin laminations are generally used on high-frequency transformers, with some of very thin steel laminations able to operate up to 10 kHz.\n\nOne common design of laminated core is made from interleaved stacks of E-shaped steel sheets capped with I-shaped pieces, leading to its name of 'E-I transformer'. Such a design tends to exhibit more losses, but is very economical to manufacture. The cut-core or C-core type is made by winding a steel strip around a rectangular form and then bonding the layers together. It is then cut in two, forming two C shapes, and the core assembled by binding the two C halves together with a steel strap. They have the advantage that the flux is always oriented parallel to the metal grains, reducing reluctance.\n\nA steel core's remanence means that it retains a static magnetic field when power is removed. When power is then reapplied, the residual field will cause a high inrush current until the effect of the remaining magnetism is reduced, usually after a few cycles of the applied AC waveform. Overcurrent protection devices such as fuses must be selected to allow this harmless inrush to pass. On transformers connected to long, overhead power transmission lines, induced currents due to geomagnetic disturbances during solar storms can cause saturation of the core and operation of transformer protection devices.\n\nDistribution transformers can achieve low no-load losses by using cores made with low-loss high-permeability silicon steel or amorphous (non-crystalline) metal alloy. The higher initial cost of the core material is offset over the life of the transformer by its lower losses at light load.\n\nPowdered iron cores are used in circuits such as switch-mode power supplies that operate above mains frequencies and up to a few tens of kilohertz. These materials combine high magnetic permeability with high bulk electrical resistivity. For frequencies extending beyond the VHF band, cores made from non-conductive magnetic ceramic materials called ferrites are common. Some radio-frequency transformers also have movable cores (sometimes called 'slugs') which allow adjustment of the coupling coefficient (and bandwidth) of tuned radio-frequency circuits.\n\nToroidal transformers are built around a ring-shaped core, which, depending on operating frequency, is made from a long strip of silicon steel or permalloy wound into a coil, powdered iron, or ferrite. A strip construction ensures that the grain boundaries are optimally aligned, improving the transformer's efficiency by reducing the core's reluctance. The closed ring shape eliminates air gaps inherent in the construction of an E-I core. The cross-section of the ring is usually square or rectangular, but more expensive cores with circular cross-sections are also available. The primary and secondary coils are often wound concentrically to cover the entire surface of the core. This minimizes the length of wire needed and provides screening to minimize the core's magnetic field from generating electromagnetic interference.\n\nToroidal transformers are more efficient than the cheaper laminated E-I types for a similar power level. Other advantages compared to E-I types, include smaller size (about half), lower weight (about half), less mechanical hum (making them superior in audio amplifiers), lower exterior magnetic field (about one tenth), low off-load losses (making them more efficient in standby circuits), single-bolt mounting, and greater choice of shapes. The main disadvantages are higher cost and limited power capacity (see Classification parameters below). Because of the lack of a residual gap in the magnetic path, toroidal transformers also tend to exhibit higher inrush current, compared to laminated E-I types.\n\nFerrite toroidal cores are used at higher frequencies, typically between a few tens of kilohertz to hundreds of megahertz, to reduce losses, physical size, and weight of inductive components. A drawback of toroidal transformer construction is the higher labor cost of winding. This is because it is necessary to pass the entire length of a coil winding through the core aperture each time a single turn is added to the coil. As a consequence, toroidal transformers rated more than a few kVA are uncommon. Relatively few toroids are offered with power ratings above 10 kVA, and practically none above 25 kVA. Small distribution transformers may achieve some of the benefits of a toroidal core by splitting it and forcing it open, then inserting a bobbin containing primary and secondary windings.\n\nA physical core is not an absolute requisite and a functioning transformer can be produced simply by placing the windings near each other, an arrangement termed an \"air-core\" transformer. The air which comprises the magnetic circuit is essentially lossless, and so an air-core transformer eliminates loss due to hysteresis in the core material. The magnetizing inductance is drastically reduced by the lack of a magnetic core, resulting in large magnetizing currents and losses if used at low frequencies. A large number of turns can be used to increase magnetizing inductance, but doing so increases winding resistance and leakage inductance. Air-core transformers are unsuitable for use in power distribution. They have however very high frequency capability, and are frequently employed in radio-frequency applications, for which a satisfactory coupling coefficient is maintained by carefully overlapping the primary and secondary windings. Air cores are also used for resonant transformers such as Tesla coils, where they can achieve reasonably low loss despite the low magnetizing inductance.\n\nHigh-frequency transformers operating in the tens to hundreds of kilohertz often have windings made of braided Litz wire to minimize the skin-effect and proximity effect losses. Large power transformers use multiple-stranded conductors as well, since even at low power frequencies non-uniform distribution of current would otherwise exist in high-current windings. Each strand is individually insulated, and the strands are arranged so that at certain points in the winding, or throughout the whole winding, each portion occupies different relative positions in the complete conductor. The transposition equalizes the current flowing in each strand of the conductor, and reduces eddy current losses in the winding itself. The stranded conductor is also more flexible than a solid conductor of similar size, aiding manufacture.\n\nThe windings of signal transformers minimize leakage inductance and stray capacitance to improve high-frequency response. Coils are split into sections, and those sections interleaved between the sections of the other winding.\n\nPower-frequency transformers may have \"taps\" at intermediate points on the winding, usually on the higher voltage winding side, for voltage adjustment. Taps may be manually reconnected, or a manual or automatic switch may be provided for changing taps. Automatic on-load tap changers are used in electric power transmission or distribution, on equipment such as arc furnace transformers, or for automatic voltage regulators for sensitive loads. Audio-frequency transformers, used for the distribution of audio to public address loudspeakers, have taps to allow adjustment of impedance to each speaker. A center-tapped transformer is often used in the output stage of an audio power amplifier in a push-pull circuit. Modulation transformers in AM transmitters are very similar.\n\nDry-type transformer winding insulation systems can be either of standard open-wound 'dip-and-bake' construction or of higher quality designs that include vacuum pressure impregnation (VPI), vacuum pressure encapsulation (VPE), and cast coil encapsulation processes. In the VPI process, a combination of heat, vacuum and pressure is used to thoroughly seal, bind, and eliminate entrained air voids in the winding polyester resin insulation coat layer, thus increasing resistance to corona. VPE windings are similar to VPI windings but provide more protection against environmental effects, such as from water, dirt or corrosive ambients, by multiple dips including typically in terms of final epoxy coat.\n\nRegarding image at top captioned, Cut view of transformer windings:\n\nIt is a rule of thumb that the life expectancy of electrical insulation is halved for about every 7 °C to 10 °C increase in operating temperature (an instance of the application of the Arrhenius equation).\n\nSmall dry-type and liquid-immersed transformers are often self-cooled by natural convection and radiation heat dissipation. As power ratings increase, transformers are often cooled by forced-air cooling, forced-oil cooling, water-cooling, or combinations of these. Large transformers are filled with transformer oil that both cools and insulates the windings. Transformer oil is a highly refined mineral oil that cools the windings and insulation by circulating within the transformer tank. The mineral oil and paper insulation system has been extensively studied and used for more than 100 years. It is estimated that 50% of power transformers will survive 50 years of use, that the average age of failure of power transformers is about 10 to 15 years, and that about 30% of power transformer failures are due to insulation and overloading failures. Prolonged operation at elevated temperature degrades insulating properties of winding insulation and dielectric coolant, which not only shortens transformer life but can ultimately lead to catastrophic transformer failure. With a great body of empirical study as a guide, transformer oil testing including dissolved gas analysis provides valuable maintenance information. This underlines the need to monitor, model, forecast and manage oil and winding conductor insulation temperature conditions under varying, possibly difficult, power loading conditions.\n\nBuilding regulations in many jurisdictions require indoor liquid-filled transformers to either use dielectric fluids that are less flammable than oil, or be installed in fire-resistant rooms. Air-cooled dry transformers can be more economical where they eliminate the cost of a fire-resistant transformer room.\n\nThe tank of liquid filled transformers often has radiators through which the liquid coolant circulates by natural convection or fins. Some large transformers employ electric fans for forced-air cooling, pumps for forced-liquid cooling, or have heat exchangers for water-cooling. An oil-immersed transformer may be equipped with a Buchholz relay, which, depending on severity of gas accumulation due to internal arcing, is used to either alarm or de-energize the transformer. Oil-immersed transformer installations usually include fire protection measures such as walls, oil containment, and fire-suppression sprinkler systems.\n\nPolychlorinated biphenyls have properties that once favored their use as a dielectric coolant, though concerns over their environmental persistence led to a widespread ban on their use. \nToday, non-toxic, stable silicone-based oils, or fluorinated hydrocarbons may be used where the expense of a fire-resistant liquid offsets additional building cost for a transformer vault. PCBs for new equipment were banned in 1981 and in 2000 for use in existing equipment in United Kingdom Legislation enacted in Canada between 1977 and 1985 essentially bans PCB use in transformers manufactured in or imported into the country after 1980, the maximum allowable level of PCB contamination in existing mineral oil transformers being 50 ppm.\n\nSome transformers, instead of being liquid-filled, have their windings enclosed in sealed, pressurized tanks and cooled by nitrogen or sulfur hexafluoride gas.\n\nExperimental power transformers in the 500‐to‐1,000 kVA range have been built with liquid nitrogen or helium cooled superconducting windings, which eliminates winding losses without affecting core losses.\n\nConstruction of oil-filled transformers requires that the insulation covering the windings be thoroughly dried of residual moisture before the oil is introduced. Drying is carried out at the factory, and may also be required as a field service. Drying may be done by circulating hot air around the core, by circulating externally heated transformer oil, or by vapor-phase drying (VPD) where an evaporated solvent transfers heat by condensation on the coil and core. The VPD process most often uses kerosene as the heat exchanging fluid. In addition to decreasing the moisture content in the insulation, the kerosene acts as a cleaning solvent which takes out any dust and dirt from the insulation surfaces. Compared to a conventional hot air drying process, the vapor-phase drying process decreases the drying time by 40% to 50%.\n\nFor small transformers, resistance heating by injection of current into the windings is used. The heating can be controlled very well, and it is energy efficient. The method is called low-frequency heating (LFH) since the current used is at a much lower frequency than that of the power grid, which is normally 50 or 60 Hz. A lower frequency reduces the effect of inductance, so the voltage required can be reduced. The LFH drying method is also used for service of older transformers.\n\nLarger transformers are provided with high-voltage insulated bushings made of polymers or porcelain. A large bushing can be a complex structure since it must provide careful control of the electric field gradient without letting the transformer leak oil.\n\nTransformers can be classified in many ways, such as the following:\nVarious specific electrical application designs require a variety of transformer types. Although they all share the basic characteristic transformer principles, they are customized in construction or electrical properties for certain installation requirements or circuit conditions.\n\nSince the high voltages carried in the wires are significantly greater than what is needed in-home, transformers are also used extensively in electronic products to decrease (or step-down) the supply voltage to a level suitable for the low voltage circuits they contain. The transformer also electrically isolates the end user from contact with the supply voltage.\nTransformers are used to increase (or step-up) voltage before transmitting electrical energy over long distances through wires. Wires have resistance which loses energy through joule heating at a rate corresponding to square of the current. By transforming power to a higher voltage transformers enable economical transmission of power and distribution. Consequently, transformers have shaped the electricity supply industry, permitting generation to be located remotely from points of demand. All but a tiny fraction of the world's electrical power has passed through a series of transformers by the time it reaches the consumer.\n\nSignal and audio transformers are used to couple stages of amplifiers and to match devices such as microphones and record players to the input of amplifiers. Audio transformers allowed telephone circuits to carry on a two-way conversation over a single pair of wires. A balun transformer converts a signal that is referenced to ground to a signal that has balanced voltages to ground, such as between external cables and internal circuits. Transformers made to medical grade standards isolate the users from the direct current. These are found commonly used in conjunction with hospital beds, dentist chairs, and other medical lab equipment.\n\nElectromagnetic induction, the principle of the operation of the transformer, was discovered independently by Michael Faraday in 1831, Joseph Henry in 1832, and others. The relationship between EMF and magnetic flux is an equation now known as Faraday's law of induction:\n\nwhere formula_6 is the magnitude of the EMF in Volts and Φ is the magnetic flux through the circuit in webers.\n\nFaraday performed early experiments on induction between coils of wire, including winding a pair of coils around an iron ring, thus creating the first toroidal closed-core transformer. However he only applied individual pulses of current to his transformer, and never discovered the relation between the turns ratio and EMF in the windings.\nThe first type of transformer to see wide use was the induction coil, invented by Rev. Nicholas Callan of Maynooth College, Ireland in 1836. He was one of the first researchers to realize the more turns the secondary winding has in relation to the primary winding, the larger the induced secondary EMF will be. Induction coils evolved from scientists' and inventors' efforts to get higher voltages from batteries. Since batteries produce direct current (DC) rather than AC, induction coils relied upon vibrating electrical contacts that regularly interrupted the current in the primary to create the flux changes necessary for induction. Between the 1830s and the 1870s, efforts to build better induction coils, mostly by trial and error, slowly revealed the basic principles of transformers.\n\nBy the 1870s, efficient generators producing alternating current (AC) were available, and it was found AC could power an induction coil directly, without an interrupter.\n\nIn 1876, Russian engineer Pavel Yablochkov invented a lighting system based on a set of induction coils where the primary windings were connected to a source of AC. The secondary windings could be connected to several 'electric candles' (arc lamps) of his own design.\n\nIn 1878, the Ganz factory, Budapest, Hungary, began producing equipment for electric lighting and, by 1883, had installed over fifty systems in Austria-Hungary. Their AC systems used arc and incandescent lamps, generators, and other equipment.\n\nLucien Gaulard and John Dixon Gibbs first exhibited a device with an open iron core called a 'secondary generator' in London in 1882, then sold the idea to the Westinghouse company in the United States. They also exhibited the invention in Turin, Italy in 1884, where it was adopted for an electric lighting system.\n\nInduction coils with open magnetic circuits are inefficient at transferring power to loads. Until about 1880, the paradigm for AC power transmission from a high voltage supply to a low voltage load was a series circuit. Open-core transformers with a ratio near 1:1 were connected with their primaries in series to allow use of a high voltage for transmission while presenting a low voltage to the lamps. The inherent flaw in this method was that turning off a single lamp (or other electric device) affected the voltage supplied to all others on the same circuit. Many adjustable transformer designs were introduced to compensate for this problematic characteristic of the series circuit, including those employing methods of adjusting the core or bypassing the magnetic flux around part of a coil.\nEfficient, practical transformer designs did not appear until the 1880s, but within a decade, the transformer would be instrumental in the War of Currents, and in seeing AC distribution systems triumph over their DC counterparts, a position in which they have remained dominant ever since.\n\nIn the autumn of 1884, Károly Zipernowsky, Ottó Bláthy and Miksa Déri (ZBD), three engineers associated with the Ganz factory, had determined that open-core devices were impracticable, as they were incapable of reliably regulating voltage. In their joint 1885 patent applications for novel transformers (later called ZBD transformers), they described two designs with closed magnetic circuits where copper windings were either a) wound around iron wire ring core or b) surrounded by iron wire core. The two designs were the first application of the two basic transformer constructions in common use to this day, which can as a class all be termed as either core form or shell form (or alternatively, core type or shell type), as in a) or b), respectively (see images). The Ganz factory had also in the autumn of 1884 made delivery of the world's first five high-efficiency AC transformers, the first of these units having been shipped on September 16, 1884. This first unit had been manufactured to the following specifications: 1,400 W, 40 Hz, 120:72 V, 11.6:19.4 A, ratio 1.67:1, one-phase, shell form.\n\nIn both designs, the magnetic flux linking the primary and secondary windings traveled almost entirely within the confines of the iron core, with no intentional path through air (see Toroidal cores below). The new transformers were 3.4 times more efficient than the open-core bipolar devices of Gaulard and Gibbs. The ZBD patents included two other major interrelated innovations: one concerning the use of parallel connected, instead of series connected, utilization loads, the other concerning the ability to have high turns ratio transformers such that the supply network voltage could be much higher (initially 1,400 to 2,000 V) than the voltage of utilization loads (100 V initially preferred). When employed in parallel connected electric distribution systems, closed-core transformers finally made it technically and economically feasible to provide electric power for lighting in homes, businesses and public spaces. Bláthy had suggested the use of closed cores, Zipernowsky had suggested the use of parallel shunt connections, and Déri had performed the experiments;\n\nTransformers today are designed on the principles discovered by the three engineers. They also popularized the word 'transformer' to describe a device for altering the EMF of an electric current, although the term had already been in use by 1882. In 1886, the ZBD engineers designed, and the Ganz factory supplied electrical equipment for, the world's first power station that used AC generators to power a parallel connected common electrical network, the steam-powered Rome-Cerchi power plant.\n\nAlthough George Westinghouse had bought Gaulard and Gibbs' patents in 1885, the Edison Electric Light Company held an option on the US rights for the ZBD transformers, requiring Westinghouse to pursue alternative designs on the same principles. He assigned to William Stanley the task of developing a device for commercial use in United States. Stanley's first patented design was for induction coils with single cores of soft iron and adjustable gaps to regulate the EMF present in the secondary winding (see image). This design was first used commercially in the US in 1886 but Westinghouse was intent on improving the Stanley design to make it (unlike the ZBD type) easy and cheap to produce.\n\nWestinghouse, Stanley and associates soon developed an easier to manufacture core, consisting of a stack of thin 'E‑shaped' iron plates, insulated by thin sheets of paper or other insulating material. Prewound copper coils could then be slid into place, and straight iron plates laid in to create a closed magnetic circuit. Westinghouse applied for a patent for the new low-cost design in December 1886; it was granted in July 1887.\n\nIn 1889, Russian-born engineer Mikhail Dolivo-Dobrovolsky developed the first three-phase transformer at the Allgemeine Elektricitäts-Gesellschaft ('General Electricity Company') in Germany.\n\nIn 1891, Nikola Tesla invented the Tesla coil, an air-cored, dual-tuned resonant transformer for producing very high voltages at high frequency.\n\nAudio frequency transformers ('repeating coils') were used by early experimenters in the development of the telephone.\n\n\n\nGeneral links:\n\nIEC Electropedia links: \n"}
{"id": "4020194", "url": "https://en.wikipedia.org/wiki?curid=4020194", "title": "Trompo", "text": "Trompo\n\nA Trompo is a top which is spun by winding a length of string around the body, and launching it so that lands spinning on its point. If the string is attached to a stick the rotation can be maintained by whipping the side of the body. The string may also be wound around the point while the trompo is spinning in order to control its position or even lift the spinning top to another surface.\n\nThese toys are popular in Latin America where the name \"trompo\" emerged, but there are very many different local names. In Spain, these toys may be called \"trompo\" or \"peonza\", \"perinola\", and \"pirinola\". In the Philippines, they are called \"trumpo\" or \"turumpo\", while in Portugal they're called \"pião\".\n\nIn Japan, similar tops are known as \"koma\", with most cities having a particular design.\n\nIn Germany a \"Peitschenkreisel\" may also be called \"Doppisch, Dildop, Pindopp, Dilledopp, Triesel or Tanzknopf\" (roughly \"dancing top\")\n\nIn Morocco it's called \"Trombia\" and it's often made out of wood and painted in a reddish brown color.\n\nTrompos have an approximately pear-shaped body and are usually made of a hard wood such as hawthorn, oak or beech although new resins and strong plastic materials have also been used. Whipping tops often have a more cylindrical shape to provide a bigger surface to be struck by the whip.\n\nA trofeo has a button-shaped on top, usually bigger than the tip on which trompo spins, and generally made of the same material as the rest of the body.\n\nThe base of a trompo is a stud or spike which may have a groove or roller-bearing to facilitate lifting the spinning trompo with the whip or string without imposing much friction on the body.\n\nThe trompo surface may be painted or decorated, and some versions incorporate synthetic sound devices. The small size diameter and low mass of most trompos means that mechanical whistles would cause excessive drag (physics) and reduce their spinning time.\n\nThe Philippine \"trumpo\" differs in the tip, which is straight and pointed. It usually looks like a nail embedded in a wooden spheroid. \n\nPlaying with a trompo consists of throwing the \"trompo\" and having it spin on the floor. Because of its shape, a trompo spins on its axis and swirls around its conic tip which is usually made of iron or steel. A trompo uses a string wrapped around it to get the necessary spin needed. The player must roll the cord around the trompo from the metallic tip up. The user must then tie the string in a knot on the button-shaped tip before releasing it. When rolling the cord around the trompo it must be done so that the cord is tightly attached to it. The technique for throwing a trompo varies. One end of the cord must be rolled around the player's fingers and with the same hand the trompo must be held with the metallic tip facing upwards.\n\nChampionships are held in different Latin American countries, especially in Mexico, Colombia, Peru, Cuba, Nicaragua, and Puerto Rico where it is very popular among children of the middle and lower classes.\n\nIn Mexico most trompos are sold made of plastic, with a metal tip, and sometimes they are made of wood. There is a popular game called \"\"picotazos\",\" where the main goal is to destroy the opponents' trompo. Another game is where a circle is drawn on the ground and a coin is placed in the middle, and the goal here is to strike the coin.\nIn Puerto Rico, one of the ways trompos are played is similar to playing marbles, with trompos being within a circle drawn on sand, the object being to knock them out of the circle, this can be played for keeps or otherwise. Failure to spin or spin within the circle causes your trompo to be added to it and another person has a turn to spin. Frequently, trompos in Puerto Rico and Chile are modified to have a sharper point, where in a game the object can be to split the other players trompo.\n\nJosé Miguel Agrelot, a Puerto Rican comedian, hosted a long-standing television program, \"Encabulla y Vuelve y Tira\", whose name described the action of throwing and spinning a trompo. One of his comedic characterizations, mischievous boy Torito Fuertes, was a one-time sponsor of a line of trompos.\n\nThe manner of playing the Filipino \"trumpo\" is basically the same, except that a knot is not tied into the tip before throwing it for the spin.\n\n"}
{"id": "49837584", "url": "https://en.wikipedia.org/wiki?curid=49837584", "title": "US Nuclear Corp", "text": "US Nuclear Corp\n\nUS Nuclear Corporation is a US radiation detection holding company headquartered in Canoga Park, CA specializing in the development and manufacturing of radiation detection instrumentation. It supplies instrumentation to nuclear power plants, national laboratories, government agencies, homeland security, military and weapon makers, universities and schools, research companies, hospitals, as well as energy companies.\n\nProducts include:\n\nUS Nuclear Corp has three divisions:\n\n\n"}
{"id": "43088577", "url": "https://en.wikipedia.org/wiki?curid=43088577", "title": "Velocity gradient", "text": "Velocity gradient\n\nIn fluid mechanics and continuum mechanics, the velocity gradient describes how the velocity of a fluid changes between different points within the fluid. Though the term can refer to the differences in velocity between layers of flow in a pipe, it is often used to mean the gradient of a flow's velocity with respect to its coordinates. The concept has implications in a variety of areas of physics and engineering, including magnetohydrodynamics, mining and water treatment. \n\nConsider the velocity field of a fluid flowing through a pipe. The layer of fluid in contact with the pipe tends to be at rest with respect to the pipe. This is called the no slip condition. If the velocity difference between fluid layers at the centre of the pipe and at the sides of the pipe is sufficiently small, then the fluid flow is observed in the form of continuous layers. This type of flow is called laminar flow.\n\nThe flow velocity difference between adjacent layers can be measured in terms of a velocity gradient, given by formula_1. Where formula_2 is the difference in flow velocity between the two layers and formula_3 is the distance between the layers.\n\nBy performing dimensional analysis, the dimensions of velocity gradient can be determined. The dimensions of velocity are formula_4 , and the dimensions of distance are formula_5. Since the velocity gradient can be expressed as formula_6. Therefore, the velocity gradient has the same dimensions as this ratio, i.e. formula_7.\n\nIn 3 dimensions, the gradient, formula_8 , of the velocity formula_9 is a second-order tensor which can be expressed as the matrix L:\n\nformula_11 can be decomposed into the sum of a symmetric matrix formula_12 and a skew-symmetric matrix formula_13 as follows\n\nformula_12 is called the strain rate tensor and describes the rate of stretching and shearing. formula_13 is called the spin tensor and describes the rate of rotation.\n\nSir Isaac Newton proposed that shear stress is directly proportional to the velocity gradient:\n\nThe constant of proportionality, formula_18, is called the dynamic viscosity.\n\nThe study of velocity gradients is useful in analysing path dependent materials and in the subsequent study of stresses and strains; e.g., Plastic deformation of metals. The near-wall velocity gradient of the unburned reactants flowing from a tube is a key parameter for characterising flame stability. The velocity gradient of a plasma can define conditions for the solutions to fundamental equations in magnetohydrodynamics. \n\n"}
{"id": "6923662", "url": "https://en.wikipedia.org/wiki?curid=6923662", "title": "Vernolic acid", "text": "Vernolic acid\n\nVernolic acid (leukotoxin) is a long chain fatty acid that is monounsaturated and contains an epoxide. It is the \"cis\" epoxide derived from the C12–C13 alkene of linoleic acid and exists both the (+)- and (–)- optical isomers. It is an isomer of coronaric acid, the two enantiomeric forms resulting instead by epoxidation at the C9–C10 alkene. Vernolic acid was first definitively characterized in 1954. It is the key component in vernonia oil, which is produced in abundance by the genera Vernonia and Euphorbia and is a potentially useful biofeedstock.\n\nIn a variety of mammalian species, vernolic acid is made by the metabolism of linoleic acid by certain cytochrome P450 epoxygenase enzymes; under these circumstances it is termed leukotoxin because of its toxic effects on leukocytes and other cell types and of its ability to produce multiple organ failure and respiratory distress when injected into rodent animal models of the acute respiratory distress syndrome. These effects appear due to the conversion of vernolic acid to its dihydroxy counterparts, 12\"S\",13\"R\"- and 12\"R\",13\"S\"-dihydroxy-\"cis\"-9-octadecenoic acids by soluble epoxide hydrolase (this dihydroxy mixture has been termed leukotoxin diol). Some studies suggest but have not yet proven that vernolic acid is responsible for or contributes to multiple organ failure, respiratory distress, and certain other cataclysmic diseases in humans (see epoxygenase subsection on linoleic acid).\n\nVernonia oil is extracted from the seeds of the \"Vernonia galamensis\" (or ironweed), a plant native to eastern Africa. The seeds contain about 40 to 42% oil of which 73 to 80% is vernolic acid. The best varieties of \"V. anthelmintica\" contain about 30% less vernolic acid. Products that can be made from vernonia oil include epoxies for manufacturing adhesives, varnishes and paints, and industrial coatings. Its low viscosity recommends its use as a nonvolatile solvent in oil-based paints since it will become incorporated in the dry paint rather than evaporating into the air.\"\n\nThis use of vernonia oil offers potential environmental benefits, since its use could reduce emissions associated with man-made chemicals.\n\nIn its application as an epoxy oil, vernonia oil competes with soybean or linseed oil, which supply most of the market for these applications. Its low viscosity makes it more desirable than fully epoxidized linseed or soybean oils. It is more comparable to partially epoxidized linseed or soybean oil.\n\nVernolic acid is not commonly found in plants in significant quantities, but some plants which do contain it are \"Vernonia\", \"Stokesia\", \"Crepis\" (from the daisy family), and \"Euphorbia lagascae\" and \"Bernardia pulchella\" from the Euphorbiaceae.\n"}
{"id": "3824441", "url": "https://en.wikipedia.org/wiki?curid=3824441", "title": "Vitallium", "text": "Vitallium\n\nVitallium is a trademark for an alloy of 65% cobalt, 30% chromium, 5% molybdenum, and other substances. The alloy is used in dentistry and artificial joints, because of its resistance to corrosion. It is also used for components of turbochargers because of its thermal resistance. Vitallium was developed by Albert W. Merrick for the Austenal Laboratories in 1932.\nIn 2016 Norman Sharp, a 91 year old British man, was recognised as having the world's oldest hip replacement implants. The two vitallium implants were implanted in November 1948 at the Royal National Orthopaedic Hospital, under the newly formed NHS. The 67 year old implants had such an unusually long life, partly because they had not required the typical replacement of such implants, but also because of Mr Sharp's young age of 23 when they were implanted, owing to a childhood case of septic arthritis.\n\nFor high-temperature use in engines, particularly turbochargers, the first alloy used was Haynes Stellite Nº 21, similar to Vitallium. This was suggested by the British engineer, and denture wearer, S.D. Heron during WWII. Although the characteristics of the material obviously suggested itself for making turbocharger blades, it was thought impossible to cast it to the precision needed. Heron demonstrated that it could be, by showing his Vitallium dentures.\n\n\n"}
{"id": "3337726", "url": "https://en.wikipedia.org/wiki?curid=3337726", "title": "Waist–hip ratio", "text": "Waist–hip ratio\n\nThe Waist-hip ratio or waist-to-hip ratio (WHR) is the dimensionless ratio of the circumference of the waist to that of the hips.\nThis is calculated as waist measurement divided by hip measurement (W ÷ H). For example, a person with a 30″ () waist and 38″ () hips has a waist-hip ratio of about 0.78.\n\nThe WHR has been used as an indicator or measure of health, and the risk of developing serious health conditions.\nWHR correlates with fertility (with different optimal values in males and females).\n\nAccording to the World Health Organization's data gathering protocol, the waist circumference should be measured at the midpoint between the lower margin of the last palpable ribs and the top of the iliac crest, using a stretch‐resistant tape that provides a constant 100 g tension. Hip circumference should be measured around the widest portion of the buttocks, with the tape parallel to the floor. Other organizations use slightly different standards. The United States National Institutes of Health and the National Health and Nutrition Examination Survey used results obtained by measuring at the top of the iliac crest. Waist measurements are usually obtained by laypersons by measurings around the waist at the navel, but research has shown that these measurements may underestimate the true waist circumference.\n\nFor both measurements, the individual should stand with feet close together, arms at the side and body weight evenly distributed, and should wear little clothing. The subject should be relaxed, and the measurements should be taken at the end of a normal respiration. Each measurement should be repeated twice; if the measurements are within 1 cm of one another, the average should be calculated. If the difference between the two measurements exceeds 1 cm, the two measurements should be repeated.\n\nPractically, however, the waist is more conveniently measured simply at the smallest circumference of the natural waist, usually just above the belly button, and the hip circumference may likewise be measured at its widest part of the buttocks or hip. Also, in case the waist is convex rather than concave, such as is the case in pregnancy, different body types, and obesity, the waist may be measured at a horizontal level 1 inch above the navel.\n\nThe WHR has been used as an indicator or measure of health, and the risk of developing serious health conditions. Research shows that people with \"apple-shaped\" bodies (with more weight around the waist) face more health risks than those with \"pear-shaped\" bodies who carry more weight around the hips.\n\nWHR is used as a measurement of obesity, which in turn is a possible indicator of other more serious health conditions. The WHO states that abdominal obesity is defined as a waist-hip ratio above 0.90 for males and above 0.85 for females, or a body mass index (BMI) above 30.0. The National Institute of Diabetes, Digestive and Kidney Diseases (NIDDK) states that women with waist-hip ratios of more than 0.8, and men with more than 1.0, are at increased health risk because of their fat distribution.\n\nWHR has been found to be a more efficient predictor of mortality in older people (>75 years of age) than waist circumference or BMI. If obesity is redefined using WHR instead of BMI, the proportion of people categorized as at risk of heart attack worldwide increases threefold. The body fat percentage is considered to be an even more accurate measure of relative weight. Of these three measurements, only the waist-hip ratio takes account of the differences in body structure. Hence, it is possible for two women to have vastly different body mass indices but the same waist–hip ratio, or to have the same body mass index but vastly different waist-hip ratios.\n\nWHR has been shown to be a better predictor of cardiovascular disease than waist circumference and body-mass index. However, other studies have found waist circumference, not WHR, to be a good indicator of cardiovascular risk factors, body fat distribution, and hypertension in type 2 diabetes.\n\nThe anti-stress hormone cortisol is regulated by the hypothalamic-pituitary-adrenal (HPA) axis and has been associated with higher levels of abdominal fat and therefore a higher WHR.\nAbdominal fat is a marker of visceral fat (stored around important internal organs such as the liver, pancreas and intestines) and has greater blood flow and more receptors for cortisol than peripheral fat. The greater the number of cortisol receptors, the more sensitive the visceral fat tissue is to cortisol. This heightened sensitivity to cortisol stimulates fat cells to further increase in size.\nWomen who have a combination of normal BMI and high WHR experience elevated cortisol reactivity to acute stressors and failure to habituate to repeated stressors, compared to women with normal WHR. This suggests that high WHR might also indicate HPA-axis dysregulation and over-exposure to cortisol.\n\nEvidence for the relationship between cortisol and central fat distribution has primarily been studied in individuals with Cushing’s syndrome.\nThis is characterized by over-exposure to cortisol due to elevated activity of the HPA axis. A primary component of Cushing’s syndrome is the accumulation of fat in the abdominal region, and it is hypothesized that elevated cortisol levels contribute to this accumulation. However, this hypothesis remains contested as cortisol levels only modestly explain variation in central fat distribution. It is more likely that a complex set of biological and neuroendocrine pathways related to cortisol secretion contribute to central adiposity, such as leptin, neuropeptide y, corticotropin releasing factor and the sympathetic nervous system.\n\nIn general, adults with growth hormone deficiencies also have increased WHRs. Adults with untreated congenital isolated growth hormone deficiency have increased WHRs, possibly from increased cortisone:cortisol ratios and insulin sensitivities. Since these individuals have increased visceral obesity, it has been suggested that a minimal growth hormone secretion would theoretically increase insulin resistance. However, because of the growth hormone deficiency, this insulin resistance point cannot be reached and these individuals are more sensitive to insulin. Increased adipose deposits are therefore more likely to form in these individuals, causing the high WHR. Growth hormone deficiencies have also been correlated with WHRs in prepubertal children; the specific baseline body statistics, such as WHRs, of pre-pubertal children with growth hormone deficiencies can predict growth response effectiveness to artificial growth hormone therapies, such as rhGH treatments.\n\nMales with congenital adrenal hyperplasia, determined by CYP21A2 mutations, have increased WHRs.\n\nA WHR of 0.9 for men and 0.7 for women has been shown to correlate strongly with general health and fertility. Women within the 0.7 range have optimal levels of estrogen and are less susceptible to major diseases such as diabetes, cardiovascular disorders and ovarian cancers. Women with high WHR (0.80 or higher) have significantly lower pregnancy rates than women with lower WHRs (0.70–0.79), independent of their BMIs. Men with WHRs around 0.9, similarly, have been shown to be more healthy and fertile with less prostate cancer and testicular cancer.\n\nEvidence suggests that WHR is an accurate somatic indicator of reproductive endocrinological status and long-term health risk. Among girls with identical body weights, those with lower WHRs show earlier pubertal endocrine activity, as measured by high levels of lutenizing hormone and follicle-stimulating hormone, as well as sex steroid (estradiol) activity. A Dutch prospective study on outcome in an artificial insemination program provides evidence for the role of WHR and fecundity. These investigators report that a 0.1 unit increase in WHR decreases the probability of conception per cycle by 30% after adjustment for age, obesity, reasons for artificial insemination, cycle length and regularity, smoking, and parity.\n\nMenopause, the natural or surgical cessation of the menstrual cycle, is due to an overall decrease in ovarian production of the hormones estradiol and progesterone. These hormonal changes are also associated with an increase in WHR independent of increases in body mass. Significantly, studies find that large premenopausal WHRs are associated with lower estradiol levels and variation in age of menopause onset. Circulating estrogen preferentially stores lipid deposits in the gluteofemoral region, including the buttocks and thighs, and evidence suggests that menopause-associated estrogen deficiency results in an accumulation of adipose deposits around the abdomen. These menopause-induced changes in body fat distribution can be counteracted with hormone replacement therapy. In contrast, aging males gradually accumulate abdominal fat, and hence increased WHR, in parallel with declining androgen levels.\n\nUsing data from the U.S. National Center for Health Statistics, William Lassek at the University of Pittsburgh in Pennsylvania and Steven Gaulin of the University of California, Santa Barbara, found a child's performance in cognitive tests correlated to their mother's waist–hip ratio, a proxy for how much fat she stores on her hips.\n\nChildren whose mothers had wide hips and a low waist–hip ratio scored highest, leading Lassek and Gaulin to suggest that fetuses benefit from hip fat, which contains long chain polyunsaturated fatty acids, critical for the development of the fetus's brain. In addition, evidence suggests that children of low-WHR teens were protected from the cognitive deficits often associated with teen birth.\n\nStudies in twins have suggested that between 22% and 61% of variability in waist-to-hip ratio may be accounted for by genetic factors.\n\nThe concept and significance of WHR as an indicator of attractiveness was first theorized by evolutionary psychologist Devendra Singh at the University of Texas at Austin in 1993. Singh argued that the WHR was a more consistent estrogen marker than the bust–waist ratio (BWR) studied at King's College, London by Dr. Glenn Wilson in the 1970s.\n\nSome researchers have found that the waist–hip ratio is a significant measure of female attractiveness. Women with a 0.7 WHR are usually rated as more attractive by men from Indo-European cultures. Preferences may vary, according to some studies, ranging from 0.6 in China, South America, and some of Africa to 0.8 in Cameroon and among the Hadza tribe of Tanzania, with divergent preferences according to the ethnicity of the observed being noted.\n\nIt appears that men in westernized societies are more influenced by female waist size than hip size:\n\nBy western standards, women in foraging populations have high numbers of pregnancies, high parasite loads, and high caloric dependence on fibrous foods. These variables change across cultures, suggesting that\nThus, a WHR that indicates pubertal onset, sex, fertility, hormonal irregularities, and/or differentiates male from female in one population may not do so in another.\n\nIn a series of 1993 studies done by Singh, men used WHR and overall body fat to determine a woman’s attractiveness. In his first study, men were shown a series of 12 drawings of women with various WHRs and body fat. Drawings with normal weight and a low WHR were associated with the most positive traits (i.e. attractive, sexy, intelligent and healthy). The drawings of thin female figures were not associated with any positive traits except youthfulness.\n\nThrough this study, Singh suggests that males and females may have developed innate mechanisms which detect and make use of the WHR to assess how healthy an individual is and (particularly for men), infer possible mate value. Having a healthy mate improves the chances of producing offspring with inherited genetic protection from various diseases and a healthy mate is more likely to be a good parent (Hamilton & Zuk, 1982; Thornhill, 1993).\n\nOther studies discovered WHR as a signal of attractiveness as well, beyond just examining body fat and fertility. Barnaby Dixson, Gina Grimshaw, Wayne Linklater, and Alan Dixson conducted a study using eye-tracking techniques to evaluate men's fixation on digitally altered photographs of the same woman, as well as asking the men to evaluate the images based on attractiveness. What they found was while men fixated on the woman's breasts in each photo, they selected the images where the woman had a 0.7 WHR as most attractive, regardless of breast size.\n\nFurthermore, referencing a 2005 study conducted by Johnson and Tassinary looking at animated human walking stimuli, Farid Pazhoohi and James R. Liddle proposed that men do not solely use WHR to evaluate attractiveness, but also a means of sex-differentiation, with higher WHR perceived as more masculine and lower WHR as an indicator of femininity. Pazhoohi and Liddle used this idea as a possible additional explanation as to why men perceive a lower WHR as more attractive – because it relates to an expression of femininity, as opposed to masculinity and a higher WHR. On this basis, it was shown that men with lower, more feminine, WHRs feel less comfortable and self-report lower body esteem and self-efficacy than men with higher, more masculine, WHRs.\n\nTo enhance their perceived attractiveness, some women may artificially alter their apparent WHR. The methods include the use of a corset to reduce the waist size and hip and buttock padding to increase the apparent size of the hips and buttocks. In an earlier attempt to quantify attractiveness, corset and girdle manufacturers of the 20th century used a calculation called \"hip spring\" (or \"hip-spring\" or \"hipspring\"), calculated by subtracting the waist measurement from the hip measurement. However this calculation fell into disuse because it is a poor indicator of attractiveness; for example, a hip spring of would likely be considered quite attractive for an average-sized adult woman, but a child or petite woman with the same number would more likely be seen as malnourished.\n\nWHR versus BMI attractiveness is related to fertility, not fat content. A study performed by Holliday used computer generated female body shapes to construct images which covary with real female body mass (indexed with BMI) and not with body shape (indexed with WHR), and vice versa. Twelve observers (6 male and 6 female) rated these images for attractiveness during an fMRI study. The attractiveness ratings were correlated with changes in BMI and not WHR. The results demonstrated that in addition to activation in higher visual areas, changes to BMI had a direct impact on activity within the brain’s reward system. This shows that BMI, not WHR, modulates reward mechanisms in the brain and that this may have important implications for judgements of ideal body size in eating disordered individuals.\n\nAnother study, conducted by Furnham, was used as an extension of Singh & Young's 1995 investigation. A total of 137 participants were in the study. There were 98 female participants. The age range was between 16 and 67. The majority of participants were undergraduates, and 90% were white British, the remainder being Asian (East Indian) and African. Their educational and socio-economic backgrounds (nearly all middle class) were fairly homogenous, and none had previously participated in any studies involving female body shape or attractiveness. It was predicted that the effect of breast size on judgment of attractiveness and age estimation would be dependent on overall body fat and the size of the waist-to-hip ratio.\n\nAll the participants were given a booklet with eight pictures in total. Each figure was identified as heavy or slender, feminine WHR or masculine WHR, and large-breasted or small-breasted. The participants rated the figures for four personal attributes (attractiveness, healthiness, femininity, and kindness/understanding).\n\nWhen ratings of the figures' attractiveness were made, generally it appeared that bust size, WHR, and their weight were all important contributory elements. The female participants rated the figures with a low WHR as more attractive, healthy, feminine looking, and in the case of the heavy figure, more kind/understanding than did male participants. This is a particularly interesting finding, as most previous studies report that young women idealize female bodies solely on the basis of thinness. As far as the breast sizes of the slender figures is concerned, whether they had large or small breasts did not appear to have any effect on the ratings of attractiveness or kindness/understanding, and having larger breasts only increased the mean ratings of health and femininity very slightly. However, a heavy figure with a high WHR and a large bust was rated as the least attractive and healthy by all participants.\n\nWaist-hip ratio is also a reliable cue to one’s sex and it is hypothesised that the \"individuals who represent a mismatch based on the cue provided by WHR (e.g., women with high WHRs or men with low WHRs) would likely be viewed as unattractive by the opposite sex.\"\n\nA number of studies have been carried out with focus on food composition of diets in relation to changes in waist circumference adjusted for body mass index.\n\nWhole-grain, ready-to-eat, oat cereal diets reduce low-density lipoprotein cholesterol and waist circumference in overweight or obese adults more than low-fibre control food diets. Weight loss did not vary between groups.\n\nThe mean annual change in waist circumference was more than 3 times as great for subjects in the white-bread cluster as subjects using a diet high in fruit, vegetables, reduced-fat dairy, and whole grains and low in red and processed meat, fast food, and soda.\n\nA 2011 study suggests that dietary pattern high in fruit and dairy and low in white bread, processed meat, margarine, and soft drinks may help to prevent abdominal fat accumulation.\n\n\n\n"}
{"id": "5065997", "url": "https://en.wikipedia.org/wiki?curid=5065997", "title": "Zirconium(IV) silicate", "text": "Zirconium(IV) silicate\n\nZirconium silicate, also zirconium orthosilicate, ZrSiO, is a chemical compound, a silicate of zirconium. It occurs in nature as zircon, a silicate mineral. Powdered zirconium silicate is also known as zircon flour.\n\nZirconium silicate is usually colorless, but impurities induce various colorations. It is insoluble in water, acids, alkali and aqua regia. Hardness is 7.5 on the Mohs scale.\n\nZirconium silicate occurs in nature as mineral zircon. Ore is mined from natural deposits and concentrated by various techniques. It is separated from sand by electrostatic and electromagnetic methods.\n\nAlso, the compound can be made by fusion of SiO and ZrO in an arc furnace, or by reacting a zirconium salt with sodium silicate in an aqueous solution.\n\nZirconium silicate is used for manufacturing refractory materials for applications where resistance to corrosion by alkali materials is required. It is also used in production of some ceramics, enamels, and ceramic glazes. In enamels and glazes it serves as an opacifier. It can be also present in some cements. Another use of zirconium silicate is as beads for milling and grinding. Thin films of zirconium silicate and hafnium silicate produced by chemical vapor deposition, most often MOCVD, can be used as a high-k dielectric as a replacement for silicon dioxide in semiconductors.\n\nZirconium silicates have also been studied for potential use in medical applications. For example, ZS-9 is a zirconium silicate that was designed specifically to trap potassium ions over other ions throughout the gastrointestinal tract. It is undergoing clinical trials for the treatment of hyperkalemia.\n\nZirconium disilicate is used in some dental crowns because of its hardness and because it is chemically nonreactive.\n\nZirconium silicate is an abrasive irritant for skin and eyes. Chronic exposure to dust can cause pulmonary granulomas, skin inflammation, and skin granuloma. However, there are no known adverse effects for normal, incidental ingestion.\n"}
