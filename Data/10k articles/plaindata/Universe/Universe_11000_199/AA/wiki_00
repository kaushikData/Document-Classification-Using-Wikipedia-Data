{"id": "39116569", "url": "https://en.wikipedia.org/wiki?curid=39116569", "title": "Adaptive fluid-infused porous film", "text": "Adaptive fluid-infused porous film\n\nAdaptive fluid-infused porous films change states when stretched, allowing for dynamic control over transparency and wettability. They were developed by researchers at Harvard University. The same team previously invented Slippery Liquid Infused Porous Surfaces (SLIPS) which served as the base technology to control wettability in Adaptive fluid-infused porous film.\n\nThe material is a thin elastic film that contains nano-sized pores. When in a normal relaxed state, if droplets of liquid are applied to the film, they will roll freely along the smooth surface. However, when the film is stretched, any droplets of liquid that are applied to the film will be held in place on the film. If the tension on the film is later released, the film will return to its normal relaxed state, and the droplet will again move along the smooth surface. The film also becomes more transparent when stretched, allowing the material to be dynamically controlled with regards to both the wettability and transparency of the material.\n"}
{"id": "35934993", "url": "https://en.wikipedia.org/wiki?curid=35934993", "title": "Agency for Renewable Resources", "text": "Agency for Renewable Resources\n\nThe Agency for Renewable Resources ( or FNR), was founded in 1993 as a government initiative intended to support research and development in the area of renewable resources. As a project managing organisation, the FNR answers to the Federal Ministry of Food and Agriculture (abbreviated BMEL in German). In May 2015, the BMEL announced the new ‘Renewable Resources’ funding programme. The Agency for Renewable Resources (FNR) has been entrusted with managing the programme. Currently, around 600 projects with a budget of 193 million euro are being funded by the FNR.\n\nIn 2016 the FNR manages 61 million euro provided from Germany’s Federal budget for the implementation of the funding programme. An additional 24.6 million euro are allocated for research and development in the field of bioenergy from the Special Energy and Climate Fund (EKF).\n\n‘Renewable Resources‘ Funding Programme\n\nThe ‘Renewable Resources’ funding programme is part of the government’s new high-tech strategy to improve Germany’s competitive position. It also supports the government’s ‘Policy Strategy on Bioeconomy’, which aims to create a resource-efficient economy that makes use of renewable resources.\n\nThe programme is intended to support the further development of a sustainable bio-based economy. This involves developing innovative, internationally competitive bio-based products, as well as processes and technologies for their production. Furthermore, the programme also supports the development of concepts aimed at improving the sustainability of the bio-based economy, while taking society’s expectations into account.\n\nThe FNR is a registered association. Its task is to effectively and continuously contribute to the research and development as well as the use of renewable resources taking into account conflicts of use, direct and indirect land use changes, biomass conversion processes as well as sustainability concepts.\n\nThe bodies of the FNR are\n\n\nOrganisation chart\n\nThe FNR’s main task is to support applied research and development in the field of sustainable development and use of renewable resources. It is our goal to develop promising processes and products to make them ready for the market. The FNR supports around 6800 projects each year.\n\nThe FNR is the first point of contact for all matters concerning renewable resources. Alongside specialist information, explanations and advice also play an important role. The FNR operates a variety of thematic portals providing information on the use of renewable resources. In addition, there are numerous publications available in the FNR’s media library. Regular events and industry and consumer expos round off the spectrum of activities.\n\nAs well as taking part in EU funded projects, the FNR is also involved in European and international committees and working groups that develop strategies for the sustainable use of biomass using innovative and efficient technologies.\n\nFossil resources like coal, mineral oil and gas decrease worldwide. Germany, like most industrial nations, has to focus more and more on alternative resources. This means that agricultural and forestry resources used in a substantial or energetic way can be a major contribution. Renewable resources are not only important for supply guarantee but also for climate protection.\n\nEconomic Factor of Renewable Resources\n\nCultivation and local and regional use of renewable resources support economic growth, added value and new jobs – especially in rural areas. They allow economic and social participation and will lead to an increase of quality of life for the people on the ground. Also in the industrial field, e.g. in the chemical industry, renewable resources are being used more and more and as a result new jobs can be provided.\n\nIn a unique pilot project (2007–2010), the FNR, in cooperation with and the nova-Institute, implemented the first government-funded project in Wikipedia: the development of the .\n\n\n"}
{"id": "4174517", "url": "https://en.wikipedia.org/wiki?curid=4174517", "title": "Algaculture", "text": "Algaculture\n\nAlgaculture is a form of aquaculture involving the farming of species of algae.\n\nThe majority of algae that are intentionally cultivated fall into the category of microalgae (also referred to as phytoplankton, microphytes, or planktonic algae). Macroalgae, commonly known as seaweed, also have many commercial and industrial uses, but due to their size and the specific requirements of the environment in which they need to grow, they do not lend themselves as readily to cultivation (this may change, however, with the advent of newer seaweed cultivators, which are basically algae scrubbers using upflowing air bubbles in small containers).\n\nCommercial and industrial algae cultivation has numerous uses, including production of food ingredients such as omega-3 fatty acids or natural food colorants and dyes, food, fertilizer, bioplastics, chemical feedstock (raw material), pharmaceuticals, and algal fuel, and can also be used as a means of pollution control.\n\nMost growers prefer monocultural production and go to considerable lengths to maintain the purity of their cultures. However, the microbiological contaminants are still under investigation. \n\nWith mixed cultures, one species comes to dominate over time and if a non-dominant species is believed to have particular value, it is necessary to obtain pure cultures in order to cultivate this species. Individual species cultures are also much needed for research purposes.\n\nA common method of obtaining pure cultures is serial dilution. Cultivators dilute either a wild sample or a lab sample containing the desired algae with filtered water and introduce small aliquots (measures of this solution) into a large number of small growing containers. Dilution follows a microscopic examination of the source culture that predicts that a few of the growing containers contain a single cell of the desired species. Following a suitable period on a light table, cultivators again use the microscope to identify containers to start larger cultures.\n\nAnother approach is to use a special medium which excludes other organisms, including invasive algae. For example, \"Dunaliella\" is a commonly grown genus of microalgae which flourishes in extremely salty water that few other organisms can tolerate.\nAlternatively, mixed algae cultures can work well for larval mollusks. First, the cultivator filters the sea water to remove algae which are too large for the larvae to eat. Next, the cultivator adds nutrients and possibly aerates the result. After one or two days in a greenhouse or outdoors, the resulting thin soup of mixed algae is ready for the larvae. An advantage of this method is low maintenance.\n\nWater, carbon dioxide, minerals and light are all important factors in cultivation, and different algae have different requirements. The basic reaction for algae growth in water is carbon dioxide + light energy + water = glucose + oxygen + water. This is called \"autotrophic\" growth. It is also possible to grow certain types of algae without light, these types of algae consume sugars (such as glucose). This is known as \"heterotrophic\" growth.\n\nThe water must be in a temperature range that will support the specific algal species being grown mostly between 15˚C and 35˚C.\n\nIn a typical algal-cultivation system, such as an open pond, light only penetrates the top of the water, though this depends on the algae density. As the algae grow and multiply, the culture becomes so dense that it blocks light from reaching deeper into the water. Direct sunlight is too strong for most algae, which can use only about the amount of light they receive from direct sunlight; however, exposing an algae culture to direct sunlight (rather than shading it) is often the best course for strong growth, as the algae underneath the surface is able to utilize more of the less intense light created from the shade of the algae above.\n\nTo use deeper ponds, growers agitate the water, circulating the algae so that it does not remain on the surface. Paddle wheels can stir the water and compressed air coming from the bottom lifts algae from the lower regions. Agitation also helps prevent over-exposure to the sun.\n\nAnother means of supplying light is to place the light \"in\" the system. Glow plates made from sheets of plastic or glass and placed within the tank offer precise control over light intensity, and distribute it more evenly. They are seldom used, however, due to high cost.\n\nThe odor associated with bogs, swamps, indeed any stagnant waters, can be due to oxygen depletion caused by the decay of deceased algal blooms. Under anoxic conditions, the bacteria inhabiting algae cultures break down the organic material and produce hydrogen sulfide and ammonia which causes the odor. This hypoxia often results in the death of aquatic animals. In a system where algae is intentionally cultivated, maintained, and harvested, neither eutrophication nor hypoxia are likely to occur.\n\nSome living algae and bacteria, also produce odorous chemicals, particularly certain (cyanobacteria) (previously classed as blue-green algae) such as \"Anabaena\". The most well-known of these odor-causing chemicals are MIB (2-methylisoborneol) and geosmin. They give a musty or earthy odor that can be quite strong. Eventual death of the cyanobacteria releases additional gas that is trapped in the cells. These chemicals are detectable at very low levels, in the parts per billion range, and are responsible for many \"taste and odor\" issues in drinking water treatment and distribution. Cyanobacteria can also produce chemical toxins that have been a problem in drinking water.\n\nNutrients such as nitrogen (N), phosphorus (P), and potassium (K) serve as fertilizer for algae, and are generally necessary for growth. Silica and iron, as well as several trace elements, may also be considered important marine nutrients as the lack of one can limit the growth of, or productivity in, a given area. Carbon dioxide is also essential; usually an input of CO2 is required for fast-paced algal growth. These elements must be dissolved into the water, in bio-available forms, for algae to grow.\n\nAlgae can be cultured in open ponds (such as raceway-type ponds and lakes) and photobioreactors. Raceway ponds may be less expensive.\n\nRaceway-type ponds and lakes are open to the elements. Open ponds are highly vulnerable to contamination by other microorganisms, such as other algal species or bacteria. Thus cultivators usually choose closed systems for monocultures. Open systems also do not offer control over temperature and lighting. The growing season is largely dependent on location and, aside from tropical areas, is limited to the warmer months.\n\nOpen pond systems are cheaper to construct, at the minimum requiring only a trench or pond. Large ponds have the largest production capacities relative to other systems of comparable cost. Also, open pond cultivation can exploit unusual conditions that suit only specific algae. For instance, \"Dunaliella salina\" grow in extremely salty water; these unusual media exclude other types of organisms, allowing the growth of pure cultures in open ponds. Open culture can also work if there is a system of harvesting only the desired algae, or if the ponds are frequently re-inoculated before invasive organisms can multiply significantly. The latter approach is frequently employed by Chlorella farmers, as the growth conditions for Chlorella do not exclude competing algae.\n\nThe former approach can be employed in the case of some chain diatoms since they can be filtered from a stream of water flowing through an outflow pipe. A \"pillow case\" of a fine mesh cloth is tied over the outflow pipe allowing other algae to escape. The chain diatoms are held in the bag and feed shrimp larvae (in Eastern hatcheries) and inoculate new tanks or ponds.\n\nEnclosing a pond with a transparent or translucent barrier effectively turns it into a greenhouse. This solves many of the problems associated with an open system. It allows more species to be grown, it allows the species that are being grown to stay dominant, and it extends the growing season – if heated, the pond can produce year round. Open race way ponds were used for removal of lead using live \"Spirulina (Arthospira) sp\".\n\nAlgae can also be grown in a photobioreactor (PBR). A PBR is a bioreactor which incorporates a light source. Virtually any translucent container could be called a PBR; however, the term is more commonly used to define a closed system, as opposed to an open tank or pond.\n\nBecause PBR systems are closed, the cultivator must provide all nutrients, including .\n\nA PBR can operate in \"batch mode\", which involves restocking the reactor after each harvest, but it is also possible to grow and harvest continuously. Continuous operation requires precise control of all elements to prevent immediate collapse. The grower provides sterilized water, nutrients, air, and carbon dioxide at the correct rates. This allows the reactor to operate for long periods. An advantage is that algae that grows in the \"log phase\" is generally of higher nutrient content than old \"senescent\" algae. Algal culture is the culturing of algae in ponds or other resources. Maximum productivity occurs when the \"exchange rate\" (time to exchange one volume of liquid) is equal to the \"doubling time\" (in mass or volume) of the algae.\n\nDifferent types of PBRs include:\n\nAlgae can be harvested using microscreens, by centrifugation, by flocculation and by froth flotation.\n\nInterrupting the carbon dioxide supply can cause algae to flocculate on its own, which is called \"autoflocculation\".\n\n\"Chitosan\", a commercial flocculant, more commonly used for water purification, is far more expensive. The powdered shells of crustaceans are processed to acquire chitin, a polysaccharide found in the shells, from which chitosan is derived via de-acetylation. Water that is more brackish, or saline requires larger amounts of flocculant. Flocculation is often too expensive for large operations.\n\nAlum and ferric chloride are other chemical flocculants.\n\nIn froth flotation, the cultivator aerates the water into a froth, and then skims the algae from the top.\n\nUltrasound and other harvesting methods are currently under development.\n\nAlgae oils have a variety of commercial and industrial uses, and are extracted through a variety of methods. Estimates of the cost to extract oil from microalgae vary, but are likely to be around three times higher than that of extracting palm oil.\n\nIn the first step of extraction, the oil must be separated from the rest of the algae. The simplest method is mechanical crushing. When algae is dried it retains its oil content, which then can be \"pressed\" out with an oil press. Different strains of algae warrant different methods of oil pressing, including the use of screw, expeller and piston. Many commercial manufacturers of vegetable oil use a combination of mechanical pressing and chemical solvents in extracting oil. This use is often also adopted for algal oil extraction.\n\nOsmotic shock is a sudden reduction in osmotic pressure, this can cause cells in a solution to rupture. Osmotic shock is sometimes used to release cellular components, such as oil.\n\nUltrasonic extraction, a branch of sonochemistry, can greatly accelerate extraction processes. Using an ultrasonic reactor, ultrasonic waves are used to create cavitation bubbles in a solvent material. When these bubbles collapse near the cell walls, the resulting shock waves and liquid jets cause those cells walls to break and release their contents into a solvent. Ultrasonication can enhance basic enzymatic extraction. The combination \"sonoenzymatic treatment\" accelerates extraction and increases yields.\n\nChemical solvents are often used in the extraction of the oils. The downside to using solvents for oil extraction are the dangers involved in working with the chemicals. Care must be taken to avoid exposure to vapors and skin contact, either of which can cause serious health damage. Chemical solvents also present an explosion hazard.\n\nA common choice of chemical solvent is hexane, which is widely used in the food industry and is relatively inexpensive. Benzene and ether can also separate oil. Benzene is classified as a carcinogen.\n\nAnother method of chemical solvent extraction is Soxhlet extraction. In this method, oils from the algae are extracted through repeated washing, or percolation, with an organic solvent such as hexane or petroleum ether, under reflux in a special glassware. The value of this technique is that the solvent is reused for each cycle.\n\nEnzymatic extraction uses enzymes to degrade the cell walls with water acting as the solvent. This makes fractionation of the oil much easier. The costs of this extraction process are estimated to be much greater than hexane extraction. The enzymatic extraction can be supported by ultrasonication. The combination \"sonoenzymatic treatment\" causes faster extraction and higher oil yields.\n\nSupercritical CO can also be used as a solvent. In this method, CO is liquefied under pressure and heated to the point that it becomes supercritical (having properties of both a liquid and a gas), allowing it to act as a solvent.\n\nOther methods are still being developed, including ones to extract specific types of oils, such as those with a high production of long-chain highly unsaturated fatty acids.\n\nSpecific algal strains can be acquired from algal culture collections, with over 500 culture collections registered with the World Federation for Culture Collections.\n\nSeveral species of algae are raised for food.\n\n\nFor centuries seaweed has been used as fertilizer. It is also an excellent source of potassium for manufacture of potash and potassium nitrate. Also some of microalgae can be used like this. \n\nBoth microalgae and macroalgae are used to make agar.\n\nWith concern over global warming, new methods for the thorough and efficient capture of CO are being sought out. The carbon dioxide that a carbon-fuel burning plant produces can feed into open or closed algae systems, fixing the CO and accelerating algae growth. Untreated sewage can supply additional nutrients, thus turning two pollutants into valuable commodities.\n\nAlgae cultivation is under study for uranium/plutonium sequestration and purifying fertilizer runoff.\n\nBusiness, academia and governments are exploring the possibility of using algae to make gasoline, bio-diesel, biogas and other fuels. Algae itself may be used as a biofuel, and additionally be used to create hydrogen. See Algae fuel.\n\nChlorella, particularly a transgenic strain which carries an extra mercury reductase gene, has been studied as an agent for environmental remediation due to its ability to reduce to the less toxic elemental mercury.\n\nCultivated algae serve many other purposes, including cosmetics, animal feed, bioplastic production, dyes and colorant production, chemical feedstock production, and pharmaceutical ingredients.\n\n"}
{"id": "14898881", "url": "https://en.wikipedia.org/wiki?curid=14898881", "title": "Battery terminal", "text": "Battery terminal\n\nBattery terminals are the electrical contacts used to connect a load or charger to a single cell or multiple-cell battery. These terminals have a wide variety of designs, sizes, and features that are often not well documented.\n\nAutomotive batteries typically have one of three types of terminals.\n\nIn recent years, the most common design was the \"SAE Post\", consisting of two lead posts in the shape of truncated cones, positioned on the top of the battery, with slightly different diameters to ensure correct electrical polarity.\n\nThe \"JIS\" type is similar to the SAE but smaller, once again positive is larger than negative but both are smaller than their SAE counterparts. Most older Japanese cars were fitted with JIS terminals.\n\nGeneral Motors, and other automobile manufacturers, have also begun using side-post battery terminals, which consist of two recessed female 3/8\" threads (SAE 3/8-16) into which bolts or various battery terminal adapters are to be attached. These side posts are of the same size and do not prevent incorrect polarity connections.\n\nL terminals consist of an L-shaped post with a bolt hole through the vertical side. These are used on some European cars, motorcycles, lawn and garden devices, snowmobiles, and other light duty vehicles.\n\nSome batteries sizes are available with terminals in two different configurations: 1) positive on left and negative on right, 2) negative on left and positive on right. Purchasing the wrong configuration may prevent battery cables from reaching the battery terminals.\n\nMarine batteries typically have two posts, a 3/8\"-16 threaded post for the positive terminal, and a 5/16\"-18 threaded post for the negative terminal.\n\nZinc battery terminals are an environmentally friendly alternative to lead battery terminals. These types of battery terminals were designed as a result of environmental directives such as Proposition 65 and ROHS. Zinc battery terminals offer advantages over lead alloy type battery terminals. These advantages include increased electrical conductivity, increased corrosion resistance, and reduced lead removal costs. \n\nThe most common sizes of sealed lead acid (SLA) batteries use Faston tabs, but some larger batteries use L terminals, while some very specialized designs use other, sometimes proprietary terminals, such as older Panasonic camcorder batteries (of the type used for VHS shoulder-mounted camcorders).\n\nBatteries designed for use inside a portable uninterruptable power supply (UPS) typically use Faston tabs, often with an adapter cable between those and the UPS's internal battery connectors. Larger external battery packs use a variety of connectors, including the Anderson Powerpole MultiPole series (as used by Tripp Lite), which are color-coded and keyed for specific voltages. Very large batteries as installed in battery rooms such as are found in datacenters use bolted connections from cell terminals to bus bars or flexible cables.\n\nMost cylindrical dry batteries (such as the AA battery) have a projection at one end (positive) and a flat base (negative). These mate with metal strips or springs in the battery holder.\n\nSix volt lantern batteries typically feature two coiled, cone-shaped spring terminals, designed to mate with flat contact plates on the inside of the battery compartment. Some lantern batteries instead feature screw terminals, while still others instead feature pin holes.\n\nNine-volt batteries have snap-on connectors.\n\nButton cell (watch batteries) have terminals as both flat sides.\n\n"}
{"id": "2073471", "url": "https://en.wikipedia.org/wiki?curid=2073471", "title": "Beta oxidation", "text": "Beta oxidation\n\nIn biochemistry and metabolism, beta-oxidation is the catabolic process by which fatty acid molecules are broken down in the cytosol in prokaryotes and in the mitochondria in eukaryotes to generate acetyl-CoA, which enters the citric acid cycle, and NADH and FADH, which are co-enzymes used in the electron transport chain. It is named as such because the beta carbon of the fatty acid undergoes oxidation to a carbonyl group. Beta-oxidation is primarily facilitated by the mitochondrial trifunctional protein, an enzyme complex associated with the inner mitochondrial membrane, although very long chain fatty acids are oxidized in peroxisomes.\n\nThe overall reaction for one cycle of beta oxidation is:\n\nFatty acid catabolism consists of:\n\nFree fatty acids cannot penetrate any biological membrane due to their negative charge. Free fatty acids must cross the cell membrane through specific transport proteins, such as the SLC27 family fatty acid transport protein. Once in the cytosol, the following processes bring fatty acids into the mitochondrial matrix so that beta-oxidation can take place.\n\nOnce the fatty acid is inside the mitochondrial matrix, beta-oxidation occurs by cleaving two carbons every cycle to form acetyl-CoA. The process consists of 4 steps.\n\n\nFatty acids are oxidized by most of the tissues in the body. However, some tissues such as the red blood cells of mammals (which do not contain mitochondria), and cells of the central nervous system do not use fatty acids for their energy requirements, but instead use carbohydrates (red blood cells and neurons) or ketone bodies (neurons only).\n\nBecause many fatty acids are not fully saturated or do not have an even number of carbons, several different mechanisms have evolved, described below.\n\nOnce inside the mitochondria, each cycle of β-oxidation, liberating a two carbon unit (acetyl-CoA), occurs in a sequence of four reactions:\n\nThis process continues until the entire chain is cleaved into acetyl CoA units. The final cycle produces two separate acetyl CoAs, instead of one acyl CoA and one acetyl CoA. For every cycle, the Acyl CoA unit is shortened by two carbon atoms. Concomitantly, one molecule of FADH, NADH and acetyl CoA are formed.\n\nIn general, fatty acids with an odd number of carbons are found in the lipids of plants and some marine organisms. Many ruminant animals form a large amount of 3-carbon propionate during the fermentation of carbohydrates in the rumen. Long-chain fatty acids with an odd number of carbon atoms are found particularly in ruminant fat and milk.\n\nChains with an odd-number of carbons are oxidized in the same manner as even-numbered chains, but the final products are propionyl-CoA and Acetyl CoA\n\nPropionyl-CoA is first carboxylated using a bicarbonate ion into D-stereoisomer of methylmalonyl-CoA, in a reaction that involves a biotin co-factor, ATP, and the enzyme propionyl-CoA carboxylase. The bicarbonate ion's carbon is added to the middle carbon of propionyl-CoA, forming a D-methylmalonyl-CoA. However, the D conformation is enzymatically converted into the L conformation by methylmalonyl-CoA epimerase, then it undergoes intramolecular rearrangement, which is catalyzed by methylmalonyl-CoA mutase (requiring B as a coenzyme) to form succinyl-CoA. The succinyl-CoA formed can then enter the citric acid cycle.\n\nHowever, whereas acetyl-CoA enters the citric acid cycle by condensing with an existing molecule of oxaloacetate, succinyl-CoA enters the cycle as a principal in its own right. Thus the succinate just adds to the population of circulating molecules in the cycle and undergoes no net metabolization while in it. When this infusion of citric acid cycle intermediates exceeds cataplerotic demand (such as for aspartate or glutamate synthesis), some of them can be extracted to the gluconeogenesis pathway, in the liver and kidneys, through phosphoenolpyruvate carboxykinase, and converted to free glucose.\n\nβ-Oxidation of unsaturated fatty acids poses a problem since the location of a cis bond can prevent the formation of a trans-Δ bond. These situations are handled by an additional two enzymes, Enoyl CoA isomerase or 2,4 Dienoyl CoA reductase.\n\nWhatever the conformation of the hydrocarbon chain, β-oxidation occurs normally until the acyl CoA (because of the presence of a double bond) is not an appropriate substrate for acyl CoA dehydrogenase, or enoyl CoA hydratase:\n\n\nTo summarize:\n\nFatty acid oxidation also occurs in peroxisomes when the fatty acid chains are too long to be handled by the mitochondria. The same enzymes are used in peroxisomes as in the mitochondrial matrix, and acetyl-CoA is generated. It is believed that very long chain (greater than C-22) fatty acids, branched fatty acids, some prostaglandins and leukotrienes undergo initial oxidation in peroxisomes until octanoyl-CoA is formed, at which point it undergoes mitochondrial oxidation.\n\nOne significant difference is that oxidation in peroxisomes is not coupled to ATP synthesis. Instead, the high-potential electrons are transferred to O, which yields HO. It does generate heat however. The enzyme catalase, found exclusively in peroxisomes, converts the hydrogen peroxide into water and oxygen.\n\nPeroxisomal β-oxidation also requires enzymes specific to the peroxisome and to very long fatty acids. There are three key differences between the enzymes used for mitochondrial and peroxisomal β-oxidation:\n\nPeroxisomal oxidation is induced by a high-fat diet and administration of hypolipidemic drugs like clofibrate.\n\nThe ATP yield for every oxidation cycle is theoretically a maximum yield of 17, as NADH produces 2.5 ATP, FADH produces 1.5 and a full rotation of the citric acid cycle produces 10. In practice it is closer to 14 ATP for a full oxidation cycle as the theoretical yield is not attained - it is generally closer to 2.5 ATP per NADH molecule produced, 1.5 for each FADH molecule produced and this equates to 10 per cycle of the TCA (according to the P/O ratio), broken down as follows:\n\nFor an even-numbered saturated fat (C), n - 1 oxidations are necessary, and the final process yields an additional acetyl CoA. In addition, two equivalents of ATP are lost during the activation of the fatty acid. Therefore, the total ATP yield can be stated as:\n\nor\nFor instance, the ATP yield of palmitate (C, \"n = 8\") is:\n\nRepresented in table form:\n\nFor sources that use the larger ATP production numbers described above, the total would be 129 ATP ={(8-1)*17+12-2} equivalents per palmitate.\n\nBeta-oxidation of unsaturated fatty acids changes the ATP yield due to the requirement of two possible additional enzymes.\n\nThe reactions of beta oxidation and part of citric acid cycle present structural similarities in three of four reactions of the beta oxidation: the oxidation by FAD, the hydration, and the oxidation by NAD. Each enzyme of these metabolic pathways presents structural similarity.\n\nThere are at least 25 enzymes and specific transport proteins in the β-oxidation pathway. Of these, 18 have been associated with human disease as inborn errors of metabolism.\n\n\n"}
{"id": "2657657", "url": "https://en.wikipedia.org/wiki?curid=2657657", "title": "Candoluminescence", "text": "Candoluminescence\n\nCandoluminescence is the light given off by certain materials at elevated temperatures (usually when exposed to a flame) that has an intensity at some wavelengths which can be higher than the blackbody emission expected from incandescence at the same temperature. The phenomenon is notable in certain transition-metal and rare-earth oxide materials (ceramics) such as zinc oxide, cerium(IV) oxide and thorium dioxide.\n\nThe existence of the candoluminescence phenomenon and the underlying mechanism have been the subject of extensive research and debate since the first reports of it in the 1800s. The topic was of particular interest before the introduction of electric lighting, when most artificial light was produced by fuel combustion. The main alternative explanation for candoluminescence is that it is simply \"selective\" thermal emission in which the material has a very high emissivity in the visible spectrum and a very weak emissivity in the part of the spectrum where the blackbody thermal emission would be highest; in such a system, the emitting material will tend to retain a higher temperature because of the lack of invisible radiative cooling. In this scenario, observations of candoluminescence would simply have been underestimating the temperature of the emitting species. Several authors in the 1950s came to the view that candoluminescence was simply an instance of selective thermal emission, and one of the most prominent researchers in the field, V. A. Sokolov, once advocated eliminating the term from the literature in his noted 1952 review article, only to revise his view several years later. The modern scientific consensus is that candoluminescence does occur, that it is not always simply due to selective thermal emission, but the mechanisms vary depending on the materials involved and the method of heating, particularly the type of flame and the position of the material relative to the flame.\n\nWhen the fuel in a flame combusts, the energy released by the combustion process is deposited in combustion products, usually molecular fragments called free radicals. The combustion products are excited to a very high temperature called the adiabatic flame temperature (that is, the temperature before any heat has been transferred away from the combustion products). This temperature is usually much higher than the temperature of the air in the flame or which an object inserted into the flame can reach. When the combustion products lose this energy by radiative emission, the radiation can thus be more intense than that of a lower-temperature blackbody inserted into the flame. The exact emission process involved varies with the material, the type of fuels and oxidizers, and the type of flame, though in many cases it is well established that the free radicals undergo radiative recombination. This energetic light emitted directly from the combustion products may be observed directly (as with a blue gas flame), depending on the wavelength, or it may then cause fluorescence in the candoluminescent material. Some free-radical recombinations emit ultraviolet light, which is only observable through fluorescence. \n\nOne important candoluminescence mechanism is that the candoluminescent material catalyzes the recombination, enhancing the intensity of the emission. Extremely narrow-wavelength emission by the combustion products is often an important feature in this process, because it reduces the rate at which the free radicals lose heat to radiation at invisible or non-fluorescence-exciting wavelengths. In other cases, the excited combustion products are thought to directly transfer their energy to luminescent species in the solid material. In any case, the key feature of candoluminescence is that the combustion products lose their energy to radiation without becoming thermalized with the environment, which allows the effective temperature of their radiation to be much higher than that of thermal emission from materials in thermal equilibrium with the environment.\n\nEarly in the 20th century, there was vigorous debate over whether candoluminescence is required to explain the behavior of Welsbach gas mantles or limelight. One counterargument was that since thorium oxide (for example) has much lower emissivity in the near infrared region than the shorter wavelength parts of the visible spectrum, it should not be strongly cooled by infrared radiation, and thus a thorium-oxide mantle can get closer to the flame temperature than can a blackbody material. The higher temperature would then lead to higher emission levels in the visible portion of the spectrum, without invoking candoluminescence as an explanation. \n\nAnother argument was that the oxides in the mantle might be actively absorbing the combustion products and thus being selectively raised to combustion-product temperatures. Some more recent authors seem to have concluded that neither Welsbach mantles nor limelight involve candoluminescence (e.g. Mason), but Ivey, in an extensive review of 254 sources, concluded that catalysis of free-radical recombination does enhance the emission of Welsbach mantles, such that they are candoluminescent.\n\n"}
{"id": "19265670", "url": "https://en.wikipedia.org/wiki?curid=19265670", "title": "Centrifugal force", "text": "Centrifugal force\n\nIn Newtonian mechanics, the centrifugal force is an inertial force (also called a \"fictitious\" or \"pseudo\" force) directed away from the axis of rotation that appears to act on all objects when viewed in a rotating frame of reference.\n\nThe concept of centrifugal force can be applied in rotating devices, such as centrifuges, centrifugal pumps, centrifugal governors, and centrifugal clutches, and in centrifugal railways, planetary orbits and banked curves, when they are analyzed in a rotating coordinate system. The term has sometimes also been used for the reactive centrifugal force that is a reaction to a centripetal force.\n\nCentrifugal force is an outward force apparent in a rotating reference frame. It does not exist when a system is described relative to an inertial frame of reference. All measurements of position and velocity must be made relative to some frame of reference. For example, an analysis of the motion of an object in an airliner in flight could be made relative to the airliner, to the surface of the Earth, or even to the Sun. A reference frame that is at rest (or one that moves with no rotation and at constant velocity) relative to the \"fixed stars\" is generally taken to be an inertial frame. Any system can be analyzed in an inertial frame (and so with no centrifugal force). However, it is often more convenient to describe a rotating system by using a rotating frame—the calculations are simpler, and descriptions more intuitive. When this choice is made, fictitious forces, including the centrifugal force, arise.\n\nIn a rotating reference frame, all objects, regardless of their state of motion, appear to be under the influence of a radially (from the axis of rotation) outward force that is proportional to their mass, to the distance from the axis of rotation of the frame, and to the square of the angular velocity of the frame. This is the centrifugal force. As humans usually experience centrifugal force from within the rotating reference frame, e.g. on a merry-go-round or vehicle, this is much more well-known than centripetal force.\n\nMotion relative to a rotating frame results in another fictitious force: the Coriolis force. If the rate of rotation of the frame changes, a third fictitious force (the Euler force) is required. These fictitious forces are necessary for the formulation of correct equations of motion in a rotating reference frame and allow Newton's laws to be used in their normal form in such a frame (with one exception: the fictitious forces do not obey Newton's third law: they have no equal and opposite counterparts).\n\nA common experience that gives rise to the idea of a centrifugal force is encountered by passengers riding in a vehicle, such as a car, that is changing direction. If a car is traveling at a constant speed along a straight road, then a passenger inside is not accelerating and, according to Newton's second law of motion, the net force acting on him is therefore zero (all forces acting on him cancel each other out). If the car enters a curve that bends to the left, the passenger experiences an apparent force that seems to be pulling him towards the right. This is the fictitious centrifugal force. It is needed within the passenger's local frame of reference to explain his sudden tendency to start accelerating to the right relative to the car—a tendency which he must resist by applying a rightward force to the car (for instance, a frictional force against the seat) in order to remain in a fixed position inside. Since he pushes the seat toward the right, Newton's third law says that the seat pushes him toward the left. The centrifugal force must be included in the passenger's reference frame (in which the passenger remains at rest): it counteracts the leftward force applied to the passenger by the seat, and explains why this otherwise unbalanced force does not cause him to accelerate. However, it would be apparent to a stationary observer watching from an overpass above that the frictional force exerted on the passenger by the seat is not being balanced; it constitutes a net force to the left, causing the passenger to accelerate toward the inside of the curve, as he must in order to keep moving with the car rather than proceeding in a straight line as he otherwise would. Thus the \"centrifugal force\" he feels is the result of a \"centrifugal tendency\" caused by inertia. Similar effects are encountered in aeroplanes and roller coasters where the magnitude of the apparent force is often reported in \"G's\".\n\nIf a stone is whirled round on a string, in a horizontal plane, the only real force acting on the stone in the horizontal plane is applied by the string (gravity acts vertically). There is a net force on the stone in the horizontal plane which acts toward the center.\n\nIn an inertial frame of reference, were it not for this net force acting on the stone, the stone would travel in a straight line, according to Newton's first law of motion. In order to keep the stone moving in a circular path, a centripetal force, in this case provided by the string, must be continuously applied to the stone. As soon as it is removed (for example if the string breaks) the stone moves in a straight line. In this inertial frame, the concept of centrifugal force is not required as all motion can be properly described using only real forces and Newton's laws of motion.\n\nIn a frame of reference rotating with the stone around the same axis as the stone, the stone is stationary. However, the force applied by the string is still acting on the stone. If one were to apply Newton's laws in their usual (inertial frame) form, one would conclude that the stone should accelerate in the direction of the net applied force—towards the axis of rotation—which it does not do. The centrifugal force and other fictitious forces must be included along with the real forces in order to apply Newton's laws of motion in the rotating frame.\n\nThe Earth constitutes a rotating reference frame because it rotates once a day on its axis. Because the rotation is slow, the fictitious forces it produces are small, and in everyday situations can generally be neglected. Even in calculations requiring high precision, the centrifugal force is generally not explicitly included, but rather lumped in with the gravitational force: the strength and direction of the local \"gravity\" at any point on the Earth's surface is actually a combination of gravitational and centrifugal forces.\n\nIf an object is weighed with a simple spring balance at one of the Earth's poles, there are two forces acting on the object: the Earth's gravity, which acts in a downward direction, and the equal and opposite restoring force in the spring, acting upward. Since the object is stationary and not accelerating, there is no net force acting on the object and the force from the spring is equal in magnitude to the force of gravity on the object. In this case, the balance shows the value of the force of gravity on the object.\n\nWhen the same object is weighed on the equator, the same two real forces act upon the object. However, the object is moving in a circular path as the Earth rotates and therefore experiencing a centripetal acceleration. When considered in an inertial frame (that is to say, one that is not rotating with the Earth), the non-zero acceleration means that force of gravity will not balance with the force from the spring. In order to have a net centripetal force, the magnitude of the restoring force of the spring must be less than the magnitude of force of gravity. Less restoring force in the spring is reflected on the scale as less weight — about 0.3% less at the equator than at the poles. In the Earth reference frame (in which the object being weighed is at rest), the object does not appear to be accelerating, however the two real forces, gravity and the force from the spring, are the same magnitude and do not balance. The centrifugal force must be included to make the sum of the forces be zero to match the apparent lack of acceleration.\n\nThis thought experiment is more complicated than the previous examples in that it requires the use of the Coriolis force as well as the centrifugal force.\n\nIf there were a railway line running round the Earth's equator, a train moving westward along it fast enough would remain stationary in a frame moving (but not rotating) with the Earth; it would stand still as the Earth spun beneath it. In this inertial frame the situation is easy to analyze. The only forces acting on the train (assuming no wind resistance or other horizontal forces) are its gravity (downward) and the equal and opposite (upward) force from the track. There is no net force on the train and it therefore remains stationary.\n\nIn a frame rotating with the Earth the train moves in a circular orbit as it travels round the Earth. In this frame, the upward reaction force from the track and the force of gravity on the train remain the same, as they are real forces. However, in the Earth's (rotating) frame, the train is traveling in a circular path and therefore requires a centripetal (downward) force to keep it on this path. Because this uses a rotating frame, the (fictitious) centrifugal force must be applied to the train. This is equal in value to the required centripetal force but acts in an upward direction — the opposite direction to that required. It would seem that there is a net upward force on the train and it should therefore accelerate upward.\n\nThe resolution to this paradox lies in the fact that the train is in motion with respect to the rotating frame and is subject to (in addition to the centrifugal force) the Coriolis force, which, in this example, acts downward direction and is twice as strong as centrifugal force.\n\nFor the following formalism, the rotating frame of reference is regarded as a special case of a non-inertial reference frame that is rotating relative to an inertial reference frame denoted the stationary frame.\n\nIn a rotating frame of reference, the time derivatives of any vector function of time—such as the velocity and acceleration vectors of an object—will differ from its time derivatives in the stationary frame. If are the components of with respect to unit vectors directed along the axes of the rotating frame (i.e. ), then the first time derivative of with respect to the rotating frame is, by definition, . If the absolute angular velocity of the rotating frame is then the derivative of with respect to the stationary frame is related to by the equation:\n\nwhere formula_1 denotes the vector cross product. In other words, the rate of change of in the stationary frame is the sum of its apparent rate of change in the rotating frame and a rate of rotation formula_2 attributable to the motion of the rotating frame. The vector has magnitude equal to the rate of rotation and is directed along the axis of rotation according to the right-hand rule.\n\nNewton's law of motion for a particle of mass \"m\" written in vector form is:\n\nwhere is the vector sum of the physical forces applied to the particle and is the absolute acceleration (that is, acceleration in an inertial frame) of the particle, given by:\n\nwhere is the position vector of the particle.\n\nBy applying the transformation above from the stationary to the rotating frame three times (twice to formula_5 and once to formula_6), the absolute acceleration of the particle can be written as:\n\nThe apparent acceleration in the rotating frame is . An observer unaware of the rotation would expect this to be zero in the absence of outside forces. However, Newton's laws of motion apply only in the inertial frame and describe dynamics in terms of the absolute acceleration . Therefore, the observer perceives the extra terms as contributions due to fictitious forces. These terms in the apparent acceleration are independent of mass; so it appears that each of these fictitious forces, like gravity, pulls on an object in proportion to its mass. When these forces are added, the equation of motion has the form:\n\nFrom the perspective of the rotating frame, the additional force terms are experienced just like the real external forces and contribute to the apparent acceleration. The additional terms on the force side of the equation can be recognized as, reading from left to right, the Euler force formula_10, the Coriolis force formula_11, and the centrifugal force formula_12, respectively. Unlike the other two fictitious forces, the centrifugal force always points radially outward from the axis of rotation of the rotating frame, with magnitude , and unlike the Coriolis force in particular, it is independent of the motion of the particle in the rotating frame. As expected, for a non-rotating inertial frame of reference formula_13 the centrifugal force and all other fictitious forces disappear. Similarly, as the centrifugal force is proportional to the distance from object to the axis of rotation of the frame, the centrifugal force vanishes for objects that lie upon the axis.\n\nThree scenarios were suggested by Newton to answer the question of whether the absolute rotation of a local frame can be detected; that is, if an observer can decide whether an observed object is rotating or if the observer is rotating.\n\n\nIn these scenarios, the effects attributed to centrifugal force are only observed in the local frame (the frame in which the object is stationary) if the object is undergoing absolute rotation relative to an inertial frame. By contrast, in an inertial frame, the observed effects arise as a consequence of the inertia and the known forces without the need to introduce a centrifugal force. Based on this argument, the privileged frame, wherein the laws of physics take on the simplest form, is a stationary frame in which no fictitious forces need to be invoked.\n\nWithin this view of physics, any other phenomenon that is usually attributed to centrifugal force can be used to identify absolute rotation. For example, the oblateness of a sphere of freely flowing material is often explained in terms of centrifugal force. The oblate spheroid shape reflects, following Clairaut's theorem, the balance between containment by gravitational attraction and dispersal by centrifugal force. That the Earth is itself an oblate spheroid, bulging at the equator where the radial distance and hence the centrifugal force is larger, is taken as one of the evidences for its absolute rotation.\n\nThe operations of numerous common rotating mechanical systems are most easily conceptualized in terms of centrifugal force. For example:\n\n\nNevertheless, all of these systems can also be described without requiring the concept of centrifugal force, in terms of motions and forces in a stationary frame, at the cost of taking somewhat more care in the consideration of forces and motions within the system.\n\nThe conception of centrifugal force has evolved since the time of Huygens, Newton, Leibniz, and Hooke who expressed early conceptions of it. Its modern conception as a fictitious force arising in a rotating reference frame evolved in the eighteenth and nineteenth centuries.\n\nCentrifugal force has also played a role in debates in classical mechanics about detection of absolute motion. Newton suggested two arguments to answer the question of whether absolute rotation can be detected: the rotating bucket argument, and the rotating spheres argument. According to Newton, in each scenario the centrifugal force would be observed in the object's local frame (the frame where the object is stationary) only if the frame were rotating with respect to absolute space. Nearly two centuries later, Mach's principle was proposed where, instead of absolute rotation, the motion of the distant stars relative to the local inertial frame gives rise through some (hypothetical) physical law to the centrifugal force and other inertia effects. Today's view is based upon the idea of an inertial frame of reference, which privileges observers for which the laws of physics take on their simplest form, and in particular, frames that do not use centrifugal forces in their equations of motion in order to describe motions correctly.\n\nThe analogy between centrifugal force (sometimes used to create artificial gravity) and gravitational forces led to the equivalence principle of general relativity.\n\nWhile the majority of the scientific literature uses the term \"centrifugal force\" to refer to the particular fictitious force that arises in rotating frames, there are a few limited instances in the literature of the term applied to other distinct physical concepts. One of these instances occurs in Lagrangian mechanics. Lagrangian mechanics formulates mechanics in terms of generalized coordinates {\"q\"}, which can be as simple as the usual polar coordinates formula_14 or a much more extensive list of variables. Within this formulation the motion is described in terms of \"generalized forces\", using in place of Newton's laws the Euler–Lagrange equations. Among the generalized forces, those involving the square of the time derivatives {(d\"q\"   ⁄ \"dt\" )} are sometimes called centrifugal forces. In the case of motion in a central potential the Lagrangian centrifugal force has the same form as the fictitious centrifugal force derived in a co-rotating frame. However, the Lagrangian use of \"centrifugal force\" in other, more general cases has only a limited connection to the Newtonian definition.\n\nIn another instance the term refers to the reaction force to a centripetal force, or reactive centrifugal force. A body undergoing curved motion, such as circular motion, is accelerating toward a center at any particular point in time. This centripetal acceleration is provided by a centripetal force, which is exerted on the body in curved motion by some other body. In accordance with Newton's third law of motion, the body in curved motion exerts an equal and opposite force on the other body. This reactive force is exerted \"by\" the body in curved motion \"on\" the other body that provides the centripetal force and its direction is from that other body toward the body in curved motion.\nThis reaction force is sometimes described as a \"centrifugal inertial reaction\",\nthat is, a force that is centrifugally directed, which is a reactive force equal and opposite to the centripetal force that is curving the path of the mass.\n\nThe concept of the reactive centrifugal force is sometimes used in mechanics and engineering. It is sometimes referred to as just \"centrifugal force\" rather than as \"reactive\" centrifugal force\nalthough this usage is deprecated in elementary mechanics.\n\n"}
{"id": "5669", "url": "https://en.wikipedia.org/wiki?curid=5669", "title": "Chromium", "text": "Chromium\n\nChromium is a chemical element with symbol Cr and atomic number 24. It is the first element in group 6. It is a steely-grey, lustrous, hard and brittle transition metal. Chromium boasts a high usage rate as a metal that is able to be highly polished while resisting tarnishing. Chromium is also the main component of stainless steel, a popular steel alloy due to its uncommonly high specular reflection. Simple polished chromium reflects almost 70% of the visible spectrum, with almost 90% of infrared light waves being reflected. The name of the element is derived from the Greek word χρῶμα, \"chrōma\", meaning color, because many chromium compounds are intensely colored.\n\nFerrochromium alloy is commercially produced from chromite by silicothermic or aluminothermic reactions and chromium metal by roasting and leaching processes followed by reduction with carbon and then aluminium. Chromium metal is of high value for its high corrosion resistance and hardness. A major development in steel production was the discovery that steel could be made highly resistant to corrosion and discoloration by adding metallic chromium to form stainless steel. Stainless steel and chrome plating (electroplating with chromium) together comprise 85% of the commercial use.\n\nIn the United States, trivalent chromium (Cr(III)) ion is considered an essential nutrient in humans for insulin, sugar and lipid metabolism. However, in 2014, the European Food Safety Authority, acting for the European Union, concluded that there was not sufficient evidence for chromium to be recognized as essential.\n\nWhile chromium metal and Cr(III) ions are not considered toxic, hexavalent chromium (Cr(VI)) is both toxic and carcinogenic. Abandoned chromium production sites often require environmental cleanup.\n\nChromium is the fourth transition metal found on the periodic table, and has an electron configuration of [Ar] 3d 4s. It is also the first element in the periodic table whose ground-state electron configuration violates the Aufbau principle. This occurs again later in the periodic table with other elements and their electron configurations, such as copper, niobium, and molybdenum. This occurs because electrons in the same orbital repel each other due to their like charges. In the previous elements, the energetic cost of promoting an electron to the next higher energy level is too great to compensate for that released by lessening inter-electronic repulsion. However, in the 3d transition metals, the energy gap between the 3d and the next-higher 4s subshell is very small, and because the 3d subshell is more compact than the 4s subshell, inter-electron repulsion is smaller between 4s electrons than between 3d electrons. This lowers the energetic cost of promotion and increases the energy released by it, so that the promotion becomes energetically feasible and one or even two electrons are always promoted to the 4s subshell. (Similar promotions happen for every transition metal atom but one, palladium.)\n\nChromium is the first element in the 3d series where the 3d electrons are starting to sink into the inert core; they thus contribute less to metallic bonding, and hence the melting and boiling points and the enthalpy of atomisation of chromium are lower than those of the preceding element vanadium. Chromium(VI) is a strong oxidising agent in contrast to the molybdenum(VI) and tungsten(VI) oxides.\n\nChromium has an unusually high specular reflection in comparison to that of other transitional metals. At 425 μm, chromium was found to have a relative maximum reflection of about 72% reflectance, before entering a depression in reflectivity, reaching a minimum of 62% reflectance at 750 μm before rising again to reflecting roughly 90% of 4000 μm of infrared waves.. When chromium is formed into a stainless steel alloy and polished, the specular reflection decreases with the inclusion of additional metals, yet is still rather high in comparison with other alloys. Between 40% and 60% of the visible spectrum is reflected off of polished stainless steel. The explanation on why chromium displays such a high turnout of reflected photon waves in general, especially the 90% of infrared waves that were reflected, can be attributed to chromium's magnetic properties. Chromium has unique magnetic properties in the sense that chromium is the only elemental solid which shows antiferromagnetic ordering at room temperature (and below). Above 38 °C, its magnetic ordering changes to paramagnetic.. The antiferromagnetic properties, which cause the chromium atoms to temporarily ionize and bond with themselves because the body-centric cubic's magnetic properties are disproportionate to the lattice periodicity. This is due to the fact that the magnetic moments at the cube's corners and the cube centers are not equal, but still antiparallel. From here, the frequency-dependent relative permittivity of chromium, deriving from Maxwell's equations in conjunction with its antiferromagnetivity, leave chromium with one of the highest infrared and visible light reflectance out of the known chemical elements.\n\nChromium metal left standing in air is passivated by oxidation, forming a thin, protective, surface layer. This layer is a spinel structure only a few molecules thick. It is very dense, and prevents the diffusion of oxygen into the underlying metal. This is different from the oxide that forms on iron and carbon steel, through which elemental oxygen continues to migrate, reaching the underlying material to cause incessant rusting. Passivation can be enhanced by short contact with oxidizing acids like nitric acid. Passivated chromium is stable against acids. Passivation can be removed with a strong reducing agent that destroys the protective oxide layer on the metal. Chromium metal treated in this way readily dissolves in weak acids.\n\nChromium, unlike such metals as iron and nickel, does not suffer from hydrogen embrittlement. However, it does suffer from nitrogen embrittlement, reacting with nitrogen from air and forming brittle nitrides at the high temperatures necessary to work the metal parts.\n\nNaturally occurring chromium is composed of three stable isotopes; Cr, Cr and Cr, with Cr being the most abundant (83.789% natural abundance). 19 radioisotopes have been characterized, with the most stable being Cr with a half-life of (more than) 1.8 years, and Cr with a half-life of 27.7 days. All of the remaining radioactive isotopes have half-lives that are less than 24 hours and the majority less than 1 minute. This element also has 2 meta states.\n\nCr is the radiogenic decay product of Mn (half-life = 3.74 million years). Chromium isotopes are typically collocated (and compounded) with manganese isotopes. This circumstance is useful in isotope geology. Manganese-chromium isotope ratios reinforce the evidence from Al and Pd concerning the early history of the solar system. Variations in Cr/Cr and Mn/Cr ratios from several meteorites indicate an initial Mn/Mn ratio that suggests Mn-Cr isotopic composition must result from in-situ decay of Mn in differentiated planetary bodies. Hence Cr provides additional evidence for nucleosynthetic processes immediately before coalescence of the solar system.\n\nThe isotopes of chromium range in atomic mass from 43 u (Cr) to 67 u (Cr). The primary decay mode before the most abundant stable isotope, Cr, is electron capture and the primary mode after is beta decay. Cr has been posited as a proxy for atmospheric oxygen concentration.\n\nChromium is a member of group 6, of the transition metals. Chromium(0) has an electron configuration of [Ar]3d4s, owing to the lower energy of the high spin configuration. Chromium exhibits a wide range of oxidation states, but chromium being ionized into a cation with a positive 3 charge serves as chromium's most stable ionic state. The +3 and +6 states occur the most commonly within chromium compounds; charges of +1, +4 and +5 for chromium are rare, but nevertheless due occasionally exist for chromium.\n\nA large number of chromium(III) compounds are known, such as chromium(III) nitrate, chromium(III) acetate, and chromium(III) oxide. Chromium(III) can be obtained by dissolving elemental chromium in acids like hydrochloric acid or sulfuric acid, but it can also be formed through the reduction of chromium(VI) by cytochrome c7. The ion has a similar radius (63 pm) to (radius 50 pm), and they can replace each other in some compounds, such as in chrome alum and alum. When a trace amount of replaces in corundum (aluminium oxide, AlO), pink sapphire or red-colored ruby is formed, depending on the amount of chromium.\n\nChromium(III) tends to form octahedral complexes. Commercially available chromium(III) chloride hydrate is the dark green complex [CrCl(HO)]Cl. Closely related compounds are the pale green [CrCl(HO)]Cl and violet [Cr(HO)]Cl. If water-free green chromium(III) chloride is dissolved in water, the green solution turns violet after some time as the chloride in the inner coordination sphere is replaced by water. This kind of reaction is also observed with solutions of chrome alum and other water-soluble chromium(III) salts.\n\nChromium(III) hydroxide (Cr(OH)) is amphoteric, dissolving in acidic solutions to form [Cr(HO)], and in basic solutions to form . It is dehydrated by heating to form the green chromium(III) oxide (CrO), a stable oxide with a crystal structure identical to that of corundum.\n\nChromium(VI) compounds are oxidants at low or neutral pH. Chromate anions () and dichromate (CrO) anions are the principal ions at this oxidation state. They exist at an equilibrium, determined by pH:\nChromium(VI) halides are known also and include the hexafluoride CrF and chromyl chloride ().\nSodium chromate is produced industrially by the oxidative roasting of chromite ore with calcium or sodium carbonate. The change in equilibrium is visible by a change from yellow (chromate) to orange (dichromate), such as when an acid is added to a neutral solution of potassium chromate. At yet lower pH values, further condensation to more complex oxyanions of chromium is possible.\n\nBoth the chromate and dichromate anions are strong oxidizing reagents at low pH:\n\nThey are, however, only moderately oxidizing at high pH:\nChromium(VI) compounds in solution can be detected by adding an acidic hydrogen peroxide solution. The unstable dark blue chromium(VI) peroxide (CrO) is formed, which can be stabilized as an ether adduct .\n\nChromic acid has the hypothetical formula . It is a vaguely described chemical, despite many well-defined chromates and dichromates being known. The dark red chromium(VI) oxide , the acid anhydride of chromic acid, is sold industrially as \"chromic acid\". It can be produced by mixing sulfuric acid with dichromate, and is a strong oxidizing agent.\n\nThe oxidation state +5 is only realized in few compounds but are intermediates in many reactions involving oxidations by chromate. The only binary compound is the volatile chromium(V) fluoride (CrF). This red solid has a melting point of 30 °C and a boiling point of 117 °C. It can be prepared by treating chromium metal with fluorine at 400 °C and 200 bar pressure. The peroxochromate(V) is another example of the +5 oxidation state. Potassium peroxochromate (K[Cr(O)]) is made by reacting potassium chromate with hydrogen peroxide at low temperatures. This red brown compound is stable at room temperature but decomposes spontaneously at 150–170 °C.\n\nCompounds of chromium(IV) (in the +4 oxidation state) are slightly more common than those of chromium(V). The tetrahalides, CrF, CrCl, and CrBr, can be produced by treating the trihalides () with the corresponding halogen at elevated temperatures. Such compounds are susceptible to disproportionation reactions and are not stable in water.\n\nMany chromium(II) compounds are known, such as the water-stable chromium(II) chloride that can be made by reducing chromium(III) chloride with zinc. The resulting bright blue solution created from dissolving chromium(II) chloride is only stable at neutral pH. Some other notable chromium(II) compounds include chromium(II) oxide , and chromium(II) sulfate . Many chromous carboxylates are known as well, the most famous of these being the red chromium(II) acetate (Cr(OCCH)) that features a quadruple bond.\n\nMost chromium(I) compounds are obtained solely by oxidation of electron-rich, octahedral chromium(0) complexes. Other chromium(I) complexes contain cyclopentadienyl ligands. As verified by X-ray diffraction, a Cr-Cr quintuple bond (length 183.51(4)  pm) has also been described. Extremely bulky monodentate ligands stabilize this compound by shielding the quintuple bond from further reactions.\n\nMany chromium(0) compounds are currently known; however, most of these compounds are derivatives of the compounds chromium hexacarbonyl or bis(benzene)chromium.\n\nChromium is the 13th most abundant element in Earth's crust with an average concentration of 100 ppm. Chromium compounds are found in the environment from the erosion of chromium-containing rocks, and can be redistributed by volcanic eruptions. Typical background concentrations of chromium in environmental media are: atmosphere <10 ng m; soil <500 mg kg; vegetation <0.5 mg kg; freshwater <10 ug L; seawater <1 ug L; sediment <80 mg kg.\n\nChromium is mined as chromite (FeCrO) ore. About two-fifths of the chromite ores and concentrates in the world are produced in South Africa, about a third in Kazakhstan, while India, Russia, and Turkey are also substantial producers. Untapped chromite deposits are plentiful, but geographically concentrated in Kazakhstan and southern Africa.\n\nAlthough rare, deposits of native chromium exist. The Udachnaya Pipe in Russia produces samples of the native metal. This mine is a kimberlite pipe, rich in diamonds, and the reducing environment helped produce both elemental chromium and diamond.\n\nThe relation between Cr(III) and Cr(VI) strongly depends on pH and oxidative properties of the location. In most cases, Cr(III) is the dominating species, but in some areas, the ground water can contain up to 39 µg/liter of total chromium of which 30 µg/liter is Cr(VI).\n\nChromium was first discovered as an element after it came to the attention of the Western world in the red crystalline mineral crocoite (which is lead(II) chromate). This mineral was discovered in 1761 and was initially used as a pigment; the distinctive color was attributed to the chromium from within the crocoite. In present day, nearly all chromium is commercially extracted from the only viable ore for extensiveness and predicted long term use, being chromite, which is iron chromium oxide (FeCrO); chromite is now the principal source of chromium for use in pigments.\n\nWeapons found in burial pits dating from the late 3rd century B.C. Qin Dynasty of the Terracotta Army near Xi'an, China, have been analyzed by archaeologists. Although these weapons were presumably buried more than two millennia ago, the ancient bronze tips of both the swords and crossbow bolts found at the site showed unexpectedly little corrosion, possibly because the bronze was deliberately coated with a thin layer of chromium oxide. Still, this oxide layer that was found on the weapons was not pure chromium metal or chrome plating as it is commonly produced today, but a mere 10-15 μm layer of chromium oxide molecules at up to 2% chromium was discovered, which turned out to be enough to protect the bronze from corroding.\n\nChromium minerals as pigments came to the attention of the west in the 18th century. On 26 July 1761, Johann Gottlob Lehmann found an orange-red mineral in the Beryozovskoye mines in the Ural Mountains which he named \"Siberian red lead\". Though misidentified as a lead compound with selenium and iron components, the mineral was in fact crocoite (or lead(II) chromate) with a formula of PbCrO. In 1770, Peter Simon Pallas visited the same site as Lehmann and found a red lead mineral that was discovered to possess useful properties as a pigment in paints. After Pallas, the use of Siberian red lead as a paint pigment began to develop rapidly throughout the region.\n\nIn 1794, Louis Nicolas Vauquelin received samples of crocoite ore. He produced chromium trioxide (CrO) by mixing crocoite with hydrochloric acid. In 1797, Vauquelin discovered that he could isolate metallic chromium by heating the oxide in a charcoal oven, for which he is credited as the one who truly discovered the element. Vauquelin was also able to detect traces of chromium in precious gemstones, such as ruby or emerald.\n\nDuring the 19th century, chromium was primarily used not only as a component of paints, but in tanning salts as well. For quite some time, the crocoite found in Russia was the main source for such tanning materials. In 1827, a larger chromite deposit was discovered near Baltimore, United States, which quickly met the demand for tanning salts much more adequately than the crocoite that had been used previously. This made the United States the largest producer of chromium products until the year 1848, when larger deposits of chromite were uncovered near the city of Bursa, Turkey.\n\nChromium is also famous for its reflective, metallic luster when polished. It is used as a protective and decorative coating on car parts, plumbing fixtures, furniture parts and many other items, usually applied by electroplating. Chromium was used for electroplating as early as 1848, but this use only became widespread with the development of an improved process in 1924.\n\nApproximately 28.8 million metric tons (Mt) of marketable chromite ore was produced in 2013, and converted into 7.5 Mt of ferrochromium. According to John F. Papp, writing for the USGS, \"Ferrochromium is the leading end use of chromite ore, [and] stainless steel is the leading end use of ferrochromium.\"\n\nThe largest producers of chromium ore in 2013 have been South Africa (48%), Kazakhstan (13%), Turkey (11%), India (10%) with several other countries producing the rest of about 18% of the world production.\n\nThe two main products of chromium ore refining are ferrochromium and metallic chromium. For those products the ore smelter process differs considerably. For the production of ferrochromium, the chromite ore (FeCrO) is reduced in large scale in electric arc furnace or in smaller smelters with either aluminium or silicon in an aluminothermic reaction.\n\nFor the production of pure chromium, the iron must be separated from the chromium in a two step roasting and leaching process. The chromite ore is heated with a mixture of calcium carbonate and sodium carbonate in the presence of air. The chromium is oxidized to the hexavalent form, while the iron forms the stable FeO. The subsequent leaching at higher elevated temperatures dissolves the chromates and leaves the insoluble iron oxide. The chromate is converted by sulfuric acid into the dichromate.\n\nThe dichromate is converted to the chromium(III) oxide by reduction with carbon and then reduced in an aluminothermic reaction to chromium.\n\nThe creation of metal alloys account for 85% of the available chromium's usage. The remainder of chromium is used in the chemical, refractory, and foundry industries.\n\nThe strengthening effect of forming stable metal carbides at the grain boundaries and the strong increase in corrosion resistance made chromium an important alloying material for steel. The high-speed tool steels contain between 3 and 5% chromium. Stainless steel, the primary corrosion-resistant metal alloy, is formed when chromium is introduced to iron in sufficient concentrations, usually where the chromium concentration is above 11%. For stainless steel's formation, ferrochromium is added to the molten iron. Also, nickel-based alloys increase in strength due to the formation of discrete, stable metal carbide particles at the grain boundaries. For example, Inconel 718 contains 18.6% chromium. Because of the excellent high-temperature properties of these nickel superalloys, they are used in jet engines and gas turbines in lieu of common structural materials.\n\nThe relative high hardness and corrosion resistance of unalloyed chromium makes chrome a reliable metal for surface coating; it is still the most popular metal concerning sheet coating with its above average durability compared to other coating metals. A layer of chromium is deposited on pretreated metallic surfaces by electroplating techniques. There are two deposition methods: thin and thick. Thin deposition involves a layer of chromium below 1 µm thickness deposited by chrome plating, and are used for decorative surfaces. Thicker chromium layers are deposited if wear-resistant surfaces are needed. Both methods use acidic chromate or dichromate solutions. To prevent the energy-consuming change in oxidation state, the use of chromium(III) sulfate is under development; for most applications of chromium, the previously established process is used.\n\nIn the chromate conversion coating process, the strong oxidative properties of chromates are used to deposit a protective oxide layer on metals like aluminium, zinc and cadmium. This passivation and the self-healing properties by the chromate stored in the chromate conversion coating, which is able to migrate to local defects, are the benefits of this coating method. Because of environmental and health regulations on chromates, alternative coating methods are under development.\n\nChromic acid anodizing (or Type I anodizing) of aluminium is another electrochemical process, which does not lead to the deposition of chromium, but uses chromic acid as electrolyte in the solution. During anodization, an oxide layer is formed on the aluminium. The use of chromic acid, instead of the normally used sulfuric acid, leads to a slight difference of these oxide layers.\nThe high toxicity of Cr(VI) compounds, used in the established chromium electroplating process, and the strengthening of safety and environmental regulations demand a search for substitutes for chromium or at least a change to less toxic chromium(III) compounds.\n\nThe mineral crocoite (which is also lead chromate PbCrO) was used as a yellow pigment shortly after its discovery. After a synthesis method became available starting from the more abundant chromite, chrome yellow was, together with cadmium yellow, one of the most used yellow pigments. The pigment does not photodegrade, but it tends to darken due to the formation of chromium(III) oxide. It has a strong color, and was used for school buses in the United States and for the Postal Service (for example, the Deutsche Post) in Europe. The use of chrome yellow has since declined due to environmental and safety concerns and was replaced by organic pigments or other alternatives that are free from lead and chromium. Other pigments that are based around chromium are, for example, the deep shade of red pigment chrome red, which is simply lead chromate with lead(II) hydroxide (PbCrO·Pb(OH)). A very important chromate pigment, which was used widely in metal primer formulations, was zinc chromate, now replaced by zinc phosphate. A wash primer was formulated to replace the dangerous practice of pre-treating aluminium aircraft bodies with a phosphoric acid solution. This used zinc tetroxychromate dispersed in a solution of polyvinyl butyral. An 8% solution of phosphoric acid in solvent was added just before application. It was found that an easily oxidized alcohol was an essential ingredient. A thin layer of about 10–15 µm was applied, which turned from yellow to dark green when it was cured. There is still a question as to the correct mechanism. Chrome green is a mixture of Prussian blue and chrome yellow, while the chrome oxide green is chromium(III) oxide.\n\nChromium oxides are also used as a green pigment in the field of glassmaking and also as a glaze for ceramics. Green chromium oxide is extremely lightfast and as such is used in cladding coatings. It is also the main ingredient in infrared reflecting paints, used by the armed forces to paint vehicles and to give them the same infrared reflectance as green leaves.\n\nNatural rubies are corundum (aluminum oxide) crystals that are colored red (the rarest type) due to chromium (III) ions (other colors of corundum gems are termed sapphires). A red-colored artificial ruby may also be achieved by doping chromium(III) into artificial corundum crystals, thus making chromium a requirement for making synthetic rubies. Such a synthetic ruby crystal was the basis for the first laser, produced in 1960, which relied on stimulated emission of light from the chromium atoms in such a crystal. A ruby laser is lasing at 694.3 nanometers, in a deep red color.\n\nBecause of their toxicity, chromium(VI) salts are used for the preservation of wood. For example, chromated copper arsenate (CCA) is used in timber treatment to protect wood from decay fungi, wood-attacking insects, including termites, and marine borers. The formulations contain chromium based on the oxide CrO between 35.3% and 65.5%. In the United States, 65,300 metric tons of CCA solution were used in 1996.\n\nChromium(III) salts, especially chrome alum and chromium(III) sulfate, are used in the tanning of leather. The chromium(III) stabilizes the leather by cross linking the collagen fibers. Chromium tanned leather can contain between 4 and 5% of chromium, which is tightly bound to the proteins. Although the form of chromium used for tanning is not the toxic hexavalent variety, there remains interest in management of chromium in the tanning industry such as recovery and reuse, direct/indirect recycling, use of less chromium or \"chrome-less\" tanning are practiced to better manage chromium in tanning.\n\nThe high heat resistivity and high melting point makes chromite and chromium(III) oxide a material for high temperature refractory applications, like blast furnaces, cement kilns, molds for the firing of bricks and as foundry sands for the casting of metals. In these applications, the refractory materials are made from mixtures of chromite and magnesite. The use is declining because of the environmental regulations due to the possibility of the formation of chromium(VI). \n\nSeveral chromium compounds are used as catalysts for processing hydrocarbons. For example, the Phillips catalyst, prepared from chromium oxides, is used for the production of about half the world's polyethylene. Fe-Cr mixed oxides are employed as high-temperature catalysts for the water gas shift reaction. Copper chromite is a useful hydrogenation catalyst.\n\n\nIn the form trivalent chromium, Cr(III), or Cr, chromium was tentatively identified as an essential nutrient in the late 1950s and later accepted as a trace element for its roles in the action of insulin, a hormone critical to the metabolism and storage of carbohydrate, fat and protein. The precise mechanism of its actions in the body, however, have not been fully defined, leaving in question whether chromium is essential for healthy people. \n\nTrivalent chromium occurs in trace amounts in foods, wine and water. In contrast, hexavalent chromium (Cr(VI) or Cr) is highly toxic and mutagenic when inhaled. Ingestion of chromium(VI) in water has been linked to stomach tumors, and it may also cause allergic contact dermatitis (ACD).\n\nChromium deficiency, involving a lack of Cr(III) in the body, or perhaps some complex of it, such as glucose tolerance factor is controversial. Some studies suggest that the biologically active form of chromium (III) is transported in the body via an oligopeptide called low-molecular-weight chromium-binding substance (LMWCr), which might play a role in the insulin signaling pathway.\n\nChromium content of common foods is generally low (1-13 micrograms per serving). Chromium content of food varies widely due to differences in soil mineral content, growing season, plant cultivar, and contamination during processing. In addition, chromium (and nickel) leach into food cooked in stainless steel, with the effect largest when the cookware is new. Acidic foods such as tomato sauce which are cooked for many hours also exacerbate this effect.\n\nThere is disagreement on chromium's status as an essential nutrient. Governmental departments from Australia, New Zealand, India, Japan and the United States consider chromium essential while the European Food Safety Authority (EFSA), representing the European Union, does not.\n\nThe National Academy of Medicine (NAM) updated the Estimated Average Requirements (EARs) and the Recommended Dietary Allowances (RDAs) for chromium in 2001. For chromium, there was not sufficient information to set EARs and RDAs, so its needs are described as estimates for Adequate Intakes (AIs). The current AIs of chromium for women ages 14 through 50 is 25 μg/day, and the AIs for women ages 50 and above is 20 μg/day. The AIs for women who are pregnant are 30 μg/day, and for women who are lactating, the set AIs are 45 μg/day. The AIs for men ages 14 through 50 are 35 μg/day, and the AIs for men ages 50 and above are 30 μg/day. For children ages 1 through 13, the AIs increase with age from 0.2 μg/day up to 25 μg/day. As for safety, the NAM sets Tolerable Upper Intake Levels (ULs) for vitamins and minerals when the evidence is sufficient. In the case of chromium, there is not yet enough information and hence no UL has been established. Collectively, the EARs, RDAs, AIs and ULs are the parameters for the nutrition recommendation system known as Dietary Reference Intake (DRI). Australia and New Zealand consider chromium to be an essential nutrient, with an AI of 35 μg/day for men, 25 μg/day for women, 30 μg/day for women who are pregnant, and 45 μg/day for women who are lactating. A UL has not been set due to the lack of sufficient data. India considers chromium to be an essential nutrient, with an adult recommended intake of 33 μg/day. Japan also considers chromium to be an essential nutrient, with an AI of 10 μg/day for adults, including women who are pregnant or lactating. A UL has not been set. The EFSA of the European Union however, does not consider chromium to be an essential nutrient; chromium is the only mineral for which the United States and the European Union disagree.\n\nFor the United States' food and dietary supplement labeling purposes, the amount of the substance in a serving is expressed as a percent of the Daily Value (%DV). For chromium labeling purposes, 100% of the Daily Value was 120 μg. As of May 27, 2016, the percentage of daily value was revised to 35 μg to bring the chromium intake into a consensus with the official Recommended Dietary Allowance. The original deadline to be in compliance was July 28, 2018, but on September 29, 2017 the Food and Drug Administration released a proposed rule that extended the deadline to January 1, 2020 for large companies and January 1, 2021 for small companies.\n\nFood composition databases such as the those maintained by the U.S. Department of Agriculture do not contain information on the chromium content of foods. A wide variety of animal-sourced and vegetable-sourced foods contain chromium. Content per serving is influenced by the chromium content of the soil in which the plants are grown and by feedstuffs fed to animals; also by processing methods, as chromium is leached into foods if processed or cooked in chromium-containing stainless steel equipment. One diet analysis study conducted in Mexico reported an average daily chromium intake of 30 micrograms. An estimated 31% of adults in the United States consume multi-vitamin/mineral dietary supplements which often contain 25 to 60 micrograms of chromium.\n\nChromium is an ingredient in total parenteral nutrition (TPN) because deficiency can occur after months of intravenous feeding with chromium-free TPN. For this reason, chromium is added to TPN solutions, along with other trace minerals. It is also in nutritional products for preterm infants. Although the mechanism in biological roles for chromium is unclear, in the United States chromium-containing products are sold as non-prescription dietary supplements in amounts ranging from 50 to 1,000 μg. Lower amounts of chromium are also often incorporated into multi-vitamin/mineral supplements consumed by an estimated 31% of adults in the United States. Chemical compounds used in dietary supplements include chromium chloride, chromium citrate, chromium(III) picolinate, chromium(III) polynicotinate, and other chemical compositions. The benefit of supplements has not been proven.\n\nIn 2005, the U.S. Food and Drug Administration had approved a Qualified Health Claim for chromium picolinate with a requirement for very specific label wording: \"One small study suggests that chromium picolinate may reduce the risk of insulin resistance, and therefore possibly may reduce the risk of type 2 diabetes. FDA concludes, however, that the existence of such a relationship between chromium picolinate and either insulin resistance or type 2 diabetes is highly uncertain.\" At the same time, in answer to other parts of the petition, the FDA rejected claims for chromium picolinate and cardiovascular disease, retinopathy or kidney disease caused be abnormally high blood sugar levels. In 2010, chromium(III) picolinate was approved by Health Canada to be used in dietary supplements. Approved labeling statements include: a factor in the maintenance of good health, provides support for healthy glucose metabolism, helps the body to metabolize carbohydrates and helps the body to metabolize fats. The European Food Safety Authority (EFSA) approved claims in 2010 that chromium contributed to normal macronutrient metabolism and maintenance of normal blood glucose concentration, but rejected claims for maintenance or achievement of a normal body weight, or reduction of tiredness or fatigue.\n\nGiven the evidence for chromium deficiency causing problems with glucose management in the context of intravenous nutrition products formulated without chromium, research interest turned to whether chromium supplementation for people who have type 2 diabetes but are not chromium deficient could benefit. Looking at the results from four meta-analyses, one reported a statistically significant decrease in fasting plasma glucose levels (FPG) and a non-significant trend in lower hemoglobin A1C. A second reported the same, a third reported significant decreases for both measures, while a fourth reported no benefit for either. A review published in 2016 listed 53 randomized clinical trials that were included in one or more of six meta-analyses. It concluded that whereas there may be modest decreases in FPG and/or HbA1C that achieve statistical significance in some of these meta-analyses, few of the trials achieved decreases large enough to be expected to be relevant to clinical outcome.\n\nTwo systematic reviews looked at chromium supplements as a mean of managing body weight in overweight and obese people. One, limited to chromium picolinate, a popular supplement ingredient, reported a statistically significant -1.1 kg (2.4 lb) weight loss in trials longer than 12 weeks. The other included all chromium compounds and reported a statistically significant -0.50 kg (1.1 lb) weight change. Change in percent body fat did not reach statistical significance. Authors of both reviews considered the clinical relevance of this modest weight loss as uncertain/unreliable. The European Food Safety Authority reviewed the literature and concluded that there was insufficient evidence to support a claim.\n\nChromium is promoted as a sports performance dietary supplement, based on the theory that it potentiated insulin activity, with anticipated results of increased muscle mass, and faster recovery of glycogen storage during post-exercise recovery. A review of clinical trials reported that chromium supplementation did not improve exercise performance or increase muscle strength. The International Olympic Committee reviewed dietary supplements for high-performance athletes in 2018 and concluded there was no need to increase chromium intake for athletes, nor support for claims of losing body fat.\n\nWater-insoluble chromium(III) compounds and chromium metal are not considered a health hazard, while the toxicity and carcinogenic properties of chromium(VI) have been known for a long time. Because of the specific transport mechanisms, only limited amounts of chromium(III) enter the cells. Several \"in vitro\" studies indicated that high concentrations of chromium(III) in the cell can lead to DNA damage. Acute oral toxicity ranges between 1.5 and 3.3 mg/kg. A 2008 review suggested that moderate uptake of chromium(III) through dietary supplements poses no genetic-toxic risk. In the US, the Occupational Safety and Health Administration (OSHA) has designated a permissible exposure limit (PEL) in the workplace as a time-weighted average (TWA) of 1 mg/m. The National Institute for Occupational Safety and Health (NIOSH) has set a recommended exposure limit (REL) of 0.5 mg/m, time-weighted average. The IDLH (immediately dangerous to life and health) value is 250 mg/m.\n\nThe acute oral toxicity for chromium(VI) ranges between 50 and 150 mg/kg. In the body, chromium(VI) is reduced by several mechanisms to chromium(III) already in the blood before it enters the cells. The chromium(III) is excreted from the body, whereas the chromate ion is transferred into the cell by a transport mechanism, by which also sulfate and phosphate ions enter the cell. The acute toxicity of chromium(VI) is due to its strong oxidational properties. After it reaches the blood stream, it damages the kidneys, the liver and blood cells through oxidation reactions. Hemolysis, renal, and liver failure result. Aggressive dialysis can be therapeutic.\n\nThe carcinogenity of chromate dust has been known for a long time, and in 1890 the first publication described the elevated cancer risk of workers in a chromate dye company. Three mechanisms have been proposed to describe the genotoxicity of chromium(VI). The first mechanism includes highly reactive hydroxyl radicals and other reactive radicals which are by products of the reduction of chromium(VI) to chromium(III). The second process includes the direct binding of chromium(V), produced by reduction in the cell, and chromium(IV) compounds to the DNA. The last mechanism attributed the genotoxicity to the binding to the DNA of the end product of the chromium(III) reduction.\n\nChromium salts (chromates) are also the cause of allergic reactions in some people. Chromates are often used to manufacture, amongst other things, leather products, paints, cement, mortar and anti-corrosives. Contact with products containing chromates can lead to allergic contact dermatitis and irritant dermatitis, resulting in ulceration of the skin, sometimes referred to as \"chrome ulcers\". This condition is often found in workers that have been exposed to strong chromate solutions in electroplating, tanning and chrome-producing manufacturers.\n\nBecause chromium compounds were used in dyes, paints, and leather tanning compounds, these compounds are often found in soil and groundwater at active and abandoned industrial sites, needing environmental cleanup and remediation. Primer paint containing hexavalent chromium is still widely used for aerospace and automobile refinishing applications.\n\nIn 2010, the Environmental Working Group studied the drinking water in 35 American cities in the first nationwide study. The study found measurable hexavalent chromium in the tap water of 31 of the cities sampled, with Norman, Oklahoma, at the top of list; 25 cities had levels that exceeded California's proposed limit.\n\n\n"}
{"id": "6571624", "url": "https://en.wikipedia.org/wiki?curid=6571624", "title": "Composition H6", "text": "Composition H6\n\nComposition H6 is a castable military explosive mixture composed of the following percentages by weight: \n\nH6 is used in a number of military applications, notably underwater munitions (e.g. naval mines, depth charges and torpedoes) where it has generally replaced torpex, being less shock sensitive and having more stable storage characteristics. It is approximately 1.35 times more powerful than pure TNT.\n\n\n"}
{"id": "24759054", "url": "https://en.wikipedia.org/wiki?curid=24759054", "title": "Conductor marking lights", "text": "Conductor marking lights\n\nConductor marking lights are a particular type of aircraft warning lights designed for overhead power lines. They are visible at night, unlike overhead wire markers which rely on daylight. \nPower distribution on overhead power lines, often suspended to widely spaced masts, represents an almost invisible obstacle to low flying aircraft, requiring the installation of warning beacons on the masts themselves. \nA simple and cost-effective solution to this problem is installing the marking lights directly on the wires. \nNevertheless, there are significant technical difficulties to a low-cost extraction of power from a distribution system which carries high voltages and wide-range AC current. \nMoreover, the warning system should not add to the existing power distribution lines the burden of additional cabling, with its potential serious isolation problems. These facts rule out the recourse to conventionally powered light sources.\n\nThe ideal warning light must be able to power itself while clamped to a single wire of the line. Lights may be powered either from the electric field surrounding the energized wire, or the magnetic field produced by current through the wire. The first approach takes advantage of the high electric potential gradient between conductors, but a strong enough capacitive coupling is requested to allow capacitive extraction of the power required from the warning light. This means that long conductors must be suspended parallel to the line using glass/ceramic isolators: in fact several meters of suspended conductor are generally required, total length being inversely proportional to the line voltage. The second approach is based on Faraday's law of induction involving magnetic flux flowing through a circuit which powers the warning light.\n\nThis beacon is powered by the magnetic field surrounding the power distribution wire and uses an electronic circuit integrated in a compact clamp-on warning light. The operating principle is that of a Rogowski coil, similar to a current transformer. This solution is usually intended for medium and high voltage lines up to 440 kV. However inductive coupling devices are able to work on any AC at 50 Hz or 60 Hz, from 15 up to 2000 Amps.\n\n\n"}
{"id": "29820802", "url": "https://en.wikipedia.org/wiki?curid=29820802", "title": "Coronal radiative losses", "text": "Coronal radiative losses\n\nIn astronomy and in astrophysics, for radiative losses of the solar corona, it is meant the energy flux radiated from the external atmosphere of the Sun (traditionally divided into chromosphere, transition region and corona), and, in particular, the processes of production of the radiation coming from the solar corona and transition region, where the plasma is optically-thin. On the contrary, in the chromosphere, where the temperature decreases from the photospheric value of 6000 K to the minimum of 4400 K, the optical depth is about 1, and the radiation is thermal.\n\nThe corona extends much further than a solar radius from the photosphere and looks very complex and inhomogeneous in the X-rays images taken by satellites (see the figure on the right taken by the XRT on board Hinode).\nThe structure and dynamics of the corona are dominated by the solar magnetic field. There are strong evidences that even the heating mechanism, responsible for its high temperature of million degrees, is linked to the magnetic field of the Sun.\n\nThe energy flux irradiated from the corona changes in active regions, in the quiet Sun and in coronal holes; actually, part of the energy is irradiated outwards, but approximately the same amount of the energy flux is conducted back towards the chromosphere, through the steep transition region. In active regions the energy flux is about 10 erg cmsec, in the quiet Sun it is roughly 8 10 – 10 erg cmsec, and in coronal holes 5 10 - 8 10 erg cmsec, including the losses due to the solar wind.\n\nThe required power is a small fraction of the total flux irradiated from the Sun, but this energy is enough to maintain the plasma at the temperature of million degrees, since the density is very low and the processes of radiation are different from those occurring in the photosphere, as it is shown in detail in the next section.\n\nThe electromagnetic waves coming from the solar corona are emitted mainly in the X-rays. This radiation is not visible from the Earth because it is filtered by the atmosphere. Before the first rocket missions, the corona could be observed only in white light during the eclipses, while in the last fifty years the solar corona has been photographed in the EUV and X-rays by many satellites (Pioneer 5, 6, 7, 8, 9, Helios, Skylab, SMM, NIXT, Yohkoh, SOHO, TRACE, Hinode).\n\nThe emitting plasma is almost completely ionized and very light, its density is about 10 - 10 g/cm. Particles are so isolated that almost all the photons can leave the Sun's surface without interacting with the matter above the photosphere: in other words, the corona is transparent to the radiation and the emission of the plasma is optically-thin. The Sun's atmosphere is not the unique example of X-ray source, since hot plasmas are present wherever in the Universe: from stellar coronae to thin galactic halos. These stellar environments are the subject of the X-ray astronomy.\n\nIn an optically-thin plasma the matter is not in thermodynamical equilibrium with the radiation, because collisions between particles and photons are very rare, and, as a matter of fact, the square root mean velocity of photons, electrons, protons and ions is not the same: we should define a temperature for each of these particle populations. The result is that the emission spectrum does not fit the spectral distribution of a blackbody radiation, but it depends only on those collisional processes which occur in a very rarefied plasma.\nWhile the Fraunhofer lines coming from the photosphere are absorption lines, principally emitted from ions which absorb photons of the same frequency of the transition to an upper energy level, coronal lines are emission lines produced by metal ions which had been excited to a superior state by collisional processes. Many spectral lines are emitted by highly ionized atoms, like calcium and iron, which have lost most of their external electrons; these emission lines can be formed only at certain temperatures, and therefore their individuation in solar spectra is sufficient to determine the temperature of the emitting plasma.\n\nSome of these spectral lines can be forbidden on the Earth: in fact, collisions between particles can excite ions to metastable states; in a dense gas these ions immediately collide with other particles and so they de-excite with an allowed transition to an intermediate level, while in the corona it is more probable that this ion remains in its metastable state, until it encounters a photon of the same frequency of the forbidden transition to the lower state. This photon induces the ion to emit with the same frequency by stimulated emission. Forbidden transitions from metastable states are often called as satellite lines.\n\nThe Spectroscopy of the corona allows the determination of many physical parameters of the emitting plasma. Comparing the intensity in lines of different ions of the same element, temperature and density can be measured with a good approximation: the different states of ionization are regulated by the Saha equation.\nThe Doppler shift gives a good measurement of the velocities along the line of sight but not in the perpendicular plane.\nThe line width should depend on the Maxwell–Boltzmann distribution of velocities at the temperature of line formation (thermal line broadening), while it is often larger than predicted.\nThe widening can be due to pressure broadening, when collisions between particles are frequent, or it can be due to turbulence: in this case the line width can be used to estimate the macroscopic velocity also on the Sun's surface, but with a great uncertainty.\nThe magnetic field can be measured thanks to the line splitting due to the Zeeman effect.\n\nThe most important processes of radiation for an optically-thin plasma\n\nare\n\nTherefore, the radiative flux can be expressed as the sum of three terms:\n\nformula_1\n\nwhere formula_2 is the number of electrons per unit volume, formula_3 the ion number density, formula_4 the Planck constant, formula_5 the frequency of the emitted radiation corresponding to the energy jump formula_6, formula_7 the coefficient of collisional de-excitation relative to the ion transition, formula_8 the radiative losses for plasma recombination and formula_9 the bremsstrahlung contribution.\n\nThe first term is due to the emission in every single spectral line. With a good approximation, the number of occupied states at the superior level formula_10 and the number of states at the inferior energy level formula_11 are given by the equilibrium between collisional excitation and spontaneous emission\n\nformula_12\n\nwhere formula_13\nis the transition probability of spontaneous emission.\n\nThe second term formula_8 is calculated as the energy emitted per unit volume and time when free electrons are captured from ions to recombinate into neutral atoms (dielectronic capture).\n\nThe third term formula_9 is due to the electron scattering by protons and ions because of the Coulomb force: every accelerated charge emits radiation according to classical elettrodynamics. This effect gives an appreciable contribution to the continuum spectrum only at the highest temperatures, above 10 MK.\n\nTaking into account all the dominant radiation processes, including satellite lines from metastable states, the emission of an optically-thin plasma can be expressed more simply as\n\nformula_16\n\nwhere formula_17 depends only on the temperature. In fact, all the radiation mechanisms require collisional processes and basically depend on the squared density (formula_18). The integral of the squared density along the line of sight is called emission measure and is often used in X-ray astronomy.\nThe function formula_17 has been modeled by many authors but many discrepancies are still in these calculations: differences derive essentially on the spectral lines they include in their models and on the atomic parameters they use.\n\nIn order to calculate the radiative flux from an optically-thin plasma, it can be used the linear fitting applied to some model calculations by Rosner et al. (1978)\nIn c.g.s. unit, in erg cm s, the function P(T) can be approximated as:\n\nformula_20\n\nformula_21\n\nformula_22\n\nformula_23\n\nformula_24\n\nformula_25\n\n\n\n\n\n"}
{"id": "44006839", "url": "https://en.wikipedia.org/wiki?curid=44006839", "title": "ECS Solid State Letters", "text": "ECS Solid State Letters\n\nECS Solid State Letters (SSL) is a peer-reviewed scientific journal covering the field of solid state science and technology. The journal was established in 2012 and is published by the Electrochemical Society. SSL ceased publication at the end of 2015. The editor-in-chief was Dennis W. Hess (Georgia Institute of Technology). According to the \"Journal Citation Reports\", the journal has a 2014 impact factor of 1.162.\n"}
{"id": "1847022", "url": "https://en.wikipedia.org/wiki?curid=1847022", "title": "Electric vehicle conversion", "text": "Electric vehicle conversion\n\nIn automobile engineering, electric vehicle conversion is the replacement of a car's combustion engine and connected components with an electric motor and batteries, to create an all-electric vehicle. Another option is to replace a large combustion engine with an electric motor (for power) and a small combustion engine (for speed), creating a hybrid electric vehicle or a plug-in hybrid electric vehicle.\n\nThe general trend appears to be that ground vehicles will \"go electric,\" and automakers have slowly but steadily responded to public demand by producing both hybrid electric vehicles which get gasoline, and electric vehicles which get similar or better mileage by mpg equivalent efficiency rating. \n\nIn general, commercially-manufactured electric vehicles are inhibited by the limited range per charge of batteries (up to ), battery charge times that are slower than gasoline filling times, apparent greater initial cost over combustion engines, and potentially high service costs for used or worn-out batteries. Electric conversions are often inhibited by either the difficult labor and specialized knowledge involved in a do-it-yourself (DIY) conversion, or the expense of a purchased conversion service with conversion components, often costing . There is a small but steadily growing DIY community for electric conversions.\n\nAssumes a simple design of a DC or AC motor. \n\n\nHobbyists often build their own EVs by converting existing production cars to run solely on electricity. There is a cottage industry supporting the conversion and construction of BEVs by hobbyists. Universities such as the University of California, Irvine even build their own custom electric or hybrid-electric cars from scratch.\n\nShort-range battery electric vehicles can offer the hobbyist comfort, utility, and quickness, sacrificing only range. Short-range EVs may be built using high-performance lead–acid batteries, using about half the mass needed for a range. The result is a vehicle with about a range, which, when designed with appropriate weight distribution (40/60 front to rear), does not require power steering, offers exceptional acceleration in the lower end of its operating range, and is freeway capable and legal. But their EVs are expensive due to the higher cost for these higher-performance batteries. By including a manual transmission, short-range EVs can obtain both better performance and greater efficiency than the single-speed EVs developed by major manufacturers. Unlike the converted golf carts used for neighborhood electric vehicles, short-range EVs may be operated on typical suburban throughways (where speed limits are typical) and can keep up with traffic typical on such roads and the short \"slow-lane\" on-and-off segments of freeways common in suburban areas.\n\nFaced with chronic fuel shortage on the Gaza Strip, Palestinian electrical engineer Waseem Othman al-Khozendar invented in 2008 a way to convert his car to run on 32 electric batteries. According to al-Khozendar, the batteries can be charged with worth of electricity to drive from . After a 7-hour charge, the car should also be able to run up to a speed of .\n\nIn 2008, several Chinese manufacturers began marketing lithium iron phosphate () batteries directly to hobbyists and vehicle conversion shops. These batteries offered much better power-to-weight ratios allowing vehicle conversions to typically achieve per charge. Prices gradually declined to approximately per kW·h by mid-2009. As the cells feature life ratings of 3,000 cycles, compared to typical lead acid battery ratings of 300 cycles, the life expectancy of cells is around 10 years. cells require more expensive battery management and charging systems than lead acid batteries.\n\nOn-board solar cells can be used to power an electric vehicle.\nThe small power generated by solar cells mounted on a vehicle means that the other components in the system must be special to compensate for this.\nFor example, the body of even a small conventional car converted to electric is still too heavy to be driven by on-board solar cells. A practical solar-powered vehicle is designed from the ground up with specially made parts.\n\nMost conversions in North America are performed by hobbyists who typically will convert a well used vehicle with a non-functioning engine, since such defective vehicles can be quite inexpensive to purchase. Other hobbyists with larger budgets may prefer to convert a later model vehicle, or a vehicle of a particular type. In some cases, the vehicle itself may be built by the converter, or assembled from a kit car.\n\nA two-stage vehicle is a vehicle that has been built by two separate manufacturers. The result is a standard, complete vehicle. In this process, vehicles may be converted by a manufacturer (as was done by Ford Motor Company to create the Ford Ranger EV). Alternatively, in a process known as \"third-party (power)trainization\", an independent converter will purchase new vehicle gliders (vehicles without a motor or related equipment) and then perform the conversion, to offer a two-stage vehicle.\n\nIn some countries, the user can choose to buy a converted vehicle of any model in the automaker dealerships only paying the cost of the batteries and motor, with no installation costs (it is called pre-conversion or previous conversion).\n\nThe electric vehicle conversion industry has grown to include conversion car garages, aftermarket kits and vehicle components.\n\nAn electric bicycle is a conventional bicycle that has been fitted with an electric motor. Converting an existing bicycle by retrofitting it with a \"conversion kit\" is the simplest and least expensive electric vehicle conversion option. Most often electric bicycles or \"e-bikes\" are powered by rechargeable batteries however some experimental electric bicycles run directly on or recharge their batteries via solar panels, fuel cells, gas generators or other alternative energy sources. Some experimenters have even used supercapacitors to store energy. Using an on-board generator may impact the legal jurisdictional definition of an electric bicycle. A few types of electric bicycles are able to re-capture a small amount of energy from braking and can re-charge the batteries while braking or traveling down hills (regenerative braking).\n\nSome electric bikes have features where the motor can move the bicycle by itself (immediate start) if the rider chooses not to pedal with a button or throttle controller, while others require the rider to pedal at all times (pedal assist). This latter type may in some jurisdictions allow the vehicle to be used on bicycle trails that otherwise prohibit motorized vehicles of any kind (See motorized bicycle).\n\nMany battery technologies are available for powering electric bikes. The most common and least expensive battery technology is sealed lead acid but LiFePO is fast becoming the battery of choice for the e-bike.\n\nConverting one's bike to electric with a conversion kit is an easy and affordable solution for most people interested in learning more about electric vehicle conversion.\n\nThere may be some problems with the warranty however on the original bicycle being converted, if an electric bike conversion kit is added.\nLow-speed scooters are not typically suitable for on-the-road use. These may be configured for either standing or sitting use. Some local laws apply bicycle laws to scooters, such as helmet and pedestrian right-of-way considerations.\n\nOwing to its light weight and efficiency, a light vehicle can make an excellent choice, particularly if care is taken in component selection and placement. It is possible to obtain conversion kits for some popular light vehicles, most notably the rear motor, rear drive Volkswagen Beetle, its Type 3 evolution, and its successor, the front motor/drive VW Golf.\n\nBy converting a light vehicle it is possible to use a smaller motor, which both weighs and costs less than a larger motor. A lighter overall vehicle weight will reduce power consumption in start-and-stop traffic and increase range in many practical driving conditions. In the same way that a gasoline-powered economy car is cheaper and more efficient to run, an electric-powered economy car is as well.\n\nA compact sedan may be a better choice than a subcompact owing to better load capacity and more room for battery placement. Some commercial EV Conversions use vehicles in this size range. One example is a 1992 Honda Civic. In this conversion, the back seat was retained, and there is still enough room to sink nine flooded lead-acid batteries low in the trunk where the spare tire was located, as well as another nine batteries under the hood. Another example is a 1987 Mitsubishi Tredia where the rear batteries have been raised above the trunk floorspace, sealed, and externally vented. With suspension modifications, increasing shock length & spring rating, the car must still be below GVWR, even with the driver and passengers. Exceeding the total design weight of the vehicle would be illegal in some states, and might result in cancellation by an insurance company.\n\nThere is an effort by several engineers in California to make the Toyota Prius a \"Plug-In Hybrid Electric Vehicle,\" or PHEV, whereby the first 40 miles are driven by all-electric power, then the gasoline engine comes on to re-charge the batteries, only if the commute is further than 40 mi. If it is less, one can just plug it into the utility grid to re-charge the batteries. The process is done by removing the nickel-metal hydride batteries, and installing different batteries, and a different battery management system.\n\nSince 30 April 2009, the Electric Car Corporation have been selling the Citroën C1 ev'ie, an all-electric conversion from the Citroën C1.\n\nFull-size sedans and minivans are generally considered to be poor candidates for EV conversion. As the suspension and tires are already operating close to the maximum permissible , it may be necessary to make substantial modifications in these areas. It may be easier to obtain upgraded suspension components for some smaller vehicles, if these are also typically used for sports racing (particularly autocross). Starting with a heavy vehicle and adding batteries will result in poor performance in acceleration, handling, braking, and economy of operation.\n\nOne of the possibilities is using the body of Audi's D2 platform A8 (1994–2003) Audi A8 or sports sedan S8 (1998–2003 or older European market models where the German model weights 1730 kg) Audi S8 both of which are all aluminium monocoque \"Audi Space Frame\" vehicle, which helped to significantly reduce weight without being any less rigid.\n\nFor a person interested in sports car performance and appearance, the creation of a satisfying conversion will likely lead to a number of difficulties in such details as battery disposition, as such vehicles generally have available space distributed in small volumes around the vehicle. This leads to complexity in securing and wiring batteries. These vehicles can offer stunning performance in the lower speed ranges owing to light weight and rear wheel drive, and may also offer good range due to their superior aerodynamics.\n\nThe 1969–1976 Porsche 914 is one of the more successful sports car conversions, as well as being one of the most popular. Once converted, it boasts better performance in range, acceleration and top speed than most other vehicles. Also, its low acquisition costs contributes to its popularity as a conversion candidate. Some manufacturers of conversion kits have made a kit specific to the 914.\n\nAnother popular sports car used for conversion is the 1984–1989 Toyota MR2. Reasons for its popularity are low weight before conversion, low cost to purchase the car, and available locations within the car to place the large batteries that most people use (lead-acid and its derivative technologies). The later MR2 body style (Mark 2) does not seem as popular, and as of March 2008, there is only one known conversion.\n\nA 1983 Mitsubishi Starion was converted to all-electric in 2009 by Carmel Morris and Nathan Bolton in Australia. This sports vehicle had divided front/rear battery load for well-balanced low center of gravity performance. The battery pack consisted of 45 x 3.2v nominal lithium-ion batteries. The tar weight of the final result was not much greater than the original specification, allowing the sports car to be engineering road-certified as a four-seat vehicle. The builders wanted to prove that an electric car conversion could also include other options such as power steering and air conditioning (as is the norm for new electric vehicles), without sacrificing excess energy or comfort. Information on the successful Electric Starion conversion can be found on the web.\n\nThe Bradley GT II as well as other VW-based kit cars are very popular conversion candidates due to their being inexpensive, extensive support groups as well as their simple sports car design. Availability of conversion kits for these cars are quite prevalent with commercial retail establishments that specialize in EV conversions.\n\nLight trucks are especially suitable for hobbyist conversion because it is easy to locate batteries remote from the passenger compartment and there is a good load handling capacity for the use of heavy batteries such as the flooded lead-acid batteries commonly used in golf carts. Light trucks also offer substantial utility in use simply because they are trucks. Even if a portion of the weight capacity is removed by the presence of batteries within or below the cargo bed, much or all of the spatial utility remains. A light truck is highly recommended as a first conversion effort because of the simplicity of component layout. With proper battery placement the stability of a late production truck can be improved over the ICE version. While a number of suitable vehicles are available in pre-2002 models, the modern evolution of this type has become taller, heavier, bulkier and less efficient, and their excessive height makes under-bed battery placement essential to keep the center of gravity low enough for stability on curves.\n\nThese are rarely converted due to their excessive weight, and aerodynamic inefficiencies. To make the situation worse, many modern trucks and SUVs continue to get bulkier, heavier, and their high stance means the height of the center of gravity leads to instability while making high speed turns, a distinct disadvantage if there is not enough room between the frame rails to enable low battery mounting. As a direct result, the payload carrying capacity and thus the GVWR of the vehicles goes down. Such a trait is not desirable because it limits the weight of the battery pack that can be carried, limiting the maximum battery-to-vehicle weight ratio that could be achieved for the vehicle when converted to an EV. (Such considerations are important due to price, weight, and performance limitations of current battery technologies.) For a given battery type, reducing the battery-to-vehicle weight ratio always results in reduced vehicle range per charge. However, despite these mostly unavoidable limitations, several SUVs and larger trucks have been successfully converted to electric power by hobbyists. Some examples include the \"Gone Postal\" van converted to an EV racer by \nRoderick Wilde and Suckamps EV Racing, the Land Rover EV converted by Wilde Evolutions, and the 1988 Jeep Cherokee EV converted by Nick Viera.\n\nAlthough technologically feasible, classic cars are not widely converted in order to maintain a car's authenticity. The German company ReeVOLT has however made it one of their business branches to convert old Trabants for use in tourism in the east of Germany in connection to the car's regional historic significance and to allow easy access to environmental zones such as city centers and in particular the partially car-free island of Rügen. The car is considered ideal for conversions because of its low used price and low weight due to the small size and resin fiber construction.\n\nThe principal efforts in the development of autonomous electric buses (this is, without trolleys and wires) have involved limited production of very expensive fuel cell vehicles.\n\nThe most economically effective development in this area involves the creation of hybrid electric buses (mainly plug-in hybrids), well suited to this application owing to frequent stops and starts and effective energy recovery and release in this cycle.\n\nAnother solution is the conversion to battery electric buses that follow the principle of replacing (discharged batteries) instead of recharging.\n\nTransdev York in the United Kingdom currently operate the world's first diesel-engined buses that have been converted to full-electric propulsion. A fleet of five late 1990s Dennis Trident 2 open-top double-decker buses, operated for a local City Sightseeing contract, have been converted from diesel to electric power.\n\nWhile this type of vehicle is usually made to be a \"street-legal\" performance machine, it may also be developed for occasional use as a drag racing vehicle. The leading vehicle in this field is the \"Maniac Mazda\" a Mazda RX-7 sports car converted from rotary engine to electric by Roderick Wilde. This vehicle can outrun Dodge Viper and Ferrari sports cars in quarter-mile drag races.\n\nEV's have proven successful in autocross competition. The electric motor's ability to deliver maximum torque at 0 RPM and a comparatively broad torque band provide good throttle response and allow running an autocross without any time lost to shifting gears. The short distance of the typical autocross requires less stored energy than most forms of motorsports. This minimizes\nelectric vehicle's most obvious competitive disadvantage, the weight penalty of batteries compared to gasoline.\n\nIntended only for specialized straight line quarter-mile (acceleration) racing this type of vehicle is used only \"off road\" at specialized \"dragstrips\".\n\nEven more specialized than the drag racer, this is intended to obtain high speeds on long, straight, and flat raceways, such as the dry lake beds found in locations such as the Bonneville Salt Flats.\n\nClosed Circuit Road racing, particularly any type of endurance racing, is one of the greatest challenges for EV's. Pound per pound, gasoline contains far more energy than even the most advanced of current batteries. An electric vehicle must be heavier or more efficient to run the same distance as its gasoline competitor. Endurance racing strategies include battery packs that can be changed quickly and \"Dump charging\"\n\nSuitable for a builder who is capable of constructing a kit car, with good abilities and equipment in machining and welding this can result in a unique vehicle. It is especially suitable for the construction of a lightweight vehicle that can offer exceptional performance. Many VW-based kit car companies have tube chassis ready to start with.\n\nA glider kit includes all components of a vehicle except the power train.\n\nA novelty vehicle or an electric powered art car may not be suitable for on-road use. Applications include electric vehicle show demonstrations, parades, parade floats, float towing, and eclectic off-road gatherings such as Burning Man.\nThis vehicle is ideal for the beach (where not prohibited) and to promote tourist places but will usually require trailering to its operating site.\n\nThe ease of registration will vary by state. Some states require safety inspections, usually to ensure body integrity in areas subject to severe corrosion from winter road de-icing materials. In any case, for general registration all functional safety equipment should be operating – turn signals, brake lights, headlights, horn, etc. The windshield should have no running cracks (small stone chips and \"stars\" may be acceptable if not in the driver's principal line of vision). If the vehicle has been reconstructed from a salvage vehicle (a vehicle whose registration has been forfeited) inspection may be more severe to ensure compliance and the legitimacy of sources of salvage components by presentation of proper purchase receipts.\n\nRegistration procedures will vary by state and will usually be more difficult (even bizarre) in states with strict emissions requirements (even though a plug-in only conversion will be a zero emission vehicle). Arguing with DMV staff is typically futile in all jurisdictions, but there may be appeal procedures available but whose availability may not be openly publicized.\n\nOn the other hand, changing the registration allows a conversion to qualify for tax incentives available in some states, such as Oregon, for either the vehicle, the charging system, or both.\n\nRegistration of a converted existing, or newly self-constructed electric vehicle in California is no longer difficult.\n\nThis falls into two categories; First, if the vehicle is built from new frame components and possibly some salvage parts, (\"i.e.\", it has never been a previously titled motorcar previously, but it has brakes or axles that were obtained used/rebuilt.) In California if you \"create\" a car from scratch and want to register and title it with DMV, you need to go through the \"Specially Constructed Vehicle Emission Control Program\" or SPCNS for short, this is also called California \"SB100\" program. Or Second, a previously registered vehicle converted to electric propulsion.\n\nSPCNC: In the past, the process of registering an SPCNS vehicle required meeting with an outside referee at a community college or one of the states mobile referee stations to verify the vehicle. The current process happens at the DMV with the inspection being done by A DMV inspector. The inspector will need to be able to see the motor and batteries and verify that there in no internal combustion engine. The vehicle will still need a brake and light inspection By an outside inspector to verify its compliance with other vehicle codes.\n\nCurrently Registered Vehicles: If you are converting an existing vehicle and it is older than 1975, you may opt to not bother to get the vehicle converted to the \"E\" code unless you are looking for the Diamond Lane stickers, since these vehicles don't require a smog inspection. For vehicles newer than 1975 you will need to go the DMV and have the vehicle inspected as a conversion. The batteries and motor will have to be exposed so the inspector can see them and verify that there is no internal combustion engine.\n\nThe next step requires the clerk at the local California Department of Motor Vehicles (DMV) office to call the Sacramento office. Only the Sacramento office can make this entry into the computer system. If the local clerk tries, the system will default to the \"Q\" code for hybrid. (These procedures are in place to inhibit fraudulent registration of ICE vehicles as Electric in order to avoid smog inspections.) A hybrid is not exempt from smog inspection. Before you leave the DMV make sure your printout has the proper \"E\" code or you will have to go back to do it again.\n\nThere is weight fee of $87 for vehicles under or $266 for vehicles from . This is in addition to any regular registration fees.\n\nIn Spain, the conversion (called transformation) is regulated by \"Real Decreto 866/2010, de 2 de julio, por el que se regula la tramitación de las reformas de vehículos\".\n\n"}
{"id": "41313479", "url": "https://en.wikipedia.org/wiki?curid=41313479", "title": "Figurative palanquin", "text": "Figurative palanquin\n\nA figurative palanquin connected with the totem of its owner is a special kind of litter used in the Greater Accra Region in Ghana. These palanquins called in the Ga language \"okadi akpakai\" belong to the royal insignias and are used only by the Ga kings or \"mantsemei\" and their sub-chiefs when they are carried in public at durbars and festivals like \"Homowo\". With these figurative palanquins the Ga create ethnic differences between themselves and their Akan neighbours that only use simple boat- or chair-shaped litters.\n\nA Ga chief whose clan uses the lion as a totem must therefore use a litter in the form of a lion. The totems and family symbols of the Ga represent animals, plants or objects. All of them are associated with the history of the clan and his ancestors.\n\nWhen a chief is carried in such a figurative palanquin, using his totem symbol ensures protection by the spirits and the ancestors which are connected with the respective symbol. At the same time the totem's magical powers are transferred to the chief who is sitting in the figurative palanquin. In contrast to the conventional boat- or chair-shaped Akan litters, the figurative palanquins of the Ga also function as marks of distinction between themselves and their Akan neighbours, and they even denote differences between the different Ga clans.\n\nIn precolonial times, the Ga did not use litters, but carried their Chiefs on human shoulders. The ethnologist Margaret Field believes that the boat-shaped Akan litters were introduced in Accra by the Akwamu living there since the 17th century. In the course of the 19th century, when the Ga took over from the Akwamu parts of their military organization, they also adopted the use of palanquins. However, there are no exact sources describing when the Ga started to use palanquins in the form of their family symbols. The social anthropologist Regula Tschumi found only a short notice in the Gold Coast Independent 1925 indicating that the King of Accra, the so-called \"Ga mantse\" used an elephant shaped palanquin in those years. According to Tschumi, the use of figurative palanquins spread in the course of the 20th century from Accra to other coastal towns where these palanquins, to some extent, are still used today.\n\nUnlike the Akan, the Ga use their palanquins only for secular sub-chiefs. Women and their highest spiritual leaders, the wulomei, do not use palanquins for different reasons.\n\nA palanquin is made new purposely for the enstoolment of a chief and is also used for the first time during his installation ceremony. After the installation the palanquin is kept as a royal insignia hidden away in the stool house of the respective family. It is not taken out again unless needed for an important occasion, such as the \"Homowo\" festival.\n\nAccordingly, the figurative palanquins of the Ga are very rarely shown in public and especially for foreigners they are difficult to see. Therefore, it is not surprising that although the palanquins look similar to the figurative coffins which are well known to the western art market, the figurative palanquins remained completely unknown until recently.\n\nThese palanquins are also, contrary to what was believed formerly by many researchers and even many Ga, still used and built by the same craftsmen who make the figurative coffins. One of the most important palanquin builders of the last 30 years was Paa Joe who was known until now only for his figurative coffins. But as artisans are not supposed to talk about figurative palanquins and other royal insignias, Paa Joe is not giving out easily information about the palanquins he had formerly built.\n\nIn the Ga culture, as Regula Tschumi has discovered during her fieldwork, initiations and funerals of the traditional chiefs must complement each other. Initiates must be buried in the same way as they were set up in office or initiated. Therefore, a chief who uses a figurative palanquin had and has to be buried in a coffin that looked the same as his previously used litter. Contrary to what Thierry Secretan and others believed, no king or chief was ever buried in his figurative palanquin, because litters belong to the royal insignia, which in the Ga culture may not be buried. Consequently, those chiefs who used a figurative palanquin at their installation ceremony had to be buried in a substitute, hence in a figurative coffin, that looked the same as their palanquin.\n\nThe first figurative coffins therefore were simply copies of the palanquins which were used as a substitute.\n\nThough outwardly similar, figurative coffins and palanquins of course belong to a different category of objects: palanquins are royal insignia made to last and to be conserved in the family house after the death of their users, while the figurative coffins may be sacrificed and are buried with the deceased. The figurative palanquin become sacred after the death of their users and the family keep them in order to maintain contact with the ancestors. For the royal families they also become distinguishing marks and function as tokens of legitimacy of their rule.\n\nWhile the figurative coffins of the Ga became world-famous on the Western art market, the figurative palanquins remain up to the present hidden and unknown as an art form. It was long believed that the Ga would no longer use figurative litters and the old palanquins no longer exist because as Thierry Secretan wrote, the Chiefs allegedly would have been buried in them.\n\nThe fact that even many Ga still believe that their chiefs were formerly buried in figurative palanquins can, as Regula Tschumi writes it, be explained easily: Burials of initiates formerly involved human sacrifices and, to this day, neither uninitiated nor Christian Ga would want or be allowed to attend such funerals. Chiefs are also buried secretly, therefore it is and was difficult to say how a chief is buried when it occurs in the middle of the night. Hence, nobody outside the royal families noticed that their chiefs were interred in substitute palanquins.\n\nAs a chief had to be buried the same way as he had been installed in office, it was imperative to bury him in a coffin that was the copy of his palanquin. The first figurative coffins used to bury the traditional chiefs, were therefore not a new invented art form but only the copies of the figurative palanquins.\n\nThe Christians and common Ga began to use figurative coffins around 1950 to 1960. As they were not allowed to use family symbols, which were still reserved for their traditional chiefs, carpenters such as Ataa Oko (1919-2012), Kane Kwei (1925-1992) and others began to produce figurative coffins avoiding the traditional totem symbols. They built coffins which did not represent forms with a deeper significance, but rather objects which were associated with the deceased’s occupation.\n\nToday all the Ga, irrespective of their religious affiliation, use figurative coffins which were formerly reserved for the traditional kings, chiefs and priests. So these coffins are now used by the broader mass of Ga people.\n\nSince the 1970s, such coffins have been recognized by the Western art world as works of art in the Western sense. The exhibition “Les Magiciens de la terre” 1989 and the theories of Thierry Secretan caused the Teshie carpenter Kane Kwei to be acclaimed as the inventor of figurative coffins and only his products to be classified as works of art. Through lack of knowledge, the Western art world erroneously attributed the invention of a supposedly new art form to a single artist and made him and the figurative coffins world-famous, while the significance of the already existing figurative palanquins remained until recently completely unknown. Therefore, in the West the figurative coffins acquired high artistic status which in fact would have belonged to the Ga original art form, the figurative palanquins.\n\nIn March 2017 the Gallery ANO in Accra showed for the first time a palanquin in the exhibition \"Accra: Portraits of A City\". The palanquin had been made by Kudjoe Affutu in 2013 for a chief in the Central Region, Ghana.\n\n"}
{"id": "50991882", "url": "https://en.wikipedia.org/wiki?curid=50991882", "title": "Finite point method", "text": "Finite point method\n\nThe finite point method (FPM) is a meshfree method for solving partial differential equations (PDEs) on scattered distributions of points. The FPM was proposed in the mid-nineties in (Oñate, Idelsohn, Zienkiewicz & Taylor, 1996a), (Oñate, Idelsohn, Zienkiewicz, Taylor & Sacco, 1996b) and (Oñate & Idelsohn, 1998a) with the purpose to facilitate the solution of problems involving complex geometries, free surfaces, moving boundaries and adaptive refinement. Since then, the FPM has evolved considerably, showing satisfactory accuracy and capabilities to deal with different fluid and solid mechanics problems.\n\nSimilar to other meshfree methods for PDEs, the finite point method (FPM) has its origins in techniques developed for scattered data fitting and interpolation, basically in the line of weighted least-squares methods (WLSQ). The latter can be regarded as particular forms of the moving least-squares method (MLS) proposed by Lancaster and Salkauskas. WLSQ methods have been widely used in meshfree techniques because allow retaining most of the MLS, but are more efficient and simple to implement. With these goals in mind, an outstanding investigation which led to the development of the FPM began in (Oñate, Idelsohn & Zienkiewicz, 1995a) and (Taylor, Zienkiewicz, Oñate & Idelsohn, 1995). The technique proposed was characterized by WLSQ approximations on local clouds of points and an equations discretization procedure based on point collocation (in the line of Batina’s works, 1989, 1992). The first applications of the FPM focused on adaptive compressible flow problems (Fischer, Onate & Idelsohn, 1995; Oñate, Idelsohn & Zienkiewicz, 1995a; Oñate, Idelsohn, Zienkiewicz & Fisher, 1995b). The effects on the approximation of the local clouds and weighting functions were also analyzed using linear and quadratic polynomial bases (Fischer, 1996). Additional studies in the context of convection-diffusion and incompressible flow problems gave the FPM a more solid base; cf. (Oñate, Idelsohn, Zienkiewicz & Taylor, 1996a) and (Oñate, Idelsohn, Zienkiewicz, Taylor & Sacco, 1996b). These works and (Oñate & Idelsohn, 1998) defined the basic FPM technique in use today.\n\nThe approximation in the FPM can be summarized as follows. For each point formula_1 in the analysis domain formula_2 (\"star point\"), an approximated solution is locally constructed by using a subset of surrounding supporting points formula_3, which belong to the problem domain (\"local cloud of points\" formula_4). The approximation is computed as a linear combination of the cloud unknown nodal values (or parameters) and certain metric coefficients. These are obtained by solving a WLSQ problem at the cloud level, in which the distances between the nodal parameters and the approximated solution are minimized in a LSQ sense. Once the approximation metric coefficients are known, the problem governing PDEs are sampled at each \"star point\" by using a collocation method. The continuous variables (and their derivatives) are replaced in the sampled equations by the discrete approximated forms, and the solution of the resulting system allows calculating the unknown nodal values. Hence, the approximated solution satisfying the governing equations of the problem can be obtained. It is important to note that the highly local character of the FPM makes the method suitable for implementing efficient parallel solution schemes.\n\nThe construction of the typical FPM approximation is described in (Oñate & Idelsohn, 1998). An analysis of the approximation parameters can be found in (Ortega, Oñate & Idelsohn, 2007) and a more comprehensive study is conducted in (Ortega, 2014). Other approaches have also been proposed, see for instance (Boroomand, Tabatabaei and Oñate, 2005). An extension of the FPM approximation is presented in (Boroomand, Najjar & Oñate, 2009).\n\nThe early lines of research and applications of the FPM to fluid flow problems are summarized in (Fischer, 1996). There, convective-diffusive problems were studied using LSQ and WLSQ polynomial approximations. The study focused on the effects of the cloud of points and weighting functions on the accuracy of the local approximation, which helped to understand the basic behavior of the FPM. The results showed that the 1D FPM approximation leads to discrete derivative forms similar to those obtained with central difference approximations, which are second-order accurate. However, the accuracy degrades to first-order for non-symmetric clouds, depending on the weighting function. Preliminary criteria about the selection of points conforming the local clouds were also defined with the aim to improve the ill-conditioning of the minimization problem. The flow solver employed in that work was based on a two-step Taylor-Galerkin scheme with explicit artificial dissipation. The numerical examples involved inviscid subsonic, transonic and supersonic two-dimensional problems, but a viscous low-Reynolds number test case was also provided. In general, the results obtained in this work were satisfactory and demonstrated that the introduction of weighting in the LSQ minimization leads to superior results (linear basis were used).\n\nIn a similar line of research, a residual stabilization technique derived in terms of flux balancing in a finite domain, known as Finite Increment Calculus (FIC) (Oñate, 1996, 1998), was introduced. The results were comparable to those obtained with explicit artificial dissipation, but with the advantage that the stabilization in FIC is introduced in a consistent manner, see (Oñate, Idelsohn, Zienkiewicz, Taylor & Sacco, 1996b) and (Oñate & Idelsohn, 1998a).\n\nAmong these developments, the issue of point generation was firstly addressed in (Löhner & Oñate, 1998). Based on an advancing front technique, the authors showed that point discretizations suitable for meshless computations can be generated more efficiently by avoiding the usual quality checks needed in conventional mesh generation. Highly competitive generation times were achieved in comparison with traditional meshers, showing for the first time that meshless methods are a feasible alternative to alleviate discretization problems.\n\nIncompressible 2D flows were first studied in (Oñate, Sacco & Idelsohn, 2000) using a projection method stabilized through the FIC technique. A detailed analysis of this approach was carried out in (Sacco, 2002). Outstanding achievements from that work have given the FPM a more solid base; among them, the definition of local and normalized approximation bases, a procedure for constructing local clouds of points based on local Delaunay triangulation, and a criterion for evaluating the quality of the resultant approximation. The numerical applications presented focused mainly on two-dimensional (viscous and inviscid) incompressible flows, but a three-dimensional application example was also provided.\n\nA preliminary application of the FPM in a Lagrangian framework, presented in (Idelsohn, Storti & Oñate, 2001), is also worth of mention. Despite the interesting results obtained for incompressible free surface flows, this line of research was not continued under the FPM and later formulations were exclusively based on Eulerian flow descriptions.\n\nThe first application of the FPM to the solution of 3D compressible flows was presented in a pioneer work by (Löhner, Sacco, Oñate & Idelsohn, 2002). There, a reliable and general procedure for constructing local clouds of points (based on a Delaunay technique) and a well-suited scheme for solving the flow equations were developed. In the solution scheme proposed, the discrete flux derivatives are written along edges connecting the cloud's points as a central difference-like expression plus an upwind biased term that provides convective stabilization. An approximate Riemann solver of Roe and van Leer flux vector splitting were used for this purpose. The approach proposed is more accurate (also more expensive) than artificial dissipation methods and, additionally, does not require the definition of geometrical measures in the local cloud and problem dependent parameters. The time integration of the equations was performed through a multi-stage explicit scheme in the line of Runge-Kutta methods.\n\nSome years later, further research was carried out in relation to 3D FPM approximations in (Ortega, Oñate & Idelsohn, 2007). This work focused on constructing robust approximations regardless of the characteristics of the local support. To this end, local automatic adjusting of the weighting function and other approximation parameters were proposed. Further 3D applications of the method involved compressible aerodynamics flows with adaptive refinement (Ortega, Oñate & Idelsohn, 2009) and moving/deforming boundary problems (Ortega, Oñate & Idelsohn, 2013). In these works, the FPM showed satisfactory robustness and accuracy, and capabilities to address practical computations. Among other achievements, it was demonstrated that a complete regeneration of the model discretization could be an affordable solution strategy, even in large simulation problems. This result presents new possibilities for the meshless analysis of moving/deforming domain problems. The FPM was also applied with success to adaptive shallow water problems in (Ortega, Oñate, Idelsohn & Buachart, 2011) and (Buachart, Kanok-Nukulchai, Ortega & Oñate, 2014). A proposal to exploit meshless advantages in high-Reynolds viscous flow problems is presented in (Ortega, Oñate, Idelsohn & Flores, 2014a).\n\nIn the same field of applications, a major study on the accuracy, computational cost and parallel performance of the FPM was carried out in (Ortega, Oñate, Idelsohn & Flores, 2014b). There, the FPM was compared with an equivalent Finite Element-based solver, which provided a standard for assessing both, the characteristics of the meshless solver and its suitability to address practical applications. Some simplifications of the FPM technique were proposed in this work to improve efficiency and reduce the performance gap with FEM. Then, grid convergence studies using a wing-body configuration were conducted. The results showed comparable accuracy and performance, revealing the FPM competitive with respect to its FEM counterpart. This is important because meshless techniques are often considered impractical due to the poor efficiency of the initial implementations.\n\nThe FPM has also been applied in aeroacoustics in (Bajko, Cermak & Jicha, 2014). The solution scheme proposed is based on a linearized Riemann solver and successfully exploits the advantages of high-order FPM approximations. The results obtained are indicative of the potential of the FPM to address sound propagation problems.\n\nCurrent efforts are mainly oriented to exploit the capabilities of the FPM to work in parallel environments for solving large-scale practical problems, particularly in areas where meshless procedures can make useful contributions, for example problems involving complex geometry, moving/deforming domain, adaptive refinement and multiscale phenomena.\n"}
{"id": "8840191", "url": "https://en.wikipedia.org/wiki?curid=8840191", "title": "Flood gun", "text": "Flood gun\n\nA flood gun is an electromechanical device that provides a steady flow of low-energy electrons to a desired target or \"flood area.\" \n\nTypically, the target is an area on an insulator or semiconductor where another \"writing gun\" has just left a net positive charge. \n\nIf the energy of a flood gun's electrons is properly balanced, each impinging flood gun electron knocks out one secondary electron from the target, thus preserving the net charge in the target area. This is called \"charge neutralization.\"\n\nFlood guns are typically used in photoelectron spectroscopy, oscilloscopes and ion beam implanters as their secondary electron gun.\n\n"}
{"id": "506331", "url": "https://en.wikipedia.org/wiki?curid=506331", "title": "Ford Transit", "text": "Ford Transit\n\nThe Ford Transit is a range of light commercial vehicles produced by Ford since 1965. Sold primarily as a cargo van, the Transit is also built as a passenger van (marketed as the Ford Tourneo since 1995), minibus, cutaway van chassis, and as a pickup truck. Over 8,000,000 Transit vans have been sold, making it the third best-selling van of all time and have been produced across four basic platform generations (debuting in 1965, 1986, 2000, and 2013 respectively), with various \"facelift\" versions of each.\n\nThe first product of the merged Ford of Europe, the Transit was marketed through Western Europe and Australia; by the end of the twentieth century, it was marketed nearly globally with the exception of North America until 2013 when it replaced the Ford E-Series in 2015. The Transit has been the best-selling light commercial vehicle in Europe for forty years, and in some countries the term \"Transit\" has passed into common usage as a generic trademark applying to any light commercial van in the Transit's size bracket. While initially designed for the European market, the Ford Transit is now produced in Asia, North America, and Europe for worldwide buyers. Upon production in North America, the Transit won second place in Motor Trend's 2015 'Truck of the Year' award, behind the newly introduced mid-size Chevrolet Colorado pickup and ahead of the new Ford F-150.\n\nAs of 2016, the Transit is the best-selling van of any type in the United States, minivan sales included. The Transit drives Ford's 57 percent share of the full-size van market in the USA.\n\nUnlike the British-built Transit \"family\", the first production Ford to wear the \"Transit\" badge was a van built in Ford's Köln (Cologne) plant in Germany. It was introduced in 1953 as FK 1000 carrying 1,000 kg) with a 1.3-litre inline-four engine from the contemporary Taunus. In 1955 the engine capacity was enlarged to 1.5 litres. From 1961, this vehicle was called the Ford Taunus Transit. Production of this model ceased in 1965.\n\nThe German vehicle was not widely exported, and the \"Mark 1\" tag has commonly been applied, retrospectively, to the 1965 to 1978 British model (see below). Whilst there have only been four basic platforms since 1965, the various facelifts and upgrades over the years have been referred to using a conflicting range of \"Mark\" numbers, with some sources counting a facelift as a new \"Mark\", some not. Ford's own historical look back at Transit production, published for the launch of the 1994 model, avoids the issue by referring to generations of Transit by years produced. This article attempts to make mention of all the common naming systems.\n\nThe first generation Transit, or the Transit Mark I in the United Kingdom, was introduced in October 1965, taking over directly from the Thames 400E.\n\nThe van was produced initially at Ford's Langley facility in Berkshire, England (a former Second World War aircraft factory which had produced Hawker Hurricane fighters), but demand outstripped the capability of the plant, and production was moved to Southampton until closure in 2013 in favour of the Turkish factory.\n\nTransits were also produced in Ford's Genk factory in Belgium and also Turkey. Transits were produced in Amsterdam for the local market from the mid-1970s until the end of 1981. This factory had ample capacity, since the Ford Transcontinental produced there had little success (total production 8000 in 6 years). Although the Transit sold well in the Netherlands, it was not enough to save the factory, which closed in December 1981.\n\nThe Transit was introduced to replace the Ford Thames 400E, a small mid-engined forward control van noted for its narrow track which was in competition with similar-looking but larger vehicles from the BMC J4 and J2 vans and Rootes Group's Commer PB ranges. In a UK market segment then dominated by the Bedford CA, Ford's Thames competitor, because of its restricted load area, failed to attract fleet users in sufficient numbers. Ford switched to a front-engined configuration, as did the 1950s by Bedford with their well-regarded CA series vans. Henry Ford II's revolutionary step was to combine the engineering efforts of Ford of Britain and Ford of Germany to create a prototype for the Ford of Europe of today—previously the two subsidiaries had avoided competing in one another's domestic markets but had been direct competitors in other European markets.\n\nThe Transit was a departure from the European commercial vehicles of the day with its American-inspired styling—its broad track gave it a huge advantage in carrying capacity over comparable vehicles of the day. Most of the Transit's mechanical components were adapted from Ford's car range of the time. Another key to the Transit's success was the sheer number of different body styles: panel vans in long and short wheelbase forms, pick-up truck, minibuses, crew-cabs to name but a few.\n\nThe engines used in the UK were the Essex V4 for the petrol-engined version in 1.7 L and 2.0 L capacities. By using relatively short V-4 engines Ford were able to minimise the additional length necessitated to place the engine ahead of the driver. Another popular development under the bonnet was the equipping of the van with an alternator at time when the UK market competitors expected buyers to be content with a dynamo. A 43 bhp (32 kW) diesel engine sourced from Perkins was also offered. As this engine was too long to fit under the Transit's stubby nose, the diesel version featured a longer bonnet. The underpowered Perkins proved unpopular, and was replaced by Ford's own \"York\" unit in 1972. For mainland Europe the Transit had the German Ford Taunus V4 engine in Cologne 1.3, 1.5, and 1.7- or Essex 2.0-litre versions. The diesel version's long nose front was also used to accommodate the Ford 3.0 L Ford Essex V6 engine (UK) for high performance applications such as vans supplied to police and ambulance services. In Australia, in 1973, to supplement the two Essex V4 engines that were available the Transit was released with the long-nose diesel front used to accommodate an inline 6-cylinder engine derived from the Ford Falcon.\n\nThe Metropolitan Police reported on this vehicle in 1972 via a Scotland Yard spokesman that 'Ford Transits are used in 95 per cent of bank raids. With the performance of a car, and space for 1.75 tonnes of loot, the Transit is proving to be the perfect getaway vehicle...', describing it as 'Britain's most wanted van'.\n\nThe adoption of a front beam axle in place of a system incorporating independent front suspension that had featured on its UK predecessor might have been seen as a backward step by some, but on the road commentators felt that the Transit's wider track and longer wheelbase more than compensated for the apparent step backwards represented by Ford's suspension choices. Drivers appreciated the elimination of the excessive noise, smell and cabin heat that resulted from placing the driver above or adjacent to the engine compartment in the Thames 400E and other forward control light vans of the 1950s and early 1960s.\n\nThe Transit was also assembled in South Africa between 1967 and 1974, the last Transit to be sold in that country until 2013, when a fully imported model was introduced.\n\nIn August 1977, a facelifted version—codenamed within Ford as the \"Transit \"—but usually referred to as the Transit Mark II, debuted with a restyled nose section, new interior, and the introduction of the Pinto engine from the Cortina in place of the Essex V4. Many fleet owners experienced premature camshaft wear in early Pinto units in the Cortina and for two years the Transit 75 was available with the 1.6 L Ford Kent cross-flow engine. High-performance versions intended for police or ambulance use used the 3.0 L V6 version of the Essex engine, Australian variants had 4.1 L (250 cu in) inline 6-cylinder engines.\n\nIn 1984, the York diesel engine was redesigned into the 2.5 L \"DI\" (direct injection) unit. At this time this generation received a minor facelift including a grey plastic front grille with integrated headlamp surrounds, wraparound indicators, longer bumper end caps and multifunction rear lights incorporating fog, indicator, reversing and side lights for the panel van. This facelift did not commonly result in a new \"Mark\" number.\n\nThe Mark II was available in 6 body styles: Van, Kombi, Chassis Cab, Parcel Van, Bus and Crewbus all available in short-wheelbase (2690 mm) and long-wheelbase (3000 mm) versions. A selection of 5 engines was available: 1.6-litre OHC Petrol, 1.6-litre OHV Petrol (Kent), 2.0-litre OHC Petrol, 2.0-litre OHC Petrol (Economy) and 2.5-litre Diesel. On top of this were 32 door combinations, 6 axle ratios and options for 12 – 17 interior seats. All of these were available in any combination when purchased with Ford's highly customizable custom plan. At the time this gave the business sector an unprecedented amount of flexibility, which was a major factor in the vehicles' ultimate success.\n\nIn 1981, for mainland European market only, the Transit Clubmobil was introduced by the Hymer company. This was fitted with a 1.6 / 2.0 OHC engine, and featured a custom interior – captain style swivel seats in velour, pile carpet, motorsport steering wheel, unique Ronal 14\" alloy wheels, unique side windows, folding back seat, luggage box, unique front spoiler, tinted glass, power assisted steering, spare wheel carrier and rear door ladder. In 3 years of production 150 were produced and less than 20 are thought to still exist.\n\nIn late 1982 a four-wheel drive version was added to the German market. This was developed together with a Ford dealer in Stuttgart. The 4x4 Transit was later offered in other markets as well.\nCodenamed \"VE6\", the third generation Transit platform appeared in January 1986 and was notable for its all-new bodyshell which was of \"one-box\" design (\"i.e.\" the windscreen and bonnet are at close to the same angle), and the front suspension was changed to a fully independent configuration on SWB versions. The engine range was carried over largely unchanged from the last of the 1978–1985 generation models, although in 1989 the high-performance 3.0 Essex V6 petrol was replaced by the Cologne 2.9 EFI V6, mainly because of emissions regulations as the Essex V6 design was nearly 25 years old by then and still used a carburettor. The third generation Transit was developed under the \"Triton\" code name. \n\nA subtle facelift in 1992 saw the fully independent front suspension adopted across the range, whilst a redesigned floor plan allowed the use of single, rather than paired, rear wheels on the LWB derivative, further increasing payload—these models are identifiable by the slightly more rounded front headlamps. In Australia, the third generation Transit did not go on sale until March 1994, after a 13-year absence from that market.\n\nA major facelift to the Transit in 1994 gave the Transit a new nose and dashboard, along with the 2.0 L DOHC 8-valve engine as found in the 1993 to 1998 Ford Scorpio. It is similar to the earlier Sierra DOHC unit but without the distributor and uses the updated OBD II-compliant EEC-V level engine control unit. Some of Ford's 16-valve engines, such as those found in the Scorpio, Escort RS2000 and Galaxy were also based on this block. At the same time air conditioning, electric windows, central locking, electric mirrors and airbags were all made available as optional extras.\n\nThe turbo diesel version came in , and version with an electronic fuel pump.\n\nFor the 30th anniversary of the Transit in 1995 Ford released a limited edition model called the Transit Hallmark. Six hundred were made and were available in three colours with 200 being made in each.\n\nIn Europe the VE83 Transit was available up to 2000, but in Vietnam it was built up to 2003 when it was exchanged in June for the new generation.\nThe Transit, introduced in July 2000, was the third all-new design, and borrowed styling cues from Ford's \"New Edge\" designs, like the Focus and Ka. Developed by Ford in the United States, the main innovation is that it is available in either front- or rear-wheel drive. Ford nomenclature makes this the V184 (rear-wheel-drive) or V185 (front-wheel-drive) model. This model features the \"Puma\"-type Duratorq turbo diesel engine also used in the 2000 Mondeo and Jaguar X-Type, with the petrol versions moving up to the 2.3 L 16-Valve edition of the straight-4 engine. With this engine, the Transit can reach in 21 seconds and reach a top speed of , returning it to car-like performance as claimed for the earliest models. A demonstration of this model's speed was shown on \"Top Gear\" in 2005, where German race driver Sabine Schmitz attempted to drive it around the Nürburgring in under ten minutes, matching Jeremy Clarkson's time in a turbodiesel Jaguar S-Type; she was unsuccessful, marking her fastest lap at 10m 8s.\n\nThis version won the International Van of the Year 2001.\n\nThe Durashift EST automatic transmission (optional on all rear-wheel-drive models) features controls mounted on the dashboard, a specially adapted manual mode, tow-haul mode, economy mode and winter mode. This is known as the ASM (automatically-shifting manual) system in the Australian market.\n\n2002 saw the introduction of the first High Pressure Common Rail diesel engine in the Transit, with the launch of the HPCR 2.0-litre in the FWD. Production of the van started at the new Ford-Otosan plant in Kocaeli, Turkey which saw the end of all production at the Genk, Belgium plant which had been producing Transits since 1965. This coincided with the introduction of the Transit Connect (also produced in Kocaeli), a smaller panel van based on the C170 (Focus) platform and aimed at replacing the older Escort and Fiesta based models. Despite the name, the Connect has no engineering commonality with the full-size Transit.\n\n2003 saw a new instrument cluster with a digital odometer.\n\n2004 saw the launch of the first RWD HPCR, the 2.4-litre variant that also introduced the 6-speed MT-82 RWD manual gearbox.\n\nThe five millionth Transit rolled off the Southampton line on Monday, 18 July 2005 and was donated to an English charity.\n\nThe fourth generation Transit, received a facelift to the body, introduced in August 2006, including new front and rear lights, a new front end and a new interior featuring the gearstick on the dashboard and Ford's new corporate radio design. Besides the styling changes, the powertrains were revised. The old petrol engine was replaced with one from the Ford Ranger, the front-wheel-drive diesel went from 2.0 to 2.2 litres capacity, and all diesel engines gained high-pressure common rail (TDCi) systems. The powertrains were changed to meet new emissions legislation. Additionally, the facelift introduced CAN bus electronics to the Transit for the first time. The new version (Ford nomenclature V347 for front-wheel drive and V348 for rear-wheel drive) won International Van of the Year for 2007 despite tough competition from several all-new rivals. This Transit arrived in Mexico to replace the Freestar after the 2007 model year. This was the first Transit having a five-cylinder engine.\n\nMid-2006 saw the launch of the \"Sport Van\", a production van featuring the engine with additional styling parts, \"Le Mans\" stripes and 18-inch alloy wheels.\n\nLate-2007 saw the launch of the engine for front wheel drives (replacing the 130 PS) complete with the VMT6 6-speed manual transaxle to cope with the extra power.\n\nThe 6-speed transaxle was introduced on the mid-power FWD in late 2008 when the engine was upped to .\n\nIn late 2008, the \"coated Diesel Particulate Filter\" (cDPF)—designed to meet higher emission standards than the current Euro IV requirement—was introduced as an option on all diesel engines. Production ended in 2013, but returned in China in two modified forms.\n\nEngines\n\n\nTo celebrate the Transit's status as International Van of the Year 2007, Ford built a stretch limousine style van — the Transit XXL. It is a unique special that is the most expensive Transit ever made.\n\nThe Ford Transit SuperSportVan is a one-off, high-performance version of the fifth-gen Transit built by Ford Europe. It uses a 3.2L turbocharged Duratorq I5, producing 198 horsepower, borrowed from a larger Transit model, mated to a 6-speed transmission.\n\nThe fifth generation of the Transit was officially launched in January 2013 at the North American International Auto Show in Detroit.\n\nA OneFord globally developed vehicle, the new-generation Transit was designed by Ford of Europe and co-developed with Ford in North America. In a break from the previous generation of the Transit, there are two distinct body forms:\n\n\nBoth versions external design look evolved from the New Edge styling used from the previous-generation model to the \"Kinetic\" design adopted by the OneFord global models since 2010; the interior drew cues from the third generation Ford Focus.\n\nThe new Transit is available in cargo van and chassis/cutaway cab configurations. In a significant departure from the E-Series, but standard for existing rest of world versions, Transit vans/wagons come in three different roof heights; extended-wheelbase vans are available with dual rear wheels. It was released in North America in the summer of 2014, released as a 2015 model. As with the much smaller Transit Connect, passenger versions are marketed under a singular Transit nameplate rather than the Tourneo name seen globally.\n\nOutside of North America, much of the Transit engine lineup and drivetrain configurations are retained from the previous generation. For the United States and Canada, the four cylinder engines and manual transmission are not offered; in these markets only the 180-hp 3.2L Duratorq I5 diesel (branded as a PowerStroke) is shared with global-market models. For North America, the standard engine is a 275-hp 3.7L Ti-VCT V6 (shared with the F-Series and Police Interceptor version of the Ford Explorer) and an optional 320-hp 3.5L EcoBoost V6 shared with the F-150 and D3-platform vehicles. All versions in North America are specified with a 6-speed automatic transmission and rear-wheel drivetrain. The 3.7L Ti-VCT V6 can be converted to run on compressed natural gas or liquefied petroleum gas with an optional Gaseous Engine Prep Package. In North America, 4x4 conversions are done through factory-supported third party installers.\n\nEach Transit model saw at least at a 600 lb. increase in maximum payload capability over the comparable E-Series/Econoline model it replaced. The Transit's body makes extensive use of boron steel in its construction.\n\nWorldwide production of Ford Transits takes place in two facilities; all European and Asian Transit production is from Ford Otosan in Kocaeli Province, Turkey; this factory will also provide a percentage of global exports. North American and South American production is primarily sourced from Kansas City Assembly in Claycomo, Missouri on the lines used for the previous generation Ford Escape; production at the Kansas City Assembly Plant began on 30 April 2014.\nThe Ford Transit VJX6541DK-M is a Chinese market specific version, and is based on the third generation VE6/VE94 platform. This generation is only built in Nanchang by Jiangling Motors and is reserved for the Chinese domestic market. Remarkable are the much larger headlights and the larger grille. The model was launched to the market in 2006. Overall, the generation has 70 improvements to its predecessor. The interior has been changed and made more ergonomic. Power windows are standard, but ABS is optional. The Chinese Transit is available with two diesel engines and one petrol. One of them has a power of 67.6 kW (Type JX493ZQ3), the other 68 kW (JX493ZQ4). Both diesels have a capacity of 2,771 cc. The later diesel version has a common-rail 2.8-litre unit (with turbocharger and intercooler) that produces 80–85 kW. The Petrol version uses a Mitsubishi 4G64S4N Engine 92 kW inline-four motor. The top speed is specified at . The Chinese Transits have a VIN in following form: LJXBMCH1××T××××××.\nThe European 2006 Ford Transit went into production in 2008. On the Chinese market it is known as the \"New Transit\". It is offered parallel to the 2006 China generation. The two facilities are currently manufacturing 300,000 units annually. Ford and Jiangling want to open a third plant in late 2012 to increase the annual production to further 3,000 units.\n\nIn January 2010, the Toyota recalls affected the products of the Ford Group because Ford used the same supplier (CTS Corporation). It was suspected that the accelerator pedals were defective and posed a danger. About 1600 Ford Transits of the 2006 China generation were affected by the recall.\n\nThe European models of the VE6 and VE8 generation can be given an aftermarket facelift to the Chinese version. This needs the Chinese components and manual skills. Production finally came to an end in 2017.\nStarting from May 2017, a version of the old Transit manufactured by JMC was rebadged and slightly redesigned. The rebadged version is called the JMC Teshun. Pricing ranges from 99,800 yuan to 139,200 yuan. \n\nA handful of companies offered four-wheel-drive conversions, such as County Tractors of Knighton in Powys, who converted vans on behalf of Ford as a Special Vehicle Operations factory option. The first Transit County models were based on the Mk2 Transit model, both long and short wheelbase. The conversion used a Dana 44F front axle and a NP208 transfer box, both lifted from the Ford Bronco, coupled to the regular Transit engine, gearbox and rear axle using three custom propshafts. The Transit rear axle was retained, mounted to a rear subframe or 'lift cradle' to give the extra ride height. Other modifications were 16-inch wheel rims, locking front hubs, a heavy-duty steering box and 305 mm diameter front brake discs.\n\nWith the introduction of the Mk3 Transit in 1986 came the next generation of the County 4x4. This would prove to be a very popular and successful version of the County Transit 4x4, and the last to use the Dana beam axle layout. Later County 4x4 models switched to using an independent front suspension setup which was inherently more complex in design than the earlier beam axle models. Later panel vans also lost the twin-wheel rear axle that had been fitted on earlier LWB versions.\n\nMainly used by utility companies such as National Grid, the Ministry Of Defence, and by mountain rescue teams, the Transit County 4x4 proved to be a capable vehicle both on and off road, with the ability to carry both crew and equipment just about anywhere.\n\nDesign and supply of drivetrain components for County 4x4 models passed to Countytrac, a division of M.J. Allen Ltd, who are still involved in the development of the latest Mk7 AWD Transit and Connect models.\n\nIntroduced as part of the 1995 redesign of the Transit, the Tourneo is a Transit-based 8 or 9-seat minibus, but over the years has become increasingly better trimmed up to the point where it can almost be classified as a large MPV. Featuring back seats and back windows similar to a minivan, the Tourneo is also considered an executive transport vehicle and is often supplied with alloy wheels. Since its introduction, the Tourneo has followed the same development cycle as the Transit; both versions receive updates at the same time.\n\nA smaller minivan version of the Tourneo was introduced in 2002; branded the Tourneo Connect, it was based on the Transit Connect mini MPV.\n\n"}
{"id": "33203678", "url": "https://en.wikipedia.org/wiki?curid=33203678", "title": "Geocarpy", "text": "Geocarpy\n\nGeocarpy is \"an extremely rare means of plant reproduction\", in which plants produce diaspores within the soil. This may occur with subterranean flowers (protogeocarpy), or from aerial flowers, which penetrate the soil after flowering (hysterocarpy). It has evolved as an effective means of ensuring a suitable environment for the plant's offspring.\n\nGeocarpy is also linked with solifluction soils, where rapid thawing and freezing of surface soil causes almost continuous movement. This phenomenon is prevalent in high altitude areas of East Africa. In order to reproduce, geocarpic plants bend their stems so that the fruit can be embedded in the soil during the freezing process while the fruit is still attached to the plant itself.\n\nGeocarpy is most frequent in tropical or semi-desert areas, and geocarpic species may be found in the families Araceae, Begoniaceae, Brassicaceae (Cruciferae), Callitrichaceae, Convolvulaceae, Cucurbitaceae, Fabaceae (Leguminosae), Loganiaceae, Moraceae and Rubiaceae. The best-known example is the peanut, \"Arachis hypogaea\".\n"}
{"id": "25203942", "url": "https://en.wikipedia.org/wiki?curid=25203942", "title": "Giving Back to Africa", "text": "Giving Back to Africa\n\nGiving Back to Africa is a 501(c)(3) Bloomington, Indiana based non-profit organization dedicated to the long-term mission of educating young people in the Democratic Republic of Congo. In partnership with local Congolese educational institutions and non-governmental organizations, its goal is to empower GBA beneficiaries - through service-centered education - to become servant-leaders capable of taking control of their own lives while serving as change agents in their local communities and throughout the Democratic Republic of Congo. The organization was featured in the Bloom Magazine, December 2008/January 2009 issue.\n\n"}
{"id": "40124726", "url": "https://en.wikipedia.org/wiki?curid=40124726", "title": "Hi-VAWT", "text": "Hi-VAWT\n\nHi-VAWT is a renewable energy solutions provider headquartered in Linkou District, New Taipei City, Taiwan. Hi-VAWT has installations in many countries/territories worldwide (Taiwan, China, Japan, South Korea, Hong Kong, India, Malaysia, Australia, New Zealand, South Africa, Spain, Austria, Czech Republic, Italy, Germany, Poland, Guam, Hawaii, the USA, Jamaica and Canada), and on every continent including Antarctica. The company also a member of Taiwan Wind Turbine Industry Association.\n\nHi-VAWT's turbines utilize a combined Darrieus and Savonius wind turbine. Savonius turbine is a drag-type wind turbine that perform well under low-wind speed condition, while Darrieus turbine is a lift-type wind turbine that perform best at high-wind speed condition. The vertical axis allowing the turbine to take wind from all directions simultaneously. While the combo-blades provide a self-starting performance while maintain a good working condition in higher wind speed situation.\n\nHi-VAWT business organization was started and is headquartered in Linkou District, New Taipei City, Taiwan. The company also has offices/distributors in China, South Korea, Japan and the US, which serve those individual regional markets.\n\nHi-VAWT’s services including vertical axis wind turbines (ranges from 300 to 3000 watts system), solar panels and hybrid street lights system. The DS-3000 wind turbine system currently is the only vertical axis wind turbine system that has been certified by Nippon Kaiji Kyokai and qualify for Feed-in tariff application in Japan and Taiwan.\n\n\n"}
{"id": "24130418", "url": "https://en.wikipedia.org/wiki?curid=24130418", "title": "Jog (dislocations)", "text": "Jog (dislocations)\n\nJog describes the turns of a dislocation line inside a crystal structure. A dislocation line is rarely uniformly straight as in the figure, often containing many curves and/or steps to facilitate movement through the crystal in incremental amounts, rather than shifting the entire line at once. One of these step types is a jog, the other is a kink. However, both are typically referred to as jogs, which can be a source of confusion.\n\nSegments of dislocation line that have a component of their sense vector normal to the glide plane are termed jogs. See image for the definitions of the sense vector and the glide plane.\nSegments of dislocation line that do not leave the original glide plane are termed kinks.\n\nJogs are often very immobile compared to kinks, and require diffusion of crystallographic defects like vacancies or interstitial atoms to climb. They are not capable of glide (movement in the glide plane) because the direction of motion is in the plane normal direction, not on the plane itself as with kinks.\n\nHirth, J. P., Lothe, J., Theory of Dislocations, 2nd edition, Krieger Publishing Company, Malabar, Florida, 1982 reprint 1992, page 261\n"}
{"id": "4059082", "url": "https://en.wikipedia.org/wiki?curid=4059082", "title": "Kadowaki–Woods ratio", "text": "Kadowaki–Woods ratio\n\nThe Kadowaki–Woods ratio is the ratio of \"A\", the quadratic term of the resistivity and \"γ\", the linear term of the specific heat. This ratio is found to be a constant for transition metals, and for heavy-fermion compounds, although at different values. \n\nIn 1968 M. J. Rice pointed out\nthat the coefficient \"A\" should vary predominantly as the square of the linear electronic specific heat coefficient γ; in particular he showed that the ratio \"A/γ\" is material independent for the pure 3d, 4d and 5d transition metals. Heavy-fermion compounds are characterized by very large values of A and γ. Kadowaki and Woods\n\nAccording to the theory of electron-electron scattering \nthe ratio \"A/γ\" contains indeed several non-universal factors, including the square of the strength of the effective electron-electron interaction. Since in general the interactions differ in nature from one group of materials to another, the same values of \"A/γ\" are only expected within a particular group. \nIn 2005 Hussey proposed a re-scaling of \"A/γ\" to account for unit cell volume, dimensionality, carrier density and multi-band effects. In 2009 Jacko, Fjaerestad, and Powell demonstrated \"f\"\"(n)A/γ\" to have the same value in transition metals, heavy fermions, organics and oxides with \"A\" varying over 10 orders of magnitude, where \"f\"\"(n)\" may be written in terms of the dimensionality of the system, the electron density and, in layered systems, the interlayer spacing or the interlayer hopping integral.\n\n"}
{"id": "30091334", "url": "https://en.wikipedia.org/wiki?curid=30091334", "title": "Mereni Wind Farm", "text": "Mereni Wind Farm\n\nThe Mereni Wind Farm is a wind power project, under construction in Constanţa County, Romania. It will consist of an individual wind farm with 30 individual wind turbines with a nominal output of around 1.5 MW which will deliver up to 45 MW of power, enough to power over 29,400 homes, with a capital investment required of approximately €50 million.\n"}
{"id": "49585095", "url": "https://en.wikipedia.org/wiki?curid=49585095", "title": "Michael Bengwayan", "text": "Michael Bengwayan\n\nMichael Bengwayan is a Filipino environmental activist best known for his advocacy of using the Petroleum nut (Pittosporum resiniferum) as an alternative bio-fuel in the Philippines, and his involvement with advocacies to save trees from being cut, notably the Save 182 Movement which petitioned to stop the earth-balling 182 trees at Luneta Hill, Baguio City, by mall developer SM, and the campaign to stop the cutting of 1,200 trees along the Manila North Road, in the towns of Binalonan and Pozorrubio, Pangasinan.\n\nBengwayan is the director of the Cordillera Ecological Center, and is also the proprietor of The Habitat, a five hectare farm in Tublay, Benguet which partly serves as an ecotourism site, an ecological reserve containing indigenous trees of Philippine Cordillera, and a demonstration farm for the intercropping of Arabica coffee, pineapple, pine trees, and petroleum nut.\n"}
{"id": "22947275", "url": "https://en.wikipedia.org/wiki?curid=22947275", "title": "Microrheology", "text": "Microrheology\n\nMicrorheology is a technique used to measure the rheological properties of a medium, such as microviscosity, via the measurement of the trajectory of a flow tracer (a micrometre-sized particle). It is a new way of doing rheology, traditionally done using a rheometer. There are two types of microrheology: \"passive microrheology\" and \"active microrheology\". Passive microrheology uses inherent thermal energy to move the tracers, whereas active microrheology uses externally applied forces, such as from a magnetic field or an optical tweezer, to do so. Microrheology can be further differentiated into 1- and 2-particle methods.\n\n\" Passive microrheology \" uses the thermal energy (\"kT\") to move the tracers, although recent evidence suggests that active random forces inside cells may instead move the tracers in a diffusive-like manner. The trajectories of the tracers are measured optically either by microscopy or by diffusing-wave spectroscopy (DWS). From the mean squared displacement with respect to time (noted MSD or <Δ\"r\"> ), one can calculate the visco-elastic moduli \"G\"′(\"ω\") and \"G\"″(\"ω\") using the generalized Stokes–Einstein relation (GSER). Here is a view of the trajectory of a particle of micrometer size.\n\nIn a standard passive microrheology test, the movement of dozens of tracers is tracked in a single video frame. The motivation is to average the movements of the tracers and calculate a robust MSD profile.\n\nObserving the MSD for a wide range of integration time scales (or frequencies) gives information on the microstructure of the medium where are diffusing the tracers.\n\nIf the tracers are having a free diffusion in a purely viscous material, the MSD should grows linearly with sampling integration time:\n\nformula_1.\n\nIf the tracers are moving in a spring-like fashion within a purely elastic material, the MSD should have no time dependence:\n\nformula_2\n\nIn most cases the tracers are presenting a sub-linear integration-time dependence, indicating the medium has intermediate viscoelastic properties. Of course, the slope changes in different time scales, as the nature of the response from the material is frequency dependent.\n\nMicrorheology is another way to do linear rheology. Since the force involved is very weak (order of 10 N), microrheology is guaranteed to be in the so-called linear region of the strain/stress relationship. It is also able to measure very small volumes (biological cell).\n\nGiven the complex viscoelastic modulus formula_3 with \"G\"′(\"ω\") the elastic (conservative) part and \"G\"″(\"ω\") the viscous (dissipative) part and \"ω\"=2\"πf\" the pulsation. The GSER is as follows:\n\nwith \n\nA related method of passive microrheology involves the tracking positions of a particle at high frequency, often with a quadrant photodiode. From the position, formula_7, the power spectrum, formula_8 can be found, and then related to the real and imaginary parts of the response function, formula_9. The response function leads directly to a calculation of the complex shear modulus, formula_10 via:\n\nThere could be many artifacts that change the values measured by the passive microrheology tests, resulting in a disagreement between microrheology and normal rheology. These artifacts include tracer-matrix interactions, tracer-matrix size mismatch and more.\n\nA different microrheological approach studies the cross-correlation of two tracers in the same sample. In practice, instead of measuring the MSD - formula_12, movements of two distinct particles are measured - formula_13. Calculating the G(ω) of the medium between the tracers follows:\n\nformula_14\n\nNotice this equation does not depend on a, but instead in depends on R - the distance between the tracers (assuming R»a).\n\nSome studies has shown that this method is better in coming to agreement with standard rheological measurements (in the relevant frequencies and materials)\n\n\"Active microrheology\" may use a magnetic field \n\nThe force applied is a sinusoidal force with amplitude A and frequency ω -\n\nformula_15\n\nThe response of the tracer is a factor of the matrix visco-elastic nature. If a matrix is totally elastic (a solid), the response to the acting force should be immediate and the tracers should be observed moving by-\n\nformula_16.\n\nwith formula_17.\n\nOn the other hand, if the matrix is totally viscous (a liquid), there should be a phase shift of formula_18 between the strain and the stress -\n\nformula_19\n\nin reality, as most materials are visco-elastic, the phase shift observed is formula_20.\n\nWhen φ>45 the matrix is considered mostly in its \"viscous domain\" and when φ<45 the matrix is considered mostly in its \"elastic domain\".\n\nGiven a measured response phase shift φ (sometimes noted as δ), this ratio applies:\n\nformula_21\n\nSimilar response phase analysis is used in regular rheology testing.\n\nMore recently, it has been developed into Force spectrum microscopy to measure contributions of random active motor proteins to diffusive motion in the cytoskeleton.\n\n"}
{"id": "43999395", "url": "https://en.wikipedia.org/wiki?curid=43999395", "title": "Mutual induction shaft mechanism", "text": "Mutual induction shaft mechanism\n\nMutual induction shaft mechanism is a power generation mechanism basically used by electrostatic charge excitation mechanism. This mechanism was developed by Shayantan Bhattacharya, an engineering student, while he was in college. This invention relates to the conversion of electrostatic charge generating materials with an external prime mover mechanism to use mutual induction shaft mechanism (MISM) to generate a charge density to get an output dynamic source of current. The output dynamic current is dependent on the amount of charge density being measured at prior sensors and multi meter and the length of the shaft. This mechanics is an trans-mechanic process. Electrostatic charge concentration of charging materials are being used for dynamic current generation by use of mutual induction shaft mechanism and the two opposite polarity is being achieved by a prime mover crank mechanism by virtue of which the circulation of wheels generating circular motion is being converted into reciprocating motion at two extreme ends of the prime mover and an appropriate charge density Q is being generated at two extreme ends and the estimation and the determination of the charge density Q is measured by multi meter at points down and connected to prime mover lattice. And in accordance of the charge density Q the following shaft is being set in mutual induction.\n\nThis mechanism can be used in electric power generation in remote areas and in particle analytical procedues including in atomic sciences to understand nuclear physics, atomic structures.\n"}
{"id": "27297715", "url": "https://en.wikipedia.org/wiki?curid=27297715", "title": "Oil &amp; Gas Journal", "text": "Oil &amp; Gas Journal\n\nThe Oil & Gas Journal is a leading petroleum industry weekly publication with a worldwide coverage. It is headquartered in Tulsa, Oklahoma and the journal has a major presence in Houston, Texas. The journal is published by PennWell Corporation. Its publisher is Paul Westervelt, Vice President of PennWell Corporation, and editor is Bob Tippee. The first issue was published in 1902. Its online information services started in 1994.\n\nLexisNexis database describes the \"Oil & Gas Journal\" as an authoritative source on the petroleum industry aimed at engineers, oil management and executives throughout the oil and gas industry. The weekly publishes news, analysis, statistics, and technology updates on exploration, drilling, production, pipeline, transportation, refining, processing and marketing. It is a subscription trade publication. \"Oil & Gas Journal\" has about 20,000 subscribers for the printed issue and 80,000 for digital subscriptions.\n\nThe \"Oil & Gas Journal\" began in 1902 as the \"Oil Investor's Journal\" of Beaumont, Texas, which covered the oil boom that followed the nearby Spindletop discovery. As the oil discoveries spread along the Gulf Coast, the magazine relocated to Houston. In 1910 the magazine was purchased by Patrick C. Boyle, long-time publisher of the \"Oil City Derrick\" in Pennsylvania. Boyle moved the publication briefly to St. Louis, Missouri, then to Tulsa, where it still resides.\n\n"}
{"id": "9815240", "url": "https://en.wikipedia.org/wiki?curid=9815240", "title": "PSE-Operator", "text": "PSE-Operator\n\nPSE SA (\"Polskie Sieci Elektroenergetyczne S.A.\") is state owned transmission system operator in Poland. 100% of its shares is owned by the State Treasury. Until 2007, PSE Operator was a part of the PSE Group (now: Polska Grupa Energetyczna).\n\n"}
{"id": "903752", "url": "https://en.wikipedia.org/wiki?curid=903752", "title": "Polyus (spacecraft)", "text": "Polyus (spacecraft)\n\nThe Polyus spacecraft (, \"pole\"), also known as Polus, Skif-DM, GRAU index 17F19DM, was a prototype orbital weapons platform designed to destroy Strategic Defense Initiative satellites with a megawatt carbon-dioxide laser. It had a Functional Cargo Block derived from a TKS spacecraft to control its orbit and it could fire test targets to demonstrate the fire control system.\n\nThe Polyus spacecraft was launched 15 May 1987 from Baikonur Cosmodrome Site 250 as part of the first flight of the Energia system, but failed to reach orbit.\n\nAccording to Yuri Kornilov, Chief Designer of the Salyut Design Bureau, shortly before Polyus' launch, Mikhail Gorbachev visited the Baikonur Cosmodrome and expressly forbade the in-orbit testing of its capabilities. Kornilov claims that Gorbachev was worried that it would be possible for Western governments to view this activity as an attempt to create a weapon in space and that such an attempt would contradict the country's previous statements on the USSR’s peaceful intent.\n\nFor technical reasons, the payload was launched upside down. It was designed to separate from the Energia, rotate 180 degrees in yaw, then 90 degrees in roll and then fire its engine to complete its boost to orbit. The Energia functioned perfectly. However, after separation from Energia, the Polyus spun a full 360 degrees instead of the planned 180 degrees. When the engine fired, it slowed and burned up in the atmosphere over the south Pacific Ocean. This failure was attributed to a faulty inertial guidance system that had not been rigorously tested due to the rushed production schedule.\n\nParts of the Polyus project's hardware were re-used in Kvant-2, Kristall, Spektr and Priroda Mir modules, as well as in the ISS module Zarya.\n\nNPO \"Energia\" received orders from the Soviet government to begin research on space-based strike weapons in the mid-70s. Even before, the USSR had been developing maneuverable satellites for the purpose of satellite interception. By the beginning of the 1980s, \"Energia\" had proposed two programs: laser-equipped \"Skif\" and guided missiles platform \"Kaskad\" (where \"Skif\" would cover the low-orbit targets, \"Kaskad\" engaged targets in high and geosynchronous orbits). Together with NPO \"Astrofizika\" and KB \"Salyut\", they began developing their orbital weapons platform based on the Salyut DOS-17K frame.\n\nLater, when the objective of ICBM interception proved too difficult, the aims of the project were shifted towards anti-satellite weapons. The 1983 announcement by the US of their SDI program prompted further political and financial support for the satellite interceptor program. In the nuclear exchange scenario, the interceptors would destroy the SDI satellites, followed by a so-called \"pre-emptive retaliation\" large-scale Soviet ICBM launch.\n\nThe laser chosen for the \"Skif\" spacecraft was the 1-megawatt carbon dioxide laser, developed for the Beriev A-60 aircraft (an Il-76 flying laboratory with a combat laser). The introduction of the \"Energia\", capable of launching about 95 tonnes into orbit, finally allowed the spacecraft to accommodate the massive laser. The massive exhaust of the carbon-dioxide laser precipitated the objective of making the laser \"recoil-less\". The \"zero-torque exhaust system\" (SBM) was developed to that end. Its testing in orbit meant the release of a large cloud of carbon dioxide, which would hint at the satellite's purpose. Instead, the xenon-krypton mix would be used to simultaneously test the SBM and perform an innocent experiment on Earth's ionosphere.\n\nIn 1985, the decision was made to test-launch the new \"Energia\" launch vehicle, which was still in the testbed phase. A 100-ton dummy payload was initially considered for the launch, but in a series of last-minute changes, it was decided that the almost-completed \"Skif\" spacecraft would be launched instead for a 30-day mission.\n\nThe development of the real \"Skif\" was completed in just one year, from September 1985 to September 1986. Testing and tweaking the \"Energia\" launch vehicle, the launch pad and the \"Skif\" itself moved the launch to February, and later to May 1987. According to Boris Gubanov, the head designer of the \"Energia\" launch vehicle, the work schedule of the preceding years was exhausting, and at the point of Mikhail Gorbachev's visit on 11 May, he asked the Soviet premier to clear the launch now, because \"there will be heart attacks\".\n\nThe catastrophic malfunction that led to \"Skif\" entering the atmosphere in the same area as \"Energia's\" second stage was successfully investigated. It was found that 568 seconds after launch, the timing control device gave the logical block a command to discard the side modules' covers and laser exhaust covers. Unknowingly, the same command was earlier used to open the solar panels and disengage the maneuvering thrusters. This wasn't discovered because of the logistics of the testing process and overall haste. Main thrusters engaged while the \"Skif\" kept turning, overshooting the intended 180-degree turn. The spacecraft lost speed and reverted to the ballistic trajectory.\n\n\n\n"}
{"id": "9712159", "url": "https://en.wikipedia.org/wiki?curid=9712159", "title": "Quietrevolution wind turbine", "text": "Quietrevolution wind turbine\n\nThe quietrevolution is a discontinued brand of vertical-axis wind turbine. The helical design is most similar to the Gorlov helical turbine. Both are an evolution of the Darrieus wind turbine. It has won several awards including the \"Sustainable Innovation Award\" in 2006.\n\nThe turbine consists of three vertical airfoil blades, each having a helical twist of 120 degrees. This feature spreads the torque evenly over the entire revolution, thus preventing the destructive pulsations of the straight-bladed giromill (Darrieus turbine). The wind pushes each blade around on both the windward and leeward sides of the turbine.\n\nThe qr5 turbine, rated for 6 kW, measures 3.0m in diameter by 5m high.\n\nPoor performance of a QR5 installed at Welsh government offices in Aberystwyth has been blamed on poor siting. The turbine's overall cost was given as £48,000 and in 2012 it generated an average of 33 kWh per month, a value of £5.28 per month. \n\n\n"}
{"id": "13664288", "url": "https://en.wikipedia.org/wiki?curid=13664288", "title": "Radiophysics", "text": "Radiophysics\n\nRadiophysics (also modern writing \"Radio Physics\") is a branch of physics focused on the theoretical and experimental study of certain kinds of radiation: its emission, propagation, and interaction with matter.\n\nThe term is used in the following major meanings: \n\nAmong the main applications of radiophysics are radio communications, radiolocation, radio astronomy, and radiology.\n\n\n\n"}
{"id": "529780", "url": "https://en.wikipedia.org/wiki?curid=529780", "title": "Railroad tie", "text": "Railroad tie\n\nA railroad tie/railway tie/crosstie (North America) or railway sleeper (Britain, Ireland, South Asia, Australasia, and Africa) is a rectangular support for the rails in railroad tracks. Generally laid perpendicular to the rails, ties transfer loads to the track ballast and subgrade, hold the rails upright and keep them spaced to the correct gauge.\n\nRailroad ties are traditionally made of wood, but pre-stressed concrete is now also widely used, especially in Europe and Asia. Steel ties are common on secondary lines in the UK; plastic composite ties are also employed, although far less than wood or concrete. As of January 2008, the approximate market share in North America for traditional and wood ties was 91.5%, the remainder being concrete, steel, azobé (red ironwood) and plastic composite.\n\nApproximately 3,520 wooden crossties are used per mile of mainline railroad track in the US (distance between ties is nominally including one tie and the crib), (40 per rail) on main lines in the UK. Rails in the US may be fastened to the tie by a railroad spike; iron/steel baseplates screwed to the tie and secured to the rail by a proprietary fastening system such as a Vossloh or Pandrol are commonly used in Europe.\n\nThe type of railroad tie used on the predecessors of the first true railway (Liverpool and Manchester Railway) consisted of a pair of stone blocks laid into the ground, with the chairs holding the rails fixed to those blocks. One advantage of this method of construction was that it allowed horses to tread the middle path without the risk of tripping. In railway use with ever heavier locomotives, it was found that it was hard to maintain the correct gauge. The stone blocks were in any case unsuitable on soft ground, such as at Chat Moss, where timber ties had to be used. Bi-block ties with a tie rod are somewhat similar.\n\nHistorically wooden rail ties were made by hewing with an axe, called axe ties or sawn to achieve at least two flat sides.\nA variety of softwood and hardwoods timbers are used as ties, oak, jarrah and karri being popular hardwoods, although increasingly difficult to obtain, especially from sustainable sources. Some lines use softwoods, including Douglas fir; while they have the advantage of accepting treatment more readily, they are more susceptible to wear but are cheaper, lighter (and therefore easier to handle) and more readily available. Softwood is treated, while creosote is the most common preservative for railway ties, preservatives are also sometimes used such as pentachlorophenol, chromated copper arsenate and a few other preservatives. Sometimes non-toxic preservatives are used, such as copper azole or micronized copper. New boron-based wood preserving technology is being employed by major US railroads in a dual treatment process in order to extend the life of wood ties in wet areas. Some timbers (such as sal, mora, jarrah or azobé) are durable enough that they can be used untreated.\n\nProblems with wooden ties include rot, splitting, insect infestation, plate-cutting, also known as chair shuffle in the UK (abrasive damage to the tie caused by lateral motion of the tie plate) and spike-pull (where the spike is gradually loosened from the tie). For more information on wooden ties the Railway Tie Association maintains a comprehensive website devoted to wood tie research and statistics.\n\nWooden ties can, of course, catch fire; as they age they develop cracks that allow sparks to lodge so that they catch fire more easily.\n\nConcrete ties are cheaper and easier to obtain than timber and better able to carry higher axle-weights and sustain higher speeds. Their greater weight ensures improved retention of track geometry, especially when installed with continuous-welded rail. Concrete ties have a longer service life and require less maintenance than timber due to their greater weight, which helps them remain in the correct position longer. Concrete ties need to be installed on a well-prepared subgrade with an adequate depth on free-draining ballast to perform well. Concrete ties amplify wheel noise, so wooden ties are often used in densely populated areas.\n\nOn the highest categories of line in the UK (those with the highest speeds and tonnages), pre-stressed concrete ties are the only ones permitted by Network Rail standards.\n\nMost European railways also now use concrete bearers in switches and crossing layouts due to the longer life and lower cost of concrete bearers compared to timber, which is increasingly difficult and expensive to source in sufficient quantities and quality.\n\nSteel ties are formed from pressed steel and are trough-shaped in section. The ends of the tie are shaped to form a \"spade\" which increases the lateral resistance of the tie. Housings to accommodate the fastening system are welded to the upper surface of the tie. Steel ties are now in widespread use on secondary or lower-speed lines in the UK where they have been found to be economical to install due their ability to be installed on the existing ballast bed. Steel ties are lighter in weight than concrete and able to stack in compact bundles unlike timber. Steel ties can be installed onto the existing ballast, unlike concrete ties which require a full depth of new ballast. Steel ties are 100% recyclable and require up to 60% less ballast than concrete ties and up to 45% less than wood ties.\n\nHistorically, steel ties have suffered from poor design and increased traffic loads over their normally long service life. These aged and often obsolete designs limited load and speed capacity but can still be found in many locations globally and performing adequately despite decades of service. There are great numbers of steel ties with over 50 years of service and in some cases they can and have been rehabilitated and continue to perform well. Steel ties were also used in specialty situations, such as the Hejaz Railway in the Arabian Peninsula, which had an ongoing problem with Bedouins who would steal wooden ties for campfires.\n\nModern steel ties handle heavy loads, have a proven record of performance in signalized track, and handle adverse track conditions. Of high importance to railroad companies is the fact that steel ties are more economical to install in new construction than creosote-treated wood ties and concrete ties. Steel ties are utilized in nearly all sectors of the worldwide railroad systems including heavy-haul, class 1s, regional, shortlines, mining, electrified passenger lines (OHLE) and all manner of industries. Notably, steel ties (bearers) have proven themselves over the last few decades to be advantageous in turnouts (switches/points) and provide the solution to the ever-growing problem of long timber ties for such use.\n\nWhen insulated to prevent conduction through the ties, steel ties may be used with track circuit based train detection and track integrity systems. Without insulation, steel ties may only be used on lines without block signaling and level crossings or on lines that use other forms of train detection such as axle counters.\n\nIn more recent times, a number of companies are selling composite railroad ties manufactured from recycled plastic resins and recycled rubber. Manufacturers claim a service life longer than wooden ties with an expected lifetime in the range of 30–80 years, that the ties are impervious to rot and insect attack, and that they can be modified with a special relief on the bottom to provide additional lateral stability. In some main track applications the hybrid plastic tie has a recessed design to be completely surrounded by ballast.\n\nAside from the environmental benefits of using recycled material, plastic ties usually replace timber ties soaked in creosote, the latter being a toxic chemical, and are themselves recyclable. Hybrid plastic railroad ties and composite ties are used in other rail applications such as underground mining operations, industrial zones, humid environments and densely populated areas. Hybrid railroad ties are also used to be partly exchanged with rotten wooden ties, which will result in continuous track stiffness. Hybrid plastic ties and composite ties also offer benefits on bridges and viaducts, because they lead to better distribution of forces and reduction of vibrations into respectively bridge girders or the ballast. This is due to better damping properties of hybrid plastic ties and composite ties, which will decrease the intensity of vibrations as well as the sound production.\n\nIn 2009, Network Rail announced that it would begin replacing wooden ties with recycled plastic ones made by I-Plas Ltd of Halifax, West Yorkshire; but I-Plas became insolvent in October 2012.\n\nThe Cable TV series \"Factory Made\" has a segment on the manufacture of plastic ties.\n\nIn 2012, New Zealand ordered a trial batch of \"EcoTrax\" brand recycled composite ties from Axion for use on turnouts and bridges, and a further 3-year order in 2015, but then Axion filed for bankruptcy in December 2015, though it continues to trade. These ties are developed by Dr. Nosker at Rutgers University.\n\nIn 2014 the KLP Hybrid Plastic Tie, by Lankhorst Engineered Products of Sneek, Netherlands, won the Innovation Award in the category Track and Infrastructure.\nTies may also be made from fiberglass.\n\nAn unusual form of tie is the Y-shaped tie, first developed in 1983. Compared to conventional ties the volume of ballast required is reduced due to the load-spreading characteristics of the Y-tie. Noise levels are high but the resistance to track movement is very good. For curves the three-point contact of a Y steel tie means that an exact geometric fit cannot be observed with a fixed attachment point.\n\nThe cross section of the ties is an I-beam.\n\nAs of 2006 less than of Y-tie track had been built, of which approximately 90 percent is in Germany.\n\nThe ZSX Twin tie is manufactured by Leonhard Moll Betonwerke GmbH & Co KG and is a pair of two pre-stressed concrete ties longitudinally connected by four steel rods. The design is said to be suitable for track with sharp curves, track subject to temperature stress such as that operated by trains with eddy brakes, and bridges, and as transition track between traditional track and slab track or bridges.\n\nConcrete monoblock ties have also been produced in a wider form (e.g. ) such that there is no ballast between the ties; this wide tie increases lateral resistance and reduces ballast pressure. The system has been used in Germany where wide ties have also been used in conjunction with the GETRAC A3 ballastless track systems.\n\nBi-block (or twinblock) ties consist of two concrete rail supports joined by a steel bar. Advantages include increased lateral resistance and lower weight than monobloc concrete ties, as well as elimination of damage from torsional forces on the ties center due the more flexible steel connections. This tie type is in common use in France, and are used on the high-speed TGV lines. Bi-block ties are also used in ballastless track systems.\n\nFrame ties (German: \"Rahmenschwelle\") comprise both lateral and longitudinal members in a single monolithic concrete casting. This system is in use in Austria; in the Austrian system the track is fastened at the four corners of the frame, and is also supported midway along the frame. Adjacent frame ties are butted close to each other. Advantages of this system over conventional cross increased support of track. In addition, construction methods used for this type of track are similar to those used for conventional track.\n\nIn ladder track the ties are laid parallel to the rails and are several meters long. The structure is similar to Brunel's baulk track; these longitudinal ties can be used with ballast, or with elastomer supports on a solid non-ballasted support.\n\nVarious methods exist for fixing the rail to the railroad ties. Historically spikes gave way to cast iron chairs fixed to the tie, more recently springs (such as Pandrol clips) are used to fix the rail to the tie chair.\n\nIn recent years, wooden railroad ties have also become popular for gardening and landscaping, both in creating retaining walls and raised-bed gardens, and sometimes for building steps as well. Traditionally, the ties sold for this purpose are decommissioned ties taken from rail lines when replaced with new ties, and their lifespan is often limited due to rot. Some entrepreneurs sell new ties. Due to the presence of wood preservatives such as coal tar, creosote or salts of heavy metals, railroad ties introduce an extra element of soil pollution into gardens and are avoided by many property owners. In the UK, new oak beams of the same size as standard railroad ties, but not treated with dangerous chemicals, are now available specifically for garden construction. They are about twice the price of the recycled product. In some places, railroad ties have been used in the construction of homes, particularly among those with lower incomes, especially near railroad tracks, including railroad employees. They are also used as cribbing for docks and boathouses.\n\nThe Spanish artist Agustín Ibarrola has used recycled ties from Renfe in several projects.\n\nIn Germany, use of wooden railroad ties as building material (namely in gardens, houses and in all places where regular contact to human skin would be likely, in all areas frequented by children and in all areas associated with the production or handling of food in any way) has been prohibited by law since 1991 because they pose a significant risk to health and environment. From 1991 to 2002, this was regulated by the \"Teerölverordnung\" (Carbolineum By-law), and since 2002 has been regulated by the \"\" (Chemicals Prohibition By-law), §1 and Annex, Parts 10 and 17.\n\nBallastless track does not require underlying ballast to maintain its integrity. The first such tracks were used for mountain railways (such as the Pilatus Railway, built in 1889), with the rails being attached directly to the mountain rock. From the late 1960s onwards, German, British, Swiss and Japanese railways experimented with alternatives to the traditional railroad tie, in an effort to create track with greater accuracy and longevity, along with lower maintenance costs.\n\nThis gave rise to the ballastless railway track, especially in tunnels, on high-speed railway lines, and on lines where high train frequency imposes greater stress on the track. Paved concrete track has the rail fastened directly to a concrete slab about a thick, without ties. A similar but less expensive alternative is to accurately position concrete ties and then pour a concrete slab between and around them. This method is called \"cast-in precast sleeper track\".\n\nThese systems offer the advantage of superior stability and almost complete absence of deformation. Ballastless track systems incur significantly lower maintenance costs compared to ballasted track. Due to the absence of any ballast, damage by flying ballast is eliminated, something that occurs at speeds in excess of . It is also useful in renovating existing railroad tunnels. Because slab track allows shallower construction than ballasted track, it may provide the extra overhead clearance necessary for converting a line to overhead electrification, or allow the passage of trains with a greater loading gauge.\n\nBuilding a slab track is more expensive than building traditional ballasted track, which has slowed its introduction outside of high-speed rail lines. Slab track is also difficult to modify after it has been installed, and the curing time of concrete makes it difficult to convert an existing, busy railway line to a ballastless set-up.\n\nSlab track can also generate significantly more noise pollution and cause more vibration than traditional ballasted track. While this is partly attributable to slab track's decreased sound absorption qualities, a more significant factor is that slab track typically uses softer rail fasteners to provide vertical compliance similar to ballasted track, and these can lead to more noise, because they permit the rail to vibrate over a greater length.\n\nWhere it is critical to reduce noise and vibration, the concrete slab can be supported on soft resilient bearings. This configuration, called \"floating slab track\", is expensive and requires more depth or height in the track structure, but it can reduce noise and vibration by around 80%. GERB quote up to 35-40dB of attenuation using their steel sprung system. Alternatively, the rail can be supported along its length by an elastic material; when combined with a smaller rail section, this can provide a significant noise reduction over traditional ballasted track.\n\n\n"}
{"id": "34302907", "url": "https://en.wikipedia.org/wiki?curid=34302907", "title": "Renewable energy policy of Bangladesh", "text": "Renewable energy policy of Bangladesh\n\nThe renewable energy policy of Bangladesh is a set of policies and programs set by the Government of Bangladesh to reach national goals in the field of renewable energy in the country.\n\nVarious government regimes have drafted such policies to supplement the Electricity sector in Bangladesh. The country faces mismanaged distribution and supply of electricity which is why it faces up to 2000 megawatts of electricity shortage. This has forced hundreds of manufacturing firms across the country to shut down taking a toll on the national GDP.\n\n"}
{"id": "21921011", "url": "https://en.wikipedia.org/wiki?curid=21921011", "title": "Retrograde condensation", "text": "Retrograde condensation\n\nRetrograde condensation occurs when gas in a tube is compressed beyond the point of condensation with the effect that the liquid evaporates again. This is the opposite of condensation: the so-called retrograde condensation.\n\nIf the volume of two gases that are kept at constant temperature and pressure below critical conditions is gradually reduced, condensation will start. When a certain volume is reached, the amount of condensation will gradually increase upon further reduction in volume until the gases are liquefied. If the composition of the gases lies between their true and pseudo critical points the condensate formed will disappear on continued reduction of volume. This disappearance of condensation is called retrograde condensation.\n\nBecause most natural gas found in petroleum reservoirs is not a pure product, when non-associated gas is extracted from a field under supercritical pressure/temperature conditions (i.e., the pressure in the reservoir decreases below dewpoint), condensate liquids may form during the isothermic depressurizing, an effect called retrograde condensation.\n\nJohannes Petrus Kuenen discovered retrograde condensation and published his findings in April 1892 with the title \"Metingen betreffende het oppervlak van Van der Waals voor mengsels van koolzuur en chloormethyl\"\n(Measurements on the Van der Waals surface for mixtures of carbonic acid and methyl chloride).\n"}
{"id": "39516879", "url": "https://en.wikipedia.org/wiki?curid=39516879", "title": "SiC–SiC matrix composite", "text": "SiC–SiC matrix composite\n\nSiC–SiC matrix composite is a particular type of ceramic matrix composite (CMC) which have been accumulating interest mainly as high temperature materials for use in applications such as gas turbines, as an alternative to metallic alloys. CMCs are generally a system of materials that are made up of ceramic fibers or particles that lie in a ceramic matrix phase. In this case, a SiC/SiC composite is made by having a SiC (silicon carbide) matrix phase and a fiber phase incorporated together by different processing methods. Outstanding properties of SiC/SiC composites include high thermal, mechanical, and chemical stability while also providing high strength to weight ratio.\n\nSiC/SiC composites are mainly processed through three different methods. However, these processing methods are often subjected to variations in order to create the desired structure or property:\n\nMechanical properties of CMCs, including SiC–SiC composites can vary depending on the properties of their various components, namely, the fiber, matrix, and interphases. For example, the size, composition, crystallinity, or alignment of the fibers will dictate the properties of the composite. The interplay between matrix microcracking and fiber-matrix debonding often dominates the failure mechanism of SiC/SiC composites. This results in SiC/SiC composites having non-brittle behavior despite being fully ceramic. Additionally, creep rates at high temperatures are also extremely low, but still dependent on its various constituents.\n\nSiC–SiC composites have a relatively high thermal conductivity and can operate at very high temperatures due to their inherently high creep and oxidation resistance. Residual porosity and stoichiometry of the material can vary its thermal conductivity, with increasing porosity leading to lower thermal conductivity and presence of Si–O–C phase also leading to lower thermal conductivity. In general, a typical well processed SiC–SiC composite can achieve a thermal conductivity of around 30 W/m-K at 1000 Celsius.\n\nSince SiC–SiC composites are generally sought for in high temperature applications, their oxidation resistance is of high importance. The oxidation mechanism for SiC–SiC composites vary depending on the temperature range, with operation in the higher temperature range (>1000 °C) being more beneficial than at lower temperatures (<1000 °C). In the former case, passive oxidation generates a protective oxide layer wheres in the latter case, oxidation degrades the fiber-matrix interface. Nonetheless, oxidation is an issue and environmental barrier coatings are being investigated to address this issue.\n"}
{"id": "21821831", "url": "https://en.wikipedia.org/wiki?curid=21821831", "title": "Sibioara Wind Farm", "text": "Sibioara Wind Farm\n\nThe Sibioara Wind Farm is a proposed wind power project in Sibioara, Vaslui County, Romania. It will have 42 individual wind turbines with a nominal output of around 1 MW which will deliver up to 42 MW of power, enough to power over 26,552 homes, with a capital investment required of approximately US$50 million.\n"}
{"id": "30325091", "url": "https://en.wikipedia.org/wiki?curid=30325091", "title": "Sodium tungsten bronze", "text": "Sodium tungsten bronze\n\nSodium tungsten bronze is a form of insertion compound with the formula NaWO, where \"x\" is equal to or less than 1. Named due to its metallic lustre, its electrical properties range from semiconducting to metallic depending on the concentration of sodium ions present; it can also exhibit superconductivity.\n\nPrepared in 1823 by the chemist Friedrich Wöhler, sodium tungsten bronze was the first alkali metal bronze to be discovered.\nThey owe some of their properties to the relative stability of the tungsten(V) cation that is formed. A similar family of molybdenum bronzes may have been discovered in 1885 by Alfred Stavenhagen and E. Engels, but they are formed in a very narrow range of temperatures and were not reported again until the 1960s.\n\nSodium tungsten bronze, like other tungsten bronzes, is resistant to chemical reaction under both acidic and basic conditions. Colour is dependent upon the proportion of sodium in the compound, ranging from golden at \"x\"≈0.9, through red, orange and deep purple, to blue-black when \"x\"≈0.3.\n\nThe electrical resistivity of the bronze depends on the proportion of sodium in the compound, with specific resistances of 1.66 mΩ being measured for some samples. It has been suggested that electrons, released when the sodium atoms are ionised, are conducted readily through the tungsten t and oxygen π orbitals. This can be observed in the XPS and UPS spectra: the peak representing the tungsten 5\"d\" band becomes more intense as \"x\" rises.\n\nFor values of \"x\" below 0.3, the bronze is semiconducting rather than metallic. When cooled sufficiently, sodium tungsten bronze becomes a superconductor, with the critical temperature (\"T\") for NaWO being approximately 2.2 kelvin. The first record of superconductivity in a tungsten bronze was in 1964, with a \"T\" of 0.57 K.\n\nWhen \"x\"=1, sodium tungsten bronze adopts a cubic phase: the perovskite crystal structure. In this form, the structure consists of corner-sharing WO octahedra with sodium ions in the interstitial gaps. For \"x\" values between 0.9 and 0.3, the structure remains similar but with an increasing deficiency of sodium ions and a smaller lattice parameter.\n\nA number of other structure types can also be adopted, with varying electrical properties: cubic, tetragonal I and hexagonal phases are metallic, whereas orthorhombic and tetragonal II structures are semiconducting.\n\nWöhler's 1823 synthesis involved reducing sodium tungstate and tungsten trioxide with hydrogen gas at red heat. A more modern approach reduces a melt of the reactants with electricity rather than with hydrogen. Microwave synthesis is also possible, using tungsten powder as the reducing agent. Hydrothermal (both batch and flow) syntheses are also possible.\n\nThe sodium in this compound can be replaced by other alkali metals to form their tungsten bronzes, and by other metals such as tin and lead. Molybdenum bronzes also exist but are less stable than their tungsten counterparts.\n"}
{"id": "2581444", "url": "https://en.wikipedia.org/wiki?curid=2581444", "title": "Spring steel", "text": "Spring steel\n\nSpring steel is a name given to a wide range of steels used in the manufacture of springs, prominently in automotive and industrial suspension applications. These steels are generally low-alloy manganese, medium-carbon steel or high-carbon steel with a very high yield strength. This allows objects made of spring steel to return to their original shape despite significant deflection or twisting.\n\nMany grades of steel can be hardened and tempered to suit application as a spring; however, some steels exhibit more desirable characteristics for spring applications.\n\n\n\nBibliography\n"}
{"id": "15398510", "url": "https://en.wikipedia.org/wiki?curid=15398510", "title": "String ribbon", "text": "String ribbon\n\nRibbon solar cells are a 1970s technology most recently sold by Evergreen Solar (which is now in receivership, i.e. bankrupt and liquidated), among other manufacturers.\n\nRibbon growth is a method of producing multi-crystalline silicon strips suitable for the photovoltaic industry.\n\nThe name describes the manufacturing process, where a sheet of silicon the ribbon is pulled vertically from a bath of molten silicon to form a multi-crystalline silicon crystals. The ribbon is then cut into lengths which are treated with traditional processes to form solar cells. The process was developed in the 1970s by Mobil-Tyco, Solar Energy Corp., Energy Materials, Corp., Motorola and IBM. Ribbons 4-5 inches wide and less than 1/100th of an inch thick were made. It is similar to the dendrite process, developed by Westinghouse in the 1970s, which had the further advantage of having no ribbon (graphite or ceramic die), using only two special seed crystals, or \"dendrites,\" which were dipped side by side into molten silicon and pulled up slowly. These demonstrated efficiencies upwards of 16% by 1980.\n\nRibbon growth has the capability of using less silicon compared to other wafer production methods as wafers are manufactured to the approximately correct specification avoiding the need for sawing of silicon blocks. Silicon accounts for more than 50% of manufacturing costs in producing first generation solar cells, where much of the silicon is discarded as waste at the sawing stage of manufacture. Employing the string ribbon process allows the manufacture of PV grade silicon wafers to the approximate dimensions while avoiding the waste encountered when sawing wafers from ingots. This manufacturing process uses about half the amount of input silicon required by traditional processes.\n\nString Ribbon technology is a technique where the ribbon is pulled from the silicon melt between two wires, it is not capable of achieving the same electrical performance as conventional wafer technology. Typically a cut wafer will convert 18-20% of the incoming light into electricity where String Ribbon Solar Cells are capable of converting 13-14%. In research laboratories the technology has reached as high as 18.3%, however it cannot be produced commercially to this specification. Wafer technologies have reached as high as 25% in laboratory conditions.\n\nWhile String Ribbon technology has certain advantages as to the shape of the crystals, the overall thickness varies enough so that not every 'silicon strip' can be processed directly into a solar cell. In addition to this drawback, the growth process is thermally very inefficient. The radiating area/gram of crystal is extremely high, leading to very high energy expenses which offset the reduced silicon use/expense.\n\n\n"}
{"id": "201308", "url": "https://en.wikipedia.org/wiki?curid=201308", "title": "Styrofoam", "text": "Styrofoam\n\nStyrofoam is a trademarked brand of closed-cell extruded polystyrene foam (XPS), commonly called \"Blue Board\" manufactured as foam continuous building insulation board used in walls, roofs, and foundations as thermal insulation and water barrier. This material is light blue in color and is owned and manufactured by The Dow Chemical Company.\n\nIn the United States and Canada, the colloquial use of the word \"styrofoam\" refers to another material that is usually white in color and made of expanded (not extruded) polystyrene foam (EPS). It is often used in disposable coffee cups and coolers, and as cushioning material in packaging. The trademarked term is used generically although it is a different material from the extruded polystyrene used for Styrofoam insulation. \n\nThe Styrofoam brand polystyrene foam, which is used for craft applications, can be identified by its roughness and the \"crunch\" it makes when cut. Additionally, it is moderately soluble in many organic solvents, cyanoacrylate, and the propellants and solvents of spray paint.\n\nIn 1947, researchers in Dow's Chemical Physics Lab found a way to make foamed polystyrene. Led by Ray McIntire, they rediscovered a method first used by Swedish inventor Carl Georg Munters.\nDow acquired exclusive rights to use Munters' patents and found ways to make large quantities of extruded polystyrene as a closed cell foam that resists moisture.\n\nStyrofoam has a variety of uses. Dow produces Styrofoam building materials, including varieties of building insulation sheathing and pipe insulation. The claimed R-value of Styrofoam insulation is five per inch.\n\nStyrofoam can be used under roads and other structures to prevent soil disturbances due to freezing and thawing.\n\nStyrofoam is composed of 98% air, making it lightweight and buoyant.\n\nDow also produces Styrofoam as structural insulated panels for use by florists and in craft products. Dow insulation Styrofoam has a distinctive blue color; Styrofoam for craft applications is available in white and green.\n\nThe EPA and International Agency for Research on Cancer consider styrene a possible human carcinogen.\n\n\n"}
{"id": "8091385", "url": "https://en.wikipedia.org/wiki?curid=8091385", "title": "Thai Airways International Flight 261", "text": "Thai Airways International Flight 261\n\nThai Airways International Flight 261 (TG261/THA261) was a scheduled domestic passenger flight from Bangkok's Don Mueang International Airport, Thailand to Surat Thani International Airport in Surat Thani, Thailand. The flight was operated by Thai Airways International, the flag carrier of Thailand. On 11 December 1998, the aircraft, an Airbus A310-204 registered in Thailand as HS-TIA, stalled and crashed in a rice paddy on its landing attempt at Surat Thani Airport. A total of 101 people were killed in the crash.\n\nThailand's Aircraft Accident Investigation Committee (AAIC) opened an investigation into the accident. The investigation revealed that the crew had become disoriented. Visibility was limited. Stress caused the crew to lose control of the aircraft. The AAIC noted also Surat Thani's minimal lighting and faulty warnings to the aircraft.\n\nHS-TIA was an Airbus A310-204, c/n 415, previously registered as F-WWBI for flight testing with Airbus. Given the name \"Phitsanulok\", HS-TIA was first flown on 3 March 1986 and delivered to Thai Airways at Don Mueang on 29 April 1986.\n\nThere were 25 foreigners on board the flight including nationalities from Austria, Australia, Britain, Finland, Germany, Japan, Norway, and the United States. Among the survivors were three Australians, three Japanese, three Germans, two Israelis and one Briton.\n\nWeather conditions were rainy at the time of the crash and the crew was unable to see the runway. At 19:05 local time, the aircraft was attempting to land on runway 22 for the third time following two aborted landings. The captain ordered another go-around attempt and when the pitch attitude reached approximately 48 degrees the aircraft entered an aerodynamic stall, began to lose altitude and crashed into the ground, bursting into flames. 101 of the 146 passengers and crew died at the scene. 45 were rushed to hospital.\n\nAirbus, the aircraft manufacturer, sent a team of specialists to assist Thai authorities in the crash investigation. Both the flight data recorder and the cockpit voice recorder were found by the search and rescue team and were taken from the crash site for further investigation. Investigators stated that bad weather was the probable cause of the accident, without ruling out pilot error. There was a project to extend the runway at Surat Thani Airport, however the project was delayed due to an economic downturn. A Thai air force pilot stated that due to the removal of the Instrument landing system (ILS), pilots had to use a radio navigation system which was less accurate.\n\nThai Airways International offered compensation payment to the families affected by the crash. Thai Airways International chairman, Thamnoon Wanglee, stated on a news conference that each relatives of the 101 victims of the crash would receive a compensation payment of US$100,000, while the 45 injured survivors would receive a compensation of 200,000 baht (US$5,600) each. The airline would pay their medical expenses.\n\n\n\n"}
{"id": "5837338", "url": "https://en.wikipedia.org/wiki?curid=5837338", "title": "The Revenge of Gaia", "text": "The Revenge of Gaia\n\nThe Revenge of Gaia: Why the Earth is Fighting Back – and How we Can Still Save Humanity (2006) is a book by James Lovelock. Some editions of the book have a different, less optimistic subtitle: Earth's Climate Crisis and the Fate of Humanity.\n\nThe book introduces the concept of the anti-CLAW hypothesis. Lovelock proposed that instead of providing negative feedback in the climate system, the components of the CLAW hypothesis may act to create a positive feedback loop.\n\nUnder future global warming, increasing temperature may stratify the world ocean, decreasing the supply of nutrients from the deep ocean to its productive euphotic zone. Consequently, phytoplankton activity will decline with a concomitant fall in the production of dimethyl sulfide (DMS). In a reverse of the CLAW hypothesis, this decline in DMS production will lead to a decrease in cloud condensation nuclei and a fall in cloud albedo. The consequence of this will be further climate warming which may lead to even less DMS production (and further climate warming). The figure to the right shows a summarising schematic diagram.\n\nEvidence for the anti-CLAW hypothesis is constrained by similar uncertainties as those of the sulfur cycle feedback loop of the CLAW hypothesis. However, researchers simulating future oceanic primary production have found evidence of declining production with increasing ocean stratification, leaving open the possibility that such a mechanism may exist.\n\n\n"}
{"id": "43979", "url": "https://en.wikipedia.org/wiki?curid=43979", "title": "Tórshavn", "text": "Tórshavn\n\nTórshavn (; 'Thor's harbour'; , ) is the capital and largest town of the Faroe Islands. Tórshavn is in the southern part on the east coast of Streymoy. To the northwest of the city lies the mountain Húsareyn, and to the southwest, the Kirkjubøreyn. They are separated by the Sandá River. The town proper has a population of 13,089 (2017), and the greater urban area a population of 21,000.\n\nThe Norse established their parliament on the Tinganes peninsula in AD 850. Tórshavn thus became the capital of the Faroe Islands and has remained so ever since. All through the Middle Ages the narrow peninsula jutting out into the sea made up the main part of Tórshavn. Early on, Tórshavn became the centre of the islands' trade monopoly, thereby being the only legal place for the islanders to sell and buy goods. In 1856, the trade monopoly was abolished and the islands were left open to free trade.\n\nIt is not known whether the site of Tórshavn was of interest to the Celtic monks who were probably the first settlers in the Faroes. The Viking settlers in the 9th century established their own parliaments, called \"tings\", in different parts of the islands, it being the tradition in each case to hold the \"ting\" at a neutral and thus uninhabited place, so no one location gave anyone an advantage. The main \"ting\" for the islands was convoked in Tórshavn in 825, on Tinganes, the peninsula that divides the harbour into two parts: \"Eystaravág\" and \"Vestaravág\". The Vikings would thus meet on the flat rocks of Tinganes every summer, as the most central place on the islands, although there was no settlement at Tinganes at that time. The Færeyinga Saga says: \"the place of the \"ting\" of the Faroese was on Streymoy, and there is the harbour that is called Tórshavn\". The Viking age ended in 1035. The \"ting\" was followed by a market which gradually grew into a permanent trading area.\n\nAll through the Middle Ages, the narrow peninsula jutting out into the sea made up the main part of Tórshavn. It belonged to the outfield of two farmers. Unlike other Faroese villages, Tórshavn was never a distinct farming community. During the 12th century, all trade between Norway and the Faroes, along with other tributary islands to the west, became centralised in Bergen. In 1271, a royal trade monopoly was established in Tórshavn by the Norwegian Crown. According to a document from 1271, two ships would sail regularly to Tórshavn from Bergen with cargoes of salt, timber and cereal. Tórshavn therefore had more contact with the outside world than did the other villages. Under the Norwegian, and then Danish rule, government officials made Tórshavn their home. All of these things, combined with the fact that Tórshavn was the seat of the \"ting\" of the islands, influenced the town's development.\n\nSources do not mention a built-up area in Tórshavn until after the Protestant reformation in 1539. In ca. 1580 a small fort, Skansin, was built by the Faroese naval hero and trader Magnus Heinason at the north end of the harbour. Later small fortifications were built at Tinganes.\n\nIn 1584 Tórshavn had 101 inhabitants. The population was divided into three equally large groups made up of farmers, their families and servants, trade and government officials and people who owned no land and therefore not much else; this included the landless proletariat from the villages that during this period came to Tórshavn in search of work. They were set to guard duty on Skansin without pay, and for clothing and food they depended on the bounty of the farmers.\n\nIn 1655 king Frederick III of Denmark granted the Faroe Islands to his favourite statesman Kristoffer Gabel, the rule of the von Gabel Family, 1655–1709, is known as \"Gablatíðin\". It is the darkest chapter in the history of Tórshavn. Gabel's administration suppressed the islanders in various ways. The trade monopoly was in the family's hands and it was not designed for the needs of the Faroese people. People across the country brought products into town and had to be satisfied with whatever price they were given. At the same time imported goods were limited and expensive. There came considerable complaints from the islands' inhabitants of unjust treatment by the civil administration in Tórshavn. These not only included the persons in charge of the monopoly trade, but also the bailiff and others. It was during this period, in 1673, that Tinganes was ravaged by a fire after a store of gunpowder kept at Tinganes had blown up. Many old houses burnt to the ground and old Faroese records were lost as were Gabel's documents.\n\nConditions improved in Tórshavn when the trade monopoly became a royal monopoly in 1709. The royal monopoly was supplied with goods from Copenhagen three times a year. However, in 1709 Tórshavn was hit by a plague of smallpox, killing nearly the entire population. The town had by this time reached a population of 300 and 250 of the inhabitants died. Still, it was during the latter half of the 18th century that Tórshavn started to develop into a small town. This was while Niels Ryberg was in charge of the trade monopoly. From 1768 and during the next 20 years onwards Ryberg was allowed to carry on an entrepot trade which was mainly based on smuggling to England. Because of the French-British conflict there was room for this kind of operation. In Tórshavn his warehouses filled up with goods. Ryberg was the first person who thought of making a financial profit from fishing, which later became the most important economic factor to the islands. He experimented with salted cod and herring but at this point in time nothing much beyond this happened.\n\nTórshavn Cathedral was first built in 1788 and partly rebuilt in 1865. Since 1990, it has been the seat of the Bishop of the Faroe Islands (in the Church of the Faroe Islands).\n\nOn 30 March 1808, during the Anglo-Danish Gunboat War, the Cruizer class brig-sloop HMS \"Clio\" entered Tórshavn and briefly captured the fort at Skansin. The fort surrendered without firing a shot as the landing party approached. The \"Clio\"'s men spiked the fort's eight 18-pounder guns and took all the smaller guns and weapons before leaving. Shortly after 6 May a German privateer who had assumed the name \"Baron von Hompesch\" plundered the defenceless city and seized the property of the Danish Crown Monopoly. The Admiralty Prize Court, however, refused to condemn it as a lawful prize. \n\nIn 1856, free trade came to the Faroe Islands. By opening the islands to the world, it transformed the economy, with Tórshavn at its centre. The farming land was rented to townspeople who could later buy it if they wished. These small plots of land enabled people to keep cows and sheep. \n\nIn 1866, Tórshavn's town council was founded. The town has been the capital of the Faroe Islands ever since. Later, in 1909, Tórshavn became a market town with the same municipal charter as Danish market towns.\n\nIn 1927, Tórshavn had a modern harbour built. This made it possible for larger ships to berth.\n\nDuring the British occupation of the Faroe Islands in World War II, Skansin was used as the headquarters of the Royal Navy Command, and two 5.5-inch guns used aboard before World War II were deployed.\n\nIn 1974, the neighbouring villages Hoyvík and Hvítanes were made part of the town area. Later, even more municipalities joined the Tórshavn municipality. In 1978 Kaldbak, in 1997 Argir, in 2001 Kollafjørður, and finally in 2005, Kirkjubøur, Hestur, and Nólsoy.\n\nTórshavn features a subpolar oceanic climate (\"Cfc\") with average summer highs around and average winter highs around , and with winter lows averaging just above freezing, and frequent cloudy skies. Average monthly precipitation is highest in autumn and winter, peaking in January, owing to frequent, intense storms crossing the area from the North Atlantic Ocean, while May and June are markedly drier months. Tórshavn is among the cloudiest places in the world, with significant sunshine records at only about 2.4 hours of sunshine per day; however, no data exists for places such as the Aleutian Islands or parts of southern Chile, which may have even less sun. Because of the cloudy weather and the ice-free water surrounding Torshavn, its winter temperatures are exceptionally mild for such a northerly location. Summer temperatures are much lower than those found in continental Scandinavia on similar parallels. The temperature amplitude in the period from 1961 to 2010 is a mere between the absolute warmest and coldest temperatures.\n\nTórshavn is the capital of the Faroe Islands, and as such is the seat of the Faroes’ self rule government. The government holds the executive power in local government affairs. Today a part of the government is located on the Tinganes peninsula of Tórshavn, the Prime Minister's office is here and the Ministry of Internal Affairs was here until it was closed in 2013. The other ministries are located in other office buildings in various places in Tórshavn, i.e. the Ministry of Health and the Ministry of Social Affairs are located near the Hospital of the Faroes in Eirargarður, and the Ministry of Finance is located in Argir in a building called Albert Hall on the street Kvíggjartún. The parliament, the Løgting, which was originally located on Tinganes, was relocated to the town square, Vaglið, in 1856.\n\nTórshavn, as the capital city, is the centre of sport in the islands; the largest sports centre is located in the Gundadalur district of Tórshavn. Also, the largest football stadium, Tórsvøllur, is located here, seating 6,000 spectators. The stadium serves as home to the Faroe Islands national football team. Around the city there are also two other football pitches, indoor tennis courts, badminton courts and a swimming pool.\n\nThe city has several football clubs, including three Premier League teams: HB Tórshavn, B36 Tórshavn and Argja Bóltfelag. Other football clubs with connections to the city are FF Giza (Nólsoy), FC Hoyvík and Undrið FF. Handball is the second most popular sport in Tórshavn. The city's handball teams are Kyndil, Neistin and Ítróttafelagið H71 and the Faroe Island's national handball team practice in the city. Tórshavn city has several popular rowing clubs, including, Havnar Róðrarfelag and Róðrarfelagið Knørrur.\n\nEvery year in July the Tour of Faroe Islands, which is a road bicycle race, is held around the islands. The race is called \"Kring Føroyar\" (Tour de Faroe / Around the Faroes), it starts in Klaksvík and ends in Tórshavn.\n\nThe Tórshavn Jazz Festival has been held annually since 1983. It attracts musicians from all over North America and Europe and has become a popular tourist event.\n\nThe harbour is served by the Smyril Line international ferry service to Denmark and Iceland. The harbour is also used by domestic ferry services of Strandfaraskip Landsins within the Faroe Islands, chiefly on the route to Tvøroyri.\n\nThe town is served by Bussleiðin - a network of local buses, with the service identified by its red livery. Tórshavn's Bussleiðin has five routes and is operated by the Tórshavn municipality. Buses within Tórshavn have been completely free of charge since 2007. This is a green initiative intended to persuade people to use public transportation rather than drive their cars. Like Bygdaleiðir, the actual buses are privately owned, but contracted to Bussleiðin. Buses also depart to villages throughout the islands.\n\nThere is a helipad in Tórshavn; the nearest airport is Vágar Airport.\n\n\n\n\nTórshavn is twinned with:\n\n\n\n"}
{"id": "17352520", "url": "https://en.wikipedia.org/wiki?curid=17352520", "title": "UNSW School of Photovoltaic and Renewable Energy Engineering", "text": "UNSW School of Photovoltaic and Renewable Energy Engineering\n\nThe School of Photovoltaic and Renewable Energy Engineering (SPREE) is one of ten schools in the UNSW Faculty of Engineering at the University of New South Wales.\n\nIt was officially recognised as a school on 1 January 2006, after having previously been known as the Centre for Photovoltaic Engineering.\n\nAs of 2003, the school includes the Australian Research Council (ARC) Photovoltaics Centre of Excellence.\n\nThe school offers two undergraduate coursework programs, combined undergraduate programs with science, arts, commerce or law, a Master of Engineering Science postgraduate coursework program, and three research programs:- Master of Philosophy, Master of Engineering by Research and a Doctor of Philosophy.\n\n\n\n"}
{"id": "19926164", "url": "https://en.wikipedia.org/wiki?curid=19926164", "title": "VORTEX projects", "text": "VORTEX projects\n\nThe Verification of the Origins of Rotation in Tornadoes Experiment or VORTEX are field projects that study tornadoes. VORTEX1 was the first time scientists completely researched the entire evolution of a tornado with an array of instrumentation, enabling a greater understanding of the processes involved with tornadogenesis. A violent tornado near Union City, Oklahoma was documented in its entirety by chasers of the Tornado Intercept Project (TIP) in 1973 and visual observations led to advancement in understanding of tornado structure and life cycles. VORTEX2 utilized enhanced technology allowing scientists to improve forecasting capabilities to improve advanced warnings to residents. VORTEX2 is seeking to explain how tornadoes form, how long they last and why they last that long, and what causes them to dissipate.\n\nThe field research phase of the VORTEX2 project concluded on July 6, 2010.\n\nThe VORTEX1 project sought to understand how a tornado is produced by deploying around 18 vehicles that were equipped with customized instruments used to measure and analyze the weather around a tornado. The project has also stated that it is interested in why some supercells, or mesocyclones within such storms, produce tornadoes while others do not. It also concerned itself with why some supercells form violent tornadoes versus weak tornadoes.\n\nThe original project took place in 1994 and 1995, while several smaller studies, such as SUB-VORTEX and VORTEX-99, were conducted from 1996 to 2008. VORTEX1 documented the entire life cycle of a tornado with significant instrumentation surrounding it for the first time. Severe weather warnings improved after the research collected from VORTEX1 and many believe that VORTEX1 contributed to this improvement. “An important finding from the original VORTEX experiment was that the factors responsible for causing tornadoes happen on smaller time and space scales than scientists had thought. New advances will allow for a more detailed sampling of a storm’s wind, temperature and moisture environment and lead to a better understanding of why tornadoes form –-and how they can be more accurately predicted,” said Stephan Nelson, NSF program director for physical and dynamic meteorology.\n\nVORTEX had the capability to fly Doppler weather radar above the tornado approximately every five minutes.\n\nVORTEX research helped the National Weather Service (NWS) to provide tornado warnings to residents with a lead time of 13 minutes. A federal research meteorologist, Don Burgess, estimates that the \"false alarms\" pertaining to severe weather by the National Weather Service have declined by 10 percent.\n\nThe movie \"Twister\" was at least partially inspired by the VORTEX project.\n\nVORTEX2 was an expanded second VORTEX project with field phases from 10 May until 13 June 2009 and 1 May until 15 June 2010. VORTEX2's goals were studying why some thunderstorms produce tornadoes while others do not, how to make more accurate and longer lead time tornado forecasts and warnings, and tornado structure. VORTEX2 was by far the largest and most ambitious tornado study ever with over 100 scientific participants from many different universities and research laboratories.\n\n\"We still do not completely understand the processes that lead to tornado formation and shape its development. We hope that VORTEX2 will provide the data we need to learn more about the development of tornadoes and in time help forecasters give people more advance warning before a tornado strikes,\" said Roger Wakimoto, director of the Earth Observing Laboratory (EOL) at the National Center for Atmospheric Research (NCAR), and a principal investigator for VORTEX2.\n\n\"Then you can get first responders to be better prepared—police, fire, medical personnel, even power companies. Now, that's not even remotely possible,\" said Stephan P. Nelson, a program director in the atmospheric sciences division of the National Science Foundation (NSF).\n\nJoshua Wurman, president of the Center for Severe Weather Research (CSWR) in Boulder, Colorado proposes, \"if we can increase that lead time from 13 minutes to half an hour, then the average person at home could do something different. Maybe they can seek a community shelter instead of just going into their bathtub. Maybe they can get their family to better safety if we can give them a longer warning and a more precise warning.\"\n\nVORTEX2 deployed 50 vehicles customized with mobile radar, including the Doppler On Wheels (DOW) radars, SMART radars, the NOXP radar, a fleet of instrumented vehicles, unmanned aerial vehicles, deployable instrument arrays called Sticknet and Podnet, and mobile weather balloon launching equipment. Over 100 scientists and crew researched tornadoes and supercell thunderstorms in the \"Tornado Alley\" region of the United States' Great Plains between Texas and Minnesota. A number of institutions and countries were involved in the US$11.9 million project, including: Finland, the National Weather Service, the Bureau of Meteorology in Australia, Italy, the Netherlands, and the United Kingdom, the National Oceanic and Atmospheric Administration (NOAA), Environment Canada, universities across the United States, and the NOAA Storm Prediction Center (SPC).\n\nThe project included DOW3, DOW6, DOW7, Rapid-Scan DOW, SMART-RADARs, NOXP, UMASS-X, UMASS-W, and CIRPAS for their mobile radar contingent. The Doppler on Wheels were supplied by the Center for Severe Weather Research, and the SMART-Radars from the University of Oklahoma (OU). National Severe Storms Laboratory (NSSL) supplied the NOXP radar, as well as several other radar units from the University of Massachusetts Amherst, the Office of Naval Research (ONR) and Texas Tech University (TTU). NSSL and CSWR supplied mobile mesonets. Mobile radiosonde launching vehicles are provided by NSSL, NCAR, and the State University of New York at Oswego (SUNY Oswego). There were quite a few other deployable state of the art instrumentation, such as Sticknets from TTU, tornado PODS from CSWR, and four disdrometers from University of Colorado CU, and the University of Illinois at Urbana-Champaign (UIUC).\n\nVORTEX2 technology allowed trucks with radar to be placed in and near tornadic storms and allowed continuous observations of the tornadic activity. Howard Bluestein, a meteorology professor at the University of Oklahoma said, \"We will be able to distinguish between rain, hail, dust, debris, flying cows.\"\n\nAdditionally, photogrammetry teams, damage survey teams, unmanned aircraft, and weather balloon launching vans helped to surround the tornadoes and thunderstorms. The equipment amassed enabled three dimensional data sets of the storms to be collected with radars and other instruments every 75 seconds (more frequently for some individual instruments), and resolution of the tornado and tornadic storm cells as close as .\n\nScientists met May 10 and held a class which taught the crews how to launch the tornado pods which will need to be sent off within 45 seconds. VORTEX2 was equipped with 12 tornado PODS which were instruments mounted onto towers which measure wind velocity (i.e. speed and direction). The aim was that some of the measurements be taken in the centre of the tornado. Once the pods are deployed, the teams repeat the process at the next location until finally the teams return to the south of the tornado to retrieve the pods with the recorded data. The process is then repeated again. This happens within or 4 minutes away from the tornado itself.\n\nThe team had 24 high portable Sticknets which can be set up at various locations around tornado storm cells to measure wind fields, provide atmospheric readings, and record acoustically the hail and precipitation.\n\nScientists are still seeking to refine understanding of which supercell thunderstorms which form mesocyclones will further produce tornadoes, by which processes, storm-scale interactions, and within which atmospheric environments.\nUpdates on the progress of the project were posted on the VORTEX2 home page. The scientists also started a blog of live reports. \"Even though this field phase seems to be the most spectacular and seems like it's a lot of work, by far the majority of what we're doing is when we go back to our labs, when we work with each other, when we work with our students to try to figure out just what is it that we've collected,\" Wurman said. \"It's going to take years to digest this data and to really get the benefit of this.\" Penn State University will feature the public release of the initial scientific findings in the fall.\n\nThe forecasters were determining the best probability of sighting a tornado. As the trucks traveled to Clinton, OK from Childress, TX, they found mammatus clouds, and lightning at sundown on May 13, 2009.\n\nThe project finally encountered its first tornado on the afternoon of June 5 when they successfully intercepted a tornado in southern Goshen County, Wyoming which lasted for approximately 25 minutes. One of their vehicles, Probe 1, suffered hail damage during the intercept. Later that evening, embedded Weather Channel reporter Mike Bettes reported that elements of VORTEX2 had intercepted a second tornado in Nebraska. Placement of the armada for this tornado was nearly ideal and it, too, was surrounded for its entire life cycle, making it the most thoroughly observed tornado in history.\n\nThe complete team comprises about 50 scientists and is supplemented by students. A complete listing of principal investigators (PIs) is at http://vortex2.org/. An alphabetical partial listing of VORTEX2 scientists and crew:\n\n\nOther smaller field projects include the previously mentioned SUB-VORTEX (1997–98) and VORTEX-99 (1999), and VORTEX-Southeast (VORTEX-SE) (2016-2018).\n\n\n"}
{"id": "10533506", "url": "https://en.wikipedia.org/wiki?curid=10533506", "title": "Water heat recycling", "text": "Water heat recycling\n\nWater heat recycling (also known as drain water heat recovery, waste water heat recovery, greywater heat recovery, or sometimes shower water heat recovery) is the use of a heat exchanger to recover energy and reuse heat from drain water from various activities such as dish-washing, clothes washing and especially showers. The technology is used to reduce primary energy consumption for water heating.\n\nThe cold water that is put into a water heating device can be preheated using the reclaimed thermal energy from a shower so that the input water doesn't need as much energy to be heated before being used in a shower, dishwasher, or sink. The water entering a storage tank is usually close to 11 °C but by recovering the energy in the hot water from a bath or dishwasher, the temperature of the water entering the holding tank can be elevated to 25 °C, saving energy required to increase the temperature of a given amount of water by 14 °C. This water is then heated up a little further to 37 °C before leaving the tank and going to the average shower.\n\nWhen recycling water from a bath (100-150 litres) or shower (50-80 litres) the waste water temperature is circa 20-25 °C. An in-house greywater recycling tank holds 150-175 litres allowing for the majority of waste water to be stored. Utilizing a built in copper heat exchange with circulation pump the residual heat is recovered and transferred to the cold feed of a combi-boiler or hot-water cylinder, reducing the energy used by the existing central heating system to heat water.\n\nHeating water accounts for 18% of the average household utility bill. Standard units save up to 60% of the heat energy that is otherwise lost down the drain when using the shower.\n\nInstalling a water heat recycler reduces energy consumption and thus greenhouse gas emissions and the overall energy dependency of the household.\n\nTypical retail price for a domestic drain water heat recovery unit ranges from around $400 to $1,000 Canadian. For a regular household, water heating is usually the second highest source of energy demand. The energy savings results in an average payback time for the initial investment of 2–10 years according to Natural Resources Canada, Canadian Center for Housing Technology and US DOE.\n\nA 2-year independent study of waste water heat recovery systems installed into residential houses in the UK found savings of 380kWh and 500kWh per person per year.\n\nThe technology is recognized in Canada and the United States by LEED for homes, and Energy Star for New Homes Canada.\n\nWaste water heat recovery has been independently tested by BRE and can be input into SAP models to increase a buildings energy performance.\n\nA heat pump can be combined with municipal sewage lines to allow a large building's HVAC system recycle the winter heat or summer cool (compared to the outside air) of water flowing out of many homes and businesses.\n\nThe reverse is also possible: heat from air conditioning and industrial chillers can be used to pre-heat water.\n\nOne of the pioneer companies focused on waste water heat recovery for multi unit residential, commercial & public buildings is International Wastewater Heat Exchange Systems \n\n"}
{"id": "47313583", "url": "https://en.wikipedia.org/wiki?curid=47313583", "title": "Zeya Aung", "text": "Zeya Aung\n\nZeya Aung (; also spelt Zeyar Aung) is the incumbent Minister of Construction. He was appointed by President Thein Sein in August 2013, following the transfer of his predecessor, Than Htay, by presidential order, on 25 July 2013. He previously served as Minister of Rail Transportation from September 2012 to July 2013. A military officer, Zeya Aung was Tatmadaw Northern Command Commander from August 2010 to September 2012, as well as a former Commander of the Light Infantry Division (LID) 88. Zeya Aung is the brother-in-law of Ye Htut.\n"}
