{"id": "56437581", "url": "https://en.wikipedia.org/wiki?curid=56437581", "title": "Belfast City and District Water Commissioners", "text": "Belfast City and District Water Commissioners\n\nThe Belfast City and District Water Commissioners was a public body in northern Ireland, established by the Belfast Water Act 1840, to improve the supply of water to the expanding city of Belfast. By 1852, the city was suffering a shortfall in supply of almost one million gallons per day.\n\nThe commissioners were responsible from 1914 for the construction of the Mourne Wall which Northern Ireland Water began to restore in 2017.\n\nBefore the Second World War the commissioners purchased a building that is still known as the Water Office.\n\nIn the later twentieth century, responsibility for providing water services was transferred to central government in Northern Ireland and, eventually, to Northern Ireland Water.\n"}
{"id": "10845121", "url": "https://en.wikipedia.org/wiki?curid=10845121", "title": "Builder's Old Measurement", "text": "Builder's Old Measurement\n\nBuilder's Old Measurement (BOM, bm, OM, and o.m.) is the method used in England from approximately 1650 to 1849 for calculating the cargo capacity of a ship. It is a volumetric measurement of cubic capacity. It estimated the tonnage of a ship based on length and maximum beam. It is expressed in \"tons burden\" (, ), and abbreviated \"tons bm\".\n\nThe formula is:\n\nwhere:\n\nThe Builder's Old Measurement formula remained in effect until the advent of steam propulsion. Steamships required a different method of estimating tonnage, because the ratio of length to beam was larger and a significant volume of internal space was used for boilers and machinery. In 1849, the Moorsom System was created in Great Britain. The Moorsom system calculates the cargo-carrying capacity in cubic feet, another method of volumetric measurement. The capacity in cubic feet is then divided by 100 cubic feet of capacity per gross ton, resulting in a tonnage expressed in tons.\n\nKing Edward I levied the first tax on the hire of ships in England in 1303 based on tons burthen. Later, King Edward III levied a tax of 3 shillings on each \"tun\" of imported wine, equal to £ today (using the last year of Edward III's reign, 1377, as the base year). At that time a \"tun\" was a wine container of 252 gallons weighing about , a weight known today as a long ton or imperial ton. In order to estimate the capacity of a ship in terms of 'tun' for tax purposes, an early formula used in England was:\n\nwhere:\nThe numerator yields the ship's volume expressed in cubic feet.\n\nIf a \"tun\" is deemed to be equivalent to 100 cubic feet, then the tonnage is simply the number of such 100 cubic feet 'tun' units of volume.\n\n\nIn 1678 Thames shipbuilders used a method assuming that a ship's burden would be 3/5 of its displacement. Since tonnage is calculated by multiplying length × beam × draft × block coefficient, all divided by 35 ft³ per ton of seawater, the resulting formula would be:\n\nwhere:\n\nOr by solving :\n\nIn 1694 a new British law required that tonnage for tax purposes be calculated according to a similar formula:\n\nThis formula remained in effect until the Builder's Old Measurement rule was put into use in 1720, and then mandated by Act of Parliament in 1773.\n\n\nThe British took the length measurement from the outside of the stem to the outside of the sternpost; the Americans measured from inside the posts. The British measured breadth from outside the planks, whereas the American measured the breadth from inside the planks. Lastly, the British divided by 94, whereas the Americans divided by 95.\n\nThe upshot was that American calculations gave a lower number than the British. For instance, when the British measured the captured , their calculations gave her a burthen of 1533 tons, whereas the American calculations gave the burthen as 1444 tons. The British measure yields values about 6% greater than the American.\n\nThe US system was in use from 1789 until 1864, when a modified version of the Moorsom System was adopted.\n\n\n"}
{"id": "5906", "url": "https://en.wikipedia.org/wiki?curid=5906", "title": "Carbon dioxide", "text": "Carbon dioxide\n\nCarbon dioxide (chemical formula ) is a colorless gas with a density about 60% higher than that of dry air. Carbon dioxide consists of a carbon atom covalently double bonded to two oxygen atoms. It occurs naturally in Earth's atmosphere as a trace gas. The current concentration is about 0.04% (410 ppm) by volume, having risen from pre-industrial levels of 280 ppm. Natural sources include volcanoes, hot springs and geysers, and it is freed from carbonate rocks by dissolution in water and acids. Because carbon dioxide is soluble in water, it occurs naturally in groundwater, rivers and lakes, ice caps, glaciers and seawater. It is present in deposits of petroleum and natural gas. Carbon dioxide is odorless at normally encountered concentrations. However, at high concentrations, it has a sharp and acidic odor.\n\nAs the source of available carbon in the carbon cycle, atmospheric carbon dioxide is the primary carbon source for life on Earth and its concentration in Earth's pre-industrial atmosphere since late in the Precambrian has been regulated by photosynthetic organisms and geological phenomena. Plants, algae and cyanobacteria use light energy to photosynthesize carbohydrate from carbon dioxide and water, with oxygen produced as a waste product.\n\nIt is a versatile industrial material, used, for example, as an inert gas in welding and fire extinguishers, as a pressurizing gas in air guns and oil recovery, as a chemical feedstock and as a supercritical fluid solvent in decaffeination of coffee and supercritical drying. It is added to drinking water and carbonated beverages including beer and sparkling wine to add effervescence. The frozen solid form of , known as \"dry ice\" is used as a refrigerant and as an abrasive in dry-ice blasting.\n\nCarbon dioxide is the most significant long-lived greenhouse gas in Earth's atmosphere. Since the Industrial Revolution anthropogenic emissions – primarily from use of fossil fuels and deforestation – have rapidly increased its concentration in the atmosphere, leading to global warming. Carbon dioxide also causes ocean acidification because it dissolves in water to form carbonic acid.\n\nCarbon dioxide was the first gas to be described as a discrete substance. In about 1640, the Flemish chemist Jan Baptist van Helmont observed that when he burned charcoal in a closed vessel, the mass of the resulting ash was much less than that of the original charcoal. His interpretation was that the rest of the charcoal had been transmuted into an invisible substance he termed a \"gas\" or \"wild spirit\" (\"spiritus sylvestris\").\n\nThe properties of carbon dioxide were further studied in the 1750s by the Scottish physician Joseph Black. He found that limestone (calcium carbonate) could be heated or treated with acids to yield a gas he called \"fixed air.\" He observed that the fixed air was denser than air and supported neither flame nor animal life. Black also found that when bubbled through limewater (a saturated aqueous solution of calcium hydroxide), it would precipitate calcium carbonate. He used this phenomenon to illustrate that carbon dioxide is produced by animal respiration and microbial fermentation. In 1772, English chemist Joseph Priestley published a paper entitled \"Impregnating Water with Fixed Air\" in which he described a process of dripping sulfuric acid (or \"oil of vitriol\" as Priestley knew it) on chalk in order to produce carbon dioxide, and forcing the gas to dissolve by agitating a bowl of water in contact with the gas.\n\nCarbon dioxide was first liquefied (at elevated pressures) in 1823 by Humphry Davy and Michael Faraday. The earliest description of solid carbon dioxide was given by Adrien-Jean-Pierre Thilorier, who in 1835 opened a pressurized container of liquid carbon dioxide, only to find that the cooling produced by the rapid evaporation of the liquid yielded a \"snow\" of solid .\n\nThe carbon dioxide molecule is linear and centrosymmetric. The carbon–oxygen bond length is 116.3 pm, noticeably shorter than the bond length of a C–O single bond and even shorter than most other C–O multiply-bonded functional groups. Since it is centrosymmetric, the molecule has no electrical dipole. Consequently, only two vibrational bands are observed in the IR spectrum – an antisymmetric stretching mode at 2349 cm and a degenerate pair of bending modes at 667 cm. There is also a symmetric stretching mode at 1388 cm which is only observed in the Raman spectrum.\n\nCarbon dioxide is soluble in water, in which it reversibly forms (carbonic acid), which is a weak acid since its ionization in water is incomplete.\n\nThe hydration equilibrium constant of carbonic acid is formula_1 (at 25 °C). Hence, the majority of the carbon dioxide is not converted into carbonic acid, but remains as molecules, not affecting the pH.\n\nThe relative concentrations of , and the deprotonated forms (bicarbonate) and (carbonate) depend on the pH. As shown in a Bjerrum plot, in neutral or slightly alkaline water (pH > 6.5), the bicarbonate form predominates (>50%) becoming the most prevalent (>95%) at the pH of seawater. In very alkaline water (pH > 10.4), the predominant (>50%) form is carbonate. The oceans, being mildly alkaline with typical pH = 8.2–8.5, contain about 120 mg of bicarbonate per liter.\n\nBeing diprotic, carbonic acid has two acid dissociation constants, the first one for the dissociation into the bicarbonate (also called hydrogen carbonate) ion (HCO):\n\nThis is the \"true\" first acid dissociation constant, defined as formula_2, where the denominator includes only covalently bound HCO and does not include hydrated (aq). The much smaller and often-quoted value near is an \"apparent\" value calculated on the (incorrect) assumption that all dissolved is present as carbonic acid, so that formula_3. Since most of the dissolved remains as molecules, \"K\"(apparent) has a much larger denominator and a much smaller value than the true \"K\".\n\nThe bicarbonate ion is an amphoteric species that can act as an acid or as a base, depending on pH of the solution. At high pH, it dissociates significantly into the carbonate ion (CO):\n\nIn organisms carbonic acid production is catalysed by the enzyme, carbonic anhydrase.\n\n is a weak electrophile. Its reaction with basic water illustrates this property, in which case hydroxide is the nucleophile. Other nucleophiles react as well. For example, carbanions as provided by Grignard reagents and organolithium compounds react with to give carboxylates:\n\nIn metal carbon dioxide complexes, serves as a ligand, which can facilitate the conversion of to other chemicals.\n\nThe reduction of to CO is ordinarily a difficult and slow reaction:\n\nPhotoautotrophs (i.e. plants and cyanobacteria) use the energy contained in sunlight to photosynthesize simple sugars from absorbed from the air and water:\n\nThe redox potential for this reaction near pH 7 is about −0.53 V \"versus\" the standard hydrogen electrode. The nickel-containing enzyme carbon monoxide dehydrogenase catalyses this process.\n\nCarbon dioxide is colorless. At low concentrations the gas is odorless; however, at sufficiently-high concentrations, it has a sharp, acidic odor. At standard temperature and pressure, the density of carbon dioxide is around 1.98 kg/m, about 1.67 times that of air.\n\nCarbon dioxide has no liquid state at pressures below . At 1 atmosphere (near mean sea level pressure), the gas deposits directly to a solid at temperatures below and the solid sublimes directly to a gas above −78.5 °C. In its solid state, carbon dioxide is commonly called dry ice.\n\nLiquid carbon dioxide forms only at pressures above 5.1 atm; the triple point of carbon dioxide is about 5.1 bar (517 kPa) at 217 K (see phase diagram). The critical point is 7.38 MPa at 31.1 °C. Another form of solid carbon dioxide observed at high pressure is an amorphous glass-like solid. This form of glass, called \"carbonia\", is produced by supercooling heated at extreme pressure (40–48 GPa or about 400,000 atmospheres) in a diamond anvil. This discovery confirmed the theory that carbon dioxide could exist in a glass state similar to other members of its elemental family, like silicon (silica glass) and germanium dioxide. Unlike silica and germania glasses, however, carbonia glass is not stable at normal pressures and reverts to gas when pressure is released.\n\nAt temperatures and pressures above the critical point, carbon dioxide behaves as a supercritical fluid known as supercritical carbon dioxide. In this state it is starting (as of 2018) to be used for power generation.\n\nCarbon dioxide can be obtained by distillation from air, but the method is inefficient. Industrially, carbon dioxide is predominantly an unrecovered waste product, produced by several methods which may be practiced at various scales.\n\nThe combustion of all carbon-based fuels, such as methane (natural gas), petroleum distillates (gasoline, diesel, kerosene, propane), coal, wood and generic organic matter produces carbon dioxide and, except in the case of pure carbon, water. As an example, the chemical reaction between methane and oxygen:\n\nIt is produced by thermal decomposition of limestone, by heating (calcining) at about , in the manufacture of quicklime (calcium oxide, ), a compound that has many industrial uses:\nIron is reduced from its oxides with coke in a blast furnace, producing pig iron and carbon dioxide:\n\nCarbon dioxide is a byproduct of the industrial production of hydrogen by steam reforming and the water gas shift reaction in ammonia production. These processes begin with the reaction of water and natural gas (mainly methane). This is a major source of food-grade carbon dioxide for use in carbonation of beer and soft drinks, and is also used for stunning animals such as poultry. In the summer of 2018 a shortage of carbon dioxide for these purposes arose in Europe due to the temporary shut-down of several ammonia plants for maintenance.\n\nAcids liberate from most metal carbonates. Consequently, it may be obtained directly from natural carbon dioxide springs, where it is produced by the action of acidified water on limestone or dolomite. The reaction between hydrochloric acid and calcium carbonate (limestone or chalk) is shown below:\nThe carbonic acid () then decomposes to water and :\nSuch reactions are accompanied by foaming or bubbling, or both, as the gas is released. They have widespread uses in industry because they can be used to neutralize waste acid streams.\n\nCarbon dioxide is a by-product of the fermentation of sugar in the brewing of beer, whisky and other alcoholic beverages and in the production of bioethanol.\nYeast metabolizes sugar to produce and ethanol, also known as alcohol, as follows:\n\nAll aerobic organisms produce when they oxidize carbohydrates, fatty acids, and proteins. The large number of reactions involved are exceedingly complex and not described easily. Refer to (cellular respiration, anaerobic respiration and photosynthesis). The equation for the respiration of glucose and other monosaccharides is:\n\nAnaerobic organisms decompose organic material producing methane and carbon dioxide together with traces of other compounds. Regardless of the type of organic material, the production of gases follows well defined kinetic pattern. Carbon dioxide comprises about 40–45% of the gas that emanates from decomposition in landfills (termed \"landfill gas\"). Most of the remaining 50–55% is methane.\n\nCarbon dioxide is used by the food industry, the oil industry, and the chemical industry.\nThe compound has varied commercial uses but one of its greatest use as a chemical is in the production of carbonated beverages; it provides the sparkle in carbonated beverages such as soda water, beer and sparkling wine.\n\nIn the chemical industry, carbon dioxide is mainly consumed as an ingredient in the production of urea, with a smaller fraction being used to produce methanol and a range of other products, such as metal carbonates and bicarbonates. Some carboxylic acid derivatives such as sodium salicylate are prepared using by the Kolbe-Schmitt reaction.\n\nIn addition to conventional processes using for chemical production, electrochemical methods are also being explored at a research level. In particular, the use of renewable energy for production of fuels from (such as methanol) is attractive as this could result in fuels that could be easily transported and used within conventional combustion technologies but have no net emissions.\n\nCarbon dioxide is a food additive used as a propellant and acidity regulator in the food industry. It is approved for usage in the EU (listed as E number E290), US and Australia and New Zealand (listed by its INS number 290).\n\nA candy called Pop Rocks is pressurized with carbon dioxide gas at about 4 x 10 Pa (40 bar, 580 psi). When placed in the mouth, it dissolves (just like other hard candy) and releases the gas bubbles with an audible pop.\n\nLeavening agents cause dough to rise by producing carbon dioxide. Baker's yeast produces carbon dioxide by fermentation of sugars within the dough, while chemical leaveners such as baking powder and baking soda release carbon dioxide when heated or if exposed to acids.\n\nCarbon dioxide is used to produce carbonated soft drinks and soda water. Traditionally, the carbonation of beer and sparkling wine came about through natural fermentation, but many manufacturers carbonate these drinks with carbon dioxide recovered from the fermentation process. In the case of bottled and kegged beer, the most common method used is carbonation with recycled carbon dioxide. With the exception of British Real Ale, draught beer is usually transferred from kegs in a cold room or cellar to dispensing taps on the bar using pressurized carbon dioxide, sometimes mixed with nitrogen.\n\nCarbon dioxide in the form of dry ice is often used during the cold soak phase in wine making to cool clusters of grapes quickly after picking to help prevent spontaneous fermentation by wild yeast. The main advantage of using dry ice over water ice is that it cools the grapes without adding any additional water that might decrease the sugar concentration in the grape must, and thus the alcohol concentration in the finished wine. Carbon dioxide is also used to create a hypoxic environment for carbonic maceration, the process used to produce Beaujolais wine.\n\nCarbon dioxide is sometimes used to top up wine bottles or other storage vessels such as barrels to prevent oxidation, though it has the problem that it can dissolve into the wine, making a previously still wine slightly fizzy. For this reason, other gases such as nitrogen or argon are preferred for this process by professional wine makers.\n\nCarbon dioxide is often used to \"stun\" animals before slaughter. \"Stunning\" may be a misnomer, as the animals are not knocked out immediately and may suffer distress.\n\nIt is one of the most commonly used compressed gases for pneumatic (pressurized gas) systems in portable pressure tools. Carbon dioxide is also used as an atmosphere for welding, although in the welding arc, it reacts to oxidize most metals. Use in the automotive industry is common despite significant evidence that welds made in carbon dioxide are more brittle than those made in more inert atmospheres. It is used as a welding gas primarily because it is much less expensive than more inert gases such as argon or helium. When used for MIG welding, use is sometimes referred to as MAG welding, for Metal Active Gas, as can react at these high temperatures. It tends to produce a hotter puddle than truly inert atmospheres, improving the flow characteristics. Although, this may be due to atmospheric reactions occurring at the puddle site. This is usually the opposite of the desired effect when welding, as it tends to embrittle the site, but may not be a problem for general mild steel welding, where ultimate ductility is not a major concern.\n\nIt is used in many consumer products that require pressurized gas because it is inexpensive and nonflammable, and because it undergoes a phase transition from gas to liquid at room temperature at an attainable pressure of approximately 60 bar (870 psi, 59 atm), allowing far more carbon dioxide to fit in a given container than otherwise would. Life jackets often contain canisters of pressured carbon dioxide for quick inflation. Aluminium capsules of are also sold as supplies of compressed gas for air guns, paintball markers/guns, inflating bicycle tires, and for making carbonated water. Rapid vaporization of liquid carbon dioxide is used for blasting in coal mines. High concentrations of carbon dioxide can also be used to kill pests. Liquid carbon dioxide is used in supercritical drying of some food products and technological materials, in the preparation of specimens for scanning electron microscopy and in the decaffeination of coffee beans.\n\nCarbon dioxide can be used to extinguish flames by flooding the environment around the flame with the gas. It does not itself react to extinguish the flame, but starves the flame of oxygen by displacing it. Some fire extinguishers, especially those designed for electrical fires, contain liquid carbon dioxide under pressure. Carbon dioxide extinguishers work well on small flammable liquid and electrical fires, but not on ordinary combustible fires, because although it excludes oxygen, it does not cool the burning substances significantly and when the carbon dioxide disperses they are free to catch fire upon exposure to atmospheric oxygen. Their desirability in electrical fire stems from the fact that, unlike water or other chemical based methods, Carbon dioxide will not cause short circuits, leading to even more damage to equipment. Because it is a gas, it is also easy to dispense large amounts of the gas automatically in IT infrastructure rooms, where the fire itself might be hard to reach with more immediate methods because it is behind rack doors and inside of cases. Carbon dioxide has also been widely used as an extinguishing agent in fixed fire protection systems for local application of specific hazards and total flooding of a protected space. International Maritime Organization standards also recognize carbon dioxide systems for fire protection of ship holds and engine rooms. Carbon dioxide based fire protection systems have been linked to several deaths, because it can cause suffocation in sufficiently high concentrations. A review of systems identified 51 incidents between 1975 and the date of the report (2000), causing 72 deaths and 145 injuries.\n\nLiquid carbon dioxide is a good solvent for many lipophilic organic compounds and is used to remove caffeine from coffee. Carbon dioxide has attracted attention in the pharmaceutical and other chemical processing industries as a less toxic alternative to more traditional solvents such as organochlorides. It is used by some dry cleaners for this reason (see green chemistry). It is used in the preparation of some aerogels because of the properties of supercritical carbon dioxide.\n\nPlants require carbon dioxide to conduct photosynthesis. The atmospheres of greenhouses may (if of large size, must) be enriched with additional to sustain and increase the rate of plant growth. At very high concentrations (100 times atmospheric concentration, or greater), carbon dioxide can be toxic to animal life, so raising the concentration to 10,000 ppm (1%) or higher for several hours will eliminate pests such as whiteflies and spider mites in a greenhouse.\n\nIt has been proposed that from power generation be bubbled into ponds to stimulate growth of algae that could then be converted into biodiesel fuel.\n\nIn medicine, up to 5% carbon dioxide (130 times atmospheric concentration) is added to oxygen for stimulation of breathing after apnea and to stabilize the balance in blood.\n\nCarbon dioxide can be mixed with up to 50% oxygen, forming an inhalable gas; this is known as Carbogen and has a variety of medical and research uses.\n\nCarbon dioxide is used in enhanced oil recovery where it is injected into or adjacent to producing oil wells, usually under supercritical conditions, when it becomes miscible with the oil. This approach can increase original oil recovery by reducing residual oil saturation by between 7% to 23% additional to primary extraction. It acts as both a pressurizing agent and, when dissolved into the underground crude oil, significantly reduces its viscosity, and changing surface chemistry enabling the oil to flow more rapidly through the reservoir to the removal well. In mature oil fields, extensive pipe networks are used to carry the carbon dioxide to the injection points.\n\nA strain of the cyanobacterium \"Synechococcus elongatus\" has been genetically engineered to produce the fuels isobutyraldehyde and isobutanol from using photosynthesis.\n\nLiquid and solid carbon dioxide are important refrigerants, especially in the food industry, where they are employed during the transportation and storage of ice cream and other frozen foods. Solid carbon dioxide is called \"dry ice\" and is used for small shipments where refrigeration equipment is not practical. Solid carbon dioxide is always below −78.5 °C at regular atmospheric pressure, regardless of the air temperature.\n\nThe global automobile industry is expected to decide on the next-generation refrigerant in car air conditioning. is one discussed option.(see Sustainable automotive air conditioning)\n\nIn enhanced coal bed methane recovery, carbon dioxide would be pumped into the coal seam to displace methane, as opposed to current methods which primarily rely on the removal of water (to reduce pressure) to make the coal seam release its trapped methane.\n\nCarbon dioxide is the lasing medium in a carbon dioxide laser, which is one of the earliest type of lasers.\n\nCarbon dioxide can be used as a means of controlling the pH of swimming pools, by continuously adding gas to the water, thus keeping the pH from rising. Among the advantages of this is the avoidance of handling (more hazardous) acids. Similarly, it is also used in the maintaining reef aquaria, where it is commonly used in calcium reactors to temporarily lower the pH of water being passed over calcium carbonate in order to allow the calcium carbonate to dissolve into the water more freely where it is used by some corals to build their skeleton.\n\nUsed as the primary coolant in the British advanced gas-cooled reactor for nuclear power generation.\n\nCarbon dioxide induction is commonly used for the euthanasia of laboratory research animals. Methods to administer include placing animals directly into a closed, prefilled chamber containing , or exposure to a gradually increasing concentration of . In 2013, the American Veterinary Medical Association issued new guidelines for carbon dioxide induction, stating that a displacement rate of 10% to 30% of the gas chamber volume per minute is optimal for the humane euthanization of small rodents. However, there is opposition to the practice of using carbon dioxide for this, on the grounds that it is cruel.\n\nCarbon dioxide is also used in several related cleaning and surface preparation techniques.\n\nCarbon dioxide in Earth's atmosphere is a trace gas, currently (mid 2018) having a global average concentration of 409 parts per million by volume (or 622 parts per million by mass). Atmospheric concentrations of carbon dioxide fluctuate slightly with the seasons, falling during the Northern Hemisphere spring and summer as plants consume the gas and rising during northern autumn and winter as plants go dormant or die and decay. Concentrations also vary on a regional basis, most strongly near the ground with much smaller variations aloft. In urban areas concentrations are generally higher and indoors they can reach 10 times background levels.\nThe concentration of carbon dioxide has risen due to human activities. Combustion of fossil fuels and deforestation have caused the atmospheric concentration of carbon dioxide to increase by about 43% since the beginning of the age of industrialization. Most carbon dioxide from human activities is released from burning coal and other fossil fuels. Other human activities, including deforestation, biomass burning, and cement production also produce carbon dioxide. Human activities emit about 29 billion tons of carbon dioxide per year, while volcanoes emit between 0.2 and 0.3 billion tons. Human activities have caused to increase above levels not seen in hundreds of thousands of years. Currently, about half of the carbon dioxide released from the burning of fossil fuels remains in the atmosphere and is not absorbed by vegetation and the oceans.\n\nWhile transparent to visible light, carbon dioxide is a greenhouse gas, absorbing and emitting infrared radiation at its two infrared-active vibrational frequencies (see the section \"Structure and bonding\" above). Light emission from the earth's surface is most intense in the infrared region between 200 and 2500 cm, as opposed to light emission from the much hotter sun which is most intense in the visible region. Absorption of infrared light at the vibrational frequencies of atmospheric carbon dioxide traps energy near the surface, warming the surface and the lower atmosphere. Less energy reaches the upper atmosphere, which is therefore cooler because of this absorption. Increases in atmospheric concentrations of and other long-lived greenhouse gases such as methane, nitrous oxide and ozone have correspondingly strengthened their absorption and emission of infrared radiation, causing the rise in average global temperature since the mid-20th century. Carbon dioxide is of greatest concern because it exerts a larger overall warming influence than all of these other gases combined and because it has a long atmospheric lifetime (hundreds to thousands of years).\n\nNot only do increasing carbon dioxide concentrations lead to increases in global surface temperature, but increasing global temperatures also cause increasing concentrations of carbon dioxide. This produces a positive feedback for changes induced by other processes such as orbital cycles. Five hundred million years ago the carbon dioxide concentration was 20 times greater than today, decreasing to 4–5 times during the Jurassic period and then slowly declining with a particularly swift reduction occurring 49 million years ago.\n\nLocal concentrations of carbon dioxide can reach high values near strong sources, especially those that are isolated by surrounding terrain. At the Bossoleto hot spring near Rapolano Terme in Tuscany, Italy, situated in a bowl-shaped depression about in diameter, concentrations of rise to above 75% overnight, sufficient to kill insects and small animals. After sunrise the gas is dispersed by convection. High concentrations of produced by disturbance of deep lake water saturated with are thought to have caused 37 fatalities at Lake Monoun, Cameroon in 1984 and 1700 casualties at Lake Nyos, Cameroon in 1986.\n\nCarbon dioxide dissolves in the ocean to form carbonic acid (HCO), bicarbonate (HCO) and carbonate (CO). There is about fifty times as much carbon dissolved in the oceans as exists in the atmosphere. The oceans act as an enormous carbon sink, and have taken up about a third of emitted by human activity.\n\nAs the concentration of carbon dioxide increases in the atmosphere, the increased uptake of carbon dioxide into the oceans is causing a measurable decrease in the pH of the oceans, which is referred to as ocean acidification. This reduction in pH affects biological systems in the oceans, primarily oceanic calcifying organisms. These effects span the food chain from autotrophs to heterotrophs and include organisms such as coccolithophores, corals, foraminifera, echinoderms, crustaceans and mollusks. Under normal conditions, calcium carbonate is stable in surface waters since the carbonate ion is at supersaturating concentrations. However, as ocean pH falls, so does the concentration of this ion, and when carbonate becomes undersaturated, structures made of calcium carbonate are vulnerable to dissolution. Corals, coccolithophore algae, coralline algae, foraminifera, shellfish and pteropods experience reduced calcification or enhanced dissolution when exposed to elevated .\n\nGas solubility decreases as the temperature of water increases (except when both pressure exceeds 300 bar and temperature exceeds 393 K, only found near deep geothermal vents) and therefore the rate of uptake from the atmosphere decreases as ocean temperatures rise.\n\nMost of the taken up by the ocean, which is about 30% of the total released into the atmosphere, forms carbonic acid in equilibrium with bicarbonate. Some of these chemical species are consumed by photosynthetic organisms that remove carbon from the cycle. Increased in the atmosphere has led to decreasing alkalinity of seawater, and there is concern that this may adversely affect organisms living in the water. In particular, with decreasing alkalinity, the availability of carbonates for forming shells decreases, although there's evidence of increased shell production by certain species under increased content.\n\nNOAA states in their May 2008 \"State of the science fact sheet for ocean acidification\" that:<br>\n\"The oceans have absorbed about 50% of the carbon dioxide () released from the burning of fossil fuels, resulting in chemical reactions that lower ocean pH. This has caused an increase in hydrogen ion (acidity) of about 30% since the start of the industrial age through a process known as \"ocean acidification.\" A growing number of studies have demonstrated adverse impacts on marine organisms, including:\n\nAlso, the Intergovernmental Panel on Climate Change (IPCC) writes in their Climate Change 2007: Synthesis Report: <br>\n\"The uptake of anthropogenic carbon since 1750 has led to the ocean becoming more acidic with an average decrease in pH of 0.1 units. Increasing atmospheric concentrations lead to further acidification ... While the effects of observed ocean acidification on the marine biosphere are as yet undocumented, the progressive acidification of oceans is expected to have negative impacts on marine shell-forming organisms (e.g. corals) and their dependent species.\"\n\nSome marine calcifying organisms (including coral reefs) have been singled out by major research agencies, including NOAA, OSPAR commission, NANOOS and the IPCC, because their most current research shows that ocean acidification should be expected to impact them negatively.\n\nCarbon dioxide is also introduced into the oceans through hydrothermal vents. The \"Champagne\" hydrothermal vent, found at the Northwest Eifuku volcano in the Marianas Trench, produces almost pure liquid carbon dioxide, one of only two known sites in the world as of 2004, the other being in the Okinawa Trough.\nThe finding of a submarine lake of liquid carbon dioxide in the Okinawa Trough was reported in 2006.\n\nCarbon dioxide is an end product of cellular respiration in organisms that obtain energy by breaking down sugars, fats and amino acids with oxygen as part of their metabolism. This includes all plants, algae and animals and aerobic fungi and bacteria. In vertebrates, the carbon dioxide travels in the blood from the body's tissues to the skin (e.g., amphibians) or the gills (e.g., fish), from where it dissolves in the water, or to the lungs from where it is exhaled. During active photosynthesis, plants can absorb more carbon dioxide from the atmosphere than they release in respiration.\n\nCarbon fixation is a biochemical process by which atmospheric carbon dioxide is incorporated by plants, algae and (cyanobacteria) into energy-rich organic molecules such as glucose, thus creating their own food by photosynthesis. Photosynthesis uses carbon dioxide and water to produce sugars from which other organic compounds can be constructed, and oxygen is produced as a by-product.\n\nRibulose-1,5-bisphosphate carboxylase oxygenase, commonly abbreviated to RuBisCO, is the enzyme involved in the first major step of carbon fixation, the production of two molecules of 3-phosphoglycerate from and ribulose bisphosphate, as shown in the diagram at left.\n\nRuBisCO is thought to be the single most abundant protein on Earth.\n\nPhototrophs use the products of their photosynthesis as internal food sources and as raw material for the biosynthesis of more complex organic molecules, such as polysaccharides, nucleic acids and proteins. These are used for their own growth, and also as the basis of the food chains and webs that feed other organisms, including animals such as ourselves. Some important phototrophs, the coccolithophores synthesise hard calcium carbonate scales. A globally significant species of coccolithophore is \"Emiliania huxleyi\" whose calcite scales have formed the basis of many sedimentary rocks such as limestone, where what was previously atmospheric carbon can remain fixed for geological timescales.\n\nPlants can grow as much as 50 percent faster in concentrations of 1,000 ppm when compared with ambient conditions, though this assumes no change in climate and no limitation on other nutrients. Elevated levels cause increased growth reflected in the harvestable yield of crops, with wheat, rice and soybean all showing increases in yield of 12–14% under elevated in FACE experiments.\n\nIncreased atmospheric concentrations result in fewer stomata developing on plants which leads to reduced water usage and increased water-use efficiency. Studies using FACE have shown that enrichment leads to decreased concentrations of micronutrients in crop plants. This may have knock-on effects on other parts of ecosystems as herbivores will need to eat more food to gain the same amount of protein.\n\nThe concentration of secondary metabolites such as phenylpropanoids and flavonoids\ncan also be altered in plants exposed to high concentrations of .\n\nPlants also emit during respiration, and so the majority of plants and algae, which use C3 photosynthesis, are only net absorbers during the day. Though a growing forest will absorb many tons of each year, a mature forest will produce as much from respiration and decomposition of dead specimens (e.g., fallen branches) as is used in photosynthesis in growing plants. Contrary to the long-standing view that they are carbon neutral, mature forests can continue to accumulate carbon and remain valuable carbon sinks, helping to maintain the carbon balance of Earth's atmosphere. Additionally, and crucially to life on earth, photosynthesis by phytoplankton consumes dissolved in the upper ocean and thereby promotes the absorption of from the atmosphere.\n\nCarbon dioxide content in fresh air (averaged between sea-level and 10 kPa level, i.e., about altitude) varies between 0.036% (360 ppm) and 0.041% (410 ppm), depending on the location.\n\nBecause it is heavier than air, in locations where the gas seeps from the ground (due to sub-surface volcanic or geothermal activity) in relatively high concentrations, without the dispersing effects of wind, it can collect in sheltered/pocketed locations below average ground level, causing animals located therein to be suffocated. Carrion feeders attracted to the carcasses are then also killed. Children have been killed in the same way near the city of Goma by emissions from the nearby volcano Mt. Nyiragongo. The Swahili term for this phenomenon is 'mazuku'.\nAdaptation to increased concentrations of occurs in humans, including modified breathing and kidney bicarbonate production, in order to balance the effects of blood acidification (acidosis). Several studies suggested that 2.0 percent inspired concentrations could be used for closed air spaces (e.g. a submarine) since the adaptation is physiological and reversible, as decrement in performance or in normal physical activity does not happen at this level of exposure for five days. Yet, other studies show a decrease in cognitive function even at much lower levels. Also, with ongoing respiratory acidosis, adaptation or compensatory mechanisms will be unable to reverse such condition.\n\nThere are few studies of the health effects of long-term continuous exposure on humans and animals at levels below 1%. Occupational exposure limits have been set in the United States at 0.5% (5000 ppm) for an eight-hour period. At this concentration, International Space Station crew experienced headaches, lethargy, mental slowness, emotional irritation, and sleep disruption. Studies in animals at 0.5% have demonstrated kidney calcification and bone loss after eight weeks of exposure. A study of humans exposed in 2.5 hour sessions demonstrated significant effects on cognitive abilities at concentrations as low as 0.1% (1000ppm) likely due to induced increases in cerebral blood flow. Another study observed a decline in basic activity level and information usage at 1000 ppm, when compared to 500 ppm.\n\nPoor ventilation is one of the main causes of excessive concentrations in closed spaces. Carbon dioxide differential above outdoor concentrations at steady state conditions (when the occupancy and ventilation system operation are sufficiently long that concentration has stabilized) are sometimes used to estimate ventilation rates per person. Higher concentrations are associated with occupant health, comfort and performance degradation. ASHRAE Standard 62.1–2007 ventilation rates may result in indoor concentrations up to 2,100 ppm above ambient outdoor conditions. Thus if the outdoor concentration is 400 ppm, indoor concentrations may reach 2,500 ppm with ventilation rates that meet this industry consensus standard. Concentrations in poorly ventilated spaces can be found even higher than this (range of 3,000 or 4,000).\n\nMiners, who are particularly vulnerable to gas exposure due to an insufficient ventilation, referred to mixtures of carbon dioxide and nitrogen as \"blackdamp,\" \"choke damp\" or \"stythe.\" Before more effective technologies were developed, miners would frequently monitor for dangerous levels of blackdamp and other gases in mine shafts by bringing a caged canary with them as they worked. The canary is more sensitive to asphyxiant gases than humans, and as it became unconscious would stop singing and fall off its perch. The Davy lamp could also detect high levels of blackdamp (which sinks, and collects near the floor) by burning less brightly, while methane, another suffocating gas and explosion risk, would make the lamp burn more brightly.\n\nThe body produces approximately of carbon dioxide per day per person, containing of carbon. In humans, this carbon dioxide is carried through the venous system and is breathed out through the lungs, resulting in lower concentrations in the arteries. The carbon dioxide content of the blood is often given as the partial pressure, which is the pressure which carbon dioxide would have had if it alone occupied the volume. In humans, the blood carbon dioxide contents is shown in the adjacent table:\n\n is carried in blood in three different ways. (The exact percentages vary depending whether it is arterial or venous blood).\n\nHemoglobin, the main oxygen-carrying molecule in red blood cells, carries both oxygen and carbon dioxide. However, the bound to hemoglobin does not bind to the same site as oxygen. Instead, it combines with the N-terminal groups on the four globin chains. However, because of allosteric effects on the hemoglobin molecule, the binding of decreases the amount of oxygen that is bound for a given partial pressure of oxygen. This is known as the Haldane Effect, and is important in the transport of carbon dioxide from the tissues to the lungs. Conversely, a rise in the partial pressure of or a lower pH will cause offloading of oxygen from hemoglobin, which is known as the Bohr effect.\n\nCarbon dioxide is one of the mediators of local autoregulation of blood supply. If its concentration is high, the capillaries expand to allow a greater blood flow to that tissue.\n\nBicarbonate ions are crucial for regulating blood pH. A person's breathing rate influences the level of in their blood. Breathing that is too slow or shallow causes respiratory acidosis, while breathing that is too rapid leads to hyperventilation, which can cause respiratory alkalosis.\n\nAlthough the body requires oxygen for metabolism, low oxygen levels normally do not stimulate breathing. Rather, breathing is stimulated by higher carbon dioxide levels. As a result, breathing low-pressure air or a gas mixture with no oxygen at all (such as pure nitrogen) can lead to loss of consciousness without ever experiencing air hunger. This is especially perilous for high-altitude fighter pilots. It is also why flight attendants instruct passengers, in case of loss of cabin pressure, to apply the oxygen mask to themselves first before helping others; otherwise, one risks losing consciousness.\n\nThe respiratory centers try to maintain an arterial pressure of 40 mm Hg. With intentional hyperventilation, the content of arterial blood may be lowered to 10–20 mm Hg (the oxygen content of the blood is little affected), and the respiratory drive is diminished. This is why one can hold one's breath longer after hyperventilating than without hyperventilating. This carries the risk that unconsciousness may result before the need to breathe becomes overwhelming, which is why hyperventilation is particularly dangerous before free diving.\n\n"}
{"id": "200127", "url": "https://en.wikipedia.org/wiki?curid=200127", "title": "Chobham armour", "text": "Chobham armour\n\nChobham armour is the informal name of a composite armour developed in the 1960s at the British tank research centre on Chobham Common, Surrey. The name has since become the common generic term for composite ceramic vehicle armour. Other names informally given to Chobham Armour include \"Burlington\" and \"Dorchester.\" \"Special armour\" is a broader informal term referring to any armour arrangement comprising \"sandwich\" reactive plates, including Chobham Armour.\n\nAlthough the construction details of the Chobham armour remain a secret, it has been described as being composed of ceramic tiles encased within a metal framework and bonded to a backing plate and several elastic layers. Due to the extreme hardness of the ceramics used, they offer superior resistance against shaped charges such as high explosive anti-tank (HEAT) rounds and they shatter kinetic energy penetrators.\n\nThe armour was first tested in the context of the development of a British prototype vehicle, the FV4211, and first applied on the preseries of the American M1. Only the M1 Abrams, Challenger 1, and Challenger 2 tanks have been disclosed as being thus armoured. The framework holding the ceramics is usually produced in large blocks, giving these tanks, and especially their turrets, a distinctive angled appearance.\n\nDue to the extreme hardness of the ceramics used, they offer superior resistance against a shaped charge jet and they shatter kinetic energy penetrators (KE-penetrators). The (pulverised) ceramic also strongly abrades any penetrator. Against lighter projectiles the hardness of the tiles causes a \"shatter gap\" effect: a higher velocity will, within a certain velocity range (the \"gap\"), not lead to a deeper penetration but destroy the projectile itself instead. Because the ceramic is so brittle the entrance channel of a shaped charge jet is not smooth—as it would be when penetrating a metal—but ragged, causing extreme asymmetric pressures which disturb the geometry of the jet, on which its penetrative capabilities are critically dependent as its mass is relatively low. This initiates a vicious circle as the disturbed jet causes still greater irregularities in the ceramic, until in the end it is defeated. The newer composites, though tougher, optimise this effect as tiles made with them have a layered internal structure conducive to it, causing \"crack deflection\". This mechanism—using the jet's own energy against it—has caused the effects of Chobham to be compared to those of reactive armour. This should not be confused with the effect used in Non-Explosive Reactive Armour: that of sandwiching an inert but soft elastic material such as rubber, between two armour plates. The impact of either a shaped charge jet or long-rod penetrator after the first layer has been perforated and while the rubber layer is being penetrated will cause the rubber to deform and expand, so deforming both the back and front plates. Both attack methods will suffer from obstruction to their expected paths, so experiencing a greater thickness of armour than there is nominally, thus lowering penetration. Also for rod penetrations, the transverse force experienced due to the deformation may cause the rod to shatter, bend, or just change its path, again lowering penetration. All versions of Chobham armour have incorporated a large volume of non-energetic reactive armour (NERA) plates either behind hard external armour to weaken the attack, or in front of the rest of the armour array intended to catch the remnants. This is another factor favouring a slab-sided or wedge-like turret: the amount of material the expanding plates push into the path of an attack increases as they are placed closer to parallel to the direction of that attack.\n\nTo date, few Chobham armour-protected tanks have been defeated by enemy fire in combat; the relevance of individual cases of lost tanks for determining the protective qualities of Chobham armour is difficult to ascertain as the extent to which such tanks are protected by ceramic modules is undisclosed.\n\nDuring the second Iraq war in 2003, a Challenger 2 tank became stuck in a ditch while fighting in Basra against Iraqi forces. The crew remained safe inside for many hours, the Burlington LV2 composite armour protecting them from enemy fire, including multiple rocket propelled grenades.\n\nCeramic tiles have a \"multiple hit capability\" problem in that they cannot sustain successive impacts without quickly losing much of their protective value. To minimise the effects of this the tiles are made as small as possible, but the matrix elements have a minimal practical thickness of about one inch (25 mm), and the ratio of coverage provided by tiles would become unfavourable, placing a practical limit at a diameter of about four inches (ten centimetres). The small hexagonal or square ceramic tiles are encased within the matrix either by isostatically pressing them into the heated matrix, or by gluing them with an epoxy resin. Since the early nineties it has been known that holding the tiles under constant compression by their matrix greatly improves their resistance to kinetic penetrators, which is difficult to achieve when using glues.\n\nThe matrix has to be backed by a plate, both to reinforce the ceramic tiles from behind and to prevent deformation of the metal matrix by a kinetic impact. Typically the backing plate has half of the mass of the composite matrix. The assemblage is again attached to elastic layers. These absorb impacts somewhat, but their main function is to prolong the service life of the composite matrix by protecting it against vibrations. Several assemblages can be stacked, depending on the available space; this way the armour can be made of a modular nature, adaptable to the tactical situation. The thickness of a typical assemblage is today about five to six centimetres. Earlier assemblages, so-called DOP (Depth Of Penetration) -matrices, were thicker. The relative interface defeat component of the protective value of a ceramic is much larger than for steel armour. Using a number of thinner matrices again enlarges that component for the entire armour package, an effect analogous to the use of alternate layers of high hardness and softer steel, which is typical for the glacis of modern Soviet tanks.\n\nCeramic tiles draw little or no advantage from sloped armour as they lack sufficient toughness to significantly deflect heavy penetrators. Indeed, because a single glancing shot could crack many tiles, the placement of the matrix is chosen so as to optimise the chance of a perpendicular hit, a reversal of the previous desired design feature for conventional armour. Ceramic armour normally even offers better protection for a given areal density when placed perpendicularly than when placed obliquely, because the cracking propagates along the surface normal of the plate. Instead of rounded forms, the turrets of tanks using Chobham armour typically have a slab-sided appearance.\n\nThe backing plate reflects the impact energy back to the ceramic tile in a wider cone. This dissipates the energy, limiting the cracking of the ceramic, but also means a more extended area is damaged. Spalling caused by the reflected energy can be partially prevented by a malleable thin graphite layer on the face of the ceramic absorbing the energy without making it strongly rebound again as a metal face plate would.\n\nTiles under compression suffer far less from impacts; in their case it can be advantageous to have a metal face plate bringing the tile also under perpendicular compression. The confined ceramic tile then reinforces the metal face plate, a reversal of the normal situation.\n\nA gradual technological development has taken place in ceramic armour: ceramic tiles, in themselves vulnerable to low energy impacts, were first reinforced by glueing them to a backplate; in the nineties their resistance was increased by bringing them under compression on two axes; in the final phase a third compression axis was added to optimise impact resistance. To confine the ceramic core several advanced techniques are used, supplementing the traditional machining and welding, including sintering the suspension material around the core; squeeze casting of molten metal around the core and spraying the molten metal onto the ceramic tile.\n\nThe whole is placed within the shell formed by the outer and inner wall of the tank turret or hull, the inner wall being the thicker.\n\nOver the years newer and tougher composites have been developed, giving about five times the protection value of the original pure ceramics, the best of which were again about five times as effective as a steel plate of equal weight. These are often a mixture of several ceramic materials, or metal matrix composites which combine ceramic compounds within a metal matrix. The latest developments involve the use of carbon nanotubes to improve toughness even further. Commercially produced or researched ceramics for such type of armour include boron carbide, silicon carbide, aluminium oxide (sapphire or \"alumina\"), aluminium nitride, titanium boride and Syndite, a synthetic diamond composite. Of these boron carbide is the hardest and lightest, but also the most expensive and brittle. Boron carbide composites are today favoured for ceramic plates protecting against smaller projectiles, such as used in body armour and armoured helicopters; this was in fact in the early sixties the first general application of ceramic armour. Silicon carbide, better suited to protect against larger projectiles, was at that time only used in some prototype land vehicles, such as the MBT-70. The ceramics can be created by pressureless sintering or hot pressing. A high density is required, so residual porosity must be minimised in the final part.\n\nA matrix using a titanium alloy is extremely expensive to manufacture but the metal is favoured for its lightness, strength and resistance to corrosion, which is a constant problem. The Rank company claims to have invented an alumina matrix for the insertion of boron carbide or silicon carbide tiles.\n\nThe backing plate can be made from steel, but, as its main function is to improve the stability and stiffness of the assemblage, aluminium is more weight-efficient in light AFVs only to be protected against light anti-tank weapons. A deformable composite backing plate can combine the function of a metal backing plate and an elastic layer.\n\nThe armour configuration of the first western tanks using Chobham armour was optimised to defeat shaped charges as guided missiles were seen as the greatest threat. In the eighties however they began to face the improved Soviet 3BM-32, then 3BM-42 kinetic energy penetrators which the ceramic layer was not particularly effective against: the original ceramics had a resistance against penetrators of about a third compared to that against HEAT rounds; for the newest composites it is about one-tenth. A typical example, the 3BM-42 is a segmented projectile which frontal segments are sacrificed in expanding the NERA plates in the front of the armour array, leaving a hole for the rear segment to strike the ceramic with full efficiency. For this reason many modern designs include additional layers of heavy metals to add more density to the overall armour package.\n\nThe introduction of more effective ceramic composite materials allows for a larger width of these metal layers within the armour shell: given a certain protection level provided by the composite matrix, it can be thinner. Because these metal layers are denser than the rest of the composite array, increasing their thickness requires reducing the armour thickness in non-critical areas of the vehicle. They typically form an inner layer placed below the much more expensive matrix, to prevent extensive damage to it should the metal layer strongly deform but not defeat a penetrator. They can also be used as the backing plate for the matrix itself, but this compromises the modularity and thus tactical adaptability of the armour system: ceramic and metal modules can then no longer be replaced independently. Furthermore, due to their extreme hardness, they deform insufficiently and would reflect too much of the impact energy, and in a too wide cone, to the ceramic tile, damaging it even further. Metals used include a tungsten alloy for the Challenger 2 or, in the case of the M1A1HA (Heavy Armor) and later American tank variants, a depleted uranium alloy. Some companies offer titanium carbide modules.\n\nThese metal modules function on the principle of perforated armour (typically employing perpendicular rods), with many expansion spaces reducing the weight by up to one third while keeping the protective qualities fairly constant. The depleted uranium alloy of the M1 has been described as \"arranged in a type of armour matrix\" and a single module as a \"stainless-steel shell surrounding a layer (probably an inch or two thick) of depleted uranium, woven into a wire-mesh blanket\".\n\nSuch modules are also used by tanks not equipped with Chobham armour. The combination of a composite matrix and heavy metal modules is sometimes informally referred to as \"second generation Chobham\".\n\nThe concept of ceramic armour goes back to 1918, when Major Neville Monroe Hopkins discovered that a plate of ballistic steel was much more resistant to penetration if covered with a thin (1–2 millimetres) layer of enamel. Additionally, the Germans experimented with ceramic armour in World War I.\n\nSince the early sixties there were, in the US, extensive research programmes ongoing aimed at investigating the prospects of employing composite ceramic materials as vehicle armour. This research mainly focused on the use of an aluminium metal matrix composite reinforced by silicon carbide whiskers, to be produced in the form of large sheets. The reinforced light metal sheets were to be sandwiched between steel layers. This arrangement had the advantage of having a good multiple-hit capability and of being able to be curved, allowing the main armour to benefit from a sloped armour effect. However, this composite with a high metal content was primarily intended to increase the protection against KE-penetrators for a given armour weight; its performance against shaped charge attack was mediocre and would have to be improved by means of a laminate spaced armour effect, as researched by the Germans within the joint MBT-70 project.\n\nAn alternative technology developed in the US was based on the use of glass modules to be inserted into the main armour; although this arrangement offered a better shaped charge protection, its multiple hit capability was poor. A similar system using glass inserts in the main steel armour was from the late fifties researched for the Soviet \"Obiekt 430\" prototype of the T-64; this was later developed into the \"Combination K\" type, having a ceramic compound mixed with the silicon oxide inserts, which offered about 50% better protection against both shaped charge and KE-penetrator threats, compared with a steel armour of the same weight. It was, later in several improved forms, incorporated into the glacis of many subsequent Soviet main battle tank designs. After an initial period of speculation in the West as to its true nature, the characteristics of this type were disclosed when the dissolution of the Soviet Union in 1991 and the introduction of a market system forced the Russian industries to find new customers by highlighting its good qualities; it is today rarely referred to as Chobham armour. Special armour much more similar to Chobham appeared in 1983 under the name of BDD on the T-62M upgrade to the T-62 , was first integrated to an armour array in 1986 on the T-72B, and has been a feature of every Soviet/Russian MBT since. In its original iteration, it is built directly into the cast steel turret of the T-72 and required lifting it to perform repairs.\nIn the United Kingdom another line of ceramic armour development had been started in the early 1960s, meant to improve the existing cast turret configuration of the Chieftain that already offered excellent heavy penetrator protection; the research by a team headed by Gilbert Harvey of the Fighting Vehicles Research and Development Establishment (FVRDE), therefore was strongly oriented at optimising the ceramic composite system for defeating shaped charge attack. The British system consisted of a honeycomb matrix with ceramic tiles backed by ballistic nylon, placed on top of the cast main armour. In July 1973 an American delegation, in search of a new armour type for the XM815 tank prototype, now that the MBT-70 project had failed, visited Chobham Common to be informed about the British system, the development of which had then cost about ₤6,000,000; earlier information had already been divulged to the US in 1965 and 1968. It was very impressed by the excellent shaped-charge protection combined with the penetrator impact damage limitation, inherent to the principle of using tiles. The Ballistic Research Laboratory at the Aberdeen Proving Ground, which later became a part of the Army Research Laboratory, initiated the development of a version that year named \"Burlington\", adapted to the specific American situation, characterised by a much higher projected tank production run and the use of a thinner rolled steel main armour. The increased threat posed by a new generation of Soviet guided missiles armed with a shaped charge warhead—as demonstrated in the Yom Kippur War of October 1973, when even older-generation missiles caused considerable tank losses on the Israeli side—made Burlington the preferred choice for the armour configuration of the XM1 (the renamed XM815) prototype.\n\nHowever, on 11 December 1974 a Memorandum of Understanding was signed between the Federal Republic of Germany and the US about the common future production of a main battle tank; this made any application of Chobham armour dependent on the eventual choice for a tank type. Earlier in 1974 the Americans had asked the Germans to redesign the existing Leopard 2 prototypes, considered by them too lightly armoured, and had suggested adoption of \"Burlington\" for this purpose, of which type the Germans had already been informed in March 1970; the Germans however in response in 1974 initiated a new armour development programme of their own. Having already designed a system that in their opinion offered satisfactory protection against shaped charges, consisting of multiple-laminate spaced armour with the spaces filled with ceramic polystyrene foam as fitted to the Leopard 1A3, they put a clear emphasis on improving KE-penetrator protection, reworking the system into a perforated metal module armour. A version with added Burlington was considered, including ceramic inserts in the various spaces, but rejected as it would push vehicle weight well over sixty metric tonnes, a weight then seen as prohibitive by both armies. The US Army in the summer of 1974 faced the choice between the German system and their own Burlington, a decision made more difficult by the fact that Burlington offered, compared with steel armour, no weight advantage against KE-penetrators: the total armour system would have a RHA equivalence against them of about 350 mm (compared to about 700 mm against shaped charges). No consensus developing, General Creighton Abrams himself decided the issue in favour of Burlington. Eventually each army procured its own national tank design, the project of a common tank failing in 1976. In February 1978 the first tanks protected by Burlington left the factory when the first of eleven pilot M1 tanks were delivered by Chrysler Corporation to the US Army.\n\nBeside these state projects, private enterprise in the US during the seventies also developed ceramic armour types, like the Noroc armour made by the Protective Products Division of the Norton Company, consisting of boron carbide sheets backed by resin-bonded glass cloth.\n\nIn the United Kingdom application of Chobham armour was delayed by the failure of several advanced tank projects: first that of a joint German-British main battle tank; then the purely British MBT-80 programme. A first directive to prepare Chobham armour technology for application in 1975 was already given in 1969. It was determined by a study of a possible Chobham-armour protected MICV that a completely new design using only Chobham armour for the most vulnerable front and side sectors (thus without an underlying steel main armour) could be 10% lighter for the same level of protection against KE-ammunition, but to limit costs it was decided to base the first design on the conventional Chieftain. The prototype, FV 4211 or the \"Aluminium Chieftain\", was fitted with a welded aluminium add-on armour, in essence a box on the front hull and front and side turret to contain the ceramic modules, of which box the fifty millimetre thick inner wall due to its relative softness could serve as their backing plate. The extra weight of the aluminium was limited to less than two tonnes and it was shown that it was not overly susceptible to cracking, as first feared. Ten test vehicles were ordered but only the original one had been built when the project was cancelled in favour of the more advanced programmes. However, the Iranian government ordered 1,225 vehicles of an upgraded Chieftain type, the \"Shir-2\" (FV 4030/3), using the same technology of adding Chobham armour to the main cast armour, bringing total weight to 62 metric tonnes. When this order was cancelled in February 1979 because of the Iranian Revolution, the British government, under pressure to modernise its tank fleet to maintain a qualitative superiority relative to the Soviet tank forces, decided to use the sudden surplus production capacity to procure a number of vehicles very close in design to the Shir-2, called the Challenger 1. On 12 April 1983 the first British tank protected by Chobham armour was delivered to the Royal Hussars.\n\nIn France from 1966 GIAT Industries performed experiments aimed at developing a light vehicle ceramic armour, in 1970 resulting in the CERALU-system consisting of aluminium-backed alumina weldable to the vehicle, offering a 50% increase in weight-efficiency against ballistic threats compared to steel plate. An improved version was later applied in helicopter seats.\n\nThe latest version of Chobham armour is used on the Challenger 2 (called \"Dorchester armour\"), and (though the composition most probably differs) the M1 Abrams series of tanks, which according to official sources is currently protected by silicon carbide tiles. Given the publicly stated protection level for the earliest M1: 350 mm steel equivalence against KE-penetrators (APFSDS), it seems to have been equipped with alumina tiles.\n\nThough it is often claimed to be otherwise, the original production model of the Leopard 2 did not use Chobham armour, but a combined spaced armour and perforated armour configuration, cheaper in terms of procurement, maintenance and replacement than a ceramic armour system. For many modern tanks, such the Italian Ariete, it is yet unknown which type is used. There was a general trend in the eighties away from ceramic armour towards perforated armour, but even many tanks from the seventies like the Leopard 1A3 and A4, the French AMX 32 and AMX 40 prototypes used the latter system; the Leclerc has an improved version.\n\nThe first ceramic plates found application in the aerospace sector: in 1965, the helicopter UH-1 Huey was modified with HFC (Hard-Faced-Composite) around the seats of pilot and copilot, protecting them against small arms fire. The plates were in boron carbide, which, though exceedingly costly, due to its superior lightness has remained the material of choice for aerospace applications. An example among many, the modern V-22 Osprey is protected similarly.\n\nJeffrey J. Swab (Editor), Dongming Zhu (General Editor), Waltraud M. Kriven (General Editor); \"Advances in Ceramic Armor: A Collection of Papers Presented at the 29th International Conference on Advanced Ceramics and Composites, January 23–28, 2005, Cocoa Beach, Florida, Ceramic Engineering and Science Proceedings, Volume 26, Number 7\"; \n\n"}
{"id": "1261603", "url": "https://en.wikipedia.org/wiki?curid=1261603", "title": "Cold trap", "text": "Cold trap\n\nIn vacuum applications, a cold trap is a device that condenses all vapors except the permanent gases into a liquid or solid. The most common objective is to prevent vapors being evacuated from an experiment from entering a vacuum pump where they would condense and contaminate it. Particularly large cold traps are necessary when removing large amounts of liquid as in freeze drying.\n\nCold traps also refer to the application of cooled surfaces or baffles to prevent oil vapours from flowing from a pump and into a chamber. In such a case, a baffle or a section of pipe containing a number of cooled vanes, will be attached to the inlet of an existing pumping system. By cooling the baffle, either with a cryogen such as liquid nitrogen, or by use of an electrically driven Peltier element, oil vapour molecules that strike the baffle vanes will condense and thus be removed from the pumped cavity.\n\nPumps that use oil either as their working fluid (diffusion pumps), or as their lubricant (mechanical rotary pumps), are often the sources of contamination in vacuum systems. Placing a cold trap at the mouth of such a pump greatly lowers the risk that oil vapours will backstream into the cavity.\n\nCold traps can also be used for experiments involving vacuum lines such as small-scale very low temperature distillations/condensations. This is accomplished through the use of a coolant such as liquid nitrogen or a freezing mixture of dry ice in acetone or a similar solvent with a low melting point.\n\nWhen performed on a larger scale, this technique is called freeze-drying, and the cold trap is referred to as the condenser.\n\nCold traps are also used in cryopump systems to generate hard vacua by condensing the major constituents of the atmosphere (nitrogen, oxygen, carbon dioxide and water) into their liquid or solid forms.\nCare should be taken when using a cold trap not to condense liquid oxygen (a light blue liquid) into the cold trap. Liquid oxygen is potentially explosive, and this is especially true if the trap has been used to trap solvent. Liquid oxygen can be condensed into a cold trap if a pump has sucked air through the trap when the trap is very cold, e.g. when cooled with liquid nitrogen. Besides oxygen, many hazardous gases emitted in reactions, e.g. sulfur dioxide, chloromethane, condense into cold traps.\n\nCold traps (C in the figure) usually consist of two parts: The bottom is a large, thick round tube with ground-glass joints (B in the figure), and the second is a cap (A in the figure), also with ground-glass connections. The length of the tube is usually selected so that, when assembled, the total reached is about half the length of the tube.\n\nCold traps should be assembled such that the down tube is connected to the source of gas whilst the cap is connected to the source of vacuum. Reversing this, connecting the down tube to the source of vacuum, places the inlet of the vacuum directly above the condensate, increasing the chances of vapour phase condensate moving up the (uncooled) down tube (towards the pump) or, should the trap begin to fill to an appreciable volume, liquid phase condensate being pulled into the pump.\n\n"}
{"id": "5075324", "url": "https://en.wikipedia.org/wiki?curid=5075324", "title": "Collectionwise Hausdorff space", "text": "Collectionwise Hausdorff space\n\nIn mathematics, in the field of topology, a topological space is said to be collectionwise Hausdorff if given any closed discrete collection of points in the topological space, there are pairwise disjoint open sets containing the points. A \"closed discrete\" set \"S\" of a topology \"X\" is one where every point of \"X\" has a neighborhood that intersects at most one point from \"S\". Every T1 space which is collectionwise Hausdorff is also Hausdorff.\n\nMetrizable spaces are collectionwise normal spaces and are hence, in particular, collectionwise Hausdorff.\n"}
{"id": "19892781", "url": "https://en.wikipedia.org/wiki?curid=19892781", "title": "Compressor station", "text": "Compressor station\n\nA compressor station is a facility which helps the transportation process of natural gas from one location to another. Natural gas, while being transported through a gas pipeline, needs to be constantly pressurized at intervals of 40 to 100 miles. Siting is dependent on terrain, and the number of gas wells in the vicinity. Frequent elevation changes and a greater number of gas wells will require more compressor stations.\n\nThe gas in compressor stations is normally pressurized by special turbines, motors and engines.\n\nThe compressor station, also called a pumping station, is the \"engine\" that powers an interstate natural gas pipeline. As the name implies, the compressor station compresses the natural gas (increasing its pressure) thereby providing energy to move the gas through the pipeline.\n\nPipeline companies install compressor stations along a pipeline route. The size of the station and the number of compressors (pumps) varies, based on the diameter of the pipe and the volume of gas to be moved. Nevertheless, the basic components of a station are similar.\n\nAs the pipeline enters the compressor station the natural gas passes through scrubbers, strainers or filter separators. These are vessels designed to remove any free liquids or dirt particles from the gas before it enters the compressors. Though the pipeline is carrying “dry gas,” some water and hydrocarbon liquids may condense out of the gas stream as the gas cools and moves through the pipeline.\nAny liquids that may be produced are collected and stored for sale or disposal. A piping system directs the gas from the separators to the gas compressor for compression.\n\nThere are three commonly used types of engines that drive the compressors and are known as \"prime movers\":\n\nThis type of compression unit uses a natural gas-fired turbine to turn a centrifugal compressor. The centrifugal compressor is similar to a large fan inside a case, which pumps the gas as the fan turns. A small portion of natural gas from the pipeline is burned to power the turbine.\n\nIn this type of compressor unit, the centrifugal compressor is driven by a high voltage electric motor. An electrified compressor may still require an air permit, as regulations vary by state an applicability analysis should be conducted whenever a compressor station will be constructed. However, a highly reliable source of electric power must be available and near the station.\n\nThese large piston engines resemble automobile engines, only much larger. Commonly known as “recips,” these engines are fueled by natural gas from the pipeline. Reciprocating pistons, located in cylinder cases on the side of the unit, compress the natural gas. The compressor pistons and the power pistons are connected to a common crankshaft. The advantage of reciprocating compressors is that the volume of gas pushed through the pipeline can be adjusted incrementally to meet small changes in customer demand.\n\n"}
{"id": "461454", "url": "https://en.wikipedia.org/wiki?curid=461454", "title": "Conservative vector field", "text": "Conservative vector field\n\nIn vector calculus, a conservative vector field is a vector field that is the gradient of some function. Conservative vector fields have the property that the line integral is path independent, i.e., the choice of any path between two points does not change the value of the line integral. Path independence of the line integral is equivalent to the vector field being conservative. A conservative vector field is also irrotational; in three dimensions, this means that it has vanishing curl. An irrotational vector field is necessarily conservative provided that the domain is simply connected.\n\nConservative vector fields appear naturally in mechanics: They are vector fields representing forces of physical systems in which energy is conserved. For a conservative system, the work done in moving along a path in configuration space depends only on the endpoints of the path, so it is possible to define a potential energy that is independent of the actual path taken.\n\nIn a two- and three-dimensional space, there is an ambiguity in taking an integral between two points as there are infinitely many paths between the two points—apart from the straight line formed between the two points, one could choose a curved path of greater length as shown in the figure. Therefore, in general, the value of the integral depends on the path taken. However, in the special case of a conservative vector field, the value of the integral is independent of the path taken, which can be thought of as a large-scale cancellation of all elements formula_1 that don't have a component along the straight line between the two points. To visualize this, imagine two people climbing a cliff; one decides to scale the cliff by going vertically up it, and the second decides to walk along a winding path that is longer in length than the height of the cliff, but at only a small angle to the horizontal. Although the two hikers have taken different routes to get up to the top of the cliff, at the top, they will have both gained the same amount of gravitational potential energy. This is because a gravitational field is conservative. As an example of a non-conservative field, imagine pushing a box from one end of a room to another. Pushing the box in a straight line across the room requires noticeably less work against friction than along a curved path covering a greater distance.\n\nM. C. Escher's painting \"Ascending and Descending\" illustrates a non-conservative vector field, impossibly made to appear to be the gradient of the varying height above ground as one moves along the staircase. It is \"rotational\" in that one can keep getting higher or keep getting lower while going around in circles. It is non-conservative in that one can return to one's starting point while ascending more than one descends or vice versa. On a real staircase, the height above the ground is a scalar potential field: If one returns to the same place, one goes upward exactly as much as one goes downward. Its gradient would be a conservative vector field and is irrotational. The situation depicted in the painting is impossible.\n\nA vector field formula_2, where formula_3 is an open subset of formula_4, is said to be conservative if and only if there exists a formula_5 scalar field formula_6 on formula_3 such that\n\nHere, formula_9 denotes the gradient of formula_6. When the equation above holds, formula_6 is called a scalar potential for formula_12.\n\nThe fundamental theorem of vector calculus states that any vector field can be expressed as the sum of a conservative vector field and a solenoidal field.\n\nA key property of a conservative vector field formula_12 is that its integral along a path depends only on the endpoints of that path, not the particular route taken. Suppose that formula_14 is a rectifiable path in formula_3 with initial point formula_16 and terminal point formula_17. If formula_18 for some formula_5 scalar field formula_6 so that formula_12 is a conservative vector field, then the gradient theorem states that\n\nThis holds as a consequence of the chain rule and the fundamental theorem of calculus.\n\nAn equivalent formulation of this is that\n\nfor every rectifiable simple closed path formula_24 in formula_3. The converse of this statement is also true: If the circulation of formula_12 around every rectifiable simple closed path in formula_3 is formula_28, then formula_12 is a conservative vector field.\n\nLet formula_30, and let formula_31 be a formula_5 vector field, with formula_3 open as always. Then formula_12 is called irrotational if and only if its curl is formula_35 everywhere in formula_3, i.e., if\n\nFor this reason, such vector fields are sometimes referred to as curl-free vector fields or curl-less vector fields. They are also referred to as longitudinal vector fields.\n\nIt is an identity of vector calculus that for any formula_38 scalar field formula_6 on formula_3, we have\n\nTherefore, every formula_5 conservative vector field on formula_3 is also an irrotational vector field on formula_3.\n\nProvided that formula_3 is simply connected, the converse of this is also true: Every irrotational vector field on formula_3 is a formula_5 conservative vector field on formula_3.\n\nThe above statement is \"not\" true in general if formula_3 is not simply connected. Let formula_3 be formula_51 with the formula_52-axis removed, i.e., formula_53. Now, define a vector field formula_12 on formula_3 by\n\nThen formula_12 has zero curl everywhere in formula_3, i.e., formula_12 is irrotational. However, the circulation of formula_12 around the unit circle in the formula_61-plane is formula_62. Indeed, note that in polar coordinates, formula_63, so the integral over the unit circle is\n\nTherefore, formula_12 does not have the path-independence property discussed above and is not conservative. (However, in any simply connected open sub-region of formula_3, it is still true that formula_12 is conservative. In fact, the field above is the gradient of formula_68. As we know from complex analysis, this is a multi-valued function that requires a branch cut from the origin to formula_69 to be defined in a continuous manner. Hence, in a region that does not go around the formula_52-axis, its gradient is conservative.)\n\nIn a simply connected open region, an irrotational vector field has the path-independence property. This can be seen by noting that in such a region, an irrotational vector field is conservative, and conservative vector fields have the path-independence property. The result can also be proved directly by using Stokes' theorem. In a simply connected open region, any vector field that has the path-independence property must also be irrotational.\n\nMore abstractly, in the presence of a Riemannian metric, vector fields correspond to differential formula_71-forms. The conservative vector fields correspond to the exact formula_71-forms, that is, to the forms which are the exterior derivative formula_73 of a function (scalar field) formula_74 on formula_3. The irrotational vector fields correspond to the closed formula_71-forms, that is, to the formula_71-forms formula_78 such that formula_79. As formula_80, any exact form is closed, so any conservative vector field is irrotational. Conversely, all closed formula_71-forms are exact if and only if formula_3 is simply connected. To see why, note that being simply connected means that the fundamental group of formula_3 vanishes. Because formula_3 is an open subset of formula_85, any loop may be straightened to a finite polygonal path, and since such paths have only finitely many self-intersections, the loop is a union of finitely many simple closed curves. It follows that the fundamental group vanishes if and only if the first singular homology group formula_86 vanishes. It also follows that formula_86 is generated by the classes of simple closed curves and is therefore torsion-free. Hence by the universal coefficient theorem, the singular homology group vanishes if and only if the singular cohomology group formula_88 vanishes. By de Rham's theorem, the latter group is isomorphic to the first de Rham cohomology group formula_89. Consequently, formula_3 is simply connected if and only if all closed formula_71-forms are exact.\n\nThe flow velocity formula_12 of a fluid is a vector field, and the vorticity formula_93 of the flow can be defined by\n\nA common alternative notation for vorticity is formula_95.\n\nIf formula_12 is irrotational, with formula_97, then the flow is said to be an irrotational flow. The vorticity of an irrotational flow is zero.\n\nKelvin's circulation theorem states that a fluid that is irrotational in an inviscid flow will remain irrotational. This result can be derived from the vorticity transport equation, obtained by taking the curl of the Navier-Stokes Equations.\n\nFor a two-dimensional flow, the vorticity acts as a measure of the \"local\" rotation of fluid elements. Note that the vorticity does \"not\" imply anything about the global behavior of a fluid. It is possible for a fluid traveling in a straight line to have vorticity, and it is possible for a fluid that moves in a circle to be irrotational.\n\nIf the vector field associated to a force formula_98 is conservative, then the force is said to be a conservative force.\n\nThe most prominent examples of conservative forces are the gravitational force and the electric force associated to an electrostatic field. According to Newton's law of gravitation, the gravitational force formula_99 acting on a mass formula_100 due to a mass formula_101, which is a distance formula_102 between them, obeys the equation\n\nwhere formula_104 is the gravitational constant and formula_105 is a \"unit\" vector pointing from formula_101 toward formula_100. The force of gravity is conservative because formula_108, where\n\nis the gravitational potential energy.\n\nFor conservative forces, \"path independence\" can be interpreted to mean that the work done in going from a point formula_16 to a point formula_17 is independent of the path chosen, and that the work formula_112 done in going around a simple closed loop is formula_28:\n\nThe total energy of a particle moving under the influence of conservative forces is conserved, in the sense that a loss of potential energy is converted to an equal quantity of kinetic energy, or vice versa.\n\n"}
{"id": "37044564", "url": "https://en.wikipedia.org/wiki?curid=37044564", "title": "Current limiting reactor", "text": "Current limiting reactor\n\nIn electrical engineering, current limiting reactors can reduce short-circuit currents, which result from plant expansions and power source additions, to levels that can be adequately handled by existing distribution equipment.\nThey can also be used in high voltage electric power transmission grids for a similar purpose. In the control of electric motors, current limiting reactors can be used to restrict starting current or as part of a speed control system.\nCurrent limiting reactors, once called current limiting reactance coils, were first presented in 1915.\nThe inventor of the current limiting reactance coil was Vern E. Alden who filed the patent on November 20, 1917 with an issue date of September 11, 1923. The original assignee was Westinghouse Electric & Manufacturing Company.\n\nA current limiting reactor is used when the prospective short-circuit current in a distribution or transmission system is calculated to exceed the interrupting rating of the associated switchgear. The inductive reactance is chosen to be low enough for an acceptable voltage drop during normal operation, but high enough to restrict a short circuit to the rating of the switchgear. \nThe amount of protection that a current limiting reactor offers depends upon the percentage increase in impedance that it provides for the system.\n\nThe main motive of using current limiting reactors is to reduce short-circuit currents so that circuit breakers with lower short circuit breaking capacity can be used. They can also be used to protect other system components from high current levels and to limit the inrush current when starting a large motor.\n\nIt is desirable that the reactor does not go into magnetic saturation during a short-circuit, so generally an air-core coil is used. At low and medium voltages, air-insulated coils are practical; for high transmission voltages, the coils may be immersed in transformer oil. \nInstallation of air-core coils requires consideration of the magnetic field produced by the coils, which may induce current in large nearby metal objects. This may result in objectionable temperature rise and waste of energy.\n\nA line reactor is an inductor wired between a power source and a load. In addition to the current limiting function, the device serves to filter out spikes of current and may also reduce injection of harmonic currents into the power supply. The most common type is designed for three-phase electric power, in which three isolated inductors are each wired in series with one of the three line phases. Line reactors are generally installed in motor driven equipment to limit starting current, and may be used to protect Variable-frequency drives and motors.:)\n\n"}
{"id": "37977625", "url": "https://en.wikipedia.org/wiki?curid=37977625", "title": "Daubréeite", "text": "Daubréeite\n\nDaubréeite is a rare bismuth oxohalide mineral with formula BiO(OH,Cl). It is a creamy-white to yellow-brown, soft, earthy clay–like mineral which crystallizes in the tetragonal crystal system. It is a member of the matlockite group.\n\nIt was first described for an occurrence in the Constanicia mine, Tazna, Bolivia, in 1876. It was named for French mineralogist Gabriel Auguste Daubrée (1814–1896). At the Tanza location it occurs as a secondary mineral formed by the oxidation of native bismuth or bismuthinite. It occurs with clay minerals. In addition to its discovery location it has also been reported from the Tintic District in the East Tintic Mountains of Juab County, Utah; in the Josephine Creek District of Josephine County, Oregon; in the Manhattan District of Nye County, Nevada; and the Rio Marina Mine on Elba, Italy.\n"}
{"id": "859285", "url": "https://en.wikipedia.org/wiki?curid=859285", "title": "Displacement (fluid)", "text": "Displacement (fluid)\n\nIn fluid mechanics, displacement occurs when an object is immersed in a fluid, pushing it out of the way and taking its place. The volume of the fluid displaced can then be measured, and from this, the volume of the immersed object can be deduced (the volume of the immersed object will be exactly equal to the volume of the displaced fluid).\n\nAn object that sinks displaces an amount of fluid equal to the object's volume. Thus buoyancy is expressed through Archimedes' principle, which states that the weight of the object is reduced by its volume multiplied by the density of the fluid. If the weight of the object is less than this displaced quantity, the object floats; if more, it sinks. The amount of fluid displaced is directly related (via Archimedes' Principle) to its volume.\n\nIn the case of an object that sinks (is totally submerged), the volume of the object is displaced. In the case of an object that floats, the amount of fluid displaced will be equal in weight to the displacing object.\n\nArchimedes' principle, a physical law of buoyancy, states that any body completely or partially submerged in a fluid (gas or liquid) at rest is acted upon by an upward, or buoyant, force the magnitude of which is equal to the weight of the fluid displaced by the body. The volume of displaced fluid is equivalent to the volume of an object fully immersed in a fluid or to that fraction of the volume below the surface of an object partially submerged in a liquid. The weight of the displaced portion of the fluid is equivalent to the magnitude of the buoyant force. The buoyant force on a body floating in a liquid or gas is also equivalent in magnitude to the weight of the floating object and is opposite in direction; the object neither rises nor sinks. If the weight of an object is less than that of the displaced fluid, the object rises, as in the case of a block of wood that is released beneath the surface of water or a helium-filled balloon that is let loose in the air. An object heavier than the amount of the fluid it displaces, though it sinks when released, has an apparent weight loss equal to the weight of the fluid displaced. In fact, in some accurate weighing, a correction must be made in order to compensate for the buoyancy effect of the surrounding air. The buoyant force, which always opposes gravity, is nevertheless caused by gravity. Fluid pressure increases with depth because of the (gravitational) weight of the fluid above. This increasing pressure applies a force on a submerged object that increases with depth. The result is buoyancy.\n\nThis method can be used to measure the volume of a solid object, even if its form is not regular. Several methods of such measuring exist. In one case the increase of liquid level is registered as the object is immersed in the liquid (usually water). In the second case, the object is immersed into a vessel full of liquid (called an overflow can), causing it to overflow. Then the spilled liquid is collected and its volume measured. In the third case, the object is suspended under the surface of the liquid and the increase of weight of the vessel is measured. The increase in weight is equal to the amount of liquid displaced by the object, which is the same as the volume of the suspended object times the density of the liquid.\n\nThe concept of Archimedes' principle is that an object immersed in a fluid is buoyed up by a force equal to the weight of the fluid displaced by the object. The weight of the displaced fluid can be found mathematically. The mass of the displaced fluid can be expressed in terms of the density and its volume, . The fluid displaced has a weight , where is acceleration due to gravity. Therefore, the weight of the displaced fluid can be expressed as . \n\nThe weight of an object or substance can be measured by floating a sufficiently buoyant receptacle in the cylinder and noting the water level. After placing the object or substance in the receptacle, the difference in weight of the water level volumes will equal the weight of the object.\n\n"}
{"id": "8435296", "url": "https://en.wikipedia.org/wiki?curid=8435296", "title": "EU Directive on Electricity Production from Renewable Energy Sources 2001/77/EC", "text": "EU Directive on Electricity Production from Renewable Energy Sources 2001/77/EC\n\nThe Directive on Electricity Production from Renewable Energy Sources 2001/77/EC is a European Union Directive for promoting renewable energy use in electricity generation. It is popularly known as the RES Directive. \n\nThe directive, which took effect in October 2001, sets national indicative targets for renewable energy production from individual member states. As the name implies, the EU does not strictly enforce these targets. However, The European Commission monitors the progress of the member states of the European Union – and will, if necessary, propose mandatory targets for those who miss their goals. \n\nThese objectives contribute toward achieving the overall indicative EU targets, which are listed in the white paper on renewable sources of energy. Regulators want a 12% share of gross renewable domestic energy consumption by 2010 and a 20% share by 2020.\n\nThe directive was superseded by Renewable Energy Directive 2009/28/EC, published on 23 April 2009.\n\nThe following table lists the indicative targets for each of the 15 original member states, and for comparison the share of renewable electricity in 1997 as well.\n\n\n"}
{"id": "33637949", "url": "https://en.wikipedia.org/wiki?curid=33637949", "title": "Eggshell and protein membrane separation", "text": "Eggshell and protein membrane separation\n\nEggshell and protein membrane separation is a recycling process. Nearly 30% of the eggs consumed each year are broken and processed or powdered into foods such as cakes, mixes, mayonnaise, noodles and fast foods. The US food industry generates 150,000 tons of shell waste a year. The disposal methods for waste eggshells are 26.6% as fertilizer, 21.1% as animal feed ingredients, 26.3% discarded in municipal dumps, and 15.8% used in other ways. Many landfills are unwilling to take the waste because the shells and the attached membrane attract vermin. Together the calcium carbonate eggshell and protein-rich membrane have no value or use. The invention of an eggshell and membrane separator has allowed for the recycling of these two valuable products.\n\nChicken eggshells are made up of 95% calcium carbonate by weight and the remaining material, 3.5%, is organic matrix.\n\nThe rich calcium carbonate shell has been used in the application for calcium deficiency therapies in humans and animals. A single eggshell has a mass of six grams which yields around 2200 mg of calcium. Eggshell particles are used in toothpaste as an anti-tartar agent. Powdered eggshells have been used for bone mineralization and growth.\n\nRecent applications of eggshells in the form of calcium lactate has been used as a firming agent, a flavor enhancer, a flavoring agent, a leavening agent, a nutrient supplement, a stabilizer, and thickener. Eggshells are also used in as a calcium supplement in orange juice.\n\nEggshells have been incorporated into fertilizers as a soil conditioner. They have also been used as a supplement to animal feed. More recently the egg calcium carbonate particles have been used as coating pigments for ink-jet printing. Powdered eggshells are also used in making paper pulp. Recently eggshell waste has been used as a low cost catalyst for biodiesel production.\n\nJoseph H. MacNeil, Professor of Food Science at Penn State University, developed a machine that uses a delicate multi-bladed knife to scrape the membrane from the surface of shell fragments. This invention uses a water-based method to separate the eggshell and protein membrane. The two products are processed in two streams to yield mm size particles of dry membrane and mm size particles of dried shell.\n\nAnother non-chemical separation technique utilizes steam heat, mechanical abrasion, and a light vacuum to separate the hard eggshell from the protein-containing membranes. This invention passes shell fragments obtained from egg-breaking facilities through a series of heated augers. Once the shell and membrane flakes reach the appropriate moisture content, the vacuum pulls them into a cyclone device. The cyclonic action further separates the heavier eggshell flakes from the lighter membrane flakes. This invention has been commercialized and can easily separate multiple metric tons per day, allowing for the production of valuable products including commercial volumes of natural eggshell membrane or NEM.\n\nLevi New invented a non-chemical and non-thermal separation technique that utilizes airflow to pull egg shells through a venturi. The material is subjected to pressure waves while passing through the venturi. A PulverDryer machine configured with a venturi sized to process egg shell pulverizes the shell component into fine powder and discharges the membrane as an intact dry flake material. The two components are separated in post processing by a standard shaker screen table, then the membrane is cleaned via one of several standard washing processes, depending on end use. Up to several tons of egg shell material can be processed per hour.\n\nThe waste eggshells are put into water and then ground to separate the eggshell from the protein membrane. Then the ground eggshell is placed in a separate vessel where air is injected into the water flow. The air and water mixture causes the lighter component (protein membrane) to float and the heavier (calcium carbonate eggshells) to sink. This unit recovers 96% of eggshell membrane and 99% of eggshell calcium carbonate in two hours.\n\nThe inventor of this method is Vladimir Vlad. The machine uses unseparated eggshells that are placed in a fluid tank, applying cavitation to separate the eggshell membrane from the eggshell. The fluid tank in this case contains distilled water and acetic acid to provide continuous processing. Also this invention has a method for collecting separated eggshells to grind them into an eggshell powder.\n"}
{"id": "760994", "url": "https://en.wikipedia.org/wiki?curid=760994", "title": "Electron mobility", "text": "Electron mobility\n\nIn solid-state physics, the electron mobility characterizes how quickly an electron can move through a metal or semiconductor, when pulled by an electric field. There is an analogous quantity for holes, called hole mobility. The term carrier mobility refers in general to both electron and hole mobility.\n\nElectron and hole mobility are special cases of electrical mobility of charged particles in a fluid under an applied electric field.\n\nWhen an electric field \"E\" is applied across a piece of material, the electrons respond by moving with an average velocity called the drift velocity, formula_1. Then the electron mobility μ is defined as\n\nElectron mobility is almost always specified in units of cm/(V·s). This is different from the SI unit of mobility, m/(V·s). They are related by 1m/(V·s) = 10cm/(V·s).\n\nConductivity is proportional to the product of mobility and carrier concentration. For example, the same conductivity could come from a small number of electrons with high mobility for each, or a large number of electrons with a small mobility for each. For metals, it would not typically matter which of these is the case, since most metal electrical behavior depends on conductivity alone. Therefore mobility is relatively unimportant in metal physics. On the other hand, for semiconductors, the behavior of transistors and other devices can be very different depending on whether there are many electrons with low mobility or few electrons with high mobility. Therefore mobility is a very important parameter for semiconductor materials. Almost always, higher mobility leads to better device performance, with other things equal.\n\nSemiconductor mobility depends on the impurity concentrations (including donor and acceptor concentrations), defect concentration, temperature, and electron and hole concentrations. It also depends on the electric field, particularly at high fields when velocity saturation occurs. It can be determined by the Hall effect, or inferred from transistor behavior.\n\nWithout any applied electric field, in a solid, electrons and holes move around randomly. Therefore, on average there will be no overall motion of charge carriers in any particular direction over time.\n\nHowever, when an electric field is applied, each electron or hole is accelerated by the electric field. If the electron were in a vacuum, it would be accelerated to ever-increasing velocity (called ballistic transport). However, in a solid, the electron repeatedly scatters off crystal defects, phonons, impurities, etc., so that it loses some energy and changes direction. The final result is that the electron moves with a finite average velocity, called the drift velocity. This net electron motion is usually much slower than the normally occurring random motion.\n\nThe two charge carriers, electrons and holes, will typically have different drift velocities for the same electric field.\n\nQuasi-ballistic transport is possible in solids if the electrons are accelerated across a very small distance (as small as the mean free path), or for a very short time (as short as the mean free time). In these cases, drift velocity and mobility are not meaningful.\n\nThe electron mobility is defined by the equation:\nwhere:\nThe hole mobility is defined by the same equation. Both electron and hole mobilities are positive by definition.\n\nUsually, the electron drift velocity in a material is directly proportional to the electric field, which means that the electron mobility is a constant (independent of electric field). When this is not true (for example, in very large electric fields), the mobility depends on the electric field.\n\nThe SI unit of velocity is m/s, and the SI unit of electric field is V/m. Therefore the SI unit of mobility is (m/s)/(V/m) = m/(V·s). However, mobility is much more commonly expressed in cm/(V·s) = 10 m/(V·s).\n\nMobility is usually a strong function of material impurities and temperature, and is determined empirically. Mobility values are typically presented in table or chart form. Mobility is also different for electrons and holes in a given material.\n\nThere is a simple relation between mobility and electrical conductivity. Let \"n\" be the number density (concentration) of electrons, and let μ be their mobility. In the electric field E, each of these electrons will move with the velocity vector formula_4, for a total current density of formula_5 (where \"e\" is the elementary charge). Therefore, the electrical conductivity σ satisfies:\nThis formula is valid when the conductivity is due entirely to electrons. In a p-type semiconductor, the conductivity is due to holes instead, but the formula is essentially the same: If \"p\" is the concentration of holes and μ is the hole mobility, then the conductivity is\nIf a semiconductor has both electrons and holes, the total conductivity is\n\nTypical electron mobility for Si at room temperature (300 K) is 1400 cm/ (V·s) and the hole mobility is around 450 cm/ (V·s).\n\nVery high mobility has been found in several low-dimensional systems, such as two-dimensional electron gases (2DEG) (35,000,000 cm/(V·s) at low temperature), carbon nanotubes (100,000 cm/(V·s) at room temperature) and more recently, graphene (200,000 cm/ V·s at low temperature).\nOrganic semiconductors (polymer, oligomer) developed thus far have carrier mobilities below 50 cm/(V·s), and usually much lower.\nAt low fields, the drift velocity \"v\" is proportional to the electric field \"E\", so mobility \"μ\" is constant. This value of \"μ\" is called the \"low-field mobility\".\n\nAs the electric field is increased, however, the carrier velocity increases sublinearly and asymptotically towards a maximum possible value, called the \"saturation velocity\" \"v\". For example, the value of \"v\" is on the order of 1×10 cm/s for both electrons and holes in Si. It is on the order of 6×10 cm/s for Ge. This velocity is a characteristic of the material and a strong function of doping or impurity levels and temperature. It is one of the key material and semiconductor device properties that determine a device such as a transistor's ultimate limit of speed of response and frequency.\n\nThis velocity saturation phenomenon results from a process called \"optical phonon scattering\". At high fields, carriers are accelerated enough to gain sufficient kinetic energy between collisions to emit an optical phonon, and they do so very quickly, before being accelerated once again. The velocity that the electron reaches before emitting a phonon is:\nwhere \"ω\" is the optical-phonon angular frequency and m* the carrier effective mass in the direction of the electric field. The value of \"E\" is 0.063 eV for Si and 0.034 eV for GaAs and Ge. The saturation velocity is only one-half of \"v\", because the electron starts at zero velocity and accelerates up to \"v\" in each cycle. (This is a somewhat oversimplified description.)\n\nVelocity saturation is not the only possible high-field behavior. Another is the Gunn effect, where a sufficiently high electric field can cause intervalley electron transfer, which reduces drift velocity. This is unusual; increasing the electric field almost always \"increases\" the drift velocity, or else leaves it unchanged. The result is negative differential resistance.\n\nIn the regime of velocity saturation (or other high-field effects), mobility is a strong function of electric field. This means that mobility is a somewhat less useful concept, compared to simply discussing drift velocity directly.\n\nRecall that by definition, mobility is dependent on the drift velocity. The main factor determining drift velocity (other than effective mass) is scattering time, i.e. how long the carrier is ballistically accelerated by the electric field until it scatters (collides) with something that changes its direction and/or energy. The most important sources of scattering in typical semiconductor materials, discussed below, are ionized impurity scattering and acoustic phonon scattering (also called lattice scattering). In some cases other sources of scattering may be important, such as neutral impurity scattering, optical phonon scattering, surface scattering, and defect scattering.\n\nElastic scattering means that energy is (almost) conserved during the scattering event. Some elastic scattering processes are scattering from acoustic phonons, impurity scattering, piezoelectric scattering, etc. In acoustic phonon scattering, electrons scatter from state k to k', while emitting or absorbing a phonon of wave vector q. This phenomenon is usually modeled by assuming that lattice vibrations cause small shifts in energy bands. The additional potential causing the scattering process is generated by the deviations of bands due to these small transitions from frozen lattice positions.\n\nSemiconductors are doped with donors and/or acceptors, which are typically ionized, and are thus charged. The Coulombic forces will deflect an electron or hole approaching the ionized impurity. This is known as \"ionized impurity scattering\". The amount of deflection depends on the speed of the carrier and its proximity to the ion. The more heavily a material is doped, the higher the probability that a carrier will collide with an ion in a given time, and the smaller the mean free time between collisions, and the smaller the mobility. When determining the strength of these interactions due to the long-range nature of the Coulomb potential, other impurities and free carriers cause the range of interaction with the carriers to reduce significantly compared to bare Coulomb interaction.\n\nIf these scatterers are near the interface, the complexity of the problem increases due to the existence of crystal defects and disorders. Charge trapping centers that scatter free carriers form in many cases due to defects associated with dangling bonds. Scattering happens because after trapping a charge, the defect becomes charged and therefore starts interacting with free carriers. If scattered carriers are in the inversion layer at the interface, the reduced dimensionality of the carriers makes the case differ from the case of bulk impurity scattering as carriers move only in two dimensions. Interfacial roughness also causes short-range scattering limiting the mobility of quasi-two-dimensional electrons at the interface.\n\nAt any temperature above absolute zero, the vibrating atoms create pressure (acoustic) waves in the crystal, which are termed phonons. Like electrons, phonons can be considered to be particles. A phonon can interact (collide) with an electron (or hole) and scatter it. At higher temperature, there are more phonons, and thus increased electron scattering, which tends to reduce mobility.\n\nPiezoelectric effect can occur only in compound semiconductor due to their polar nature. It is small in most semiconductors but may lead to local electric fields that cause scattering of carriers by deflecting them, this effect is important mainly at low temperatures where other scattering mechanisms are weak. These electric fields arise from the distortion of the basic unit cell as strain is applied in certain directions in the lattice.\n\nSurface roughness scattering caused by interfacial disorder is short range scattering limiting the mobility of quasi-two-dimensional electrons at the interface. From high-resolution transmission electron micrographs, it has been determined that the interface is not abrupt on the atomic level, but actual position of the interfacial plane varies one or two atomic layers along the surface. These variations are random and cause fluctuations of the energy levels at the interface, which then causes scattering.\n\nIn compound (alloy) semiconductors, which many thermoelectric materials are, scattering caused by the perturbation of crystal potential due to the random positioning of substituting atom species in a relevant sublattice is known as alloy scattering. This can only happen in ternary or higher alloys as their crystal structure forms by randomly replacing some atoms in one of the sublattices (sublattice) of the crystal structure. Generally, this phenomenon is quite weak but in certain materials or circumstances, it can become dominant effect limiting conductivity. In bulk materials, interface scattering is usually ignored.\n\nDuring inelastic scattering processes, significant energy exchange happens. As with elastic phonon scattering also in the inelastic case, the potential arises from energy band deformations caused by atomic vibrations. Optical phonons causing inelastic scattering usually have the energy in the range 30-50 meV, for comparison energies of acoustic phonon are typically less than 1 meV but some might have energy in order of 10 meV. There is significant change in carrier energy during the scattering process. Optical or high-energy acoustic phonons can also cause intervalley or interband scattering, which means that scattering is not limited within single valley.\n\nDue to the Pauli exclusion principle, electrons can be considered as non-interacting if their density does not exceed the value 10~10 cm or electric field value 10 V/cm. However, significantly above these limits electron–electron scattering starts to dominate. Long range and nonlinearity of the Coulomb potential governing interactions between electrons make these interactions difficult to deal with.\n\nA simple model gives the approximate relation between scattering time (average time between scattering events) and mobility. It is assumed that after each scattering event, the carrier's motion is randomized, so it has zero average velocity. After that, it accelerates uniformly in the electric field, until it scatters again. The resulting average drift mobility is:\nwhere \"q\" is the elementary charge, m* is the carrier effective mass, and is the average scattering time.\n\nIf the effective mass is anisotropic (direction-dependent), m* is the effective mass in the direction of the electric field.\n\nNormally, more than one source of scattering is present, for example both impurities and lattice phonons. It is normally a very good approximation to combine their influences using \"Matthiessen's Rule\" (developed from work by Augustus Matthiessen in 1864):\n\nwhere µ is the actual mobility, formula_12 is the mobility that the material would have if there was impurity scattering but no other source of scattering, and formula_13 is the mobility that the material would have if there was lattice phonon scattering but no other source of scattering. Other terms may be added for other scattering sources, for example\nMatthiessen's rule can also be stated in terms of the scattering time:\nwhere \"τ\" is the true average scattering time and τ is the scattering time if there was impurity scattering but no other source of scattering, etc.\n\nMatthiessen's rule is an approximation and is not universally valid. This rule is not valid if the factors affecting the mobility depend on each other, because individual scattering probabilities cannot be summed unless they are independent of each other. The average free time of flight of a carrier and therefore the relaxation time is inversely proportional to the scattering probability. For example, lattice scattering alters the average electron velocity (in the electric-field direction), which in turn alters the tendency to scatter off impurities. There are more complicated formulas that attempt to take these effects into account.\n\nWith increasing temperature, phonon concentration increases and causes increased scattering. Thus lattice scattering lowers the carrier mobility more and more at higher temperature. Theoretical calculations reveal that the mobility in non-polar semiconductors, such as silicon and germanium, is dominated by acoustic phonon interaction. The resulting mobility is expected to be proportional to \"T\" , while the mobility due to optical phonon scattering only is expected to be proportional to \"T\" . Experimentally, values of the temperature dependence of the mobility in Si, Ge and GaAs are listed in table.\n\nAs formula_16, where formula_17 is the scattering cross section for electrons and holes at a scattering center and formula_18 is a thermal average (Boltzmann statistics) over all electron or hole velocities in the lower conduction band or upper valence band, temperature dependence of the mobility can be determined. In here, the following definition for the scattering cross section is used: number of particles scattered into solid angle dΩ per unit time divided by number of particles per area per time (incident intensity), which comes from classical mechanics. As Boltzmann statistics are valid for semiconductors \nformula_19.\n\nFor scattering from acoustic phonons, for temperatures well above Debye temperature, the estimated cross section Σ is determined from the square of the average vibrational amplitude of a phonon to be proportional to T. The scattering from charged defects (ionized donors or acceptors) leads to the cross section formula_20. This formula is the scattering cross section for \"Rutherford scattering\", where a point charge (carrier) moves past another point charge (defect) experiencing Coulomb interaction.\n\nThe temperature dependencies of these two scattering mechanism in semiconductors can be determined by combining formulas for τ, Σ and formula_18, to be for scattering from acoustic phonons \nformula_22 and from charged defects formula_23.\n\nThe effect of ionized impurity scattering, however, \"decreases\" with increasing temperature because the average thermal speeds of the carriers are increased. Thus, the carriers spend less time near an ionized impurity as they pass and the scattering effect of the ions is thus reduced.\n\nThese two effects operate simultaneously on the carriers through Matthiessen's rule. At lower temperatures, ionized impurity scattering dominates, while at higher temperatures, phonon scattering dominates, and the actual mobility reaches a maximum at an intermediate temperature.\n\nCarrier mobility is most commonly measured using the Hall effect. The result of the measurement is called the \"Hall mobility\" (meaning \"mobility inferred from a Hall-effect measurement\").\n\nConsider a semiconductor sample with a rectangular cross section as shown in the figures, a current is flowing in the \"x\"-direction and a magnetic field is applied in the \"z\"-direction. The resulting Lorentz force will accelerate the electrons (\"n\"-type materials) or holes (\"p\"-type materials) in the (−\"y\") direction, according to the right hand rule and set up an electric field \"ξ\". As a result there is a voltage across the sample, which can be measured with a high-impedance voltmeter. This voltage, \"V\", is called the Hall voltage. \"V\" is negative for \"n\"-type material and positive for \"p\"-type material.\n\nMathematically, the Lorentz force acting on a charge \"q\" is given by\n\nFor electrons:\n\nFor holes:\n\nIn steady state this force is balanced by the force set up by the Hall voltage, so that there is no net force on the carriers in the \"y\" direction. For electron,\n\nFor electrons, the field points in the -\"y\" direction, and for holes, it points in the +\"y\" direction.\n\nThe electron current \"I\" is given by formula_29. Sub \"v\" into the expression for \"ξ\",\n\nwhere \"R\" is the Hall coefficient for electron, and is defined as\n\nSince formula_32\n\nSimilarly, for holes\n\nFrom the Hall coefficient, we can obtain the carrier mobility as follows:\n\nSimilarly,\n\nHere the value of \"V (Hall voltage), t (sample thickness), I (current) and B (magnetic field)\" can be measured directly, and the conductivities σ or σ are either known or can be obtained from measuring the resistivity.\n\nThe mobility can also be measured using a field-effect transistor (FET). The result of the measurement is called the \"field-effect mobility\" (meaning \"mobility inferred from a field-effect measurement\").\n\nThe measurement can work in two ways: From saturation-mode measurements, or linear-region measurements. (See MOSFET for a description of the different modes or regions of operation.)\n\nIn this technique, for each fixed gate voltage V, the drain-source voltage V is increased until the current I saturates. Next, the square root of this saturated current is plotted against the gate voltage, and the slope m is measured. Then the mobility is:\nwhere \"L\" and \"W\" are the length and width of the channel and \"C\" is the gate insulator capacitance per unit area. This equation comes from the approximate equation for a MOSFET in saturation mode:\nwhere V is the threshold voltage. This approximation ignores the Early effect (channel length modulation), among other things. In practice, this technique may underestimate the true mobility.\n\nIn this technique, the transistor is operated in the linear region (or \"ohmic mode\"), where V is small and formula_40 with slope m. Then the mobility is:\nThis equation comes from the approximate equation for a MOSFET in the linear region:\nIn practice, this technique may overestimate the true mobility, because if V is not small enough and V is not large enough, the MOSFET may not stay in the linear region.\n\nElectron mobility may be determined from non-contact laser photo-reflectance measurements. A series of photo-reflectance measurements are made as the sample is stepped through focus. The electron diffusion length and recombination time are determined by a regressive fit to the data. Then the Einstein relation is used to calculate the mobility. \n\nElectron mobility can be calculated from time-resolved terahertz probe measurement. Femtosecond laser pulses excite the semiconductor and the resulting photoconductivity is measured using a terahertz probe, which detects changes in the terahertz electric field.\n\nThe charge carriers in semiconductors are electrons and holes. Their numbers are controlled by the concentrations of impurity elements, i.e. doping concentration. Thus doping concentration has great influence on carrier mobility.\n\nWhile there is considerable scatter in the experimental data, for noncompensated material (no counter doping) for heavily doped substrates (i.e. formula_43 and up), the mobility in silicon is often characterized by the empirical relationship: \n\nwhere \"N\" is the doping concentration (either \"N\" or \"N\"), and \"N\" and α are fitting parameters. At room temperature, the above equation becomes:\n\nMajority carriers: \n\nMinority carriers: \n\nThese equations apply only to silicon, and only under low field.\n\n\n"}
{"id": "2861802", "url": "https://en.wikipedia.org/wiki?curid=2861802", "title": "Environmental management scheme", "text": "Environmental management scheme\n\nAn environmental management scheme is a mechanism by which landowners and other individuals and bodies responsible for land management can be incentivised to manage their environment.\n\nSeveral schemes (prgrammes) are or have been in operation in Australia, including:\n\nOther programmes exist with the various states, for example;\nNew South Wales - Catchment Action Plan\n\nSeveral schemes are or have been in operation in the United Kingdom, including:\n\nCurrently England operates:\n\n\nAll of these schemes are adminisitered by Natural England\n\nIn 2007 Scotland adopted the SRDP (Scottish Rural Development Programme - a £1.6 billion programme of economic, environmental and social measures designed to develop rural Scotland. Individuals and groups may seek support to help deliver the Government's strategic objectives in rural Scotland.\n\nUntil recently the prevailing agri-environment scheme in Wales was Tir Gofal, which means literally ‘Land Care’. It was the first scheme in Wales, and indeed in Europe, aimed at promoting whole farm conservation and management. It was different from previous schemes, as it brought farming and conservation into a different level of partnership.\n\nIt was recently announced that this scheme has ceased, with a bridging payment scheme until its replacement is launched.\n\nIn some countries such as France, such schemes may be initiated by the central government:\n\nSwitzerland started reforming its agricultural policies in 1993 and after a referendum in 1996, since 1998 the country has linked the attribution of farm subsidies with the strict observance of good environmental practice. Before farmers can apply for subsidies, they must obtain certificates of environmental management systems (EMS) proving that they: “make a balanced use of fertilizers; use at least 7% of their farmland as ecological compensation areas; regularly rotate crops; adopt appropriate measures to protect animals and soil; make limited and targeted use of pesticides.”\n\n"}
{"id": "10454708", "url": "https://en.wikipedia.org/wiki?curid=10454708", "title": "European Environmental Press", "text": "European Environmental Press\n\nThe European Environmental Press (EEP) is a Europe-wide association of nearly two dozen environmental magazines. The combined circulation is 800.000. Each member is considered the leader in its country and is committed to building links between 400,000 environmental professionals across Europe in both the public and private sectors. The EEP is unique in bringing together the leading national business-to-business magazines as an expert network for the dissemination of high-quality environmental information throughout Europe. The target market for EEP is highly educated individuals. It deals with different environmental issues across the board, including water supply, waste management, recycling,remediation of contaminated land, air pollution, noise, energy, and tracking technologies as well as environmental administration. The magazine is highly technologically optimistic and has focused on many technological solutions to environmental problems.\n\nThe European Environmental Press presents various companies with awards that relate to creating clever and efficient solutions to environmental problems as well as in their efforts to reduce pollution and contribute to bettering the environment. They do this in association with Pollutec,a French trade-show company and is endorsed by The European Network of Environmental Professionals (EFAP). Applications are announced and nobody is exempt from the opportunity to participate.A committee gathers and nominates ten winners from all the ones that applied. The committee then decides the winners from the nominated batch. There are three winners, gold, silver and bronze. The gold winners receive spreads in the EEP magazines and they also get the opportunity to present their projects at various conferences across Europe the most important one being Pollutec. Being nominated is also useful for the companies as their work often gets cited in the magazine, sparking discussion, fundraising and inspiration. \nThe Guardian and the EEP collaborated in May 2010 in recognizing China's efforts at reducing pollution and taking steps towards a more sustainable future. This was also a journalism event, honoring journalists who were doing new and thorough investigations of the country's environment.\n\n\n"}
{"id": "736240", "url": "https://en.wikipedia.org/wiki?curid=736240", "title": "Extremes on Earth", "text": "Extremes on Earth\n\nThis article describes extreme locations on Earth. Entries listed in bold are Earth-wide extremes.\n\nTemperatures measured directly on the ground may exceed air temperatures by 30 to 50 °C. A ground temperature of 84 °C (183.2 °F) has been recorded in Port Sudan, Sudan. A ground temperature of 93.9 °C (201 °F) was recorded in Furnace Creek, Death Valley, California, United States on 15 July 1972; this may be the highest natural ground surface temperature ever recorded. The theoretical maximum possible ground surface temperature has been estimated to be between 90 and 100 °C for dry, darkish soils of low thermal conductivity.\n\nSatellite measurements of ground temperature taken between 2003 and 2009, taken with the MODIS infrared spectroradiometer on the Aqua satellite, found a maximum temperature of 70.7 °C (159.3 °F), which was recorded in 2005 in the Lut Desert, Iran. The Lut Desert was also found to have the highest maximum temperature in 5 of the 7 years measured (2004, 2005, 2006, 2007 and 2009). These measurements reflect averages over a large region and so are lower than the maximum point surface temperature.\n\nSatellite measurements of the surface temperature of Antarctica, taken between 1982 and 2013, found a coldest temperature of −93.2 °C (−136 °F) on 10 August 2010, at . Although this is not comparable to an air temperature, it is believed that the air temperature at this location would have been lower than the official record lowest air temperature of −89.2 °C.\n\nIce sheets on land, but having the base below sea level. Places under ice are not considered to be on land.\n\nThe Gould Coast (Coordinates: ) is the southernmost point of ocean while the southernmost open sea is nearby Bay of Whales at 78°30'S, at the edge of Ross Ice Shelf.\n\n"}
{"id": "360781", "url": "https://en.wikipedia.org/wiki?curid=360781", "title": "Fuel tax", "text": "Fuel tax\n\nA fuel tax (also known as a petrol, gasoline or gas tax, or as a fuel duty) is an excise tax imposed on the sale of fuel. In most countries the fuel tax is imposed on fuels which are intended for transportation. Fuels used to power agricultural vehicles, and/or home heating oil which is similar to diesel are taxed at a different, usually lower rate. The fuel tax receipts are often dedicated or hypothecated to transportation projects so that the fuel tax is considered by many a user fee. In other countries, the fuel tax is a source of general revenue. Sometimes, the fuel tax is used as an ecotax, to promote ecological sustainability. Fuel taxes are often considered regressive taxes.\n\nTaxes on transportation fuels have been advocated as a way to reduce pollution and the possibility of global warming and conserve energy. Placing higher taxes on fossil fuels makes petrol just as expensive as other fuels such as natural gas, biodiesel or electric batteries, at a cost to the consumer in the form of inflation as transportation costs rise to transport goods all over the country.\n\nProponents advocate that automobiles should pay for the roads they use and argue that the user tax should not be applied to mass transit projects.\n\nThe Intergovernmental Panel on Climate Change, the International Energy Agency, the International Monetary Fund, and the World Bank have called on governments to increase gasoline tax rates in order to combat the social and environmental costs of gasoline consumption.\n\nInternational pump prices for diesel and gasoline for November 2010 are available at https://web.archive.org/web/20110315151539/http://www.gtz.de/en/themen/29957.htm . Price history from surveys taken in November of even number years are also available. Price differences mostly reflect differences in tax policy. \n\nA Nature study has shown that while gasoline taxes have increased in more countries than they have decreased in during the period 2003-2015, the global mean gasoline tax has decreased due to greater consumption in the low tax countries. \nChinese gasoline taxes have increased the most among the top twenty CO2-emitting countries over the period 2003-2015.\n\nIn China, fuel tax has been a very contentious issue. Efforts by the State Council to institute a fuel tax in order to finance the National Trunk Highway System have run into strong opposition from the National People's Congress, largely out of concern for its impact on farmers. This has been one of the uncommon instances in which the legislature has asserted its authority.\n\nIn Hong Kong, leaded petrol is taxed at HK$6.82 per litre and unleaded petrol at HK$6.06 per litre. The tax on Euro V diesel is $0.\nIn India, the pricing of fuel varies by state, though central taxes still are part of the pump price of fuel. The Central and state government's taxes make up nearly half of petrol's pump price. The Central govt has different taxes, which amount to about 24–26% of the final cost. The states taxes vary, but on average end up making about 20–25% of the final cost. As a result, approximately 51% of the pump cost goes to the government in the form of different taxes.\n\nFor example, in Bengaluru, Karnataka as of May 16, 2011, price of petrol is per litre. Out of this, go to Govt of India in the form of excise and customs tax. is collected by state government in the form of sales tax and entry tax. Thus, a total of is collected due to various taxes (which accounts for around 47% of the total price).\n\nPetroleum products destined for utilisation by aircraft engaged in commercial flights outside of the customs territory of continental France are exempt from all customs duties and domestic taxes.\n\nFuel taxes in Germany are €0.4704 per litre for ultra-low sulphur Diesel and €0.6545 per litre for conventional unleaded petrol, plus Value Added Tax (19%) on the fuel itself and the Fuel Tax. That adds up to prices of €1.12 per litre for ultra-low sulphur Diesel and €1.27 per litre (approximately US$6.14 per US gallon) for unleaded petrol (December 2014).\n\nThe sale of fuels in the Netherlands is levied with an excise tax. As of 2015, petrol excise tax is EUR0.766 per litre and diesel excise tax is EUR0.482 per litre, while LPG excise tax is EUR0.185 per litre. The 2007 fuel tax was €0.684 per litre or $3.5 per gallon. On top of that is 21% VAT over the entire fuel price, making the Dutch taxes one of the highest in the world. In total, taxes account for 68.84% of the total price of petrol and 56.55% of the total price of diesel. A 1995 excise was raised by Dutch gulden 25 cents (€0.11), the \"Kok Quarter\" (€0.08 raise per litre gasoline and €0.03 raise per litre diesel), by then Prime-Minister Wim Kok is now specifically set aside by the second Balkenende cabinet for use in road creation and road and public transport maintenance.\n\nMotor fuel is taxed with both a \"road use tax\" and a \"CO-tax\". The road use tax on petrol is NOK 4.62 per litre and the CO-tax on petrol is NOK 0.88 per litre. The road use tax on auto diesel is NOK 3.62 per litre mineral oil and NOK 1.81 per litre bio diesel. The CO-tax on mineral oil is NOK 0.59 per litre.\n\nIn Poland half of the end-user price charged at a petrol station goes towards 3 distinct taxes:\nExcise and fuel tax are prescribed by European Commission law, and therefore cannot be lower in any EU nation. However it is even higher than this EU minimum in Poland, a policy pursued by the former Minister of finance.\nTax on mineral resource extraction (2008–2009):\n\nExcise tax on motor fuel 2008–2009:\n\nOther fuel (like avia gasoline, jet fuel, heavy oils, natural gas and autogas) prices has no excise tax.\n\nValue Added Tax — 18% on fuel and taxes.\n\nFull tax rate is near 55% of motor fuel prices (ministry of industry and energy facts 2006).\n\nThe fuel tax in Sweden comprises a carbon tax and an energy tax. The total tax (including value added tax) is, from July 1, 2018, per liter petrol and per liter diesel.\n\nFrom 23 March 2011 the UK duty rate for the road fuels unleaded petrol, diesel, biodiesel and bioethanol is per litre (£2.63 per imperial gallon or £2.19 per U.S. gallon).\n\nValue Added Tax at 20% is also charged on the price of the fuel and on the duty. An additional vehicle excise duty, depending on a vehicle's theoretical CO2 production per kilometre, which is applied regardless of the amount of fuel actually consumed, is also levied.\n\nDiesel for use by farmers and construction vehicles is coloured red (Red Diesel) and has a much reduced tax, currently per litre (£0.52 per imperial gallon or £0.43 per U.S. gallon).\n\nJet fuel used for international aviation attracts no duty, and no VAT.\n\nFuel taxes in Canada can vary greatly between locales. On average, about one-third of the total price of gas at the pump is tax. Excise taxes on gasoline and diesel are collected both federal and provincial governments, as well as by some select municipalities (Montreal, Vancouver, and Victoria); with combined excise taxes varying from 16.2 ¢/L (73.6 ¢/imperial gal; 61.2 ¢/US gal) in the Yukon to 30.5 ¢/L ($1.386/imperial gal; $1.552/?US gal) in Vancouver. As well, the federal government and some provincial governments (Newfoundland and Labrador, Nova Scotia, and Quebec) collect sales tax (GST and PST) on top of the retail price \"and\" the excise taxes.\n\nThe first U.S. state to enact a gas tax was Oregon in 1919. The states of Colorado, North Dakota, and New Mexico followed shortly thereafter. By 1929, all existing 48 states had enacted some sort of gas tax. Today, fuel taxes in the United States vary by state. The United States federal excise tax on gasoline is 18.4 cents per gallon (4.86 ¢/L) and 24.4 cents per gallon (6.45 ¢/L) for diesel fuel. On average, as of July 2016, state and local taxes add 29.78 cents to gasoline and 29.81 cents to diesel for a total US average fuel tax of 48.18 cents per gallon for gas (12.89 ¢/L) and 54.21 cents per gallon for diesel (14.37 ¢/L).\n\nThe state and local tax figures includes fixed-per-gallon taxes as well as variable-rate taxes such as those levied as a percentage of the sales price. For state-level fuel taxes, nineteen states and the District of Columbia levy variable-rate taxes of some kind. The other thirty one states do not tie the per-gallon tax rate to inflation, gas prices, or other factors, and the rate changes only by legislation. As of July 2016, twenty one states had gone ten years or more without an increase in their per-gallon gasoline tax rate.\n\nBecause the fuel tax is universally styled as a \"road use\" tax, exempting off-road farming, marine, etc. use; states impose a tax on commercial operators traveling through their state as if the fuel used was bought there, wherever the fuel is purchased. While most commercial truck drivers have an agent to handle the required paperwork: what's reported is how much tax was collected in each state, how much should have been paid to each state, the net tax for each state and the combined net tax for all states to be paid by or refunded to the operator by their base jurisdiction where they file. The operator carries paperwork proving compliance. The member jurisdictions, the US states and the CA provinces, transmit the return information to each other and settle their net tax balances with each other either by a single transmittal through a clearinghouse set up by the IFTA and operated by Morgan Stanley, or by separate transfers with the other member jurisdictions.\n\nThe fuel tax system in Australia is very similar to Canada in terms of featuring both a fixed and a variable tax, but varies in the case of exemptions including tax credits and certain excise free fuel.\n\nSince October 2018, the fuel tax in Australia is A$0.412 per litre for petrol and ultra-low sulphur diesel (conventional diesel being taxed at A$0.412 per litre) and the excise for LPG is $0.282 per litre. Since 2000, there is also the GST (goods and services tax) on top of the fuel tax and inflation calculated twice a year called CPI (consumer price index) into the fuel tax since 2015.\n\nFuel taxes in New Zealand are considered an excise applied by the New Zealand Customs Service on shipments brought into the country. A breakdown of the fuel taxes is published by the Ministry of Economic Development. Excise as at 1 August 2012 totals 50.524 cents per litre ($1.89/imperial gal; $1.58/US gal) on petrol. In addition the national compulsory Accident Compensation Corporation motor vehicle account receives a contribution of 9.9 cents/litre (37.1¢/imperial gal; 30.9¢/US gal). The ethanol component of bio blended petrol currently attracts no excise duty. This was to be reviewed in 2012. Diesel is not taxed at the pump, but road users with vehicles over 3.5 tonne in Gross Laden Weight and any vehicles not powered wholly by any combination of petrol, LPG or CNG must pay the Road User Charge instead. The Goods and Services Tax (15%) is then applied to the combined total of the value of the commodity and the various taxes. On 25 July 2007 the Minister of Transport Annette King announced that from 1 July 2008 all fuel excise collected would be hypothecated to the National Land Transport Programme.\n\nDiesel tax - for road use ( $ per gallon ): $1.46. Gasoline tax - for road use ( $ per gallon ): $1.58.\n\n\n"}
{"id": "53141433", "url": "https://en.wikipedia.org/wiki?curid=53141433", "title": "Glidden Doman", "text": "Glidden Doman\n\nGlidden Doman (January 28, 1921 – June 6, 2016) was an American aeronautical engineer and pioneer in helicopters and modern wind turbines. He founded one of America's original six helicopter companies (Doman Helicopters, Inc.) after making major contributions to the use of Sikorsky helicopters during World War II. Doman Helicopters' most prominent achievement was the Doman LZ-5/YH-31 eight-place helicopter, which received FAA certification on December 30, 1955. The unique feature of this helicopter was its hinge-less but gimbaled, tilting rotor hub that greatly reduced stress and vibration in the blades and in the whole helicopter.\nDoman was one of the first to transfer knowledge of helicopter rotor dynamics technology to wind turbines. The 1973 arab oil embargo prompted NASA Glenn Research Center in Cleveland, Ohio to lead a 7-year US wind energy program for the development of utility-scale horizontal axis wind turbines. This program featured the creation of Boeing's MOD-2 with the Doman conceived flexible rotor design, two-bladed wind turbine with a teeter hinge. Following the NASA project, while working for Boeing, Hamilton Standard division of United Technologies, and Aeritalia (later known as Alenia) in Italy, Doman developed large two-bladed, teeter-hinged wind turbines, including the WTS-3, WTS-4, and the Gamma 60. After testing the Gamma 60 in Sardinia for 7 years, Doman and Italian nuclear mechanical engineer Silvestro Caruso founded Gamma Ventures, Inc. to further develop and market this technology. Gamma Ventures subsequently invested in, and sold a license to Seawind of the Netherlands, to commercialize the same two-bladed, teeter-hinge wind turbine concept.\n\nDoman, along with noted German-born aerospace engineer Kurt Hohenemser (a partner and confidant of the well-known German airplane and helicopter designer Anton Flettner), maintained that a flexible two-bladed helicopter type wind turbine rotor design that is compliant with the forces of nature was more suitable for producing electricity than the rigid industry standard three-bladed airplane type wind turbine rotors that, by design, can only be constructed to resist the forces of nature.\n\nTwo of Doman's helicopters, the converted Sikorsky R-6 (Doman LZ-1A) and a Doman LZ-5/YH-31, are on display at the New England Air Museum in Windsor Locks, Connecticut.\n\nGlidden Sweet Doman, the son of Albert E. Doman and Ruth Sweet Doman, was born in Syracuse, New York, on January 28, 1921. Living in the small upstate New York village of Elbridge, Doman came from a family of inventors and entrepreneurs. His father Albert and uncles Lewis and George Doman were the first to provide electricity for Elbridge, in 1890. Doman's elder half-brother, Carl T. Doman, designed air cooled Franklin engines for Franklin automobiles and for aircraft – including for the pioneering Sikorsky VS-300 helicopter.\n\nIn his teens, Doman built a series of six motorized \"go carts\", and in 1936, at the age of 15, he built a Soapbox Derby racer, the first to be aerodynamically streamlined. He won the regional Soapbox Derby race held in Syracuse and placed well in the national championship race in Akron, Ohio. (2)(4) Doman also attempted to build an airplane, completing much of the fuselage, but was never able to obtain an engine for it.\n\nIn 1938, Doman enrolled at the University of Michigan (Ann Arbor) College of Engineering, where he majored in aeronautical engineering. While at Michigan, Doman joined and later became president of the flying club. Another member of the flying club, Joan Hamilton, Doman's future bride, was the only female member of the club. Hamilton obtained her private's pilot license and later during World War II she was invited by the famous aviatrix Jackie Cochran to join the WASPs (Women Air Force Service Pilots), but she declined the offer.\n\nDoman graduated from Michigan in June 1942, just six months after the bombing of Pearl Harbor. After his graduation, he went to work for the Ranger Aircraft Engine Division of Fairchild Aviation in Long Island, New York. Doman had prior experience working with engine technologies while employed during summer vacations at the Franklin Engine Company (where his brother Carl worked). At Ranger, Doman gained experience in using strain gauges to analyze the vibratory loads on the rotating parts of engines.\n\nCarl Doman, who knew Igor Sikorsky due to the Franklin engine's use in the Sikorsky VS-300, invited Doman to attend a Society of Automotive Engineers meeting in New York. At this event, with Sikorsky as a featured speaker, Doman became interested in the vibratory loads in helicopter rotors. As a new and unrefined invention at that time, helicopters were suffering from rotor blades with a very short fatigue life. In fact, not long after Doman met Sikorsky, a Sikorsky YR-4 had a blade fracture during a delivery flight to the Army, which stimulated Doman's interest all the more.\n\nIn August 1943, Doman married Joan Hamilton and they immediately moved to Stamford, Connecticut where he commuted to and from work for Sikorsky in Bridgeport, CT. Doman put strain gages all over the rotor system of a Sikorsky YR-4 to locate the weak points. He then figured out where and how to change the rotor system to achieve longer blade life. Doman's contributions were so vital that Igor Sikorsky himself appealed to Doman's Draft Board to keep him on the test program instead of sending him into the Army or Navy. Doman discovered that the design of the rotor hub was the real cause of the blade failures. For the duration of World War II, he ran a program to match and balance the blades of all the Sikorsky helicopters before they were delivered. Throughout his tenure at Sikorsky, Doman was continuously learning about rotor dynamics.\n\nDoman had one very harrowing experience at Sikorsky when he was flying as a test engineer on a YR-4. The pilot had slowed the helicopter to very low speed, and it got into the so-called \"vortex ring state\" where the blades lose lift and the helicopter thrashes around in an almost uncontrollable fashion. Fatal crashes from such an event have resulted more than once in the history of rotary wing aviation. In this case, however, the pilot somehow got the helicopter flying backwards, which enabled the blades to regain their lifting ability and prevent a crash. While at Sikorsky, Doman got to know Igor Sikorsky, and for years afterwards Doman liked to repeat his favorite quote from Igor Sikorsky, \"God takes care of helicopters.\"\n\nTogether with Clinton Frazier, a mathematician and colleague at Sikorsky, Doman left Sikorsky after WW II to further develop and implement his ideas. Doman-Frazier Helicopters, Inc. was established in the autumn of 1945. The company was initially located in the back of a law office in New York City, but soon moved into a barn in Stratford, Connecticut, not far from the Sikorsky plant. With financial assistance from the United States Army Air Corps, Doman obtained a war surplus R-6 Sikorsky helicopter. Doman and his engineering team modified the R-6 by creating an entirely new rotor system which featured a unique gimbaled rotor hub that was subsequently used in all Doman helicopters. After extensive successful flight testing of the modified Sikorsky R-6 (Doman LZ-1A), Doman's team designed and built, in a joint venture with the Curtiss-Wright Corp., an all-new Doman helicopter, the CW-40. The CW-40 was followed a couple of years later by the Doman LZ-5/YH-31. The many innovative features in this helicopter were soon covered by numerous Doman patents. Meanwhile, after the departure of Mr. Frazier, the company was renamed Doman Helicopters, Inc. and moved to Danbury, CT. Doman Helicopters continued for nearly 20 years, building and testing several helicopters and performing other related aircraft work to sustain the company while it attempted to establish itself as a major helicopter manufacturer. The Doman LZ-5/YH-31 helicopters were extensively tested by the company and by the U.S. Army and Navy. They received certification from the United States FAA, and from the equivalent Canadian authority, for sale and commercial use, after passing the rigorous tests necessary for such approvals. Doman LZ-5/YH-31 helicopters toured the U.S. and were well received by potential customers, especially in the oil industry along the Gulf Coast. One even toured in France and Italy performing at the Paris Air Show in 1960.\n\nDoman helicopters were a technical success but the company was unable to raise sufficient venture capital to set up an assembly line for mass production. The company ceased operations in 1969 and was legally dissolved shortly thereafter.\n\nIn January 1970, Doman went to work for Boeing Vertol – Boeing's helicopter division in Philadelphia. There he continued his helicopter pioneering, working on U.S. Army funded research and patenting additional design improvements for helicopter speed increases and vibration reductions. The 1973 arab oil embargo triggered a great interest in wind energy at Boeing and numerous other companies. While continuing his helicopter innovations, Doman began to conduct wind turbine research. He adapted computer simulation models designed for helicopters to study wind turbines. Doman and his team then built scale-model wind turbines to test them in a wind tunnel normally used for helicopters and airplanes. Doman soon understood the similarities and key differences between wind turbines and helicopters. Wind turbine designers with no experience in helicopters might not know the lessons-learned from the history of helicopter technology, but by the mid-1970s Doman had more than 30 years of such learning that was directly applicable to wind turbines. Boeing's MOD-2 with the Doman-conceived flexible design, two-bladed wind turbine with a teeter hinge, became a flagship achievement in a 7-year NASA managed wind energy program for the U.S. Department of Energy and the U.S. Department of the Interior.\n\nIn January 1978, Doman returned to Connecticut as Chief Systems Engineer of the wind energy program at Hamilton Standard division of United Technologies. Drawing on Doman's extensive knowledge of rotor dynamics for both helicopters and wind turbines, United Technologies designed and built two of the largest wind turbines ever built up to that time (i.e. WTS-3, WTS-4). Key features of those turbines, in addition to their size, were the use of only two blades (instead of three, which is more common) and the mounting of the blades on a teeter hinge hub. This was analogous to Doman's use of the gimbaled hub on his helicopters, which had 4-bladed rotors. One turbine (WTS-4) was installed and successfully tested at Medicine Bow, Wyoming, and the other (WTS-3) in Sweden. The WTS-4 wind turbine held the world power output record for over 20 years. Throughout all the research studies and the testing of the actual turbines, there was rapid evolution in Doman's thinking on the best design concepts. However, when oil prices plummeted in the mid-1980s, United Technologies deemed the wind energy market to be uneconomical and halted the program.\n\nThe Italian government took notice of Doman's work, and in July 1987, when Doman retired from United Technologies, he was hired by Aeritalia (the Italian manufacturer later known as Alenia) and moved to Rome to head Italy's wind energy program. In this program, Doman applied his latest thinking to a new machine, considerably improved over the WTS-4 and WTS-3 machines he had designed for installation in Wyoming and Sweden respectively. Under Doman's leadership a team of Italian engineers designed the Gamma 60 turbine. The Gamma 60 was the world's first variable speed wind turbine with a teeter hinge. Three Gamma 60 turbines were manufactured, and one was erected and successfully tested for 4 years on the Mediterranean island of Sardinia. There had been intentions to build many more Gamma 60 turbines, but Italian politics and a lack of urgency due to relatively low oil prices in the 1990s resulted in the program being cancelled.\n\nIn 2003, Doman and Italian nuclear mechanical engineer Silvestro Caruso formed a new company, Gamma Ventures Inc., and bought two unused turbines, the engineering drawings, and manufacturing rights from the Italian Gamma 60 project. In 2007, Gamma Ventures sold a turbine and manufacturing rights to the Dutch company – Blue H Technologies – that was adapting offshore oil platform technology to put wind turbines on floating platforms in deep water far from shore. Blue H Technologies installed the world's first floating offshore wind turbine in 2008 in the Southern Adriatic Sea with a two-bladed wind turbine. After some revisions in corporate structures and plans, the Gamma rights were eventually transferred to a new company, Seawind, which is working on placing turbines with Gamma-type rotors on concrete offshore support structures, aiming to do so in numerous locations around the world.\n\nDoman remained active right up to the end of his life in 2016; thoroughly engaged in business, technical strategies, and analysis toward superior wind turbine performance at less cost in the application of his novel rotor technologies. He was also an avid supporter of the New England Air Museum (in Windsor Locks, Connecticut), where two of his helicopters – the converted R-6 (Doman-LZ1A) and a Doman LZ-5/YH-31 – are on display.\n\nOf the original 6 helicopter companies in the United States, Doman was the last company founder to pass away. He was one of the few helicopter pioneers to have transferred rotor dynamics technology from helicopters to wind turbines.\n\nA Norwegian project announced in February 2017, involving Dr. Techn. Olav Olsen and Seawind, aims to demonstrate the wind energy applications put forth by Glidden Doman, Anton Flettner, and Kurt Hohenemser in harsh wind and sea conditions. Doman worked through to his final days to ensure that one day wind-generated electricity will be produced in great quantities, much less expensively than now, using his advanced rotor technology concepts.\n\n\n"}
{"id": "1711121", "url": "https://en.wikipedia.org/wiki?curid=1711121", "title": "HVDC Tjæreborg", "text": "HVDC Tjæreborg\n\nThe HVDC Tjæreborg is a 4.3 kilometre long bipolar high-voltage direct current (HVDC) electric power transmission line for demonstrating and testing DC interconnection of a wind park to the Danish power grid. The line was constructed to connect an existing onshore wind park consisting of 4 windmills in Tjæreborg Enge to the grid through an additional DC line. Thus the mills can be connected to the grid by both the new HVDC, the old AC or a combination of both lines.\n\nThe HVDC Tjæreborg was commissioned in 2000 and is designed for a voltage of 9 kV and a maximum power rating of 7.2 megawatt.\n\nThe technology of HVDC allows a better regulation of power peaks.\n\n"}
{"id": "2258803", "url": "https://en.wikipedia.org/wiki?curid=2258803", "title": "IEC 60364", "text": "IEC 60364\n\nIEC 60364 \"Electrical Installations for Buildings\" is the International Electrotechnical Commission's international standard on \"electrical installations of buildings\". This standard is an attempt to harmonize national wiring standards in an IEC standard and is published in the European Union by CENELEC as \"HD 60364\". The latest versions of many European wiring regulations (e.g., BS 7671 in the UK) follow the section structure of IEC 60364 very closely, but contain additional language to cater for historic national practice and to simplify field use and determination of compliance by electrical tradesmen and inspectors. National codes and site guides are meant to attain the common objectives of IEC 60364, and provide rules in a form that allows for guidance of persons installing and inspecting electrical systems.\n\nThe standard has several parts:\n\n"}
{"id": "1916615", "url": "https://en.wikipedia.org/wiki?curid=1916615", "title": "JCO (company)", "text": "JCO (company)\n\nJCO was a Japanese nuclear fuel cycle company established in October 1979 as a wholly owned subsidiary of Sumitomo Metal Mining Co., Ltd. as Japan Nuclear Fuel Conversion Co. . As of 2000, stock capitalization was 1 billion USD.\n\n"}
{"id": "49976907", "url": "https://en.wikipedia.org/wiki?curid=49976907", "title": "Japan Photovoltaic Energy Association", "text": "Japan Photovoltaic Energy Association\n\nJapan Photovoltaic Energy Association (JPEA) is the best-known body for solar photovoltaic energy companies in Japan. Founded on April 23, 1987, it has a number of objectives and activities, one of which is providing the JPEC certification for solar modules in the Japanese market. The association serves as a spokesperson for the photovoltaics energy industry in Japan.\n\nJPEA set a pathway for the photovoltaic industry in Japan to reach 100 GW by 2030, from roughly 30 GW in 2015. The group comprises multiple companies in the industry, and reported a 13% drop in total photovoltaic solar panel shipments in 2015.\n"}
{"id": "29549283", "url": "https://en.wikipedia.org/wiki?curid=29549283", "title": "José Benito Vives de Andréis Marine and Coastal Research Institute", "text": "José Benito Vives de Andréis Marine and Coastal Research Institute\n\nThe Marine and Coastal Research Institute \"José Benito Vives de Andréis\", is a non profit marine and coastal research institute of Colombia, linked to the Ministry of Environment and Sustainable Development.\n"}
{"id": "17862372", "url": "https://en.wikipedia.org/wiki?curid=17862372", "title": "Karl Weissenberg", "text": "Karl Weissenberg\n\nKarl Weissenberg (11 June 1893, Vienna – 6 April 1976, The Hague) was an Austrian physicist, notable for his contributions to rheology and crystallography.\n\nThe Weissenberg effect was named after him, as was the Weissenberg number. He invented a Goniometer to study X-ray diffraction of crystals for which he received the Duddell Medal of the Institute of Physics in 1946, The European Society of Rheology offers a \"Weissenberg award\" in his honour. and the Weissenberg rheogoniometer, a type of rheometer.\n\nHe was born on 11 June 1893 in Vienna, Austria and died in 1976 in the Netherlands. He studied at the Universities of Vienna, Berlin and Jena with Mathematics as his main subject. He published on the theories of Symmetry groups and Tensor and Matrix algebra, then applied mathematics and experimentation to crystallography, rheology and medical science.\n\n"}
{"id": "41671071", "url": "https://en.wikipedia.org/wiki?curid=41671071", "title": "Kaunas Combined Heat and Power Plant", "text": "Kaunas Combined Heat and Power Plant\n\nKaunas Combined Heat and Power Plant is a natural gas-fired power plant in Kaunas, Lithuania.\n"}
{"id": "8124411", "url": "https://en.wikipedia.org/wiki?curid=8124411", "title": "Leningrad Nuclear Power Plant", "text": "Leningrad Nuclear Power Plant\n\nLeningrad Nuclear Power Plant ( ()) is a nuclear power plant located in the town of Sosnovy Bor in Russia's Leningrad Oblast, on the southern shore of the Gulf of Finland, some to the west of the city centre of Saint Petersburg. It consists of four nuclear reactors of the RBMK-1000 type. These reactors are similar to reactors No. 1 and 2 of the Chernobyl Nuclear Power Plant. Two units of the VVER-1200 type are under construction at Power Plant II to replace the current RBMK reactors when they reach the end of their service lives.\n\nOn 25 October 2008, Saint Petersburg Atomenergoproekt began concreting the foundation plate of the reactor building of the Leningrad Nuclear Power Plant II, Unit 1. Cost of the project is estimated to be almost 70 billion Russian ruble (RUR). A construction licence was issued on 22 July 2009.\n\nFrom May 2012 to December 2013, Unit 1 was offline while repairs were made related to some deformed graphite moderator blocks.\n\nThe first accident at the plant occurred shortly after the first unit came online. On 7 January 1975 a concrete tank containing radioactive gases from Unit 1 exploded; there were no reported accident victims or radiation releases.\n\nLess than one month later, on 6 February 1975, the secondary cooling circuit of Unit 1 ruptured, releasing contaminated water into the environment. Three people were killed, and the accident was not reported in the media.\n\nOn 28 November 1975 a fuel channel in Unit 1 suffered a loss of coolant, resulting in the degradation of a nuclear fuel assembly that led to a significant release of radiation lasting for one month. Immediately after the accident, the radiation level in Sosnovy Bor, from the affected power unit, was 600 mR/h; in total, 1.5 MCi was released into the environment. The exposed inhabitants of the Baltic region were not notified of the danger. The accident was not reported in the media. (Practically the same accident occurred in Unit 1 of the Chernobyl Power Station in 1982.)\n\nIn July 1976 and again in September 1979, due to a poor safety culture, fire broke out in a concrete vault containing radioactive waste. Water used in extinguishing the fires was contaminated, leaked into the environment, and entered the water table. This was not reported in the media.\n\nOn 28 December 1990, during refurbishment of Unit 1, it was noticed that the space between the fuel channels and the graphite stack (contaminated during the 1975 accident) had widened. The contaminated graphite was spilled, and the radiation levels in the space under the reactor increased. Radiation was detected 6 km away from the unit, but this was not reported in the media.\n\nOn 3 December 1991, due to faulty equipment and lax safety rule compliance, 10 new fuel rods were dropped and damaged. The staff tried to conceal the accident from the management.\n\nIn March 1992, an accident at the Sosnovy Bor nuclear plant leaked radioactive gases and iodine into the air through a ruptured fuel channel. This was the first accident at the station that was announced in the news media.\n\nOn 27 August 2009, the third unit was stopped when a hole was found in the discharge header of a pump. According to the automated radiation control system, the radiation situation at the plant and in its monitoring zone was normal. The plant's management refuted rumors of an accident and stated that the third unit was stopped for a \"short-term unscheduled maintenance\", with a restart scheduled for 31 August 2009.\n\nOn 19 December 2015, unit 2 was stopped (scrammed) due to a broken steam pipe. No radioactively contaminated material was released. The situation inside Leningrad Nuclear Power Plant and the industrial area around the station has not changed and the radiation level remains within the limits of natural background values.\n\n\n"}
{"id": "17880294", "url": "https://en.wikipedia.org/wiki?curid=17880294", "title": "Maceration (sewage)", "text": "Maceration (sewage)\n\nMaceration, in sewage treatment, is the use of a machine that reduces solids to small pieces in order to deal with rags and other solid waste. Macerating toilets use a grinding or blending mechanism to reduce human waste to a slurry, which can then be moved by pumping. This is useful when, for example, water pressure is low or one wishes to install a toilet below the sewer drain pipe.\n\nMaceration can be achieved by using a chopper pump in the sewage lift station or at the wastewater treatment plant. It is sometimes referred to as a \"rotary biological contacter process\".\n\nAt Antarctic research stations with an average summer population of more than 30 people, maceration is the minimum treatment level required before sewage can be disposed of in the sea. This procedure is outlined in the Madrid Protocol, an international treaty outlining environmental practices to be followed in Antarctica. Research stations that do not meet this population threshold are allowed to dump untreated, unmacerated sewage directly into the sea. The treaty also allows ships carrying more than ten people to discharge macerated wastewater (including sewage and food waste) directly into the sea, provided that the vessel is more than 12 nautical miles from shore.\n\nIn food processing plants, maceration refers to the use of a chopper pump to create a \"blended\" slurry of food waste and other organic byproducts. The macerated substance, which can be described as a protein-rich slurry, is often used for animal feed, fertilizer, and for co-digestion feedstock in biogas plants.\n\n"}
{"id": "33902476", "url": "https://en.wikipedia.org/wiki?curid=33902476", "title": "Minusheet perfusion culture system", "text": "Minusheet perfusion culture system\n\nMinusheet perfusion culture system is used for advanced cell culture experiments in combination with adherent cells and to generate specialized tissues in combination with selected biomaterials, special tissue carriers and compatible perfusion culture containers.\n\nThe technical development of the Minusheet perfusion culture system was driven by the idea to create under in vitro conditions an environment resembling as near as possible the situation of specialized tissues found within the organism. Basis of this invention is therefore individually selected biomaterials for optimal cell adhesion mounted in Minusheet tissue carriers. Moreover, to always offer fresh nutrition including respiratory gas and to simulate a tissue-specific fluid environment, the tissue carriers can be inserted into compatible perfusion culture containers. As a result, a variety of publications illustrates that tissues generated by this innovative approach exhibit an excellent and stable quality. Thus, on the one hand the system provides a highly adaptable basis for the culture of adherent cells and the generation of specialized tissues. On the other hand the Minusheet perfusion culture system is bridging a methodical gap between the conventional static 24 well culture plate and modern perfusion culture technology.\n\nSpecialized tissues in culture are urgently needed in regenerative medicine, tissue engineering, nanotechnology, biomaterial research and advanced toxicity testing of newly developed pharmaceuticals. However, it is often observed that raised tissues do not exhibit expected functional features. Instead dedifferentiation is observed [1-4]. These cell biological alterations arise after isolation of cells and proceed during static culture in a dish due to suboptimal fluid environment and minor adhesion on biomaterials. Further uncontrolled supply with nutrition and respiratory gas, an overshoot of metabolites and paracrine factors or missing rheological stress can increase the degree of dedifferentiation. In consequence, regarding an optimal generation of specialized tissues a powerful strategy has to exclude as much as possible harmful parameters, while factors supporting the process of tissue development must be intensified [5].\n\nUnder natural conditions a prerequisite for an optimal tissue development is a cell-specific interaction with the extracellular matrix, while under in vitro conditions a substitute for the extracellular matrix has to be selected. However, the crucial problem is that a biomaterial can influence the development of functional features within a maturing tissue in a good and in a bad sense. In consequence, the suitability of a decellularized extracellular matrix, newly developed synthetic polymers, biodegradable scaffolds, ceramics or metal alloys cannot be predicted but must be tested. \nTo meet parameters positively influencing cell adhesion and communication, the technical concept is based on a Minusheet tissue carrier (Fig. 1). By the help of this tool cell adhesion and development of tissue can be tested with individually selected biomaterials. These experiments can be performed first under static (Fig. 2) and then under dynamic (Fig. 3) culture conditions [6]. In both cases a Minusheet tissue carrier prevents damage but supports development of contained cells or tissues during experimentation. \n\nTo stay compatible with a conventional 24 well culture plate a selected biomaterial must be punched in a diameter of 13 mm. In this format many materials are also commercially available. Further materials can be applied in form of filters, foils, nets, fleeces and scaffolds (Fig. 1a). For an easy handling and to prevent damage during development the selected specimens are placed in the base part of a Minusheet tissue carrier (Fig. 1b). Pressing down a tension ring the biomaterial is held in position (Fig. 1c). After mounting a tissue carrier is enveloped in a bag and sterilized.\n\nFor cell seeding the mounted tissue carrier is transferred by a forceps in a 24 well culture plate (Fig. 2). To concentrate cells on top of a tissue carrier culture medium is added to a level so that the selected biomaterial is just wetted. Then an aliquot of cells is transferred by a pipette to the surface of the mounted biomaterial.\n\nA standard culture protocol with a tissue carrier can be initiated by seeding cells onto the upper side. When a tissue carrier is turned, cells can also be seeded on the other side so that co-culture experiments with two different cell types become possible.\nNot only single cells but also a thin slice of tissue can be mounted between two pieces of a woven net within a Minusheet tissue carrier. Further flexible materials such as collagen sheets can be used in a tissue carrier like the skin of a drum. Last but not least excellent results were obtained by mounting a polyester fleece as an artificial interstitium for spatial parenchyma development [5,6,8]. It is obvious that for each specialized tissue very individual spatial environments within a tissue carrier can be created.\n\nIt has been shown that the static environment within a 24 well culture plate leads to a decrease of nutrition and hormones, an uncontrollable increase of metabolites and an overshoot of paracrine factors during time. Due to these reasons a Minusheet tissue carrier with adherent cells is used only for the short period of cell seeding in a 24 well culture plate.\nIn consequence, after adhesion of cells the tissue carrier is transferred to a perfusion culture container to offer a dynamic fluid environment. To meet the individual requirements of specialized tissues a variety of perfusion culture containers was constructed (Fig. 3).\n\nEach of the perfusion culture containers has at least one inlet and one outlet for the transport of culture medium. A basic version of a container allows the simple bathing of cells respectively growing tissues under continuous medium transport (Fig. 4a). In a gradient container the tissue carrier is placed between the base and the lid so that both sides can be provided with individual media mimicking a typical environment for epithelia (Fig. 4b). A further culture container is made of a transparent lid and base allowing the microscopic observation during tissue development (Fig. 4c).\n\nIn addition, a perfusion culture container can exhibit a flexible silicone lid. Applying force to this lid by an eccentric rotor simulates a mechanical load as required in cartilage and bone tissue engineering. Shaped tissues such as an auricle or different forms of cartilage can be generated with individual scaffolds in a special tissue engineering container. Finally, spatial extension of tubules derived from renal stem/progenitor cells is obtained within a perfusion container filled with an artificial interstitium made of polyester fleece. Finally, all of these containers are machined out of a special polycarbonate (Makrolon®) so that all of them can be autoclaved for multiple uses.\n\nTo maintain the necessary temperature of 37 °C within a perfusion culture container, a heating plate (MEDAX-Nagel, Kiel, Germany) and a cover lid (not shown) are used during performance of culture experiments over weeks (Fig. 5, 7). The transport of culture medium is best accomplished using a slowly rotating peristaltic pump (ISMATEC, IPC N8, Wertheim, Germany). It is able to deliver adjustable and exact pump rates between 0.1 and 5 ml per hour.\n\nOn the passage from the storage bottle through the perfusion culture container medium is transported along a mounted tissue carrier to provide contained cells. The exact geometrical placement of the tissue carrier within a perfusion culture container guarantees during transport of medium provision with always fresh nutrition and respiratory gas from all sides. At the same time it prevents an unphysiological accumulation of metabolic products and an overshoot of paracrine factors. To maintain for the whole culture period this controlled environment, the metabolized medium is collected in a separate waste bottle. In consequence, medium is not recirculated.\n\nNormally cell culture experiments are performed in a CO incubator. Also perfusion culture experiments can be performed in such an atmosphere. However, a much better solution is the performance of perfusion culture experiments under atmospheric air on a laboratory table, since it facilitates the complete handling. However, in this case the culture medium has to be adjusted to atmospheric air.\nKeeping media in a 5% CO atmosphere within an incubator always a relatively high amount of NaHCO is contained to maintain a constant pH between 7.2 and 7.4. If such a formulated medium is used for perfusion culture outside a CO incubator, the pH will shift from the physiological range to much more alkaline values due to the low content of CO (0.3%) in atmospheric air.\nFor that reason any medium used for perfusion culture outside a CO incubator has to be stabilized by reducing the NaHCO concentration and/or by adding biological buffers such as HEPES (GIBCO/Invitrogen, Karlsruhe, Germany) or BUFFER ALL (Sigma-Aldrich-Chemie, München, Germany). The necessary amount can be easily determined by admixing increasing amounts of biological buffer solution to an aliquot of medium. Then the medium must equilibrate over night on a thermo plate at 37 °C under atmospheric air. For example, application of 50 mmol/l HEPES or an equivalent of BUFFER ALL (ca. 1%) to IMDM (Iscove’s Modified Dulbecco’s Medium, GIBCO/Invitrogen) will maintain a constant pH of 7.4 throughout long term perfusion culture under atmospheric air on a laboratory table.\n\nTo obtain in a perfusion culture experiment a high saturation of O a selected medium such as IMDM has to be transported through a gas permeable silicone tube. The use of a silicone tube provides a large surface for gas exchange by diffusion due to a thin wall (1 mm), the small inner diameter (1 mm) and its extended length (1 m). For example, analysis of IMDM (3024 mg/l NaHCO, 50 mmol/l HEPES) equilibrated against atmospheric air during a standard perfusion culture experiment shows constant partial pressures of at least 160 mmHg O [7].\n\nIt has been shown that growing cells and tissues have very individual oxygen requirements. Due to this reason it is important that the content of oxygen can be adapted in individual perfusion culture experiments. The technical solution is a gas exchanger module containing a gas inlet and outlet (Fig. 6a). Further a spiral with a long thin-walled silicon tube for medium transport is mounted inside the module. Since the tube is highly gas-permeable, it guarantees optimal diffusion of gases between culture medium and internal atmosphere of the gas exchange module. In consequence, the desired gas atmosphere can be adjusted by a constant flow of a specific gas mixture through the module. This way the content of oxygen or any other gases can be modulated in the medium by diffusion. Applying this simple protocol it became possible to decrease the oxygen partial pressure within the transported medium during long term culture experiments under absolutely sterile conditions [7].\n\nPerforming perfusion culture experiments it always has to be considered that gas bubbles are forming during slow transport of culture medium. They arise during suction of medium in the storage bottle, during transport within the tube, during distribution within the culture container and during elimination on the way to the waste bottle. Due to unknown reasons gas bubbles accumulate especially at material transitions between tubes, connectors and perfusion containers. First these gas bubbles are so small that they cannot be observed with the human eye, but during ongoing transport of culture medium they increase in size and are able to form an embolus that massively impedes medium flow. Within a culture container gas bubbles are leading to a regional shortage of medium supply and are causing breaks in the fluid continuum so that massive fluid pressure changes result. In a gradient perfusion culture container, where two media are transported at exactly the same speed, embolic effects can lead to pressure differences destroying in turn the contained epithelial barrier [5,9].\n\nTo avoid the concentration of gas bubbles within a perfusion culture experiment, a gas expander module was developed (Fig. 6b). This module removes gas bubbles from the medium during transport. When medium is entering the gas expander module, it rises within a small reservoir and expands before it drops down a barrier. During this process gas bubbles are separated from the medium at the top of the gas expander module. In consequence, medium leaving the container is oxygen-saturated but free of gas bubbles [8,9].\n\nIn the last years numerous papers were published dealing with the Minusheet perfusion culture system. The wide spectrum illustrates that the modular system was applied to generate specialized tissues in excellent cell biological quality used in tissue engineering, biomaterial research and advanced pharmaceutical drug toxicity testing. A complete list of these applications is found in the data bank ‘Proceedings in perfusion culture’ (see 'External links').\nAs demonstrated by numerous patents (DE 39 23 279, DE 42 00 446, DE 42 08 805, DE 44 43 902, DE 19530 556, DE 196 48 876 C2, DE 199 52 847 B4, US 5 190 878, US 5 316 945, US 5 665 599, J 2847669, DE 10 2005 002 938, PA 10 2004 054 125.6, PA 10 2005 001 747.9, patents pending) Will W. Minuth has invented the presented Minusheet perfusion culture system.\n\nNumerous pilot experiments with the Minusheet perfusion culture system were performed in the last years by Lucia Denk and Will W. Minuth. The experimental work is presently focusing on the creation of an artificial polyester interstitium to repair injured renal parenchyma.\n\nIn 1992 the Minusheet perfusion culture system received the Philip Morris research award ‘Challenge of the Future’ in Munich, Germany. The award was handed over by Henry Kissinger, Hans Joachim Friedrichs and Paul Müller.\n\nTo introduce the Minusheet perfusion culture system on the market, Katharina Lorenz-Minuth founded non-profit orientated Minucells and Minutissue Vertriebs GmbH (D-93077 Bad Abbach/Germany).\n\n\n"}
{"id": "2312185", "url": "https://en.wikipedia.org/wiki?curid=2312185", "title": "Missile Datcom", "text": "Missile Datcom\n\nMissile DATCOM is a widely used semi-empirical datasheet component build-up method for the preliminary design and analysis of missile aerodynamics and performance. It has been in continual development for over twenty years, with the latest version released in March 2011. It has traditionally been supplied free of charge by the United States Air Force to American defense contractors. The code is considered restricted under International Traffic in Arms Regulations (ITAR) and should not be distributed outside the United States.\n\n"}
{"id": "17971159", "url": "https://en.wikipedia.org/wiki?curid=17971159", "title": "Mott transition", "text": "Mott transition\n\nA Mott transition is a metal-nonmetal transition in condensed matter. Due to electric field screening the potential energy becomes much more sharply (exponentially) peaked around the equilibrium position of the atom and electrons become localized and can no longer conduct a current.\n\nIn a semiconductor at low temperatures, each 'site' (atom or group of atoms) contains a certain number of electrons and is electrically neutral. For an electron to move away from a site requires a certain amount of energy, as the electron is normally pulled back toward the (now positively charged) site by Coulomb forces. If the temperature is high enough that formula_1 of energy is available per site, the Boltzmann distribution predicts that a significant fraction of electrons will have enough energy to escape their site, leaving an electron hole behind and becoming conduction electrons that conduct current. The result is that at low temperatures a material is insulating, and at high temperatures the material conducts.\n\nWhile the conduction in an n- (p-) type doped semiconductor sets in at high temperatures because the conduction (valence) band is partially filled with electrons (holes) with the original band structure being unchanged, the situation is different in the case of the Mott transition where the band structure itself changes. Mott argued that the transition must be sudden, occurring when the density of free electrons N and the Bohr radius formula_2 satisfies formula_3.\n\nSimply put, a Mott Transition is a change in a material's behavior from insulating to metallic due to various factors. This transition is known to exist in various systems: mercury metal vapor-liquid, metal NH solutions, transition metal chalcogenides and transition metal oxides. In the case of transition metal oxides, the material typically switches from being a good electrical insulator to a good electrical conductor. The insulator-metal transition can also be modified by changes in temperature, pressure or composition (doping). As observed by Mott in his 1949 publication on Ni-oxide, the origin of this behavior is correlations between electrons and the close relationship this phenomenon has to magnetism.\n\nThe physical origin of the Mott transition is the interplay between the Coulomb repulsion of electrons and their degree of localization (band width). Once the carrier density becomes too high (e.g. due to doping), the energy of the system can be lowered by the localization of the formerly conducting electrons (band width reduction), leading to the formation of a band gap, e.g. by pressure (i.e. a semiconductor/insulator).\n\nIn a semiconductor, the doping level also affects the Mott transition. It has been observed that higher dopant concentrations in a semiconductor creates internal stresses that increase the free energy (acting as a change in pressure) of the system, thus reducing the ionization energy.\n\nThe reduced barrier causes easier transfer by tunneling or by thermal emission from donor to its adjacent donor. The effect is enhanced when pressure is applied for the reason stated previously. When the transport of carriers overcomes a minute activation energy, the semiconductor has undergone a Mott transition and become metallic.\n\nOther examples of metal–insulator transition include:\n\nThe theory was first proposed by Nevill Francis Mott in a 1949 paper. Mott also wrote a review of the subject (with a good overview) in 1968. The subject has been thoroughly reviewed in a comprehensive paper by Imada, Fujimori and Tokura\n\n"}
{"id": "849891", "url": "https://en.wikipedia.org/wiki?curid=849891", "title": "NLGI consistency number", "text": "NLGI consistency number\n\nThe NLGI consistency number (sometimes called \"“NLGI grade”\") expresses a measure of the relative hardness of a grease used for lubrication, as specified by the \"standard classification of lubricating grease\" established by the National Lubricating Grease Institute (NLGI). Reproduced in standards \n\nThe NLGI consistency number alone is not sufficient for specifying the grease required by a particular application. However, it complements other classifications (such as and ). Besides consistency, other properties (such as structural and mechanical stability, apparent viscosity, resistance to oxidation, etc.) can be tested to determine the suitability of a grease to a specific application.\n\nNLGI's classification defines nine grades, each associated to a range of ASTM worked penetration values, measured using the test defined by standard \"“cone penetration of lubricating grease”\". This involves two test apparatus. The first apparatus consists of a closed container and a piston-like plunger. The face of the plunger is perforated to allow grease to flow from one side of the plunger to another as the plunger is worked up and down. The test grease is inserted into the container and the plunger is stroked while the test apparatus and grease are maintained at a temperature of .\n\nOnce worked, the grease is placed in a penetration test apparatus. This apparatus consists of a container, a specially-configured cone and a dial indicator. The container is filled with the grease and the top surface of the grease is smoothed over. The cone is placed so that its tip just touches the grease surface and the dial indicator is set to zero at this position. When the test starts, the weight of the cone will cause it to penetrate into the grease. After a specific time interval the depth of penetration is measured.\n\nThe following table shows the NLGI classification and compares each grade with household products of similar consistency. \nCommon greases are in the range 1 through 3. Those with a NLGI No. of 000 to 1 are used in low viscosity applications. Examples include enclosed gear drives operating at low speeds and open gearing. Grades 0, 1 and 2 are used in highly loaded gearing. Grades 1 through 4 are often used in rolling contact bearings. Greases with a higher number are firmer, tend to stay in place and are a good choice when leakage is a concern.\n"}
{"id": "38890", "url": "https://en.wikipedia.org/wiki?curid=38890", "title": "Natural science", "text": "Natural science\n\nNatural science is a branch of science concerned with the description, prediction, and understanding of natural phenomena, based on empirical evidence from observation and experimentation. Mechanisms such as peer review and repeatability of findings are used to try to ensure the validity of scientific advances.\n\nNatural science can be divided into two main branches: life science (or biological science) and physical science. Physical science is subdivided into branches, including physics, chemistry, astronomy and earth science. These branches of natural science may be further divided into more specialized branches (also known as fields).\n\nIn Western society's analytic tradition, the empirical sciences and especially natural sciences use tools from formal sciences, such as mathematics and logic, converting information about nature into measurements which can be explained as clear statements of the \"laws of nature\". The social sciences also use such methods, but rely more on qualitative research, so that they are sometimes called \"soft science\", whereas natural sciences, insofar as they emphasize quantifiable data produced, tested, and confirmed through the scientific method, are sometimes called \"hard science\".\n\nModern natural science succeeded more classical approaches to natural philosophy, usually traced to ancient Greece. Galileo, Descartes, Bacon, and Newton debated the benefits of using approaches which were more mathematical and more experimental in a methodical way. Still, philosophical perspectives, conjectures, and presuppositions, often overlooked, remain necessary in natural science. Systematic data collection, including discovery science, succeeded natural history, which emerged in the 16th century by describing and classifying plants, animals, minerals, and so on. Today, \"natural history\" suggests observational descriptions aimed at popular audiences.\n\nPhilosophers of science have suggested a number of criteria, including Karl Popper's controversial falsifiability criterion, to help them differentiate scientific endeavors from non-scientific ones. Validity, accuracy, and quality control, such as peer review and repeatability of findings, are amongst the most respected criteria in the present-day global scientific community.\n\nThis field encompasses a set of disciplines that examines phenomena related to living organisms. The scale of study can range from sub-component biophysics up to complex ecologies. Biology is concerned with the characteristics, classification and behaviors of organisms, as well as how species were formed and their interactions with each other and the environment.\n\nThe biological fields of botany, zoology, and medicine date back to early periods of civilization, while microbiology was introduced in the 17th century with the invention of the microscope. However, it was not until the 19th century that biology became a unified science. Once scientists discovered commonalities between all living things, it was decided they were best studied as a whole.\n\nSome key developments in biology were the discovery of genetics; evolution through natural selection; the germ theory of disease and the application of the techniques of chemistry and physics at the level of the cell or organic molecule.\n\nModern biology is divided into subdisciplines by the type of organism and by the scale being studied. Molecular biology is the study of the fundamental chemistry of life, while cellular biology is the examination of the cell; the basic building block of all life. At a higher level, anatomy and physiology looks at the internal structures, and their functions, of an organism, while ecology looks at how various organisms interrelate.\n\nConstituting the scientific study of matter at the atomic and molecular scale, chemistry deals primarily with collections of atoms, such as gases, molecules, crystals, and metals. The composition, statistical properties, transformations and reactions of these materials are studied. Chemistry also involves understanding the properties and interactions of individual atoms and molecules for use in larger-scale applications.\n\nMost chemical processes can be studied directly in a laboratory, using a series of (often well-tested) techniques for manipulating materials, as well as an understanding of the underlying processes. Chemistry is often called \"the central science\" because of its role in connecting the other natural sciences.\n\nEarly experiments in chemistry had their roots in the system of Alchemy, a set of beliefs combining mysticism with physical experiments. The science of chemistry began to develop with the work of Robert Boyle, the discoverer of gas, and Antoine Lavoisier, who developed the theory of the Conservation of mass.\n\nThe discovery of the chemical elements and atomic theory began to systematize this science, and researchers developed a fundamental understanding of states of matter, ions, chemical bonds and chemical reactions. The success of this science led to a complementary chemical industry that now plays a significant role in the world economy.\n\nPhysics embodies the study of the fundamental constituents of the universe, the forces and interactions they exert on one another, and the results produced by these interactions. In general, physics is regarded as the fundamental science, because all other natural sciences use and obey the principles and laws set down by the field. Physics relies heavily on mathematics as the logical framework for formulation and quantification of principles.\n\nThe study of the principles of the universe has a long history and largely derives from direct observation and experimentation. The formulation of theories about the governing laws of the universe has been central to the study of physics from very early on, with philosophy gradually yielding to systematic, quantitative experimental testing and observation as the source of verification. Key historical developments in physics include Isaac Newton's theory of universal gravitation and classical mechanics, an understanding of electricity and its relation to magnetism, Einstein's theories of special and general relativity, the development of thermodynamics, and the quantum mechanical model of atomic and subatomic physics.\n\nThe field of physics is extremely broad, and can include such diverse studies as quantum mechanics and theoretical physics, applied physics and optics. Modern physics is becoming increasingly specialized, where researchers tend to focus on a particular area rather than being \"universalists\" like Isaac Newton, Albert Einstein and Lev Landau, who worked in multiple areas.\n\nThis discipline is the science of celestial objects and phenomena that originate outside the Earth's atmosphere. It is concerned with the evolution, physics, chemistry, meteorology, and motion of celestial objects, as well as the formation and development of the universe.\n\nAstronomy includes the examination, study and modeling of stars, planets, comets, galaxies and the cosmos. Most of the information used by astronomers is gathered by remote observation, although some laboratory reproduction of celestial phenomena has been performed (such as the molecular chemistry of the interstellar medium).\n\nWhile the origins of the study of celestial features and phenomena can be traced back to antiquity, the scientific methodology of this field began to develop in the middle of the 17th century. A key factor was Galileo's introduction of the telescope to examine the night sky in more detail.\n\nThe mathematical treatment of astronomy began with Newton's development of celestial mechanics and the laws of gravitation, although it was triggered by earlier work of astronomers such as Kepler. By the 19th century, astronomy had developed into a formal science, with the introduction of instruments such as the spectroscope and photography, along with much-improved telescopes and the creation of professional observatories.\n\nEarth science (also known as geoscience), is an all-embracing term for the sciences related to the planet Earth, including geology, geophysics, hydrology, meteorology, physical geography, oceanography, and soil science.\n\nAlthough mining and precious stones have been human interests throughout the history of civilization, the development of the related sciences of economic geology and mineralogy did not occur until the 18th century. The study of the earth, particularly palaeontology, blossomed in the 19th century. The growth of other disciplines, such as geophysics, in the 20th century led to the development of the theory of plate tectonics in the 1960s, which has had a similar effect on the Earth sciences as the theory of evolution had on biology. Earth sciences today are closely linked to petroleum and mineral resources, climate research and to environmental assessment and remediation.\n\nThough sometimes considered in conjunction with the earth sciences, due to the independent development of its concepts, techniques and practices and also the fact of it having a wide range of sub disciplines under its wing, the atmospheric sciences is also considered a separate branch of natural science. This field studies the characteristics of different layers of the atmosphere from ground level to the edge of the time. The timescale of study also varies from days to centuries. Sometimes the field also includes the study of climatic patterns on planets other than earth.\n\nThe serious study of oceans began in the early to mid-20th century. As a field of natural science, it is relatively young but stand-alone programs offer specializations in the subject. Though some controversies remain as to the categorization of the field under earth sciences, interdisciplinary sciences or as a separate field in its own right, most modern workers in the field agree that it has matured to a state that it has its own paradigms and practices. As such a big family of related studies spanning every aspect of the oceans is now classified under this field.\n\nThe distinctions between the natural science disciplines are not always sharp, and they share a number of cross-discipline fields. Physics plays a significant role in the other natural sciences, as represented by astrophysics, geophysics, chemical physics and biophysics. Likewise chemistry is represented by such fields as biochemistry, chemical biology, geochemistry and astrochemistry.\n\nA particular example of a scientific discipline that draws upon multiple natural sciences is environmental science. This field studies the interactions of physical, chemical, geological, and biological components of the environment, with a particular regard to the effect of human activities and the impact on biodiversity and sustainability. This science also draws upon expertise from other fields such as economics, law and social sciences.\n\nA comparable discipline is oceanography, as it draws upon a similar breadth of scientific disciplines. Oceanography is sub-categorized into more specialized cross-disciplines, such as physical oceanography and marine biology. As the marine ecosystem is very large and diverse, marine biology is further divided into many subfields, including specializations in particular species.\n\nThere are also a subset of cross-disciplinary fields which, by the nature of the problems that they address, have strong currents that run counter to\nspecialization. Put another way: In some fields of integrative application, specialists in more than one field are a key part of most dialog. Such integrative fields, for example, include nanoscience, astrobiology, and complex system informatics.\n\nMaterials science is a relatively new, interdisciplinary field which deals with the study of matter and its properties; as well as the discovery and design of new materials. Originally developed through the field of metallurgy, the study of the properties of materials and solids has now expanded into all materials. The field covers the chemistry, physics and engineering applications of materials including metals, ceramics, artificial polymers, and many others. The core of the field deals with relating structure of material with it properties.\n\nIt is at the forefront of research in science and engineering. It is an important part of forensic engineering (the investigation of materials, products, structures or components that fail or do not operate or function as intended, causing personal injury or damage to property) and failure analysis, the latter being the key to understanding, for example, the cause of various aviation accidents. Many of the most pressing scientific problems that are faced today are due to the limitations of the materials that are available and, as a result, breakthroughs in this field are likely to have a significant impact on the future of technology.\n\nThe basis of materials science involves studying the structure of materials, and relating them to their properties. Once a materials scientist knows about this structure-property correlation, they can then go on to study the relative performance of a material in a certain application. The major determinants of the structure of a material and thus of its properties are its constituent chemical elements and the way in which it has been processed into its final form. These characteristics, taken together and related through the laws of thermodynamics and kinetics, govern a material's microstructure, and thus its properties.\n\nSome scholars trace the origins of natural science as far back as pre-literate human societies, where understanding the natural world was necessary for survival. People observed and built up knowledge about the behavior of animals and the usefulness of plants as food and medicine, which was passed down from generation to generation. These primitive understandings gave way to more formalized inquiry around 3500 to 3000 BC in the Mesopotamian and Ancient Egyptian cultures, which produced the first known written evidence of natural philosophy, the precursor of natural science. While the writings show an interest in astronomy, mathematics and other aspects of the physical world, the ultimate aim of inquiry about nature's workings was in all cases religious or mythological, not scientific.\n\nA tradition of scientific inquiry also emerged in Ancient China, where Taoist alchemists and philosophers experimented with elixirs to extend life and cure ailments. They focused on the yin and yang, or contrasting elements in nature; the yin was associated with femininity and coldness, while yang was associated with masculinity and warmth. The five phases – fire, earth, metal, wood and water – described a cycle of transformations in nature. Water turned into wood, which turned into fire when it burned. The ashes left by fire were earth. Using these principles, Chinese philosophers and doctors explored human anatomy, characterizing organs as predominantly yin or yang and understood the relationship between the pulse, the heart and the flow of blood in the body centuries before it became accepted in the West.\n\nLittle evidence survives of how Ancient Indian cultures around the Indus River understood nature, but some of their perspectives may be reflected in the Vedas, a set of sacred Hindu texts. They reveal a conception of the universe as ever-expanding and constantly being recycled and reformed. Surgeons in the Ayurvedic tradition saw health and illness as a combination of three humors: wind, bile and phlegm. A healthy life was the result of a balance among these humors. In Ayurvedic thought, the body consisted of five elements: earth, water, fire, wind and empty space. Ayurvedic surgeons performed complex surgeries and developed a detailed understanding of human anatomy.\n\nPre-Socratic philosophers in Ancient Greek culture brought natural philosophy a step closer to direct inquiry about cause and effect in nature between 600 and 400 BC, although an element of magic and mythology remained. Natural phenomena such as earthquakes and eclipses were explained increasingly in the context of nature itself instead of being attributed to angry gods. Thales of Miletus, an early philosopher who lived from 625 to 546 BC, explained earthquakes by theorizing that the world floated on water and that water was the fundamental element in nature. In the 5th century BC, Leucippus was an early exponent of atomism, the idea that the world is made up of fundamental indivisible particles. Pythagoras applied Greek innovations in mathematics to astronomy, and suggested that the earth was spherical.\n\nLater Socratic and Platonic thought focused on ethics, morals and art and did not attempt an investigation of the physical world; Plato criticized pre-Socratic thinkers as materialists and anti-religionists. Aristotle, however, a student of Plato who lived from 384 to 322 BC, paid closer attention to the natural world in his philosophy. In his \"History of Animals\", he described the inner workings of 110 species, including the stingray, catfish and bee. He investigated chick embryos by breaking open eggs and observing them at various stages of development. Aristotle's works were influential through the 16th century, and he is considered to be the father of biology for his pioneering work in that science. He also presented philosophies about physics, nature and astronomy using inductive reasoning in his works \"Physics\" and \"Meteorology\".\n\nWhile Aristotle considered natural philosophy more seriously than his predecessors, he approached it as a theoretical branch of science. Still, inspired by his work, Ancient Roman philosophers of the early 1st century AD, including Lucretius, Seneca and Pliny the Elder, wrote treatises that dealt with the rules of the natural world in varying degrees of depth. Many Ancient Roman Neoplatonists of the 3rd to the 6th centuries also adapted Aristotle's teachings on the physical world to a philosophy that emphasized spiritualism. Early medieval philosophers including Macrobius, Calcidius and Martianus Capella also examined the physical world, largely from a cosmological and cosmographical perspective, putting forth theories on the arrangement of celestial bodies and the heavens, which were posited as being composed of aether.\n\nAristotle's works on natural philosophy continued to be translated and studied amid the rise of the Byzantine Empire and Abbasid Caliphate. \n\nIn the Byzantine Empire John Philoponus, an Alexandrian Aristotelian commentator and Christian theologian, was the first who questioned Aristotle's teaching of physics. Unlike Aristotle who based his physics on verbal argument, Philoponus instead relied on observation, and argued for observation rather than resorting into verbal argument. He introduced the theory of impetus. John Philoponus' criticism of Aristotelian principles of physics served as inspiration for Galileo Galilei during the Scientific Revolution.\n\nA revival in mathematics and science took place during the time of the Abbasid Caliphate from the 9th century onward, when Muslim scholars expanded upon Greek and Indian natural philosophy. The words \"alcohol\", \"algebra\" and \"zenith\" all have Arabic roots.\n\nAristotle's works and other Greek natural philosophy did not reach the West until about the middle of the 12th century, when works were translated from Greek and Arabic into Latin. The development of European civilization later in the Middle Ages brought with it further advances in natural philosophy. European inventions such as the horseshoe, horse collar and crop rotation allowed for rapid population growth, eventually giving way to urbanization and the foundation of schools connected to monasteries and cathedrals in modern-day France and England. Aided by the schools, an approach to Christian theology developed that sought to answer questions about nature and other subjects using logic. This approach, however, was seen by some detractors as heresy. By the 12th century, Western European scholars and philosophers came into contact with a body of knowledge of which they had previously been ignorant: a large corpus of works in Greek and Arabic that were preserved by Islamic scholars. Through translation into Latin, Western Europe was introduced to Aristotle and his natural philosophy. These works were taught at new universities in Paris and Oxford by the early 13th century, although the practice was frowned upon by the Catholic church. A 1210 decree from the Synod of Paris ordered that \"no lectures are to be held in Paris either publicly or privately using Aristotle's books on natural philosophy or the commentaries, and we forbid all this under pain of excommunication.\"\n\nIn the late Middle Ages, Spanish philosopher Dominicus Gundissalinus translated a treatise by the earlier Persian scholar Al-Farabi called \"On the Sciences\" into Latin, calling the study of the mechanics of nature \"scientia naturalis\", or natural science. Gundissalinus also proposed his own classification of the natural sciences in his 1150 work \"On the Division of Philosophy\". This was the first detailed classification of the sciences based on Greek and Arab philosophy to reach Western Europe. Gundissalinus defined natural science as \"the science considering only things unabstracted and with motion,\" as opposed to mathematics and sciences that rely on mathematics. Following Al-Farabi, he then separated the sciences into eight parts, including physics, cosmology, meteorology, minerals science and plant and animal science.\n\nLater philosophers made their own classifications of the natural sciences. Robert Kilwardby wrote \"On the Order of the Sciences\" in the 13th century that classed medicine as a mechanical science, along with agriculture, hunting and theater while defining natural science as the science that deals with bodies in motion. Roger Bacon, an English friar and philosopher, wrote that natural science dealt with \"a principle of motion and rest, as in the parts of the elements of fire, air, earth and water, and in all inanimate things made from them.\" These sciences also covered plants, animals and celestial bodies. Later in the 13th century, Catholic priest and theologian Thomas Aquinas defined natural science as dealing with \"mobile beings\" and \"things which depend on matter not only for their existence, but also for their definition.\" There was wide agreement among scholars in medieval times that natural science was about bodies in motion, although there was division about the inclusion of fields including medicine, music and perspective. Philosophers pondered questions including the existence of a vacuum, whether motion could produce heat, the colors of rainbows, the motion of the earth, whether elemental chemicals exist and where in the atmosphere rain is formed.\n\nIn the centuries up through the end of the Middle Ages, natural science was often mingled with philosophies about magic and the occult. Natural philosophy appeared in a wide range of forms, from treatises to encyclopedias to commentaries on Aristotle. The interaction between natural philosophy and Christianity was complex during this period; some early theologians, including Tatian and Eusebius, considered natural philosophy an outcropping of pagan Greek science and were suspicious of it. Although some later Christian philosophers, including Aquinas, came to see natural science as a means of interpreting scripture, this suspicion persisted until the 12th and 13th centuries. The Condemnation of 1277, which forbade setting philosophy on a level equal with theology and the debate of religious constructs in a scientific context, showed the persistence with which Catholic leaders resisted the development of natural philosophy even from a theological perspective. Aquinas and Albertus Magnus, another Catholic theologian of the era, sought to distance theology from science in their works. \"I don't see what one's interpretation of Aristotle has to do with the teaching of the faith,\" he wrote in 1271.\n\nBy the 16th and 17th centuries, natural philosophy underwent an evolution beyond commentary on Aristotle as more early Greek philosophy was uncovered and translated. The invention of the printing press in the 15th century, the invention of the microscope and telescope, and the Protestant Reformation fundamentally altered the social context in which scientific inquiry evolved in the West. Christopher Columbus's discovery of a new world changed perceptions about the physical makeup of the world, while observations by Copernicus, Tyco Brahe and Galileo brought a more accurate picture of the solar system as heliocentric and proved many of Aristotle's theories about the heavenly bodies false. A number of 17th-century philosophers, including Thomas Hobbes, John Locke and Francis Bacon made a break from the past by rejecting Aristotle and his medieval followers outright, calling their approach to natural philosophy as superficial.\n\nThe titles of Galileo's work \"Two New Sciences\" and Johannes Kepler's \"New Astronomy\" underscored the atmosphere of change that took hold in the 17th century as Aristotle was dismissed in favor of novel methods of inquiry into the natural world. Bacon was instrumental in popularizing this change; he argued that people should use the arts and sciences to gain dominion over nature. To achieve this, he wrote that \"human life [must] be endowed with new discoveries and powers.\" He defined natural philosophy as \"the knowledge of Causes and secret motions of things; and enlarging the bounds of Human Empire, to the effecting of all things possible.\" Bacon proposed scientific inquiry supported by the state and fed by the collaborative research of scientists, a vision that was unprecedented in its scope, ambition and form at the time. Natural philosophers came to view nature increasingly as a mechanism that could be taken apart and understood, much like a complex clock. Natural philosophers including Isaac Newton, Evangelista Torricelli and Francesco Redi conducted experiments focusing on the flow of water, measuring atmospheric pressure using a barometer and disproving spontaneous generation. Scientific societies and scientific journals emerged and were spread widely through the printing press, touching off the scientific revolution. Newton in 1687 published his \"The Mathematical Principles of Natural Philosophy\", or \"Principia Mathematica\", which set the groundwork for physical laws that remained current until the 19th century.\n\nSome modern scholars, including Andrew Cunningham, Perry Williams and Floris Cohen, argue that natural philosophy is not properly called a science, and that genuine scientific inquiry began only with the scientific revolution. According to Cohen, \"the emancipation of science from an overarching entity called 'natural philosophy' is one defining characteristic of the Scientific Revolution.\" Other historians of science, including Edward Grant, contend that the scientific revolution that blossomed in the 17th, 18th and 19th centuries occurred when principles learned in the exact sciences of optics, mechanics and astronomy began to be applied to questions raised by natural philosophy. Grant argues that Newton attempted to expose the mathematical basis of nature – the immutable rules it obeyed – and in doing so joined natural philosophy and mathematics for the first time, producing an early work of modern physics.\nThe scientific revolution, which began to take hold in the 17th century, represented a sharp break from Aristotelian modes of inquiry. One of its principal advances was the use of the scientific method to investigate nature. Data was collected and repeatable measurements made in experiments. Scientists then formed hypotheses to explain the results of these experiments. The hypothesis was then tested using the principle of falsifiability to prove or disprove its accuracy. The natural sciences continued to be called natural philosophy, but the adoption of the scientific method took science beyond the realm of philosophical conjecture and introduced a more structured way of examining nature.\n\nNewton, an English mathematician and physicist, was the seminal figure in the scientific revolution. Drawing on advances made in astronomy by Copernicus, Brahe and Kepler, Newton derived the universal law of gravitation and laws of motion. These laws applied both on earth and in outer space, uniting two spheres of the physical world previously thought to function independently of each other, according to separate physical rules. Newton, for example, showed that the tides were caused by the gravitational pull of the moon. Another of Newton's advances was to make mathematics a powerful explanatory tool for natural phenomena. While natural philosophers had long used mathematics as a means of measurement and analysis, its principles were not used as a means of understanding cause and effect in nature until Newton.\n\nIn the 18th century and 19th century, scientists including Charles-Augustin de Coulomb, Alessandro Volta, and Michael Faraday built upon Newtonian mechanics by exploring electromagnetism, or the interplay of forces with positive and negative charges on electrically charged particles. Faraday proposed that forces in nature operated in \"fields\" that filled space. The idea of fields contrasted with the Newtonian construct of gravitation as simply \"action at a distance\", or the attraction of objects with nothing in the space between them to intervene. James Clerk Maxwell in the 19th century unified these discoveries in a coherent theory of electrodynamics. Using mathematical equations and experimentation, Maxwell discovered that space was filled with charged particles that could act upon themselves and each other, and that they were a medium for the transmission of charged waves.\n\nSignificant advances in chemistry also took place during the scientific revolution. Antoine Lavoisier, a French chemist, refuted the phlogiston theory, which posited that things burned by releasing \"phlogiston\" into the air. Joseph Priestley had discovered oxygen in the 18th century, but Lavoisier discovered that combustion was the result of oxidation. He also constructed a table of 33 elements and invented modern chemical nomenclature. Formal biological science remained in its infancy in the 18th century, when the focus lay upon the classification and categorization of natural life. This growth in natural history was led by Carl Linnaeus, whose 1735 taxonomy of the natural world is still in use. Linnaeus in the 1750s introduced scientific names for all his species.\n\nBy the 19th century, the study of science had come into the purview of professionals and institutions. In so doing, it gradually acquired the more modern name of \"natural science.\" The term \"scientist\" was coined by William Whewell in an 1834 review of Mary Somerville's \"On the Connexion of the Sciences\". But the word did not enter general use until nearly the end of the same century.\n\nAccording to a famous 1923 textbook \"Thermodynamics and the Free Energy of Chemical Substances\" by the American chemist Gilbert N. Lewis and the American physical chemist Merle Randall, the natural sciences contain three great branches:\n\nAside from the logical and mathematical sciences, there are three great branches of \"natural science\" which stand apart by reason of the variety of far reaching deductions drawn from a small number of primary postulates — they are mechanics, electrodynamics, and thermodynamics.\n\nToday, natural sciences are more commonly divided into life sciences, such as botany and zoology; and physical sciences, which include physics, chemistry, geology, astronomy and materials science.\n\n\n\n"}
{"id": "2622218", "url": "https://en.wikipedia.org/wiki?curid=2622218", "title": "North American blizzard of 1996", "text": "North American blizzard of 1996\n\nThe Blizzard of 1996 was a severe nor'easter that paralyzed the U.S. East Coast with up to of wind-driven snow from January 6 to January 8, 1996. This storm was a classic example of a Nor'easter, but the storm would not have been as historically significant without the presence of the arctic high pressure system located to the north of New York. It was followed by another storm, an Alberta Clipper, on January 12, then unusually warm weather and torrential rain which caused rapid melting and river flooding. Along with the March Superstorm of 1993, it is one of only two snowstorms to receive the top rating of 5, or \"Extreme\", on the Northeast Snowfall Impact Scale (NESIS).\n\nMost of Virginia was impacted with the more central and western parts receiving one to three feet (300–900 mm) of snow. Roanoke got a record-breaking 23 inches. The heaviest snow fell in Page County, with around 37 inches (940 mm). Snowfall reached three feet in the Shenandoah Valley and exceeded two feet in much of the Virginia mountain and Piedmont areas. Gov. George Allen declared a state of emergency as power lines went down, people were trapped in their houses, and at least eight weather-related deaths occurred. High winds that accompanied the blizzard caused white out conditions and drifts of up to 10 feet (900 mm) in Patrick County.\n\nSnow began falling on Washington, D.C. and Baltimore during the early afternoon of January 6 and continued at a consistent rate until mid-afternoon the next day. At that time, the metro area received , and after a few hours of sleet and then a complete stop for several hours, it seemed the worst was over. But overnight, as the storm slowly crawled northward, extremely heavy bands of snow came in from the east. These bands created whiteout conditions as winds gusted past , along with thunder and lightning.\nBy the morning of January 8, the bands tapered off, and the metro area was left with a blanket of of snow. Baltimore received and Washington Dulles International Airport received . Many areas north and west in Maryland and West Virginia received well over with a few locations in the mountains of West Virginia and Virginia receiving up to .\n\nIn DC, it was known as the \"Blizzard of '96\" or the \"Great Furlough Storm,\" because it occurred just after the 1996 federal government shutdown and since the Federal government was closed due to the storm, lengthened the time federal employees were away from their jobs in the DC area. Because of unseasonably warm weather in mid-January and a warm rain exceeding an inch on Monday, January 15, the snow melted quickly and caused the worst winter flooding in decades for river and stream valleys from Southwest Virginia to New York.\n\n30.7 inches (78 cm) of snow fell in Philadelphia (as measured at the official city observation location, Philadelphia International Airport), the most of any major city in the storm's path. It remains the city's all-time greatest snowstorm, compared to its previous greatest snowstorm which was a \"mere\" 21.3 inches. Most of those 30.7 inches, 27.6, fell in just 24 hours, a new record for the city for the most snow in 24 hours. The mayor declared a state of emergency, and only police and other emergency workers were permitted to drive on city streets leaving the city to pedestrians.\n\nFor three days, city trucks loaded with plowed snow dumped their contents into the Schuylkill and Delaware rivers eventually causing major problems with the natural flow of the rivers. It is a rare occurrence for trucks to \"dispose of snow\" in the Philadelphia area, since the snowfall amounts typically do not warrant it. However the snow was so extensive, that plowing would cause massive snow piles. City officials had no choice but to resort to hauling the snow to nearby rivers. Disposal of snow became a major issue but temperatures quickly returned to normal and began to quickly clear the snow. This resulted in flooding, when on January 19, a jet stream disturbance from the Gulf of Mexico caused a rapid melt, followed by thunderstorms, which both brought three inches of rain, and caused the snow to melt 20 inches in one day; or equivalent to 2 inches of rain. Philadelphia saw its worst flooding in twenty years. Damages were estimated to reach $1 billion.\nSnowfall accumulations averaged 20 to 22 inches in Monroe, Carbon and eastern Schuylkill Counties, around 2 feet in Lehigh and Northampton Counties, 24 to 33 inches in Berks County, 20 to 26 inches in Chester and Delaware Counties, 20 to 30 inches in Montgomery and Bucks Counties. Other individual accumulations included 33 inches in Ontelaunee Township (Berks), 30 inches in Reading (Berks) and Palm and Souderton (Montgomery), 28 inches in Perkasie (Bucks), and 26 inches in Glenmoore (Chester).\n\n26.5 inches (78 cm) of snow fell in Pittsburgh (as measured at the official city observation location, Pittsburgh International Airport). In 30 hours, 19 inches fell severely. Mayor Tom Murphy had declared Pittsburgh a state of Emergency with only emergency officals allowed to drive in city streets and highways. Snowfall accumulations averaged 17 to 20 inches in Allegheny, Westmorland, and Beaver counties. Butler County had an amount of 27 inches.\n\nSnow began falling during the predawn hours of the 7th and became heavy at times during the morning. Blizzard conditions developed during the afternoon and evening as strong northeast winds developed around the intensifying low pressure. During the afternoon hours, precipitation in far southern New Jersey changed to sleet and freezing rain as the low brought in warm air at mid-levels, but remained all snow across the rest of the state. During the evening and overnight hours the snow mixed with sleet as far north as central sections of the state as the low center approached the state from the south. A lull developed in the precipitation in the pre-dawn hours of the 8th as the low center was just off the New Jersey coast, but wraparound moisture brought another period of snow to the state as the low pulled away during the later morning and early afternoon hours of the 8th. Accumulations averaged 24 to 27 inches in Sussex County, 20 to 27 inches in Warren County, 23 to 28 inches in Morris County, generally 20 to 30 inches in Bergen, Passaic, Union, Hudson, Essex, Hunterdon, Somerset, Mercer, and Monmouth counties, 19 to 32 inches in Middlesex County, 18 to 31 inches in Burlington County, 16 to 24 inches in Salem, Gloucester, and Camden counties, 24 inches inland and 10 to 14 inches at the coast in Ocean County, and 10 to 18 inches in Cumberland, Atlantic, and Cape May counties. In addition to the heavy snow, wind gusts reached hurricane force along the coast during the evening of the 7th, with an 81 mph gust recorded in Ocean Grove. While accumulations were lighter along the shore, the strong northeasterly flow produced moderate coastal flooding at the time of high tide on the evening of the 7th, with tides 3 to 4 feet above normal. The tide reached 7.5 feet above mean low water in Atlantic County and 8.5 feet above mean low water in Cape May County. Fortunately, winds switched to the northwest before worse flooding could occur at high tide on the morning of the 8th.\n\nThe state of New Jersey recorded its second-largest snowstorm at Edison, where 32 inches (81 cm) fell (the greatest single storm record being 34 inches (86 cm) at Cape May in the Great Blizzard of 1899). Elizabeth, New Jersey also reported 32 inches of snow. Newark, the state's largest city, received a record-setting , while Trenton, the capital, received . All roads in the state were closed, including the entire length of the New Jersey Turnpike for the first time in that road's history. Over two-thirds of the state was buried under of snow, making this storm the state's most paralyzing snowstorm of the 20th century. Places such as Roselle and Linden received around 30 inches. New Jersey was, along with other states, put into a state of emergency.\n\n\"Two feet of snow was blasted into Greater Trenton by 50 mph winds that created giant drifts, paralyzing the region and the rest of the Northeast for a week, One writer called it the most “sadistic” storm of our century. \nThe Blizzard of '96 stranded hundreds of people at Trenton's train station; left thousands without electricity and heat for days; closed most schools and government offices for a week; cut off an estimated $1 billion in Jersey commerce; forced the National Guard into service rescuing state troopers from snowbound police cruisers; and created strife between officials and residents demanding the plowing of their little streets. In the second week of January 1996, it can be said, many locales across the region looked like they did in the old pictures taken of them during the infamous Blizzard of '88: Houses drifted under; snowed-in main streets that looked like ghost towns; no cars or people on the streets.\" \n\nNew York City's Central Park officially recorded for its fourth-largest single snowfall (records going back to 1869), but many locations in the other boroughs and suburbs recorded over of snow. Schools in New York City's boroughs closed because of snow for the first time since the Blizzard of 1978, 18 years earlier. While most suburban districts in the area close for snow several times each winter, in the city itself they rarely do because of relatively easy access to underground subways whose ability to run is not appreciably affected by snowstorms of moderate to large accumulation; however, in this snowstorm, the transit network was significantly disrupted. Buses were unable to run, and subway service was limited. Lines that ran in open cut and surface routes were shut down for two days.\n\nProvidence, Rhode Island received of snow, while Boston and Hartford, Connecticut both received . Up to of snow fell in the Berkshire Mountains of western Massachusetts and the northern hills of Connecticut. While this was a major snow event for southern New England, the Blizzard of 1996 was not as intense as other recent events, notably the Blizzard of 1978 and the March 1993 Superstorm. Snowfall amounts decreased sharply as one moved further north, with northern Vermont, New Hampshire, and Maine receiving little more than snow flurries from this event.\n\nDespite the storm's common name as the \"Blizzard of 1996\", some of the few observing sites to record true blizzard conditions were Trenton-Mercer Airport near Trenton, New Jersey and Morristown Municipal Airport located east of Morristown, New Jersey, with Morristown recording wind gusts in excess of 63 mph (101 km/h). During the afternoon of January 7, the airports recorded the necessary three consecutive hours of frequent wind gusts of at least combined with a prevailing visibility consistently below along with falling and blowing snow, meeting the official NWS standard of a blizzard. This indicates that the sites experienced some of the worst conditions on the entire East Coast. All other New Jersey observing sites, as well as most sites in neighboring states, failed to observe true blizzard conditions, though many stations did observe blizzard conditions for less than the necessary three consecutive hours.\n\n"}
{"id": "900733", "url": "https://en.wikipedia.org/wiki?curid=900733", "title": "Plasma oscillation", "text": "Plasma oscillation\n\nPlasma oscillations, also known as Langmuir waves (after Irving Langmuir), are rapid oscillations of the electron density in conducting media such as plasmas or metals in the ultraviolet region. The oscillations can be described as an instability in the dielectric function of a free electron gas. The frequency only depends weakly on the wavelength of the oscillation. The quasiparticle resulting from the quantization of these oscillations is the plasmon.\n\nLangmuir waves were discovered by American physicists Irving Langmuir and Lewi Tonks in the 1920s. They are parallel in form to Jeans instability waves, which are caused by gravitational instabilities in a static medium.\n\nConsider an electrically neutral plasma in equilibrium, consisting of a gas of positively charged ions and negatively charged electrons. If one displaces by a tiny amount an electron or a group of electrons with respect to the ions, the Coulomb force pulls the electrons back, acting as a restoring force.\n\nIf the thermal motion of the electrons is ignored, it is possible to show that the charge density oscillates at the \"plasma frequency\" <br>\n\nwhere \"formula_3 \" is the number density of electrons, \"e\" is the electric charge, \"m\" is the effective mass of the electron, and formula_4 is the permittivity of free space. Note that the above formula is derived under the approximation that the ion mass is infinite. This is generally a good approximation, as the electrons are so much lighter than ions. (This expression must be modified in the case of electron-positron plasmas, often encountered in astrophysics). Since the frequency is independent of the wavelength, these oscillations have an infinite phase velocity and zero group velocity.\n\nNote that, when formula_5, the plasma frequency, formula_6, depends only on physical constants and electron density formula_3. The numeric expression for angular plasma frequency is\n\nMetals are only transparent to light with frequency higher than the metal's plasma frequency. For typical metals such as copper or silver, \"formula_3\" is approximately 10 cm, which brings the plasma frequency into the ultraviolet region. This is why most metals reflect visible light and appear shiny.\n\nWhen the effects of the electron thermal speed formula_10 are taken into account, the electron pressure acts as a restoring force as well as the electric field and the oscillations propagate with frequency and wavenumber related by the longitudinal Langmuir wave:\n\ncalled the Bohm-Gross dispersion relation. If the spatial scale is large compared to the Debye length, the oscillations are only weakly modified by the pressure term, but at small scales the pressure term dominates and the waves become dispersionless with a speed of formula_12. For such waves, however, the electron thermal speed is comparable to the phase velocity, i.e., \nso the plasma waves can accelerate electrons that are moving with speed nearly equal to the phase velocity of the wave. This process often leads to a form of collisionless damping, called Landau damping. Consequently, the large-\"k\" portion in the dispersion relation is difficult to observe and seldom of consequence.\n\nIn a bounded plasma, fringing electric fields can result in propagation of plasma oscillations, even when the electrons are cold.\n\nIn a metal or semiconductor, the effect of the ions' periodic potential must be taken into account. This is usually done by using the electrons' effective mass in place of \"m\".\n\n"}
{"id": "292941", "url": "https://en.wikipedia.org/wiki?curid=292941", "title": "Polyethylene terephthalate", "text": "Polyethylene terephthalate\n\nPolyethylene terephthalate (sometimes written poly(ethylene terephthalate)), commonly abbreviated PET, PETE, or the obsolete PETP or PET-P, is the most common thermoplastic polymer resin of the polyester family and is used in fibres for clothing, containers for liquids and foods, thermoforming for manufacturing, and in combination with glass fibre for engineering resins.\n\nIt may also be referred to by the brand names Terylene in the UK, Lavsan in Russia and the former Soviet Union, and Dacron in the US.\n\nThe majority of the world's PET production is for synthetic fibres (in excess of 60%), with bottle production accounting for about 30% of global demand. In the context of textile applications, PET is referred to by its common name, polyester, whereas the acronym \"PET\" is generally used in relation to packaging. Polyester makes up about 18% of world polymer production and is the fourth-most-produced polymer after polyethylene (PE), polypropylene (PP) and polyvinyl chloride (PVC).\n\nPET consists of polymerized units of the monomer ethylene terephthalate, with repeating (CHO) units. PET is commonly recycled, and has the number \"1\" as its resin identification code (RIC).\n\nDepending on its processing and thermal history, polyethylene terephthalate may exist both as an amorphous (transparent) and as a semi-crystalline polymer. The semicrystalline material might appear transparent (particle size less than 500 nm) or opaque and white (particle size up to a few micrometers) depending on its crystal structure and particle size.\n\nThe monomer bis(2-hydroxyethyl) terephthalate can be synthesized by the esterification reaction between terephthalic acid and ethylene glycol with water as a byproduct, or by transesterification reaction between ethylene glycol and dimethyl terephthalate (DMT) with methanol as a byproduct. Polymerization is through a polycondensation reaction of the monomers (done immediately after esterification/transesterification) with water as the byproduct.\n\nPlastic bottles made from PET are widely used for soft drinks (see carbonation). For certain specialty bottles, such as those designated for beer containment, PET sandwiches an additional polyvinyl alcohol (PVOH) layer to further reduce its oxygen permeability.\n\nBiaxially oriented PET film (often known by one of its trade names, \"Mylar\") can be aluminized by evaporating a thin film of metal onto it to reduce its permeability, and to make it reflective and opaque (MPET). These properties are useful in many applications, including flexible food packaging and thermal insulation (such as space blankets). Because of its high mechanical strength, PET film is often used in tape applications, such as the carrier for magnetic tape or backing for pressure-sensitive adhesive tapes.\n\nNon-oriented PET sheet can be thermoformed to make packaging trays and blister packs. If crystallizable PET is used, the trays can be used for frozen dinners, since they withstand both freezing and oven baking temperatures. Both amorphous PET and BoPET are transparent to the naked eye. Color-conferring dyes can easily be formulated into PET sheet.\n\nWhen filled with glass particles or fibres, it becomes significantly stiffer and more durable.\n\nPET is also used as a substrate in thin film solar cells.\n\nTerylene (a trademark formed by inversion of (polyeth)ylene ter(ephthalate)) is also spliced into bell rope tops to help prevent wear on the ropes as they pass through the ceiling.\n\nPET is used since late 2014 as liner material in type IV composite high pressure gas cylinders. PET works as a much better barrier to oxygen than earlier used (LD)PE.\n\nPET is used as a 3D printing filament, as well as in the 3D printing plastic PETG.\n\nPET was patented in 1941 by John Rex Whinfield, James Tennant Dickson and their employer the Calico Printers' Association of Manchester, England. E. I. DuPont de Nemours in Delaware, United States, first used the trademark Mylar in June 1951 and received registration of it in 1952. It is still the best-known name used for polyester film. The current owner of the trademark is DuPont Teijin Films US, a partnership with a Japanese company.\n\nIn the Soviet Union, PET was first manufactured in the laboratories of the Institute of High-Molecular Compounds of the USSR Academy of Sciences in 1949, and its name \"Lavsan\" is an acronym thereof (лаборатории Института высокомолекулярных соединений Академии наук СССР).\n\nThe PET bottle was patented in 1973 by Nathaniel Wyeth.\n\nPET in its natural state is a colorless, semi-crystalline resin. Based on how it is processed, PET can be semi-rigid to rigid, and it is very lightweight. It makes a good gas and fair moisture barrier, as well as a good barrier to alcohol (requires additional \"barrier\" treatment) and solvents. It is strong and impact-resistant. PET becomes white when exposed to chloroform and also certain other chemicals such as toluene.\n\nAbout 60% crystallization is the upper limit for commercial products, with the exception of polyester fibers. Clear products can be produced by rapidly cooling molten polymer below T glass transition temperature to form an amorphous solid. Like glass, amorphous PET forms when its molecules are not given enough time to arrange themselves in an orderly, crystalline fashion as the melt is cooled. At room temperature the molecules are frozen in place, but, if enough heat energy is put back into them by heating above T, they begin to move again, allowing crystals to nucleate and grow. This procedure is known as solid-state crystallization.\n\nWhen allowed to cool slowly, the molten polymer forms a more crystalline material. This material has spherulites containing many small crystallites when crystallized from an amorphous solid, rather than forming one large single crystal. Light tends to scatter as it crosses the boundaries between crystallites and the amorphous regions between them. This scattering means that crystalline PET is opaque and white in most cases. Fiber drawing is among the few industrial processes that produce a nearly single-crystal product.\n\nOne of the most important characteristics of PET is referred to as intrinsic viscosity (IV).\n\nThe intrinsic viscosity of the material, found by extrapolating to zero concentration of relative viscosity to concentration which is measured in deciliters per gram (dℓ/g). Intrinsic viscosity is dependent upon the length of its polymer chains but has no units due to being extrapolated to zero concentration. The longer the polymer chains the more entanglements between chains and therefore the higher the viscosity. The average chain length of a particular batch of resin can be controlled during polycondensation.\n\nThe intrinsic viscosity range of PET:\n\nFiber grade:\nFilm grade:\nBottle grade:\nMonofilament, engineering plastic\n\nPET is hygroscopic, meaning that it absorbs water from its surroundings. However, when this \"damp\" PET is then heated, the water hydrolyzes the PET, decreasing its resilience. Thus, before the resin can be processed in a molding machine, it must be dried. Drying is achieved through the use of a desiccant or dryers before the PET is fed into the processing equipment.\n\nInside the dryer, hot dry air is pumped into the bottom of the hopper containing the resin so that it flows up through the pellets, removing moisture on its way. The hot wet air leaves the top of the hopper and is first run through an after-cooler, because it is easier to remove moisture from cold air than hot air. The resulting cool wet air is then passed through a desiccant bed. Finally, the cool dry air leaving the desiccant bed is re-heated in a process heater and sent back through the same processes in a closed loop. Typically, residual moisture levels in the resin must be less than 50 parts per million (parts of water per million parts of resin, by weight) before processing. Dryer residence time should not be shorter than about four hours. This is because drying the material in less than 4 hours would require a temperature above 160 °C, at which level hydrolysis would begin inside the pellets before they could be dried out.\n\nPET can also be dried in compressed air resin dryers. Compressed air dryers do not reuse drying air. Dry, heated compressed air is circulated through the PET pellets as in the desiccant dryer, then released to the atmosphere.\n\nIn addition to pure (homopolymer) PET, PET modified by copolymerization is also available.\n\nIn some cases, the modified properties of copolymer are more desirable for a particular application. For example, cyclohexane dimethanol (CHDM) can be added to the polymer backbone in place of ethylene glycol. Since this building block is much larger (6 additional carbon atoms) than the ethylene glycol unit it replaces, it does not fit in with the neighboring chains the way an ethylene glycol unit would. This interferes with crystallization and lowers the polymer's melting temperature. In general, such PET is known as PETG or PET-G (Polyethylene terephthalate glycol-modified; Eastman Chemical, SK Chemicals Selenis are some PETG manufacturers). PETG is a clear amorphous thermoplastic that can be injection molded, sheet extruded or extruded as filament for 3D printing. It can be colored during processing.\n\nAnother common modifier is isophthalic acid, replacing some of the 1,4-(\"para-\") linked terephthalate units. The 1,2-(\"ortho-\") or 1,3-(\"meta\"-) linkage produces an angle in the chain, which also disturbs crystallinity.\n\nSuch copolymers are advantageous for certain molding applications, such as thermoforming, which is used for example to make tray or blister packaging from co-PET film, or amorphous PET sheet (A-PET/PETA) or PETG sheet. On the other hand, crystallization is important in other applications where mechanical and dimensional stability are important, such as seat belts. For PET bottles, the use of small amounts of isophthalic acid, CHDM, diethylene glycol (DEG) or other comonomers can be useful: if only small amounts of comonomers are used, crystallization is slowed but not prevented entirely. As a result, bottles are obtainable via stretch blow molding (\"SBM\"), which are both clear and crystalline enough to be an adequate barrier to aromas and even gases, such as carbon dioxide in carbonated beverages.\n\nPolyethylene terephthalate is produced from ethylene glycol and dimethyl terephthalate(DMT) (CH(COCH)) or terephthalic acid.\n\nThe former is a transesterification reaction, whereas the latter is an esterification reaction.\n\nIn dimethyl terephthalate(DMT) process, this compound and excess ethylene glycol are reacted in the melt at 150–200 °C with a basic catalyst. Methanol (CHOH) is removed by distillation to drive the reaction forward. Excess ethylene glycol is distilled off at higher temperature with the aid of vacuum. The second transesterification step proceeds at 270–280 °C, with continuous distillation of ethylene glycol as well.\n\nThe reactions are idealized as follows:\n\n\nIn the terephthalic acid process, esterification of ethylene glycol and terephthalic acid is conducted directly at moderate pressure (2.7–5.5 bar) and high temperature (220–260 °C). Water is eliminated in the reaction, and it is also continuously removed by distillation:\n\nPET is subjected to various types of degradations during processing. The main degradations that can occur are hydrolytic, and probably most important, thermal oxidation. When PET degrades, several things happen: discoloration, chain scissions resulting in reduced molecular weight, formation of acetaldehyde, and cross-links (\"gel\" or \"fish-eye\" formation). Discoloration is due to the formation of various chromophoric systems following prolonged thermal treatment at elevated temperatures. This becomes a problem when the optical requirements of the polymer are very high, such as in packaging applications. The thermal and thermooxidative degradation results in poor processibility characteristics and performance of the material.\n\nOne way to alleviate this is to use a copolymer. Comonomers such as CHDM or isophthalic acid lower the melting temperature and reduce the degree of crystallinity of PET (especially important when the material is used for bottle manufacturing). Thus, the resin can be plastically formed at lower temperatures and/or with lower force. This helps to prevent degradation, reducing the acetaldehyde content of the finished product to an acceptable (that is, unnoticeable) level. See copolymers, above. Another way to improve the stability of the polymer is to use stabilizers, mainly antioxidants such as phosphites. Recently, molecular level stabilization of the material using nanostructured chemicals has also been considered.\n\nAcetaldehyde is a colorless, volatile substance with a fruity smell. Although it forms naturally in some fruit, it can cause an off-taste in bottled water. Acetaldehyde forms by degradation of PET through the mishandling of the material. High temperatures (PET decomposes above 300 °C or 570 °F), high pressures, extruder speeds (excessive shear flow raises temperature), and long barrel residence times all contribute to the production of acetaldehyde. When acetaldehyde is produced, some of it remains dissolved in the walls of a container and then diffuses into the product stored inside, altering the taste and aroma. This is not such a problem for non-consumables (such as shampoo), for fruit juices (which already contain acetaldehyde), or for strong-tasting drinks like soft drinks. For bottled water, however, low acetaldehyde content is quite important, because, if nothing masks the aroma, even extremely low concentrations (10–20 parts per billion in the water) of acetaldehyde can produce an off-taste.\n\nAntimony (Sb) is a metalloid element that is used as a catalyst in the form of compounds such as antimony trioxide (SbO) or antimony triacetate in the production of PET. After manufacturing, a detectable amount of antimony can be found on the surface of the product. This residue can be removed with washing. Antimony also remains in the material itself and can, thus, migrate out into food and drinks. Exposing PET to boiling or microwaving can increase the levels of antimony significantly, possibly above USEPA maximum contamination levels.\nThe drinking water limit assessed by WHO is 20 parts per billion (WHO, 2003), and the drinking water limit in the United States is 6 parts per billion. Although antimony trioxide is of low toxicity when taken orally, its presence is still of concern. The Swiss Federal Office of Public Health investigated the amount of antimony migration, comparing waters bottled in PET and glass: The antimony concentrations of the water in PET bottles were higher, but still well below the allowed maximum concentration. The Swiss Federal Office of Public Health concluded that small amounts of antimony migrate from the PET into bottled water, but that the health risk of the resulting low concentrations is negligible (1% of the \"tolerable daily intake\" determined by the WHO). A later (2006) but more widely publicized study found similar amounts of antimony in water in PET bottles.\nThe WHO has published a risk assessment for antimony in drinking water.\n\nFruit juice concentrates (for which no guidelines are established), however, that were produced and bottled in PET in the UK were found to contain up to 44.7 µg/L of antimony, well above the EU limits for tap water of 5 µg/L.\n\nAt least one species of bacterium in the genus \"Nocardia\" can degrade PET with an esterase enzyme.\n\nJapanese scientists have isolated a bacterium \"Ideonella sakaiensis\" that possesses two enzymes which can break down the PET into smaller pieces that the bacterium can digest. A colony of \"I. sakaiensis\" can disintegrate a plastic film in about six weeks.\n\nCommentary published in \"Environmental Health Perspectives\" in April 2010 suggested that PET might yield endocrine disruptors under conditions of common use and recommended research on this topic. Proposed mechanisms include leaching of phthalates as well as leaching of antimony.\nAn article published in \"Journal of Environmental Monitoring\" in April 2012 concludes that antimony concentration in deionized water stored in PET bottles stays within EU's acceptable limit even if stored briefly at temperatures up to 60 °C (140 °F), while bottled contents (water or soft drinks) may occasionally exceed the EU limit after less than a year of storage at room temperature.\n\nThere are two basic molding methods for PET bottles, one-step and two-step. In two-step molding, two separate machines are used. The first machine injection molds the preform, which resembles a test tube, with the bottle-cap threads already molded into place. The body of the tube is significantly thicker, as it will be inflated into its final shape in the second step using stretch blow molding.\n\nIn the second step, the preforms are heated rapidly and then inflated against a two-part mold to form them into the final shape of the bottle. Preforms (uninflated bottles) are now also used as robust and unique containers themselves; besides novelty candy, some Red Cross chapters distribute them as part of the Vial of Life program to homeowners to store medical history for emergency responders.\n\nIn one-step machines, the entire process from raw material to finished container is conducted within one machine, making it especially suitable for molding non-standard shapes (custom molding), including jars, flat oval, flask shapes, etc. Its greatest merit is the reduction in space, product handling and energy, and far higher visual quality than can be achieved by the two-step system.\n\nIn 2016, it was estimated that 56 million tons of PET are produced each year. While most thermoplastics can, in principle, be recycled, PET bottle recycling is more practical than many other plastic applications because of the high value of the resin and the almost exclusive use of PET for widely used water and carbonated soft drink bottling. PET has a resin identification code of 1. The prime uses for recycled PET are polyester fiber, strapping, and non-food containers.\n\nBecause of the recyclability of PET and the relative abundance of post-consumer waste in the form of bottles, PET is rapidly gaining market share as a carpet fiber. Mohawk Industries released everSTRAND in 1999, a 100% post-consumer recycled content PET fiber. Since that time, more than 17 billion bottles have been recycled into carpet fiber. Pharr Yarns, a supplier to numerous carpet manufacturers including Looptex, Dobbs Mills, and Berkshire Flooring, produces a BCF (bulk continuous filament) PET carpet fiber containing a minimum of 25% post-consumer recycled content.\n\nPET, like many plastics, is also an excellent candidate for thermal disposal (incineration), as it is composed of carbon, hydrogen, and oxygen, with only trace amounts of catalyst elements (but no sulfur). PET has the energy content of soft coal.\n\nWhen recycling polyethylene terephthalate or PET or polyester, in general three ways have to be differentiated:\n\nChemical recycling of PET will become cost-efficient only applying high capacity recycling lines of more than 50,000 tons/year. Such lines could only be seen, if at all, within the production sites of very large polyester producers. Several attempts of industrial magnitude to establish such chemical recycling plants have been made in the past but without resounding success. Even the promising chemical recycling in Japan has not become an industrial breakthrough so far. The two reasons for this are: at first, the difficulty of consistent and continuous waste bottles sourcing in such a huge amount at one single site, and, at second, the steadily increased prices and price volatility of collected bottles. The prices of baled bottles increased for instance between the years 2000 and 2008 from about 50 Euro/ton to over 500 Euro/ton in 2008.\n\nMechanical recycling or direct circulation of PET in the polymeric state is operated in most diverse variants today. These kinds of processes are typical of small and medium-size industry. Cost-efficiency can already be achieved with plant capacities within a range of 5000–20,000 tons/year. In this case, nearly all kinds of recycled-material feedback into the material circulation are possible today. These diverse recycling processes are being discussed hereafter in detail.\n\nBesides chemical contaminants and degradation products generated during first processing and usage, mechanical impurities are representing the main part of quality depreciating impurities in the recycling stream. Recycled materials are increasingly introduced into manufacturing processes, which were originally designed for new materials only. Therefore, efficient sorting, separation and cleaning processes become most important for high quality recycled polyester.\n\nWhen talking about polyester recycling industry, we are concentrating mainly on recycling of PET bottles, which are meanwhile used for all kinds of liquid packaging like water, carbonated soft drinks, juices, beer, sauces, detergents, household chemicals and so on. Bottles are easy to distinguish because of shape and consistency and separate from waste plastic streams either by automatic or by hand-sorting processes. The established polyester recycling industry consists of three major sections:\n\n\nIntermediate product from the first section is baled bottle waste with a PET content greater than 90%. Most common trading form is the bale but also bricked or even loose, pre-cut bottles are common in the market. In the second section, the collected bottles are converted to clean PET bottle flakes. This step can be more or less complex and complicated depending on required final flake quality. During the third step, PET bottle flakes are processed to any kind of products like film, bottles, fiber, filament, strapping or intermediates like pellets for further processing and engineering plastics.\n\nBesides this external (post-consumer) polyester bottle recycling, numbers of internal (pre-consumer) recycling processes exist, where the wasted polymer material does not exit the production site to the free market, and instead is reused in the same production circuit. In this way, fiber waste is directly reused to produce fiber, preform waste is directly reused to produce preforms, and film waste is directly reused to produce film.\n\nThe success of any recycling concept is hidden in the efficiency of purification and decontamination at the right place during processing and to the necessary or desired extent.\n\nIn general, the following applies: The earlier in the process foreign substances are removed, and the more thoroughly this is done, the more efficient the process is.\n\nThe high plasticization temperature of PET in the range of is the reason why almost all common organic impurities such as PVC, PLA, polyolefin, chemical wood-pulp and paper fibers, polyvinyl acetate, melt adhesive, coloring agents, sugar, and protein residues are transformed into colored degradation products that, in their turn, might release in addition reactive degradation products. Then, the number of defects in the polymer chain increases considerably. The particle size distribution of impurities is very wide, the big particles of 60–1000 µm—which are visible by naked eye and easy to filter—representing the lesser evil, since their total surface is relatively small and the degradation speed is therefore lower. The influence of the microscopic particles, which—because they are many—increase the frequency of defects in the polymer, is relatively greater.\n\nThe motto \"What the eye does not see the heart cannot grieve over\" is considered to be very important in many recycling processes. Therefore, besides efficient sorting, the removal of visible impurity particles by melt filtration processes plays a particular part in this case.\n\nIn general, one can say that the processes to make PET bottle flakes from collected bottles are as versatile as the different waste streams are different in their composition and quality. In view of technology there isn't just one way to do it. Meanwhile, there are many engineering companies that are offering flake production plants and components, and it is difficult to decide for one or other plant design. Nevertheless, there are processes that are sharing most of these principles. Depending on composition and impurity level of input material, the general following process steps are applied.\n\nThe number of possible impurities and material defects that accumulate in the polymeric material is increasing permanently—when processing as well as when using polymers—taking into account a growing service lifetime, growing final applications and repeated recycling. As far as recycled PET bottles are concerned, the defects mentioned can be sorted in the following groups:\n\n\nTaking into consideration the above-mentioned chemical defects and impurities, there is an ongoing modification of the following polymer characteristics during each recycling cycle, which are detectable by chemical and physical laboratory analysis.\n\nIn particular:\n\n\nThe recycling of PET bottles is meanwhile an industrial standard process that is offered by a wide variety of engineering companies.\n\nRecycling processes with polyester are almost as varied as the manufacturing processes based on primary pellets or melt. Depending on purity of the recycled materials, polyester can be used today in most of the polyester manufacturing processes as blend with virgin polymer or increasingly as 100% recycled polymer. Some exceptions like BOPET-film of low thickness, special applications like optical film or yarns through FDY-spinning at > 6000 m/min, microfilaments, and micro-fibers are produced from virgin polyester only.\n\nThis process consists of transforming bottle waste into flakes, by drying and crystallizing the flakes, by plasticizing and filtering, as well as by pelletizing.\nProduct is an amorphous re-granulate of an intrinsic viscosity in the range of 0.55–0.7 dℓ/g, depending on how complete pre-drying of PET flakes has been done.\n\nSpecial feature are: Acetaldehyde and oligomers are contained in the pellets at lower level; the viscosity is reduced somehow, the pellets are amorphous and have to be crystallized and dried before further processing.\n\nProcessing to:\n\nChoosing the re-pelletizing way means having an additional conversion process that is, at the one side, energy-intensive and cost-consuming, and causes thermal destruction. At the other side, the pelletizing step is providing the following advantages:\n\n\nThis process is, in principle, similar to the one described above; however, the pellets produced are directly (continuously or discontinuously) crystallized and then subjected to a solid-state polycondensation (SSP) in a tumbling drier or a vertical tube reactor. During this processing step, the corresponding intrinsic viscosity of 0.80–0.085 dℓ/g is rebuild again and, at the same time, the acetaldehyde content is reduced to < 1 ppm.\n\nThe fact that some machine manufacturers and line builders in Europe and the United States make efforts to offer independent recycling processes, e.g. the so-called bottle-to-bottle (B-2-B) process, such as BePET, Starlinger, URRC or BÜHLER, aims at generally furnishing proof of the \"existence\" of the required extraction residues and of the removal of model contaminants according to FDA applying the so-called challenge test, which is necessary for the application of the treated polyester in the food sector. Besides this process approval it is nevertheless necessary that any user of such processes has to constantly check the FDA limits for the raw materials manufactured by themselves for their process.\n\nIn order to save costs, an increasing number of polyester intermediate producers like spinning mills, strapping mills, or cast film mills are working on the direct use of the PET-flakes, from the treatment of used bottles, with a view to manufacturing an increasing number of polyester intermediates. For the adjustment of the necessary viscosity, besides an efficient drying of the flakes, it is possibly necessary to also reconstitute the viscosity through polycondensation in the melt phase or solid-state polycondensation of the flakes. The latest PET flake conversion processes are applying twin screw extruders, multi-screw extruders or multi-rotation systems and coincidental vacuum degassing to remove moisture and avoid flake pre-drying. These processes allow the conversion of undried PET flakes without substantial viscosity decrease caused by hydrolysis.\n\nWith regard to the consumption of PET bottle flakes, the main portion of about 70% is converted to fibers and filaments. When using directly secondary materials such as bottle flakes in spinning processes, there are a few processing principles to obtain.\n\nHigh-speed spinning processes for the manufacture of POY normally need a viscosity of 0.62–0.64 dℓ/g. Starting from bottle flakes, the viscosity can be set via the degree of drying. The additional use of TiO is necessary for full dull or semi dull yarn. In order to protect the spinnerets, an efficient filtration of the melt is, in any case is necessary. For the time-being, the amount of POY made of 100% recycling polyester is rather low because this process requires high purity of spinning melt. Most of the time, a blend of virgin and recycled pellets is used.\n\nStaple fibers are spun in an intrinsic viscosity range that lies rather somewhat lower and that should be between 0.58 and 0.62 dℓ/g. In this case, too, the required viscosity can be adjusted via drying or vacuum adjustment in case of vacuum extrusion. For adjusting the viscosity, however, an addition of chain length modifier like ethylene glycol or diethylene glycol can also be used.\n\nSpinning non-woven—in the fine titer field for textile applications as well as heavy spinning non-woven as basic materials, e.g. for roof covers or in road building—can be manufactured by spinning bottle flakes. The spinning viscosity is again within a range of 0.58–0.65 dℓ/g.\n\nOne field of increasing interest where recycled materials are used is the manufacture of high-tenacity packaging stripes, and monofilaments. In both cases, the initial raw material is a mainly recycled material of higher intrinsic viscosity. High-tenacity packaging stripes as well as monofilament are then manufactured in the melt spinning process.\n\nPolyethylene terephthalate can be depolymerized to yield the constituent monomers. After purification, the monomers can be used to prepare new polyethylene terephthalate. The ester bonds in polyethylene terephthalate may be cleaved by hydrolysis, or by transesterification. The reactions are simply the reverse of those used in production.\n\nPartial glycolysis (transesterification with ethylene glycol) converts the rigid polymer into short-chained oligomers that can be melt-filtered at low temperature. Once freed of the impurities, the oligomers can be fed back into the production process for polymerization.\n\nThe task consists in feeding 10–25% bottle flakes while maintaining the quality of the bottle pellets that are manufactured on the line. This aim is solved by degrading the PET bottle flakes—already during their first plasticization, which can be carried out in a single- or multi-screw extruder—to an intrinsic viscosity of about 0.30 dℓ/g by adding small quantities of ethylene glycol and by subjecting the low-viscosity melt stream to an efficient filtration directly after plasticization. Furthermore, temperature is brought to the lowest possible limit. In addition, with this way of processing, the possibility of a chemical decomposition of the hydro peroxides is possible by adding a corresponding P-stabilizer directly when plasticizing.\nThe destruction of the hydro peroxide groups is, with other processes, already carried out during the last step of flake treatment for instance by adding HPO. The partially glycolyzed and finely filtered recycled material is continuously fed to the esterification or prepolycondensation reactor, the dosing quantities of the raw materials are being adjusted accordingly.\n\nThe treatment of polyester waste through total glycolysis to fully convert the polyester to bis(2-hydroxyethyl) terephthalate (CH(COCHCHOH)). This compound is purified by vacuum distillation, and is one of the intermediates used in polyester manufacture (see production). The reaction involved is as follows:\n\nThis recycling route has been executed on an industrial scale in Japan as experimental production.\n\nSimilar to total glycolysis, methanolysis converts the polyester to dimethyl terephthalate(DMT), which can be filtered and vacuum distilled:\n\nMethanolysis is only rarely carried out in industry today because polyester production based on dimethyl terephthalate(DMT) has shrunk tremendously, and many dimethyl terephthalate (DMT) producers have disappeared.\n\nAlso as above, polyethylene terephthalate can be hydrolyzed to terephthalic acid and ethylene glycol under high temperature and pressure. The resultant crude terephthalic acid can be purified by recrystallization to yield material suitable for re-polymerization:\n\nThis method does not appear to have been commercialized yet.\n\n\n"}
{"id": "7457668", "url": "https://en.wikipedia.org/wiki?curid=7457668", "title": "Randy Stoltmann", "text": "Randy Stoltmann\n\nRandy Stoltmann (September 28, 1962-May 22, 1994) was an outdoorsman, and a campaigner for the preservation of wilderness areas in British Columbia, Canada.\n\nStoltmann was the author of three books: \"Hiking Guide to the Big Trees of British Columbia\", \"Written by the Wind\", and \"Hiking the Ancient Forests of British Columbia and Washington.\"\n\nStoltmann became an expert on big trees in British Columbia at a young age. He played a key role in the campaign to prevent the logging of the Carmanah Valley on Vancouver Island. This led to the establishment of the Carmanah Walbran Provincial Park in 1990.\n\nIn May 1994 Stoltmann was killed when he fell into a crevasse while mountaineering in the Kitlope area. \n\nThe provincial government honoured Stoltmann after his death by naming the Randy Stoltmann Commemorative Grove in Carmanah Walbran Provincial Park after him. In addition, activists working to preserve the Elaho Valley north of Squamish designated this area as the Stoltmann Wilderness.\n"}
{"id": "39484776", "url": "https://en.wikipedia.org/wiki?curid=39484776", "title": "Rotational sampling in wind turbines", "text": "Rotational sampling in wind turbines\n\nThe loads on both horizontal axis wind turbines (HAWTs) and vertical axis wind turbines (VAWTs) are cyclic; that is, the thrust and torque acting on the blades is dependent on where the blade is. In a horizontal axis wind turbine, both the apparent wind speed seen by the blade and the angle of attack depend on the position of the blade. This phenomenon is described as rotational sampling. This article will provide an insight into the cyclic nature of the loads that arise because of rotational sampling for a horizontal axis wind turbine.\n\nRotational sampling can be divided into two parts: deterministic and stochastic. Deterministic processes present themselves as spikes on a power spectrum, whereas stochastic processes are broader i.e. spread over a wider frequency range.\n\nAnalysis of the loads on a wind turbine can be carried out through use of power spectra. A power spectrum is defined as the power spectral density function of a signal plotted against frequency. The power spectral density function of a plot is defined as the Fourier transform of the covariance function. Regarding analysis of loads, the analysis involves time series, in which case the covariance function becomes the autocovariance function. In the signal processing sense, the autocovariance can be related to the autocorrelation function.\n\nUpon completing a single revolution, a blade has produced an ever-changing torque, and so power. Some of these changes are due to deterministic processes i.e. processes that can be determined and do not require statistical methods. Examples of deterministic processes are listed below:\n\n\nAs a blade sweeps through each cycle, gravity is acting on the blade. Depending on the part of the cycle, gravity might be acting to accelerate the blade, or decelerate it. The additional torque that arises on a blade due to gravity is given by\n\nformula_1\n\nwhere \"r\" is the length of the blade, \"m\" is the mass of the blade, \"g\" is the gravitational field strength, \"t\" is the time, and formula_2 is the angular velocity of the blade.\n\nIn fluid dynamics, the flow of a fluid is dependent upon boundary conditions. Boundary conditions are influenced by the presence of solid bodies. In a wind turbine, the presence of the tower results in a reduction of the wind speed directly in front of it; that is, the blades experience a reduced wind speed when they pass in front of the tower.\n\nIn fluid dynamics, there exists the no slip boundary condition. This states that the velocity of a fluid at the surface of a solid body, such as the Earth, is zero. A consequence of that is that the wind speed varies with height above ground. This effect is known as wind shear. As a result, a blade at the highest part of its cycle will experience a greater wind speed than that of one at the lowest part of its cycle.\n\nThe drive train of a wind turbine comprises the hub, the low speed shaft, the gearbox, the high speed shaft, and the generator. The torque at the hub is strongly influenced by the rotor dynamics. The instantaneous hub torque is found by summing all the torques from all the blades of the wind turbine at any instant in time.\n\nConsider an formula_3 bladed wind turbine. Each blade is separated angularly from a neighbouring blade by formula_4 degrees. That is, for a 3-bladed wind turbine, the blades are 120 degrees apart.\n\nThe torque acting on the blade is defined as the z-component of formula_5, where r is the radius from the axis of rotation (in this case the hub), and F is the force acting on the blade. If the torque is defined as the z-component of this cross product, then the torque is simply \"rF\" where \"F\" is the force perpendicular to the radius vector, or tangential to the instantaneous velocity of the blade (See figure below)\n\nFrom the figure above, it can be seen that the torque, \"T\", due to gravitational forces acting on a single blade is given by the following expression:\n\nwhere \"m\" is the mass of the blade, \"g\" is the gravitational field strength, \"k\" is a multiplicative integer, formula_2 is the angular velocity of the blade, and \"t\" is the time.\n\nFor an n-bladed rotor, the instantaneous torque at the hub from all \"n\" blades by gravity is determined by summing the effects of all the blades at any one instant. Remembering that the blades are offset from each other by \"360/n\", the instantaneous torque at the hub from gravity is given by the following expression:\n\nformula_7\n\nSimple trigonometry reveals that only non-zero terms arise when \"k\" is a multiple of \"n\". Thus, the overall effect of gravity on the torque at the hub is\n\nformula_8\n\nThe covariance function of a sum of sinusoids is itself a sum of sinusoidal functions. Thus, the power spectral density function is a set of Dirac delta functions. The locations of these are at multiples of \"n\". Thus, on a power spectrum, deterministic processes such as gravitational loading manifest themselves as spikes. This can be seen from analysing generator torque.\n\nFor analysis of torque on a single blade, the spikes occur at formula_9 where \"k\"' is 1,2,3.. This can be seen from taking the autocovariance of equation 1, and then taking the Fourier transform of this result.\n\nSpectral analysis of component loading is useful in fatigue analysis.\n"}
{"id": "6422097", "url": "https://en.wikipedia.org/wiki?curid=6422097", "title": "S4G reactor", "text": "S4G reactor\n\nThe S4G reactor is a naval reactor used by the United States Navy to provide electricity generation and propulsion on warships. The S4G designation stands for:\n\n\nThis nuclear reactor is the shipboard equivalent of the S3G reactor. It was installed in a dual-configuration on the USS \"Triton\" (SSRN-586) with two reactors and two five-blade propellers, which was the first ship to make a submerged circumnavigation of the world.\n"}
{"id": "52635570", "url": "https://en.wikipedia.org/wiki?curid=52635570", "title": "Silicon Module Super League", "text": "Silicon Module Super League\n\nThe Silicon Module Super League (SMSL) is a group of big-six crystalline silicon (c-Si) module suppliers in the modern solar PV industry. The 'big six' industry group members were Canadian Solar, Hanwha Q CELLS, JA Solar, Jinko Solar, and Trina Solar. LONGi the world's largest solar monocrystalline silicon manufacturer and GCL, the world's largest solar poly crystalline silicon manufacturer, both joined the SMSL in mid-2016.\n\nThe achievement scale of this solar module industry breakaway group is notable. These seven module suppliers have been forecast to ship about half of the world's end-market supply during 2017.\n\nChinese solar engineers at Trina Solar hold the record for a 22.61% PV efficiency in mono-PERC-cells with Passivated Emitter and Rear Cell (PERC) technology developed at its State Key Laboratory of PV Science and Technology of China That one-third efficiency improvement was achieved in four years of PV research in China.\n\nSMSL group members have held the PV efficiency record for some time. Trina Solar in 2015 had set the previous record of 22.13%. Again, in July 2016, Trina Solar claimed that its production lines were able to produce the same kind of PERC cells in large volume with an average efficiency of 21.12%. This new, later higher cell efficiency which surpasses Trina Solar's record, has been independently confirmed by the Fraunhofer ISE CalLab in Germany. Trina Solar achieves 24.13% conversion efficiency for IBC solar cell.\n\n"}
{"id": "15798382", "url": "https://en.wikipedia.org/wiki?curid=15798382", "title": "Sodium aluminium hydride", "text": "Sodium aluminium hydride\n\nSodium aluminium hydride or sodium alanate is an inorganic compound with the chemical formula NaAlH. It is a white pyrophoric solid that dissolves in tetrahydrofuran (THF), but not in diethyl ether or hydrocarbons. It has been evaluated as an agent for the reversible storage of hydrogen and it is used as a reagent for the chemical synthesis of organic compounds. Similar to lithium aluminium hydride, it is a salt consisting of separated sodium cations and tetrahedral AlH anions.\n\nSodium tetrahydroaluminate adopts the structure of (is isostructural with) calcium tungstate. As such, the tetrahedral AlH centers are linked with eight-coordinat Na+ cations.\nThe compound is prepared from the elements under high pressures of H at 200 °C using triethylaluminium catalyst:\n\nAs a suspension in diethyl ether, it reacts with lithium chloride to give the popular reagent lithium aluminium hydride:\n\nThe compound reacts rapidly, even violently, with protic reagents, such as water, as described in this idealized equation:\n\nSodium alanate has been explored for hydrogen storage in hydrogen tanks. The relevant reactions are:\n\nSodium tetrahydroaluminate can release up to 7.4 wt % of hydrogen when heated at . Absorption can be slow, with several minutes being required to fill a tank. Both release and uptake are catalysed by titanium.\n\nSodium aluminium hydride is a strong reducing agent, very similar in reactivity to lithium aluminum hydride (LAH) and, to some extent, Diisobutylaluminium hydride (DIBAL) in organic reactions. It is much more powerful reducing agent than sodium borohydride due to the weaker and more polar Al-H bond compared to the B-H bond. Like LAH, it reduces esters to alcohols.\n\nSodium aluminium hydride is highly flammable. It does not react in dry air at room temperature but is very sensitive to moisture. It ignites or explodes on contact with water.\n\n"}
{"id": "4980632", "url": "https://en.wikipedia.org/wiki?curid=4980632", "title": "Solar humidification", "text": "Solar humidification\n\nThe solar humidification–dehumidification method (HDH) is a thermal water desalination method. It is based on evaporation of sea water or brackish water and subsequent condensation of the generated humid air, mostly at ambient pressure. This process mimics the natural water cycle, but over a much shorter time frame.\n\nThe simplest configuration is implemented in the solar still, evaporating the sea water inside a glass covered box and condensing the water vapor on the lower side of the glass cover. More sophisticated designs separate the solar heat gain section from the evaporation-condensation chamber. An optimized design comprises separated evaporation and condensation sections. A significant part of the heat consumed for evaporation can be regained during condensation. An example for such an optimized thermal desalination cycle is the multiple-effect humidification (MEH) method of desalination.\n\nSolar humidification takes place in every greenhouse. Water evaporates from the surfaces of soil, water and plants because of thermal input. In this way the humidification process is naturally integrated within the architecture of the greenhouse. Several companies like Seawater greenhouse utilize this inherent feature of a greenhouse in order to conduct desalination inside the atmosphere of the facility.\nThe integrated biotectural system, or IBTS Greenhouse mimics the natural process of cloud formation respectively desalination exactly.\n\nThere are successful small-scale agricultural experimentation done in arid regions such as Israel, West Africa, and Peru. The major difficulty lies in effectively concentrating the energy of sun on a small area to speed up evaporation. \n\n"}
{"id": "56122", "url": "https://en.wikipedia.org/wiki?curid=56122", "title": "Sorghum", "text": "Sorghum\n\nSorghum is a genus of flowering plants in the grass family Poaceae. Seventeen of the 25 species are native to Australia, with the range of some extending to Africa, Asia, Mesoamerica, and certain islands in the Indian and Pacific Oceans. One species is grown for grain, while many others are used as fodder plants, either cultivated in warm climates worldwide or naturalized, in pasture lands. \"Sorghum\" is in the subfamily Panicoideae and the tribe Andropogoneae (the tribe of big bluestem and sugarcane).\n\nOne species, \"Sorghum bicolor\", native to Africa with many cultivated forms now, is an important crop worldwide, used for food (as grain and in sorghum syrup or \"sorghum molasses\"), animal fodder, the production of alcoholic beverages, and biofuels. Most varieties are drought- and heat-tolerant, and are especially important in arid regions, where the grain is one of the staples for poor and rural people. These varieties form important components of forage in many tropical regions. \"S. bicolor\" is an important food crop in Africa, Central America, and South Asia, and is the fifth-most important cereal crop grown in the world.\n\nIn the early stages of the plants' growth, some species of sorghum can contain levels of hydrogen cyanide, hordenine, and nitrates which are lethal to grazing animals. When stressed by drought or heat, plants can also contain toxic levels of cyanide and nitrates at later stages in growth.\nAnother \"Sorghum\" species, Johnson grass (\"S. halapense\"), is classified as an invasive species in the US by the Department of Agriculture.\n\nMany species once considered part of \"Sorghum\", but now considered better suited to other genera include: \"Andropogon, Arthraxon, Bothriochloa, Chrysopogon, Cymbopogon, Danthoniopsis, Dichanthium, Diectomis, Diheteropogon, Exotheca, Hyparrhenia, Hyperthelia, Monocymbium, Parahyparrhenia, Pentameris, Pseudosorghum, Schizachyrium, \"and\" Sorghastrum\".\n\n\n"}
{"id": "7408567", "url": "https://en.wikipedia.org/wiki?curid=7408567", "title": "Starfish Hill Wind Farm", "text": "Starfish Hill Wind Farm\n\nStarfish Hill Wind Farm is a wind power station spread over two hills near Cape Jervis, South Australia. It has 23 wind turbines, eight on Starfish Hill itself and 15 on Salt Creek Hill, with a combined generating capacity of 34.5 MW of electricity.\n\nStarfish Hill Wind Farm was commissioned in September 2003, making it the first major wind farm in the state.\n\nStarfish Hill Wind Farm was developed by Starfish Hill Wind Farm Pty Ltd, a wholly owned subsidiary of Tarong Energy at a cost of A$65 million. RATCH Australia (at that time Transfield Services) acquired the wind farm in December 2007.\n\n"}
{"id": "20485476", "url": "https://en.wikipedia.org/wiki?curid=20485476", "title": "The Shetland Experience", "text": "The Shetland Experience\n\nThe Shetland Experience is a 1977 British short documentary film directed by Derek Williams. It is about environmental measures taken by the oil industry at the Sullom Voe Terminal in the Shetlands. It was a sponsored film, produced for the environmental advisory group of the Sullom Voe Association, to which the Shetland Islands Council and oil companies belonged.\n\nIt was nominated for an Academy Award for Best Documentary Short.\n"}
{"id": "40167849", "url": "https://en.wikipedia.org/wiki?curid=40167849", "title": "Tianping Reservoir", "text": "Tianping Reservoir\n\nThe Tianping Reservoir () is a large reservoir located in the western part of Ningxiang City, Hunan, China. It is the largest body of water in Ningxiang and the second largest reservoir in Ningxiang. The reservoir is the source of the Wei River ().\n\nCreated by damming some small rivers, the Tianping Reservoir has an area of and a capacity of .\n\nIn the 1970s Yang Shifang (), head of the People's Government of Ningxiang, planned to build a reservoir for irrigation, flood control, electricity generation and fish farming. Due to poverty, the government mobilized the masses and used a large amount of human resources to complete the build rather than use heavy construction equipment.\n"}
{"id": "3668182", "url": "https://en.wikipedia.org/wiki?curid=3668182", "title": "Uranium carbide", "text": "Uranium carbide\n\nUranium carbide, a carbide of uranium, is a hard refractory ceramic material. It comes in several stoichiometries (), such as uranium methanide (UC, CAS number 12070-09-6), uranium sesquicarbide (UC, CAS number 12076-62-9),\nand uranium acetylide (UC, CAS number 12071-33-9).\n\nLike uranium dioxide and some other uranium compounds, uranium carbide can be used as a nuclear fuel for nuclear reactors, usually in the form of pellets or tablets. Uranium carbide fuel was used in late designs of nuclear thermal rockets.\n\nUranium carbide pellets are used as fuel kernels for the US version of pebble bed reactors; the German version uses uranium dioxide instead.\n\nAs nuclear fuel, uranium carbide can be used either on its own, or mixed with plutonium carbide (PuC and PuC). The mixture is also labeled as uranium-plutonium carbide (PuC U).\n\nUranium carbide is also a popular target material for particle accelerators.\n\nAmmonia synthesis from nitrogen and hydrogen is sometimes accomplished in the presence of uranium carbide acting as a catalyst.\n\n"}
{"id": "1801099", "url": "https://en.wikipedia.org/wiki?curid=1801099", "title": "Vogtle Electric Generating Plant", "text": "Vogtle Electric Generating Plant\n\nThe Alvin W. Vogtle Electric Generating Plant, also known as Plant Vogtle (), is a 2 unit nuclear power plant located in Burke County, near Waynesboro, Georgia, in the southeastern United States. It is named after a former Alabama Power and Southern Company board chairman, Alvin Vogtle.\n\nEach unit has a Westinghouse pressurized water reactor (PWR), with a General Electric steam turbine and electric generator. \nUnits 1 and 2 were completed in 1987 and 1989, respectively. \nEach unit has a gross electricity generation capacity of 1,215 MW, for a combined capacity of 2,430 MW. \nThe twin natural-draft cooling towers are tall and provide cooling to the plant's main condensers. \nFour smaller mechanical draft cooling towers provide nuclear service cooling water (NSCW) to safety and auxiliary non-safety components, as well as remove the decay heat from the reactor when the plant is offline. \nOne natural-draft tower and two NSCW towers serve each unit. \nIn 2009, the Nuclear Regulatory Commission (NRC) renewed the licenses for both units for an additional 20 years to 1/16/2047 for Unit 1, and 2/9/2049 for Unit 2.\n\nTwo additional units utilizing Westinghouse AP1000 reactors are under construction. \nNatural-draft type cooling towers were also selected, and the two new cooling towers are nearly tall. \nThe units have suffered several delays and cost overruns. \nThe certified construction & capital costs incurred by Georgia Power for these two new units were originally $4.418 billion which escalated to an estimated $8.77 billion ($12.17 billion including financing costs) according to the Seventeenth Semi-annual Vogtle Construction Monitoring Report in 2017. \nThis last report blames the latest increase of costs on the contractor not completing work as scheduled. \nAnother complicating factor in the construction process is the bankruptcy of Westinghouse in 2017.\nIn 2018 costs were estimated to be about $25 billion.\nUpon completion of Units 3 and 4, Vogtle will become the largest nuclear power station in the United States and the largest power plant of any fuel type nationwide by annual net generation.\n\nThe Vogtle Electric Generating Plant has two operational reactors with two additional units under construction.\nIn 2008, both reactors were increased in power by 1.7% by an \"Appendix K\" uprate, also called a Measurement Uncertainty Recapture (MUR) uprate. Measurement uncertainty recapture power uprates are less than 2 percent, and are achieved by implementing enhanced techniques for calculating reactor power. This involves the use of state-of-the-art feedwater flow measurement devices to more precisely measure feedwater flow, which is used to calculate reactor power. More precise measurements reduce the degree of uncertainty in the power level, which is used by analysts to predict the ability of the reactor to be safely shut down under postulated accident conditions. Because the reactor power can now be calculated with much greater accuracy than with the old venturi type measurement, the plant can safely run within a tighter margin of error to its limits. The new flowmeter works by comparing the time it takes ultrasonic sound pulses to travel upstream versus downstream inside the pipe, and uses the time differential to figure the flow rate of the water in the pipe.\n\nThe NRC approved Vogtle's License Amendment Request (LAR) in March 2008. The NRC staff determined that Southern Nuclear could safely increase the reactor’s power output primarily through more accurate means of measuring feedwater flow. NRC staff also reviewed Southern Nuclear’s evaluations showing that the plant’s design can handle the increased power level. Unit 1 was uprated during its Spring 2008 refueling outage, and Unit 2 was uprated in the Fall outage of the same year.\n\nA loss of electrical power in the plant occurred on March 20, 1990.\n\nAt 9:20 a.m., a truck carrying fuel and lubricants in the plant's 230kV switchyard backed into a support column for the feeder line supplying power to the Unit 1-A reserve auxiliary transformer (RAT). At the time, the 1-B RAT was de-energized for maintenance and RAT 1-A was powering both trains of emergency electrical power. The non-emergency electrical trains were being powered by back-feeding from the switchyard through the main step-up transformer to the 1-A and 1-B unit auxiliary transformers (UAT). Additionally, emergency diesel generator (EDG) 1-B was out of service for planned maintenance. After the power loss, EDG 1-A failed to start due to a protective safety trip. The resulting loss of electrical power in the plant's \"vital circuits\" shut down the residual heat removal (RHR) pump that was cooling the core of Unit 1 (which was nearing the end of a refueling outage) and prevented the backup RHR from activating. Even though Unit 1 was offline at the time, residual heat from the natural decay of the radioactive fuel must be removed to prevent a dangerous rise in core temperature. While the non-safety power was not interrupted, there was no physical connection between the vital and non-vital electrical trains, preventing the vital trains from receiving power from the unaffected path through the UATs.\n\nAt 9:40 a.m., the plant operators declared a site area emergency (SAE) per existing procedures which called for an SAE whenever \"vital\" power is lost for more than 15 minutes. At 9:56 a.m., after trying multiple times to start the 1-A EDG normally, plant operators performed an emergency startup of the EDG by activating the generator's emergency start \"break-glass\" which bypassed most of the EDG's safeties and forced it to start. The startup was successful. RHR-A was then started using power from EDG-A. With core cooling restored, the SAE was downgraded to an alert at 10:15 a.m. At 11:40 a.m., crews energized RAT 1-B which had been shut down for maintenance, restoring power to the \"B\" safety electrical train. At 12:57 p.m., the \"A\" safety train was switched from the EDG to RAT 1-B and the EDG was shut down. With both trains receiving offsite power, the alert was terminated at 1:47 p.m.\n\nThe temperature of the Unit 1 core coolant increased from to during the 36 minutes required to re-energize the A-side bus. Throughout the event, non-vital power was continuously available to Unit 1 from off-site sources. However, the Vogtle electrical system was not designed to permit easy interconnection of the Unit 1 vital busses to non-vital power or the Unit 2 electrical busses. Since this incident, Plant Vogtle has implemented changes to the plant that allow the non-vital electrical buses to transfer power to the vital buses in this type of scenario.\n\nThis electrical fault also affected Unit 2 by causing breakers in the 230kV switchyard to trip, cutting off power to RAT 2-B and vital bus \"B.\" EDG 2-B subsequently started and restored power to the vital bus. At the same time, the electrical disturbance from the falling line striking the ground was detected by protective safeties on the Unit 2 main step-up transformer and a protective relay actuated, opening the transformer's output breaker. This caused a full load rejection to Unit 2, leading to a turbine trip and subsequently, a reactor scram. After Unit 2 tripped, the \"B\" non-vital electrical train lost power as it attempted to transfer from UAT 2-B (powered by the turbine generator) to the failed RAT 2-B, causing two of the reactor coolant pumps and one of the main feedwater pumps to trip. Despite this, plant cool-down proceeded safely. At 9:03 p.m., the RAT 2-B breakers in the switchyard were reset and offsite power was restored to the vital and non-vital \"B\" electrical trains, allowing reactor coolant pumps 2 and 4 to be restarted. EDG 2-B was shutdown. It was later determined that the fault disturbance caused by the line falling was not of significant magnitude to trip the protective relay per design and should not have caused Unit 2 to shut down. Further investigation found that current transformers on the main transformer were improperly set. The controls were adjusted to the proper setting. Had the CTs been properly set initially, the Unit 2 would have remained online.\n\nOn August 15, 2006, Southern Nuclear formally applied for an Early Site Permit (ESP) for two additional units. \nThe ESP determined whether the site was appropriate for additional reactors, and this process is separate from the Combined Construction and Operating License (COL) application process. \nOn March 31, 2008, Southern Nuclear announced that it had submitted an application for a COL, a process which would take at least 3 to 4 years. \nOn April 9, 2008, Georgia Power Company reached a contract agreement for two AP1000 reactors designed by Westinghouse; owned by Toshiba. \nWestinghouse partnered with the Shaw Group (Baton Rouge, LA) and its Stone & Webster division to manage the project with Westinghouse responsible for engineering, design, and overall management, and Shaw responsible for manufacturing the pre-fabricated component modules and managing the on-site construction. \nThe contract represented the first agreement for new nuclear development in the United States since the Three Mile Island accident in 1979, and it received approval from the Georgia Public Service Commission on March 17, 2009.\n\nOn August 26, 2009, the Nuclear Regulatory Commission (NRC) issued an Early Site Permit and a Limited Work Authorization. \nLimited construction at the new reactor sites began, with Unit 3 then expected to be operational in 2016, followed by Unit 4 in 2017, pending final issuance of the Combined Construction and Operating License by the NRC. \nThese dates have since slipped to 2021 and 2022 for Units 3 and 4, respectively.\n\nOn February 16, 2010, President Barack Obama announced $8.33 billion in federal loan guarantees toward the construction cost, although as of December 2013, Georgia Power had not availed itself of those guarantees. \nThe expected building cost for the two reactors is $14 billion. \nGeorgia power's share is around $6.1 billion, while remaining ownership of the two reactors is split among Oglethorpe Power Corp., the Municipal Electric Authority of Georgia (MEAG Power), and Dalton Utilities.\n\nIn February 2012, the NRC approved the construction license of the two proposed AP1000 reactors at Vogtle. NRC Chairman Gregory Jaczko cast the lone dissenting vote on plans to build and operate the two new nuclear power reactors, citing safety concerns stemming from Japan's 2011 Fukushima nuclear disaster, saying, \"I cannot support issuing this license as if Fukushima never happened.\" One week after Southern Company received the license to begin construction, many environmental and anti-nuclear groups sued to stop the expansion project, claiming \"public safety and environmental problems since Japan's Fukushima-Daiichi nuclear reactor accident have not been taken into account\". On July 11, 2012, the lawsuit was rejected by the Washington D.C. Circuit Court of Appeals.\n\nIn February 2013, the project's construction contractor, Shaw, was purchased by Chicago Bridge & Iron Company (CB&I). On March 12, 2013, construction on Unit 3 officially began with the pour of the basemat concrete for the nuclear island. This operation was completed on March 14. During the weekend of June 1, 2013, assembly of the containment vessel began with the bottom head of the vessel being lifted into place on the nuclear island. By June 2013, the construction schedule had been extended by at least 14 months. On November 21, 2013, the basemat pour for Unit 4 was completed.\n\nIn February 2014, the Department of Energy approved a $6.5 billion loan guarantee for Southern Company subsidiary Georgia Power and Oglethorpe Power Corp. The Department of Energy initially demanded a credit subsidy fee, but the demand was ultimately dropped given the financial strength of Southern Co. and the Vogtle project.\n\nFurther delays and cost increases were incorporated in a revised schedule in early 2015. As a result of the increased delays and cost overruns, contractor CB&I exited the project and Westinghouse took direct control of the project as contractor and hired construction firm Fluor to replace CB&I/Shaw on-site managing the day-to-day work. Westinghouse purchased certain assets of the former Shaw Group from CB&I to allow the project to go forward. In 2016, Southern Company and Westinghouse added construction firm Bechtel to the project to share construction management responsibilities.\n\nRecent construction milestones include setting the final of the \"big six\" structural modules for Unit 3 (CA-02 and CA-03, which form the walls of a storage tank that is part of the reactor's passive cooling system). The \"big six\" modules also include the previously installed CA-01, CA-04, and CA-05 in-containment structural modules, as well as the previously installed CA-20 structural module which forms part of the internal structure of the auxiliary building, containing many of the reactor's support systems. CA-02 and CA-03 were placed within the containment vessel in May 2016. The setting of these modules is a fairly significant milestone and allows other construction activities to commence. In June 2016, the final reactor coolant pump for Unit 3 was received on site. In November 2016, the reactor vessel for Unit 3 was set within the nuclear island. 2017 progress includes the installation of the reactor coolant loop piping and both steam generators at Unit 3. Progress has also been made in the turbine, auxiliary, and annex building. Unit 4 has also seen progress with the installation of the final two \"big six\" structural modules. Construction of both cooling towers is complete, with each nearly tall.\n\nAs of October 2017, recent progress includes completion of a critical 71 hour continuous concrete pour within the Unit 3 containment vessel, installation of the CA-33 floor module within Unit 3, placement of the Unit 4 deaerator within the turbine building, and the setting of the Unit 4 CA-03 module within the containment vessel.\n\nIn November 2017 the Georgia Public Service Commission (GPSC) requested additional documentation following concerns that design blueprints had not been approved by appropriately licensed engineers, which has legal implications. On December 21, 2017, the PSC approved the continuation of construction on Units 3 and 4, with conditions that reduced the costs that can be recovered from ratepayers over the life of the project.\n\nOn January 22, 2018, the Unit 3 pressurizer was installed within the containment building.\nOn March 29, 2018, the Unit 4 reactor vessel was placed inside the Unit 4 containment building.\n\nIn the February 2018 Vogtle Construction Monitoring Report (VCM), GPSC approved November 2021 and November 2022 as the target in-service dates for Units 3 & 4 respectively. The report notes that the project is being completed on an accelerated schedule and is currently tracking ahead of the 2021 & 2022 in-service target dates.\n\nIn August 2018 a $2.3 billion increase in costs was recognised. The total cost, including financing costs, is estimated at about $25 billion.\nIn September 2018, in order to sustain the project, Georgia Power agreed to pay an additional proportion of the costs of the smaller project partners if the cost of completion went beyond $9.2 billion.\n\nOn October 5th, 2018 the first reactor coolant pump (RCP) was placed in the Unit 3 containment vessel. The RCPs are critical components of the power generation process in pressurized water reactors. The RCPs circulate the high pressure reactor coolant through the primary reactor loop through the reactor core where it absorbs heat from the nuclear fission and on to the steam generators where the steam is produced in the secondary non nuclear loop. A total of eight RCPs will be placed, four per unit.\n\nIn March 2017, Westinghouse Electric Company filed for Chapter 11 bankruptcy due to $9 billion of losses from its two U.S. nuclear construction projects. The U.S. government has given $8.3 billion of loan guarantees to help finance construction of the Vogtle reactors, and a way forward to completing the plant has been agreed upon. On July 31, 2017 Southern Company division, Southern Nuclear, officially took over construction from Westinghouse and opened a bid for a new construction management contract to manage the day-to-day work on the site. Southern received bids from both Fluor and Bechtel. On August 31, 2017, Southern announced its decision to move forward with Bechtel to be the day-to-day construction manager for the remainder of the project. Bechtel will replace Fluor, who will no longer be involved in the project.\n\nThe Nuclear Regulatory Commission defines two emergency planning zones around nuclear power plants: a plume exposure pathway zone with a radius of , concerned primarily with exposure to, and inhalation of, airborne radioactive contamination, and an ingestion pathway zone of about , concerned primarily with ingestion of food and liquid contaminated by radioactivity.\n\nThe 2010 U.S. population within of Vogtle was 5,845, a decrease of 16.3 percent in a decade, according to an analysis of U.S. Census data for msnbc.com. The 2010 U.S. population within was 726,640, an increase of 8.8 percent since 2000. Cities within 50 miles include Augusta, GA (26 miles to city center).\n\nThe Nuclear Regulatory Commission's estimate of the risk each year of an earthquake intense enough to cause core damage to either reactor at Vogtle was 1 in 140,845, according to an NRC study published in August 2010.\n\n"}
{"id": "46981733", "url": "https://en.wikipedia.org/wiki?curid=46981733", "title": "World Solar Challenge 2001", "text": "World Solar Challenge 2001\n\nThe 2001 World Solar Challenge was one of a biennial series of solar-powered car races, covering about through the Australian Outback, from Darwin, Northern Territory to Adelaide, South Australia. The winner was a Nuna \"Alpha Centauri\" car built by Nuon of the Netherlands.\n\nhttps://web.archive.org/web/20020613175609/http://www.wsc.org.au/Results/2001/final.solar\n"}
