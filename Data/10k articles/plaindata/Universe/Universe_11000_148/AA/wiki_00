{"id": "1322100", "url": "https://en.wikipedia.org/wiki?curid=1322100", "title": "Anchor portal", "text": "Anchor portal\n\nAn anchor portal or H-frame tower is a gantry structure supporting overhead power lines in a switchyard. Their static function is similar to a dead-end tower. Anchor portals are almost always steel-tube or steel-framework constructions.\n"}
{"id": "31199101", "url": "https://en.wikipedia.org/wiki?curid=31199101", "title": "Bioliquids", "text": "Bioliquids\n\nBioliquids are liquid fuels made from biomass for energy purposes other than transport (i.e. heating and electricity).\n\nBioliquids are usually made from virgin or used vegetable and seed oils, like palm or soya oil. These oils are burned in a power station to create heat, which can then be used to warm homes or boil water to make steam. This steam can then be used to drive a turbine to generate electricity.\n\nRudolf Diesel's first public exhibition of the internal combustion engine, that was to later bear his name, ran on peanut oil.\n\nBioliquids have been used for many years to provide heat for homes on a small scale but now big energy providers are looking at their use on a larger scale.\n\nA controversial plant in Bristol (UK) was recently given the go ahead despite receiving several hundred complaints. The plant will be built and operated by W4B and provide enough power for 25,000 homes.\n\nBioliquids have several key advantages over other sources of renewable energy:\n\nMany of the same problems that affect biofuels also affect bioliquids and there are various social, economic, environmental and technical issues, which have been discussed in the popular media and scientific journals. These include: the effect of moderating oil prices, the \"food vs fuel\" debate, poverty reduction potential, carbon emissions levels, sustainable biofuel production, deforestation and soil erosion, loss of biodiversity, impact on water resources, as well as energy balance and efficiency.\n\nBioliquids also have several key problems compared to other sources of renewable energy:\n"}
{"id": "49096023", "url": "https://en.wikipedia.org/wiki?curid=49096023", "title": "Dilip K. Biswas", "text": "Dilip K. Biswas\n\nDilip K. Biswas is an Indian environmentalist and former chairman of the Central Pollution Control Board of India. He was a member of the panel which conducted ecological studies on Silent Valley and checked the feasibility of a hydro-electric project in the area, eventually recommending against project, leading to the declaration of Silent Valley as a National Park. He is the author of \"Implementation of the Clean Development Mechanism in Asia and the Pacific: Issues, Challenges, and Opportunities\", a report published by the United Nations as a guideline for the implementation of the Clean Development Mechanism (CDM), prescribed by Kyoto Protocol. His contributions are also reported behind the drafting of the environment management laws in Lucknow, the capital city of the Indian state of Uttar Pradesh. The Government of India awarded him the fourth highest civilian honour of the Padma Shri, in 2007, for his contributions to science and technology.\n\n"}
{"id": "37277820", "url": "https://en.wikipedia.org/wiki?curid=37277820", "title": "Dry dung fuel", "text": "Dry dung fuel\n\nDry dung fuel (or dry manure fuel) is animal feces that has been dried in order to be used as a fuel source. It is used as a fuel in many countries around the world. Using dry manure as a fuel source is an example of reuse of excreta. A disadvantage of using this kind of fuel is increased air pollution. In India, this kind of fuel source is known as \"dung cakes\".\n\nDry dung is more commonly used than moist dung, because it burns more easily. Dry manure is typically defined as having a moisture content less than 30 percent.\n\n\"Dung cakes\", made from the by-products of animal husbandry, are traditionally used as fuel in India for making food in a domestic hearth called a Chulha. They are made by hand by village women and are traditionally made from cow or buffalo dung. One dung cake of an average size gives 2100 kJ worth of energy. Dung cakes are also known as \"goitha\", \"uple\", \"kande\", \"gosse\" or \"thepdi\".\n\nThese are the cakes of cow dung molded by bare hands with a curvature to be able to keep stuck to the walls. Once dried they are put in a pile and covered with thatch called \"bitauda\". These bitaudas are visible in parts of rural India albeit with different names. The size and shape of the cake might vary with region. Its also not uncommon to see these cakes directly used in earthen ovens.\n\nThis bio-fuel has been used primarily for two reasons: for easy disposal of cow dung and as easily available and cheap fuel. \n\nHuman feces can in principle also be dried and used as a fuel source if they are collected in a type of dry toilet, for example an incinerating toilet. Since 2011, the Bill & Melinda Gates Foundation is supporting the development of such toilets as part of their \"Reinvent the Toilet Challenge\" to promote safer, more effective ways to treat human excreta. The omni-processor is another example of using human feces contained in fecal sludge or sewage sludge as a fuel source.\n\nThe benefits of using dry animal dung include:\n\nAlso, camel dung is used as fuel in Egypt.\n\n\n\n\n\nDry animal dung was used from prehistoric times,\nincluding in Ancient Persia and Ancient Egypt. In Equatorial Guinea archaeological evidence has been found of the practice and biblical records indicate animal and human dung were used as fuel.\n\n\n"}
{"id": "4400091", "url": "https://en.wikipedia.org/wiki?curid=4400091", "title": "Durio zibethinus", "text": "Durio zibethinus\n\nDurio zibethinus is the most common tree species in the genus \"Durio\" that are known as durian and have edible fruit also known as durian.\n\nAs with other durian species, the edible flesh emits a distinctive odour that is strong and penetrating even when the husk is intact. Some people regard the durian as having a pleasantly sweet fragrance; others find the aroma overpowering and revolting. The smell evokes reactions from deep appreciation to intense disgust, and has been described variously as rotten onions, turpentine, and raw sewage. The persistence of its odour has led to the fruit's banishment from certain hotels and public transportation in Southeast Asia.\n\nThere are 30 recognised \"Durio\" species, at least nine of which produce edible fruit. \"D. zibethinus\" is the only species available in the international market: other species are sold in their local regions. There are hundreds of cultivars of \"D. zibethinus\"; many consumers express preferences for specific cultivars, which fetch higher prices in the market.\n\nThe wood of \"D. zibethinus\" is reddish brown.\n\n\"D. zibethinus\" flowers are visited by bats which eat the pollen and pollinate the flowers. The flowers open in the afternoon and shed pollen in the evening. By the following morning, the calyx, petals, and stamens have fallen off to leave only the gynoecium of the flower.\n\nOver the centuries, numerous durian cultivars, propagated by vegetative clones, have arisen in southeast Asia. They used to be grown with mixed results from seeds of trees bearing superior quality fruit, but now are propagated by layering, marcotting, or more commonly, by grafting, including bud, veneer, wedge, whip or U-grafting onto seedlings of randomly selected rootstocks. Different cultivars may be distinguished to some extent by variations in the fruit shape, such as the shape of the spines. Durian consumers express preferences for specific cultivars, which fetch higher prices in the market.\n\nMost cultivars have a common name and a code number starting with \"D\". For example, some popular clones are Kop (D99 ), Chanee (D123, ), Berserah or Green Durian or Tuan Mek Hijau (D145 ), Kan Yao (D158, ), Mon Thong (D159, ), Kradum Thong ( ), and with no common name, D24 and D169. Each cultivar has a distinct taste and odour. More than 200 cultivars of \"D. zibethinus\" exist in Thailand.\n\nMon thong is the most commercially sought after for its thick, full-bodied creamy and mild sweet tasting flesh with relatively moderate smell emitted and smaller seeds, while Chanee is the best in terms of its resistance to infection by \"Phytophthora palmivora\". Kan Yao is somewhat less common, but prized for its longer window of time when it is both sweet and odorless at the same time. Among all the cultivars in Thailand, five are currently in large-scale commercial cultivation: Chanee, Mon Thong, Kan Yao, Ruang, and Kradum. There are more than 100 registered cultivars since 1920's in Malaysia and up to 193 cultivar by 1992, and many superior cultivars have been identified through competitions held at the annual Malaysian Agriculture, Horticulture, and Agrotourism Show. In Vietnam, the same process has been achieved through competitions held by the Southern Fruit Research Institute. A recently popular variety is Musang King.\n\nBy 2007, Songpol Somsri, a Thai government scientist, had crossbred more than ninety varieties of durian to create Chantaburi No.Â 1, a cultivar without the characteristic odour. Another hybrid, Chantaburi No. 3, develops the odour about three days after the fruit is picked, which enables an odourless transport yet satisfies consumers who prefer the pungent odour. On 22 May 2012, two other cultivars from Thailand that also lack the usual odour, Long Laplae and Lin Laplae, were presented to the public by Yothin Samutkhiri, governor of Uttaradit Province from where these cultivars were developed locally, while he announced the dates for the yearly durian fair of Laplae District, and the name giver to both cultivars.\n\n\n"}
{"id": "4515124", "url": "https://en.wikipedia.org/wiki?curid=4515124", "title": "Electric blanket", "text": "Electric blanket\n\nAn electric blanket is a blanket containing integrated electrical heating wires. There are several types; underblankets, overblankets, throws and duvets. An electric \"underblanket\" is placed above the mattress and below the bottom bed sheet. This is the most common type in the UK and Commonwealth countries, where it is known by default as an \"electric blanket\"; in the U.S. and Canada, where it is less common, it is called an electric heated mattress pad. An electric \"overblanket\" is placed above the top bed sheet, and is the most common type in the U.S. and Canada, where it is called an \"electric blanket\".\n\nElectric blankets usually have a control unit that adjusts the amount of heat the blanket produces. Blankets for two-person beds often have separate controls for each side of the bed. The electric blanket may be used to pre-heat the bed before use or to keep the occupant warm while in bed.\n\nSome modern \"low voltage\" electric blankets have thin carbon fiber wires and work on 12 to 24 volts.\n\nMuch like heating pads, electric blankets use an insulated wire or heating element inserted into a fabric that heats when it is plugged in. The temperature control unit, located between the blanket and the electrical outlet, manages the amount of current entering into the heat elements in the blanket.\n\nSome modern electric blankets use carbon fiber elements that are far less bulky and conspicuous than older heating wires. Carbon fiber is also used as the heating element in many high-end heated car seats. Blankets can be purchased with rheostats that regulate the heat by managing body heat and blanket temperatures, ensuring a comfortable experience.\n\nNewer electric blankets have a shutoff mechanism to prevent the blanket from overheating or catching fire. Older blankets (prior to about 2001) may not have a shut-off mechanism; users run the risk of overheating. Older blankets are considered fire hazards.\n\nSome electric blankets work on a low voltage of 12 to 24 volts, including those which plug into ordinary household electrical outlets; in the US, such blankets are sold by Soft Heat, Serta, and Select Comfort. Such blankets also include 12-volt blankets designed for in-car use; they tend to shut off automatically every 45 minutes or so.\n\nOld or damaged blankets concern fire safety officials internationally. The use of such blankets is of concern due to the combination of heat, electricity, the abundance of flammable bedding material, and a sleeping occupant. In the United Kingdom, it is estimated that 5,000 fires per year are caused by faulty electric blankets, of which 99% are believed to have been caused by blankets 10 years or older.\n\nElectric blankets also present a burn risk to those who cannot feel pain or are unable to react to it. Individuals included in this group are small children, diabetics, and the elderly.\n\nAs with any source of heat over the groin, use of electric blankets can reduce fertility in men.\n\nNo mechanism by which ELF(Extremely low frequency)-EMFs(Electromagnetic field) or radiofrequency radiation could cause cancer has been identified. Unlike high-energy (ionizing) radiation, EMFs in the non-ionizing part of the electromagnetic spectrum cannot damage DNA or cells directly. Some scientists have speculated that ELF-EMFs could cause cancer through other mechanisms, such as by reducing levels of the hormone melatonin. There is some evidence that melatonin may suppress the development of certain tumors.\n\nA cartoon electrical blanket with its electrical temperature control acting as an anthropomorphic face named \"Blanky\" was portrayed in the 1987 film \"The Brave Little Toaster\".\n\n\n"}
{"id": "14736351", "url": "https://en.wikipedia.org/wiki?curid=14736351", "title": "Elmira, Prince Edward Island", "text": "Elmira, Prince Edward Island\n\nElmira is a community in the Canadian province of Prince Edward Island, located in Lot 47 of Kings County, northeast of Souris.\n\nCBC Television's CBCT and CBC Radio One's CBCT-FM maintain rebroadcasters at Elmira to serve the portion of eastern Prince Edward Island which lies outside the broadcast range of those stations' main transmitters in Charlottetown.\n\nIs a wind farm located at Souris-Elmira, PEI, Canada. It was completed January 22, 2007. It is owned and operated by PEI Energy Corporation.\n\nThe wind farm consists of ten Vestas V90 wind turbines 3 MW. Annual production will be 90-95 million kilowatt hours. The average house uses about 8,000 kilowatt hours of electricity annually so the wind farm will produce enough electricity to power about 12,000 homes.\nThe Eastern Kings Wind Farm will supply about 7.5% of PEIâs electricity and displace 75,000 tonnes of greenhouse gases per year. The project capital cost was approximately $47 million.\n"}
{"id": "41407054", "url": "https://en.wikipedia.org/wiki?curid=41407054", "title": "Forced outage", "text": "Forced outage\n\nIn electrical engineering, forced outage is the shutdown condition of a power station, transmission line or distribution line when the generating unit is unavailable to produce power due to unexpected breakdown.\n\nForced outage can be caused by equipment failures, disruption in the power plant fuel supply chain, operator error etc.\n\nForced outage rate (FOR or FOAR) of a power station unit is the probability that the unit will not be available for service when required.\n\nFOR is defined as the number of hours the unit is on forced outage over the total number of hours in a year (which is the sum of hours the power station is available for service and hours the power station is in forced outage).\n\nThe risk occurred from force outage can be minimized by having it insured.\n\n"}
{"id": "165206", "url": "https://en.wikipedia.org/wiki?curid=165206", "title": "Fujita scale", "text": "Fujita scale\n\nThe Fujita scale (F-Scale), or FujitaâPearson scale (FPP scale), is a scale for rating tornado intensity, based primarily on the damage tornadoes inflict on human-built structures and vegetation. The official Fujita scale category is determined by meteorologists and engineers after a ground or aerial damage survey, or both; and depending on the circumstances, ground-swirl patterns (cycloidal marks), weather radar data, witness testimonies, media reports and damage imagery, as well as photogrammetry or videogrammetry if motion picture recording is available. The Fujita scale was replaced with the Enhanced Fujita scale (EF-Scale) in the United States in February 2007. In April 2013, Canada adopted the EF-Scale over the Fujita scale along with 31 \"Specific Damage Indicators\" used by Environment Canada (EC) in their ratings.\n\nThe scale was introduced in 1971 by Tetsuya Fujita of the University of Chicago, in collaboration with Allen Pearson, head of the National Severe Storms Forecast Center/NSSFC (currently the Storm Prediction Center/SPC). The scale was updated in 1973, taking into account path length and width. In the United States, starting in 1973, tornadoes were rated soon after occurrence. The Fujita scale was applied retroactively to tornadoes reported between 1950 and 1972 in the National Oceanic and Atmospheric Administration (NOAA) National Tornado Database. Fujita rated tornadoes from 1916â1992 and Tom Grazulis of The Tornado Project retroactively rated all known significant tornadoes (F2âF5 or causing a fatality) in the U.S. back to 1880.\nThe Fujita scale was adopted in most areas outside of Great Britain.\n\nIn 2007, the Fujita scale was updated, and the Enhanced Fujita Scale was introduced in the United States. The new scale more accurately matches wind speeds to the severity of damage caused by the tornado.\n\nThough each damage level is associated with a wind speed, the Fujita scale is effectively a damage scale, and the wind speeds associated with the damage listed aren't rigorously verified. The Enhanced Fujita Scale was formulated due to research which suggested that the wind speeds required to inflict damage by intense tornadoes on the Fujita scale are greatly overestimated. A process of expert elicitation with top engineers and meteorologists resulted in the EF scale wind speeds, however, these are biased to United States construction practices. The EF scale also improved damage parameter descriptions.\n\nThe original scale as derived by Fujita was a theoretical 13-level scale (F0âF12) designed to smoothly connect the Beaufort scale and the Mach number scale. F1 corresponds to the twelfth level of the Beaufort scale, and F12 corresponds to Mach number 1.0. F0 was placed at a position specifying no damage (approximately the eighth level of the Beaufort scale), in analogy to how the Beaufort's zeroth level specifies little to no wind. From these wind speed numbers, qualitative descriptions of damage were made for each category of the Fujita scale, and then these descriptions were used to classify tornadoes. The diagram on the right illustrates the relationship between the Beaufort, Fujita, and Mach number scales.\n\nAt the time Fujita derived the scale, little information was available on damage caused by wind, so the original scale presented little more than educated guesses at wind speed ranges for specific tiers of damage. Fujita intended that only F0âF5 be used in practice, as this covered all possible levels of damage to frame homes as well as the expected estimated bounds of wind speeds. He did, however, add a description for F6, which he phrased as \"inconceivable tornado\", to allow for wind speeds exceeding F5 and for possible future advancements in damage analysis which might show it.\n\nFurthermore, the original wind speed numbers have since been found to be higher than the actual wind speeds required to incur the damage described at each category. The error manifests itself to an increasing degree as the category increases, especially in the range of F3 through F5. NOAA notes that \"...Â precise wind speed numbers are actually guesses and have never been scientifically verified. Different wind speeds may cause similar-looking damage from place to placeâeven from building to building. Without a thorough engineering analysis of tornado damage in any event, the actual wind speeds needed to cause that damage are unknown.\" Since then, the Enhanced Fujita Scale has been created using better wind estimates by engineers and meteorologists.\n\nSome sources add level F+\", meaning a tornado with winds below 39 mph; on rare occasions tornadoes this weak are observed covering a wide range of path widths and lengths.\n\nThe five categories are listed here, in order of increasing intensity.\n\nFor purposes such as tornado climatology studies, Fujita scale ratings may be grouped into classes.\n\nThe Fujita scale, introduced in 1971 as a means to differentiate tornado intensity and path area, assigned wind speeds to damage that were, at best, educated guesses. Fujita and others recognized this immediately and intensive engineering analysis was conducted through the rest of the 1970s. This research, as well as subsequent research, showed that tornado wind speeds required to inflict the described damage were actually much lower than the F-scale indicated, particularly for the upper categories. Also, although the scale gave general descriptions for the type of damage a tornado could cause, it gave little leeway for strength of construction and other factors that might cause a building to receive higher damage at lower wind speeds. Fujita tried to address these problems somewhat in 1992 with the Modified Fujita Scale, but by then he was semi-retired and the National Weather Service was not in a position for the undertaking of updating to an entirely new scale, so it went largely unenacted.\n\nIn the United States, on February 1, 2007, the Fujita scale was decommissioned in favor of what these scientists believe is a more accurate Enhanced Fujita Scale, which replaces it. The meteorologists and engineers who designed the EF Scale believe it an improvement on the F-scale on many countsâit accounts for different degrees of damage that occur with different types of structures, both man-made and natural. The expanded and refined damage indicators and degrees of damage standardize what was somewhat ambiguous. It also is thought to provide a much better estimate for wind speeds, and sets no upper limit on the wind speeds for the strongest level, EF5. Several countries continue to use the original Fujita Scale. Environment Canada has begun using the Enhanced Fujita scale in Canada as of April 18, 2013.\n\n\n"}
{"id": "54223579", "url": "https://en.wikipedia.org/wiki?curid=54223579", "title": "Gaza electricity crisis", "text": "Gaza electricity crisis\n\nThe Gaza electricity crisis is an ongoing and growing electricity crisis faced by nearly two million citizens of the Gaza Strip, with regular power supply being provided only for a few hours a day on a rolling blackout schedule. Some Gazans use private electric generators, solar panels and uninterruptible power supply units to consume power when regular power is not available. The crisis is a result of the tensions between Hamas, who rules Gaza, and the Palestinian Authority/Fatah, who rules the West Bank over custom tax revenue, funding of Gaza, and political authority. \n\nAs of 2017, Gaza's normal energy needs are estimated to be approximately 400â600 megawatts for full 24-hour supply to all residents, which are normally supplied by a diesel power plant in Gaza with has a nominal rating of 60â140Â MW (figures vary due to degree of operation and damage to the plant) which is reliant on fuel imports, an additional 125Â MW imported from Israel via 10 power lines, and 27Â MW of power imported from Egypt. Even in normal conditions, the current rated supply of Gaza is inadequate to meet growing needs, and the crisis has led to further closure and reductions to each of these power sources.\n\nPrior to June 2013, fuel for the power plant was smuggled from Egypt, where fuel at the time was highly subsidized. Following measures taken by the Egyptians against the Gaza Strip smuggling tunnels, these cheap imports were halted and the power plant began operating in a partial capacity with fuel supplied via Israel, which charges a high import duty.\nEven though Gaza is ruled by Hamas, they are currently dependent on the Palestinian Authority (PA) to help provide electricity â the import duties on Gaza's fuel purchases via Israel are passed to the PA, making Hamas reliant on the PA for funding, and the PA, not Hamas, pays the bill to Israel and Egypt for the electricity they provide to Gaza. As of 2017, the Palestine Authority has ceased making some of the payments.\n\nIsraeli human rights group B'Tselem has documented how common people cope in Gaza with electricity being provided on a rolling blackout schedule of a few hours a day, and has further said that Israel should take responsibility for the crisis, a responsibility Israel denies, saying that Hamas should allocate funds for electricity rather than personal gain and military expenditure on equipment and military tunnels.\n\nIn April 2017, Gaza's sole power plant ran out of fuel. Hamas blamed the Palestinian Authority for levying taxes on the fuel (levied by Israel as per Protocol on Economic Relations, and passed to the PA) while not passing tax revenue back to Gaza. The PA claims that the Hamas officials in Gaza are simply incapable of running the plant efficiently.\n\nFrom April 2017, power supply from Egypt is reportedly not operational. According to Asharq Al-Awsat Egypt offered, in June 2017, to supply Gaza with electricity in exchange for the extradition of 17 wanted terrorists and other security demands.\n\nFollowing the PA's refusal to pay for electricity in Gaza and instructions to reduce supply, Israel further reduced supply to Gaza in May and June 2017, saying this was an internal Palestinian matter. The Israeli military and the UN have warned that the electricity crisis and resulting humanitarian crisis may lead Gaza to initiate military hostilities. Hamas has labelled Israel's decision as \"dangerous and catastrophic\", threatening to renew violence.\n\nOn the 20th of June 2017, reports emerged that Egypt and Hamas reached an understanding according to which Egypt would supply 500 tons of diesel fuel daily. This supply wouldn't face Israeli custom duty rates (passed to the PA), as stipulated by the Paris Protocol, potentially placing Gaza in a separate economic bloc from Israel and the PA.\n\nIn the summer of 2017, sewage was directed untreated to the sea due to the lack of electricity, severely polluting Gaza's beaches. As a result the number of beach goers plummeted. The Israeli beach of Zikim was closed as well due to the sewage pollution from Gaza. Gazan sewage was also pumped into Israel via Nahal Hanoun from Beit Hanoun and Beit Lahia polluting the Israeli coastal groundwater aquifer.\n\nIn August 2017, the United Nations human rights office called Israel, the State of Palestinian and Hamas authorities to resolve the conflict, saying \"\"We are deeply concerned about the steady deterioration in the humanitarian conditions and the protection of human rights in Gaza\", and that the supply of electricity for less than four a hours a day since April \"has a grave impact on the provision of essential health, water and sanitation services\".\n\n"}
{"id": "42218795", "url": "https://en.wikipedia.org/wiki?curid=42218795", "title": "Glossary of power generation", "text": "Glossary of power generation\n\nThe following is a list of common definitions related to power generation.\n\nThe Power Generation Body of Knowledge, a Wikipedia Book, contains a wealth of relevant information regarding power generation. The book is published in several volumes:\n\nOnline power generation glossaries:\n"}
{"id": "19284371", "url": "https://en.wikipedia.org/wiki?curid=19284371", "title": "Greentech Media", "text": "Greentech Media\n\nGreentech Media, a subsidiary of Wood Mackenzie, is a media company based in Massachusetts, United States, that generates daily report, market research study and news on electricity systems and green technology and green jobs.\n\nIt was founded in February 2007 by Scott Clavenna and Rick Thompson, and announced that it had raised $1 million in Series-A venture capital funding the following May. It announced another $2.75 million Series-B round in May 2008. The company currently has four offices, in Boston, New York, San Francisco and Munich.\n\nGreentech Media Inc. has released quarterly data showing that venture capital investment in green technologies totaled US $1.2 billion in 85 deals in the second quarter of 2009. This is up from $836 million in 59 deals in the first quarter of 2009.\n\nGreentech Media produces two weekly podcasts: \"Energy Gang\", a weekly digest of energy topics, hosted by Stephen Lacey, Katherine Hamilton, and Jigar Shah, and \"The Interchange,\" a more technical energy podcast featuring industry insights, hosted by Stephen Lacey and Shayle Kann.\n\n\n"}
{"id": "2659722", "url": "https://en.wikipedia.org/wiki?curid=2659722", "title": "Ground propulsion", "text": "Ground propulsion\n\nGround propulsion is any mechanism for propelling solid bodies along the ground, usually for the purposes of transportation. The propulsion system often consists of a combination of an engine or motor, a gearbox and wheel and axles (or caterpillar tracks) in standard applications.\n\nThe primary and most natural type of propulsion is the use of muscle power. The invention of the wheel allowed for the development of vehicles like Carts and Wagons that make more efficient use of muscle power, allowing larger loads to be transported. Vehicles drawn by humans and domesticated animals are not economically as important as they once were, but they still exist. Examples of human-powered vehicles are rickshaws and cycle rickshaws.\n\nThe development of the steam engine and internal combustion engine allowed for the development of rail vehicles and motor vehicles, all of which have some form of a powertrain. The electric motor allowed for quieter vehicles with lower emissions, and frequently higher engine efficiency.\n\nSome less commonly used or experimental engines are:\n"}
{"id": "6933141", "url": "https://en.wikipedia.org/wiki?curid=6933141", "title": "Hawthorne Nevada Airlines Flight 708", "text": "Hawthorne Nevada Airlines Flight 708\n\nHawthorne Nevada Airlines Flight 708 was a domestic non-scheduled passenger flight between Hawthorne Industrial Airport, Nevada (HTH) and Hollywood-Burbank Airport, California (BUR/KBUR), that crashed into the tallest mountain in the contiguous United States, Mount Whitney, near Lone Pine, on February 18, 1969, killing all 35 passengers and crew on board.\n\nThe aircraft, a Douglas DC-3, was operating on a visual flight rules plan. It departed at 3:50 A.M. PST and last contact was made at 4:06 A.M. when the flight spoke with the Tonopah Flight Service Station. One hour later, at 5:10 A.M., the plane hit a sheer cliff face on the east side of Mount Whitney at 11,770 feet (3,558 m). The main body of the wreckage then slid down the cliff and stopped some 500 feet (152 m) back from the cliff, where it caught fire. All 32 passengers and all three crew members were killed.\n\nExtensive searches from air and ground were launched after the aircraft went missing, but snow, low clouds, and mountainous terrain hampered the search. The aircraft was finally located on August 8, 1969. It is not thought that the delay had any impact on the lack of survivors as it is thought that all on board died on impact.\n\nThe National Transportation Safety Board launched an extensive investigation upon the location of the wreckage. Its conclusions were as follows:\n\nThe accident was caused by the deviation from the prescribed route of flight, as authorized in the company's FAA-approved operations specifications, resulting in the aircraft being operated under IFR weather conditions, in high mountainous terrain, in an area where there was a lack of radio navigation aids. The weather was also a contributing factor.\n\n"}
{"id": "22644904", "url": "https://en.wikipedia.org/wiki?curid=22644904", "title": "Het is een schone dag geweest", "text": "Het is een schone dag geweest\n"}
{"id": "58912", "url": "https://en.wikipedia.org/wiki?curid=58912", "title": "Heteroecious", "text": "Heteroecious\n\nA heteroecious parasite is one that requires at least two hosts. The \"primary host\" is the host in which the parasite spends its adult life; the other is the \"secondary host\". Both the primary host and an unrelated alternate host are required for the parasite to complete its life cycle. This can be contrasted with an autoecious parasite which can complete its life cycle on a single host species. Many rust fungi are prime examples of a heteroecious life cycle.\n\nParasitic heteroecious fungi include \n\nThe phenomenon of heteroecy was first discovered by A.S. Ãrsted in 1863.\n"}
{"id": "113282", "url": "https://en.wikipedia.org/wiki?curid=113282", "title": "Hewing", "text": "Hewing\n\nIn woodworking, hewing is the process of converting a log from its rounded natural form into lumber (timber) with more or less flat surfaces using primarily an axe. It is an ancient method, and before the advent of the industrial-era type of sawmills, it was a standard way of squaring up wooden beams for timber framing. Today it is still used occasionally for that purpose by anyone who has logs, needs beams, and cannot or would prefer not to pay for finished lumber. Thus homesteaders on frugal budgets, for example, may hew their own lumber rather than buy it.\n\n\"Hew\" is a general term meaning to strike or blow with a tool such as an axe or sword; to chop or gash, and is used in warfare, stone and wood cutting, and coal and salt mining in this sense. Hewing wood is to shape the wood with a sharp instrument such as an axe, specifically flattening one or more sides of a log.\n\nAs an ancient method of timber \"conversion\", different methods of each step in hewing have developed in history.\n\nAfter a tree is selected and felled, hewing can take place where the log landed or be skidded or twitched (skidded with a horse or oxen) out of the woods to a work site. The log is placed across two other smaller logs near the ground or up on trestles about waist height; stabilized either by notching the support logs, or using a 'timber dog' (also called a log dog, a long bar of iron with a tooth on either end that jams into the logs and prevents movement). \nThe hewer measures and locates the timber within the log on both ends and marks lines along the length of a log, usually with a chalk line.\n\nThe next step is to chop notches every foot or two, almost as deep as the marked line using a chopping or scoring axe, called scoring. At least three methods are used in scoring. 1) Standing on the log and swinging an axe to chop the score; 2) In Germany a method of two carpenters standing on the ground with the log on trestles and swinging downward to slice the scores. (see video in link below); 3) A chainsaw is used to notch the log, the sections created by the notching are then split off using a felling axe.\n\nThe pieces of wood between the notches are knocked off with an axe, this process called juggling or joggling. This results in a rough surface pared down just shy of the marked line. Scoring and juggling remove a fair amount of wood, make hewing easier and prevent long shreds of wood being torn off.\n\nHewing is the last step in this whole process, which is also collectively referred to as hewing. Hewing is done on the logs sides with a broadaxe. Hewing occurs from the bottom of the stem upwards towards what was the top of the standing tree, reducing the tendency of the broken fibers to migrate inwards towards the eventual beam.\n\nIt is widely published that an adze was used to hew the top surface of a log flat in the same manner as an axe is used on the sides of a log. However, physical evidence of looking at the marks left by the hewing tools in historic buildings, called tracology, are swung in an arc and thus made by an axe, not an adz. Shipbuilders frequently used adzes in shaping ship timbers, the choice of tool being made by the position of the surface being hewn, the sides best hewn with an axe and the face best hewn with an adze. Historic illustrations do show some Asian carpenters hewing building timbers with an adz. Further smoothing can then be done using a hand plane, drawknife, yariganna (an ancient Japanese cutting tool) or any other established or improvised means.\n\nSome 19th-century timber buildings in the U.S. have hewn long timbers in the same framing with vertically sawn and the later technology of circular sawn timbers. The reason for this is the long timbers were easier to hew with an axe than to take to a sawmill due to poor transportation routes.\n\nHewn railroad ties are known as axe ties and were made by a \"tiehacker\".\n\nAlthough still used in niche modern building, salvaged hand-hewn beams are now commonly recycled as architectural details popular in new construction and renovation of homes. They are also popular as decor in commercial and restaurant spaces.\n\n"}
{"id": "1966606", "url": "https://en.wikipedia.org/wiki?curid=1966606", "title": "Hydroxyapatite", "text": "Hydroxyapatite\n\nHydroxyapatite, also called hydroxylapatite (HA), is a naturally occurring mineral form of calcium apatite with the formula Ca(PO)(OH), but it is usually written Ca(PO)(OH) to denote that the crystal unit cell comprises two entities. Hydroxyapatite is the hydroxyl endmember of the complex apatite group. The OH ion can be replaced by fluoride, chloride or carbonate, producing fluorapatite or chlorapatite. It crystallizes in the hexagonal crystal system. Pure hydroxyapatite powder is white. Naturally occurring apatites can, however, also have brown, yellow, or green colorations, comparable to the discolorations of dental fluorosis.\n\nUp to 50% by volume and 70% by weight of human bone is a modified form of hydroxyapatite, known as bone mineral. Carbonated calcium-deficient hydroxyapatite is the main mineral of which dental enamel and dentin are composed. Hydroxyapatite crystals are also found in the small calcifications, within the pineal gland and other structures, known as corpora arenacea or 'brain sand'.\n\nHydroxyapatite can be synthesized via several methods, such as wet chemical deposition, biomimetic deposition, sol-gel route (wet-chemical precipitation) or electrodeposition. Yagai and Aoki proposed the hydroxyapatite nanocrystal suspension can be prepared by a wet chemical precipitation reaction following the reaction equation below:\n\n10 Ca(OH) + 6 HPO â Ca(PO)(OH) + 18 HO\n\nSeveral studies have shown that hydroxyapatite synthesis via the wet-chemical route can be improved by high-power ultrasound. The ultrasonically assisted synthesis (sono-synthesis) of hydroxyapatite is a successful technique for the production of nanostructured hydroxyapatite to high quality standards. The ultrasonic route allows the production of nano-crystalline hydroxyapatite as well as modified particles, e.g. core-shell nanospheres and composites.\n\nCalcium deficient (non-stochiometric) hydroxyapatite, Ca(PO)(HPO)(OH) (where x is between 0 and 1) has a Ca/P ratio between 1.67 and 1.5. The Ca/P ratio is often used in the discussion of calcium phosphate phases. Stoichiometric apatite Ca(PO)(OH) has a Ca/P ratio of 10:6 normally expressed as 1.67. The non-stoichiometric phases have the hydroxyapatite structure with cation vacancies (Ca) and anion (OH) vacancies. The sites occupied solely by phosphate anions in stochiometric hydroxyapatite, are occupied by phosphate or hydrogen phosphate, HPO, anions. \nPreparation of these calcium deficient phases can be prepared by precipitation from a mixture of calcium nitrate and diammonium phosphate with the desired Ca/P ratio, for example to make a sample with a Ca/P ratio of 1.6:\n\nSintering these non-stoichiometric phases forms a solid phase which is an intimate mixture of tricalcium phosphate and hydroxyapatite, termed biphasic calcium phosphate:\n\nThe clubbing appendages of the \"Odontodactylus scyllarus\" (peacock mantis shrimp) are made of an extremely dense form of the mineral which has a higher specific strength and toughness than any synthetic composite material; these properties have led to its investigation for potential synthesis and engineering use. Their dactyl appendages have excellent impact resistance due to the impact region being composed of mainly crystalline hydroxyapatite, which offers significant hardness. A periodic layer underneath the impact layer composed of hydroxyapatite with lower calcium and phosphorus content (thus resulting in a much lower modulus) inhibits crack growth by forcing new cracks to change directions. This periodic layer also reduces the energy transferred across both layers due to the large difference in modulus, even reflecting some of the incident energy. \n\nHydroxyapatite is present in bone and teeth; bone is made primarily of HA crystals interspersed in a collagen matrix -- 65 to 70% of the mass of bone is HA. Similarly HA is 70 to 80% of the mass of dentin and enamel in teeth. In enamel, the matrix for HA is formed by amelogenins and enamelins instead of collagen.\n\nHydroxylapatite deposits in tendons around joints results in the medical condition calcific tendinitis.\n\nHA is increasingly used to make bone grafting materials as well as dental prosthetics and repair. Some implants, e.g. hip replacements, dental implants and bone conduction implants, are coated with HA.. As the native dissolution rate of hydroxyapatite in-vivo, around 10 wt% per year, is significantly lower than the growth rate of newly formed bone tissue, towards its use as bone replacement materials measures are often sought to enhance its solubility rate and thus promote better bioactivity . \n\nMicrocrystalline hydroxyapatite (MH) is marketed as a \"bone-building\" supplement with superior absorption in comparison to calcium. It is a second-generation calcium supplement derived from bovine bone. In the 1980s, bone meal calcium supplements were found to be contaminated with heavy metals, and although the manufacturers claim their MH is free from contaminants, people are advised to avoid it because its effect in the body has not been well-tested.\n\nThe mechanism of hydroxyapatite (HA) chromatography is complicated and has been described as \"mixed-mode\" ion exchange. It involves nonspecific interactions between positively charged calcium ions and negatively charged phosphate ions on the stationary phase HA resin with protein negatively charged carboxyl groups and positively charged amino groups. It may be difficult to predict the effectiveness of HA chromatography based on physical and chemical properties of the desired protein to be purified. For elution, a buffer with increasing phosphate concentration is typically used for application.\n\nIn archaeology, hydroxyapatite from human and animal remains can be analysed to reconstruct ancient diets, migrations and palaeoclimate. The mineral fractions of bone and teeth act as a reservoir of trace elements, including carbon, oxygen and strontium. Stable isotope analysis of human and faunal hydroxyapatite can be used to indicate whether a diet was predominantly terrestrial or marine in nature (carbon, strontium); the geographical origin and migratory habits of an animal or human (oxygen, strontium) and to reconstruct past temperatures and climate shifts (oxygen). Post-depositional alteration of bone can contribute to the degradation of bone collagen, the protein required for stable isotope analysis.\n\n"}
{"id": "265421", "url": "https://en.wikipedia.org/wiki?curid=265421", "title": "Inverted pendulum", "text": "Inverted pendulum\n\nAn inverted pendulum is a pendulum that has its center of mass above its pivot point. It is unstable and without additional help will fall over. It can be suspended stably in this inverted position by using a control system to monitor the angle of the pole and move the pivot point horizontally back under the center of mass when it starts to fall over, keeping it balanced. The inverted pendulum is a classic problem in dynamics and control theory and is used as a benchmark for testing control strategies. It is often implemented with the pivot point mounted on a cart that can move horizontally under control of an electronic servo system as shown in the photo; this is called a cart and pole apparatus. Most applications limit the pendulum to 1 degree of freedom by affixing the pole to an axis of rotation. Whereas a normal pendulum is stable when hanging downwards, an inverted pendulum is inherently unstable, and must be actively balanced in order to remain upright; this can be done either by applying a torque at the pivot point, by moving the pivot point horizontally as part of a feedback system, changing the rate of rotation of a mass mounted on the pendulum on an axis parallel to the pivot axis and thereby generating a net torque on the pendulum, or by oscillating the pivot point vertically. A simple demonstration of moving the pivot point in a feedback system is achieved by balancing an upturned broomstick on the end of one's finger. \n\nA second type of inverted pendulum is a tiltmeter for tall structures, which consists of a wire anchored to the bottom of the foundation and attached to a float in a pool of oil at the top of the structure that has devices for measuring movement of the neutral position of the float away from its original position.\n\nA pendulum with its bob hanging directly below the support pivot is at a stable equilibrium point; there is no torque on the pendulum so it will remain motionless, and if displaced from this position will experience a restoring torque which returns it toward the equilibrium position. A pendulum with its bob in an inverted position, supported on a rigid rod directly above the pivot, 180Â° from its stable equilibrium position, is at an unstable equilibrium point. At this point again there is no torque on the pendulum, but the slightest displacement away from this position will cause a gravitation torque on the pendulum which will accelerate it away from equilibrium, and it will fall over. \nIn order to stabilize a pendulum in this inverted position, a feedback control system can be used, which monitors the pendulum's angle and moves the position of the pivot point sideways when the pendulum starts to fall over, to keep it balanced. The inverted pendulum is a classic problem in dynamics and control theory and is widely used as a benchmark for testing control algorithms (PID controllers, state space representation, neural networks, fuzzy control, genetic algorithms, etc.). Variations on this problem include multiple links, allowing the motion of the cart to be commanded while maintaining the pendulum, and balancing the cart-pendulum system on a see-saw. The inverted pendulum is related to rocket or missile guidance, where the center of gravity is located behind the center of drag causing aerodynamic instability. The understanding of a similar problem can be shown by simple robotics in the form of a balancing cart. Balancing an upturned broomstick on the end of one's finger is a simple demonstration, and the problem is solved by self-balancing personal transporters such as the Segway PT, the self-balancing hoverboard and the self-balancing unicycle.\n\nAnother way that an inverted pendulum may be stabilized, without any feedback or control mechanism, is by oscillating the pivot rapidly up and down. This is called Kapitza's pendulum. If the oscillation is sufficiently strong (in terms of its acceleration and amplitude) then the inverted pendulum can recover from perturbations in a strikingly counterintuitive manner. If the driving point moves in simple harmonic motion, the pendulum's motion is described by the Mathieu equation.\n\nThe equations of motion of inverted pendulums are dependent on what constraints are placed on the motion of the pendulum. Inverted pendulums can be created in various configurations resulting in a number of Equations of Motion describing the behavior of the pendulum.\n\nIn a configuration where the pivot point of the pendulum is fixed in space, the equation of motion is similar to that for an uninverted pendulum. The equation of motion below assumes no friction or any other resistance to movement, a rigid massless rod, and the restriction to 2-dimensional movement.\n\nWhere formula_2 is the angular acceleration of the pendulum, formula_3 is the standard gravity on the surface of the Earth, formula_4 is the length of the pendulum, and formula_5 is the angular displacement measured from the equilibrium position.\n\nWhen added to both sides, it will have the same sign as the angular acceleration term:\n\nThus, the inverted pendulum will accelerate away from the vertical unstable equilibrium in the direction initially displaced, and the acceleration is inversely proportional to the length. Tall pendulums fall more slowly than short ones.\n\nDerivation using torque and moment of inertia:\nThe pendulum is assumed to consist of a point mass, of mass formula_7, affixed to the end of a massless rigid rod, of length formula_4, attached to a pivot point at the end opposite the point mass.\n\nThe net torque of the system must equal the moment of inertia times the angular acceleration:\nThe torque due to gravity providing the net torque:\nWhere formula_11 is the angle measured from the inverted equilibrium position.\n\nThe resulting equation:\n\nThe moment of inertia for a point mass:\nIn the case of the inverted pendulum the radius is the length of the rod, formula_14.\n\nSubstituting in formula_15\n\nMass and formula_17 is divided from each side resulting in:\n\nAn inverted pendulum on a cart consists of a mass formula_19 at the top of a pole of length formula_14 pivoted on a horizontally moving base as shown in the adjacent image. The cart is restricted to linear motion and is subject to forces resulting in or hindering motion.\n\nThe essentials of stabilizing the inverted pendulum can be summarized qualitatively in three steps.\n1. If the tilt angle formula_21 is to the right, the cart must accelerate to the right and vice versa.\n\n2. The position of the cart formula_22 relative to track center is stabilized by slightly modulating the null angle (the angle error that the control system tries to null) by the position of the cart, that is, null angle formula_23 where formula_24 is small. This makes the pole want to lean slightly toward track center and stabilize at track center where the tilt angle is exactly vertical. Any offset in the tilt sensor or track slope that would otherwise cause instability translates into a stable position offset. A further added offset gives position control.\n\n3. A normal pendulum subject to a moving pivot point such as a load lifted by a crane, has a peaked response at the pendulum radian frequency of formula_25. To prevent uncontrolled swinging, the frequency spectrum of the pivot motion should be suppressed near formula_26. The inverted pendulum requires the same suppression filter to achieve stability.\n\nNote that, as a consequence of the null angle modulation strategy, the position feedback is positive, that is, a sudden command to move right will produce an initial cart motion to the left followed by a move right to rebalance the pendulum. The interaction of the pendulum instability and the positive position feedback instability to produce a stable system is a feature that makes the mathematical analysis an interesting and challenging problem.\n\nThe equations of motion can be derived using Lagrange's equations. We refer to the drawing to the right where formula_27 is the angle of the pendulum of length formula_28 with respect to the vertical direction and the acting forces are gravity and an external force \"F\" in the x-direction. Define formula_29 to be the position of the cart. The Lagrangian formula_30 of the system is:\n\nwhere formula_32 is the velocity of the cart and formula_33 is the velocity of the point mass formula_34.\nformula_32 and formula_33 can be expressed in terms of x and formula_5 by writing the velocity as the first derivative of the position;\nSimplifying the expression for formula_33 leads to:\n\nThe Lagrangian is now given by:\nand the equations of motion are:\n\nsubstituting formula_45 in these equations and simplifying leads to the equations that describe the motion of the inverted pendulum:\nThese equations are nonlinear, but since the goal of a control system would be to keep the pendulum upright the equations can be linearized around formula_48.\n\nOftentimes it is beneficial to use Newton's Second Law instead of Lagrange's equations because Newton's equations give the reaction forces at the joint between the pendulum and the cart. These equations give rise to two equations for each body one in the x-direction and the other in the y-direction. The equations of motion of the cart are shown below where the LHS is the sum of the forces on the body and the RHS is the acceleration.\n\nIn the equations above formula_51 and formula_52 are reaction forces at the joint. formula_53 is the normal force applied to the cart. This second equation only depends on the vertical reaction force thus the equation can be used to solve for the normal force. The first equation can be used to solve for the horizontal reaction force. In order to complete the equations of motion, the acceleration of the point mass attached to the pendulum must be computed. The position of the point mass can be given in inertial coordinates as\n\nTaking two derivatives yields the acceleration vector in the inertial reference frame.\n\nThen, using Newton's second law, two equations can be written in the x-direction and the y-direction. Note that the reaction forces are positive as applied to the pendulum and negative when applied to the cart. This is due to Newton's Third Law.\n\nThe first equation allows yet another way to compute the horizontal reaction force in the event the applied force formula_58 is not known. The second equation can be used to solve for the vertical reaction force. The first equation of motion is derived by substituting formula_59 into formula_60 which yields\n\nBy inspection this equation is identical to the result from Lagrange's Method. In order to obtain the second equation the pendulum equation of motion must be dotted with a unit vector which runs perpendicular to the pendulum at all times and is typically noted as the x-coordinate of the body frame. In inertial coordinates this vector can be written using a simple 2-D coordinate transformation\n\nThe pendulum equation of motion written in vector form is formula_63. Dotting formula_64 with both sides yields the following on the LHS (note that a transpose is the same as a dot product)\n\nIn the above equation the relationship between body frame components of the reaction forces and inertial frame components of reaction forces is used. The assumption that the bar connecting the point mass to the cart is massless implies that this bar cannot transfer any load perpendicular to the bar. Thus, the inertial frame components of the reaction forces can be written simply as formula_66 which signifies that the bar can only transfer loads along the axis of the bar itself. This gives rise to another equation which can be used to solve for the tension in the rod itself\n\nThe RHS of the equation is computed similarly by dotting formula_64 with the acceleration of the pendulum. The result (after some simplification) is shown below.\n\nCombining the LHS with the RHS and dividing through by m yields\n\nwhich again is identical to the result of Lagrange's method. The benefit of using Newton's method is that all reaction forces are revealed to ensure that nothing will be damaged.\nAn inverted pendulum in which the pivot is oscillated rapidly up and down can be stable in the inverted position. This is called Kapitza's pendulum, after Russian physicist Pyotr Kapitza who first analysed it. The equation of motion for a pendulum connected to a massless, oscillating base is derived the same way as with the pendulum on the cart. The position of the point mass is now given by:\nand the velocity is found by taking the first derivative of the position:\n\nThe Lagrangian for this system can be written as:\nand the equation of motion follows from:\nresulting in:\nIf \"y\" represents a simple harmonic motion, formula_76, the following differential equation is:\n\nThis equation does not have elementary closed-form solutions, but can be explored in a variety of ways. It is closely approximated by the Mathieu equation, for instance, when the amplitude of oscillations are small. Analyses show that the pendulum stays upright for fast oscillations. The first plot shows that when formula_78 is a slow oscillation, the pendulum quickly falls over when disturbed from the upright position. The angle formula_5 exceeds 90Â° after a short time, which means the pendulum has fallen on the ground. If formula_78 is a fast oscillation the pendulum can be kept stable around the vertical position. The second plot shows that when disturbed from the vertical position, the pendulum now starts an oscillation around the vertical position (formula_81). The deviation from the vertical position stays small, and the pendulum doesn't fall over.\nAchieving stability of an inverted pendulum has become a common engineering challenge for researchers. There are different variations of the inverted pendulum on a cart ranging from a rod on a cart to a multiple segmented inverted pendulum on a cart. Another variation places the inverted pendulum's rod or segmented rod on the end of a rotating assembly. In both, (the cart and rotating system) the inverted pendulum can only fall in a plane. The inverted pendulums in these projects can either be required to only maintain balance after an equilibrium position is achieved or be able to achieve equilibrium by itself. Another platform is a two-wheeled balancing inverted pendulum. The two wheeled platform has the ability to spin on the spot offering a great deal of maneuverability. Yet another variation balances on a single point. A spinning top, a unicycle, or an inverted pendulum atop a spherical ball all balance on a single point. As derived above the inverted pendulum can also be achieved by having a vertically oscillating base.\n\nArguably the most prevalent example of a stabilized inverted pendulum is a human being. A person standing upright acts as an inverted pendulum with his feet as the pivot, and without constant small muscular adjustments would fall over. The human nervous system contains an unconscious feedback control system, the sense of balance or righting reflex, that uses proprioceptive input from the eyes, muscles and joints, and orientation input from the vestibular system consisting of the three semicircular canals in the inner ear, and two otolith organs, to make continual small adjustments to the skeletal muscles to keep us standing upright. Walking, running, or balancing on one leg puts additional demands on this system. Certain diseases and alcohol or drug intoxication can interfere with this reflex, causing dizziness and disequilibration, an inability to stand upright. A field sobriety test used by police to test drivers for the influence of alcohol or drugs, tests this reflex for impairment. \n\nSome simple examples include balancing brooms or meter sticks by hand. \nThe inverted pendulum has been employed in various devices and trying to balance an inverted pendulum presents a unique engineering problem for researchers. The inverted pendulum was a central component in the design of several early seismometers due to its inherent instability resulting in a measurable response to any disturbance.\n\nThe inverted pendulum model has been used in some recent personal transporters, such as the two-wheeled self-balancing scooters and single-wheeled electric unicycles. These devices are kinematically unstable and use an electronic feedback servo system to keep them upright.\n\n\n\n\n"}
{"id": "44817256", "url": "https://en.wikipedia.org/wiki?curid=44817256", "title": "Keel (unit)", "text": "Keel (unit)\n\nKeel was a unit used to measure coal in the northeast of England, being the quantity of coal carried by a keelboat on the Tyne and Wear rivers. In 1750 it was said to be equal to 8 Newcastle chaldrons (waggons), a measure of volume, or a weight of 21 tons 4 cwt (approx. 21.5 metric tons).\n\n"}
{"id": "11543195", "url": "https://en.wikipedia.org/wiki?curid=11543195", "title": "Kudurrus of Isin (Babylonian) king Marduk-nadin-ahhe (ca 1099-1082 BC)", "text": "Kudurrus of Isin (Babylonian) king Marduk-nadin-ahhe (ca 1099-1082 BC)\n\nThe Kudurrus of Isin (Babylonian Kingdom), king Marduk-nadin-ahhe, late 2nd millennium BC, c. 1099-1082 BC.\n\nThe \"British Museum kudurru\" is a black limestone boundary stone (kudurru) of Marduk-nadin-ahhe, who ruled ca. 1100 BC â 1082 BC in Dynasty IV of Babylon.\n\nSome kudurrus are known for their representations of the king, etc., who consigned it. Most kudurrus represent Mesopotamian gods, which are often displayed graphically in segmented registers on the stone.\n\nThe \"Marduk-nadin-ahhe kudurru\" shows the king standing in royal garb, holding a bow and two arrows. Above his portrayal is one register displaying the gods represented on the boundary stone contract. A caption attests that he is the \"Avenger of the People\".\n\nThe obverse of the kudurru with King Marduk-nadin-ahhe is separated from the reverse by part of a snake, and a text in cuneiform comprises the reverse of the kudurru.\n\nWalters museum no. 2110 is a damaged kudurru, but fortunately parts in exquisite shape, as well as legible cuneiform text.\n\nA deed (kudurru) recording the purchase of five of corn-land by Marduk-nasir, the king's officer. The kudurru appears to be a flat, inscribed stone type.\n\nThe British Museum kudurru, Land grant to Adad-zer-iqiÅ¡a, is a large, stone kudurru, in fine shape, with multiple images of gods, and their iconic graphic symbols; the gods, and symbols encircle the top, approximately cone, circle-shaped top of the kudurru. Cuneiform is written amongst the graphic symbols of the upper cone. An extensive cuneiform script, below an encircling register line, around the top cone, tells the story of the kudurru.\n\n\n"}
{"id": "43034743", "url": "https://en.wikipedia.org/wiki?curid=43034743", "title": "List of green political parties", "text": "List of green political parties\n\nThis is a list of parties in the world that consider themselves to be upholding the principles and values of green politics. Some are also members of the Global Greens, the European Green Party, the Nordic Green Left Alliance or other international organizations. Note that, in some cases, a party's self-described adherence to environmentalism may be disputed by its critics.\n\n\n\n\n\n\n\n\n\n\nThe Uttarakhand Parivartan Party (UKPP) - directly translated in English as the \"Uttarakhand Transformation Party\", is the first Green party in India and is registered in the state of Uttarakhand.\nUKPP was formed on January 18, 2009 after two years of grassroots deliberations. The party fielded two candidates in the 2009 parliamentary elections and 15 candidates in the 2012 state assembly elections. UKPP is raising awareness about Green politics among the masses, although yet to win representation in government.\n\nThe India Greens - The Green Party of India was created in 2018 , they have held their National Convention on 17th November,2018 and have elected Mr. Suresh Nautiyal as their National President\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "22558639", "url": "https://en.wikipedia.org/wiki?curid=22558639", "title": "Marc Ona", "text": "Marc Ona\n\nMarc Ona Essangui is founder of the environmental NGO Brainforest and president of Environment Gabon, a network of NGOs. Marc Ona Essangui led efforts to expose agreements behind a Chinese mining project in Gabon, a country in West Central Africa, that threatened equatorial rainforest ecosystems. According to Ona Essangui, the proposed Belinga development, a $3.5 billion project, was secretly negotiated. Local communities were not consulted and are unaware of the effect that the project would have on their environment. Ona won the 2009 Africa Goldman Environmental Prize for his work. The project is currently on hold due to a lack of financing. In March 2013, Ona Essangui was sentenced to a six-month suspended prison sentence and an approximately $10,000 USD fine for defamation of Liban Soleman, senior advisor to President Ali Bongo Ondimba.\n"}
{"id": "50847639", "url": "https://en.wikipedia.org/wiki?curid=50847639", "title": "NASA X-57 Maxwell", "text": "NASA X-57 Maxwell\n\nThe NASA X-57 Maxwell is an experimental aircraft being developed by NASA, intended to demonstrate technology to reduce fuel use, emissions, and noise.\n\nThe experiment involves replacing the wings on a twin-engined Italian-built Tecnam P2006T (a conventional four-seater light aircraft) with Distributed electric propulsion (DEP) wings each containing electrically driven propellers. In 2015, test flights were planned to commence in 2017.\n\nThe first test phase uses an 18-engine truck-mounted wing. The second phase will install the cruise props and motors on a standard P2006T for ground- and flight-test experience. Phase 3 tests will involve the high-lift DEP wing and demonstrate increased high-speed cruise efficiency. The leading-edge nacelles will be fitted, but the high-lift props, motors and controllers will not be installed. Phase 4 adds the DEP motors and folding props to demonstrate lift-augmentation.\n\nThe \"Leading Edge Asynchronous Propeller Technology\" (\"LEAPTech\") project is a NASA project developing an experimental electric aircraft technology involving many small electric motors driving individual small propellers distributed along the edge of each aircraft wing. To optimize performance, each motor can be operated independently at different speeds, decreasing reliance on fossil fuels, improving aircraft performance and ride quality, and reducing aircraft noise.\n\nThe LEAPTech project began in 2014 when researchers from NASA Langley Research Center and NASA Armstrong Flight Research Center partnered with two California companies, Empirical Systems Aerospace (ESAero) in Pismo Beach and Joby Aviation in Santa Cruz, California. ESAero is the prime contractor responsible for system integration and instrumentation, while Joby is responsible for design and manufacture of the electric motors, propellers, and carbon fiber wing section.\n\nIn 2015, NASA researchers were ground testing a span, carbon composite wing section with 18 electric motors powered by lithium iron phosphate batteries.\nPreliminary testing up to 40 mph took place in January at Oceano County Airport on Californiaâs Central Coast.\nMounted on a specially modified truck, it was tested at up to 70 mph across a dry lakebed at Edwards Air Force Base later in 2015.\n\nThe experiment precedes the X-57 Maxwell X-plane demonstrator proposed under NASAâs Transformative Aeronautics Concepts program. A piloted X-plane should fly within a couple of years, after replacing a Tecnam P2006T wings and engines with an improved version of the LEAPTech wing and motors. Using an existing airframe will allow engineers to easily compare the performance of the X-plane with the original P2006T.\n\nThe X-57 project was publicly revealed by NASA Administrator Charles Bolden on 16 June 2016 in a keynote speech to the American Institute of Aeronautics and Astronautics (AIAA) at its Aviation 2016 exposition.\nNASA's first X-plane in over a decade, it is part of NASA's New Aviation Horizons initiative, which will also produce up to five larger-scale aircraft. The X-57 will be built by the agency's project, over a four-year development period at Armstrong Flight Research Center, California, with a first flight initially planned for 2017.\n\nIn July 2017, Scaled Composites was modifying a first P2006T to the X-57 Mod II configuration by replacing the piston engines with Joby Aviation electric motors, to fly early in 2018.\nMod III configuration will move the motors to the wingtips to increase propulsive efficiency.\nMod IV configuration will see the installation of the Xperimental, LLC high aspect ratio wing with 12 smaller props along its leading edge to augment its takeoff and landing aerodynamic lift.\n\nThe donor Tecnam P2006T was received in California in July 2016.\nIn a December 2016 test, a battery cell was shorted and the overheating spread to other cells, requiring the packaging to be redesigned from eight to 16 modules with aluminum honeycomb separators.\nThe Rotax 912s will be replaced by electric motors for the Mod II.\nThe Mod III weight target is from the P2006T and aims for 500% higher high-speed cruise efficiency as the higher wing loading will reduce cruise drag.\nThe Mod IV with 12 propellers to take off and land at the same speeds as the P2006T is yet unfunded.\n\nIn December 2017, the redesigned passively cooled battery module with 320 lithium-ion cell down from 640 passed testing.\nThe experience helped Electric Power Systems develop a battery for the Bye Aerospace Sun Flyer 2 which made its first flight in April 2018.\nJoby Aviation delivered three cruise motors in 2017, and was assembling the final pair in June 2018.\nMotor acceptance testing involving a 80-hr. endurance test was to be simplified before vehicle integration.\nContractor ES Aero will lead extensive ground-tests over months, culminating in a mission-like 30 min at full power test, before flying within 2019.\n\nBy September 2018, the first Joby Aviation JM-X57 electric cruise motor were mounted with controllers, batteries and new cockpit displays at Scaled Composites in Mojave, before flight tests in mid-2019.\nConstruction of the ESAero high aspect ratio, low drag composite wing was then almost finished, to fly the Mod 3 by mid-2020.\n\nModified from a Tecnam P2006T, the X-57 will be an electric aircraft, with 14 electric motors driving propellers mounted on the wing leading edges.\nAll 14 electric motors will be used during takeoff and landing, with only the outer two used during cruise.\nThe additional airflow over the wings created by the additional motors generates greater lift, allowing for a narrower wing.\nThe aircraft seats two.\nIt will have a range of and a maximum flight time of approximately one hour.\nThe X-57's designers hope to reduce by five-fold the energy necessary to fly a light aircraft at .\n\nDistributed propulsion increases the number and decreases the size of airplane engines. Electric motors are substantially smaller and lighter than jet engines of equivalent power. This allows them to be placed in different, more favorable locations. In this case, the engines are to be mounted above and distributed along the wings rather than suspended below them.\n\nThe propellers are mounted above the wing. They will increase the air flow over the wing at lower speeds, increasing its lift. The increased lift allows it to take operate on shorter runways. Such a wing could be only a third of the width of the wing it replaces, saving weight and fuel costs. Typical light aircraft wings are relatively large to prevent the craft from stalling (which happens at low airspeeds, when the wing cannot provide sufficient lift). Large wings are inefficient at cruising speed because they create excess drag. The wings will be optimised for cruise, with the engines protecting it from low-speed stalls and achieving the small aircraft standard of .\n\nThe speed of each propeller can be controlled independently, offering the ability to change the over-wing airflow pattern to cope with flying conditions, such as wind gusts. When cruising, the propellers closer to the fuselage could be folded back to further reduce drag, leaving those towards the wing tips to move the plane.\nSuch aircraft would have no in-flight emissions, operate with less noise and reduce operating costs by an estimated 30%.\nCruising efficiency was expected to increase 3.5-5-fold.\n\nThe span wing with an aspect ratio of 15 compares to and 8.8 for the stock P2006T wing, the slender wing's chord is at the wing root and at the tip.\nThe wing features 12 diameter cruise propellers that each require of motor power at and turn at 4,548 rpm. The five-blade props fold in cruise to reduce drag. Each wingtip hosts two 3-blade diameter cruise props that each require at and turn at 2,250 rpm. The wingtip location offers favorable interaction with the wingtip vortices, expected to provide a 5% drag saving.\nThe battery packs weight for a 47000/round0 Wh/kg density.\n\n\n"}
{"id": "17927720", "url": "https://en.wikipedia.org/wiki?curid=17927720", "title": "National Wind", "text": "National Wind\n\nNational Wind, LLC, A Trishe Group of Company, is a Minneapolis company founded in 2003 that is a developer of large-scale, community-based wind energy projects. The company, along with National Wind Assessments, has 50 employees based in Minneapolis, MN and Grand Forks, North Dakota. National Wind claims to be the nationâs leading developer of community-based wind farms.\n\nNational Wind operates under a community-based business model. Each wind project is structured as its own limited liability company (LLC) and is formed as a partnership between National Wind and local community members and landowners. Although National Wind is generally the developer and manager of its wind projects, it shares ownership of each project company with members of the communities where the wind farm is located. National Wind highlights the benefits of the community model, saying that community ownership benefits local economies along with the environment.\n\nNational Windâs business model differs from the structure of most other wind developers. Traditionally, when a company builds a wind farm, they become the sole owners of the development. The owners of the land where the farm is built tend to have minimal involvement with projects developer. In the traditional model, landowners do not own shares of the wind project, but instead are either paid a monthly lease or given royalties from the sale of electricity. On the other hand, National Wind's model aims to share both revenue and influence within local communities.\n\nNational Wind develops and manages wind farms that produce, at minimum, 50 megawatts of renewable energy. Most of the companyâs developments are located in the Midwestern United States in Iowa, Minnesota, North Dakota, South Dakota, Colorado, and Ohio. Currently, National Wind has twelve families of utility-scale wind projects in development or operation.\n\n\nNational Wind Assessments, a subdivision of National Wind LLC, specializes in the planning, permitting, and overall design of wind farms. Based in North Dakota, the assessment team analyzes potential wind farm sites using met tower installation, wind data acquisition, and environmental impact studies.\n\nThe demand for sustainable energy sources continues to bolster Americaâs wind energy industry. As more and more companies begin to develop wind farms, competition amongst industry leaders remains strong.\n\nAlthough National Wind claims to be the dominant community-based wind energy developer, it also faces competition from wind companies who follow a non-community business model. Its primary competitors include Florida Power and Light Company, Enxco, Invenergy LLC, and EcoEnergy LLC.\n\nOver the last few years, National Wind has garnered attention from a variety of local and national media outlets. On February 22, 2008, Minnesotaâs largest newspaper, the Star Tribune, featured National Wind on the cover of the business section. National Wind has also been featured in Twin Cities Business, the Lincoln Star, and various other local publications.\n\nCEO Leon Steinberg, representing one America's \"leading wind-energy developers,\" was also cited in a July 2008 US News and World Report article on the importance of the federal production tax credit for the growing industry.\n\n"}
{"id": "7587333", "url": "https://en.wikipedia.org/wiki?curid=7587333", "title": "Nitronate", "text": "Nitronate\n\nA nitronate (IUPAC: azinate) in organic chemistry is a functional group with the general structure . It is the anion of a nitronic acid, a tautomeric form of a nitro compound. Just as ketones and aldehydes exist in equilibrium with their enol tautomer in basic and acidic conditions, nitro compounds exist in equilibrium under basic conditions with their nitronate tautomer. The base deprotonates the Î± carbon, or the carbon directly attached to the nitrogen. The nitronate has two different resonance structures, one with a negative charge on the Î± carbon and a double bond between the nitrogen and one of the oxygens, and another resonance structure with a double bond between the nitrogen and the Î± carbon, and no double bond between the nitrogen and the oxygens. A nitronic acid is also called an aci form. In the Nef reaction nitronic acids are degraded to ketones. They can be alkylated on oxygen and used as a dipole in 1,3-dipolar cycloadditions.\n"}
{"id": "423640", "url": "https://en.wikipedia.org/wiki?curid=423640", "title": "Paperboard", "text": "Paperboard\n\nPaperboard is a thick paper-based material. While there is no rigid differentiation between paper and paperboard, paperboard is generally thicker (usually over 0.30Â mm, 0.012 in, or 12 points) than paper and has certain superior attributes such as foldability and rigidity. According to ISO standards, paperboard is a paper with a grammage above 250 g/m, but there are exceptions. Paperboard can be single- or multi-ply. \n\nPaperboard can be easily cut and formed, is lightweight, and because it is strong, is used in packaging. Another end-use is high quality graphic printing, such as book and magazine covers or postcards. Paperboard is also used in fine arts for creating sculptures.\n\nSometimes it is referred to as \"cardboard\", which is a generic, lay term used to refer to any heavy paper pulpâbased board, however this usage is deprecated in the paper, printing and packaging industries as it does not adequately describe each product type.\n\nIn 1817, the first paperboard carton was produced in England. Folding cartons first emerged around the 1860s and were shipped flat to save space, ready to be set up by customers when they were required. 1879 saw the development of mechanical die cutting and creasing of blanks. In 1911 the first kraft sulphate mill was built in Florida. In 1915 the gable top milk carton was patented and in 1935 the first dairy plant was observed using them. Ovenable paperboard was introduced in 1974.\n\nTerminology and classifications of paperboard are not always uniform. Differences occur depending on specific industry, locale, and personal choice. In general, the following are often used:\n\nFibrous material is turned into pulp (paper)/pulp and bleaching of wood bleached, to create one or more layers of board, which can be optionally coated for a better surface and/or improved visual appearance. pulp board are produced on pulping machines that can handle higher grammage and several plies.\n\nThe above-mentioned fibrous material can either come from fresh (virgin) sources (e.g. wood) or from recycled waste paper. Around 90% of virgin paper is made from wood pulp.\nToday paperboard packaging in general, and especially products from certified sustainable sources, are receiving new attention, as manufacturers dealing with environmental, health, and regulatory issues look to renewable resources to meet increasing demand. It is now mandatory in many countries for paper-based packaging to be manufactured wholly or partially from recycled material.\nRaw materials include:\n\n\n\nTwo principal methods for extracting fibres from their sources are:\n\n\nPulp used in the manufacture of paperboard can be bleached to decrease colour and increase purity. Virgin fibre pulp is naturally brown in colour, because of the presence of lignin. Recycled paperboard may contain traces of inks, bonding agents and other residue which colors it grey.\nAlthough bleaching is not necessary for all end-uses, it is vital for many graphical and packaging purposes. There are various methods of bleaching, which are used according to a number of factors for example, the degree of colour change required, chemicals chosen and method of treatment. There are three categories of bleaching methods:\n\n\nMulti-ply paperboard generally has higher creasing and folding performance than single-ply as a result of layering different types of pulp into a single product. In cases where the same kind of pulp is being used in several layers, each separate layer is treated and shaped individually in order to create the highest possible quality.\n\nIn order to improve whiteness, smoothness and gloss of paperboard, one or more layers of coating is applied. Coated paper is usually made up of:\n\n\nAdditional components could be OBA (optical brightening agents).\n\nThe DIN Standard 19303 \"Paperboard - Terms and grades\" (Publication date : 2005-09) defines different grades of paperboard based on the surface treatment (first letter), the main furnish (second letter) and the colour (non-D grade) or bulk (D grade only) (numbering).\n\nExample: GC1 would be a \"pigment coated\", \"virgin mechanical pulp\" board with a \"white reverse side\". Often the used paperboard type would be FBB, which was coated on both sides.\n\nBasis Weight (US): Is the weight in of paperboard.\n\nBrightness: Brightness is a technical term that is defined as the amount of blue-white light that a paper reflects. This property is very subjective and individual to each buyer and end use, as skin colour and food are better reproduced on âwarmâ (yellow) whites and not blue whites.\n\nGrammage: The grammage of the paperboard is assessed in accordance ISO 536.\nGrammage expresses mass per unit area and is measured in g/m.\n\nPH: Surface pH is measured on a water extract and is on a scale of 0â14. 0 is acidic, 7 is neutral and 14 is alkaline.\n\nStiffness: Stiffness is one of the most important properties of paperboard as it affects the ability of cartons to run smoothly through the machine that erects, fills and closes them. Stiffness also gives strength and reduces the propensity of a carton to bulge under the weight of settling flowable contents such as cereals.\n\nAlthough most paper strength properties increase with increasing sheet density, stiffness does not. A rule of thumb is that stiffness is proportional to the 1.6 power of sheet caliper.\n\nThe species of fiber used has an effect on stiffness, other things being equal. Northern softwood species impart superior stiffness compared to southern softwoods.\n\nOther factors which affect board stiffness include coatings and moisture content.\n\nSmoothness: Smoothness is particularly important when being used for printing, the smoother the paperboard, the better the image quality, because of better ink coverage. Smoothness is measured using air leak methods â the greater the rate of air leakage, at a specific air pressure, from under a cylindrical knife placed on the surface, the rougher the surface.\n\nCaliper/Thickness: In the United States caliper is usually expressed in thousandths of an inch (0.001â) or points, where a sheet of paperboard with a thickness of 0.024â would be 24 points. In Europe it is often sold in g/m, however the thickness of the board is measured in micron (Î¼m).\n\nPaperboard also tends to be referred to with thickness rather than weight.\n\nWhiteness: It refers ideally to the equal presence of all colours, because a truly white sheet will reflect all wavelengths of visible light equally.\n\nThe paperboard sector is mainly looked at in conjunction with the paper industry. The Paper & Paperboard market size (2007) had a value of 630.9 billion USD and a volume of 320.3 million metric tons. Of that market 40.1% is European. About 50% of all produced paper is used for packaging, followed by printing and writing. According to ProCarton, the consumption of paper and paperboard seem to correlate with economic trends (GDP). Sales of carton in Europe sum up to around 8 billion Euros worth. Over 1,100 printers produce 5.4 million tonnes of cartonboard yearly. Cartons make up one third of paper and board packaging and 15% of all packaging. A bit more than half (54%) of the European carton is produced using recovered fibre or waste paper.\nThe paper and paperboard industry is quite energy and capital intensive. Just a coated board machine itself can cost around 90 - 120 million Euros\n(about 125 - 166 million USD in 11/2011). Economies of scale apply, because of which a few large players often dominate the market place. E.g. in North America the top 5 producers have a market share of 85%.\n\n\n\n"}
{"id": "1251318", "url": "https://en.wikipedia.org/wiki?curid=1251318", "title": "Plasma window", "text": "Plasma window\n\nThe plasma window (not to be confused with a \"plasma shield\") is a technology that fills a volume of space with plasma confined by a magnetic field. With current technology, this volume is quite small and the plasma is generated as a flat plane inside a cylindrical space.\n\nPlasma is any gas that has had its atoms or molecules ionized and is a separate phase of matter. This is most commonly achieved by heating the gas to extremely high temperatures, although other methods also exist. Plasma becomes increasingly viscous at higher temperatures, to the point where other matter has trouble passing through.\n\nA plasma window's viscosity allows it to separate gas at standard atmospheric pressure from a total vacuum, and can reportedly withstand a pressure difference of up to nine atmospheres. At the same time, the plasma window will allow radiation such as lasers and electron beams to pass. This property is the key to the plasma window's usefulness â the technology of the plasma window permits for radiation that can only be generated in a vacuum to be applied to objects in an atmosphere. Electron-beam welding is a major application of plasma windows, making EBW practical outside a hard vacuum.\n\nThe plasma window was invented at Brookhaven National Laboratory by Ady Hershcovitch and patented in 1995.\n\nFurther inventions using this principle include the plasma valve in 1996.\n\nA related technology is the plasma valve, invented shortly after the plasma window. A plasma valve is a layer of gas in the shell of a particle accelerator. The ring of a particle accelerator contains a vacuum, and ordinarily a breach of this vacuum is disastrous. If, however, an accelerator equipped with plasma valve technology breaches, the gas layer is ionized within a nanosecond, creating a seal that prevents the accelerator's recompression. This gives researchers time to shut off the particle beam in the accelerator and slowly recompress the accelerator ring to avoid damage.\n\nThe physical properties of the plasma window vary depending on application. The initial patent cited temperatures around .\n\nThe only limit to the size of the plasma window are current energy limitations as generating the window consumes around 20 kilowatts per inch (8Â kW/cm) in the diameter of a round window.\n\nThe plasma window emits a bright glow, with the color being dependent on the gas used.\n\nIn science fiction, such as the television series \"Star Trek\", a fictional technology known as the \"force field\" is often used as a device. In some cases it is used as an external \"door\" to hangars on spacecraft, to prevent the ship's internal atmosphere from venting into outer space. Plasma windows could theoretically serve such a purpose if enough energy were available to produce them. The StarTram proposal plans on use of a power-demanding MHD window over a multi-meter diameter launch tube periodically, but briefly at a time, to prevent excessive loss of vacuum during the moments when a mechanical shutter temporarily opens in advance of a hypervelocity spacecraft.\n\n\n\n\n"}
{"id": "28228267", "url": "https://en.wikipedia.org/wiki?curid=28228267", "title": "Prodema", "text": "Prodema\n\nProdema is a wood based material used for the interior and exterior of buildings.It is made up of natural wood with a bakelite core. Also known as a composite wooden panel, it has been in use since the early 1900s.\nIt is renowned for its durability and versatility, ranging in thickness from 6-22mm. Due to its composite properties, Prodema is often used for insulation to structures.\n\nProdema can be adapted to many forms, including flooring, walls, exterior of buildings, archways, doors, ceilings, furniture. As it comes in panel form, the wood is treated with advanced machinery and chemicals to provide a longer life and supported with a selection of metals such as aluminium, stainless steel, and iron.\n\nProdema is a composite panel faced with a natural wood veneer and coated with a proprietary coating,\nbased on synthetic resins and PVDF, which protect the panel from the effects of sunlight, chemical\nattack (anti-graffiti) and the damage caused by atmospheric agents.\n"}
{"id": "11291216", "url": "https://en.wikipedia.org/wiki?curid=11291216", "title": "Renewable energy in Asia", "text": "Renewable energy in Asia\n\nRenewable energy is a viable means of generating energy in Asia.\n\nFor solar power, South Asia has the ideal combination of both high solar insolation and a high density of potential customers.\n\nCheap solar can bring electricity to a major chunk of subcontinent's people who still live off-grid, bypassing the need of installation of expensive grid lines. Also since the costs of energy consumed for temperature control squarely influences a regions energy intensity, and with cooling load requirements roughly in phase with the sun's intensity, cooling from intense solar radiation could make perfect energy-economic sense in the subcontinent.\n\nIn Bangladesh, biomass, hydro and solar are the main sources of renewable energy and altogether these sources contribute about 60% of the nation's primary energy supply. A number of domestic solar energy systems are in use in houses around the country. The use of solar energy on this scale is highly potential and advantageous as more than 60% of areas in the country do not have access to main grid electricity. The World Bank is backing a program of making solar energy available to wider population in Bangladesh, as part of the Rural Electrification and Renewable Energy Development Project (REREDP), which subsidizes solar energy systems.\n\nA typical 'solar home system' can power two to eight 'low energy' lights, plus a socket for TV, radio or battery recharging, and a mobile telephone charging unit, too. Each system consists of a solar photovoltaic panel, mounted on the house roof. Depending on its size, this provides between 40W and 135W of electricity in full sunlight (the most common being 50W).\n\nGrameen Shakti is the largest organization installing rural based solar home system (SHS) in Bangladesh. Other companies working on similar solar energy based SHS are Rural Services Foundation (RSF), Brac, Hilfulfujal and so on. The model of micro finance based SHS is now being copied in other parts of the world as a successful business model.\n\nRahimafrooz is a major supplier of high quality solar batteries and other solar components for the program. Rahimafrooz Renewable Energy Ltd (RRE) has been the pioneer in installing solar powered centralized systems, water pumps for irrigation and pure drinking water, water heaters, street lights, and solar-powered telecom solutions to various organizations. They are working closely with pertinent government organizations in installing solar powered medical refrigerator that provides emergency live saving medicines in the off-grid rural areas.\n\nA company named Digital Technology is doing research and development of solar PV products like solar billboard lighting, mini grid system for irrigation etc.\n\nIn China there now are six factories producing at least 2 GW/year each of monocrystalline, poly-crystalline and non-crystalline Photovoltaic cells. These factories include the LDK Solar Co, Wuxi Suntech Solar Energy Co., Ltd., which produces approximately 50 MW/year of solar cells and photovoltaic modules; the Yunnan Semi-conductor Parts Plant, which manufactures approximately 2 MW/year of mono-crystalline cells; the Baoding Yingli Solar Energy Modules Plant, which manufactures approximately 6 MW/year of polycrystalline cells and modules; the Shanghai Jiaoda Guofei Solar Energy Battery Factory, which produces approximately 1 MW/year of modules; and the Shanghai PV Science and Technology Co., Ltd., which produces approximately 5 MW/year of modules.\n\nChina has become a world leader in the manufacture of solar photovoltaic technology, with its six biggest solar companies having a combined value of over $15 billion. Around 820 megawatts of solar PV were produced in China in 2007, second only to Japan. Suntech Power Holdings Co based in Jiangsu, is the world's third- biggest supplier of solar cells.\n\nThere are some obstacles to the further development of the Chinese solar energy sector that China faces. These obstacles include the lack of a nationwide comprehensive photovoltaic (PV) plan, the lack of updated facilities and sufficient financial resources to support PV research at research institutes, the lack of sufficient facilities and resources at companies manufacturing PV products, the failure of companies to be able to produce high quality, reliable and low cost PV products and the relatively weak educational and training opportunities in China for PV science and technology.\n\nAbout 50 MW of installed solar capacity was added in 2008, more than double the 20 MW in 2007, but still a relatively small amount. According to some studies, the demand in China for new solar modules could be as high as 232 MW each year from now on until 2012. The government has announced plans to expand the installed capacity to 1,800 MW by 2020. If Chinese companies manage to develop low cost, reliable solar modules, then the sky is the limit for a country that is desperate to reduce its dependence on coal and oil imports as well as the pressure on its environment by using renewable energy.\n\nIn 2009 centre to the PRC Governmentâs plans is the recently announced \"Golden Sun\" stimulus program. Under this program the Ministry of Finance will subsidize half of the total construction\ncosts of an on-grid solar power plant, including transmission expenses. The Ministry of Finance will also pay subsidies of up to 70% to develop independent photovoltaic power generating systems in remote regions. The strong handed move by the Government is meant to encourage more solar projects to increase the current solar power capacity, which at 2008 stood at a paltry 40MW. As the Government targets to increase Chinaâs solar power capacity up to 20GW by 2020, this will provide significant opportunities for solar cell and module manufacturers. Many of the solar industry players therefore will expect for chances to be benefited from the government programs especially the solar cell manufacturers. With the hope of increase in local demand, some of the new developments have been going on with this region, like Anwell Technologies Limited, a Singapore listed company having its solar cell manufacturing plant in China, has produced its first thin film solar panel with its own developed production lines in September 2009.\n\nAccording to the speech given by the Chinese President Hu Jintao's at the UN climate summit held on September 22, 2009 in New York, China will intensify effort and adopt ambitious plans to plant enough forest to cover an area the size of Norway and use 15 percent of its energy from renewable sources within a decade.\n\nIndia is both densely populated and has high solar insolation, providing an ideal combination for solar power in India. Much of the country does not have an electrical grid, so one of the first applications of solar power has been for water pumping, to begin replacing India's four to five million diesel powered water pumps, each consuming about 3.5Â kilowatts, and off-grid lighting. Some large projects have been proposed, and a 35,000Â kmÂ² area of the Thar Desert has been set aside for solar power projects, sufficient to generate 700 to 2,100Â gigawatts.\n\nThe Indian Solar Loan Programme, supported by the United Nations Environment Programme has won the prestigious Energy Globe World award for Sustainability for helping to establish a consumer financing program for solar home power systems. Over the span of three years more than 16,000 solar home systems have been financed through 2,000 bank branches, particularly in rural areas of South India where the electricity grid does not yet extend.\n\nLaunched in 2003, the Indian Solar Loan Programme was a four-year partnership between UNEP, the UNEP Risoe Centre, and two of India's largest banks, the Canara Bank and Syndicate Bank.\n\nAccording to Development Counsellors International (DCI), a United States marketing company, India is the second best country, after China, for business investment. United Nations Environment Programme (UNEP) has reported that India has seen a 12% increase in investment in the renewable energy sector with an investment of $3.7 billion in 2008. The largest share was asset finance at $3.2 billion which grew by 25%. The clean renewable energy includes wind, solar, biomass and small-hydro projects. The major portion of investment has been made in wind energy sector. The investment in wind energy sector grew at 17% from $2.2 billion to $2.6 billion.\n\nJapan currently produces about 10% of its electricity from renewable sources. The renewable share goal is 20% by 2020.\n\nSolar power in Pakistan discusses the generation and development of electricity via solar thermal or photovoltaic technology in that country. The country has solar plants in Pakistani Kashmir, Punjab, Sindh and Balochistan. Initiatives are under development by the International Renewable Energy Agency, the Japan International Cooperation Agency, Chinese companies, and Pakistani private sector energy companies. The country aims to build the world's largest solar power park, the Quaid-e-Azam Solar Power Park (QASP) in the Cholistan Desert, Punjab, by 2017 with a 1 GW capacity. A plant of this size would be enough to power around 320,000 homes.\n\nOn May 29, 2012, Pakistan inaugurated its first solar power on-grid power plant in Islamabad. Introduction of Clean Energy by Solar Electricity Generation System is a special grant aid project by the Japan International Cooperation Agency (JICA) under the Coolio Earth Partnership. This project includes the installation of two 178 kW photovoltaic (PV) systems at the premises of the Planning Commission and Pakistan Engineering Council.\n\nThis is the first on-grid solar PV project that employs net-metering, thereby allowing the beneficiaries to sell surplus electricity to the Islamabad Electric Supply Company (IESCO), the electricity distribution company of the Islamabad Division. The project was executed with grant assistance, worth 480 million Yen (approx. 553.63 million Pakistani Rupees) over three years commencing in 2010.\n\nAviation Enclave Karachi installed the first high quality integrated solar energy system with a 15 kW power generation capacity capable of grid tie-in at, Aviation Enclave Karachi in Sep 2016. It was a pilot project for Central Facilitation Agency & Central Builders & Developers \n\nBeaconhouse installed the second high quality integrated solar energy system with a 10 kW power generation capacity capable of grid tie-in at Beaconhouse Canal Side Campus, Lahore. It was a pilot project for BSS designed by U.S. consultants, based upon feasibility by the U.S. Trade and Development Agency (USTDA).\n\n50 to 100 MW of photovoltaics is expected to be installed in 2013, and at least 300 MW in 2014. In May 2015, 100 MW of a planned 1,000 MW were installed in the Quaid-e-Azam Solar Park.\n\nSolar irradiance in Pakistan is 5.3 kWh/mÂ²/day. Pakistan set a target to add approximately 10 GW of renewable capacity by 2030 in addition to replacing 5% diesel with biodiesel by 2015 and 10% by 2025.\n\nYear Installations in MWp Notes\nCumulative\nCapacity Added\nCapacity\n2014 400 Calculated back from 2015 added capacity data.\n2015 1,000 600 Preliminary data.\nRaja Pervaiz Ashraf, former Federal Minister of Water & Power announced on July 2, 2009 that 7,000 villages would be electrified using solar energy by 2014. Senior adviser Sardar Zulfiqar Khosa stated that the Punjab government would begin new projects aimed at power production through coal, solar energy and wind power; this would generate additional resources.\n\nThe Government of Pakistan allowed the provincial government of Sindh to conduct feasibility research. The government planned to install a desalination plant powered by solar energy.\n\nThe Philippine government sees the growth of the renewable energy sector essential for national energy security. The Philippines' fossil fuel sector is unsustainable, being dependent on the import of nonrenewable fuel, including petroleum, but has significant potential in the renewable energy sector. Based on a report of an Australian consulting firm, International Energy Consultants, the Philippines has the highest electricity rate in Asia, followed by Japan. Transmitting power and transporting fuel throughout the Philippine archipelago is problematic due to very high cost.\n\nThe Philippines could be considered a world leader in renewable energy, with 30 percent of its power generation being powered by the renewable energy sector. The Philippines is the world's second largest generator of geothermal energy and was the first Southeast Asian nation to invest in large-scale solar and wind technologies.\n\nIn 2008, South Korea came 4th in the list of installed PV capacity according to EPIA statistics as a result of the favorable feed-in tariff system with a cap of 500MW in 2008. According to Displaybank, the new âPV Market Creation Planâ announced in 2009 is expected to boost the Korean PV installment market to increase to 200MW by 2012. The government further announced plans to increase more than double its financing for renewable R&D projects to 3.5 trillion won ($2.9/Â£1.9bn) by 2013. The government also plans to expand its system of tax breaks to cover new technologies in solar such as wind and thermal power, low-emission vehicles and rechargeable batteries etc.\n\n"}
{"id": "39094032", "url": "https://en.wikipedia.org/wiki?curid=39094032", "title": "Robert L. Hunt", "text": "Robert L. Hunt\n\nRobert Leroy âBobâ Hunt (b. 1933) is a fisheries biologist. He grew up in McFarland, Wisconsin, on Lake Waubesa. He served in the US Army, then attended the University of Wisconsin-Madison, where he earned a bachelor's degree and a master's degree. He joined the Wisconsin Conservation Department in 1959. \nHe did pioneering research on wild trout conservation, and led efforts to save streams from damage done by cattle operations and human activities. He won numerous national and international awards for his research and conservation work. He wrote the book \"Trout Stream Therapy\" in 1993. He served as president of the Wisconsin chapter of the American Fisheries Society in 1973. He studied troutstream rehabilitation project and developed a system for evaluating the results of a project. His work showed that altering the stream habitat by modifying the creek banks with more overhanging cover could triple the weight of trout in a stream over a six year period.\n\nHe served on committees from the local level to the international level. Like his brother Dick Hunt, he was inducted into the Conservation Hall of Fame.\n"}
{"id": "34995370", "url": "https://en.wikipedia.org/wiki?curid=34995370", "title": "Rock Island Clean Line", "text": "Rock Island Clean Line\n\nThe Rock Island Clean Line is a proposed 500-mile high voltage direct current (HVDC) transmission line. The stated purpose of the line is to transport electrical power from wind energy dense states in the Midwestern United States (Iowa, Nebraska, South Dakota and Minnesota) to the energy markets in the Chicago, Illinois, and cities further east. The planned line would run from the Sanborn, Iowa area to near Morris, Illinois. The developer, Clean Line Energy Partners, is targeting an operational date of 2017.\n\nThe region where the line would start has large potential for wind generation, but currently already can produce all the electricity it can use or transport out of the region on existing transmission lines. The line would have the capacity to transport up to 3500 megawatts of power out of the region. An economic study for the line estimates that it could transmit the power of eight 500Â MW wind farms in the siouxland area.\n\nThe Federal Electricity Regulatory Commission approved the line in May 2012. In November 2014, the Illinois Commerce Commission approved it. , the Iowa Utilities Board has not yet approved it.\n\nOpposition to the Rock Island Clean Line is organized by Block RICL, which claims that the line will carry mostly non-wind energy (i.e., coal) and will use an unreasonably large amount of land.\n\n\n"}
{"id": "34162256", "url": "https://en.wikipedia.org/wiki?curid=34162256", "title": "SS Wafra oil spill", "text": "SS Wafra oil spill\n\nThe SS \"Wafra\" oil spill occurred on 27 February 1971, when SS \"Wafra\", an oil tanker, ran aground while under tow near Cape Agulhas, South Africa. Approximately 200,000 barrels of crude oil were leaked into the ocean. The larger part of the ship was refloated, towed out to sea, and then sunk by the South African Air Force to prevent further oil contamination of the coastline.\n\nThe \"Wafra\" left Ras Tanura in Saudi Arabia on 12 February 1971 bound for Cape Town, South Africa, with a cargo of 472,513 barrels (63,174 tonnes) of Arabian crude oil on board. Half the cargo was owned by Chevron Oil Sales Co., and the other half by Texaco Export, Inc.\n\nThe ship was rounding the southern tip of Africa at 6:30 am on 27 February 1971 when the piping that brought seawater on board to cool her steam turbine failed. The engine room flooded, incapacitating the ship. She was taken under tow the following day by the Russian steam tanker \"Gdynia\", which â finding the task too difficult â handed the tow over to the \"Pongola\" off Cape Agulhas, later the same day. The tow cable subsequently broke, and the \"Wafra\" grounded on a reef near Cape Agulhas at 5:30 pm on 28 February. All six of the port cargo tanks, as well as two of the six center tanks, were ruptured, resulting in approximately 26,000 tons of oil leaking at the grounding site, of which 6,000 tonnes washed up at Cape Agulhas. A by oil spill resulted, which affected a colony of 1200Â African penguins on Dyer Island near Gansbaai.\n\nThe ship was refloated and pulled off the reef on 8 March by the German tug , but started to break apart. To prevent further oil contamination of the coastline, the larger section was towed out to sea to the edge of the continental shelf (), leaving a oil slick in her wake. On 10 March 1971, Buccaneer aircraft of the South African Air Force attempted to sink her with AS-30 missiles, but succeeded only in starting a fire. The ship burned for two days before a Shackleton aircraft was eventually able to sink it with depth charges in of water.\n\nIf the \"Wafra\" had been a twin screw, two engine room ship, loss of an engine would most likely not have caused the loss of the whole ship. At the time, the oil spill was in the top twenty most disastrous tanker spills on record.\n\nIn the wake of the accident, the South African Department of Transport realised that despite many Very Large Crude Carriers (VLCCs) using the Cape sea route each year, the authorities did not have ocean-going tugs that were able to assist them in distress, and to protect sensitive marine areas by breaking up oil spills with chemical dispersants. They therefore set up an oil spill prevention service known as \"Kuswag\" (Coastwatch) and commissioned two new salvage tugs, the \"John Ross\" and \"Wolrade Woltemade\". The two tugs, with their engines, held the record as the world's largest salvage tugs.\n\n\n"}
{"id": "33053242", "url": "https://en.wikipedia.org/wiki?curid=33053242", "title": "Sakri solar plant", "text": "Sakri solar plant\n\nSakri Photovoltaic solar energy project is a 125Â MW solar photovoltaic power plant. The project was developed by Mahagenco in Shivajinagar in Sakri taluka of Dhule district in Maharashtra, India. Its cost was about Rs 20Â billion and it became operational in 2013. Completion was in March 2013.\n\nKfW, a German financial institution, has agreed to finance the project. Work has already started on 50Â MW (5x25MW) Solar Photovoltaic part while for remaining 100Â MW part, based on crystalline technology, international bids have been invited.\n"}
{"id": "18663291", "url": "https://en.wikipedia.org/wiki?curid=18663291", "title": "Sandwich plate system", "text": "Sandwich plate system\n\nSandwich Plate System (SPS) is a structural composite material composed of steel and polyurethane elastomer.\n\nSPS is used in engineered structures including ships, buildings, stadia, arenas and bridges and was invented by Dr Stephen Kennedy following his primary research in the field of ice strengthened structures at Carleton University in Ottawa and first patented in 1996.\n\nThe SPS technology is a direct replacement for stiffened steel and reinforced concrete in heavy engineering projects.\n\nThe first recorded project involving SPS was carried out on the P&O Pride of Cherbourg, a Lloydâs Register approved vessel, in 1999 and over 200 projects have been completed to date.\n\nSome recent buildings, stadia and bridges incorporating SPS include:\n\nAnd some recent vessels incorporating SPS include:\n\n"}
{"id": "43911629", "url": "https://en.wikipedia.org/wiki?curid=43911629", "title": "Seaweed cultivator", "text": "Seaweed cultivator\n\nA seaweed cultivator is a device which grows seaweed, usually in weekly or bi-weekly cycles. It is not to be confused with people who are seaweed cultivators themselves, usually performing manual cultivation of seaweed using large nets in open waters (mainly in Asian countries).\n\nSeaweed cultivators (the devices) are just now becoming available to the public, mainly because of the LED (light-emitting diode) which has made low-cost, reliable, and waterproof illumination sources (especially in the red 660-nanometer spectrum) feasible. Seaweed cultivators are an offshoot of algae scrubbers, which were developed to filter aquariums. By replacing the aquarium with a reservoir of fertilized seawater, a stand-alone cultivator becomes possible, and even a desktop cultivator is possible if the size is small enough or if the reservoir is placed beneath the desk.\n\nThe type of seaweed that can be grown in a seaweed cultivator is currently limited to green types, mostly Angel Hair and sea lettuce, because other types such as Nori require more complex control of the water temperature and other variables. Angel Hair and sea lettuce, however, can be grown at home at room temperature.\n"}
{"id": "938894", "url": "https://en.wikipedia.org/wiki?curid=938894", "title": "Sedimentation", "text": "Sedimentation\n\nSedimentation is the tendency for particles in suspension to settle out of the fluid in which they are entrained and come to rest against a barrier. This is due to their motion through the fluid in response to the forces acting on them: these forces can be due to gravity, centrifugal acceleration, or electromagnetism. In geology, sedimentation is often used as the opposite of erosion, i.e., the terminal end of sediment transport. In that sense, it includes the termination of transport by saltation or true bedload transport. Settling is the falling of suspended particles through the liquid, where as sedimentation is the termination of the settling process. In estuarine environments, settling can be influenced by the presence or absence of vegetation. Trees such as mangroves are crucial to the attenuation of waves or currents, promoting the settlement of suspended particles.\n\nSedimentation may pertain to objects of various sizes, ranging from large rocks in flowing water to suspensions of dust and pollen particles to cellular suspensions to solutions of single molecules such as proteins and peptides. Even small molecules supply a sufficiently strong force to produce significant sedimentation.\n\nThe term is typically used in geology to describe the deposition of sediment which results in the formation of sedimentary rock, but it is also used in various chemical and environmental fields to describe the motion of often-smaller particles and molecules. This process is also used in the biotech industry to separate cells from the culture media.\n\nIn a sedimentation experiment, the applied force accelerates the particles to a terminal velocity formula_1 at which the applied force is exactly canceled by an opposing drag force. For small enough particles (low Reynolds number), the drag force varies linearly with the terminal velocity, i.e., formula_2 (Stokes flow) where \"f\" depends only on the properties of the particle and the surrounding fluid. Similarly, the applied force generally varies linearly with some coupling constant (denoted here as \"q\") that depends only on the properties of the particle, formula_3. Hence, it is generally possible to define a sedimentation coefficient formula_4 that depends only on the properties of the particle and the surrounding fluid. Thus, measuring \"s\" can reveal underlying properties of the particle.\n\nIn many cases, the motion of the particles is blocked by a hard boundary; the resulting accumulation of particles at the boundary is called a sediment. The concentration of particles at the boundary is opposed by the diffusion of the particles.\n\nThe sedimentation of a single particle under gravity is described by the MasonâWeaver equation, which has a simple exact solution. The sedimentation coefficient \"s\" in this case equals formula_5, where formula_6 is the buoyant mass.\n\nThe sedimentation of a single particle under centrifugal force is described by the Lamm equation, which likewise has an exact solution. The sedimentation coefficient \"s\" also equals formula_5, where formula_6 is the buoyant mass. However, the Lamm equation differs from the MasonâWeaver equation because the centrifugal force depends on radius from the origin of rotation, whereas in the MasonâWeaver equation gravity is constant. The Lamm equation also has extra terms, since it pertains to sector-shaped cells, whereas the MasonâWeaver equation is one-dimensional.\n\nClassification of sedimentation:\n\nIn geology, sedimentation is the deposition of particles carried by a fluid flow. For suspended load, this can be expressed mathematically by the Exner equation, and results in the formation of depositional landforms and the rocks that constitute sedimentary record. An undesired increased transport and sedimentation of suspended material is called siltation, and it is a major source of pollution in waterways in some parts of the world. High sedimentation rates can be a result of poor land management and a high frequency of flooding events. If not managed properly, it can be detrimental to fragile ecosystems on the receiving end, such as coral reefs. Climate change also affects siltation rates.\n\nIn chemistry, sedimentation has been used to measure the size of large molecules (macromolecule), where the force of gravity is augmented with centrifugal force in an ultracentrifuge.\n\n"}
{"id": "32512530", "url": "https://en.wikipedia.org/wiki?curid=32512530", "title": "Severe thunderstorm outbreak", "text": "Severe thunderstorm outbreak\n\nA severe thunderstorm outbreak, also called a severe weather outbreak or simply a severe outbreak, is an event in which a weather system or combination of weather systems produces a multitude of severe thunderstorms in a region over a continuous span of time. A severe outbreak which is most notable for its tornadoes is called a tornado outbreak. The four kinds of severe weather produced in these outbreaks are tornadoes, severe wind, large hail, and flash flooding.\n\nA Tornado outbreak is the occurrence of multiple tornadoes in a region over a relatively short span of time. Usually, a tornado outbreak is the result of multiple supercells.\n\nA squall line (commonly abbreviated SQLN) is a line of thunderstorms, most or all of which have attained severe limits, traveling in an organized fashion. The greatest threats within a SQLN are damaging winds, large hail, and flash flooding, though tornadoes are possible.\n\nA derecho is a squall line which is long-lived and consistently produces damaging winds across its entire track. Derechos almost exclusively cause flash flooding and wind damage, which can be very severe.\n\nA mesoscale convective system is a mesoscale organized system which may produce severe weather along a relatively narrow area or path. The greatest threats in an mesoscale convective system are damaging winds, large hail, and flash flooding, though tornadoes are occasionally possible.\n\nA mesoscale convective vortex is a tropical cyclone-like, warm core, a feature which may develop in a squall line, derecho, or MCS. Severe MCVs can become what are essentially small tropical storms or hurricanes, and, can in fact become such cyclones. An MCV will often trail a squall line on its south side. The greatest threats in an MCV are (in the center of circulation and south of the center) extreme winds and (north of the center) flash flooding, in addition to tropical cyclone formation.\n"}
{"id": "18932369", "url": "https://en.wikipedia.org/wiki?curid=18932369", "title": "Shut-in (oil drilling)", "text": "Shut-in (oil drilling)\n\nIn the petroleum industry, shutting-in is the implementation of a production cap set lower than the available output of a specific site. This may be part of an attempt to constrict the oil supply or a necessary precaution when crews are evacuated ahead of a natural disaster.\n"}
{"id": "225256", "url": "https://en.wikipedia.org/wiki?curid=225256", "title": "Silicon carbide", "text": "Silicon carbide\n\nSilicon carbide (SiC), also known as carborundum , is a semiconductor containing silicon and carbon. It occurs in nature as the extremely rare mineral moissanite. Synthetic SiC powder has been mass-produced since 1893 for use as an abrasive. Grains of silicon carbide can be bonded together by sintering to form very hard ceramics that are widely used in applications requiring high endurance, such as car brakes, car clutches and ceramic plates in bulletproof vests. Electronic applications of silicon carbide such as light-emitting diodes (LEDs) and detectors in early radios were first demonstrated around 1907. SiC is used in semiconductor electronics devices that operate at high temperatures or high voltages, or both. Large single crystals of silicon carbide can be grown by the Lely method and they can be cut into gems known as synthetic moissanite. SiC with high surface area can be produced from SiO contained in plant material.\n\nNon-systematic, less-recognized and often unverified syntheses of silicon carbide include:\n\nWide-scale production is credited to Edward Goodrich Acheson in 1890. Acheson was attempting to prepare artificial diamonds when he heated a mixture of clay (aluminium silicate) and powdered coke (carbon) in an iron bowl. He called the blue crystals that formed \"carborundum\", believing it to be a new compound of carbon and aluminium, similar to corundum. In 1893, Ferdinand Henri Moissan discovered the very rare naturally occurring SiC mineral while examining rock samples found in the Canyon Diablo meteorite in Arizona. The mineral was named moissanite in his honor. Moissan also synthesized SiC by several routes, including dissolution of carbon in molten silicon, melting a mixture of calcium carbide and silica, and by reducing silica with carbon in an electric furnace.\n\nAcheson patented the method for making silicon carbide powder on February 28, 1893. Acheson also developed the electric batch furnace by which SiC is still made today and formed the Carborundum Company to manufacture bulk SiC, initially for use as an abrasive. In 1900 the company settled with the Electric Smelting and Aluminum Company when a judge's decision gave \"priority broadly\" to its founders \"for reducing ores and other substances by the incandescent method\". It is said that Acheson was trying to dissolve carbon in molten corundum (alumina) and discovered the presence of hard, blue-black crystals which he believed to be a compound of carbon and corundum: hence carborundum. It may be that he named the material \"carborundum\" by analogy to corundum, which is another very hard substance (9 on the Mohs scale).\n\nThe first use of SiC was as an abrasive. This was followed by electronic applications. In the beginning of the 20th century, silicon carbide was used as a detector in the first radios. In 1907 Henry Joseph Round produced the first LED by applying a voltage to a SiC crystal and observing yellow, green and orange emission at the cathode. Those experiments were later repeated by O. V. Losev in the Soviet Union in 1923.\n\nNaturally occurring moissanite is found in only minute quantities in certain types of meteorite and in corundum deposits and kimberlite. Virtually all the silicon carbide sold in the world, including moissanite jewels, is synthetic. Natural moissanite was first found in 1893 as a small component of the Canyon Diablo meteorite in Arizona by Dr. Ferdinand Henri Moissan, after whom the material was named in 1905. Moissan's discovery of naturally occurring SiC was initially disputed because his sample may have been contaminated by silicon carbide saw blades that were already on the market at that time.\n\nWhile rare on Earth, silicon carbide is remarkably common in space. It is a common form of stardust found around carbon-rich stars, and examples of this stardust have been found in pristine condition in primitive (unaltered) meteorites. The silicon carbide found in space and in meteorites is almost exclusively the beta-polymorph. Analysis of SiC grains found in the Murchison meteorite, a carbonaceous chondrite meteorite, has revealed anomalous isotopic ratios of carbon and silicon, indicating that these grains originated outside the solar system.\n\nBecause natural moissanite is extremely scarce, most silicon carbide is synthetic. Silicon carbide is used as an abrasive, as well as a semiconductor and diamond simulant of gem quality. The simplest process to manufacture silicon carbide is to combine silica sand and carbon in an Acheson graphite electric resistance furnace at a high temperature, between and . Fine SiO particles in plant material (e.g. rice husks) can be converted to SiC by heating in the excess carbon from the organic material. The silica fume, which is a byproduct of producing silicon metal and ferrosilicon alloys, can also be converted to SiC by heating with graphite at .\n\nThe material formed in the Acheson furnace varies in purity, according to its distance from the graphite resistor heat source. Colorless, pale yellow and green crystals have the highest purity and are found closest to the resistor. The color changes to blue and black at greater distance from the resistor, and these darker crystals are less pure. Nitrogen and aluminium are common impurities, and they affect the electrical conductivity of SiC.\n\nPure silicon carbide can be made by the Lely process, in which SiC powder is sublimed into high-temperature species of silicon, carbon, silicon dicarbide (SiC), and disilicon carbide (SiC) in an argon gas ambient at 2500Â Â°C and redeposited into flake-like single crystals, sized up to 2Ã2Â cm, at a slightly colder substrate. This process yields high-quality single crystals, mostly of 6H-SiC phase (because of high growth temperature).\n\nA modified Lely process involving induction heating in graphite crucibles yields even larger single crystals of 4Â inches (10Â cm) in diameter, having a section 81 times larger compared to the conventional Lely process.\n\nCubic SiC is usually grown by the more expensive process of chemical vapor deposition (CVD). Homoepitaxial and heteroepitaxial SiC layers can be grown employing both gas and liquid phase approaches. Pure silicon carbide can also be prepared by the thermal decomposition of a polymer, poly(methylsilyne), under an inert atmosphere at low temperatures. Relative to the CVD process, the pyrolysis method is advantageous because the polymer can be formed into various shapes prior to thermalization into the ceramic.\n\nSilicon carbide exists in about 250 crystalline forms. The polymorphism of SiC is characterized by a large family of similar crystalline structures called polytypes. They are variations of the same chemical compound that are identical in two dimensions and differ in the third. Thus, they can be viewed as layers stacked in a certain sequence.\n\nAlpha silicon carbide (Î±-SiC) is the most commonly encountered polymorph. It's formed at temperatures greater than 1700Â Â°C and has a hexagonal crystal structure (similar to Wurtzite). The beta modification (Î²-SiC), with a zinc blende crystal structure (similar to diamond), is formed at temperatures below 1700Â Â°C. Until recently, the beta form has had relatively few commercial uses, although there is now increasing interest in its use as a support for heterogeneous catalysts, owing to its higher surface area compared to the alpha form.\n\nPure SiC is colorless. The brown to black color of the industrial product results from iron impurities. The rainbow-like luster of the crystals is caused by a passivation layer of silicon dioxide that forms on the surface.\n\nThe high sublimation temperature of SiC (approximately 2700Â Â°C) makes it useful for bearings and furnace parts. Silicon carbide does not melt at any known temperature. It is also highly inert chemically. There is currently much interest in its use as a semiconductor material in electronics, where its high thermal conductivity, high electric field breakdown strength and high maximum current density make it more promising than silicon for high-powered devices. SiC also has a very low coefficient of thermal expansion (4.0 Ã 10/K) and experiences no phase transitions that would cause discontinuities in thermal expansion.\n\nSilicon carbide is a semiconductor, which can be doped n-type by nitrogen or phosphorus and p-type by beryllium, boron, aluminium, or gallium. Metallic conductivity has been achieved by heavy doping with boron, aluminium or nitrogen.\n\nSuperconductivity has been detected in 3C-SiC:Al, 3C-SiC:B and 6H-SiC:B at the same temperature of 1.5 K. A crucial difference is however observed for the magnetic field behavior between aluminium and boron doping: SiC:Al is type-II, same as Si:B. On the contrary, SiC:B is type-I. In attempt to explain this difference, it was noted that Si sites are more important than carbon sites for superconductivity in SiC. Whereas boron substitutes carbon in SiC, Al substitutes Si sites. Therefore, Al and B \"see\" different environments that might explain different properties of SiC:Al and SiC:B.\n\nIn the arts, silicon carbide is a popular abrasive in modern lapidary due to the durability and low cost of the material. In manufacturing, it is used for its hardness in abrasive machining processes such as grinding, honing, water-jet cutting and sandblasting. Particles of silicon carbide are laminated to paper to create sandpapers and the grip tape on skateboards.\n\nIn 1982 an exceptionally strong composite of aluminium oxide and silicon carbide whiskers was discovered. Development of this laboratory-produced composite to a commercial product took only three years. In 1985, the first commercial cutting tools made from this alumina and silicon carbide whisker-reinforced composite were introduced into the market.\n\nIn the 1980s and 1990s, silicon carbide was studied in several research programs for high-temperature gas turbines in Europe, Japan and the United States. The components were intended to replace nickel superalloy turbine blades or nozzle vanes. However, none of these projects resulted in a production quantity, mainly because of its low impact resistance and its low fracture toughness.\n\nLike other hard ceramics (namely alumina and boron carbide), silicon carbide is used in composite armor (e.g. Chobham armor), and in ceramic plates in bulletproof vests. Dragon Skin, which was produced by Pinnacle Armor, used disks of silicon carbide.\n\nSilicon carbide is used as a support and shelving material in high temperature kilns such as for firing ceramics, glass fusing, or glass casting. SiC kiln shelves are considerably lighter and more durable than traditional alumina shelves.\n\nIn December 2015, infusion of silicon carbide nano-particles in molten magnesium was mentioned as a way to produce a new strong and plastic alloy suitable for use in aeronautics, aerospace, automobile and micro-electronics.\n\nSilicon-infiltrated carbon-carbon composite is used for high performance \"ceramic\" brake discs, as it is able to withstand extreme temperatures. The silicon reacts with the graphite in the carbon-carbon composite to become carbon-fiber-reinforced silicon carbide (C/SiC). These discs are used on some road-going sports cars, supercars, as well as other performance cars including the Porsche Carrera GT, the Bugatti Veyron, the Chevrolet Corvette ZR1, the McLaren P1, Bentley, Ferrari, Lamborghini and some specific high-performance Audi cars. Silicon carbide is also used in a sintered form for diesel particulate filters. It's also used as an oil additive to reduce friction, emissions, and harmonics.\n\nSiC is used in crucibles for holding melting metal in small and large foundry applications.\n\nThe earliest electrical application of SiC was in lightning arresters in electric power systems. These devices must exhibit high resistance until the voltage across them reaches a certain threshold V at which point their resistance must drop to a lower level and maintain this level until the applied voltage drops below V.\n\nIt was recognized early on that SiC had such a voltage-dependent resistance, and so columns of SiC pellets were connected between high-voltage power lines and the earth. When a lightning strike to the line raises the line voltage sufficiently, the SiC column will conduct, allowing strike current to pass harmlessly to the earth instead of along the power line. The SiC columns proved to conduct significantly at normal power-line operating voltages and thus had to be placed in series with a spark gap. This spark gap is ionized and rendered conductive when lightning raises the voltage of the power line conductor, thus effectively connecting the SiC column between the power conductor and the earth. Spark gaps used in lightning arresters are unreliable, either failing to strike an arc when needed or failing to turn off afterwards, in the latter case due to material failure or contamination by dust or salt. Usage of SiC columns was originally intended to eliminate the need for the spark gap in lightning arresters. Gapped SiC arresters were used for lightning-protection and sold under the GE and Westinghouse brand names, among others. The gapped SiC arrester has been largely displaced by no-gap varistors that use columns of zinc oxide pellets.\n\nSilicon carbide was the first commercially important semiconductor material. A crystal radio \"carborundum\" (synthetic silicon carbide) detector diode was patented by Henry Harrison Chase Dunwoody in 1906. It found much early use in shipboard receivers. \n\nSilicon carbide is a semiconductor in research and early mass production providing advantages for fast, high-temperature and/or high-voltage devices. The first devices available were Schottky diodes, followed by junction-gate FETs and MOSFETs for high-power switching. Bipolar transistors and thyristors are currently developed. A major problem for SiC commercialization has been the elimination of defects: edge dislocations, screw dislocations (both hollow and closed core), triangular defects and basal plane dislocations. As a result, devices made of SiC crystals initially displayed poor reverse blocking performance though researchers have been tentatively finding solutions to improve the breakdown performance. Apart from crystal quality, problems with the interface of SiC with silicon dioxide have hampered the development of SiC-based power MOSFETs and insulated-gate bipolar transistors. Although the mechanism is still unclear, nitridation has dramatically reduced the defects causing the interface problems. In 2008, the first commercial JFETs rated at 1200 V were introduced to the market, followed in 2011 by the first commercial MOSFETs rated at 1200 V. Beside SiC switches and SiC Schottky diodes (also Schottky barrier diode, SBD) in the popular TO-247 and TO-220 packages, companies started even earlier to implement the bare chips into their power electronic modules. SiC SBD diodes found wide market spread being used in PFC circuits and IGBT power modules.\nConferences such as the International Conference on Integrated Power Electronics Systems (CIPS) report regularly about the technological progress of SiC power devices.\nMajor challenges for fully unleashing the capabilities of SiC power devices are:\n\nThe phenomenon of electroluminescence was discovered in 1907 using silicon carbide and the first commercial LEDs were based on SiC. Yellow LEDs made from 3C-SiC were manufactured in the Soviet Union in the 1970s\nand blue LEDs (6H-SiC) worldwide in the 1980s. The production was soon stopped because gallium nitride showed 10â100 times brighter emission. This difference in efficiency is due to the unfavorable indirect bandgap of SiC, whereas GaN has a direct bandgap which favors light emission. However, SiC is still one of the important LED components â it is a popular substrate for growing GaN devices, and it also serves as a heat spreader in high-power LEDs.\n\nThe low thermal expansion coefficient, high hardness, rigidity and thermal conductivity make silicon carbide a desirable mirror material for astronomical telescopes. The growth technology (chemical vapor deposition) has been scaled up to produce disks of polycrystalline silicon carbide up to in diameter, and several telescopes like the Herschel Space Telescope are already equipped with SiC optics, as well the Gaia space observatory spacecraft subsystems are mounted on a rigid silicon carbide frame, which provides a stable structure that will not expand or contract due to heat.\n\nSilicon carbide fibers are used to measure gas temperatures in an optical technique called thin filament pyrometry. It involves the placement of a thin filament in a hot gas stream. Radiative emissions from the filament can be correlated with filament temperature. Filaments are SiC fibers with a diameter of 15 micrometers, about one fifth that of a human hair. Because the fibers are so thin, they do little to disturb the flame and their temperature remains close to that of the local gas. Temperatures of about 800â2500 K can be measured.\n\nReferences to silicon carbide heating elements exist from the early 20th century when they were produced by Acheson's Carborundum Co. in the U.S. and EKL in Berlin. Silicon carbide offered increased operating temperatures compared with metallic heaters. Silicon carbide elements are used today in the melting of glass and non-ferrous metal, heat treatment of metals, float glass production, production of ceramics and electronics components, igniters in pilot lights for gas heaters, etc.\n\nSilicon carbide is an important material in TRISO-coated fuel particles, the type of nuclear fuel found in high temperature gas cooled reactors such as the Pebble Bed Reactor. A layer of silicon carbide gives coated fuel particles structural support and is the main diffusion barrier to the release of fission products.\n\nSilicon carbide composite material has been investigated for use as a replacement for Zircaloy cladding in light water reactors. The composite consists of SiC fibers wrapped around a SiC inner layer and surrounded by an SiC outer layer. Problems have been reported with the ability to join the pieces of the SiC composite.\n\nAs a gemstone used in jewelry, silicon carbide is called \"synthetic moissanite\" or just \"moissanite\" after the mineral name. Moissanite is similar to diamond in several important respects: it is transparent and hard (9â9.5 on the Mohs scale, compared to 10 for diamond), with a refractive index between 2.65 and 2.69 (compared to 2.42 for diamond). Moissanite is somewhat harder than common cubic zirconia. Unlike diamond, moissanite can be strongly birefringent. For this reason, moissanite jewels are cut along the optic axis of the crystal to minimize birefringent effects. It is lighter (density 3.21 g/cm vs. 3.53 g/cm), and much more resistant to heat than diamond. This results in a stone of higher luster, sharper facets, and good resilience. Loose moissanite stones may be placed directly into wax ring moulds for lost-wax casting, as can diamond, as moissanite remains undamaged by temperatures up to . Moissanite has become popular as a diamond substitute, and may be misidentified as diamond, since its thermal conductivity is closer to diamond than any other substitute. Many thermal diamond-testing devices cannot distinguish moissanite from diamond, but the gem is distinct in its birefringence and a very slight green or yellow fluorescence under ultraviolet light. Some moissanite stones also have curved, string-like inclusions, which diamonds never have.\n\nSilicon carbide, dissolved in a basic oxygen furnace used for making steel, acts as a fuel. The additional energy liberated allows the furnace to process more scrap with the same charge of hot metal. It can also be used to raise tap temperatures and adjust the carbon and silicon content. Silicon carbide is cheaper than a combination of ferrosilicon and carbon, produces cleaner steel and lower emissions due to low levels of trace elements, has a low gas content, and does not lower the temperature of steel.\n\nThe natural resistance to oxidation exhibited by silicon carbide, as well as the discovery of new ways to synthesize the cubic Î²-SiC form, with its larger surface area, has led to significant interest in its use as a heterogeneous catalyst support. This form has already been employed as a catalyst support for the oxidation of hydrocarbons, such as n-butane, to maleic anhydride.\n\nSilicon carbide is used in carborundum printmaking â a collagraph printmaking technique. Carborundum grit is applied in a paste to the surface of an aluminium plate. When the paste is dry, ink is applied and trapped in its granular surface, then wiped from the bare areas of the plate. The ink plate is then printed onto paper in a rolling-bed press used for intaglio printmaking. The result is a print of painted marks embossed into the paper.\n\nSilicon carbide can be used in the production of graphene because of its chemical properties that promote the epitaxial production of graphene on the surface of SiC nanostructures.\n\nWhen it comes to its production, silicon is used primarily as a substrate to grow the graphene. But there are actually several methods that can be used to grow the graphene on the silicon carbide. The confinement controlled sublimation (CCS) growth method consists of a SiC chip that is heated under vacuum with graphite. Then the vacuum is released very gradually to control the growth of graphene. This method yields the highest quality graphene layers. But other methods have been reported to yield the same product as well.\n\nAnother way of growing graphene would be thermally decomposing SiC at a high temperature within a vacuum. But this method turns out to yield graphene layers that contain smaller grains within the layers. So there have been efforts to improve the quality and yield of graphene. One such method is to perform \"ex situ\" graphitization of silicon terminated SiC in an atmosphere consisting of argon. This method has proved to yield layers of graphene with larger domain sizes than the layer that would be attainable via other methods. This new method can be very viable to make higher quality graphene for a multitude of technological applications.\n\nWhen it comes to understanding how or when to use these methods of graphene production, most of them mainly produce or grow this graphene on the SiC within a growth enabling environment. It is utilized most often at rather higher temperatures (such as 1300ËC) because of SiC thermal properties. However, there have been certain procedures that have been performed and studied that could potentially yield methods that use lower temperatures to help manufacture graphene. More specifically this different approach to graphene growth has been observed to produce graphene within a temperature environment of around 750ËC. This method entails the combination of certain methods like chemical vapor deposition (CVD) and surface segregation. And when it comes to the substrate, the procedure would consist of coating a SiC substrate with thin films of a transition metal. And after the rapid heat treating of this substance, the carbon atoms would then become more abundant at the surface interface of the transition metal film which would then yield graphene. And this process was found to yield graphene layers that were more continuous throughout the substrate surface.\n\nSilicon carbide can host point defects in the crystal lattice known as color centers. These defects can produce single photons on demand and thus serve as a platform for single-photon source. Such device is a fundamental resource for many emerging applications of the quantum information science. If one pumps a color center via an external optical source or electrical current, the color center will be brought to the excited state and will then relax with the emission of one photon.\n\n\n"}
{"id": "26321550", "url": "https://en.wikipedia.org/wiki?curid=26321550", "title": "Soilon", "text": "Soilon\n\nSoilon is a fine mesh made from poly lactic acid (PLA) polymer resin by lactic fermentation of glucose, derived from starch (such as corn starch), by an enzyme and polymerization. By thermal polymerizing of lactic acid, an aliphatic polyester resin having melting point of 170 C and 57C second order transition point respectively is gotten. Due to its plastic character, PLA can be melt-spun into fibers (filaments), which are woven for beverage filter media. It was designed for (and often used in) teabags. It was chosen to replace the materials in teabags, since it can be biodegraded and broken down readily by microorganisms in the soil. It is a safe and non-toxic alternative to the paper bags usually employed.\n"}
{"id": "37673633", "url": "https://en.wikipedia.org/wiki?curid=37673633", "title": "Sokol Eshelon", "text": "Sokol Eshelon\n\nSokol Eshelon () is a Russian, formerly Soviet, laser weapon based anti-satellite system. It is an airborne laser based on a Beriev A-60 aircraft. In 2012 it was reported that the project is back under development and is intended for the Russian Aerospace Defence Forces once completed.\n\nThe Beriev A-60 aircraft is a laser research plane produced by Beriev based on their IL-76 transport. Development seems to have started in 1981 with the laser installed in 1983. A second plane was built in 1991. The plane contains a special nose cone with a laser targeting system.\n\nThe Sokol Eshelon project started in 2003 and was first made public in the annual report of contractor Khimpromavtomatika in 2005. Other mentions include Almaz-Antey's annual report in 2006 and in a report by Radiofizika in 2009 where they mentioned their development of radar for the plane. Details also emerged due to a court case between Almaz-Antey and Beriev over payments to a subcontractor. The contract between the two parties was connected to a contract no. 5933 Almaz-Antey holds with military unit 21055. This contract is supervised by the Ministry of Defence's 27th Military Representative Office. The project seems to be known under the codename \"ÐÑÑÐ»ÑÐ½Ñ\" (Duelyant, en. duelist).\n\nIn 2009 the plane was involved in a test to illuminate Japanese satellite AJISAI which was at an orbital height of 1,500Â km. The test involved seeing if a reflection of the laser off the satellite could be picked up, and wasn't intended to damage the satellite.\n\nThe laser has been given the codename 1LK222. The purpose of the laser is to blind the sensors of enemy satellites rather than destroy them. Its developmental work was reportedly finished as of early 2018.\n\nRelated development:\n\nComparable systems:\n"}
{"id": "587654", "url": "https://en.wikipedia.org/wiki?curid=587654", "title": "Speculum metal", "text": "Speculum metal\n\nSpeculum metal is a mixture of around two-thirds copper and one-third tin making a white brittle alloy that can be polished to make a highly reflective surface. It was used historically to make different kinds of mirrors from personal grooming aids to reflecting telescope optical mirrors until it was replaced by more modern materials.\n\nLarge speculum metal mirrors are hard to manufacture and the alloy is prone to tarnish, requiring frequent re-polishing. However, it was the only practical choice for large mirrors in high-precision optical equipment between mid-17th and mid-19th century, before the invention of glass silvering.\n\nSpeculum metal mixtures usually contain two parts copper to one part tin along with a small amount of arsenic, although there are other mixtures containing silver, lead, or zinc. The knowledge of making very hard white high luster metal out of bronze-type high-tin alloys may date back more than 2000 years in China although it could also be an invention of western civilizations. Such metals were used in sculpture and to make more effective mirrors than the more common yellow easily tarnishing bronze mirrors. In that era mirrors of speculum metal, or any precious metal, were rare and only owned by the wealthy \n\nSpeculum metal found an application in early modern Europe as the only known good reflecting surface for mirrors in reflecting telescopes. In contrast to household mirrors, where the reflecting metal layer is coated on the back of a glass pane and covered with a protective varnish, precision optical equipment like telescopes needs first surface mirrors that can be ground and polished into complex shapes such as parabolic reflectors. For nearly 200 years speculum metal was the only mirror substance that could perform this task. One of the earliest designs, James Gregoryâs Gregorian telescope could not be built because Gregory could not find a craftsman capable of fabricating the complex speculum mirrors needed for the design.\n\nIsaac Newton was the first to successfully build a reflecting telescope in 1668. His first reflecting telescope (a design which came to be known as a Newtonian reflector) had a 33-mm (1.3-inch) diameter speculum metal primary mirror of his own formulation.\nNewton was likewise confronted with the problem of fabricating the complex parabolic shape needed to create the image, but simply settled on a spherical shape. The composition of speculum metal was further refined and went on to be used in the 1700s and 1800s in many designs of reflecting telescopes. The ideal composition was around 68.21% copper to 31.7 % tin; more copper made the metal more yellow, more tin made the metal more blue in color. Ratios with up to 45% tin were used for resistance to tarnishing.\n\nAlthough speculum metal mirror reflecting telescopes could be built very large, such as William Herschel's 126-cm (49.5-inch) \"40-foot telescope\" of 1789 and Lord Rosse 1845 183-cm (72-inch) mirror of his \"Leviathan of Parsonstown\", impracticalities in using the metal made most astronomers prefer their smaller refracting telescope counterparts.\nSpeculum metal was very hard to cast and shape. It only reflected 66 percent of the light that hit it. Speculum also had the unfortunate property of tarnishing in open air with a sensitivity to humidity, requiring constant re-polishing to maintain its usefulness. This meant the telescopes mirrors had to be constantly removed, polished, and re-figured to the correct shape. This sometimes proved difficult, with some mirrors having to be abandoned. It also required that two or more mirrors had to be fabricated for each telescope so that one could be used while the other was being polished. Rapidly cooling night time air would cause stresses in large speculum metal mirrors, distorting their shape and causing them to produce poor images. Lord Rosse had a system of adjustable levers on his 72-inch metal mirror so he could adjust the shape when it was hit or miss at producing an acceptable image.\n\nIn 1856â57 an improvement over speculum mirrors was invented when Karl August von Steinheil and LÃ©on Foucault introduced the process of depositing an ultra thin layer of silver on the front surface (first surface) of a ground block of glass. Silvered glass mirrors were a vast improvement since silver reflects 90 percent of the light that hits it and is much slower to tarnish than speculum. Silver coatings can also be removed from the glass, so a tarnished mirror could be resilvered without changing the delicate precision polished shape of the glass substrate. Glass is also more thermally stable than speculum metal, allowing it to hold its shape better through temperature changes. This marked the end of the speculum-mirror reflecting telescope, with the last large one, the Great Melbourne Telescope with its 122-cm (48-inch) mirror, being completed in 1867. The era of the large glass-mirror reflector had begun, with telescopes such as Andrew Ainslie Common's 1879 36Â inch (91Â cm) and 1887 60Â inch (152Â cm) reflectors built at Ealing, and the first of the \"modern\" large glass mirror research reflectors, 60Â inch (150Â cm) Mount Wilson Observatory Hale telescope of 1908, the 100Â inch (2.5 m) Mount Wilson Hooker telescope in 1917 and the 200Â inch (5 m) Mount Palomar Hale telescope in 1948.\n\n\n"}
{"id": "3901932", "url": "https://en.wikipedia.org/wiki?curid=3901932", "title": "Thorium fuel cycle", "text": "Thorium fuel cycle\n\nThe thorium fuel cycle is a nuclear fuel cycle that uses an isotope of thorium, , as the fertile material. In the reactor, is transmuted into the fissile artificial uranium isotope which is the nuclear fuel. Unlike natural uranium, natural thorium contains only trace amounts of fissile material (such as ), which are insufficient to initiate a nuclear chain reaction. Additional fissile material or another neutron source is necessary to initiate the fuel cycle. In a thorium-fuelled reactor, absorbs neutrons to produce . This parallels the process in uranium breeder reactors whereby fertile absorbs neutrons to form fissile . Depending on the design of the reactor and fuel cycle, the generated either fissions in situ or is chemically separated from the used nuclear fuel and formed into new nuclear fuel.\n\nThe thorium fuel cycle has several potential advantages over a uranium fuel cycle, including thorium's greater abundance, superior physical and nuclear properties, reduced plutonium and actinide production, and better resistance to nuclear weapons proliferation when used in a traditional light water reactor though not in a molten salt reactor.\n\nConcerns about the limits of worldwide uranium resources motivated initial interest in the thorium fuel cycle. It was envisioned that as uranium reserves were depleted, thorium would supplement uranium as a fertile material. However, for most countries uranium was relatively abundant and research in thorium fuel cycles waned. A notable exception was India's three-stage nuclear power programme.\nIn the twenty-first century thorium's potential for improving proliferation resistance and waste characteristics led to renewed interest in the thorium fuel cycle.\n\nAt Oak Ridge National Laboratory in the 1960s, the Molten-Salt Reactor Experiment used as the fissile fuel in an experiment to demonstrate a part of the Molten Salt Breeder Reactor that was designed to operate on the thorium fuel cycle. Molten salt reactor (MSR) experiments assessed thorium's feasibility, using thorium(IV) fluoride dissolved in a molten salt fluid that eliminated the need to fabricate fuel elements. The MSR program was defunded in 1976 after its patron Alvin Weinberg was fired.\n\nIn 2006, Carlo Rubbia proposed the concept of an energy amplifier or \"accelerator driven system\" (ADS), which he saw as a novel and safe way to produce nuclear energy that exploited existing accelerator technologies. Rubbia's proposal offered the potential to incinerate high-activity nuclear waste and produce energy from natural thorium and depleted uranium.\n\nKirk Sorensen, former NASA scientist and Chief Technologist at Flibe Energy, has been a long-time promoter of thorium fuel cycle and particularly liquid fluoride thorium reactors (LFTRs). He first researched thorium reactors while working at NASA, while evaluating power plant designs suitable for lunar colonies. In 2006 Sorensen started \"energyfromthorium.com\" to promote and make information available about this technology.\n\nA 2011 MIT study concluded that although there is little in the way of barriers to a thorium fuel cycle, with current or near term light-water reactor designs there is also little incentive for any significant market penetration to occur. As such they conclude there is little chance of thorium cycles replacing conventional uranium cycles in the current nuclear power market, despite the potential benefits.\n\nIn the thorium cycle, fuel is formed when captures a neutron (whether in a fast reactor or thermal reactor) to become . This normally emits an electron and an anti-neutrino () by decay to become . This then emits another electron and anti-neutrino by a second decay to become , the fuel:\n\nNuclear fission produces radioactive fission products which can have half-lives from days to greater than 200,000 years. According to some toxicity studies, the thorium cycle can fully recycle actinide wastes and only emit fission product wastes, and after a few hundred years, the waste from a thorium reactor can be less toxic than the uranium ore that would have been used to produce low enriched uranium fuel for a light water reactor of the same power.\nOther studies assume some actinide losses and find that actinide wastes dominate thorium cycle waste radioactivity at some future periods.\n\nIn a reactor, when a neutron hits a fissile atom (such as certain isotopes of uranium), it either splits the nucleus or is captured and transmutes the atom. In the case of , the transmutations tend to produce useful nuclear fuels rather than transuranic wastes. When absorbs a neutron, it either fissions or becomes . The chance of fissioning on absorption of a thermal neutron is about 92%; the capture-to-fission ratio of , therefore, is about 1:12 â which is better than the corresponding capture vs. fission ratios of (about 1:6), or or (both about 1:3).\nThe result is less transuranic waste than in a reactor using the uranium-plutonium fuel cycle.\n\n, like most actinides with an even number of neutrons, is not fissile, but neutron capture produces fissile . If the fissile isotope fails to fission on neutron capture, it produces , , , and eventually fissile and heavier isotopes of plutonium.\nThe can be removed and stored as waste or retained and transmuted to plutonium, where more of it fissions, while the remainder becomes , then americium and curium, which in turn can be removed as waste or returned to reactors for further transmutation and fission.\n\nHowever, the (with a half-life of ) formed via (\"n\",2\"n\") reactions with (yielding that decays to ), while not a transuranic waste, is a major contributor to the long-term radiotoxicity of spent nuclear fuel.\n\nUranium-232 is also formed in this process, via (\"n\",2\"n\") reactions between fast neutrons and , , and :\n\nUranium-232 has a relatively short half-life (), and some decay products emit high energy gamma radiation, such as , and particularly . The full decay chain, along with half-lives and relevant gamma energies, is:\n\nThorium-cycle fuels produce hard gamma emissions, which damage electronics, limiting their use in bombs. cannot be chemically separated from from used nuclear fuel; however, chemical separation of thorium from uranium removes the decay product and the radiation from the rest of the decay chain, which gradually build up as reaccumulates. The contamination could also be avoided by using a molten-salt breeder reactor and separating the Pa-233 before it decays into U-233. The hard gamma emissions also create a radiological hazard which requires remote handling during reprocessing.\n\nAs a fertile material thorium is similar to , the major part of natural and depleted uranium. The thermal neutron absorption cross section (Ï) and resonance integral (average of neutron cross sections over intermediate neutron energies) for are about three and one third times those of the respective values for .\n\nThorium is estimated to be about three to four times more abundant than uranium in Earth's crust, although present knowledge of reserves is limited. Current demand for thorium has been satisfied as a by-product of rare-earth extraction from monazite sands.\n\nAlthough the thermal neutron fission cross section (Ï) of the resulting is comparable to and , it has a much lower capture cross section (Ï) than the latter two fissile isotopes, providing fewer non-fissile neutron absorptions and improved neutron economy. Finally, the ratio of neutrons released per neutron absorbed (Î·) in is greater than two over a wide range of energies, including the thermal spectrum; as a result, thorium-based fuels can be the basis for a thermal breeder reactor. A breeding reactor in the uranium - plutonium cycle needs to use a fast neutron spectrum, because in the thermal spectrum one neutron absorbed by on average leads to less than two neutrons.\n\nThorium-based fuels also display favorable physical and chemical properties that improve reactor and repository performance. Compared to the predominant reactor fuel, uranium dioxide (), thorium dioxide () has a higher melting point, higher thermal conductivity, and lower coefficient of thermal expansion. Thorium dioxide also exhibits greater chemical stability and, unlike uranium dioxide, does not further oxidize.\n\nBecause the produced in thorium fuels is significantly contaminated with in proposed power reactor designs, thorium-based used nuclear fuel possesses inherent proliferation resistance. cannot be chemically separated from and has several decay products that emit high-energy gamma radiation. These high-energy photons are a radiological hazard that necessitate the use of remote handling of separated uranium and aid in the passive detection of such materials.\n\nThe long-term (on the order of roughly to ) radiological hazard of conventional uranium-based used nuclear fuel is dominated by plutonium and other minor actinides, after which long-lived fission products become significant contributors again. A single neutron capture in is sufficient to produce transuranic elements, whereas five captures are generally necessary to do so from . 98â99% of thorium-cycle fuel nuclei would fission at either or , so fewer long-lived transuranics are produced. Because of this, thorium is a potentially attractive alternative to uranium in mixed oxide (MOX) fuels to minimize the generation of transuranics and maximize the destruction of plutonium.\n\nThere are several challenges to the application of thorium as a nuclear fuel, particularly for solid fuel reactors:\n\nIn contrast to uranium, naturally occurring thorium is effectively mononuclidic and contains no fissile isotopes; fissile material, generally , or plutonium, must be added to achieve criticality. This, along with the high sintering temperature necessary to make thorium-dioxide fuel, complicates fuel fabrication. Oak Ridge National Laboratory experimented with thorium tetrafluoride as fuel in a molten salt reactor from 1964â1969, which was expected to be easier to process and separate from contaminants that slow or stop the chain reaction.\n\nIn an open fuel cycle (i.e. utilizing in situ), higher burnup is necessary to achieve a favorable neutron economy. Although thorium dioxide performed well at burnups of 170,000 MWd/t and 150,000 MWd/t at Fort St. Vrain Generating Station and AVR respectively, challenges complicate achieving this in light water reactors (LWR), which compose the vast majority of existing power reactors.\n\nIn a once-through thorium fuel cycle the residual is a long-lived radioactive isotope in the waste.\n\nAnother challenge associated with the thorium fuel cycle is the comparatively long interval over which breeds to . The half-life of is about 27 days, which is an order of magnitude longer than the half-life of . As a result, substantial develops in thorium-based fuels. is a significant neutron absorber and, although it eventually breeds into fissile , this requires two more neutron absorptions, which degrades neutron economy and increases the likelihood of transuranic production.\n\nAlternatively, if solid thorium is used in a closed fuel cycle in which is recycled, remote handling is necessary for fuel fabrication because of the high radiation levels resulting from the decay products of . This is also true of recycled thorium because of the presence of , which is part of the decay sequence. Further, unlike proven uranium fuel recycling technology (e.g. PUREX), recycling technology for thorium (e.g. THOREX) is only under development.\n\nAlthough the presence of complicates matters, there are public documents showing that has been used once in a nuclear weapon test. The United States tested a composite -plutonium bomb core in the MET (Military Effects Test) blast during Operation Teapot in 1955, though with much lower yield than expected.\n\nThough thorium-based fuels produce far less long-lived transuranics than uranium-based fuels,\nsome long-lived actinide products constitute a long-term radiological impact, especially .\n\nAdvocates for liquid core and molten salt reactors such as LFTRs claim that these technologies negate thorium's disadvantages present in solid fuelled reactors. As only two liquid-core fluoride salt reactors have been built (the ORNL ARE and MSRE) and neither have used thorium, it is hard to validate the exact benefits.\n\nThorium fuels have fueled several different reactor types, including light water reactors, heavy water reactors, high temperature gas reactors, sodium-cooled fast reactors, and molten salt reactors.\n\nFrom IAEA TECDOC-1450 \"Thorium Fuel Cycle - Potential Benefits and Challenges\", Table 1: Thorium utilization in different experimental and power reactors. Additionally, Dresden 1 in the United States used \"thorium oxide corner rods\".\n\n\n"}
{"id": "1835568", "url": "https://en.wikipedia.org/wiki?curid=1835568", "title": "Titanium nitride", "text": "Titanium nitride\n\nTitanium nitride () (sometimes known as tinite) is an extremely hard ceramic material, often used as a coating on titanium alloys, steel, carbide, and aluminium components to improve the substrate's surface properties.\n\nApplied as a thin coating, TiN is used to harden and protect cutting and sliding surfaces, for decorative purposes (due to its golden appearance), and as a non-toxic exterior for medical implants. In most applications a coating of less than is applied.\n\nTiN has a Vickers hardness of 1800â2100, a modulus of elasticity of 251Â GPa, a thermal expansion coefficient of 9.35Â K, and a superconducting transition temperature of 5.6Â K.\n\nTiN will oxidize at 800Â Â°C in a normal atmosphere. It is chemically stable at 20Â Â°C, according to laboratory tests, but can be slowly attacked by concentrated acid solutions with rising temperatures.\nDepending on the substrate material and surface finish, TiN will have a coefficient of friction ranging from 0.4 to 0.9 against another TiN surface (non-lubricated). The typical TiN formation has a crystal structure of NaCl-type with a roughly 1:1 stoichiometry; TiN compounds with \"x\" ranging from 0.6 to 1.2 are, however, thermodynamically stable. \n\nTiN becomes superconducting at cryogenic temperatures, with critical temperature up to 6.0Â K for single crystals. Superconductivity in thin-film TiN has been studied extensively, with the superconducting properties strongly varying depending on sample preparation.\nA thin film of TiN was chilled to near absolute zero, converting it into the first known superinsulator, with resistance suddenly increasing by a factor of 100,000.\n\nA well-known use for TiN coating is for edge retention and corrosion resistance on machine tooling, such as drill bits and milling cutters, often improving their lifetime by a factor of three or more.\n\nBecause of TiN's metallic gold color, it is used to coat costume jewelry and automotive trim for decorative purposes. TiN is also widely used as a top-layer coating, usually with nickel (Ni) or chromium (Cr) plated substrates, on consumer plumbing fixtures and door hardware. As a coating it is used in aerospace and military applications and to protect the sliding surfaces of suspension forks of bicycles and motorcycles as well as the shock shafts of radio controlled cars. TiN is non-toxic, meets FDA guidelines and has seen use in medical devices such as scalpel blades and orthopedic bone saw blades where sharpness and edge retention are important. TiN coatings have also been used in implanted prostheses (especially hip replacement implants) and other medical implants.\n\nThough less visible, thin films of TiN are also used in microelectronics, where they serve as a conductive connection between the active device and the metal contacts used to operate the circuit, while acting as a diffusion barrier to block the diffusion of the metal into the silicon. In this context, TiN is classified as a \"barrier metal\", even though it is clearly a ceramic from the perspective of chemistry or mechanical behavior. Recent chip design in the 45Â nm technology and beyond also makes use of TiN as a \"metal\" for improved transistor performance. In combination with gate dielectrics (e.g. HfSiO) that have a higher permittivity compared to standard SiO the gate length can be scaled down with low leakage, higher drive current and the same or better threshold voltage. Additionally, TiN thin films are currently under consideration for coating zirconium alloys for accident-tolerant nuclear fuels. \n\nDue to their high biostability, TiN layers may also be used as electrodes in bioelectronic applications like in intelligent implants or in-vivo biosensors that have to withstand the severe corrosion caused by body fluids. TiN electrodes have already been applied in the subretinal prosthesis project as well as in biomedical microelectromechanical systems (BioMEMS).\n\nThe most common methods of TiN thin film creation are physical vapor deposition (PVD, usually sputter deposition, cathodic arc deposition or electron beam heating) and chemical vapor deposition (CVD). In both methods, pure titanium is sublimed and reacted with nitrogen in a high-energy, vacuum environment. TiN film may also be produced on Ti workpieces by reactive growth (for example, annealing) in a nitrogen atmosphere. PVD is preferred for steel parts because the deposition temperatures exceeds the austenitizing temperature of steel. TiN layers are also sputtered on a variety of higher melting point materials such as stainless steels, titanium and titanium alloys. Its high Young's modulus (values between 450 and 590 GPa have been reported in the literature ) means that thick coatings tend to flake away, making them much less durable than thin ones. Titanium nitride coatings can also be deposited by thermal spraying whereas TiN powders are produced by nitridation of titanium with nitrogen or ammonia at 1200Â Â°C.\n\nBulk ceramic objects can be fabricated by packing powdered metallic titanium into the desired shape, compressing it to the proper density, then igniting it in an atmosphere of pure nitrogen. The heat released by the chemical reaction between the metal and gas is sufficient to sinter the nitride reaction product into a hard, finished item. See powder metallurgy.\n\nThere are several commercially used variants of TiN that have been developed in the past decade, such as titanium carbon nitride (TiCN), titanium aluminium nitride (TiAlN or AlTiN), and titanium aluminum carbon nitride, which may be used individually or in alternating layers with TiN. These coatings offer similar or superior enhancements in corrosion resistance and hardness, and additional colors ranging from light gray to nearly black, to a dark iridescent bluish-purple depending on the exact process of application. These coatings are becoming common on sporting goods, particularly knives and handguns, where they are used for both cosmetic and functional reasons.\n\nTitanium nitride is also produced intentionally within some steels by judicious addition of titanium to the alloy. TiN forms at very high temperatures because of its very low enthalpy of formation, and even nucleates directly from the melt in secondary steelmaking. It forms discrete, micrometre-sized cubic particles at grain boundaries and triple points, and prevents grain growth by Ostwald ripening up to very high homologous temperatures. Titanium nitride has the lowest solubility product of any metal nitride or carbide in austenite, a useful attribute in microalloyed steel formulas.\n\n\"Osbornite\" is a very rare natural form of titanium nitride, found almost exclusively in meteorites.\n"}
{"id": "197667", "url": "https://en.wikipedia.org/wiki?curid=197667", "title": "Tongue and groove", "text": "Tongue and groove\n\nTongue and groove is a method of fitting similar objects together, edge to edge, used mainly with wood, in flooring, parquetry, panelling, and similar constructions. Tongue and groove joints allow two flat pieces to be joined strongly together to make a single flat surface. Before plywood became common, tongue and groove boards were also used for sheathing buildings and to construct concrete formwork.\n\nA strong joint, the tongue and groove joint is widely used for re-entrant angles. The effect of wood shrinkage is concealed when the joint is beaded or otherwise moulded. In expensive cabinet work, glued dovetail and multiple tongue and groove are used.\n\nEach piece has a slot (the \"groove\") cut all along one edge, and a thin, deep ridge (the \"tongue\") on the opposite edge. The tongue projects a little less than the depth of the groove. Two or more pieces thus fit together closely. The joint is not normally glued, as shrinkage would then pull the tongue off.\n\nIn another assembly method, the pieces are end-matched. This method eliminates the need for mitre joints, face nailing, and the use of joints on or centres of conventional framing.\n\nFor many uses, tongue and groove boards have been rendered obsolete by the introduction of plywood and later composite wood boards, but the method is still used in higher-quality boards. Plywood may also be tongued all round to fit it flush into a framed structure, and plywood for sub-floors used in platform framing is often supplied with tongue and groove edges.\n\nWhen joining thicker materials, several tongue and groove joints may be used one above the other.\n\n\"Tongue and groove\" is sometimes abbreviated as T&G (for example, on price tags and shelf tags).\n\nOne of the following woodworking tools may be used to produce the tongue and groove:\n\nTongue-in-groove is similar to tongue and groove, but instead of the tongue forming part of one of the edges, it is a separate, loose piece that fits between two identically grooved edges. The tongue may or may not be of the same material as the grooved pieces joined by the tongue. For example, plywood flooring is commonly grooved at the edges, and plastic tongues are used to form the joint.\n"}
{"id": "5611195", "url": "https://en.wikipedia.org/wiki?curid=5611195", "title": "Uranium-232", "text": "Uranium-232\n\nUranium-232 (, , U-232) is an isotope of uranium. It has a half-life of 68.9 years and is a side product in the thorium cycle. It has been cited as an obstacle to nuclear proliferation using U as the fissile material, because the intense gamma radiation emitted by Tl (a daughter of U, produced relatively quickly) makes the U contaminated with it more difficult to handle.\n\nProduction of U (through the neutron irradiation of Th) invariably produces small amounts of U as an impurity, because of parasitic (n,2n) reactions on uranium-233 itself, or on protactinium-233, or on thorium-232:\n\nAnother channel involves neutron capture reaction on small amounts of thorium-230, which is a tiny fraction of natural thorium present due to the decay of uranium-238:\n\nThe decay chain of U quickly yields strong gamma radiation emitters:\n\nThis makes manual handling in a glove box with only light shielding (as commonly done with plutonium) too hazardous, (except possibly in a short period immediately following chemical separation of the uranium from thorium-228, radium-224, radon-220, and polonium-216) and instead requiring remote manipulation for fuel fabrication.\n\nUnusually for an isotope with even mass number, U has a significant neutron absorption cross section for fission (thermal neutrons , resonance integral ) as well as for neutron capture (thermal , resonance integral ).\n"}
{"id": "7174449", "url": "https://en.wikipedia.org/wiki?curid=7174449", "title": "White Juan", "text": "White Juan\n\nWhite Juan is the unofficial name given to the hurricane-strength nor'easter blizzard of February 2004 that affected most of Atlantic Canada between February 17 and 20, 2004âfive months after Hurricane Juan devastated Nova Scotia and Prince Edward Island.\n\nThe storm dropped heavy snowfall throughout the Nova Scotia peninsula and Prince Edward Island, with accumulations in the hardest-hit areas ranging from .\n\nThe storm formed approximately southeast of Cape Hatteras, North Carolina, on February 17, 2004. It intensified over the Gulf Stream, moving at 35â40Â km/h (21â24 mi/h) before striking Atlantic Canada.\n\nSnow fell at a rate of per hour for 12 straight hours, and winds blew at up to per hour.\n\nThe snowstorm dropped a record-breaking of snow on CFB Shearwater, beating the previous record of set February 1, 1960. It also broke the record for the most snow in Yarmouth with of snow, surpassing the that fell on January 16, 1977.\n\nNumerous unofficial reports placed a snowfall of nearly in many regions across the province. The storm also produced sustained winds ranging from 60 to 80Â km/h through much of New Brunswick, Nova Scotia, Prince Edward Island, and Newfoundland and Labrador with maximum 1 minute gusts of per hour reported at many stations. Much of central, northern and western New Brunswick received little to no snow or wind as the storm tracked toward the east.\n\nTwo weather stations in the Halifax Regional Municipality reported 10-second gusts nearing . However, these reports have never been confirmed by Environment Canada. Weather radar observations, as well as synoptic report, showed extensive thundersnow embedded within the blizzard; in the heaviest bands, accumulation rates exceeded .\n\nThe wind combined with the intense snow rates produced visibilities of or less in most areas for brief periods, however these conditions persisted for at least eight hours in much of Nova Scotia. The wind also whipped up snow drifts which in some cases covered two and three-storey buildings and made many roads impassable to both common motor vehicles and snow removal equipment.\n\nThe humid continental climate of coastal Nova Scotia typically does not experience extreme snowfalls, compared with northern Nova Scotia, Cape Breton Island, Prince Edward Island and New Brunswick. Thus the blizzard and heavy snowfall had a crippling effect on the Halifax Urban Area for several days following the storm as public works personnel struggled to clear streets and roads. For several nights following the storm, a 10 p.m. curfew was implemented on residents in the Halifax Regional Municipality to permit operation of snow removal equipment. Due to a lack of space to displace the excess snow, the municipality had to receive permission from the federal government to begin dumping the snow into Halifax Harbour from federally owned docks in addition to the usual privately owned docks.\n\nThe storm also had an effect on the 2004 Special Olympic National Winter Games, which were being held in Charlottetown, PEI.\n\n\nAll schools were closed in Nova Scotia and Prince Edward Island as province-wide states of emergency were declared.\n\nWhile conditions on land proved to be serious, the storm produced hurricane-force winds out at sea with 10â15Â meter (32â49 ft) swells, prompting a special marine warning. A storm surge equivalent to that associated with a CategoryÂ 1 hurricane also affected portions of the Northumberland Strait in southeast New Brunswick and to a lesser extent Prince Edward Island.\n\nIn 2011, the Halifax Regional Municipality announced that it had submitted bills to the federal government totalling $19.9 million in the wake of Hurricane Juan and White Juan, and that it was still waiting for nearly $6 million.\n\nIt wasn't until ten years later that the federal government announced that it would pay the province $3.6 million under the Disaster Financial Assistance Arrangements program which helps cover the costs of evacuations, emergency shelters and infrastructure.\n\n"}
{"id": "729998", "url": "https://en.wikipedia.org/wiki?curid=729998", "title": "Zinc sulfide", "text": "Zinc sulfide\n\nZinc sulfide (or zinc sulphide) is an inorganic compound with the chemical formula of ZnS. This is the main form of zinc found in nature, where it mainly occurs as the mineral sphalerite. Although this mineral is usually black because of various impurities, the pure material is white, and it is widely used as a pigment. In its dense synthetic form, zinc sulfide can be transparent, and it is used as a window for visible optics and infrared optics.\n\nZnS exists in two main crystalline forms, and this dualism is often a salient example of polymorphism. In each form, the coordination geometry at Zn and S is tetrahedral. The more stable cubic form is known also as zinc blende or sphalerite. The hexagonal form is known as the mineral wurtzite, although it also can be produced synthetically. The transition from the sphalerite form to the wurtzite form occurs at around 1020Â Â°C. A tetragonal form is also known as the very rare mineral called polhemusite, with the formula (Zn,Hg)S.\n\nZinc sulfide, with addition of few ppm of suitable activator, exhibits strong phosphorescence (described by Nikola Tesla in 1893 ), and is currently used in many applications, from cathode ray tubes through X-ray screens to glow in the dark products. When silver is used as activator, the resulting color is bright blue, with maximum at 450 nanometers. Using manganese yields an orange-red color at around 590 nanometers. Copper gives long-time glow, and it has the familiar greenish glow-in-the-dark. Copper-doped zinc sulfide (\"ZnS plus Cu\") is used also in electroluminescent panels. It also exhibits phosphorescence due to impurities on illumination with blue or ultraviolet light.\n\nZinc sulfide is also used as an infrared optical material, transmitting from visible wavelengths to just over 12 micrometers. It can be used planar as an optical window or shaped into a lens. It is made as microcrystalline sheets by the synthesis from hydrogen sulfide gas and zinc vapour, and this is sold as FLIR-grade (Forward Looking IR), where the zinc sulfide is in a milky-yellow, opaque form. This material when hot isostatically pressed (HIPed) can be converted to a water-clear form known as Cleartran (trademark). Early commercial forms were marketed as Irtran-2 but this designation is now obsolete.\n\nZinc sulfide is a common pigment, sometimes called sachtolith. When combined with barium sulfate, zinc sulfide forms lithopone.\n\nFine ZnS powder is an efficient photocatalyst, which produces hydrogen gas from water upon illumination. Sulfur vacancies can be introduced in ZnS during its synthesis; this gradually turns the white-yellowish ZnS into a brown powder, and boosts the photocatalytic activity through enhanced light absorption.\n\nBoth sphalerite and wurtzite are intrinsic, wide-bandgap semiconductors. These are prototypical II-VI semiconductors, and they adopt structures related to many of the other semiconductors, such as gallium arsenide. The cubic form of ZnS has a band gap of about 3.54 electron volts at 300 kelvins, but the hexagonal form has a band gap of about 3.91 electron volts. ZnS can be doped as either an n-type semiconductor or a p-type semiconductor.\n\nThe phosphorescence of ZnS was first reported by the French chemist ThÃ©odore Sidot in 1866. His findings were presented by A. E. Becquerel, who was renowned for the research on luminescence. ZnS was used by Ernest Rutherford and others in the early years of nuclear physics as a scintillation detector, because it emits light upon excitation by x-rays or electron beam, making it useful for X-ray screens and cathode ray tubes. This property made zinc sulfide useful in the dials of radium watches.\n\nZinc sulfide is usually produced from waste materials from other applications. Typical sources include smelter, slag, and pickle liquors. It is also a by-product of the synthesis of ammonia from methane where zinc oxide is used to scavenge hydrogen sulfide impurities in the natural gas:\n\nIt is easily produced by igniting a mixture of zinc and sulfur. Since zinc sulfide is insoluble in water, it can also be produced in a precipitation reaction. Solutions containing Zn salts readily form a precipitate ZnS in the presence of sulfide ions (e.g., from HS).\nThis reaction is the basis of a gravimetric analysis for zinc.\n\n"}
