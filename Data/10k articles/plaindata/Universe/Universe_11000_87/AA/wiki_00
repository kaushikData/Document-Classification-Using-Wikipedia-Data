{"id": "40030297", "url": "https://en.wikipedia.org/wiki?curid=40030297", "title": "2003 Etobicoke gas explosion", "text": "2003 Etobicoke gas explosion\n\nThe 2003 Etobicoke gas explosion was a disaster which occurred on April 24, 2003, after a backhoe operated by Enbridge contractor Precision breached a pipeline on Bloor Street in the Etobicoke district of Toronto, Ontario. The resulting explosion destroyed a nearby two-story mixed commercial and residential building killing seven persons and injured another four. Enbridge and several other companies were fined for the disaster in 2011. The explosion was the worst pipeline disaster in Canada since the LaSalle Heights Disaster in 1965. A memorial garden was dedicated at the site of the explosion in 2008.\n\n"}
{"id": "2618310", "url": "https://en.wikipedia.org/wiki?curid=2618310", "title": "Aaron Lynch (writer)", "text": "Aaron Lynch (writer)\n\nAaron Lynch (February 18, 1957 – November 14, 2005) was an American writer, best known for his book \"Thought Contagion: How Belief Spreads Through Society\".\n\nAfter obtaining bachelor's degrees in mathematics and philosophy from the University of Illinois, Lynch accepted a position in 1979 as an engineering physicist at Fermilab where he spent some time working on the PDP-11 hardware project. In his spare time, he worked on developing his thesis into a book, which he planned to title \"Abstract Evolution.\"\n\nLynch worked extensively on the theoretical underpinnings of idea self-replication, developing a symbolic language and deriving mathematics from epidemiologic formulae to describe idea transmission through populations. While conducting a literature search for his book, Lynch discovered the work of anthropologist F.T. Cloak on socially transmitted technology in birds, and a brief proposal for a field of Memetics in Richard Dawkins' book, \"The Selfish Gene\", although Lynch was not aware of these authors' work until after his own theory was substantially developed. Early chapters of his book came to the attention of Douglas Hofstadter, who featured it in his \"Scientific American\" column \"Metamagical Themas\" in 1983. The first draft of the book was complete as early as 1984. A grant from a former colleague who had become a video-technology millionaire enabled Lynch to leave Fermilab in 1990 and concentrate full-time on writing.\n\nIn the early 1990s, he contributed theoretical and mathematical models on idea transmission to the \"Journal of Ideas\", the first scholarly journal dedicated to memetics.\n\nLynch's book, after considerable revision, was eventually published in 1996 as \"Thought Contagion: How Belief Spreads Through Society\".\n\nIn 1998, Lynch's \"Units, Events and Dynamics in Memetic Evolution\" provided much of the scholarly theoretical work omitted from \"Thought Contagion\". The paper detailed precise conceptual definitions of memetic terms, symbolic language to model idea replication, and mathematics to model population level idea transmission summarizing a decade of his conceptual work.\n\nIn August 2004, Lynch appeared to accuse \"Fast Company\" magazine and Seth Godin of plagiarism, claiming his complaint was backed, or even encouraged, by an unnamed 'major writers organization'.\n\nAaron Lynch died on November 14, 2005, at the age of 47, from anoxic encephalopathy after taking an overdose of an opiate-based pain killer, described as an accident in the Cook County, Illinois Coroner's Report. His remains are buried in Homewood Gardens in Homewood, Illinois.\n\nLynch first developed the themes of \"Thought Contagion\" in his 1979 undergraduate senior thesis entitled \"Abstract Evolution.\" The thesis explored the notion that an idea which can influence human behavior may blindly evolve the capacity to influence its own prevalence in the human population by motivating its human hosts to engage in behavior that spreads the idea. Just as a virus which elicits sneezes from its human host is more likely to survive by passing from host to host than a similar but non-sneeze-provoking virus, Lynch hypothesized that an idea which stimulated its host to proselytize, e.g., \"Go and make disciples of all nations\" (Gospel of Matthew 28:19), would be more likely to survive and become popular than an idea which did not elicit such activity. He identified other mechanisms which might also increase an idea's market share and longevity, such as influencing the human host to produce more children than it otherwise would, to instruct one's children in the belief earlier and more rigorously than one otherwise might, to isolate or effectively immunize oneself or one's children from exposure to competing ideas, to actively impede the communications of nonbelievers, or to utilize mass communications media to spread the idea to people that the host would never personally meet.\n\nCultural anthropology had long held that cultural beliefs and information—i.e., socially propagated ideas—survive and propagate because of the survival value they provide to the human groups that adopt them. Lynch embraced this notion of host-benefiting idea propagation, but his analysis added to this the notion that ideas could also propagate at the expense of their human hosts. He noted, for example, that beliefs which induced their hosts into self-sacrifice before sufficiently large audiences (e.g. earlier Christians refusing to worship the Emperor and dying serenely in Roman arenas or Islamist suicide bombers taping farewell videos for posthumous broadcast to worldwide audiences) could survive or even multiply just by capturing one or more hosts to replace the one it sacrificed.\n\nLynch questions whether ideas might be \"shopping\" for humans as well as vice versa, and asked whether humans own their most cherished beliefs or the beliefs own them. However, Lynch stopped short of suggesting that ideas have consciousness, will or planning abilities. To Lynch, ideas were information encoded in human neurons or other media. Like computer viruses, they are the products of human thinking and are in no way aware of or deliberating controlling their self-replicating abilities. However, unlike computer viruses, ideas often evolve new or improved contagious properties without intentional human design, through copying infidelity mutations or recombination into powerful new belief sets. According to Lynch, Natural selection determines which ideas survive and propagate successfully through human populations and which lose market share to the point of extinction.\n\nLynch's thesis offered an explanation for how not only true and useful ideas, but also unprovable or even false notions with sufficiently \"contagious\" properties could, over generations, become the predominant beliefs of whole societies. While he insisted that the contagiousness of ideas was largely independent of their truth value, as he immersed himself in this analysis, his frequently uttered motto became, \"People don't learn from each other's mistakes. They learn each other's mistakes.\"\n\nLynch's work encountered mixed reception among academics and among other memeticists. Some typical criticisms include the following:\n\n\nLynch addressed such criticism in the Journal of Artificial Societies and Social Simulation,\n\n"}
{"id": "3867687", "url": "https://en.wikipedia.org/wiki?curid=3867687", "title": "Advanced Distribution Automation", "text": "Advanced Distribution Automation\n\nAdvanced Distribution Automation (ADA) is a term coined by the IntelliGrid project in North America to describe the extension of intelligent control over electrical power grid functions to the distribution level and beyond. It is related to distribution automation that can be enabled via the smart grid. The electrical power grid is typically separated logically into transmission systems and distribution systems. Electric power transmission systems typically operate above 110kV, whereas Electricity distribution systems operate at lower voltages. Normally, electric utilities with SCADA systems have extensive control over transmission-level equipment, and increasing control over distribution-level equipment via distribution automation. However, they often are unable to control smaller entities such as Distributed energy resources (DERs), buildings, and homes. It may be advantageous to extend control networks to these systems for a number of reasons:\n\n\nThe goal of Advanced Distribution Automation is real-time adjustment to changing loads, generation, and failure conditions of the distribution system, usually without operator intervention. This necessitates control of field devices, which implies enough information technology (IT) development to enable automated decision making in the field and relaying of critical information to the utility control center. The IT infrastructure includes real-time data acquisition and communication with utility databases and other automated systems. Accurate modeling of distribution operations supports optimal decision making at the control center and in the field.\n\nAutomated control of devices in distribution systems is closed-loop control of switching devices, voltage controllers, and capacitors based on recommendations of the distribution optimization algorithms.\n\nDistribution System Reliability: Distribution Automation currently increased system reliability, and new technology such as solid state transformers\n\nIncreasing Utilization of Existing Infrastructure: As a component of ADA infrastructure, the new system concepts will enable more efficient operation of the power system, allowing closer control of voltage profiles (e.g. conservation voltage reduction, closely related to voltage optimisation) and maximization of energy throughput.\n\nDistribution System of the Future: The new system concepts will enable ADA functions in the distribution system that contribute to outage prevention and recovery, optimal system performance under changing conditions, and reduced operating costs.\nDistribution automation technologies are commercially available for wide scale utility deployment. The key is identifying and unlocking the values which provide the best return on investment in ways that can be measured by utilities. Applications which may have greatest potential are operations and efficiency, management of peak loads via [demand response], predictive technologies and communications for equipment, and system restoration technologies.\n\nNew transformer technologies are being considered by EPRI, including solid state transformers that can reduce power losses due to step-up and step-down voltages conversion.\n\nFor a full listing of the capabilities being proposed by the IntelliGrid project, please see the first external link below.\n\n"}
{"id": "51388", "url": "https://en.wikipedia.org/wiki?curid=51388", "title": "Bioenergy Europe", "text": "Bioenergy Europe\n\nBioenergy Europe (former AEBIOM) is a European trade association opens to national biomass associations and bioenergy companies active in Europe. AEBIOM was founded in 1990 under the leadership of french senator with the aim to promote energy generation from biomass in all its forms (Bio-power, Bio-heat, Biofuel for transport).\n\nBioenergy Europe is the umbrella organisation of the European Pellet Council (EPC), and the International Biomass Torrefaction Council (IBTC).. \n\nBioenergy Europe is managing two international certifications for wood fuels. ENplus® certifies wood pellets quality and GoodChips® which aims at guaranteeing wood chips and hog fuel quality . \n\nAs a European trade federation, Bioenergy Europe governance is ensured by its members (see list below) and structured around a General Assembly, a Board of Directors and a Core Groups that decide on the strategic orientations and political lines of the organisation based on the advises of Bioenergy Europe's Working Groups and Secretariat. Since 2016, Didzis Palejs (Latvia) is president of the organisation taking over Gustav Melin (Sweden) who was president from 2010 to 2016. \n\n\n\n\n"}
{"id": "172928", "url": "https://en.wikipedia.org/wiki?curid=172928", "title": "BoPET", "text": "BoPET\n\nBoPET (biaxially-oriented polyethylene terephthalate) is a polyester film made from stretched polyethylene terephthalate (PET) and is used for its high tensile strength, chemical and dimensional stability, transparency, reflectivity, gas and aroma barrier properties, and electrical insulation.\n\nA variety of companies manufacture boPET and other polyester films under different brand names. In the UK and US, the most well-known trade names are Mylar, Melinex, and Hostaphan.\n\nBoPET film was developed in the mid-1950s, originally by DuPont, Imperial Chemical Industries (ICI), and Hoechst.\n\nIn 1955 Eastman Kodak used Mylar as a support for photographic film and called it \"ESTAR Base\". The very thin and tough film allowed reels to be exposed on long-range U-2 reconnaissance flights.\n\nIn 1964, NASA launched Echo II, a diameter balloon constructed from a thick mylar film sandwiched between two layers of thick aluminum foil bonded together.\n\nThe manufacturing process begins with a film of molten polyethylene terephthalate (PET) being extruded onto a chill roll, which quenches it into the amorphous state. It is then biaxially oriented by drawing. The most common way of doing this is the sequential process, in which the film is first drawn in the machine direction using heated rollers and subsequently drawn in the transverse direction, i.e. orthogonally to the direction of travel, in a heated oven. It is also possible to draw the film in both directions simultaneously, although the equipment required for this is somewhat more elaborate. Draw ratios are typically around 3 to 4 in each direction.\n\nOnce the drawing is completed, the film is \"heat set\" or crystallized under tension in the oven at temperatures typically above . The heat setting step prevents the film from shrinking back to its original unstretched shape and locks in the molecular orientation in the film plane. The orientation of the polymer chains is responsible for the high strength and stiffness of biaxially oriented PET film, which has a typical Young's modulus of about 4 GPa. Another important consequence of the molecular orientation is that it induces the formation of many crystal nuclei. The crystallites that grow rapidly reach the boundary of the neighboring crystallite and remain smaller than the wavelength of visible light. As a result, biaxially oriented PET film has excellent clarity, despite its semicrystalline structure.\n\nIf it were produced without any additives, the surface of the film would be so smooth that layers would adhere strongly to one another when the film is wound up, similar to the sticking of clean glass plates when stacked. To make handling possible, microscopic inert inorganic particles are usually embedded in the PET to roughen the surface of the film.\n\nBiaxially oriented PET film can be metallized by vapor deposition of a thin film of evaporated aluminium, gold, or other metal onto it. The result is much less permeable to gases (important in food packaging) and reflects up to 99% of light, including much of the infrared spectrum. For some applications like food packaging, the aluminized boPET film can be laminated with a layer of polyethylene, which provides sealability and improves puncture resistance. The polyethylene side of such a laminate appears dull and the PET side shiny.\n\nOther coatings, such as conductive indium tin oxide (ITO), can be applied to boPET film by sputter deposition.\n\nUses for boPET polyester films include, but are not limited to:\n\n\n\n\n\n\n\n\n\n"}
{"id": "994556", "url": "https://en.wikipedia.org/wiki?curid=994556", "title": "Born–Haber cycle", "text": "Born–Haber cycle\n\nThe Born–Haber cycle is an approach to analyze reaction energies. It was named after the two German scientists Max Born and Fritz Haber, who developed it in 1919. It was also independently formulated by Kasimir Fajans. The cycle is concerned with the formation of an ionic compound from the reaction of a metal (often a Group I or Group II element) with a halogen or other non-metallic element such as oxygen.\n\nBorn–Haber cycles are used primarily as a means of calculating lattice energy (or more precisely enthalpy), which cannot otherwise be measured directly. The lattice enthalpy is the enthalpy change involved in the formation of an ionic compound from gaseous ions (an exothermic process), or sometimes defined as the energy to break the ionic compound into gaseous ions (an endothermic process). A Born–Haber cycle applies Hess's law to calculate the lattice enthalpy by comparing the standard enthalpy change of formation of the ionic compound (from the elements) to the enthalpy required to make gaseous ions from the elements.\n\nThis latter calculation is complex. To make gaseous ions from elements it is necessary to atomise the elements (turn each into gaseous atoms) and then to ionise the atoms. If the element is normally a molecule then we first have to consider its bond dissociation enthalpy (see also bond energy). The energy required to remove one or more electrons to make a cation is a sum of successive ionization energies; for example, the energy needed to form Mg is the ionization energy required to remove the first electron from Mg, plus the ionization energy required to remove the second electron from Mg. Electron affinity is defined as the amount of energy released when an electron is added to a neutral atom or molecule in the gaseous state to form a negative ion.\n\nThe Born–Haber cycle applies only to fully ionic solids such as certain alkali halides. Most compounds include covalent and ionic contributions to chemical bonding and to the lattice energy, which is represented by an extended Born-Haber thermodynamic cycle. The extended Born–Haber cycle can be used to estimate the polarity and the atomic charges of polar compounds.\n\nThe enthalpy of formation of lithium fluoride (LiF) from its elements lithium and fluorine in their stable forms is modeled in five steps in the diagram:\n\n\nThe same calculation applies for any metal other than lithium or any non-metal other than fluorine.\n\nThe sum of the energies for each step of the process must equal the enthalpy of formation of the metal and non-metal, formula_1.\n\n\nThe net enthalpy of formation and the first four of the five energies can be determined experimentally, but the lattice energy cannot be measured directly. Instead, the lattice energy is calculated by subtracting the other four energies in the Born–Haber cycle from the net enthalpy of formation.\n\nThe word cycle refers to the fact that one can also equate to zero the total enthalpy change for a cyclic process, starting and ending with LiF(s) in the example. This leads to \nwhich is equivalent to the previous equation.\n\n\n"}
{"id": "29413418", "url": "https://en.wikipedia.org/wiki?curid=29413418", "title": "Bumping (chemistry)", "text": "Bumping (chemistry)\n\nBumping is a phenomenon in chemistry where homogenous liquids boiled in a test tube or other container will superheat and, upon nucleation, rapid boiling will expel the liquid from the container. In extreme cases, the container may be broken.\n\nBumping occurs when liquids are heated or has its pressure reduced very rapidly, typically in smooth, clean glassware. The hardest part of bubble formation is the formation of the small bubble; once a bubble has formed, it can grow quickly. Because the liquid is typically above its boiling point, when the liquid finally starts to boil, a large vapor bubble is formed that pushes the liquid out of the test tube, typically at high speed. This rapid expulsion of boiling liquid poses a serious hazard to others and oneself in the lab. Furthermore, if a liquid is boiled and cooled back down, the chance of bumping increases on each subsequent boil, because each heating cycle progressively de-gasses the liquid, reducing the number of remaining nucleation sites.\n\nThe most common way of preventing bumping is by adding one or two boiling chips to the reaction vessel. However, these alone may not prevent bumping and for this reason it is advisable to boil liquids in a boiling tube, a boiling flask, or an Erlenmeyer flask. In addition, heating test tubes should never be pointed towards any person, just in case bumping does occur. Whenever a liquid is cooled below its boiling point and re-heated to a boil, a new boiling chip will be needed, as the pores in the old boiling chip tend to fill with solvent, rendering it ineffective.\n\nA sealed capillary tube can also be placed in a boiling solution to provide a nucleation site, reducing the bumping risk and allowing its easy removal from a system.\n\nStirring a liquid also lessens the chances of bumping, as the resulting vortex breaks up any large bubbles that might form, and the stirring itself creates bubbles.\n"}
{"id": "8786647", "url": "https://en.wikipedia.org/wiki?curid=8786647", "title": "Cash (unit)", "text": "Cash (unit)\n\nCash or li () is a traditional Chinese unit of weight.\n\nThe terms \"cash\" or \"le\" were documented to have been used by British explorers in the 1830s when trading in Qing territories of China.\n\nUnder the Hong Kong statute of the Weights and Mesaures Ordinance, 1 cash is about . Currently, it is candareen or catty, namely .\n\n\n"}
{"id": "8491596", "url": "https://en.wikipedia.org/wiki?curid=8491596", "title": "Davydov soliton", "text": "Davydov soliton\n\nDavydov soliton is a quantum quasiparticle representing an excitation propagating along the protein α-helix self-trapped amide I. It is a solution of the Davydov Hamiltonian. It is named for the Soviet and Ukrainian physicist Alexander Davydov. The Davydov model describes the interaction of the amide I vibrations with the hydrogen bonds that stabilize the α-helix of proteins. The elementary excitations within the α-helix are given by the phonons which correspond to the deformational oscillations of the lattice, and the excitons which describe the internal amide I excitations of the peptide groups. Referring to the atomic structure of an α-helix region of protein the mechanism that creates the Davydov soliton (polaron, exciton) can be described as follows: vibrational energy of the C=O stretching (or amide I) oscillators that is localized on the α-helix acts through a phonon coupling effect to distort the structure of the α-helix, while the helical distortion reacts again through phonon coupling to trap the amide I oscillation energy and prevent its dispersion. This effect is called \"self-localization\" or \"self-trapping\". Solitons in which the energy is distributed in a fashion preserving the helical symmetry are dynamically unstable, and such symmetrical solitons once formed decay rapidly when they propagate. On the other hand, an asymmetric soliton which spontaneously breaks the local translational and helical symmetries possesses the lowest energy and is a robust localized entity.\n\nDavydov's Hamiltonian is formally similar to the Fröhlich-Holstein Hamiltonian for the interaction of electrons with a polarizable lattice. Thus the Hamiltonian of the energy operator formula_1 is\n\nwhere formula_3 is the quasiparticle (exciton) Hamiltonian, which describes the motion of the amide I excitations between adjacent sites; formula_4 is the phonon Hamiltonian, which describes\nthe vibrations of the lattice; and formula_5 is the interaction Hamiltonian, which describes the interaction of the amide I excitation with the lattice.\n\nThe quasiparticle (exciton) Hamiltonian formula_3 is:\n\nwhere the index formula_11 counts the peptide groups along the α-helix spine, the index formula_12 counts each α-helix spine, formula_13 J is the energy of the amide I\nvibration (CO stretching), formula_14 J is the dipole-dipole coupling energy between a particular amide I bond and those ahead and behind along the same spine, formula_15 J is the\ndipole-dipole coupling energy between a particular amide I bond and those on adjacent spines in the same unit cell of the protein α-helix, formula_16 and formula_17 are respectively\nthe boson creation and annihilation operator for a quasiparticle at the peptide group. formula_18\n\nThe phonon Hamiltonian formula_4 is\n\nwhere formula_21 is the displacement operator from the equilibrium position of the peptide group formula_18, formula_23 is the momentum operator of the peptide group formula_18, M is the mass of each peptide group, and formula_25 N mformula_26 is an effective elasticity coefficient of the lattice (the spring constant of a hydrogen bond).\n\nFinally, the interaction Hamiltonian formula_5 is\n\nwhere formula_29 pN is an anharmonic parameter arising from the coupling between the quasiparticle (exciton) and the lattice displacements (phonon) and parameterizes the strength of the exciton-phonon interaction. The value of this parameter for α-helix has been determined via comparison of the theoretically calculated absorption line shapes with the experimentally measured ones.\n\nThe mathematical techniques that are used to analyze Davydov's soliton are similar to some that have been developed in polaron theory. In this context the Davydov's soliton corresponds to a polaron that is (i) \"large\" so the continuum limit approximation is justified, (ii) \"acoustic\" because the self-localization arises from interactions\nwith acoustic modes of the lattice, and (iii) \"weakly coupled\" because the anharmonic energy is small compared with the phonon bandwidth.\n\nThe Davydov soliton is a \"quantum quasiparticle\" and it obeys Heisenberg's uncertainty principle. Thus any model that does not impose translational invariance is flawed by construction. Supposing that the Davydov soliton is localized to 5 turns of the α-helix results in significant uncertainty in the velocity of the soliton formula_30 m/s, a fact that is obscured if one models the Davydov soliton as a classical object.\n\nThere are three possible fundamental approaches towards Davydov model: (i) the quantum theory, in which both the amide I vibration (excitons) and the lattice site motion (phonons) are treated quantum mechanically; (ii) the mixed quantum-classical theory, in which the amide I vibration is treated quantum mechanically but the lattice is classical; and (iii) the classical theory, in which both the amide I and the lattice motions are treated classically.\n"}
{"id": "261366", "url": "https://en.wikipedia.org/wiki?curid=261366", "title": "Dimer (chemistry)", "text": "Dimer (chemistry)\n\nA dimer () (\"di-\", \"two\" + \"-mer\", \"parts\") is an oligomer consisting of two monomers joined by bonds that can be either strong or weak, covalent or intermolecular. The term \"homodimer\" is used when the two molecules are identical (e.g. A-A) and \"heterodimer\" when they are not (e.g. A-B). The reverse of dimerisation is often called dissociation. When two oppositely charged ions associate into dimers, they are referred to as \"Bjerrum pairs\".\n\nCarboxylic acids form dimers by hydrogen bonding of the acidic hydrogen and the carbonyl oxygen when anhydrous. For example, acetic acid forms a dimer in the gas phase, where the monomer units are held together by hydrogen bonds. Under special conditions, most OH-containing molecules form dimers, e.g. the water dimer.\n\nBorane (\"BH\") occurs as the dimer diborane (BH), due to the high Lewis acidity of the boron center.\n\nExcimers and exciplexes are excited structures with a short lifetime. For example, noble gases do not form stable dimers, but do form the excimers Ar*, Kr* and Xe* under high pressure and electrical stimulation.\n\nMolecular dimers are often formed by the reaction of two identical compounds e.g.: 2A → A-A. In this example, monomer \"A\" is said to dimerise to give the dimer \"A-A\". An example is a diaminocarbene, which dimerise to give a tetraaminoethylene:\nCarbenes are highly reactive and readily form bonds.\n\nDicyclopentadiene is an asymmetrical dimer of two cyclopentadiene molecules that have reacted in a Diels-Alder reaction to give the product. Upon heating, it \"cracks\" (undergoes a retro-Diels-Alder reaction) to give identical monomers:\n\nMany nonmetallic elements occur as dimers: hydrogen, nitrogen, oxygen, the halogens, i.e. fluorine, chlorine, bromine and iodine. Noble gases can form dimers linked by van der Waals bonds, for example dihelium or diargon. Mercury occurs as a mercury(I) cation (Hg), formally a dimeric ion. Other metals may form a proportion of dimers in their vapour. Known metallic dimers include Li, Na, K, Rb and Cs.\n\nMany small organic molecules, most notably formaldehyde, easily form dimers. The dimer of formaldehyde (CHO) is dioxetane (CHO).\n\nIn the context of polymers, \"dimer\" also refers to the degree of polymerization 2, regardless of the stoichiometry or condensation reactions. \n\nThis is applicable to disaccharides. For example, cellobiose is a dimer of glucose, even though the formation reaction produces water:\nHere, the dimer has a stoichiometry different from the pair of monomers.\n\nAmino acids can also form dimers, which are called dipeptides. An example is glycylglycine, consisting of two glycine molecules joined by a peptide bond. Other examples are aspartame and carnosine.\n\nPyrimidine dimers are formed by a photochemical reaction from pyrimidine DNA bases. This cross-linking causes DNA mutations, which can be carcinogenic, causing skin cancers.\n\n"}
{"id": "319379", "url": "https://en.wikipedia.org/wiki?curid=319379", "title": "Dodge Caravan", "text": "Dodge Caravan\n\nThe Dodge Caravan is a minivan manufactured and marketed by Fiat Chrysler Automobiles (and predecessor Chrysler companies) and marketed under the Dodge brand. Introduced for the 1984 model year, it is the longest-used nameplate currently in use by Chrysler. Introduced as the Dodge version of the Chrysler minivans alongside the Plymouth Voyager (and the later Chrysler Town & Country), the Dodge Caravan is currently in its fifth generation of production.\n\nLargely marketed in the United States and Canada, outside North America, the Dodge Caravan was marketed as the Chrysler Voyager. In North America, the Grand Caravan served as the basis for the Volkswagen Routan assembled by Chrysler. Since 1984, more than 11 million Chrysler minivans (including rebadged variants and export versions) have been sold worldwide.\n\nSince its 1983 introduction, the Dodge Caravan has been assembled at Windsor Assembly, in Windsor, Ontario, Canada; prior to 2010, Saint Louis Assembly (in Fenton, Missouri) was an additional source of production. \n\nLee Iacocca and Hal Sperlich had conceived their idea for this type of vehicle during their earlier tenure at Ford Motor Company. Henry Ford II rejected the idea (and a prototype) of a minivan in 1974. Iaccoca followed Sperlich to Chrysler, and together they created what was internally designated the T-115 minivan – a prototype that was to become the Caravan and Voyager, known in initial marketing as the Magic-wagons. Chrysler introduced the Dodge Caravan and the Plymouth Voyager in November 1983 for the 1984 model year, using the Chrysler S platform, an extended derivative of the Chrysler K platform. The Renault Espace launched in Europe the same year, and Chrysler began selling the Chrysler Voyager in Europe four years later.\n\nDuring the 1960s, Dodge did sell a small cargo van adapted for passenger use, called the Dodge A100 which was discontinued in 1970 and replaced by the larger, fullsized Dodge B-series van in 1971.\n\nInterior trim, controls, and instrumentation were borrowed from the Chrysler K platform, and coupled with the lower floor enabled by the front-wheel-drive, the Caravan featured car-like ease of entry. There were three trim levels: base, SE, and LE.\n\nBase vans came equipped for five passengers in two rows of seating. The LE came with seven passengers standard in three rows of seating. The base van had two bucket seats with attached armrests and open floor space between them in the front, a three-person bench seat in the second row. The seven-passenger came with two bucket seats with attached armrests and open floor space between them in the front, a two-person bench seat in the second row, and a three-person bench seat in the back row. The two bench seats in the rear were independently removable, and the large three-person bench could also be installed in the second row location via a second set of attachment points on the van's floor, ordinarily hidden with snap-in plastic covers. This configuration allowed for conventional five person seating with a sizable cargo area in the rear. The latching mechanisms for the benches were easy to operate though removing and replacing the seats typically required two adults. A front low-back 60/40 split bench, accommodating a third front passenger in the middle, was offered in the SE trim level in 1985 only, allowing for a maximum of eight passengers. This configuration was subsequently dropped. Base model curb weight 2,910 lbs.\n\nSafety features consisted of 3-point seat belts for the front two passengers, with simple lap belts for the rear five. Seats on base models and cloth-trimmed SEs had no headrests, which were not mandated due to the van's \"light truck\" legal status. However, the two front seats were equipped with non-adjustable headrests on the LE model and in conjunction with vinyl upholstery on the SE. Side-impact reinforcements were mandated, and were at all seating positions front and rear. Neither airbags nor anti-lock braking systems were available.\n\nAccess to the rear rows of seating was by a large passenger-side sliding door enabling easy access in confined situations, e.g., parking. Because only one sliding door was offered, the smaller 2nd row bench seat was shifted to the driver's side of the van, facilitating passenger access to the 3rd row seat. To facilitate variable cargo storage behind the rear seat, the seat could be adjusted forward in two increments, the first of which removed roughly of legroom from the back row passengers, and the second of which would push the bench all the way to the back of the 2nd row, making the seats unusable. The seat back of the rear bench could also be folded forward, providing a flat cargo shelf. The smaller 2nd row bench was not adjustable, nor foldable; it could only be removed entirely.\n\nCargo access to the rear was via a hatchback, similar to the one on the K platform station wagons. The hatch was hinged at the top and held open by gas struts.\n\nA long wheelbase variant, marketed as the Grand Caravan, was introduced in May 1987. It allowed more cargo space behind the rear seat.\n\nA cargo version of the Caravan, called the Mini Ram Van, was also introduced for 1984, with a flat floored cargo space four feet tall and with four feet between the wheel wells. The load capacity was . It was renamed the Caravan C/V for 1989 and was then discontinued after 1995. It was initially available with the short wheelbase; a long-wheelbase variant was introduced alongside the Grand Caravan. Unique to the Caravan C/V was the option of either having the traditional hatch door in the back or the optional swing-out bi-parting doors (with or without windows), similar to those of more traditional cargo vans. These doors were made of fiberglass and required the C/V vans to be \"drop shipped\", as these doors were custom installed by another vendor. Also based on the Mini Ram and C/V were aftermarket conversion vans sold through official Chrysler dealers and from the conversion companies themselves.\n\nBoth a three-speed TorqueFlite automatic transmission and a five-speed manual were available with all inline-four engines, including the turbocharged 2.5-liter (this was a rare combination). The Plymouth Voyager, which was a rebadged version of the Caravan, was also available with a manual transmission. The Chrysler Town & Country, which was a more luxurious repackaged version of the Caravan, had no manual transmission option. Manual transmissions were not available on V6 models of the passenger Caravan, but were an option on the Mini Ram Van and Caravan C/V's long wheelbase models with a 3.0 L V6.\n\nThe V6 engines were only offered with the venerable fully hydraulically operated TorqueFlite, until the computer controlled Ultradrive four-speed automatic became available in 1989. The Ultradrive offered much better fuel economy and responsiveness, particularly when paired with the inline-four engine. However, it suffered from reliability problems, usually stemming from what is known as \"gear hunt\" or \"shift busyness\", resulting in premature wear of the internal clutches. It also required an uncommon type of automatic transmission fluid and is not clearly labeled as such, leading many owners to use the more common Dexron II rather than the specified \"Mopar ATF+3\", resulting in transmission damage and eventual failure.\n\nThe Ultradrive received numerous design changes in subsequent model years to improve reliability, and many early model transmissions would eventually be retrofitted or replaced with the updated versions by dealers, under warranty. These efforts were mostly successful, and most first-generation Caravans eventually got an updated transmission.\n\nFor the first three years of production, two engines were offered in the Caravan – both inline-4 engines with 2 barrel carburetors. The base 2.2 L was borrowed from the Chrysler K-cars, and produced horsepower. The higher performance fuel-injected version of the 2.2 L engine later offered in the K-cars was never offered in the Caravan, and the 2-bbl version would remain the base power plant until mid-1987. Alongside the 2.2 L, an optional Mitsubishi 2.6 L engine was available, producing horsepower.\n\nIn mid-1987, the base 2.2 L I4 was replaced with a fuel-injected 2.5 L I4, which produced , while the Mitsubishi \"G54B\" I4 was replaced with the new fuel-injected 3.0 L Mitsubishi V6 producing in March of that year.\nShortly thereafter in model year 1989, a more powerful engine became optional, with a turbocharged version of the base 2.5 L producing . Revisions to the Mitsubishi V6 upped its output to that same year, and in 1990 a new 3.3 L V6 was added to the option list. The V6 engines became popular as sales of the 2.5 turbo dwindled and it was dropped at the end of the year. In these years, the ES model debuted (short wheelbase only) to highlight the new engines, the turbo 2.5 in particular. The ES was introduced to the long wheelbase Grand Caravan for 1991 and continued throughout 2003, before it was discontinued and replaced with the SXT.\n\n\nFrom 1991 through 1995, the Caravan used the Chrysler AS platform; they were the last minivans derived from the Chrysler K platform. The increasing popularity of the short wheelbase Caravan had a dramatic effect on the captive import from Mitsubishi, called the Dodge Colt Vista, and was no longer imported to North America starting in 1991.\n\nThe second generation had features including:\n\n1992 models included revised roof-racks and doorhandles. 1993 marked the final year for optional woodgrain and wire wheels on higher level models. 1994 models had a redesigned interior, with slightly different seat contours/fabrics, along with a new dash, in order to accommodate a passenger-side air bag.\n\n1994 exterior trim changes included a body colored grill and moldings on certain models, as well as the addition of a one-year only \"10th Anniversary Edition\" package. Available as an option on a mid-level Caravan and Grand Caravan (also offered on Voyager models as well), the package featured 2-tone paint schemes, which included a contrasting light gray colored lower break, along with a gold fender badge. Only select colors were available on this model.\n\n\nThe third-generation Chrysler minivans were available in long- and short– wheelbase models; three- and four-door configurations; and eight different powertrains, including electric and compressed natural gas; on a single, flexible platform.\n\nIn development for nearly 5 years from early 1990 (full development from 1991) to December 26, 1994 (final design by Don Renkert was approved on September 23, 1991 and frozen in May 1992), the 1996 model was introduced at the 1995 North American International Auto Show using the Chrysler S platform. It included a number of innovations, including a driver's side sliding door (optional initially, to become standard equipment later), a first for Chrysler and a non-compact minivan for the United States and Canada (the Honda Odyssey had introduced the first four-door non-compact minivan for the United States & Canada in 1994, although the first-generation Odyssey had conventional hinged rear doors). With Generation III, Chrysler introduced a seat management system marketed as \"Easy Out Roller Seats\". A conventional door handle and lock was added to the rear hatch, eliminating the confusing pop-and-lift maneuver which had been required on earlier models.\n\nBase models of the Caravan were offered in most states with either a 2.4 L four-cylinder or the 3.0 L Mitsubishi 6G72 V6 engine, except in several northeastern states, where the Mitsubishi did not meet emissions standards. In those areas, the 3.3 L engine was offered as the V6 option from 1997 through 2000.\n\nThe 1996 Caravan, along with the Plymouth Voyager and the Chrysler Town & Country won the North American Car of the Year award. The Caravan itself won \"Motor Trend\" magazine's Car of the Year 1996 and appeared on the Car and Driver Ten Best for 1996 and 1997. 1999 also saw the addition of a one-year only 15th anniversary \"Platinum Edition\", to mark Caravan's 15th year of production. This package was offered on various trim levels, and included Platinum Metallic paint, and fender badges. The 2000 model year offered packages which included the \"2000+\" and \"Millennium\" package, however these were little more than unique fender badges on vans with popular equipment.\n\nThe Caravan received minor updates in 1997 for the 1998 model year. These changes came in the form of new colors, new wheels for trims above SE, new interior fabric, optional heated seats and a new design for the optional auto-dimming rear view mirror. In calendar year 1998, the Caravan's HVAC vents on the driver's side and center of the dashboard were updated to have a more conventional design. Later that year, the 1999 Caravan received new front styling on all trims above SE, while the Sport and ES models received even sportier styling. The ES model was the first minivan to receive the \"AutoStick\" transmission and 17 inch wheels. A cargo net between the driver and front passenger seats was added. Color keyed door and lift-gate handles were standard across the whole range, in addition to a new keyless entry remote. Base and SE models had options for a spoiler as well as color keyed bumpers and trim (grey or color molded bumpers and trim were standard). The driver's side sliding door became standard. Chrysler had updates of the Plymouth Voyager in 1996 for the 1997 model year and the Chrysler Town & Country in 1997 for the 1998 model year, prior to the 1998–2007 DaimlerChrysler era; it was the only exterior update of the NS Dodge Caravan.\n\n\nOther plans for this model year included three minivan concepts all to be made in the Windsor Assembly, the Dodge Caravan R/T, Voyager XG, and the Chrysler Pacifica 1999 concept. The Caravan R/T (originally ESS) was to include the most powerful engine ever for a minivan, rated at . It had two Dodge Viper hoodscoops, a brushed aluminum instrument panel, racing-style pedals, and black and white rubber flooring. The Voyager XG was more rugged, featured a diesel engine and manual transmission, and included many outdoor amenities, such as a built-in ice pack. The Chrysler Pacifica, based on the Town & Country, was more luxurious, had power leather seats and footrests, overhead bins and lighting, an LHS grille, and roof-long skylights. The skylight feature was used by Nissan in the Quest. The Pacifica actually did come to be in 2004, based on the fifth generation Caravan, except that it became a crossover SUV rather than a minivan; the nameplate was eventually applied to a minivan in 2016.\n\nIn 1999, Dodge introduced the Caravan EPIC, a fully electric minivan. The EPIC was powered by 28 12-volt NiMH batteries and was capable of traveling up to on a single charge. The EPIC was sold as a fleet-only lease vehicle. Production of the EPIC was discontinued in 2001. Only a few hundred of these vehicles were produced and sold. After the leases expired they were returned and crushed. Approximately 10 vans remain in private hands today.\n\nThe 1996–2000 Dodge Grand Caravan received a \"Marginal\" rating in the Insurance Institute for Highway Safety's 40 mph offset test. The structural performance and restraints were graded \"Acceptable\", but the foot injuries were very high.\n\nIn the NHTSA crash tests, it received 4 stars for the driver and front passenger in the frontal-impact. In the side-impact test, it received 5 stars for the driver, and 3 stars for the rear occupant, and resulted in a fuel leak that could cause a fire hazard.\n\nUnveiled at the 2000 North American International Auto Show (NAIAS) on Monday, January 10, 2000, the redesigned 2001 Dodge Caravan and 2001 Chrysler Town & Country were released for sale in August 2000. The release was part of a promotional tie-in with Nabisco, which unveiled their new \"Mini Oreos\" inside the van during the unveiling. The first vans rolled off the line at the Windsor Assembly Plant on July 24. The fourth generation vans were available in the trim levels; \"SE\", \"Sport\", \"SXT\", \"AWD Sports\", \"base\" model, \"AWD Choice\", \"eL\", \"C/V\", \"ES\", \"EX\", \"AWD Wagon\", and \"SXT All-Wheel-Drive\".\n\nIn development from February 1996 to December 1999, the Generation IV minivans were based on the Chrysler RS platform and featured a larger body frame with modified headlights and taillights. Design work was done by Brandon Faurote from January 1997 and reached production approval in 1998.\n\nIn addition to other detailed changes, power sliding doors and a power hatch became available as options. The Mitsubishi 3.0 L V6, which no longer met emissions standards in California and the northeastern U.S., was discontinued, and a more powerful 3.8L engine, based on the 3.3L, became available. All Wheel Drive continued to be offered on high end models. Other innovative available features included remote operated sliding doors and rear hatch, which could be opened and closed at the push of a button, either inside the vehicle, or with the keyless entry fob.\n\nIn the 2002 model year, DaimlerChrysler stopped using the \"DODGE\" badges on the front doors, like with all Dodge vehicles.\n\nIn 2003, the Caravan C/V and Grand Caravan C/V returned after having been discontinued in 1995. The C/V featured the option of deleted side windows (replaced by composite panels), optional rear seats, a cargo floor made of plastic material similar to pickup truck bedliners, rubber flooring in lieu of carpeting and normal hatch at the rear. Minor changes were made to the Grand Caravan ES including many of the features included in Option Group 29S becoming standard, the 17 inch Titan Chrome wheels no longer being an option replaced with standard 16 inch chrome wheels, and the disappearance of the AutoStick Transmission option. This year also saw the appearance of an optional factory-installed rear seat DVD system with single disc player mounted below the HVAC controls.\n\n2004 offered an exclusive one year only \"Anniversary Edition\" package to mark Caravan's 20th year in production. This package was offered on higher level SXT models, and included chrome wheels, body color moldings, special interior accents and a unique fender badge.\n\n2005 changes to the Caravan included a revised grille, new foglight fascia, and a system of in-floor folding second and third row seats, marketed as \"Stow 'n Go seating\".\n\nProduction of this generation continued in China from 2008, when the Taiwanese Chrysler Town & Country production line was relocated there, until late 2010 when the fifth generation Chrysler Voyager was introduced to the Chinese market. The Caravan was subsequently replaced by the Journey, although a page for the Caravan still exists on the Dodge China site. The Chinese Caravan was produced alongside the Town & Country, now using the Grand Voyager nameplate, by Soueast, and did not share any aesthetic components with the North American Caravan aside from the wheels. Instead, the Chinese Caravan was identical to the Taiwanese Town & Country, aside from the lack of chrome trim on the exterior door panels, and used a modified version of the Town & Country front bumper with a Dodge grille. Chinese vans were equipped with Mitsubishi 6G72 engines, and came in three trim levels: Classic, SXT, and Luxury.\n\nIn Canada, the 3.3 L V6 was standard on all models.\n\nThe 2001 model of this version earned a \"Poor\" rating in the Insurance Institute for Highway Safety\n's 40 mph offset test. It did protect its occupants reasonably well, and the dummy movement was well controlled, however, a fuel leak occurred. Chrysler corrected this problem starting with the 2002 models, moving it up to an \"Acceptable\" rating.\n\nThe 2006 model year brought optional side curtain airbags and a stronger B-pillar, which was tested by the Insurance Institute for Highway Safety's side impact crash test. With the side airbags, it got an \"Acceptable\" rating. For the driver, there is a chance of serious neck injuries, rib fractures and/or internal organ injuries. The rear passengers, however, could leave this accident unharmed, as there is a low risk of significant injury in a crash of this severity for them.\n\nThe fifth generation van had debuted at the 2007 North American International Auto Show with exterior styling by Ralph Gilles. Beginning with Generation V in model year 2008, Chrysler only made the long wheelbase Grand Caravan. With discontinuation of the short-wheelbase Caravan, Dodge offered the Journey on nearly an identical wheelbase and as a crossover rather than a minivan. Although the SWB model, which had accounted for half of all sales in Canada, cost approximately $2,000 less and offered a four-cylinder engine option with improved fuel economy, Chrysler executives stated the SWB Caravan was discontinued to accommodate new features offered in the Grand Caravan, consistent with the demands of the majority of the minivan market.\n\nA new six-speed automatic transmission became standard with the 3.8 L V6 and the new 4.0 L V6. The four-speed automatic transmission is standard with the 3.3 L Flex-Fuel V6. This generation of Grand Caravan and its Town & Country counterpart were not available with an all-wheel-drive system. The previously unavailable Electronic Stability Control was made standard on this generation.\n\nChrysler introduced a seat management system marketed as \"Swivel'n Go\" seating, the MyGIG entertainment system (a stereo with built in hard drive for recording, storing, and playing music), second and third row video screens, powered second row windows, standard side curtain airbags, and dashboard-mounted transmission controls. The gear shift lever moved to the instrument panel, the location used by competitors.\n\nThe market shifted briefly away from minivans and SUVs with the gasoline price spikes of the earlier part of 2008. This trend began to reverse itself towards the fall of 2008. In 2009 and 2010 the Dodge Grand Caravan continued to be the top selling minivan in Canada, with over 60% of the market's monthly sales.\n\n\"Automotive News\" reported that, from January to October in 2010, Dodge sold about a third of its 2010 Grand Caravans to rental fleets. The number of returned ex-rental 2010 Grand Caravan to the market jumped fourfold between July to October, depressing prices of used 2009 and 2010 Dodge minivans by as much as 20%.\n\nBoth the 3.8 L and 4.0 L engines were paired with Chrysler's 62TE 6-speed automatic transmission with variable line pressure (VLP) technology (See Ultradrive#62TE).\n\nIn Canada (2008–2010) the 3.3 L was the standard engine across the range, combined with the 4-speed 41TE automatic transmission. The 4.0 L engine and six-speed combination was available as an option on only the top of the range SXT models. In 2011 the six-speed transmission was specified as standard on the Town & Country.\n\nIn the U.S. the National Highway Traffic Safety Administration's (NHTSA) New Car Assessment Program crash testing, the 2010 Dodge Grand Caravan achieved a five star (top safety) rating in several categories.\n\nThe Grand Caravan underwent a mid-cycle refresh for the 2011 model year, which included major changes in both styling and functionality. The suspension was heavily re-tuned, with both Dodge and Chrysler minivans gaining a larger front sway bar and new rear sway bar, increased rear roll center height, adjusted spring rates, a new steering gear, a revised front static camber setting, and lowered ride height. This dramatically improved handling in both the Chrysler and Dodge.\n\nAll three of the former engine choices were replaced by the new Pentastar 3.6-liter V6 with six-speed automatic transmission, now the sole powertrain choice for all models. Interior trim was restyled on both vans, in addition to major exterior revisions highlighted by the new \"double-crosshair\" grille on the Grand Caravan and a new chrome grille for the Town & Country.\n\nOther changes included extra sound insulation, acoustic glass, new seats, softer-touch surfaces, new LED ambient lighting and center console, and halogen projector headlamps with LED accents. The Chrysler models were adjusted so that instead of competing against equivalent Dodge trim levels, they were above Dodge in trim and features.\n\nFor 2012, a new, basic style trim called \"AVP\" was introduced, while some features previously unavailable for \"SE\" (like touch navigation panel) become available as options (The \"SE\" now also received a floor console, similar to the one available for \"SXT\"). The same year, the front logo design was changed the two slanted rectangles in red to match the rest of the Dodge lineup. For 2013, the \"AVP\" trim starting MSRP reduced by $1,000 from the year before.\n\nFor the 2014 model year, three new packages are introduced: American Value Package (US; Canada Value Package in Canada), Blacktop package (US only) and the 30th Anniversary Edition. They are all different sets of the SE and SXT trims, and include new luxury features for basically the same price. The Grand Caravan AVP also gained easy-clean floor mats that came with the optional second-row Stow'n Go seats (standard on SE, SXT and R/T).\n\nThe \"Blacktop package\", based on SE and SXT, came equipped with 17-inch polished aluminum wheels with gloss black pockets, a gloss black grille, black headlamp bezels, an all-black interior including headliner, door panels and console, unique black cloth seats and door trim panels with silver accent stitching, a leather-wrapped steering wheel with silver accent stitching and a leather shift knob, choice of 6 body colors (Granite Crystal, Billet Silver, Brilliant Black, Maximum Steel, Redline Red, Bright White). SXT models also include fog lamps.\n\nThe \"SE 30th Anniversary Edition\", based on the SE trim, came equipped with 17-inch satin carbon aluminum wheels, body-color heated exterior mirrors, 30th Anniversary badging on the front fenders, silver accent stitching and piano black accents throughout, black cloth seats, a black leather-wrapped steering wheel, black leather-wrapped shift knob, black headliner and overhead console, bright heating and air conditioning trim bezels, power second- and third-row windows, and 30th Anniversary logo on the key fob\n\nThe \"SXT 30th Anniversary Edition\", based on the SXT trim, was packaged with 17-inch polished aluminum wheels with satin carbon pockets, bright chrome roof rack, bright window trim moldings, fog lamps, automatic headlamps and special 30th Anniversary badging, Black Torino leatherette seats with premium suede inserts and silver accent stitching, power 10-way driver's seat, piano black accent trim bezels, and bright chrome accents throughout.\n\nBoth 30th Anniversary Editions included an available special body color, Granite Crystal Pearl Coat, a customized gauge cluster with 30th Anniversary badging, as well as the UConnect Handsfree Group (SiriusXM Satellite Radio with a one-year subscription, Bluetooth streaming audio and voice command and an auto-dimming rear view mirror). The vehicles arrived at dealerships at Q3 2013.\n\nAlso new for the 2014 model year, the Grand Caravan R/T gained standard auto headlamps with black bezels and the Security Group featuring remote start and security alarm.\n\nA small 2017 model year update yields SE, SE Plus, SXT, and GT models. The 6.5\" Touchscreen (Radio 430 and 430 NAV) became standard equipment. (Navigation is standard on GT and optional on SXT.)\n\nFCA unveiled the Chrysler Pacifica, a new crossover minivan for the 2017 model year, at the 2016 North American International Auto Show. The Pacifica replaced the Chrysler Town & Country, but it was originally stated that the 2016 model of the Dodge Caravan would be produced for one more model year as a lower-cost alternative to the Pacifica, before being discontinued. In January 2017, FCA CEO Sergio Marchionne stated that it was not clear if the Caravan would be discontinued, and that they would have to look into offering \"some level of affordable access to the Pacifica at the lower end to try and replace the outgoing models\". Despite the introduction of the higher-cost Pacifica model, sales of the Caravan increased by 26% in 2016.\n\nProduction of Grand Caravans was temporarily suspended from September to November 2017 with the assembly of the 2018 model year versions starting in December 2017. The production line required changes to install airbags that meet U.S. Federal Motor Vehicle Safety Standard 226 calling for larger and more robust side-impact air curtains, as well as for them to deploy in the event of either a side-on collision or a rollover.\n\nStandard on all 2018 models are advanced multistage driver and front-passenger airbags that include low-risk deployment, a driver's-side inflatable knee blocker, front seat-mounted side airbags, and side-curtains for outboard passengers in all three rows.\n\nOriginally from 2007 to June 1, 2011, the van was known in Europe as the Chrysler G. Voyager, but since June 1, 2011, it has been known in the UK and Ireland as a Chrysler G. Voyager and in the rest of Europe as a Lancia Voyager. For the European market, the Lancia/Chrysler Voyager will be available in three trim levels (Silver, Gold and Platinum) and two choices of engine: a 3.6-liter V6 petrol unit and a 2.8-liter CRD diesel with particulate filter as standard delivering and .\n\nFor more information on the European specification Voyager, see Chrysler Voyager.\n\nThe Ram Cargo Tradesman, or Ram C/V Tradesman, debuted for the 2012 model year, replacing the Dodge Grand Caravan C/V. It is based on the Dodge Grand Caravan, but with solid metal instead of rear windows and a flat load space with of interior storage, and a . cargo payload plus a towing capability of up to . The Ram C/V included a 3.6-liter Pentastar V6 engine and 6-speed automatic transmission. The C/V Tradesman was discontinued after the 2015 model year in favor of the Fiat Doblò-based ProMaster City.\nBeginning with Generation V, Volkswagen began marketing the Volkswagen Routan, a rebadged variant of the Chrysler RT platform minivan with revised styling and content, for the North American market. The original contract between Chrysler and Volkswagen called for five years of production (through 2013).\n\nThe Routan, made at Windsor Assembly alongside the Grand Caravan, debuted in 2008 at the Chicago Auto Show. Sales began in autumn of 2008, and features neither Chrysler's Stow'n Go nor Swivel'n Go seating systems. When the Chrysler and Dodge badged versions received new features for 2011, not all were shared with the Routan.\n\nTotal sales as of September 2010 were reported as being under 10,000 units, far lower than the target of 5% of the minivan market. The Routan was discontinued after the 2013 model year. A total of 2,500 were produced by Chrysler during the calendar year.\n\n\n\nThe Caravan has incorporated numerous seating systems for their minivans to enhance interior flexibility.\n\nIn 1991, Dodge introduced a second row bench seat integrating two child booster seats on 1992 models. These seats continued as an available option through Generation V until they were discontinued in 2010.\n\nIn 1995, Dodge introduced a system of seats to simplify installation, removal, and re-positioning, marketed as \"Easy-Out Roller Seats\". When installed, the seats are latched to floor-mounted strikers. When unlatched, eight rollers lift each seat, allowing it to be rolled fore and aft. Tracks have locator depressions for rollers, to simplify installation. Ergonomic levers at the seat backs release the floor latches single-handedly, without tools, and raise the seats onto the rollers in a single motion. Additionally, seat backs were designed to fold forward. Seat roller tracks are permanently attached to the floor and seat stanchions are aligned, facilitating the longitudinal rolling of the seats. Bench seat stanchions were moved inboard to reduce bending stress in the seat frames, allowing them to be lighter.\n\nWhen configured as two and three person benches (available through Generation IV), the Easy Out Roller Seats could be unwieldy. Beginning in 2000, second and third row seats became available in a 'quad' configuration – bucket or captain chairs in the second row and a third row three-person 50/50 split \"bench\" – with each section weighing under . The Easy-out system remained in use through Generation V – where certain models featured a two-person bench \"and\" the under-floor compartments from the \"Stow'n Go\" system.\n\nAll the rebadged nameplate variants of the Chrysler minivans use the Easy Out Roller Seats on their second row seating, where not the Stow and Go system.\n\nIn 2004, Dodge introduced a system of second- and third-row seating that folded completely into under-floor compartments. It was marketed as \"Stow 'N Go\" and was available exclusively on long-wheelbase models.\n\nIn a development program costing $400 million, engineers initially used an Erector Set to visualize the complex interaction of the design and redesigned under-floor components. The system included the spare tire well, fuel tank, exhaust system, parking brake cables, rear climate control lines, and rear suspension but precluded all-wheel drive (AWD).\n\nThe system, in turn, creates a combined volume of of under-floor storage when second-row seats are deployed. With both rows folded, the vans have a flat-load floor and a maximum cargo volume of .\n\nThe Stow 'n Go system received the Popular Science Magazine's \"Best of What's New\" for 2005 award, and was never offered on the Volkswagen Routan, the rebadged nameplate variant of the Chrysler minivans.\n\nFor model year 2011 Chrysler revised the system, rebranding it as \"Super Stow 'n Go\". New pivoting head restraints with taller seatbacks and a revised folding mechanism (marketed as \"single action\") improved stowage ease – with the head restraints folding on themselves automatically and the entire seat automatically folding down to a position just over its floor recess.\n\nDodge introduced a seating system in 2007, marketed as \"Swivel 'n Go\". In the seating system, two full size second row seats swivel to face the third row. A detachable table can be placed between the second and third row seats. The Swivel 'n Go seating system includes the third row seating from the Stow 'n Go system. The system is offered on the Dodge Grand Caravan and Chrysler Town & Country, but not the Volkswagen Routan, a rebadged nameplate variant of the Chrysler minivans.\n\nThese Swivel 'n Go Seats are manufactured by Intier Corp. a division of Magna. The tracks, risers and swivel mechanisms are assembled by Camslide, a division of Intier. The swivel mechanism was designed by and is produced by Toyo Seat USA Corp.\n\nThe system is noted for its high strength. The entire load of the seat in the event of a crash is transferred through the swivel mechanism, which is almost twice as strong as the minimum government requirement.\n\nThe swivel mechanism includes bumpers that stabilize the seat while in the lock position. When rotated the seat comes off these bumpers to allow easy rotation.\n\nThe seat is not meant to be left in an unlocked position or swiveled with the occupant in it, although this will not damage the swivel mechanism.\n\n\"Swivel 'n Go\" was dropped after the 2010 model year and is no longer an option on 2011 and later Chrysler and Dodge vans. However, the seats can still be installed by modifying the van with a few basic tools and parts. However, it is impossible to install the table.\n\nThe long-wheelbase Dodge Grand Caravan with \"Stow 'n Go\" seats are built in Windsor, Ontario, Canada at Windsor Assembly (WAP Plant 3) by members of Canadian Auto Workers Local 444. Both wheelbase models were also produced in Fenton, Missouri at Saint Louis Assembly by members of the United Auto Workers Local 110 until 31 October 2008.\n\n"}
{"id": "8934267", "url": "https://en.wikipedia.org/wiki?curid=8934267", "title": "Edge-localized mode", "text": "Edge-localized mode\n\nAn edge-localized mode (“ELM”) is a disruptive instability occurring in the edge region of a tokamak plasma due to the quasi-periodic relaxation of a transport barrier previously formed during an transition (i.e., in This phenomenon was first observed in the ASDEX tokamak in 1981.\n\nThe development of edge-localized modes poses a major challenge in magnetic fusion research with tokamaks, as these instabilities can damage wall components (in particular divertor plates) by ablating them away due to their extremely high energy transfer rate (GW/m).\n\nIn 2006 an initiative (called Project Aster) was started to simulate a full ELM cycle including its onset, the highly non-linear phase, and its decay. However, this did not constitute a “true” ELM cycle, since a true ELM cycle would require modeling the slow growth after the crash, in order to have a second ELM. In 2015, results of the first simulation to demonstrate repeated ELM cycling was published. A key element to obtaining repeated relaxations was to include diamagnetic effects in the model equations. Diamagnetic effects have also been shown to expand the size of the parameter space in which solutions of repeated sawteeth can be recovered compared to a resistive MHD model.\n\nResearch involving prevention of edge localized mode formation is underway. A paper was recently published that suggested a novel method of countering this phenomenon by injecting static magnetic noisy energy into the containment field as a containment-stabilization regime; this may decrease ELM amplitude. ASDEX Upgrade has had some success using pellet injection to increase the frequency and thereby decrease the severity of ELM bursts.\n\nAs of late 2011, several research facilities have demonstrated active control or suppression of ELMs in tokamak plasmas. For example, the KSTAR tokamak uses specific asymmetric three-dimensional magnetic field configurations to achieve this goal.\n\n"}
{"id": "4990175", "url": "https://en.wikipedia.org/wiki?curid=4990175", "title": "Faisalabad Electric Supply Company", "text": "Faisalabad Electric Supply Company\n\nFaisalabad Electric Supply Company (FESCO) is an electric distribution company which supplies electricity to Faisalabad, Punjab, Pakistan. This company generates electric power from water (hydro-electric power) and distributes it to approximately 22 million people of the area. Faisalabad Electric Supply Company was founded in 1998.\n\n\n\n \n"}
{"id": "9305752", "url": "https://en.wikipedia.org/wiki?curid=9305752", "title": "Fanno flow", "text": "Fanno flow\n\nFanno flow is the adiabatic flow through a constant area duct where the effect of friction is considered. Compressibility effects often come into consideration, although the Fanno flow model certainly also applies to incompressible flow. For this model, the duct area remains constant, the flow is assumed to be steady and one-dimensional, and no mass is added within the duct. The Fanno flow model is considered an irreversible process due to viscous effects. The viscous friction causes the flow properties to change along the duct. The frictional effect is modeled as a shear stress at the wall acting on the fluid with uniform properties over any cross section of the duct.\n\nFor a flow with an upstream Mach number greater than 1.0 in a sufficiently long enough duct, deceleration occurs and the flow can become choked. On the other hand, for a flow with an upstream Mach number less than 1.0, acceleration occurs and the flow can become choked in a sufficiently long duct. It can be shown that for flow of calorically perfect gas the maximum entropy occurs at \"M\" = 1.0. Fanno flow is named after Gino Girolamo Fanno.\n\nThe Fanno flow model begins with a differential equation that relates the change in Mach number with respect to the length of the duct, \"dM/dx\". Other terms in the differential equation are the heat capacity ratio, \"γ\", the Fanning friction factor, \"f\", and the hydraulic diameter, \"D\":\n\nAssuming the Fanning friction factor is a constant along the duct wall, the differential equation can be solved easily. One must keep in mind, however, that the value of the Fanning friction factor can be difficult to determine for supersonic and especially hypersonic flow velocities. The resulting relation is shown below where \"L*\" is the required duct length to choke the flow assuming the upstream Mach number is supersonic. The left-hand side is often called the Fanno parameter.\n\nEqually important to the Fanno flow model is the dimensionless ratio of the change in entropy over the heat capacity at constant pressure, \"c\".\n\nThe above equation can be rewritten in terms of a static to stagnation temperature ratio, which, for a calorically perfect gas, is equal to the dimensionless enthalpy ratio, \"H\":\n\nThe equation above can be used to plot the Fanno line, which represents a locus of states for given Fanno flow conditions on an \"H\"-\"ΔS\" diagram. In the diagram, the Fanno line reaches maximum entropy at \"H\" = 0.833 and the flow is choked. According to the Second law of thermodynamics, entropy must always increase for Fanno flow. This means that a subsonic flow entering a duct with friction will have an increase in its Mach number until the flow is choked. Conversely, the Mach number of a supersonic flow will decrease until the flow is choked. Each point on the Fanno line corresponds with a different Mach number, and the movement to choked flow is shown in the diagram. \n\nThe Fanno line defines the possible states for a gas when the mass flow rate and total enthalpy are held constant, but the momentum varies. Each point on the Fanno line will have a different momentum value, and the change in momentum is attributable to the effects of friction.\n\nAs was stated earlier, the area and mass flow rate in the duct are held constant for Fanno flow. Additionally, the stagnation temperature remains constant. These relations are shown below with the * symbol representing the throat location where choking can occur. A stagnation property contains a 0 subscript.\n\nDifferential equations can also be developed and solved to describe Fanno flow property ratios with respect to the values at the choking location. The ratios for the pressure, density, temperature, velocity and stagnation pressure are shown below, respectively. They are represented graphically along with the Fanno parameter.\n\nThe Fanno flow model is often used in the design and analysis of nozzles. In a nozzle, the converging or diverging area is modeled with isentropic flow, while the constant area section afterwards is modeled with Fanno flow. For given upstream conditions at point 1 as shown in Figures 3 and 4, calculations can be made to determine the nozzle exit Mach number and the location of a normal shock in the constant area duct. Point 2 labels the nozzle throat, where \"M\" = 1 if the flow is choked. Point 3 labels the end of the nozzle where the flow transitions from isentropic to Fanno. With a high enough initial pressure, supersonic flow can be maintained through the constant area duct, similar to the desired performance of a blowdown-type supersonic wind tunnel. However, these figures show the shock wave before it has moved entirely through the duct. If a shock wave is present, the flow transitions from the supersonic portion of the Fanno line to the subsonic portion before continuing towards \"M\" = 1. The movement in Figure 4 is always from the left to the right in order to satisfy the second law of thermodynamics.\n\nThe Fanno flow model is also used extensively with the Rayleigh flow model. These two models intersect at points on the enthalpy-entropy and Mach number-entropy diagrams, which is meaningful for many applications. However, the entropy values for each model are not equal at the sonic state. The change in entropy is 0 at \"M\" = 1 for each model, but the previous statement means the change in entropy from the same arbitrary point to the sonic point is different for the Fanno and Rayleigh flow models. If initial values of \"s\" and \"M\" are defined, a new equation for dimensionless entropy versus Mach number can be defined for each model. These equations are shown below for Fanno and Rayleigh flow, respectively.\n\nFigure 5 shows the Fanno and Rayleigh lines intersecting with each other for initial conditions of \"s\" = 0 and \"M\" = 3. The intersection points are calculated by equating the new dimensionless entropy equations with each other, resulting in the relation below.\n\nThe intersection points occur at the given initial Mach number and its post-normal shock value. For Figure 5, these values are \"M\" = 3 and 0.4752, which can be found the normal shock tables listed in most compressible flow textbooks. A given flow with a constant duct area can switch between the Fanno and Rayleigh models at these points.\n\n\n"}
{"id": "868029", "url": "https://en.wikipedia.org/wiki?curid=868029", "title": "Forest Stewardship Council", "text": "Forest Stewardship Council\n\nThe Forest Stewardship Council (FSC) is an international non-profit, multi-stakeholder organization established in 1993 to promote responsible management of the world’s forests. The FSC does this by setting standards on forest products, along with certifying and labeling them as eco-friendly.\n\nThe FSC’s stated mission is to \"promote environmentally appropriate, socially beneficial and economically viable management of the world's forests\". To this end the body has published a global strategy with five goals:\n\nThese goals are being promoted by activities which are managed and developed through six program areas: forests, chain of custody, social policy, monitoring and evaluation, quality assurance and ecosystem services.\n\nIt claims that forests managed to its standards offer benefits to both local and wider communities and these are said to include cleaner air and water, and a contribution to mitigating the effects of climate change.\n\nDirectly or indirectly, FSC addresses issues such as illegal logging, deforestation and global warming and some reports indicate positive effects on economic development, environmental conservation, poverty alleviation and social and political empowerment.\n\nUsing the FSC logo signifies that the product comes from responsible sources—environmentally appropriate, socially beneficial and economically viable. The FSC label is used on a wide range of timber and non-timber products from paper and furniture to medicine and jewelry., and aims to give consumers the option of supporting responsible forestry.\n\nAccording to the Food and Agriculture Organization of the United Nations, half of the world’s forests have already been altered, degraded, destroyed or converted into other land uses. Much of the remaining forests today suffer from illegal exploitation and otherwise poor management. FSC was established as a response to these concerns over global deforestation.\n\nTropical deforestation as a global concern rose to prominence in the 1980s and can be somewhat attributed to a fight for action by environmentalists and northern countries over the need to protect tropical woodland. Prior to this, a number of other economic and regulatory mechanisms such as financial aid, policy frameworks and trade conventions were established in the fight against deforestation. These include the International Tropical Timber Agreement (1983), the Convention of International Trade on Endangered Species (1975) and the Global Environment Facility (1991). Despite the increased level of concern on the run-up to the 1992 Earth Summit held in Rio de Janeiro, tensions between the North and the global South over access to finance and technology for the preservation of forests protracted negotiations. Although many Northern countries had hoped for a legally binding convention the resulting Statement of Forest Principles represents the \"mean position of the lowest common denominator\" and is voluntary. Disappointed with the outcome of the Earth Summit, NGOs such as the World Wide Fund for Nature (WWF) began to turn their attention to industry for a more meaningful governance-orientated resolution to the problem of deforestation.\n\nIn the lead up to the Earth Summit, social groups, NGOs and industries were also beginning to consult on the issue of deforestation. In America the consultation process that eventually led to the establishment of the FSC was initiated in 1990 and concluded in the confirmation of support for the development of a voluntary worldwide certification and accreditation governance system that would cover all forest types. In the UK, NGO WWF began to facilitate action through the establishment of the 1995 Group, recruiting organisations that had been spurred on by instances of direct action and boycotting over the sale of tropical wood to form an NGO-business partnership. Through stakeholder involvement it became apparent that a standard-setting body would be required to verify the source of wood products and define sustainable forest management. After 18 months of consultation in ten different countries, the Forest Stewardship Council was finally established in 1993.\n\nThe failure of governments to reach any notable form of consensus in the form of an internationally reaching and legally binding agreement caused both disillusionment and an opportunity for change through the involvement of civil society and business actors to form \"soft law\". As such the establishment of the Forest Stewardship Council as the response to this disillusionment also represents a global shift from government to governance and its creation is a primary example of the use of market and economic factors to create movement on a global environmental issue. The evolving historical context in which the FSC was formed is theorised to reflect a much broader skepticism towards state power and as a consequence a shift away from traditional state-centric forms of regulation. That said, although the FSC transcends national boundaries, the state continues to play a part in the regulatory landscape of the domestic forest and as such the FSC must develop appropriate domestic governance to reflect this.\n\nFSC is an international membership organization with a governance structure based on participation, democracy, equity and transparency. It is a platform for forest owners, timber industries, social groups and environmental organizations to come together to find solutions to improve forest management practices.\n\nIt is governed by its members, who join either as individuals or as representatives of organisations; they come from diverse backgrounds including environmental NGOs, the timber trade, community forest groups and forest certification organizations. Members apply to join one of three chambers – environmental, social and economic; each chamber is divided into northern and southern sub-chambers and votes are weighted to ensure that north and south each have 50%; this system is designed to ensure that influence is shared equally between different interest groups, without having to limit the number of members.\n\nFSC has three levels of decision making bodies: The General Assembly, the Board of Directors and the Executive Director.\n\n\n\n\nWhile the FSC International Center is based in Bonn, Germany, it has a decentralized network of FSC Network Partners that promote responsible forest management on behalf of FSC. FSC Network Partners include FSC National Offices, FSC National Representatives and FSC National Focal Points.\n\nFSC is a global forest certification system established for forests and forest products; from the perspective of the WWF this voluntary mechanism can be regarded as one of the more interesting initiatives of the last decade to promote better forest management. while a number of alternative national and regional forest certification bodies also exist around the globe\nIt has 10 Principles and associated Criteria (FSC P&C) that form the basis for all FSC forest management standards and certification. FSC International sets the framework for developing and maintaining international, national and sub-national standards. This is intended to ensure that the process for developing FSC policies and standards is transparent, independent and participatory.\n\nIn February 2012 the membership approved the first major revision to the FSC P&C, in order to bring its certification up to date. The review and revision of the FSC P&C began in 2008 and gathered feedback from many FSC members and other stakeholders.\nThis revision also marked the start of a process of developing baseline requirements for each of the revised Criteria; These requirements - called International Generic Indicators (IGIs) - are intended to ensure consistent application of the FSC P&C across all countries. Where national standards are not currently established, the IGIs will be used as interim standards.\n\nForest management certification is a voluntary process for verifying responsible forest practices. It is up to a forest owner, or representative of a group of forest owners and operators, to initiate the certification process by requesting an independent certifier to inspect the forest and to see if the management meets the FSC requirements, encapsulated in the FSC P&C. Only an FSC accredited certification body can evaluate, monitor and certify companies to FSC standards, and only continuous compliance can assure that certificate holders can keep their certificates; these holders are charged an annual fee to renew their accreditation.\n\nThe FSC P&C apply to all tropical, temperate and boreal forests and many to plantations and partially replanted forests. Though mainly designed for forest management for timber products, they are also largely relevant for non-timber products (e.g. Brazil nuts) and other environmental services such as clean water and air and carbon sequestration. The FSC Principles are a complete package and their sequence does not represent an ordering of priority. Prior to the revision process which ended in February 2012, the FSC P&C were the following:\n\nThe Revised P&C were approved in February 2012 but will not be used for audit until the FSC IGIs are also complete. The full text of the revised P&C can be found on the FSC website but the basic principles are listed below.\n\n\nFSC accredited certification bodies certify and audit each individual forest management operation. If the forest management is in full compliance with FSC requirements, the FSC certificate is awarded. If the forest management is not fully compliant, pre-conditions are noted which must be fulfilled before the FSC certificate can be awarded. If minor non-compliances are noted, the certificate can be issued with conditions that have to be met within a clearly determined timeframe.\n\nOnce certification is awarded, FSC accredited certification bodies audit each FSC certificate at least once a year. If during these audits the certification body finds that a company has non-compliances with FSC requirements, Corrective Action Requests (CARs) are issued and the company is required to make the prescribed changes within a given timeframe or lose its FSC certificate. Depending on the seriousness of the infringement, the timeline can go from one year for minor administrative infringements to immediate action for major infringements.\n\nThe FSC Chain of Custody (CoC) system allows the tracking of FSC certified material from the forest to the consumer. It is a method by which companies can show their commitment to the environment and responsible forest management. Only companies that have FSC chain of custody certification are allowed to use the FSC trademarks and labels to promote their products. The FSC label therefore provides a link between responsible production and responsible consumption and helps the consumer to make socially and environmentally responsible buying decisions.\n\nOnce a forest is certified it is important to be able to trace the products that come from it throughout the supply chain to ensure that any claims on the origin of the product are credible and verifiable. FSC chain of custody certification is a voluntary process. It is a tracking system that allows manufacturers and traders to demonstrate that timber comes from a forest that is responsibly managed in accordance with the FSC P&C. It tracks the flow of certified wood through the supply chain and across borders through each successive stage - including processing, transformation and manufacturing - all the way to the final product. It is up to a company to initiate the certification process by requesting the services of an independent certification body to inspect its internal tracking procedures. Only FSC-accredited certification bodies can evaluate, monitor and certify companies to FSC standards. Companies committing to FSC include home-improvement or DIY companies, publishers and retailers, amongst many others.\n\nAll operations that want to produce an FSC certified product or want to make corresponding sales claims must comply with FSC’s international standards for chain of custody. An operation must specify the range of products they wish to sell as FSC certified and promote with the FSC trademark. The certification body inspects the operation to ensure that controls are in place to identify eligible sources for the specified product range and to prevent certified and recycled material from mixing with material from unacceptable sources. If an operation complies with FSC standards, the company is issued an FSC chain of custody certificate. Major failure to comply with the standard will normally disqualify the candidate from certification or lead to de-certification.\n\nThe FSC Mix label was introduced in 2004. It allows manufacturers to mix FSC certified material with uncertified materials in FSC labelled products under controlled conditions. It aims to avoid the use of wood products from \"unacceptable\" sources in FSC labelled products. Unacceptable sources include illegally harvested wood, wood harvested in violation of traditional and civil rights, wood harvested in HCV forests and wood harvested from areas where genetically modified trees are planted.\n\nTo maintain independence between the standards it sets and the operations seeking certification of both kinds, FSC does not conduct certification audits itself. FSC has developed rigorous procedures and standards to evaluate whether organizations of certifiers (certification bodies) can provide independent and competent evaluation (certification) services. This process is known as ‘accreditation’.\n\nA potential certification body must gain FSC accreditation to be able to evaluate, monitor and certify companies to FSC standards. To become FSC accredited, certifiers have to comply with an extensive set of rules and procedures which are verified by ASI - Accreditation Services International GmbH - an international third-party accreditation body, which also serves as sole accreditation body for other sustainability standards such as MSC and RSPO. This includes an office audit and the witnessing of one or more trial audits in the field. ASI monitors accredited certification bodies to ensure the appropriateness of their operations can be continuously guaranteed.\n\nTo control the continued implementation of FSC rules and procedures, every year ASI conducts at least one office and one field assessment for each FSC accredited certification body. The exact number and distribution of ASI assessments takes a number of complex factors into account (geographic areas, policies or products that carry increased risk) and the number of FSC certificates handled by an accredited certification body, and is meant to ensure that the certification services delivered by the certifier meet the requirements of the FSC.\n\nSome summaries of ASI surveillance audits are publicly available on the ASI website. If an FSC accredited certification body is found to not fully comply with FSC rules and procedures, nonconformities (NCs) are raised (see above).\n\nIn September 2012, some 165 million hectares were certified to FSC’s Principles and Criteria in 80 countries. Around 24,000 FSC Chain of Custody certificates were active in 107 countries. The FSC website has statistics on regional distributions, ownership and forest type and numbers of FSC certificates representing all valid forest management and chain of custody certificates.\n\nThe expenses for successful forest management certification typically are divided into:\n\n\nFSC Friday is a once-a-year-event dedicated to the celebration of forests around the globe, and the promotion of responsible forest management worldwide. The first international FSC Friday took place in Bonn on 25 September 2009. On FSC Friday, people are invited to investigate what’s in their shopping basket, and look for the FSC logo. Events related to FSC Friday take place around the world with companies and supporters promoting the FSC logo and what it stands for. FSC Friday in 2010 took place on 24 September and the following week. Events took place at FSC certified forests, schools, universities and community centers around the world, including the United Kingdom, Austria, South Africa, Germany, Brazil, Argentina, Belgium, Netherlands, Denmark, France, India, Wales, Switzerland and Singapore.\n\nFSC has around 850 members. The range of member organizations illustrates the diversity of support for FSC. Members include\nNumerous governments worldwide have strengthened market-based incentives for timber certification by providing tax benefits to certified companies, referencing certified products as requirements in their procurement policies and supporting projects linked to FSC through their international development agencies. Some companies also choose timber certification as a tool to demonstrate their commitment to sustainability. Such activities demonstrate broad support for FSC certification as good practice.\n\nFSC is a member of the International Social and Environmental Accreditation and Labelling (ISEAL) Alliance, an association of voluntary international standard setting and certification organizations focused on social and environmental issues. Since 2006, FSC has complied with ISEAL’s Code of Good Practice for Setting Social and Environmental standards, aimed at assuring high standards for credible behavior in ethical trade.\n\nIn 2009, FSC began a pilot project with Fairtrade International (FLO) to help community-based and small-scale timber producers get a fair price for their products and gain visibility in the marketplace. The first jointly labelled FSC-FLO products went onto the market in 2011.\n\nFSC also works in liaison with the International Organization for Standardization (ISO). It contributes to the ISO’s committees on sustainable criteria for bioenergy, environmental auditing and environmental labelling.\n\nSince it was founded, FSC has been criticized for a range of different reasons.\n\nIn recent years, a number of well-known NGOs and environmental organizations have canceled their support for FSC. These include FERN (2011), Friends of the Earth UK (2008), ROBINWOOD (2009), the Swedish Society for Nature Conservation (SSNC) (2011), and smaller groups such as Rainforest Rescue and the Association for the Ecological Defence of Galicia (ADEGA).\n\nOther NGO members, instead of withdrawing support, have critically evaluated FSC’s performance and made recommendations to improve the credibility of the organization. These include Greenpeace International, whose 2008 report ‘Holding the line with FSC’ focused on controversial certificates and ways forward. A revised version of the report was released in 2011, which concluded that some progress had been made. But it also identified ongoing weaknesses, including lack of guidance on HCVFs and activities in controversial areas like the Congo Basin and problems with the Controlled Wood label, the Chain of Custody system and logo integrity. The report concluded with 10 immediate changes needed to \"restore FSC’s credibility\". To improve its process for tracking FSC certified products and maintaining the veracity of FSC claims, FSC is currently developing an Online Claims Platform (http://claims-forum.fsc.org/) and is pursuing opportunities for fiber testing. Chamber-balanced Working Groups have been established to strengthen the Controlled Wood system and the Chain of Custody standard, and options are being pursued to address the performance of certification bodies.\n\nGreenpeace International announced on 3/26/18 that it is not renewing its membership in the Forest Stewardship Council (FSC) (https://www.greenpeace.org/international/press-release/15589/greenpeace-international-to-not-renew-fsc-membership/)\n\nFSC has been harshly criticized by Simon Counsel, one of its founding members, now the director of the Rainforest Foundation. In 2008, he described the FSC as the \"Enron of Forestry\". He cited case studies from six countries [reference needed] which suggested that in these cases FSC was not properly controlling accredited auditors or certifiers. The FSC reviewed the certificates in question, and showed that some of the initial investigations were justified. This resulted in the removal of the license to certify from the Thai company, Forest Industry Organisation.\n\nFSC-Watch is a website critical of FSC which is run by a group of people, including Simon Counsel, who are concerned about what they perceive as the constant and serious erosion of the FSC's reliability and credibility. Its website offers a wide range of extensive and detailed criticisms of FSC. FSC-Watch commonly accuses FSC of practices that are a form of greenwashing.\n\nIn 2008, the EcoEarth/Rainforest Portal, an Internet ecological news aggregation service, publicly questioned the FSC-endorsed policy of old-growth forest logging. They asserted that research does not support the idea that this type of logging is carbon positive or sustainable, though these views are disputed.\n\nCritics are encouraged to file complaints and disputes against FSC in case of any differences. But complainants must meet a number of conditions to be able to file complaints and it is disputed whether the FSC takes effective action even in the case of some formal complaints. Recently, FSC has implemented a series of actions to strengthen its stakeholder engagement process, including the establishment of a Quality Assurance Unit within FSC, time-bound dispute resolution procedures and a web-based portal for tracking disputes (https://ic.fsc.org/dispute-resolution.139.htm).\n\nSome critics point out that FSC certification has a linked set of weaknesses: that it is not suited for small businesses, that it is anti-competitive and that therefore in a wider view it is counter-ecological:\n\n\nFewer local companies mean that goods have to travel farther, and the resources and skills of local communities degrade.\n\nPartly in response to these criticisms, to make certification more accessible to small and medium-sized businesses, FSC instituted the Small and Low Intensity Managed Forests (SLIMF) initiative and group certification. SLIMF adapts the FSC system to the realities and needs of small and low intensity forest operations by offering special streamlined procedures, with less rigorous requirements for a number of its forest management criteria. SLIMF are defined as forests of 100 hectares or less. Group certification allows a group of forest owners to share certification costs. A documented success of group certification in a developing country involved smallholder farmers growing acacia in Vietnam. In that case, farmers were able to sell group certified wood at a premium versus comparable non-certified wood. In October 2012, worldwide, 8.7% of FSC certificates were held by community-owned forests. SLIMF and group certification are intended to allow FSC to promote responsible forest management in small-scale forests as well as large ones.\n\nThe world's largest manager of FSC certified forests, Resolute Forest Products, has been accused of illegal logging on Barriere Lake Algonquin territory and of violating indigenous rights. In July 2012, members of the Algonquin community in southern Quebec staged a camp to observe and deter the logging of their unceded territory. In an interview Michel Thusky, an elder of the Barriere Lake Algonquin, had this to say:\n\n\"The worst thing about it is they have gotten the Forest Stewardship Council Certification for their products. That is a total shame... They [Resolute Forest Products] cut last Tuesday and we interrupted them. We asked them to stop until we have a meaningful consultation. We extended our respect but they failed. On July 9th they sent a riot squad and many police officers, what they call Sûreté du Québec, security of Quebec.\nThey put conditions on us. They put a blockade on us, in the Algonquin birth place. If we go past a certain line, they say they will arrest us.\"\n\nThe firm has a different view of related events, stating that their \"right to harvest in the area [had] been approved by the QMNRW, following appropriate consultation with the Barrière Lake Algonquin band council.\"\n\nBecause it works outside of state regulations, some academics have classified FSC as an example of a non-state market driven (NSMD) form of environmental governance. This means that it uses the market to drive the sustainable management of forests. As Cashore (2002) observes the FSC network does not have the political authority of a traditional nation state and no one can be fined or imprisoned for failing to comply with its regulations. In addition, governments are forbidden from being members of the FSC and their only engagement with FSC is as land owners. The authority of the FSC is determined by the approval of external audiences, such as environmental NGOs.\n\nThe FSC Label is an example of the use of public purchasing power to create shifts in industry and regulate the negative environmental impacts of deforestation. The FSC Label ‘works’ by providing an incentive for responsible forestry in the market place. It offers manufacturers a competitive advantage and thereby increases market access to new markets and maintains access to existing ones.\n\nNon-state market-driven methods are often used because of allegations of ‘state failure’ in managing the environment. In the neoliberal view, market based methods are seen as one of the most effective and efficient ways to meet environmental goals. The market is seen as the key mechanism for producing the maximum social good and governance networks are seen as the most efficient way to regulate environmental concerns.\n\nThe FSC transnational NGO network demonstrates theories of global governance, more specifically in governance network theory. FSC is an example of how network governance can create change in industry and encourage organizations to improve the sustainability of industrial forestry practices. As Bäckstrand (2008) states, the FSC governance network brings together private companies, organizations and civil society in a non-hierarchical fashion, to voluntarily address certain goals. According to governance network theory, actors in the network are dependent on each other and collaborate to reach specific goals, through exchanging information or resources.\n\nThrough the chamber system, governance of FSC has checks at local, national and international levels which mean that it includes interests regardless of their geographical location. This gives FSC some advantages over state governance systems. In theory, as a governance network, FSC members share knowledge, environmental goals and knowledge of what those goals entail. This means that they coordinate effectively, improving the process and outcomes of the environmental policies they pursue. Moreover, knowledge sharing and collaboration enables the FSC network to deal with complex and interrelated issues.\n\nSome critiques however suggest that network governance does not always work this way. Network governance theory suggests that partnerships should be equal, but inequalities of power within networks can result in hierarchical relationships determined by more dominant actors. Within FSC, lareger international actors may have a stronger influence than smaller stakeholders, meaning that the FSC governance network may not represent all participants fairly. FSC has instituted the chamber system to try and tackle this imbalance.\n\nFurthermore, actors in networks operate as representatives of certain groups but also as individuals with their own agendas and values, and members in the FSC network are usually motivated by pragmatic rather than moral considerations. Moreover, Sorenson and Torfing (2005) argue that for governance networks to achieve their goals they should be controlled by democratically elected politicians. Although formerly there were no elections in the FSC governance system, reforms mean that the Board of Directors is now democratically elected by the membership chambers.\n\nThere are a number of certification schemes for forest management apart from FSC certification.\n\nThe main competing forest certification system is the Programme for the Endorsement of Forest Certification (PEFC), established by a number of stakeholders, including associations of the forest industry, pulp-and-paper production and forest owners in response to the creation and increasing popularity of FSC. PEFC has been criticized for having little influence from local people or environmental organizations, lack of transparency and non-objective requirements.\n\nOther certification schemes include American Forest and Paper Association’s Sustainable Forestry Initiative (SFI), the Malaysian Timber Certification Council, the Australian Forestry Standard, and Keurhout.\n\n\n"}
{"id": "379303", "url": "https://en.wikipedia.org/wiki?curid=379303", "title": "Gas exchange", "text": "Gas exchange\n\nGas exchange is the physical process by which gases move passively by diffusion across a surface. For example, this surface might be the air/water interface of a water body, the surface of a gas bubble in a liquid, a gas-permeable membrane, or a biological membrane that forms the boundary between an organism and its extracellular environment.\n\nGases are constantly consumed and produced by cellular and metabolic reactions in most living things, so an efficient system for gas exchange between, ultimately, the interior of the cell(s) and the external environment is required. Small, particularly unicellular organisms, such as bacteria and protozoa, have a high surface-area to volume ratio. In these creatures the gas exchange membrane is typically the cell membrane. Some small multicellular organisms, such as flatworms, are also able to perform sufficient gas exchange across the skin or cuticle that surrounds their bodies. However, in most larger organisms, which have a small surface-area to volume ratios, specialised structures with convoluted surfaces such as gills, pulmonary alveoli and spongy mesophyll provide the large area needed for effective gas exchange. These convoluted surfaces may sometimes be internalised into the body of the organism. This is the case with the alveoli, which form the inner surface of the mammalian lung, the spongy mesophyll, which is found inside the leaves of some kinds of plant, or the gills of those molluscs that have them, which are found in the mantle cavity.\n\nIn aerobic organisms, gas exchange is particularly important for respiration, which involves the uptake of oxygen () and release of carbon dioxide (). Conversely, in oxygenic photosynthetic organisms such as most land plants, uptake of carbon dioxide and release of both oxygen and water vapour are the main gas-exchange processes occurring during the day. Other gas-exchange processes are important in less familiar organisms: \"e.g.\" carbon dioxide, methane and hydrogen are exchanged across the cell membrane of methanogenic archaea. In nitrogen fixation by diazotrophic bacteria, and denitrification by heterotrophic bacteria (such as \"Paracoccus denitrificans\" and various pseudomonads), nitrogen gas is exchanged with the environment, being taken up by the former and released into it by the latter, while giant tube worms rely on bacteria to oxidize hydrogen sulfide extracted from their deep sea environment, using dissolved oxygen in the water as an electron acceptor.\n\nThe exchange of gases occurs as a result of diffusion down a concentration gradient. Gas molecules move from a region in which they are at high concentration to one in which they are at low concentration. Diffusion is a passive process, meaning that no energy is required to power the transport, and it follows Fick’s Law: \n\nIn relation to a typical biological system, where two compartments ('inside' and 'outside'), are separated by a membrane barrier, and where a gas is allowed to spontaneously diffuse down its concentration gradient:\n\n\nFig. 1. Fick's Law for gas-exchange surface\n\nGases must first dissolve in a liquid in order to diffuse across a membrane, so all biological gas exchange systems require a moist environment. In general, the higher the concentration gradient across the gas-exchanging surface, the faster the rate of diffusion across it. Conversely, the thinner the gas-exchanging surface (for the same concentration difference), the faster the gases will diffuse across it.\n\nIn the equation above, \"J\" is the flux expressed per unit area, so increasing the area will make no difference to its value. However, an increase in the available surface area, will increase the \"amount\" of gas that can diffuse in a given time. This is because the amount of gas diffusing per unit time (d\"q\"/d\"t\") is the product of \"J\" and the area of the gas-exchanging surface, \"A\":\n\nSingle-celled organisms such as bacteria and amoebae do not have specialised gas exchange surfaces, because they can take advantage of the high surface area they have relative to their volume. The amount of gas an organism produces (or requires) in a given time will be in rough proportion to the volume of its cytoplasm. The volume of a unicellular organism is very small, therefore it produces (and requires) a relatively small amount of gas in a given time. In comparison to this small volume, the surface area of its cell membrane is very large, and adequate for its gas-exchange needs without further modification. However, as an organism increases in size, its surface area and volume do not scale in the same way. Consider an imaginary organism that is a cube of side-length, \"L\". Its volume increases with the cube (\"L\") of its length, but its external surface area increases only with the square (\"L\") of its length. This means the external surface rapidly becomes inadequate for the rapidly increasing gas-exchange needs of a larger volume of cytoplasm. Additionally, the thickness of the surface that gases must cross (d\"x\" in Fick's Law) can also be larger in larger organisms: in the case of a single-celled organism, a typical cell membrane is only 10 nm thick; but in larger organisms such as roundworms (Nematoda) the equivalent exchange surface - the cuticle - is substantially thicker at 0.5 µm.\n\nIn multicellular organisms therefore, specialised respiratory organs such as gills or lungs are often used to provide the additional surface area for the required rate of gas exchange with the external environment. However the distances between the gas exchanger and the deeper tissues are often too great for diffusion to meet gaseous requirements of these tissues. The gas exchangers are therefore frequently coupled to gas-distributing circulatory systems, which transport the gases evenly to all the body tissues regardless of their distance from the gas exchanger.\n\nSome multicellular organisms such as flatworms (Platyhelminthes) are relatively large but very thin, allowing their outer body surface to act as a gas exchange surface without the need for a specialised gas exchange organ. Flatworms therefore lack gills or lungs, and also lack a circulatory system. Other multicellular organisms such as sponges (Porifera) have an inherently high surface area, because they are very porous and/or branched. Sponges do not require a circulatory system or specialised gas exchange organs, because their feeding strategy involves one-way pumping of water through their porous bodies using flagellated collar cells. Each cell of the sponge's body is therefore exposed to a constant flow of fresh oxygenated water. They can therefore rely on diffusion across their cell membranes to carry out the gas exchange needed for respiration.\n\nIn organisms that have circulatory systems associated with their specialized gas-exchange surfaces, a great variety of systems are used for the interaction between the two.\n\nIn a countercurrent flow system, air (or, more usually, the water containing dissolved air) is drawn in the \"opposite\" direction to the flow of blood in the gas exchanger. A countercurrent system such as this maintains a steep concentration gradient along the length of the gas-exchange surface (see lower diagram in Fig. 2). This is the situation seen in the gills of fish and many other aquatic creatures. The gas-containing environmental water is drawn unidirectionally across the gas-exchange surface, with the blood-flow in the gill capillaries beneath flowing in the opposite direction. Although this theoretically allows almost complete transfer of a respiratory gas from one side of the exchanger to the other, in fish less than 80% of the oxygen in the water flowing over the gills is generally transferred to the blood.\n\nAlternative arrangements are cross current systems found in birds. and dead-end air-filled sac systems found in the lungs of mammals. In a cocurrent flow system, the blood and gas (or the fluid containing the gas) move in the same direction through the gas exchanger. This means the magnitude of the gradient is variable along the length of the gas-exchange surface, and the exchange will eventually stop when an equilibrium has been reached (see upper diagram in Fig. 2).\nCocurrent flow gas exchange systems are not known to be used in nature.\n\nThe gas exchanger in mammals is internalized to form lungs, as it is in most of the larger land animals. Gas exchange occurs in microscopic dead-end air-filled sacs called alveoli, where a very thin membrane (called the blood-air barrier) separates the blood in the alveolar capillaries (in the walls of the alveoli) from the alveolar air in the sacs.\n\nThe membrane across which gas exchange takes place in the alveoli (i.e. the blood-air barrier) is extremely thin (in humans, on average, 2.2 μm thick). It consists of the alveolar epithelial cells, their basement membranes and the endothelial cells of the pulmonary capillaries (Fig. 4). The large surface area of the membrane comes from the folding of the membrane into about 300 million alveoli, with diameters of approximately 75-300 µm each. This provides an extremely large surface area (approximately 145 m) across which gas exchange can occur.\n\nAir is brought to the alveoli in small doses (called the tidal volume), by breathing in (inhalation) and out (exhalation) through the respiratory airways, a set of relatively narrow and moderately long tubes which start at the nose or mouth and end in the alveoli of the lungs in the chest. Air moves in and out through the same set of tubes, in which the flow is in one direction during inhalation, and in the opposite direction during exhalation.\n\nDuring each inhalation, at rest, approximately 500 ml of fresh air flows in through the nose. Its is warmed and moistened as it flows through the nose and pharynx. By the time it reaches the trachea the inhaled air's temperature is 37 °C and it is saturated with water vapor. On arrival in the alveoli it is diluted and thoroughly mixed with the approximately 2.5–3.0 liters of air that remained in the alveoli after the last exhalation. This relatively large volume of air that is semi-permanently present in the alveoli throughout the breathing cycle is known as the functional residual capacity (FRC).\n\nAt the beginning of inhalation the airways are filled with unchanged alveolar air, left over from the last exhalation. This is the dead space volume, which is usually about 150 ml. It is the first air to re-enter the alveoli during inhalation. Only after the dead space air has returned to the alveoli does the remainder of the tidal volume (500 ml - 150 ml = 350 ml) enter the alveoli. The entry of such a small volume of fresh air with each inhalation, ensures that the composition of the FRC hardly changes during the breathing cycle (Fig. 5). The alveolar partial pressure of oxygen remains very close to 13–14 kPa (100 mmHg), and the partial pressure of carbon dioxide varies minimally around 5.3 kPa (40 mmHg) throughout the breathing cycle (of inhalation and exhalation). The corresponding partial pressures of oxygen and carbon dioxide in the ambient (dry) air at sea level are 21 kPa (160 mmHg) and 0.04 kPa (0.3 mmHg) respectively.\nThis alveolar air, which constitutes the FRC, completely surrounds the blood in the alveolar capillaries (Fig. 6). Gas exchange in mammals occurs between this alveolar air (which differs significantly from fresh air) and the blood in the alveolar capillaries. The gases on either side of the gas exchange membrane equilibrate by simple diffusion. This ensures that the partial pressures of oxygen and carbon dioxide in the blood leaving the alveolar capillaries, and ultimately circulates throughout the body, are the same as those in the FRC.\n\nThe marked difference between the composition of the alveolar air and that of the ambient air can be maintained because the functional residual capacity is contained in dead-end sacs connected to the outside air by long, narrow, tubes (the airways: nose, pharynx, larynx, trachea, bronchi and their branches and sub-branches down to the bronchioles). This anatomy, and the fact that the lungs are not emptied and re-inflated with each breath, provides mammals with a \"portable atmosphere\", whose composition differs significantly from the present-day ambient air.\n\nThe composition of the air in the FRC is carefully monitored, by measuring the partial pressures of oxygen and carbon dioxide in the arterial blood. If either gas pressure deviates from normal, reflexes are elicited that change the rate and depth of breathing in such a way that normality is restored within seconds or minutes.\n\nAll the blood returning from the body tissues to the right side of the heart flows through the alveolar capillaries before being pumped around the body again. On its passage through the lungs the blood comes into close contact with the alveolar air, separated from it by a very thin diffusion membrane which is only, on average, about 2 μm thick. The gas pressures in the blood will therefore rapidly equilibrate with those in the alveoli, ensuring that the arterial blood that circulates to all the tissues throughout the body has an oxygen tension of 13−14 kPa (100 mmHg), and a carbon dioxide tension of 5.3 kPa (40 mmHg). These arterial partial pressures of oxygen and carbon dioxide are homeostatically controlled. A rise in the arterial formula_3, and, to a lesser extent, a fall in the arterial formula_3, will reflexly cause deeper and faster breathing till the blood gas tensions return to normal. The converse happens when the carbon dioxide tension falls, or, again to a lesser extent, the oxygen tension rises: the rate and depth of breathing are reduced till blood gas normality is restored.\n\nSince the blood arriving in the alveolar capillaries has a formula_3 of, on average, 6 kPa (45 mmHg), while the pressure in the alveolar air is 13 kPa (100 mmHg), there will be a net diffusion of oxygen into the capillary blood, changing the composition of the 3 liters of alveolar air slightly. Similarly, since the blood arriving in the alveolar capillaries has a formula_3 of also about 6 kPa (45 mmHg), whereas that of the alveolar air is 5.3 kPa (40 mmHg), there is a net movement of carbon dioxide out of the capillaries into the alveoli. The changes brought about by these net flows of individual gases into and out of the functional residual capacity necessitate the replacement of about 15% of the alveolar air with ambient air every 5 seconds or so. This is very tightly controlled by the continuous monitoring of the arterial blood gas tensions (which accurately reflect partial pressures of the respiratory gases in the alveolar air) by the aortic bodies, the carotid bodies, and the blood gas and pH sensor on the anterior surface of the medulla oblongata in the brain. There are also oxygen and carbon dioxide sensors in the lungs, but they primarily determine the diameters of the bronchioles and pulmonary capillaries, and are therefore responsible for directing the flow of air and blood to different parts of the lungs.\n\nIt is only as a result of accurately maintaining the composition of the 3 liters alveolar air that with each breath some carbon dioxide is discharged into the atmosphere and some oxygen is taken up from the outside air. If more carbon dioxide than usual has been lost by a short period of hyperventilation, respiration will be slowed down or halted until the alveolar formula_3 has returned to 5.3 kPa (40 mmHg). It is therefore strictly speaking untrue that the primary function of the respiratory system is to rid the body of carbon dioxide “waste”. In fact the total concentration of carbon dioxide in arterial blood is about 26 mM (or 58 ml per 100 ml), compared to the concentration of oxygen in saturated arterial blood of about 9 mM (or 20 ml per 100 ml blood). This large concentration of carbon dioxide plays a pivotal role in the determination and maintenance of the pH of the extracellular fluids. The carbon dioxide that is breathed out with each breath could probably be more correctly be seen as a byproduct of the body’s extracellular fluid carbon dioxide and pH homeostats\n\nIf these homeostats are compromised, then a respiratory acidosis, or a respiratory alkalosis will occur. In the long run these can be compensated by renal adjustments to the H and HCO concentrations in the plasma; but since this takes time, the hyperventilation syndrome can, for instance, occur when agitation or anxiety cause a person to breathe fast and deeply thus blowing off too much CO from the blood into the outside air, precipitating a set of distressing symptoms which result from an excessively high pH of the extracellular fluids.\n\nOxygen has a very low solubility in water, and is therefore carried in the blood loosely combined with hemoglobin. The oxygen is held on the hemoglobin by four ferrous iron-containing heme groups per hemoglobin molecule. When all the heme groups carry one O molecule each the blood is said to be “saturated” with oxygen, and no further increase in the partial pressure of oxygen will meaningfully increase the oxygen concentration of the blood. Most of the carbon dioxide in the blood is carried as HCO ions in the plasma. However the conversion of dissolved CO into HCO (through the addition of water) is too slow for the rate at which the blood circulates through the tissues on the one hand, and alveolar capillaries on the other. The reaction is therefore catalyzed by carbonic anhydrase, an enzyme inside the red blood cells. The reaction can go in either direction depending on the prevailing partial pressure of carbon dioxide. A small amount of carbon dioxide is carried on the protein portion of the hemoglobin molecules as carbamino groups. The total concentration of carbon dioxide (in the form of bicarbonate ions, dissolved CO, and carbamino groups) in arterial blood (i.e. after it has equilibrated with the alveolar air) is about 26 mM (or 58 ml/100 ml), compared to the concentration of oxygen in saturated arterial blood of about 9 mM (or 20 ml/100 ml blood).\n\nThe dissolved oxygen content in fresh water is approximately 8–10 milliliters per liter compared to that of air which is 210 milliliters per liter. Water is 800 times more dense than air and 100 times more viscous. Therefore, oxygen has a diffusion rate in air 10,000 times greater than in water. The use of sac-like lungs to remove oxygen from water would therefore not be efficient enough to sustain life. Rather than using lungs, gaseous exchange takes place across the surface of highly vascularized gills. Gills are specialised organs containing filaments, which further divide into lamellae. The lamellae contain capillaries that provide a large surface area and short diffusion distances, as their walls are extremely thin. Gill rakers are found within the exchange system in order to filter out food, and keep the gills clean.\n\nGills use a countercurrent flow system that increases the efficiency of oxygen-uptake (and waste gas loss). Oxygenated water is drawn in through the mouth and passes over the gills in one direction while blood flows through the lamellae in the opposite direction. This countercurrent maintains steep concentration gradients along the entire length of each capillary (see the diagram in the \"Interaction with circulatory systems\" section above). Oxygen is able to continually diffuse down its gradient into the blood, and the carbon dioxide down its gradient into the water. The deoxygenated water will eventually pass out through the operculum (gill cover). Although countercurrent exchange systems theoretically allow an almost complete transfer of a respiratory gas from one side of the exchanger to the other, in fish less than 80% of the oxygen in the water flowing over the gills is generally transferred to the blood.\n\nAmphibians have three main organs involved in gas exchange: the lungs, the skin, and the gills, which can be used singly or in a variety of different combinations. The relative importance of these structures differs according to the age, the environment and species of the amphibian. The skin of amphibians and their larvae is highly vascularised, leading to relatively efficient gas exchange when the skin is moist. The larvae of amphibians, such as the pre-metamorphosis tadpole stage of frogs, also have external gills. The gills are absorbed into the body during metamorphosis, after which the lungs will then take over. The lungs are usually simpler than in the other land vertebrates, with few internal septa and larger alveoli; however, toads, which spend more time on land, have a larger alveolar surface with more developed lungs. To increase the rate of gas exchange by diffusion, amphibians maintain the concentration gradient across the respiratory surface using a process called buccal pumping. The lower floor of the mouth is moved in a \"pumping\" manner, which can be observed by the naked eye.\n\nAll reptiles breathe using lungs. In squamates (the lizards and snakes) ventilation is driven by the axial musculature, but this musculature is also used during movement, so some squamates rely on buccal pumping to maintain gas exchange efficiency.\n\nDue to the rigidity of turtle and tortoise shells, significant expansion and contraction of the chest is difficult. Turtles and tortoises depend on muscle layers attached to their shells, which wrap around their lungs to fill and empty them. Some aquatic turtles can also pump water into a highly vascularised mouth or cloaca to achieve gas-exchange.\n\nCrocodiles have a structure similar to the mammalian diaphragm - the diaphragmaticus - but this muscle helps create a unidirectional flow of air through the lungs rather than a tidal flow: this is more similar to the air-flow seen in birds than that seen in mammals. During inhalation, the diaphragmaticus pulls the liver back, inflating the lungs into the space this creates. Air flows into the lungs from the bronchus during inhalation, but during exhalation, air flows out of the lungs into the bronchus by a different route: this one-way movement of gas is achieved by aerodynamic valves in the airways.\n\nBirds have lungs but no diaphragm. They rely mostly on air sacs for ventilation. These air sacs do not play a direct role in gas exchange, but help to move air unidirectionally across the gas exchange surfaces in the lungs. During inhalation, fresh air is taken from the trachea down into the posterior air sacs and into the parabronchi which lead from the posterior air sacs into the lung. The air that enters the lungs joins the air which is already in the lungs, and is drawn forward across the gas exchanger into anterior air sacs. During exhalation, the posterior air sacs force air into the same parabronchi of the lungs, flowing in the same direction as during inhalation, allowing continuous gas exchange irrespective of the breathing cycle. Air exiting the lungs during exhalation joins the air being expelled from the anterior air sacs (both consisting of \"spent air\" that has passed through the gas exchanger) entering the trachea to be exhaled (Fig. 10). Selective bronchoconstriction at the various bronchial branch points ensures that the air does not ebb and flow through the bronchi during inhalation and exhalation, as it does in mammals, but follows the paths described above.\n\nThe unidirectional airflow through the parabronchi exchanges respiratory gases with a \"crosscurrent\" blood flow (Fig. 9). The partial pressure of O (formula_3) in the parabronchioles declines along their length as O diffuses into the blood. The capillaries leaving the exchanger near the entrance of airflow take up more O than capillaries leaving near the exit end of the parabronchi. When the contents of all capillaries mix, the final formula_3 of the mixed pulmonary venous blood is higher than that of the exhaled air, but lower than that of the inhaled air.\n\nGas exchange in plants is dominated by the roles of carbon dioxide, oxygen and water vapor. is the only carbon source for autotrophic growth by photosynthesis, and when a plant is actively photosynthesising in the light, it will be taking up carbon dioxide, and losing water vapor and oxygen. At night, plants respire, and gas exchange partly reverses: water vapor is still lost (but to a smaller extent), but oxygen is now taken up and carbon dioxide released.\nPlant gas exchange occurs mostly through the leaves. Gases diffuse into and out of the intercellular spaces within the leaf through pores called stomata, which are typically found on the lower surface of the leaf. Gases enter into the photosynthetic tissue of the leaf through dissolution onto the moist surface of the palisade and spongy mesophyll cells. The spongy mesophyll cells are loosely packed, allowing for an increased surface area, and subsequently an increased rate of gas-exchange. Uptake of carbon dioxide necessarily results in some loss of water vapor, because both molecules enter and leave by the same stomata, so plants experience a gas exchange dilemma: gaining enough without losing too much water. Therefore, water loss from other parts of the leaf is minimised by the waxy cuticle on the leaf's epidermis. The size of a stoma is regulated by the opening and closing of its two guard cells: the turgidity of these cells determines the state of the stomatal opening, and this itself is regulated by water stress. Plants showing crassulacean acid metabolism are drought-tolerant xerophytes and perform almost all their gas-exchange at night, because it is only during the night that these plants open their stomata. By opening the stomata only at night, the water vapor loss associated with carbon dioxide uptake is minimised. However, this comes at the cost of slow growth: the plant has to store the carbon dioxide in the form of malic acid for use during the day, and it cannot store unlimited amounts.\n\nGas exchange measurements are important tools in plant science: this typically involves sealing the plant (or part of a plant) in a chamber and measuring changes in the concentration of carbon dioxide with an infrared gas analyzer. If the environmental conditions (humidity, concentration, light and temperature) are fully controlled, the measurements of uptake and water release reveal important information about the assimilation and transpiration rates. The intercellular concentration reveals important information about the photosynthetic condition of the plants. Simpler methods can be used in specific circumstances: hydrogencarbonate indicator can be used to monitor the consumption of in a solution containing a single plant leaf at different levels of light intensity, and oxygen generation by the pondweed \"Elodea\" can be measured by simply collecting the gas in a submerged test-tube containing a small piece of the plant.\n\nThe mechanism of gas exchange in invertebrates depends their size, feeding strategy, and habitat (aquatic or terrestrial).\n\nThe sponges (Porifera) are sessile creatures, meaning they are unable to move on their own and normally remain attached to their substrate. They obtain nutrients through the flow of water across their cells, and they exchange gases by simple diffusion across their cell membranes. Pores called ostia draw water into the sponge and the water is subsequently circulated through the sponge by cells called choanocytes which have hair-like structures that move the water through the sponge.\n\nThe cnidarians include corals, sea anemones, jellyfish and hydras. These animals are always found in aquatic environments, ranging from fresh water to salt water. They do not have any dedicated respiratory organs; instead, every cell in their body can absorb oxygen from the surrounding water, and release waste gases to it. One key disadvantage of this feature is that cnidarians can die in environments where water is stagnant, as they deplete the water of its oxygen supply. Corals often form symbiosis with other organisms, particularly photosynthetic dinoflagellates. In this symbiosis, the coral provides shelter and the other organism provides nutrients to the coral, including oxygen.\n\nThe roundworms (Nematoda), flatworms (Platyhelminthes), and many other small invertebrate animals living in aquatic or otherwise wet habitats do not have a dedicated gas-exchange surface or circulatory system. They instead rely on diffusion of and directly across their cuticle. The cuticle is the semi-permeable outermost layer of their bodies.\n\nOther aquatic invertebrates such as most molluscs (Mollusca) and larger crustaceans (Crustacea) such as lobsters, have gills analogous to those of fish, which operate in a similar way.\nUnlike the invertebrates groups mentioned so far, insects are usually terrestrial, and exchange gases across a moist surface in direct contact with the atmosphere, rather than in contact with surrounding water. The insect's exoskeleton is impermeable to gases, including water vapor, so they have a more specialised gas exchange system, requiring gases to be directly transported to the tissues via a complex network of tubes. This respiratory system is separated from their circulatory system. Gases enter and leave the body through openings called spiracles, located laterally along the thorax and abdomen. Similar to plants, insects are able to control the opening and closing of these spiracles, but instead of relying on turgor pressure, they rely on muscle contractions. These contractions result in an insect's abdomen being pumped in and out. The spiracles are connected to tubes called tracheae, which branch repeatedly and ramify into the insect's body. These branches terminate in specialised tracheole cells which provides a thin, moist surface for efficient gas exchange, directly with cells.\n\nThe other main group of terrestrial arthropod, the arachnids (spiders, scorpion, mites, and their relatives) typically perform gas exchange with a book lung.\n\n"}
{"id": "26780188", "url": "https://en.wikipedia.org/wiki?curid=26780188", "title": "Governess cart", "text": "Governess cart\n\nA Governess cart is a small two-wheeled horse-drawn cart. Their distinguishing feature is a small tub body, with two opposed inward-facing seats. They could seat four, although there was little room for four large adults. The driver sat sideways on one of these seats. The centre rear of the body was lowered, or else had a small hinged door, and there was a step beneath. The wheels were of moderate size, always fitted with mud guards, and usually carried on elliptical springs. The axle was either straight or dropped, giving a low, stable, centre of gravity. \n\nThe purpose of the cart was to be light enough to be drawn by a well-tempered pony or cob, who would be gentle enough, according to the mores of the time, to be handled by a lady. This gave rise to the cart's name, as they were frequently used by governesses to transport their child charges. The governess rode in the cart with the passengers, where they could easily be observed. The cart was also relatively safe, being difficult to either fall from, overturn, or to injure oneself with either the horse or wheels.\n\nThe governess cart was a relatively late development in horse-drawn vehicles, appearing around 1900 as a substitute for the dogcart. These were a similar light cart, but their high exposed seats had a poor safety record for passengers, particularly children, falling from them.\n"}
{"id": "37763283", "url": "https://en.wikipedia.org/wiki?curid=37763283", "title": "Himpunan Hijau", "text": "Himpunan Hijau\n\nHimpunan Hijau (English: \"Green Assembly\") is a Malaysian environmentalist movement protesting against the Lynas Advanced Materials Plant (LAMP), a rare earth processing plant operating in Gebeng, Kuantan, Pahang set up by the Australian company Lynas. The refinery process is accused of dumping tonnes of toxic and radioactive waste on the local lives, livelihoods, the environment and the health of future generations. The \"Save Malaysia Stop Lynas\" (SMSL) group is currently led by Bentong parliamentary candidate Wong Tack.\n\nThe refining facility entered production in 2013, producing 1,089 tonnes of rare earth oxides in the first quarter of 2014, with a target of 11,000 tonnes per annum. On 2 September 2014, Lynas was issued a 2-year Full Operating Stage License (FOSL) by the Malaysian Atomic Energy Licensing Board (AELB).\n\n"}
{"id": "49485101", "url": "https://en.wikipedia.org/wiki?curid=49485101", "title": "Horse Trough at 315 S 9th St", "text": "Horse Trough at 315 S 9th St\n\nA Horse Trough at 315 S 9th St in Philadelphia is made of stone. It's listed in the Philadelphia Register of Historic Places.\n"}
{"id": "7318679", "url": "https://en.wikipedia.org/wiki?curid=7318679", "title": "IEEE Nuclear and Plasma Sciences Society", "text": "IEEE Nuclear and Plasma Sciences Society\n\nThe IEEE Nuclear and Plasma Sciences Society (NPSS) is a transnational group of about 3000 professional engineers and scientists. The IEEE-affiliated Society sponsors five major annual, and 12 biennial conferences and symposia. It also sponsors or co-sponsor four peer-reviewed academic journals.\n\nThe IEEE is the world's largest technical professional organization with more than 420,000 members in around 160 countries.\n\nIn 1947, the Institute of Radio Engineers (IRE) formed the Nuclear Studies Committee to assess the role of the IRE in the field. Two years later, the Professional Group on Nuclear Science was formed. In 1954 the first issue of the Transactions on Nuclear Science appeared, increasing to four issues per year in 1956.\n\nIn 1963 the American Institute of Electrical Engineers (AIEE) merged with the IRE to form the IEEE, and the AIEE Nucleonics Committee and the Committee on Nucleaonic and Radiation Instruments merged with the IRE Professional Group on Nuclear Science on October 29, 1963, to become the Nuclear Science Group of the IEEE.\n\nIn 1972, the scope of the group was widened to include plasma sciences, and the group became a Society, its name changing to the Nuclear and Plasma Sciences Society.\n\nThe first issue of the Transactions on Nuclear Science first appeared in 1954, the number of issues rising to four per year in 1956 and bimonthly in 2017. In 2005, the journal was ranked No.3 for impact factor in Nuclear Science & Engineering by Thomson. The journal focuses on instrumentation for the detector and measurement of ionizing radiation, particle accelerators and their controls, nuclear medicine and its application, effects of radiation on materials, components, and systems, nuclear reactor instrumentation and controls as well as measurement of radiation in space.\n\nThe first issue of the Transactions on Plasma Science appeared in 1973. The 'Tennessee Orange,' cover color was picked by Igor Alexeff, Professor, University of Tennessee, Knoxville. Published bimonthly, its scope includes all aspects of the theory and application of plasma science, and regularly features \"Special issues\" (e.g. eight in 2004). Between 2000-2004, the journal was ranked No.10 for impact factor in \"Physics - Fluids & Plasmas\" by Thomson.\n\nThe journal encourages papers on the imaging of body structures, usually \"in situ\", rather than microscopic biological entities. In the Journal Citation Reports (JCR) from Thomson, which ranks journals by their impact factor, in 2005, the journal was ranked:\n\n\nThe journal is co-sponsored by NPSS and the Engineering in Medicine and Biology Society (EMBS) and focuses on technology and application areas related to radiation- and plasma-based medical sciences. The first issue has been published in January 2017.\n\nThe IEEE Nuclear and Plasma Sciences Society organizes a number of conferences. A calendar of upcoming conferences can be found on the IEEE NPSS conference website. Following is a list of several IEEE NPSS conferences.\n\n\nAreas of technical activity include:\n\n\n"}
{"id": "11655845", "url": "https://en.wikipedia.org/wiki?curid=11655845", "title": "Indoor air pollution in developing nations", "text": "Indoor air pollution in developing nations\n\nIndoor air pollution in developing nations is a significant form of indoor air pollution (IAP) that is little known to those in the developed world.\n\nThree billion people in developing countries across the globe rely on biomass, in the form of wood, charcoal, dung, and crop residue, as their domestic cooking fuel. Because much of the cooking is carried out indoors in environments that lack proper ventilation, millions of people, primarily poor women and children face serious health risks. Globally, 4.3 million deaths were attributed to exposure to IAP in developing countries in 2012, almost all in low and middle income countries. The South East Asian and Western Pacific regions bear most of the burden with 1.69 and 1.62 million deaths, respectively. Almost 600,000 deaths occur in Africa, \n200,000 in the Eastern Mediterranean region, 99,000 in Europe and 81,000 in the Americas. The remaining 19,000 deaths occur in high income countries.\n\nEven though the rate of dependence on biomass fuel is declining, this dwindling resource will not keep up with population growth which could ultimately put environments at even greater risk.\n\nOver the past several decades, there have been numerous studies investigating the air pollution generated by traditional household solid fuel combustion for space heating, lighting, and cooking in developing countries. It is now well established that, throughout much of the developing world, indoor burning of solid fuels (biomass, coal, etc.) by inefficient, often insufficiently vented, combustion devices results in elevated exposures to household air pollutants. This is due to the poor combustion efficiency of the combustion devices and the elevated nature of the emissions. In addition, they are often released directly into living areas. Smoke from traditional household solid fuel combustion commonly contains a range of incomplete combustion products, including both fine and coarse particulate matter (e.g., PM, PM), carbon monoxide (CO), nitrogen dioxide (NO), sulfur dioxide (SO), and a variety of organic air pollutants (e.g., formaldehyde, 1,3-butadiene, benzene, acetaldehyde, acrolein, phenols, pyrene, benzopyrene, benzo(a)pyrene, dibenzopyrenes, dibenzocarbazoles, and cresols). In a typical solid fuel stove, about 6–20% of the solid fuel is converted into toxic emissions (by mass). The exact quantity and relative composition is determined by factors such as the fuel type and moisture content, stove type and operation influencing the amount.\n\nWhile many pollutants can evolve, most measurements have been focused on breathing-zone exposure levels of particulate matter (PM) and carbon monoxide (CO), which are the main products of incomplete combustion and are considered to pose the greatest health risks. Indoor PM exposure levels have been consistently reported to be in the range of hundreds to thousands of micrograms per cubic meter (μg/m). Similarly, CO exposure levels have been measured to be as high as hundreds to greater than 1000 milligrams per cubic meter (mg/m). A recent study of 163 households in two rural Chinese counties reported geometric mean indoor PM concentrations of 276 μg/m (combinations of different plant materials, including wood, tobacco stems, and corncobs), 327 μg/m (wood), 144 μg/m (smoky coal), and 96 μg/m (smokeless coal) for homes using a variety of different fuel types and stove configurations (e.g., vented, unvented, portable, fire pit, mixed ventilation stove).\n\nRural Kenya has been the site of various applied research projects to determine the intensity of emissions that commonly occur from use of biomass fuels, particularly wood, dung, and crop residue. Smoke is the result of the incomplete combustion of solid fuel which women and children are exposed to up to seven hours each day in closed environments.\nThese emissions vary from day to day, season to season and with changes in the amount of airflow within the residence. Exposure in poor homes far exceeds accepted safety levels by as much as one hundred times over. Because many Kenyan women utilize a three-stone fire, the worst offender, one kilogram of burning wood produces tiny particles of soot which can clog and irritate the bronchial pathways. The smoke also contains various poisonous gases such as aldehydes, benzene, and carbon monoxide. Exposure to IAP from combustion of solid fuels has been implicated, with varying degrees of evidence, as a causal agent of several diseases. Acute lower respiratory infections (ALRI) and chronic obstructive pulmonary disease (COPD) are the leading causes of disease and death from exposure to smoke. Cataracts and blindness, lung cancer, tuberculosis, premature births and low birth weight are also suspected of being caused by IAP.\n\nWomen and girls are largely responsible for collecting fuel-wood for cooking in most households, particularly in rural communities and in refugee camps. This increases their vulnerability to incidences of violence, including beating and bodily injury, assault, and rape. This time could be spent in more productive ways such as attending school or income production – improved cookstove interventions in refugee camps have shown significant decreases in fuelwood collection times reported by women. Moreover, removing the need for women to collect fuelwood also significantly reduces reported rapes during firewood collection. The use of biomass coupled with inefficient cooking apparatus leads to a web of social and environmental concerns which directly links to the United Nations Millennium Development Goals. Women and girls also make up the majority of deaths from household air pollution (HAP), which has been shown to affect nearly three billion people worldwide. Replacing traditional kerosene/firewood stoves with cleaner ethanol stoves in Nigeria has been shown to mitigate adverse pregnancy outcomes from HAP.\n\nUnfortunately, finding an affordable solution to address the many effects of IAP – improving combustion, reducing smoke exposure, improving safety and reducing labor, reducing fuel costs, and addressing sustainability – is complex and in need of continual improvement. Efforts to improve cook stoves in the past, beginning in the 1950s, were primarily aimed at minimizing deforestation with no concern for IAP, though the effectiveness of these efforts to save firewood is debatable. Various attempts had various outcomes. For example, some improved stove designs in Kenya significantly reduced particulate emissions but produced higher CO and SO emissions. Flues to remove smoke were difficult to design and were fragile.\n\nCurrent improved interventions however, include smoke hoods which operate in much the same manner as flues, to extract smoke, but are found to reduce levels of IAP more effectively than homes that relied solely on windows for ventilation.\nSome features of newly improved stoves include a chimney, enclosing the fire to retain heat, designing a pot holder to maximize heat transfer, dampers to control air flow, a ceramic insert to reduce heat loss, and multi-pot systems to allow for cooking multiple dishes.\n\nStoves are now known to be one of the least-cost means to achieve the combined objective of reducing the health burden of IAP and in some areas reducing environmental stress from biomass harvesting. Some success in installation of interventions, including improved cook stoves, has been achieved primarily due to an interdisciplinary approach which includes multiple stakeholders. These projects have discovered that key socio-economic issues must be addressed to ensure the success of intervention programs. A multitude of complex issues indicate improved stoves are not merely a tool to save fuel.\n\nThe following information represents one successful intervention known as the Kenya Smoke and Health Project (1998–2001) which involved fifty rural households in two separate regions, Kajiado and West Kenya. These areas were chosen due to different climate, geographic, and cultural implications. Community participation was the primary focus for this project and as a result, those involved indicated the results far exceeded their expectations. Local women's groups and, in the case of the project in West Kenya, men were actively involved. By involving the end-users the project resulted in more widespread acceptance and created the further benefit of providing local income.\n\nThree key interventions were discussed and disseminated; ventilation by enlarging windows or opening eaves spaces, adding smoke hoods over the cooking area, or the option of installing an improved cook stove such as the Upesi stove. Smoke hoods are free-standing units that act like flues or chimneys in their effort to draw smoke out of the dwelling. They can be used over traditional open fires and this study showed they contribute to considerably lower levels of IAP. The smoke hood models were made with hard manila paper and then transferred to heavy-gauge galvanized sheet metal and manufactured locally. This resulted in further employment opportunities for the artisans who were trained by the project. The Upesi stove, made of clay and kiln-fired, was developed by Practical Action and East African partners to utilize wood and agricultural wastes. Because this stove was designed and adapted for local needs it produced several winning features. Not only does it cut the use of fuel-wood by approximately half, and reduce exposure to household smoke, it also empowers local women by creating employment as they are the ones who make and market the stoves. These women's groups gain access to technical training in production and marketing and enjoy higher wage earnings and improved social status as a result of the introduction of this improved stove.\n\nVarious benefits were realized including improved health; the most important aspect to each of the villagers involved. The people reported less internal heat allowing for better sleep, fewer headaches and less fatigue, less eye irritation and coughs and dizziness. Safety increased due to the smoke hoods preventing goats and children from falling into the fire and less soot contamination was observed, along with snakes and rodents not entering the home. Windows allowed for the ability to view cattle from indoors, and also reduced kerosene needs due to improved interior lighting. Overall, the indoor environment improved greatly from various simple things that are taken for granted in modern western homes. Greater indoor light also allows for more income generation for women as they can do beadwork by the window when weather doesn't allow for this work outdoors. Children also benefit from increased lighting for homework.\n\nInterpersonal relationships developed among the women due to the project, and men better supported their wives initiative when the end result benefited them as well. While initial efforts to improve stoves were limited in success, current efforts are more successful due to the recognition that sustainable domestic energy resources are \"central to reducing poverty and hunger, improving health…and improving the lives of women and children\" The optimal short-term goal in minimizing rural poverty is to provide inexpensive and acceptable solutions to the local people. Not only can stoves contribute to this intervention, but the use of cleaner fuels will also provide further benefits.\n\nSimilar improved-stove projects have proven successful in other regions of the world. Improved stoves installed as part of the Randomized Exposure Study of Pollution Indoors and Respiratory Effects (RESPIRE) study in Guatemala were found to be acceptable to the population and produce significant health benefits for both mothers and children. Mothers in the intervention group had lower blood pressure and reductions in eye discomfort and back pain. Intervention households were also found to have lower levels of small particles and carbon monoxide. Children in these households also had lower rates of asthma. This initial pilot program has evolved into CRECER (Chronic Respiratory Effects of Early Childhood Exposure to Respirable Particulate Matter), which will attempt to follow children in intervention households for a longer period of time to determine whether the improved stoves also contribute to greater health over the lifespan.\n\nThe National Program on Improved Chulhas in India has also had some success in encouraging the use of improved stoves among at-risk populations. Begun in the mid-1980s, this program provides subsidies to encourage families to purchase the longer-lasting chulhas and have a chimney installed. A 2005 study showed that stoves with chimneys are associated with a lower incidence of cataracts in women. Much of the available information from India is more of a characterization of the issue and there is less data available from intervention trials.\n\nChina has been particularly successful at encouraging the use of improved stoves, with hundreds of millions of stoves installed since the beginning of the project in the early 1980s. The government very intentionally targeted poorer, rural households, and by the late 1990s nearly 75% of such households contained \"improved kitchens.\" A 2007 review of 3500 households showed an improvement in indoor air quality in intervention households characterized by lower concentrations of small particles and carbon monoxide in household air. The program in China involved intervention on a large scale, but the cost of stoves was heavily subsidized so it is not known if its success could be replicated.\n\nMortality and burden of disease are not the only detrimental effects from utilizing inefficient energy technology such as the combustion of biomass. Kenya's pre-dominant energy source is biomass, providing more than 90 per cent of rural household energy needs, about one-third in the form of charcoal and the rest from firewood. Biomass energy sourced primarily from savannah woodlands includes firewood for inhabitants and charcoal for urban use. A small percentage is sourced by neighboring communities from closed and protected forests which are generally found in high population density areas. While biomass harvesting in sensitive areas is problematic, it is now determined that the great majority of biomass clearing is due to agricultural expansion and land conversion. Approximately 38% of households 'in high agro-ecological zones' utilize agricultural waste due to frequent shortages of conventional fuel-wood. Use of crop residue and animal waste for domestic energy has detrimental results on soil quality and agricultural and livestock productivity. These materials are ultimately not available as soil conditioners, organic fertilizer, and livestock fodder, not to mention the \"cumulative effects on national food security\".\n\nMost farmers are aware however, that when agricultural waste and dung are not used for energy, they are important elements to maintaining soil fertility. One of the most efficient ways to utilize crop waste and dung for domestic energy is to produce briquettes. The process of compacting the material into a donut shape creates more efficient combustion which contributes to reduced emission levels. A simple device allows for this process and it can be done locally.\n\nLarge-scale combustion of biomass is only feasible if carried out in a sustainable manner. Concern is paramount for regeneration of renewable and sustainable fuel-wood sources if it is to continue to be available long-term. Attempts at sustainable solutions in Kenya could include developing energy crops (trees and shrubs) which would also provide additional income for farmers. This solution would benefit cropland or rangeland prone to erosion and flooding as the root systems and leaf litter would enhance soil stability. Careful selection of regenerating varieties would be most sustainable because soil stability is not disrupted due to tilling and planting. Some people view this solution as a way to further exploit forests, but with proper management of forest resources this could be a viable solution.\n\nFuels and cook stove technology can be assessed on two factors: energy efficiency and emissions in the household. High-performing efficient stoves can improve environmental outcomes to an extent even with unclean fuels (such as firewood and biomass). According to a study comparing environmental, social, and economic life cycle impacts of cooking fuels, with more efficient stoves “more of the heating value of the fuel is converted into useful cooking energy and therefore less fuel must be produced, transported, and burned to deliver the same amount of cooking.”\n\nOther sustainable options include liquid and gas fuels that are combusted in high-performing efficient stoves. For instance, ethanol produced from cellulosic/non-food feedstocks (wood, agricultural residue) has lower environmental life cycle impacts compared to ethanol produced from sugar and starch materials. LPG, though made from non-renewable fossil fuels, still has lower negative environmental impacts than traditional fuels – thus even though it is not a sustainable alternative, it creates far less emissions impacts than traditional fuels.\n\nThere exist trade-offs between efficiency and sustainability on the supply side of the cook stoves and fuels market. While developing cook stoves that use efficient and clean fuel sources is environmentally ideal, often this is not a viable solution due to barriers to scaling up production and consumption of these fuels. For instance, electric stoves are cited as emission-free ‘clean’ alternatives to biomass at the household level. \n\nOn the demand side, challenges exist in terms of creating an enabling environment for cook stove acquisition. Incorporating culturally sensitive behavior change techniques (BCTs) into demand interventions is necessary to foster large-scale behavior change, as discussed below. The other prohibitive aspect of cook stove interventions that do not involve paternalistic good provision is the high up-front costs of improved stoves. Consumers at the bottom of the income pyramid are often the target end-users of these improved technologies, but due to a lack of collateral or isolation, they do not have access to traditional forms of consumer finance and credit. Innovations in business models and the increased proliferation of microfinance institutions (MFIs) are addressing these issues – however, MFIs face challenges of scaling up.\n\nGiven the negative externalities associated with unclean cook stove technology – the negative impact on women and girls; lack of environmental sustainability; and increased risk of diseases associated with HAP – there is a strong case for government intervention. For instance, one form of intervention could be direct subsidies linked to health and climate impacts – for example, targeted subsidies carbon markets. The provision of public goods such as consumer education, access to consumer finance would also be beneficial interventions. Subsidies towards investment in R&D for cleaner technologies and fuels, as well as for the implementation of a baseline standards and testing framework for cleanliness and efficiency (also provision of a public good), is necessary to create an effective and sustainable supply chain.\n\nThere have been significant developments in energy efficient cooking solutions, such as the Wonderbag, which can also significantly reduce fuel requirements for residential cooking. Improvements in technology have allowed for the use of more sustainable cooking solutions with traditional fuels, such as the BioLite Home Stove, a biomass stove which reduces fuel consumption by 50% and emissions by up to 95%. Innovations in business models have also allowed for cook stove suppliers to “dramatically improve both manufacturer and end-user economics, while achieving high levels of health and environmental benefits.” For instance, Inyeneri is a for-profit energy company in Rwanda that operates as more as a ‘cooking fuel utility company’. Its model successfully addresses a number of problems with stove adoption including prohibitively high upfront stove costs, consumer tendency to combine new and old cooking solutions, and lack of commercial viability of these enterprises. Additionally, innovations in mobile technology have allowed companies like PayGo Energy in Kenya and KopaGas in Tanzania to overcome the cost barrier that low-income consumers face, including the high up-front cost of stoves and the inaccessibility of purchasing fuels in small quantities (a form of the poverty penalty). MFIs have also begun to turn their attention to clean energy access, as seen with the success of the USAID-funded Renewable Energy Microfinance and Microenterprise Program (REMMP).\n\nEducational intervention can contribute to reducing exposure to smoke by implementing behaviour change techniques that people to the dangers and encourage a willingness to alter living and cultural practices which could have a significant impact on mitigating exposure to IAP. Behavior change is one aspect of influencing demand that can be achieved through targeted social marketing campaigns, which are usually of two types: either mass market campaigns, or focused approaches at the local and household level that employ demonstrations and follow up visits. Studies show that mass marketing campaigns do in fact increase awareness of risks of household air pollution, but are often unsuccessful in raising purchases of improved stoves.\n\nDesign of these interventions must be undertaken with the knowledge that “consumer needs and preferences are complex and are influenced by many contextual and social factors that require a deep understanding of culture, going beyond technology and economics.” These factors include intra-gender issues, felt needs, cultural significance of food, and religious and cultural beliefs. Evidence of one successful government intervention was revealed by China who, between 1980 and 1995, disseminated 172 million improved cookstoves. This effort proved more successful due to the inclusion of local users, particularly women, who were involved in the design and fieldwork process.\n\nChildren up to five years of age spend 90% of their time at home. Globally, 50% of pneumonia deaths among children under five years of age are due to particulate matter inhaled from indoor air pollution. Many homes around the world used solid fuels for cooking. These fuels release large amounts of carbon monoxide and fine particulate matter. These chemical irritants when inhaled may cause different pulmonary conditions ranging from pulmonary epithelial cancer or acute pulmonary tract infection.\n\nAs of 2004, Kenya has shown a willingness to undertake biomass energy issues with the understanding that consumption is associated with indoor air pollution and environmental degradation. Suggestions from the United Nations Development Programme include establishing an institution that will deal exclusively with biomass energy by developing policy guidelines on sustainable firewood, charcoal, and modern biomass such as cleaner fuels and wind, solar, and small scale hydropower. Short-term solutions rest in more efficient domestic energy use by way of improved cook stoves which provide more affordable options in the near future than a complete shift to nonsolid fuels. Long-term solutions rest on transition to modern cleaner fuels and alternative energy sources within a broad international and national policy and economic agenda. Government support for long-term solutions is feasible as witnessed by current efforts in Zambia to develop policy to promote biofuels.\n\nKenya is the world leader in the number of solar power systems installed per capita (but not the number of watts added). More than 30,000 small solar panels, each producing 12 to 30 watts, are sold in Kenya\nannually. For an investment of as little as $100 for the panel and wiring, the PV system can be used to charge a car battery, which can then provide power to run a fluorescent lamp or a small television for a few hours a day. More Kenyans adopt solar power every year than make connections to the country's electric grid.\n\nNational and international effort must be stepped up to advance short and long term solutions for the millions of women and children who suffer from poverty and disease as a result of indoor air pollution. Scientists predict the African continent will be the first to experience the effects of global warming where widespread poverty will put millions at further risk due to their limited capabilities to adapt. The potential is great for a more sustainable Africa with commitment from within and outside the region. Pneumonia is the number one killer of children in the world and indoor air pollution is a strongly significant risk factor for severe pneumonia. The global health community designated 2 November to be World Pneumonia Day in order to raise awareness about the disease and its causes.\n\n\n"}
{"id": "20077496", "url": "https://en.wikipedia.org/wiki?curid=20077496", "title": "Jillian Marsh", "text": "Jillian Marsh\n\nJillian Marsh was raised in the coal-mining town of Leigh Creek, in South Australia’s Flinders Ranges, and she has had a long interest in mining issues and indigenous communities. In 1998 Marsh received the prestigious Jill Hudson Environmental Award for her work in educating people living near the Beverly Uranium Mine about the toxic dangers of uranium mining. In 2004 Marsh won a Doctoral candidacy at Adelaide University’s Geographical and Environmental Studies Department. Her PhD research topic is \"A Look at the Approval of Beverley Mine and the Ways that Decisions are Made When Mining Takes Place in Adnyamathanha Country\". In 2008, Jillian Marsh received the Nuclear-Free Future Award.\n\n\n"}
{"id": "23702522", "url": "https://en.wikipedia.org/wiki?curid=23702522", "title": "Joule thief", "text": "Joule thief\n\nA joule thief is a minimalist self-oscillating voltage booster that is small, low-cost, and easy to build, typically used for driving small loads. This circuit is also known by other names such as \"blocking oscillator\", \"joule ringer\", \"vampire torch\".\n\nIt can use nearly all of the energy in a single-cell electric battery, even far below the voltage where other circuits consider the battery fully discharged (or \"dead\"); hence the name, which suggests the notion that the circuit is \"stealing\" energy or \"joules\" from the source - the term is a pun on the age-old expression \"jewel thief\".\n\nThe circuit is a variant of the blocking oscillator that forms an unregulated voltage boost converter. The output voltage is increased at the expense of higher current draw on the input, but the integrated (average) current of the output is lowered and brightness of a luminescence decreased.\n\nThe joule thief is not a new concept. Basically, it adds an LED to the output of a self-oscillating voltage booster, which was patented many decades ago.\n\n\nIn November 1999 issue of \"Everyday Practical Electronics\" (\"EPE\") magazine, the \"Ingenuity Unlimited\" (reader ideas) section had a novel circuit idea entitled \"One Volt LED - A Bright Light\" by from Swindon, Wilts, UK. Three example circuits were shown for operating LEDs from supply voltages below 1.5 Volts. The basic circuits consisted of a transformer-feedback ZTX450 NPN transistor voltage converter based on the blocking oscillator. Kaparnik acknowledged the concept predated World War II.\n\nIn 2002, the name \"Joule Thief\" was coined by Clive Mitchell and given to his variant of Kaparnik's circuit (from 1999 EPE magazine) which consisted of a single cell, a single BC549 NPN transistor, a coil with two windings, a single resistor (typically 1000 ohms), and a single white LED. Clive originally named the circuit \"Vampire Torch\", because it sucked the last remnants of life from a battery.\n\nMitchell's newer circuit is essentially the same as Kaparnik's older circuit, except for component values:\n\nThe circuit works by rapidly switching the transistor. Initially, current begins to flow through the resistor, secondary winding, and base-emitter junction (see diagram) which causes the transistor to begin conducting collector current through the primary winding. Since the two windings are connected in opposing directions, this induces a voltage in the secondary winding which is positive (due to the winding polarity, see dot convention) which turns the transistor on with higher bias. This self-stroking/positive-feedback process almost instantly turns the transistor on as hard as possible (putting it in the saturation region), making the collector-emitter path look like essentially a closed switch (since V will be only about 0.1 volts, assuming that the base current is high enough). With the primary winding effectively across the battery, the current increases at a rate proportional to the supply voltage divided by the inductance. Transistor switch-off takes place by different mechanisms dependent upon supply voltage.\n\nThe gain of a transistor is not linear with V. At low supply voltages (typically 0.75 V and below) the transistor requires a larger base current to maintain saturation as the collector current increases. Hence, when it reaches a critical collector current, the base drive available becomes insufficient and the transistor starts to pinch off and the previously described positive feedback action occurs turning it hard off.\n\nTo summarize, once the current in the coils stops increasing for any reason, the transistor goes into the cutoff region (and opens the collector-emitter \"switch\"). The magnetic field collapses, inducing however much voltage is necessary to make the load conduct, or for the secondary-winding current to find some other path.\n\nWhen the field is back to zero, the whole sequence repeats; with the battery ramping-up the primary-winding current until the transistor switches on.\n\nIf the load on the circuit is very small the rate of rise and ultimate voltage at the collector is limited only by stray capacitances, and may rise to more than 100 times the supply voltage. For this reason, it is imperative that a load is always connected so that the transistor is not damaged. Because V is mirrored back to the secondary, failure of the transistor due to a small load will occur through the reverse V limit for the transistor being exceeded (this occurs at a much lower value than Vmax).\n\nThe transistor dissipates very little energy, even at high oscillating frequencies, because it spends most of its time in the fully on or fully off state, so either voltage over or current through the transistor is zero, thus minimizing the switching losses.\n\nThe switching frequency in the example circuit opposite is about . The light-emitting diode will blink at this rate, but the persistence of the human eye means that the blinking will not be noticed.\n\nA simple modification of the previous schematic replaces the LED with three components to create a simple zener diode based voltage regulator. Diode D1 acts as a half-wave rectifier to allow capacitor C to charge up only when a higher voltage is available from the joule thief on the left side of diode D1. The Zener diode D2 limits the output voltage.\n\nA better solution is shown in the next schematic example.\n\nWhen a more constant output voltage is desired, the joule thief can be given a closed-loop control. In the example circuit, the Schottky diode D1 blocks the charge built up on capacitor C1 from flowing back to the switching transistor Q1 when it is turned on. A 5.6 Volt Zener diode D2 and transistor Q2 forms the feedback control: when the voltage across the capacitor C1 is higher than the threshold voltage formed by Zener voltage of D2 plus the base-emitter turn-on voltage of transistor Q2, transistor Q2 is turned on diverting the base current of the switching transistor Q1, impeding the oscillation and prevents the voltage across capacitor C1 from rising even further. When the voltage across C1 drops below the threshold voltage Q2 turns off, allowing the oscillation to happen again. If the load requires even lower ripple, in this example some delicate digital circuitry like a microcontroller, a linear regulator can be used after this to smooth the ripple out.\n\n\n"}
{"id": "2947991", "url": "https://en.wikipedia.org/wiki?curid=2947991", "title": "Kelda Group", "text": "Kelda Group\n\nKelda Group is a British utility company. It is based in Bradford, England. It was formerly listed on the London Stock Exchange and a constituent of the FTSE 100 Index, but was taken private by a group of investors, Saltaire Water, in 2008. Saltaire Water members include CitiBank and HSBC Holdings.\n\nKelda Group, originally known as \"Yorkshire Water plc\", was one of the regional water companies privatised in 1989. It changed to its current name in 1999. In 2000 Kelda purchased the United States water supply business Aquarion and subsequently announced the conditional sale of this asset in February 2006. It was taken private in a £3.04 billion deal in February 2008 by Saltaire Water, a consortium of investment companies including Citigroup and HSBC.\n\nYorkshire Water remains its principal UK business. It provides water to 4.5 million people in northern England, mainly within the geographical county of Yorkshire. It has interests in non-regulated businesses related to its main activity, such as environmental management.\n\n"}
{"id": "33297884", "url": "https://en.wikipedia.org/wiki?curid=33297884", "title": "Lan Sang National Park", "text": "Lan Sang National Park\n\nLan Sang National Park () is in the Dawna Range, Tak Province, northern Thailand. Established in 1979, it is an IUCN Category II protected area measuring . On the Tak-Mae Sot Highway in Mueang Tak District, it became the country's 15th national park.\n\nThe precise meaning of the name is not known. Each of the two words have various interpretations. \n\nLan Sang National Park covers an area of 65,000 rai. Various types of forest, such as rain forest, coniferous forest, hill evergreen forest, deciduous dipterocarp forest, and mixed deciduous forest, are found in different geographical areas. Wild animals found are common wild pig, barking deer, Siamese big-headed turtle, serow, civet, Black-crested Bulbul, and flying lizards\n"}
{"id": "37849809", "url": "https://en.wikipedia.org/wiki?curid=37849809", "title": "Liftra", "text": "Liftra\n\nLiftra is an engineering company specialized in lifting and transport equipment for wind turbines. Since the start-up in 2003 the company has carried out over 500 projects; its most extensive development project has been a self-hoisting add-on crane for wind turbines in the +4MW class, that is able to replace 19 Ton components.\n\nThe main office is in Aalborg, Northern Jutland, Denmark with approx. 20 engineers. Offices in Germany, USA, Spain and China.\n\n"}
{"id": "13363621", "url": "https://en.wikipedia.org/wiki?curid=13363621", "title": "Lundquist number", "text": "Lundquist number\n\nIn plasma physics, the Lundquist number (denoted by formula_1) is a dimensionless ratio which compares the timescale of an Alfvén wave crossing to the timescale of resistive diffusion. It is a special case of the Magnetic Reynolds number when the Alfvén velocity is the typical velocity scale of the system, and is given by\n\nwhere formula_3 is the typical length scale of the system, formula_4 is the magnetic diffusivity and formula_5 is the Alfvén velocity of the plasma. \n\nHigh Lundquist numbers indicate highly conducting plasmas, while low Lundquist numbers indicate more resistive plasmas. Laboratory plasma experiments typically have Lundquist numbers between formula_6, while in astrophysical situations the Lundquist number can be greater than formula_7. Considerations of Lundquist number are especially important in magnetic reconnection.\n\n"}
{"id": "459872", "url": "https://en.wikipedia.org/wiki?curid=459872", "title": "Magnesium diboride", "text": "Magnesium diboride\n\nMagnesium diboride is the inorganic compound with the formula MgB. It is a dark gray, water-insoluble solid. The compound has attracted attention because it becomes superconducting at T = 39K. In terms of its composition, MgB differs strikingly from most superconductors of comparable T's, which feature transition metals.\n\nMagnesium diboride's superconducting properties were discovered in 2001. Its critical temperature (\"T\") of is the highest amongst conventional superconductors. Among conventional (phonon-mediated) superconductors, it is unusual. Its electronic structure is such that there exist two types of electrons at the Fermi level with widely differing behaviours, one of them (sigma-bonding) being much more strongly superconducting than the other (pi-bonding). This is at odds with usual theories of phonon-mediated superconductivity which assume that all electrons behave in the same manner. Theoretical understanding of the properties of MgB has nearly been achieved by modelling two energy gaps. In 2001 it was regarded as behaving more like a metallic than a cuprate superconductor.\n\nUsing BCS theory and the known energy gaps of the pi and sigma bands of electrons (2.2 and 7.1 meV, respectively), the pi and sigma bands of electrons have been found to have two different coherence lengths (51 nm and 13 nm, respectively). The corresponding London penetration depths are 33.6 nm and 47.8 nm. This implies that the Ginzburg-Landau parameters are 0.66±0.02 and 3.68, respectively. The first is less than 1/ and the second is greater, therefore the first seems to indicate marginal type I superconductivity and the second type II superconductivity.\n\nIt has been predicted that when two different bands of electrons yield two quasiparticles, one of which has a coherence length that would indicate type I superconductivity and one of which would indicate type II, then in certain cases, vortices attract at long distances and repel at short distances. In particular, the potential energy between vortices is minimized at a critical distance. As a consequence there is a conjectured new phase called the semi-Meissner state, in which vortices are separated by the critical distance. When the applied flux is too small for the entire superconductor to be filled with a lattice of vortices separated by the critical distance, then there are large regions of type I superconductivity, a Meissner state, separating these domains.\n\nExperimental confirmation for this conjecture has arrived recently in MgB experiments at 4.2 Kelvin. The authors found that there are indeed regimes with a much greater density of vortices. Whereas the typical variation in the spacing between Abrikosov vortices in a type II superconductor is of order 1%, they found a variation of order 50%, in line with the idea that vortices assemble into domains where they may be separated by the critical distance. The term type-1.5 superconductivity was coined for this state.\n\nMagnesium diboride was synthesized and its structure confirmed in 1953. The simplest synthesis involves high temperature reaction between boron and magnesium powders. Formation begins at 650 °C; however, since magnesium metal melts at 652 °C, the reaction may involve diffusion of magnesium vapor across boron grain boundaries. At conventional reaction temperatures, sintering is minimal, although grain recrystallization is sufficient for Josephson quantum tunnelling between grains.\n\nSuperconducting magnesium diboride wire can be produced through the powder-in-tube (PIT) \"ex situ\" and \"in situ\" processes. In the \"in situ\" variant, a mixture of boron and magnesium is reduced in diameter by conventional wire drawing. The wire is then heated to the reaction temperature to form MgB. In the \"ex situ\" variant, the tube is filled with MgB powder, reduced in diameter, and sintered at 800 to 1000 °C. In both cases, later hot isostatic pressing at approximately 950 °C further improves the properties.\n\nAn alternative technique, disclosed in 2003, employs reactive liquid infiltration of magnesium inside a granular preform of boron powders and was called Mg-RLI technique. The method allowed the manufacture of both high density (more than 90% of the theoretical density for MgB) bulk materials and special hollow fibers. This method is equivalent to similar melt growth based methods such as the Infiltration and Growth Processing method used to fabricate bulk YBCO superconductors where the non-superconducting Y2Ba1Cu1O5 is used as granular preform inside which YBCO based liquid phases are infiltrated to make superconductive YBCO bulk. This method has been copied and adapted for MgB2 superconductor and rebranded as Reactive Mg Liquid Infiltration. The process of Reactive Mg Liquid Infiltration in a boron preform to obtain MgB has been a subject of patent applications by Edison S.p.A. (Italy).\n\nHybrid physical-chemical vapor deposition (HPCVD) has been the most effective technique for depositing magnesium diboride (MgB) thin films. The surfaces of MgB films deposited by other technologies are usually rough and non-stoichiometric. In contrast, the HPCVD system can grow high-quality \"in situ\" pure MgB films with smooth surfaces, which are required to make reproducible uniform Josephson junctions, the fundamental element of superconducting circuits.\n\nProperties depend greatly on composition and fabrication process. Many properties are anisotropic due to the layered structure. 'Dirty' samples, e.g., with oxides at the crystal boundaries, are different from 'clean' samples.\n\n\nVarious means of doping MgB with carbon (e.g. using 10% malic acid) can improve the upper critical field and the maximum current density\n(also with polyvinyl acetate).\n\n5% doping with carbon can raise H from 16 to 36 T while lowering \"T\" only from 39 K to 34 K. The maximum critical current (\"J\") is reduced, but doping with TiB can reduce the decrease. (Doping MgB with Ti is patented.)\n\nThe maximum critical current (\"J\") in magnetic field is enhanced greatly (approx double at 4.2 K) by doping with ZrB.\n\nEven small amounts of doping lead both bands into the type II regime and so no semi-Meissner state may be expected.\n\nMgB is a multi-band superconductor, that is each Fermi surface has different superconducting energy gap. For MgB, sigma bond of boron is strong, and it induces large s-wave superconducting gap, and pi bond is weak and induces small s-wave gap. \nThe quasiparticle states of the vortices of large gap are highly confined to the vortex core.\nOn the other hand, the quasiparticle states of small gap are loosely bound to the vortex core. Thus they can be delocalized and overlap easily between adjacent vortices. Such delocalization can strongly contribute to the thermal conductivity, which shows abrupt increase above H.\n\nSuperconducting properties and low cost make magnesium diboride attractive for a variety of applications. For those applications, MgB powder is compressed with silver metal (or 316 stainless steel) into wire and sometimes tape via the PIT process.\nIn 2006 a 0.5 tesla open MRI superconducting magnet system was built using 18 km of MgB wires. This MRI used a closed-loop cryocooler, without requiring externally supplied cryogenic liquids for cooling.\n\n\"...the next generation MRI instruments must be made of MgB coils instead of NbTi coils, operating in the 20–25 K range without liquid helium for cooling. ...\nBesides the magnet applications MgB conductors have potential uses in superconducting transformers, rotors and transmission cables at temperatures of around 25 K, at fields of 1 T.\"\n\nThe IGNITOR tokamak is designed to use MgB for its poloidal coils.\n\nThin coatings can be used in superconducting radio frequency cavities to minimize energy loss and reduce the inefficiency of liquid helium cooled niobium cavities.\n\nBecause of the low cost of its constituent elements, MgB has promise for use in superconducting low to medium field magnets, electric motors and generators, fault current limiters and current leads.\n\nUnlike elemental boron whose combustion is incomplete through the glassy oxide layered impeding oxygen diffusion, magnesium diboride burns completely when ignited in oxygen or in mixtures with oxidizers. Thus magnesium boride has been proposed as fuel in ram jets. In addition the use of MgB in blast-enhanced explosives and propellants has been proposed for the same reasons. Most recently it could be shown that decoy flares containing magnesium diboride/Teflon/Viton display 30–60% increased spectral efficiency, E (J gsr), compared to classical Magnesium/Teflon/Viton(MTV) payloads.\n\n"}
{"id": "4350770", "url": "https://en.wikipedia.org/wiki?curid=4350770", "title": "Ministry of Petroleum", "text": "Ministry of Petroleum\n\nA Ministry of Petroleum or Ministry of Oil is a kind of government ministry often found in countries that are producers and exporters of petroleum.\n\nExamples include:\n"}
{"id": "19836", "url": "https://en.wikipedia.org/wiki?curid=19836", "title": "Molecular mass", "text": "Molecular mass\n\nRelative molecular mass or molecular weight is the mass of a molecule. It is calculated as the sum of the relative atomic masses of each constituent element multiplied by the number of atoms of that element in the molecular formula. The molecular mass of small to medium size molecules, measured by mass spectrometry, determines stoichiometry. For large molecules such as proteins, methods based on viscosity and light-scattering can be used to determine molecular mass when crystallographic data are not available.\n\nBoth atomic and molecular masses are usually obtained relative to the mass of the isotope C (carbon 12), which by definition is equal to 12. For example, the molecular mass of methane, whose molecular formula is CH, is calculated as follows:\nA more proper term would be \"relative molecular mass\". However the adjective 'relative' is omitted as it is universally assumed that atomic and molecular masses are relative to the mass of C. Relative atomic and molecular mass values are dimensionless but are given the \"unit\" Dalton (formerly atomic mass unit) to indicate that the number is equal to the mass of one molecule divided by of the mass of one atom of C. The mass of 1 mol of substance is designated as molar mass. By definition, it has the unit gram.\n\nIn the example above the atomic mass of carbon is given as 12.011, not 12. This is because naturally occurring carbon is a mixture of the isotopes C, C and C which have relative atomic masses of 12, 13 and 14 respectively. Moreover, the proportion of the isotopes varies between samples, so 12.011 is an average value. By contrast, there is less variation in naturally occurring hydrogen so the average atomic mass is known more precisely. The precision of the molecular mass is determined by precision of the least precise atomic mass value, in this case that of carbon. In high-resolution mass spectrometry the isotopomers CH and CH are observed as distinct molecules, with molecular masses of 16 and 17, respectively. The intensity of the mass-spectrometry peaks is proportional to the isotopic abundances in the molecular species. C H H can also be observed with molecular mass of 17.\n\nIn mass spectrometry, the molecular mass of a small molecule is usually reported as the monoisotopic mass, that is, the mass of the molecule containing only the most common isotope of each element. Note that this also differs subtly from the molecular mass in that the choice of isotopes is defined and thus is a single specific molecular mass of the many possible. The masses used to compute the monoisotopic molecular mass are found on a table of isotopic masses and are not found on a typical periodic table. The average molecular mass is often used for larger molecules since molecules with many atoms are unlikely to be composed exclusively of the most abundant isotope of each element. A theoretical average molecular mass can be calculated using the standard atomic weights found on a typical periodic table, since there is likely to be a statistical distribution of atoms representing the isotopes throughout the molecule. This however may differ from the true average molecular mass of the sample due to natural (or artificial) variations in the isotopic distributions.\n\nTo a first approximation, the basis for determination of molecular mass according to Mark–Houwink relations is the fact that the intrinsic viscosity of solutions (or suspensions) of macromolecules depends on volumetric proportion of the dispersed particles in a particular solvent. Specifically, the hydrodynamic size as related to molecular mass depends on a conversion factor, describing the shape of a particular molecule. This allows the apparent molecular mass to be described from a range of techniques sensitive to hydrodynamic effects, including DLS, SEC (also known as GPC when the eluent is an organic solvent), viscometry, and diffusion ordered nuclear magnetic resonance spectroscopy (DOSY). The apparent hydrodynamic size can then be used to approximate molecular mass using a series of macromolecule-specific standards. As this requires calibration, it's frequently described as a \"relative\" molecular mass determination method.\n\nIt is also possible to determine absolute molecular mass directly from light scattering, traditionally using the Zimm method. This can be accomplished either via classical static light scattering or via multi-angle light scattering detectors. Molecular masses determined by this method do not require calibration, hence the term \"absolute\". The only external measurement required is refractive index increment, which describes the change in refractive index with concentration.\n\n\n"}
{"id": "50408270", "url": "https://en.wikipedia.org/wiki?curid=50408270", "title": "Nickel compounds", "text": "Nickel compounds\n\nCompounds of nickel are chemical compounds containing the element nickel which is a member of the group 10 of the periodic table. Most compounds in the group have an oxidation state of +2. Nickel is classified as a transition metal with nickel(II) having much chemical behaviour in common with iron(II) and cobalt(II). Many salts of nickel(II) are isomorphous with salts of magnesium due to the ionic radii of the cations being almost the same. Nickel forms many coordination complexes. Nickel tetracarbonyl was the first pure metal carbonyl produced, and is unusual in its volatility. Metalloproteins containing nickel are found in biological systems.\n\nNickel forms simple binary compounds with non metals including halogens, chalcogenides, and pnictides. Nickel ions can act as a cation in salts with many acids, including common oxoacids Salts of the hexaaqua ion (Ni•6HO) are especially well known. Many double salts containing nickel with another cation are known. There are organic acid salts. Nickel can be part of a negatively charged ion (anion) making what is called a nickellate. Numerous quaternary compounds (with four elements) of nickel have been studied for super conductivity properties, as nickel is adjacent to copper and iron in the periodic table can can form compounds with the same structure as the high-temperature superconductors that are known.\n\nMost of the common salts of nickel are green due to the presence of hexaaquanickel(II) ion, Ni(HO).\n\nNickel atoms can connect to surrounding atoms or ligands in a variety of ways. Six coordinated nickel is the most common and is octahedral, but this can be distorted if ligands are not equivalent. For four coordinate nickel arrangements can be square planar, or tetrahedral. Five coordinated nickel is rarer.\n\nSome nickel compounds are ferromagnetic at sufficiently low temperatures. In order to show magnetic properties the nickel atoms have to be close enough together in the solid structure.\n\nA binary compound of nickel contains one other element. Substances that contain only nickel atoms are not actually compounds.\n\nIn a noble gas matrix, nickel can form dimers, a molecule with two nickel atoms: Ni. Ni has a bonding energy of 2.07±0.01 eV. For Ni the bond energy is around 3.3 eV. Nickel dimers and other clusters can also be formed in a gas and plasma phase by shooting a powerful laser at a nickel rod in cold helium gas.\n\nNickel oxides include Nickel(II) oxide and Nickel(III) oxide.\n\nNickel hydroxides are used in nickel–cadmium and Nickel–metal hydride batteries.\nNickel(II) hydroxide Ni(OH), the main hydroxide of nickel is coloured apple green. It is known as the mineral theophrastite.\nβ-NiO(OH) is a black powder with nickel in the +3 oxidation state. It can be made by oxidising nickel nitrate in a cold alkaline solution with bromine. A mixed oxidation state hydroxide NiO(OH) is made if oxidation happens in a hot alkaline solution. A Ni hydroxide: nickel peroxide hydrate NiO•HO, can be made by oxidising with alkaline peroxide. It is black, and unstable and oxidises water.\n\nNickel(II) fluoride NiF is yellow, crystallising in the rutile structure and can form a trihydrate, NiF·3HO. A tetrahydrate also exists.\n\nNickel chloride NiCl is yellow, crystallising in the cadmium chloride structure. It can form a hexahydrate, NiCl·6HO, a tetrahydrate NiCl·4HO over 29 °C and a dihydrate, NiCl·2HO over 64 °C.\n\nnickel bromide NiBr is yellow, also crystallising in the cadmium chloride structure. It can form a hexahydrate, NiBr·6HO. Crystallisation above 29° forms a trihydrate NiBr·3HO, and a dihydrate NiBr·2HO. Enneahydrate, NiBr·9HO can crstallise from water below 2 °C. Nickelous hexammine bromide Ni(NH)Bris violet or blue. It is soluble in boiling aqueous ammonia, but is insoluble in cold. Diammine, monoammine, and dihydrazine nickel bromides also exist.\n\nWith four bromide atoms nickel(II) forms a series of salts called tetrabromonickelates.\n\nNickel iodide NiI is black, also crystallising in the cadmium chloride structure. It can form a green hexahydrate, NiI·6HO. Nickel iodide has a brown diammine NiI•2NH and a bluish-violet hexammine NiI•6NH.\n\nNickel(III) fluoride NiF\n\nNickel(IV) fluoride NiF\n\nBy reacting nickel with chalcogens, nickel sulfide, nickel selenide, and nickel telluride are formed.\nThere are numerous sulfides: NiS, NiS, NiS (heazlewoodite), NiS (polydymite), NiS (godlevskite), NiS (millerite) and two other NiS forms, NiS (vaesite) in pyrite structure. Black nickel tetrasulfide NiS is formed from ammonium polysulfide and nickel in water solution. Mixed and double sulfides of nickel also exist. Nickel with selenium forms several compounds NiSe 0≤x≤0.15, NiSe, NiSe also known as a mineral penroseite.\n\nNickel forms two different polonides by heating nickel and polonium together: NiPo and NiPo.\n\nNon-stoichiometric compounds of nickel with phosphorus, arsenic and antimony exist, and some are found in nature. One interstitial nitride has formula NiN (hexagonal P6322, Z = 2, a = 4.6224 Å and c = 4.3059 Å).\nIn a solid nitrogen matrix, nickel atoms combine with nitrogen molecules to yield Ni(N).\n\nNickel phosphide NiP has density 7.33 and melts at 1100 °C.\n\nThe mineral Nickelskutterudite has formula NiAs, nickeline has formula NiAs and breithauptite has formula NiSb. NiAs melts at 967° and has density 7.77. NiSb melts at 1174°. It has the highest density of a nickel compound at 8.74 g/cm.\n\nNiAsS gersdorffite, and NiSbS ullmannite, NiAsSe Jolliffeite are pnictide/chalcogenide compounds that occur as minerals.\n\nNickel also forms carbides and borides. Nickel boride can take the forms NiB (a green/black solid), NiB, NiB, \"o\"-NiB and \"m\"-NiB.\nNickel hydride NiH is only stable under high pressures of hydrogen.\n\nHot nickel vapour reacting with other atoms in the gas phase can produce molecules consisting of two atoms.\nNickel monofluoride can be observed by its emission spectrum in the gas phase.\n\nNickel subchloride NiCl is formed in gaseous form when nickel chloride is vapourised, and is the most common in the gas phase above 1450 K. It is formed when nickel is exposed to hot, low pressure chlorine.\n\nNickel monobromide, NiBr can exist in the gas phase when an electric discharge goes through NiBr gas.\n\nNickelmonoiodide can exist in the gas phase.\n\nCompounds of nickel with other metals can be called alloys. The substances with fixed composition include nickel aluminide (NiAl) melting at 1638° with hexagonal structure.\nNiY, NiY, NiY, NiY, NiGd,\n\nBaNiGe changes structure from orthorhombic to tetragonal around 480 °C. THis is a ternary intermetallic compound. Others include BaNiSn and the superconductors SrNiGe, SrNiP, SrNiAs, BaNiP, BaNiAs.\n\nNickel(II) sulfate can crystallise with six water molecules yielding Retgersite or with seven making Morenosite which is isomorphic to Epsom salts. These contain the hexaquanickel(II) ion.\nThere is also an anhydrous form, a dihydrate and a tetrahydrate, the last two crystallised from sulfuric acid. The hexahydrate has two forms, a blue tetragonal form, and a green monoclinic form, with a transition temperature around 53 °C. The heptahydrate crystallises from water below 31.5 above this blue hexhydrate forms, and above 53.3 the green form. Heating nickel sulfate dehydrates it, and then 700° it loses sulfur trioxide, sulfur dioxide and oxygen.\n\nNickel sulfite can be formed by bubbling sulfur dioxide through nickel carbonate suspended in water. A solution is formed that slowly loses sulfur dioxide, and which crystallises nickel sulfite hexahydrate. Crystals are frequently in the shape of stars, caused by the two opposite triangular enantiomorphs growing base to base. nickel sulfite hexahydrate is highly piezoelectric. Optically it is uniaxial negative with refractive indexes ω=1.552 ε=1.509. When heated it dehydrates and then ends up making nickel oxide and nickel sulfate.\n\nNickel thiosulfate NiSO has the same structure as the magnesium salt. It has alternating layers of octahedral shaped nickel hexahydrate, and tetrahedral shaped SO perpendicular to the β direction. When heated to 90 °C it decomposes to form NiS. NiSO can be made from BaSO and NiSO. Nickel sulfamate can be used for nickel or mixed nickel-tungsten plating. It can be formed by the action of sulfamic acid on nickel carbonate.\n\nNickel selenite NiSeO has many different hydrates, anhydrous NiSeO•HO, NiSeO•HO, NiSeO•2HO (which is also a mineral called ahlfeldite), and NiSeO•4HO.\n\nNickel nitrate commonly crystallises with six water molecules, but can also be anhydrous, or with two, four or nine waters.\ntriphenylphosphine oxide nickel nitrate [(CH)PO]Ni(NO) is non ionic, with nitrato as a ligand. It can be made from nickel perchlorate. It is yellow and melts at 266 °C.\n\nNickel carbonate NiCO•6HO, hellyerite, crystallising with six water molecules, precipitates when an alkali bicarbonate is added to a Ni aqueous solution. Basic nickel carbonate, zaratite, with the formula NiCO(OH)(HO), is produced when alkali carbonates are added to a nickel solution. Nickel phosphate, Ni(PO)•7HO is also insoluble. A number of other phosphates have been made, including nanoporous substances resembling zeolites named with \"Versailles Santa Barbara\" or VSB. The nanoporous nickel phosphates can accommodate sufficiently small molecules and selectively catalyse reactions on them. A nickel arsenate, Ni(AsO)·8HO occurs as the mineral annabergite.\n\nNickel perchlorate, Ni(ClO)•6HO, nickel chlorate, Ni(ClO)•6HOnickel chromate (NiCrO), nickel chromite (NiCrO), nickel(II) titanate, nickel bromate Ni(BrO)•6HO, nickel iodate (Ni(IO)•4HO), nickel stannate (NiSnO•2HO) are some other oxy-salts.\n\nThe uranates include NiUO, NiUO α and β forms (orthorhombic a=6.415 Å; b=6.435 Å; c=6.835 Å), and NiUO\n\nNickel tetrafluoroborate, Ni(BF)•6HO is very soluble in water, alcohol and acetonitrile. It is prepared by dissolving nickel carbonate in tetrafluoroboric acid. Nickel tetrafluoroberyllate NiBeF•\"x\"HO, can be hydrated with six or seven water molecules.\nBoth nickel hexafluorostannate <chem>NiSnF6.6H2O</chem> and nickel fluorosilicate <chem>NiSnF6.6H2O</chem> crystallise in the trigonal system. Nickel hexafluorogermanate NiGeF has a rosy-tan colour and a hexagonal crystal with a = 5.241 Å unit cell volume is 92.9 Å. It is formed in the reaction with GeF and KNiF. Nickel fuorotitanate <chem>NiTiF6.6H2O</chem> crystallises in hexagonal green crystals. It can be made by dissolving nickel carbonate, and titanium dioxide in hydrofluoric acid. The crystal dimensions are a = 9.54, c = 9.91 density = 2.09 (measure 2.03).\n\nNi(AsF), Ni(SbF), Ni(BiF) are made by reacting the hexafluoro acid with NiF in hydrofluoric acid. They all have hexagonal crystal structure, resembling the similar salts of the other first row transition metals. For Ni(AsF) a = 4.98, c = 26.59, and V = 571, formula weight Z=3. Ni(SbF) is yellow with a = 5.16Å, c = 27.90Å Z = 3. The structure resembles LiSbF, but with every second metal along the c axis missing.\n\nOthers include the green fluorohafnate NiHfF•6HO, and NiHfF•12HO, NiZrF•6HO\n\nNickel tetrachloroiodate Ni(ICl) can be made by reacting iodine with nickel chloride. It consists of green needles.\n\nNickel cyanide tetrahydrate Ni(CN)•4HO is insoluble in water, but dissolves in aqueous ammonia. It forms double salts with interesting structures.\n\nNickel azide Ni(N) is a sensitive explosive. It can be made by treating nickel carbonate with hydrazoic acid. Acetone causes the precipitation of the hydrous solid salt, which is green. At 490K it slowly decomposes to nitrogen and nickel metal powder, losing a half of the nitrogen in four hours. Nickel azide is complexed by one azo group when dissolved in water, but in other solvents, the nickel atom can have up to four azo groups attached. Nickel azide forms a dihydrate: Ni(N)•2HO and a basic salt called nickel hydroxy azide Ni(OH)N.\n\nNickel amide, Ni(NH) is a deep red compound that contains Ni clusters surrounded by 12 NH groups. Nickel amide also forms a series of double salts. Other homoleptic nickel amides derived by substituting the hydrogen atoms are Ni[N(CH)] (diphenyl) and boryl amides Ni[NBMesMes] and Ni[NBMesCH].\n\nNickel acetate has the formula (CHCOO)Ni·4HO. It has monodentate acetate and hydrogen bonding. A dihdrate also exists. Nickel acetate is used to seal anodised aluminium.\n\nNickel formate <chem>Ni(HCOO)2.2H2O</chem> decomposes when heated to yield carbon dioxide, carbon monoxide, hydrogen, water and finely divided porous nickel. All the nickel atoms are six coordinated, but half have four water molecules and two formate oxygens close to the atom, and the other half are coordinated by six oxygens of formate groups.\n\n\"Aspergillus niger\" is able to dispose of otherwise toxic levels of nickel in its environment by forming nickel oxalate dihydrate crystals. nickel oxalate can also be formed in to various namorods and nanofibres by use of surfacants. When heated nickel oxalate dihydrate dehydrates at 258° and decomposes to NiO over 316 °C. Double oxalate salts where oxalate is a ligand on the nickel atom may be called oxalatonickelates.\n\nOther organic acid salts of nickel include nickel oleate, nickel propionate, nickel butyrate, nickel caprylate, nickel lactate, nickel benzoate, nickel bis(acetyl acetonate), nickel salicylate, nickel alkyl phenyl salicylate. Nickel stearate forms a green solution, however when precipitated with alcohol a gel is produced, that also contains a mixture of basic salts, and free stearic acid.\n\nNickel malonate, and nickel hydrogen malonate both crystallise with two molecules of water. They decomposes when heated to yield gaseous water, carbon dioxide, carbon monoxide, ethanol, acetic acid, methyl formate and ethyl formate. Nickel acetate exists as an intermediate and the final result is that solid nickel, nickel oxide, NiC and carbon remain. With malonate nickel can form a bis-malonato-nickelate anion, which can form double salts. Nickel maleate can be made from maleic acid and nickel carbonate in boiling water. A dihydrate crystallises from the water solution. Nickel fumarate prepared from fumaric acid and nickel carbonate is pale green as a tetrahydrate, and mustard coloured as an anhydride. It decomposes when heated to 300° to 340° in vacuum. Decomposition mostly produces nickel carbide, carbon dioxide, carbon monoxide and methane. But also produced were butanes, benzene, toluene, and organic acid.\n\nNickel succinate can form metal organic framework compounds.\n\nNickel citrate complexes are found in leaves of some nickel accumulating plant species in New Caledonia such as \"Pycnandra acuminata\". Citrate complexes include NiHcit, NiHcit, Nicit, Nicit, and NiHcit. (ordered from low to high pH). Also there is NiHcit. Nickel citrate is important in nickel plating. When precipitation of nickel citrate is attempted a gel forms. This apparently consists of tangled fibres of [(CHO)Ni], which can be reduced to nickel metal fibres less than a micron thick, and meters long. Double nickel citrates exist, including tetraanion citrate when pH is over 9.5. An amorphous nickel iron citrate NiFeO(CHO)·6HO produces carbon monoxide, carbon dioxide and acetone when heated over 200 °C leaving Trevorite, NiFeO a nickel ferrite. A green crystalline nickel citrate with formula Ni(CHO)·10HO melts at 529K and decomposition starts at 333K.\n\nNickel glutarate in the form called Mil-77, [Ni{(CHO)(HO)}]⋅40HO is pale green. It crystallises in a porous structure containing twenty member rings. The 40 water molecules \"occluded\" in the porous channels come out when it is heated to 150 °C retaining the crystal framework. At 240 °C the crystal form changes and over 255° the remaining water is lost. Between 330° and 360° the organic components burn and it is destroyed.\n\nCyclopropane carboxylic acid forms two basic salts with nickel, a hydrate Ni(OH)(HO)(CHO)8·2HO with density 1.554 Mg/m and an anhydrous form Ni(OH)(CHO) with density 2.172 mg/m.\n\nNickel trifluoroacetate tetrahydrate exists, as well as two emerald green acid trifluoroacetates, a bridged trinuclear form [Ni(CFCOO)(CFCOOH)](CFCOOH) and a hydrated acid form [Ni(CFCOO)(CFCOOH)(HO)](CFCOOH) both with triclinic crystal form. The first has density 2.205 and the second 2.124. They are made by dissolving the nickel trifluoroacetate tetrahydrate in trifluoroacetic acid either anhydrous or 1% hydrated.\n\nNickel naphthenate is used as a fuel additive to suppress smoke, as a rubber catalyst and as an oil additive.\n\nWhen Nickel benzoate is heated in a vacuum, carbon dioxide, carbon monoxide, benzene, benzoic acid, phenol, biphenyl, nickel, nickel oxide, and nickel carbide are formed. It can crystallise as anhydrous, a trihydrate or a tetrahydrate.\n\nNickel terephthalate can be made by a double decomposition of sodium terephthalate and nickel nitrate. Nickel terephthalate precipitates. Its solubility is 0.38 g/100g water at 25 °C. In ammonium hydroxide a violet solution forms. Boiling acetic acid converts the nickel to nickel acetate. The terephthalate converts to a basic salt when boiled in water. Understating this compound is important when reducing coloured contaminants in polymers made from terephthalate.\n\nNickel is one of the metals that can form Tutton's salts. The singly charged ion can be any of the full range of potassium, rubidium, cesium, ammonium (NH4), or thallium. As a mineral the ammonium nickel salt, (NH)Ni(SO)·6 HO, can be called nickelboussingaultite. With sodium, the double sulfate is nickelblödite NaNi(SO)·4 HO from the blödite family. Nickel can be substituted by other divalent metals of similar sized to make mixtures that crystallise in the same form.\n\nNickel forms double salts with Tutton's salt structure with tetrafluoroberyllate with the range of cations of ammonia, potassium, rubidium, cesium, and thallium.\n\nAnhydrous salts of the formula MNi(SO), which can be termed metal nickel triusulfates, belong to the family of langbeinites. The known salts include (NH)Ni(SO), KNi(SO) and RbNi(SO), and those of Tl and Cs are predicted to exist.\n\nSome minerals are double salts, for example Nickelzippeite Ni(UO)(SO)(OH) · 16HO which is isomorphic to cobaltzippeite, magnesiozippeite and zinczippeite, part of the zippeite group.\n\nDouble hydrides of nickel exist, such as MgNiH.\n\nDouble fluorides include the above-mentioned fluoroanion salts, and those fluoronickelates such as NiF and NiF. Other odd ones include an apple green coloured KNiF·HO and NaNiF·HO, aluminium nickel pentafluoride AlNiF·7HO, ceric nickelous decafluoride CeNiF·7HO, niobium nickel fluoride NiHNbF·19HO, vanadium nickel pentafluoride VNiF·7HO, vanadyl nickel tetrafluoride VONiF·7HO, chromic nickelous pentafluoride CrNiF·7HO, molybdenum nickel dioxytetrafluoride NiMoOF·6HO, tungsten nickel dioxytetrafluoride NiWOF·6HO and NiWOF·10HO, manganic nickel pentafluoride MnNiF·7HO, nickelous ferric fluoride FeNiF·7HO.\n\nNickel trichloride double salts exist which are polymers. Nickel is in octahedral coordination, with double halogen bridges. Examples of this include RbNiCl, pinkish tan coloured HNN(CH)NiCl. Other double trichlorides include potassium nickel trichloride KNiCl·5HO, yellow cesium nickel trichloride CsNiCl, lithium nickel trichloride LiNiCl·3HO, hyrdrazinium nickel tetrachloride, and nickel ammonium chloride hexahydrate NHNiCl·6HO.\n\nThe tetrachloronickelates contain a tetrahedral NiCl and are dark blue. Some salts of organic bases are ionic liquids at standard conditions. tetramethylammonium nickel trichloride is pink and very insoluble.\nOther tetrachlorides include rubidium nickel tetrachloride, lithium nickel tetrachloride LiNiCl·4HO stable from 23 to 60°, stannous nickel tetrachloride <chem>SnCl2.NiCl2.6H2O</chem>, stannic nickel hexachloride <chem>SnCl4.NiCl2.6H2O</chem> is tetragonal.\n\nLithium nickel hexachloride LiNiCl·10HO is stable from 0 to 23°.\n\nCopper nickel dioxychloride 2CuO·NiCl·6HO, and copper nickel trioxychloride 3CuO·NiCl·4HO.\n\nCadmium dinickel hexachloride, <chem>CdCl2.2NiCl2.12H2O</chem> crystallises in hexagonal system, dicadmium dinickel hexachloride, <chem>2CdCl2.NiCl2.12H2O</chem> has rhombic crystals, and is pleochroic varying from light to dark green.\n\nThallic nickel octochloride <chem>2TlCl3.NiCl2.8H2O</chem> is bright green.\n\nDouble bromides include the tetrabromonickelates, and also caesium nickel tribromide, CsNiBr\ncopper nickel trioxybromide, 3Cu0·NiBr·4HO\nmercuric nickel bromide, HgNiBr6, HgNiBr.\nAqueous nickel bromide reacting with mercuric oxide yields mercuric nickel oxybromide, <chem>6NiO.NiBr2.HgBr2.20H2O</chem>\ndidymium nickel bromide, <chem>2(Pr,Nd)Br3.3NiBr2.18H2O</chem> is reddish brown (mixture of praseodymium and neodymium)\nLanthanum nickel bromide, <chem>2LaBr3.3NiBr2.18H2O</chem>\nnickel stannic bromide (or nickel bromostannate) NiSnBr·8HO is apple green.\n\nThe tetraiodonickelates are blood red coloured salts of the NiI ion with large cations. Double iodides known include mercuric nickel hexaiodide 2HgI•NiI•6HO, mercuric nickel tetraiodide HgI•NiI•6HO, and lead nickel hexaiodide I•2NiI•3HO.\n\nThe diperiodatonickelates of nickel IV are strong oxidisers, and akali monoperiodatonickelates also are known.\n\nNickel forms double nitrates with the lighter rare earth elements. The solid crystals have the formula <chem>Ni3Me2(NO3)12.24H2O</chem>. The metals include La Ce Pr Nd Sm Gd and the non rare earth Bi. Nickel can also be replaced by similar divalent ions, Mg, Mn Co Zn. For the nickel salts melting temperatures range from 110.5° for La, 108.5° for Ce, 108° for Pr, 105.6° for Nd, 92.2° for Sm and down to 72.5° for Gd The Bi salt melting at 69°. Crystal structure is hexagonal with Z=3. <chem>Ni3La2(NO3)12.24H2O</chem> becomes ferromagnetic below 0.393 K. These double nickel nitrates have been used to separate the rare earth elements by fractional crystallization.\n\nNickel thorium nitrate has formula NiTh(NO)•8HO. Nickel atoms can be substituted by other ions with radius 0.69 to 0.83 Å. The nitrates are coordinated on the thorium atom and the water to the nickel. Enthalp of solution of the octahydrate is 7 kJ/mol. Enthalpy of formation is -4360 kJ/mol. At 109° the octahydrate becomes <chem>NiTh(NO3)6.6H2O</chem>, and at 190° <chem>NiTh(NO3)6.3H2O</chem> and anhydrous at 215°.\nThe hexahydrate has \"Pa\" cubic structure.\n\nVarious double amides containing nickel clusters have been made using liquid ammonia as a solvent. Substances made include red LiNi(NH)·NH (Pna21; Z = 4; a = 16.344(3) Å; b = 12.310(2) Å; c = 8.113(2) Å v=1631 D=1.942), and CsNi(NH)•NH (P21/c; Z = 4; a =9.553(3) Å; b = 8.734(3) Å; c = 14.243(3) Å; β = 129.96(3)° V=910 D=2.960). These are called amidonickel compounds. Yet others include LiNi(NH)·NH, NaNi(NH), orange red NaNi(NH)•2NH, NaNi(NH)•NH, KNi(NH)•0.23KNH, and RbNi(NH)•0.23RbNH.\n\nNickel dihydrogen phosphide (Ni(PH)) can form orange, green or black double salts KNi(PH)) that crystallise from liquid ammonia. They are unstable above -78 °C, giving off ammonia, phosphine and hydrogen.\n\nNickel forms a series of double nickel oxides with other elements, which may be termed \"nickelates\". These double nickel oxides are not listed on this page. There are also many well defined double compounds with sulfur, selenium and tellurium.\n\nNickel can enter into metal oxygen clusters with other high oxidation state elements to form polyoxometalates. These may stabilise higher oxidation states of nickel, or show catalytic properties.\n\nNonamolybdonickelate(IV), [NiMoO] can oxidise aromatic hydrocarbons to alcohols.\n\nThere is a dark brown heptamolybdonickelate(IV) potassium salt, KHNiMoO·6HO.\n\n13-Vanadonickelate(IV) compounds exist such as KNiVO•16HO with black octahedral crystals. It can be made from isopolyvanadate, with nickel(II) oxidised by peroxydisulfate at a pH around 4.\nNickel(IV) heteropolyniobates also exists such as the dark maroon NaNiNbO•21HO. An alternate orange red hydrate perhaps with 44 water molecules also exists. With nickel-II (tetramethylammonium)[HNiNbO•17HO forms a green salt that is very soluble in water, but hardly soluble in ethanol.\n\nHKNaNb NiO is a nickel-cation-bridged polyoxoniobate which crystallises in the monoclinic system with cell dimensions a=15.140 b=24.824 c=25.190 Å and β=103.469 and two formulas per unit cell.\n\nNaLi[Ni(PWO)]•74HO forms a sandwich structure, and NaLi[Ni(OH)(HO)PWO]•48HO is a Wells-Dawson polyoxometalate.\n\nNickel hydrofluoride, HNiF·6HO is made by using excess hydrofluoric acid solution on nickel carbonate. It is deep green.\n\nNickel oxyfluoride NiFO(OH) is green.\n\nNickelous enneaoxydiiodide 9NiO•Nil•15HO forms when solutions of nickel iodide are exposed to air and evaporated.\n\nSimple complexes of nickel include hexaquonickel(II), yellow tetracyanonickelate [Ni(CN)], red pentacyanonickelate [Ni(CN)] only found in solution, [Ni(SCN)] and [Ni(SCN)]. Halo- complexes include [NiCl], [NiF], [NiF], [NiCl(HO)] [Ni(NH)(HO)], [Ni(NH)], [Ni(en))]. Some complexes have fivefold coordination. N[CH2CH2NMe] (tris(N,N-dimethyl-2-aminoethyl)amine); P(o-CHSMe); P(CHCHCHAsMe).\n\nOther ligands for octahedral coordination include PPh, PPhMe and thiourea.\n\nNickel tetrahedral complexes are often bright blue and 20 times or more intensely coloured than the octahedral complexes. The ligands can include selections of neutral amines, arsines, arsine oxides, phosphines or phosphine oxides and halogens.\n\nSeveral nickel atoms can cluster together in a compound with other elements to produce nickel cluster complexes. One example where nickel atoms form a square pyramid is a nickel hydride cluster complexed by triphenyl phosphine ligands and bonding a hydrogen atom on each edge. Another example has a square planar NiH shape in its core.\n\nNickel bis(dimethylglyoximate), an insoluble red solid is important for gravimetric analysis.\n\nCofactor F430 contains nickel in a tetrapyrrole derivative, and is used in the production of methane. Some hydrogenase enzymes contain a nickel-iron cluster as an active site in which the nickel atom is held in place by cysteine or selenocysteine. Plant ureases contain a bis-μ-hydroxo dimeric nickel cluster. CO-methylating acetyl-CoA synthase contains two active nickel atoms, one is held in a square planar coordination by two cysteine and two amide groups, and the other nickel is held by three sulfur atoms. It is used to catalyse the reduction of carbon monoxide to acetyl-CoA.\n\nNickel superoxide dismutase (or Ni-SOD) from \"Streptomyces\" contains six nickel atoms. The nickel holding is done by a \"nickel binding hook\" which as the amino acid pattern HN-His-Cys-X-X-Pro-Cys-Gly-X-Tyr-rest of protein, where the bold bits are ligands for the nickel atom.\n\nNickel transporter proteins exist to move nickel atoms in the cell. in \"E. coli\" these are termed \"Nik\"A, \"Nik\"B, \"Nik\"C, \"Nik\"D, \"Nik\"E. In order to come through a cell membrane a nickel permease protein is used. In \"Alcaligenes eutrophus\" the gene for this is \"hox\"N.\n\nWell known nickel organometalic (or organonickel) compounds include Nickelocene, bis(cyclooctadiene)nickel(0) and nickel tetracarbonyl.\nNickel\n\nNickel tetracarbonyl was the first discovered organonickel compound. It was discovered that carbon monoxide corroded a nickel reaction chamber valve. And then that the gas coloured a bunsen burner flame green, and then that a nickel mirror condensed from heating the gas. The Mond process was thus inspired to purify nickel. The Nickel tetracarbonyl molecule is tetrahedral, with a bond length for nickel to carbon of 1.82 Å.\nNickel tetracarbonyl easily starts breaking apart over 36° forming Ni(CO), Ni(CO), and Ni. Ni(CO) and NiC appear in mass spectroscopy of nickel carbonyl.\n\nThere are several nickel carbonyl cluster anions formed by reduction from nickel carbonyl. These are [Ni(CO)], dark red [Ni(CO)], [Ni(CO)], [Ni(CO)], [Ni(CO)]. Salts such as Cd[Ni(CO)] and Li[Ni(CO)]•5acetone can be crystallised.\n\nMixed cluster carbonyl anions like [CrNi(CO)], [MoNi(CO)] and [WNi(CO)] [Mo<Ni(CO)] can form salts with bulky cations like tetraethylammonium. The brown [NiCo(CO)] changes to red [NiCo(CO)].\n\nWith oxygen or air the explosive Ni(CO)O can be formed from nickel carbonyl.\n\nYet other ligands can substitute for carbon monoxide in nickel carbonyl. These lewis base ligands include triphenylphosphine, triphenoxyphosphine, trimethoxyphosphine, tributylphosphine, triethoxyphosphine, triethylisonitrolphosphine, triphenylarsine, and triphenylstibine.\n\nNickel forms dark blue planar complexes with 1,2-Diimino-3,5-cyclohexadiene or bisacetylbisaniline [(CHN-C(CH)=)]Ni. Another planar bis compound of nickel is formed with phenylazothioformamide CHN=NC(S)NR, and dithizone CHN=NC(S)NHNHCH. tetrasulfur tetranitride when reduced with nickel carbonyl makes Ni[NSH] also coloured dark violet.\n\nOne nickellabenzene is known where nickel substitutes for carbon in benzene. At nickel the plane of the molecule is bent, however the connection to the ring has aromatic character.\n\nNickel \"tert\"-butoxide Ni[OC(CH)] is coloured violet. It is formed in the reaction of di-\"tert\"-butylperoxide with nickel carbonyl.\n\nNickel dimethoxide is coloured green. \nThere are also nickel chloride methoxides with formulae: NiClOMe, NiCl(OMe) and NiCl(OMe) in which Nickel and oxygen appear to form a cubane-type cluster.\n\nOther alkoxy compounds known for nickel include nickel dipropoxide, nickel di-isopropoxide, nickel \"tert\"-amyloxide, and nickel di-\"tert\"-hexanoxide. These can be formed by crystallising nickel chloride from the corresponding alcohol, which forms an adduct. This is then heated with a base. Nickel(II) alkoxy compounds are polymeric and non-volatile.\n\nZiegler catalysis uses nickel as a catalyst. In addition it uses diethylaluminum ethoxide, phenylacetylene and triethylaluminium It converts ethylene into 1-butene. It can dimerise propylene. The catalyst, when combined with optically active phosphines, can produce optically active dimers. An intermediate formed is tris(ethylene)nickel.(CH=CH)Ni in which the ethylene molecules connect to the nickel atom side on.\n\nHomoletptic bimetallic alkoxides have two different metals, and the same alkoxy group. They include Ni[(μ-OMe)AlOMe], Ni[Al(OBu)] (nickel tetra-\"tert\"-butoxyaluminate) and Ni[Al(OPr)]. (nickel tetra-isopropoxyaluminate a pink liquid) Potassium hexaisoproxynoibate and tantalate can react with nickel chloride to make Ni[Nb(OPr)] and Ni[Ta(OPr)]. Ni[Zr(OPr)] The bimetallic alkoxides are volatile and can dissolve in organic solvents. A trimetallic one exists [Zr(OPr)]Ni[Al(OPr)]. NiGe(OBu)], NiSn(OBu)] and NiPb(OBu)] are tricyclic. [Niμ3-OEt)(μ-OEt)Sb(OEt)]\n\nHeteroleptic bitmetallic ethoxides have more than one variety of alkoxy group, e.g. Ni[(μ-OPr)(μ-OBu)Al(OBu)] which is a purple solid.\n\nOxoalkoxides contain extra oxygen in addition to the alcohol. With only nickel, none are known, but with antimony an octanuclear molecule exists [NiSb(μ4-O)(μ3-OEt)(-OEt)(OEt)(EtOH)].\n\nThere are many nickel compounds with the formula template Ni(OAr)XL and Ni(OAr)L. L is a ligand with phosphorus or nitrogen atoms. OAr is a phenol group or O- attached to an aromatic ring. Often an extra molecule of the phenol is hydrogen bonded to the oxygen attached to nickel.\n\nOthers include cyclododecatriene nickel.\n\nNickel bis-dithiobenzoate can form a violet coloured sodium salt.\n\nTwo bisperfluoromethyl-l,2-dithietene molecules react with nickel carbonyl to make a double ring compound with nickel linked to four sulfur atoms. This contains four trifluoromethyl groups and is dark purple. Instead of this methyl or phenyl can substitute. These can be made by substituted acetylenes with sulfur on nickel carbonyl, or on nickel sulfide. Bis-diphenyldithiene nickel has a planar structure\n\nA hexameric compound [Ni(SR)] is produced in the reaction of nickel carbonyl with dialkyl sulfides (RSR).\n\nNickel can be part of a cubane-type cluster with iron and chalcogens. The metal atoms are arranged in a tetrahedron shape, with the sulfur or selenium making up another tetrahedron that combines to make a cube. For example, the [NiFeS(PPh)(SEt)] is a dianion that has a tetraethyl ammonium salt. Similar ion clusters are [NiFeSe(PPh)(SEt)] and [NiFeSe(SEt)]. In the natural world cube shaped metal sulfur clusters can have sulfur atoms that are part of cysteine.\n\n[NiSe] has a cube with Ni(IV)Se at its core, and then the nickel atoms are bridge across the cube faces by five Se chains and one Se chain. It is formed as a tetraethylammonium salt, from LiSe, Se, NEtCl and nickel dixanthate in dimethylformamide as a solvent. This reaction also produces (NEt)Ni(Se).\n\nWhen liquid nickel carbonyl is dissolved in liquid hydrogen chloride, it can react with nitrosyl chloride to form a dimer Ni(NOCl). This then decomposes to Ni(NO)Cl, which is polymeric.\n\nNickel carbonyl reacting with nitric oxide yields blue coloured mononitrosyl nickel NiNO. With cyclohexane as well, pale blue Ni(NO)NO is produced with nitrous oxide as a side product. With cyclopentadiene as well, π-CHNiNO is produced.\n\n"}
{"id": "41996391", "url": "https://en.wikipedia.org/wiki?curid=41996391", "title": "Niska Gas Storage Partners", "text": "Niska Gas Storage Partners\n\nNiska Gas Storage Partners is a global gas storage service provider and the largest gas storage firm in North America. It operates business in Alberta, Canada and California, US and Oklahoma. The company owns a natural gas storage capacity of 225.5 billion cubic feet. It generates revenue by charging monthly natural gas storage fees to a variety of customers including institutions, gas generators, etc. AECO Hub, consisting of three sub-facilities located both in Canada and US, is the core storage facility of the company and is one of the most important natural gas producing sites in North America. The company has a market capitalization of $1.05 billion and an enterprise value of 1.05 billion.\n\nThe company is named after a Cree Indian word, \"niska\" meaning \"Canada goose\" indicating the company’s root as the Canadian developer of the first major trading point for natural gas in Alberta.\n\nIn the early 2000s, the company acquired the former Manchester Gas Storage facility in Oklahoma, which was renamed Salt Plains Gas Storage, which is one of the three facilities that make up for AECO Hub. The company operated its business under the name, Niska Gas Storage, in 2006 when the natural gas storage business was acquired by Riverstone Holdings LLC. Four years later, the company launched an Initial Public Offering\n\n"}
{"id": "19935614", "url": "https://en.wikipedia.org/wiki?curid=19935614", "title": "Norwegian Oil and Gas", "text": "Norwegian Oil and Gas\n\nNorwegian Oil & Gas () is an employers' organisation in Norway, organized under the national Confederation of Norwegian Enterprise. Up to 2012 it was called the Norwegian Oil Industry Association (, OLF)\n\nThe current director-general is Gro Brækken. Chairman of the board is Steinar Våge.\n\nIts predecessor organisation, the Norwegian Employers Association for Operator Companies, was founded in 1981.\n\n"}
{"id": "303094", "url": "https://en.wikipedia.org/wiki?curid=303094", "title": "Outline of green politics", "text": "Outline of green politics\n\nThe following outline is provided as an overview and topical guide to green politics:\n\nGreen politics – political ideology that aims for the creation of an ecologically sustainable society rooted in environmentalism, social liberalism, and grassroots democracy. It began taking shape in the western world in the 1970s; since then Green parties have developed and established themselves in many countries across the globe, and have achieved some electoral success.\n\nGreen politics can be described as:\n\n\nGreen politics\n\n\nGreen politics shares many ideas with the following movements:\n\n\n\n\n\n\n\nGreen economics\n\nA few issues affect most of the green parties around the world, and can often inhibit global cooperation. Some affect structure, and others affect policy:\n\n\nOn matters of ecology, extinction, biosafety, biosecurity, safe trade and health security, \"Greens\" generally agree. There are very substantial policy differences between and among Green Parties in various countries and cultures, and a continuing debate about the degree to which natural ecology and human needs align. Agreement on particular issues is often reached using a consensus decision making process.\n\n\nThe member parties of the Global Greens (see for details) are organised into four continental federations .\n\n\nThe European Federation of Green Parties formed itself as the European Green Party on 22 February 2004, in the run-up to European Parliament elections in June, 2004, a further step in trans-national integration.\n\n\n\n\n\n\n\n\n\n\"\"Green\" articles that don't relate in any way to Green politics or parties\"\n\n\n\n"}
{"id": "20568810", "url": "https://en.wikipedia.org/wiki?curid=20568810", "title": "Paul Kogerman", "text": "Paul Kogerman\n\nPaul Nikolai Kogerman ( in Tallinn – 27 July 1951 in Tallinn) was an Estonian chemist and founder of modern research in oil shale.\n\nPaul Kogerman was born into a family of gas factory worker (and former sailor). He went to an elementary school in 1901–1904 and a town school in 1904–1908. After town school Kogerman earned a living by teaching in church manors near Tallinn. In 1913, he was graduated from the Alexander Gymnasium in Tallinn (Reval) as an extern. Starting in 1913, he studied at the University of Tartu, graduating from its Department of Chemistry in 1918. In the Estonian War of Independence he fought in a unit of Tallinn teachers. In 1919–1920 he got a state scholarship to study at the Imperial College London. In May 1921 he was given a qualification of chemical technologist by the University of London and in 1922 he received the degree of Master of Sciences.\n\nFrom 1921 to 1936, Kogerman was active at the University of Tartu. After the defence of his Master's thesis on the thermal decay of oil-shale, he was elected docent of Organic Chemistry of the University in 1922. He went on to become extraordinary professor in 1924 and full professor in 1925. In 1926 and 1933 he was guest lecturer at the ETH Zürich and in 1927–1928 at Harvard University. In 1934, he defended, in Zürich, his doctoral thesis on the combining and polymerization reactions of the isolated double bond dienes.\n\nFrom 1936 to 1941, Kogerman was professor of organic chemistry at the Tallinn University of Technology, in 1936–1939 he was also a rector of the University. In 1938, Kogerman was selected to the newly established Estonian Academy of Sciences and in 1946 reselected after re-establishment of the Academy as the Academy of Sciences of the Estonian Soviet Socialist Republic. He was the president of the Estonian Naturalists' Society in 1929–1936.\nIn 1938–1939 Kogerman was \"ex officio\" member of the National Council \"(Riiginõukogu).\" From October 1939 until the Soviet occupation of Estonia on 21 June 1940 Kogerman served as Minister of Public Education.\n\nIn 1941, Kogerman, together with his family, was deported by Soviet authorities to the prisoner camp in Sverdlovsk Oblast. He was prematurely released and allowed to return to Estonia in 1945. From 1945 to 1951 he was director of the chair for organic chemistry at Tallinn University of Technology. From 1947 to 1950 he served also as the director of the Chemistry Institute of the Academy of Sciences.\n\nIn 1927, Kogerman was decorated with the insignia of the Légion d'honneur and in 1938 with the Second Class of the Order of the White Star. In 2006, the Paul Kogerman scholarship was founded, to be granted to successful master's and doctoral level students of the Faculties of Science and Chemical and Materials Technology of the Tallinn University of Technology.\n\nPaul Kogerman won international reputation with his work on oil shale. He initiated systematic research of oil shale and its products by establishing, together with professor Michael Wittlich, a laboratory to study oil-bearing shales in 1925. Kogerman submitted fundamental work on the structure and origin of oil shale and its chemical characteristics, as well as work on thermal processes.\n\n\n\n"}
{"id": "3510949", "url": "https://en.wikipedia.org/wiki?curid=3510949", "title": "Pelamis Wave Energy Converter", "text": "Pelamis Wave Energy Converter\n\nThe Pelamis Wave Energy Converter was a technology that used the motion of ocean surface waves to create electricity. The machine was made up of connected sections which flex and bend as waves pass; it is this motion which is used to generate electricity.\n\nDeveloped by the now defunct \nScottish company Pelamis Wave Power (formerly Ocean Power Delivery), the Pelamis became the first offshore wave machine to generate electricity into the grid, when it was first connected to the UK grid in 2004. Pelamis Wave Power then went on to build and test five additional Pelamis machines: three first-generation P1 machines, which were tested in a farm off the coast of Portugal in 2009, and two second-generation machines, the Pelamis P2, were tested off Orkney between 2010 and 2014. The company went into administration in November 2014, with the intellectual property transferred to the Scottish Government body Wave Energy Scotland.\n\nThe Pelamis machine is an offshore wave energy converter, operating in water depths greater than 50m. The machine consists of a series of semi-submerged cylindrical sections linked by hinged joints. As waves pass along the length of the machine, the sections move relative to one another. The wave-induced motion of the sections is resisted by hydraulic cylinders which pump high pressure oil through hydraulic motors via smoothing hydraulic accumulators. The hydraulic motors drive electrical generators to produce electricity. Electricity from all the joints is fed down a single umbilical cable to a junction on the sea bed. Several devices can be connected and linked to shore through a single seabed cable.\n\nThe Pelamis is an attenuating wave energy converter. The machine responds to the curvature of the waves (their shape) rather than the wave height. As waves can only reach a certain curvature before naturally breaking, this limits the range of motion through which the machine must move but maintains large motion at the joints in small waves.\n\nThe P2 Pelamis design was Pelamis Wave Power's second generation Pelamis machine. The Pelamis P2 is 180m long, 4m diameter and approximately 1350 tonnes in weight. Consisting of five tube sections and four flexible joints, the design is longer and fatter than the previous P1 design.\n\nIn 2010, Pelamis Wave Power began tests of the first Pelamis P2 machine at the European Marine Energy Centre, Orkney, Scotland. The machine was owned by the German utility company, E.ON, and was the UK's first commercial supply contract in the marine energy sector. In March 2010 Pelamis Wave Power announced a second order for a P2 machine, from ScottishPower Renewables, part of Iberdrola Renovables. This second machine was first installed at EMEC in May 2012. The two utility companies announced that they will work together to share and collaborate in testing of the P2 Pelamis technology.\n\nFollowing the demise of the company, the P2-001 device was acquired by Wave Energy Scotland, having completed over 15,000 hours of operation. The device was decommissioned in April 2016 and sold to the Orkney Island Council for £1. The other device, P2-002 was sold to the European Marine Energy Centre for use as a test rig.\n\nE.ON and ScottishPower Renewables announced plans to build larger projects using Pelamis machines in the waters off Orkney's west coast. Both companies won leases in 2010 from The Crown Estate, who own the seabed around the UK, for projects of up to 50MW. The \"Pentland Firth and Orkney Waters Leasing Round\" was the world's first commercial scale wave and tidal energy leasing opportunity.\n\nPelamis Wave Power tested their first full-scale prototype at the European Marine Energy Centre in Orkney, Scotland between 2004 and 2007. The machine, which was rated at 750 kW, was the world's first offshore wave power machine to generate electricity into the grid system.\n\nThe prototype was 120m long and 3.5m in diameter. It consisted of four tube sections linked by three, shorter, and power conversion modules\n\nIn 2008 Pelamis tested three first generation, P1 Pelamis waves at the Aguçadoura Wave Farm. Located off the northwest coast of Portugal near Póvoa de Varzim, the farm had an installed capacity of 2.25MW and was the world's first multiple machine wave power project. The project was part funded by Portuguese utility Enersis, at the time owned by Australian global investment company Babcock & Brown. The farm first generated electricity in July 2008 but was taken offline in November 2008 at the same time as Babcock & Brown encountered financial difficulties.\n\n\"Pelamis platurus\" is a yellow-bellied sea snake that lives in tropical and subtropical waters. It prefers shallow inshore waters.\n\n\n"}
{"id": "15685143", "url": "https://en.wikipedia.org/wiki?curid=15685143", "title": "Peter Nijkamp", "text": "Peter Nijkamp\n\nPeter Nijkamp (born 26 February 1946) is a Dutch economist, Professor of Regional Economics and Economic Geography at the Vrije Universiteit, Amsterdam, the Netherlands, a fellow of the Tinbergen Institute and President of the Governing Board of the Netherlands Research Council (NWO). He is ranked among the top 100 economists in the world according to IDEAS/RePEc, and is by far the most prolific economist. Towards the end of his career at the VU university Nijkamp faced accusations of (self)plagiarism and VU-appointed investigators have criticised referencing methods in some of his work.\n\nBorn in Dalfsen, Overijssel, Nijkamp received his MSc in Econometrics and Regional Economics in 1970, and his PhD in Regional Economics in 1972, both from the Erasmus University, Rotterdam, the Netherlands.\n\nHe has honorary doctorates from the Vrije Universiteit Brussel and the National Technical University of Athens. He is a winner of the Spinozapremie (1996), the European Prize in Regional Science, and Founder's Medal of the Regional Science Association International.\n\nHe has been member of the Royal Netherlands Academy of Arts and Sciences since 1987.\n\nIn 2013, the PhD thesis of one of his students was withdrawn one day before the scheduled defence. It consisted almost entirely of papers jointly written with Nijkamp. An internal investigation at the VU determined that some of these papers were recycled versions of yet earlier papers, with other co-authors, who had not been cited properly and this might be considered a form of plagiarism. The VU's rector, Frank van der Duyn Schouten, stated that Nijkamp had acted improperly but had not committed plagiarism. The student involved was allowed to adapt the thesis and the graduation ceremony took place in 2014.\n\nFollowing extensive press reports that claimed to have found cases of self-plagiarism in Nijkamp's work and an analysis by an anonymous whistleblower, the VU announced it would investigate Nijkamp's entire publishing record. It would also inquire with the Royal Netherlands Academy of Arts and Sciences on the boundary between self-plagiarism and \"self-citation\".\n\nThe investigation committee concluded in March 2015 that Nijkamp had re-used earlier publications in new papers on a certain scale, without proper referencing and the committee qualified this as \"questionable research practice\". The VU accepted some, but not all, of the committee conclusions. In a separate investigation the Dutch National Board on Research Integrity found Nijkamp \"careless\" but not in breach of scientific integrity. It also found university regulations on self-citation insufficient at the time. Nijkamp himself stated that the investigation committee's methodology and selection of publications is too thin a base for any conclusions.\n\nHis books include \n\nHis journal papers include\n\n"}
{"id": "1517190", "url": "https://en.wikipedia.org/wiki?curid=1517190", "title": "Pulsed power", "text": "Pulsed power\n\nPulsed power is the science and technology of accumulating energy over a relatively long period of time and releasing it very quickly, thus increasing the instantaneous power.\n\nEnergy is typically stored within electrostatic fields (capacitors), magnetic fields (inductor), as mechanical energy (using large flywheels connected to special purpose high current alternators), or as chemical energy (high-current lead-acid batteries, or explosives). By releasing the stored energy over a very short interval (a process that is called energy compression), a huge amount of peak power can be delivered to a load. For example, if one joule of energy is stored within a capacitor and then evenly released to a load over one second, the average power delivered to the load would only be 1 watt. However, if all of the stored energy were released within one microsecond, the average power over one second would still be one watt, but the instantaneous peak power would be one megawatt, a million times greater. Examples where pulsed power technology is commonly used include radar, particle accelerators, ultrastrong magnetic fields, fusion research, electromagnetic pulses, and high power pulsed lasers.\n\nPulsed Power was first developed during World War II for use in Radar. Radar requires short high power pulses. After the war, development continued in other applications, leading to the super pulsed power machines at Sandia National Laboratories.\n\n\n"}
{"id": "21504869", "url": "https://en.wikipedia.org/wiki?curid=21504869", "title": "Second Wind (company)", "text": "Second Wind (company)\n\nSecond Wind was a firm which developed instrumentation and software for remote sensing in the wind energy industry. They were situated in Somerville, Massachusetts and had a testing site on Chappaquiddick Island. In August 2013, Second Wind was acquired by Vaisala, a Finnish manufacturer of environmental and industrial measurement instrumentation.\n\n"}
{"id": "741842", "url": "https://en.wikipedia.org/wiki?curid=741842", "title": "Synchro", "text": "Synchro\n\nA synchro (also known as Selsyn and by other brand names) is, in effect, a transformer whose primary-to-secondary coupling may be varied by physically changing the relative orientation of the two windings. Synchros are often used for measuring the angle of a rotating machine such as an antenna platform. In its general physical construction, it is much like an electric motor. The primary winding of the transformer, fixed to the rotor, is excited by an alternating current, which by electromagnetic induction, causes currents to flow in three Y-connected secondary windings fixed at 120 degrees to each other on the stator. The relative magnitudes of secondary currents are measured and used to determine the angle of the rotor relative to the stator, or the currents can be used to directly drive a receiver synchro that will rotate in unison with the synchro transmitter. In the latter case, the whole device may be called a selsyn (a portmanteau of \"self\" and \"synchronizing\"). \n\nSynchro systems were first used in the control system of the Panama Canal in the early 1900s to transmit lock gate and valve stem positions, and water levels, to the control desks.\nFire-control system designs developed during World War II used synchros extensively, to transmit angular information from guns and sights to an analog fire control computer, and to transmit the desired gun position back to the gun location. Early systems just moved indicator dials, but with the advent of the amplidyne, as well as motor-driven high-powered hydraulic servos, the fire control system could directly control the positions of heavy guns.\n\nSmaller synchros are still used to remotely drive indicator gauges and as rotary position sensors for aircraft control surfaces, where the reliability of these rugged devices is needed.\nDigital devices such as the rotary encoder have replaced synchros in most other applications. \n\nSelsyn motors were widely used in motion picture equipment to synchronize movie cameras and sound recording equipment, before the advent of crystal oscillators and microelectronics.\n\nLarge synchros were used on naval warships, such as destroyers, to operate the steering gear from the wheel on the bridge.\n\nThere are two types of synchro systems: Torque systems and control systems.\n\nIn a torque system, a synchro will provide a low-power mechanical output sufficient to position an indicating device, actuate a sensitive switch or move light loads without power amplification. In simpler terms, a torque synchro system is a system in which the transmitted signal does the usable work. In such a system, accuracy on the order of one degree is attainable.\n\nIn a control system, a synchro will provide a voltage for conversion to torque through an amplifier and a servomotor. Control type synchros are used in applications that require large torques or high accuracy such as follow-up links and error detectors in servo, automatic control systems (such as an autopilot system). In simpler terms, a control synchro system is a system in which the transmitted signal controls a source of power which does the usable work.\n\nQuite often, one system will perform both torque and control functions. Individual units are designed for use in either torque or control systems. Some torque units can be used as control units, but control units cannot replace torque units.\n\nA synchro will fall into one of eight functional categories. They are as follows:\n\n\n\n\n\n\n\n\n\nOn a practical level, synchros resemble motors, in that there is a rotor, stator, and a shaft. Ordinarily, slip rings and brushes connect the rotor to external power. A synchro transmitter's shaft is rotated by the mechanism that sends information, while the synchro receiver's shaft rotates a dial, or operates a light mechanical load. Single and three-phase units are common in use, and will follow the other's rotation when connected properly. One transmitter can turn several receivers; if torque is a factor, the transmitter must be physically larger to source the additional current. In a motion picture interlock system, a large motor-driven distributor can drive as many as 20 machines, sound dubbers, footage counters, and projectors.\n\nSynchros designed for terrestrial use tend to be driven at 50 or 60 hertz (the mains frequency in most countries), while those for marine or aeronautical use tend to operate at 400 hertz (the frequency of the on-board electrical generator driven by the engines).\n\nSingle phase units have five wires: two for an exciter winding (typically line voltage) and three for the output/input. These three are bussed to the other synchros in the system, and provide the power and information to align the shafts of all the receivers. Synchro transmitters and receivers must be powered by the same branch circuit, so to speak; the mains excitation voltage sources must match in voltage and phase. The safest approach is to bus the five or six lines from transmitters and receivers at a common point. Different makes of selsyns, used in interlock systems, have different output voltages. In all cases, three-phase systems will handle more power and operate a bit more smoothly. The excitation is often 208/240 V 3-phase mains power. Many synchros operate on 30 to 60 V AC also.\n\nSynchro transmitters are as described, but 50 and 60-Hz synchro receivers require rotary dampers to keep their shafts from oscillating when not loaded (as with dials) or lightly loaded in high-accuracy applications.\n\nA different type of receiver, called a control transformer (CT), is part of a position servo that includes a servo amplifier and servo motor. The motor is geared to the CT rotor, and when the transmitter's rotor moves, the servo motor turns the CT's rotor and the mechanical load to match the new position. CTs have high-impedance stators and draw much less current than ordinary synchro receivers when not correctly positioned.\n\nSynchro transmitters can also feed synchro to digital converters, which provide a digital representation of the shaft angle.\n\nSo called 'brushless synchros' use rotary transformers (that have no magnetic interaction with the usual rotor and stator) to feed power to the rotor. These transformers have stationary primaries, and rotating secondaries. The secondary is somewhat like a spool wound with magnet wire, the axis of the spool concentric with the rotor's axis. The \"spool\" is the secondary winding's core, its flanges are the poles, and its coupling does not vary significantly with rotor position. The primary winding is similar, surrounded by its magnetic core, and its end pieces are like thick washers. The holes in those end pieces align with the rotating secondary poles.\n\nFor high accuracy in gun fire control and aerospace work, so called multi-speed synchro data links were used. For instance, a two-speed link had two transmitters, one rotating for one turn over the full range (such as a gun's bearing), while the other rotated one turn for every 10 degrees of bearing. The latter was called a 36-speed synchro. Of course, the gear trains were made accordingly. At the receiver, the magnitude of the 1X channel's error determined whether the \"fast\" channel was to be used instead. A small 1X error meant that the 36x channel's data was unambiguous. Once the receiver servo settled, the fine channel normally retained control.\n\nFor very critical applications, three-speed synchro systems have been used.\n\nSo called multispeed synchros have stators with many poles, so that their output voltages go through several cycles for one physical revolution. For two-speed systems, these do not require gearing between the shafts.\n\nDifferential synchros are another category. They have three-lead rotors and stators like the stator described above, and can be transmitters or receivers. A differential transmitter is connected between a synchro transmitter and a receiver, and its shaft's position adds to (or subtracts from, depending upon definition) the angle defined by the transmitter. A differential receiver is connected between two transmitters, and shows the sum (or difference, again as defined) between the shaft positions of the two transmitters. There are synchro-like devices called transolvers, somewhat like differential synchros, but with three-lead rotors and four-lead stators.\n\nA resolver is similar to a synchro, but has a stator with four leads, the windings being 90 degrees apart physically instead of 120 degrees. Its rotor might be synchro-like, or have two sets of windings 90 degrees apart. Although a pair of resolvers could theoretically operate like a pair of synchros, resolvers are used for computation.\n\nA special T-connected transformer arrangement invented by Scott (\"Scott T\") interfaces between resolver and synchro data formats; it was invented to interconnect two-phase AC power with three-phase power, but can also be used for precision applications.\n\n\n"}
{"id": "30044", "url": "https://en.wikipedia.org/wiki?curid=30044", "title": "Thorium", "text": "Thorium\n\nThorium is a weakly radioactive metallic chemical element with symbol Th and atomic number 90. Thorium is silvery and tarnishes black when it is exposed to air, forming thorium dioxide; it is moderately hard, malleable, and has a high melting point. Thorium is an electropositive actinide whose chemistry is dominated by the +4 oxidation state; it is quite reactive and can ignite in air when finely divided.\n\nAll known thorium isotopes are unstable. The most stable isotope, Th, has a half-life of 14.05 billion years, or about the age of the universe; it decays very slowly via alpha decay, starting a decay chain named the thorium series that ends at stable Pb. In the universe, thorium and uranium are the only two radioactive elements that still occur naturally in large quantities as primordial elements. It is estimated to be over three times more abundant than uranium in the Earth's crust, and is chiefly refined from monazite sands as a by-product of extracting rare-earth metals.\n\nThorium was discovered in 1829 by the Norwegian amateur mineralogist Morten Thrane Esmark and identified by the Swedish chemist Jöns Jacob Berzelius, who named it after Thor, the Norse god of thunder. Its first applications were developed in the late 19th century. Thorium's radioactivity was widely acknowledged during the first decades of the 20th century. In the second half of the century, thorium was replaced in many uses due to concerns about its radioactivity.\n\nThorium is still being used as an alloying element in TIG welding electrodes but is slowly being replaced in the field with different compositions. It was also a material in high-end optics and scientific instrumentation, and as the light source in gas mantles, but these uses have become marginal. It has been suggested as a replacement for uranium as nuclear fuel in nuclear reactors, and several thorium reactors have been built.\n\nThorium is a moderately soft, paramagnetic, bright silvery radioactive actinide metal. In the periodic table, it lies to the right of actinium, to the left of protactinium, and below cerium. Pure thorium is very ductile and, as normal for metals, can be cold-rolled, swaged, and drawn. At room temperature, thorium metal has a face-centred cubic crystal structure; it has two other forms, one at high temperature (over 1360 °C; body-centred cubic) and one at high pressure (around 100 GPa; body-centred tetragonal).\n\nThorium metal has a bulk modulus (a measure of resistance to compression of a material) of 54 GPa, about the same as tin's (58.2 GPa). Aluminium's is 75.2 GPa; copper's 137.8 GPa; and mild steel's is 160–169 GPa. Thorium is about as hard as soft steel, so when heated it can be rolled into sheets and pulled into wire.\n\nThorium is nearly half as dense as uranium and plutonium and is harder than either of them. It becomes superconductive below 1.4 K. Thorium's melting point of 1750 °C is above both those of actinium (1227 °C) and protactinium (1568 °C). At the start of period 7, from francium to thorium, the melting points of the elements increase (as in other periods), because the number of delocalised electrons each atom contributes increases from one in francium to four in thorium, leading to greater attraction between these electrons and the metal ions as their charge increases from one to four. After thorium, there is a new downward trend in melting points from thorium to plutonium, where the number of f electrons increases from about 0.4 to about 6: this trend is due to the increasing hybridisation of the 5f and 6d orbitals and the formation of directional bonds resulting in more complex crystal structures and weakened metallic bonding. (The f-electron count for thorium is a non-integer due to a 5f–6d overlap.) Among the actinides up to californium, which can be studied in at least milligram quantities, thorium has the highest melting and boiling points and second-lowest density; only actinium is lighter. Thorium's boiling point of 4788 °C is the fifth-highest among all the elements with known boiling points.\n\nThe properties of thorium vary widely depending on the degree of impurities in the sample. The major impurity is usually thorium dioxide (ThO); even the purest thorium specimens usually contain about a tenth of a percent of the dioxide. Experimental measurements of its density give values between 11.5 and 11.66 g/cm: these are slightly lower than the theoretically expected value of 11.7 g/cm calculated from thorium's lattice parameters, perhaps due to microscopic voids forming in the metal when it is cast. These values lie between those of its neighbours actinium (10.1 g/cm) and protactinium (15.4 g/cm), part of a trend across the early actinides.\n\nThorium can form alloys with many other metals. Addition of small proportions of thorium improves the mechanical strength of magnesium, and thorium-aluminium alloys have been considered as a way to store thorium in proposed future thorium nuclear reactors. Thorium forms eutectic mixtures with chromium and uranium, and it is completely miscible in both solid and liquid states with its lighter congener cerium.\n\nAll but two elements up to bismuth (element 83) have an isotope that is practically stable for all purposes (\"classically stable\"), with the exceptions being technetium and promethium (elements 43 and 61). All elements from polonium (element 84) onward are measurably radioactive. Th is one of the three nuclides beyond bismuth (the other two being U and U) that have half-lives measured in billions of years; its half-life is 14.05 billion years, about three times the age of the earth, and slightly longer than the age of the universe. Four-fifths of the thorium present at Earth's formation has survived to the present. Th is the only isotope of thorium occurring in quantity in nature. Its stability is attributed to its closed nuclear shell with 142 neutrons. Thorium has a characteristic terrestrial isotopic composition, with atomic weight 232.0377(4). It is one of only three radioactive elements (along with protactinium and uranium) that occur in large enough quantities on Earth for a standard atomic weight to be determined.\n\nThorium nuclei are susceptible to alpha decay because the strong nuclear force cannot overcome the electromagnetic repulsion between their protons. The alpha decay of Th initiates the 4\"n\" decay chain which includes isotopes with a mass number divisible by 4 (hence the name; it is also called the thorium series after its progenitor). This chain of consecutive alpha and beta decays begins with the decay of Th to Ra and terminates at Pb. Any sample of thorium or its compounds contains traces of these daughters, which are isotopes of thallium, lead, bismuth, polonium, radon, radium, and actinium. Natural thorium samples can be chemically purified to extract useful daughter nuclides, such as Pb, which is used in nuclear medicine for cancer therapy. Th also very occasionally undergoes spontaneous fission rather than alpha decay, and has left evidence of doing so in its minerals (as trapped xenon gas formed as a fission product), but the partial half-life of this process is very large at over 10 years and alpha decay predominates.\nThirty radioisotopes have been characterised, which range in mass number from 209 to 238. After Th, the most stable of them (with respective half-lives) are Th (75,380 years), Th (7,340 years), Th (1.92 years), Th (24.10 days), and Th (18.68 days). All of these isotopes occur in nature as trace radioisotopes due to their presence in the decay chains of Th, U, U, and Np: the last of these is long extinct in nature due to its short half-life (2.14 million years), but is continually produced in minute traces from neutron capture in uranium ores. All of the remaining thorium isotopes have half-lives that are less than thirty days and the majority of these have half-lives that are less than ten minutes.\n\nIn deep seawaters the isotope Th makes up to 0.04% of natural thorium. This is because its parent U is soluble in water, but Th is insoluble and precipitates into the sediment. Uranium ores with low thorium concentrations can be purified to produce gram-sized thorium samples of which over a quarter is the Th isotope, since Th is one of the daughters of U. The International Union of Pure and Applied Chemistry (IUPAC) reclassified thorium as a binuclidic element in 2013; it had formerly been considered a mononuclidic element.\n\nThorium has three known nuclear isomers (or metastable states), Th, Th, and Th. Th has the lowest known excitation energy of any isomer, measured to be . This is so low that when it undergoes isomeric transition, the emitted gamma radiation is in the ultraviolet range.\n\nDifferent isotopes of thorium are chemically identical, but have slightly differing physical properties: for example, the densities of pure Th, Th, Th, and Th are respectively expected to be 11.5, 11.6, 11.6, and 11.7 g/cm. The isotope Th is expected to be fissionable with a bare critical mass of 2839 kg, although with steel reflectors this value could drop to 994 kg. Th is not fissionable, but it is fertile as it can be converted to fissile U by neutron capture and subsequent beta decay.\n\nTwo radiometric dating methods involve thorium isotopes: uranium–thorium dating, based on the decay of U to Th, and ionium–thorium dating, which measures the ratio of Th to Th. These rely on the fact that Th is a primordial radioisotope, but Th only occurs as an intermediate decay product in the decay chain of U. Uranium–thorium dating is a relatively short-range process because of the short half-lives of U and Th relative to the age of the Earth: it is also accompanied by a sister process involving the alpha decay of U into Th, which very quickly becomes the longer-lived Pa, and this process is often used to check the results of uranium–thorium dating. Uranium–thorium dating is commonly used to determine the age of calcium carbonate materials such as speleothem or coral, because uranium is more soluble in water than thorium and protactinium, which are selectively precipitated into ocean-floor sediments, where their ratios are measured. The scheme has a range of several hundred thousand years. Ionium–thorium dating is a related process, which exploits the insolubility of thorium (both Th and Th) and thus its presence in ocean sediments to date these sediments by measuring the ratio of Th to Th. Both of these dating methods assume that the proportion of Th to Th is a constant during the period when the sediment layer was formed, that the sediment did not already contain thorium before contributions from the decay of uranium, and that the thorium cannot migrate within the sediment layer.\n\nA thorium atom has 90 electrons, of which four are valence electrons. Three atomic orbitals are theoretically available for the valence electrons to occupy: 5f, 6d, and 7s. Despite thorium's position in the f-block of the periodic table, it has an anomalous [Rn]6d7s electron configuration in the ground state, as the 5f and 6d subshells in the early actinides are very close in energy, even more so than the 4f and 5d subshells of the lanthanides: thorium's 6d subshells are lower in energy than its 5f subshells, because its 5f subshells are not well-shielded by the filled 6s and 6p subshells and are destabilised. This is due to relativistic effects, which become stronger near the bottom of the periodic table, specifically the relativistic spin–orbit interaction. The closeness in energy levels of the 5f, 6d, and 7s energy levels of thorium results in thorium almost always losing all four valence electrons and occurring in its highest possible oxidation state of +4. This is different from its lanthanide congener cerium, in which +4 is also the highest possible state, but +3 plays an important role and is more stable. Thorium is much more similar to the transition metals zirconium and hafnium than to cerium in its ionisation energies and redox potentials, and hence also in its chemistry: this transition-metal-like behaviour is the norm in the first half of the actinide series.\n\nDespite the anomalous electron configuration for gaseous thorium atoms, metallic thorium shows significant 5f involvement. This was first realised in 1995, when it was pointed out that a hypothetical metallic state of thorium that had the [Rn]6d7s configuration with the 5f orbitals above the Fermi level should be hexagonal close packed like the group 4 elements titanium, zirconium, and hafnium, and not face-centred cubic as it actually is. The actual crystal structure can only be explained when the 5f states are invoked, proving that thorium, and not protactinium, acts as the first actinide metallurgically. The 5f character of thorium is also clear in the rare and highly unstable +3 oxidation state, in which thorium exhibits the electron configuration [Rn]5f.\n\nTetravalent thorium compounds are usually colourless or yellow, like those of silver or lead, as the Th ion has no 5f or 6d electrons. Thorium chemistry is therefore largely that of an electropositive metal forming a single diamagnetic ion with a stable noble-gas configuration, indicating a similarity between thorium and the main group elements of the s-block. Thorium and uranium are the most investigated of the radioactive elements because their radioactivity is low enough not to require special handling in the laboratory.\n\nThorium is a highly reactive and electropositive metal. With a standard reduction potential of −1.90 V for the Th/Th couple, it is somewhat more electropositive than zirconium or aluminium. Finely divided thorium metal can exhibit pyrophoricity, spontaneously igniting in air. When heated in air, thorium turnings ignite and burn with a brilliant white light to produce the dioxide. In bulk, the reaction of pure thorium with air is slow, although corrosion may occur after several months; most thorium samples are contaminated with varying degrees of the dioxide, which greatly accelerates corrosion. Such samples slowly tarnish, becoming grey and finally black at the surface.\n\nAt standard temperature and pressure, thorium is slowly attacked by water, but does not readily dissolve in most common acids, with the exception of hydrochloric acid, where it dissolves leaving a black insoluble residue of ThO(OH,Cl)H. It dissolves in concentrated nitric acid containing a small quantity of catalytic fluoride or fluorosilicate ions; if these are not present, passivation by the nitrate can occur, as with uranium and plutonium.\n\nMost binary compounds of thorium with nonmetals may be prepared by heating the elements together. In air, thorium burns to form ThO, which has the fluorite structure. Thorium dioxide is a refractory material, with the highest melting point (3390 °C) of any known oxide. It is somewhat hygroscopic and reacts readily with water and many gases; it dissolves easily in concentrated nitric acid in the presence of fluoride. When heated, it emits intense blue light through incandescence; the light becomes white when ThO is mixed with its lighter homologue cerium dioxide (CeO, ceria): this is the basis for its previously common application in gas mantles. Several binary thorium chalcogenides and oxychalcogenides are also known with sulfur, selenium, and tellurium.\n\nAll four thorium tetrahalides are known, as are some low-valent bromides and iodides: the tetrahalides are all 8-coordinated hygroscopic compounds that dissolve easily in polar solvents such as water. Many related polyhalide ions are also known. Thorium tetrafluoride has a monoclinic crystal structure like those of zirconium tetrafluoride and hafnium tetrafluoride, where the Th ions are coordinated with F ions in somewhat distorted square antiprisms. The other tetrahalides instead have dodecahedral geometry. Lower iodides ThI (black) and ThI (gold-coloured) can also be prepared by reducing the tetraiodide with thorium metal: they do not contain Th(III) and Th(II), but instead contain Th and could be more clearly formulated as electride compounds. Many polynary halides with the alkali metals, barium, thallium, and ammonium are known for thorium fluorides, chlorides, and bromides. For example, when treated with potassium fluoride and hydrofluoric acid, Th forms the complex anion , which precipitates as an insoluble salt, KThF.\n\nThorium borides, carbides, silicides, and nitrides are refractory materials, like those of uranium and plutonium, and have thus received attention as possible nuclear fuels. All four heavier pnictogens (phosphorus, arsenic, antimony, and bismuth) also form binary thorium compounds. Thorium germanides are also known. Thorium reacts with hydrogen to form the thorium hydrides ThH and ThH, the latter of which is superconducting below 7.5–8 K; at standard temperature and pressure, it conducts electricity like a metal. The hydrides are thermally unstable and readily decompose upon exposure to air or moisture.\n\nIn an acidic aqueous solution, thorium occurs as the tetrapositive aqua ion [Th(HO)], which has tricapped trigonal prismatic molecular geometry: at pH < 3, the solutions of thorium salts are dominated by this cation. The Th ion is the largest of the tetrapositive actinide ions, and depending on the coordination number can have a radius between 0.95 and 1.14 Å. It is quite acidic due to its high charge, slightly stronger than sulfurous acid: thus it tends to undergo hydrolysis and polymerisation (though to a lesser extent than Fe), predominantly to [Th(OH)] in solutions with pH 3 or below, but in more alkaline solution polymerisation continues until the gelatinous hydroxide Th(OH) forms and precipitates out (though equilibrium may take weeks to be reached, because the polymerisation usually slows down before the precipitation). As a hard Lewis acid, Th favours hard ligands with oxygen atoms as donors: complexes with sulfur atoms as donors are less stable and are more prone to hydrolysis.\n\nHigh coordination numbers are the rule for thorium due to its large size. Thorium nitrate pentahydrate was the first known example of coordination number 11, the oxalate tetrahydrate has coordination number 10, and the borohydride (first prepared in the Manhattan Project) has coordination number 14. These thorium salts are known for their high solubility in water and polar organic solvents.\n\nMany other inorganic thorium compounds with polyatomic anions are known, such as the perchlorates, sulfates, sulfites, nitrates, carbonates, phosphates, vanadates, molybdates, and chromates, and their hydrated forms. They are important in thorium purification and the disposal of nuclear waste, but most of them have not yet been fully characterised, especially regarding their structural properties. For example, thorium nitrate is produced by reacting thorium hydroxide with nitric acid: it is soluble in water and alcohols and is an important intermediate in the purification of thorium and its compounds. Thorium complexes with organic ligands, such as oxalate, citrate, and EDTA, are much more stable. In natural thorium-containing waters, organic thorium complexes usually occur in concentrations orders of magnitude higher than the inorganic complexes, even when the concentrations of inorganic ligands are much greater than those of organic ligands.\n\nMost of the work on organothorium compounds has focused on the cyclopentadienyl complexes and cyclooctatetraenyls. Like many of the early and middle actinides (up to americium, and also expected for curium), thorium forms a cyclooctatetraenide complex: the yellow Th(CH), thorocene. It is isotypic with the better-known analogous uranium compound uranocene. It can be prepared by reacting KCH with thorium tetrachloride in tetrahydrofuran (THF) at the temperature of dry ice, or by reacting thorium tetrafluoride with MgCH. It is unstable in air and decomposes in water or at 190 °C. Half sandwich compounds are also known, such as (\"η\"-CH)ThCl(THF), which has a piano-stool structure and is made by reacting thorocene with thorium tetrachloride in tetrahydrofuran.\n\nThe simplest of the cyclopentadienyls are Th(CH) and Th(CH): many derivatives are known. The former (which has two forms, one purple and one green) is a rare example of thorium in the formal +3 oxidation state; a formal +2 oxidation state occurs in a derivative. The chloride derivative [Th(CH)Cl] is prepared by heating thorium tetrachloride with limiting K(CH) used (other univalent metal cyclopentadienyls can also be used). The alkyl and aryl derivatives are prepared from the chloride derivative and have been used to study the nature of the Th–C sigma bond.\n\nOther organothorium compounds are not well-studied. Tetrabenzylthorium, Th(CHCH), and tetraallylthorium, Th(CH), are known, but their structures have not been determined. They decompose slowly at room temperature. Thorium forms the monocapped trigonal prismatic anion [Th(CH)], heptamethylthorate, which forms the salt [Li(tmeda)][ThMe] (tmeda= MeNCHCHNMe). Although one methyl group is only attached to the thorium atom (Th–C distance 257.1 pm) and the other six connect the lithium and thorium atoms (Th–C distances 265.5–276.5 pm), they behave equivalently in solution. Tetramethylthorium, Th(CH), is not known, but its adducts are stabilised by phosphine ligands.\n\nTh is a primordial nuclide, having existed in its current form for over ten billion years; it was forged in the cores of dying stars through the r-process and scattered across the galaxy by supernovae and neutron star mergers. The letter \"r\" stands for \"rapid neutron capture\", and occurs in core-collapse supernovae, where heavy seed nuclei such as Fe rapidly capture neutrons, running up against the neutron drip line, as neutrons are captured much faster than the resulting nuclides can beta decay back toward stability. Neutron capture is the only way for stars to synthesise elements beyond iron because of the increased Coulomb barriers that make interactions between charged particles difficult at high atomic numbers and the fact that fusion beyond Fe is endothermic. Because of the abrupt loss of stability past Bi, the r-process is the only process of stellar nucleosynthesis that can create thorium and uranium; all other processes are too slow and the intermediate nuclei alpha decay before they capture enough neutrons to reach these elements.\nIn the universe, thorium is among the rarest of the primordial elements, because it is one of the two elements that can be produced only in the r-process (the other being uranium), and also because it has slowly been decaying away from the moment it formed. The only primordial elements rarer than thorium are thulium, lutetium, tantalum, and rhenium, the odd-numbered elements just before the third peak of r-process abundances around the heavy platinum group metals, as well as uranium. Neutron capture by nuclides beyond \"A\"= 209 often results in nuclear fission instead of neutron absorption, reducing the fraction of nuclei that cross the gap of instability past bismuth to become actinides such as thorium. In the distant past the abundances of thorium and uranium were enriched by the decay of plutonium and curium isotopes, and thorium was enriched relative to uranium by the decay of U to Th and the natural depletion of U, but these sources have long since decayed and no longer contribute.\n\nIn the Earth's crust, thorium is much more abundant: with an abundance of 8.1 parts per million (ppm), it is one of the most abundant of the heavy elements, almost as abundant as lead (13 ppm) and more abundant than tin (2.1 ppm). This is because thorium is likely to form oxide minerals that do not sink into the core; it is classified as a lithophile. Common thorium compounds are also poorly soluble in water. Thus, even though the Earth contains the same abundances of the elements as the Solar System as a whole, there is more accessible thorium than heavy platinum group metals in the crust.\n\nNatural thorium is usually almost pure Th, which is the longest-lived and most stable isotope of thorium, having a half-life comparable to the age of the universe. Its radioactive decay is the largest single contributor to the Earth's internal heat; the other major contributors are the shorter-lived primordial radionuclides, which are U, K, and U in descending order of their contribution. (At the time of the Earth's formation, K and U contributed much more by virtue of their short half-lives, but they have decayed more quickly, leaving the contribution from Th and U predominant.) Its decay accounts for a gradual decrease of thorium content of the Earth: the planet currently has around 85% of the amount present at the formation of the Earth. The other natural thorium isotopes are much shorter-lived; of them, only Th is usually detectable, occurring in secular equilibrium with its parent U, and making up at most 0.04% of natural thorium.\n\nThorium only occurs as a minor constituent of most minerals, and was for this reason previously thought to be rare. Soil normally contains about 6 ppm of thorium.\n\nIn nature, thorium occurs in the +4 oxidation state, together with uranium(IV), zirconium(IV), hafnium(IV), and cerium(IV), and also with scandium, yttrium, and the trivalent lanthanides which have similar ionic radii. Because of thorium's radioactivity, minerals containing it are often metamict (amorphous), their crystal structure having been damaged by the alpha radiation produced by thorium. An extreme example is ekanite, (Ca,Fe,Pb)(Th,U)SiO, which almost never occurs in nonmetamict form due to the thorium it contains.\n\nMonazite (chiefly phosphates of various rare-earth elements) is the most important commercial source of thorium because it occurs in large deposits worldwide, principally in India, South Africa, Brazil, Australia, and Malaysia. It contains around 2.5% thorium on average, although some deposits may contain up to 20%. Monazite is a chemically unreactive mineral that is found as yellow or brown sand; its low reactivity makes it difficult to extract thorium from it. Allanite (chiefly silicates-hydroxides of various metals) can have 0.1–2% thorium and zircon (chiefly zirconium silicate, ZrSiO) up to 0.4% thorium.\n\nThorium dioxide occurs as the rare mineral thorianite. Due to its being isotypic with uranium dioxide, these two common actinide dioxides can form solid-state solutions and the name of the mineral changes according to the ThO content. Thorite (chiefly thorium silicate, ThSiO), also has a high thorium content and is the mineral in which thorium was first discovered. In thorium silicate minerals, the Th and ions are often replaced with M (where M= Sc, Y, or Ln) and phosphate () ions respectively. Because of the great insolubility of thorium dioxide, thorium does not usually spread quickly through the environment when released. The Th ion is soluble, especially in acidic soils, and in such conditions the thorium concentration can reach 40 ppm.\n\nIn 1815, the Swedish chemist Jöns Jacob Berzelius analysed an unusual sample of gadolinite from a copper mine in Falun, central Sweden. He noted impregnated traces of a white mineral, which he cautiously assumed to be an earth (oxide in modern chemical nomenclature) of an unknown element. Berzelius had already discovered two elements, cerium and selenium, but he had made a public mistake once, announcing a new element, \"gahnium\", that turned out to be zinc oxide. Berzelius privately named the putative element \"thorium\" in 1817 and its supposed oxide \"thorina\" after Thor, the Norse god of thunder. In 1824, after more deposits of the same mineral in Vest-Agder, Norway, were discovered, he retracted his findings, as the mineral (later named xenotime) proved to be mostly yttrium orthophosphate.\n\nIn 1828, Morten Thrane Esmark found a black mineral on Løvøya island, Telemark county, Norway. He was a Norwegian priest and amateur mineralogist who studied the minerals in Telemark, where he served as vicar. He commonly sent the most interesting specimens, such as this one, to his father, Jens Esmark, a noted mineralogist and professor of mineralogy and geology at the Royal Frederick University in Christiania (today called Oslo). The elder Esmark determined that it was not a known mineral and sent a sample to Berzelius for examination. Berzelius determined that it contained a new element. He published his findings in 1829, having isolated an impure sample by reducing KThF with potassium metal. Berzelius reused the name of the previous supposed element discovery and named the source mineral thorite.\n\nBerzelius made some initial characterisations of the new metal and its chemical compounds: he correctly determined that the thorium–oxygen mass ratio of thorium oxide was 7.5 (its actual value is close to that, ~7.3), but he assumed the new element was divalent rather than tetravalent, and so calculated that the atomic mass was 7.5 times that of oxygen (120 amu); it is actually 15 times as large. He determined that thorium was a very electropositive metal, ahead of cerium and behind zirconium in electropositivity. Metallic thorium was isolated for the first time in 1914 by Dutch entrepreneurs Dirk Lely Jr. and Lodewijk Hamburger.\n\nIn the periodic table published by Dmitri Mendeleev in 1869, thorium and the rare-earth elements were placed outside the main body of the table, at the end of each vertical period after the alkaline earth metals. This reflected the belief at that time that thorium and the rare-earth metals were divalent. With the later recognition that the rare earths were mostly trivalent and thorium was tetravalent, Mendeleev moved cerium and thorium to group IV in 1871, which also contained the modern carbon group (group 14) and titanium group (group 4), because their maximum oxidation state was +4. Cerium was soon removed from the main body of the table and placed in a separate lanthanide series; thorium was left with group 4 as it had similar properties to its supposed lighter congeners in that group, such as titanium and zirconium.\n\nWhile thorium was discovered in 1828 its first application dates only from 1885, when Austrian chemist Carl Auer von Welsbach invented the gas mantle, a portable source of light which produces light from the incandescence of thorium oxide when heated by burning gaseous fuels. Many applications were subsequently found for thorium and its compounds, including ceramics, carbon arc lamps, heat-resistant crucibles, and as catalysts for industrial chemical reactions such as the oxidation of ammonia to nitric acid.\n\nThorium was first observed to be radioactive in 1898, by the German chemist Gerhard Carl Schmidt and later that year, independently, by the Polish-French physicist Marie Curie. It was the second element that was found to be radioactive, after the 1896 discovery of radioactivity in uranium by French physicist Henri Becquerel. Starting from 1899, the British physicist Ernest Rutherford and the American electrical engineer Robert Bowie Owens studied the radiation from thorium; initial observations showed that it varied significantly. It was determined that these variations came from a short-lived gaseous daughter of thorium, which they found to be a new element. This element is now named radon, the only one of the rare radioelements to be discovered in nature as a daughter of thorium rather than uranium.\n\nAfter accounting for the contribution of radon, Rutherford, now working with the British physicist Frederick Soddy, showed how thorium decayed at a fixed rate over time into a series of other elements in work dating from 1900 to 1903. This observation led to the identification of the half-life as one of the outcomes of the alpha particle experiments that led to the disintegration theory of radioactivity. The biological effect of radiation was discovered in 1903. The newly discovered phenomenon of radioactivity excited scientists and the general public alike. In the 1920s, thorium's radioactivity was promoted as a cure for rheumatism, diabetes, and sexual impotence. In 1932, most of these uses were banned in the United States after a federal investigation into the health effects of radioactivity. 10,000 individuals in the United States had been injected thorium during X-ray diagnosis; they were later found to suffer health issues such as leukaemia and abnormal chromosomes. Public interest in radioactivity had declined by the end of the 1930s.\n\nUp to the late 19th century, chemists unanimously agreed that thorium and uranium were analogous to hafnium and tungsten; the existence of the lanthanides in the sixth row was considered to be a one-off fluke. In 1892, British chemist Henry Bassett postulated a second extra-long periodic table row to accommodate known and undiscovered elements, considering thorium and uranium to be analogous to the lanthanides. In 1913, Danish physicist Niels Bohr published a theoretical model of the atom and its electron orbitals, which soon gathered wide acceptance. The model indicated that the seventh row of the periodic table should also have f-shells filling before the d-shells that were filled in the transition elements, like the sixth row with the lanthanides preceding the 5d transition metals. The existence of a second inner transition series, in the form of the actinides, was not accepted until similarities with the electron structures of the lanthanides had been established; Bohr suggested that the filling of the 5f orbitals may be delayed to after uranium.\n\nIt was only with the discovery of the first transuranic elements, which from plutonium onward have dominant +3 and +4 oxidation states like the lanthanides, that it was realised that the actinides were indeed filling f-orbitals rather than d-orbitals, with the transition-metal-like chemistry of the early actinides being the exception and not the rule. In 1945, when American physicist Glenn T. Seaborg and his team had discovered the transuranic elements americium and curium, he realised that thorium was the second member of the actinide series and was filling an f-block row, instead of being the heavier congener of hafnium filling a fourth d-block row.\n\nIn the 1990s, most applications that do not depend on thorium's radioactivity declined quickly due to safety and environmental concerns as suitable safer replacements were found. Despite its radioactivity, the element has remained in use for applications where no suitable alternatives could be found. A 1981 study by the Oak Ridge National Laboratory in the United States estimated that using a thorium mantle every weekend would be safe for a person, but this was not the case for the dose received by people manufacturing the mantles or for the soils around some factory sites. Some manufacturers have changed to other materials, such as yttrium. As recently as 2007, some companies continued to manufacture and sell thorium mantles without giving adequate information about their radioactivity, with some even falsely claiming them to be non-radioactive.\n\nThorium has been used as a power source. The earliest thorium-based reactor was built at the Indian Point Energy Center in the United States in 1962. India has one of the largest supplies of thorium in the world and not much uranium, and in the 1950s targeted achieving energy independence with their three-stage nuclear power programme. In most countries, uranium was relatively abundant and the progress of thorium-based reactors was slow; in the 20th century, three reactors were built in India and twelve elsewhere. Large-scale research was begun in 1996 by the International Atomic Energy Agency to study the use of thorium reactors; a year later, the United States Department of Energy started their research. Alvin Radkowsky of Tel Aviv University in Israel was the head designer of Shippingport Atomic Power Station in Pennsylvania, the first American civilian reactor to breed thorium. He founded a consortium to develop thorium reactors, which included other laboratories: Raytheon Nuclear Inc. and Brookhaven National Laboratory in the United States, and the Kurchatov Institute in Russia. In the 21st century, thorium's potential for reducing nuclear proliferation and its waste characteristics led to renewed interest in the thorium fuel cycle.\n\nDuring the Cold War the United States explored the possibility of using Th as a source of U to be used in a nuclear bomb; they fired a test bomb in 1955. They concluded that a U-fired bomb would be a very potent weapon, but it bore few sustainable \"technical advantages\" over the contemporary uranium–plutonium bombs, especially since U is difficult to produce isotopically pure.\n\nThe low demand makes working mines for extraction of thorium alone not profitable, and it is almost always extracted with the rare earths, which themselves may be by-products of production of other minerals. The current reliance on monazite for production is due to thorium being largely produced as a by-product; other sources such as thorite contain more thorium and could easily be used for production if demand rose. Present knowledge of the distribution of thorium resources is poor, as low demand has led to exploration efforts being relatively minor. In 2014, world production of the monazite concentrate, from which thorium would be extracted, was 2,700 tonnes.\n\nThe common production route of thorium constitutes concentration of thorium minerals; extraction of thorium from the concentrate; purification of thorium; and (optionally) conversion to compounds, such as thorium dioxide.\n\nThere are two categories of thorium minerals for thorium extraction: primary and secondary. Primary deposits occur in acidic granitic magmas and pegmatites. They are concentrated, but of small size. Secondary deposits occur at the mouths of rivers in granitic mountain regions. In these deposits, thorium is enriched along with other heavy minerals. Initial concentration varies with the type of deposit.\n\nFor the primary deposits, the source pegmatites, which are usually obtained by mining, are divided into small parts and then undergo flotation. Alkaline earth metal carbonates may be removed after reaction with hydrogen chloride; then follow thickening, filtration, and calcination. The result is a concentrate with rare-earth content of up to 90%. Secondary materials (such as coastal sands) undergo gravity separation. Magnetic separation follows, with a series of magnets of increasing strength. Monazite obtained by this method can be as pure as 98%.\n\nIndustrial production in the 20th century relied on treatment with hot, concentrated sulfuric acid in cast iron vessels, followed by selective precipitation by dilution with water, as on the subsequent steps. This method relied on the specifics of the technique and the concentrate grain size; many alternatives have been proposed, but only one has proven effective economically: alkaline digestion with hot sodium hydroxide solution. This is more expensive than the original method but yields a higher purity of thorium; in particular, it removes phosphates from the concentrate.\n\nAcid digestion is a two-stage process, involving the use of up to 93% sulfuric acid at 210–230 °C. First, 60% sulfuric acid is added, thickening the reaction mixture as products are formed. Then, fuming sulfuric acid is added and the mixture is kept at the same temperature for another five hours to reduce the volume of solution remaining after dilution. The concentration of the sulfuric acid is selected based on reaction rate and viscosity, which both increase with concentration, albeit with viscosity retarding the reaction. Increasing the temperature also speeds up the reaction, but temperatures of 300 °C and above must be avoided, because they cause insoluble thorium pyrophosphate to form. Since dissolution is very exothermic, the monazite sand cannot be added to the acid too quickly. Conversely, at temperatures below 200 °C the reaction does not go fast enough for the process to be practical. To ensure that no precipitates form to block the reactive monazite surface, the mass of acid used must be twice that of the sand, instead of the 60% that would be expected from stoichiometry. The mixture is then cooled to 70 °C and diluted with ten times its volume of cold water, so that any remaining monazite sinks to the bottom while the rare earths and thorium remain in solution. Thorium may then be separated by precipitating it as the phosphate at pH 1.3, since the rare earths do not precipitate until pH 2.\n\nAlkaline digestion is carried out in 30–45% sodium hydroxide solution at about 140 °C for about three hours. Too high a temperature leads to the formation of poorly soluble thorium oxide and an excess of uranium in the filtrate, and too low a concentration of alkali leads to a very slow reaction. These reaction conditions are rather mild and require monazite sand with a particle size under 45 μm. Following filtration, the filter cake includes thorium and the rare earths as their hydroxides, uranium as sodium diuranate, and phosphate as trisodium phosphate. This crystallises trisodium phosphate decahydrate when cooled below 60 °C; uranium impurities in this product increase with the amount of silicon dioxide in the reaction mixture, necessitating recrystallisation before commercial use. The hydroxides are dissolved at 80 °C in 37% hydrochloric acid. Filtration of the remaining precipitates followed by addition of 47% sodium hydroxide results in the precipitation of thorium and uranium at about pH 5.8. Complete drying of the precipitate must be avoided, as air may oxidise cerium from the +3 to the +4 oxidation state, and the cerium(IV) formed can liberate free chlorine from the hydrochloric acid. The rare earths again precipitate out at higher pH. The precipitates are neutralised by the original sodium hydroxide solution, although most of the phosphate must first be removed to avoid precipitating rare-earth phosphates. Solvent extraction may also be used to separate out the thorium and uranium, by dissolving the resultant filter cake in nitric acid. The presence of titanium hydroxide is deleterious as it binds thorium and prevents it from dissolving fully.\n\nHigh thorium concentrations are needed in nuclear applications. In particular, concentrations of atoms with high neutron capture cross-sections must be very low (for example, gadolinium concentrations must be lower than one part per million by weight). Previously, repeated dissolution and recrystallisation was used to achieve high purity. Today, liquid solvent extraction procedures involving selective complexation of Th are used. For example, following alkaline digestion and the removal of phosphate, the resulting nitrato complexes of thorium, uranium, and the rare earths can be separated by extraction with tributyl phosphate in kerosene.\n\nNon-radioactivity-related uses have been in decline since the 1950s due to environmental concerns largely stemming from the radioactivity of thorium and its decay products.\n\nMost thorium applications use its dioxide (sometimes called \"thoria\" in the industry), rather than the metal. This compound has a melting point of 3300 °C (6000 °F), the highest of all known oxides; only a few substances have higher melting points. This helps the compound remain solid in a flame, and it considerably increases the brightness of the flame; this is the main reason thorium is used in gas mantles. All substances emit energy (glow) at high temperatures, but the light emitted by thorium is nearly all in the visible spectrum, hence the brightness of thorium mantles. Energy, some of it in the form of visible light, is emitted when thorium is exposed to a source of energy itself, such as a cathode ray, heat or ultraviolet light. This effect is shared by cerium dioxide, which converts ultraviolet light into visible light more efficiently, but thorium dioxide gives a higher flame temperature, emitting less infrared light. Thorium in mantles, though still common, has been progressively replaced with yttrium since the late 1990s. According to the 2005 review by the United Kingdom's National Radiological Protection Board, \"although [thoriated gas mantles] were widely available a few years ago, they are not any more.\"\n\nDuring the production of incandescent filaments, recrystallisation of tungsten is signifiantly lowered by adding small amounts of thorium dioxide to the tungsten sintering powder before drawing the filaments. A small addition of thorium to tungsten thermocathodes considerably reduces the work function of electrons; as a result, electrons are emitted at considerably lower temperatures. Tungsten forms a one-atom-thick layer on the surface of thorium. The work function from a thorium surface is lowered possibly because of the electric field on the interface between thorium and tungsten formed due to thorium's greater electropositivity. Since the 1920s, thoriated tungsten wires have been used in electronic tubes and in the cathodes and anticathodes of X-ray tubes and rectifiers. Thanks to the reactivity of thorium with atmospheric oxygen and nitrogen, thorium also marks impurities in the evacuated tubes. The introduction of transistors in the 1950s significantly diminished this use, but not entirely. Thorium dioxide is used in gas tungsten arc welding (GTAW) to increase the high-temperature strength of tungsten electrodes and improve arc stability. Thorium oxide is being replaced in this use with other oxides, such as those of zirconium, cerium, and lanthanum.\n\nThorium dioxide is found in heat-resistant ceramics, such as high-temperature laboratory crucibles, either as the primary ingredient or as an addition to zirconium dioxide. An alloy of 90% platinum and 10% thorium is an effective catalyst for oxidising ammonia to nitrogen oxides, but this has been replaced by an alloy of 95% platinum and 5% rhodium because of its better mechanical properties and greater durability.\n\nWhen added to glass, thorium dioxide helps increase its refractive index and decrease dispersion. Such glass finds application in high-quality lenses for cameras and scientific instruments. The radiation from these lenses can darken them and turn them yellow over a period of years and degrade film, but the health risks are minimal. Yellowed lenses may be restored to their original colourless state by lengthy exposure to intense ultraviolet radiation. Thorium dioxide has since been replaced by rare-earth oxides in this application, as they provide similar effects and are not radioactive.\n\nThorium tetrafluoride is used as an antireflection material in multilayered optical coatings. It is transparent to electromagnetic waves having wavelengths in the range of 0.35–12 µm, a range that includes near ultraviolet, visible and mid infrared light. Its radiation is primarily due to alpha particles, which can be easily stopped by a thin cover layer of another material. Replacements for thorium tetrafluoride are being developed as of the 2010s.\n\nThe main nuclear power source in a reactor is the neutron-induced fission of a nuclide; the synthetic fissile nuclei U and Pu can be bred from neutron capture by the naturally occurring quantity nuclides Th and U. U occurs naturally and is also fissile. In the thorium fuel cycle, the fertile isotope Th is bombarded by slow neutrons, undergoing neutron capture to become Th, which undergoes two consecutive beta decays to become first Pa and then the fissile U:\n\nU is fissile and can be used as a nuclear fuel in the same way as U or Pu. When U undergoes nuclear fission, the neutrons emitted can strike further Th nuclei, continuing the cycle. This parallels the uranium fuel cycle in fast breeder reactors where U undergoes neutron capture to become U, beta decaying to first Np and then fissile Pu.\n\nThorium is more abundant than uranium and can satisfy world energy demands for longer.\n\nTh absorbs neutrons more readily than U, and U has a higher probability of fission upon neutron capture (92.0%) than U (85.5%) or Pu (73.5%). It also releases more neutrons upon fission on average. A single neutron capture by U produces transuranic waste along with the fissile Pu, but Th only produces this waste after five captures, forming Np. This number of captures does not happen for 98–99% of the Th nuclei because the intermediate products U or U undergo fission, and fewer long-lived transuranics are produced. Because of this, thorium is a potentially attractive alternative to uranium in mixed oxide fuels to minimise the generation of transuranics and maximise the destruction of plutonium.\n\nThorium fuels also result in a safer and better-performing reactor core because thorium dioxide has a higher melting point, higher thermal conductivity, and a lower coefficient of thermal expansion and is more stable chemically than the now-common fuel uranium dioxide, which can further oxidise to triuranium octoxide (UO).\n\nThe used fuel is difficult and dangerous to reprocess because many of the daughters of Th and U are strong gamma emitters. All U production methods result in impurities of U, either from parasitic knock-out (n,2n) reactions on Th, Pa, or U that result in the loss of a neutron, or from double neutron capture of Th, an impurity in natural Th:\n\nU by itself is not particularly harmful, but quickly decays to produce the strong gamma emitter Tl. (Th follows the same decay chain, but its much longer half-life means that the quantities of Tl produced are negligible.) These impurities of U make U easy to detect and dangerous to work on, and the impracticality of their separation limits the possibilities of nuclear proliferation using U as the fissile material. Pa has a relatively long half-life of 27 days and a high cross section for neutron capture. Thus it is a neutron poison: instead of rapidly decaying to the useful U, a significant amount of Pa converts to U and consumes neutrons, degrading the reactor efficiency. To avoid this, Pa is extracted from the active zone of thorium molten salt reactors during their operation, so that it only decays to U.\n\nThe irradiation of Th with neutrons, followed by its processing, need to be mastered before these advantages can be realised, and this requires more advanced technology than the uranium and plutonium fuel cycle; research continues in this area. Others cite the low commercial viability of the thorium fuel cycle: the international Nuclear Energy Agency predicts that the thorium cycle will never be commercially viable while uranium is available in abundance—a situation which may persist \"in the coming decades\". The isotopes produced in the thorium fuel cycle are mostly not transuranic, but some of them are still very dangerous, such as Pa, which has a half-life of 32,760 years and is a major contributor to the long-term radiotoxicity of spent nuclear fuel.\n\nNatural thorium decays very slowly compared to many other radioactive materials, and the emitted alpha radiation cannot penetrate human skin. As a result, handling small amounts of thorium, such as those in gas mantles, is considered safe, although the use of such items may pose some risks. Exposure to an aerosol of thorium, such as contaminated dust, can lead to increased risk of cancers of the lung, pancreas, and blood, as lungs and other internal organs can be penetrated by alpha radiation. Exposure to thorium internally leads to increased risk of liver diseases.\n\nThe decay products of Th include more dangerous radionuclides such as radium and radon. Although relatively little of those products are created as the result of the slow decay of thorium, a proper assessment of the radiological toxicity of Th must include the contribution of its daughters, some of which are dangerous gamma emitters, and which are built up quickly following the initial decay of Th due to the absence of long-lived nuclides along the decay chain. As the dangerous daughters of thorium have much lower melting points than thorium dioxide, they are volatilised every time the mantle is heated for use. In the first hour of use large fractions of the thorium daughters Ra, Ra, Pb, and Bi are released. Most of the radiation dose by a normal user arises from inhaling the radium, resulting in a radiation dose of up to 0.2 millisieverts per use, about a third of the dose sustained during a mammogram.\n\nSome nuclear safety agencies make recommendations about the use of thorium mantles and have raised safety concerns regarding their manufacture and disposal; the radiation dose from one mantle is not a serious problem, but that from many mantles gathered together in factories or landfills is.\n\nThorium is odourless and tasteless. The chemical toxicity of thorium is low because thorium and its most common compounds (mostly the dioxide) are poorly soluble in water, precipitating out before entering the body as the hydroxide. Some thorium compounds are chemically moderately toxic, especially in the presence of strong complex-forming ions such as citrate that carry the thorium into the body in soluble form. If a thorium-containing object has been chewed or sucked, it loses 0.4% of thorium and 90% of its dangerous daughters to the body. Three quarters of the thorium that has penetrated the body accumulates in the skeleton. Absorption through the skin is possible, but is not a likely means of exposure. Thorium's low solubility in water also means that excretion of thorium by the kidneys and faeces is rather slow.\n\nTests on the thorium uptake of workers involved in monazite processing showed thorium levels above recommended limits in their bodies, but no adverse effects on health were found at those moderately low concentrations. No chemical toxicity has yet been observed in the tracheobronchial tract and the lungs from exposure to thorium. People who work with thorium compounds are at a risk of dermatitis. It can take as much as thirty years after the ingestion of thorium for symptoms to manifest themselves. Thorium has no known biological role.\n\nPowdered thorium metal is pyrophoric: it ignites spontaneously in air. In 1964, the United States Department of the Interior listed thorium as \"severe\" on a table entitled \"Ignition and explosibility of metal powders\". Its ignition temperature was given as 270 °C (520 °F) for dust clouds and 280 °C (535 °F) for layers. Its minimum explosive concentration was listed as 0.075 oz/cu ft (0.075 kg/m); the minimum igniting energy for (non-submicron) dust was listed as 5 mJ.\n\nIn 1956, the Sylvania Electric Products explosion occurred during reprocessing and burning of thorium sludge in New York City, United States. Nine people were injured; one died of complications caused by third-degree burns.\n\nThorium exists in very small quantities everywhere on Earth although larger amounts exist in certain parts: the average human contains about 40 micrograms of thorium and typically consumes three micrograms per day. Most thorium exposure occurs through dust inhalation; some thorium comes with food and water, but because of its low solubility, this exposure is negligible.\n\nExposure is raised for people who live near thorium deposits or radioactive waste disposal sites, those who live near or work in uranium, phosphate, or tin processing factories, and for those who work in gas mantle production. Thorium is especially common in the Tamil Nadu coastal areas of India, where residents may be exposed to a naturally occurring radiation dose ten times higher than the worldwide average. It is also common in northern Brazilian coastal areas, from south Bahia to Guarapari, a city with highly radioactive monazite sand beaches, with radiation levels up to 50 times higher than world average background radiation.\n\n"}
{"id": "8660043", "url": "https://en.wikipedia.org/wiki?curid=8660043", "title": "Undark", "text": "Undark\n\nUndark was a trade name for luminous paint made with a mixture of radioactive radium and zinc sulfide, as produced by the U.S. Radium Corporation between 1917 and 1938. It was used primarily in watch and clock dials. The people working in the industry who applied the radioactive paint became known as the Radium Girls, because many of them became ill and some died from exposure to the radiation emitted by the radium contained within the product. The product was the direct cause of radium jaw in the dial painters. Undark was also available as a kit for general consumer use and marketed as glow-in-the-dark paint.\n\nMixtures similar to Undark, consisting of radium and zinc sulphide were used by other companies. Trade names include:\nand\n\n\n\n"}
{"id": "8289092", "url": "https://en.wikipedia.org/wiki?curid=8289092", "title": "Vaccenic acid", "text": "Vaccenic acid\n\nVaccenic acid, also known as (\"E\")-octadec-11-enoic acid is a naturally occurring trans-fatty acid found in the fat of ruminants and in dairy products such as milk, butter, and yogurt. It is also the predominant fatty acid comprising trans fat in human milk.\n\nIts IUPAC name is (\"E\")-11-octadecenoic acid, and its lipid shorthand name is 18:1 \"trans\"-11. The name was derived from the Latin \"vacca\" (cow).\n\nVaccenic acid was discovered in 1928 in animal fats and butter. It is the main \"trans\" fatty acid isomer present in milk fat. \nMammals convert it into rumenic acid, a conjugated linoleic acid,\nwhere it shows anticarcinogenic properties.\n\nIts stereoisomer, \"cis\"-vaccenic acid, is an omega-7 fatty acid, is found in Sea Buckthorn (\"Hippophae rhamnoides\") oil. Its IUPAC name is (\"Z\")-11-octadecenoic acid, and its lipid shorthand name is 18:1 \"cis\"-11.\n\nA 2008 study at the University of Alberta suggests that vaccenic acid feeding in rats over 16 weeks resulted in lowered total cholesterol, lowered LDL cholesterol and lower triglyceride levels. The researchers are preparing to conduct further research, including human clinical trials.\n\nVaccenic acid is also found in human orbitofrontal cortex of patients with bipolar disorder and schizophrenia.\n\nAlkaline phosphatase inhibited 25% by vaccenic acid in osteoblasts.\n\nOxidation of omega-7 unsaturated fatty acids on the skin surface, such as palmitoleic acid and vaccenic acid, may be the cause of the phenomenon commonly known as old person smell.\n"}
{"id": "3164096", "url": "https://en.wikipedia.org/wiki?curid=3164096", "title": "Vanadium carbide", "text": "Vanadium carbide\n\nVanadium carbide is the inorganic compound with the formula VC. It is an extremely hard refractory ceramic material. With a hardness of 9-9.5 Mohs, it is possibly the hardest metal-carbide known. It is of interest because it is prevalent in vanadium metal and alloys.\n\nBeing isomorphous with vanadium monoxide, it crystallizes in the rock salt structure. Because VC and VO are miscible, samples of VC typically contain an impurity of the oxide. It is produced by heating vanadium oxides with carbon at around 1000 °C. Vanadium carbide can be formed in the (111) orientation, when formed by radio frequency magnetron sputtering. Although VC is thermodynamically stable, it converts to VC at higher temperatures.\n\nVanadium carbide is used as an additive to tungsten carbide to refine the carbide crystals to improve the property of the cermet.\n\nVanadium Carbide has an elastic modulus of approximately 380 GPa.\n"}
{"id": "32505", "url": "https://en.wikipedia.org/wiki?curid=32505", "title": "Vapor", "text": "Vapor\n\nIn physics a vapor (American) or vapour (British and Canadian) is a substance in the gas phase at a temperature lower than its critical temperature, which means that the vapor can be condensed to a liquid by increasing the pressure on it without reducing the temperature. A vapor is different from an aerosol. An aerosol is a suspension of tiny particles of liquid, solid, or both within a gas.\n\nFor example, water has a critical temperature of , which is the highest temperature at which liquid water can exist. In the atmosphere at ordinary temperatures, therefore, gaseous water (known as water vapor) will condense into a liquid if its partial pressure is increased sufficiently.\n\nA vapor may co-exist with a liquid (or a solid). When this is true, the two phases will be in equilibrium, and the gas-partial pressure will be equal to the equilibrium vapor pressure of the liquid (or solid).\n\n\"Vapor\" refers to a gas phase at a temperature where the same substance can also exist in the liquid or solid state, below the critical temperature of the substance. (For example, water has a critical temperature of 374 °C (647 K), which is the highest temperature at which liquid water can exist.) If the vapor is in contact with a liquid or solid phase, the two phases will be in a state of equilibrium. The term \"gas\" refers to a compressible fluid phase. Fixed gases are gases for which no liquid or solid can form at the temperature of the gas, such as air at typical ambient temperatures. A liquid or solid does not have to boil to release a vapor.\n\nVapor is responsible for the familiar processes of cloud formation and condensation. It is commonly employed to carry out the physical processes of distillation and headspace extraction from a liquid sample prior to gas chromatography.\n\nThe constituent molecules of a vapor possess vibrational, rotational, and translational motion. These motions are considered in the kinetic theory of gases.\n\nThe vapor pressure is the equilibrium pressure from a liquid or a solid at a specific temperature. The equilibrium vapor pressure of a liquid or solid is not affected by the amount of contact with the liquid or solid interface.\n\nThe normal boiling point of a liquid is the temperature at which the vapor pressure is equal to normal atmospheric pressure.\n\nFor two-phase systems (e.g., two liquid phases), the vapor pressure of the individual phases are equal. In the absence of stronger inter-species attractions between like-like or like-unlike molecules, the vapor pressure follows Raoult's law, which states that the partial pressure of each component is the product of the vapor pressure of the pure component and its mole fraction in the mixture. The total vapor pressure is the sum of the component partial pressures.\n\n\nSince it is in the gas phase, the amount of vapor present is quantified by the partial pressure of the gas. Also, vapors obey the barometric formula in a gravitational field, just as conventional atmospheric gases do.\n\n"}
{"id": "13337703", "url": "https://en.wikipedia.org/wiki?curid=13337703", "title": "Whinstone", "text": "Whinstone\n\nWhinstone is a term used in the quarrying industry to describe any hard dark-coloured rock. Examples include the igneous rocks, basalt and dolerite, as well as the sedimentary rock, chert.\n\nMassive outcrops of whinstone occur include the Pentland Hills, Scotland and the Whin Sills, England. \n\nThe name 'whin' derives from the sound it makes when struck with a hammer. It is used for road chippings and dry stone walls, but its natural angular shapes do not fit together well and are not easy to build with, and its hardness makes it a difficult material to work. A common use is in the laying of patios and driveways in its ground/by product state called Whindust.\n"}
{"id": "1731136", "url": "https://en.wikipedia.org/wiki?curid=1731136", "title": "Yttrium aluminium garnet", "text": "Yttrium aluminium garnet\n\nYttrium aluminium garnet (YAG, YAlO) is a synthetic crystalline material of the garnet group. It is also one of three phases of the yttrium-aluminium composite, the other two being yttrium aluminium monoclinic (YAM, YAlO) and yttrium aluminium perovskite (YAP, YAlO). \n\nYAG, like garnet and sapphire, has no uses as a laser medium when pure. However, after being doped with an appropriate ion, YAG is commonly used as a host material in various solid-state lasers. Rare earth elements such as neodymium and erbium can be doped into YAG as active laser ions, yielding and lasers, respectively. Cerium-doped YAG (Ce:YAG) is used as a phosphor in cathode ray tubes and white light-emitting diodes, and as a scintillator.\n\nYAG for a period was used in jewelry as a diamond and other gemstone simulant. Colored variants and their doping elements include: green (chromium), blue (cobalt), red (manganese), yellow (titanium), purple (neodymium), pink, and orange. As faceted gems they are valued (as synthetics) for their clarity, durability, high refractive index and dispersion. The critical angle of YAG is 33 degrees. YAG cuts like natural garnet, with polishing being performed with alumina or diamond (50,000 or 100,000 grit) on common polishing laps. YAG has low heat sensitivity.\n\nAs a synthetic gemstone YAG has numerous varietal and trade names, as well as a number of misnomers. Synonymous names include: \"alexite\", \"amamite\", \"circolite\", \"dia-bud\", \"diamite\", \"diamogem\", \"diamonair\", \"diamone\", \"diamonique\", \"diamonite\", \"diamonte\", \"di'yag\", \"geminair\", \"gemonair\", \"kimberly\", \"Linde simulated diamond\", \"nier-gem\", \"regalair\", \"replique\", \"somerset\", \"triamond\", \"YAIG\", and \"yttrium garnet\". Production for the gem trade decreased after the introduction of synthetic cubic zirconia; there was little production. Some demand exists as synthetic garnet, and for designs where the very high refractive index of cubic zirconia is not desirable.\n\nNeodymium-doped YAG (Nd:YAG) was developed in the early 1960s, and the first working Nd:YAG laser was invented in 1964. Neodymium-YAG is the most widely used active laser medium in solid-state lasers, being used for everything from low-power continuous-wave lasers to high-power Q-switched (pulsed) lasers with power levels measured in the kilowatts. The thermal conductivity of Nd:YAG is higher and its fluorescence lifetime is about twice as long as that of Nd:YVO crystals, however it is not as efficient and is less stable, requiring more precisely controlled temperatures. The best absorption band of Nd:YAG for pumping the laser is centered at 807.5 nm, and is 1 nm wide.\n\nMost Nd:YAG lasers produce infrared light at a wavelength of 1064 nm. Light at this wavelength is rather dangerous to vision, since it can be focused by the eye's lens onto the retina, but the light is invisible and does not trigger the blink reflex. Nd:YAG lasers can also be used with frequency doubling or frequency tripling crystals, to produce green light with a wavelength of 532 nm or ultraviolet light at 355 nm, respectively.\n\nThe dopant concentration in commonly used Nd:YAG crystals usually varies between 0.5 and 1.4 molar percent. Higher dopant concentration is used for pulsed lasers; lower concentration is suitable for continuous-wave lasers. Nd:YAG is pinkish-purple, with lighter-doped rods being less intensely colored than heavier-doped ones. Since its absorption spectrum is narrow, the hue depends on the light under which it is observed.\n\nYAG doped with neodymium and chromium (Nd:Cr:YAG or Nd/Cr:YAG) has absorption characteristics which are superior to Nd:YAG. This is because energy is absorbed by the broad absorption bands of the Cr dopant and then transferred to Nd by dipole-dipole interactions. This material has been suggested for use in solar-pumped lasers, which could form part of a solar power satellite system.\n\nErbium-doped YAG (Er:YAG) is an active laser medium lasing at 2940 nm. Its absorption bands suitable for pumping are wide and located between 600 and 800 nm, allowing for efficient flashlamp pumping. The dopant concentration used is high: about 50% of the yttrium atoms are replaced. The Er:YAG laser wavelength couples well into water and body fluids, making this laser especially useful for medicine and dentistry uses; it is used for treatment of tooth enamel and in cosmetic surgery. Er:YAG is used for noninvasive monitoring of blood sugar. The mechanical properties of Er:YAG are essentially the same as Nd:YAG. Er:YAG operates at wavelengths where the threshold for eye damage is relatively high (since the light is absorbed before striking the retina), works well at room temperature, and has high slope efficiency. Er:YAG is pale green.\n\nYtterbium-doped YAG (Yb:YAG) is an active laser medium lasing at 1030 nm, with a broad, 18 nm wide absorption band at 940 nm. It is one of the most useful media for high-power diode-pumped solid state lasers. The dopant levels used range between 0.2% and 30% of replaced yttrium atoms. Yb:YAG has very low fractional heating, very high slope efficiency, and no excited-state absorption or up-conversion, high mechanical strength and high thermal conductivity. Yb:YAG can be pumped by reliable InGaAs laser diodes at 940 or 970 nm.\n\nYb:YAG is a good substitute for 1064 nm Nd:YAG in high-power applications, and its frequency-doubled 515 nm version can replace the 514 nm argon lasers.\n\nNeodymium-cerium double-doped YAG (Nd:Ce:YAG, or Nd,Ce:YAG) is an active laser medium material very similar to Nd:YAG. The added cerium atoms strongly absorb in the ultraviolet region and transfer their energy to the neodymium atoms, increasing the pumping efficiency; the result is lower thermal distortion and higher power output than Nd:YAG at the same pumping level. The lasing wavelength, 1064 nm, is the same as for Nd:YAG. The material has a good resistance to damage caused by UV from the pump source, and low lasing threshold. Usually 1.1–1.4% of Y atoms are replaced with Nd, and 0.05–0.1% with Ce.\n\nHolmium-chromium-thulium triple-doped YAG (Ho:Cr:Tm:YAG, or Ho,Cr,Tm:YAG) is an active laser medium material with high efficiency. It lases at 2080 nm and can be pumped by a flashlamp or a laser diode. It is widely used in military, medicine, and meteorology. It works well at room temperature, has high slope efficiency, and operates at a wavelength where the threshold for eye damage is relatively high. When pumped by a diode, the 785 nm band for Tm ion can be used. Other major pump bands are located between 400 and 800 nm. The dopant levels used are 0.35 atom.% Ho, 5.8 atom.% Tm, and 1.5 at.% Cr. The rods have green color, imparted by chromium(III).\n\nThulium-doped YAG (Tm:YAG) is an active laser medium that operates between 1930 and 2040 nm. It is suitable for diode pumping. A dual-mode Tm:YAG laser emits two frequencies separated by 1 GHz.\n\nChromium (IV)-doped YAG (Cr:YAG) provides a large absorption cross section in the 0.9-1.2 micrometer spectral region, which makes it an attractive choice as a passive Q-switch for Nd-doped lasers. The resulting devices are solid-state, compact and low-cost. Cr:YAG has high damage threshold, good thermal conductivity, good chemical stability, resists ultraviolet radiation, and is easily machinable. It is replacing more traditional Q-switching materials like lithium fluoride and organic dyes. The dopant levels used range between 0.5 and 3 percent (molar). Cr:YAG can be used for passive Q-switching of lasers that operate at wavelengths between 1000 and 1200 nm, such as those based on Nd:YAG, Nd:YLF, Nd:YVO, and Yb:YAG.\n\nCr:YAG can be also used as a laser gain medium itself, producing tunable lasers with outputs adjustable between 1350 and 1550 nm. The Cr:YAG laser can generate ultrashort pulses (in the femtoseconds range) when it is pumped at 1064 nm by a Nd:YAG laser.\n\nCr:YAG has been demonstrated in an application of non-linear optics as a self-pumped phase-conjugate mirror in a Nd:YAG \"loop resonator\". Such a mirror provides compensation of both phase and polarization aberrations induced into the loop resonator.\n\nDysprosium-doped YAG (Dy:YAG) is a temperature-sensitive phosphor used in temperature measurements. The phosphor is excited by a laser pulse and its temperature-dependent fluorescence is observed. Dy:YAG is sensitive in ranges of 300-1700 K. The phosphor can be applied directly to the measured surface, or to an end of an optical fiber. Furthermore, it has also been study as single-phase white emitting phosphor in phosphor-converted white light emitting diodes.\n\nSamarium-doped YAG (Sm:YAG) is a temperature-sensitive phosphor similar to Dy:YAG.\n\nTerbium-doped YAG (Tb:YAG) is a phosphor used in cathode ray tubes. It emits at yellow-green color, at 544 nm.\n\nCerium(III)-doped YAG (Ce:YAG or YAG:Ce) is a phosphor, or a scintillator when in pure single-crystal form, with wide range of uses. It emits yellow light when subjected to blue or ultraviolet light, or to x-ray light. It is used in white light-emitting diodes, as a coating on a high-brightness blue InGaN diode, converting part of the blue light into yellow, which then appears as white. Such an arrangement gives less than ideal color rendering. The output brightness decreases with increasing temperature, further altering device color output.\n\nCe:YAG is also used in some mercury-vapor lamps as one of the phosphors, often together with Eu:Y(P,V)O (yttrium phosphate-vanadate). It is also used as a phosphor in cathode ray tubes, where it emits green (530 nm) to yellow-green (550 nm) light. When excited by electrons, it has virtually no afterglow (70 ns decay time). It is suitable for use in photomultipliers.\n\nCe:YAG is used in PET scanners, high-energy gamma radiation and charged particle detectors, and high-resolution imaging screens for gamma, x-rays, beta radiation and ultraviolet radiation.\n\nCe:YAG can be further doped with gadolinium.\n\n"}
{"id": "35191608", "url": "https://en.wikipedia.org/wiki?curid=35191608", "title": "Zalău explosion", "text": "Zalău explosion\n\nThe Zalău explosion occurred on September 14, 2007 in a block of flats in Zalău, Romania as the result of a gas leak. Two people died and eight were injured in the explosion. The structure was severely affected and the block of flats E24 was demolished in October 2007. A total of 19 families were affected by the deflagration. They had been petitioning local natural gas distributor E.ON Gaz for two years before the explosion as a strong smell of gas could be sensed both inside and outside the block of flats. The gas network system had been eight years overdue before the explosion.\n\nOn September 19, 2007, hundreds of people participated in the funeral of those who died in the explosion. Because E.ON Gaz did not provide support to the affected families, they protested in front of the headquarters of the company in Târgu Mureș on September 14, 2009. In December 2010, six persons were sentenced to prison in connection with the explosion, but in December 2011 their sentences were suspended. Compensations owed to individuals ranged from 67,000 to 222,000 lei. In March 2012, the former owners started to rebuild the block of flats.\n\n"}
{"id": "40133068", "url": "https://en.wikipedia.org/wiki?curid=40133068", "title": "Çetintepe Dam", "text": "Çetintepe Dam\n\nThe Çetintepe Dam is a gravity dam under construction on the Ortasu River (a tributary of the Hezil River) in Uludere district of Şırnak Province, southeast Turkey. Under contract from Turkey's State Hydraulic Works, Ozerka Insatt began construction on the dam in 2008 and a completion date has not been announced.\n\nThe reported purpose of the dam is water storage and it can also support a 2 MW hydroelectric power station in the future. Another purpose of the dam which has been widely reported in the Turkish press is to reduce the freedom of movement of Kurdistan Workers' Party (PKK) militants. Blocking and flooding valleys in close proximity to the Iraq–Turkey border is expected to help curb cross-border PKK smuggling and deny caves in which ammunition can be stored. A total of 11 dams along the border; seven in Şırnak Province and four in Hakkâri Province were implemented for this purpose. In Şırnak they are the Silopi, Şırnak, Uludere, Balli, Kavşaktepe and Musatepe Dams downstream of the Çetintepe Dam. In Hakkari are the Gölgeliyamaç (since cancelled) and Çocuktepe Dams on the Güzeldere River and the Aslandağ and Beyyurdu Dams on the Bembo River.\n\n"}
{"id": "7730229", "url": "https://en.wikipedia.org/wiki?curid=7730229", "title": "Ébéniste", "text": "Ébéniste\n\nÉbéniste () is the French word for a cabinet-maker.\n\nAs opposed to \"ébéniste\", \"menuisier\" denotes a woodcarver or chairmaker in French. The English equivalent for \"ébéniste\", \"ebonist\", is not commonly used. Originally, an \"ébéniste\" was one who worked with ebony, a favoured luxury wood for mid-seventeenth century Parisian cabinets, originating in imitation of elite furniture being made in Antwerp. The word is 17th century in origin. Early Parisian ébénistes often came from the Low Countries themselves: an outstanding example is Pierre Golle, who worked at the \"Gobelins manufactory\" making cabinets and table tops veneered with marquetry, the traditional enrichment of \"ébénisterie\", or \"cabinet-work\".\n\nÉbénistes make case furniture, which may be veneered or painted. Under Parisian guild regulations, the application of painted varnishes, generically called \"vernis Martin\", was carried out in separate workshops, sawdust being an enemy to freshly varnished surfaces. At the outset of the French Revolution the guilds in Paris and elsewhere were abolished, and with them went all their regulations. One result of this is that Paris chairmakers were now able to produce veneered chairs, as London furniture-makers, less stringently ruled, had been able to make since the first chairs with splats had been produced shortly before 1720, in imitation of Chinese chairs.\n\nBecause of this amalgamation, chairs and other seat furniture began to use veneering techniques which were formerly the guarded privilege of ébénistes. This privilege became less distinct after the relaxation of guild rules of the Ancien Régime, and after the French Revolution's abolition of guilds in 1791. Seat furniture in the Empire style was often veneered with mahogany, and later in pale woods also.\n\nFrom the mid-nineteenth century onward, the two French trades, \"ébéniste\" and \"menuisier,\" were often assembled under the single roof of a \"furnisher\", and the craft began to make way for the industry. \n\nFrom the mid-17th century through the 18th, a notable number of ébénistes of German and Low Countries extraction were pre-eminent among Parisian furniture-makers, as the abbreviated list below suggests.\n\n\n\n\n"}
