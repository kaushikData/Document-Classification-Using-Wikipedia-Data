{"id": "17465173", "url": "https://en.wikipedia.org/wiki?curid=17465173", "title": "2008 Ijegun pipeline explosion", "text": "2008 Ijegun pipeline explosion\n\nOn 15 May 2008 a pipeline explosion occurred in the community of Ijegun, a suburb north of Lagos, Nigeria. The explosion took place after a bulldozer struck an oil pipeline. The Lagos police have stated that the explosion appears to be an accident, and not the work of thieves, as in past pipeline explosions near Lagos. Construction workers accidentally broke an underground pipeline from which fuel started to spill out; moments later an explosion occurred.\n\nThe fire started from the Isolo end of Ijegun, which spread to surrounding homes and schools. It reportedly spread through buildings with occupants inside. The Ijegun Primary School was also damaged. More than 15 homes and more than 20 vehicles were burned in the fire. Firefighters and volunteers used sand and water in attempts to extinguish the fires. Forty minutes after the fire started, NNPC (Nigerian National Petroleum Corporation) firefighters arrived on the scene. The Lagos State Fire Service later arrived.\n\nAlthough the Nigerian Emergency Management Agency puts the current death toll at ten, the Nigerian Red Cross claims at least 100 people have died. Other accounts state that the number dead is 39, including school children. AllAfrica.com reports that the death toll is up to 43. The bodies were taken to the Ikeja General Hospital. A pregnant woman and her 4-year-old son were also among the dead. The Nigerian Red Cross compiled names of the deceased and injured. They set up camps near the disaster as well. Local council and government officials claim that the actual death toll is much lower than what the Nigerian Red Cross is claiming.\nVictims were taken to a hospital and are said to be suffering from serious burns.\n\nIt's believed that when the explosion occurred, gate men at the Ijegun Primary School initially locked the gates apparently to prevent the pupils from running into danger. But when the atmosphere became tense, the gate men and other staff were said to have run for their lives leaving everyone to their fate.\n\nAt that point, students from Ijegun Comprehensive School who ran into the primary school premises were said to have jointly brought down the wall of the school to facilitate their escape. Most of them were trampled on. Eight out of sixteen pupils who were rushed to Corner Stone Hospital reportedly died on 15 May and another died a day later. Three residents were killed after being run over by vehicles whose drivers attempted to flee the fire zone.\n\n\n"}
{"id": "27881960", "url": "https://en.wikipedia.org/wiki?curid=27881960", "title": "50Hertz Transmission GmbH", "text": "50Hertz Transmission GmbH\n\n50Hertz Transmission GmbH (former name: Vattenfall Europe Transmission) is one of the four transmission system operators for electricity in Germany. It is a member of the European Network of Transmission System Operators for Electricity (ENTSO-E). It is wholly owned by Eurogrid GmbH, indirectly owned and managed by the Belgian transmission system operator Elia System Operator and Australian-based IFM Investors.\n\nThe company has headquarters in Berlin and owns the high power network in eastern Germany as well as the area around Hamburg. It operates the 220kV and 380kV networks and has about 10,200 km of power lines covering about 30% of Germany by area. The company employs 1.043 employees.\n\nOn 12 March 2010, Elia System Operator and Industry Funds Management acquired 50Hertz Transmission from Vattenfall. The deal was approved by the European Commission on 10 May 2010.\nOn 23 March 2018, Elia announced it has decided to exercise its pre-emption right and to increase its share in Eurogrid, the holding company above 50Hertz, from 60% to 80%. The transaction price amounts to € 976.5 million. Later that year, Elia announced the closing of the transactions with IFM and the German state-owned bank Kreditanstalt für Wiederaufbau (KfW) regarding a 20% stake in Eurogrid International. With the close of these transactions, KfW, on behalf of the German Federal Government, replaces IFM as shareholder in Eurogrid.\n\n"}
{"id": "40922903", "url": "https://en.wikipedia.org/wiki?curid=40922903", "title": "Airship N.S.11 crash", "text": "Airship N.S.11 crash\n\nThe Airship \"NS11\" crash was an airship accident which occurred on 15 July 1919. The Royal Air Force (RAF) airship exploded off the east coast of England over the North Sea, killing all nine crew on board.\n\n\"NS11\" was one of 14 \"North Sea\"-class airships ordered by the Royal Navy for the Royal Naval Air Service, but by the time \"NS11\" was delivered in September 1918, the Royal Naval Air Service had been amalgamated with the Royal Flying Corps to form the RAF. The airship was built and tested at RNAS Kingsnorth near Kingsnorth in Kent. She was fitted with two 260 hp (195 kW) Fiat engines and had an envelope with a capacity of cubic feet. Prior to the accident, she had made voyages of more than 1000 miles (1600 km) over the North Sea, setting a world record for non-rigid airships.\n\n\"NS11\" had taken off from RAF Pulham in Pulham St Mary, Norfolk, around midnight on the night of 14/15 July 1919 and was heading over the North Sea on a mine-hunting patrol. In the early hours of 15 July, she was seen to fly beneath a long \"greasy black cloud\" off the village of Cley next the Sea on the Norfolk coast when locals reported an abnormal noise from her engines (which may have suggested she was experiencing engine trouble). She was returning towards the coast when she exploded into a ball of flames, causing a vivid glare lasting for several minutes as the burning airship descended, plunging into the sea after a second explosion. None of the nine crew members on board the airship survived. The Sheringham lifeboat was launched but its crew could only find a small part of the aluminium wreckage.\n\nThe accident occurred less than 48 hours after the airship \"R34\" arrived at RAF Pulham after a successful double-crossing of the Atlantic Ocean, including the first-ever east-west crossing by air.\n\nThe findings of the official Court of Enquiry were inconclusive, but amongst other possibilities it was thought that a lightning strike may have caused the explosion.\n\nThere is a memorial plaque and drinking fountain in the grounds of the Viaduct Sports & Social Club in Earlestown, Merseyside. The names of the crew are commemorated on Hollybrook Memorial, Southampton. One of the crew was buried at Ann’s Hill Cemetery in Gosport.\n\n"}
{"id": "16026071", "url": "https://en.wikipedia.org/wiki?curid=16026071", "title": "Antimony regulus", "text": "Antimony regulus\n\nAntimony regulus or antimony metal is a partially purified form of the element antimony. In modern commerce, it typically contains 0.4% to 1.0% of impurities, which typically include primarily arsenic, and smaller amounts of sulfur, zinc and iron. Selenium as an impurity is rare, but for some purposes must be avoided; other problematic impurities for various applications include copper, nickel, and lead.\n\nTypical commercial antimony is unsuitable for production of solid-state-electronics devices, and for these 99.95% pure material is typically demanded.\n"}
{"id": "20611325", "url": "https://en.wikipedia.org/wiki?curid=20611325", "title": "Australia (continent)", "text": "Australia (continent)\n\nThe continent of Australia, sometimes known in technical contexts by the names Sahul, Australinea or Meganesia to distinguish it from the country of Australia, consists of the land masses which sit on Australia's continental shelf. This includes mainland Australia, Tasmania, and the island of New Guinea (comprising Papua New Guinea and two Indonesian provinces). Situated in the geographical region of Oceania, it is the smallest of the seven traditional continents in the English conception.\n\nThe continent lies on a continental shelf overlain by shallow seas which divide it into several landmasses—the Arafura Sea and Torres Strait between mainland Australia and New Guinea, and Bass Strait between mainland Australia and Tasmania. When sea levels were lower during the Pleistocene ice age, including the Last Glacial Maximum about 18,000 BC, they were connected by dry land. During the past 10,000 years, rising sea levels overflowed the lowlands and separated the continent into today's low-lying arid to semi-arid mainland and the two mountainous islands of New Guinea and Tasmania. \n\nThe Australian continent, being part of the Indo-Australian plate, is the lowest, flattest, and oldest landmass on Earth and it has had a relatively stable geological history. New Zealand is not part of the continent of Australia, but of the separate, submerged continent of Zealandia. New Zealand and Australia are both part of the Oceanian sub-region known as Australasia, with New Guinea being in Melanesia. The term Oceania is often used to denote the region encompassing the Australian continent, Zealandia and various islands in the Pacific Ocean that are not included in the seven-continent model. \n\nPapua New Guinea, a country within the continent, is one of the most culturally and linguistically diverse countries in the world. It is also one of the most rural, as only 18 \npercent of its people live in urban centres. West Papua, a province of Indonesia, is home to an estimated 44 uncontacted tribal groups. Australia, the largest landmass in the continent, is highly urbanised, and has the world's 13th-largest economy with the second-highest human development index globally. Australia also has the world's 9th largest immigrant population. The first settlers of Australia, New Guinea, and the large islands just to the east arrived between 50,000 and 30,000 years ago.\n\nArchaeological terminology for this region has changed repeatedly. Before the 1970s, the single Pleistocene landmass was called \"Australasia\", derived from the Latin \"australis\", meaning \"southern\", although this word is most often used for a wider region that includes lands like New Zealand that are not on the same continental shelf. In the early 1970s, the term \"Greater Australia\" was introduced for the Pleistocene continent. Then at a 1975 conference and consequent publication, the name \"Sahul\" was extended from its previous use for just the Sahul Shelf to cover the continent.\n\nIn 1984 W. Filewood suggested the name \"Meganesia\", meaning \"great island\" or \"great island-group\", for both the Pleistocene continent and the present-day lands, and this name has been widely accepted by biologists. Others have used \"Meganesia\" with different meanings: travel writer Paul Theroux included New Zealand in his definition and others have used it for Australia, New Zealand and Hawaii. Another biologist, Richard Dawkins, coined the name \"Australinea\" in 2004. \"Australia-New Guinea\" has also been used.\n\nWith a total land area of , the Australian continent is the smallest, and second-lowest human inhabited (after Antarctica) continent on Earth. The continental shelf connecting the islands, half of which is less than deep, covers some , including the Sahul Shelf and Bass Strait.\nAs the country of Australia is mostly on a single landmass, and comprises most of the continent, it is sometimes informally referred to as an island continent, surrounded by oceans.\n\nGeological forces such as tectonic uplift of mountain ranges or clashes between tectonic plates occurred mainly in Australia's early history, when it was still a part of Gondwana. Australia is situated in the middle of the tectonic plate, and therefore currently has no active volcanism.\n\nThe continent primarily sits on the Indo-Australian Plate. Because of its central location on its tectonic plate Australia doesn't have any active volcanic regions, the only continent with this distinction. The lands were joined with Antarctica as part of the southern supercontinent Gondwana until the plate began to drift north about 96 million years ago. For most of the time since then, Australia–New Guinea remained a continuous landmass. When the last glacial period ended in about 10,000 BC, rising sea levels formed Bass Strait, separating Tasmania from the mainland. Then between about 8,000 and 6,500 BC, the lowlands in the north were flooded by the sea, separating New Guinea, the Aru Islands, and the Australian mainland.\n\nA northern arc consisting of the New Guinea Highlands, the Raja Ampat Islands, and Halmahera was uplifted by the northward migration of Australia and subduction of the Pacific Plate. The Outer Banda Arc was accreted along the northwestern edge the continent; it includes the islands of Timor, Tanimbar, and Seram. Papua New Guinea has several volcanoes, as it is situated along the Pacific Ring of Fire. Volcanic eruptions are not rare, and the area is prone to earthquakes and tsunamis because of this. Mount Wilhelm in Papua New Guinea is the second highest mountain in the continent, and at above sea level, Puncak Jaya is the highest mountain.\n\nAmong the fungi, the remarkable association between \"Cyttaria\" \"gunnii\" (one of the \"golf-ball\" fungi) and its associated trees in the genus \"Nothofagus\" is evidence of that drift: the only other places where this association is known are New Zealand and southern Argentina and Chile. Prominent features of the Australian flora are adaptations to aridity and fire which include scleromorphy and serotiny. These adaptations are common in species from the large and well-known families Proteaceae (\"Banksia\"), Myrtaceae (\"Eucalyptus\" – gum trees), and Fabaceae (\"Acacia\" – wattle). \n\nFor about 40 million years Australia–New Guinea was almost completely isolated. During this time, the continent experienced numerous changes in climate, but the overall trend was towards greater aridity. When South America eventually separated from Antarctica, the development of the cold Antarctic Circumpolar Current changed weather patterns across the world. For Australia–New Guinea, it brought a marked intensification of the drying trend. The great inland seas and lakes dried out. Much of the long-established broad-leaf deciduous forest began to give way to the distinctive hard-leaved sclerophyllous plants that characterise the modern Australian landscape. The flora of New Guinea is a mixture of many tropical rainforest species with origins in Asia, together with typically Australasian flora. Typical Southern Hemisphere flora include the conifers \"Podocarpus\" and the rainforest emergents \"Araucaria\" and \"Agathis\", as well as tree ferns and several species of \"Eucalyptus\".\n\nFor many species, the primary refuge was the relatively cool and well-watered Great Dividing Range. Even today, pockets of remnant vegetation remain in the cool uplands, some species not much changed from the Gondwanan forms of 60 or 90 million years ago. Eventually, the Australia–New Guinea tectonic plate collided with the Eurasian plate to the north. The collision caused the northern part of the continent to buckle upwards, forming the high and rugged mountains of New Guinea and, by reverse (downwards) buckling, the Torres Strait that now separates the two main landmasses. The collision also pushed up the islands of Wallacea, which served as island 'stepping-stones' that allowed plants from Southeast Asia's rainforests to colonise New Guinea, and some plants from Australia–New Guinea to move into Southeast Asia. The ocean straits between the islands were narrow enough to allow plant dispersal, but served as an effective barrier to exchange of land mammals between Australia–New Guinea and Asia.\n\nProminent features of the Australian flora are adaptations to aridity and fire which include scleromorphy and serotiny. These adaptations are common in species from the large and well-known families Proteaceae (\"Banksia\"), Myrtaceae (\"Eucalyptus\" – gum trees), and Fabaceae (\"Acacia\" – wattle). Due to the spread of animals, fungi and plants across the single Pleistocene landmass the separate lands have a related biota. There are over 300 bird species in West Papua, of which at least 20 are unique to the ecoregion, and some live only in very restricted areas. These include the grey-banded munia, Vogelkop bowerbird, and the king bird-of-paradise. \n\nAustralia has a huge variety of animals; some 83% of mammals, 89% of reptiles, 24% of fish and insects and 93% of amphibians that inhabit the continent are endemic to Australia. This high level of endemism can be attributed to the continent's long geographic isolation, tectonic stability, and the effects of an unusual pattern of climate change on the soil and flora over geological time. Australia and its territories are home to around 800 species of bird; 45% of these are endemic to Australia. Predominant bird species in Australia include the Australian magpie, Australian raven, the pied currawong, crested pigeons and the laughing kookaburra. The koala, emu, platypus and kangaroo are national animals of Australia, and the Tasmanian devil is also one of the well-known animals in the country. The goanna is a predatory lizard native to the Australian mainland.\n\nAs the continent drifted north from Antarctica, a unique fauna, flora and mycobiota developed. Marsupials and monotremes also existed on other continents, but only in Australia–New Guinea did they out-compete the placental mammals and come to dominate. New Guinea has 284 species and six orders of mammals: monotremes, three orders of marsupials, rodents and bats; 195 of the mammal species (69%) are endemic. New Guinea has a rich diversity of coral life and 1,200 species of fish have been found. Also about 600 species of reef-building coral—the latter equal to 75 percent of the world’s known total. New Guinea has 578 species of breeding birds, of which 324 species are endemic.Bird life also flourished — in particular, the songbirds (order Passeriformes, suborder Passeri) are thought to have evolved 50 million years ago in the part of Gondwana that later became Australia, New Zealand, New Guinea, and Antarctica, before radiating into a great number of different forms and then spreading around the globe.\n\nAnimal groups such as macropods, monotremes, and cassowaries are endemic to Australia. There were three main reasons for the enormous diversity that developed in animal, fungal and plant life.\n\n\nAlthough New Guinea is the most northerly part of the continent, and could be expected to be the most tropical in climate, the altitude of the New Guinea highlands is such that a great many animals and plants that were once common across Australia–New Guinea now survive only in the tropical highlands where they are severely threatened by overpopulation pressures.\n\nIn New Guinea, the climate is mostly monsoonal (December to March), southeast monsoon (May to October), and tropical rainforest with slight seasonal temperature variation. In lower altitudes, the temperature is around 80 °F (27 °C) year round. But the higher altitudes, such as Mendi, are constantly around 70 °F (21 °C) with cool lows nearing 52 °F (11 °C), with abundant rainfall and high humidity. The New Guinea Highlands are one of the few regions close to the equator that experience snowfall, which occurs in the most elevated parts of the mainland. Some areas in the island experience an extraordinary amount of precipitation, averaging roughly of rainfall annually.\n\nThe Australian landmass's climate is mostly desert or semi-arid, with the southern coastal corners having a temperate climate, such as oceanic and humid subtropical climate in the east coast and Mediterranean climate in the west. The northern parts of the country have a tropical climate. Snow falls frequently on the highlands near the east coast, in the states of Victoria, New South Wales, Tasmania and in the Australian Capital Territory. Temperatures in Australia have ranged from above to well below . Nonetheless, minimum temperatures are moderated. The El Niño-Southern Oscillation is associated with seasonal abnormality in many areas in the world. Australia is one of the continents most affected and experiences extensive droughts alongside considerable wet periods.\n\nIndigenous Australians are the original inhabitants of the Australian continent and nearby islands who migrated from Africa to Asia around 70,000 years ago and arrived in Australia around 50,000 years ago. They are believed to be among the earliest human migrations out of Africa. There is evidence of genetic and linguistic interchange between Australians in the far north and the Austronesian peoples of modern-day New Guinea and the islands, but this may be the result of recent trade and intermarriage.\n\nIn the quest for Terra Australis, Spanish explorations in the 17th century, such as the expedition led by the Portuguese navigator Pedro Fernandes de Queirós, discovered the Pitcairn and Vanuatu archipelagos, and sailed the Torres Strait between Australia and New Guinea, named after navigator Luís Vaz de Torres.\n\nDutch navigator Willem Janszoon, made the first documented European sight and landing on the continent of Australia, in Cape York Peninsula (1606). Abel Janszoon Tasman circumnavigated and landed on parts of the Australian continental coast and discovered Van Diemen's Land (now Tasmania), New Zealand in 1642, and Fiji islands. He was the first known European explorer to reach these islands.\n\nOn 23 April 1770 British explorer James Cook made his first recorded direct observation of indigenous Australians at Brush Island near Bawley Point. On 29 April, Cook and crew made their first landfall on the mainland of the continent at a place now known as the Kurnell Peninsula. It is here that James Cook made first contact with an aboriginal tribe known as the Gweagal. His expedition became the first recorded Europeans to have encountered its eastern coastline of Australia.\n\nThe Commonwealth of Australia came into being when the Federal Constitution was proclaimed by the Governor-General, Lord Hopetoun, on 1 January 1901. From that point a system of federalism in Australia came into operation, entailing the establishment of an entirely new national government (the Commonwealth government) and an ongoing division of powers between that government and the States. With the encouragement of Queensland, in 1884, a British protectorate had been proclaimed over the southern coast of New Guinea and its adjacent islands. British New Guinea, was annexed outright in 1888. The possession was placed under the authority of the newly federated Commonwealth of Australia in 1902 and with passage of the Papua Act of 1905, British New Guinea became the Australian Territory of Papua, with formal Australian administration beginning in 1906. \n\nThe bombing of Darwin on 19 February 1942 was the largest single attack ever mounted by a foreign power on Australia. In an effort to isolate Australia, the Japanese planned a seaborne invasion of Port Moresby, in the Australian Territory of New Guinea. Between July and November 1942, Australian forces repulsed Japanese attempts on the city by way of the Kokoda Track, in the highlands of New Guinea. The Battle of Buna–Gona, between November 1942 and January 1943, set the tone for the bitter final stages of the New Guinea campaign, which persisted into 1945. The offensives in Papua and New Guinea of 1943–44 were the single largest series of connected operations ever mounted by the Australian armed forces.\n\nFollowing the 1998 commencement of reforms across Indonesia, Papua and other Indonesian provinces received greater regional autonomy. In 2001, \"Special Autonomy\" status was granted to Papua province, although to date, implementation has been partial and often criticized. The region was administered as a single province until 2003, when it was split into the provinces of Papua and West Papua. Elections in 1972 resulted in the formation of a ministry headed by Chief Minister Michael Somare, who pledged to lead the country to self-government and then to independence. Papua New Guinea became self-governing on 1 December 1973 and achieved independence on 16 September 1975. The country joined the United Nations (UN) on 10 October 1975.\n\nMigration brought large numbers of southern and central Europeans to Australia for the first time. A 1958 government leaflet assured readers that unskilled non-British migrants were needed for \"labour on rugged projects ...work which is not generally acceptable to Australians or British workers\". Australia fought on the side of Britain in the two world wars and became a long-standing ally of the United States when threatened by Imperial Japan during World War II. Trade with Asia increased and a post-war immigration program received more than 6.5 million migrants from every continent. Supported by immigration of people from more than 200 countries since the end of World War II, the population increased to more than 23 million by 2014.\n\nChristianity is the predominant religion in the continent, although large proportions of Australians belong to no religion. Other religions in the region include Islam, Buddhism and Hinduism, which are prominent minority religions in Australia. Traditional religions are often animist, found in New Guinea. Islam is prevalent in the Indonesian New Guinea. Many Papuans combine their Christian faith with traditional indigenous beliefs and practices.\n\n\"Aboriginal Australian languages\", including the large Pama–Nyungan family, \"Papuan languages\" of New Guinea and neighbouring islands, including the large Trans–New Guinea family, and \"Tasmanian languages\" are generic terms for the native languages of the continent other than those of Austronesian family. Predominant non-native languages include English in Australia, Tok Pisin in Papua New Guinea, and Indonesian (Malay) in Indonesian New Guinea. Immigration to Australia have brought overseas languages such as Italian, Greek, Arabic, Filipino, Mandarin, Vietnamese and Spanish, among others. Contact between Austronesian and Papuan resulted in several instances in mixed languages such as Maisin. Tok Pisin is an English creole language spoken in Papua New Guinea. Papua New Guinea has more languages than any other country, with over 820 indigenous languages, representing 12% of the world's total, but most have fewer than 1,000 speakers.\n\nSince 1945, more than 7 million people have settled in Australia. From the late 1970s, there was a significant increase in immigration from Asian and other non-European countries, making Australia a multicultural country. Sydney is the most multicultural city in Oceania, having more than 250 different languages spoken with about 40 percent of residents speaking a language other than English at home. Furthermore, 36 percent of the population reported having been born overseas, with top countries being Italy, Lebanon, Vietnam and Iraq, among others. Melbourne is also fairly multicultural, having the largest Greek-speaking population outside of Europe, and the second largest Asian population in Australia after Sydney.\n\nAustralia is the only first world country in the Australian-New Guinea continent, although the economy of Australia is by far the largest and most dominant economy in the region and one of the largest in the world. Australia's per-capita GDP is higher than that of the UK, Canada, Germany, and France in terms of purchasing power parity. The Australian Securities Exchange in Sydney is the largest stock exchange in Australia and in the South Pacific. In 2012, Australia was the 12th largest national economy by nominal GDP and the 19th-largest measured by PPP-adjusted GDP. Tourism in Australia is an important component of the Australian economy. In the financial year 2014/15, tourism represented 3.0% of Australia's GDP contributing A$47.5 billion to the national economy. In 2015, there were 7.4 million visitor arrivals.\n\nMercer Quality of Living Survey ranks Sydney tenth in the world in terms of quality of living, making it one of the most livable cities. It is classified as an Alpha+ World City by GaWC. Melbourne also ranked highly in the world's most liveable city list, and is a leading financial centre in the Asia-Pacific region. \n\nPapua New Guinea is rich in natural resources, which account for two thirds of their export earnings. Though PNG is filled with resources, the lack of country's development led foreign countries to take over few sites and continued foreign demand for PNG's resources and as a result, the United States constructed an oil company and began to export in 2004 and this was the largest project in PNG's history. Papua New Guinea is classified as a developing economy by the International Monetary Fund. Strong growth in Papua New Guinea's mining and resource sector led to the country becoming the sixth fastest-growing economy in the world in 2011.\n\nAustralia is a federal parliamentary constitutional monarchy with Elizabeth II at its apex as the Queen of Australia, a role that is distinct from her position as monarch of the other Commonwealth realms. The Queen is represented in Australia by the Governor-General at the federal level and by the Governors at the state level, who by convention act on the advice of her ministers. There are two major political groups that usually form government, federally and in the states: the Australian Labor Party and the Coalition which is a formal grouping of the Liberal Party and its minor partner, the National Party. Within Australian political culture, the Coalition is considered centre-right and the Labor Party is considered centre-left. \n\nPapua New Guinea is a Commonwealth realm. As such, Queen Elizabeth II is its sovereign and head of state. The constitutional convention, which prepared the draft constitution, and Australia, the outgoing metropolitan power, had thought that Papua New Guinea would not remain a monarchy. The founders, however, considered that imperial honours had a cachet. The monarch is represented by the Governor-General of Papua New Guinea, currently Bob Dadae. Papua New Guinea (and the Solomon Islands) are unusual among Commonwealth realms in that governors-general are elected by the legislature, rather than chosen by the executive branch.\n\nSince 1788, the primary influence behind Australian culture has been Anglo-Celtic Western culture, with some Indigenous influences. The divergence and evolution that has occurred in the ensuing centuries has resulted in a distinctive Australian culture. Since the mid-20th century, American popular culture has strongly influenced Australia, particularly through television and cinema. Other cultural influences come from neighbouring Asian countries, and through large-scale immigration from non-English-speaking nations. The Australian Museum in Sydney and the National Gallery of Victoria in Melbourne are the oldest and largest museums in the continent, as well as in Oceania. Sydney's New Year's Eve celebrations are the largest in the continent.\n\nIt is estimated that more than 7000 different cultural groups exist in Papua New Guinea, and most groups have their own language. Because of this diversity, in which they take pride, many different styles of cultural expression have emerged; each group has created its own expressive forms in art, performance art, weaponry, costumes and architecture. Papua New Guinea is one of the few cultures in Oceania to practice the tradition of bride price. In particular, Papua New Guinea is world-famous for carved wooden sculpture: masks, canoes, story-boards.\n\nAustralia has a tradition of Aboriginal art which is thousands of years old, the best known forms being rock art and bark painting. Evidence of Aboriginal art in Australia can be traced back at least 30,000 years. Examples of ancient Aboriginal rock artworks can be found throughout the continent – notably in national parks such as those of the UNESCO listed sites at Uluru and Kakadu National Park in the Northern Territory, but also within protected parks in urban areas such as at Ku-ring-gai Chase National Park in Sydney. Aboriginal culture includes a number of practices and ceremonies centered on a belief in the Dreamtime. Reverence for the land and oral traditions are emphasized.\n\nPopular sports in Papua New Guinea include various codes of football (rugby league, rugby union, soccer, and Australian rules football), cricket, volleyball, softball, netball, and basketball. Other Olympic sports are also gaining popularity, such as boxing and weightlifting. Rugby league is the most popular sport in Papua New Guinea (especially in the highlands), which also unofficially holds the title as the \"national sport\". The most popular sport in Australia is cricket, the most popular sport among Australian women is netball, while Australian rules football is the most popular sport in terms of spectatorship and television ratings. \n\nAustralia has hosted two Summer Olympics: Melbourne 1956 and Sydney 2000. Australia has also hosted five editions of the Commonwealth Games (Sydney 1938, Perth 1962, Brisbane 1982, Melbourne 2006, and Gold Coast 2018). In 2006 Australia joined the Asian Football Confederation and qualified for the 2010 and 2014 World Cups as an Asian entrant.\n\n"}
{"id": "24077852", "url": "https://en.wikipedia.org/wiki?curid=24077852", "title": "Bangladesh Atomic Energy Commission", "text": "Bangladesh Atomic Energy Commission\n\nBangladesh Atomic Energy Commission is a scientific research organization and regulatory body of Bangladesh. Its main objective is to promote use of atomic energy for peaceful purposes It was established on 27 February 1973., after the liberation of Bangladesh.\n\nAt first BAEC started its activities at a building for Jute Research Institute. Afterwards it was transferred to Dhaka University campus at 4 Kazi Nazrul Islam Avenue. Before 1988 it was spelled 'Bangladesh Anobik Shakti Kamishon' (Literary Meaning: Bangladesh Molecular Energy Commission) in Bangla, from then it was renamed 'Bangladesh Paramanu Shakti Kamishon' (Bangladesh Atomic Energy Commission (BAEC)). In 2006, the headquarter (HQ) of BAEC was shifted to the newly built 'Poromanu Bhaban' building at Agargaon, Shere Bangla Nagar, Dhaka which is beside the only science museum in Bangladesh. All kinds of research and development activities of BAEC are conducted under three research wings: physical sciences, biological sciences and engineering.\n\n1. Atomic Energy Centre, Dhaka,<br>\n2. Atomic Energy Research Establishment (AERE), Savar<br>\n3. Nuclear Safety and Radiation Control Division (NS&RCD)<br>\n4. Radiation Testing & Monitoring Laboratory (RTML), Chittagong<br>\n5. Beach Sand Minerals Exploitation Centre (BSMEC), Cox’s Bazar<br>\n6. Nuclear Power & Energy Division (NPED)<br>\n7. Institute of Nuclear Medicine and Ultrasound<br>\n\nBAEC has memorandum of understanding (MoU) with Rosatom to enhance cooperation between the two countries in peaceful use of nuclear energy. Main purpose of the MoU is to take necessary steps for setting up a 600-1000 MW Ruppur Nuclear Power Plant in Bangladesh.\n\n\n"}
{"id": "26411907", "url": "https://en.wikipedia.org/wiki?curid=26411907", "title": "Bear Creek Wind Power Project", "text": "Bear Creek Wind Power Project\n\nThe Bear Creek Wind Power Project is a wind farm located in Bear Creek Township near Wilkes-Barre in Luzerne County, Pennsylvania, USA, with twelve Gamesa 2.0 MW wind turbines that began commercial operation in March 2006. The wind farm has a combined total nameplate capacity of 24 MW, producing about 61.4 gigawatt-hours of electricity annually. Bear Creek usually operates at roughly a 30% capacity factor until the winter months when winds are stronger and it runs near 70 percent capacity.\n\nThe wind energy facility is located in the Poconos region of Northeastern Pennsylvania, less than 10 miles southeast of Wilkes-Barre near the village of Bear Creek. Construction of the wind farm was made possible by commitments from PPL Energy Plus to purchase the output of the project and leading wind energy customers such as the University of Pennsylvania and PEPCO Energy Services.\n\nThe wind farm is visible while heading south on the Pennsylvania Turnpike's Northeast Extension.\n\n"}
{"id": "5791186", "url": "https://en.wikipedia.org/wiki?curid=5791186", "title": "Bone density", "text": "Bone density\n\nBone density, or bone mineral density (BMD), is the amount of bone mineral in bone tissue. The concept is of mass of mineral per volume of bone (relating to density in the physics sense), although clinically it is measured by proxy according to optical density per square centimetre of bone surface upon imaging. Bone density measurement is used in clinical medicine as an indirect indicator of osteoporosis and fracture risk. It is measured by a procedure called densitometry, often performed in the radiology or nuclear medicine departments of hospitals or clinics. The measurement is painless and non-invasive and involves low radiation exposure. Measurements are most commonly made over the lumbar spine and over the upper part of the hip. The forearm may be scanned if the hip and lumbar spine are not accessible.\n\nThere is a statistical association between poor bone density and higher probability of fracture. Fractures of the legs and pelvis due to falls are a significant public health problem, especially in elderly women, leading to much medical cost, inability to live independently and even risk of death. Bone density measurements are used to screen people for osteoporosis risk and to identify those who might benefit from measures to improve bone strength.\n\nBone density tests are not necessary for people without risk factors for weak bones. Unnecessary testing is more likely to result in superfluous treatment rather than discovery of a true problem.\n\nThe following are risk factors for low bone density and primary considerations for the need for a bone density test.\n\n\nOther considerations that are related to risk of low bone density and the need for a test include smoking habits, drinking habits, the long-term use of corticosteroid drugs, and a vitamin D deficiency.\n\nFor those people who do have bone density tests, two conditions which may be detected are osteoporosis and osteopenia. The usual response to either of these indications is consultation with a physician.\n\nResults are often reported in 3 terms:\n\nWhile there are many different types of BMD tests, all are non-invasive. Most tests differ according to which bones are measured to determine the BMD result.\n\nThese tests include:\n\nDXA is currently the most widely used, but quantitative ultrasound (QUS) has been described as a more cost-effective approach to measure bone density. The DXA test works by measuring a specific bone or bones, usually the spine, hip, and wrist. The density of these bones is then compared with an average index based on age, sex, and size. The resulting comparison is used to determine risk for fractures and the stage of osteoporosis (if any) in an individual.\n\nAverage bone mineral density = BMC / W [g/cm]\n\nResults are generally scored by two measures, the T-score and the Z-score. Scores indicate the amount one's bone mineral density varies from the mean. Negative scores indicate lower bone density, and positive scores indicate higher.\n\nThe T-score is the relevant measure when screening for osteoporosis. It is the bone mineral density (BMD) at the site when compared to the young normal reference mean. It is a comparison of a patient's BMD to that of a healthy 30-year-old. The US standard is to use data for a 30-year-old of the same sex and ethnicity, but the WHO recommends using data for a 30-year-old white female for everyone. Values for 30-year-olds are used in post-menopausal women and men over age 50 because they better predict risk of future fracture. The criteria of the World Health Organization are:\n\nThe Z-score is the comparison to the age-matched normal and is usually used in cases of severe osteoporosis. This is the number of standard deviations a patient's BMD differs from the average BMD of their age, sex, and ethnicity. This value is used in premenopausal women, men under the age of 50, and in children. It is most useful when the score is less than 2 standard deviations below this normal. In this setting, it is helpful to scrutinize for coexisting illnesses or treatments that may contribute to osteoporosis such as glucocorticoid therapy, hyperparathyroidism, or alcoholism.\n\nUse of BMD has several limitations.\n"}
{"id": "298011", "url": "https://en.wikipedia.org/wiki?curid=298011", "title": "Canada–United States softwood lumber dispute", "text": "Canada–United States softwood lumber dispute\n\nThe Canada–U.S. softwood lumber dispute is one of the largest and most enduring trade disputes between both nations. This conflict arose in 1982 and its effects are still seen today. British Columbia, the major Canadian exporter of softwood lumber to the United States, was most affected, reporting losses of 9,494 direct and indirect jobs between 2004 and 2009.\n\nThe heart of the dispute is the claim that the Canadian lumber industry is unfairly subsidized by federal and provincial governments, as most timber in Canada is owned by the provincial governments. The prices charged to harvest the timber (stumpage fee) are set administratively, rather than through the competitive marketplace, the norm in the United States. In the United States, softwood lumber lots are privately owned, and the owners form an effective political lobby. The United States claims that the Canadian arrangement constitutes an unfair subsidy, and is thus subject to U.S. trade remedy laws, where foreign trade benefiting from subsidies can be subject to a countervailing duty tariff, to offset the subsidy and bring the price of the commodity back up to market rates.\n\nThe Canadian government and lumber industry dispute this assertion, based on a number of factors, including that Canadian timber is provided to such a wide range of industries, and that lack of specificity makes it ineligible to be considered a subsidy under U.S. law. Under U.S. trade remedy law, a countervailable subsidy must be specific to a particular industry. This requirement precludes imposition of countervailing duties on government programs, such as roads, that are meant to benefit a broad array of interests. Since 1982, there have been four major iterations of the dispute.\n\nThe softwood lumber industry is vital to the Canadian economy and has employed thousands of people. The forest industry has contributed to direct jobs for about 232,700 individuals. Indirectly, 289,000 people have been hired to work in other sectors that depend on Canada's forests. They include engineering, transportation, and construction. Such benefit from this industry can be seen in the nation's GDP, which added $21.2 billion in 2014. That accounted for around 1.3% of real GDP. Canada has the biggest trade surplus in relation to forest products ($21.7 billion in 2015). As the largest market, the U.S. is heavily dependent on Canada's lumber. The needs of the US outweigh the domestic supply. Canada has also been expanding rapidly into the Asian market, with China being the second-largest importer. The U.S. accounted for 69% of Canada's softwood lumber exports in 2015. This is an increased share of Canadian softwood lumber exports, which reached its lowest level in 2011, accounting for only 54%. China in that same year accounted for 21%.\n\nIn April 2006, the United States and Canada announced that they had reached a tentative settlement to end the dispute. The Softwood Lumber Agreement (SLA), which this became known as, went into full effect in October 2006. The conditions stated that the period for this agreement would last anywhere between seven and nine years. Both countries, in 2012, approved a two-year extension. Under the preliminary terms, the United States would lift countervailing and anti-dumping duties provided lumber prices continue to stay above a certain range. Below the specified range, a mixed export tax and quota regime would be implemented on imports of Canadian lumber. On Canada's part, the nation agreed to enforce regulations, such as in the form of taxes on lumber exports headed to the U.S. The provincial governments of Canada, specifically, were encouraged to make changes to their pricing systems. Such changes would allow for a non-subsidizing system. As a part of the deal, more than $5 billion in duty deposits collected would be returned. The SLA establishes a dispute settlement mechanism based around the London Court of International Arbitration (LCIA), a nongovernmental institution. Either country may initiate dispute settlement of matters arising under the SLA or implementation thereof. Hearings are to be open to the public, as are pleadings and other documents. The agreement states that hearings are to be held in either the United States or Canada (the venue is selected by the arbitration tribunal). The SLA also provides that decisions of an arbitration panel are binding on the two parties.\n\nThe beginnings of the softwood lumber dispute, commonly referred to as Lumber I, were in 1982, when the U.S. lumber industry petitioned the U.S. Department of Commerce (DoC) to impose a countervailing duty. Ultimately, the DoC found that Canada's stumpage system was not specific to any single industry and thus not countervailable. While the DoC made this claim, the United States International Trade Commission (USITC) believed that these Canadian imports did in fact hinder U.S. producers. The U.S. lumber industry chose not to appeal.\n\nThe second phase, Lumber II, began in 1986, when a U.S. lumber industry group, the Coalition for Fair Lumber Imports, petitioned the Department of Commerce. The USITC once again arrived at the conclusion that Canada's exports unfairly impacted American producers. This time, the DoC did find Canadian forest programs to be countervailable and set a preliminary duty of 15%. Before the subsidy was imposed, the United States and Canada agreed to a Memorandum of Understanding that created a phased tariff. One of the terms of the MOU was that Canada levy an export tax on lumber traveling to the United States. Provinces that were affected had the chance to reduce this tax, if they performed any action meant to counterbalance their subsidies. British Columbia had the tax removed in 1987 while Quebec had it partly lifted in 1988.\n\nLumber III started in 1991 when Canada informed the United States it was withdrawing from the Memorandum of Understanding. In response, the Department of Commerce initiated a countervailing duty investigation, resulting in the DoC imposing countervailing duties.\n\nThis time, the Department of Commerce's determination was reviewed by a binational panel organized under the Canada–U.S. Free Trade Agreement (CUSFTA), the predecessor to the North American Free Trade Agreement. Prior to the signing of NAFTA, the DoC decision would have been reviewed by the United States Court of International Trade, but under CUSFTA, Canada had the option to have it reviewed by a binational panel, and they selected that option. The panel of three Canadians and two Americans found that the DoC's determination could not be supported by substantial evidence; which was a controversial decision, because the vote was along national lines, and the majority decision was based on the concept that U.S. law required the Department of Commerce to not only establish the existence of a subsidy, but also prove that the subsidy benefited the Canadian lumber industry. The U.S. Congress subsequently amended the law to explicitly state there was no \"effects test\". Additionally, the United States claimed that two of the Canadian panelists had conflicts of interests, and brought it before an extraordinary challenge committee. Again, this committee split along national lines. Malcolm Richard Wilkey, a retired judge with the United States Court of Appeals for the District of Columbia judge, wrote a dissent claiming that the panelists had conflicts of interest, and that its decision violated many of the rules of an appellate review of agency decision-making. One of the Canadian judges found that while the panelists were remiss in their disclosure obligations, the alleged conflicts were not severe enough to warrant their recusal.\n\nIn 1996, the United States and Canada reached a five-year trade agreement, \"The Softwood Lumber Agreement\", officially ending Lumber III. Under its terms, Canadian lumber exports to the United States were limited to 14.7 billion board feet (34.7 million cubic meters) per year. However, when the agreement expired on April 2, 2001, the two countries were unable to reach consensus on a replacement agreement.\n\nThree days after the Softwood Lumber Agreements expired, the U.S. lumber industry petitioned the Department of Commerce to impose countervailing duties. In addition, the U.S. industry for the first time brought an anti-dumping claim, arguing Canadian lumber companies were also engaged in unfair price discrimination. On April 25, 2002, U.S. DoC announced it had determined subsidy and anti-dumping rates, of 18.79% and an 8.43% respectively, to give a combined rate of 27.22%, although specific companies were charged varying rates. By February 26, 2003, 15,000 workers had been laid off, primarily in British Columbia, as a result of the duties imposed by the United States.\n\nOn May 27, the World Trade Organization issued a non-binding ruling in Canada's favor with regard to U.S. anti-dumping duties. The decision was appealed to a legally binding NAFTA panel. On August 13, the panel ruled that while the Canadian lumber industry could be considered subsidized, the DoC had improperly calculated duties based on U.S. stumpage prices: there was no \"world market price\" for timber, as the DoC had asserted, and it was therefore improper for DoC to calculate duties based on U.S. timber prices rather than Canadian market conditions. It accordingly ordered DoC to reassess its method of calculating duties.\n\nTwo weeks later, a WTO panel similarly concluded that the U.S. had imposed improperly high duties on Canadian lumber. The panel agreed with the DoC's contention that provincial stumpage fees did provide a \"financial benefit\" to Canadian producers, but ruled that this benefit did not rise to the level that would constitute a subsidy, and could not justify the U.S. duties.\n\nOn January 19, 2004, the WTO Appellate Body issued a final ruling with respect to the countervailing duty determination largely in Canada's favor (WTO Dispute 257). On August 11 of that same year, the Appellate Body issued a final ruling with respect to U.S. anti-dumping duties (WTO Dispute 264). In the meantime, because of an adverse WTO decision, the USITC reopened the administrative record, pursuant to a special provision in U.S. law, and issued a new affirmative threat of injury determination in December 2004. This new determination allowed the countervailing and anti-dumping duty tariffs to remain in place.\n\nBetween June 7, 2004, and October 5, 2005, DoC submitted five revised estimates of justifiable duties to the NAFTA panel, each successively lower than the last, the last being 1.21%, but each time the NAFTA panel found errors with each one and ordered it to recalculate.\n\nOn April 15, 2005, the Canadian Minister of Trade, Jim Peterson, announced that the federal government would provide Canadian softwood lumber associations with $20 million in compensation for their legal expenses stemming from the dispute with the United States. That same year, another NAFTA Chapter 19 panel reviewed a determination made by the USITC that the U.S. lumber industry was under a threat of injury due to Canadian imports. Since the United States ceded jurisdiction to the World Trade Organization, it was necessary for the U.S. government to establish that a domestic industry was suffering injury, or faced a threat of injury, before countervailing duties can be imposed. The NAFTA panel found the USITC's determination invalid. In addition, the panel made the controversial decision to deny the USITC to reopen the administrative record, ordering the USITC to issue a negative determination based on the existing record. Unlike the panel during the Lumber III stage, this panel's decision was unanimous. However, the U.S. government challenged its decision before an extraordinary challenge committee, which, on August 10, 2005, issued a unanimous decision against the United States, finding that the NAFTA panel's determination was not sufficiently invalid to require \"vacatur\" or remand, under the standards of NAFTA.\n\nOn August 15, 2005, the United States said it would not abide by the NAFTA decision, because the Section 129 determination superseded the decision which was reviewed by the NAFTA panel. Two weeks later, on August 30, the WTO, which had previously ruled against the ITC, this time upheld their new Section 129 \"threat of injury\" ruling. In September 2005, a U.S. lumber industry associate filed suit in the U.S. Court of Appeals for the District of Columbia Circuit, challenging the constitutionality of the NAFTA Chapter 19 dispute settlement system. On November 24, 2005, the U.S. Commerce Department announced it would comply with a separate NAFTA panel's order to cut a 16 percent duty on Canadian softwood lumber imports for now. The following month, the DoC announced recalculated countervailing and anti-dumping duties on softwood, totaling 10.8 percent.\n\nIn March 2006, a NAFTA panel ruled in Canada's favor, finding that the subsidy to the Canadian lumber industry was de minimis, i.e., a subsidy of less than one percent. Under U.S. trade remedy law, countervailing duty tariffs are not imposed for de minimis subsidies. A tentative deal was reached in July 2006, in which Canada got $4 billion of the $5.3 billion it lost because of the penalties with no additional tariffs to be imposed. After initial opposition from several large Canadian lumber concerns, the Harper government, without specifying how many companies endorsed it, was confident that there would be enough support to culminate the deal. In August 2006, Prime Minister Stephen Harper brought the new deal to Parliament for discussion and a possible confidence vote. If the House of Commons had voted against the deal, it would have automatically forced a general election and annulled the deal. The Conservatives were in favor of the deal, while the New Democratic Party and the Liberal Party were against, leaving the Bloc Québécois as the deciding party.\n\nOn September 7, Bloc Québécois Leader Gilles Duceppe endorsed the softwood lumber deal, effectively neutralizing any chance of an election coming out of a non-confidence vote. Five days later, Canadian International Trade Minister David Emerson, along with U.S. counterpart Susan Schwab, officially signed the deal in Ottawa. Despite supporters' claims that it was the best deal possible, Elliott Feldman, an international and economic law specialist from the firm Baker & Hostetler in Washington, D.C. and a former director of the Canadian–American Business Council, criticized the deal as \"one-sided\" and a \"bad deal for Canada\". On September 19, 2006, the deal passed its first reading in the Canadian House of Commons with a 172–116 majority. On September 27, the Canadian Press reported that Canada did not meet an October 1 deadline imposed by itself to implement the agreement. Withdrawal of some of the 30 issues regarding the deal was the main reason for the delay on complying to the deal.\n\nOn March 30, 2007, the United States requested formal consultations with Canada to resolve concerns regarding Canada's lack of implementation of the export measures. The following month, on April 19, formal consultations took place between the two governments On August 7, the United States, pursuant to a settlement mechanism established in the 2006 Softwood Lumber Agreement (SLA), initiated arbitration in the London Court of International Arbitration (LCIA, a private body). The official request for arbitration took place on August 13. Canada responded to this request for arbitration on September 12. The next year, on January 18, the U.S. government filed a second arbitration request, this one focused on the provincial implementation programs of Ontario and Québec. Canada responded on February 18, 2008. On March 4, the LCIA ruled (in the first arbitration) that Canada was in violation of the 2006 SLA in its eastern provinces, but not in its western provinces. The panel had been made up of a Belgian arbitrator nominated by Canada, a British arbitrator named by the United States, and a panel president from Germany. On February 26, 2009, the LCIA announced its ruling in the second arbitration case: Canada was in breach of the softwood lumber agreement as a result of its failure to properly calculate quotas from January to June in 2007. The arbitration body ordered that sawmills in the provinces of Ontario, Quebec, Manitoba, and Saskatchewan pay an additional ten percent export charge (up to $68.26 million). The tribunal imposed a 30-day deadline to rectify the breach.\n\nThe Softwood Lumber Agreement expired on October 12, 2015. Canadian producers of softwood lumber now have unfettered access to the US softwood lumber market.\n\nThere are several impediments that have kept Canada and the US from negotiating a new agreement to replace the SLA. One key factor is that Canada held a federal election campaign through the late summer and fall of 2015, and any decisions of considerable magnitude had to wait until after the election. Given the acrimonious history of the US–Canada lumber trade prior to the SLA, the negotiation process will likely be lengthy.\n\nSome in Western Canada have expressed a desire to renew the SLA as it currently stands, while other are demanding revisions. Some officials in Quebec believe that the province has made the necessary changes to their forestry practices to make them exempt from any future agreement, much the same as the Maritime provinces. And some producers in Saskatchewan have expressed a desire to switch to the Option A system used in BC and Alberta, which assesses a larger tax but has no quota restrictions.\n\nCanadian ownership of US sawmills continues to climb with the count now reaching 40 mills, up from just two a decade earlier. West Fraser now owns more sawmills in the US South (16) than in Canada (13), Canfor Corp owns 11 sawmills in the South, one less than its Canadian total. Interfor owns 13 sawmills in the US – nine in the South and four in the Northwest. It owns five sawmills in Canada. The growing trend of Canadian ownership of US mills is driven by the potential for a lumber trade conflict, by timber availability and lower labour costs in the US. Amid the Mountain Pine Beetle infestation in Western Canada, these firms' ability to grow is severely diminished without looking outside of Canada.\n\nMajor industry organizations in the United States, on the other hand, do not want to renew the contract. Executive director of the U.S. Lumber Coalition Zoltan Van Heyningen has expressed his disapproval for the ongoing format of the agreement. One of the reasons for this is changing timber costs, which the U.S. believes has not been incorporated in B.C. lumber costs. The US Lumber Coalition is emphatic that, should negotiations fail, its legal standing to petition the US Commerce Department to file a new case is secure. To do so, US trade law requires support from at least 25% of the US industry and at least 50% of those in the US industry expressing a view. Van Heyningen said that unless Canadian companies can show that anti-dumping or countervailing duties would harm their US production, they would be excluded from the calculation of the 50% support level - both from the numerator and the denominator - \"thereby increasing the overall level of support in the rest of the US industry.\"\n\nIn early March 2016, Canadian Prime Minister Justin Trudeau and U.S. President Obama instructed their respective cabinet members responsible for international trade to explore all options for resolving the trade dispute. Canada's international trade minister, Chrystia Freeland, said that \"what we have committed to is to make significant, meaningful progress towards a deal—to have the structure, the key elements there a 100 days from now\".\n\nOn 12 October 2016, a one-year moratorium on trade actions since the expiration of the deal ended. As of November 2016, Freeland had formally met with U.S. Trade Representative Michael Froman twelve times in the past year. During these talks, Canada sent ten proposals and papers to the United States, which sent four to Canada.\n\nIn November 2016, CNN obtained a leaked memo from the Donald Trump transition team showing that Trump was being advised to include the softwood lumber dispute during any renegotiations of the North American Free Trade Agreement and to get more favourable terms for the United States.\n\nOn April 24, 2017, U.S. Commerce Secretary Wilbur Ross said his agency will impose new anti-subsidy tariffs averaging 20 percent on Canadian softwood lumber imports, a move that escalates a long-running trade dispute between the two countries. A Commerce Department fact sheet on the pending announcement seen by Reuters shows that West Fraser Mills will pay the highest duties at 24.12 percent, followed by Canfor Corp at 20.26 percent. Resolute FP Canada Ltd will pay a 12.82 percent duty, while Tolko Marketing and Sales and Tolko Industries will pay a 19.50 percent duty and J. D. Irving Ltd, will pay 3.02 percent. All other Canadian producers face a 19.88 percent duty, according to the document.\n\nOn April 25, 2017, the Trump administration announced plans to impose duties of up to 24% on most Canadian lumber, charging that lumber companies are subsidized by the government. The duties are on the five firms: West Fraser Mills, Tolko Marketing and Sales, J. D. Irving, Canfor Corporation, and Resolute FP Canada. West Fraser Mills will pay the highest duty of 24%.\n\nThe preliminary determination directs U.S. Customs and Border Protection to require cash deposits for the duties on all new imports as well as softwood products imported over the past 90 days. To remain in effect, however, the duties need to be finalized by Commerce and then confirmed by the U.S. International Trade Commission after an investigation that includes testimony from both sides. In response, the Canadian federal government indicated that it was exploring the possibility of banning United States coal from being exported through Canadian ports and imposing a retaliatory tariff on lumber exports from Oregon.\n\nBy November 2017, the U.S. International Trade Commission decided to levy heavy countervailing and anti-dumping duties on lumber imports, citing that the U.S. industry has been harmed by unfair practices. The decision was unanimous for the four-member trade panel, which ruled on the petition filed by the U.S. Lumber Coalition. Canada immediately lodged a legal challenge to the decision under NAFTA's Chapter 19 dispute settlement mechanism, with an official statement declaring that it was \"unfair, unwarranted, and troubling.\" Canada has previously won several NAFTA challenges involving softwood issues in the past. In March 2018, Canada escalated the dispute to the WTO, asking the international trade body to set up an adjudication panel to judge its dispute with the United States. Canadian representatives argued that talks with the U.S. have failed.\n\n\n"}
{"id": "29453694", "url": "https://en.wikipedia.org/wiki?curid=29453694", "title": "Cobalt-chrome", "text": "Cobalt-chrome\n\nCobalt-chrome or cobalt-chromium (CoCr) is a metal alloy of cobalt and chromium. Cobalt-chrome has a very high specific strength and is commonly used in gas turbines, dental implants, and orthopedic implants.\n\nCo-Cr alloy was first discovered by Elwood Haynes in the early 1900s by fusing cobalt and chromium. The alloy was first discovered with many other elements such as tungsten and molybdenum in it. Haynes reported his alloy was capable of resisting oxidation and corrosive fumes and exhibited no visible sign of tarnish even when subjecting the alloy to boiling nitric acid. Under the name Stellite™, Co-Cr alloy has been used in various fields where high wear-resistance was needed including aerospace industry, cutlery, bearings, blades, etc. \n\nCo-Cr alloy started receiving more attention as its biomedical application was found. In the 20th century, the alloy was first used in medical tool manufacturing, and in 1960, the first Co-Cr prosthetic heart valve was implanted, which happened to last over 30 years showing its high wear-resistance. Recently, due to excellent resistant properties, biocompatibility, high melting points, and incredible strength at high temperatures, Co-Cr alloy is used for the manufacture of many artificial joints including hips and knees, dental partial bridge work, gas turbines, and many others.\n\nThe common Co-Cr alloy production requires the extraction of cobalt and chromium from cobalt oxide and chromium oxide ores. Both of the ores need to go through reduction process to obtain pure metals. Chromium usually goes through aluminothermic reduction technique, and pure cobalt can be achieved through many different ways depending on the characteristics of the specific ore. Pure metals are then fused together under vacuum either by electric arc or by induction melting. Due to the chemical reactivity of metals at high temperature, the process requires vacuum conditions or inert atmosphere to prevent oxygen uptake by the metal. ASTM F75, a Co-Cr-Mo alloy, is produced in an inert argon atmosphere by ejecting molten metals through a small nozzle that is immediately cooled to produce a fine powder of the alloy.\n\nHowever, synthesis of Co-Cr alloy through the method mentioned above is very expensive and difficult. Recently, in 2010, scientists at the University of Cambridge have produced the alloy through a novel electrochemical, solid-state reduction technique known as the FFC Cambridge Process which involves the reduction of an oxide precursor cathode in a molten chloride electrolyte.\n\nCo-Cr alloys show high resistance to corrosion due to the spontaneous formation of a protective passive film composed of mostly CrO, and minor amounts of cobalt and other metal oxides on the surface. As its wide application in biomedical industry indicates, Co-Cr alloys are well known for their biocompatibility. Biocompatibility also depends on the film and how this oxidized surface interacts with physiological environment. Good mechanical properties that are similar to stainless steel are a result of a multiphase structure and precipitation of carbides, which increase the hardness of Co-Cr alloys tremendously. The hardness of Co-Cr alloys varies ranging 550-800 MPa, and tensile strength varies ranging 145-270 MPa. Moreover, tensile and fatigue strength increases radically as they are heat-treated. However, Co-Cr alloys tend to have low ductility, which can cause component fracture. This is a concern as the alloys are commonly used in hip replacements. In order to overcome the low ductility, nickel, carbon, and/or nitrogen are added. These elements stabilize the γ phase, which has better mechanical properties compared to other phases of Co-Cr alloys.\n\nThere are several Co-Cr alloys that are commonly produced and used in various fields. F75 and F799 are Co-Cr-Mo alloys with very similar composition yet slightly different production processes, F90 is a Co-Cr-W-Ni alloy, and F562 is a Co-Ni-Cr-Mo-Ti alloy.\n\nDepending on the percent composition of cobalt or chromium and the temperature, Co-Cr alloys show different structures. The σ phase, where the alloy contains approximately 60-75% cobalt, tends to be brittle and subject to a fracture. FCC crystal structure is found in the γ phase, and the γ phase shows improved strength and ductility compared to the σ phase. FCC crystal structure is commonly found in cobalt rich alloys, while chromium rich alloys tend to have BCC crystal structure. The γ phase Co-Cr alloy can be converted into the ε phase at high pressures, which shows a HCP crystal structure.\n\nCo-Cr alloys are most commonly used to make artificial joints including knee and hip joints due to high wear-resistance and biocompatibility. Co-Cr alloys tend to be corrosion resistant, which reduces complication with the surrounding tissues when implanted, and chemically inert that they minimize the possibility of irritation, allergic reaction, and immune response. Co-Cr alloy has also been widely used in the manufacture of stent and other surgical implants as Co-Cr alloy demonstrates excellent biocompatibility with blood and soft tissues as well. The alloy composition used in orthopedic implants is described in industry standard ASTM-F75: cobalt with 27 to 30% chromium, 5 to 7% molybdenum, and limits on other important elements such as manganese and silicon, less than 1%, iron, less than 0.75%, nickel, less than 0.5%, and carbon, nitrogen, tungsten, phosphorus, sulfur, boron etc. \n\nBesides cobalt-chromium-molybdenum (CoCrMo), cobalt-nickel-chromium-molybdenum (CoNiCrMo) is also used for implants. The possible toxicity of released Ni ions from CoNiCr alloys and also their limited frictional properties are a matter of concern in using these alloys as articulating components. Thus, CoCrMo is usually the dominant alloy for total joint arthroplasty.\n\nCo-Cr alloy dentures and cast partial dentures have been commonly manufactured since 1929 due to lower cost and lower density compared to gold alloys; however, Co-Cr alloys tend to exhibit a higher modulus of elasticity and cyclic fatigue resistance, which are significant factors for dental prosthesis. The alloy is a commonly used as a metal framework for dental partials. A well known brand for this purpose is Vitallium.\n\nDue to mechanical properties such as high corrosion and wear resistance, Co-Cr alloys (eg. Stellites) are used in making wind turbines, engine components, and many other industrial/mechanical components where high wear-resistance is needed. \n\nCo-Cr alloy is also very commonly used in fashion industry to make jewellery, especially wedding bands.\n\nMetals released from Co-Cr alloy tools and prosthetics may cause allergic reactions and skin eczema. Prosthetics or any medical equipment with high nickel mass percentage Co-Cr alloy should be avoided due to low biocompatibility, as nickel is the most common metal sensitizer in the human body.\n\n"}
{"id": "203921", "url": "https://en.wikipedia.org/wiki?curid=203921", "title": "Crusher", "text": "Crusher\n\nA crusher is a machine designed to reduce large rocks into smaller rocks, gravel, or rock dust.\n\nCrushers may be used to reduce the size, or change the form, of waste materials so they can be more easily disposed of or recycled, or to reduce the size of a solid mix of raw materials (as in rock ore), so that pieces of different composition can be differentiated. Crushing is the process of transferring a force amplified by mechanical advantage through a material made of molecules that bond together more strongly, and resist deformation more, than those in the material being crushed do. Crushing devices hold material between two parallel or tangent solid surfaces, and apply sufficient force to bring the surfaces together to generate enough energy within the material being crushed so that its molecules separate from (fracturing), or change alignment in relation to (deformation), each other. The earliest crushers were hand-held stones, where the weight of the stone provided a boost to muscle power, used against a stone anvil. Querns and mortars are types of these crushing devices.\n\nIn industry, crushers are machines which use a metal surface to break or compress materials into small fractional chunks or denser masses. Throughout most of industrial history, the greater part of crushing and mining part of the process occurred under muscle power as the application of force concentrated in the tip of the miners pick or sledge hammer driven drill bit. Before explosives came into widespread use in bulk mining in the mid-nineteenth century, most initial ore crushing and sizing was by hand and hammers at the mine or by water powered trip hammers in the small charcoal fired smithies and iron works typical of the Renaissance through the early-to-middle industrial revolution. It was only after explosives, and later early powerful steam shovels produced large chunks of materials, chunks originally reduced by hammering in the mine before being loaded into sacks for a trip to the surface, chunks that were eventually also to lead to rails and mine railways transporting bulk aggregations that post-mine face crushing became widely necessary. The earliest of these were in the foundries, but as coal took hold the larger operations became the coal breakers that fueled industrial growth from the first decade of the 1600s to the replacement of breakers in the 1970s through the fuel needs of the present day. The gradual coming of that era and displacement of the cottage industry based economies was itself accelerated first by the utility of wrought and cast iron as a desired materials giving impetus to larger operations, then in the late-sixteenth century by the increasing scarcity of wood lands for charcoal production to make the newfangled window glass material that had become—along with the chimney—\" 'all the rage'  \" among the growing middle-class and affluence of the sixteenth-and-seventeenth centuries;and as always, the charcoal needed to smelt metals, especially to produce ever larger amounts of brass and bronze, pig iron, cast iron and wrought iron demanded by the new consumer classes. Other metallurgical developments such as silver and gold mining mirrored the practices and developments of the bulk material handling methods and technologies feeding the burgeoning appetite for more and more iron and glass, both of which were rare in personal possessions until the 1700s. \nThings only became worse when the English figured out how to cast the more economical iron cannons (1547), following on their feat of becoming the armorers of the European continent's powers by having been leading producers brass and bronze guns, and eventually by various acts of Parliament, gradually banned or restricted the further cutting of trees for charcoal in larger and larger regions in the United Kingdom. In 1611, a consortium led by courtier Edward Zouch was granted a patent for the reverberatory furnace, a furnace using coal, not precious national timber reserves, which was immediately employed in glass making. An early politically connected and wealthy Robber Baron figure Sir Robert Mansell bought his way into the fledgling furnace company wrested control of it, and by 1615 managed to have James I issued a proclamation forbidding the use of wood to produce glass, giving his families extensive coal holdings a monopoly on both source and means of production for nearly half-a-century. Abraham Darby a century later relocated to Bristol where he had established a building brass and bronze industry by importing Dutch workers and using them to raid Dutch techniques. Both materials were considered superior to iron for cannon, and machines as they were better understood. But Darby would change the world in several key ways.\nWhere the Dutch had failed in casting iron, one of Darby's apprentices, John Thomas succeeded in 1707 and as Burke put it: \"\"had given England the key to the Industrial Revolution\"\". At the time, mines and foundries were virtually all small enterprises except for the tin mines (driven by the price and utility of brass) and materials came out of the mines already hammered small by legions of miners who had to stuff their work into carry sacks for pack animal slinging. Concurrently, mines needed drained resulting in Savery and Newcomen's early steam driven pumping systems. The deeper the mines went, the larger the demand became for better pumps, the greater the demand for iron, the greater the need for coal, the greater the demand for each. Seeing ahead clearly, Darby, sold off his brass business interests and relocated to Coalbrookdale with its plentiful coal mines, water power and nearby ore supplies. Over that decade his foundries developed iron casting technologies and began to supplant other metals in many applications. He adapted Coking of his fuel by copying Brewers practices. In 1822 the pumping industries needs for larger cylinders met up with Darby's ability to melt sufficient quantities of pig iron to cast large inexpensive iron cylinders instead of costly brass ones, reducing the cost of cylinders by nine-tenths. \nWith gunpowder being increasingly applied to mining, rock chunks from a mining face became much larger, and the blast dependent mining itself had become dependent upon an organized group, not just an individual swinging a pick. Economies of scale gradually infused industrial enterprises, while transport became a key bottleneck as the volume of moved materials continued to increase following demand. This spurred numerous canal projects, inspired laying first wooden, then iron protected rails using draft animals to pull loads in the emerging bulk goods transportation dependent economy. In the coal industry, which grew up hand in hand as the preferred fuel for smelting ores, crushing and preparation (cleaning) was performed for over a hundred years in coal breakers, massive noisy buildings full of conveyors, belt-powered trip-hammer crushing stages and giant metal grading/sorting grates. Like mine pumps, the internal conveyors and trip-hammers contained within these 7—11 story buildings.\n\nMining operations use crushers, commonly classified by the degree to which they fragment the starting material, with primary and secondary crushers handling coarse materials, and tertiary and quaternary crushers reducing ore particles to finer gradations. Each crusher is designed to work with a certain maximum size of raw material, and often delivers its output to a screening machine which sorts and directs the product for further processing. Typically, crushing stages are followed by milling stages if the materials need to be further reduced. Additionally rockbreakers are typically located next to a crusher to reduce oversize material too large for a crusher. Crushers are used to reduce particle size enough so that the material can be processed into finer particles in a grinder. A typical processing line at a mine might consist of a crusher followed by a SAG mill followed by a ball mill. In this context, the SAG mill and ball mill are considered grinders rather than crushers.\n\nIn operation, the raw material (of various sizes) is usually delivered to the primary crusher's hopper by dump trucks, excavators or wheeled front-end loaders. A feeder device such as an apron feeder, conveyor or vibrating grid controls the rate at which this material enters the crusher, and often contains a preliminary screening device which allows smaller material to bypass the crusher itself, thus improving efficiency. Primary crushing reduces the large pieces to a size which can be handled by the downstream machinery.\n\nSome crushers are mobile and can crush rocks as large as 60 inches. Primarily used in-pit at the mine face these units are able to move with the large infeed machines (mainly shovels) to increase the tonnage produced. In a mobile road operation, these crushed rocks are directly combined with concrete and asphalt which are then deposited on to a road surface. This removes the need for hauling oversized material to a stationary crusher and then back to the road surface.\n\n \nThe following table describes typical uses of commonly used crushers:\nA jaw crusher uses compressive force for breaking of particle. This mechanical pressure is achieved by the two jaws of the crusher of which one is fixed while the other reciprocates. A jaw or toggle crusher consists of a set of vertical jaws, one jaw is kept stationary and is called a fixed jaw while the other jaw called a swing jaw, moves back and forth relative to it, by a cam or pitman mechanism, acting like a class II lever or a nutcracker. The volume or cavity between the two jaws is called the crushing chamber. The movement of the swing jaw can be quite small, since complete crushing is not performed in one stroke. The inertia required to crush the material is provided by a weighted flywheel that moves a shaft creating an eccentric motion that causes the closing of the gap.\n\nJaw crushers are heavy duty machines and hence need to be robustly constructed. The outer frame is generally made of cast iron or steel. The jaws themselves are usually constructed from cast steel. They are fitted with replaceable liners which are made of manganese steel, or Ni-hard (a Ni-Cr alloyed cast iron). Jaw crushers are usually constructed in sections to ease the process transportation if they are to be taken underground for carrying out the operations.\n\nJaw crushers are classified on the basis of the position of the pivoting of the swing jaw\n\nThe Blake crusher was patented by Eli Whitney Blake in 1858. The Blake type jaw crusher has a fixed feed area and a variable discharge area. Blake crushers are of two types- single toggle and double toggle jaw crushers.\n\nIn the single toggle jaw crushers, the swing jaw is suspended on the eccentric shaft which leads to a much more compact design than that of the double toggle jaw crusher. The swing jaw, suspended on the eccentric, undergoes two types of motion- swing motion towards the fixed jaw due to the action of toggle plate and vertical movement due the rotation of the eccentric. These two motions, when combined, lead to an elliptical jaw motion. This motion is useful as it assists in pushing the particles through the crushing chamber. This phenomenon leads to higher capacity of the single toggle jaw crushers but it also results in higher wear of the crushing jaws. These type of jaw crushers are preferred for the crushing of softer particles.\n\nIn the double toggle jaw crushers, the oscillating motion of the swing jaw is caused by the vertical motion of the pitman. The pitman moves up and down. The swing jaw closes, i.e., it moves towards the fixed jaw when the pitman moves upward and opens during the downward motion of the pitman. This type is commonly used in mines due to its ability to crush tough and abrasive materials.\n\nIn the Dodge type jaw crushers, the jaws are farther apart at the top than at the bottom, forming a tapered chute so that the material is crushed progressively smaller and smaller as it travels downward until it is small enough to escape from the bottom opening. The Dodge jaw crusher has a variable feed area and a fixed discharge area which leads to choking of the crusher and hence is used only for laboratory purposes and not for heavy duty operations.\n\nA gyratory crusher is similar in basic concept to a jaw crusher, consisting of a concave surface and a conical head; both surfaces are typically lined with manganese steel surfaces. The inner cone has a slight circular movement, but does not rotate; the movement is generated by an eccentric arrangement. As with the jaw crusher, material travels downward between the two surfaces being progressively crushed until it is small enough to fall out through the gap between the two surfaces.\n\nA gyratory crusher is one of the main types of primary crushers in a mine or ore processing plant. Gyratory crushers are designated in size either by the gape and mantle diameter or by the size of the receiving opening. Gyratory crushers can be used for primary or secondary crushing. The crushing action is caused by the closing of the gap between the mantle line (movable) mounted on the central vertical spindle and the concave liners (fixed) mounted on the main frame of the crusher. The gap is opened and closed by an eccentric on the bottom of the spindle that causes the central vertical spindle to gyrate. The vertical spindle is free to rotate around its own axis. The crusher illustrated is a short-shaft suspended spindle type, meaning that the main shaft is suspended at the top and that the eccentric is mounted above the gear. The short-shaft design has superseded the long-shaft design in which the eccentric is mounted below the gear.\n\nWith the rapid development of mining technology, the cone crusher can be divided into four types: compound cone crusher, spring cone crusher, hydraulic cone crusher and gyratory crusher. According to different models, the cone crusher is divided into VSC series cone crusher (compound cone crusher), Symons cone crusher, PY cone crusher, single cylinder hydraulic cone crusher, multi-cylinder hydraulic cone crusher, gyratory crusher, etc.\n\nA cone crusher is similar in operation to a gyratory crusher, with less steepness in the crushing chamber and more of a parallel zone between crushing zones. A cone crusher breaks rock by squeezing the rock between an eccentrically gyrating spindle, which is covered by a wear-resistant mantle, and the enclosing concave hopper, covered by a manganese concave or a bowl liner. As rock enters the top of the cone crusher, it becomes wedged and squeezed between the mantle and the bowl liner or concave. Large pieces of ore are broken once, and then fall to a lower position (because they are now smaller) where they are broken again. This process continues until the pieces are small enough to fall through the narrow opening at the bottom of the crusher.\n\nA cone crusher is suitable for crushing a variety of mid-hard and above mid-hard ores and rocks. It has the advantage of reliable construction, high productivity, easy adjustment and lower operational costs. The spring release system of a cone crusher acts an overload protection that allows tramp to pass through the crushing chamber without damage to the crusher.\n\nCompound cone crusher(VSC series cone crusher) can crush materials of over medium hardness. It is mainly used in mining, chemical industry, road and bridge construction, building, etc. As for VSC series cone crusher, there are four crushing cavities (coarse, medium, fine and superfine) to choose. Compared with the same type, VSC series cone crusher, whose combination of crushing frequency and eccentricity is the best, can make materials have higher comminution degree and higher yield. In addition, VSC series cone crusher’s enhanced laminating crushing effect on material particles makes the cubic shape of crushed materials better, which increases the selling point.\n\nSymons cone crusher (spring cone crusher) can crush materials of above medium hardness. And it is widely used in metallurgy, building, hydropower, transportation, chemical industry, etc. When used with jaw crusher, it can be used as secondary, tertiary or quaternary crushing. Generally speaking, the standard type of Symons cone crusher is applied to medium crushing. The medium type is applied to fine crushing. The short head type is applied to coarse fine crushing. As casting steel technique is adopted, the machine has good rigidity and large high strength.\n\nSingle cylinder hydraulic cone crusher is mainly composed of main frame, transmission device, eccentric shaft, bowl-shaped bearing, crushing cone, mantle, bowl liner, adjusting device, adjusting sleeve, hydraulic control system, hydraulic safety system, dust-proof ring, feed plate, etc.\n\nMulti-cylinder hydraulic cone crusher is mainly composed of main frame, eccentric shaft, crushing cone, mantle, bowl liner, adjusting device, dust ring, transmission device, bowl-shaped bearing, adjusting sleeve, hydraulic control system, hydraulic safety system, etc. The electric motor of the cone crusher drives the eccentric shaft to make periodic swing movement under the shaft axis, and consequently surface of mantle approaches and leaves the surface of bowl liner now and then, so that the material is crushed due to squeezing and grinding inside the crushing chamber. The safety cylinder of the machine can ensure safety as well as lift supporting sleeve and static cone by a hydraulic system and automatically remove the blocks in the crushing chamber when the machine is suddenly stuffy. Thus the maintenance rate is greatly reduced and production efficiency is greatly improved as it can remove blocks without disassembling the machine.\n\nImpact crushers involve the use of impact rather than pressure to crush material. The material is contained within a cage, with openings on the bottom, end, or side of the desired size to allow pulverized material to escape. There are two types of impact crushers: horizontal shaft impactor and vertical shaft impactor.\n\nThe HSI crushers break rock by impacting the rock with hammers that are fixed upon the outer edge of a spinning rotor. HSI machines are sold in Stationary, trailer mounted and crawler mounted configurations. HSI's are used in recycling, hard rock and soft materials. In earlier years the practical use of HSI crushers is limited to soft materials and non abrasive materials, such as limestone, phosphate, gypsum, weathered shales, however improvements in metallurgy has changed the application of these machines.\n\nVSI crushers use a different approach involving a high speed rotor with wear resistant tips and a crushing chamber designed to 'throw' the rock against. The VSI crushers utilize velocity rather than surface force as the predominant force to break rock. In its natural state, rock has a jagged and uneven surface. Applying surface force (pressure) results in unpredictable and typically non-cubical resulting particles. Utilizing velocity rather than surface force allows the breaking force to be applied evenly both across the surface of the rock as well as through the mass of the rock. Rock, regardless of size, has natural fissures (faults) throughout its structure. As rock is 'thrown' by a VSI Rotor against a solid anvil, it fractures and breaks along these fissures. Final particle size can be controlled by 1) the velocity at which the rock is thrown against the anvil and 2) the distance between the end of the rotor and the impact point on the anvil. The product resulting from VSI Crushing is generally of a consistent cubical shape such as that required by modern SUPERPAVE highway asphalt applications. Using this method also allows materials with much higher abrasiveness to be crushed than is capable with an HSI and most other crushing methods.\n\nVSI crushers generally utilize a high speed spinning rotor at the center of the crushing chamber and an outer impact surface of either abrasive resistant metal anvils or crushed rock. Utilizing cast metal surfaces 'anvils' is traditionally referred to as a \"Shoe and Anvil VSI\". Utilizing crushed rock on the outer walls of the crusher for new rock to be crushed against is traditionally referred to as \"rock on rock VSI\". VSI crushers can be used in static plant set-up or in mobile tracked equipment.\n\nThe basic concept of the mineral sizer is the use of two rotors with large teeth, on small diameter shafts, driven at a low speed by a direct high torque drive system. This design produces three major principles which all interact when breaking materials using sizer technology. The unique principles are the three-stage breaking action, the rotating screen effect, and the deep scroll tooth pattern.\n\nThe three-stage breaking action: initially, the material is gripped by the leading faces of opposed rotor teeth. These subject the rock to multiple point loading, inducing stress into the material to exploit any natural weaknesses. At the second stage, material is broken in tension by being subjected to a three-point loading, applied between the front tooth faces on one rotor, and rear tooth faces on the other rotor. Any lumps of material that still remain oversize, are broken as the rotors chop through the fixed teeth of the breaker bar, thereby achieving a three dimensional controlled product size.\n\nThe rotating screen effect: The interlaced toothed rotor design allows free flowing undersize material to pass through the continuously changing gaps generated by the relatively slow moving shafts.\n\nThe deep scroll tooth pattern: The deep scroll conveys the larger material to one end of the machine and helps to spread the feed across the full length of the rotors. This feature can also be used to reject oversize material from the machine.\n\nA crusher bucket is an attachment for hydraulic excavators. Its way of working consists on a bucket with two crushing jaws inside, one of them is fixed and the other one moves back and forth relative to it, as in a Jaw crusher.\nThey are manufactured with a high inertia power train, circular jaw movement and an antiestagnation plate, which prevents large shredding pieces from getting stuck in the bucket's mouth, not allowing them to enter the crushing jaws. They have also the crushing jaws placed in a cross position. This position together with its circular motion gives these crusher buckets the faculty of grinding wet material.\nFor the most part advances in crusher design have moved slowly. Jaw crushers have remained virtually unchanged for sixty years. More reliability and higher production have been added to basic cone crusher designs that have also remained largely unchanged. Increases in rotating speed have provided the largest variation. For instance, a 48-inch (120 cm) cone crusher manufactured in 1960 may be able to produce 170 tons/h of crushed rock, whereas the same size crusher manufactured today may produce 300 tons/h. These production improvements come from speed increases and better crushing chamber designs.\n\nThe largest advance in cone crusher reliability has been seen in the use of hydraulics to protect crushers from being damaged when uncrushable objects enter the crushing chamber. Foreign objects, such as steel, can cause extensive damage to a cone crusher, and additional costs in lost production. The advance of hydraulic relief systems has greatly reduced downtime and improved the life of these machines.\n\n"}
{"id": "30021909", "url": "https://en.wikipedia.org/wiki?curid=30021909", "title": "Cyrtosperma merkusii", "text": "Cyrtosperma merkusii\n\nCyrtosperma merkusii or giant swamp taro, is a crop grown throughout Oceania and into South and Southeast Asia. It is a riverine and \"swamp crop\" similar to taro, but \"with bigger leaves and larger, coarser roots.\" There are no demonstrably wild populations today, but it is believed to be native to Indonesia. It is known as \"puraka\" in Cook Islands, \"Lak\" in Yap (FSM), Babai in Kiribati, \"pula’a\" in Samoa, \"via kan\" in Fiji, Pulaka in Tokelau and Tuvalu, \"simiden\" in Chuuk, \"swam taro\" in Papua New Guinea, \"navia\" in Vanuatu and \"palawan\" in the Philippines.\n\nThe same species is also known by the names \"Cyrtosperma lasioides\", \"Cyrtosperma chamissonis\" and \"Cyrtosperma edule\".\n\nIn the harsh atoll environments of the Central Pacific, especially Tuvalu and Kiribati, swamp taro is an important source of carbohydrates in a diet dominated by fish and coconut. Its cultivation is difficult and time-consuming, and the plant has deep cultural as well as practical significance. The roots need to be cooked for hours to reduce toxicity in the corms, but are rich in nutrients, especially calcium. The cultivation of Pulaka in Tuvalu, and babai in Kiribati, is an important cultural and culinary tradition, now under threat from rising sea level and displacement from the growing use of imported food products.\n\nIn Nepal, Giant Swamp Taro is called \"mane\" and grows in the tropical and sub tropical forests along stream banks. It is gathered in January–February and all plant parts (leaf, stem, rhizomes) are savored after being boiled and roasted. The stem requires prolonged boiling and the water is replaced once to remove irritating chemicals. If cooked carefully, the rhizomes taste like taro and the leaves like spinach. But without careful washing, the food causes an unpleasant tingling or scratchy sensation.\n\nGiant swamp taro is the largest of the root crop plants known collectively as Taro, which are cultivated throughout Southeast Asia and the Pacific. Although outwardly similar to Colocasia esculenta, the most widely cultivated taro, it belongs to a different genus. The plant may reach heights of 4–6 metres, with leaves and roots much larger than Colocasia esculenta. The sagitate leaves are up to 6' 7\" (2 meters) long by up to four feet (120 cm) in width, borne atop petioles or stalks up to 19' 6\" (6 meters)in length and four inches (10 cm) wide. It is relatively resistant to disease and pests but is susceptible to taro beetle. The corm, which can reach weights of 80 kg or even 220 pounds (100 kg) with a diameter of up to 39 inches (1 meter) and equally long. is starchy and cream or pink in colour, with a taste similar to sweet potato, though it is drier in texture.\n\nGiant swamp taro is not suitable for growing in upland or rainfed conditions; it has adapted to growth within fresh water and coastal swamps. It exhibits some shade tolerance and is considered mildly tolerant of saline growing conditions compared to other taro species; that is, it can be grown in mildly brackish water. It is a slow growing crop which can take up to 15 years to mature.\n\nGiant swamp taro is nearly the only carbohydrate crop that can be cultivated on low-lying coral atolls, where it is grown in purpose-built swamp pits dug to below the level of the freshwater lens. The cultivation of Pulaka in Tuvalu, and of babai in Kiribati, has deep cultural significance. In these harsh environments, its cultivation is increasingly threatened by rising sea levels caused by global warming: the plant does not thrive in brackish water, which rots the roots, turns the leaves yellow, and stunts the plant's growth. Climate change is affecting its cultivation in two ways; more frequent droughts increase the salinity of the freshwater lens, and more extreme high tides and coastal erosion lead to saltwater intrusions where seawater enters the cultivation pits.\n\nGiant swamp taro contains toxins which must be removed by long cooking. It may be field stored in the ground for very long periods – up to 30 years or more – and \naccordingly has traditionally been an important emergency crop in times of natural disaster and food scarcity. The cooked corms can be dried in the sun and stored for later use. Different methods of preparation are used for pulaka in Tuvalu, and babai in Kiribati.\n\n"}
{"id": "1045142", "url": "https://en.wikipedia.org/wiki?curid=1045142", "title": "Debris", "text": "Debris\n\nDebris or débris (, ) is rubble, wreckage, ruins, litter and discarded garbage/refuse/trash, scattered remains of something destroyed, discarded, or as in geology, large rock fragments left by a melting glacier etc. Depending on context, \"debris\" can refer to a number of different things. The first apparent use of the French word in English is in a 1701 description of the army of Prince Rupert upon its retreat from a battle with the army of Oliver Cromwell, in England.\n\nIn disaster scenarios, tornados leave behind large pieces of houses and mass destruction overall. This debris also flies around the tornado itself when it is in progress. The tornado's winds capture debris it kicks up in its wind orbit, and spins it inside its vortex. The tornado's wind radius is larger than the funnel itself. tsunamis and hurricanes also bring large amounts of debris, such as Hurricane Katrina in 2005 and Hurricane Sandy in 2012. Earthquakes rock cities to rubble debris.\n\nIn geology, debris usually applies to the remains of geological activity including landslides, volcanic explosions, avalanches, mudflows or Glacial lake outburst floods (Jökulhlaups) and moraine, lahars, and lava eruptions. Geological debris sometimes moves in a stream called a debris flow. When it accumulates at the base of hillsides, it can be called \"talus\" or \"scree\".\n\nIn mining, debris called \"attle\" usually consists of rock fragments which contain little or no ore.\n\n\"Marine debris\" applies to floating garbage such as bottles, cans, styrofoam, cruise ship waste, offshore oil and gas exploration and production facilities pollution, and fishing paraphernalia from professional and recreational boaters. Marine debris is also called litter or flotsam and jetsam. Objects that can constitute marine debris include used automobile tires, detergent bottles, medical wastes, discarded fishing line and nets, soda cans, and bilge waste solids.\n\nIn addition to being unsightly, it can pose a serious threat to marine life, boats, swimmers, divers, and others. For example, each year millions of seabirds, sea turtles, fish, and marine mammals become entangled in marine debris, or ingest plastics which they have mistaken for food. As many as 30,000 northern fur seals per year get caught in abandoned fishing nets and either drown or suffocate. Whales mistake plastic bags for squid, and birds may mistake plastic pellets for fish eggs. At other times, animals accidentally eat the plastic while feeding on natural food.\n\nThe largest concentration of marine debris is the Great Pacific Garbage Patch.\n\nMarine debris most commonly originates from land-based sources. Various international agencies are currently working to reduce marine debris levels around the world.\n\nIn meteorology, debris usually applies to the remains of human habitation and natural flora after storm related destruction. This debris is also commonly referred to as storm debris. Storm debris commonly consists of roofing material, downed tree limbs, downed signs, downed power lines and poles, and wind-blown garbage. Storm debris can become a serious problem immediately after a storm, in that it often blocks access to individuals and communities that may require emergency services. This material frequently exists in such large quantities that disposing of it becomes a serious issue for a community. In addition, storm debris is often hazardous by its very nature, since, for example, downed power lines annually account for storm-related deaths.\n\n\"Space debris\" usually refers to the remains of spacecraft that have either fallen to Earth or are still orbiting Earth. Space debris may also consist of natural components such as chunks of rock and ice. The problem of space debris has grown as various space programs have left legacies of launches, explosions, repairs, and discards in both low Earth orbit and more remote orbits. These orbiting fragments have reached a great enough proportion to constitute a hazard to future space launches of both satellite and manned vehicles. Various government agencies and international organizations are beginning to track space debris and also research possible solutions to the problem. While many of these items, ranging in size from nuts and bolts to entire satellites and spacecraft, may fall to Earth, other items located in more remote orbits may stay aloft for centuries. The velocity of some of these pieces of space junk have been clocked in excess of 17,000 miles per hour (27,000 km/h). A piece of space debris falling to Earth leaves a fiery trail, just like a meteor.\n\nA debris disk is a circumstellar disk of dust and debris in orbit around a star.\n\nIn medicine, debris usually refers to biological matter that has accumulated or lodged in surgical instruments and is referred to as surgical debris. The presence of surgical debris can result in cross-infections or nosocomial infections if not removed and the affected surgical instruments or equipment properly disinfected.\n\nIn the aftermath of a war, large areas of the region of conflict are often strewn with \"war debris\" in the form of abandoned or destroyed hardware and vehicles, mines, unexploded ordnance, bullet casings and other fragments of metal.\n\nMuch war debris has the potential to be lethal and continues to kill and maim civilian populations for years after the end of a conflict. The risks from war debris may be sufficiently high to prevent or delay the return of refugees. In addition war debris may contain hazardous chemicals or radioactive components that can contaminate the land or poison civilians who come into contact with it. Many Mine clearance agencies are also involved in the clearance of war debris.\n\nLand mines in particular are very dangerous as they can remain active for decades after a conflict, which is why they have been banned by international war regulations.\n\nIn November 2006 the Protocol on Explosive Remnants of War\ncame into effect with 92 countries subscribing to the treaty that requires the parties involved in a conflict to assist with the removal of unexploded ordnance following the end of hostilities.\n\nSome of the countries most affected by war debris are Afghanistan, Angola, Cambodia, Iraq and Laos.\n\nSimilarly \"military debris\" may be found in and around firing ranges and military training areas.\n\nDebris can also be used as cover for military purposes, depending on the situation.\n\nIn South Louisiana's Creole and Cajun cultures, debris (pronounced \"DAY-bree\") refers to chopped organs such as liver, heart, kidneys, tripe, spleen, brain, lungs and pancreas.\n\n\n"}
{"id": "40073915", "url": "https://en.wikipedia.org/wiki?curid=40073915", "title": "Dyakonov surface waves", "text": "Dyakonov surface waves\n\nIn 1988, the Russian Soviet physicist Mikhail (Michel) I. Dyakonov theoretically predicted a new class of surface electromagnetic waves, now called Dyakonov surface waves (DSWs). Unlike other types of acoustic and electromagnetic surface waves, the DSW's existence is due to the difference in symmetry of materials forming the interface. He considered the interface between an isotropic transmitting medium and an anisotropic uniaxial crystal, and showed that under certain conditions waves localized at the interface should exist. Later, similar waves were predicted to exist at the interface between two identical uniaxial crystals with different orientations.\nThe previously known electromagnetic surface waves, surface plasmons and surface plasmon polaritons, exist under the condition that the permittivity of one of the materials forming the interface is negative, while the other one is positive (for example, this is the case for the air/metal interface below the plasma frequency). In contrast, the DSW can propagate when both materials are transparent; hence they are virtually lossless, which is their most fascinating property.\n\nIn recent years, the significance and potential of the DSW have attracted the attention of many researchers: a change of the constitutive properties of one or both of the two partnering materials – due to, say, infiltration by any chemical or biological agent – could measurably change the characteristics of the wave. Consequently, numerous potential applications are envisaged, including devices for integrated optics, chemical and biological surface sensing, etc.\nHowever, it is not easy to satisfy the necessary conditions for the DSW, and because of this the first proof-of-principle experimental observation of DSW \nwas reported only 20 years after the original prediction.\n\nA large number of theoretical work appeared dealing with various aspects of this phenomenon, see the detailed review. In particular, DSW propagation at magnetic interfaces, in left-handed materials, in electro-optical, and chiral materials was studied. Resonant transmission due to DSW in structures using prisms was predicted, and combination and interaction between DSW and surface plasmons (Dyakonov plasmons) was studied.\n\nThe simplest configuration considered in Ref. 1 consists of an interface between an isotropic material with permittivity ε and a uniaxial crystal with permittivities εo and εe for the ordinary and the extraordinary waves respectively. The crystal \"C\" axis is parallel to the interface. For this configuration, the DSW can propagate along the interface within certain angular intervals with respect to the \"C\" axis, provided that the condition of εe > ε > εo is satisfied. Thus DSW are supported by interfaces with positive birefringent crystals only (εe > εo). The angular interval is defined by the parameter η =εe/εo − 1. The angular intervals for the DSW phase and group velocities (Δθph and Δθgr) are different. The phase velocity interval is proportional to η^2 and even for the most strongly birefringent natural crystals is very narrow Δθph ≃ 1° (rutile) and Δθph ≃4° (calomel). However the physically more important group velocity interval is substantially larger (proportional to η). Calculations give Δθgr ≃ 7° for rutile, and Δθgr ≃ 20° for calomel.\n\nA widespread experimental investigation of DSW material systems and evolution of related practical devices has been largely limited by the stringent anisotropy conditions necessary for successful DSW propagation, particularly the high degree of birefringence of at least one of the constituent materials and the limited number of naturally available materials fulfilling this requirement. However, this is about to change in light of novel artificially engineered metamaterials and revolutionary material synthesis techniques.\n\nThe extreme sensitivity of DSW to anisotropy, and thereby to stress, along with their low-loss (long-range) character render them particularly attractive for enabling high sensitivity tactile and ultrasonic sensing for next-generation high-speed transduction and read-out technologies.\n\n"}
{"id": "3915631", "url": "https://en.wikipedia.org/wiki?curid=3915631", "title": "Edison Sault Electric Company", "text": "Edison Sault Electric Company\n\nEdison Sault Electric Company was a public utility that provided electricity to the eastern portion of Michigan's Upper Peninsula. Its service area covered four counties (Chippewa, Mackinac, Schoolcraft and Delta).\n\nThe company was founded in 1892 in Sault Ste. Marie.\n\nWisconsin Energy Corporation acquired the Edison Sault Electric Company with its purchase of its parent company, ESELCO in 1998.\n\nIn 2009, Wisconsin Energy announced it had reached a definitive agreement to sell Edison Sault Electric to the Cloverland Electric Cooperative of Dafter.\n\nEdison Sault's only generating station was the Saint Marys Falls Hydropower Plant located on the St. Marys River in downtown Sault Ste. Marie. During the night when demand was low, the company was able to sell power from this small dam to larger companies such as Upper Peninsula Power Company and Consumers Energy. Edison Sault's transmission system voltage is 138,000 volts. The subtransmission system voltage is 69,000 volts. The distribution system voltage is 13,200 volts.\n\nEdison Sault had four 138 kV interconnections with other utilities. Two were with Consumers Energy in lower Michigan. (McGulpin-Straits #1 and McGulpin-Straits #2) These two lines are submerged under the Straits of Mackinac. Edison Sault has two interconnections with its former sister company, Wisconsin Electric (Arnold-Indian Lake #1 and Arnold-Indian Lake #2).\n\nEdison Sault had interconnections with Cloverland Electric and Upper Peninsula Power on its 69 kV subtransmission system.\n\n\n"}
{"id": "53590889", "url": "https://en.wikipedia.org/wiki?curid=53590889", "title": "Egegaz Aliağa LNG Storage Facility", "text": "Egegaz Aliağa LNG Storage Facility\n\nEgegaz Aliağa LNG Storage Facility () is a floating production storage and offloading unit (FPSO) for \nliquefied natural gas (LNG) in İzmir Province, western Turkey. It is country's first floating LNG storage facility.\n\nThe floating LNG storage facility is the vessel MT \"GDF Suez Neptune\", which sailed from France to Turkey and moored at a special pier of Egegaz Terminal in Aliağa district of İzmir Province on December 11, 2016. The 2009-built Norway-flagged LNG carrier is long and has a beam of . She is capable of storing LNG. She can regasify LNG delivered from other ships. The floating storage facility went in service as Turkey's first one of its kind on December 23, 2016. It is planned that the annual storage capacity of the Aliağa Terminal, operated by Egegaz, will be 5.3 million tons. It will increase country's daily natural gas supply capacity from up to .\n\n"}
{"id": "22964435", "url": "https://en.wikipedia.org/wiki?curid=22964435", "title": "Energy security of the People's Republic of China", "text": "Energy security of the People's Republic of China\n\nEnergy security of the People's Republic of China concerns the need for the People's Republic of China to guarantee itself and its industries long- term access to sufficient energy and raw materials. China has been endeavoring to sign international agreements and secure such supplies; its energy security involves the internal and foreign energy policy of China. Currently, China's energy portfolio consists mainly of domestic coal, oil and gas from domestic and foreign sources, and small quantities of uranium. China has also created a strategic petroleum reserve, to secure emergency supplies of oil for temporary price and supply disruptions. Chinese policy focuses on diversification to reduce oil imports, which rely almost exclusively on producers in the Middle East.\n\nAccording to Professor Zha Daojiong, China's dependence on foreign sources of energy is not a threat to China's energy security, since the world energy market is not opposed to China's pursuit of growth and prosperity. The key issue is actually internal: growing internal consumption without energy efficiency threatens both China's growth and world oil markets. Chinese imports are a new determinant encouraging oil price rises on the world market, a concern to developed countries. The international community advocates a move toward energy efficiency and more transparency in China's quest for energy worldwide, to confirm China's responsibility as a member of the international community. Energy efficiency is the only way to avoid excessive Chinese demands on oil at the expense of industrialized and industrializing countries. International projects and technology transfers are ongoing, improving China's energy consumption and benefit the whole energy-importing world; this will also calm Western-Chinese diplomatic tensions. China is trying to establish long-term energy security by investment in oil and gas fields abroad and by diversifying its providers.\n\nThanks to the transfer of Soviet oil extraction technologies prior to July 1960 and domestic reserves such as the Daqing oil field, the PRC became oil self-sufficient in 1963. A US-led embargo isolated the Chinese oil industry from 1950 to 1970, preventing it from selling on the world oil market. After the embargo was lifted, China reactivated its links with Japan and other industrialized nations thanks to its oil exports, which helped bring in foreign currencies and fund key industrial plants and technologies for developing its own export-oriented economy. Chinese oil exports peaked in 1985 at 30 million tons. Rapid reforms, in turn, increased domestic oil demand and led China to become a net oil importer in 1993, and net crude oil importer in 1996.\nSince 1996 Chinese oil production has slowly and continuously decreased, while demand and imports have steadily increased. Future Chinese oil reserves (such as the Tarim basin) are difficult to extract, requiring specific technologies as well as the construction of pipelines thousands of kilometers long. As a result, such reserves would be very difficult to develop and not cost-effective, given current market prices.\n\nChina accounts for 40% of the 2004 oil-consumption increase, and thus is a key part of the cycle which had led to the oil price increase worldwide. China's import dependence remains at 60% as of 2014. In 2005, a campaign to increase energy efficiency was launched without official Ministry of Energy approval; since the campaign was sporadic, this objective seems hard to meet. Zha Daojiong encouraged increased management of oil and energy in China, noting that \"It is fair to say that the threat from ineffective energy industry governance is probably as great as that from the international energy market.\". A projection that China would reach South Korean levels of per-capita oil consumption in 30 years, combined with the current average global decline in production, could mean that up to (barrels per day) in production would have to be found in the next decade to keep up with increased demand and production declines. That would be the equivalent of roughly five times Saudi Arabia's production. Superimpose a production plateau of , and significant real-price increases would be necessary to balance supply and demand. Such increases might have severe effects on the growth of emerging market economies such as China's.\n\nNuclear power in China accounts for approximately 1.4% of China's electricity, which is considered relatively low compared to other developing nations. China still mainly relies on coal for electricity. China is first in the world in both coal production and consumption, which has sparked environmental concerns. In order to achieve environmental targets in combating pollution and global warming, China must ultimately improve its coal efficiency and switch to alternative energy sources.\n\nChina's eastern and southern regions have chronic energy shortages, causing blackouts and limiting economic growth. For supplying these regions, liquefied natural gas from Australia and Indonesia is more feasible and cheaper to import than the Tarim basin pipeline. However, the first West–East Gas Pipeline from Xinjiang to Shanghai was commissioned in 2004, and construction of the second pipeline from Xinjiang to Guangzhou in Guangdong began in 2008.\n\nSinopec accounts for 80% of Chinese oil imports. Refinery capacity is continuously strained, and perennially lags behind fast domestic-demand growth. China has had to rely on entrepôt refineries located in Singapore, Japan and Korea. Oil and gas exploration in the Tarim Basin is ongoing. However, developing this potential reserve is currently not cost-effective due to technological limitations coupled with fluctuations in world oil prices. Therefore, this is considered by some as a last-resort option.\n\nIn China, the gas price is not market-driven, which causes uncertainty in the production process.\n\nA key point for China's energy-security goal of reducing oil imports is to improve the efficiency of its domestic energy markets by accelerating pricing, regulatory and other reforms. China is actively looking for smart-energy technology.\n\nOn the issue of energy security, China relies mainly on Persian Gulf exports. In contrast with the USA, China is not associated with the Arab–Israeli conflicts and may focus simply on oil supply from an economic standpoint. The increase in Chinese dependence on Persian Gulf oil also means an associated increasing economic dependence on Arabian exporters, who will probably not join hands to block exports to China.\n\nChinese dependence on the Middle East is also a cause of concern for the US. In 2004, when the Bush administration actively discouraged oil companies from investing in Iran, the Chinese company Sinopec did not comply with its call.\n\nRecently, China has changed its anti-Western diplomatic stance to a softer, global, more efficient diplomacy with a focus on energy and raw-materials security. In post-2003 Iraq, China does its best to comply with UN sanctions.\n\nWhen China became an oil importer during the 1990s, its relations with neighboring countries (as exporter to East Asia and importer of Korean and Japanese oil) changed. Its main oil provider changed in a few years from domestic production, to East Asian production, and then to Mideast production. On the other hand, despite insufficient domestic oil output China does its best to stabilize exports to Japan and Korea. China endeavors to continue energy relationships it has created with developed nations, since they contribute to China's energy security with investment and technology. More Chinese oil output is in Japanese, Korean, Chinese, and world interests. Since China lacks strategic entrepôt refineries, it relies heavily on refineries in Singapore, Japan, and Korea.\n\nChina's dependence on foreign oil weakens its ability to pressure Taiwan, since a conflict may trigger a US oil embargo as a consequence. Since Sudan is pro-Chinese and Chad was pro-Taiwan (and an oil producer since 2003), China had an interest in replacing Chad's president Idriss Déby with a pro-Chinese leader. The FUC Chad rebellion, based in Sudan and aiming to overthrow the pro-Taiwanese Déby, seems to have received Chinese diplomatic support as well as weapons and Sudanese oil. The 2006 Chadian coup d'état attempt failed after French Air Force intervention, but Déby then switched his friendship to Beijing; the field defeat became a Chinese strategic victory.\n\nIn February 2009, Russia and China signed an agreement in which a spur of the Eastern Siberia–Pacific Ocean oil pipeline to China would be built and Russia would supply China with 15 million tonnes of oil ( per day) each year for 20 years, in exchange for a loan worth US$25 billion to Russian companies Transneft and Rosneft for pipeline and oilfield development.\n\nOn August 19, 2009, Chinese petroleum company PetroChina signed an A$50 billion deal with American multinational petroleum company ExxonMobil to purchase liquefied natural gas from the Gorgon field in Western Australia; this was believed to be the largest contract ever signed between China and America – ensuring China a steady supply of LPG fuel for 20 years, and comprising China's largest supply of relatively clean energy. This agreement has been formalised despite relations between Australia and China being at their lowest point in years following the Rio Tinto espionage case and the granting of a visa to Rebiya Kadeer to visit Australia.\n\nChina has constructed an oil pipeline from Kazakhstan and started construction of a Central Asia–China gas pipeline.\n\nRatification of the Law of the Sea Treaty is linked to China's need to secure its oil and raw materials shipping from the Middle East, Africa, and Europe, since those materials have to pass through the Strait of Malacca and the Red Sea.\n\nThe appearance of China on the world energy scene is somewhat disturbing for developed nations. China's relative energy inexperience also raises diplomatic difficulties. Strengthening ties with oil producers such as Iran, Sudan, Uzbekistan, Angola and Venezuela also raised concerns for U.S. and other Western diplomacy, since several of these countries are known to be anti-American and/or known for human rights abuses, political censorship, and widespread corruption. These moves seem to challenge Western powers, by strengthening anti-Western countries. But this is unlikely; as a developing consumer economy, China does not have much of a choice in its sources of supply.\n\nIt is claimed that Chinese oil companies are unaccustomed to political risks and avoiding diplomatic conflict. In any case, the Chinese government will still be seen as ultimately responsible for conflict resolution. Communication has also been a weak point for Chinese companies. Lack of transparency in cases such as Chinese involvement in Sudan have raised concern in the US, until it was revealed that most of the oil produced was sold on international markets. Lack of cooperation with other major oil companies has led to business clashes, spilling into the diplomatic arena when both sides call their respective governments to support their interests (CNOOC versus Chevron-Texaco for Unocal, for example).\n\n\n\n"}
{"id": "18707673", "url": "https://en.wikipedia.org/wiki?curid=18707673", "title": "Ford Taurus (third generation)", "text": "Ford Taurus (third generation)\n\nThe third-generation Ford Taurus is an automobile that was manufactured by Ford from 1995 to 1999.\n\nThe third generation of Ford Taurus was the first to be completely redesigned from the ground up, and used a rounded, oval-derived design that was very controversial at the time, considered to be the main reason for this model's downfall in the market. It was designed to appeal to buyers of the Toyota Camry and Honda Accord — both of which were similarly styled — as well as to make Ford a design leader in the North American market, a title that was attributed to the Chrysler Corporation. Among the most controversial features of the design were both the front fascia that was composed of separate circular headlights, circular turn signals and the oval shaped rear window (the Sable, unlike the Taurus, had a more conventional window).\n\nThis generation of Taurus was released for sale in late 1995 to mixed reactions from consumers. It managed to retain its status as the best selling car in America through the 1996 model year; however, this was achieved through heavy sales to car rental companies: only 49% of Taurus sales in 1996 were to private customers. Because of this, the Taurus lost its bestseller status in 1997 to the Toyota Camry. It was replaced by the more conservatively styled fourth-generation Ford Taurus in 1999.\n\nDevelopment for the third-generation Taurus began in 1991, and its designers and engineers believed that they were faced with a daunting task; they compared completely redesigning the Taurus to repainting the Mona Lisa. Like the first-generation Taurus, the new Taurus was developed by a team effort, in which the exterior and interior designers, engineers, and marketing staff had input on the new car.\n\nMany designs were considered during the development process, from designs that resembled the second generation cars, to more radically styled cars. They eventually decided on a radical new styling scheme based upon oval derived design elements, which would prove to be the car's Achilles heel in the marketplace. Chief designer Jack Telnack, who oversaw the development of the first and second generation Taurus, said that his Taurus was designed the way it was to stand out in the marketplace, and that the use of the oval was becoming the new global design theme for Ford.\n\nBreaking down and testing competing cars, as well as listening to customer input played a large part in the development of the third-generation Taurus, just like it did during the development of the first generation. Many competing cars were broken down and extensively tested in order for the Taurus to be designed to be superior to them in terms of comfort, performance, and refinement. Most notably, the Toyota Camry and Honda Accord were extensively tested, and the Taurus' suspension was designed to emulate these cars' ride and handling techniques. Customer input played a large part into the design of the third generation Taurus' interior.\n\nThe dashboard's design originated from a large number of complaints from customers that the previous Taurus' radio and climate control modules were cluttered with many small and similar feeling buttons, as well as small graphics, which caused the driver to have to look away from the road to be able to operate them properly. As a result, a large portion of the third generation's dash was devoted to the radio and climate control, with each button on these modules containing a unique design, making it easier for the driver to operate the radio and climate control without taking their eyes off the road. This would eventually lead to the creation of the Integrated Control Panel.\n\nMaking the new Taurus pleasing to the senses was a recurring theme throughout the third-generation Taurus' development. Ford's engineers specially tuned every panel and component, so that every sound that the Taurus made, from the doors closing to the engine running, was acoustically pleasing. Ford's trim designers specially selected every one of the Taurus' interior materials, so that every surface, as well as every button and control, was pleasing to the touch.\n\nThe third-generation Taurus and Sable sedans were unveiled at the Cobo Center during the 1995 North American International Auto Show in Detroit, and garnered more attention from journalists and publications than any other car at the show. The wagon was unveiled on February 9, 1995 at the Chicago Auto Show, and garnered similar amounts of attention. After their respective unveilings, both vehicles became among the most anticipated new cars of the 1996 model year, similar to the first generation Taurus.\n\nThe first third-generation Taurus rolled off the assembly line on July 12, 1995 at the Chicago Assembly Plant. Ford Chairman Alexander Trotman, who took part in the ceremonies, was joined by state and local politicians and union and plant officials in dipping their hands in yellow paint to \"autograph\" the hood of the first Taurus off the line. The hood is earmarked for permanent display at the plant. The Taurus was released to showrooms on September 24 of that year, and sales began a week later on October 1. The Taurus was released almost a week later than the Sable, as Ford designers consulted a $500 Sherman Oaks, California astrologer to figure out the best solstice date to release the car.\n\nReception to the new Taurus by automotive publications was generally positive. Road & Track gave the Taurus a good review upon its release, and found its handling and refinement impressive. Motor Trend also gave the Taurus a positive review, although they found the oval styling awkward at first glance. Despite this, they found it to have many redeeming qualities. However, unlike the first-generation Taurus, it fell short of their Car of the Year award, which was instead awarded to the redesigned 1996 Dodge Caravan.\n\nConsumer reaction was mixed, however. Detractors of the new design pejoratively refer to this generation as the \"Bubble\" Taurus or \"Submarine\" Taurus based on the window shaping and broad curves used in the front end being seen as a negative. Ford had hoped the radical redesign would lead to the same success it had with the 1986 Taurus, and went as far as predicting that the new Taurus would continue the outgoing model's record of selling over 400,000 units a year. Sales were somewhat slow at first, prompting Ford to add a low priced \"G\" model mid-year 1996. Ford also offered incentives such as a six-month lease for Toyota Camry and Honda Accord owners, a $250 Cash Allowance on lease renewals, and a $50 dinner certificate for test driving the vehicle. It managed to keep its position as the best selling car in the United States in 1996, although this was because of heavy sales to rental fleets, which comprised 51% of all Taurus sales for that year. This is opposed to the Toyota Camry, of which its largest amount of sales were through retail outlets to individual customers. In 1997, the Taurus lost its bestselling title, as it slipped to #3 behind a redesigned Toyota Camry and the Honda Accord. \n\nThe exterior of the third-generation Taurus was completely redesigned for 1996. It used a controversial new shape that chief designer Jack Telnack claimed was penned to make the Taurus stand out to sedan buyers, and compared the current Taurus to the likeness of a pair of slippers. This shape was based upon that of an oval, which was perhaps inspired by that of Ford's own logo, and while the previous Taurus used a flat, streamlined shape, this Taurus used a rounded shape similar to that of the Chrysler Concorde. The heavily-contoured sides took their cue from the Lincoln Mark VIII. Station wagons also got new sheet metal, although from the firewall back, the Taurus and Mercury Sable wagons again shared the same panels, with all station wagon doors being the same as those used on the Sable sedans. As the new-generation Taurus was aimed at a more mature, affluent customer base, its exterior contained many upscale styling touches. For example, the LX came with chrome alloy wheels, chrome dual exhaust tips, and the \"Taurus\" badge on the back was written in script, as opposed to the block letters used in previous generations.\nIn 1998, the exterior of the Taurus received a slight redesign to make the shape more mainstream, in order to appeal to a wider customer base. The front bumper was redesigned to have a full-length opening, as well as moving the Ford logo down onto a chrome bar mounted in the grille, similar to that of the third-generation Taurus SHO. These years were also fitted with different front turn signal lenses, featuring completely clear lenses and a multi-reflector surface in the rear of the housing, around the bulb. The rear turn signal lenses were also changed from amber to red, to match the rest of the lightbar assembly.\n\nThe interior was also completely redesigned for the 1996 model year. Like that of the previous two generations, the interior was designed to be user-friendly. The gauges were placed in an oval pod while the dashboard wrapped slightly around the driver; all of the main controls were placed within easy reach, and were designed to be recognizable by touch and to be operated by drivers without taking their eyes off the road. The controls for the radio and climate control(some of which, like the preset buttons, were oval shaped) were combined into an oval-shaped \"Integrated Control Panel\" mounted in the center of the dash, which was created in response to many complaints from Taurus owners that they couldn't easily operate the main controls of the radios and climate control systems without taking their eyes off the road. Gone was wide and couch-like front row bench seating of previous generations.\n\nAnother innovation was the \"Flip-Fold\" center console on cars not equipped with a standard floor console. It was a seat in between the bucket seats that could be transformed into a console, reducing the seating count by one. The seat cushion folded out into a console with a lockable storage bin and cupholders, resting against the dashboard. The seatback also folded down to become an armrest. A traditional center console with a floor-mounted shifter was installed on cars equipped without the flip fold console.\n\nThis generation's interior wasn't as configurable as that of the first two generations. The G and GL models (see below) were only available with steering column-mounted shifter, along with a cloth front bench seat and the \"Flip-Fold\" center console. The LX, on the other hand, came standard with bucket seats that could be ordered with either cloth or leather upholstery, along with a center console and floor-mounted shifter. However, a column-mounted shifter and a cloth front bench was available as a no-cost option. In 1999, the interior was again available in three configurations, like that of the first-generation Taurus; a front bench seat with a column-mounted shifter, front bucket seats with a center console and floor-mounted shifter, or front bucket seats with a center console and a column-mounted shifter. Each configuration was offered in both Taurus models (see below).\n\nThe Ford Taurus has many seating options, ranging from 5 people to 8 people. On wagons and sedans equipped with the floor mounted shifter, it seated 5 people standard, and wagons could seat up to 7 people, with a bench in the third row. Also, models equipped with the flip fold console could seat either 6 or 8 people, depending on whether the wagons had a third row bench or not. The sedans could seat a maximum of 6 people with the flip fold console, and 5 people with a regular console.\n\nAt its launch, the Taurus was available in two models, the GL, which was the entry level/value model, and the LX, which was the top of the line model. The GL was basic, and didn't have many optional features, while the LX came with many standard features and was highly configurable.\n\nTo close the price gap between the Taurus and the Ford Contour, as well as to increase sales, an entry-level G model was added midway into the 1996 model year, as essentially a slightly de-contented GL. For the 1998 model year, the models of Taurus were completely changed. The G and GL were dropped, and the LX became the entry-level/value model. The SE model from 1995 was revived, and it became the top-tier model. The SE was offered with two special packages; the Comfort package which added chrome wheels carried over from the LX, automatic climate control, the Duratec engine V6 (see below), and bucket seats with a center console and a floor-mounted shifter, or the Sport package, which contained all of the features in the comfort package, but substituted the chrome wheels for 5-spoke alloy wheels.\n\nThe 1996 model year was the first time that the Taurus did not have an optional engine. The base G and GL had the 3.0 L Vulcan V6, which was a carryover from the previous model. The top of the line LX, however, got the new 3.0 L DOHC Duratec 30 V6, which produced and was developed specifically for this model. When Ford rearranged the Taurus models for 1998, the Vulcan became the standard engine, and the Duratec became optional on all models. However, it was only available on the SE for 1999. Vulcan-equipped models came with the 4-speed AX4S automatic transmission, while Duratec equipped models got the 4-speed AX4N transmission. However, some Vulcan-equipped models randomly received the AX4N transmission for no apparent reason. Ford also deleted some features for the 1999 model year, in order to lower the price about one thousand dollars, and possibly boost sales. Some deleted features included the floor courtesy lamps and glove compartment lamp.\n\nThe Mercury Sable, a sister model of the Taurus aimed at a more upscale audience, was also redesigned for the 1996 model year. As with previous generations, the Sable used the same mechanical parts as the Taurus with a unique body. However, the 1996 Sable ditched the unique styling cues of the previous generations, specifically the front light bar, wrap-around rear glass and skirted rear fender, in favor of a design theme that more closely resembled that of the Taurus. For the first time, the Sable didn't get a unique interior design, instead sharing all of its interior components with the Taurus with the exception of branded parts and unique woodgrain trim. Trim levels were carried over from the previous generation: A base model GS and high-end LS, with a base G model briefly offered during the second half of the 1996 model year. The Sable had the same powertrains and most features as the Taurus, though no counterpart to the high performance SHO was offered. Prices increased, corresponding with the increase in price of the Taurus. While the design of the Taurus was polarizing, the more conservative design of the Sable was praised; Automobile Magazine selected the Sable as its 1996 design of the year.Unlike the Taurus, sales of the Sable stayed steady with the previous model, and styling revisions for 1998 were limited to a new grille and headlamps. The Taurus and Sable were produced concurrently through the 1999 model year.\n\nAs with the Taurus, a new model of Taurus SHO was launched for 1996. Although it contained less aggressive styling than its predecessors, it still differed from the normal Taurus with different seats, wheels, bumpers, drivetrain, as well as a fin being put on the driver's side windshield wiper, to keep it on the windshield at high speeds. A aluminum 3.4 L V8 engine with heads from Yamaha and block from Cosworth was specified for the SHO model, but it was given the same 4-speed transmission as the LX: the manual gearbox option was no longer offered on the SHO. This was partly because this model of SHO was designed for comfort instead of performance.\n\nThis model of SHO gained a reputation of being trouble-prone due to a large number of engine failures at around the 50,000-mile mark due to separation of the camshaft from its sprocket. However, this problem can be rectified by having the camshafts welded. Possibly because of this, this generation of SHO sold substantially worse than its predecessor, with sales peaking at 9,000 in 1997. As a result, the SHO was discontinued outright after the 1999 model year.\n\nIn addition to being sold in the North American market, third-generation models were exported to many other countries, in left and right-hand drive configurations. However, this action proved unsuccessful. Export models wore Mercury Sable headlamps and a unique front bumper cover to conform to these countries' regulations regarding automotive lighting. Right-hand-drive versions used a hand-operated parking brake in the center console instead of a foot-operated parking brake as in North America. Australians tended to stay away from the Taurus, due to their high price; a well-equipped, larger-engined rear-wheel drive Ford Fairmont cost around the same amount. Australian buyers could only opt for a single-spec sedan, known as the \"Taurus Ghia\". However, in New Zealand the Taurus was quite popular. While Australia only saw the 1996 model, both the sedan and station wagon models were sold in New Zealand from 1996 to 1998. \n\nLaunched while Japan was in a recession following the 1991 collapse of the Japanese asset price bubble, this generation Taurus was exported to Japan in limited numbers, and sold at Japanese auto dealerships called Autorama (a joint venture with Mazda), where the sedan and wagon versions with right-hand driving positions until 1997. To Japanese buyers, it was regarded as a luxury vehicle as the exterior dimensions and engine displacement exceeded Japanese Government regulations, and buyers in Japan were liable for additional taxes. The engine displacement also put operating costs for Japanese owners in a higher annual road tax obligation.\n\nIn 1999, Ford launched the fourth-generation Ford Taurus to replace the third generation. Although the doors and mechanical parts were carried over, this generation was designed with a more conservative design in hopes of increasing its appeal to customers. This new shape contained a more upright trunk and rear roof panel, of which increased headroom and trunk space substantially. This generation also contained a redesigned interior that sported a more conservative look, while retaining many features offered in the third generation.\n\nHowever, this model of Taurus is most notable for offering a special system that Ford called the \"Personal Safety System\". This system had sensors in the seat that detected a passenger's weight and position, and inflated the airbags to match. This generation of Taurus was sold from 1999 through the beginning of the 2007 model year, in which it was discontinued with the entire Taurus nameplate. However, the Taurus nameplate would later be revived in a matter of months due to the insistence of then-new Ford CEO Alan Mulally.\n\n"}
{"id": "28494297", "url": "https://en.wikipedia.org/wiki?curid=28494297", "title": "François Massieu", "text": "François Massieu\n\nFrançois Jacques Dominique Massieu (4 August 1832 – 5 February 1896) was a French thermodynamics engineer noted for his two 1869 characteristic functions, each of which known as a Massieu function (the first of which sometimes called free entropy), as cited by American engineer Willard Gibbs in his 1876 \"On the Equilibrium of Heterogeneous Substances\". \n\n"}
{"id": "23864675", "url": "https://en.wikipedia.org/wiki?curid=23864675", "title": "Full City oil spill", "text": "Full City oil spill\n\nThe Full City Oil Spill is a major fuel oil spill incident that occurred on July 31, 2009 when the bulk carrier Full City ran aground on the island of Såstein / Saastein south of Langesund, Telemark, Norway. The ship, said to be operated by COSCO (H.K.) Shipping Co. Ltd., spilled around 700,000 kg, or 200 tons, of IFO-380 heavy fuel oil. The oil contaminated 75 km of Norwegian coastline, including Langesund, Vestfold, and the Lille Såstein Bird Sanctuary. There were oil slicks in approximately 200 locations along the shoreline between Larvik Municipality and\nLilles. Thousands of sea birds were covered in oil, and although volunteers made efforts to save them, many of the birds had to be shot due to the irreversible damage to their health. The Institute of Marine Research ran tests on the affected areas to track any significant ecological impacts but noted that the marine and fish life suffered no significant changes. The research was included in a study of four oil spills that occurred in the Norwegian coastal area, including the Rocknes Oil Spill, the Server Oil Spill, and the Godafoss Oil Spill.\n\nThe Master and Third Officer of the ship were both charged with violating the Pollution Act due to their failure to take adequate measures to prevent pollution. The Master was sentenced to 6 months with 120 days suspended, and the Third Officer was sentenced to 60 days with 39 days suspended. As of April 2010 the ship was in Gothenburg for repairs in drydock.\n\n"}
{"id": "23387527", "url": "https://en.wikipedia.org/wiki?curid=23387527", "title": "Hohm", "text": "Hohm\n\nMicrosoft Hohm was an online web application by Microsoft that enables consumers to analyze their energy usage and provides energy saving recommendations.\n\nAnnounced on June 24, 2009, Microsoft Hohm was built on the Windows Azure cloud operating system. Microsoft licensed the Home Energy Saver energy simulation program developed at Lawrence Berkeley National Laboratory to drive Microsoft Hohm. It was publicly released on July 6, 2009.\n\nHome Energy Saver and Hohm received an R&D 100 Award in 2010.\n\nThe Hohm service was discontinued on May 31, 2012 due to a lack of consumer uptake.\n\n\n\n"}
{"id": "41239386", "url": "https://en.wikipedia.org/wiki?curid=41239386", "title": "Hydnocarpic acid", "text": "Hydnocarpic acid\n\nHydnocarpic acid is an unsaturated fatty acid. It differs from most fatty acids by having a cyclic ring system at the terminus, rather than being entirely straight chain. It is found in the oil from plants of the genus \"Hydnocarpus\" from which it derives its name.\n"}
{"id": "13578878", "url": "https://en.wikipedia.org/wiki?curid=13578878", "title": "Impulse excitation technique", "text": "Impulse excitation technique\n\nThe impulse excitation technique (IET) is a non-destructive material characterization technique to determine the elastic properties and internal friction of a material of interest. It measures the resonant frequencies in order to calculate the Young's modulus, shear modulus, Poisson's ratio and internal friction of predefined shapes like rectangular bars, cylindrical rods and disc shaped samples. The measurements can be performed at room temperature or at elevated temperatures (up to 1700 °C) under different atmospheres.\n\nThe measurement principle is based on tapping the sample with a small projectile and recording the induced vibration signal with a piezoelectric sensor, microphone, laser vibrometer or accelerometer. To optimize the results a microphone or a laser vibrometer can be used as there is no contact between the test-piece and the sensor. Laser vibrometers are preferred to measure signals in vacuum. Afterwards, the acquired vibration signal in the time domain is converted to the frequency domain by a fast Fourier transformation. Dedicated software will determine the resonant frequency with high accuracy to calculate the elastic properties based on the classical beam theory.\n\nDifferent resonant frequencies can be excited dependent on the position of the support wires, the mechanical impulse and the microphone. The two most important resonant frequencies are the flexural which is controlled by the Young's modulus of the sample and the torsional which is controlled by the shear modulus for isotropic materials.\n\nFor predefined shapes like rectangular bars, discs, rods and grinding wheels, dedicated software calculates the sample's elastic properties using the sample dimensions, weight and resonant frequency (ASTM E1876-15).\n\nThe first figure gives an example of a test-piece vibrating in the flexure\nmode. This induced vibration is also referred as the out-of-plane vibration mode. The in-plane vibration will be excited by turning the sample 90° on the axis parallel to its length. The natural frequency of this flexural vibration mode is characteristic for the dynamic Young's modulus.\nTo minimize the damping of the test-piece, it has to be supported at the nodes where the vibration amplitude is zero. The test-piece is mechanically excited at one of the anti-nodes to cause maximum vibration.\n\nThe second figure gives an example of a test-piece vibrating in the torsion mode. The natural frequency of this vibration is characteristic for the shear modulus. \nTo minimize the damping of the test-piece, it has to be supported at the center of both axis. The mechanical excitation has to be performed in one corner in order to twist the beam rather than flexing it.\n\nThe Poisson's ratio is a measure in which a material tends to expand in directions perpendicular to the direction of compression. After measuring the Young's modulus and the shear modulus, dedicated software determines the Poisson's ratio using Hooke's law which can only be applied to isotropic materials according to the different standards.\n\nMaterial damping or internal friction is characterized by the decay of the vibration amplitude of the sample in free vibration as the logarithmic decrement. The damping behaviour originates from anelastic processes occurring in a strained solid i.e. thermoelastic damping, magnetic damping, viscous damping, defect damping, ... For example, different materials defects (dislocations, vacancies, ...) can contribute to an increase in the internal friction between the vibrating defects and the neighboring regions.\n\nConsidering the importance of elastic properties for design and engineering applications, a number of experimental techniques are developed and these can be classified into 2 groups; static and dynamic methods. Statics methods (like the four-point bending test and nanoindentation) are based on direct measurements of stresses and strains during mechanical tests. Dynamic methods (like ultrasound spectroscopy and impulse excitation technique) provide an advantage over static methods because the measurements are relatively quick and simple and involve small elastic strains. Therefore, IET is very suitable for porous and brittle materials like ceramics, refractories,… The technique can also be easily modified for high temperature experiments and only a small amount of material needs to be available.\n\nThe most important parameters to define the measurement uncertainty are the mass and dimensions of the sample. Therefore, each parameter has to be measured (and prepared) to a level of accuracy of 0.1%. Especially, the sample thickness is most critical (third power in the equation for Young's modulus). In that case, an overall accuracy of 1% can be obtained practically in most applications.\n\nThe impulse excitation technique can be used in a wide range of applications. Nowadays, IET equipment can perform measurements between −50 °C and 1700 °C in different atmospheres (air, inert, vacuum). IET is mostly used in research and as quality control tool to study the transitions as function of time and temperature.\nA detailed insight into the material crystal structure can be obtained by studying the elastic and damping properties. For example, the interaction of dislocations and point defects in carbon steels are studied. Also the material damage accumulated during a thermal shock treatment can be determined for refractory materials. This can be an advantage in understanding the physical properties of certain materials.\nFinally, the technique can be used to check the quality of systems. In this case, a reference piece is required to obtain a reference frequency spectrum. Engine blocks for example can be tested by tapping them and comparing the recorded signal with a pre-recorded signal of a reference engine block.\n\nwith\n\nwith\n\n\"G\" the shear modulus\n\nwith\n\nwith\n\nIf the Young's modulus and shear modulus are known, the Poisson's ratio can be calculated according to:\n\nThe induced vibration signal (in the time domain) is fitted as a sum of exponentially damped sinusoidal functions according to:\n\nwith\n\n"}
{"id": "36714637", "url": "https://en.wikipedia.org/wiki?curid=36714637", "title": "Institute of Energy Conversion", "text": "Institute of Energy Conversion\n\nThe Institute of Energy Conversion (IEC), located at the University of Delaware is the oldest solar energy research institute in the world. It was established by Karl Boer in 1972 to pioneer research on thin film solar cells. The IEC performs independent research and collaborates with businesses alongside training undergraduate and graduate students in solar cell engineering. In 2011, the organization was the highest recipient of the United States Department of Energy's (DOE) SunShot Initiative and was awarded five grants totaling $9.1 million to research next generation solar cells to reduce the cost of solar cells by 75 percent by the end of the decade.\n"}
{"id": "888571", "url": "https://en.wikipedia.org/wiki?curid=888571", "title": "Journey to the Ants", "text": "Journey to the Ants\n\nJourney to the Ants: a Story of Scientific Exploration is a 1994 book by Bert Hölldobler and Edward O. Wilson. The book was written as a popularized account for the layman of the science earlier presented in their winner of the Pulitzer Prize for General Non-Fiction in 1991, \"The Ants\" (1990).\n\n"}
{"id": "21547084", "url": "https://en.wikipedia.org/wiki?curid=21547084", "title": "Kingsbury Colliery", "text": "Kingsbury Colliery\n\nKingsbury Colliery was a coal mine in Kingsbury, Warwickshire, which operated between 1897 and 1968.\n\nIt was opened in 1897 and changed the nature of the village almost overnight from a predominantly agriculturally based community to a mining village, and helped Kingsbury's expansion. Coal extracted from Kingsbury Colliery was used mainly for industry in nearby Birmingham, although the Lurghi Gas Plant at Coleshill was also a major customer. \n\nThe colliery operated throughout the first half of the 20th century, and in 1904 the village of Piccadilly was built close by to house some of the mine's workers. Following the pit's closure in 1968, some of the land was used for the construction of the Kingsbury Oil Terminal.\n\nIn 2009 a memorial wall was built in Piccadilly to remember those who worked in both Kingsbury Colliery, and the neighbouring Dexter Colliery. The wall contains the names of all the miners who worked at both mines. The centrepiece of the wall is a miner's lamp that is always lit to commemorate those who have died and those who remember working down the mines.\n\nNotable people who have worked at the colliery include the footballer Sid Ireland.\n\n\n"}
{"id": "3454341", "url": "https://en.wikipedia.org/wiki?curid=3454341", "title": "Kraft paper", "text": "Kraft paper\n\nKraft paper or kraft is paper or paperboard (cardboard) produced from chemical pulp produced in the kraft process.\n\n\"Sack kraft paper\", or just \"sack paper\", is a porous kraft paper with high elasticity and high tear resistance, designed for packaging products with high demands for strength and durability.\n\nPulp produced by the kraft process is stronger than that made by other pulping processes; acidic sulfite processes degrade cellulose more, leading to weaker fibers, and mechanical pulping processes leave most of the lignin with the fibers, whereas kraft pulping removes most of the lignin present originally in the wood. Low lignin is important to the resulting strength of the paper, as the hydrophobic nature of lignin interferes with the formation of the hydrogen bonds between cellulose (and hemicellulose) in the fibers.\n\nKraft pulp is darker than other wood pulps, but it can be bleached to make very white pulp. Fully bleached kraft pulp is used to make high quality paper where strength, whiteness, and resistance to yellowing are important.\n\nWood pulp for sack paper is made from softwood by the kraft process. The long fibers provide the paper its strength and wet strength chemicals are added to even further improve the strength. Both white and brown grades are made. Sack paper is then produced on a paper machine from the wood pulp. The paper is microcrepped to give porosity and elasticity. Microcrepping is done by drying with loose draws allowing it to shrink. This causes the paper to elongate 4% in the machine direction and 10% in the cross direction without breaking. Machine direction elongation can be further improved by pressing between very elastic cylinders causing more microcrepping. The paper may be coated with polyethylene (PE) to ensure an effective barrier against moisture, grease and bacteria. A paper sack can be made of several layers of sack paper depending on the toughness needed.\n\nKraft paper is produced on paper machines with moderate machine speeds. The raw material is normally softwood pulp from the kraft process.\n\nMaintaining a high effective sulfur ratio or sulfidity is important for the highest possible strength using the kraft process.\n\nThe kraft process can use a wider range of fiber sources than most other pulping processes. All types of wood, including very resinous types like southern pine, and non-wood species like bamboo and kenaf can be used in the kraft process.\n\n\n\n\n\n"}
{"id": "37776", "url": "https://en.wikipedia.org/wiki?curid=37776", "title": "Lyonesse", "text": "Lyonesse\n\nLyonesse is a country in Arthurian legend, particularly in the story of Tristan and Iseult. Said to border Cornwall, it is most notable as the home of the hero Tristan, whose father was king. In later traditions Lyonesse is said to have sunk beneath the waves some time after the Tristan stories take place, making it similar to Ys and other lost lands in medieval Celtic tales, and perhaps connecting it with the Isles of Scilly.\n\nIn medieval Arthurian legend, there are no references to the sinking of Lyonesse, because the name originally referred to a still-existing place. Lyonesse is an English alteration of French \"Léoneis\" or \"Léonois\" (earlier \"Loönois\"), a development of \"Lodonesia\", the Latin name for Lothian in Scotland. Continental writers of Arthurian romances were often puzzled by the internal geography of Great Britain; thus it is that the author French Prose \"Tristan\" appears to place Léonois contiguous, by land, to Cornwall. \n\nIn English adaptations of the French tales, Léonois, now \"Lyonesse\", becomes a kingdom wholly distinct from Lothian, and closely associated with the Cornish region, though its exact geographical location remained unspecified. The name was not attached to Cornish legends of lost coastal lands until the reign of Elizabeth I of England, however. However, the legendary lost land between Land's End and Scilly has a distinct Cornish name: \"Lethowsow\". This derives from the Cornish name for the Seven Stones reef, on the reputed site of the lost land's capital and the site of the notorious wreck of the . The name means \"the milky ones\", from the constant white water surrounding the reef.\n\nAlfred, Lord Tennyson's Arthurian epic \"Idylls of the King\" describes Lyonesse as the site of the final battle between Arthur and Mordred. One passage in particular references legends of Lyonesse as a land fated to sink beneath the ocean:\n\n<poem>Then rose the King and moved his host by night\nAnd ever pushed Sir Mordred, league by league,\nBack to the sunset bound of Lyonesse—\nA land of old upheaven from the abyss\nBy fire, to sink into the abyss again;\nWhere fragments of forgotten peoples dwelt,\nAnd the long mountains ended in a coast\nOf ever-shifting sand, and far away\nThe phantom circle of a moaning sea.</poem>\n\nDeriving from a false etymology of Lyonesse, the 'City of Lions' was said in some later traditions to be the capital of the legendary kingdom, situated on what is today the Seven Stones Reef, some eighteen miles west of Land's End and eight miles north-east of the Isles of Scilly.\n\nThe legend of a sunken kingdom appears in both Cornish and Breton mythology. In Christian times it came to be viewed as a sort of Cornish Sodom and Gomorrah, an example of divine wrath provoked by unvirtuous living.\n\nThere is a Breton parallel in the tale of the Cité d'Ys, similarly drowned as a result of its debauchery with a single virtuous survivor escaping on a horse, in this case King Gradlon. The Welsh equivalent to Lyonesse and Ker Ys is Cantre'r Gwaelod, a legendary drowned kingdom in Cardigan Bay.\n\nIt is often suggested that the tale of Lyonesse represents an extraordinary survival of folk memory of the flooding of the Isles of Scilly and Mount's Bay near Penzance. For example, the Cornish name of St Michael's Mount is \"Karrek Loos y'n Koos\" - literally, \"the grey rock in the wood\". Cornish people around Penzance still get occasional glimpses at extreme low water of a sunken forest in Mount's Bay, where petrified tree stumps become visible. The importance of the maintenance of this memory can be seen in that it came to be associated with the legendary Brython hero Arthur, although the date of its inundation is actually c. 2500 BC.\n\n\n\n\n\n\n\n\n"}
{"id": "277295", "url": "https://en.wikipedia.org/wiki?curid=277295", "title": "Magnetite", "text": "Magnetite\n\nMagnetite is a rock mineral and one of the main iron ores, with the chemical formula FeO. It is one of the oxides of iron, and is ferrimagnetic; it is attracted to a magnet and can be magnetized to become a permanent magnet itself. It is the most magnetic of all the naturally-occurring minerals on Earth. Naturally-magnetized pieces of magnetite, called lodestone, will attract small pieces of iron, which is how ancient peoples first discovered the property of magnetism. Today it is mined as iron ore.\n\nSmall grains of magnetite occur in almost all igneous and metamorphic rocks. Magnetite is black or brownish-black with a metallic luster, has a Mohs hardness of 5–6 and leaves a black streak.\n\nThe chemical IUPAC name is iron(II,III) oxide and the common chemical name is ferrous-ferric oxide.\n\nIn addition to igneous rocks, magnetite also occurs in sedimentary rocks, including banded iron formations and in lake and marine sediments as both detrital grains and as magnetofossils. Magnetite nanoparticles are also thought to form in soils, where they probably oxidize rapidly to maghemite.\n\nThe chemical composition of magnetite is FeFeO. The main details of its structure were established in 1915. It was one of the first crystal structures to be obtained using X-ray diffraction. The structure is inverse spinel, with O ions forming a face centered cubic lattice and iron cations occupying interstitial sites. Half of the Fe cations occupy tetrahedral sites while the other half, along with Fe cations, occupy octahedral sites. The unit cell consists of 32 O ions and unit cell length is \"a\" = 0.839 nm.\n\nMagnetite contains both ferrous and ferric iron, requiring environments containing intermediate levels of oxygen availability to form.\n\nMagnetite differs from most other iron oxides in that it contains both divalent and trivalent iron.\n\nAs a member of the spinel group, magnetite can form solid solutions with similarly structured minerals, including ulvospinel (FeTiO), hercynite (FeAlO) and chromite (FeCrO).\nTitanomagnetite, also known as titaniferous magnetite, is a solid solution between magnetite and ulvospinel that crystallizes in many mafic igneous rocks. Titanomagnetite may undergo oxyexsolution during cooling, resulting in ingrowths of magnetite and ilmenite.\n\nNatural and synthetic magnetite occurs most commonly as octahedral crystals bounded by {111} planes and as rhombic-dodecahedra. Twinning occurs on the {111} plane.\n\nHydrothermal synthesis usually produce single octahedral crystals which can be as large as 10mm across. In the presence of mineralizers such as 0.1M HI or 2M NHCl and at 0.207 MPa at 416-800 °C, magnetite grew as crystals whose shapes were a combination of rhombic-dodechahedra forms. The crystals were more rounded than usual. The appearance of higher forms was considered as a result from a decrease in the surface energies caused by the lower surface to volume ratio in the rounded crystals.\n\nMagnetite has been important in understanding the conditions under which rocks form. Magnetite reacts with oxygen to produce hematite, and the mineral pair forms a buffer that can control oxygen fugacity. Commonly, igneous rocks contain solid solutions of both titanomagnetite and hemoilmenite or titanohematite. Compositions of the mineral pairs are used to calculate how oxidizing was the magma (i.e., the oxygen fugacity of the magma): a range of oxidizing conditions are found in magmas and the oxidation state helps to determine how the magmas might evolve by fractional crystallization. Magnetite also is produced from peridotites and dunites by serpentinization.\n\nLodestones were used as an early form of magnetic compass. Magnetite typically carries the dominant magnetic signature in rocks, and so it has been a critical tool in paleomagnetism, a science important in understanding plate tectonics and as historic data for magnetohydrodynamics and other scientific fields.\n\nThe relationships between magnetite and other iron-rich oxide minerals such as ilmenite, hematite, and ulvospinel have been much studied; the reactions between these minerals and oxygen influence how and when magnetite preserves a record of the Earth's magnetic field.\n\nAt low temperatures, magnetite undergoes a crystal structure phase transition from a monoclinic structure to a cubic structure known as the Verwey transition. Optical studies show that this metal to insulator transition is sharp and occurs around 120 K. The Verwey transition is dependent on grain size, domain state, pressure, and the iron-oxygen stoichiometry. An isotropic point also occurs near the Verwey transition around 130 K, at which point the sign of the magnetocrystalline anisotropy constant changes from positive to negative. The Curie temperature of magnetite is .\n\nIf magnetite is in a large enough quantity it can be found in aeromagnetic surveys using a magnetometer which measures magnetic intensities.\n\nMagnetite is sometimes found in large quantities in beach sand. Such black sands (mineral sands or iron sands) are found in various places, such as Lung Kwu Tan of Hong Kong; California, United States; and the west coast of the North Island of New Zealand. The magnetite, eroded from rocks, is carried to the beach by rivers and concentrated by wave action and currents. Huge deposits have been found in banded iron formations. These sedimentary rocks have been used to infer changes in the oxygen content of the atmosphere of the Earth.\n\nRemote sensing has the potential to be a big part in locating magnetite sands as even small amounts of magnetite in sand can drastically alter the sands albedo which is the amount of electromagnetic radiation the sand will reflect. The darker magnetite will lower the sands albedo compared to sands that do not contain magnetite.\n\nLarge deposits of magnetite are also found in the Atacama region of Chile; the Valentines region of Uruguay; Kiruna, Sweden; the Pilbara, Midwest and Northern Goldfields regions in Western Australia; the Eyre Peninsula in South Australia; the Tallawang Region of New South Wales; and in the Adirondack region of New York in the United States. Kediet ej Jill, the highest mountain of Mauritania, is made entirely of the mineral. Deposits are also found in Norway, Germany, Italy, Switzerland, South Africa, India, Indonesia, Mexico, Hong Kong, and in Oregon, New Jersey, Pennsylvania, North Carolina, West Virginia, Virginia, New Mexico, Utah, and Colorado in the United States. In 2005, an exploration company, Cardero Resources, discovered a vast deposit of magnetite-bearing sand dunes in Peru. The dune field covers 250 square kilometers (100 sq mi), with the highest dune at over 2,000 meters (6,560 ft) above the desert floor. The sand contains 10% magnetite.\n\nIn large enough quantities magnetite can affect compass navigation. In Tasmania there are many areas with highly magnetized rocks that can greatly influence compasses. Extra steps and repeated observations are required when using a compass in Tasmania to keep navigation problems to the minimum.\n\nMagnetite crystals with a cubic habit have been found in just one location: Balmat, St. Lawrence County, New York.\n\nMagnetite can also be found in fossils due to biomineralization and are referred to as magnetofossils. There are also instances of magnetite with origins in space coming from meteorites.\n\nBiomagnetism is usually related to the presence of biogenic crystals of magnetite, which occur widely in organisms. These organisms range from bacteria (e.g., \"Magnetospirillum magnetotacticum\") to animals, including humans, where magnetite crystals (and other magnetically-sensitive compounds) are found in different organs, depending on the species. Biomagnetites account for the effects of weak magnetic fields on biological systems. There is also a chemical basis for cellular sensitivity to electric and magnetic fields (galvanotaxis).\n\nPure magnetite particles are biomineralized in magnetosomes, which are produced by several species of magnetotactic bacteria. Magnetosomes consist of long chains of oriented magnetite particle that are used by bacteria for navigation. After the death of these bacteria, the magnetite particles in magnetosomes may be preserved in sediments as magnetofossils. Some types of anaerobic bacteria that are not magnetotactic can also create magnetite in oxygen free sediments by reducing amorphic ferric oxide to magnetite.\n\nSeveral species of birds are known to incorporate magnetite crystals in the upper beak for magnetoreception, which (in conjunction with cryptochromes in the retina) gives them the ability to sense the direction, polarity, and magnitude of the ambient magnetic field.\n\nChitons, a type of mollusk, have a tongue-like structure known as a radula, covered with magnetite-coated teeth, or denticles. The hardness of the magnetite helps in breaking down food, and its magnetic properties may additionally aid in navigation. It has also been proposed that biological magnetite may store information.\n\nLiving organisms can produce magnetite. In humans, magnetite can be found in various parts of the brain including the frontal, parietal, occipital, and temporal lobes, brainstem, cerebellum and basal ganglia. Iron can be found in three forms in the brain – magnetite, hemoglobin (blood) and ferritin (protein), and areas of the brain related to motor function generally contain more iron. Magnetite can be found in the hippocampus. The hippocampus is associated with information processing, specifically learning and memory. However, magnetite can have toxic effects due to its charge or magnetic nature and its involvement in oxidative stress or the production of free radicals. Research suggests that beta-amyloid plaques and tau proteins associated with neurodegenerative disease frequently occur after oxidative stress and the build-up of iron.\n\nSome researchers also suggest that humans possess a magnetic sense, proposing that this could allow certain people to use magnetoreception for navigation. The role of magnetite in the brain is still not well understood, and there has been a general lag in applying more modern, interdisciplinary techniques to the study of biomagnetism.\n\nElectron microscope scans of human brain-tissue samples are able to differentiate between magnetite produced by the body's own cells and magnetite absorbed from airborne pollution, the natural forms being jagged and crystalline, while magnetite pollution occurs as rounded nanoparticles. Potentially a human health hazard, airborne magnetite is a result of pollution specifically combustion. These nanoparticles can travel to the brain via the olfactory nerve increasing the concentration of magnetite in the brain. In some brain samples, the nanoparticle pollution outnumbers the natural particles by as much as 100:1, and such pollution-borne magnetite particles may be linked to abnormal neural deterioration. In one study, the characteristic nanoparticles were found in the brains of 37 people: 29 of these, aged 3 to 85, had lived and died in Mexico City, a significant air pollution hotspot. A further eight, aged 62 to 92, came from Manchester, and some had died with varying severities of neurodegenerative diseases. According to researchers led by Prof. Barbara Maher at Lancaster University and published in the Proceedings of the National Academy of Sciences, such particles could conceivably contribute to diseases like Alzheimer's disease. Though a causal link has not been established, laboratory studies suggest that iron oxides like magnetite are a component of protein plaques in the brain, linked to Alzheimer's disease.\n\nIncreased iron levels, specifically magnetic iron, have been found portions of the brain in Alzheimer's patients. Monitoring changes in iron concentrations may make it possible to detect the loss of neurons and the development of neurodegenerative diseases prior to the onset of symptoms due to the relationship between magnetite and ferritin. In tissue, magnetite and ferritin can produce small magnetic fields which will interact with magnetic resonance imaging (MRI) creating contrast. Huntington patients have not shown increased magnetite levels; however, high levels have been found in study mice.\n\nDue to its high iron content, magnetite has long been a major iron ore. It is reduced in blast furnaces to pig iron or sponge iron for conversion to steel.\n\nAudio recording using magnetic acetate tape was developed in the 1930s. The German magnetophon utilized magnetite powder as the recording medium. Following World War II, 3M Company continued work on the German design. In 1946, the 3M researchers found they could improve the magnetite-based tape, which utilized powders of cubic crystals, by replacing the magnetite with needle-shaped particles of gamma ferric oxide (γ-FeO).\n\nMagnetite is the catalyst for the industrial synthesis of ammonia. It is also used to catalyze the breakdown of hydrogen peroxide into hydroxyl free radicals and to decompose organic contaminants such as \"p\"-nitrophenol (\"p\"-NP), which results from chemical processes such as oil refining, petrochemical manufacturing, pulp and paper mills, and the production of many paints, plastics, and pesticides. The removal of such contaminants is an important environmental application.\n\nMagnetite micro- and nanoparticles are used in a variety of applications, from biomedical to environmental. One use is in water purification: in high gradient magnetic separation, magnetite nanoparticles introduced into contaminated water will bind to the suspended particles (solids, bacteria, or plankton, for example) and settle to the bottom of the fluid, allowing the contaminants to be removed and the magnetite particles to be recycled and reused. This method works with radioactive and carcinogenic particles as well, making it an important cleanup tool in the case of heavy metals introduced into water systems. These heavy metals can enter watersheds due to a variety of industrial processes that produce them, which are in use across the country. Being able to remove contaminants from potential drinking water for citizens is an important application, as it greatly reduces the health risks associated with drinking contaminated water.\n\nAnother application of magnetic nanoparticles is in the creation of ferrofluids. These are used in several ways, in addition to being fun to play with. Ferrofluids can be used for targeted drug delivery in the human body. The magnetization of the particles bound with drug molecules allows “magnetic dragging” of the solution to the desired area of the body. This would allow the treatment of only a small area of the body, rather than the body as a whole, and could be highly useful in cancer treatment, among other things. Ferrofluids are also used in magnetic resonance imaging (MRI) technology.\n\n\n\n"}
{"id": "34976154", "url": "https://en.wikipedia.org/wiki?curid=34976154", "title": "Matthew L. Scullin", "text": "Matthew L. Scullin\n\nMatthew L. Scullin is a San Francisco-based scientist, energy entrepreneur, and musician.\n\nHe is currently the CEO of Hayward, California based Alphabet Energy, which he co-founded in 2008 with Peidong Yang. Alphabet Energy is the first company to manufacture silicon thermoelectric devices and deploy commercial thermoelectric waste heat recovery systems.\n\nIn 2013, Alphabet Energy was named a Technology Pioneer by the World Economic Forum. Scullin received the company's recognition at a ceremony in China in September 2013 and subsequently attended the World Economic Forum in Davos in January 2014. Alphabet Energy has previously won the Cleantech Open's People's Choice, National Runner Up, California Sustainability, and California Finalist awards. The company has also won awards from Opportunity Green and R&D100, and is backed by leading investors in the field of clean energy technology including TPG, Claremont Creek Ventures, and CalCEF. For his pioneering work in the field of alternative energy, Scullin was named to the \"Forbes\" list of \"30 Under 30\" in 2012.\n\nScullin was born and raised in the Alphabet City neighborhood of Manhattan, where he attended the United Nations International School. He graduated from the University of Pennsylvania \"magna cum laude\", where he won the R.M. Brick Award in Materials Science, and then went on to complete his Ph.D. in Materials Science at the University of California, Berkeley with adviser Arun Majumdar. He is the author of several peer-reviewed publications and patents, and has lectured about cleantech and entrepreneurship at universities or labs including the Kellogg School of Management, Haas School of Business, HEC Paris, University of Pennsylvania, Stanford, and the Lawrence Berkeley National Laboratory. He has also authored work on natural resources, including an analysis of the element Tellurium.\n\nMost of Scullin's work relates to ideas in crystallography and symmetry. In 2007 Scullin collaborated with architects Aranda\\Lasch on a commissioned piece entitled \"Rules of Six\" for the exhibit Design and the Elastic Mind at the MoMA in New York. Since then Scullin has shown individual work.\n\nHe is also a DJ and musician, and has performed with and DJed for M.I.A. and BC Kingdom.\n"}
{"id": "31099377", "url": "https://en.wikipedia.org/wiki?curid=31099377", "title": "Mauro Solar Riser", "text": "Mauro Solar Riser\n\nThe Mauro Solar Riser is an American biplane ultralight electric aircraft that was the first manned aircraft to fly on solar power. It was also only the second solar-powered aircraft to fly, after the unmanned AstroFlight Sunrise, which had first flown 4 1/2 years earlier.\n\nThe president of Ultralight Flying Machines, Larry Mauro, created the Solar Riser by converting a stock UFM Easy Riser hang glider to solar power. Normally foot-launched, the Solar Riser had wheeled landing gear added. Power is supplied by a Bosch electric starter motor of connected to a 30 volt DC Nickel-cadmium battery pack taken from a Hughes 500 helicopter, powering a propeller through a reduction drive made from a timing belt and two pulleys. The battery is charged by a series of photovoltaic solar panels mounted in the top wing that provide 350 Watts of power. The solar cells are not sufficient to provide power in flight and all flights were made by recharging the battery on the ground from the solar cells and then flying using energy stored in the battery. A charge in bright sunshine for an hour and a half yields a flight of 3–5 minutes.\n\nBecause the battery power is enough to launch the aircraft for a soaring flight it is theoretically possible to launch on battery power, soar while the batteries are being charged by sunlight and then continue powered flight. The Solar Riser did not employ the most efficient cells available at the time and the upper wing had room for twice the number of cells to be installed. Early plans called for upgrading and increasing the number of cells so that sustained electric flight could be made, using only solar energy and not battery power, but these plans were never completed.\n\nThe Solar Riser made the first man-carrying flight on solar power at noon on 29 April 1979 at Flabob Airport in Riverside, California. The aircraft reached a maximum height of about and flew . A number of other flights of similar height and duration were flown, including demonstration flights at EAA AirVenture Oshkosh before the aircraft was retired to a museum.\n\n\n"}
{"id": "188896", "url": "https://en.wikipedia.org/wiki?curid=188896", "title": "Neutron moderator", "text": "Neutron moderator\n\nIn nuclear engineering, a neutron moderator is a medium that reduces the speed of fast neutrons, thereby turning them into thermal neutrons capable of sustaining a nuclear chain reaction involving uranium-235 or a similar fissile nuclide.\n\nCommonly used moderators include regular (light) water (roughly 75% of the world's reactors), solid graphite (20% of reactors) and heavy water (5% of reactors).\nBeryllium has also been used in some experimental types, and hydrocarbons have been suggested as another possibility.\n\nNeutrons are normally bound into an atomic nucleus, and do not exist free for long in nature. The unbound neutron has a half-life of 10 minutes and 11 seconds. The release of neutrons from the nucleus requires exceeding the binding energy of the neutron, which is typically 7-9 MeV for most isotopes. Neutron sources generate free neutrons by a variety of nuclear reactions, including nuclear fission and nuclear fusion. Whatever the source of neutrons, they are released with energies of several MeV.\n\nAccording to the equipartition theorem, the average kinetic energy, formula_1, can be related to temperature, formula_2,via:\nwhere formula_4 is the neutron mass, formula_5 is the average neutron speed, and formula_6 is the Boltzmann constant. The characteristic neutron temperature of several-MeV neutrons is several tens of billions kelvin.\n\nModeration is the process of the reduction of the initial high speed (high kinetic energy) of the free neutron. Since energy is conserved, this reduction of the neutron speed takes place by transfer of energy to a material called a \"moderator\".\n\nThe probability of scattering of a neutron from a nucleus is given by the scattering cross section. The first couple of collisions with the moderator may be of sufficiently high energy to excite the nucleus of the moderator. Such a collision is inelastic, since some of the kinetic energy is transformed to potential energy by exciting some of the internal degrees of freedom of the nucleus to form an excited state. As the energy of the neutron is lowered, the collisions become predominantly elastic, i.e., the total kinetic energy and momentum of the system (that of the neutron and the nucleus) is conserved.\n\nGiven the , as neutrons are very light compared to most nuclei, the most efficient way of removing kinetic energy from the neutron is by choosing a moderating nucleus that has near identical mass.\n\nA collision of a neutron, which has mass of 1, with a H nucleus (a proton) could result in the neutron losing virtually all of its energy in a single head-on collision. More generally, it is necessary to take into account both glancing and head-on collisions. The \"mean logarithmic reduction of neutron energy per collision\", formula_7, depends only on the atomic mass, formula_8, of the nucleus and is given by:\n\nformula_9.\n\nThis can be reasonably approximated to the very simple form formula_10. From this one can deduce formula_11, the expected number of collisions of the neutron with nuclei of a given type that is required to reduce the kinetic energy of a neutron from formula_12 to formula_13\n\nSome nuclei have larger absorption cross sections than others, which removes free neutrons from the flux. Therefore, a further criterion for an efficient moderator is one for which this parameter is small. The \"moderating efficiency\" gives the ratio of the macroscopic cross sections of scattering, formula_15, weighted by formula_7 divided by that of absorption, formula_17: i.e., formula_18. For a compound moderator composed of more than one element, such as light or heavy water, it is necessary to take into account the moderating and absorbing effect of both the hydrogen isotope and oxygen atom to calculate formula_7. To bring a neutron from the fission energy of formula_12 2 MeV to an formula_21 of 1 eV takes an expected formula_11 of 16 and 29 collisions for HO and DO, respectively. Therefore, neutrons are more rapidly moderated by light water, as H has a far higher formula_15. However, it also has a far higher formula_17, so that the moderating efficiency is nearly 80 times higher for heavy water than for light water.\n\n\"The ideal moderator is of low mass, high scattering cross section, and low absorption cross section\".\nAfter sufficient impacts, the speed of the neutron will be comparable to the speed of the nuclei given by thermal motion; this neutron is then called a thermal neutron, and the process may also be termed \"thermalization\". Once at equilibrium at a given temperature the distribution of speeds (energies) expected of rigid spheres scattering elastically is given by the Maxwell–Boltzmann distribution. This is only slightly modified in a real moderator due to the speed (energy) dependence of the absorption cross-section of most materials, so that low-speed neutrons are preferentially absorbed, so that the true neutron velocity distribution in the core would be slightly hotter than predicted.\n\nIn a thermal-neutron reactor, the nucleus of a heavy fuel element such as uranium absorbs a slow-moving free neutron, becomes unstable, and then splits (\"fissions\") into two smaller atoms (\"fission products\"). The fission process for U nuclei yields two fission products, two to three fast-moving free neutrons, plus an amount of energy primarily manifested in the kinetic energy of the recoiling fission products. The free neutrons are emitted with a kinetic energy of ~2 MeV each. Because more free neutrons are released from a uranium fission event than thermal neutrons are required to initiate the event, the reaction can become self-sustaining — a chain reaction — under controlled conditions, thus liberating a tremendous amount of energy (see article nuclear fission).\n\nThe probability of further fission events is determined by the fission cross section, which is dependent upon the speed (energy) of the incident neutrons. For thermal reactors, high-energy neutrons in the MeV-range are much less likely to cause further fission. (Note: It is not \"impossible\" for fast neutrons to cause fission, just much less likely.) The newly released fast neutrons, moving at roughly 10% of the speed of light, must be slowed down or \"moderated,\" typically to speeds of a few kilometres per second, if they are to be likely to cause further fission in neighbouring U nuclei and hence continue the chain reaction. This speed happens to be equivalent to temperatures in the few hundred Celsius range.\n\nIn all moderated reactors, some neutrons of all energy levels will produce fission, including fast neutrons. Some reactors are more fully \"thermalised\" than others; for example, in a CANDU reactor nearly all fission reactions are produced by thermal neutrons, while in a pressurized water reactor (PWR) a considerable portion of the fissions are produced by higher-energy neutrons. In the proposed water-cooled supercritical water reactor (SCWR), the proportion of fast fissions may exceed 50%, making it technically a fast neutron reactor.\n\nA fast reactor uses no moderator, but relies on fission produced by unmoderated fast neutrons to sustain the chain reaction. In some fast reactor designs, up to 20% of fissions can come from direct fast neutron fission of uranium-238, an isotope which is not fissile at all with thermal neutrons.\n\nModerators are also used in non-reactor neutron sources, such as plutonium-beryllium and spallation sources.\n\nThe form and location of the moderator can greatly influence the cost and safety of a reactor. Classically, moderators were precision-machined blocks of high purity graphite with embedded ducting to carry away heat. They were in the hottest part of the reactor, and therefore subject to corrosion and ablation. In some materials, including graphite, the impact of the neutrons with the moderator can cause the moderator to accumulate dangerous amounts of Wigner energy. This problem led to the infamous Windscale fire at the Windscale Piles, a nuclear reactor complex in the United Kingdom, in 1957.\n\nSome pebble-bed reactors' moderators are not only simple, but also inexpensive: the nuclear fuel is embedded in spheres of reactor-grade pyrolytic carbon, roughly of the size of tennis balls. The spaces between the balls serve as ducting. The reactor is operated above the Wigner annealing temperature so that the graphite does not accumulate dangerous amounts of Wigner energy.\n\nIn CANDU and PWR reactors, the moderator is liquid water (heavy water for CANDU, light water for PWR). In the event of a loss-of-coolant accident in a PWR, the moderator is also lost and the reaction will stop. This negative void coefficient is an important safety feature of these reactors. In CANDU the moderator is located in a separate heavy-water circuit, surrounding the pressurized heavy-water coolant channels. This design gives CANDU reactors a positive void coefficient, although the slower neutron kinetics of heavy-water moderated systems compensates for this, leading to comparable safety with PWRs.\"\n\nGood moderators are also free of neutron-absorbing impurities such as boron. In commercial nuclear power plants the moderator typically contains dissolved boron. The boron concentration of the reactor coolant can be changed by the operators by adding boric acid or by diluting with water to manipulate reactor power. The Nazi Nuclear Program suffered a substantial setback when its inexpensive graphite moderators failed to work. At that time, most graphites were deposited on boron electrodes, and the German commercial graphite contained too much boron. Since the war-time German program never discovered this problem, they were forced to use far more expensive heavy water moderators. In the U.S., Leó Szilárd, a former chemical engineer, discovered the problem.\n\nSome moderators are quite expensive, for example beryllium, and reactor-grade heavy water. Reactor-grade heavy water must be 99.75% pure to enable reactions with unenriched uranium. This is difficult to prepare because heavy water and regular water form the same chemical bonds in almost the same ways, at only slightly different speeds.\n\nThe much cheaper light water moderator (essentially very pure regular water) absorbs too many neutrons to be used with unenriched natural uranium, and therefore uranium enrichment or nuclear reprocessing becomes necessary to operate such reactors, increasing overall costs. Both enrichment and reprocessing are expensive and technologically challenging processes, and additionally both enrichment and several types of reprocessing can be used to create weapons-usable material, causing proliferation concerns. Reprocessing schemes that are more resistant to proliferation are currently under development.\n\nThe CANDU reactor's moderator doubles as a safety feature. A large tank of low-temperature, low-pressure heavy water moderates the neutrons and also acts as a heat sink in extreme loss-of-coolant accident conditions. It is separated from the fuel rods that actually generate the heat. Heavy water is very effective at slowing down (moderating) neutrons, giving CANDU reactors their important and defining characteristic of high \"neutron economy\".\n\nEarly speculation about nuclear weapons assumed that an \"atom bomb\" would be a large amount of fissile material, moderated by a neutron moderator, similar in structure to a nuclear reactor or \"pile\". Only the Manhattan project embraced the idea of a chain reaction of fast neutrons in pure metallic uranium or plutonium. Other moderated designs were also considered by the Americans; proposals included using uranium deuteride as the fissile material. In 1943 Robert Oppenheimer and Niels Bohr considered the possibility of using a \"pile\" as a weapon. The motivation was that with a graphite moderator it would be possible to achieve the chain reaction without the use of any isotope separation. In August 1945, when information of the atomic bombing of Hiroshima was relayed to the scientists of the German nuclear program, interred at Farm Hall in England, chief scientist Werner Heisenberg hypothesized that the device must have been \"something like a nuclear reactor, with the neutrons slowed by many collisions with a moderator\".\n\nAfter the success of the Manhattan project, all major have relied on fast neutrons in their weapons designs. The notable exception is the \"Ruth\" and \"Ray\" test explosions of Operation Upshot–Knothole. The aim of the University of California Radiation Laboratory designs was the exploration of deuterated polyethylene charge containing uranium as a candidate thermonuclear fuel, hoping that deuterium would fuse (becoming an active medium) if compressed appropriately. If successful, the devices could also lead to a compact primary containing minimal amount of fissile material, and powerful enough to ignite RAMROD a thermonuclear weapon designed by UCRL at the time. For a \"hydride\" primary, the degree of compression would not make deuterium to fuse, but the design could be subjected to boosting, raising the yield considerably. The cores consisted of a mix of uranium deuteride (UD), and deuterated polyethylene. The core tested in \"Ray\" used uranium low enriched in U, and in both shots deuterium acted as the neutron moderator. The predicted yield was 1.5 to 3 kt for \"Ruth\" (with a maximum potential yield of 20 kt) and 0.5-1 kt for \"Ray\". The tests produced yields of 200 tons of TNT each; both tests were considered to be fizzles.\n\nThe main benefit of using a moderator in a nuclear explosive is that the amount of fissile material needed to reach criticality may be greatly reduced. Slowing of fast neutrons will increase the cross section for neutron absorption, reducing the critical mass. A side effect is however that as the chain reaction progresses, the moderator will be heated, thus losing its ability to cool the neutrons.\n\nAnother effect of moderation is that the time between subsequent neutron generations is increased, slowing down the reaction. This makes the containment of the explosion a problem; the inertia that is used to confine implosion type bombs will not be able to confine the reaction. The end result may be a fizzle instead of a bang.\n\nThe explosive power of a fully moderated explosion is thus limited, at worst it may be equal to a chemical explosive of similar mass. Again quoting Heisenberg: \"One can never make an explosive with slow neutrons, not even with the heavy water machine, as then the neutrons only go with thermal speed, with the result that the reaction is so slow that the thing explodes sooner, before the reaction is complete.\"\n\nWhile a nuclear bomb working on thermal neutrons may be impractical, modern weapons designs may still benefit from some level of moderation. A beryllium tamper used as a neutron reflector will also act as a moderator.\n\n\nOther light-nuclei materials are unsuitable for various reasons. Helium is a gas and it requires special design to achieve sufficient density; lithium-6 and boron-10 absorb neutrons.\n\n"}
{"id": "49736384", "url": "https://en.wikipedia.org/wiki?curid=49736384", "title": "OneSubsea", "text": "OneSubsea\n\nOneSubsea is a Schlumberger company, headquartered in Houston, Texas. The company is a subsea supplier for the subsea oil and gas market.\n\nOneSubsea has more than 5,000 employees in over 23 countries operating in six divisions - Integrated Solutions, Production Systems, Processing Systems, Control Systems, Swivel and Marine Systems, and Subsea Services – that provide products and services to oil and gas operators around the world including the FRIEND Remote Surveillance & Diagnostic System.\n\nThe integration of Cameron subsea section and Schlumberger-owned Framo Engineering in 2013 formed Onesubsea as a 60% Cameron and 40% Schlumberger joint venture. This cooperation, finally led to major acquisition of Cameron and OneSubsea in a $14.8 billion deal in late 2015.\n\nIn August 2015, it was announced that OneSubsea was awarded a contract to supply subsea processing systems for the Shell Offshore Inc. Stones development in the Gulf of Mexico. The project was deemed the industry's first 15000-PSI (super high pressure) subsea pump system with high capacity.\n\n\n"}
{"id": "6678342", "url": "https://en.wikipedia.org/wiki?curid=6678342", "title": "Open-circuit voltage", "text": "Open-circuit voltage\n\nOpen-circuit voltage (abbreviated as OCV or V ) is the difference of electrical potential between two terminals of a device when disconnected from any circuit. There is no external load connected. No external electric current flows between the terminals. Alternatively, the open-circuit voltage may be thought of as the voltage that must be applied to a solar cell or a battery to stop the current. It is sometimes given the symbol V. In network analysis this voltage is also known as the Thévenin voltage.\n\nThe open-circuit voltages of batteries and solar cells are often quoted under particular conditions (state-of-charge, illumination, temperature, etc.).\n\nThe potential difference mentioned for batteries and cells is usually the open-circuit voltage. The open-circuit voltage is also known as the electromotive force (emf), which is the maximum potential difference when there is no current and the circuit is not closed.\nTo calculate the open-circuit voltage, one can use a method similar to that below:\n\nConsider the circuit:\n\nIf we want to find the open-circuit voltage across 5Ω resistor. First disconnect it from the circuit:\nFind the equivalent resistance in loop 1 and hence find the current in the loop. Use Ohm’s Law to find the potential drop across the resistance C. The resistor B does not affect the open-circuit voltage. Since no current is flowing through it, there is no potential drop across it. So we can easily ignore it.\n\nTherefore, the potential drop across the resistance C is \"V.\"\n\nThis is just an example. Many other ways can be used.\n\n"}
{"id": "48871287", "url": "https://en.wikipedia.org/wiki?curid=48871287", "title": "Overhead wire marker", "text": "Overhead wire marker\n\nOverhead wire markers are safety instruments applied to the overhead power lines marking transmission lines and ropeways along the flight path during the day.\n\n"}
{"id": "1691106", "url": "https://en.wikipedia.org/wiki?curid=1691106", "title": "Phosphonium", "text": "Phosphonium\n\nThe phosphonium (more obscurely: phosphinium) cation describes polyatomic cations with the chemical formula (R = H, alkyl, aryl, halide). They are tetrahedral and generally colorless.\n\nThe parent phosphonium is . One example is phosphonium iodide . Salts of the parent are rarely encountered, but this ion is an intermediate in the preparation of the industrially useful tetrakis(hydroxymethyl)phosphonium chloride:\n\nMany phosphonium salts are produced by protonation of primary, secondary, and tertiary phosphines:\nThe basicity of phosphines follows the usual trends, with R = alkyl being more basic than R = aryl.\n\nThe most common phosphonium compounds have four organic substituents attached to phosphorus. The quaternary phosphonium cations include tetraphenylphosphonium, (CH)P and tetramethylphosphonium .\nQuaternary phosphonium cations () are produced by alkylation of organophosphines. For example, the reaction of triphenylphosphine with methyl iodide gives methyltriphenylphosphonium iodide, the precursor to a Wittig reagent:\n\nSolid phosphorus pentachloride is an ionic compound, formulated , i.e. a salt containing tetrachlorophosphonium cation. Dilute solutions dissociate according to the following equilibrium:\n\nTriphenylphosphine dichloride (PhPCl) exists both as the pentacoordinate phosphorane and as the chlorotriphenylphosphonium chloride, depending on the medium. The situation is similar to that of PCl. It is an ionic compound (PPhCl)Cl in polar solutions and a molecular species with trigonal bipyramidal molecular geometry in apolar solution.\n\nTetrakis(hydroxymethyl)phosphonium chloride has industrial importance in the production of crease-resistant and flame-retardant finishes on cotton textiles and other cellulosic fabrics. A flame-retardant finish can be prepared from THPC by the Proban Process, in which THPC is treated with urea. The urea condenses with the hydroxymethyl groups on THPC. The phosphonium structure is converted to phosphine oxide as the result of this reaction.\n\nOrganic phosphonium cations are lipophilic and can be useful in phase transfer catalysis, much like quaternary ammonium salts.\nThe cation tetraphenylphosphonium () is a useful precipitating agent.\n\nWittig reagents are used in organic synthesis. They are derived from phosphonium salts, which is in turn prepared by deprotonation of alkylphosphonium salts. A strong base such as butyllithium or sodium amide is required for the deprotonation:\n\nOne of the simplest ylide is methylenetriphenylphosphorane (PhP=CH).\n\nThe compounds PhPX (X = Cl, Br) are used in the Kirsanov reaction.\n\nThe Kinnear–Perren reaction is used to prepare alkylphosphonyl dichlorides (RP(O)Cl) and alkylphosphonate esters (RP(O)(OR')). Alkylation of phosphorus trichloride in the presence of aluminium trichloride give the alkyltrichlorophosphonium salts, which are versatile intermediates:\n\n"}
{"id": "16440097", "url": "https://en.wikipedia.org/wiki?curid=16440097", "title": "Prydniprovsky Chemical Plant radioactive dumps", "text": "Prydniprovsky Chemical Plant radioactive dumps\n\nThe now-defunct Prydniprovsky Chemical Plant (; \"Prydniprovsky khimichnyi zavod\", PHZ) in the city of Kamianske, Ukraine, enriched uranium ore for the Soviet nuclear program from 1948 through 1991, preparing Yellowcake.\n\nIts processing wastes are now stored in nine open-air dumping grounds containing about 36 million tones of sand-like low-radioactive residue, occupying an area of 2,5 million square meters. The sites, improperly constructed from the very beginning, have been abandoned by the industry long ago and remain in very poor condition. The top concern is the dumps’ closeness to both the large Dnieper River and city residential areas. According to government experts, the dams separating the grounds from soil water are already leaking, causing the pollution of Dnieper basin. It is believed that further deterioration of the dams, irrespective of any outer accidents, may cause a devastating radioactive mudslide. The Ukrainian government is now tightening control over the grounds and seeking international aid in projects aimed at securing and the gradual re-processing of the PHZ wastes. Recently, the International Atomic Energy Agency has evaluated the condition of the sites and is considering dispatching a major observation and aid mission to Kamianske.\n\nThe isolated dump grounds (about nine altogether, at a depth of 3 m) of the former plant are now located in different parts of the city and operated by the purposely-created \"Barrier\" State Enterprise - with an obscure-meaning new name that has yet to be widely known. That is why the sites, the company, and the whole problem is still commonly referred to as the \"Prydniprovsky Chemical Plant (PHZ) wastes\".\n\n"}
{"id": "4661664", "url": "https://en.wikipedia.org/wiki?curid=4661664", "title": "Radiation intelligence", "text": "Radiation intelligence\n\nRadiation intelligence, or RINT, is military intelligence gathered and produced from unintentional radiation created as induction from electrical wiring, usually of computers, data connections and electricity networks.\n\n"}
{"id": "30746713", "url": "https://en.wikipedia.org/wiki?curid=30746713", "title": "Rain dust", "text": "Rain dust\n\nRain dust or snow dust, traditionally known as muddy rain, red rain, or coloured rain, is a variety of rain (or any other form of precipitation) which contains enough desert dust for the dust to be visible without using a microscope.\n\nThe rain dust phenomenon was studied by Italian scientist Giuseppe Maria Giovene (1753–1837), who managed to correctly explain the phenomenon as early as 1803. On 7 March 1803, rain dust fell over South Italy's region Apulia. At that time, people believed that the rain was caused by the explosions of Italy's volcanoes Mount Vesuvius or Etna, or that it was due to the transport of matter coming from the sea floor and raised by vapor. Giuseppe Maria Giovene brilliantly managed to relate the phenomenon to the wind which occurred prior to the rain event, and he came to the conclusion that the sand had come from Africa and that it had been pushed by the wind coming from south-east.\n\nRain dust is common in the Western and Southern Mediterranean, where the dust supply comes from the atmospheric depressions going through the northern part of North Africa. The main sources of desert dust reach the Iberian Peninsula and the Balearic Islands in the form of dust transported by wind or rain from the Sahara, Atlas Mountains in Morocco and Central Algeria.\n\nMud rains are relatively frequent and had been increasing in early 1990s in the Mediterranean Basin. \n\nIt also occurs in arid desert regions of North America such as west Texas or Arizona. It occasionally happens in the grasslands as it did in Bexar County, Texas on March 18, 2008.\n\nIt may also occur in some regions of South Italy (sand coming from North Africa), but it is a very rare phenomenon.\n\nThe rain dust is very alkaline. Some of the large particles contain mixtures of chemicals such as sulfate and sea salt (chiefly with sodium, chlorine and magnesium). Major minerals in order of decreasing abundance are: illite, quartz, smectite, palygorskite, kaolinite, calcite, dolomite and feldspars. In Majorca a study finds that the size, by volume, 89% of the particles from rain dust fraction corresponded to silt (between 0.002 mm and 0.063 mm) and that there was virtually no clay sized particles (less than 0.29%).\n\n\nRain dust is the most common cause of blood rain.\n\nRed rain is however not always rain dust, see for example the Red rain in Kerala.\n\n\n"}
{"id": "35962051", "url": "https://en.wikipedia.org/wiki?curid=35962051", "title": "Ryder Scott", "text": "Ryder Scott\n\nRyder Scott Company is a petroleum consulting firm based in Houston, Texas, the United States. The firm independently estimates oil and gas reserves, future production profiles and cashflow economics, including discounted net present values. It assess oil reserves and evaluates oil and gas properties. \n\nThe company was founded in Bradford, Pennsylvania on July 1, 1937, by Harry M. Ryder, a prominent petroleum engineer, who died in 1954. and David Scott, Jr.\n\nIn 1967, Ryder Scott acquired Robert W. Harrison & Co., moved to Houston and transitioned from waterflood design to evaluation engineering, which is the core business of Ryder Scott.\n\n"}
{"id": "291368", "url": "https://en.wikipedia.org/wiki?curid=291368", "title": "Saab 9-3", "text": "Saab 9-3\n\nThe Saab 9-3 is a compact executive car that was originally developed and manufactured by the Swedish automaker Saab.\n\nThe 9-3 was first based on the GM2900 platform and subsequently changed to the GM Epsilon platform. Other vehicles using this platform included the Opel Vectra and Cadillac BLS. Saab's last owners, National Electric Vehicle Sweden (NEVS) were assembling the 9-3 sedan (saloon) as Saab's only model.\n\nThe car was badged as 9 starting in the 1998 model year, when Saab revised the naming strategy of their small car to match that of the larger 9. The model was advertised as 9-3, pronounced as \"nine three\". The Saab 9-3 was launched in 1997 for the 1998 model year essentially as a rebadged 2nd Generation Saab 900 (1994–1997 model), and succeeded by a redesigned 9-3 for the 2003 model year. It is not to be confused with the Saab 93, pronounced \"ninety three\", which was a car produced by Saab from 1955 to 1960.\n\nThe first generation 9-3, an updated Saab 900 (NG) was launched in 1998 for the 1999 model year. It was known to enthusiasts as the OG 9-3 (old generation) and internally as body style 9400. It continued as a full line through the 2002 model year. (In 2003, Saab produced only the convertible model of this line – other models were replaced by the second generation 9-3). \nSaab claimed that 1,100 changes were made, including a revised suspension in an attempt to tighten up the handling characteristics of its predecessor, the Saab 900 (1994–1998 model). It featured revised styling with some models receiving a black rear spoiler and removed Saab's trademark centrally mounted \"snow flap\". It was available as a three or five-door hatchback, and as a two-door convertible. It was the last small Saab to use the company's \"H engine\". Improvements over the Saab 900 (NG) included better crashworthiness with more extensive A-pillar reinforcements, stronger door sills and frames, standard torso/head side-airbags and Saab Active Head Restraints. Other notable changes were stronger AC compressor and a switch to an hydraulically operated convertible top instead of electric. \n\nThe 9-3 was available with a new variant of the B204 engine (B204E, ), a low pressure turbo (LPT) engine based on the B204L used in the last generation Saab 900. For the U.S. market, all 9-3s received turbocharged petrol engines with the \"full pressure turbo\" (B204L, ) as the standard offering, and a \"HOT\" (B204R, 200 hp) variant in the SE models for the 1999 model year. The 2000 model year saw a revision from SAAB's Trionic T5.5 to Trionic 7 engine management system. The T7 based engines were the B205L with and the B205R HOT engine with . The first generation 9-3 was also the first Saab available with a diesel engine, a unit also found in the Opel Vectra, Astra G, Signum, Zafira A. Unlike the Saab 900 (NG), the 9-3 is fitted with a CAN bus like the Saab 9-5.\n\nA Saab innovation is the 'Night Panel', carried over from the Saab 900, which permits dousing of the instrument panel lighting, except for essential information, for less distraction when night driving.\n\nA total of 326,370 first generation 9-3s were built.\n\nA high-powered version of the Saab 9-3, the \"Viggen\" (English: \"Thunderbolt\"), was marketed from 1999 to 2002. It was named after the Saab 37 Viggen aircraft.\n\nThe Viggen included a turbocharged 2.3 L engine (B235R). Initially it was rated at and later at on of boost from its Mitsubishi TD04-HL15-5 turbocharger. The cars were equipped with a higher capacity intercooler, performance tuned ECU, flow through muffler and tip, heavy duty clutch and pressure plate, stiffened and lowered springs, firmer dampers, as well as stronger CV joints and driveshafts.\n\nIn 1999, the Viggen was the first 9-3 to use Saab's Trionic 7 engine management system. The 2001 model year introduced a Traction Control System (TCS). The TCS was later made available in the SE line.\n\nThe car featured a rear wing that required relocating the radio antenna, aerodynamically designed bumpers and side skirts, special bolstered and colored leather seats (in four colors: black with black inserts (charcoal), black with blue inserts (deep blue), black with orange inserts (flame ochre), or tan with tan inserts), sportier suspension, as well as bigger wheels and upgraded brakes.\n\nThe Viggen was only available with a five-speed manual transmission, CD player, power moonroof, and (what were initially) Viggen-specific motorised and heated leather seats with the Viggen delta logo embossed in the backrest; these were later also available in the Aero model (U.S. market 'SE' model) without the embossed Viggen logo. Some colors featured carbon-fibre interior trim from its introduction to the middle of the 2001 model year. Cars built afterward came with a less expensive printed gray pattern for the dash and standard trim.\n\nNew buyers of Viggens in the U.S. were offered two days of advanced driving instruction at Road Atlanta and an opportunity to dine with Saab USA executives from nearby Norcross, Georgia.\n\nSome motoring journalists were critical of untamed torque steer in low gears.\n\nA total of 4,600 Viggens were manufactured until production ended in June 2002; of which 500 units were produced for the UK market. For 1999, 426 3-door Viggens were imported into the U.S.; of those 420 were blue, 2 were silver, 2 were Monte Carlo yellow, and 2 were black.\n\nOther than the diesel engines, all the first generation engines were versions of the Saab H engine. Other than the Saab 9-5, the first generation 9-3 was the last to utilise this all Saab engine design. All versions of this engine feature a DOHC 16-valve design with Saab's Saab Direct Ignition. All turbocharged engines utilise Saab's Trionic engine management system which works hand in hand with the Direct Ignition's IDM module (mounted to the top of the engine, directly engaging the spark plugs). The later two technologies were migrated into other GM products during the ten years that GM controlled Saab. All of the engines, other than the normally aspirated version and the low-pressure turbo, had high specific power outputs. The B205R generated per litre and of torque.\nNotes:\n\nThe second-generation 9-3 was launched in January 2002, at the North American International Auto Show for MY03. Originally, the 9-3 was due to début with the Opel Vectra in October 2001, at the Frankfurt Motor Show, but in July 2001, it was announced that delays had forced General Motors to postpone the introduction. Both cars were eventually introduced in March 2002, at the Geneva Motor Show. The convertible version of the second-generation 9-3 began with the MY04, and SportCombi with MY05.\n\nThe new 9-3, like all other Saabs, remained a front-wheel drive car. The most drastic change from the former generation was the elimination of the hatchback design. The second-generation 9-3 was available as a four-door saloon, an estate (introduced in late 2005, known as the SportWagon, SportCombi or Sport-Hatch dependant on the market), and a two-door convertible (introduced in 2004). It included Saab Active Head Restraints (SAHR II) to reduce whiplash and ReAxs, a passive rear wheel steering design and passive toe-in to help reduce understeer under heavy braking.\n\nThe new 9-3 departed from the EcoPower engine used previously for a new 2.0 L inline-four engine \"Ecotec\" engine from General Motors' for the petrol powered models. There are three different versions of the turbocharged inline-four, with the amount of turbo boost determining the power output. The version (1.8t) was standard in the non-U.S. market Linear form (trim-level). The version (2.0t) was standard in U.S. market Linear or non-U.S. market Vector form, mated with a 5-speed manual transmission or a 5-speed 'Sentronic' which is a traditional automatic, not to be confused with SAAB's earlier 'Sensonic' which was a manual transmission which allowed for shifting without a clutch pedal. The 210 horsepower 2.0T (B207R engine) was available in both the Arc and Vector forms, and as the Aero in Australia and the United Kingdom. In 2003 Aero, Arc,Linear and Vector models, the standard manual transmission was a 5-speed gearbox with the 6 speed a £200 optional extra (UK market price.)\n\nThe 9-3 and the Opel Vectra were the first of the global GM Epsilon platform, which was then lengthened to accommodate four new cousins, the Chevrolet Malibu/Malibu Maxx, the Pontiac G6, and the Saturn Aura. A proprietary fiber-optic electric/electronic system, the possibility of AWD (exploited from 2008 on, dubbed Saab XWD), and ReAxs as described above, are just a few of the features exclusive to the 9-3. On February 22, 2012, the final 47 Saabs were built. They were all 9-3 \"Independence Edition\" convertible models built by one of Sweden's largest car dealers, ANA, in Trollhättan.\n\nThere were 21 LHD cars, and 26 RHD ones. The final Saab was a Saab 9-3 Aero Independence Edition TTiD convertible.\n\nNote: diesel engines are not available in North America. Starting from late 2004 diesel engines are Fiat-sourced common rail units.\n\nThe Vector form was replaced with the Aero in the USA. In addition, the Arc received the 5-speed manual in place of the 6-speed. In the UK 210 horsepower 2.0T was also available as the 6 speed manual 9-3 Aero.\n\nUnited States versions were sold with 16-inch wheels standard (17-inch for the Aero) unlike the 15-inch wheels which were previously found in the Linear version. In the United States, but not in most countries, the 2005 was the last year of the Linear and Arc versions. In addition, the 6-speed manual was dropped and both the Arc and Aero received the 5-speed manual.\n\nA new V6 engine was introduced. A new 2.8-liter turbocharged 6-cylinder was also added for 2006 along with an improved 2.0-liter turbocharged four-cylinder that produced 35 more horsepower. The 4-cylinder option had 12.3 psi maximum turbo boost pressure and turned out , while the 6-cylinder had 8.7 psi boost and turned out . In certain markets, like Switzerland, a 230 hp variant of the 6-cylinder was also offered in Vector trim. The 2.0-litre 16-valve turbo four-cylinder model was marketed in the United States as the 2.0T, replacing the Linear and Arc models sold until the 2005 model year. The United States 2.0T version was similar to the 2005 Arc except for U.S. Linear wheels were used. The Linear and Arc versions continued to be sold in most other countries. A special \"20 Years Edition Aero Convertible\" for the American market was unveiled at the Los Angeles Auto Show in January 2006 to celebrate 20 years since the introduction of the Saab 900 convertible.\n\nThe dashboard was revamped for 2007, with the Saab Information Display moved from its high mounted position to the main instrument binnacle. The button-heavy climate control system disappeared, replaced by the Saab 9-5 climate control system, OnStar was re-introduced and required when Nav was ordered in North America, and the corporate GM head unit debuted, which allowed for satellite radio and MP3 CD capability. The suspension went from harsh to firm, and the cabin was quietened. Steel Gray was also replaced with Titan Gray as an exterior color choice.\nIn the U.S. market, only the 210 hp 2.0-liter 16-valve turbo engine and the 250 hp 2.8-litre V-6 turbo were available. The manual transmission in the 2.0 model was changed from a 5-speed to a 6-speed.\n\nA 60th Anniversary Edition was also offered for sedan, wagon and convertible body styles for 2007 to celebrate 60 years of SAAB. The package was available on 2.0T cars, and included unique five-spoke 17-inch alloy wheels, black leather sport seats with grey inserts and SAAB embossments on the front seats, dark walnut trim, black floormats with grey binding, front fog lamps and a BOSE audio system with 6-disc CD changer and satellite radio. Sedan and convertible models also received trunk-lid spoilers. An Ice Blue metallic paint was offered for the edition, as well as standard SAAB paint colors. 60th Anniversary Edition sedans were offered for $24,820 USD, SportCombi wagons for $30,065 USD, and $40,065 USD for convertibles.\n\nSaab Turbo X debuted at the 2007 Frankfurt auto show. It was made to celebrate SAAB's 30 years of turbocharging. All Turbo X were offered in metallic jet black with matte grey trim. The Turbo X is SAAB's first production car with the XWD all-wheel drive system from Haldex Traction and eLSD. It is powered by a 2.8-litre V6 producing mated to a six-speed manual or automatic gearbox. It has larger brakes as well as stiffer springs and shocks. The dash, shift lever and door panels have carbon fiber look and the turbo boost gauge draws its inspiration from the Saab 900.\n\nSaab claimed over 2000 changes were made to model year 2008 cars. The 2008 range, first presented at the Saab Festival in Trollhättan, Sweden (June 10, 2007) included new frontal styling inspired by the Saab Aero-X and Saab 9-2X, Saab's first use of LED \"signature\" lighting in the revised headlamps, new door panels, a new clamshell bonnet, new rear bumper, and frosted \"ice block\" rear lamps. Black replaced charcoal gray as an interior color choice. Snow Silver became a new exterior color. The 2.8T V6 powering the Aero models received a mild output boost from 250 hp to 255 hp. Some additional exterior modifications are available on the limited-edition XWD 9-3 Turbo X, presented at the Frankfurt Motor Show (9/07). The Turbo X made its North American debut at the New England Auto show in late November. Saab also released an all-wheel-drive version of the Aero, with the system dubbed \"XWD\", in March 2008.\n\nThe 2009 9-3 series expands the trim levels while dropping the limited-edition Turbo X saloon and estate from the lineup. The 2.0T and Aero saloon and estate models are now available with Saab's all-wheel drive(XWD). The convertible range lacked the all-wheel-drive option. The new Saab 9-3 was unchanged from the 2008 model. During 2009 the 9-3X was launched at the Geneva auto show. The 9-3X is a four-wheel-drive XUV version of the 9-3 SportWagon. The new 9-3X came with two engine choices: the 1.9-litre diesel (producing 180bhp) and the 2.0-litre petrol motor (producing the 210bhp). Only the 2.0L petrol engine is equipped with the XWD while the diesel version is available only with front-wheel-drive. \n\nFor 2010, the Saab 9-3 Aero's turbocharged V6 was eliminated. All models used the 2.0-liter turbo-4.\n\n2010 marked the 50th anniversary of Erik Carlsson's first win for Saab on the RAC Rally in a Saab 96. A limited edition of 96 Aero Carlsson 9-3 was released priced at £26,495. The 9-3 Aero Carlsson features Saab's cross wheel drive (XWD) system, a turbocharged engine, 2.8 litre V6 producing 280hp and 400Nm of torque through a 6-speed automatic sentronic gearbox.\n\nThe Saab 9-3 ePower electric car was unveiled at the 2010 Paris Auto Show and became Saab's first electric vehicle. The ePower concept car is based on the 9-3 SportWagon, has a 35.5kWh lithium-ion battery pack, a top speed of , and an estimated driving range of . Saab had scheduled to run a two-year trial with 70 ePower demonstrators in Sweden by late 2011. The new owner of the Saab estate, National Electric Vehicle Sweden, initially stated that they intended to start producing the all-electric 9-3 ePower to be launched in China by late 2013 or early 2014.\n\nThe production version was slated to be unveiled at the 2014 Frankfurt Motor Show and market launch for 2015. In April 2014 NEVS began production of a batch of 200 units to be tested in Qingdao, China by mid-2014. After the test, sales are scheduled to begin in Sweden in 2015.\n\nThe 9-3 received some revisions in 2011 for the 2012 model year. Changes were in the engine range with an overall reduction in diesel and petrol engine fuel consumption of 12% and 7% respectively. An entry-level 163 hp, 2.0-litre gasoline / BioPower engine was added for 9-3 saloon, estate, and 9-3X models with Saab XWD. Other changes included rear badging in line with all new Saab 9-5 saloon, 'ice block' style headlights, New bumper design, titanium metallic effect trim around instrument panel, gearshift, doors and glove box. Aero gets graphite fiber effect. Contrast stitching on leather upholstery.\n\nIn most markets, car was badged 'Griffin'. The three-spoke alloy wheel returned in 16- to 18-inch choices. An \"Independence Edition\" convertible was released with a total of 366 units to commemorate the first anniversary of the sale to Spyker Cars.\n\nNational Electric Vehicle Sweden (NEVS) restarted production of the Saab 9-3 Aero Sedan MY14 on December 2, 2013 in Saab's former Trollhättan assembly plant. The only exterior difference on the MY14 model is the lack of the Griffin badge, to which NEVS does not own the rights. The Griffin is replaced with a badge displaying the Saab logotype, as well as new seats. The 9-3 Aero MY14 features a 220-horsepower 2.0-liter direct-injected twin-scroll turbocharged engine and went on sale in Sweden on December 10. The first cars were to delivered in Spring 2014 as a \"Limited Edition\" model. Only two colors were available, black and Silver.\n\nThe 9-3 no longer meets the latest Euro NCAP tests regarding pedestrian safety; therefore, only 1,000 cars of each body model could be sold in Europe, as a low-volume manufacturer. The only other market was China. An electric version was to be launched in spring 2014 in the Chinese market.\n\nThe updated 9-3 have been tested favourably by motoring magazines. Vi Bilägare wrote that it feels modern and feels sporty yet comfortable.\n\nSaab automobile production ended as of May 2014 because Qingbo Investment, one of NEVS shareholders, was not able to reach a financing agreement. By the end of 2014, India's Mahindra & Mahindra agreed to buy a majority stake in NEVS. In February 2015, it was announced that the remaining 100 cars that were stuck on the halted production line since May 2014 would be completed.\n\nWork on a third generation Saab 9-3 started in 2007, when designers in General Motors facilities in Rüsselsheim and Detroit began work on a design study. The design language was supervised by Simon Padian, and the design team managed to produce a clay model and several computer models before General Motors announced it had put the Saab brand \"under review\" in December 2008.\n\nAfter 2009, during which an intended sale of Saab to Swedish supercar manufacturer Koenigsegg ultimately failed, General Motors reached an agreement with Dutch manufacturer Spyker N.V. in January 2010. The sale of Saab to Spyker was completed in late February 2010 and work on a replacement for the 9-3 was restarted virtually immediately. The new management of Saab, headed by CEO Victor Muller, felt, however, that a new design language was needed to distance a newly independent Saab from General Motors.\n\nMuller hired Jason Castriota in June 2010 to work on a scalable car platform that would serve as the basis for future Saabs, beginning with the replacement for the 9-3. In October 2010 a number of prototypes were produced and evaluated against the prototypes made in 2007. Eventually, Castriota's prototype was chosen and the design team was instructed to develop a five-door combi coupé, a convertible and a crossover on the new platform.\n\nThe work on the new platform culminated in the unveiling of the Saab PhoeniX concept car at the Geneva Motor Show in March 2011. By that time, Saab had run into serious cash flow problems, but work on the PhoeniX platform and the 9-3 replacement continued, even when Saab went into voluntary reconstruction in September 2011. The replacement of the 9-3, which had been renamed 900 by that time, was to have 1.6 liter turbo engine supplied by BMW, which was also to supply the car's start-stop system. The car was to have a hybrid drivetrain and was to be released in both a premium Aero and an economy Vector variant.\n\nWhen Saab finally filed for bankruptcy in December 2011, Castriota and his team had finished most work on the car's body and its engineering, with the interior remaining the last hurdle before completing the car, which was planned for Fall 2012. The main assets of the bankrupt company were acquired by National Electric Vehicle Sweden (NEVS), which may revisit the PhoeniX platform. NEVS was focusing its efforts on producing an electric variant of the second generation 9-3.\n\n2010\n\n2006\n\n"}
{"id": "23499775", "url": "https://en.wikipedia.org/wiki?curid=23499775", "title": "Saint Charles Reservoir", "text": "Saint Charles Reservoir\n\nSaint Charles Reservoir is the name of three reservoirs in Pueblo County, Colorado. The first one, called just Saint Charles Reservoir, is located in the mountains of southwestern Pueblo County, southwest of Beulah, at an elevation of . It is located at .\n\nSaint Charles Reservoir Number 2 and Saint Charles Reservoir Number 3 are located next to each other in Stem Beach, just south of Pueblo. Reservoirs number 2 and 3 were built by Colorado Fuel and Iron in the early 1900s. All three reservoirs are privately held and not open to the public.\n"}
{"id": "345758", "url": "https://en.wikipedia.org/wiki?curid=345758", "title": "Solar constant", "text": "Solar constant\n\nThe solar constant (G) is a flux density measuring mean solar electromagnetic radiation (solar irradiance) per unit area. It is measured on a surface perpendicular to the rays, one astronomical unit (AU) from the Sun (roughly the distance from the Sun to the Earth).\n\nThe solar constant includes all types of solar radiation, not just the visible light. It is measured by satellite as being 1.361 kilowatts per square meter (kW/m²) at solar minimum and approximately 0.1% greater (roughly 1.362 kW/m²) at solar maximum.\n\nThe solar \"constant\" is not a physical constant in the modern CODATA scientific sense; that is, it is not like the Planck constant or the speed of light which are absolutely constant in physics. The solar constant is an average of a varying value. In the past 400 years it has varied less than 0.2 percent.\n\nThis constant is used in the calculation of radiation pressure, which aids in the calculation of a force on a solar sail.\n\nSolar irradiance is measured by satellite above Earth's atmosphere, and is then adjusted using the inverse square law to infer the magnitude of solar irradiance at one Astronomical Unit (AU) to evaluate the solar constant. The approximate average value cited, 1.3608 ± 0.0005  kW/m², which is 81.65 kJ/m² per minute, is equivalent to approximately 1.951 calories per minute per square centimeter, or 1.951 langleys per minute.\n\nSolar output is nearly, but not quite, constant. Variations in total solar irradiance (TSI) were small and difficult to detect accurately with technology available before the satellite era (±2% in 1954). Total solar output is now measured as varying (over the last three 11-year sunspot cycles) by approximately 0.1%; see solar variation for details.\n\nIn 1838, Claude Pouillet made the first estimate of the solar constant. Using a very simple pyrheliometer he developed, he obtained a value of 1.228 kW/m², close to the current estimate.\n\nIn 1875, Jules Violle resumed the work of Pouillet and offered a somewhat larger estimate of 1.7 kW/m² based, in part, on a measurement that he made from Mont Blanc in France.\n\nIn 1884, Samuel Pierpont Langley attempted to estimate the solar constant from Mount Whitney in California. By taking readings at different times of day, he tried to correct for effects due to atmospheric absorption. However, the final value he proposed, 2.903 kW/m², was much too large.\n\nIn 1954 the solar constant was evaluated as 2.00 cal/min/sq cm ± 2%. Current results are about 2.5 percent lower.\n\nThe actual direct solar irradiance at the top of the atmosphere fluctuates by about 6.9% during a year (from 1.412 kW/m² in early January to 1.321 kW/m² in early July) due to the Earth's varying distance from the Sun, and typically by much less than 0.1% from day to day. Thus, for the whole Earth (which has a cross section of 127,400,000 km²), the power is 1.730×10 W (or 173,000 terawatts), plus or minus 3.5% (half the approximately 6.9% annual range). The solar constant does not remain constant over long periods of time (see Solar variation), but over a year the solar constant varies much less than the solar irradiance measured at the top of the atmosphere. This is because the solar constant is evaluated at a fixed distance of 1 Astronomical Unit (AU) while the solar irradiance will be affected by the eccentricity of the Earth's orbit. Its distance to the Sun varies annually between 147.1·10 km at aphelion and 152.1·10 km at perihelion.\n\nThe Earth receives a total amount of radiation determined by its cross section (π·R²), but as it rotates this energy is distributed across the entire surface area (4·π·R²). Hence the average incoming solar radiation, taking into account the angle at which the rays strike and that at any one moment half the planet does not receive any solar radiation, is one-fourth the solar constant (approximately 340 W/m²). The amount reaching the Earth's surface (as insolation) is further reduced by atmospheric attenuation, which varies. At any given moment, the amount of solar radiation received at a location on the Earth's surface depends on the state of the atmosphere, the location's latitude, and the time of day.\n\nThe solar constant includes all wavelengths of solar electromagnetic radiation, not just the visible light (see Electromagnetic spectrum). It is positively correlated with the apparent magnitude of the Sun which is −26.8. The solar constant and the magnitude of the Sun are two methods of describing the apparent brightness of the Sun, though the magnitude is based on the Sun's visual output only.\n\nThe angular diameter of the Earth as seen from the Sun is approximately 1/11,700 radians (about 18 arcseconds), meaning the solid angle of the Earth as seen from the Sun is approximately 1/175,000,000 of a steradian. Thus the Sun emits about 2.2 billion times the amount of radiation that is caught by Earth, in other words about 3.86×10 watts.\n\nSpace-based observations of solar irradiance started in 1978. These measurements show that the solar constant is not constant. It varies with the 11-year sunspot solar cycle.\nWhen going further back in time, one has to rely on irradiance reconstructions, using sunspots for the past 400 years or cosmogenic radionuclides for going back 10,000 years.\nSuch reconstructions show that solar irradiance varies with distinct periodicities. These cycles are: 11 years (Schwabe), 88 years (Gleisberg cycle), 208 years (DeVries cycle) and 1,000 years (Eddy cycle).\n\nOver billions of years, the Sun is gradually expanding, and emitting more energy from the resultant larger surface area. The unsolved question of how to account for the clear geological evidence of liquid water on the Earth billions of years ago, at a time when the sun's luminosity was only 70% of its current value, is known as the faint young Sun paradox.\n\nAt most about 75% of the solar energy actually reaches the earth's surface, as even with a cloudless sky it is partially reflected and absorbed by the atmosphere. Even light cirrus clouds reduce this to 50%, stronger cirrus clouds to 40%. Thus the solar energy arriving at the surface can vary from 550 W/m² with cirrus clouds to 1025 W/m² with a clear sky.\n\n"}
{"id": "1701055", "url": "https://en.wikipedia.org/wiki?curid=1701055", "title": "Sol–gel process", "text": "Sol–gel process\n\nIn materials science, the sol–gel process is a method for producing solid materials from small molecules. The method is used for the fabrication of metal oxides, especially the oxides of silicon (Si) and titanium (Ti). The process involves conversion of monomers into a colloidal solution (\"sol\") that acts as the precursor for an integrated network (or \"gel\") of either discrete particles or network polymers. Typical precursors are metal alkoxides.\n\nIn this chemical procedure, a \"sol\" (a colloidal solution) is formed that then gradually evolves towards the formation of a gel-like diphasic system containing both a liquid phase and solid phase whose morphologies range from discrete particles to continuous polymer networks. In the case of the colloid, the volume fraction of particles (or particle density) may be so low that a significant amount of fluid may need to be removed initially for the gel-like properties to be recognized. This can be accomplished in any number of ways. The simplest method is to allow time for sedimentation to occur, and then pour off the remaining liquid. Centrifugation can also be used to accelerate the process of phase separation.\n\nRemoval of the remaining liquid (solvent) phase requires a drying process, which is typically accompanied by a significant amount of shrinkage and densification. The rate at which the solvent can be removed is ultimately determined by the distribution of porosity in the gel. The ultimate microstructure of the final component will clearly be strongly influenced by changes imposed upon the structural template during this phase of processing.\n\nAfterwards, a thermal treatment, or firing process, is often necessary in order to favor further polycondensation and enhance mechanical properties and structural stability via final sintering, densification, and grain growth. One of the distinct advantages of using this methodology as opposed to the more traditional processing techniques is that densification is often achieved at a much lower temperature.\n\nThe precursor sol can be either deposited on a substrate to form a film (e.g., by dip-coating or spin coating), cast into a suitable container with the desired shape (e.g., to obtain monolithic ceramics, glasses, fibers, membranes, aerogels), or used to synthesize powders (e.g., microspheres, nanospheres). The sol–gel approach is a cheap and low-temperature technique that allows the fine control of the product’s chemical composition. Even small quantities of dopants, such as organic dyes and rare-earth elements, can be introduced in the sol and end up uniformly dispersed in the final product. It can be used in ceramics processing and manufacturing as an investment casting material, or as a means of producing very thin films of metal oxides for various purposes. Sol–gel derived materials have diverse applications in optics, electronics, energy, space, (bio)sensors, medicine (e.g., controlled drug release), reactive material, and separation (e.g., chromatography) technology.\n\nThe interest in sol–gel processing can be traced back in the mid-1800s with the observation that the hydrolysis of tetraethyl orthosilicate (TEOS) under acidic conditions led to the formation of SiO in the form of fibers and monoliths. Sol–gel research grew to be so important that in the 1990s more than 35,000 papers were published worldwide on the process.\n\nThe sol–gel process is a wet-chemical technique used for the fabrication of both glassy and ceramic materials. In this process, the sol (or solution) evolves gradually towards the formation of a gel-like network containing both a liquid phase and a solid phase. Typical precursors are metal alkoxides and metal chlorides, which undergo hydrolysis and polycondensation reactions to form a colloid. The basic structure or morphology of the solid phase can range anywhere from discrete colloidal particles to continuous chain-like polymer networks.\n\nThe term \"colloid\" is used primarily to describe a broad range of solid-liquid (and/or liquid-liquid) mixtures, all of which contain distinct solid (and/or liquid) particles which are dispersed to various degrees in a liquid medium. The term is specific to the size of the individual particles, which are larger than atomic dimensions but small enough to exhibit Brownian motion. If the particles are large enough, then their dynamic behavior in any given period of time in suspension would be governed by forces of gravity and sedimentation. But if they are small enough to be colloids, then their irregular motion in suspension can be attributed to the collective bombardment of a myriad of thermally agitated molecules in the liquid suspending medium, as described originally by Albert Einstein in his dissertation. Einstein concluded that this erratic behavior could adequately be described using the theory of Brownian motion, with sedimentation being a possible long-term result. This critical size range (or particle diameter) typically ranges from tens of angstroms (10 m) to a few micrometres (10 m).\n\n\nIn either case (discrete particles or continuous polymer network) the sol evolves then towards the formation of an inorganic network containing a liquid phase (gel). Formation of a metal oxide involves connecting the metal centers with oxo (M-O-M) or hydroxo (M-OH-M) bridges, therefore generating metal-oxo or metal-hydroxo polymers in solution.\n\nIn both cases (discrete particles or continuous polymer network), the drying process serves to remove the liquid phase from the gel, yielding a micro-porous amorphous glass or micro-crystalline ceramic. Subsequent thermal treatment (firing) may be performed in order to favor further polycondensation and enhance mechanical properties.\n\nWith the viscosity of a sol adjusted into a proper range, both optical quality glass fiber and refractory ceramic fiber can be drawn which are used for fiber optic sensors and thermal insulation, respectively. In addition, uniform ceramic powders of a wide range of chemical composition can be formed by precipitation.\n\nThe Stöber process is a well-studied example of polymerization of an alkoxide, specifically TEOS. The chemical formula for TEOS is given by Si(OCH), or Si(OR), where the alkyl group R = CH. Alkoxides are ideal chemical precursors for sol–gel synthesis because they react readily with water. The reaction is called hydrolysis, because a hydroxyl ion becomes attached to the silicon atom as follows:\n\nDepending on the amount of water and catalyst present, hydrolysis may proceed to completion to silica:\n\nComplete hydrolysis often requires an excess of water and/or the use of a hydrolysis catalyst such as acetic acid or hydrochloric acid. Intermediate species including [(OR)−Si−(OH)] or [(OR)−Si−(OH)] may result as products of partial hydrolysis reactions. Early intermediates result from two partially hydrolyzed monomers linked with a siloxane [Si−O−Si] bond:\n\nor\n\nThus, polymerization is associated with the formation of a 1-, 2-, or 3-dimensional network of siloxane [Si−O−Si] bonds accompanied by the production of H−O−H and R−O−H species.\n\nBy definition, condensation liberates a small molecule, such as water or alcohol. This type of reaction can continue to build larger and larger silicon-containing molecules by the process of polymerization. Thus, a polymer is a huge molecule (or macromolecule) formed from hundreds or thousands of units called monomers. The number of bonds that a monomer can form is called its functionality. Polymerization of silicon alkoxide, for instance, can lead to complex branching of the polymer, because a fully hydrolyzed monomer Si(OH) is tetrafunctional (can branch or bond in 4 different directions). Alternatively, under certain conditions (e.g., low water concentration) fewer than 4 of the OR or OH groups (ligands) will be capable of condensation, so relatively little branching will occur. The mechanisms of hydrolysis and condensation, and the factors that bias the structure toward linear or branched structures are the most critical issues of sol–gel science and technology. This reaction is favored in both basic and acidic conditions.\n\nSonication is an efficient tool for the synthesis of polymers. The cavitational shear forces, which stretch out and break the chain in a non-random process, result in a lowering of the molecular weight and poly-dispersity. Furthermore, multi-phase systems are very efficient dispersed and emulsified, so that very fine mixtures are provided. This means that ultrasound increases the rate of polymerisation over conventional stirring and results in higher molecular weights with lower polydispersities. Ormosils (organically modified silicate) are obtained when silane is added to gel-derived silica during sol–gel process. The product is a molecular-scale composite with improved mechanical properties. Sono-Ormosils are characterized by a higher density than classic gels as well as an improved thermal stability. An explanation therefore might be the increased degree of polymerization.\n\nFor single cation systems like SiO and TiO, hydrolylsis and condensation processes naturally give rise to homogenous compositions. For systems involving multiple cations, such as strontium titante, SrTiO and other perovskite systems, the concept of steric immobilisation becomes relevant. To avoid the formation of multiple phases of binary oxides as the result of differing hydrolysis and condensation rates, the entrapment of cations in a polymer network is an effective approach, generally termed the Pechini Process . In this process, a chelating agent is used, most often citric acid, to surround aqueous cations and sterically entrap them. Subsequently a polymer network is formed to immobilize the chelated cations in a gel or resin. This is most often achieved by poly-esterification using ethylene glycol. The resulting polymer is then combusted under oxidising conditions to remove organic content and yield a product oxide with homogeneously dispersed cations.\n\nIn the processing of fine ceramics, the irregular particle sizes and shapes in a typical powder often lead to non-uniform packing morphologies that result in packing density variations in the powder compact. Uncontrolled flocculation of powders due to attractive van der Waals forces can also give rise to microstructural heterogeneities.\n\nDifferential stresses that develop as a result of non-uniform drying shrinkage are directly related to the rate at which the solvent can be removed, and thus highly dependent upon the distribution of porosity. Such stresses have been associated with a plastic-to-brittle transition in consolidated bodies, and can yield to crack propagation in the unfired body if not relieved.\n\nIn addition, any fluctuations in packing density in the compact as it is prepared for the kiln are often amplified during the sintering process, yielding heterogeneous densification.\nSome pores and other structural defects associated with density variations have been shown to play a detrimental role in the sintering process by growing and thus limiting end-point densities. Differential stresses arising from heterogeneous densification have also been shown to result in the propagation of internal cracks, thus becoming the strength-controlling flaws.\n\nIt would therefore appear desirable to process a material in such a way that it is physically uniform with regard to the distribution of components and porosity, rather than using particle size distributions which will maximize the green density. The containment of a uniformly dispersed assembly of strongly interacting particles in suspension requires total control over particle-particle interactions. Monodisperse colloids provide this potential.\n\nMonodisperse powders of colloidal silica, for example, may therefore be stabilized sufficiently to ensure a high degree of order in the colloidal crystal or polycrystalline colloidal solid which results from aggregation. The degree of order appears to be limited by the time and space allowed for longer-range correlations to be established. Such defective polycrystalline structures would appear to be the basic elements of nanoscale materials science, and, therefore, provide the first step in developing a more rigorous understanding of the mechanisms involved in microstructural evolution in inorganic systems such as sintered ceramic nanomaterials.\n\nThe applications for sol gel-derived products are numerous. For example, scientists have used it to produce the world’s lightest materials and also some of its toughest ceramics. One of the largest application areas is thin films, which can be produced on a piece of substrate by spin coating or dip-coating. Protective and decorative coatings, and electro-optic components can be applied to glass, metal and other types of substrates with these methods. Cast into a mold, and with further drying and heat-treatment, dense ceramic or glass articles with novel properties can be formed that cannot be created by any other method. Other coating methods include spraying, electrophoresis, inkjet printing, or roll coating.\n\nWith the viscosity of a sol adjusted into a proper range, both optical and refractory ceramic fibers can be drawn which are used for fiber optic sensors and thermal insulation, respectively. Thus, many ceramic materials, both glassy and crystalline, have found use in various forms from bulk solid-state components to high surface area forms such as thin films, coatings and fibers.\n\nUltra-fine and uniform ceramic powders can be formed by precipitation. These powders of single and multiple component compositions can be produced on a nanoscale particle size for dental and biomedical applications. Composite powders have been patented for use as agrochemicals and herbicides. Powder abrasives, used in a variety of finishing operations, are made using a sol–gel type process. One of the more important applications of sol–gel processing is to carry out zeolite synthesis. Other elements (metals, metal oxides) can be easily incorporated into the final product and the silicate sol formed by this method is very stable.\n\nAnother application in research is to entrap biomolecules for sensory (biosensors) or catalytic purposes, by physically or chemically preventing them from leaching out and, in the case of protein or chemically-linked small molecules, by shielding them from the external environment yet allowing small molecules to be monitored. The major disadvantages are that the change in local environment may alter the functionality of the protein or small molecule entrapped and that the synthesis step may damage the protein. To circumvent this, various strategies have been explored, such as monomers with protein friendly leaving groups (e.g. glycerol) and the inclusion of polymers which stabilize protein (e.g. PEG).\n\nOther products fabricated with this process include various ceramic membranes for microfiltration, ultrafiltration, nanofiltration, pervaporation, and reverse osmosis. If the liquid in a wet gel is removed under a supercritical condition, a highly porous and extremely low density material called aerogel is obtained. Drying the gel by means of low temperature treatments (25-100 °C), it is possible to obtain porous solid matrices called xerogels. In addition, a sol–gel process was developed in the 1950s for the production of radioactive powders of UO and ThO for nuclear fuels, without generation of large quantities of dust.\n\nMacroscopic optical elements and active optical components as well as large area hot mirrors, cold mirrors, lenses, and beam splitters all with optimal geometry can be made quickly and at low cost via the sol–gel route. In the processing of high performance ceramic nanomaterials with superior opto-mechanical properties under adverse conditions, the size of the crystalline grains is determined largely by the size of the crystalline particles present in the raw material during the synthesis or formation of the object. Thus a reduction of the original particle size well below the wavelength of visible light (~500 nm) eliminates much of the light scattering, resulting in a translucent or even transparent material.\n\nFurthermore, results indicate that microscopic pores in sintered ceramic nanomaterials, mainly trapped at the junctions of microcrystalline grains, cause light to scatter and prevented true transparency. it has been observed that the total volume fraction of these nanoscale pores (both intergranular and intragranular porosity) must be less than 1% for high-quality optical transmission. I.E. The density has to be 99.99% of the theoretical crystalline density.\nUnique properties of the sol–gel provide the possibility of their use for a variety of medical applications. A sol–gel processed alumina can be used as a carrier for the sustained delivery of drugs and as an established wound healer. A marked decrease in scar size was observed because of the wound healing composite including sol–gel processed alumina. A novel approach to thrombolysis treatment is possible by developing a new family of injectable composites: plasminogen activator entrapped within alumina.\n\n"}
{"id": "316617", "url": "https://en.wikipedia.org/wiki?curid=316617", "title": "Spring (device)", "text": "Spring (device)\n\nA spring is an elastic object that stores mechanical energy. Springs are typically made of spring steel. There are many spring designs. In everyday use, the term often refers to coil springs.\n\nWhen a conventional spring, without stiffness variability features, is compressed or stretched from its resting position, it exerts an opposing force approximately proportional to its change in length (this approximation breaks down for larger deflections). The \"rate\" or \"spring constant\" of a spring is the change in the force it exerts, divided by the change in deflection of the spring. That is, it is the gradient of the force versus deflection curve. An extension or compression spring's rate is expressed in units of force divided by distance, for example or N/m or lbf/in. A torsion spring is a spring that works by twisting; when it is twisted about its axis by an angle, it produces a torque proportional to the angle. A torsion spring's rate is in units of torque divided by angle, such as N·m/rad or ft·lbf/degree. The inverse of spring rate is compliance, that is: if a spring has a rate of 10 N/mm, it has a compliance of 0.1 mm/N. The stiffness (or rate) of springs in parallel is additive, as is the compliance of springs in series.\n\nSprings are made from a variety of elastic materials, the most common being spring steel. Small springs can be wound from pre-hardened stock, while larger ones are made from annealed steel and hardened after fabrication. Some non-ferrous metals are also used including phosphor bronze and titanium for parts requiring corrosion resistance and beryllium copper for springs carrying electrical current (because of its low electrical resistance).\n\nSimple non-coiled springs were used throughout human history, e.g. the bow (and arrow). In the Bronze Age more sophisticated spring devices were used, as shown by the spread of tweezers in many cultures. Ctesibius of Alexandria developed a method for making bronze with spring-like characteristics by producing an alloy of bronze with an increased proportion of tin, and then hardening it by hammering after it was cast.\n\nCoiled springs appeared early in the 15th century, in door locks. The first spring powered-clocks appeared in that century and evolved into the first large watches by the 16th century.\n\nIn 1676 British physicist Robert Hooke postulated Hooke's law, which states that the force a spring exerts is proportional to its extension.\n\nSprings can be classified depending on how the load force is applied to them:\nThey can also be classified based on their shape:\nThe most common types of spring are:\n\nOther types include :\n\nAs long as not stretched or compressed beyond their elastic limit, most springs obey Hooke's law, which states that the force with which the spring pushes back is linearly proportional to the distance from its equilibrium length:\nwhere\n\nCoil springs and other common springs typically obey Hooke's law. There are useful springs that don't: springs based on beam bending can for example produce forces that vary nonlinearly with displacement.\n\nIf made with constant pitch (wire thickness), conical springs have a variable rate. However, a conical spring can be made to have a constant rate by creating the spring with a variable pitch. A larger pitch in the larger-diameter coils and a smaller pitch in the smaller-diameter coils forces the spring to collapse or extend all the coils at the same rate when deformed.\n\nSince force is equal to mass, \"m\", times acceleration, \"a\", the force equation for a spring obeying Hooke's law looks like:\n\nThe mass of the spring is small in comparison to the mass of the attached mass and is ignored. Since acceleration is simply the second derivative of x with respect to time,\nThis is a second order linear differential equation for the displacement formula_4 as a function of time. Rearranging:\nthe solution of which is the sum of a sine and cosine:\nformula_7 and formula_8 are arbitrary constants that may be found by considering the initial displacement and velocity of the mass. The graph of this function with formula_9 (zero initial position with some positive initial velocity) is displayed in the image on the right.\n\nIn classical physics, a spring can be seen as a device that stores potential energy, specifically elastic potential energy, by straining the bonds between the atoms of an elastic material.\n\nHooke's law of elasticity states that the extension of an elastic rod (its distended length minus its relaxed length) is linearly proportional to its tension, the force used to stretch it. Similarly, the contraction (negative extension) is proportional to the compression (negative tension).\n\nThis law actually holds only approximately, and only when the deformation (extension or contraction) is small compared to the rod's overall length. For deformations beyond the elastic limit, atomic bonds get broken or rearranged, and a spring may snap, buckle, or permanently deform. Many materials have no clearly defined elastic limit, and Hooke's law can not be meaningfully applied to these materials. Moreover, for the superelastic materials, the linear relationship between force and displacement is appropriate only in the low-strain region.\n\nHooke's law is a mathematical consequence of the fact that the potential energy of the rod is a minimum when it has its relaxed length. Any smooth function of one variable approximates a quadratic function when examined near enough to its minimum point as can be seen by examining the Taylor series. Therefore, the force—which is the derivative of energy with respect to displacement—approximates a linear function.\n\nForce of fully compressed spring\n\nwhere\n\n\"Zero-length spring\" is a term for a specially designed coil spring that would exert zero force if it had zero length; if there were no constraint due to the finite wire diameter of such a helical spring, it would have zero length in the unstretched condition. That is, in a line graph of the spring's force versus its length, the line passes through the origin. Obviously a coil spring cannot contract to zero length, because at some point the coils touch each other and the spring can't shorten any more. Zero length springs are made by manufacturing a coil spring with built-in tension (A twist is introduced into the wire as it is coiled during manufacture. This works because a coiled spring \"unwinds\" as it stretches.), so if it \"could\" contract further, the equilibrium point of the spring, the point at which its restoring force is zero, occurs at a length of zero. In practice, zero length springs are made by combining a \"negative length\" spring, made with even more tension so its equilibrium point would be at a \"negative\" length, with a piece of inelastic material of the proper length so the zero force point would occur at zero length.\n\nA zero length spring can be attached to a mass on a hinged boom in such a way that the force on the mass is almost exactly balanced by the vertical component of the force from the spring, whatever the position of the boom. This creates a horizontal \"pendulum\" with very long oscillation period. Long-period pendulums enable seismometers to sense the slowest waves from earthquakes. The LaCoste suspension with zero-length springs is also used in gravimeters because it is very sensitive to changes in gravity. Springs for closing doors are often made to have roughly zero length, so that they exert force even when the door is almost closed, so they can hold it closed firmly.\n\n11. Springs with Dynamically Variable Stiffness https://patents.google.com/patent/WO2017077541A9/en\n12. Smart Springs and their Combinations https://patents.google.com/patent/US20170051808A1/en\n\n\n"}
{"id": "55023394", "url": "https://en.wikipedia.org/wiki?curid=55023394", "title": "Strategic energy management", "text": "Strategic energy management\n\nStrategic energy management (SEM) is a set of processes for business energy management. SEM is often deployed via programs that target the businesses or other organizations within a utility territory or a government area. SEM is codified in the ISO 50001 standard for energy management systems.\n\nThe main goal of SEM is to help a company achieve continuous improvement in its energy performance over a longer-term period. Some energy benefits of SEM include reduced energy consumption through improved energy efficiency and energy conservation, improved peak demand management and reduced demand charges, decreased overall energy cost, reduced energy costs, greenhouse gas (GHG) emissions and improved reliability through integration of distributed energy resources (e.g. onsite renewables, localized energy storage, combined heat and power), and improved electrical price stability and reduced GHG emissions through integration of long-term renewable energy contracts.\nAlthough often focused on energy management, SEM also supports other goals, including: reducing downtime and increasing productivity through improved maintenance practices, reducing CO2 and other air emissions through reduced on-site fuel combustion (through efficiency or through beneficial electrification), and improved employee productivity through increased employee morale.\n\nMost companies take several years to develop all the elements of SEM. The core or minimum elements of SEM for achieving the goal and benefits just described include management commitment to long-term energy performance goals, energy planning and implementation, and system for measuring and reporting energy performance.\n\nSEM Collaboratives\nRegional SEM collaboratives have met in the Northwestern and Northeastern United States for several years, with interest expressed in other regions for similar collaborative forums.\n\nThe first North American SEM Summit was conducted in Denver Colorado on August 15, 2017, with representatives from across the United States and Canada.\n\nWhile organizations can implement SEM on their own, many have found value in leveraging SEM specialists or SEM-based programs.\n\nPrivate companies provide SEM-based services to their customers as a dedicated consulting engagement. Other companies integrate SEM concepts into their more traditional energy engineering, energy planning, or energy project implementations.\n\nEnergy utilities in the U.S. and Canada often deploy dedicated SEM programs or deploy SEM components within other programs. These utilities deploy SEM for multiple reasons increased energy savings to support utility Demand Side Management (DSM) goals, measured in electrical, gas, or other energy savings, increased customer motivation to pursue energy saving opportunities (with less required utility outreach/marketing), reduced peak demand to address constraints in generation and/or transmission/distribution, and increased long-term customer satisfaction\n\nThe target customers for these programs are typically larger organizations, often manufacturers or larger building environments such as hospitals, water utilities, and universities.\n\nSome utility SEM programs work with customers on an individual basis, and some provide training and coaching to groups of companies, often referred to as cohorts. The cohort approach tends to be more cost-effective for the utility, since at least some of the coach/trainer’s time can serve several customers at once. In addition, the cohort can provide positive peer pressure for participants to take action and save more energy.\n\nGovernment agencies around the world often deploy SEM programs to drive adoption of ISO 50001, to increase competitiveness of the businesses within their borders, and/or to address climate priorities.\n\nISO 50001 codifies SEM by establishing universal requirements for management systems for energy (EnMSs). ISO 50001 is aligned to other management system standards for quality and environmental impact. It includes requirements for management participation, objectives, monitoring and measurement, and several other elements. \n"}
{"id": "5446616", "url": "https://en.wikipedia.org/wiki?curid=5446616", "title": "The Indonesian Forum for Environment", "text": "The Indonesian Forum for Environment\n\nThe Indonesian Forum for Environment (\"Wahana Lingkungan Hidup Indonesia\", WALHI) is an Indonesian environmental non-governmental organization, which is part of the Friends of the Earth International (FoEI) network. \n\nWALHI was founded in 1980 and joined FoEI in 1989. WALHI is the largest and oldest environmental advocacy NGO in Indonesia. WALHI unites more than 479 NGO's and 156 individuals throughout Indonesia's vast archipelago, with independent offices and grassroot constituencies located in 27 of the nation's 31 provinces. Its newsletter is published in both English and the native language. WALHI works on a wide range of issues, including agrarian conflict over access to natural resources, indigenous rights and peasants, coastal and marine, and deforestation. WALHI also has several cross cutting issues such as climate change, women and disaster risk management.\n\nIts scope is broader than just environmental concerns: \"It stands for social transformation, peoples sovereignty, and sustainability of life and livelihoods.\" The website also reports that WALHI volunteers assisted after the 2006 Yogyakarta earthquake in Yogyakarta.\n\n"}
{"id": "4109488", "url": "https://en.wikipedia.org/wiki?curid=4109488", "title": "Urban forest", "text": "Urban forest\n\nAn urban forest is a forest or a collection of trees that grow within a city, town or a suburb. In a wider sense it may include any kind of woody plant vegetation growing in and around human settlements. In a narrower sense (also called forest park) it describes areas whose ecosystems are inherited from wilderness leftovers or remnants. Care and management of urban forests is called urban forestry. Urban forests may be publicly-owned municipal forests, but the latter may also be located outside of the town or city to which they belong.\n\nUrban forests play an important role in ecology of human habitats in many ways: they filter air, water, sunlight, provide shelter to animals and recreational area for people. They moderate local climate, slowing wind and stormwater, and shading homes and businesses to conserve energy. They are critical in cooling the urban heat island effect, thus potentially reducing the number of unhealthful ozone days that plague major cities in peak summer months.\n\nIn many countries there is a growing understanding of the importance of the natural ecology in urban forests. There are numerous projects underway aimed at restoration and preservation of ecosystems, ranging from simple elimination of leaf-raking and elimination of invasive plants to full-blown reintroduction of original species and riparian ecosystems.\n\nSome sources claim that the largest man-made urban forest in the world is located in Johannesburg in South Africa.But others claim that this could be a myth. Tijuca Forest, in Rio de Janeiro, has also been considered to be the largest one.\n\nThe benefits of urban trees and shrubs are many, including beautification, reduction of the urban heat island effect, reduction of stormwater runoff, reduction of air pollution, reduction of energy costs through increased shade over buildings, enhancement of property values, improved wildlife habitat, and mitigation of overall urban environmental impact.\n\nThe presence of trees reduces stress, and trees have long been seen to benefit the health of urban dwellers. The shade of trees and other urban green spaces make place for people to meet and socialize and play. The Biophilia hypothesis argues that people are instinctively drawn to nature, while Attention Restoration Theory goes on to demonstrate tangible improvements in medical, academic and other outcomes, from access to nature. Proper planning and community involvement are important for the positive results to be realized.\n\nTrees and shrubs provide nesting sites and food for birds and other animals. People appreciate watching, feeding, photographing, and painting urban wildlife and the environment they live in. Urban trees, shrubs and wildlife help people maintain their connection with nature.\n\nThe economic benefits of trees and various other plants have been understood for a long time. Recently, more of these benefits are becoming quantified. Quantification of the economic benefits of trees helps justify public and private expenditures to maintain them. One of the most obvious examples of economic utility is the example of the deciduous tree planted on the south and west of a building (in the Northern Hemisphere), or north and east (in the Southern Hemisphere). The shade shelters and cools the building during the summer, but allows the sun to warm it in the winter after the leaves fall.\n\nThe USDA Guide notes on page 17 that \"Businesses flourish, people linger and shop longer, apartments and office space rent quicker, tenants stay longer, property values increase, new business and industry is attracted\" by trees. The physical effects of trees—the shade (solar regulation), humidity control, wind control, erosion control, evaporative cooling, sound and visual screening, traffic control, pollution absorption and precipitation—all have economic benefits.\n\nAs cities struggle to comply with air quality standards, trees can help to clean the air. The most serious pollutants in the urban atmosphere are ozone, nitrogen oxides (NOx), sulfuric oxides (SOx) and particulate pollution. Ground-level ozone, or smog, is created by chemical reactions between NOx and volatile organic compounds (VOCs) in the presence of sunlight. High temperatures increase the rate of this reaction. Vehicle emissions (especially diesel), and emissions from industrial facilities are the major sources of NOx. Vehicle emissions, industrial emissions, gasoline vapors, chemical solvents, trees and other plants are the major sources of VOCs. Particulate pollution, or particulate matter (PM10 and PM25), is made up of microscopic solids or liquid droplets that can be inhaled and retained in lung tissue causing serious health problems. Most particulate pollution begins as smoke or diesel soot and can cause serious health risk to people with heart and lung diseases and irritation to healthy citizens. Trees are an important, cost-effective solution to reducing pollution and improving air quality.\n\nWith an extensive and healthy urban forest air quality can be drastically improved. Trees help to lower air temperatures and the urban heat island effect in urban areas (see: 'Trees are energy savers' for more information on this process). This reduction of temperature not only lowers energy use, it also improves air quality, as the formation of ozone is dependent on temperature. Trees reduce temperature not only by directly shading: when there is a large number of trees it create a difference in temperatures between the area when they are located and the neighbor area. This creates a difference in atmospheric pressure between the two areas, which creates wind. This phenomenon is called urban breeze cycle if the forest is near the city and park breeze cycle if the forest is in the city. That wind helps to lower temperature in the city.\n\nTemperature reduction from shade trees in parking lots lowers the amount of evaporative emissions from parked cars. Unshaded parking lots can be viewed as miniature heat islands, where temperatures can be even higher than surrounding areas. Tree canopies will reduce air temperatures significantly. Although the bulk of hydrocarbon emissions come from tailpipe exhaust, 16% of hydrocarbon emissions are from evaporative emissions that occur when the fuel delivery systems of parked vehicles are heated. These evaporative emissions and the exhaust emissions of the first few minutes of engine operation are sensitive to local microclimate. If cars are shaded in parking lots, evaporative emissions from fuel and volatilized plastics will be greatly reduced. \nThe volatile components of asphalt pavement evaporate more slowly in shaded parking lots and streets. The shade not only reduces emissions, but reduces shrinking and cracking so that maintenance intervals can be lengthened. Less maintenance means less hot asphalt (fumes) and less heavy equipment (exhaust). The same principle applies to asphalt-based roofing.\n\nTrees also reduce pollution by actively removing it from the atmosphere. Leaf stomata, the pores on the leaf surface, take in polluting gases which are then absorbed by water inside the leaf. Some species of trees are more susceptible to the uptake of pollution, which can negatively affect plant growth. Ideally, trees should be selected that take in higher quantities of polluting gases and are resistant to the negative effects they can cause.\n\nA study across the Chicago region determined that trees removed approximately 17 tonnes of carbon monoxide (CO), 93 tonnes of sulfur dioxide (SO2), 98 tonnes of nitrogen dioxide (NO), and 210 tonnes of ozone (O) in 1991.\n\nUrban forest managers are sometimes interested in the amount of carbon removed from the air and stored in their forest as wood in relation to the amount of carbon dioxide released into the atmosphere while running tree maintenance equipment powered by fossil fuels.\n\nIn addition to the uptake of harmful gases, trees act as filters intercepting airborne particles and reducing the amount of harmful particulate matter. The particles are captured by the surface area of the tree and its foliage. These particles temporarily rest on the surface of the tree, as they can be washed off by rainwater, blown off by high winds, or fall to the ground with a dropped leaf. Although trees are only a temporary host to particulate matter, if they did not exist, the temporarily housed particulate matter would remain airborne and harmful to humans. Increased tree cover will increase the amount of particulate matter intercepted from the air. \n\nOne important thing to consider when assessing the urban forest's effect on air quality is that trees emit some biogenic volatile organic compounds (BVOCs). These are the chemicals (primarily isoprene and monoterpenes) that make up the essential oils, resins, and other organic compounds that plants use to attract pollinators and repel predators. As mentioned above, VOCs react with nitrogen oxides (NOx) to form ozone. BVOCs account for less than 10% of the total amount of BVOCs emitted in urban areas. This means that BVOC emissions from trees can contribute to the formation of ozone. Although their contribution may be small compared with other sources, BVOC emissions could exacerbate a smog problem.\n\nNot all species of trees, however, emit high quantities of BVOCs. The tree species with the highest isoprene emission rates should be planted with caution: \n\nTrees that are well adapted to and thrive in certain environments should not be replaced just because they may be high BVOC emitters. The amount of emissions spent on maintaining a tree that may emit low amounts of BVOCs, but is not well suited to an area, could be considerable and outweigh any possible benefits of low BVOC emission rates.\n\nTrees should not be labeled as polluters because their total benefits on air quality and emissions reduction far outweigh the possible consequences of BVOC emissions on ozone concentrations. Emission of BVOCs increase exponentially with temperature. Therefore, higher emissions will occur at higher temperatures. In desert climates, locally native trees adapted to drought conditions emit significantly less BVOCs than plants native to wet regions. As discussed above, the formation of ozone is also temperature dependent. Thus, the best way to slow the production of ozone and emission of BVOCs is to reduce urban temperatures and the effect of the urban heat island. As suggested earlier, the most effective way to lower temperatures is with an increased canopy cover.\n\nThese effects of the urban forest on ozone production have only recently been discovered by the scientific community, so extensive and conclusive research has not yet been conducted. There have been some studies quantifying the effect of BVOC emissions on the formation of ozone, but none have conclusively measured the effect of the urban forest. Important questions remain unanswered. For instance, it is unknown if there are enough chemical reactions between BVOC emissions and NOx to produce harmful amounts of ozone in urban environments. It is therefore, important for cities to be aware that this research is still continuing and conclusions should not be drawn before proper evidence has been collected. New research may resolve these issues.\n\n\n\n"}
{"id": "36057002", "url": "https://en.wikipedia.org/wiki?curid=36057002", "title": "Wind power in Lithuania", "text": "Wind power in Lithuania\n\nWind power in Lithuania is a form of renewable energy in Lithuania. At the end of 2011 wind power capacity in Lithuania was 179 MW and the wind energy share of total electricity consumption was 3,8%. At the end of 2018 wind power capacity in Lithuania was 521 MW and the wind energy share of total electricity consumption was 12%.\n\nInstalled wind power capacity in Lithuania and generation in recent years is shown in the table below:\n\n"}
{"id": "463601", "url": "https://en.wikipedia.org/wiki?curid=463601", "title": "Yttrium barium copper oxide", "text": "Yttrium barium copper oxide\n\nYttrium barium copper oxide (YBCO) is a family of crystalline chemical compounds, famous for displaying high-temperature superconductivity. It includes the first material ever discovered to become superconducting above the boiling point of liquid nitrogen (77 K) at about 90 K. Many YBCO compounds have the general formula YBaCuO (also known as Y123), although materials with other Y:Ba:Cu ratios exist, such as YBaCuO (Y124) or YBaCuO (Y247).\n\nIn April 1986, Georg Bednorz and Karl Müller, working at IBM in Zurich, discovered that certain semiconducting oxides became superconducting at relatively high temperature, in particular, a lanthanum barium copper oxide becomes superconducting at 35 K. This oxide was an oxygen-deficient perovskite-related material that proved promising and stimulated the search for related compounds with higher superconducting transition temperatures. In 1987, Bednorz and Müller were jointly awarded the Nobel Prize in Physics for this work.\n\nFollowing Bednorz and Müller's work, in 1987 Maw-Kuen Wu and Chu Ching-wu and their graduate students Ashburn and Torng at the University of Alabama in Huntsville, discovered that YBCO has a critical temperature (\"T\") of 93 K. The first samples were YBaCuO; but this was an average composition for two phases, a black and a green one. To identify the phases, Chu turned to Dave Mao and Robert Hazen at the Geophysical Laboratory in the Carnegie Institution of Washington. They found that the black one (which turned out to be the superconductor) had the composition YBaCuO. The article reporting this material led to rapid discovery of several new high-temperature superconducting materials, ushering in a new era in material science and chemistry.\n\nYBCO was the first material found to become superconducting above 77 K, the boiling point of liquid nitrogen. All materials developed before 1986 became superconducting only at temperatures near the boiling points of liquid helium (\"T\" = 4.2 K) or liquid hydrogen (\"T\" = 20.28 K) — the highest being NbGe at 23 K. The significance of the discovery of YBCO is the much lower cost of the refrigerant used to cool the material to below the critical temperature.\n\nRelatively pure YBCO was first synthesized by heating a mixture of the metal carbonates at temperatures between 1000 and 1300 K.\n\nModern syntheses of YBCO use the corresponding oxides and nitrates.\n\nThe superconducting properties of YBaCuO are sensitive to the value of \"x\", its oxygen content. Only those materials with 0 ≤ \"x\" ≤ 0.65 are superconducting below \"T\", and when \"x\" ~ 0.7, the material superconducts at the highest temperature of 95 K, or in highest magnetic fields: 120 T for B perpendicular and 250 T for B parallel to the CuO planes.\n\nIn addition to being sensitive to the stoichiometry of oxygen, the properties of YBCO are influenced by the crystallization methods used. Care must be taken to sinter YBCO. YBCO is a crystalline material, and the best superconductive properties are obtained when crystal grain boundaries are aligned by careful control of annealing and quenching temperature rates.\n\nNumerous other methods to synthesize YBCO have developed since its discovery by Wu and his co-workers, such as chemical vapor deposition (CVD), sol-gel, and aerosol methods. These alternative methods, however, still require careful sintering to produce a quality product.\n\nHowever, new possibilities have been opened since the discovery that trifluoroacetic acid (TFA), a source of fluorine, prevents the formation of the undesired barium carbonate (BaCO). Routes such as CSD (chemical solution deposition) have opened a wide range of possibilities, particularly in the preparation of long YBCO tapes. This route lowers the temperature necessary to get the correct phase to around 700 °C. This, and the lack of dependence on vacuum, makes this method a very promising way to get scalable YBCO tapes.\n\nYBCO crystallizes in a defect perovskite structure consisting of layers. The boundary of each layer is defined by planes of square planar CuO units sharing 4 vertices. The planes can sometimes be slightly puckered. Perpendicular to these CuO planes are CuO ribbons sharing 2 vertices. The yttrium atoms are found between the CuO planes, while the barium atoms are found between the CuO ribbons and the CuO planes. This structural feature is illustrated in the figure to the right.\n\nAlthough YBaCuO is a well-defined chemical compound with a specific structure and stoichiometry, materials with fewer than seven oxygen atoms per formula unit are non-stoichiometric compounds. The structure of these materials depends on the oxygen content. This non-stoichiometry is denoted by the x in the chemical formula YBaCuO. When \"x\" = 1, the O(1) sites in the Cu(1) layer are vacant and the structure is tetragonal. The tetragonal form of YBCO is insulating and does not superconduct. Increasing the oxygen content slightly causes more of the O(1) sites to become occupied. For \"x\" < 0.65, Cu-O chains along the \"b\" axis of the crystal are formed. Elongation of the \"b\" axis changes the structure to orthorhombic, with lattice parameters of \"a\" = 3.82, \"b\" = 3.89, and \"c\" = 11.68 Å. Optimum superconducting properties occur when \"x\" ~ 0.07, i.e., almost all of the O(1) sites are occupied, with few vacancies.\n\nIn experiments where other elements are substituted on the Cu and Ba sites, evidence has shown that conduction occurs in the Cu(2)O planes while the Cu(1)O(1) chains act as charge reservoirs, which provide carriers to the CuO planes. However, this model fails to address superconductivity in the homologue Pr123 (praseodymium instead of yttrium). This (conduction in the copper planes) confines conductivity to the \"a\"-\"b\" planes and a large anisotropy in transport properties is observed. Along the \"c\" axis, normal conductivity is 10 times smaller than in the \"a\"-\"b\" plane. For other cuprates in the same general class, the anisotropy is even greater and inter-plane transport is highly restricted.\n\nFurthermore, the superconducting length scales show similar anisotropy, in both penetration depth (λ ≈ 150 nm, λ ≈ 800 nm) and coherence length, (ξ ≈ 2 nm, ξ ≈ 0.4 nm). Although the coherence length in the \"a\"-\"b\" plane is 5 times greater than that along the \"c\" axis it is quite small compared to classic superconductors such as niobium (where ξ ≈ 40 nm). This modest coherence length means that the superconducting state is more susceptible to local disruptions from interfaces or defects on the order of a single unit cell, such as the boundary between twinned crystal domains. This sensitivity to small defects complicates fabricating devices with YBCO, and the material is also sensitive to degradation from humidity.\n\nMany possible applications of this and related high temperature superconducting materials have been discussed. For example, superconducting materials are finding use as magnets in magnetic resonance imaging, magnetic levitation, and Josephson junctions. (The most used material for power cables and magnets is BSCCO.)\n\nYBCO has yet to be used in many applications involving superconductors for two primary reasons:\n\nCooling materials to liquid nitrogen temperature (77 K) is often not practical on a large scale, although many commercial magnets are routinely cooled to liquid helium temperatures (4.2 K).\n\nThe most promising method developed to utilize this material involves deposition of YBCO on flexible metal tapes coated with buffering metal oxides. This is known as coated conductor. Texture (crystal plane alignment) can be introduced into the metal tape itself (the RABiTS process) or a textured ceramic buffer layer can be deposited, with the aid of an ion beam, on an untextured alloy substrate (the IBAD process). Subsequent oxide layers prevent diffusion of the metal from the tape into the superconductor while transferring the template for texturing the superconducting layer. Novel variants on CVD, PVD, and solution deposition techniques are used to produce long lengths of the final YBCO layer at high rates. Companies pursuing these processes include American Superconductor, Superpower (a division of Furukawa Electric), Sumitomo, Fujikura, Nexans Superconductors, Commonwealth Fusion Systems, and European Advanced Superconductors. A much larger number of research institutes have also produced YBCO tape by these methods.\n\nThe superconducting tape may be the key to a tokamak fusion reactor design that can achieve breakeven energy production. YBCO is often categorized as a rare-earth barium copper oxide (REBCO).\n\nSurface modification of materials has often led to new and improved properties. Corrosion inhibition, polymer adhesion and nucleation, preparation of organic superconductor/insulator/high-\"T\" superconductor trilayer structures, and the fabrication of metal/insulator/superconductor tunnel junctions have been developed using surface-modified YBCO.\n\nThese molecular layered materials are synthesized using cyclic voltammetry. Thus far, YBCO layered with alkylamines, arylamines, and thiols have been produced with varying stability of the molecular layer. It has been proposed that amines act as Lewis bases and bind to Lewis acidic Cu surface sites in YBaCuO to form stable coordination bonds.\n\nShortly after it was discovered, the U.K. Journal \"New Scientist\" published the 'recipe' for synthesizing YBCO by Heidi Grant (Daughter of author Paul Grant, and herself a high school student at the time of publication) with tools and equipment available in a high-school science laboratory of the time in their edition of 30th July 1987.\n\nThanks to this and other publications at the time, YBCO is a popular high-temperature superconductor for use by hobbyists and in education, as the magnetic levitation effect can be easily demonstrated utilising liquid nitrogen as coolant.\n\n"}
