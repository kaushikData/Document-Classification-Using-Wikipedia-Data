{"id": "3222990", "url": "https://en.wikipedia.org/wiki?curid=3222990", "title": "Akaganéite", "text": "Akaganéite\n\nAkaganeite is an iron(III) oxide-hydroxide / chloride mineral with formula: FeO(OH,Cl) e.g.; β-FeO(OH). It is formed by the weathering of pyrrhotite (FeS). Nickel also has been reported in the structure. It has a monoclinic crystal structure, a metallic luster and a brownish yellow streak. It is named after the Akagane mine in Iwate, Japan, in which it was discovered. It has also been found in widely dispersed locations around the world and in rocks from the Moon that were brought back during the Apollo Project. The occurrences in meteorites and the lunar sample are thought to have been produced by interaction with Earth's atmosphere. It has been detected on Mars through orbital imaging spectroscopy . \n\nIt was described by the Japanese mineralogist Matsuo Nambu in 1968 but named as early as 1961. The name comes from the topotype, the Akagane Mine in Iwate Prefecture, Japan.\n\nList of minerals\n\n"}
{"id": "9946905", "url": "https://en.wikipedia.org/wiki?curid=9946905", "title": "Ammonium perchlorate composite propellant", "text": "Ammonium perchlorate composite propellant\n\nAmmonium perchlorate composite propellant (APCP) is a modern solid-propellant rocket used in rocket vehicles. It differs from many traditional solid rocket propellants such as black powder or zinc-sulfur, not only in chemical composition and overall performance, but also by the nature of how it is processed. APCP is cast into shape, as opposed to powder pressing as with black powder. This provides manufacturing regularity and repeatability, which are necessary requirements for use in the aerospace industry.\n\nAmmonium perchlorate composite propellant is typically used in aerospace propulsion applications, where simplicity and reliability are desired and specific impulses (depending on the composition and operating pressure) of 180–260 seconds are adequate. Because of these performance attributes, APCP is regularly implemented in booster applications such as in the Space Shuttle Solid Rocket Boosters, aircraft ejection seats, and specialty space exploration applications such as NASA's Mars Exploration Rover descent stage retrorockets. In addition, the high-power rocketry community regularly uses APCP in the form of commercially available propellant \"reloads\", as well as single-use motors. Experienced experimental and amateur rocketeers also often work with APCP, processing the APCP themselves.\n\nAmmonium perchlorate composite propellant is a composite propellant, meaning that it has both fuel and oxidizer mixed with a rubbery binder, all combined into a homogeneous mixture. The propellant is most often composed of ammonium perchlorate (AP), an elastomer binder such as hydroxyl-terminated polybutadiene (HTPB) or polybutadiene acrylic acid acrylonitrile prepolymer (PBAN), powdered metal (typically aluminum), and various burn rate catalysts. In addition, curing additives induce elastomer binder cross-linking to solidify the propellant before use. The perchlorate serves as the oxidizer, while the binder and aluminum serve as the fuel. Burn rate catalysts determine how quickly the mixture burns. The resulting cured propellant is fairly elastic (rubbery), which also helps limit fracturing during accumulated damage (such as shipping, installing, cutting) and high acceleration applications such as hobby or military rocketry.\n\nThe composition of APCP can vary significantly depending on the application, intended burn characteristics, and constraints such as nozzle thermal limitations or specific impulse (Isp). Rough mass proportions (in high performance configurations) tend to be about 70/15/15 AP/HTPB/Al, though fairly high performance \"low-smoke\" can have compositions of roughly 80/18/2 AP/HTPB/Al. While metal fuel is not required in APCP, most formulations include at least a few percent as a combustion stabilizer, propellant opacifier (to limit excessive infrared propellant preheating), and increase the temperature of the combustion gases (increasing Isp).\n\nOxidizers:\n\nHigh energy fuels:\n\nLow energy fuels acting as binders:\n\nThough increasing the ratio of metal fuel to oxidizer up to the stoichiometric point increases the combustion temperature, the presence of an increasing molar fraction of metal oxides, particularly aluminum oxide (AlO) precipitating from the gaseous solution creates globules of solids or liquids that slow down the flow velocity as the mean molecular mass of the flow increases. In addition, the chemical composition of the gases change, varying the effective heat capacity of the gas. Because of these phenomena, there exists an optimal non-stoichiometric composition for maximizing Isp of roughly 16% by mass, assuming the combustion reaction goes to completion inside the combustion chamber.\n\nThe combustion time of the aluminum particles in the hot combustion gas varies depending on aluminum particle size and shape. In small APCP motors with high aluminum content, the residence time of the combustion gases does not allow for full combustion of the aluminum and thus a substantial fraction of the aluminum is burned outside the combustion chamber, leading to decreased performance. This effect is often mitigated by reducing aluminum particle size, inducing turbulence (and therefore a long characteristic path length and residence time), and/or by reducing the aluminum content to ensure a combustion environment with a higher net oxidizing potential, ensuring more complete aluminum combustion. Aluminum combustion inside the motor is the rate-limiting pathway since the liquid-aluminum droplets (even still liquid at temperatures 3000 K) limit the reaction to a heterogeneous globule interface, making the surface area to volume ratio an important factor in determining the combustion residence time and required combustion chamber size/length.\n\nThe propellant particle size distribution has a profound impact on APCP rocket motor performance. Smaller AP and Al particles lead to higher combustion efficiency but also lead to increased linear burn rate. The burn rate is heavily dependent on mean AP particle size as the AP absorbs heat to decompose into a gas before it can oxidize the fuel components. This process may be a rate-limiting step in the overall combustion rate of APCP. The phenomenon can be explained by considering the heat-flux-to-mass ratio: As the particle radius increases the volume (and, therefore, mass and heat capacity) increase as the cube of the radius. However, the surface area increases as the square of the radius, which is roughly proportional to the heat flux into the particle. Therefore, a particle's rate of temperature rise is maximized when the particle size is minimized.\n\nCommon APCP formulations call for 30-400 µm AP particles (often spherical), as well as 2–50 µm Al particles (often spherical). Because of the size discrepancy between the AP and Al, Al will often take an interstitial position in a pseudo-lattice of AP particles.\n\nAPCP deflagrates from the surface of exposed propellant in the combustion chamber. In this fashion, the geometry of the propellant inside the rocket motor plays an important role in the overall motor performance. As the surface of the propellant burns the shape evolves (a subject of study in internal ballistics), most often changing the propellant surface area exposed to the combustion gases. The mass flux (kg/s) [and therefore pressure] of combustion gases generated is a function of the instantaneous surface area formula_1 (m), propellant density formula_2 (kg/m), and linear burn rate formula_3 (m/s):\n\nformula_4\n\nSeveral geometric configurations are often used depending on the application and desired thrust curve:\n\nWhile the surface area can be easily tailored by careful geometric design of the propellant, the burn rate is dependent on several subtle factors:\n\n\nIn summary, however, most formulations have a burn rate between 1–3 mm/s at STP and 6–12 mm/s at 68 atm. The burn characteristics (such as linear burn rate) are often determined prior to rocket motor firing using a strand burner test. This test allows the APCP manufacturer to characterize the burn rate as a function of pressure. Empirically, APCP adheres fairly well to the following power-function model:\n\nformula_5\n\nIt is worth noting that typically for APCP, 0.3<n<0.5 indicating that APCP is sub-critically pressure sensitive. That is, if surface area were maintained constant during a burn the combustion reaction would not runaway to (theoretically) infinite as the pressure would reach an internal equilibrium. This isn't to say that APCP cannot cause an explosion, but rather that the explosion would be caused by the pressure surpassing the burst pressure of the container (rocket motor).\n\nCommercial APCP rocket engines usually come in the form of reloadable motor systems (RMS) and fully assembled single-use rocket motors. For RMS, the APCP \"grains\" (cylinders of propellant) are loaded into the reusable motor casing along with a sequence of insulator disks and o-rings and a (graphite or glass-filled phenolic resin) nozzle. The motor casing and closures are typically bought separately from the motor manufacturer and are often precision-machined from aluminum. The assembled RMS contains both reusable (typically metal) and disposable components.\n\nThe major APCP suppliers for hobby use are:\n\nTo achieve different visual effects and flight characteristics, hobby APCP suppliers offer a variety of different characteristic propellant types. These can range from fast-burning with little smoke and blue flame to classic white smoke and white flame. In addition, colored formulations are available to display reds, greens, blues, and even black smoke.\n\nIn medium- and high-power rocket applications, APCP has largely replaced black powder as a rocket propellant. Compacted black powder slugs become prone to fracture in larger applications, which can result in catastrophic failure in rocket vehicles. APCP's elastic material properties make it less vulnerable to fracture from accidental shock or high-acceleration flights. Due to these attributes, widespread adoption of APCP and related propellant types in the hobby has significantly enhanced the safety of rocketry.\n\nThe exhaust from APCP solid rocket motors contains mostly water, carbon dioxide, hydrogen chloride, and a metal oxide (typically aluminium oxide). The hydrogen chloride can easily dissolve in water and create corrosive hydrochloric acid. The environmental fate of the hydrogen chloride is not well documented. The hydrochloric acid component of APCP exhaust leads to the condensation of atmospheric moisture in the plume and this enhances the visible signature of the contrail. This visible signature, among other reasons, led to research in cleaner burning propellants with no visible signatures. Minimum signature propellants contain primarily nitrogen-rich organic molecules (e.g., ammonium dinitramide) and depending on their oxidizer source can be hotter burning than APCP composite propellants.\n\nIn the United States, APCP for hobby use is regulated indirectly by two non-government agencies: the National Association of Rocketry (NAR), and the Tripoli Rocketry Association (TRA). Both agencies set forth rules regarding the impulse classification of rocket motors and the level of certification required by rocketeers in order to purchase certain impulse (size) motors. The NAR and TRA require motor manufactures to certify their motors for distribution to vendors and ultimately hobbyists. The vendor is charged with the responsibility (by the NAR and TRA) to check hobbyists for high power rocket certification before a sale can be made. The amount of APCP that can be purchased (in the form of a rocket motor reload) correlates to the impulse classification, and therefore the quantity of APCP purchasable by hobbyist (in any single reload kit) is regulated by the NAR and TRA.\n\nThe overarching legality concerning the implementation of APCP in rocket motors is outlined in NFPA 1125. Use of APCP outside hobby use is regulated by state and municipal fire codes. On March 16, 2009, it was ruled that APCP is not an explosive and that manufacture and use of APCP no longer requires a license or permit from the ATF.\n\n"}
{"id": "1810267", "url": "https://en.wikipedia.org/wiki?curid=1810267", "title": "Analytical balance", "text": "Analytical balance\n\nAn analytical balance (often called a \"lab balance\") is a class of balance designed to measure small mass in the sub-milligram range. The measuring pan of an analytical balance (0.1 mg or better) is inside a transparent enclosure with doors so that dust does not collect and so any air currents in the room do not affect the balance's operation. This enclosure is often called a draft shield. The use of a mechanically vented balance safety enclosure, which has uniquely designed acrylic airfoils, allows a smooth turbulence-free airflow that prevents balance fluctuation and the measure of mass down to 1 μg without fluctuations or loss of product. Also, the sample must be at room temperature to prevent natural convection from forming air currents inside the enclosure from causing an error in reading. Single pan mechanical substitution balance maintains consistent response throughout the useful capacity is achieved by maintaining a constant load on the balance beam, thus the fulcrum, by subtracting mass on the same side of the beam to which the sample is added. \n\nElectronic analytical scales measure the force needed to counter the mass being measured rather than using actual masses. As such they must have calibration adjustments made to compensate for gravitational differences. They use an electromagnet to generate a force to counter the sample being measured and outputs the result by measuring the force needed to achieve balance. Such measurement device is called electromagnetic force restoration sensor.\n\nTriple beam balance is an instrument used to measure mass very precisely. The device has reading error of +/- 0.05 gram. The name refers to the three beams including the middle beam which is the largest size, the front beam which is the medium size, and the far beam which is the smallest size . The difference in size of the beams indicate the difference in weights and reading scale that each beam carry. The reading scale can be enumerated that the middle beam reads in 100 gram increments, the front beam can read from 0 to 10 grams, and the far beam can read in 10 gram increments. The triple beam balance can be used to measure mass directly into\nfrom the objects, find mass by difference for liquid, and measure out a substance. The parts of triple beam balance are identified as the following.\nBefore using triple beam balance, the scale pointer should be at zero. The zero adjustment knob can be used to adjust the scale pointer. Place the objects on the pan and adjust the riders. The hundred rider should be initially adjusted and follow by the tens rider. Adjust the ones rider until the scale pointer is at zero again.\n\n"}
{"id": "1358453", "url": "https://en.wikipedia.org/wiki?curid=1358453", "title": "Astrophysical jet", "text": "Astrophysical jet\n\nAn astrophysical jet is an astronomical phenomenon where outflows of ionised matter are emitted as an extended beam along the axis of rotation. When this greatly accelerated matter in the beam approaches the speed of light, astrophysical jets become relativistic jets as they show effects from special relativity.\n\nThe formation and powering of astrophysical jets are highly complex phenomena that are associated with many types of high-energy astronomical sources. They likely arise from dynamic interactions within accretion disks, whose active processes are commonly connected with compact central objects such as black holes, neutron stars or pulsars. One explanation is that tangled magnetic fields are organised to aim two diametrically opposing beams away from the central source by angles only several degrees wide Jets may also be influenced by a general relativity effect known as frame-dragging.\n\nMost of the largest and most active jets are created by supermassive black holes (SMBH) in the centre of active galaxies such as quasars and radio galaxies or within galaxy clusters. Such jets can exceed millions of parsecs in length. Other astronomical objects that contain jets include cataclysmic variable stars, X-ray binaries and Gamma ray bursters (GRB). Others are associated with star forming regions including T Tauri stars and Herbig–Haro objects, which are caused by the interaction of jets with the interstellar medium. Bipolar outflows or jets may also be associated with protostars, or with evolved post-AGB stars, planetary nebulae and bipolar nebulae.\n\nRelativistic jets are beams of ionised matter accelerated close to the speed of light. Most have been observationally associated with central black holes of some active galaxies, radio galaxies or quasars, and also by galactic stellar black holes, neutron stars or pulsars. Beam lengths may extend between several thousand, hundreds of thousands or millions of parsecs. Jet velocities when approaching the speed of light show significant effects of the special theory of relativity; for example, relativistic beaming that changes the apparent beam brightness (see the 'one-sided' jets below). \nMassive central black holes in galaxies have the most powerful jets, but their structure and behaviours are similar to those of smaller galactic neutron stars and black holes. These SMBH systems are often called microquasars and show a large range of velocities. SS433 jet, for example, has a velocity of 0.23c. Relativistic jet formation may also explain observed gamma-ray bursts. Notably, even weaker and less relativistic jets may be associated with many binary systems.\n\nMechanisms behind the composition of jets remain uncertain, though some studies favour models where jets are composed of an electrically neutral mixture of nuclei, electrons, and positrons, while others are consistent with jets composed of positron–electron plasma. Trace nuclei swept up in a relativistic positron–electron jet would be expected to have extremely high energy, as these heavier nuclei should attain velocity equal to the positron and electron velocity.\n\nBecause of the enormous amount of energy needed to launch a relativistic jet, some jets are possibly powered by spinning black holes. However, the frequency of high-energy astrophysical sources with jets suggest combination of different mechanisms indirectly identified with the energy within the associated accretion disk and X-ray emissions from the generating source. Two early theories have been used to explain how energy can be transferred from a black hole into an astrophysical jet:\n\n\nJets may also be observed from spinning neutron stars. An example is pulsar IGR J11014-6103, which has the largest jet so far observed in the Milky Way Galaxy whose velocity is estimated at 80% the speed of light. (0.8c.) X-ray observations have been obtained but there is no detected radio signature nor accretion disk. Initially, this pulsar was presumed to be rapidly spinning but later measurements indicate the spin rate is only 15.9 Hz. Such a slow spin rate and lack of accretion material suggest the jet is neither rotation nor accretion powered, though it appears aligned with the pulsar rotation axis and perpendicular to the pulsar's true motion.\n\n\n\n"}
{"id": "16819554", "url": "https://en.wikipedia.org/wiki?curid=16819554", "title": "BARS apparatus", "text": "BARS apparatus\n\nBARS (or \"split sphere\", transliteration from abbreviation of , (bespressovaya apparatura vysokogo davleniya «razreznaya sfera»), \"press-free high-pressure setup «split sphere»\") a high-pressure high-temperature apparatus usually used for growing or processing minerals, especially diamond and boron nitride. Typical pressures and temperatures achievable with BARS are and .\n\nThe BARS technology was invented around 1989–1991 by the scientists from the Institute of Geology and Geophysics of the Siberian Branch of the Academy of Sciences of the USSR. In the center of the device, there is a ceramic cylindrical reaction cell of about 2 cm in size. The cell is placed into a cubic-shaped pressure-transmitting material, which is pressed by elements made from cemented carbide (VK10 hard alloy). The outer octahedral cavity is pressed by 8 steel sectors. After mounting, the whole assembly is locked in a disc-type barrel with a diameter ~1 meter. The barrel is filled with oil, which pressurizes upon heating; the oil pressure is transferred to the central cell. The central cell is heated up by a coaxial graphite heater. Temperature is measured with a thermocouple. The exterior size is 2.2 х 1.0 х 1.2 meters. Weight of the sphere is . Claimed energy consumption is in between 1.5 - 2 kWh.\n\nThe growth rate for type Ib (yellow, nitrogen-rich) crystals using Fe–Ni catalyst reaches as high as ~20 mg/h towards the end of 100 h growth cycle, \"i.e.\" crystals of to can be grown in less than 100 h.\n\n"}
{"id": "34543357", "url": "https://en.wikipedia.org/wiki?curid=34543357", "title": "Bilbao Crystallographic Server", "text": "Bilbao Crystallographic Server\n\nBilbao Crystallographic Server is an open access website offering online crystallographic database and programs aimed at analyzing, calculating and visualizing problems of structural and mathematical crystallography, solid state physics and structural chemistry. Initiated in 1997 by the Materials Laboratory of the Department of Condensed Matter Physics at the University of the Basque Country, Bilbao, Spain, the Bilbao Crystallographic Server is developed and maintained by academics.\n\nFocusing on crystallographic data and applications of the group theory in solid state physics, the server is built on a core of databases and contains different shells.\n\nThe set of databases includes data from \"International Tables of Crystallography, Vol. A: Space-Group Symmetry\", and the data of maximal subgroups of space groups as listed in \"International Tables of Crystallography, Vol. A1: Symmetry relations between space groups\". A k-vector database with Brillouin zone figures and classification tables of the k-vectors for space groups is also available via the KVEC tool.\n\nIn 2011, the Magnetic Space Groups data compiled from H.T. Stokes & B.J. Campbell's and D. Litvin's's works general positions/symmetry operations and Wyckoff positions for different settings, along with systematic absence rules have also been incorporated into the server and a new shell has been dedicated to the related tools (MGENPOS, MWYCKPOS, MAGNEXT).\n\nThis shell contains applications which are essential for problems involving group-subgroup relations between space groups. Given the space group types of G and H and their index, the program SUBGROUPGRAPH provides graphs of maximal subgroups for a group-subgroup pair G > H, all the different subgroups H and their distribution into conjugacy classes. The Wyckoff position splitting rules for a group-subgroup pair are calculated by the program WYCKSPLIT.\n\nThe fourth shell includes programs on representation theory of space and point groups. REPRES constructs little group and full group irreducible representations for a given space group and a k-vector; CORREL deals with the correlations between the irreducible representations of group-subgroup related space groups. The program POINT lists character tables of crystallographic point groups, Kronecker multiplication tables of their irreducible representations and further useful symmetry information.\n\nThis shell is related to solid state physics and structural chemistry. The program PSEUDO performs an evaluation of the \"pseudosymmetry\" of a given structure with respect to supergroups of its space group. AMPLIMODES performs the symmetry-mode analysis of any distorted structure of displacive type. The analysis consists in decomposing the symmetry-breaking distortion present in the distorted structure into contributions from different symmetry-adapted modes. Given the high and low symmetry structures, the program calculates the amplitudes and polarization vectors of the distortion modes of different symmetry frozen in the structure. The program SAM calculates symmetry-adapted modes for the centre of the Brillouin zone and classifies them according to their infrared and Raman activity. NEUTRON computes the phonon extinction rules in inelastic neutron scattering. Its results are also relevant for diffuse-scattering experiments.\n\nA set of structure utilities has been included for various applications such as: the transformation of unit cells (CELLTRAN) or complete structures (TRANSTRU); strain tensor calculation (STRAIN); assignment of Wyckoff Positions (WPASSIGN); equivalent descriptions of a given structure (EQUIVSTRU); comparison of different structures with support for the affine normalizers of monoclinic space groups. STRUCTURE RELATIONS calculates the possible transformation matrices for a given pair of group-subgroup related structures.\n\nThe Bilbao Crystallographic Server also hosts the B-IncStrDB: Bilbao Incommensurate Crystal Structures Database, a database for incommensurately modulated and composite structures.\n\nIn addition to receiving citations from scientific articles and theses, the Bilbao Crystallographic Server also actively publishes research reports in internationally reviewed articles, as well as hosting/participating in international workshops, summer schools and conferences. A list of these publications and events are accessible from the server's web page..\n\nThe Bilbao Crystallographic Server came to life in 1997 as a scientific project by the Departments of Condensed Matter Physics and Applied Physics II of the University of the Basque Country (EHU) under the supervision of J. Manuel Perez-Mato (EHU) and Mois I. Aroyo (EHU), in coordination with Gotzon Madariaga (EHU) and Hans Wondratschek (Karlsruhe Institute of Technology, Germany) with funding from the Basque government and several ministries of the Spanish government. The initial code was written by then Ph.D. students Eli Kroumova (EHU) and Svet Ivantchev (EHU) and the very first shells related to retrieval tools, group-subgroup relations and space group representations have soon appeared online.\n\nAfterwards, in collaboration with Harold T. Stokes and Dorian M. Hatch from Brigham Young University, USA, the server extended its services to include symmetry modes analysis. Asen K. Kirov, a Ph.D. student from Sofia University, Bulgaria contributed to the server, working on programs dedicated to irreducible representations and extinction rules.\n\nIn 2001, Ph.D. student Cesar Capillas began his research on the server and became the main developer and system administrator focusing on structure relations, such as pseudosymmetry and phase transitions. Danel Orobengoa, also a Ph.D. student, joined the developer team in 2005 and worked mainly on symmetry modes, k-vector classification tables and non-characteristic orbits (in collaboration with Massimo Nespolo of the Nancy-Université, France), writing his Ph.D. thesis on the applications of the server for ferroic materials.\n\nIn 2009, Ph.D. student Gemma de la Flor and post-doc Emre S. Tasci were recruited for the development team: de la Flor working mainly on the identification and interpretation of symmetry operations, structure comparison and Tasci becoming the new system administrator and main developer, focusing in the structure relations concerning phase transitions. The Bilbao Crystallographic Server team took its current (2012) line-up in 2010 with the addition of Ph.D. student Samuel Vidal Gallego, his main research field being the magnetic space groups.\n\n"}
{"id": "32758582", "url": "https://en.wikipedia.org/wiki?curid=32758582", "title": "Brokke Hydroelectric Power Station", "text": "Brokke Hydroelectric Power Station\n\nThe Brokke Power Station is a hydroelectric power station located in the municipality Valle in Aust-Agder county, Norway. It is located on the west shore of the river Otra, about north of the village of Rysstad. The Norwegian National Road 9 runs past the station. The facility operates at an installed capacity of . The average annual production is . The power station receives its water from the lake Botnsvatnet via a long tunnel from the lake high up in the mountains. The water flowing down through the tunnel is used to produce the hydroelectric power.\n"}
{"id": "432276", "url": "https://en.wikipedia.org/wiki?curid=432276", "title": "Brownian ratchet", "text": "Brownian ratchet\n\nIn the philosophy of thermal and statistical physics, the Brownian ratchet or Feynman-Smoluchowski ratchet is a thought experiment about an apparent perpetual motion machine first analysed in 1912 by Polish physicist Marian Smoluchowski and popularised by American Nobel laureate physicist Richard Feynman in a physics lecture at the California Institute of Technology on May 11, 1962, during his Messenger Lectures series The Character of Physical Law in Cornell University in 1964 and in his text \"The Feynman Lectures on Physics\" as an illustration of the laws of thermodynamics. The simple machine, consisting of a tiny paddle wheel and a ratchet, appears to be an example of a Maxwell's demon, able to extract useful work from random fluctuations (heat) in a system at thermal equilibrium in violation of the second law of thermodynamics. Detailed analysis by Feynman and others showed why it cannot actually do this.\n\nThe device consists of a gear known as a ratchet that rotates freely in one direction but is prevented from rotating in the opposite direction by a pawl. The ratchet is connected by an axle to a paddle wheel that is immersed in a fluid of molecules at temperature formula_1. The molecules constitute a heat bath in that they undergo random Brownian motion with a mean kinetic energy that is determined by the temperature. The device is imagined as being small enough that the impulse from a single molecular collision can turn the paddle. Although such collisions would tend to turn the rod in either direction with equal probability, the pawl allows the ratchet to rotate in one direction only. The net effect of many such random collisions would seem to be that the ratchet rotates continuously in that direction. The ratchet's motion then can be used to do work on other systems, for example lifting a weight (\"m\") against gravity. The energy necessary to do this work apparently would come from the heat bath, without any heat gradient. Were such a machine to work successfully, its operation would violate the second law of thermodynamics, one form of which states: \"It is impossible for any device that operates on a cycle to receive heat from a single reservoir and produce a net amount of work.\"\n\nAlthough at first sight the Brownian ratchet seems to extract useful work from Brownian motion, Feynman demonstrated that if the entire device is at the same temperature, the ratchet will not rotate continuously in one direction but will move randomly back and forth, and therefore will not produce any useful work. The reason is that since the pawl is at the same temperature as the paddle, it will also undergo Brownian motion, \"bouncing\" up and down. It therefore will intermittently fail by allowing a ratchet tooth to slip backward under the pawl while it is up. Another issue is that when the pawl rests on the sloping face of the tooth, the spring which returns the pawl exerts a sideways force on the tooth which tends to rotate the ratchet in a backwards direction. Feynman demonstrated that if the temperature formula_2 of the ratchet and pawl is the same as the temperature formula_1 of the paddle, then the failure rate must equal the rate at which the ratchet ratchets forward, so that no net motion results over long enough periods or in an ensemble averaged sense. A simple but rigorous proof that no net motion occurs no matter what shape the teeth are was given by Magnasco. \n\nIf, on the other hand, formula_2 is smaller than formula_1, the ratchet will indeed move forward, and produce useful work. In this case, though, the energy is extracted from the temperature gradient between the two thermal reservoirs, and some waste heat is exhausted into the lower temperature reservoir by the pawl. In other words, the device functions as a miniature heat engine, in compliance with the second law of thermodynamics. Conversely, if formula_2 is greater than formula_1, the device will rotate in the opposite direction.\n\nThe Feynman ratchet model led to the similar concept of Brownian motors, nanomachines which can extract useful work not from thermal noise but from chemical potentials and other microscopic nonequilibrium sources, in compliance with the laws of thermodynamics. Diodes are an electrical analog of the ratchet and pawl, and for the same reason cannot produce useful work by rectifying Johnson noise in a circuit at uniform temperature.\n\nMillonas \nas well as Mahato\nextended the same notion to correlation ratchets driven by mean-zero (unbiased) nonequilibrium noise with a \nnonvanishing correlation function of odd order greater than one.\n\nThe ratchet and pawl was first discussed as a Second Law-violating device by Gabriel Lippmann in 1900. In 1912, Polish physicist Marian Smoluchowski gave the first correct qualitative explanation of why the device fails; thermal motion of the pawl allows the ratchet's teeth to slip backwards. Feynman did the first quantitative analysis of the device in 1962 using the Maxwell–Boltzmann distribution, showing that if the temperature of the paddle \"T\" was greater than the temperature of the ratchet \"T\", it would function as a heat engine, but if \"T\" = \"T\" there would be no net motion of the paddle. In 1996, Juan Parrondo and Pep Español used a variation of the above device in which no ratchet is present, only two paddles, to show that the axle connecting the paddles and ratchet conducts heat between reservoirs; they argued that although Feynman's conclusion was correct, his analysis was flawed because of his erroneous use of the quasistatic approximation, resulting in incorrect equations for efficiency. Magnasco and Stolovitzky (1998) extended this analysis to consider the full ratchet device, and showed that the power output of the device is far smaller than the Carnot efficiency claimed by Feynman. A paper in 2000 by Derek Abbott, Bruce R. Davis and Juan Parrondo, reanalyzed the problem and extended it to the case of multiple ratchets, showing a link with Parrondo's paradox.\n\nLéon Brillouin in 1950 discussed an electrical circuit analogue that uses a rectifier (such as a diode) instead of a ratchet. The idea was the diode would rectify the Johnson noise thermal current fluctuations produced by the resistor, generating a direct current which could be used to perform work. In the detailed analysis it was shown that the thermal fluctuations within the diode generate an electromotive force that cancels the voltage from rectified current fluctuations. Therefore, just as with the ratchet, the circuit will produce no useful energy if all the components are at thermal equilibrium (at the same temperature); a DC current will be produced only when the diode is at a lower temperature than the resistor.\n\nResearchers from the University of Twente, the University of Patras in Greece, and the Foundation for Fundamental Research on Matter have constructed a Feynman-Smoluchowski engine which, when not in thermal equilibrium, converts pseudo-Brownian motion into work by means of a granular gas, which is a conglomeration of solid particles vibrated with such vigour that the system assumes a gas-like state. The constructed engine consisted of four vanes which were allowed to rotate freely in a vibrofluidized granular gas. Because the ratchet's gear and pawl mechanism, as described above, permitted the axle to rotate only in one direction, random collisions with the moving beads caused the vane to rotate. This seems to contradict Feynman's hypothesis. However, this system is not in perfect thermal equilibrium: energy is constantly being supplied to maintain the fluid motion of the beads. Vigorous vibrations on top of a shaking device mimic the nature of a molecular gas. Unlike an ideal gas, though, in which tiny particles move constantly, stopping the shaking would simply cause the beads to drop. In the experiment, this necessary out-of-equilibrium environment was thus maintained. Work was not immediately being done, though; the ratchet effect only commenced beyond a critical shaking strength. For very strong shaking, the vanes of the paddle wheel interacted with the gas, forming a convection roll, sustaining their rotation. The experiment was filmed.\n\n\n\n"}
{"id": "20212950", "url": "https://en.wikipedia.org/wiki?curid=20212950", "title": "Chameleon particle", "text": "Chameleon particle\n\nThe chameleon is a hypothetical scalar particle that couples to matter more weakly than gravity, postulated as a dark energy candidate. Due to a non-linear self-interaction, it has a variable effective mass which is an increasing function of the ambient energy density—as a result, the range of the force mediated by the particle is predicted to be very small in regions of high density (for example on Earth, where it is less than 1mm) but much larger in low-density intergalactic regions: out in the cosmos chameleon models permit a range of up to several thousand parsecs. As a result of this variable mass, the hypothetical fifth force mediated by the chameleon is able to evade current constraints on equivalence principle violation derived from terrestrial experiments even if it couples to matter with a strength equal or greater than that of gravity. Although this property would allow the chameleon to drive the currently observed acceleration of the universe's expansion, it also makes it very difficult to test for experimentally.\n\nChameleon particles were proposed in 2003 by Khoury and Weltman.\n\nIn most theories, chameleons have a mass that scales as some power of the local energy density: formula_1, where formula_2.\n\nChameleons also couple to photons, allowing photons and chameleons to oscillate between each other in the presence of an external magnetic field.\n\nChameleons can be confined in hollow containers because their mass increases rapidly as they penetrate the container wall, causing them to reflect. One strategy to search experimentally for chameleons is to direct photons into a cavity, confining the chameleons produced, and then to switch off the light source. Chameleons would be indicated by the presence of an afterglow as they decay back into photons.\n\nA number of experiments have attempted to detect chameleons along with axions.\n\nThe GammeV experiment is a search for axions, but has been used to look for chameleons too. It consists of a cylindrical chamber inserted in a 5 T magnetic field. The ends of the chamber are glass windows, allowing light from a laser to enter and afterglow to exit. GammeV set the limited coupling to photons in 2009.\n\nCHASE (CHameleon Afterglow SEarch) results published in November 2010, improve the limits on mass by 2 orders of magnitude and 5 orders for photon coupling.\n\nA 2014 neutron mirror measurement excluded chameleon field for values of the coupling constant formula_3, where the effective potential of the chameleon quanta is written as formula_4, formula_5 being the mass density of the environment, formula_6 the chameleon potential and formula_7 the reduced Planck mass.\n\nThe CERN Axion Solar Telescope has been suggested as a tool for detecting chameleons.\n\n"}
{"id": "19323590", "url": "https://en.wikipedia.org/wiki?curid=19323590", "title": "Conservation reserve", "text": "Conservation reserve\n\nA conservation reserve is a protected area set aside for conservation purposes.\n\nIn South Australia, a conservation reserve is a type of protected area declared under the \"Crown Land Management Act 2009\" for parcels of 'land set aside for conservation of natural and cultural features.'\n\nIn the United States the Conservation Reserve Program offers annual payments for 10-15 year contracts to participants who establish grass, shrub and tree cover on environmentally sensitive lands. It was reauthorized in the 1996 Farm Bill and the 2002 Farm Bill.\n"}
{"id": "4410101", "url": "https://en.wikipedia.org/wiki?curid=4410101", "title": "Croydon (carriage)", "text": "Croydon (carriage)\n\nA Croydon is a type of horse-drawn two-wheeled carriage. The first examples were seen around 1850 and were made of wicker-work, but they were later made of wood.\n\n\n"}
{"id": "825767", "url": "https://en.wikipedia.org/wiki?curid=825767", "title": "Cyclopropene", "text": "Cyclopropene\n\nCyclopropene is an organic compound with the formula . It is the simplest cycloalkene. Because the ring is highly strained, cyclopropene is difficult to prepare and highly reactive. This colorless gas has been the subject for many fundamental studies of bonding and reactivity. It does not occur naturally, but derivatives are known in some fatty acids. Derivatives of cyclopropene are used commercially to control ripening of some fruit.\n\nThe molecule has a triangular structure. The reduced length of the double bond compared to a single bond causes the angle opposite the double bond to narrow to about 51° from the 60° angle found in cyclopropane. As with cyclopropane, the carbon–carbon bonding in the ring has increased p character: the alkene carbons use sp hybridization for the ring.\n\nThe first confirmed synthesis of cyclopropene, carried out by Dem'yanov and Doyarenko, involved the thermal decomposition of trimethylcyclopropylammonium hydroxide over platinized clay at 320–330 °C under a CO atmosphere. This reaction produces mainly trimethylamine and dimethylcyclopropyl amine, together with about 5% of cyclopropene. Cyclopropene can also be obtained in about 1% yield by thermolysis of the adduct of cycloheptatriene and dimethyl acetylenedicarboxylate.\n\nAllyl chloride undergoes dehydrohalogenation upon treatment with the base sodium amide at 80 °C to produce cyclopropene in about 10% yield.\nThe major byproduct of the reaction is allylamine. Adding allyl chloride to sodium bis(trimethylsilyl)amide in boiling toluene over a period of 45–60 minutes produces the targeted compound in about 40% yield with an improvement in purity:\n1-Methylcyclopropene is synthesized similarly but at room temperature from methallylchloride using phenyllithium as the base:\n\nTreatment of nitrocyclopropanes with sodium methoxide eliminates the nitrite, giving the respective cyclopropene derivative. The synthesis of purely aliphatic cyclopropenes was first illustrated by the copper-catalyzed additions of carbenes to alkynes. In the presence of a copper catalyst, ethyl diazoacetate reacts with acetylenes to give cyclopropenes. 1,2-Dimethylcyclopropene-3-carboxylate arises via this method from 2-butyne. Copper, as copper sulfate and copper dust, are among the more popular forms of copper used to promote such reactions. Rhodium acetate has also been used. The addition of dichlorocarbene to tetrachloroethylene gives tetrachlorocyclopropene.\n\nStudies on cyclopropene mainly focus on the consequences of its high ring strain. At 425 °C, cyclopropene isomerizes to methylacetylene (propyne).\n\nAttempted fractional distillation of cyclopropene at –36 °C (its predicted boiling point) results in polymerization. The mechanism is assumed to be a free-radical chain reaction, and the product, based on NMR spectra, is thought to be polycyclopropane.\n\nCyclopropene undergoes the Diels–Alder reaction with cyclopentadiene to give endo-tricyclo[3.2.1.0]oct-6-ene. This reaction is commonly used to check for the presence of cyclopropene, following its synthesis.\n\n\n"}
{"id": "12569240", "url": "https://en.wikipedia.org/wiki?curid=12569240", "title": "Dam failure", "text": "Dam failure\n\nA dam is a barrier across flowing water that obstructs, directs or slows down the flow, often creating a reservoir, lake or impoundments. Most dams have a section called a \"spillway or weir\" over or through which water flows, either intermittently or continuously, and some have hydroelectric power generation systems installed.\n\nDams are considered \"installations containing dangerous forces\" under International humanitarian law due to the massive impact of a possible destruction on the civilian population and the environment. Dam failures are comparatively rare, but can cause immense damage and loss of life when they occur. In 1975 the failure of the Banqiao Reservoir Dam and other dams in Henan Province, China caused more casualties than any other dam failure in history. The disaster killed an estimated 171,000 people and 11 million people lost their homes.\n\nCommon causes of dam failure include:\n\nA notable case of deliberate dam failure (prior to the Humanitarian Law rulings) was the British Royal Air Force Dambusters raid on Germany in World War II (codenamed \"Operation Chastise\"), in which six German dams were selected to be breached in order to impact on German infrastructure and manufacturing and power capabilities deriving from the Ruhr and Eder rivers. This raid later became the basis for several films.\n\nOther cases include the Chinese bombing of multiple dams during Typhoon Nina (1975) in an attempt to drain them before their reservoirs overflowed. The typhoon produced what is now considered a 1-in-2000 years flood, which few if any of these dams were designed to survive.\n\n\n"}
{"id": "45652259", "url": "https://en.wikipedia.org/wiki?curid=45652259", "title": "Degassed water", "text": "Degassed water\n\nDegassed water is water subjected to a process of degassing, which essentially consists in the removal of gas dissolved in the liquid.\n\n"}
{"id": "1669741", "url": "https://en.wikipedia.org/wiki?curid=1669741", "title": "District heating", "text": "District heating\n\nDistrict heating (also known as heat networks or teleheating) is a system for distributing heat generated in a centralized location through a system of insulated pipes for residential and commercial heating requirements such as space heating and water heating. The heat is often obtained from a cogeneration plant burning fossil fuels or biomass, but heat-only boiler stations, geothermal heating, heat pumps and central solar heating are also used, as well as nuclear power. District heating plants can provide higher efficiencies and better pollution control than localized boilers. According to some research, district heating with combined heat and power (CHPDH) is the cheapest method of cutting carbon emissions, and has one of the lowest carbon footprints of all fossil generation plants. A combination of CHP and centralized heat pumps are used in the Stockholm multi energy system. This allows the production of heat through electricity when there is an abundance of intermittent power production and cogeneration of electric power and district heating when the availability of intermittent power production is low.\n\nDistrict heating traces its roots to the hot water-heated baths and greenhouses of the ancient Roman Empire. Usually, a hot water distribution system in Chaudes-Aigues in France is regarded as the first real district heating system. It used geothermal energy to provide heat for about 30 houses and started operation in the 14th century. \n\nThe U.S. Naval Academy in Annapolis began steam district heating service in 1853.\n\nAlthough these and numerous other systems have operated over the centuries, the first commercially successful district heating system was launched in Lockport, New York, in 1877 by American hydraulic engineer Birdsill Holly, considered the founder of modern district heating.\n\nGenerally, 4 different generations of district heating systems can be distinguished. \n\nThe first generation was a steam-based system fueled by coal and was first introduced in the US in the 1880s and became popular in some European countries, too. It was state of the art until the 1930s and used concrete ducts, operated with very high temperatures and was therefore not very efficient. There were also problems with reliability and safety due to the hot pressurised steam tubes. Nowadays, this generation is technologically outdated. However, some of these systems are still in use, for example in New York or Paris. Other systems originally built have subsequently been converted to later generations. \n\nThe second generation was developed in the 1930s and was built until the 1970s. It burned coal and oil, the energy was transmitted through pressurised hot water as heat carrier. The systems usually had supply temperatures above 100 °C, used water pipes in concrete ducts, mostly assembled on site, and heavy equipment. A main reason for these systems were the primary energy savings, which arose from using combined heat and power plants. While also used in other countries, typical systems of this generation were the sovjet-style district heating systems that were built after WW2 in several countries in Eastern Europe. \n\nIn the 1970s the third generation was developed and was subsequently used in most of the following systems all over the world. This generation is also called the “Scandinavian district heating technology”, because a lot of the district heating component manufacturers are based in Scandinavia. The third generation uses prefabricated, pre-insulated pipes, which are directly buried into the ground and operates with lower temperatures, usually below 100°C. A primary motivation for building these systems was security of supply by improving the energy efficiency after the two oil crises led to disruption of the oil supply. Therefore those systems usually used coal, biomass and waste as energy sources, while oil was mostly neglected. In some system also geothermal energy and solar energy are used in the energy mix of those systems. For example Paris has been using geothermal heating from a 55-70 °C source 1–2 km below the surface since the 1970s for domestic heating.\n\nCurrently, the 4th generation is being developed, with the transition to 4th generation already in process in Denmark. The 4th generation is designed to combat climate change and integrate high shares of variable renewable energy into the district heating by providing high flexibility to the electricity system.\n\nAccording to the review by Lund et al. those systems have to have the following abilities:\n\nCompared to the previous generations the temperature levels have been reduced to increase the energy efficiency of the system, with supply side temperatures of 70°C and lower. Potential heat sources are waste heat from industry, CHP plants burning waste, biomass power plants, geothermal and solarthermal energy (central solar heating), large scale heat pumps, waste heat from cooling purposes and data centers and other sustainable energy sources. With those energy sources and large scale thermal energy storage, including seasonal thermal energy storage, 4th generation district heating systems are expected to provide flexibility for balancing wind and solar power generation, for example by using heat pumps to integrate surplus power as heat when there is much wind energy or providing electricity by biomass plants when back-up power is needed. Therefore, large scale heat pumps are regarded as a key technology for smart energy systems with high shares of renewable energy up to 100 % and advanced 4th generation district heating systems.\n\nHeat sources in use for various district heating systems include: power plants designed for combined heat and power (CHP, also called co-generation), including both combustion and nuclear power plants; and simple combustion of a fossil fuel or biomass; geothermal heat; solar heat; industrial heat pumps which extract heat from seawater, river or lake water, sewage, or waste heat from industrial processes.\n\nThe core element of many district heating systems is a heat-only boiler station. Additionally a cogeneration plant (also called combined heat and power, CHP) is often added in parallel with the boilers. Both have in common that they are typically based on combustion of primary energy carriers. The difference between the two systems is that, in a cogeneration plant, heat and electricity are generated simultaneously, whereas in heat-only boiler stations – as the name suggests – only heat is generated.\n\nIn the case of a fossil fueled cogeneration plant, the heat output is typically sized to meet half of the peak heat load but over the year will provide 90% of the heat supplied. The boiler capacity will be able to meet the entire heat demand unaided and can cover for breakdowns in the cogeneration plant. It is not economic to size the cogeneration plant alone to be able to meet the full heat load. In the New York City steam system, that is around 2.5 GW. Germany has the largest amount of CHP in Europe.\n\nThe combination of cogeneration and district heating is very energy efficient. A simple thermal power station can be 20–35% efficient, whereas a more advanced facility with the ability to recover waste heat can reach total energy efficiency of nearly 80%. Some may exceed 100% based on the lower heating value by condensing the flue gas as well.\n\nWaste heat from nuclear power plants is sometimes used for district heating. The principles for a conventional combination of cogeneration and district heating applies the same for nuclear as it does for a thermal power station. Russia has several cogeneration nuclear plants which together provided 11.4 PJ of district heat in 2005. Russian nuclear district heating is planned to nearly triple within a decade as new plants are built.\n\nOther nuclear-powered heating from cogeneration plants are in Ukraine, the Czech Republic, Slovakia, Hungary, Bulgaria, and Switzerland, producing up to about 100 MW per power station. One use of nuclear heat generation was with the Ågesta Nuclear Power Plant in Sweden closed in 1974.\n\nIn Switzerland, the Beznau Nuclear Power Plant provides heat to about 20,000 people.\n\nGeothermal district heating was used in Pompeii, and in Chaudes-Aigues since the 14th Century.\n\nUnited States\n\nDirect use geothermal district heating systems, which tap geothermal reservoirs and distribute the hot water to multiple buildings for a variety of uses, are uncommon in the United States, but have existed in America for over a century.\n\nIn 1890, the first wells were drilled to access a hot water resource outside of Boise, Idaho. In 1892, after routing the water to homes and businesses in the area via a wooden pipeline, the first geothermal district heating system was created.\n\nAs of a 2007 study, there were 22 geothermal district heating systems (GDHS) in the United States. As of 2010, two of those systems have shut down. The table below describes the 20 GDHS currently operational in America.\n\nUse of solar heat for district heating has been increasing in Denmark and Germany in recent years. The systems usually include interseasonal thermal energy storage for a consistent heat output day to day and between summer and winter. Good examples are in Vojens at 50 MW, Dronninglund at 27 MW and Marstal at 13 MW in Denmark. These systems have been incrementally expanded to supply 10% to 40% of their villages' annual space heating needs. The solar-thermal panels are ground-mounted in fields. The heat storage is pit storage, borehole cluster and the traditional water tank. In Alberta, Canada the Drake Landing Solar Community has achieved a world record 97% annual solar fraction for heating needs, using solar-thermal panels on the garage roofs and thermal storage in a borehole cluster.\n\nIn Stockholm, the first heat pump was installed in 1977 to deliver district heating sourced from IBM servers. Today the installed capacity is about 660 MW heat, utilizing treated sewage water, sea water, district cooling, data centers and grocery stores as heat sources. Another example is the Drammen Fjernvarme District Heating project in Norway which produces 14 MW from water at just 8 °C, industrial heat pumps are demonstrated heat sources for district heating networks. Among the ways that industrial heat pumps can be utilized are:\n\n\nConcerns have existed about the use of hydroflurocarbons as the working fluid (refrigerant) for large heat pumps. Whilst leakage is not usually measured, it is generally reported to be relatively low, such as 1% (compared to 25% for supermarket cooling systems). A 30-megawatt heatpump could therefore leak (annually) around 75 kg of R134a or other working fluid. Given the high global warming potential of some HFCs, this could equate to over of car travel per year.\n\nHowever, recent technical advances allow the use of natural heat pump refrigerants that have very low global warming potential (GWP). CO2 refrigerant (R744, GWP=1) or ammonia (R717, GWP=0) also have the benefit, depending on operating conditions, of resulting in higher heat pump efficiency than conventional refrigerants. An example is a 14 MW(thermal) district heating network in Drammen, Norway which is supplied by seawater-source heatpumps that use R717 refrigerant, and has been operating since 2011. 90 °C water is delivered to the district loop (and returns at 65 °C). Heat is extracted from seawater (from depth) that is 8 to 9 °C all year round, giving an average coefficient of performance (COP) of about 3.15. In the process the seawater is chilled to 4 °C; however, this resource is not utilized. In a district system where the chilled water could be utilized for air conditioning, the effective COP would be considerably higher.\n\nIn the future, industrial heat pumps will be further de-carbonised by using, on one side, excess renewable electrical energy (otherwise spilled due to meeting of grid demand) from wind, solar, etc. and, on the other side, by making more of renewable heat sources (lake and ocean heat, geothermal, etc.). Furthermore, higher efficiency can be expected through operation on the high voltage network.\n\nWith European countries such as Germany and Denmark moving to very high levels (80% and 100% respectively by 2050) of renewable energy for all energy uses there will be increasing periods of excess production of renewable electrical energy. Storage of this energy as potential electrical energy (e.g. pumped hydro) is very costly and reduces total round-trip efficiency. However, storing it as heat in district heating systems, for use in buildings where there is demand, is significantly less costly. Whilst the quality of the electrical energy is degraded, high voltage grid MW sized heat pumps would maximise efficiency whilst not wasting excess renewable electricity. Such coupling of the electricity sector with the heating sector (Power-to-X) is regarded as a key factor for energy systems with high shares of renewable energy, because it allows storage to be used mainly in form of cheap heat storage. Therefore the usage of rather expensive electricity storage can be minimized, as the heat sector balances the variable production of renewable energy sources with flexible loads and heat storage. Stockholm at present has about 660 MW of heat pumps connected to its district heating system.\n\nHeat pumps can also be used at the consumer-side instead of an substation, especially in so called cold district heating systems. These systems operate at very low temperature (about 10 to 25 °C) and can be used both for heating and cooling. Hot water and space heating is provided by heat pumps, which use the cold district heating system as heat source. The system itself can be fed by various (low-temperature) heat sources including ambient heat, ambient water from rivers, lakes, sea or lagoons, and waste heat from industrial or commercial sources.\n\nIncreasingly large heat stores are being used with district heating networks to maximise efficiency and financial returns. This allows cogeneration units to be run at times of maximum electrical tariff, the electrical production having much higher rates of return than heat production, whilst storing the excess heat production. It also allows solar heat to be collected in summer and redistributed off season in very large but relatively low-cost in-ground insulated reservoirs or borehole systems. The expected heat loss at the 203,000m³ insulated pond in Vojens is about 8%.\n\nAfter generation, the heat is distributed to the customer via a network of insulated pipes. District heating systems consist of feed and return lines. Usually the pipes are installed underground but there are also systems with overground pipes. Within the system heat storage units may be installed to even out peak load demands.\n\nThe common medium used for heat distribution is water or pressurized hot water, but steam is also used. The advantage of steam is that in addition to heating purposes it can be used in industrial processes due to its higher temperature. The disadvantage of steam is a higher heat loss due to the high temperature. Also, the thermal efficiency of cogeneration plants is significantly lower if the cooling medium is high-temperature steam, reducing electric power generation. Heat transfer oils are generally not used for district heating, although they have higher heat capacities than water, as they are expensive and have environmental issues.\n\nAt customer level the heat network is usually connected to the central heating system of the dwellings via heat exchangers (heat substations): the working fluids of both networks (generally water or steam) do not mix. However, direct connection is used in the Odense system.\n\nTypical annual loss of thermal energy through distribution is around 10%, as seen in Norway's district heating network.\n\nThe amount of heat provided to customers is often recorded with a heat meter to encourage conservation and maximize the number of customers which can be served, but such meters are expensive. Due to the expense of heat metering, an alternative approach is simply to meter the water – water meters are much cheaper than heat meters, and have the advantage of encouraging consumers to extract as much heat as possible, leading to a very low return temperature, which increases the efficiency of power generation.\n\nMany systems were installed under a socialist economy (such as in the former Eastern Bloc) which lacked heat metering and means to adjust the heat delivery to each apartment. This led to great inefficiencies – users had to simply open windows when too hot – wasting energy and minimising the numbers of connectable customers.\n\nDistrict heating systems can vary in size. Some systems cover entire cities such as Stockholm or Flensburg, using a network of large 1000 mm diameter primary pipes linked to secondary pipes – 200 mm diameter perhaps, which in turn link to tertiary pipes of perhaps 25 mm diameter which might connect to 10 to 50 houses.\n\nSome district heating schemes might only be sized to meet the needs of a small village or area of a city in which case only the secondary and tertiary pipes will be needed.\n\nSome schemes may be designed to serve only a limited number of dwellings – 20–50 – in which case only tertiary sized pipes are needed.\n\nDistrict heating has various advantages compared to individual heating systems. Usually district heating is more energy efficient, due to simultaneous production of heat and electricity in combined heat and power generation plants. This has the added benefit of reducing carbon emissions. The larger combustion units also have a more advanced flue gas cleaning than single boiler systems. In the case of surplus heat from industries, district heating systems do not use additional fuel because they recover heat which would otherwise be dispersed to the environment.\n\nDistrict heating requires a long-term financial commitment that fits poorly with a focus on short-term returns on investment. Benefits to the community include avoided costs of energy through the use of surplus and wasted heat energy, and reduced investment in individual household or building heating equipment. District heating networks, heat-only boiler stations, and cogeneration plants require high initial capital expenditure and financing. Only if considered as long-term investments will these translate into profitable operations for the owners of district heating systems, or combined heat and power plant operators. District heating is less attractive for areas with low population densities, as the investment per household is considerably higher. Also it is less attractive in areas of many small buildings; e.g. detached houses than in areas with a fewer larger buildings; e.g. blocks of flats, because each connection to a single-family house is quite expensive.\n\nIndividual heating systems can be completely shutdown intermittently according to local heating demand which is not the case with a district heating system.\n\nIn many cases large combined heat and power district heating schemes are owned by a single entity. This was typically the case in the old Eastern bloc countries. However, for many schemes, the ownership of the cogeneration plant is separate from the heat using part.\n\nExamples are Warsaw which has such split ownership with PGNiG Termika owning the cogeneration unit, the Dalkia Polska owning 85% of the heat distribution, the rest of the heat distribution is owned by municipality and workers. Similarly all the large CHP/CH schemes in Denmark are of split ownership.\n\nSweden provides an alternative example where the heating market is deregulated. In Sweden it is most common that the ownership of the district heating network is not separated from the ownership of the cogeneration plants, the district cooling network or the centralized heat pumps. There are also examples where the competition has spawned parallel networks and interconnected networks where multiple utilities cooperate.\n\nIn the United Kingdom there have been complaints that district heating companies have too much of a monopoly and are insufficiently regulated, an issue the industry is aware of, and has taken steps to improve consumer experience through the use of customer charters as set out by the heat trust. Some customers are taking legal action against the supplier for Misrepresentation & Unfair Trading, claiming district Heating is delivering the savings promised by many heat suppliers.\n\nSince conditions from city to city differ, every district heating system is unique. In addition, nations have different access to primary energy carriers and so they have a different approach on how to address heating markets within their borders.\n\nSince 1954, district heating has been promoted in Europe by Euroheat & Power. They have compiled an analysis of district heating and cooling markets in Europe within their Ecoheatcool project supported by the European Commission. A separate study, entitled Heat Roadmap Europe, has indicated that district heating can reduce the price of energy in the European Union between now and 2050. The legal framework in the member states of the European Union is currently influenced by the EU's CHP Directive.\n\nThe EU has actively incorporated cogeneration into its energy policy via the CHP Directive. In September 2008 at a hearing of the European Parliament's Urban Lodgment Intergroup, Energy Commissioner Andris Piebalgs is quoted as saying, \"security of supply really starts with energy efficiency.\" Energy efficiency and cogeneration are recognized in the opening paragraphs of the European Union's Cogeneration Directive 2004/08/EC. This directive intends to support cogeneration and establish a method for calculating cogeneration abilities per country. The development of cogeneration has been very uneven over the years and has been dominated throughout the last decades by national circumstances.\n\nAs a whole, the European Union currently generates 11% of its electricity using cogeneration, saving Europe an estimated 35 Mtoe per annum. However, there are large differences between the member states, with energy savings ranging from 2% to 60%. Europe has the three countries with the world's most intensive cogeneration economies: Denmark, the Netherlands and Finland.\n\nOther European countries are also making great efforts to increase their efficiency. Germany reports that over 50% of the country's total electricity demand could be provided through cogeneration. Germany set a target to double its electricity cogeneration from 12.5% of the country's electricity to 25% by 2020 and has passed supporting legislation accordingly in \"Federal Ministry of Economics and Technology\", (BMWi), Germany, August 2007. The UK is also actively supporting district heating. In the light of UK's goal to achieve an 80% reduction in carbon dioxide emissions by 2050, the government had set a target to source at least 15% of government electricity from CHP by 2010. Other UK measures to encourage CHP growth are financial incentives, grant support, a greater regulatory framework, and government leadership and partnership.\n\nAccording to the IEA 2008 modelling of cogeneration expansion for the G8 countries, expansion of cogeneration in France, Germany, Italy and the UK alone would effectively double the existing primary fuel savings by 2030. This would increase Europe's savings from today's 155 TWh to 465 TWh in 2030. It would also result in a 16% to 29% increase in each country's total cogenerated electricity by 2030.\n\nGovernments are being assisted in their CHP endeavors by organizations like COGEN Europe who serve as an information hub for the most recent updates within Europe's energy policy. COGEN is Europe's umbrella organization representing the interests of the cogeneration industry, users of the technology and promoting its benefits in the EU and the wider Europe. The association is backed by the key players in the industry including gas and electricity companies, ESCOs, equipment suppliers, consultancies, national promotion organisations, financial and other service companies.\n\nA 2016 EU energy strategy suggests increased use of district heating.\n\nThe largest district heating system in Austria is in Vienna (Fernwärme Wien) – with many smaller systems distributed over the whole country.\n\nDistrict heating in Vienna is run by Wien Energie. In the business year of 2004/2005 a total of 5.163 GWh was sold, 1.602 GWh to 251.224 private apartments and houses and 3.561 GWh to 5211 major customers. The three large municipal waste incinerators provide 22% of the total in producing 116 GWh electric power and 1.220 GWh heat. Waste heat from municipal power plants and large industrial plants account for 72% of the total. The remaining 6% is produced by peak heating boilers from fossil fuel. A biomass-fired power plant has produced heat since 2006.\n\nIn the rest of Austria the newer district heating plants are constructed as biomass plants or as CHP-biomass plants like the biomass district heating of Mödling or the biomass district heating of Baden.\n\nMost of the older fossil-fired district heating systems have a district heating accumulator, so that it is possible to produce the thermal district heating power only at that time where the electric power price is high.\n\nBulgaria has district heating in around a dozen towns and cities. The largest system is in the capital Sofia, where there are four power plants (two CHPs and two boiler stations) providing heat to the majority of the city. The system dates back to 1949.\n\nThe largest district heating system in the Czech Republic is in Prague owned and operated by Pražská teplárenská, serving 265,000 households and selling c. 13 PJ of heat annually. Most of the heat is actually produced as waste heat in 30 km distant thermal power station in Mělník. There are many smaller central heating systems spread around the country including waste heat usage, municipal waste incineration and .\n\nIn Denmark district heating covers more than 64% of space heating and water heating. In 2007, 80.5% of this heat was produced by combined heat and power plants. Heat recovered from waste incineration accounted for 20.4% of the total Danish district heat production. In 2013, Denmark imported 158,000 ton waste for incineration. Most major cities in Denmark have big district heating networks, including transmission networks operating with up to 125 °C and 25 bar pressure and distribution networks operating with up to 95 °C and between 6 and 10 bar pressure. The largest district heating system in Denmark is in the Copenhagen area operated by CTR I/S and VEKS I/S. In central Copenhagen, the CTR network serves 275,000 households (90-95% of the area's population) through a network of 54 km double district heating distribution pipes providing a peak capacity of 663 MW. The consumer price of heat from CTR is approximately €49 per MWh plus taxes (2009).\n\nThe Danish island of Samsø has three straw-fueled plants producing district heating.\n\nIn Finland district heating accounts for about 50% of the total heating market, 80% of which is produced by combined heat and power plants. Over 90% of apartment blocks, more than half of all terraced houses, and the bulk of public buildings and business premises are connected to a district heating network. Natural gas is mostly used in the south-east gas pipeline network, imported coal is used in areas close to ports, and peat is used in northern areas where peat is a natural resource. Other renewables, such as wood chips and other paper industry combustible by-products, are also used, as is the energy recovered by the incineration of municipal solid waste. Industrial units which generate heat as an industrial by-product may sell otherwise waste heat to the network rather than release it into the environment. Excess heat and power from pulp mill recovery boilers is a significant source in mill towns. In some towns waste incineration can contribute as much as 8% of the district heating heat requirement. Availability is 99.98% and disruptions, when they do occur, usually reduce temperatures by only a few degrees.\n\nIn Helsinki, an underground datacenter next to the President's palace releases excess heat into neighboring homes, producing enough heat to heat approximately 500 large houses.\n\nIn Germany district heating has a market share of around 14% in the residential buildings sector. The connected heat load is around 52,729 MW. The heat comes mainly from cogeneration plants (83%). Heat-only boilers supply 16% and 1% is surplus heat from industry. The cogeneration plants use natural gas (42%), coal (39%), lignite (12%) and waste/others (7%) as fuel.\n\nThe largest district heating network is located in Berlin whereas the highest diffusion of district heating occurs in Flensburg with around 90% market share. In Munich about 70% of the electricity produced comes from district heating plants.\n\nDistrict heating has rather little legal framework in Germany. There is no law on it as most elements of district heating are regulated in governmental or regional orders. There is no governmental support for district heating networks but a law to support cogeneration plants. As in the European Union the CHP Directive will come effective, this law probably needs some adjustment.\n\nGreece has district heating mainly in the Province of Western Macedonia, Central Macedonia and the Peloponnese Province. The largest system is the city of Ptolemaida, where there are five power plants (thermal power stations or TPS in particular) providing heat to the majority of the largest towns and cities of the area and some villages. The first small installation took place in Ptolemaida in 1960, offering heating to Proastio village of Eordaea using the TPS of Ptolemaida. Today District heating installations are also available in Kozani, Ptolemaida, Amyntaio, Philotas, Serres and Megalopolis using nearby power plants. In Serres the power plant is a Hi-Efficiency CHP Plant using natural gas, while coal is the primary fuel for all other district heating networks.\nAccording to the 2011 census there were 607,578 dwellings (15.5% of all) in Hungary with district heating, mostly panel flats in urban areas. The largest district heating system located in Budapest, the municipality-owned \"Főtáv Zrt.\" (\"Metropolitan Teleheating Company\") provides heat and piped hot water for 238,000 households and 7,000 companies.\n\nWith 95% of all housing (mostly in the capital of Reykjavík) enjoying district heating services – mainly from geothermal energy, Iceland is the country with the highest penetration of district heating.\n\nMost of Iceland's district heating comes from three geothermal power plants, producing over 800 MWth:\n\n\nThe Dublin Waste-to-Energy Plant will provide district heating for up to 50,000 homes in Poolbeg and surrounding areas. \nTralee in Co Kerry has a 1 MW district heating system providing heat to an apartment complex, sheltered housing for the elderly, a library and over 100 individual houses. The system is fuelled by locally produced wood chip.<br>\nIn Glenstal Abbey in Co Limerick there exists a pond-based 150 kW heating system for a school.\n\nIn Italy, district heating is used in some cities (Bergamo, Brescia, Cremona, Bolzano, Ferrara, Imola, Reggio Emilia, Terlan, Turin, Lodi, and now Milan). The district heating of Turin is the biggest of the country and it supplies 550.000 people (62% of the whole city population).\n\nDistrict heating is used in Rotterdam and Amsterdam, with more expected as the government has mandated a transition away from natural gas for all homes in the country by 2050.\n\nIn Norway district heating only constitutes approximately 2% of energy needs for heating. This is a very low number compared to similar countries. One of the main reasons district heating has a low penetration in Norway is access to cheap hydro-based electricity, and 80% of private electricity consumption goes to heat rooms and water. However, there is district heating in the major cities.\n\nIn 2009, 40% of Polish households used district heating, most of them in urban areas. Heat is provided primarily by combined heat and power plants, most of which burn hard coal. The largest district heating system is in Warsaw, owned and operated by Veolia Warszawa, distributing approx. 34 PJ annually.\n\nThe largest district heating system in Romania is in Bucharest. Owned and operated by RADET, it distributes approximately 24 PJ annually, serving 570 000 households. This corresponds to 68% of Bucharest's total domestic heat requirements (RADET fulfills another 4% through single-building boiler systems, for a total of 72%).\n\nIn most Russian cities, district-level combined heat and power plants () produce more than 50% of the nation's electricity and simultaneously provide hot water for neighbouring city blocks. They mostly use coal and oil-powered steam turbines for cogeneration of heat. Now, gas turbines and combined cycle designs are beginning to be widely used as well.\n\nIn Serbia, district heating is used throughout the main cities, particularly in the capital, Belgrade. The first district heating plant was built in 1961 as a means to provide effective heating to the newly built suburbs of Novi Beograd. Since then, numerous plants have been built to heat the ever-growing city. They use natural gas as fuel, because it has less of an effect on the environment. The district heating system of Belgrade possesses 112 heat sources of 2,454 MW capacity, over 500 km of pipeline, and 4365 connection stations, providing district heating to 240,000 apartments and 7,500 office/commercial buildings of total floor area exceeding 17,000,000 square meters.\n\nSweden has a long tradition for using district heating in urban areas. In 2015, about 60% of Sweden's houses (private and commercial) were heated by district heating, according to the Swedish association of district heating.\nThe city of Växjö reduced its fossil fuel consumption by 30% between 1993 and 2006, and aimed for a 50% reduction by 2010. This was to be achieved largely by way of biomass fired teleheating. Another example is the plant of Enköping, combining the use of short rotation plantations both for fuel as well as for phytoremediation.\n\n47% of the heat generated in Swedish teleheating systems are produced with renewable bioenergy sources, as well as 16% in waste-to-energy plants, 7% is provided by heat pumps, 10% by flue-gas condensation and 6% by industrial waste heat recovery. The remaining are mostly fossil fuels: oil (3%), natural gas (3%), peat (2%), and coal (1%).\n\nBecause of the law banning traditional landfills, waste is commonly used as a fuel.\n\nIn the United Kingdom, district heating became popular after World War II, but on a restricted scale, to heat the large residential estates that replaced areas devastated by the Blitz. In 2013 there were 1,765 district heating schemes with 920 based in London alone. In total around 210,000 homes and 1,700 businesses are supplied by heat networks in the UK.\n\nThe Pimlico District Heating Undertaking (PDHU) first became operational in 1950 and continues to expand to this day. The PDHU once relied on waste heat from the now-disused Battersea Power Station on the South side of the River Thames. It is still in operation, the water now being heated locally by a new energy centre which incorporates 3.1 MWe / 4.0 MWth of gas fired CHP engines and 3 × 8 MW gas-fired boilers.\n\nOne of the United Kingdom's largest district heating schemes is EnviroEnergy in Nottingham. The plant initially built by Boots is now used to heat 4,600 homes, and a wide variety of business premises, including the Concert Hall, the Nottingham Arena, the Victoria Baths, the Broadmarsh Shopping Centre, the Victoria Centre, and others. The heat source is a waste-to-energy incinerator. Scotland has several district heating systems with the first in the UK being installed at Aviemore and others following at Lochgilphead, Fort William and Forfar.\n\nSheffield's district heating network was established in 1988 and is still expanding today. It saves an equivalent 21,000 plus tonnes of CO2 each year when compared to conventional sources of energy – electricity from the national grid and heat generated by individual boilers. There are currently over 140 buildings connected to the district heating network. These include city landmarks such as the Sheffield City Hall, the Lyceum Theatre, Sheffield University, Sheffield Hallam University, hospitals, shops, offices and leisure facilities plus 2,800 homes. More than 44 km of underground pipes deliver energy which is generated at Sheffield Energy Recovery Facility. This converts 225,000 tonnes of waste into energy, producing up to 60 MWe of thermal energy and up to 19 MWe of electrical energy.\n\nThe Southampton District Energy Scheme was originally built to use just geothermal energy, but now also uses the heat from a gas fired CHP generator. It supplies heating and district cooling to many large premises in the city, including the WestQuay shopping centre, the De Vere Grand Harbour hotel, the Royal South Hants Hospital, and several housing schemes. In the 1980s Southampton began utilising combined heat and power district heating, taking advantage of geothermal heat \"trapped\" in the area. The geothermal heat provided by the well works in conjunction with the Combined Heat and Power scheme. Geothermal energy provides 15-20%, fuel oil 10%, and natural gas 70% of the total heat input for this scheme and the combined heat and power generators use conventional fuels to make electricity. \"Waste heat\" from this process is recovered for distribution through the 11 km mains network.\n\nLerwick District Heating Scheme is of note because it is one of the few schemes where a completely new system was added to a previously existing small town.\n\nADE has an online map of district heating installations in the UK. ADE estimates that 54 percent of energy used to produce electricity is being wasted via conventional power production, which relates to £9.5 billion ($US12.5 billion) per year.\n\nThe largest district heating system in Spain is located in Soria. It is called \"Ciudad del Medio Ambiente\" (Environmental Town) and will receive 41 MW from a biomass power plant.\n\nIn North America, district heating systems fall into two general categories. Those that are owned by and serve the buildings of a single entity are considered institutional systems. All others fall into the commercial category.\n\nDistrict Heating is becoming a growing industry in Canadian cities, with many new systems being built in the last ten years. Some of the major systems in Canada include:\n\nMany Canadian universities operate central campus heating plants.\n\nThe Holly Steam Combination Company was the first steam heating company to commercially distribute district heating from a central steam heating system. \nThe city of Milwaukee, Wisconsin has been using district heating for its central business district since the Valley Power Plant commenced operations in 1968. Amazingly, the air quality in the immediate vicinity of the plant, based on the sensor located on César Chavez Drive, qualifies as the best in Southeastern Wisconsin, at least with regard to ozone concentration. The recent (2012) conversion of the plant, which changed the fuel input from coal to natural gas, is expected to further improve air quality at both the local César Chavez sensor as well as Antarctic sensors . Interesting to note about Wisconsin power plants is their dual use as breeding grounds for peregrines .\n\n\n\nDistrict heating is also used on many college campuses, often in combination with district cooling and electricity generation. Colleges using district heating include the University of Texas at Austin; Rice University; Brigham Young University; Georgetown University; Cornell University, which also employs deep water source cooling using the waters of nearby Cayuga Lake; Purdue University; University of Notre Dame; Michigan State University; Eastern Michigan University; Case Western Reserve University; Iowa State University; University of Delaware; University of Maryland, College Park , University of Wisconsin–Madison, and several campuses of the University of California. MIT installed a cogeneration system in 1995 that provides electricity, heating and cooling to 80% of its campus buildings. The University of New Hampshire has a cogeneration plant run on methane from an adjacent landfill, providing the University with 100% of its heat and power needs without burning oil or natural gas.\nNorth Dakota State University (NDSU) in Fargo, North Dakota has used district heating for over a century from their coal-fired heating plant.\n\n87 district heating enterprises are operating in Japan, serving 148 districts.\n\nMany companies operate district cogeneration facilities that provide steam and/or hot water to many of the office buildings. Also, most operators in the Greater Tokyo serve district cooling.\n\nIn southern China, there are nearly no district heating systems. In northern China, district heating systems are common. Most district heating system which are just for heating instead of CHP use hard coal. For air pollution in China has become quite serious, many cities gradually are now using natural gas rather than coal in district heating system. There is also some amount of geothermal heating and sea heat pump systems.\n\nPenetration of district heating (DH) into the heat market varies by country. Penetration is influenced by different factors, including environmental conditions, availability of heat sources, economics, and economic and legal framework.\n\nIn the year 2000 the percentage of houses supplied by district heat in some European countries was as follows:\n\nIn Iceland the prevailing positive influence on DH is availability of easily captured geothermal heat. In most Eastern European countries, energy planning included development of cogeneration and district heating. Negative influence in the Netherlands and UK can be attributed partially to milder climate, along with competition from natural gas. The tax on domestic gas prices in the UK is a third of that in France and a fifth of that in Germany.\n\n\n"}
{"id": "1872297", "url": "https://en.wikipedia.org/wiki?curid=1872297", "title": "Dust bunny", "text": "Dust bunny\n\nDust bunnies (or dustbunnies) are small clumps of dust that form under furniture and in corners that are not cleaned regularly. They are made of hair, lint, dead skin, spider webs, dust, and sometimes light rubbish and debris, and are held together by static electricity and felt-like entanglement. They can house dust mites or other parasites, and can lower the efficiency of dust filters by clogging. The movement of a single large particle can start the formation of a dust bunny.\n\nDust bunnies are harmful to electronics, as they can obstruct air flow through heat sinks, raising temperatures significantly, and therefore shortening the life of electronic components.\n\nA trademark for \"Dustbunny\" was registered in 2006 for the \"Dustbunny Cleaner\", a robotic ball with an electrostatic sleeve that rolls around under furniture to collect dust bunnies and other \n\nDust bunnies have been used as an analogy for the accretion of cosmic matter in planetoids.\n\n\n"}
{"id": "146689", "url": "https://en.wikipedia.org/wiki?curid=146689", "title": "Earth radius", "text": "Earth radius\n\nEarth radius is the distance from a selected center of Earth to a point on its surface, which is often chosen to be sea level, or more commonly, the surface of an idealized ellipsoid representing the shape of Earth. Because Earth is not a perfect sphere, the determination of Earth's radius can have several values, depending on how it is measured; from its equatorial radius of about to its polar radius of about . \n\nWhen only one radius is stated, the International Astronomical Union (IAU) prefers that it be Earth's equatorial radius. \n\nThe International Union of Geodesy and Geophysics (IUGG) gives three global average radii, the arithmetic mean of the radii of the ellipsoid (R), the radius of a sphere with the same surface area as the ellipsoid or authalic radius (R), and the radius of a sphere with the same volume as the ellipsoid (R). All three IUGG average radii are about . A fourth global average radius not mentioned by the IUGG is the rectifying radius, the radius of a sphere with a circumference equal to the perimeter of the polar cross section of the ellipsoid, about . The radius of curvature at any point on the surface of the ellipsoid depends on its coordinates and its azimuth, north-south (meridional), east-west (prime vertical), or somewhere in between. \n\nEarth's rotation, internal density variations, and external tidal forces cause its shape to deviate systematically from a perfect sphere. Local topography increases the variance, resulting in a surface of profound complexity. Our descriptions of Earth's surface must be simpler than reality in order to be tractable. Hence, we create models to approximate characteristics of Earth's surface, generally relying on the simplest model that suits the need.\n\nEach of the models in common use involve some notion of the geometric radius. Strictly speaking, spheres are the only solids to have radii, but broader uses of the term \"radius\" are common in many fields, including those dealing with models of Earth. The following is a partial list of models of Earth's surface, ordered from exact to more approximate:\n\nIn the case of the geoid and ellipsoids, the fixed distance from any point on the model to the specified center is called \"a radius of the Earth\" or \"the radius of the Earth at that point\". It is also common to refer to any \"mean radius\" of a spherical model as \"the radius of the earth\". When considering the Earth's real surface, on the other hand, it is uncommon to refer to a \"radius\", since there is generally no practical need. Rather, elevation above or below sea level is useful.\n\nRegardless of the model, any radius falls between the polar minimum of about 6,357 km and the equatorial maximum of about 6,378 km (3,950 to 3,963 mi). Hence, the Earth deviates from a perfect sphere by only a third of a percent, which supports the spherical model in many contexts and justifies the term \"radius of the Earth\". While specific values differ, the concepts in this article generalize to any major planet.\n\nRotation of a planet causes it to approximate an \"oblate ellipsoid/spheroid\" with a bulge at the equator and flattening at the North and South Poles, so that the \"equatorial radius\" is larger than the \"polar radius\" by approximately . The \"oblateness constant\" is given by\nwhere is the angular frequency, is the gravitational constant, and is the mass of the planet. For the Earth , which is close to the measured inverse flattening . Additionally, the bulge at the equator shows slow variations. The bulge had been decreasing, but since 1998 the bulge has increased, possibly due to redistribution of ocean mass via currents.\n\nThe variation in density and crustal thickness causes gravity to vary across the surface and in time, so that the mean sea level differs from the ellipsoid. This difference is the \"geoid height\", positive above or outside the ellipsoid, negative below or inside. The geoid height variation is under on Earth. The geoid height can change abruptly due to earthquakes (such as the Sumatra-Andaman earthquake) or reduction in ice masses (such as Greenland).\n\nNot all deformations originate within the Earth. The gravity of the Moon and Sun cause the Earth's surface at a given point to undulate by tenths of meters over a nearly 12-hour period (see Earth tide).\n\nGiven local and transient influences on surface height, the values defined below are based on a \"general purpose\" model, refined as globally precisely as possible within of reference ellipsoid height, and to within of mean sea level (neglecting geoid height).\n\nAdditionally, the radius can be estimated from the curvature of the Earth at a point. Like a torus, the curvature at a point will be greatest (tightest) in one direction (north–south on Earth) and smallest (flattest) perpendicularly (east–west). The corresponding radius of curvature depends on the location and direction of measurement from that point. A consequence is that a distance to the true horizon at the equator is slightly shorter in the north/south direction than in the east-west direction.\n\nIn summary, local variations in terrain prevent defining a single \"precise\" radius. One can only adopt an idealized model. Since the estimate by Eratosthenes, many models have been created. Historically, these models were based on regional topography, giving the best reference ellipsoid for the area under survey. As satellite remote sensing and especially the Global Positioning System gained importance, true global models were developed which, while not as accurate for regional work, best approximate the Earth as a whole.\n\nThe following radii are fixed and do not include a variable location dependence. They are derived from the World Geodetic System 1984 (WGS-84) standard ellipsoid.\n\nThe value for the equatorial radius is defined to the nearest 0.1 m in WGS-84. The value for the polar radius in this section has been rounded to the nearest 0.1 m, which is expected to be adequate for most uses. Refer to the WGS-84 ellipsoid if a more precise value for its polar radius is needed.\n\nThe radii in this section are for an idealized surface. Even the idealized radii have an uncertainty of ±2 m. The discrepancy between the ellipsoid radius and the radius to a physical location may be significant. When identifying the position of an observable location, the use of more precise values for WGS-84 radii may not yield a corresponding improvement in accuracy.\n\nThe symbol given for the named radius is used in the formulae found in this article.\n\nThe Earth's equatorial radius , or semi-major axis, is the distance from its center to the equator and equals . The equatorial radius is often used to compare Earth with other planets.\n\nThe Earth's polar radius , or semi-minor axis, is the distance from its center to the North and South Poles, and equals .\n\nThe distance from the Earth's center to a point on the spheroid surface at geodetic latitude is:\n\nwhere and are, respectively, the equatorial radius and the polar radius.\n\n\nThere are two principal radii of curvature: along the meridional and prime-vertical normal sections.\n\nIn particular, the Earth's \"radius of curvature in the (north–south) meridian\" at is:\nThis is the radius that Eratosthenes measured.\n\nIf one point had appeared due east of the other, one finds the approximate curvature in the east–west direction.\n\nThis \"radius of curvature in the prime vertical\" which is perpendicular (normal or orthogonal) to at geodetic latitude is:\nThis radius is also called the transverse radius of curvature. At the equator, .\n\nThe Earth's meridional radius of curvature at the equator equals the meridian's semi-latus rectum:\n\nThe Earth's polar radius of curvature is:\n\nThe Earth's radius of curvature along a course at an azimuth (measured clockwise from north) at is derived from Euler's curvature formula as follows:\n\nIt is possible to combine the principal radii of curvature above in a non-directional manner.\n\nThe Earth's Gaussian radius of curvature at latitude is:\n\nThe Earth's mean radius of curvature at latitude is:\n\nThe Earth can be modeled as a sphere in many ways. This section describes the common ways. The various radii derived here use the notation and dimensions noted above for the Earth as derived from the WGS-84 ellipsoid; namely,\n\nA sphere being a gross approximation of the spheroid, which itself is an approximation of the geoid, units are given here in kilometers rather than the millimeter resolution appropriate for geodesy.\n\nIn geophysics, the International Union of Geodesy and Geophysics (IUGG) defines the mean radius (denoted ) to be\nFor Earth, the mean radius is .\n\nIn astronomy, the International Astronomical Union denotes the \"nominal equatorial Earth radius\" as formula_9, which is defined to be . The \"nominal polar Earth radius\" is defined as formula_10 = . These values correspond to the zero tide radii. Equatorial radius is conventionally used as the nominal value unless the polar radius is explicitly required.\n\nEarth's authalic (\"equal area\") radius is the radius of a hypothetical perfect sphere that has the same surface area as the reference ellipsoid. The IUGG denotes the authalic radius as .\n\nA closed-form solution exists for a spheroid:\nwhere and is the surface area of the spheroid.\n\nFor the Earth, the authalic radius is .\n\nAnother spherical model is defined by the volumetric radius, which is the radius of a sphere of volume equal to the ellipsoid. The IUGG denotes the volumetric radius as .\nFor Earth, the volumetric radius equals .\n\nAnother mean radius is the \"rectifying radius\", giving a sphere with circumference equal to the perimeter of the ellipse described by any polar cross section of the ellipsoid. This requires an elliptic integral to find, given the polar and equatorial radii:\n\nFor integration limits of [0,], the integrals for rectifying radius and mean radius evaluate to the same result, which, for Earth, amounts to .\n\nThe meridional mean is well approximated by the semicubic mean of the two axes,\n\nwhich differs from the exact result by less than ; the mean of the two axes,\n\nabout , can also be used.\n\nMost global mean radii are based on the reference ellipsoid, which approximates the geoid. The geoid has no direct relationship with surface topography, however. An alternative calculation averages elevations everywhere, resulting in a mean radius larger than the IUGG mean radius, the authalic radius, or the volumetric radius. This average is with uncertainty of .\n\nThe best local spherical approximation to the ellipsoid in the vicinity of a given point is the osculating sphere. Its radius equals the Gaussian radius of curvature as above, and its radial direction coincides with the ellipsoid normal direction. The center of the osculating sphere is offset from the center of the ellipsoid, but is at the center of curvature for the given point on the ellipsoid surface. This concept aids the interpretation of terrestrial and planetary radio occultation refraction measurements and in some navigation and surveillance applications.\n\nThis table summarizes the accepted values of the Earth's radius.\n\nThe first published reference to the Earth's size appeared around 350 BC, when Aristotle reported in his book \"On the Heavens\" that mathematicians had guessed the circumference of the Earth to be 400,000 stadia. Scholars have interpreted Aristotle's figure to be anywhere from highly accurate to almost double the true value. The first known scientific measurement and calculation of the circumference of the Earth was performed by Eratosthenes in about 240 BC. Estimates of the accuracy of Eratosthenes’s measurement range from 0.5% to 17%. For both Aristotle and Eratosthenes, uncertainty in the accuracy of their estimates is due to modern uncertainty over which stadion length they meant.\n\n"}
{"id": "2324294", "url": "https://en.wikipedia.org/wiki?curid=2324294", "title": "Eddoe", "text": "Eddoe\n\nEddoe or eddo is a tropical vegetable often considered identifiable as the species \"Colocasia antiquorum\", closely related to taro (dasheen, \"Colocasia esculenta\"), which is primarily used for its thickened stems (corms). It has smaller corms than taro, and in most cultivars there is an acrid taste that requires careful cooking. The young leaves can also be cooked and eaten, but (unlike taro) they have a somewhat acrid taste.\n\nEddoes appear to have been developed as a crop in China and Japan and introduced from there to the West Indies where they are sometimes called \"Chinese eddoes\". They grow best in rich loam soil with good drainage, but they can be grown in poorer soil, in drier climates, and in cooler temperatures than taro.\n\nEddoes are also called malangas in Spanish-speaking areas, but that name is also used for other plants of the Araceae family, including tannia (\"Xanthosoma\" spp.).\n\nEddoes make part of the generic classification cará or inhame of the Portuguese language which, beside taro, also includes root vegetables of the genera \"Alocasia\" and \"Dioscorea\". They are the most commonly eaten in the states of São Paulo, Rio de Janeiro and Espírito Santo, as well as surrounding regions of all. They are also fairly common in Northeastern Brazil, where they might be called (literally \"potato\"), but less so than true yams of the genus Colocasia. According to Brazilian folk knowledge, the eddoes most appropriate to be cooked are those that are more deeply pink, or at least pinkish lavender, in the area where the leaves were cut.\n\nThe 1889 book \"The Useful Native Plants of Australia\" records that Colocasia antiquorum: \n\nLinnaeus originally described two species which are now known as \"Colocasia esculenta\" and \"Colocasia antiquorum\" of the cultivated plants that are known by many names including eddoes, dasheen, taro, but many later botanists consider them all to be members of a single, very variable species, the correct name for which is \"Colocasia esculenta\".\n"}
{"id": "40680098", "url": "https://en.wikipedia.org/wiki?curid=40680098", "title": "Energy policy of Turkey", "text": "Energy policy of Turkey\n\nThe energy policy of Turkey is to secure national energy supply and meet forecast increased demand by using energy efficiently and limiting imports: energy costs are a large part of Turkey’s import bill, especially gas from Russia and oil. However the policy has been criticized for not looking much beyond 2023, not sufficiently involving the private sector, and for being inconsistent with Turkey’s climate policy. \n\nTurkey meets a quarter of its energy demand from national resources: much of the rest is supplied by imports of oil and gas from Russia, Iraq and Iran; which is a geopolitical vulnerability and the more than $50 billion annually accounts for three quarters of the current account deficit ,which is probably the biggest structural vulnerability of the country’s economy; and contributed to the debt problems in 2018.\n\nTo secure energy supply the government is building new gas pipelines and diversifying energy sources. The government aims at meeting the forecast increase in demand for electricity by building nuclear power plants and more solar, wind, hydro and coal-fired power plants.\n\nIn the long term a carbon tax would reduce import dependency by speeding development of national solar and wind energy. page 9\n\nIn the 21st century fossil fuel subsidies are around 0.2% of GDP. The energy minister Fatih Dönmez suppports coal and most energy subsidies are to coal.\nHowever diesel is taxed at 200TL/tonne CO2 and petrol at over 300TL/tonne CO2.\n\nThe purpose of the capacity market for electricity is claimed to be to secure supply: however despite almost all natural gas being imported some gas-fired power plants received capacity payments in 2018 whereas non-fossil firm power such as demand response and Turkey’s many hydro power plants cannot.\nState-owned BOTAŞ controls 80% of the natural gas market and the household price of natural gas was the cheapest in Europe in 2017\nTurkey’s long-term contracts with all its current suppliers –Russia, Azerbaijan and Iran– are due to expire in the 2020s.\nExploration for gas in the Eastern Mediterranean is subsidized. and is a cause of geopolitical tension due to the Cyprus dispute.\n\nCoal in Turkey is heavily subsidized.\n\n\"With all of its domestic coal reserves tapped, Turkey plans to reach a total installed capacity of 120,000 MWs in 2023.\" \n\nHowever \"this focus on coal is a huge burden for the sustainability of Turkey’s energy systems.\"\nEnvironmentalists claim \"the environmental impacts of coal power plants are inadequately assessed, while Turkey's viable, clean alternatives to coal are neither being analysed or discussed seriously by senior policy- and decision-makers.\" \n\nThere have been protests against coal power plants. \n\nEven in cities where natural gas is available the government supports poor households with free coal.\n\"Current shortcomings in Turkey's regulatory setup and human resources have to be addressed for ensuring a safe and secure transition to nuclear energy.\"\n\nThe government appears to be considering further pumped-storage possibly pump-back.\n\nAlthough the coal industry and the government are said to have a close relationship, economic downturn and the falling cost of wind and solar may increase pressure on coal subsidies. Hydroelectric plants, especially new ones, are sometimes controversial in local, international and environmental politics.\n\nThere is an Energy Efficiency Law.\n\nIn 2018 a new regulator was set up and $0.15 per kWh of generated electricity will be set aside for waste management.\n\nAmending regulations on rooftop solar panels has been suggested to simplify installation on existing buildings and mandate for new buildings.\n\nState energy companies include: Eti Mine, Turkish Coal Enterprises, Turkish Hard Coal Enterprises, the Electricity Generation Company, BOTAŞ and TEİAŞ - the electricity trading and transmission company. The government holds a quarter of total installed electricity supply and often offers prices below market levels.\n\nAlthough Turkish air pollution limits are above the current EU Industrial Emissions Directive as of June 2019 large power plants are required to keep to limits in the previous EU standard, the Large Combustion Plant Directive.\n\nRetrofitting equipment for pollution control such as flue-gas desulfurization at old plants such as Soma\nmight not be financially possible, as Turkey!s lignite plants use outdated technology that pollute far beyond the limits of the IED. Data on SO, NO and particulate air pollution from each large plant is collected by government but not published.\n\nTurkey's greenhouse gas inventory is incomplete. The energy policy aim of reducing imports (e.g. of gas) conflicts with the climate change policy aim of reducing emission of greenhouse gases as some local resources (e.g. lignite) emit a lot of CO2. In particular considering an emissions trading scheme \"any carbon pricing instrument shall be structured in a way that shall not be considered as a barrier for utilization of domestic fossil fuel based sources for electricity generation until decreasing the share of export fuel to a reasonable level, to be determined by policy makers.\" \n\n\n"}
{"id": "182839", "url": "https://en.wikipedia.org/wiki?curid=182839", "title": "Filter paper", "text": "Filter paper\n\nFilter paper is a semi-permeable paper barrier placed perpendicular to a liquid or air flow. It is used to separate fine substances from liquids or air. It is used in science labs to remove solids from liquids. This can be used to remove sand from water.\n\nFilter paper has various properties. The important parameters are wet strength, porosity, particle retention, volumetric flow rate, compatibility, efficiency and capacity.\n\nThere are two mechanisms of filtration with paper; volume and surface. By volume filtration the particles are caught in the bulk of the filter paper. By surface filtration the particles are caught on the paper surface. Filter paper is mostly used because even a small piece of filter paper will absorb a significant volume of liquid\n\nThe raw materials are different paper pulps. The pulp may be from softwood, hardwood, fiber crops, mineral fibers. For high quality filters, dissolving pulp and mercerised pulp are used. Most filter papers are made on small paper machines. For laboratory filters the machines may be as small as 50 cm width. The paper is often crêped to improve porosity. The filter papers may also be treated with reagents or impregnation to get the right properties.\n\nThe main application for air filters are combustion air to engines. The filter papers are transformed into filter cartridges, which then is fitted to a holder. The construction of the cartridges mostly requires that the paper is stiff enough to be self-supporting. A paper for air filters needs to be very porous and have a weight of 100 - 200 g/m. Normally particularly long fibrous pulp that is mercerised is used to get these properties. The paper is normally impregnated to improve the resistance to moisture. Some heavy duty qualities are made to be rinsed and thereby extend the life of the filter.\n\nCoffee filters of paper are made from about 100 g/m crêped paper. The crêping allows the coffee to flow freely between the filter and the filtration funnel. The raw materials (pulp) for the filter paper are coarse long fiber, often from fast growing trees. Both bleached and unbleached qualities are made. Coffee filters are made in different shapes and sizes to fit into different holders. Important parameters are strength, compatibility, efficiency and capacity.\n\nThe paper used for fuel filters is a crêped paper with controlled porosity, which is pleated and wound to cartridges. The raw material for filter paper used in fuel filters are made of a mixture of hardwood and softwood fibres. The basis weight of the paper is 50 - 80 g/m.\n\nFilter papers are widely used in laboratory experiments across many different fields, from biology to chemistry. The type of filter used will differ according to the purpose of the procedure and the chemicals involved. Generally, filter papers are used with laboratory techniques such as gravity or vacuum filtration.\n\nQualitative filter paper is used in qualitative analytical techniques to determine materials. There are different grades of qualitative filter paper according to different pore size. There are total 13 different grades of qualitative filter paper. The largest pore size is grade 4; the smallest pore size is grade 602 h; the most commonly used grades are grade 1 to grade 4.\n\nGrade 1 qualitative filter paper has the pore size of 11 µm. This grade of filter paper is widely used for many different fields in agricultural analysis, air pollution monitoring and other similar experiments.\n\nGrade 2 qualitative filter paper has the pore size of 8 µm. This grade of filter paper requires more filtration time than Grade 1 filter paper. This filter paper is used for monitoring specific contaminants in the atmosphere and soil testing.\n\nGrade 3 qualitative filter paper has the pore size of 6 µm. This grade of filter paper is very suitable for carrying samples after filtration.\nGrade 4 qualitative filter paper has the pore size of 20~25 µm. This grade of filter paper has the largest pore size among all standard qualitative filter papers. It is very useful as rapid filter for cleanup of geological fluids or organic extracts during experiment.\n\nGrade 602 h qualitative filter paper has the pore size of 2 µm. This grade of filter paper has the smallest pore size among all standard qualitative filter papers. It is used for collecting or removing fine particles.\n\nQuantitative filter paper, also called ash-free filter paper, is used for quantitative and gravimetric analysis. During the manufacturing, producer use acid to make the paper ash-less and achieve high purity.\nChromatography is a method chemists use to separate compounds. This type of filter paper has specific water flow rate and absorption speed to maximize the result of paper chromatography. The absorption speed of this type of filter paper is from 6 cm to 18 cm and the thickness is from 0.17 mm from 0.93 mm.\n\nExtraction thimbles are rod-shape filter paper often used in Soxhlet extractors or atomized extractors. It is ideal for very sensitive detection, the performance it depends on the thickness of inner diameter. Also, it is usually used in areas of food control and environmental monitoring.\n\nGlass fiber filter has the pore size of 1 µm, it is useful for filtering highly contaminated solutions or difficult-to-filter solution. Also, glass fiber filter has extends filter life, wide range of particulate loads and can prevent sample contamination. In addition, different types of glass fiber filter are suitable for different filtration situation. There are 7 different types of glass fiber filters and the major difference is thickness.\n\nQuartz fiber filter paper is high resistance chemical, does not absorb NOx and SOx dioxides, unaffected by humidity and easily sterilized. Thus, it is mostly use for air pollution analysis.\n\nPTFE filter has wide operating temperature (-120 °C~260 °C) with high air permeability. The resistance to high temperature makes PTFE filter paper suitable for use in autoclaves. It is often used to filter hot oils, strong solvents and collecting airborne particulates.\n\nEngine oil is filtered to remove impurities. Filtration of oil is normally done with volume filtration. Filter papers for lubrication oils are impregnated to resist high temperatures.\n\nTea bags are made from abacá fibers, a very thin and long fiber manilla hemp. Often the paper is augmented with a minor portion of synthetic fibers. The bag paper is very porous and thin and has high wet strength.\n\n"}
{"id": "4376219", "url": "https://en.wikipedia.org/wiki?curid=4376219", "title": "Fioravanti (automotive)", "text": "Fioravanti (automotive)\n\nFioravanti is an Italian automotive design studio in Moncalieri outside the city of Turin. The company began in 1987 as an architectural practice working on projects in Japan, and since 1991, it has focused its activities on automotive design. \n\nFioravanti was founded by C.E.O. Leonardo Fioravanti, who worked twenty-four years with Pininfarina on such vehicles as the Ferrari Daytona, Ferrari Dino, Ferrari 512 Berlinetta Boxer, the Ferrari 308 GTB, Ferrari 288 GTO and the Ferrari F40.\n\n\n"}
{"id": "37375706", "url": "https://en.wikipedia.org/wiki?curid=37375706", "title": "Gran Casino", "text": "Gran Casino\n\nGran Casino (Alternate title: En el viejo Tampico) is a 1947 Mexican film. It was written by Mauricio Magdaleno and Edmundo Baez, based on a story by Michel Weber, and directed by Luis Buñuel.\n\nGerardo (Jorge Negrete) and his friend Demetrio (Julio Villarreal) are a pair of footloose cowboys in turn-of-the-century Mexico who are in prison on dubious charges. As Gerardo sings and strums on his guitar, Demetrio saws the bars of their cell, enabling them to escape. They come upon a small oil field operated by José Enrique (Francisco Jambrina), an entrepreneur from Argentina who is refusing to sell out to evil oil barons who threaten the workers. Gerardo persuades José to give work to him and his friends, and after he and Demetrio recruit more workers, they're able to rejuvenate the struggling operation. Just as their fortunes are on the rise, however, the oilman disappears and is feared murdered. Demetrio takes over the operation next, but, again, the night before the oil is to start pumping, he goes to the casino and falls for Camelia (Mercedes Barba), the same girl José was last seen with before he vanished, and he too disappears.\n\nJosé's sister Mercedes (Libertad Lamarque) travels to Mexico to find out what's become of him, and when she learns that Gerardo has taken over as manager, she's convinced that Gerardo and his pals are to blame. Wanting to know more about Gerardo and his cronies, she takes a job as a singer at \"Gran Casino,\" a rowdy nightclub near the oil fields. In time, she strikes up a romance with the good-hearted roughneck and learns the identity of her brother's real enemy—Don Fabio (José Baviera), the local front for Big Oil.\n\nFrom the release of the film \"Honeysuckle\" in 1938, Libertad Lamarque was the most popular artist in Argentine cinema. She had made her name on stage and radio as a tango singer and she was able to capitalize on that success by combining her singing and acting abilities in pictures that pulled their melodramatic plots straight from the tales told in popular tangos. She often played the part of a tango singer whose romance with a wealthy suitor is thwarted by his snobbish family, making her a compelling symbol of porteño popular culture with a strong anti-elitist identification. In 1945, Larmarque starred in La cabalgata del circo, a pot-boiler about a theatrical troupe in nineteenth-century Argentina, which included in the cast as a supporting player, Eva Duarte, who was being courted by then Colonel Juan Perón, who was on his way to becoming President of the country. Tensions on the set ran high, as Duarte flaunted her relationship with Argentina's strongman by turning up late every day, having him pick her up from the studio in his state limousine and generally behaving as if she were the star of the picture. When Duarte sat in Lamarque's chair one day, Lamarque slapped her across the face, sparking a cause célèbre that delighted Duarte's many enemies. Lamarque added to the intrigue by suggesting that the two had been vying for the attentions of Peron. After October 17, 1945, Loyalty Day, when demonstrations organized with the cooperation of Duarte resulted in Perón's release from a brief stay in jail, Lamarque's films were banned in Argentina. The next year, after Duarte and Perón had married and Perón had been elected President of Argentina, Lamarque fled Buenos Aires for Mexico City, where her films had been extremely popular for years.\n\nA French national of Russian-Jewish origin, Oscar Dancigers had fled the Nazis in 1940, due to his membership in the Communist Party, and settled in Mexico, where in short order he had founded Ultramar Films and achieved considerable success as a small independent producer. Dancigers specialized in assisting U.S. film companies with on-location production in Mexico. As such, his company was a direct beneficiary of the American Good Neighbor Policy, under which Mexico received enormous exportations of raw film stock from the U.S. government.\n"}
{"id": "5751182", "url": "https://en.wikipedia.org/wiki?curid=5751182", "title": "Human-based evolutionary computation", "text": "Human-based evolutionary computation\n\nHuman-based evolutionary computation (HBEC) is a set of evolutionary computation techniques that rely on human innovation. Human-based evolutionary computation techniques can be classified into three more specific classes analogous to ones in evolutionary computation. There are three basic types of innovation: initialization, mutation, and recombination. Here is a table illustrating which type of human innovation are supported in different classes of HBEC:\n\nAll these three classes also have to implement selection, performed either by humans or by computers.\n\nHuman-based selection strategy is a simplest human-based evolutionary computation procedure. It is used heavily today by websites outsourcing collection and selection of the content to humans (user-contributed content). Viewed as evolutionary computation, their mechanism supports two operations: initialization (when a user adds a new item) and selection (when a user expresses preference among items). The website software aggregates the preferences to compute the fitness of items so that it can promote the fittest items and discard the worst ones. Several methods of human-based selection were analytically compared in (Kosorukoff, 2000; Gentry, 2005).\n\nBecause the concept seems too simple, most of the websites implementing the idea can't avoid the common pitfall: informational cascade in soliciting human preference. For example, digg-style implementations, pervasive on the web, heavily bias subsequent human evaluations by prior ones by showing how many votes the items already have. This makes the aggregated evaluation depend on a very small initial sample of rarely independent evaluations. This encourages many people to game the system that might add to digg's popularity but detract from the quality of the featured results. It is too easy to submit evaluation in digg-style system based only on the content title, without reading the actual content supposed to be evaluated.\n\nA better example of a human-based selection system is Stumbleupon. In Stumbleupon, users first experience the content (stumble upon it), and can then submit their preference by pressing a thumb-up or thumb-down button. Because the user doesn't see the number of votes given to the site by previous users, Stumbleupon can collect a relatively unbiased set of user preferences, and thus evaluate content much more precisely.\n\nIn this context and maybe generally, the Wikipedia software is the best illustration of a working human-based evolution strategy wherein the (targeted) evolution of any given page comprises the fine tuning of the knowledge base of such information that relates to that page. Traditional evolution strategy has three operators: initialization, mutation, and selection. In the case of Wikipedia, the initialization operator is page creation, the mutation operator is incremental page editing. The selection operator is less salient. It is provided by the revision history and the ability to select among all previous revisions via a revert operation. If the page is vandalised and no longer a good fit to its title, a reader can easily go to the revision history and select one of the previous revisions that fits best (hopefully, the previous one). This selection feature is crucial to the success of the Wikipedia.\n\nAn interesting fact is that the original wiki software was created in 1995, but it took at least another six years for large wiki-based collaborative projects to appear. Why did it take so long? One explanation is that the original wiki software lacked a selection operation and hence couldn't effectively support content evolution. The addition of revision history and the rise of large wiki-supported communities coincide in time. From an evolutionary computation point of view, this is not surprising: without a selection operation the content would undergo an aimless genetic drift and would unlikely to be useful to anyone. That is what many people expected from Wikipedia at its inception. However, with a selection operation, the utility of content has a tendency to improve over time as beneficial changes accumulate. This is what actually happens on a large scale in Wikipedia.\n\nHuman-based genetic algorithm (HBGA) provides means for human-based recombination operation (a distinctive feature of genetic algorithms). Recombination operator brings together highly fit parts of different solutions that evolved independently. This makes the evolutionary process more efficient.\n\n"}
{"id": "17943941", "url": "https://en.wikipedia.org/wiki?curid=17943941", "title": "Immersed boundary method", "text": "Immersed boundary method\n\nIn computational fluid dynamics, the immersed boundary method originally referred to an approach developed by Charles Peskin in 1972 to simulate fluid-structure (fiber) interactions. Treating the coupling of the structure deformations and the fluid flow poses a number of challenging problems for numerical simulations (the elastic boundary changes the flow of the fluid and the fluid moves the elastic boundary simultaneously). In the immersed boundary method the fluid is represented on an Eulerian coordinate and the structure is represented on a Lagrangian coordinate. For Newtonian fluids governed by the incompressible Navier–Stokes equations, the fluid equations are\n\nand in case of incompressible fluids (assuming constant density) we have the condition\n\nThe immersed structures are typically represented as a collection of one-dimensional fibers, denoted by formula_3. Each fiber can be viewed as a parametric curve formula_4 where formula_5 is the parameter and formula_6 is time. Physics of the fiber is represented via the fiber force distribution formula_7. Spring forces, bending resistance or any other type of behavior can be built into this term. The force exerted by the structure on the fluid is then interpolated as a source term in the momentum equation using\n\nwhere formula_9 is the Dirac function. The forcing can be extended to multiple dimensions to model elastic surfaces or three-dimensional solids. Assuming a massless structure, the elastic fiber moves with the local fluid velocity and can be interpolated via the delta function\n\nwhere formula_11 denotes the entire fluid domain. \nDiscretization of these equations can be done by assuming an Eulerian grid on the fluid and a separate Lagrangian grid on the fiber. \nApproximations of the Delta distribution by smoother functions will allow us to interpolate between the two grids. \nAny existing fluid solver can be coupled to a solver for the fiber equations to solve the Immersed Boundary equations.\nVariants of this basic approach have been applied to simulate a wide variety of mechanical systems involving elastic structures which interact with fluid flows.\n\nSince the original development of this method by Peskin, a variety of approaches have been developed to simulate flow over complicated immersed bodies on grids that do not conform to the surface of the body. These include methods such as the immersed interface method, the Cartesian grid method, the ghost fluid method and the cut-cell method. Mittal and Iaccarino refer to all these (and other related) methods as Immersed Boundary Methods and provide various categorizations of these methods. From the point of view of implementation, they categorize immersed boundary methods into \"continuous forcing\" and \"discrete forcing\" methods. In the former, a force term is added to the continuous Navier-Stokes equations before discretization, whereas in the latter, the forcing is applied (explicitly or implicitly) to the discretized equations. Under this taxonomy, Peskin's original method is a \"continuous forcing\" method whereas Cartesian grid, cut-cell and the ghost-fluid methods are \"discrete forcing\" methods.\n\n\n\n"}
{"id": "2318163", "url": "https://en.wikipedia.org/wiki?curid=2318163", "title": "Italian Power Exchange", "text": "Italian Power Exchange\n\nThe Italian Power Exchange or IPEX, managed by (or GME in Italian) is the exchange for electricity and natural gas spot trading in Italy.\n\nIt was established by the Italian legislature on 16 March 1999 and issued general rules on 8 May 2001. \n\nIPEX comprises the following spot markets:\n\n\n"}
{"id": "6042516", "url": "https://en.wikipedia.org/wiki?curid=6042516", "title": "Kearny air pump", "text": "Kearny air pump\n\nThe Kearny air pump is an expedient air pump used to ventilate a shelter. The design is such that a person with normal mechanical skills can construct and operate one. It is usually human-powered and designed to be employed during a time of crisis. It was designed to be used in a fallout shelter, but can be used in any situation where emergency ventilation is needed, as after a hurricane.\n\nIt was developed from research performed at Oak Ridge National Laboratory by Cresson Kearny and published in Nuclear War Survival Skills.\n\nThe basic principle is to create a flat surface with vanes that close when moving air and open when going back to the starting position. The design was derived from the punkah.\n\n\n"}
{"id": "17561", "url": "https://en.wikipedia.org/wiki?curid=17561", "title": "Lithium", "text": "Lithium\n\nLithium (from ) is a chemical element with symbol Li and atomic number 3. It is a soft, silvery-white alkali metal. Under standard conditions, it is the lightest metal and the lightest solid element. Like all alkali metals, lithium is highly reactive and flammable, and is stored in mineral oil. When cut, it exhibits a metallic luster, but moist air corrodes it quickly to a dull silvery gray, then black tarnish. It never occurs freely in nature, but only in (usually ionic) compounds, such as pegmatitic minerals which were once the main source of lithium. Due to its solubility as an ion, it is present in ocean water and is commonly obtained from brines. Lithium metal is isolated electrolytically from a mixture of lithium chloride and potassium chloride.\n\nThe nucleus of the lithium atom verges on instability, since the two stable lithium isotopes found in nature have among the lowest binding energies per nucleon of all stable nuclides. Because of its relative nuclear instability, lithium is less common in the solar system than 25 of the first 32 chemical elements even though its nuclei are very light: it is an exception to the trend that heavier nuclei are less common. For related reasons, lithium has important uses in nuclear physics. The transmutation of lithium atoms to helium in 1932 was the first fully man-made nuclear reaction, and lithium deuteride serves as a fusion fuel in staged thermonuclear weapons.\n\nLithium and its compounds have several industrial applications, including heat-resistant glass and ceramics, lithium grease lubricants, flux additives for iron, steel and aluminium production, lithium batteries, and lithium-ion batteries. These uses consume more than three quarters of lithium production.\n\nLithium is present in biological systems in trace amounts; its functions are uncertain. Lithium salts have proven to be useful as a mood-stabilizing drug in the treatment of bipolar disorder in humans.\n\nLike the other alkali metals, lithium has a single valence electron that is easily given up to form a cation. Because of this, lithium is a good conductor of heat and electricity as well as a highly reactive element, though it is the least reactive of the alkali metals. Lithium's low reactivity is due to the proximity of its valence electron to its nucleus (the remaining two electrons are in the 1s orbital, much lower in energy, and do not participate in chemical bonds).\n\nLithium metal is soft enough to be cut with a knife. When cut, it possesses a silvery-white color that quickly changes to gray as it oxidizes to lithium oxide. While it has one of the lowest melting points among all metals (180 °C), it has the highest melting and boiling points of the alkali metals.\n\nLithium has a very low density (0.534 g/cm), comparable with pine wood. It is the least dense of all elements that are solids at room temperature; the next lightest solid element (potassium, at 0.862 g/cm) is more than 60% denser. Furthermore, apart from helium and hydrogen, it is less dense than any liquid element, being only two thirds as dense as liquid nitrogen (0.808 g/cm). Lithium can float on the lightest hydrocarbon oils and is one of only three metals that can float on water, the other two being sodium and potassium.\n\nLithium's coefficient of thermal expansion is twice that of aluminium and almost four times that of iron. Lithium is superconductive below 400 μK at standard pressure and at higher temperatures (more than 9 K) at very high pressures (>20 GPa). At temperatures below 70 K, lithium, like sodium, undergoes diffusionless phase change transformations. At 4.2 K it has a rhombohedral crystal system (with a nine-layer repeat spacing); at higher temperatures it transforms to face-centered cubic and then body-centered cubic. At liquid-helium temperatures (4 K) the rhombohedral structure is prevalent. Multiple allotropic forms have been identified for lithium at high pressures.\n\nLithium has a mass specific heat capacity of 3.58 kilojoules per kilogram-kelvin, the highest of all solids. Because of this, lithium metal is often used in coolants for heat transfer applications.\n\nLithium reacts with water easily, but with noticeably less vigor than other alkali metals. The reaction forms hydrogen gas and lithium hydroxide in aqueous solution. Because of its reactivity with water, lithium is usually stored in a hydrocarbon sealant, often petroleum jelly. Though the heavier alkali metals can be stored in more dense substances, such as mineral oil, lithium is not dense enough to be fully submerged in these liquids. In moist air, lithium rapidly tarnishes to form a black coating of lithium hydroxide (LiOH and LiOH·HO), lithium nitride (LiN) and lithium carbonate (LiCO, the result of a secondary reaction between LiOH and CO).\nWhen placed over a flame, lithium compounds give off a striking crimson color, but when it burns strongly the flame becomes a brilliant silver. Lithium will ignite and burn in oxygen when exposed to water or water vapors. Lithium is flammable, and it is potentially explosive when exposed to air and especially to water, though less so than the other alkali metals. The lithium-water reaction at normal temperatures is brisk but nonviolent because the hydrogen produced does not ignite on its own. As with all alkali metals, lithium fires are difficult to extinguish, requiring dry powder fire extinguishers (Class D type). Lithium is one of the few metals that react with nitrogen under normal conditions.\n\nLithium has a diagonal relationship with magnesium, an element of similar atomic and ionic radius. Chemical resemblances between the two metals include the formation of a nitride by reaction with N, the formation of an oxide () and peroxide () when burnt in O, salts with similar solubilities, and thermal instability of the carbonates and nitrides. The metal reacts with hydrogen gas at high temperatures to produce lithium hydride (LiH).\n\nOther known binary compounds include halides (LiF, LiCl, LiBr, LiI), sulfide (), superoxide (), and carbide (). Many other inorganic compounds are known in which lithium combines with anions to form salts: borates, amides, carbonate, nitrate, or borohydride (). Lithium aluminium hydride () is commonly used as a reducing agent in organic synthesis.\n\nMultiple organolithium reagents are known in which there is a direct bond between carbon and lithium atoms, effectively creating a carbanion. These are extremely powerful bases and nucleophiles. In many of these organolithium compounds, the lithium ions tend to aggregate into high-symmetry clusters by themselves, which is relatively common for alkali cations. LiHe, a very weakly interacting van der Waals compound, has been detected at very low temperatures.\n\nNaturally occurring lithium is composed of two stable isotopes, Li and Li, the latter being the more abundant (92.5% natural abundance). Both natural isotopes have anomalously low nuclear binding energy per nucleon (compared to the neighboring elements on the periodic table, helium and beryllium); lithium is the only low numbered element that can produce net energy through nuclear fission. The two lithium nuclei have lower binding energies per nucleon than any other stable nuclides other than deuterium and helium-3. As a result of this, though very light in atomic weight, lithium is less common in the Solar System than 25 of the first 32 chemical elements. Seven radioisotopes have been characterized, the most stable being Li with a half-life of 838 ms and Li with a half-life of 178 ms. All of the remaining radioactive isotopes have half-lives that are shorter than 8.6 ms. The shortest-lived isotope of lithium is Li, which decays through proton emission and has a half-life of 7.6 × 10 s.\n\nLi is one of the primordial elements (or, more properly, primordial nuclides) produced in Big Bang nucleosynthesis. A small amount of both Li and Li are produced in stars, but are thought to be \"burned\" as fast as produced. Additional small amounts of lithium of both Li and Li may be generated from solar wind, cosmic rays hitting heavier atoms, and from early solar system Be and Be radioactive decay. While lithium is created in stars during stellar nucleosynthesis, it is further burned. Li can also be generated in carbon stars.\n\nLithium isotopes fractionate substantially during a wide variety of natural processes, including mineral formation (chemical precipitation), metabolism, and ion exchange. Lithium ions substitute for magnesium and iron in octahedral sites in clay minerals, where Li is preferred to Li, resulting in enrichment of the light isotope in processes of hyperfiltration and rock alteration. The exotic Li is known to exhibit a nuclear halo. The process known as laser isotope separation can be used to separate lithium isotopes, in particular Li from Li.\n\nNuclear weapons manufacture and other nuclear physics applications are a major source of artificial lithium fractionation, with the light isotope Li being retained by industry and military stockpiles to such an extent that it has caused slight but measurable change in the Li to Li ratios in natural sources, such as rivers. This has led to unusual uncertainty in the standardized atomic weight of lithium, since this quantity depends on the natural abundance ratios of these naturally-occurring stable lithium isotopes, as they are available in commercial lithium mineral sources.\n\nBoth stable isotopes of lithium can be laser cooled and were used to produce the first quantum degenerate Bose-Fermi mixture.\n\nThough it was synthesized in the Big Bang, lithium (together with beryllium and boron), is markedly less abundant in the universe than other elements. This is a result of the comparatively low stellar temperatures necessary to destroy lithium, along with a lack of common processes to produce it.\n\nAccording to modern cosmological theory, lithium—in both stable isotopes (lithium-6 and lithium-7)—was one of the three elements synthesized in the Big Bang. Though the amount of lithium generated in Big Bang nucleosynthesis is dependent upon the number of photons per baryon, for accepted values the lithium abundance can be calculated, and there is a \"cosmological lithium discrepancy\" in the universe: older stars seem to have less lithium than they should, and some younger stars have much more. The lack of lithium in older stars is apparently caused by the \"mixing\" of lithium into the interior of stars, where it is destroyed, while lithium is produced in younger stars. Though it transmutes into two atoms of helium due to collision with a proton at temperatures above 2.4 million degrees Celsius (most stars easily attain this temperature in their interiors), lithium is more abundant than current computations would predict in later-generation stars.\nLithium is also found in brown dwarf substellar objects and certain anomalous orange stars. Because lithium is present in cooler, less-massive brown dwarfs, but is destroyed in hotter red dwarf stars, its presence in the stars' spectra can be used in the \"lithium test\" to differentiate the two, as both are smaller than the Sun. Certain orange stars can also contain a high concentration of lithium. Those orange stars found to have a higher than usual concentration of lithium (such as Centaurus X-4) orbit massive objects—neutron stars or black holes—whose gravity evidently pulls heavier lithium to the surface of a hydrogen-helium star, causing more lithium to be observed.\n\nAlthough lithium is widely distributed on Earth, it does not naturally occur in elemental form due to its high reactivity. The total lithium content of seawater is very large and is estimated as 230 billion tonnes, where the element exists at a relatively constant concentration of 0.14 to 0.25 parts per million (ppm), or 25 micromolar; higher concentrations approaching 7 ppm are found near hydrothermal vents.\n\nEstimates for the Earth's crustal content range from 20 to 70 ppm by weight. In keeping with its name, lithium forms a minor part of igneous rocks, with the largest concentrations in granites. Granitic pegmatites also provide the greatest abundance of lithium-containing minerals, with spodumene and petalite being the most commercially viable sources. Another significant mineral of lithium is lepidolite which is now an obsolete name for a series formed by polylithionite and trilithionite. A newer source for lithium is hectorite clay, the only active development of which is through the Western Lithium Corporation in the United States. At 20 mg lithium per kg of Earth's crust, lithium is the 25th most abundant element.\n\nAccording to the \"Handbook of Lithium and Natural Calcium\", \"Lithium is a comparatively rare element, although it is found in many rocks and some brines, but always in very low concentrations. There are a fairly large number of both lithium mineral and brine deposits but only comparatively few of them are of actual or potential commercial value. Many are very small, others are too low in grade.\"\n\nThe US Geological Survey estimates that in 2010, Chile had the largest reserves by far (7.5 million tonnes) and the highest annual production (8,800 tonnes). One of the largest \"reserve bases\" of lithium is in the Salar de Uyuni area of Bolivia, which has 5.4 million tonnes. Other major suppliers include Australia, Argentina and China. As of 2015, the Czech Geological Survey considered the entire Ore Mountains in the Czech Republic as lithium province. Five deposits are registered, one near is considered as a potentially economical deposit, with 160 000 tonnes of lithium.\n\nIn June 2010, \"The New York Times\" reported that American geologists were conducting ground surveys on dry salt lakes in western Afghanistan believing that large deposits of lithium are located there. \"Pentagon officials said that their initial analysis at one location in Ghazni Province showed the potential for lithium deposits as large as those of Bolivia, which now has the world's largest known lithium reserves.\" These estimates are \"based principally on old data, which was gathered mainly by the Soviets during their occupation of Afghanistan from 1979–1989\". Stephen Peters, the head of the USGS's Afghanistan Minerals Project, said that he was unaware of USGS involvement in any new surveying for minerals in Afghanistan in the past two years. 'We are not aware of any discoveries of lithium,' he said.\"\n\nLithia (\"lithium brine\") is associated with tin mining areas in Cornwall, England and an evaluation project from 400-meter deep test boreholes is under consideration. If successful the hot brines will also provide geothermal energy to power the lithium extraction and refining process.\n\nLithium is found in trace amount in numerous plants, plankton, and invertebrates, at concentrations of 69 to 5,760 parts per billion (ppb). In vertebrates the concentration is slightly lower, and nearly all vertebrate tissue and body fluids contain lithium ranging from 21 to 763 ppb. Marine organisms tend to bioaccumulate lithium more than terrestrial organisms. Whether lithium has a physiological role in any of these organisms is unknown.\n\nPetalite (LiAlSiO) was discovered in 1800 by the Brazilian chemist and statesman José Bonifácio de Andrada e Silva in a mine on the island of Utö, Sweden. However, it was not until 1817 that Johan August Arfwedson, then working in the laboratory of the chemist Jöns Jakob Berzelius, detected the presence of a new element while analyzing petalite ore. This element formed compounds similar to those of sodium and potassium, though its carbonate and hydroxide were less soluble in water and more alkaline. Berzelius gave the alkaline material the name \"\"lithion\"/\"lithina\"\", from the Greek word \"λιθoς\" (transliterated as \"lithos\", meaning \"stone\"), to reflect its discovery in a solid mineral, as opposed to potassium, which had been discovered in plant ashes, and sodium, which was known partly for its high abundance in animal blood. He named the metal inside the material \"lithium\".\n\nArfwedson later showed that this same element was present in the minerals spodumene and lepidolite. In 1818, Christian Gmelin was the first to observe that lithium salts give a bright red color to flame. However, both Arfwedson and Gmelin tried and failed to isolate the pure element from its salts. It was not isolated until 1821, when William Thomas Brande obtained it by electrolysis of lithium oxide, a process that had previously been employed by the chemist Sir Humphry Davy to isolate the alkali metals potassium and sodium. Brande also described some pure salts of lithium, such as the chloride, and, estimating that lithia (lithium oxide) contained about 55% metal, estimated the atomic weight of lithium to be around 9.8 g/mol (modern value ~6.94 g/mol). In 1855, larger quantities of lithium were produced through the electrolysis of lithium chloride by Robert Bunsen and Augustus Matthiessen. The discovery of this procedure led to commercial production of lithium in 1923 by the German company Metallgesellschaft AG, which performed an electrolysis of a liquid mixture of lithium chloride and potassium chloride.\n\nThe production and use of lithium underwent several drastic changes in history. The first major application of lithium was in high-temperature lithium greases for aircraft engines and similar applications in World War II and shortly after. This use was supported by the fact that lithium-based soaps have a higher melting point than other alkali soaps, and are less corrosive than calcium based soaps. The small demand for lithium soaps and lubricating greases was supported by several small mining operations, mostly in the US.\n\nThe demand for lithium increased dramatically during the Cold War with the production of nuclear fusion weapons. Both lithium-6 and lithium-7 produce tritium when irradiated by neutrons, and are thus useful for the production of tritium by itself, as well as a form of solid fusion fuel used inside hydrogen bombs in the form of lithium deuteride. The US became the prime producer of lithium between the late 1950s and the mid 1980s. At the end, the stockpile of lithium was roughly 42,000 tonnes of lithium hydroxide. The stockpiled lithium was depleted in lithium-6 by 75%, which was enough to affect the measured atomic weight of lithium in many standardized chemicals, and even the atomic weight of lithium in some \"natural sources\" of lithium ion which had been \"contaminated\" by lithium salts discharged from isotope separation facilities, which had found its way into ground water. \n\nLithium was used to decrease the melting temperature of glass and to improve the melting behavior of aluminium oxide when using the Hall-Héroult process. These two uses dominated the market until the middle of the 1990s. After the end of the nuclear arms race, the demand for lithium decreased and the sale of department of energy stockpiles on the open market further reduced prices. In the mid 1990s, several companies started to extract lithium from brine which proved to be a less expensive option than underground or open-pit mining. Most of the mines closed or shifted their focus to other materials because only the ore from zoned pegmatites could be mined for a competitive price. For example, the US mines near Kings Mountain, North Carolina closed before the beginning of the 21st century.\n\nThe development of lithium ion batteries increased the demand for lithium and became the dominant use in 2007. With the surge of lithium demand in batteries in the 2000s, new companies have expanded brine extraction efforts to meet the rising demand.\n\nLithium production has greatly increased since the end of World War II. The metal is separated from other elements in igneous minerals. The metal is produced through electrolysis from a mixture of fused 55% lithium chloride and 45% potassium chloride at about 450 °C.\n\nAs of 2015, most of the world's lithium production is in South America, where lithium-containing brine is extracted from underground pools and concentrated by solar evaporation. The standard extraction technique is to evaporate water from brine. Each batch takes from 18 to 24 months.\n\nIn 1998, the price of lithium was about (or 43 USD/lb).\n\nWorldwide identified reserves in 2018 are estimated by the US Geological Survey (USGS) to be 16 million tonnes, though an accurate estimate of world lithium reserves is difficult. One reason for this is that most lithium classification schemes are developed for solid ore deposits, whereas brine is a fluid that is problematic to treat with the same classification scheme due to varying concentrations and pumping effects. The world has been estimated to contain about 15 million tonnes of lithium reserves, while 65 million tonnes of known resources are reasonable. A total of 75% of everything can typically be found in the ten largest deposits of the world. Another study noted that 83% of the geological resources of lithium are located in six brine, two pegmatite, and two sedimentary deposits.\n\nThe world’s top 3 lithium-producing countries from 2016, as reported by the US Geological Survey are Australia, Chile and Argentina. The intersection of Chile, Bolivia, and Argentina make up the region known as the Lithium Triangle. The Lithium Triangle is known for its high quality salt flats including Bolivia's Salar de Uyuni, Chile's Salar de Atacama, and Argentina's Salar de Arizaro. The Lithium Triangle is believed to contain over 75% of existing known lithium reserves. Deposits are found in South America throughout the Andes mountain chain. Chile is the leading producer, followed by Argentina. Both countries recover lithium from brine pools. According to USGS, Bolivia's Uyuni Desert has 5.4 million tonnes of lithium. Half the world's known reserves are located in Bolivia along the central eastern slope of the Andes. In 2009, Bolivia negotiated with Japanese, French, and Korean firms to begin extraction.\n\nIn the US, lithium is recovered from brine pools in Nevada. A deposit discovered in 2013 in Wyoming's Rock Springs Uplift is estimated to contain 228,000 tons. Additional deposits in the same formation were estimated to be as much as 18 million tons.\n\nOpinions differ about potential growth. A 2008 study concluded that \"realistically achievable lithium carbonate production will be sufficient for only a small fraction of future PHEV and EV global market requirements\", that \"demand from the portable electronics sector will absorb much of the planned production increases in the next decade\", and that \"mass production of lithium carbonate is not environmentally sound, it will cause irreparable ecological damage to ecosystems that should be protected and that LiIon propulsion is incompatible with the notion of the 'Green Car'\".\n\nAccording to a 2011 study by Lawrence Berkeley National Laboratory and the University of California, Berkeley, the currently estimated reserve base of lithium should not be a limiting factor for large-scale battery production for electric vehicles because an estimated 1 billion 40 kWh Li-based batteries could be built with current reserves - about 10 kg of lithium per car. Another 2011 study at the University of Michigan and Ford Motor Company found enough resources to support global demand until 2100, including the lithium required for the potential widespread transportation use. The study estimated global reserves at 39 million tons, and total demand for lithium during the 90-year period annualized at 12–20 million tons, depending on the scenarios regarding economic growth and recycling rates.\n\nOn 9 June 2014, the \"Financialist\" stated that demand for lithium was growing at more than 12% a year. According to Credit Suisse, this rate exceeds projected availability by 25%. The publication compared the 2014 lithium situation with oil, whereby \"higher oil prices spurred investment in expensive deepwater and oil sands production techniques\"; that is, the price of lithium will continue to rise until more expensive production methods that can boost total output receive the attention of investors.\n\nOn 16 July 2018 2.5 million tonnes of high-grade lithium resources and 124 million pounds of uranium resources were found in the Falchani hard rock deposit in the region Puno, Peru.\n\nAfter the 2007 financial crisis, major suppliers, such as Sociedad Química y Minera (SQM), dropped lithium carbonate pricing by 20%. Prices rose in 2012. A 2012 Business Week article outlined the oligopoly in the lithium space: \"SQM, controlled by billionaire Julio Ponce, is the second-largest, followed by Rockwood, which is backed by Henry Kravis’s KKR & Co., and Philadelphia-based FMC\", with Talison mentioned as the biggest producer. Global consumption may jump to 300,000 metric tons a year by 2020 from about 150,000 tons in 2012, to match the demand for lithium batteries that has been growing at about 25% a year, outpacing the 4% to 5% overall gain in lithium production.\n\nLithium salts are extracted from water in mineral springs, brine pools, and brine deposits. Brine excavation is probably the only lithium extraction technology widely used today, as actual mining of lithium ores is much more expensive and has been priced out of the market.\n\nLithium is present in seawater, but commercially viable methods of extraction have yet to be developed.\n\nAnother potential source of lithium is the leachates of geothermal wells, which are carried to the surface. Recovery of lithium has been demonstrated in the field; the lithium is separated by simple filtration. The process and environmental costs are primarily those of the already-operating well; net environmental impacts may thus be positive.\n\nCurrently, there are a number of options available in the marketplace to invest in the metal. While buying physical stock of lithium is hardly possible, investors can buy shares of companies engaged in lithium mining and producing. Also, investors can purchase a dedicated lithium ETF offering exposure to a group of commodity producers.\n\nLithium oxide is widely used as a flux for processing silica, reducing the melting point and viscosity of the material and leading to glazes with improved physical properties including low coefficients of thermal expansion. Worldwide, this is one of the largest use for lithium compounds. Glazes containing lithium oxides are used for ovenware. Lithium carbonate (LiCO) is generally used in this application because it converts to the oxide upon heating.\n\nLate in the 20th century, lithium became an important component of battery electrolytes and electrodes, because of its high electrode potential. Because of its low atomic mass, it has a high charge- and power-to-weight ratio. A typical lithium-ion battery can generate approximately 3 volts per cell, compared with 2.1 volts for lead-acid and 1.5 volts for zinc-carbon. Lithium-ion batteries, which are rechargeable and have a high energy density, differ from lithium batteries, which are disposable (primary) batteries with lithium or its compounds as the anode. Other rechargeable batteries that use lithium include the lithium-ion polymer battery, lithium iron phosphate battery, and the nanowire battery.\n\nThe third most common use of lithium is in greases. Lithium hydroxide is a strong base and, when heated with a fat, produces a soap made of lithium stearate. Lithium soap has the ability to thicken oils, and it is used to manufacture all-purpose, high-temperature lubricating greases.\n\nLithium (e.g. as lithium carbonate) is used as an additive to continuous casting mould flux slags where it increases fluidity, a use which accounts for 5% of global lithium use (2011). Lithium compounds are also used as additives (fluxes) to foundry sand for iron casting to reduce veining.\n\nLithium (as lithium fluoride) is used as an additive to aluminium smelters (Hall–Héroult process), reducing melting temperature and increasing electrical resistance, a use which accounts for 3% of production (2011).\n\nWhen used as a flux for welding or soldering, metallic lithium promotes the fusing of metals during the process and eliminates the forming of oxides by absorbing impurities. Alloys of the metal with aluminium, cadmium, copper and manganese are used to make high-performance aircraft parts (see also Lithium-aluminium alloys).\n\nLithium has been found effective in assisting the perfection of silicon nano-welds in electronic components for electric batteries and other devices.\n\nLithium compounds are used as pyrotechnic colorants and oxidizers in red fireworks and flares.\n\nLithium chloride and lithium bromide are hygroscopic and are used as desiccants for gas streams. Lithium hydroxide and lithium peroxide are the salts most used in confined areas, such as aboard spacecraft and submarines, for carbon dioxide removal and air purification. Lithium hydroxide absorbs carbon dioxide from the air by forming lithium carbonate, and is preferred over other alkaline hydroxides for its low weight.\n\nLithium peroxide (LiO) in presence of moisture not only reacts with carbon dioxide to form lithium carbonate, but also releases oxygen. The reaction is as follows:\nSome of the aforementioned compounds, as well as lithium perchlorate, are used in oxygen candles that supply submarines with oxygen. These can also include small amounts of boron, magnesium, aluminum, silicon, titanium, manganese, and iron.\n\nLithium fluoride, artificially grown as crystal, is clear and transparent and often used in specialist optics for IR, UV and VUV (vacuum UV) applications. It has one of the lowest refractive indexes and the furthest transmission range in the deep UV of most common materials. Finely divided lithium fluoride powder has been used for thermoluminescent radiation dosimetry (TLD): when a sample of such is exposed to radiation, it accumulates crystal defects which, when heated, resolve via a release of bluish light whose intensity is proportional to the absorbed dose, thus allowing this to be quantified. Lithium fluoride is sometimes used in focal lenses of telescopes.\n\nThe high non-linearity of lithium niobate also makes it useful in non-linear optics applications. It is used extensively in telecommunication products such as mobile phones and optical modulators, for such components as resonant crystals. Lithium applications are used in more than 60% of mobile phones.\n\nOrganolithium compounds are widely used in the production of polymer and fine-chemicals. In the polymer industry, which is the dominant consumer of these reagents, alkyl lithium compounds are catalysts/initiators. in anionic polymerization of unfunctionalized olefins. For the production of fine chemicals, organolithium compounds function as strong bases and as reagents for the formation of carbon-carbon bonds. Organolithium compounds are prepared from lithium metal and alkyl halides.\n\nMany other lithium compounds are used as reagents to prepare organic compounds. Some popular compounds include lithium aluminium hydride (LiAlH), lithium triethylborohydride, n-butyllithium and tert-butyllithium are commonly used as extremely strong bases called superbases.\n\nMetallic lithium and its complex hydrides, such as Li[AlH], are used as high-energy additives to rocket propellants. Lithium aluminum hydride can also be used by itself as a solid fuel.\n\nThe Mark 50 torpedo stored chemical energy propulsion system (SCEPS) uses a small tank of sulfur hexafluoride gas, which is sprayed over a block of solid lithium. The reaction generates heat, creating steam to propel the torpedo in a closed Rankine cycle.\n\nLithium hydride containing lithium-6 is used in thermonuclear weapons, where it serves as fuel for the fusion stage of the bomb.\n\nLithium-6 is valued as a source material for tritium production and as a neutron absorber in nuclear fusion. Natural lithium contains about 7.5% lithium-6 from which large amounts of lithium-6 have been produced by isotope separation for use in nuclear weapons. Lithium-7 gained interest for use in nuclear reactor coolants.\nLithium deuteride was the fusion fuel of choice in early versions of the hydrogen bomb. When bombarded by neutrons, both Li and Li produce tritium — this reaction, which was not fully understood when hydrogen bombs were first tested, was responsible for the runaway yield of the Castle Bravo nuclear test. Tritium fuses with deuterium in a fusion reaction that is relatively easy to achieve. Although details remain secret, lithium-6 deuteride apparently still plays a role in modern nuclear weapons as a fusion material.\n\nLithium fluoride, when highly enriched in the lithium-7 isotope, forms the basic constituent of the fluoride salt mixture LiF-BeF used in liquid fluoride nuclear reactors. Lithium fluoride is exceptionally chemically stable and LiF-BeF mixtures have low melting points. In addition, Li, Be, and F are among the few nuclides with low enough thermal neutron capture cross-sections not to poison the fission reactions inside a nuclear fission reactor.\n\nIn conceptualized (hypothetical) nuclear fusion power plants, lithium will be used to produce tritium in magnetically confined reactors using deuterium and tritium as the fuel. Naturally occurring tritium is extremely rare, and must be synthetically produced by surrounding the reacting plasma with a 'blanket' containing lithium where neutrons from the deuterium-tritium reaction in the plasma will fission the lithium to produce more tritium:\n\nLithium is also used as a source for alpha particles, or helium nuclei. When Li is bombarded by accelerated protons Be is formed, which undergoes fission to form two alpha particles. This feat, called \"splitting the atom\" at the time, was the first fully man-made nuclear reaction. It was produced by Cockroft and Walton in 1932.\n\nIn 2013, the US Government Accountability Office said a shortage of lithium-7 critical to the operation of 65 out of 100 American nuclear reactors “places their ability to continue to provide electricity at some risk”. The problem stems from the decline of US nuclear infrastructure. The equipment needed to separate lithium-6 from lithium-7 is mostly a cold war leftover. The US shut down most of this machinery in 1963, when it had a huge surplus of separated lithium, mostly consumed during the twentieth century. The report said it would take five years and $10 million to $12 million to reestablish the ability to separate lithium-6 from lithium-7.\n\nReactors that use lithium-7 heat water under high pressure and transfer heat through heat exchangers that are prone to corrosion. The reactors use lithium to counteract the corrosive effects of boric acid, which is added to the water to absorb excess neutrons.\n\nLithium is useful in the treatment of bipolar disorder. Lithium salts may also be helpful for related diagnoses, such as schizoaffective disorder and cyclic major depression. The active part of these salts is the lithium ion Li. They may increase the risk of developing Ebstein's cardiac anomaly in infants born to women who take lithium during the first trimester of pregnancy.\n\nLithium has also been researched as a possible treatment for cluster headaches.\n\nPrimary food sources of lithium are grains and vegetables, and, in some areas, drinking water also contains significant amounts. Human intake varies depending on location and diet.\n\nLithium was first detected in human organs and fetal tissues in the late 19th century. In humans there are no defined lithium deficiency diseases, but low lithium intakes from water supplies were associated with increased rates of suicides, homicides and the arrest rates for drug use and other crimes. The biochemical mechanisms of action of lithium appear to be multifactorial and are intercorrelated with the functions of several enzymes, hormones and vitamins, as well as with growth and transforming factors. Evidence now appears to be sufficient to accept lithium as essential; a provisional RDA of 1,000 µg/day is suggested for a 70 kg adult.\n\nLithium is corrosive and requires special handling to avoid skin contact. Breathing lithium dust or lithium compounds (which are often alkaline) initially irritate the nose and throat, while higher exposure can cause a buildup of fluid in the lungs, leading to pulmonary edema. The metal itself is a handling hazard because contact with moisture produces the caustic lithium hydroxide. Lithium is safely stored in non-reactive compounds such as naphtha.\n\nSome jurisdictions limit the sale of lithium batteries, which are the most readily available source of lithium for ordinary consumers. Lithium can be used to reduce pseudoephedrine and ephedrine to methamphetamine in the Birch reduction method, which employs solutions of alkali metals dissolved in anhydrous ammonia.\n\nCarriage and shipment of some kinds of lithium batteries may be prohibited aboard certain types of transportation (particularly aircraft) because of the ability of most types of lithium batteries to fully discharge very rapidly when short-circuited, leading to overheating and possible explosion in a process called thermal runaway. Most consumer lithium batteries have built-in thermal overload protection to prevent this type of incident, or are otherwise designed to limit short-circuit currents. Internal shorts from manufacturing defect or physical damage can lead to spontaneous thermal runaway.\n\n\n"}
{"id": "43780534", "url": "https://en.wikipedia.org/wiki?curid=43780534", "title": "Magnetically controlled shunt reactor", "text": "Magnetically controlled shunt reactor\n\nMagnetically controlled shunt reactor (MCSR, CSR) represents electrotechnical equipment purposed for compensation of reactive power and stabilization of voltage level in high voltage (HV) electric networks rated for voltage classes 36 – 750 kV. MCSR is shunt-type static device with smooth regulation by means of inductive reactance.\n\nMagnetically controlled shunt reactors are intended for automatic control over reactive power and stabilization of voltage levels; these ensure the following: \n\n\nOn the assumption of tasks to be solved by MCSRs, as well as with consideration of existing experience of their operation, application field of controlled reactors covers (but not limited) the following areas of the power networks:\n\nAmple opportunities of MCSRs ensure expediency of their application for different voltage classes. Furthermore, expected effect could be shown up both at the level of local area consumer’s grids, and at solving the primary tasks of the national power system as a whole.\n\nIn the context of building-up of the market relations in the electric energy sector and increase of investments for development of power networks, MCSRs offer the complete series of considerable benefits for all economic entities:\n\nMagnetically controlled shunt reactor is the transformer-type device which additionally provides functions of semiconducting key apparatus; this is ensured by means of reactor magnetic system operation in the domain of deep saturation. The basing principle allowed optimal employment of existing designs both in transformer production industry, and in the field of power electronics.Magnetic system of MCSR single phase includes two cores with windings, vertical and horizontal yokes. Control windings with opposite connection and power windings with series (accordant) connection are arranged on CST magnetic system cores. MCSR magnetic system cores are free from nonmagnetic gaps, and owing to this effect in case of the reactor connection to the network it will be in no-load condition. Herewith, the value of reactive power consumed from the grid will not exceed 3% of nominal magnitude. To increase reactor load as for reactive power, its operating range should be offset to non-linear area of hysteresis characteristic; and this is achieved for the account of additional biasing of magnetic system. At connection of regulated dc voltage source to the control windings, increase of the bias flux is ensured.\n\nDue to the fact that ac flow of power winding is superimposed on the bias flux, the net flux is offset to the saturation domain of magnetic system cores. Respectively, saturation of the cores is resulted in occurrence of current in the power winding. In case of energy input to or output from the control circuit, the transient process of increase or decrease of network current and, respectively, of reactive power consumed by reactor is ensured.\n\nReactor power winding current is regulated according to proportional control mode, when control angle of rectified current source thyristors is changed according to proportional mode depending on mismatching between the prescribed voltage setting and the voltage at the point of reactor connection. In case of necessity to implement the rapid transfer of the reactor from one quasi-steady-state mode to another one, the scheme of overexcitation/underexcitation is realized. In such case, time to gain full power starting from no-load condition is reduced up to 0,3 s. Constructively, it is possible to ensure every speed of the reactor power variation. However, based on practical experience of MCSR application, the optimal balance between the reactor operating speed and capacity of biasing system has been determined: speed of power increase/relief within 0,3 – 1,0 s, capacity of biasing system – 1 – 2% of the reactor rated capacity. Depending upon desired requirements, MCSR is adjusted in such a way that would be possible to realize either voltage level stabilization, or consumed reactive power value, or consumed current magnitude. Controlled reactors, same as their non-controlled analogues, are subdivided into bus reactors and line reactors. Based on this principle, MCSR design would be completed with additional element which ensures pre-biasing of electromagnetic part and subsequent inertialess energizing of the reactor (with power increase time less than one cycle of power frequency). Similar to all transformer equipment, MCSR is able to withstand long-term overload up to 120 – 130% as well as should-term overload up to 200%. Moreover, considering the additional measures and the control algorithms, MCSR realizes all functions of uncontrolled shunt reactor including ability to operate within the interval of single-phase automatic reclosing.\n\n"}
{"id": "6260387", "url": "https://en.wikipedia.org/wiki?curid=6260387", "title": "Mangalore Electricity Supply Company Limited", "text": "Mangalore Electricity Supply Company Limited\n\nMangalore Electricity Supply Company Limited (abbreviated as MESCOM) is an Indian electricity supplier to the districts of Karnataka namely Dakshina Kannada, Udupi district, Chickmagalur and Shimoga. It has its headquarters at Mangaluru.The company was formed on June 2002.\n\nKarnataka Electricity Board (KEB) which was earlier involved in Transmission & Distribution of electricity in state of Karnataka was corporatised into Karnataka Power Transmission Corporation Limited (KPTCL).\n\nLater distribution wing was carved out of this company & five companies were formed to cater distribution of electricity to different regions of Karnataka state. Initially four entities were formed first namely MESCOM, BESCOM, HESCOM and GESCOM. Later CESCOM was carved out of MESCOM in 2005 to cater electricity consumers of Mysore region.\n\nCurrently KPTCL looks after only Transmission and the Electricity Supply Companies (ESCOM's) look after distribution.The distribution to last mile or end users is done by this Electricity Supply Companies (ESCOM's ) in the state of Karnataka.The electricity generated by Karnataka Power Corporation Limited and other producers is distributed by these ESCOM's.\n\n\n"}
{"id": "19080286", "url": "https://en.wikipedia.org/wiki?curid=19080286", "title": "Natural resource economics", "text": "Natural resource economics\n\nNatural resource economics deals with the supply, demand, and allocation of the Earth's natural resources. One main objective of natural resource economics is to better understand the role of natural resources in the economy in order to develop more sustainable methods of managing those resources to ensure their availability to future generations. Resource economists study interactions between economic and natural systems, with the goal of developing a sustainable and efficient economy.\n\nNatural resource economics is a transdisciplinary field of academic research within economics that aims to address the connections and interdependence between human economies and natural ecosystems. Its focus is how to operate an economy within the ecological constraints of earth's natural resources. Resource economics brings together and connects different disciplines within the natural and social sciences connected to broad areas of earth science, human economics, and natural ecosystems. Economic models must be adapted to accommodate the special features of natural resource inputs. The traditional curriculum of natural resource economics emphasized fisheries models, forestry models, and minerals extraction models (i.e. fish, trees, and ore). In recent years, however, other resources, notably air, water, the global climate, and \"environmental resources\" in general have become increasingly important to policy-making.\n\nAcademic and policy interest has now moved beyond simply the optimal commercial exploitation of the standard trio of resources to encompass management for other objectives. For example, natural resources more broadly defined have recreational, as well as commercial values. They may also contribute to overall social welfare levels, by their mere existence.\n\nThe economics and policy area focuses on the human aspects of environmental problems. Traditional areas of environmental and natural resource economics include welfare theory, land/location use, pollution control, resource extraction, and non-market valuation, and also resource exhaustibility, sustainability, environmental management, and environmental policy. Research topics could include the environmental impacts of agriculture, transportation and urbanization, land use in poor and industrialized countries, international trade and the environment, climate change, and methodological advances in non-market valuation, to name just a few.\n\nHotelling's rule is a 1938 economic model of non-renewable resource management by Harold Hotelling. It shows that efficient exploitation of a nonrenewable and nonaugmentable resource would, under otherwise stable economic conditions, lead to a depletion of the resource. The rule states that this would lead to a net price or \"Hotelling rent\" for it that rose annually at a rate equal to the rate of interest, reflecting the increasing scarcity of the resource. Nonaugmentable resources of inorganic materials (i.e. minerals) are uncommon; most resources can be augmented by recycling and by the existence and use of substitutes for the end-use products (see below).\n\nVogely has stated that the development of a mineral resource occurs in five stages: (1) The current operating margin (rate of production) governed by the proportion of the reserve (resource) already depleted. (2) The intensive development margin governed by the trade-off between the rising necessary investment and quicker realization of revenue. (3) The extensive development margin in which extraction is begun of known but previously uneconomic deposits. (4) The exploration margin in which the search for new deposits (resources) is conducted and the cost per unit extracted is highly uncertain with the cost of failure having to be balanced against finding usable resources (deposits) that have marginal costs of extraction no higher than in the first three stages above. (5) The technology margin which interacts with the first four stages. The Gray-Hotelling (exhaustion) theory is a special case, since it covers only Stages 1–3 and not the far more important Stages 4 and 5.\n\nSimon has stated that the supply of natural resources is infinite (i.e. perpetual) \n\nThese conflicting views will be substantially reconciled by considering resource-related topics in depth in the next section, or at least minimized.\n\nFurthermore, Hartwick's rule provides insight to the sustainability of welfare in an economy that uses non-renewable resources.\n\nThe perpetual resource concept is a complex one because the concept of resource is complex and changes with the advent of new technology (usually more efficient recovery), new needs, and to a lesser degree with new economics (e.g. changes in prices of the material, changes in energy costs, etc.). On the one hand, a material (and its resources) can enter a time of shortage and become a strategic and critical material (an immediate exhaustibility crisis), but on the other hand a material can go out of use, its resource can proceed to being perpetual if it was not before, and then the resource can become a paleoresource when the material goes almost completely out of use (e.g. resources of arrowhead-grade flint). Some of the complexities influencing resources of a material include the extent of recyclability, the availability of suitable substitutes for the material in its end-use products, plus some other less important factors.\n\nThe Federal Government suddenly became compellingly interested in resource issues on December 7, 1941, shortly after which Japan cut the U.S. off from tin and rubber and made some other materials very difficult to obtain, such as tungsten. This was the worst case for resource availability, becoming a strategic and critical material. After the war a government stockpile of strategic and critical materials was set up, having around 100 different materials which were purchased for cash or obtained by trading off U.S. agricultural commodities for them. In the longer term, scarcity of tin later led to completely substituting aluminum foil for tin foil and polymer lined steel cans and aseptic packaging substituting for tin electroplated steel cans.\n\nResources change over time with technology and economics; more efficient recovery leads to a drop in the ore grade needed. The average grade of the copper ore processed has dropped from 4.0% copper in 1900 to 1.63% in 1920, 1.20% in 1940, 0.73% in 1960, 0.47% in 1980, and 0.44% in 2000.\n\nCobalt had been in an iffy supply status ever since the Belgian Congo (world's only significant source of cobalt) was given a hasty independence in 1960 and the cobalt-producing province seceded as Katanga, followed by several wars and insurgencies, local government removals, railroads destroyed, and nationalizations. This was topped off by an invasion of the province by Katangan rebels in 1978 that disrupted supply and transportation and caused the cobalt price to briefly triple. While the cobalt supply was disrupted and the price shot up, nickel and other substitutes were pressed into service.\n\nFollowing this, the idea of a \"Resource War\" by the Soviets became popular. Rather than the chaos that resulted from the Zairean cobalt situation, this would be planned, a strategy designed to destroy economic activity outside the Soviet bloc by the acquisition of vital resources by noneconomic means (military?) outside the Soviet bloc (Third World?), then withholding these minerals from the West.\n\nAn important way of getting around a cobalt situation or a \"Resource War\" situation is to use substitutes for a material in its end-uses. Some criteria for a satisfactory substitute are (1) ready availability domestically in adequate quantities or availability from contiguous nations, or possibly from overseas allies, (2) possessing physical and chemical properties, performance, and longevity comparable to the material of first choice, (3) well-established and known behavior and properties particularly as a component in exotic alloys, and (4) an ability for processing and fabrication with minimal changes in existing technology, capital plant, and processing and fabricating facilities. Some suggested substitutions were alunite for bauxite to make alumina, molybdenum and/or nickel for cobalt, and aluminum alloy automobile radiators for copper alloy automobile radiators. Materials can be eliminated without material substitutes, for example by using discharges of high tension electricity to shape hard objects that were formerly shaped by mineral abrasives, giving superior performance at lower cost, or by using computers/satellites to replace copper wire (land lines).\n\nAn important way of replacing a resource is by synthesis, for example, industrial diamonds and many kinds of graphite, although a certain kind of graphite could be almost replaced by a recycled product. Most graphite is synthetic, for example, graphite electrodes, graphite fiber, graphite shapes (machined or unmachined), and graphite powder.\n\nAnother way of replacing or extending a resource is by recycling the material desired from scrap or waste. This depends on whether or not the material is dissipated or is available as a no longer usable durable product. Reclamation of the durable product depends on its resistance to chemical and physical breakdown, quantities available, price of availability, and the ease of extraction from the original product. For example, bismuth in stomach medicine is hopelessly scattered (dissipated) and therefore impossible to recover, while bismuth alloys can be easily recovered and recycled. A good example where recycling makes a big difference is the resource availability situation for graphite, where flake graphite can be recovered from a renewable resource called kish, a steelmaking waste created when carbon separates out as graphite within the kish from the molten metal along with slag. After it is cold, the kish can be processed.\n\nSeveral other kinds of resources need to be introduced. If strategic and critical materials are the worst case for resources, unless mitigated by substitution and/or recycling, one of the best is an abundant resource. An abundant resource is one whose material has so far found little use, such as using high-aluminous clays or anorthosite to produce alumina, and magnesium before it was recovered from seawater. An abundant resource is quite similar to a perpetual resource. The reserve base is the part of an identified resource that has a reasonable potential for becoming economically available at a time beyond when currently proven technology and current economics are in operation. Identified resources are those whose location, grade, quality, and quantity are known or estimated from specific geologic evidence. Reserves are that part of the reserve base that can be economically extracted at the time of determination; reserves should not be used as a surrogate for resources because they are often distorted by taxation or the owning firm's public relations needs.\n\nHarrison Brown and associates stated that humanity will process lower and lower grade \"ore\". Iron will come from low-grade iron-bearing material such as raw rock from anywhere in an iron formation, not much different from the input used to make taconite pellets in North America and elsewhere today. As coking coal reserves decline, pig iron and steel production will use non-coke-using processes (i.e. electric steel). The aluminum industry could shift from using bauxite to using anorthosite and clay. Magnesium metal and magnesia consumption (i.e. in refractories), currently obtained from seawater, will increase. Sulfur will be obtained from pyrites, then gypsum or anhydrite. Metals such as copper, zinc, nickel, and lead will be obtained from manganese nodules or the Phosphoria formation (sic!). These changes could occur irregularly in different parts of the world. While Europe and North America might use anorthosite or clay as raw material for aluminum, other parts of the world might use bauxite, and while North America might use taconite, Brazil might use iron ore. New materials will appear (note: they have), the result of technological advances, some acting as substitutes and some with new properties. Recycling will become more common and more efficient (note: it has!). Ultimately, minerals and metals will be obtained by processing \"average\" rock. Rock, 100 tonnes of \"average\" igneous rock, will yield eight tonnes of aluminum, five tonnes of iron, and 0.6 tonnes of titanium.\n\nThe USGS model based on crustal abundance data and the reserve-abundance relationship of McKelvey, is applied to several metals in the Earth's crust (worldwide) and in the U.S. crust. The potential currently recoverable (present technology, economy) resources that come closest to the McKelvey relationship are those that have been sought for the longest time, such as copper, zinc, lead, silver, gold and molybdenum. Metals that do not follow the McKelvey relationship are ones that are byproducts (of major metals) or haven't been vital to the economy until recently (titanium, aluminum to a lesser degree). Bismuth is an example of a byproduct metal that doesn't follow the relationship very well; the 3% lead reserves in the western U.S. would have only 100 ppm bismuth, clearly too low-grade for a bismuth reserve. The world recoverable resource potential is 2,120 million tonnes for copper, 2,590 million tonnes for nickel, 3,400 million tonnes for zinc, 3,519 BILLION tonnes for aluminum, and 2,035 BILLION tonnes for iron.\n\nDiverse authors have further contributions. Some think the number of substitutes is almost infinite, particularly with the flow of new materials from the chemical industry; identical end products can be made from different materials and starting points. Plastics can be good electrical conductors. Since all materials are 100 times weaker than they theoretically should be, it ought to be possible to eliminate areas of dislocations and greatly strengthen them, enabling lesser quantities to be used. To summarize, \"mining\" companies will have more and more diverse products, the world economy is moving away from materials towards services, and the population seems to be levelling, all of which implies a lessening of demand growth for materials; much of the materials will be recovered from somewhat uncommon rocks, there will be much more coproducts and byproducts from a given operation, and more trade in minerals and materials.\n\nAs radical new technology impacts the materials and minerals world more and more powerfully, the materials used are more and more likely to have perpetual resources. There are already more and more materials that have perpetual resources and less and less materials that have nonrenewable resources or are strategic and critical materials. Some materials that have perpetual resources such as salt,stone, magnesium, and common clay were mentioned previously. Thanks to new technology, synthetic diamonds were added to the list of perpetual resources, since they can be easily made from a lump of another form of carbon. Synthetic graphite, is made in large quantities (graphite electrodes, graphite fiber) from carbon precursors such as petroleum coke or a textile fiber. A firm named Liquidmetal Technologies, Inc. is utilizing the removal of dislocations in a material with a technique that overcomes performance limitations caused by inherent weaknesses in the crystal atomic structure. It makes amorphous metal alloys, which retain a random atomic structure when the hot metal solidifies, rather than the crystalline atomic structure (with dislocations) that normally forms when hot metal solidifies. These amorphous alloys have much better performance properties than usual; for example, their zirconium-titanium Liquidmetal alloys are 250% stronger than a standard titanium alloy. The Liquidmetal alloys can supplant many high performance alloys.\n\nExploration of the ocean bottom in the last fifty years revealed manganese nodules and phosphate nodules in many locations. More recently, polymetallic sulfide deposits have been discovered and polymetallic sulfide \"black muds\" are being presently deposited from \"black smokers\" The cobalt scarcity situation of 1978 has a new option now: recover it from manganese nodules. A Korean firm plans to start developing a manganese nodule recovery operation in 2010; the manganese nodules recovered would average 27% to 30% manganese, 1.25% to 1.5% nickel, 1% to 1.4% copper, and 0.2% to 0.25% cobalt (commercial grade) Nautilus Minerals Ltd. is planning to recover commercial grade material averaging 29.9% zinc, 2.3% lead, and 0.5% copper from massive ocean-bottom polymetallic sulfide deposits using an underwater vacuum cleaner-like device that combines some current technologies in a new way. Partnering with Nautilus are Tech Cominco Ltd. and Anglo-American Ltd., world-leading international firms.\n\nThere are also other robot mining techniques that could be applied under the ocean. Rio Tinto is using satellite links to allow workers 1500 kilometers away to operate drilling rigs, load cargo, dig out ore and dump it on conveyor belts, and place explosives to subsequently blast rock and earth. The firm can keep workers out of danger this way, and also use fewer workers. Such technology reduces costs and offsets declines in metal content of ore reserves. Thus a variety of minerals and metals are obtainable from unconventional sources with resources available in huge quantities.\n\nFinally, what is a perpetual resource? The ASTM definition for a perpetual resource is \"one that is virtually inexhaustible on a human time-scale\". Examples given include solar energy, tidal energy, and wind energy, to which should be added salt, stone, magnesium, diamonds, and other materials mentioned above. A study on the biogeophysical aspects of sustainability came up with a rule of prudent practice that a resource stock should last 700 years to achieve sustainability or become a perpetual resource, or for a worse case, 350 years.\n\nIf a resource lasting 700 or more years is perpetual, one that lasts 350 to 700 years can be called an abundant resource, and is so defined here. How long the material can be recovered from its resource depends on human need and changes in technology from extraction through the life cycle of the product to final disposal, plus recyclability of the material and availability of satisfactory substitutes. Specifically, this shows that exhaustibility does not occur until these factors weaken and play out: the availability of substitutes, the extent of recycling and its feasibility, more efficient manufacturing of the final consumer product, more durable and longer-lasting consumer products, and even a number of other factors.\n\nThe most recent resource information and guidance on the kinds of resources that must be considered is covered on the Resource Guide-Update \n\nPerpetual resources can transition to being a paleoresource. A paleoresource is one that has little or no demand for the material extracted from it; an obsolescent material, humans no longer need it. The classic paleoresource is an arrowhead-grade flint resource; no one makes flint arrowheads or spearheads anymore—making a sharpened piece of scrap steel and using it is much simpler. Obsolescent products include tin cans, tin foil, the schoolhouse slate blackboard, and radium in medical technology. Radium has been replaced by much cheaper cobalt-60 and other radioisotopes in radiation treatment. Noncorroding lead as a cable covering has been replaced by plastics.\n\nPennsylvania anthracite is another material where the trend towards obsolescence and becoming a paleoresource can be shown statistically. Production of anthracite was 70.4 million tonnes in 1905, 49.8 million tonnes in 1945, 13.5 million tonnes in 1965, 4.3 million tonnes in 1985, and 1.5 million tonnes in 2005. The amount used per person was 84 kg per person in 1905, 7.1 kg in 1965, and 0.8 kg in 2005. Compare this to the USGS anthracite reserves of 18.6 billion tonnes and total resources of 79 billion tonnes; the anthracite demand has dropped so much that these resources are more than perpetual.\n\nSince anthracite resources are so far into the perpetual resource range and demand for anthracite has dropped so far, is it possible to see how anthracite might become a paleoresource? Probably by customers continuing to disappear (i.e. convert to other kinds of energy for space heating), the supply network atrophy as anthracite coal dealers can't retain enough business to cover costs and close, and mines with too small a volume to cover costs also close. This is a mutually reinforcing process: customers convert to other forms of cleaner energy that produce less pollution and carbon dioxide, then the coal dealer has to close because of lack of enough sales volume to cover costs. The coal dealer's other customers are then forced to convert unless they can find another nearby coal dealer. Finally the anthracite mine closes because it doesn't have enough sales volume to cover its costs.\n\n\n"}
{"id": "12786595", "url": "https://en.wikipedia.org/wiki?curid=12786595", "title": "Network protector", "text": "Network protector\n\nA network protector is a type of electric protective device used in electricity distribution systems. The network protector automatically disconnect its associated distribution transformer from the secondary network when the power starts flowing in reverse direction. Network protectors are used on both spot networks and grid networks. The secondary grid system improves continuity of service for customers, since multiple sources are available to supply the load; a fault with any one supply is automatically isolated by the network protector and does not interrupt service from the other sources. Secondary grids are often used in downtown areas of cities where there are many customers in a small area.\n\nTypically the network protector is set to close when the voltage difference and phase angle are such that the transformer will supply power to the secondary grid, and is set to open when the secondary grid would back-feed through the transformer and supply power to the primary circuit.\n\nNetwork protectors typically have three settings, \"automatic\", \"open\", and \"close\". The top side is fed from multiple protectors and is always energized unless all units on a spot network are in the open position. Grid units will always be energized on the top side from the many other units tied into the grid.\nA spot network is 2 or more transformers dedicated to a single customer. The grid feeds multiple customers.\nA network protector has a circuit breaker set of contacts and a controlling protection relay. The components are enclosed in a protective housing; some network protectors are installed on transformers below grade and must be in water-resistant enclosures. The mechanism contains electrical and mechanical parts to switch open and close the secondary contacts. The controlling relay monitors voltage and current in the transformer, and opens or closes the contact mechanism through electrical signals. The relay uses a power/time curve so that small, short term reverse power flow (such as from elevator hoists) are ignored. Spot units will be 277/480 and the grid units will be 125/216.\n\nThe network protector does not protect the (secondary) network cable from overload. The network protector is installed to protect the stability and reliability of the secondary grid by preventing power flow away from the customers and into the primary feeders. \n\nIf there is a fault on the primary feeder, the substation circuit-breaker is meant to open, disconnecting the primary feeder from one side. The problem is that this primary cable is also connected to a network transformer, which is interconnected to other network transformers on its secondary side. The secondary network will energize the primary feeder through the network transformer. This can be very dangerous, because a fault will continue to be 'fed' from the secondary network side of transformer. Even without a fault, if the electric utility wants to perform maintenance on that primary cable, they must have a way to fully disconnect that primary cable, without worrying about the cable being energized by the secondary network through the network transformer. Thus, the network protector is designed to open its contacts if the relay senses backwards flowing current. \n\nHowever, if there is a fault on the secondary grid, the network protector is not designed to open its contacts up. The secondary fault will continue to be fed from the primary side of the system. In some cases, networks are designed with cable limiters (like fuses) to melt and disconnect the secondary fault under the right conditions. In other cases, the utility lets cable 'burn clear', in which case the fault is allowed to remain fed until the cables fuse, then the fault is isolated. \n\nAnalysis of the system is required to ensure that the system can, indeed, supply enough current to fuse the cables, wherever the fault is. This method tends to work well at 120 volts, but it is less reliable at higher voltages. The danger in depending on the cable to 'burn clear' is that some conditions will not cause the cable to burn in this manner and instead, the entire section of cable can be damaged from excessive, long-term overloading, causing fires and damage to the secondary network. \n\nTypically, network protectors are contained inside a submersible enclosure which is bolted to the throat of the network transformer and placed in underground vaults. IEEE standard C57.12.44 covers network protectors.\n\n\n"}
{"id": "31279073", "url": "https://en.wikipedia.org/wiki?curid=31279073", "title": "Nuclear magnetic resonance crystallography", "text": "Nuclear magnetic resonance crystallography\n\nNuclear magnetic resonance crystallography (NMR crystallography) is a method which utilizes primarily NMR spectroscopy to determine the structure of solid materials on the atomic scale. Thus, solid-state NMR spectroscopy would be used primarily, possibly supplemented by quantum chemistry calculations (e.g. density functional theory), powder diffraction etc. If suitable crystals can be grown, any crystallographic method would generally be preferred to determine the crystal structure comprising in case of organic compounds the molecular structures and molecular packing. The main interest in NMR crystallography is in microcrystalline materials which are amenable to this method but not to X-ray, neutron and electron diffraction. This is largely because interactions of comparably short range are measured in NMR crystallography.\n\nWhen applied to organic molecules, NMR crystallography aims at including structural information not only of a single molecule but also on the molecular packing (i.e. crystal structure). Contrary to X-ray, single crystals are not necessary with solid-state NMR and structural information can be obtained from high-resolution spectra of disordered solids. E.g. polymorphism is an area of interest for NMR crystallography since this is encountered occasionally (and may often be previously undiscovered) in organic compounds. In this case a change in the molecular structure and/or in the molecular packing can lead to polymorphism, and this can be investigated by NMR crystallography.\n\nThe spin interaction that is usually employed for structural analyses via solid state NMR spectroscopy is the magnetic dipolar interaction.\nAdditional knowledge about other interactions within the studied system like the chemical shift or the electric quadrupole interaction can be helpful as well, and in some cases solely the chemical shift has been employed as e.g. for zeolites.\nThe “dipole coupling”-based approach parallels protein NMR spectroscopy to some extent in that e.g. multiple residual dipolar couplings are measured for proteins in solution, and these couplings are used as constraints in the protein structure calculation.\n\nIn NMR crystallography the observed spins in case of organic molecules would often be spin-1/2 nuclei of moderate frequency (, , , etc.). I.e. is excluded due to its large magnetogyric ratio and high spin concentration leading to a network of strong homonuclear dipolar couplings. There are two solutions with respect to H: spin diffusion experiments (see below) and specific labelling with spins (spin = 1). The latter is also popular e.g. in NMR spectroscopic investigations of hydrogen bonds in solution and the solid state.\nBoth intra- and intermolecular structural elements can be investigated e.g. via deuterium REDOR (an established solid state NMR pulse sequence to measure dipolar couplings between deuterons and other spins).\nThis can provide an additional constraint for an NMR crystallographic structural investigation in that it can be used to find and characterize e.g. intermolecular hydrogen bonds.\n\nThe above-mentioned dipolar interaction can be measured directly, e.g. between pairs of heteronuclear spins like C/N in many organic compounds. Furthermore, the strength of the dipolar interaction modulates parameters like the longitudinal relaxation time or the spin diffusion rate which therefore can be examined to obtain structural information. E.g. H spin diffusion has been measured providing rich structural information.\n\nThe chemical shift interaction can be used in conjunction with the dipolar interaction to determine the orientation of the dipolar interaction frame (principal axes system) with respect to the molecular frame (dipolar chemical shift spectroscopy). For some cases there are rules for the chemical shift interaction tensor orientation as for the C spin in ketones due to symmetry arguments (sp hybridisation). If the orientation of a dipolar interaction (between the spin of interest and e.g. another heteronucleus) is measured with respect to the chemical shift interaction coordinate system, these two pieces of information (chemical shift tensor/molecular orientation and the dipole tensor/chemical shift tensor orientation) combined give the orientation of the dipole tensor in the molecular frame. However, this method is only suitable for small molecules (or polymers with a small repetition unit like polyglycine) and it provides only selective (and usually intramolecular) structural information.\n\nThe dipolar interaction yields the most direct information with respect to structure as it makes it possible to measure the distances between the spins. The sensitivity of this interaction is however lacking and even though dipolar-based NMR crystallography makes the elucidation of structures possible, other methods are necessary to obtain high resolution structures. For these reasons much work was done to include the use other NMR observables such as chemical shift anisotropy, J-coupling and the quadrupolar interaction. These anisotropic interactions are highly sensitive to the 3D local environment making it possible to refine the structures of powdered samples to structures rivaling the quality of single crystal X-ray diffraction. These however rely on adequate methods for predicting these interactions as they do not depend in a straightforward fashion on the structure.\n\nA drawback of NMR crystallography is that the method is typically more time consuming and more expensive (due to spectrometer costs and isotope labelling) than X-ray crystallography, it often elucidates only part of the structure, and isotope labelling and experiments may have to be tailored to obtain key structural information. Also not always is a molecular structure suitable for a pure NMR-based NMR crystallographic approach, but it can still play an important role in a multimodality (NMR+diffraction) study.\n\nUnlike in the case of diffraction methods, it appears that NMR crystallography needs to work on a case by case basis. This is the case since difference systems will have different spin physics and different observables which can be probed. The method may therefore not find widespread use as different systems will require qualified individuals to design experiments to study them.\n"}
{"id": "35638295", "url": "https://en.wikipedia.org/wiki?curid=35638295", "title": "Nukissiorfiit", "text": "Nukissiorfiit\n\nNukissiorfiit is a government-owned Greenland energy company. Nukissiorfiit means \"where energies are created\". The company supplies most of Greenland with electricity, water and heat. Most of the electricity is produced by renewable sources (mostly hydro power) such as the Qorlortorsuaq Dam. 70% of Greenland’s energy is produced by renewable sources. The rest is produced by oil burned plants. The company employs 400 people, spread on 17 cities and 54 villages.\n\nReferences\n"}
{"id": "30883607", "url": "https://en.wikipedia.org/wiki?curid=30883607", "title": "Photoelectrochemistry", "text": "Photoelectrochemistry\n\nPhotoelectrochemistry is a subfield of study within physical chemistry concerned with the interaction of light with electrochemical systems. It is an active domain of investigation. One of the pioneers of this field of electrochemistry was the German electrochemist Heinz Gerischer. The interest in this domain is high in the context of development of renewable energy conversion and storage technology.\n\nPhotoelectrochemistry has been intensively studied in the 70-80s because of the first peak oil crisis. Because fossil fuels are non-renewable, it is necessary to develop processes to obtain renewable resources and use clean energy. Artificial photosynthesis, photoelectrochemical water splitting and regenerative solar cells are of special interest in this context. Discovered by Alexander Edmund Becquerel.\n\nH. Gerischer, H. Tributsch, AJ. Nozik, AJ. Bard, A. Fujishima, K. Honda, PE. Laibinis, K. Rajeshwar, TJ Meyer, PV. Kamat, N.S. Lewis, R. Memming, JOM. Bockris are researchers which have contributed a lot to the field of photoelectrochemistry.\n\nSemiconductor materials have energy band gaps, and will generate a pair of electron and hole for each absorbed photon if the energy of the photon is higher than the band gap energy of the semiconductor. This property of semiconductor materials has been successfully used to convert solar energy into electrical energy by photovoltaic devices.\n\nIn photocatalysis the electron-hole pair is immediately used to drive a redox reaction. However, the electron-hole pairs suffer from fast recombination. In photoelectrocatalysis, a differential potential is applied to diminish the number of recombinations between the electrons and the holes. This allows an increase in the yield of light's conversion into chemical energy.\n\nWhen a semiconductor comes into contact with a liquid (redox species), to maintain electrostatic equilibrium, there will be a charge transfer between the semiconductor and liquid phase if formal redox potential of redox species lies inside semiconductor band gap. At thermodynamic equilibrium, the Fermi level of semiconductor and the formal redox potential of redox species are aligned at the interface between semiconductor and redox species. This introduces a downward band bending in a n-type semiconductor for n-type semiconductor/liquid junction (Figure 1(a)) and an upward band bending in a p-type semiconductor for a p-type semiconductor/liquid junction (Figure 1(b)). This characteristic of semiconductor/liquid junctions is similar to a rectifying semiconductor/metal junction or Schottky junction. Ideally to get a good rectifying characteristics at the semiconductor/liquid interface, the formal redox potential must be close to the valence band of the semiconductor for a n-type semiconductor and close to the conduction band of the semiconductor for a p-type semiconductor. The semiconductor/liquid junction has one advantage over the rectifying semiconductor/metal junction in that the light is able to travel through to the semiconductor surface without much reflection; whereas most of the light is reflected back from the metal surface at a semiconductor/metal junction. Therefore, semiconductor/liquid junctions can also be used as photovoltaic devices similar to solid state p–n junction devices. Both n-type and p-type semiconductor/liquid junctions can be used as photovoltaic devices to convert solar energy into electrical energy and are called photoelectrochemical cells. In addition, a semiconductor/liquid junction could also be used to directly convert solar energy into chemical energy by virtue of photoelectrolysis at the semiconductor/liquid junction.\nSemiconductors are usually studied in a photoelectrochemical cell. Different configurations exist with a three electrode device. The phenomenon to study happens at the working electrode WE while the differential potential is applied between the WE and a reference electrode RE (saturated calomel, Ag/AgCl). The current is measured between the WE and the counter electrode CE (carbon vitreous, platinum gauze). The working electrode is the semiconductor material and the electrolyte is composed of a solvent, an electrolyte and a redox specie.\n\nA UV-vis lamp is usually used to illuminate the working electrode. The photoelectrochemical cell is usually made with a quartz window because it does not absorb the light. A monochromator can be used to control the wavelength sent to the WE.\n\nC(diamond), Si, Ge, SiC, SiGe\n\nBN, BP, BAs, AlN, AlP, AlAs, GaN, GaP, GaAs, InN, InP, InAs...\n\nCdS, CdSe, CdTe, ZnO, ZnS, ZnSe, ZnTe, MoS, MoSe, MoTe, WS, WSe\n\nTiO, FeO, CuO\n\nMethylene blue...\n\nPhotoelectrochemistry has been intensively studied in the field of hydrogen production from water and solar energy. The photoelectrochemical splitting of water was historically discovered by Fujishima and Honda in 1972 onto TiO electrodes. Recently many materials have shown promising properties to split efficiently water but TiO remains cheap, abundant, stable against photo-corrosion. The main problem of TiO is its bandgap which is 3 or 3.2 eV according to its crystallinity (anatase or rutile). These values are too high and only the wavelength in the UV region can be absorbed. To increase the performances of this material to split water with solar wavelength, it is necessary to sensitize the TiO. Currently Quantum Dots sensitization is very promising but more research is needed to find new materials able to absorb the light efficiently.\n\nPhotosynthesis is the natural process that converts CO using light to produce hydrocarbon compounds such as sugar. The depletion of fossil fuels encourages scientists to find alternatives to produce hydrocarbon compounds. Artificial photosynthesis is a promising method mimicking the natural photosynthesis to produce such compounds. The photoelectrochemical reduction of CO2 is much studied because of its worldwide impact. Many researchers aim to find new semiconductors to develop stable and efficient photo-anodes and photo-cathodes.\n\nDye-sensitized solar cells or DSSCs use TiO and dyes to absorb the light. This absorption induces the formation of electron-hole pairs which are used to oxidize and reduce the same redox couple, usually I/I. Consequently, a differential potential is created which induces a current.\n\n"}
{"id": "13055749", "url": "https://en.wikipedia.org/wiki?curid=13055749", "title": "Planet in Peril", "text": "Planet in Peril\n\nPlanet in Peril is a two-part, four-hour documentary on CNN that premiered on October 23, 2007, broadcast in the CNN Presents format. It also aired as a special presentation on December 2 and 3, 2007 on Animal Planet & Animal Planet HD. CNN's Anderson Cooper, Sanjay Gupta and Animal Planet's Jeff Corwin investigate the current state of our planet, focusing on four major areas: global warming, overpopulation, deforestation and species loss.\n\nThey report from a wide variety of locations including Alaska, Brazil, Madagascar, Southeast Asia, and Yellowstone National Park, examining the effects of population growth, rising temperatures, poaching and illegal wildlife trade, among others, on the global environment.\n\nSpecific areas it deals with are the conflict in the Niger Delta (oil exploitation) and La Oroya, Peru (industrial pollution).\n\nOn December 11, 2008, CNN premiered a sequel to \"Planet in Peril\", called \"\". It featured Anderson Cooper, Sanjay Gupta, and Lisa Ling from National Geographic Explorer.\n\n"}
{"id": "28043615", "url": "https://en.wikipedia.org/wiki?curid=28043615", "title": "Removal Units", "text": "Removal Units\n\nA Removal Unit (RMU) is a tradable carbon credit or 'Kyoto unit' representing an allowance to emit one metric tonne of greenhouse gases absorbed by a removal or Carbon sink activity in an Annex I country. \n\nRemoval Units are generated and issued by Kyoto Protocol Annex I Parties for carbon absorption by land use, land-use change, and forestry (LULUCF) activities such as reforestation.\n\nUnder Article 3.3 of the Kyoto Protocol, Annex I Parties can recognise the biosequestration, the removal of carbon dioxide from the atmosphere by Carbon sinks, created by direct human-induced afforestation, reforestation and deforestation since 1990, in determining whether they have met their emission reduction commitments under the Protocol. When sinks have resulted in the net removal of greenhouse gases from the atmosphere, Annex I Parties can issue removal units (RMUs).\n\n"}
{"id": "28768628", "url": "https://en.wikipedia.org/wiki?curid=28768628", "title": "Rębielice Królewskie Wind Turbine", "text": "Rębielice Królewskie Wind Turbine\n\nRębielice Królewskie Wind Turbine situated south of Rębielice Królewskie, Poland is a wind power turbine built in 2003. It was designed by Józef Antos (born in 1932), graduate of technical high school in Bytom. The design process took nearly 20 years. It has a total height of 54 metres and rotor with 33 metres diameter. The constructor said it would deliver 35 kW at a wind speed of 2 m/s.\n\n"}
{"id": "57877", "url": "https://en.wikipedia.org/wiki?curid=57877", "title": "Sodium hydroxide", "text": "Sodium hydroxide\n\nSodium hydroxide, also known as lye and caustic soda, is an inorganic compound with the formula NaOH. It is a white solid ionic compound consisting of sodium cations and hydroxide anions .\n\nSodium hydroxide is a highly caustic base and alkali that decomposes proteins at ordinary ambient temperatures and may cause severe chemical burns. It is highly soluble in water, and readily absorbs moisture and carbon dioxide from the air. It forms a series of hydrates NaOH·\"n\". The monohydrate NaOH· crystallizes from water solutions between 12.3 and 61.8 °C. The commercially available \"sodium hydroxide\" is often this monohydrate, and published data may refer to it instead of the anhydrous compound. As one of the simplest hydroxides, it is frequently utilized alongside neutral water and acidic hydrochloric acid to demonstrate the pH scale to chemistry students.\n\nSodium hydroxide is used in many industries: in the manufacture of pulp and paper, textiles, drinking water, soaps and detergents, and as a drain cleaner. Worldwide production in 2004 was approximately 60 million tonnes, while demand was 51 million tonnes.\n\nPure sodium hydroxide is a colorless crystalline solid that melts at without decomposition, and with a boiling point of . It is highly soluble in water, with a lower solubility in polar solvents such as ethanol and methanol. NaOH is insoluble in ether and other non-polar solvents.\n\nSimilar to the hydration of sulfuric acid, dissolution of solid sodium hydroxide in water is a highly exothermic reaction where a large amount of heat is liberated, posing a threat to safety through the possibility of splashing. The resulting solution is usually colorless and odorless. As with other alkaline solutions, it feels slippery with skin contact due to the process of saponification that occurs between NaOH and natural skin oils.\n\nSodium hydroxide, NaOH, as a fluid solution, demonstrates a characteristic viscosity, 78 mPa·s that is much greater than water (1.0 mPa·s) and near that of olive oil (85 mPa·s) at room temperature. The viscosity of NaOH, as with any chemical, is inversely related to its service temperature; meaning viscosity decreases as temperature increases, with the opposite being true also. The viscosity of sodium hydroxide plays a direct role in its application as well as its storage.\n\nSodium hydroxide can form several hydrates NaOH·\"n\", which result in a complex solubility diagram that was described in detail by S. U. Pickering in 1893. The known hydrates and the approximate ranges of temperature and concentration (mass percent of NaOH) of their saturated water solutions are:\n\n\nEarly reports refer to hydrates with \"n\" = 0.5 or \"n\" = 2/3, but later careful investigations failed to confirm their existence.\n\nThe only hydrates with stable melting points are NaOH· (65.10 °C) and NaOH·3.5 (15.38 °C). The other hydrates, except the metastable ones NaOH·3 and NaOH·4 (β) can be crystallized from solutions of the proper composition, as listed above. However, solutions of NaOH can be easily supercooled by many degrees, which allows the formation of hydrates (including the metastable ones) from solutions with different concentrations.\n\nFor example, when a solution of NaOH and water with 1:2 mole ratio (52.6% NaOH by mass) is cooled, the monohydrate normally starts to crystallize (at about 22 °C) before the dihydrate. However, the solution can easily be supercooled down to -15 °C, at which point it may quickly crystallize as the dihydrate. When heated, the solid dihydrate might melt directly into a solution at 13.35 °C; however, once the temperature exceeds 12.58 °C. it often decomposes into solid monohydrate and a liquid solution. Even the \"n\" = 3.5 hydrate is difficult to crystallize, because the solution supercools so much that other hydrates become more stable.\n\nA hot water solution containing 73.1% (mass) of NaOH is an eutectic that solidifies at about 62.63 °C as an intimate mix of anhydrous and monohydrate crystals.\n\nA second stable eutectic composition is 45.4% (mass) of NaOH, that solidifies at about 4.9 °C into a mixture of crystals of the dihydrate and of the 3.5-hydrate.\n\nThe third stable eutectic has 18.4% (mass) of NaOH. It solidifies at about −28.7 °C as a mixture of water ice and the heptahydrate NaOH·7.\n\nWhen solutions with less than 18.4% NaOH are cooled, water ice crystallizes first, leaving the NaOH in solution.\n\nThe α form of the tetrahydrate has density 1.33 g/cm. It melts congruously at 7.55 °C into a liquid with 35.7% NaOH and density 1.392 g/cm, and therefore floats on it like ice on water. However, at about 4.9 °C it may instead melt incongruously into a mixture of solid NaOH·3.5 and a liquid solution.\n\nThe β form of the tetrahydrate is metastable, and often transforms spontaneously to the α form when cooled below −20 °C. Once initiated, the exothermic transformation is complete in a few minutes, with a 6.5% increase in volume of the solid. The β form can be crystallized from supercooled solutions at −26 °C, and melts partially at −1.83 °C.\n\nThe \"sodium hydroxide\" of commerce is often the monohydrate (density 1.829 g/cm). Physical data in technical literature may refer to this form, rather than the anhydrous compound.\n\nThe monohydrate crystallizes in the space group Pbca, with cell dimensions a = 1.1825, b = 0.6213, c = 0.6069 nm. The atoms are arranged in a hydrargillite-like layer structure /O Na O O Na O/... Each sodium atom is surrounded by six oxygen atoms, three each from hydroxyl anions and three from water molecules. The hydrogen atoms of the hydroxyls form strong bonds with oxygen atoms within each O layer. Adjacent O layers are held together by hydrogen bonds between water molecules.\n\nSodium hydroxide reacts with protic acids to produce water and the corresponding salts. For example, when sodium hydroxide reacts with hydrochloric acid, sodium chloride is formed:\n\nIn general, such neutralization reactions are represented by one simple net ionic equation:\n\nThis type of reaction with a strong acid releases heat, and hence is exothermic. Such acid-base reactions can also be used for titrations. However, sodium hydroxide is not used as a primary standard because it is hygroscopic and absorbs carbon dioxide from air.\n\nSodium hydroxide also reacts with acidic oxides, such as sulfur dioxide. Such reactions are often used to \"scrub\" harmful acidic gases (like SO and HS) produced in the burning of coal and thus prevent their release into the atmosphere. For example,\n\nGlass reacts slowly with aqueous sodium hydroxide solutions at ambient temperatures to form soluble silicates. Because of this, glass joints and stopcocks exposed to sodium hydroxide have a tendency to \"freeze\". Flasks and glass-lined chemical reactors are damaged by long exposure to hot sodium hydroxide, which also frosts the glass. Sodium hydroxide does not attack iron at room temperatures, since iron does not have amphoteric properties (i.e., it only dissolves in acid, not base). \nNevertheless at high temperatures (e.g. above 500°C), iron can react endothermically with sodium hydroxide to form iron(III) oxide, sodium metal, and hydrogen gas. This is due to the lower enthalpy of formation of iron(III) oxide (-824.2kJ/mol) compared to sodium hydroxide (-427kJ/mol), thus the reaction is thermodynamically favorable, although its endothermic nature indicates non-spontaneity. Consider the following reaction between molten sodium hydroxide and finely divided iron filings:\n\nA few transition metals, however, may react vigorously with sodium hydroxide.\n\nIn 1986, an aluminium road tanker in the UK was mistakenly used to transport 25% sodium hydroxide solution, causing pressurization of the contents and damage to the tanker. The pressurization was due to the hydrogen gas which is produced in the reaction between sodium hydroxide and aluminium:\n\nUnlike sodium hydroxide, the hydroxides of most transition metals are insoluble, and therefore sodium hydroxide can be used to precipitate transition metal hydroxides. The following colours are observed: blue-copper, green-iron(II), yellow/brown-iron(III). Zinc and lead salts dissolve in excess sodium hydroxide to give a clear solution of NaZnO or NaPbO.\n\nAluminium hydroxide is used as a gelatinous flocculant to filter out particulate matter in water treatment. Aluminium hydroxide is prepared at the treatment plant from aluminium sulfate by reacting it with sodium hydroxide or bicarbonate.\n\nSodium hydroxide can be used for the base-driven hydrolysis of esters (as in saponification), amides and alkyl halides. However, the limited solubility of sodium hydroxide in organic solvents means that the more soluble potassium hydroxide (KOH) is often preferred. Touching sodium hydroxide solution with the bare hands, while not recommended, produces a slippery feeling. This happens because oils on the skin such as sebum are converted to soap.\nDespite solubility in propylene glycol it is unlikely to replace water in saponificaction due to propylene glycol primary reaction with fat before reaction between sodium hydroxide and fat.\n\nSodium hydroxide is industrially produced as a 50% solution by variations of the electrolytic chloralkali process. Chlorine gas is also produced in this process. Solid sodium hydroxide is obtained from this solution by the evaporation of water. Solid sodium hydroxide is most commonly sold as flakes, prills, and cast blocks.\n\nIn 2004, world production was estimated at 60 million dry metric tonnes of sodium hydroxide, and demand was estimated at 51 million tonnes. In 1998, total world production was around 45 million tonnes. North America and Asia each contributed around 14 million tonnes, while Europe produced around 10 million tonnes. In the United States, the major producer of sodium hydroxide is the Dow Chemical Company, which has annual production around 3.7 million tonnes from sites at Freeport, Texas, and Plaquemine, Louisiana. Other major US producers include Oxychem, PPG, Olin, Pioneer Companies, Inc. (PIONA, which was purchased by Olin), and Formosa. All of these companies use the chloralkali process.\n\nHistorically, sodium hydroxide was produced by treating sodium carbonate with calcium hydroxide in a metathesis reaction. (Sodium hydroxide is soluble while calcium carbonate is not.) This process was called causticizing.\n\nThis process was superseded by the Solvay process in the late 19th century, which was in turn supplanted by the chloralkali process which we use today.\n\nSodium hydroxide is also produced by combining pure sodium metal with water. The byproducts are hydrogen gas and heat, often resulting in a flame, making this a common demonstration of the reactivity of alkali metals in academic environments; however, it is not commercially viable, as the isolation of sodium metal is typically performed by reduction or electrolysis of sodium compounds including sodium hydroxide.\n\nSodium hydroxide is a popular strong base used in industry. Around 56% of sodium hydroxide produced is used by industry, 25% of which is used in the paper industry. Sodium hydroxide is also used in the manufacture of sodium salts and detergents, pH regulation, and organic synthesis. It is used in the Bayer process of aluminium production. In bulk, it is most often handled as an aqueous solution, since solutions are cheaper and easier to handle.\n\nSodium hydroxide is used in many scenarios where it is desirable to increase the alkalinity of a mixture, or to neutralize acids.\n\nFor example, in the petroleum industry, sodium hydroxide is used as an additive in drilling mud to increase alkalinity in bentonite mud systems, to increase the mud viscosity, and to neutralize any acid gas (such as hydrogen sulfide and carbon dioxide) which may be encountered in the geological formation as drilling progresses.\n\nPoor quality crude oil can be treated with sodium hydroxide to remove sulfurous impurities in a process known as \"caustic washing\". As above, sodium hydroxide reacts with weak acids such as hydrogen sulfide and mercaptans to yield non-volatile sodium salts, which can be removed. The waste which is formed is toxic and difficult to deal with, and the process is banned in many countries because of this. In 2006, Trafigura used the process and then dumped the waste in Africa. \n\nSodium hydroxide is also widely used in pulping of wood for making paper or regenerated fibers. Along with sodium sulfide, sodium hydroxide is a key component of the white liquor solution used to separate lignin from cellulose fibers in the kraft process. It also plays a key role in several later stages of the process of bleaching the brown pulp resulting from the pulping process. These stages include oxygen delignification, oxidative extraction, and simple extraction, all of which require a strong alkaline environment with a pH > 10.5 at the end of the stages.\n\nIn a similar fashion, sodium hydroxide is used to digest tissues, as in a process that was used with farm animals at one time. This process involved placing a carcass into a sealed chamber, then adding a mixture of sodium hydroxide and water (which breaks the chemical bonds that keep the flesh intact). This eventually turns the body into a liquid with coffee-like appearance, and the only solid that remains are bone hulls, which could be crushed between one's fingertips. Sodium hydroxide is frequently used in the process of decomposing roadkill dumped in landfills by animal disposal contractors. Due to its low cost and availability, it has been used to dispose of corpses by criminals. Italian serial killer Leonarda Cianciulli used this chemical to turn dead bodies into soap. In Mexico, a man who worked for drug cartels admitted disposing of over 300 bodies with it. Sodium hydroxide is a dangerous chemical due to its ability to hydrolyze protein. If a dilute solution is spilled on the skin, burns may result if the area is not washed thoroughly and for several minutes with running water. Splashes in the eye can be more serious and can lead to blindness.\n\nStrong bases attack aluminium. Sodium hydroxide reacts with aluminium and water to release hydrogen gas. The aluminium takes the oxygen atom from sodium hydroxide, which in turn takes the oxygen atom from the water, and releases the two hydrogen atoms, The reaction thus produces hydrogen gas and sodium aluminate. In this reaction, sodium hydroxide acts as an agent to make the solution alkaline, which aluminium can dissolve in. This reaction can be useful in etching, removing anodizing, or converting a polished surface to a satin-like finish, but without further passivation such as anodizing or alodining the surface may become degraded, either under normal use or in severe atmospheric conditions.\n\nIn the Bayer process, sodium hydroxide is used in the refining of alumina containing ores (bauxite) to produce alumina (aluminium oxide) which is the raw material used to produce aluminium metal via the electrolytic Hall-Héroult process. Since the alumina is amphoteric, it dissolves in the sodium hydroxide, leaving impurities less soluble at high pH such as iron oxides behind in the form of a highly alkaline red mud. \n\nOther amphoteric metals are zinc and lead which dissolve in concentrated sodium hydroxide solutions to give sodium zincate and sodium plumbate respectively.\n\nSodium hydroxide is traditionally used in soap making (cold process soap, saponification). It was made in the nineteenth century for a hard surface rather than liquid product because it was easier to store and transport.\n\nFor the manufacture of biodiesel, sodium hydroxide is used as a catalyst for the transesterification of methanol and triglycerides. This only works with anhydrous sodium hydroxide, because combined with water the fat would turn into soap, which would be tainted with methanol. NaOH is used more often than potassium hydroxide because it is cheaper and a smaller quantity is needed.\n\nFood uses of sodium hydroxide include washing or chemical peeling of fruits and vegetables, chocolate and cocoa processing, caramel coloring production, poultry scalding, soft drink processing, and thickening ice cream. Olives are often soaked in sodium hydroxide for softening; Pretzels and German lye rolls are glazed with a sodium hydroxide solution before baking to make them crisp. Owing to the difficulty in obtaining food grade sodium hydroxide in small quantities for home use, sodium carbonate is often used in place of sodium hydroxide.\n\nSpecific foods processed with sodium hydroxide include:\n\n\nSodium hydroxide is frequently used as an industrial cleaning agent where it is often called \"caustic\". It is added to water, heated, and then used to clean process equipment, storage tanks, etc. It can dissolve grease, oils, fats and protein-based deposits. It is also used for cleaning waste discharge pipes under sinks and drains in domestic properties. Surfactants can be added to the sodium hydroxide solution in order to stabilize dissolved substances and thus prevent redeposition. A sodium hydroxide soak solution is used as a powerful degreaser on stainless steel and glass bakeware. It is also a common ingredient in oven cleaners.\n\nA common use of sodium hydroxide is in the production of parts washer detergents. Parts washer detergents based on sodium hydroxide are some of the most aggressive parts washer cleaning chemicals. The sodium hydroxide-based detergents include surfactants, rust inhibitors and defoamers. A parts washer heats water and the detergent in a closed cabinet and then sprays the heated sodium hydroxide and hot water at pressure against dirty parts for degreasing applications. Sodium hydroxide used in this manner replaced many solvent-based systems in the early 1990s when trichloroethane was outlawed by the Montreal Protocol. Water and sodium hydroxide detergent-based parts washers are considered to be an environmental improvement over the solvent-based cleaning methods.\n\nSodium hydroxide is used in the home as a type of drain opener to unblock clogged drains, usually in the form of a dry crystal or as a thick liquid gel. The alkali dissolves greases to produce water soluble products. It also hydrolyzes the proteins such as those found in hair which may block water pipes. These reactions are sped by the heat generated when sodium hydroxide and the other chemical components of the cleaner dissolve in water. Such alkaline drain cleaners and their acidic versions are highly corrosive and should be handled with great caution.\n\nSodium hydroxide is used in some relaxers to straighten hair. However, because of the high incidence and intensity of chemical burns, manufacturers of chemical relaxers use other alkaline chemicals in preparations available to average consumers. Sodium hydroxide relaxers are still available, but they are used mostly by professionals.\n\nA solution of sodium hydroxide in water was traditionally used as the most common paint stripper on wooden objects. Its use has become less common, because it can damage the wood surface, raising the grain and staining the colour.\n\nSodium hydroxide is sometimes used during water purification to raise the pH of water supplies. Increased pH makes the water less corrosive to plumbing and reduces the amount of lead, copper and other toxic metals that can dissolve into drinking water.\n\nSodium hydroxide has been used for detection of carbon monoxide poisoning, with blood samples of such patients turning to a vermilion color upon the addition of a few drops of sodium hydroxide. Today, carbon monoxide poisoning can be detected by CO oximetry.\n\nSodium hydroxide is used in some cement mix plasticisers. This helps homogenise cement mixes, preventing segregation of sands and cement, decreases the amount of water required in a mix and increases workability of the cement product, be it mortar, render or concrete.\n\nSee: Sodium hydroxide test for flavonoids\n\nEMPA researchers are experimenting with concentrated sodium hydroxide (NaOH) as the thermal storage or seasonal reservoir medium for domestic space-heating. If water is added to solid or concentrated sodium hydroxide (NaOH), heat is released. The dilution is exothermic - chemical energy is released in the form of heat. Conversely, by applying heat energy into a dilute sodium hydroxide solution the water will evaporate so that the solution becomes more concentrated and thus stores the supplied heat as latent chemical energy.\n\nLike other corrosive acids and alkalis, drops of sodium hydroxide solutions can readily decompose proteins and lipids in living tissues via amide hydrolysis and ester hydrolysis, which consequently cause chemical burns and may induce permanent blindness upon contact with eyes. Solid alkali can also express its corrosive nature if there is water, such as water vapor. Thus, protective equipment, like rubber gloves, safety clothing and eye protection, should always be used when handling this chemical or its solutions. The standard first aid measures for alkali spills on the skin is, as for other corrosives, irrigation with large quantities of water. Washing is continued for at least ten to fifteen minutes.\n\nLithium battery cells, if ingested, cause serious injuries, even if not crushed. The damage is caused, not by the contents of the battery, but by the electric current it creates, which causes sodium hydroxide to build up and burn through the oesophagus and into major blood vessels, which can cause fatal bleeding.\n\nMoreover, dissolution of sodium hydroxide is highly exothermic, and the resulting heat may cause heat burns or ignite flammables. It also produces heat when reacted with acids.\nSodium hydroxide is also mildly corrosive to glass, which can cause damage to glazing or cause ground glass joints to bind. Sodium hydroxide is corrosive to several metals, like aluminium which reacts with the alkali to produce flammable hydrogen gas on contact: \n\nCareful storage is needed when handling sodium hydroxide for use, especially bulk volumes. Following proper NaOH storage guidelines and maintaining worker/environment safety is always recommended given the chemical's burn hazard. Sodium hydroxide is often stored in bottles--as in laboratories, small-scale use--within intermediate bulk containers--medium volume containers for cargo handling and transport--or within large stationary storage tanks with volumes up to 100,000 gallons--as in manufacturing or waste water plants with extensive NaOH use. Common materials that are compatible with sodium hydroxide and often utilized for NaOH storage include: polyethylene (HDPE, usual, XLPE, less common), carbon steel, polyvinyl chloride (PVC), stainless steel, and fiberglass reinforced plastic (FRP, with a resistant liner).\n\nSodium hydroxide was first prepared by soap makers. A procedure for making sodium hydroxide appeared as part of a recipe for making soap in an Arab book of the late 13th century: \"Al-mukhtara` fi funun min al-suna`\" (Inventions from the Various Industrial Arts), which was compiled by al-Muzaffar Yusuf ibn `Umar ibn `Ali ibn Rasul (d. 1295), a king of Yemen. The recipe called for passing water repeatedly through a mixture of \"alkali\" (Arabic: \"al-qily\", where \"qily\" is ash from saltwort plants, which are rich in sodium ; hence \"alkali\" was impure sodium carbonate) and quicklime (calcium oxide, CaO), whereby a solution of sodium hydroxide was obtained. European soap makers also followed this recipe. When in 1791 the French chemist and surgeon Nicolas Leblanc (1742–1806) patented a process for mass-producing sodium carbonate, natural \"soda ash\" (impure sodium carbonate that was obtained from the ashes of plants that are rich in sodium) was replaced by this artificial version. However, by the 20th century, the electrolysis of sodium chloride had become the primary method for producing sodium hydroxide.\n\n\n\n"}
{"id": "37431", "url": "https://en.wikipedia.org/wiki?curid=37431", "title": "Solvent", "text": "Solvent\n\nA solvent (from the Latin \"solvō\", \"loosen, untie, solve\") is a substance that dissolves a solute (a chemically distinct liquid, solid or gas), resulting in a solution. A solvent is usually a liquid but can also be a solid, a gas, or a supercritical fluid. The quantity of solute that can dissolve in a specific volume of solvent varies with temperature. Common uses for organic solvents are in dry cleaning (e.g. tetrachloroethylene), as paint thinners (e.g. toluene, turpentine), as nail polish removers and glue solvents (acetone, methyl acetate, ethyl acetate), in spot removers (e.g. hexane, petrol ether), in detergents (citrus terpenes) and in perfumes (ethanol). Water is a solvent for polar molecules and the most common solvent used by living things; all the ions and proteins in a cell are dissolved in water within a cell. Solvents find various applications in chemical, pharmaceutical, oil, and gas industries, including in chemical syntheses and purification processes.\n\nWhen one substance is dissolved into another, a solution is formed. This is opposed to the situation when the compounds are insoluble like sand in water. In a solution, all of the ingredients are uniformly distributed at a molecular level and no residue remains. A solvent-solute mixture consists of a single phase with all solute molecules occurring as \"solvates\" (solvent-solute complexes), as opposed to separate continuous phases as in suspensions, emulsions and other types of non-solution mixtures. The ability of one compound to be dissolved in another is known as solubility; if this occurs in all proportions, it is called miscible.\n\nIn addition to mixing, the substances in a solution interact with each other at the molecular level. When something is dissolved, molecules of the solvent arrange around molecules of the solute. Heat transfer is involved and entropy is increased making the solution more thermodynamically stable than the solute and solvent separately. This arrangement is mediated by the respective chemical properties of the solvent and solute, such as hydrogen bonding, dipole moment and polarizability. Solvation does not cause a chemical reaction or chemical configuration changes in the solute. However, solvation resembles a coordination complex formation reaction, often with considerable energetics (heat of solvation and entropy of solvation) and is thus far from a neutral process.\n\nSolvents can be broadly classified into two categories: \"polar\" and \"non-polar\". A special case is mercury, whose solutions are known as amalgams; also, other metal solutions exist which are liquid at room temperature. Generally, the dielectric constant of the solvent provides a rough measure of a solvent's polarity. The strong polarity of water is indicated by its high dielectric constant of 88 (at 0 °C). Solvents with a dielectric constant of less than 15 are generally considered to be nonpolar. The dielectric constant measures the solvent's tendency to partly cancel the field strength of the electric field of a charged particle immersed in it. This reduction is then compared to the field strength of the charged particle in a vacuum. Heuristically, the dielectric constant of a solvent can be thought of as its ability to reduce the solute's effective internal charge. Generally, the dielectric constant of a solvent is an acceptable predictor of the solvent's ability to dissolve common ionic compounds, such as salts.\n\nDielectric constants are not the only measure of polarity. Because solvents are used by chemists to carry out chemical reactions or observe chemical and biological phenomena, more specific measures of polarity are required. Most of these measures are sensitive to chemical structure.\n\nThe \"Grunwald–Winstein mY scale\" measures polarity in terms of solvent influence on buildup of positive charge of a solute during a chemical reaction.\n\n\"Kosower's Z scale\" measures polarity in terms of the influence of the solvent on UV-absorption maxima of a salt, usually pyridinium iodide or the pyridinium zwitterion.\n\n\"Donor number and donor acceptor scale\" measures polarity in terms of how a solvent interacts with specific substances, like a strong Lewis acid or a strong Lewis base.\n\nThe \"Hildebrand parameter\" is the square root of cohesive energy density. It can be used with nonpolar compounds, but cannot accommodate complex chemistry.\n\nReichardt's dye, a solvatochromic dye that changes color in response to polarity, gives a scale of \"E\"(30) values. \"E\" is the transition energy between the ground state and the lowest excited state in kcal/mol, and (30) identifies the dye. Another, roughly correlated scale (\"E\"(33)) can be defined with Nile red.\n\nThe polarity, dipole moment, polarizability and hydrogen bonding of a solvent determines what type of compounds it is able to dissolve and with what other solvents or liquid compounds it is miscible. Generally, polar solvents dissolve polar compounds best and non-polar solvents dissolve non-polar compounds best: \"like dissolves like\". Strongly polar compounds like sugars (e.g. sucrose) or ionic compounds, like inorganic salts (e.g. table salt) dissolve only in very polar solvents like water, while strongly non-polar compounds like oils or waxes dissolve only in very non-polar organic solvents like hexane. Similarly, water and hexane (or vinegar and vegetable oil) are not miscible with each other and will quickly separate into two layers even after being shaken well.\n\nPolarity can be separated to different contributions. For example, the Kamlet-Taft parameters are dipolarity/polarizability (\"π*\"), hydrogen-bonding acidity (\"α\") and hydrogen-bonding basicity (\"β\"). These can be calculated from the wavelength shifts of 3–6 different solvatochromic dyes in the solvent, usually including Reichardt's dye, nitroaniline and diethylnitroaniline. Another option, Hansen's parameters, separate the cohesive energy density into dispersion, polar and hydrogen bonding contributions.\n\nSolvents with a dielectric constant (more accurately, relative static permittivity) greater than 15 (i.e. polar or polarizable) can be further divided into protic and aprotic. Protic solvents solvate anions (negatively charged solutes) strongly via hydrogen bonding. Water is a protic solvent. Aprotic solvents such as acetone or dichloromethane tend to have large dipole moments (separation of partial positive and partial negative charges within the same molecule) and solvate positively charged species via their negative dipole. In chemical reactions the use of polar protic solvents favors the S1 reaction mechanism, while polar aprotic solvents favor the S2 reaction mechanism. These polar solvents are capable of forming hydrogen bonds with water to dissolve in water whereas non polar solvents are not capable of strong hydrogen bonds.\n\nIt is the combination of substances that causes the large functionality of these products and their consumer properties.\n\nThe solvents are grouped into nonpolar, polar aprotic, and polar protic solvents, with each group ordered by increasing polarity. The properties of solvents which exceed those of water are bolded.\n\nThe Hansen solubility parameter values are based on dispersion bonds (δD), polar bonds (δP) and hydrogen bonds (δH). These contain information about the inter-molecular interactions with other solvents and also with polymers, pigments, nanoparticles, etc. This allows for rational formulations knowing, for example, that there is a good HSP match between a solvent and a polymer. Rational substitutions can also be made for \"good\" solvents (effective at dissolving the solute) that are \"bad\" (expensive or hazardous to health or the environment). The following table shows that the intuitions from \"non-polar\", \"polar aprotic\" and \"polar protic\" are put numerically – the \"polar\" molecules have higher levels of δP and the protic solvents have higher levels of δH. Because numerical values are used, comparisons can be made rationally by comparing numbers. For example, acetonitrile is much more polar than acetone but exhibits slightly less hydrogen bonding.\n\nIf, for environmental or other reasons, a solvent or solvent blend is required to replace another of equivalent solvency, the substitution can be made on the basis of the Hansen solubility parameters of each. The values for mixtures are taken as the weighted averages of the values for the neat solvents. This can be calculated by trial-and-error, a spreadsheet of values, or HSP software. A 1:1 mixture of toluene and 1,4 dioxane has δD, δP and δH values of 17.8, 1.6 and 5.5, comparable to those of chloroform at 17.8, 3.1 and 5.7 respectively. Because of the health hazards associated with toluene itself, other mixtures of solvents may be found using a full HSP dataset.\n\nThe boiling point is an important property because it determines the speed of evaporation. Small amounts of low-boiling-point solvents like diethyl ether, dichloromethane, or acetone will evaporate in seconds at room temperature, while high-boiling-point solvents like water or dimethyl sulfoxide need higher temperatures, an air flow, or the application of vacuum for fast evaporation.\n\nMost organic solvents have a lower density than water, which means they are lighter than and will form a layer on top of water. Important exceptions are most of the halogenated solvents like dichloromethane or chloroform will sink to the bottom of a container, leaving water as the top layer. This is crucial to remember when partitioning compounds between solvents and water in a separatory funnel during chemical syntheses.\n\nOften, specific gravity is cited in place of density. Specific gravity is defined as the density of the solvent divided by the density of water at the same temperature. As such, specific gravity is a unitless value. It readily communicates whether a water-insoluble solvent will float (SG < 1.0) or sink (SG > 1.0) when mixed with water.\n\nMost organic solvents are flammable or highly flammable, depending on their volatility. Exceptions are some chlorinated solvents like dichloromethane and chloroform. Mixtures of solvent vapors and air can explode. Solvent vapors are heavier than air; they will sink to the bottom and can travel large distances nearly undiluted. Solvent vapors can also be found in supposedly empty drums and cans, posing a flash fire hazard; hence empty containers of volatile solvents should be stored open and upside down.\n\nBoth diethyl ether and carbon disulfide have exceptionally low autoignition temperatures which increase greatly the fire risk associated with these solvents. The autoignition temperature of carbon disulfide is below 100 °C (212 °F), so objects such as steam pipes, light bulbs, hotplates, and recently-extinguished bunsen burners are able to ignite its vapours.\n\nIn addition some solvents, such as methanol, can burn with a very hot flame which can be nearly invisible under some lighting conditions. This can delay or prevent the timely recognition of a dangerous fire, until flames spread to other materials.\n\nEthers like diethyl ether and tetrahydrofuran (THF) can form highly explosive organic peroxides upon exposure to oxygen and light. THF is normally more likely to form such peroxides than diethyl ether. One of the most susceptible solvents is diisopropyl ether, but all ethers are considered to be potential peroxide sources.\n\nThe heteroatom (oxygen) stabilizes the formation of a free radical which is formed by the abstraction of a hydrogen atom by another free radical. The carbon-centred free radical thus formed is able to react with an oxygen molecule to form a peroxide compound. The process of peroxide formation is greatly accelerated by exposure to even low levels of light, but can proceed slowly even in dark conditions. \n\nUnless a desiccant is used which can destroy the peroxides, they will concentrate during distillation, due to their higher boiling point. When sufficient peroxides have formed, they can form a crystalline, shock-sensitive solid precipitate at the mouth of a container or bottle. Minor mechanical disturbances, such as scraping the inside of a vessel or the dislodging of a deposit, merely twisting the cap may provide sufficient energy for the peroxide to explode or detonate. Peroxide formation is not a significant problem when fresh solvents are used up quickly; they are more of a problem in laboratories which may take years to finish a single bottle. Low-volume users should acquire only small amounts of peroxide-prone solvents, and dispose of old solvents on a regular periodic schedule.\n\nTo avoid explosive peroxide formation, ethers should be stored in an aritight container, away from light, because both light and air can encourage peroxide formation.\n\nA number of tests can be used to detect the presence of a peroxide in an ether; one is to use a combination of iron(II) sulfate and potassium thiocyanate. The peroxide is able to oxidize the Fe ion to an Fe ion, which then forms a deep-red coordination complex with the thiocyanate.\n\nPeroxides may be removed by washing with acidic iron(II) sulfate, filtering through alumina, or distilling from sodium/benzophenone. Aluminum does not destroy the peroxides but merely traps them, and must be disposed of properly. The advantage of using sodium/benzophenone is that moisture and oxygen are removed as well.\n\nGeneral health hazards associated with solvent exposure include toxicity to the nervous system, reproductive damage, liver and kidney damage, respiratory impairment, cancer, and dermatitis.\n\nMany solvents can lead to a sudden loss of consciousness if inhaled in large amounts. Solvents like diethyl ether and chloroform have been used in medicine as anesthetics, sedatives, and hypnotics for a long time. Ethanol (grain alcohol) is a widely used and abused psychoactive drug. Diethyl ether, chloroform, and many other solvents e.g. from gasoline or glues are abused recreationally in glue sniffing, often with harmful long term health effects like neurotoxicity or cancer. \nFraudulent substitution of 1,5-pentanediol for the psychoactive 1,4-butanediol by a subcontractor caused the Bindeez product recall.\nIf ingested, the so called toxic alcohols (other than ethanol) such as methanol, propanol, and ethylene glycol metabolize into toxic aldehydes and acids, which cause potentially fatal metabolic acidosis. The commonly available alcohol solvent methanol can cause permanent blindness or death if ingested. The solvent 2-butoxyethanol, used in fracking fluids, can cause hypotension and metabolic acidosis.\n\nSome solvents including chloroform and benzene a common ingredient in gasoline are known to be carcinogenic, while many others are considered by the World Health Organization to be likely carcinogens. Solvents can damage internal organs like the liver, the kidneys, the nervous system, or the brain. The cumulative effects of long-term or repeated exposure to solvents are called chronic solvent-induced encephalopathy (CSE).\n\nChronic exposure to organic solvents in the work environment can produce a range of adverse neuropsychiatric effects. For example, occupational exposure to organic solvents has been associated with higher numbers of painters suffering from alcoholism. Ethanol has a synergistic effect when taken in combination with many solvents; for instance, a combination of toluene/benzene and ethanol causes greater nausea/vomiting than either substance alone.\n\nMany solvents are known or suspected to be cataractogenic, greatly increasing the risk of developing cataracts in the lens of the eye. Solvent exposure has also been associated with neurotoxic damage causing hearing loss and color vision losses.\n\nA major pathway to induce health effects arises from spills or leaks of solvents that reach the underlying soil. Since solvents readily migrate substantial distances, the creation of widespread soil contamination is not uncommon; this is particularly a health risk if aquifers are affected. Vapor intrusion can occur from sites with extensive subsurface solvent contamination.\n\n\n"}
{"id": "33219517", "url": "https://en.wikipedia.org/wiki?curid=33219517", "title": "South East Europe Pipeline", "text": "South East Europe Pipeline\n\nThe South East Europe Pipeline was a proposal for a natural gas pipeline from eastern Turkey to Baumgarten an der March in Austria. It was seen as an option for diversification of natural gas potential delivery routes for Europe from Azerbaijan. The pipeline would allow Azerbaijan to supply Europe with of natural gas a year. The main source of the gas would be Shah Deniz gas field when its second stage comes online.\n\nThe pipeline was proposed by BP on 24 September 2011 as an alternative to the existing Southern Gas Corridor projects, including the Nabucco pipeline, Trans Adriatic Pipeline, and Interconnector Turkey–Greece–Italy. The pipeline was to use existing pipelines, but also needed (by other sources ) of new pipeline to be laid in different countries. The total route is about .\n\nOn 28 June 2012 the BP-led Shah Deniz consortium announced it will choose between Nabucco West and Trans Adriatic Pipeline as an export option, and accordingly development of the South East Europe Pipeline project will cease.\n\n"}
{"id": "14966086", "url": "https://en.wikipedia.org/wiki?curid=14966086", "title": "Steam cannon", "text": "Steam cannon\n\nA steam cannon is a cannon that launches a projectile using only heat and water, or using a ready supply of high-pressure steam from a boiler. The first steam cannon was designed by Archimedes during the Siege of Syracuse. Leonardo Da Vinci was also known to have designed one (the \"Architonnerre\").\n\nThe early device would consist of a large metal tube, preferably copper due to its high thermal conductivity, which would be placed in a furnace. One end of the tube would be capped and the other loaded with a projectile. Once the tube reached a high enough temperature, a small amount of water would be injected in behind the projectile. In theory, Leonardo da Vinci believed, the water would rapidly expand into vapour, blasting the projectile out the front of the barrel.\n\nVarious unsuccessful efforts were made during the age of steam to create working steam machine guns and cannons using methods and technology derived from steam locomotives.\n\nA successful World War II steam cannon was the Holman Projector, which was used to launch explosive grenades into the air to create a defensive barrage against low-flying enemy aircraft. These later steam cannons were fired by rapidly introducing a burst of highly pressurized steam into the chamber behind the projectile, accelerating the projectile up the barrel at a high rate of speed, much like an air gun, only more powerful. The fundamental function of the device was basically the same as a steam engine, only with a projectile taking place of a piston. The Holman Projector was produced by Holman Brothers of Cornwall, who specialised in pneumatic equipment for mining. The first Projectors were powered by compressed air, stored in high pressure cylinders. After a successful defence of the in August 1940, downing two Heinkel floatplanes, there was a demand for more projectors to be fitted to small naval trawlers. As these vessels were steam-powered but had no compressed air system, the Admiralty requested Holmans to develop a steam-powered version of the Projector. With concerns by Treve Holman over the effects of heat on the Mills bomb projectile, a test was arranged where a borrowed council steam roller was used to 'cook' the Projector and projectile at 190 °C for 20 minutes before the projectile was safely fired to 1,000 feet altitude.\n\n\n\n"}
{"id": "29779240", "url": "https://en.wikipedia.org/wiki?curid=29779240", "title": "Tego film", "text": "Tego film\n\nTego film is an adhesive sheet, used in the manufacture of waterproof plywood. It is applied dry and cured by heat, which allows for high-quality laminates that are free from internal voids and warping. Tego film plywoods were used in aircraft manufacture in Germany during World War II, and the loss of the plant during a 1943 bombing raid was a serious blow to several aircraft projects.\n\nTego film was developed in Germany around 1930 as a glue for waterproof plywood. It comprised a paper sheet pre-impregnated with a resole phenolic resin. When heated, assembled between wood veneers and then compressed, a strong and waterproof laminated plywood was formed. Most plywood at this time used other adhesives, such as casein. These adhesives were generally applied as aqueous solutions, which caused warping of thin veneers and made it difficult to achieve a solid laminate without risk of voids. As Tego film was used dry, it gave a high integrity result, solid and without risk of hidden weaknesses. This became an important factor in time, when it was applied to use in aircraft construction.\n\nThis adhesive was unique at its time and was only available as the \"Goldschmidt\" product line from its makers Goldmand AG of Wuppertal. By 1932 it was being exported worldwide.\n\nThe de Havilland \"Albatross\" airliner of 1936 had a fuselage of wooden sandwich construction: wafers of birch plywood were spaced apart by a balsa sheet and glued by a casein adhesive. This same construction, but with Aerolite, a urea-formaldehyde adhesive, achieved fame with its wartime use in the DH.98 \"Mosquito\" fast bomber. As well as being a construction of light weight and high performance, it also avoided the use of aluminium, a strategic material during wartime, and could use the skills of woodworkers, rather than those of specialised aircraft metalworkers.\n\nWhen Germany attempted to emulate this aircraft with the Ta 154 \"Moskito\", it used Tego film. Flight testing and development of the first Tego film-bonded prototypes from Summer 1943 to 1944 was highly successful. However RAF bombing of Wuppertal in February 1943 (incidentally one of the first Oboe-equipped Pathfinder squadron bombing raids) had already destroyed the only Goldman factory producing Tego film.\nFor the production aircraft, an ersatz cold adhesive was used, produced by Dynamit AG of Leverkusen. During a test flight on 28 June 1944, one of the two aircraft broke up in flight. Investigation showed that the glue left an acidic residue after curing, that in turn damaged the structure of the timber. Mass production of the aircraft never took place after this.\n\nAnother case of an aircraft design whose existence was threatened by the acidic replacement adhesive was the largely wood-structure Heinkel He 162 \"Spatz\", the winner of Nazi Germany's last significant aircraft design competition, for a \"Volksjäger\", or \"people's fighter\" single engine jet-powered fighter-interceptor as part of the \"Jägernotprogramm\" in the concluding months of the war.\n\nThe term 'Tego film' remains in use around plywood manufacturing today, although it has become generic. It is now used to refer to the phenolic resins still used for the manufacture of exterior plywood, although no longer indicates manufacture in Germany. Most of the plywood produced of this type is now from Indonesia.\n\n"}
{"id": "22129319", "url": "https://en.wikipedia.org/wiki?curid=22129319", "title": "Umbric horizon", "text": "Umbric horizon\n\nThe umbric horizon (\"Latin: umbra, shade\") is a thick, dark coloured, surface Soil horizon rich in organic matter. It is identified by its dark colour and structure. \nNormally it has a pH of less than 5.5 representing a base saturation of less than 50 percent. An indication for Soil acidity is a rooting pattern whereby the roots tend to be horizontal.\n\nSimilar thick, dark coloured, organic-rich, base-desaturated surface horizons occur in anthropedogenic horizons from human activities such as deep cultivation, kitchen middens and the addition of organic manures etc. \nThese horizons can usually be differentiated by the presence of artifacts, spade marks or by checking the agricultural history of the area\n\nThe World Reference Base for Soil Resources diagnostic criteria states that an umbric horizon must have:\n\n"}
{"id": "32643995", "url": "https://en.wikipedia.org/wiki?curid=32643995", "title": "United Kingdom National Renewable Energy Action Plan", "text": "United Kingdom National Renewable Energy Action Plan\n\nThe United Kingdom National Renewable Energy Action Plan is the National Renewable Energy Action Plan (NREAP) for the United Kingdom. The plan was commissioned by the Directive 2009/28/EC which required Member States of the European Union to notify the European Commission with a road map. The report describes how the United Kingdom planned to achieve its legally binding target of a 15% share of energy from renewable sources in gross final consumption of energy by 2020. \n\nThe history of energy production in the UK has been based on natural resources of fossil fuels. This means that UK has not been as active in exploitation of renewable resources. Compared to many other Member States, the UK is starting from a very low level of renewable energy consumption and thus the challenge of meeting the 2020 targets is even greater.\n\nThe Renewable Energy Directive (2009) sets a target for the UK to achieve 15% of its energy consumption from renewable sources by 2020. This compares to only 1.5% in 2005. There has been a small increase in renewable energy use in recent years; there must be a much greater level of deployment over the next decade in order to meet the target.\n\nThe share of RES heat in 2005 was 0,48 ktoe and will increase up to 6,2 Mtoe. The share will increase from 0,7% to 12%. Bioenergy in 2020 will still have the biggest share of RES heat and the amount would be 3,6 Mtoe with a seven-fold increase.\nThe consumption of RES electricity was 2005 1,5 TWh and the share 4,7%.The target for RES electricity for 2020 is 116 TWh. This means a several times increase. The biggest share will come from wind (78 TWh and capacity 2800 MW). The bioelectricity would be 20,6 TWh and from biogas 5,6 TWh. The increase for biomass would be five-fold and for biogas 120%.\n\nThe RES in traffic sector was in 2005 0,188 Mtoe and the increase would be really massive, up to 4,5 Mtoe, in 2020. The share would increase from 0,2% to 10,17%. The consumption of ethanol use would be 1,7 Mtoe and biodiesel use would be 2,5 Mtoe.\n\n"}
{"id": "145193", "url": "https://en.wikipedia.org/wiki?curid=145193", "title": "Windpump", "text": "Windpump\n\nA windpump is a type of windmill which is used for pumping water.\nWindpumps were used to pump water since at least the 9th century in what is now Afghanistan, Iran and Pakistan. The use of wind pumps became widespread across the Muslim world and later spread to China and India. Windmills were later used extensively in Europe, particularly in the Netherlands and the East Anglia area of Great Britain, from the late Middle Ages onwards, to drain land for agricultural or building purposes.\n\nSimon Stevin's work in the \"waterstaet\" involved improvements to the sluices and spillways to control flooding. Windmills were already in use to pump the water out, but in \"Van de Molens\" (\"On mills\"), he suggested improvements, including the idea that the wheels should move slowly, and a better system for meshing of the gear teeth. These improvements increased the efficiency of the windmills used to pump water out of the polders by three times. He received a patent on his innovation in 1586.\n\nEight- to ten-bladed windmills were used in the Region of Murcia, Spain, to raise water for irrigation purposes. The drive from the windmill's rotor was led down through the tower and back out through the wall to turn a large wheel known as a \"noria\". The \"noria\" supported a bucket chain which dangled down into the well. The buckets were traditionally made of wood or clay. These windmills remained in use until the 1950s, and many of the towers are still standing.\n\nEarly immigrants to the New World brought with them the technology of windmills from Europe. On US farms, particularly on the Great Plains, wind pumps were used to pump water from farm wells for cattle. In California and some other states, the windmill was part of a self-contained domestic water system, including a hand-dug well and a redwood water tower supporting a redwood tank and enclosed by redwood siding (tankhouse). The self-regulating farm wind pump was invented by Daniel Halladay in 1854. Eventually, steel blades and steel towers replaced wooden construction, and at their peak in 1930, an estimated 600,000 units were in use, with capacity equivalent to 150 megawatts. Very large lighter wind pumps in Australia directly crank the pump with the rotor of the windmill. Extra back gearing between small rotors for high wind areas and the pump crank prevents trying to push the pump rods down on the downstroke faster than they can fall by gravity. Otherwise pumping too fast leads to the pump rods buckling, making the seal of the stuffing box leak and wearing through the wall of the rising main (UK) or the drop-pipe (US) so all output is lost.\n\nThe multi-bladed wind pump or wind turbine atop a lattice tower made of wood or steel hence became, for many years, a fixture of the landscape throughout rural America. These mills, made by a variety of manufacturers, featured a large number of blades so that they would turn slowly with considerable torque in moderate winds and be self-regulating in high winds. A tower-top gearbox and crankshaft converted the rotary motion into reciprocating strokes carried downward through a rod to the pump cylinder below. Today, rising energy costs and improved pumping technology are increasing interest in the use of this once declining technology.\n\nThe Netherlands is well known for its windmills. Most of these iconic structures situated along the edge of polders are actually windpumps, designed to drain the land. These are particularly important as much of the country lies below sea level.\n\nIn the UK, the term \"windpump\" is rarely used, and they are better known as \"drainage windmills\". Many of these were built in The Broads and The Fens of East Anglia for the draining of land, but most of them have since been replaced by diesel or electric powered pumps. Many of the original windmills still stand in a derelict state although some have been restored.\n\nWindpumps are used extensively in Southern Africa, Australia, and on farms and ranches in the central plains and Southwest of the United States. In South Africa and Namibia thousands of windpumps are still operating. These are mostly used to provide water for human use as well as drinking water for large sheep stocks.\n\nKenya has also benefited from the African development of windpump technologies. At the end of the 1970s, the UK NGO Intermediate Technology Development Group provided engineering support to the Kenyan company Bobs Harries Engineering Ltd for the development of the Kijito windpumps. Bobs Harries Engineering Ltd is still manufacturing the Kijito windpumps, and more than 300 of them are operating in the whole of East Africa.\n\nIn many parts of the world, a rope pump is being used in conjunction with wind turbines. This easy-to- construct pump works by pulling a knotted rope through a pipe (usually a simple PVC pipe) causing the water to be pulled up into the pipe. This type of pump has become common in Nicaragua and other places.\n\nTo construct a windpump, the bladed rotor needs to be matched to the pump. With non-electric windpumps, high solidity rotors are best used in conjunction with positive displacement (piston) pumps, because single-acting piston pumps need about three times as much torque to start them as to keep them going. Low solidity rotors, on the other hand, are best used with centrifugal pumps, waterladder pumps and chain and washer pumps, where the torque needed by the pump for starting is less than that needed for running at design speed. Low solidity rotors are best used if they are intended to drive an electricity generator; which in turn can drive the pump.\n\nMulti-bladed wind pumps can be found worldwide and are manufactured in the United States, Argentina, China, New Zealand, and South Africa. A 16 ft (4.8 m) diameter wind pump can lift up to 1600 US gallons (about 6.4 metric tons) of water per hour to an elevation of 100 ft with a 15 to 20 mph wind (24–32 km/h). However they take a strong wind to start so they turn over the crank of the piston pump. Wind pumps require little maintenance—usually only a change of gear box oil annually. An estimated 60,000 wind pumps are still in use in the United States. They are particularly attractive for use at remote sites where electric power is not available and maintenance is difficult to provide.\n\nA common multi-bladed windpump usefully pumps with about 4%–8% of the annual windpower passing through the area it sweeps This lower conversion is due to poor load matching between wind rotors and fixed-stroke piston pumps.\n\nThe main design feature of a multi-bladed rotor is \"high starting torque\", which is necessary for cranking a piston pump operation. Once started a multi-bladed rotor runs at too high a tipspeed ratio at less than its best efficiency of 30% . On the other hand, modern wind rotors can operate at an aerodynamic efficiency of more than 40% at higher tipspeed ratio for a smaller swirl added and wasted to the wind. But they would need a highly variable stroke mechanism rather than just a crank to piston pump.\n\nA multi-bladed windmill is a mechanical device with a piston pump. Because a piston pump has a fixed stroke, the energy demand of this type of pump is proportional to pump speed only. On the other hand, the energy supply of a wind rotor is proportional to the cube of wind speed. Because of that, a wind rotor runs at over speed (more speed than needed), yielding a loss of aerodynamic efficiency.\n\nA variable stroke would match the rotor speed according to wind speed, functioning like a \"variable-speed generator\". The flow rate of variable stroke windpump can be increased two times, compared to fixed stroke windpumps at the same wind speed.\n\nA piston pump has a very light suction phase, but the upstroke is heavy and puts a big backtorque on a starting rotor when the crank is horizontal and ascending. A counterweight on the crank up in the tower and yawing with the wind direction can at least spread the torque to the crank descent.\n\nAlthough multi-bladed windpumps are based on proven technology and are widely used, they have the fundamental problems mentioned above and need a practical variable stroke mechanism .\n\nBetween 1988 and 1990, a variable stroke windpump was tested at the USDA-Agriculture Research Center-Texas, based on two patented designs (Don E. Avery Patent #4.392.785, 1983 and Elmo G. Harris Patent #617.877, 1899). Control systems of the variable stroke wind pumps were mechanical and hydraulic; however, those experiments did not attract the attention of any windpump manufacturer. After experiments with this variable stroke windpump, research focused on wind-electric water pumping systems; no commercial variable stroke windpump exists yet.\n\nFluttering windpumps have been developed in Canada with a pump stroke varying strongly with amplitude to absorb all the variable power in the wind and to stop the uniblade from swinging too far beyond horizontal from its vertical mean position. They are much lighter and use less material than multiblade windpumps and can pump effectively in lighter wind regimes.\n\nA Turkish engineer re-designed the variable stroke windpump technology by using modern electronic control equipment. Research began in 2004, with governmental R&D support. The first commercial new generation variable stroke wind pumps have been designed after ten years of R&D. The 30 kW variable stroke windpump design includes a Darrieus-type modern wind rotor, counterbalance and regenerative brake technology.\n\nIn the Netherlands, the \"tjasker\" is a drainage mill with common sails connected to an Archimedean screw. This is used for pumping water in areas where only a small lift is required. The windshaft sits on a tripod which allows it to pivot. The Archimedean screw lifts water into a collecting ring, where it is drawn off into a ditch at a higher level, thus draining the land.\n\nIn Thailand, windpumps are traditionally built on Chinese windpump designs. These pumps are constructed from wire-braced bamboo poles carrying fabric or bamboo-mat sails; a paddle pump or waterladder pump is fixed to a Thai bladed rotor. They are mainly used in salt pans where the water lift required is typically less than 1 meter.\n\n\n"}
{"id": "33521256", "url": "https://en.wikipedia.org/wiki?curid=33521256", "title": "Yogo sapphire", "text": "Yogo sapphire\n\nYogo sapphires are a variety of corundum found only in Yogo Gulch, part of the Little Belt Mountains in Judith Basin County, Montana, United States, on land once inhabited by the Piegan Blackfeet people. Yogos are typically cornflower blue, a result of trace amounts of iron and titanium. They have high uniform clarity and maintain their brilliance under artificial light. Because Yogo sapphires occur within a vertically dipping resistive igneous dike, mining efforts have been sporadic and rarely profitable. It is estimated that at least 28 million carats () of Yogos are still in the ground. Jewelry containing Yogos was given to First Ladies Florence Harding and Bess Truman; in addition, many gems were sold in Europe, though promoters' claims that Yogos are in the crown jewels of England or the engagement ring of Princess Diana are dubious. Today, several Yogo sapphires are part of the Smithsonian Institution's gem collection.\n\nYogo sapphires were not initially recognized or valued. Gold was discovered at Yogo Creek in 1866, and though \"blue pebbles\" were noticed alongside gold in the stream alluvium by 1878, it was not until 1894 that the \"blue pebbles\" were recognized as sapphires. Sapphire mining began in 1895 after a local rancher named Jake Hoover sent a cigar box of gems he had collected to an assay office, which in turn sent them to Tiffany's in New York, where an appraiser pronounced them \"the finest precious gemstones ever found in the United States\". Hoover then purchased the original mother lode from a sheepherder, later selling it to other investors. This became the highly profitable \"English Mine\", which flourished from 1899 until the 1920s. A second operation, the \"American Mine\", was owned by a series of investors in the western section of the Yogo dike, but was less profitable and bought out by the syndicate that owned the English Mine. In 1984, a third set of claims, known as the Vortex mine, opened.\n\nThe term \"Yogo sapphire\" is the preferred wording for gems found in the Yogo Gulch, whereas \"Montana sapphire\" generally refers to gems found in other Montana locations. More gem-quality sapphires are produced in Montana than anywhere else in North America. Sapphires were first discovered in Montana in 1865, in alluvium along the Missouri River. Finds in other locations in the western half of the state occurred in 1889, 1892, and 1894. The Rock Creek location, near Phillipsburg, is the most productive site in Montana, and its gems inspired the name of the nearby Sapphire Mountains. In 1969, the sapphire was co-designated along with the agate as Montana's state gemstones.\n\nIn the early 1980s, Intergem Limited, which controlled most of the Yogo sapphire mining at the time, rocked the gem world by marketing Yogos as the world's only guaranteed \"untreated\" sapphire, exposing a practice of the time wherein 95 percent of all the world's sapphires were heat-treated to enhance their natural color. Although Intergem went out of business, the gems it mined appeared on the market through the 1990s because the company had paid its salesmen in sapphires during its financial demise. Citibank had obtained a large stock of Yogos as a result of Intergem's collapse, and after keeping them in a vault for nearly a decade, sold its collection in 1994 to a Montana jeweler. Mining activity today is largely confined to hobby miners in the area; the major mines are currently inactive.\n\nYogo sapphires are mined in Montana at Yogo Gulch (), which is in Judith Basin County, Montana, southwest of Utica, west-southwest of Lewistown, and east of Great Falls. The site was in Fergus County when Yogo sapphires were discovered, but in 1920, because of the re-designation of county boundaries, Judith Basin County was carved out from parts of western Fergus County and eastern Cascade County.\n\nYogo Gulch and the corresponding natural features of Yogo Peak (), Yogo Creek, and the Yogo dike, where the gems are mined, are all in the Little Belt Mountains within Judith Basin County. The Gulch is located along the lower reaches of Yogo Creek and west of the Judith River. The west end of the Yogo dike outcrops just southwest of Yogo Creek, about north of Yogo Creek's confluence with the Middle Fork of the Judith River; from there it runs east-northeast and ends about from the Judith River. Yogo Creek starts just south of Yogo Peak, which is about west of the Judith River. From there the creek flows southeast into the Middle Fork of the Judith River. The Judith River then flows northeast from the Little Belts toward Utica. East of the Judith River is Pig-Eye Basin, where Jake Hoover, credited as the person who discovered Yogo sapphires, owned a ranch.\n\nBecause Yogo Gulch lies in a region historically inhabited by the Piegan Blackfeet people, promoters of Yogo sapphires claim that \"yogo\" may mean \"romance\" or \"blue sky\" in the Blackfoot language, although there is little evidence to support this claim. Other meanings for \"yogo\" have been suggested, including \"Going over the hill\". The meaning of the word \"Yogo\" had been lost by 1878, when placer gold was found in Yogo Creek. Thus, its true meaning is uncertain.\n\nSapphires are a color variety of corundum, a crystalline form of aluminium oxide (). Corundum is one of the hardest minerals, rating 9 on the Mohs scale. Corundum gems of most colors are called sapphires, except for red ones, which are called rubies. The term \"Yogo sapphire\" refers only to sapphires from the Yogo Gulch. The cornflower blue color of the Yogo results from trace amounts of iron and titanium. Yogo sapphires are unique in that they are free of cavities and inclusions, have high uniform clarity, lack color zoning, and do not need heat treating because their cornflower blue coloring is uniform and deep. Unlike Asian sapphires, they maintain their brilliance in artificial light. Yogos present an advantage to gemcutters: since they are found as primary constituent minerals within an igneous bedrock rather than in sedimentary alluvial deposits where most other sapphires are located, they retain a perfect or near perfect crystalline shape, making cutting much easier, as does their lack of inclusions, color zoning, or cloudiness. Yogos also exhibit a triangular pattern on the basal plane of the flattened crystals, with thin rhombohedral crystal faces, a feature absent in sapphires from other parts of Montana.\nYogos tend to be beautiful, small, and very expensive. The United States Geological Survey and many gem experts have stated that Yogos are \"among the world's finest sapphires.\" The roughs tend to be small and flat, so cut Yogo gems heavier than are rare. Only about 10 percent of cut pieces are over . The largest recorded Yogo rough, found in 1910, weighed and was cut into an gem. The largest cut Yogo is . Because of the rarity of large rough Yogo sapphires, Yogo gem prices begin rising sharply when they are over , and skyrocket when they are over .\n\nMontana sapphires in general come in a variety of colors, but Yogos are almost always blue. About two percent of Yogos are purple, due to trace amounts of chromium. A very small number of rubies have been found at Yogo Gulch.\n\nYogo sapphires were first discovered in alluvial streambed sediments during gold mining operations in Yogo Gulch downstream from the Yogo dike, but were later traced to their source within igneous bedrock. Worldwide, other than the Yogo Gulch deposit and one small site in the Kashmir region, most other corundum is mined from the sand and gravel created by the weathering of metamorphic rock. Alluvial sapphires are found in the Far East, Australia, and in three other Montana locations—the upper Missouri River, Rock Creek, and Dry Cottonwood Creek. The location of most Yogo sapphires within igneous rock rather than from alluvial placer deposits requires difficult hard rock mining. Coupled with American labor costs, this makes their extraction fairly expensive. At least are estimated to still be in the ground. The Yogo dike is \"the only known igneous rock from which sapphire is mined\".\n\nThe sapphire bearing Yogo dike is a dark gray to green intrusive rock known as a lamprophyre. The lamprophyre is an unusual igneous rock that contains a low content of silica. The rock has a porphyritic texture with large crystals of orthopyroxene and phlogopite set in a fine grained matrix. The phlogopite crystals have been used to determine the age of the dike and its crystallization temperature (900 °C (1,650 °F)). The dike also contains fragments of other rock types. These xenoliths include pieces of limestone, clastic sedimentary rocks, and gneiss. In some locations, due to the abundance of xenoliths, the dike has the appearance of a limestone breccia in an igneous matrix. One gneiss fragment found as a xenolith contains corundum. The Yogo sapphires themselves are rimmed with a reaction layer of spinel and are etched, indicating that the sapphires were not in chemical equilibrium with their host, the lamprophyre magma. This suggests the sapphire crystals may have originated in an earlier rock, such as a corundum-bearing gneiss, later assimilated by the lamprophyre magma at depth. Earlier investigators had assumed that the sapphire had crystallized from the magma with the necessary high aluminium content provided by assimilation of clay rich shales of the Proterozoic Belt Supergroup sediments which are known to be present at depth in the region.\nThe Yogo dike is a narrow subvertical sheet-like igneous body. It varies from thick and extends for , striking at an azimuth of 255°. The dike is broken into three offset en echelon segments, and dates to 48.6 mya using Ar dating on phlogopite. The dike intrudes Mississippian age (360 to 325 mya) limestone and other sedimentary rocks of the Madison and Big Snowy Groups.\n\nThere has been considerable debate over the years as to the depth of the Yogo dike and how many ounces of rough sapphires per ton it contains. In the late 1970s and early 1980s, Delmer L. Brown, a geological engineer and gemologist, conducted the most thorough scientific exploration up to that time, concluding that the dike was at least deep and that the concentration of rough sapphires was not constant throughout the deposit. Brown found that the dike had intruded into a pre-existing fault that had been a conduit for groundwater circulation. The overlying shale, the Kibbey Formation, was deposited on an unconformity, an ancient Mississippian-age karst erosion surface, and was not intruded by the dike. This groundwater action produced collapsed zones which were intruded by the dike to form breccia zones. Recent erosion in the area removed the overlying shales and again exposed the limestone to groundwater action which produced collapse breccias which include fragments of the dike rock. He determined that the erosion of the dike in the current erosion cycle was minimal.\n\nBrown also showed that the unique characteristics of the Yogo sapphires are related to their geological history. Most sapphires are formed under low pressure and temperature over geologically short periods of time, and this is why most non-Yogo sapphires have imperfections and inconsistent coloring. Yogos show crystalline formation under very high temperatures and pressures corresponding to a great depth, over geologically long periods of time. Brown also showed that distribution of gem rough through the dike was not consistent, so using an average \"ounces per ton\" was misleading. For example, the section which, despite several ownership and name changes over the years, is generally known as the \"American Mine,\" was developed in an area dominated by post-dike breccia with significantly lower ounces per ton than the English Mine.\n\n\"Yogo sapphire\" is the preferred term for gems found in the Yogo Gulch, whereas \"Montana sapphire\" generally refers to gems found in other Montana locations. More gem-quality sapphires are produced in Montana than anywhere else in North America. Montana sapphires come in a variety of colors, though rubies are rare.\n\nThe first sapphires found in the United States were discovered on May 5, 1865, along the Missouri River, about east of Helena, in Lewis and Clark County, by Ed \"Sapphire\" Collins. Collins sent the sapphires to Tiffany's in New York City, and to Amsterdam for evaluation; however, those sapphires were of poor coloring and low overall quality, garnering little notice and giving Montana sapphires a poor reputation. Corundum was also found at Dry Cottonwood Creek near Butte in 1889, Rock Creek near Philipsburg in 1892, and Quartz Gulch near Bozeman in 1894. By 1890, the English-owned Sapphire and Ruby Mining Company had bought several thousand acres of land where Montana sapphires were found, but the venture failed after a few years because of fraudulent practices by the owners.\n\nSapphires from these three sites are routinely heat-treated to enhance color. While millions of carats of sapphires have been mined from the Missouri River deposits, there has been little commercial activity there since the 1990s because of the high cost of recovery and environmental concerns. Production at Dry Cottonwood Creek has been sporadic and low-yielding. The Rock Creek area, also known as Gem Mountain, continues to be the most productive site in Montana, even more so than Yogo Gulch, producing over of sapphires since its inception in 1906. Other than Yogo, Montana sapphire mines have been less successful because they have few blue sapphires and non-blue sapphires have low profit margins.\n\nThese gems inspired the names of features: the mountains near Rock Creek are known as the Sapphire Mountains. Garnets are also found at some Montana sapphire sites, inspiring the name of the Garnet Range, which lies to the north of the Sapphire Mountains. In 1969, the sapphire and agate were jointly declared Montana's two official state gemstones.\n\nMining of Yogo sapphires was exceptionally difficult and remains sporadic today. Even so, Yogo sapphire mining turned out to be more valuable than several gold strikes. The Yogo area also produced small amounts of silver, copper, and iron.\n\nYogo Gulch lies in a region originally inhabited by the Piegan Blackfeet people. Gold was first discovered at Yogo Creek in 1866, but the small numbers of early prospectors were driven off by local Native Americans. During a Gold Rush in 1878, about a thousand miners came to Yogo Creek, which was one of the gold-bearing streams in Montana not yet actively mined. \"Blue pebbles\" were noted along with small quantities of gold. The mining camp at Yogo City only flourished for roughly three years, and eventually the population dwindled to only a few people.\n\nYogo City was briefly known as Hoover City, after Jake Hoover. Hoover was part of a partnership that had been placer mining for gold and is credited as the discoverer of Yogo Sapphires. For several years, he also owned a ranch in nearby Pig-Eye Basin. He later prospected for gold in Alaska and was a deep-sea fishing guide in Seattle before eventually returning to the Judith Basin. Western painter C.M. Russell arrived in the area in 1880 as a young cowhand and was hired by Hoover. Russell stated that he learned most of his frontier skills from Hoover, and the two men remained lifelong friends. Millie Ringold, a former slave born in 1845, settled in Fort Benton, Montana after having worked as a nurse and servant for an army general. When gold was discovered at Yogo Creek, Ringold sold her boarding house in Fort Benton and left for the Yogo gold fields, setting up a hotel, restaurant, and saloon in Yogo City where she sang and played music. Ringold later cooked for the English mine, but also worked her own gold claims, even after gold mining was on the decline. She was known as a superb cook and ultimately died in Yogo City in 1906, the last resident of the community. The nearby town of Utica was featured in Russell's 1907 painting \"A Quiet Day In Utica\", which was originally known as \"Tinning a Dog\". Hoover, Ringold, store owner Charles Lehman, and Russell himself are all depicted in the painting, placed between the hitching post and door of the general store.\n\nIn 1894, the \"blue pebbles\" were recognized as sapphires. One story credits a local school teacher for recognizing the blue pebbles as sapphires. A variation is that the teacher lived in Maine, but was a friend of a local miner, who had mailed her a small box with some gold and a few \"blue pebbles\" in it. Another story credits a miner named S.S. Hobson for surmising that the blue stones might be sapphires, and his guess was confirmed by a jeweler in Helena. Ultimately, in 1895, Jake Hoover sent a cigar box containing those he had collected while mining gold to an assay office, which in turn sent them via regular, uninsured mail to Tiffany's in New York City for appraisal by Dr. George Frederick Kunz, the leading American gemologist of the time. Impressed by their quality and color, Kunz pronounced them \"the finest precious gemstones ever found in the United States\". Tiffany's sent Hoover a check for $3,750 (approximately $ as of 2018), along with a letter that described the blue pebbles as \"sapphires of unusual quality\".\n\nYogos were ultimately traced from the alluvium to their source. In February 1896, a sheepherder named Jim Ettien found the sapphire mother lode: the Yogo dike. Ettien was prospecting for gold, and found sapphires after washing gravel he found in a fissure within a limestone outcrop. Ettien staked two claims. The vein turned out to be long and several other miners promptly staked claims along it. Ettien sold his claims to Hoover; Hoover in turn sold his interest in eight original mining stakes, known as the \"New Mine Sapphire Syndicate\", to his two partners for $5,000 (approximately $ as of 2018). This site was from Yogo City. In 1899, Johnson, Walker and Tolhurst, Ltd. of London purchased the New Mine Sapphire Syndicate for $100,000 (approximately $ million as of 2018). At that point, the operation became unofficially known as the \"English Mine\".\n\nOn July 4, 1896, two other Americans, John Burke and Pat Sweeney, staked six mining claims on the western portion of the Yogo dike—areas Hoover had deemed unfit for mining. These claims were collectively known as the \"Fourth of July Claim\", and became known as the \"American Mine\". In 1904, the mine was bought by the American Gem Syndicate, and it sold in 1907 to the American Sapphire Company.\nOne of the Englishmen who came to the area was Charles Gadsden of Berkhamsted, Hertfordshire. By 1902, Gadsden was promoted to resident supervisor of the English Mine, and he quickly turned its focus from gold to sapphires. Gadsden's security measures were very tight, as weight-for-weight, rough sapphires were and continue to be worth much more than gold. The English Mine flourished until the 1920s, but floods on July 26, 1923, so severely damaged the mines that they never fully recovered. Between the aftermath of flooding and hard economic times, the English Mine finally failed in 1929. It had recovered more than of rough sapphires that produced of finished gems valued at $25 million in 1929 dollars (approximately $ million as of 2018). A series of other firms mined sapphires there, but with marginal success. For much of the 1930s and 1940s Gadsden worked the mine alone and used his own money to pay its property taxes. He remained caretaker of the mines until shortly before his death on March 11, 1954.\n\nThe American Mine operations were less profitable than those of the English Mine. While the English Mine used superior mining and management techniques on a richer lode, the American Mine suffered from insufficient space and lack of water for ore weathering. Roughs from the English Mine were shipped to London and sold in Europe, often with claims they were sapphires from the Far East, while the American Mine had difficulty marketing its gems within the United States. The American Sapphire Company, which used local gemcutters from Great Falls, went bankrupt in 1909; a new firm, the Yogo American Sapphire Company, bought the American Mine, but was bankrupt by 1913. Gadsden and his wife had convinced the New Mine Sapphire Syndicate to buy out the Yogo American Sapphire Company in 1914, and in doing so, the English syndicate gained control of all known Yogo deposits. They quickly recouped the purchase price by washing the tailings left behind by previous operators of the American Mine.\n\nMontana sapphires were heavily mined during for industrial abrasive and cutting purposes. However, because the Yogo mines were still owned by the English, the United States government could not control those operations, so the mines were little affected by the war, even though industrial sapphires were critical to the war effort. The Yogo Sapphire Mining Corporation of Billings, Montana, was the next company to try to run the English Mine. They made an initial offer in 1946, and reached a deal by 1949. However, the purchase was not complete until 1956 because of legal issues. The sale was finally completed for $65,000 cash and some stock considerations because the company's capital was exhausted, similar to previous Yogo ventures. The Yogo Sapphire Mining Corporation then changed its name to be the same as the former English firm's name: New Mine Sapphire Syndicate. It became informally known as the \"American Syndicate\" to distinguish it from the previous \"English Syndicate\". Production was poor and mining ceased in September 1959. From 1959 to 1963, the mine itself was left unattended and unsecured, resulting in hobbyists, picnickers, and rockhounds' coming from all over the US and Canada to gather loose rough sapphires. The American Syndicate took action to stop this in 1963, with fences and threats of prosecution. The American Syndicate then tried leasing the mine to several operators. One of these was Siskon, Inc. of Nevada, which lost a significant amount of money. They sued, and in May 1965 the Montana Supreme Court ruled in Siskon's favor. Siskon bought the mine at a sheriff's sale and in turn leased it to a group headed by Arnold Baron, who had a background in gemcutting and jewelry. Baron organized German and Thai gemcutters and had success in marketing Yogos in America—the first such success in 50 years. However, owing to the difficulty in mining the hard rock site, he did not exercise his option to buy the mine, and Siskon sold it in August 1968 to Herman Yaras of Oxnard, California, for $585,000.\nIn 1969, Yaras' Sapphire Village, Inc. created the Sapphire Village, a nearby homesite development offering buyers limited mining rights to gather their own sapphires with hand tools. Having done no significant mining or marketing, Sapphire Village, Inc. sold in 1973 to one of its investors, Chikara Kunisaki, a celery farmer from Oxnard, California. Kunisaki renamed the business Sapphire International Corporation and attempted to create a commercial mining operation. He built a modern tunnel at the site of the old American Mine, named the \"Kunisaki Tunnel\". But operation costs were so high that Sapphire International Corporation shut down in late 1976. This was the last actual attempt to mine the American Mine section of the Yogo dike, and today, only the locked portal to the tunnel still exists.\n\nIn January 1977, Victor di Suvero and his firm Sapphire-Yogo Mines became the next owner to tackle the Yogo dike. Di Suvero was a native-born Italian who grew up in Tientsin, China, and had been successful with a jade mine in California. Di Suvero's expertise was in marketing: he formed a company called Sapphire Trading to cut and market the Yogos. He had novel marketing ideas but was not knowledgeable about the mining side of the business. Unable to make payments, his venture folded in late 1979.\n\nBy 1980, only four American owners had been successful at Yogo Gulch, all early in its mining history. The English syndicate had been the most profitable of any venture, and even that venture was short-lived. At least thirteen American-owned Yogo mining efforts had failed. Besides inherent difficulties with financing and the challenges of hard rock mining, the American owners generally did not understand how to effectively market the gems.\n\nKunisaki put his mine up for sale, asking $6 million to recoup his expenses. Even though mine profits had been poor over the decades, prices of precious gems were very high at the time due to the worldwide oil crises of the 1970s and early 1980s. Four individuals or groups seriously considered Kunisaki's offer. Relying heavily upon Delmer Brown's expertise, Harry C. Bullock and J. R. Edington formed the limited partnership American Yogo Sapphire Limited, becoming the 14th American company to work the Yogo dike. Bullock and Brown had Yogo mine experience, as they had worked with di Suvero. Bullock's plan included mining, cutting, making jewelry, and marketing—the whole spectrum of the business. They paid the $6 million asked by Kunisaki and then raised another $7.2 million in funding by October 1981. Brown located quality gemcutters in Thailand, and set up the American Yogo Sapphire Company there. Brown also set up a thorough, computerized security system that tracked gems from the mine to the gemcutters. Bigger roughs were sent to American cutters, specialty cuts were done in Germany, a few cuts were done in Hong Kong, and the vast majority were done in Thailand. American Yogo Sapphire Limited secured a $5 million line of credit with Citibank. Desiring a more modern name, American Yogo Sapphire Limited changed its name to Intergem Limited in early 1982. Intergem marketed the Yogo as the \"Royal American Sapphire.\" Their first line of jewelry appeared in mid-1982, first marketed regionally in the American west and later at the national level. Intergem also developed a system of authorized dealers, and found success in its first four years, with sales over $3 million in 1984 alone.\nIntergem rocked the gem trade by marketing the Yogo as the world's only guaranteed untreated sapphire. By 1982, the practice of routinely heat treating gems had become a major issue in the industry. At the time, 95 percent of all the world's sapphires were being heated to enhance their natural color. Thai traders had even purchased large quantities of naturally colorless Sri Lankan sapphires, known as geuda, and heated them to an artificial blue. A problem with the practice was that heated gems often fade over time, though trained gemologists can detect a heated gem with 95 percent accuracy. Intergem's marketing of guaranteed untreated Yogos set them against many in the gem industry. In 1985 there was a movement in Pennsylvania to require disclosure that a gem had been treated. Intergem's strategy resulted in large numbers of gem professionals visiting Yogo Gulch.\n\nIntergem began planning to dig even deeper into the Yogo dike, which held more known reserves than all the world's other known sapphire deposits combined, albeit deep underground rather than near the surface in the manner of the other known deposits. They also set up a washing plant and maintenance sheds at the site of the former American mine. Intergem had made a $1.5 million down payment and agreed to make semi-annual payments to Kunisaki's Sapphire International Corporation, which had been renamed to Roncor. Intergem also had loan and interest payments on the $7.2 million loan to make to Citibank. While the company's sales were steadily increasing, their profits were still too low and in May 1985 they missed a $250,000 payment to Roncor. Simultaneously, their collateral of gems, held by Citibank, declined because the value of their collateral was declining; as a result, Citibank called in its loan. Intergem had over $1 million in sales lined up for the 1985 Christmas season, but could only fill a tiny portion because they did not have enough operating capital to manufacture the Yogo jewelry. In mid-1986, Roncor regained full ownership even though Intergem had sold loose gems and jewelry worth millions of dollars.\n\nVarious companies attempted to lease the mine from Roncor, but in the meantime, two local couples, Lanny and Joy Perry and Chuck and Marie Ridgeway, discovered a new site at Yogo Gulch in January 1984 by following a trail to an unused section of the dike that had previously been deemed unsuitable. They began mining the site and named it the \"Vortex Mine\", forming a company named Vortex Mining. The mine shaft was deep and contained two Yogo ore-bearing veins. The portion of the dike they had mined was an extension of the main dike. The Vortex Mine, renamed Yogo Creek Mining, was successful for years but eventually declined and closed in 2004.\nIn 1992, Roncor found an rough. AMAX Exploration, operating as the Yogo Sapphire Project, signed a 22-month lease with Roncor in March 1993 and had some success in the middle and eastern portions of the dike; it decided not to continue after the end of its lease due to the cost of underground mining, depletion of easily accessible Yogos, and the relatively small size of Yogos then easily accessible. During this time, additional dikes were found in the area using geophysical magnetometer surveys. Low-grade sapphire rough was found in the Eastern Flats Dike, a parallel dike some 500 feet northeast of the main dike. Pacific Cascade Sapphires, a Canadian company, had a mining lease with Roncor in 2000 and 2001 but ran out of funds and their option expired. By this time, most of the easily accessible Yogos had been mined and miners had to dig deeper, further increasing costs.\n\nIn 1995, Intergem's stock of gems began to reappear on the market because the company had paid its salesmen in sapphires during its financial demise. After Intergem collapsed, many of its salesmen continued to sell Yogos, especially after AMAX ceased operations. Citibank also had obtained a large stock of Yogos, reputedly worth $3.5 million (approximately $ as of 2018), as a result of Intergem's collapse: of rough, of cut gems, and 2,000 pieces of jewelry, all of which sat in the bank's vaults until 1991 when Sofus Michelsen, director of the Center for Gemstone Evaluation and creator of the \"Michelsen Gemstone Index\", became interested. In 1992, he and Jim Adair, a Missoula, Montana, jeweler who is the world's largest retailer of Yogos, got together, and by October 1994 Adair had purchased Citibank's four sealed bags of Yogo material. However, only one of the bags was truly valuable. Adair and Michelsen designed custom cutting techniques for Yogos.\n\nA new owner, Michael Duane Roberts, bought the Vortex Mine in 2008. Its operations were designed to be environmentally friendly, using methods such as recycling all water and not using other chemicals. Roberts died in a mining accident in 2012. , there was also mining activity by individual hobby miners on small parcels at Sapphire Village, but the Roncor mines remained inactive. In 2017, Vortex Mines was sold to Don Baide who plans to continue operations.\n\nSeveral Yogo sapphires are kept at the Smithsonian Institution. The earliest donations were noted in the museum's annual report on June 30, 1899, when the institution reported that Dr. L. T. Chamberlain gave them two cut Yogos and 21 other sapphires for their Dr. Isaac Lea gem and mineral collection. The record-setting cut Yogo is also held by the Smithsonian. In 2006, gemologist Robert Kane of Fine Gems International in Helena, which has the world's largest selection of Montana sapphires, donated 333 Montana sapphires, weighing a total of , to the Smithsonian's Gem and Mineral Collection, along with 98.48 grams of 18K yellow gold for the creation of a piece of jewelry. A representative of the Smithsonian asked Paula Crevoshay, a jewelry designer from Albuquerque, New Mexico, to create a piece of finished jewelry from these gems. Crevoshay felt that a butterfly motif would best represent America's natural beauty, honor her mother's love of butterflies, and display the wide range of colors found in Montana sapphires. Crevoshay named the brooch \"Conchita\" in honor of her mother; it is also referred to as the \"Sapphire Butterfly Brooch\", \"Conchita Sapphire Butterfly\", and the \"Montana Butterfly Brooch\". Two of the sapphires used are cabochon cut and the rest are brilliant cut. The majority are from the Rock Creek deposit. The largest one, however, is a blue Yogo used for the butterfly's head. Other sapphires used included yellow, purple, pink, and orange gems. Crevoshay completed the brooch in 2007; she and Kane presented the finished brooch to Smithsonian curator Jeffrey Post on May 7, 2007, in Washington, DC.\nIn the earliest years of Yogo sapphire mining, before Yogos achieved their own reputation, Oriental sapphires were sold in Montana with claims they were Yogos, while in Europe, Yogos were sold as Oriental sapphires. However, Yogos became notable in their own right. Paulding Farnham (1859–1927) used Yogos in several jewelry pieces he designed for the 1900 Exposition Universelle in Paris, where Yogo sapphires received a silver medal among all gems for color and clarity. An entry of uncut loose Yogo sapphires also won a bronze medal at the 1904 Louisiana Purchase Exposition in St. Louis, Missouri. Farnham was the creator of the most elaborate piece of jewelry ever made with Yogos, the life-size Tiffany Iris Brooch, a brooch ornament, which contains 120 Yogo sapphires set in platinum, and sold on March 17, 1900, for $6,906.84. In 1923, First Lady Florence Harding was given an \"all Montana\" ring made from a Yogo sapphire and Montana gold. In 1952, Gadsden gave cut Yogos to President Harry Truman, his wife Bess, and their daughter Margaret. Many Yogos were also sold in Europe, as some Yogo mining was conducted by British interests. Yogos may have been in the personal collections of some members of the British royal family in the 1910s, but promotional claims that Yogos are in any of the crown jewels of England cannot be conclusively proven or disproven. Claims that the gem in the engagement ring of Lady Diana Spencer and Kate Middleton is a Yogo are dubious; the gem is thought to be of Sri Lankan origin. The story that the gem is a Yogo can be traced to a 1984 \"Los Angeles Times\" article that described the ring as a sapphire, and quoted Intergem president Dennis Brown's claim that the gem may have come from a British-owned Yogo mine.\n\n\n\n"}
{"id": "35340238", "url": "https://en.wikipedia.org/wiki?curid=35340238", "title": "Zhoushan Island Overhead Powerline Tie", "text": "Zhoushan Island Overhead Powerline Tie\n\nThe Zhoushan Island Overhead Powerline Tie is a 220 kV three-phase AC interconnection of the power grid of Zhoushan Island with that of the Chinese mainland. It runs over several islands and consists of several long distance spans, the longest with a length of south of Damao Island. This span uses two pylons, which are the tallest electricity pylons in the world. The north tower on Damao Island (, ') was completed in 2009, and the south tower on Liangmao Island (, ') was completed in 2010. These pylons resemble those of the Messina Strait, but are steel-tube lattice structures.\n\n\n"}
