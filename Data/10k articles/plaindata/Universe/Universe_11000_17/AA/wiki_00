{"id": "1542", "url": "https://en.wikipedia.org/wiki?curid=1542", "title": "Amaranth", "text": "Amaranth\n\nAmaranthus, collectively known as amaranth, is a cosmopolitan genus of annual or short-lived perennial plants. Some amaranth species are cultivated as leaf vegetables, pseudocereals, and ornamental plants. Most of the \"Amaranthus\" species are summer annual weeds and are commonly referred to as pigweed. Catkin-like cymes of densely packed flowers grow in summer or autumn. Approximately 60 species are recognized, with inflorescences and foliage ranging from purple, through red and green to gold. Members of this genus share many characteristics and uses with members of the closely related genus \"Celosia\".\n\n\"Amaranth\" derives from Greek (), \"unfading\", with the Greek word for \"flower\", (), factoring into the word's development as \"amaranth\". \"Amarant\" is an archaic variant.\n\n\"Amaranthus\" shows a wide variety of morphological diversity among and even within certain species. Although the family (Amaranthaceae) is distinctive, the genus has few distinguishing characters among the 70 species included. This complicates taxonomy and \"Amaranthus\" has generally been considered among systematists as a \"difficult\" genus.\n\nFormerly, Sauer (1955) classified the genus into two subgenera, differentiating only between monoecious and dioecious species: \"Acnida\" (L.) Aellen ex K.R. Robertson and \"Amaranthus\". Although this classification was widely accepted, further infrageneric classification was (and still is) needed to differentiate this widely diverse group.\n\nCurrently, \"Amaranthus\" includes three recognized subgenera and 70 species, although species numbers are questionable due to hybridization and species concepts. Infrageneric classification focuses on inflorescence, flower characters and whether a species is monoecious/dioecious, as in the Sauer (1955) suggested classification. A modified infrageneric classification of \"Amaranthus\" was published by Mosyakin & Robertson (1996) and includes three subgenera: \"Acnida\", \"Amaranthus\", and \"Albersia\". The taxonomy is further differentiated by sections within each of the subgenera.\n\nSpecies include:\n\nOne cup (2.4 dl, 245 g) of cooked amaranth grain (from about 65 g raw) provides 251 calories and is an excellent source (20% or more of the Daily Value, DV) of protein, dietary fiber, and some dietary minerals. Amaranth is particularly rich in manganese (105% DV), magnesium (40% DV), iron (29% DV), and selenium (20% DV). Cooked amaranth leaves are a rich source of vitamin A, vitamin C, calcium, manganese, and folate. Amaranth does not contain gluten.\n\nAmaranth contains phytochemicals that may be antinutrient factors, such as polyphenols, saponins, tannins, and oxalates which are reduced in content and effect by cooking.\n\nKnown to the Aztecs as , amaranth is thought to have represented up to 80% of their energy consumption before the Spanish conquest. Another important use of amaranth throughout Mesoamerica was to prepare ritual drinks and foods. To this day, amaranth grains are toasted much like popcorn and mixed with honey, molasses, or chocolate to make a treat called , meaning \"joy\" in Spanish. Diego Durán described the festivities for , the name of which means \"left side of the hummingbird\" or \"hummingbird left-hand\" (Real hummingbirds feed on amaranth flowers). The Aztec month of (7 December to 26 December) was dedicated to . People decorated their homes and trees with paper flags; ritual races, processions, dances, songs, prayers, and finally human sacrifices were held. This was one of the more important Aztec festivals, and the people prepared for the whole month. They fasted or ate very little; a statue of the god was made out of amaranth seeds and honey, and at the end of the month, it was cut into small pieces so everybody could eat a little piece of the god. After the Spanish conquest, cultivation of amaranth was outlawed, while some of the festivities were subsumed into the Christmas celebration.\n\nBecause of its importance as a symbol of indigenous culture, its palatability, ease of cooking, and a protein that is particularly well-suited to human nutritional needs, interest in grain amaranth (especially \"A. cruentus\" and \"A. hypochondriacus\") revived in the 1970s. It was recovered in Mexico from wild varieties and is now commercially cultivated. It is a popular snack sold in Mexico, sometimes mixed with chocolate or puffed rice, and its use has spread to Europe and parts of North America. Amaranth and quinoa are not grasses and are called pseudocereals because of their similarities to cereals in flavor and cooking.\n\nSeveral species are raised for amaranth \"grain\" in Asia and the Americas.\n\nAncient amaranth grains still used include the three species, \"Amaranthus caudatus\", \"Amaranthus cruentus\", and \"Amaranthus hypochondriacus\". Although amaranth was cultivated on a large scale in ancient Mexico, Guatemala, and Peru, nowadays it is only cultivated on a small scale there, along with India, China, Nepal, and other tropical countries; thus, the potential exists for further cultivation in those countries, as well as in the U.S. In a 1977 article in \"Science\", amaranth was described as \"the crop of the future\". It has been proposed as an inexpensive native crop that could be cultivated by indigenous people in rural areas for several reasons:\n\nAmaranth species are cultivated and consumed as a leaf vegetable in many parts of the world. Four species of \"Amaranthus\" are documented as cultivated vegetables in eastern Asia: \"Amaranthus cruentus\", \"Amaranthus blitum, Amaranthus dubius\", and \"Amaranthus tricolor\".\n\nIn Indonesia and Malaysia, leaf amaranth is called . In the Philippines, the Ilocano word for the plant is ; the Tagalog word for the plant is or . In Uttar Pradesh and Bihar in India, it is called \"chaulai\" and is a popular green leafy vegetable (referred to in the class of vegetable preparations called \"saag\"). It is called \"chua\" in Kumaun area of Uttarakhand, where it is a popular red-green vegetable. In Karnataka in India, it is called \"harive\". It is used to prepare curries such as \"hulee, palya, majjigay-hulee\", and so on. In Kerala, it is called \"cheera\" and is consumed by stir-frying the leaves with spices and red chillies to make \"cheera thoran\". In Tamil Nadu, it is called \"mulaikkira\" and is regularly consumed as a favourite dish, where the greens are steamed, and mashed, with light seasoning of salt, red chili, and cumin. It is called \"keerai masial\". In \"Andhra Pradesh\", this leaf is added in preparation of a popular \"dal\" called in (Telugu). In Maharashtra, it is called \"shravani maath\" and is available in both red and white colour. In Orissa, it is called \"khada saga\", it is used to prepare \"saga bhaja\", in which the leaf is fried with chili and onions.\n\nIn China, the leaves and stems are used as a stir-fry vegetable, or in soups. In Vietnam, it is called and is used to make soup. Two species are popular as edible vegetable in Vietnam: (\"Amaranthus tricolor\") and or (\"Amaranthus viridis\").\n\nA traditional food plant in Africa, amaranth has the potential to improve nutrition, boost food security, foster rural development and support sustainable land care.\n\nIn Bantu regions of Uganda and western Kenya, it is known as \"doodo\" or \"litoto\". It is also known among the Kalenjin as a drought crop (\"chepkerta\"). In Lingala (spoken in the Congo), it is known as or . In Nigeria, it is a common vegetable and goes with all Nigerian starch dishes. It is known in Yoruba as a short form of (meaning make the husband fat) or (meaning \"we have money left over for fish\"). In the Caribbean, the leaves are called \"bhaji\" in Trinidad and \"callaloo\" in Jamaica, and are sautéed with onions, garlic, and tomatoes, or sometimes used in a soup called pepperpot soup. In Botswana, it is referred to as \"morug\" and cooked as a staple green vegetable.\n\nIn Greece, green amaranth (\"A. viridis\") is a popular dish called , or . It is boiled, then served with olive oil and lemon juice like a salad, sometimes alongside fried fish. Greeks stop harvesting the plant (which also grows wild) when it starts to bloom at the end of August.\n\nIn Brazil, green amaranth was, and to a degree still is, frequently regarded as an invasive species as all other species of amaranth (except the generally imported \"A. caudatus\" cultivar), though some have traditionally appreciated it as a leaf vegetable, under the names of or , which is consumed cooked, generally accompanying the staple food, rice and beans.\n\nMaking up about 5% of the total fatty acids of amaranth, squalene is extracted as a vegetable-based alternative to the more expensive shark oil for use in dietary supplements and cosmetics.\n\nThe flowers of the 'Hopi Red Dye' amaranth were used by the Hopi (a tribe in the western United States) as the source of a deep red dye. Also a synthetic dye was named \"amaranth\" for its similarity in color to the natural amaranth pigments known as betalains. This synthetic dye is also known as Red No. 2 in North America and E123 in the European Union.\n\nThe genus also contains several well-known ornamental plants, such as \"Amaranthus caudatus\" (love-lies-bleeding), a vigorous, hardy annual with dark purplish flowers crowded in handsome drooping spikes. Another Indian annual, \"A. hypochondriacus\" (prince's feather), has deeply veined, lance-shaped leaves, purple on the under face, and deep crimson flowers densely packed on erect spikes.\n\nAmaranths are recorded as food plants for some Lepidoptera (butterfly and moth) species including the nutmeg moth and various case-bearer moths of the genus \"Coleophora\": \"C. amaranthella\", \"C. enchorda\" (feeds exclusively on \"Amaranthus\"), \"C. immortalis\" (feeds exclusively on \"Amaranthus\"), \"C. lineapulvella\", and \"C. versurella\" (recorded on \"A. spinosus\").\n\nAmaranth weed species have an extended period of germination, rapid growth, and high rates of seed production, and have been causing problems for farmers since the mid-1990s. This is partially due to the reduction in tillage, reduction in herbicidal use and the evolution of herbicidal resistance in several species where herbicides have been applied more often. The following 9 species of \"Amaranthus\" are considered invasive and noxious weeds in the U.S and Canada: \"A. albus\", \"A. blitoides\", \"A. hybridus\", \"A. palmeri\", \"A. powellii\", \"A. retroflexus\", \"A. spinosus\", \"A. tuberculatus\", and \"A. viridis\".\n\nA new herbicide-resistant strain of \"Amaranthus palmeri\" has appeared; it is glyphosate-resistant and so cannot be killed by herbicides using the chemical. Also, this plant can survive in tough conditions. The species \"Amaranthus palmeri\" (Palmer amaranth) causes the greatest reduction in soybean yields and has the potential to reduce yields by 17-68% in field experiments. Palmer amaranth is among the \"top five most troublesome weeds\" in the southeast of the United States and has already evolved resistances to dinitroaniline herbicides and acetolactate synthase inhibitors. This makes the proper identification of \"Amaranthus\" species at the seedling stage essential for agriculturalists. Proper weed control needs to be applied before the species successfully colonizes in the crop field and causes significant yield reductions.\n\nAn evolutionary lineage of around 90 species within the genus has acquired the carbon fixation pathway, which increases their photosynthetic efficiency. This probably occurred in the Miocene.\n\n\n\n"}
{"id": "32221795", "url": "https://en.wikipedia.org/wiki?curid=32221795", "title": "Andrea Rossi (entrepreneur)", "text": "Andrea Rossi (entrepreneur)\n\nAndrea Rossi (born 3 June 1950) is an Italian entrepreneur who claims to have invented a cold fusion device.\n\nIn the 1970s, Rossi claimed to have invented a process to convert organic waste into petroleum, and in 1978 he founded a company named Petroldragon to process waste. In the 1989 the company was shut down by the Italian government amid allegations of fraud, and Rossi was arrested. In 1996 Rossi moved to the United States and from 2001 to 2003 he worked under a U.S. Army contract to make a thermoelectric device that, while promising to be superior to other devices, produced only around 1/1000 of the claimed performance.\n\nIn 2008 Rossi attempted to patent a device called an Energy Catalyzer (or E-Cat), which is a purported cold fusion or Low-Energy Nuclear Reaction (LENR) thermal power source. Rossi claims that the device produces massive amounts of excess heat that can be used to produce electricity, but independent attempts to reproduce the effect have failed.\n\nAndrea Rossi was born on June 3, 1950 in Milan.\nIn 1973, Rossi graduated in philosophy at the University of Milan writing a thesis on Albert Einstein's theory of relativity and its interrelationship with Edmund Husserl's phenomenology.\n\nAndrea Rossi is married to Maddalena Pascucci.\n\nIn 1974, Rossi registered a patent for an incineration system. In 1978, he wrote \"The Incineration of Waste and River Purification\", published in Milan by Tecniche Nuove.\nHe then founded Petroldragon, a company that was paid to process toxic waste, claiming to use Rossi's process to convert the waste into usable petroleum products.\nIn 1989 Italian customs seized several Petroldragon waste deposit sites and assets. Investigations showed that petroleum presumably produced by the company was never been placed on the market, and that mixtures of toxic waste and harmful chemical solvents were being stored in silos or illegally dumped into the environment. Rossi himself was arrested and eventually tried on 56 counts, 5 of which ended in convictions related to tax fraud. Rossi wrote that he was acquitted in the other 51 trials. As of 2004 the government of Lombardy has spent over forty million euros to dispose of the 70,000 metric tons of toxic waste that Petroldragon had improperly dumped. According to the mayor of Lacchiarella, Luigi Acerbi, \"In the years when [Rossi] was working here, he didn't produce a single drop of oil, as far as we know.\"\n\nIn the US Rossi started the consulting firm Leonardo Technologies, Inc. (LTI). He secured a defense contract to evaluate the potential of generating electricity from waste heat by using thermoelectric generators. Such devices are normally only used for heating or cooling (Peltier effect), because the efficiency for generating electrical power is only a few percent. Rossi suggested that his devices could attain 20% efficiency. Larger modules would be manufactured in Italy. Rossi sent 27 thermoelectric devices for evaluation to the Engineer Research and Development Center; 19 of these did not produce any electricity at all. The remaining units produced less than 1 watt each, instead of the expected 800–1000 watts.\n\nIn January 2011, Andrea Rossi and Sergio Focardi claimed to have successfully demonstrated commercially viable nuclear power in a device called an Energy Catalyzer. The international patent application received an unfavorable international preliminary report on patentability because it seemed to \"offend against the generally accepted laws of physics and established theories\" and to overcome this problem the application should have contained either experimental evidence or a firm theoretical basis in current scientific theories. Journalists were not allowed to examine the core of the reactor, and there is uncertainty about the viability of the invention.\n\n"}
{"id": "9355858", "url": "https://en.wikipedia.org/wiki?curid=9355858", "title": "BC Oil and Gas Commission", "text": "BC Oil and Gas Commission\n\nThe BC Oil and Gas Commission is a Crown Corporation of the province of British Columbia, Canada, established in 1998. Its mandate is to regulate oil and gas activities and pipelines in British Columbia. Their mandate does not extend to regulating consumer gas prices at the pump.\n\nThe Oil and Gas Commission (OGC) was created and defined under the 1998 \"Oil and Gas Commission Act\" by and for the Canadian province of British Columbia.\nThe OGC is a crown corporation acting as \"an agent of the [provincial] government\", where the Minister of Finance is its fiscal agent. It is headed by 3 directors.The overall employees that are working would be 659. The deputy minister is a director and the chair of the OGC, and the Lieutenant Governor in Council may appoint 2 directors, for a term not longer than 5 years, one of whom is the commissioner and vice chair of the commission.\n\nThe OGCs purposes are to\n\n(a) regulate fossil fuel activities, i.e. the petroleum industry, which includes but is not limited to petroleum licensing, regulating hydrocarbon exploration, oil and gas well drilling, shale oil extraction, natural gas processing, oil pipeline in British Columbia in a manner that\n(b)\" provide for effective and efficient processes for the review of applications related to oil and gas activities or pipelines, and to ensure that applications that are approved are in the public interest having regard to environmental, economic and social effects\",\n\n(c) \"encourage the participation of First Nations and aboriginal peoples in processes affecting them\",\n\n(d) \"participate in planning processes\", and\n\n(e) \"undertake programs of education and communication in order to advance safe and efficient practices and the other purposes of the commission.\"\n\nThe OGC issues various authorizations under the \"Petroleum and Natural Gas Act\" and the \"Pipeline Act,\" including a \"general development permit\" which is \"an approval in principle for oil and gas activities and pipelines in an area of British Columbia\".\n\nThe OGC is a single window regulator It handles applications and at the same checks the compliance and enforces its regulations with penalties for violations.\n\nThe OGC has offices in four cities: Fort St. John, Fort Nelson, Kelowna and Victoria.\n\nIn 2010-11, the OGC \"issued 15 penalty tickets with fines of $575 (the maximum allowed for tickets) or less, which included unlawful water withdrawals and failure to promptly report a spill. Court prosecutions included a $20,000 fine for a Water Act stream violation, $10,575 for another stream violation and $250,0000 for a sour gas release. [...] The commission would not release the names of the companies convicted\".\nPer the OGC, in 2O12, of \"more than 800 deficiencies, 80 resulted in charges, largely under the provincial Water Act for the non-reporting of water volumes and a smaller portion under the provincial Environment Management Act. Another 13 resulted in orders under the provincial Oil and Gas Activities Act, 22 in warnings, 76 in letters requiring action and three in referrals to other agencies\".\nPaul Jeakins, OGC commissioner and CEO, has publicly acknowledged that OGC inspection and enforcement reports are \"a bit of a gap\".\n\nIn November 2013, Ecojustice, the Sierra Club and the Wilderness Committee filed a lawsuit against the OGC and Encana about Encana's water use from lakes and rivers for its hydraulic fracturing for shale gas, \"granted by repeated short-term water permits, a violation of the provincial water act\". In 2012, the OGC had granted Encana access to 20.4 million cubic metres of surface water, 7 million of which were for fracking and 54% of that were through short-term approvals. In October 2014 the Supreme Court of British Columbia found no violation and dismissed the case.\n\nThe agency has been criticized to be \"too industry-friendly\", to have \"vague regulations\" and to issue non transparent fracking violation reports, for example by not naming convicted companies. The B.C. Ministry of Environment and other B.C. Crown corporations of B.C. like WorkSafeBC have reported company names and details of those penalties for years. OGC reports prior to 2011 were available on the OGC website, but for 2012 they had to be requested.\n\n\n"}
{"id": "29581529", "url": "https://en.wikipedia.org/wiki?curid=29581529", "title": "Barqi Tojik", "text": "Barqi Tojik\n\nBarqi Tojik is a national integrated power company of Tajikistan. The chairman of the company is Rustam Rahmatzoda.\n\nBarqi Tojik operates the Nurek Hydroelectric Power Plant, the largest station in Central Asia with an installed generation capacity of 3 gigawatts (GW) and produces over 75% of Tajikistan’s electricity.\n"}
{"id": "8102393", "url": "https://en.wikipedia.org/wiki?curid=8102393", "title": "Bauernroulette", "text": "Bauernroulette\n\nBauernroulette is a game that was apparently invented in Germany, where several companies sell it. The name Bauernroulette indicates it is a \"poor man's roulette\", since Bauer is German for 'peasant', 'farmer' or 'one of a chiefly European class of agricultural laborers'. In Bauernroulette, a spinning top is spun in the middle of a wooden circular playing surface that contains 6 wooden balls. The balls bounce off the top in random directions, and sometimes land within one of several hollow indentations within the surface, or pass through a small hole into chambers that are located outside the spinning surface area. Typically, the most points are scored by landing balls within these outer chambers. Skilled players are often able to spin the top such that it remains in motion for more than 30 seconds.\n\n\nThe players can also stipulate that the score reached by the green and white balls is only valid if the red one also scores.\n\n"}
{"id": "11292083", "url": "https://en.wikipedia.org/wiki?curid=11292083", "title": "Being Caribou", "text": "Being Caribou\n\nBeing Caribou is a 2005 documentary film that chronicles the travels of husband and wife Karsten Heuer and Leanne Allison following the migration of the Porcupine caribou Herd, in order to explore the Arctic Refuge drilling controversy. The journey lasted 5 months, starting from the community of Old Crow, Yukon on April 8, 2003 and ending September 8, 2003. The film is produced by the National Film Board of Canada.\n\nKarsten Heuer documented this trek with Leanne Allison, which was also their honeymoon, in his book \"Being Caribou: Five Months on Foot with an Arctic Herd\". The book was published in 2005.\n\nAllison, an environmentalist, and Heuer, a wildlife biologist, follow a herd of 120,000 caribou on foot, across 1,500 kilometres (900 Miles) of Arctic tundra, in order to raise awareness of threats to the caribou's survival. At stake is the herd's delicate habitat, which is threatened by proposed petroleum and natural gas development in the herd's calving grounds in Alaska's Arctic National Wildlife Refuge.\n\nWinner of approximately 20 awards and honours, including a Gemini Award and most popular Canadian film at the Vancouver International Film Festival.\n\n\n"}
{"id": "21383115", "url": "https://en.wikipedia.org/wiki?curid=21383115", "title": "Blue Investment Baia Wind Farm", "text": "Blue Investment Baia Wind Farm\n\nThe Baia Wind Farm is an under construction wind power project in Baia, Tulcea County, Romania. It will have 14 individual wind turbines with a nominal output of around 2.5 MW which will deliver up to 35 MW of power, enough to power over 21,000 homes, with a capital investment required of approximately US$84 million.\n"}
{"id": "1198458", "url": "https://en.wikipedia.org/wiki?curid=1198458", "title": "Boiling chip", "text": "Boiling chip\n\nA boiling chip, boiling stone, porous bits or anti-bumping granule is a tiny, unevenly shaped piece of substance added to liquids to make them boil more calmly. Boiling chips are frequently employed in distillation and heating. When a liquid becomes superheated, a particle of dust or a stirring rod can cause violent flash boiling. Boiling chips work by providing nucleation sites so the liquid boils smoothly without becoming superheated or bumping.\n\nBoiling chips should not be added to liquid that is already near its boiling point, as this could also induce flash boiling.\n\nThe structure of a boiling chip traps liquid while in use, meaning that they cannot be re-used in laboratory setups. They also don't work well under vacuum; if a solution is boiling under vacuum, it's best to constantly stir it instead.\n\nBoiling chips are typically made of a porous material, such as alumina, silicon carbide, calcium carbonate, calcium sulfate, porcelain or carbon, and often have a nonreactive coating of PTFE. This ensures that the boiling chips will provide effective nucleation sites, yet are chemically inert. In less demanding situations, like school laboratories, pieces of broken porcelainware or glassware are often used.\n"}
{"id": "18729164", "url": "https://en.wikipedia.org/wiki?curid=18729164", "title": "Borzești Power Station", "text": "Borzești Power Station\n\nThe Borzeşti Power Station is a large thermal power plant located in Borzeşti, having 7 generation groups, 3 of 25 MW, 2 of 50 MW, 1 of 60 MW and 1 of 210 MW having a total electricity generation capacity of 655 MW.\n\n"}
{"id": "12348732", "url": "https://en.wikipedia.org/wiki?curid=12348732", "title": "Caliper log", "text": "Caliper log\n\nA caliper log is a well logging tool that provides a continuous measurement of the size and shape of a borehole along its depth and is commonly used in hydrocarbon exploration when drilling wells. The measurements that are recorded can be an important indicator of cave ins or shale swelling in the borehole, which can affect the results of other well logs.\n\nThe caliper tool measures the variation in borehole diameter as it is withdrawn from the bottom of the hole, using two or more articulated arms that push against the borehole wall. Each arm is typically connected to a potentiometer which causes the resistance to change as the diameter of the borehole changes, creating a varying electrical signal that represents the changing shape of the borehole. This variation in output is translated into changes of diameter after a simple calibration and the caliper log is printed as a continuous series of values of hole diameter with depth.\n\nKnown challenges with caliper logging include borehole spiralling. The position of the drill bit may precess as it drills, leading to spiraling shapes in the wellbore wall, as if the hole had been drilled by a screw. If the arms of the caliper log follow the grooves of the spiral, it will report too high an average diameter. Moving in and out of the grooves, the caliper will give erratic or periodically varying readings.\n\nIn most cases, the borehole's circumference will not be a perfect circle and therefore a caliper tool with several arms is required to obtain a true understanding of the size and shape of the borehole. The borehole can change to an oval shape after drilling, which can cause the caliper log to overestimate the size of the borehole if its arms get stuck axis.\n"}
{"id": "35071429", "url": "https://en.wikipedia.org/wiki?curid=35071429", "title": "Cesare Bortolotti", "text": "Cesare Bortolotti\n\nCesare Bortolotti (1950–1990) was an Italian entrepeuner that was president of Atalanta B.C from 1980 to 1990.\n\nSon of Achille in 1980 he was named president of Atalanta, the team was playing in Serie B, the next season the team was relegated in Serie C1. In the following years the team was soon promoted in the highest leagues and in June 1985 Atalanta reached the Serie A.\nDuring his tenure as president Atalanta reached the semifinal of European Cup Winners where Atalanta was defeated by Malines.\nIn 1990, he was killed in a car accident near his house on the shore of Lake of Iseo.\n"}
{"id": "7747964", "url": "https://en.wikipedia.org/wiki?curid=7747964", "title": "Cruising rod", "text": "Cruising rod\n\nA cruising rod is a simple device used to quickly estimate the number of pieces of lumber yielded by a given piece of timber. Similarly to a yardstick, it is a rod with markings. The estimation is carried out as follows. Standing at arm's length from the tree, estimate its average diameter by taking a note on the rod's markings. Walk away to see the whole tree; hold the rod upright at the distance from the eye at which the rod and the tree appear of the same diameter; the noted mark on the rod will show an approximate location of an 8-foot log cut along the tree height. \n\n"}
{"id": "42365457", "url": "https://en.wikipedia.org/wiki?curid=42365457", "title": "Dasu Dam", "text": "Dasu Dam\n\nThe Dasu Dam is a gravity dam currently being constructed on the Indus River near Dasu in Kohistan District, Khyber Pakhtunkhwa Province, Pakistan. The tall dam will support a 4,320 MW hydroelectric power station which will be built in two 2,160 MW stages. Water from the reservoir will be diverted to the power station located about downstream. The first stage was approved by the Executive Committee of the National Economic Council on 29 March 2014. That stage will cost an estimated US$4.278 billion. Funding is being provided by the World Bank (US$700 million), the Industrial and Commercial Bank of China (US$1.5 billion), Deutsche Bank (US$1 billion) and Aga Khan Development Network (US$500 Million). Completion of stage one is expected in March 2019. Prime Minister Nawaz Sharif attended the dam's groundbreaking ceremony on 25 June 2014.\n\nPakistan Water and Power Development Authority (Wapda) on December 15th, 2015 has signed a contract worth Rs571.95 million with China Railway First Group for construction at resettlement sites and building the Shatial museum – two schemes that are part of the Dasu hydroelectric power project.\n\nThe contract includes construction at three sites for the resettlement of displaced people belonging to Choochang village and developing an open-air museum at the archeological site of Shatial to conserve pre-historic rock carvings. Work on these schemes is expected to be completed in one and a half years.\n\nThe main works of DASU Hydropower project are being constructed by China Gezhouba Group Company Limited .\n"}
{"id": "381669", "url": "https://en.wikipedia.org/wiki?curid=381669", "title": "Delta Air Lines Flight 191", "text": "Delta Air Lines Flight 191\n\nDelta Air Lines Flight 191 was a regularly scheduled Delta Air Lines domestic service from Fort Lauderdale, Florida, to Los Angeles with an intermediate stop at Dallas/Fort Worth International Airport (DFW). On August 2, 1985, the Lockheed L-1011 TriStar operating Flight 191 encountered a microburst while on approach to land at DFW. The aircraft struck the ground over a mile short of the runway, struck a car near the airport, and then collided with two water tanks and disintegrated. The crash killed 137 people and injured 28 others. The National Transportation Safety Board (NTSB) determined that the crash resulted from the flight crew's decision to fly through a thunderstorm, the lack of procedures or training to avoid or escape microbursts, and the lack of hazard information on wind shear.\n\nThe aircraft was a Lockheed L-1011-385-1 TriStar (registration number \"N726DA\"). It was delivered to Delta on February 28, 1979, and the airline had operated it continuously since that date. Three Rolls-Royce RB211-22B engines powered the aircraft.\n\nThree flight crew and eight cabin crew members manned Flight 191. \n\nThe captain, Edward N. Connors, age 57, had been a Delta Air Lines employee since 1954. He qualified to captain the TriStar in 1979 and had passed his proficiency checks.. The NTSB report mentioned that other flight crew that had flown with Connors prior to the accident described him as a meticulous pilot who strictly adhered to company policies.. The report also stated that Connors \"deviated around thunderstorms even if other flights took more direct routes\" and \"willingly accepted suggestions from his flightcrew.\" Since his qualification in 1979, Connors had passed all eight en route inspections he had undergone, and the NTSB report notes that he had received \"favorable comments\" regarding \"cockpit discipline and standardization.\". Connors had logged over 29,300 hours of flight time, 3,000 of which had been in the TriStar.\n\nFlight 191's first officer was Rudolph P. Price Jr, age 42. Delta captains who flew with Price described him as an \"above average first officer\" and possessing \"excellent knowledge\" of the TriStar. Price had logged 6,500 flight hours, including 1,200 in the TriStar.. The flight engineer, Nick N. Nassick, age 43, had logged 6,500 hours of flight time, including 4,500 in the TriStar. Fellow Delta employees described him as \"observant, alert, and professional.\"\n\nEight flight attendants were aboard the flight: Frances Alford, Jenny Amatulli, Freida Artz, Vicki Chavis, Diane Johnson, Alyson Lee, Joan Modzelewski, and Wendy Robinson. Flight attendants Jenny Amatulli, Vicki Chavis, and Wendy Robinson were the only crew members who survived. \n\nOf the dead, 73 originated from the Miami metropolitan area. Of them, 45 were from Broward County, 19 were from Palm Beach County, and 9 were from Dade County. One of the passengers was Don Estridge, known to the world as the father of the IBM PC; he died aboard the flight along with his wife, Mary Ann, two IBM summer interns, and six additional family members of IBM employees. The NTSB lists 126 passenger fatalities, but notes that an additional 2 of the passengers listed as survivors died more than 30 days after the crash.\n\nFlight 191 was a regularly scheduled passenger flight from Fort Lauderdale–Hollywood International Airport in Fort Lauderdale, Florida to Los Angeles International Airport in Los Angeles, California, with a scheduled stop at Dallas/Fort Worth International Airport. The flight departed Fort Lauderdale on an instrument flight rules (IFR) flight plan at 14:10 Central Daylight Time (). The flight's dispatch weather forecast for DFW stated a \"possibility of widely scattered rain showers and thunderstorms.\" Another dispatch weather alert warned of \"an area of isolated thunderstorms ... over Oklahoma and northern and northeastern Texas.\" The flight crew reviewed these notices before takeoff.\n\nAs the aircraft flew past New Orleans, Louisiana, a weather formation near the Gulf Coast strengthened. The flight crew decided to deviate from the intended route to make the more northerly Blue Ridge arrival to DFW. The flight held for 10–15 minutes over the Texarkana, Arkansas VORTAC. At 17:35, the crew received an Automatic Terminal Information Service (ATIS) broadcast for weather on approach to DFW, and the Fort Worth Air Route Traffic Control Center (ARTCC) air traffic controller cleared the flight to the Blue Ridge, Texas VORTAC and instructed the flight to descend to .\n\nAt 17:43:45, the Fort Worth ARTCC controller cleared the flight down to . The controller suggested they fly a heading of 250° toward the Blue Ridge approach, but Captain Connors replied that the route would take them through a storm cell, stating, \"I'd rather not go through it, I'd rather go around it one way or the other.\" After a brief exchange, the controller gave the flight a new heading. At 17:46:50, the controller cleared the flight direct to Blue Ridge and instructed the flight crew to descend to . The captain expressed his relief that the controller didn't send them on the original trajectory. At 17:51:19, the second officer commented, \"Looks like it's raining over Fort Worth.\" At 17:51:42, the Fort Worth ARTCC controller transferred the flight to DFW Airport Approach Control, which cleared the flight to descend to . Two minutes later, the controller asked the Delta flight to deviate by ten degrees and to slow their airspeed to . The flight acknowledged the request. As the flight descended, the crew prepared the aircraft for landing. At 17:56:19, the feeder controller cleared the flight down to . Nine seconds later, the controller announced that there was rain north of the airport, and that the airport would be using instrument landing system (ILS) approaches.\n\nAt 17:59:47, Price said, \"We're gonna get our airplane washed.\" At around the same time, the captain switched to the arrival radio frequency and informed the approach controller that they were flying at . The controller replied that the flight should expect to approach Runway 17L (now Runway 17C). At 18:00:36, the approach controller asked an American Airlines flight that was two aircraft ahead of Flight 191, and on the same approach, if they could see the airport. The flight responded, \"As soon as we break out of this rain shower we will.\" At 18:00:51, Flight 191 was instructed to slow to and to turn to heading 270°. Flight 191 was instructed to descend to at 18:01:34. One minute later, the approach controller turned the flight toward Runway 17L and cleared them for an ILS approach at or above . Half a minute afterward, the controller asked the flight to reduce their speed to , which the flight crew acknowledged. At 18:03:30 the controller advised, \"And we're getting some variable winds out there due to a shower ... out there north end of DFW.\" Several seconds later, an unidentified flight crew member commented, \"Stuff is moving in.\"\n\nJust three miles ahead of Flight 191 was a Learjet 25 on the same approach to Runway 17L. While on final approach, the Learjet flew through the storm north of the airport and encountered what was later described as \"light to moderate turbulence\". The Learjet encountered heavy rain and lost all forward visibility, but was able to continue its ILS approach and land safely. When later asked why he did not report weather conditions to the tower, the Learjet's captain testified that he had nothing to report because \"the only thing that we encountered was the heavy rain.\" The tower controller handling landings on Runway 17L saw lightning from the storm cell after the Learjet landed, but before he saw Flight 191 emerge from the storm.\n\nAt 18:03:46, the approach controller once again asked Flight 191 to reduce its speed, this time to , and then handed the flight over to the tower controller. Twelve seconds later, the captain radioed the tower and said, \"Tower[:] Delta one ninety one heavy, out here in the rain, feels good.\" The tower controller advised Flight 191 that the wind was blowing at with gusts up to , which the captain acknowledged. The flight crew lowered the landing gear and extended their flaps for landing. At 18:04:18, Price commented, \"Lightning coming out of that one. ... Right ahead of us.\" The captain called out that they were at at 18:05:05. Fourteen seconds later, he cautioned Price to watch his airspeed. At the same time, the cockpit voice recorder (CVR) captured the beginning of a sound identified as rain hitting the cockpit. The captain warned Price, \"You're gonna lose it all of a sudden, there it is.\" At 18:05:26, the captain told Price, \"Push it up, push it way up.\" Several seconds later, the CVR recorded the sound of the engines spooling up. Connors then said, \"That's it.\" At 18:05:36, Connors exclaimed, \"Hang on to the son of a bitch!\" From this point, the aircraft began a descent from which it never recovered. The angle of attack (AOA) was over 30° and began to vary wildly over the next few seconds. The pitch angle began to sink and the aircraft started descending below the glideslope.\n\nAt 18:05:44, with the aircraft descending at more than per second the ground proximity warning system (GPWS) began a series of \"whoop whoop pull up\" audible warnings. The captain responded by declaring \"TOGA\", aviation shorthand for the order to apply maximum thrust and abort a landing by going around. The first officer responded by pulling up and raising the nose of the aircraft, which slowed but did not stop the plane's descent. At 18:05:52, still descending at a rate of approximately per second, the aircraft's landing gear made contact with a plowed field north of the runway and east of the runway centerline. Remaining structurally intact, Flight 191 remained on the ground while rolling at high speed across the farmland. The main landing gear left shallow depressions in the field that extended for before disappearing and reappearing a couple of times as the aircraft approached Texas State Highway 114.\n\nThe aircraft struck a highway street light, and its nose gear touched down on the westbound lane of Highway 114, skidding across the road at least 200 mph. The aircraft's left engine hit a Toyota Celica driven by 28-year-old William Mayberry, killing him instantly. As the aircraft continued south, it hit two more street lights on the eastbound side of the highway and began fragmenting. The left horizontal stabilizer, some engine pieces, portions of the wing control surfaces, and parts of the nose gear came off of the aircraft as it continued along the ground. Some witnesses later testified that fire was emerging from the left wing root. Surviving passengers reported that fire began entering the cabin through the left wall while the plane was still moving. A survivor stated that he watched passengers attempt to escape the fire by unbuckling their seatbelt and try to flee but were sucked out of the plane, while others who stayed caught on fire due to leaking jet fuel, he only survived due to being dosed by rain from openings in the plane. The aircraft's roll across open land ended when it crashed into a pair of water tanks on the edge of the airport property; the aircraft grazed one water tank about south of Highway 114, and then struck the second water tank. As the left wing and nose struck the water tank, the fuselage rotated counterclockwise and was engulfed in a fireball. The fuselage from the nose rearward to row 34 was destroyed. The tail section emerged from the fireball, skidding backwards, and came to rest on its left side before wind gusts rotated it upright.\n\nAll airport fire and emergency units were alerted within one minute of the crash. 45 seconds after first being alerted, three fire trucks from the airport's fire station No. 3 arrived at the crash and began fighting the fire. Additional units from fire stations No. 1 and No. 3 arrived within 5 minutes, and despite high wind gusts and heavy rain, the fire was mostly under control within 10 minutes after the alert was sounded.\n\nThe first paramedics arrived within 5 minutes of the crash, and immediately established triage stations. In later testimony to NTSB officials, on-site EMTs estimated that without the on-scene triage procedures, at least half of the surviving passengers would have died. Most of the survivors of Flight 191 were located in the aircraft's rear smoking section, which broke free from the main fuselage before the aircraft hit the water tanks. Authorities transported most of the survivors to Parkland Memorial Hospital.\n\nThe cockpit and passenger section forward of seat row 34 had been completely fragmented by impact with the water tanks and post-crash fires; all but eight of the occupants in this section were killed. The remainder of the surviving passengers and crew were in the rear cabin and tail section, which separated relatively intact and landed on its side in an open field. Overall, the disintegration of the Tristar was so extensive that the NTSB investigation was quite difficult. Survivors reported that fire broke out in the cabin prior to hitting the tanks, and began spreading through the aircraft's interior, which is consistent with the right wing's collision with the light pole and fuel tank ignition. Some of the people in the tail section were unable to free themselves due to injuries and rescue crews had to extricate them. Most survivors were also soaked with jet fuel, further adding to the difficulty of exiting the wreckage.\n\nTwo of the passengers who initially survived the crash died more than thirty days later. On the ground, an airline employee who assisted in rescuing survivors was hospitalized overnight for chest and arm pain. The crash ultimately killed 137 people, including 128 of the 152 passengers and 8 of the 11 crew, and the driver of the car.\n\nDelta Air Lines Flight 191 has the second-highest death toll of any aviation accident involving a Lockheed L-1011 anywhere in the world, after Saudia Flight 163.\n\nNumerous public safety agencies responded to the crash, including the Dallas/Fort Worth Airport Department of Public Safety, the Texas Department of Public Safety, the Irving Fire Department, the Irving Police Department and all available third-watch personnel from the Dallas Police Department's Northwest Patrol Division and the Northeastern Sector of the Fort Worth Police Department's Patrol Division.\n\nAfter a long investigation, the National Transportation Safety Board deemed the cause of the crash to be attributable to pilot error (for their decision to fly through a thunderstorm), combined with extreme weather phenomena associated with microburst-induced wind shear. The NTSB also determined that a lack of specific training, policies, and procedures for avoiding and escaping low-altitude wind shear was a contributing factor.\n\nThe NTSB attributed the accident to lack of the ability to detect microbursts aboard aircraft – the radar equipment aboard aircraft at the time was unable to detect wind changes, only thunderstorms. After the investigation, NASA researchers at Langley Research Center modified a Boeing 737-200 as a testbed for an on-board Doppler weather radar. The resultant airborne wind shear detection and alert system was installed on many commercial airliners in the United States after the FAA mandated that all commercial aircraft must have on-board windshear detection systems.\n\nThe NTSB was also critical of the airport for failing to notify emergency services in surrounding municipalities in a timely manner. While the airport's on-site emergency services were notified almost immediately, the DFW Department of Public Safety (DPS) Communications Center did not begin notifying off-site emergency services until nearly 10 minutes after the crash, and did not finish its notifications until 45 minutes after the crash. During notifications, DPS also failed to request ambulances from the adjacent communities of Irving, Grapevine, and Hurst; however, Hurst responded with ambulances after personnel at its ambulance company overheard the airport crash report on a radio-frequency scanner. The NTSB concluded that the overall emergency response was effective due to the rapid response of on-airport personnel, but found \"several problem areas\" which under different circumstances \"could affect adversely the medical treatment and survival of accident victims at the airport\".\n\nFollowing the crash and the ensuing NTSB report, DFW's DPS made improvements to its post-crash notification system, including the introduction of an automated voice notification system to reduce notification times. In 1988, following the crash of Delta Air Lines Flight 1141 while taking off from DFW, DPS completed its notification of nearby emergency services in 21 minutes; the NTSB described this as a \"significant improvement\" over response times after the Delta Flight 191 crash. Based on the improved response times, the NTSB issued a Safety Recommendation on January 9, 1990, calling for airport executives nationwide to consider the benefits of using automated voice notification systems for their emergency aid notifications. Pilots were also requried to train to react to microbursts and to quickly take evasive action in order to safely land the plane.\n\nThe Delta Flight 191 crash resulted in the longest aviation trial in American history, lasting fourteen months from 1988 to 1989 and presided over by federal judge David Owen Belew Jr. The trial featured the first use of computer graphic animation as substantive evidence in federal court; while the use of such animation is now routine, its use in the Flight 191 litigation was novel enough that it became the featured cover story of a 1989 issue of \"ABA Journal\", the magazine of the American Bar Association. Preparing the animated video for trial cost the Department of Justice around $100,000 to $150,000, and required nearly two years of work. The court found that both government personnel and the Delta flight crew were negligent, but that Delta was ultimately responsible because its pilots' negligence was the proximate cause of the accident, and the ruling was upheld on appeal to the Fifth Circuit Court of Appeals.\n\nThe crash was the subject of the television movie \"Fire and Rain\".\n\nThe Discovery Channel Canada / National Geographic TV series \"Mayday\" dramatized the crash of Flight 191 in a Season 5 episode titled \"Invisible Killer\". The crash had previously been discussed in the \"Mayday\" Season 1 episode \"Racing the Storm\", which covered the weather-related crash landing of American Airlines Flight 1420.\n\nThe crash was featured on an episode of \"When Weather Changed History\" and \"Why Planes Crash\" on The Weather Channel, and the episode \"Deadly Weather\" of Survival in the Sky on The Learning Channel.\n\nThe crash was mentioned in the feature film \"Rain Man\".\n\nWorking as a reporter for the \"Fort Lauderdale News and Sun-Sentinel\" in 1986, future renowned mystery author Michael Connelly and two other reporters conducted extensive interviews of survivors of Delta Flight 191 and wrote an article detailing their experiences during and after the crash. The article explored the topic of survivor guilt and earned Connelly and his co-writers a finalist position for the Pulitzer Prize.\n\nTen years after the crash, survivors and family members of victims gathered in Florida to recognize the 10th anniversary of the crash. In 2010, 25 years after the accident, a memorial was installed at Dallas-Fort Worth International Airport's Founders Plaza in Grapevine.\n\n\n"}
{"id": "13236255", "url": "https://en.wikipedia.org/wiki?curid=13236255", "title": "Ecojustice Canada", "text": "Ecojustice Canada\n\nEcojustice Canada (formerly \"Sierra Legal Defence Fund\" prior to Sept. 2007), is a Canadian non-profit environmental law organization that provides funding to lawyers to use litigation to defend and protect the environment. Ecojustice is Canada’s largest environmental law charity.\n\n\n"}
{"id": "2021625", "url": "https://en.wikipedia.org/wiki?curid=2021625", "title": "Electrical tape", "text": "Electrical tape\n\nElectrical tape (or insulating tape) is a type of pressure-sensitive tape used to insulate electrical wires and other materials that conduct electricity. It can be made of many plastics, but vinyl is most popular, as it stretches well and gives an effective and long lasting insulation. Electrical tape for class H insulation is made of fiberglass cloth.\n\nA wide variety of electrical tapes is available; some for highly specialized purposes. \"The primary tapes used in electrical applications are vinyl, rubber, mastic, and varnished cambric.\" Electricians generally use only black tape for insulation purposes. The other colors are used to indicate the voltage level and phase of the wire. (In fact, the colored tape is referred to as \"phasing tape.\") This is done on large wire which is available only in black insulation. When wires are phased, a ring of tape is placed on each end near the termination so that the purpose of the wire is obvious. The following table(s) describe the use of electrical tape.\nTape that is approved for electrical applications will carry an approval label from an agency such as Underwriters Laboratories. Skills and training are important as well.\n\nToday electrical tape is simply, \"another form of insulation\". The original electrical insulating tape was made of cloth tape impregnated with Chatterton's compound, an adhesive material manufactured using Gutta-percha. This type of tape was often used to insulate soldered splices on knob and tube wiring. It was commonly referred to as \"friction tape\", and had the unique property of being sticky on both sides. Because of this, no matter how it was used it stuck to itself very readily.\n\nIn the early 1940s, vinyl plastic emerged as a versatile material for a wide range of applications, from shower curtains to cable insulation. A major ingredient in vinyl film was tricresyl phosphate (TCP), which was used as a plasticizer. Unfortunately, TCP tended to migrate, giving the surface of the vinyl film an oily quality and degrading every tape adhesive known. Research chemists and engineers at 3M set out to create a dependable, pressure-sensitive tape made of vinyl film that would have the required electrical, physical and chemical properties.\n\nExperiments were conducted combining new plasticizers with the white, flour-like vinyl resin. Finally, in January 1946, inventors Snell, Oace, and Eastwold of 3M applied for a patent for a vinyl electrical tape with a plasticizer system and non-sulfur-based rubber adhesive that were compatible. The first commercially available version of the tape was sold for use as a wire-harness wrapping. This original black tape wasn't black. Tapes formulated for high-temperature were yellow, and later versions were white. Electrically insulating tapes are essential for enhancing functionality and reliability in a wide range of applications. Some of the most popular types include electrically insulating adhesive tape and electrically insulating film, both of which provide reliable electronic isolation and ensure that direct electrical connection is not made between two or more circuits or their adjacent parts. There is also electrically conducting tape for shielding and similar applications. White tape, because of its instability in ultraviolet light, was eventually replaced with black tape, although colored vinyl tapes are still used as identification and marking tapes. Black became the standard industry color for vinyl standard tape, primarily because of its ultraviolet resistance. Thicknesses originally were 4 mil (100 µm), 8 mil (200 µm) and 12 mil (300 µm). These were standardized to 7 mil (180 µm) and 10 mil (250 µm) in 1948.\n\n\n"}
{"id": "19185629", "url": "https://en.wikipedia.org/wiki?curid=19185629", "title": "Eolus Vind Wind Farm", "text": "Eolus Vind Wind Farm\n\nThe Eolus Vind Wind Farm is a proposed wind power project in Constanţa County, Romania. It will have 15 individual wind turbines with a nominal output of around 2 MW which will deliver up to 50 MW of power, enough to power over 19,800 homes, with a capital investment required of approximately US$23 million.\n"}
{"id": "20297550", "url": "https://en.wikipedia.org/wiki?curid=20297550", "title": "Fires of Kuwait", "text": "Fires of Kuwait\n\nFires of Kuwait is a 1992 American documentary film on the Kuwaiti oil fires directed by David Douglas. It was nominated for an Academy Award for Best Documentary Feature.. The film was the winner of the 2005 Hall of Fame Award from Giant Screen Cinema Association.\n"}
{"id": "58858795", "url": "https://en.wikipedia.org/wiki?curid=58858795", "title": "Gas turbine engine thrust", "text": "Gas turbine engine thrust\n\nThe familiar explanation for jet thrust is a \"black box\" description which only looks at what goes into the jet engine, air and fuel, and what comes out, exhaust gas and an unbalanced force. This force, called thrust, is the sum of the momentum difference between entry and exit and any unbalanced pressure force between entry and exit, as explained in \"Thrust calculation\". \n\nAs an example, an early turbojet, the Bristol Olympus Mk. 101, had a momentum thrust of 9300 lb. and a pressure thrust of 1800 lb. giving a total of 11,100 lb. Looking inside the \"black box\" shows that the thrust results from all the unbalanced momentum and pressure forces created within the engine itself. These forces, some forwards and some rearwards, are across all the internal parts, both stationary and rotating, such as ducts, compressors, etc., which are in the primary gas flow which flows through the engine from front to rear. The algebraic sum of all these forces is delivered to the airframe for propulsion. \"Flight\" gives examples of these internal forces for two early jet engines, the Rolls-Royce Avon Ra.14 and the de Havilland Goblin\n\nThe engine thrust acts along the engine centreline. The aircraft \"holds\" the engine on the outer casing of the engine at some distance from the engine centreline (at the engine mounts). This arrangement causes the engine casing to bend (known as backbone bending) and the round rotor casings to distort (ovalization). Distortion of the engine structure has to be controlled with suitable mount locations to maintain acceptable rotor and seal clearances and prevent rubbing. A well-publicized example of excessive structural deformation occurred with the original Pratt & Whitney JT9D engine installation in the Boeing 747 aircraft. The engine mounting arrangement had to be revised with the addition of an extra thrust frame to reduce the casing deflections to an acceptable amount.\n\nThe rotor thrust on a thrust bearing is not related to the engine thrust. It may even change direction at some RPM. The bearing load is determined by bearing life considerations. Although the aerodynamic loads on the compressor and turbine blades contribute to the rotor thrust they are small compared to cavity loads inside the rotor which result from the secondary air system pressures and sealing diameters on discs, etc. To keep the load within the bearing specification seal diameters are chosen accordingly as, many years ago, on the backface of the impeller in the de Havilland Ghost engine. Sometimes an extra disc known as a balance piston has to be added inside the rotor. An early turbojet example with a balance piston was the Rolls-Royce Avon.\n\nThe net thrust (F) of an engine is given by:\n\nMost types of jet engine have an air intake, which provides the bulk of the fluid exiting the exhaust. Conventional rocket engines, however, do not have an intake, so ṁ is zero. Therefore, rocket engines do not have ram drag and the gross thrust of the rocket engine nozzle is the net thrust of the engine. Consequently, the thrust characteristics of a rocket motor are different from that of an air breathing jet engine, and thrust is independent of velocity.\n\nIf the velocity of the jet from a jet engine is equal to sonic velocity, the jet engine's nozzle is said to be choked. If the nozzle is choked, the pressure at the nozzle exit plane is greater than atmospheric pressure, and extra terms must be added to the above equation to account for the pressure thrust.\n\nThe rate of flow of fuel entering the engine is often very small compared with the rate of flow of air. When the contribution of fuel to the nozzle gross thrust can be ignored, the net thrust is:\n\nThe velocity of the jet (v) must exceed the true airspeed of the aircraft (v) if there is to be a net forward thrust on the aircraft. The velocity (v) can be calculated thermodynamically based on adiabatic expansion.\n\nThrust augmentation has taken many forms, most commonly to supplement inadequate take-off thrust. Some early jet aircraft needed rocket assistance to take off from high altitude airfields or when the day temperature was high. A more recent aircraft, the Tupolev Tu-22 supersonic bomber, was fitted with four SPRD-63 boosters for take-off. Possibly the most extreme requirement needing rocket assistance, and which was short-lived, was zero-length launching. Almost as extreme, but very common, is catapult assistance from aircraft carriers. Rocket assistance has also been used during flight. The SEPR 841 booster engine was used on the Dassault Mirage for high altitude interception.\n\nEarly aft-fan arrangements which added bypass airflow to a turbojet were known as thrust augmentors. The aft-fan fitted to the General Electric CJ805-3 turbojet augmented the take-off thrust from 11,650lb to 16,100lb.\n\nWater, or other coolant, injection into the compressor or combustion chamber and fuel injection into the jetpipe (afterburning/reheat) became standard ways to increase thrust, known as 'wet' thrust to differentiate with the no-augmentation 'dry' thrust.\n\nCoolant injection (pre-compressor cooling) has been used, together with afterburning, to increase thrust at supersonic speeds. The 'Skyburner' McDonnell Douglas F-4 Phantom II set a world speed record using water injection in front of the engine.\n\nAt high Mach numbers afterburners supply progressively more of the engine thrust as the thrust from the turbomachine drops off towards zero at which speed the engine pressure ratio (epr) has fallen to 1.0 and all the engine thrust comes from the afterburner. The afterburner also has to make up for the pressure loss across the turbomachine which is a drag item at higher speeds where the epr will be less than 1.0.\n\nThrust augmentation of existing afterburning engine installations for special short-duration tasks has been the subject of studies for launching small payloads into low earth orbits using aircraft such as McDonnell Douglas F-4 Phantom II, McDonnell Douglas F-15 Eagle, Dassault Rafale and Mikoyan MiG-31, and also for carrying experimental packages to high altitudes using a Lockheed SR-71. In the first case an increase in the existing maximum speed capability is required for orbital launches. In the second case an increase in thrust within the existing speed capability is required. Compressor inlet cooling is used in the first case. A compressor map shows that the airflow reduces with increasing compressor inlet temperature although the compressor is still running at maximum RPM (but reduced aerodynamic speed). Compressor inlet cooling increases the aerodynamic speed and flow and thrust. In the second case a small increase in the maximum mechanical speed and turbine temperature were allowed, together with nitrous oxide injection into the afterburner and simultaneous increase in afterburner fuel flow.\n"}
{"id": "3833379", "url": "https://en.wikipedia.org/wiki?curid=3833379", "title": "GeSbTe", "text": "GeSbTe\n\nGeSbTe (germanium-antimony-tellurium or GST) is a phase-change material from the group of chalcogenide glasses used in rewritable optical discs and phase-change memory applications. Its recrystallization time is 20 nanoseconds, allowing bitrates of up to 35 Mbit/s to be written and direct overwrite capability up to 10 cycles. It is suitable for land-groove recording formats. It is often used in rewritable DVDs. New phase-change memories are possible using n-doped GeSbTe semiconductor. The melting point of the alloy is about 600 °C (900 K) and the crystallization temperature is between 100 and 150 °C.\n\nDuring writing, the material is erased, initialized into its crystalline state, with low-intensity laser irradiation. The material heats up to its crystallization temperature, but not its melting point, and crystallizes. The information is written at the crystalline phase, by heating spots of it with short (<10 ns), high-intensity laser pulses; the material melts locally and is quickly cooled, remaining in the amorphous phase. As the amorphous phase has lower reflectivity than the crystalline phase, data can be recorded as dark spots on the crystalline background. Recently, novel liquid organogermanium precursors, such as isobutylgermane (IBGe) and tetrakis(dimethylamino)germane (TDMAGe) were developed and used in conjunction with the metalorganics of antimony and tellurium, such as tris-dimethylamino antimony (TDMASb) and di-isopropyl telluride (DIPTe) respectively, to grow GeSbTe and other chalcogenide films of very high purity by metalorganic chemical vapor deposition (MOCVD). Dimethylamino germanium trichloride (DMAGeC) is also reported as the chloride containing and a superior dimethylaminogermanium precursor for Ge deposition by MOCVD.\n\nGeSbTe is a ternary compound of germanium, antimony, and tellurium, with composition GeTe-SbTe. In the GeSbTe system, there is a pseudo-line as shown upon which most of the alloys lie. Moving down this pseudo-line, it can be seen that as we go from SbTe to GeTe, the melting point and glass transition temperature of the materials increase, crystallization speed decreases and data retention increases. Hence, in order to get high data transfer rate, we need to use material with fast crystallization speed such as SbTe. This material is not stable because of its low activation energy. On the other hand, materials with good amorphous stability like GeTe has slow crystallization speed because of its high activation energy. In its stable state, crystalline GeSbTe has two possible configurations: hexagonal and a metastable face centered cubic (FCC) lattice. When it is rapidly crystallized however, it was found to have a distorted rocksalt structure. GeSbTe has a glass transition temperature of around 100 °C. GeSbTe also has many vacancy defects in the lattice, of 20 to 25% depending on the specific GeSbTe compound. Hence, Te has an extra lone pair of electrons, which are important for many of the characteristics of GeSbTe. Crystal defects are also common in GeSbTe and due to these defects, an Urbach tail in the band structure is formed in these compounds. GeSbTe is generally p type and there are many electronic states in the band gap accounting for acceptor and donor like traps. GeSbTe has two stable states, crystalline and amorphous. The phase change mechanism from high resistance amorphous phase to low resistance crystalline phase in nano-timescale and threshold switching are two of the most important characteristic of GeSbTe.\n\nThe unique characteristic that makes phase-change memory useful as a memory is the ability to effect a reversible phase change when heated or cooled, switching between stable amorphous and crystalline states. These alloys have high resistance in the amorphous state ‘0’ and are semimetals in the crystalline state ‘1’. In amorphous state, the atoms have short-range atomic order and low free electron density. The alloy also has high resistivity and activation energy. This distinguishes it from the crystalline state having low resistivity and activation energy, long-range atomic order and high free electron density. When used in phase-change memory, use of a short, high amplitude electric pulse such that the material reaches melting point and rapidly quenched changes the material from crystalline phase to amorphous phase is widely termed as RESET current and use of a relatively longer, low amplitude electric pulse such that the material reaches only the crystallization point and given time to crystallize allowing phase change from amorphous to crystalline is known as SET current.\n\nThe early devices were slow, power consuming and broke down easily due to the large currents. Therefore, it did not succeed as SRAM and flash memory took over. In the 1980s though, the discovery of germanium-antimony-tellurium (GeSbTe) meant that phase-change memory now needed less time and power to function. This resulted in the success of the rewriteable optical disk and created renewed interest in the phase-change memory. The advances in lithography also meant that previously excessive programming current has now become much smaller as the volume of GeSbTe that changes phase is reduced.\n\nPhase-change memory has many near ideal memory qualities such as non-volatility, fast switching speed, high endurance of more than 10 read –write cycles, non-destructive read, direct overwriting and long data retention time of more than 10 years. The one advantage that distinguishes it from other next generation non-volatile memory like magnetic random access memory (MRAM) is the unique scaling advantage of having better performance with smaller sizes. The limit to which phase-change memory can be scaled is hence limited by lithography at least until 45 nm. Thus, it offers the biggest potential of achieving ultra-high memory density cells that can be commercialized.\n\nThough phase-change memory offers much promise, there are still certain technical problems that need to be solved before it can reach ultra-high density and commercialized. The most important challenge for phase-change memory is to reduce the programming current to the level that is compatible with the minimum MOS transistor drive current for high-density integration. Currently, the programming current in phase-change memory is substantially high. This high current limits the memory density of the phase-change memory cells as the current supplied by the transistor is not sufficient due to their high current requirement. Hence, the unique scaling advantage of phase-change memory cannot be fully utilized.\nThe typical phase-change memory device design is shown. It has layers including the top electrode, GST, the GeSbTe layer, BEC, the bottom electrode and the dielectric layers. The programmable volume is the GeSbTe volume that is in contact with the bottom electrode. This is the part that can be scaled down with lithography. The thermal time constant of the device is also important. The thermal time constant must be fast enough for GeSbTe to cool rapidly into the amorphous state during RESET but slow enough to allow crystallization to occur during SET state. The thermal time constant depends on the design and material the cell is built. To read, a low current pulse is applied to the device. A small current ensures the material does not heat up. Information stored is read out by measuring the resistance of the device.\n\nThreshold switching occurs when GeSbTe goes from a high resistive state to a conductive state at the threshold field of about 56 V/um. This can be seen from the current-voltage (IV) plot, where current is very low in the amorphous state at low voltage until threshold voltage is reached. Current increases rapidly after the voltage snapback. The material is now in the amorphous \"ON\" state, where the material is still amorphous, but in a pseudo- crystalline electric state. In crystalline state, the IV characteristics is ohmic. There had been debate on whether threshold switching was an electrical or thermal process. There were suggestions that the exponential increase in current at threshold voltage must have been due to generation of carriers that vary exponentially with voltage such as impact ionization or tunneling.\nRecently, much research has focused on the material analysis of the phase-change material in an attempt to explain the high speed phase change of GeSbTe. Using EXAFS, it was found that the most matching model for crystalline GeSbTe is a distorted rocksalt lattice and for amorphous a tetrahedral structure. The small change in configuration from distorted rocksalt to tetrahedral suggests that nano-timescale phase change is possible as the major covalent bonds are intact and only the weaker bonds are broken.\n\nUsing the most possible crystalline and amorphous local structures for GeSbTe, the fact that density of crystalline GeSbTe is less than 10% larger than amorphous GeSbTe, and the fact that free energies of both amorphous and crystalline GeSbTe have to be around the same magnitude, it was hypothesized from density functional theory simulations that the most stable amorphous state was the spinel structure, where Ge occupies tetrahedral positions and Sb and Te occupy octahedral positions, as the ground state energy was the lowest of all the possible configurations. By means of Car-Parrinello molecular dynamics simulations this conjecture have been theoretically confirmed.\n\nAnother similar material is AgInSbTe. It offers higher linear density, but has lower overwrite cycles by 1-2 orders of magnitude. It is used in groove-only recording formats, often in rewritable CDs. AgInSbTe is known as a growth-dominated material while GeSbTe is known as a nucleation-dominated material. In GeSbTe, the nucleation process of crystallization is long with many small crystalline nuclei being formed before a short growth process where the numerous small crystals are joined together. In AgInSbTe, there are only a few nuclei formed in the nucleation stage and these nuclei grow bigger in the longer growth stage such that they eventually form one crystal.\n"}
{"id": "20138540", "url": "https://en.wikipedia.org/wiki?curid=20138540", "title": "Harvest (1967 film)", "text": "Harvest (1967 film)\n\nHarvest is a 1967 American documentary film produced by Carroll Ballard. It was nominated for an Academy Award for Best Documentary Feature. The film portrays the American farm and farmer at harvest time, beginning in Texas with the first cutting of winter wheat, and following the season north to the Canada–United States border.\n\n"}
{"id": "1964288", "url": "https://en.wikipedia.org/wiki?curid=1964288", "title": "Jahn–Teller effect", "text": "Jahn–Teller effect\n\nThe Jahn–Teller effect (JT effect or JTE) is an important mechanism of spontaneous symmetry breaking in molecular and solid-state systems which has far-reaching consequences for different fields, and it is related to a variety of applications in spectroscopy, stereochemistry and crystal chemistry, molecular and solid-state physics, and materials science. The effect is named for Hermann Arthur Jahn and Edward Teller, who first reported studies about it in 1937.\n\nThe Jahn–Teller effect, sometimes also known as Jahn–Teller distortion, describes the geometrical distortion of molecules and ions that is associated with certain electron configurations. The Jahn–Teller theorem essentially states that any nonlinear molecule with a spatially degenerate electronic ground state will undergo a geometrical distortion that removes that degeneracy, because the distortion lowers the overall energy of the species. For a description of another type of geometrical distortion that occurs in crystals with substitutional impurities see article off-center ions.\n\nThe Jahn–Teller effect is most often encountered in octahedral complexes of the transition metals. The phenomenon is very common in six-coordinate copper(II) complexes. The \"d\" electronic configuration of this ion gives three electrons in the two degenerate \"e\" orbitals, leading to a doubly degenerate electronic ground state. Such complexes distort along one of the molecular fourfold axes (always labelled the \"z\" axis), which has the effect of removing the orbital and electronic degeneracies and lowering the overall energy. The distortion normally takes the form of elongating the bonds to the ligands lying along the \"z\" axis, but occasionally occurs as a shortening of these bonds instead (the Jahn–Teller theorem does not predict the direction of the distortion, only the presence of an unstable geometry). When such an elongation occurs, the effect is to lower the electrostatic repulsion between the electron-pair on the Lewis basic ligand and any electrons in orbitals with a \"z\" component, thus lowering the energy of the complex. The inversion centre is preserved after the distortion.\n\nIn octahedral complexes, the Jahn–Teller effect is most pronounced when an odd number of electrons occupy the \"e\" orbitals. This situation arises in complexes with the configurations \"d\", low-spin \"d\" or high-spin \"d\" complexes, all of which have doubly degenerate ground states. In such compounds the \"e\" orbitals involved in the degeneracy point directly at the ligands, so distortion can result in a large energetic stabilisation. Strictly speaking, the effect also occurs when there is a degeneracy due to the electrons in the \"t\" orbitals (\"i.e.\" configurations such as \"d\" or \"d\", both of which are triply degenerate). In such cases, however, the effect is much less noticeable, because there is a much smaller lowering of repulsion on taking ligands further away from the \"t\" orbitals, which do not point \"directly\" at the ligands (see the table below). The same is true in tetrahedral complexes (e.g. manganate: distortion is very subtle because there is less stabilisation to be gained because the ligands are not pointing directly at the orbitals.\n\nThe expected effects for octahedral coordination are given in the following table:\nw: weak Jahn–Teller effect (\"t\" orbitals unevenly occupied)\n\ns: strong Jahn–Teller effect expected (\"e\" orbitals unevenly occupied)\n\nblank: no Jahn–Teller effect expected.\n\nThe Jahn–Teller effect is manifested in the UV-VIS absorbance spectra of some compounds, where it often causes splitting of bands. It is readily apparent in the structures of many copper(II) complexes. Additional, detailed information about the anisotropy of such complexes and the nature of the ligand binding can be however obtained from the fine structure of the low-temperature electron spin resonance spectra.\n\nThe underlying cause of the Jahn–Teller effect is the presence of molecular orbitals that are both degenerate and open shell (i.e., incompletely occupied). This situation is not unique to coordination complexes and can be encountered in other areas of chemistry. In organic chemistry the phenomenon of antiaromaticity has the same cause and also often sees molecules distorting; as in the case of cyclobutadiene and cyclooctatetraene (COT).\n\nThe JT theorem can be stated in different forms, two of which are given here:\n\nAlternatively and considerably shorter:\n\nSpin-degeneracy was an exception in the original treatment and was later treated separately.\n\nThe formal mathematical proof of the Jahn–Teller theorem rests heavily on symmetry arguments, more specifically the theory of molecular point groups. The argument of Jahn and Teller assumes no details about the electronic structure of the system. Jahn and Teller made no statement about the strength of the effect, which may be so small that it is immeasurable. Indeed, for electrons in non-bonding or weakly bonding molecular orbitals, the effect is expected to be weak. However, in many situations the JT effect is important.\n\nInterest in the JTE increased after its first experimental verification. Various model systems were developed probing the degree of degeneracy and the type of symmetry. These were solved partly analytically and partly numerically to obtain the shape of the pertinent potential energy surfaces (PES) and the energy levels for the nuclear motion on the JT-split PES. These energy levels are not vibrational energy levels in the traditional sense because of the intricate coupling to the electronic motion that occurs, and are better termed vibronic energy levels. The new field of ‘vibronic coupling’ or ‘vibronic coupling theory’ was born.\n\nA further breakthrough occurred upon the advent of modern (\"ab initio\") electronic structure calculations whereby the relevant parameters characterising JT systems can be reliably determined from first principles. Thus one could go beyond studies of model systems that explore the effect of parameter variations on the PES and vibronic energy levels; one could also go on beyond fitting these parameters to experimental data without clear knowledge about the significance of the fit. Instead, well-founded theoretical investigations became possible which greatly improved the insight into the phenomena at hand and into the details of the underlying mechanisms.\nWhile recognizing the JTE distortion as a concrete example of the general spontaneous symmetry breaking mechanism, the exact degeneracy of the involved electronic state was identified as a non-essential ingredient for this symmetry breaking in polyatomic systems. Even systems that in the undistorted symmetric configuration present electronic states which are near in energy but not precisely degenerate, can show a similar tendency to distort. The distortions of these systems can be treated within the related theory of the pseudo Jahn–Teller effect (in the literature often referred to as \"second-order JTE\"). This mechanism is associated to the vibronic couplings between adiabatic PES separated by nonzero energy gaps across the configuration space: its inclusion extends the applicability of JT-related models to symmetry breaking in a far broader range of molecular and solid-state systems.\n\n\"Chronology:\"\n\n\n\nA given JT problem will have a particular point group symmetry, such as T symmetry for magnetic impurity ions in semiconductors or I symmetry for the fullerene C. JT problems are conventionally classified using labels for the irreducible representations (irreps) that apply to the symmetry of the electronic and vibrational states. For example, E ⊗ e would refer to an electronic doublet state transforming as E coupled to a vibrational doublet state transforming as e.\n\nIn general, a vibrational mode transforming as Λ will couple to an electronic state transforming as Γ if the symmetric part of the Kronecker product [Γ ⊗ Γ] contains Λ, unless Γ is a double group representation when the antisymmetric part {Γ ⊗ Γ} is considered instead. Modes which do couple are said to be JT-active.\n\nAs an example, consider a doublet electronic state E in cubic symmetry. The symmetric part of E ⊗ E is A + E. Therefore, the state E will couple to vibrational modes formula_1 transforming as a and e. However, the a modes will result in the same energy shift to all states and therefore do not contribute to any JT splitting. They can therefore be neglected. The result is an E ⊗ e JT effect. This JT effect is experienced by triangular molecules X, tetrahedral molecules ML, and octahedral molecules ML when their electronic state has E symmetry.\n\nComponents of a given vibrational mode are also labelled according to their transformation properties. For example, the two components of an e mode are usually labelled formula_2 and formula_3, which in octahedral symmetry transform as formula_4 and formula_5 respectively.\n\nEigenvalues of the Hamiltonian of a polyatomic system define PESs as functions of normal modes formula_1 of the system (i.e. linear combinations of the nuclear displacements with specific symmetry properties . At the reference point of high symmetry, where the symmetry-induced degeneracy occurs, several of the eigenvalues coincide. By a detailed and laborious analysis, Jahn and Teller showed that – excepting linear molecules – there are always first-order terms in an expansion of the matrix elements of the Hamiltonian in terms of symmetry-lowering (in the language of group theory: non-totally symmetric) normal modes. These linear terms represent forces that distort the system along these coordinates and lift the degeneracy. The point of degeneracy can thus not be stationary, and the system distorts toward a stationary point of lower symmetry where stability can be attained.\n\nProof of the JT theorem follows from the theory of molecular symmetry (point group theory). A less rigorous but more intuitive explanation is given in section .\n\nTo arrive at a quantitative description of the JT effect, the forces appearing between the component wave functions are described by expanding the Hamiltonian in a power series in the formula_1. Owing to the very nature of the degeneracy, the Hamiltonian takes the form of a matrix referring to the degenerate wave function components. A matrix element between states formula_8 and formula_9 generally reads as:\n\nThe expansion can be truncated after terms linear in the formula_1, or extended to include terms quadratic (or higher) in the formula_1.\n\nThe adiabatic potential energy surfaces (APES) are then obtained as the eigenvalues of this matrix. In the original paper it is proven that there are always linear terms in the expansion. It follows that the degeneracy of the wave function cannot correspond to a stable structure.\n\nIn mathematical terms, the APESs characterising the JT distortion arise as the eigenvalues of the potential energy matrix (as described in ). Generally, the APESs take the characteristic appearance of a double cone, circular or elliptic, where the point of contact, i.e. degeneracy, denotes the high-symmetry configuration for which the JT theorem applies. For the above case of the linear E ⊗ e JT effect the situation is illustrated by the APES\n\ndisplayed in the figure, with part cut away to reveal its shape, which is known as a Mexican Hat potential. Here, formula_14 is the frequency of the vibrational e mode, formula_15 is its mass and formula_16 is a measure of the strength of the JT coupling. \n\nThe conical shape near the degeneracy at the origin makes it immediately clear that this point cannot be stationary, that is, the system is unstable against asymmetric distortions, which leads to a symmetry lowering. In this particular case there are infinitely many isoenergetic JT distortions. The formula_1 giving these distortions are arranged in a circle, as shown by the red curve in the figure. Quadratic coupling or cubic elastic terms lead to a warping along this \"minimum energy path\", replacing this infinite manifold by three equivalent potential minima and three equivalent saddle points. In other JT systems, linear coupling results in discrete minima.\n\nThe high symmetry of the double-cone topology of the linear E ⊗ e JT system directly reflects the high underlying symmetry. It is one of the earliest (if not the earliest) examples in the literature of a conical intersection of potential energy surfaces. Conical intersections have received wide attention in the literature starting in the 1990s and are now considered paradigms of nonadiabatic excited-state dynamics, with far-reaching consequences in molecular spectroscopy, photochemistry and photophysics. Some of these will be commented upon further below. In general, conical intersections are far less symmetric than depicted in the figure. They can be tilted and elliptical in shape etc., and also peaked and sloped intersections have been distinguished in the literature. Furthermore, for more than two degrees of freedom, they are not point-like structures but instead they are seams and complicated, curved hypersurfaces, also known as intersection space. The coordinate sub-space displayed in the figure is also known as a branching plane.\n\nThe characteristic shape of the JT-split APES has specific consequences for the nuclear dynamics, here considered in the fully quantum sense. For sufficiently strong JT coupling, the minimum points are sufficiently far (at least by a few vibrational energy quanta) below the JT intersection. Two different energy regimes are then to be distinguished, those of low and high energy.\n\n\n\nAs already stated above, the distinction of low and high energy regimes is valid only for sufficiently strong JT couplings, that is, when several or many vibrational energy quanta fit into the energy window between the conical intersection and the minimum of the lower JT-split APES. For the many cases of small to intermediate JT couplings this energy window and the corresponding adiabatic low-energy regime does not exist. Rather, the levels on both JT-split APES are intricately mixed for all energies and the nuclear motion always proceeds on both JT split APES simultaneously.\n\nIn 1965, Frank Ham proposed that the dynamic JTE could reduce the expected values of observables associated with the orbital wavefunctions due to the superposition of several electronic states in the total vibronic wavefunction. This effect leads, for example, to a partial quenching of the spin-orbit interaction and allowed the results of previous Electron Paramagnetic Resonance (EPR) experiments to be explained.\n\nIn general, the result of an orbital operator acting on vibronic states can be replaced by an effective orbital operator acting on purely electronic states. In first order, the effective orbital operator equals the actual orbital operator multiplied by a constant, whose value is less than one, known as a first-order (Ham) reduction factor. For example, within a triplet T electronic state, the spin-orbit coupling operator formula_18 can be replaced by formula_19, where formula_20 is a function of the strength of the JT coupling which varies from 1 in zero coupling to 0 in very strong coupling. Furthermore, when second-order perturbation corrections are included, additional terms are introduced involving additional numerical factors, known as second-order (Ham) reduction factors. These factors are zero when there is no JT coupling but can dominate over first-order terms in strong coupling, when the first-order effects have been significantly reduced.\nReduction factors are particularly useful for describing experimental results, such as EPR and optical spectra, of paramagnetic impurities in semiconducting, dielectric, diamagnetic and ferrimagnetic hosts.\n\nFor a long time, applications of JT theory consisted mainly in parameter studies (model studies) where the APES and dynamical properties of JT systems have been investigated as functions on the system parameters such as coupling constants etc. Fits of these parameters to experimental data were often doubtful and inconclusive. The situation changed in the 1980s when efficient ab initio methods were developed and computational resources became powerful enough to allow for a reliable determination of these parameters from first principles. Apart from wave function-based \ntechniques (which are sometimes considered genuinely ab initio in the literature) the advent of density functional theory (DFT) opened up new avenues to treat larger systems including solids. This allowed details of JT systems to be characterised and experimental findings to be reliably interpreted. It lies at the heart of most developments addressed in Section .\n\nTwo different strategies are conceivable and have been used in the literature. One can\n\n\nNaturally, the more accurate approach (2) may be limited to smaller systems, while the simpler approach (1) lends itself to studies of larger systems.\n\nThe JT distortion of small molecules (or molecular ions) is directly deduced from electronic structure calculations of their APES (through DFT and/or ab initio computations). These molecules / ions are often radicals, such as trimers of alkali atoms (Li and Na), that have unpaired spins and in particular in (but not restricted to) doublet states. Besides the JTE in E' and E\" states, also the between an E state and a nearby A state may play a role. The JT distortion reduces the symmetry from D to C (see figure), and it depends on the details of the interactions whether the isosceles triangle has an acute or an obtuse-angled (such as Na) minimum energy structure. Natural extensions are systems like NO and NH where a JT distortion has been documented in the literature for ground or excited electronic states. \nA somewhat special role is played by tetrahedral systems like CH and P. Here threefold degenerate electronic states and vibrational modes come into play. Nevertheless, also twofold degeneracies continue to be important.\n\nAmong larger systems, a focus in the literature has been on benzene and its radical cation, as well as on their halo (especially fluoro) derivatives. Already in the early 1980s, a wealth of information emerged from the detailed analysis of experimental emission spectra of 1,3,5- trifluoro- and hexafluoro (and chloro) benzene radical cations. For the parent benzene cation one has to rely on photoelectron spectra with comparatively lower resolution because this species does not fluoresce (see also Section on ). Rather detailed ab initio calculations have been carried out which \ndocument the JT stabilization energies for the various (four) JT active modes and also quantify the moderate barriers for the JT pseudorotation.\n\nFinally, a somewhat special role is played by systems with a fivefold symmetry axis like the cyclopentadienyl radical. Careful laser spectroscopic investigations have shed useful light on the JT interactions. In particular they reveal that the barrier to pseudorotation almost vanishes (the system is highly \"fluxional\") which can be attributed to the fact that the 2nd-order coupling terms vanish by symmetry and the leading higher-order terms are of 4th order.\n\nThe JTE is usually stronger where the electron density associated with the degenerate orbitals is more concentrated. This effect therefore plays a large role in determining the structure of transition metal complexes with active internal 3d orbitals. \n\nThe most iconic and prominent of the JT systems in coordination chemistry is probably the case of Cu(II) octahedral complexes. While in perfectly equivalent coordination, like a CuF complex associated to a Cu(II) impurity in a cubic crystal like KMgF, perfect octahedral (O) symmetry is expected. In fact a lower tetragonal symmetry is usually found experimentally. The origin of this JTE distortion it revealed by examining the electronic configuration of the undistorted complex. For an octahedral geometry, the five 3d orbitals partition into t and e orbitals (see diagram). These orbitals are occupied by nine electrons corresponding to the formula_21 electronic configuration of Cu(II). Thus, the t shell is filled, and the e shell contains 3 electrons. Overall the unpaired electron produces a E state, which is Jahn–Teller active. The third electron can occupy either of the orbitals comprising the e shell: the mainly formula_4 orbital or the mainly formula_5 orbital. If the electron occupies the mainly formula_4 level, which antibonding orbital the final geometry of the complex would be elongated as the axial ligands will be pushed away to reduce the global energy of the system. On the other hand, if the electron went into the mainly formula_5 antibonding orbital the complex would distort into a compressed geometry. Experimentally elongated geometries are overwhelmingly observed and this fact has been attributed both to metal-ligand anharmonic interactions and 3d-4s hybridisations. Given that all the directions containing a fourfold axis are equivalent the distortion is equally likely to happen in any of these orientations. From the electronic point of view this means that the formula_4 and formula_5 orbitals, that are degenerate and free to hybridise in the octahedral geometry, will mix to produce appropriate equivalent orbitals in each direction like formula_28 or formula_29.\n\nThe JTE is not just restricted to Cu(II) octahedral complexes. There are many other configurations, involving changes both in the initial structure and electronic configuration of the metal that yield degenerate states and, thus, JTE. However, the amount of distortion and stabilisation energy of the effect is strongly dependent on the particular case. In octahedral Cu(II), the JTE is particularly strong because\n\n\nIn other configurations involving π or δ bonding, like for example when the degenerate state is associated to the t orbitals of an octahedral configuration, the distortion and stabilisation energies are usually much smaller and the possibility of not observing the distortion due to dynamic JT effects is much higher. Similarly for rare-earth ions where covalency is very small, the distortions associated to the JTE are usually very weak.\n\nImportantly, the JTE is associated with strict degeneracy in the electronic subsystem and so it cannot appear in systems without this property. For example, the JTE is often associated to cases like quasi-octahedral CuXY complexes where the distances to X and Y ligands are clearly different. However, the intrinsic symmetry of these complexes is already tetragonal and no degenerate e orbital exists, having split into a (mainly formula_4) and b (mainly formula_5) orbitals due to the different electronic interactions with axial X ligands and equatorial Y ligands. In this and other similar cases some remaining vibronic effects related to the JTE are still present but are quenched with respect to the case with degeneracy due to the splitting of the orbitals.\n\nFrom spectra with rotational resolution, moments of inertia and hence bond lengths and angles can be determined \"directly\" (at least in principle). From less well-resolved spectra one can still determine important quantities like JT stabilization energies and energy barriers (e.g. to pseudorotation). However, in the whole spectral intensity distribution formula_32 of an electronic transition more information is encoded. It has been used to decide on the presence (or absence) of the geometric phase which is accumulated during the pseudorotational motion around the JT (or other type of) conical intersection. Prominent examples of either type are the ground (X) or an excited (B) state of Na. The Fourier transform of formula_32, the so-called autocorrelation function formula_34 reflects the motion of the wavepacket after an optical (= vertical) transition to the APES of the final electronic state. Typically it will move on the timescale of a vibrational period which is (for small molecules) of the order of 5-50 fs, i.e. ultrafast. Besides a nearly periodic motion, mode-mode interactions with very irregular (also chaotic) behaviour and spreading of the wavepacket may also occur. Near a conical intersection this will be accompanied/complemented by nonradiative transitions (termed internal conversion) to other APESs occurring on the same ultrafast time scale.\n\nFor the JT case the situation is somewhat special, as compared to a general conical intersection, because the different JT potential sheets are symmetry-related to each other and have (exactly or nearly) the same energy minimum. The \"transition\" between them is thus more oscillatory than one would normally expect, and their time-averaged populations are close to 1/2. For a more typical scenario a more general conical intersection is \"required\".\n\nThe JT effect still comes into play, namely in combination with a different nearby, in general non-degenerate electronic state. The result is a pseudo Jahn–Teller effect, for example, of an E state interacting with an A state. This situation is common in JT systems, just as interactions between two nondegenerate electronic states are common for non-JT systems. Examples are excited electronic states of NH and the benzene radical cation. Here, crossings between the E and A state APESs amount to triple intersections, which are associated with very complex spectral features (dense line structures and diffuse spectral envelopes under low resolution). The population transfer between the states is also ultrafast, so fast that fluorescence (proceeding on a nanosecond time scale) cannot compete. This helps to understand why the benzene cation, like many other organic radical cation, does not fluoresce.\n\nTo be sure, photochemical reactivity emerges when the internal conversion makes the system explore the nuclear configuration space such that new chemical species are formed. There is a plethora of femtosecond pump-probe spectroscopic techniques to reveal details of these processes occurring, for example, in the process of vision.\n\nAs proposed originally by Landau \nfree electrons in a solid, introduced for example by doping or irradiation, can interact with the vibrations of the lattice to form a localized quasi-particle known as a polaron. Strongly localized polarons (also called Holstein polarons) can condensate around high-symmetry sites of the lattice with electrons or holes occupying local degenerate orbitals that experience the JTE. These Jahn–Teller polarons break both translational and point group symmetries of the lattice where they are found and have been attributed important roles in effects like colossal magnetoresistance and superconductivity.\n\nParamagnetic impurities in semiconducting, dielectric, diamagnetic and ferrimagnetic hosts can all be described using a JT model. For example, these models were used extensively in the 1980s and 1990s to describe ions of Cr, V and Ti substituting for Ga in GaAs and GaP.\n\nThe fullerene C can form solid compounds with alkali metals known as fullerides. CsC can be superconducting at temperatures up to 38K under applied pressure, whereas compounds of the form AC are insulating (as reviewed by Gunnarsson ). JT effects both within the C molecules (intramolecular) and between C molecules (intermolecular) play a part in the mechanisms behind various observed properties in these systems. For example, they could mean that the Migdal-Eliashberg treatment of superconductivity breaks down. Also, the fullerides can form a so-called new state of matter known as a Jahn–Teller metal, where localised electrons coexist with metallicity and JT distortions on the C molecules persist \n\nThe JTE is usually associated with degeneracies that are well localised in space, like those occurring in a small molecule or associated to an isolated transition metal complex. However, in many periodic high-symmetry solid-state systems, like perovskites, some crystalline sites allow for electronic degeneracy giving rise under adequate compositions to lattices of JT-active centers. This can produce a cooperative JTE, where global distortions of the crystal occur due to local degeneracies.\n\nIn order to determine the final electronic and geometric structure of a cooperative JT system, it is necessary to take into account both the local distortions and the interaction between the different sites, which will take such form necessary to minimise the global energy of the crystal.\n\nWhile works on the cooperative JTE started in the late fifties \n\n, it was in 1960 that Kanamori published the first work on the cooperative JTE where many important elements present in the modern theory for this effect were introduced. This included the use of pseudospin notation to discuss orbital ordering, and discussions of the importance of the JTE to discuss magnetism, the competition of this effect with the spin-orbit coupling and the coupling of the distortions with the strain of the lattice. This point was later stressed in the review by Gehring and Gehring as being the key element to establish long-range order between the distortions in the lattice. An important part of the modern theory of the cooperative JTE, can lead to structural phase transitions.\n\nIt is important to note that many cooperative JT systems would be expected to be metals from band theory as, to produce them, a degenerate orbital has to be partially filled and the associated band would be metallic. However, under the perturbation of the symmetry-breaking distortion associated to the cooperative JTE, the degeneracies in the electronic structure are destroyed and the ground state of these systems is often found to be insulating (see e.g.). In many important cases like the parent compound for colossal magnetoresistance perovskites, LaMnO, an increase of temperature leads to disorder in the distortions which lowers the band splitting due to the cooperative JTE, thus triggering a metal-insulator transition.\n\nIn modern solid-state physics, it is common to classify systems according to the kind of degrees of freedom they have available, like electron (metals) or spin (magnetism). In crystals that can display the JTE, and before this effect is realised by symmetry-breaking distortions, it is found that there exists an orbital degree of freedom consisting of how electrons occupy the local degenerate orbitals. As initially discussed by Kugel and Khomskii, not all configurations are equivalent. The key is the relative orientation of these occupied orbital, in the same way that spin orientation is important in magnetic systems, and the ground state can only be realised for some particular orbital pattern. Both this pattern and the effect giving rise to this phenomenon is usually denominated orbital-ordering.\n\nIn order to predict the orbital-ordering pattern, Kugel and Khomskii used a particularisation of the Hubbard model. In particular they established how superexchange interactions, usually described by the Anderson–Kanamori–Goodenough rules, change in the presence of degenerate orbitals. Their model, using a pseudospin representation for the local orbitals, leads to a Heisenberg-like model in which the ground state is a combination of orbital and spin patterns. Using this model it can be shown, for example, that the origin of the unusual ground insulating ferromagnetic state of a solid like KCuF can be traced to its orbital ordering.\n\nEven when starting from a relatively high-symmetry structure the combined effect of exchange interactions, spin-orbit coupling, orbital-ordering and crystal deformations activated by the JTE can lead to very low symmetry magnetic patterns with specific properties. For example, in CsCuCl an incommensurable helicoidal pattern appears both for the orbitals and the distortions along the formula_35-axis. Moreover, many of these compounds show complex phase diagrams when varying temperature or pressure.\n\n\n"}
{"id": "1524935", "url": "https://en.wikipedia.org/wiki?curid=1524935", "title": "K-65 residues", "text": "K-65 residues\n\nK-65 residues are the very radioactive mill residues resulting from a uniquely concentrated uranium ore discovered before WW II in Katanga province (Shinkolobwe) of the Democratic Republic of the Congo (formerly called the Belgian Congo).\n\nAccording to Zoellner, \"Remnants from typical uranium from the southwestern United States give a radioactive signature of about forty picocuries per gram, about ten times the amount of picocuries per liter of air that is considered safe for humans to breathe. The Shinkolobwe remnants, by contrast, emit a stunning 520,000 picocuries per gram.\" The Linde Air Products Company and Electro Metallurgical plant near Niagara built the ring-and-plug in Little Boy. Linde Air used the Lake Ontario Ordnance Works site at the end of the war to dispose of it atomic waste, cuttings from the African uranium, some 200 dump trucks worth. The eventual location of all this waste is called the \"Interim Waste Containment Structure\" of the Niagara Falls Storage Site of the U.S. Army Corps of Engineers.\n\nThis ore, dubbed \"K-65\", had a record 65% uranium content. It also held very high concentrations of thorium and radium (and their decay products, including radon gas) which are retained in the tailings (residues). The very high concentrations of these extremely toxic, long-lived radionuclides present in these wastes prompted the National Academy of Sciences' National Research Council to categorize them as indistinguishable in hazard from High-Level Waste in its 1995 report, \"Safety of the High-Level Uranium Ore Residues at the Niagara Falls Storage Site, Lewiston, New York\" . \n\nThe K-65 ores were refined as a key part of the Manhattan Project during World War II at the Linde Ceramics Plant at Tonawanda, NY, and at the Mallinckrodt Chemical Works in St. Louis, MO; these ores were the primary raw material source of ~80% of the uranium used in the Hiroshima bomb. The Mallinckrodt \"K-65 residues\" were later moved to the Feed Materials Production Center, a Cold War era uranium refinery at Fernald, OH (outside of Cincinnati) which commenced operations in 1951. The refining of \"K-65\" ore was continued at Fernald. The Linde \"K-65 residues\" were transported to a storage silo built at the federally appropriated Lake Ontario Ordnance Works site outside of Lewiston, NY, a short distance from Niagara Falls, NY.\n\n"}
{"id": "49760337", "url": "https://en.wikipedia.org/wiki?curid=49760337", "title": "Karot Hydropower Project", "text": "Karot Hydropower Project\n\nThe Karot Hydropower Project is an under construction run-of-river concrete-core rockfill gravity dam in Pakistan with an installed capacity of 720 MW. The Karot Hydropower Station in Pakistan is the first investment project of the Silk Road Fund, is part of the much larger CPEC China–Pakistan Economic Corridor, and is expected to be completed in 2020. \n\nThe Karot project is being developed by Karot Power Company comprising Three Gorges South Asia Investment Limited, a subsidiary of China Three Gorges Corporation, China-CTGC and Associated Technologies of Pakistan. It is the first hydropower project financed by China’s Silk Road Project.\n\nAfter completion, the company will run and maintain the project for 30 years at a levelised tariff of 7.57 cents per unit after which it will be transferred to the Punjab government at a notional price of Rs.1.\n\nThe Jhelum River is the largest river of Indus Basin River System, and its hydropower potential was identified by various studies carried out by international agencies, with the first report issued by the Canadian Consultant group MONENCO in 1983, followed by a 1994 study by the German Agency for Technical Cooperation (GTZ) that formally proposed the Karot Hydropower Project.\n\nThe Karot Hydropower Project is planned on Jhelum River near Karot Village some 1.7 kilometers upstream of Karot Bridge and 74 km upstream of Mangla Dam. The Project site is accessible through the road from Islamabad – Kahuta – Kotli Road approximately 29 kilometers from Kahuta village, and 65 kilometers from Islamabad. Groundbreaking on the project took place on January 10, 2016.\n\nThe major project features include construction of concrete gravity 95.5 meters high dam with a crest length of 320 meters near the village of Gohra. The dam's reservoir will be approximately 164.5 million cubic meters in volume, with a length of 27 kilometers. 72 homes and 58 businesses are expected to require relocation as a result of construction, while 2.8 kilometers of the Karot-Kotli road, and 8.9 kilometers of the Azad Pattan-Kahuta road will need relocation.\n\nThe power intake structure will be constructed on right bank of the river immediately upstream of Dam site and will divert the water into headrace tunnels entering into Cavern Powerhouse. The water will be discharged back to River Jhelum through tail-race channel located at right bank of the River Jhelum immediately downstream of Karot village. The dam will generate mean annual energy 3,436 GWh, and will connect to Pakistan's national electricity grid.\n\nOn September 28, 2016, the federal and Azad Jammu and Kashmir governments signed an implementation agreement with a Chinese consortium for development and operation on river Jhelum at a levelised tariff of 7.57 cents per unit for 30 years.\n\nFinancial close of the project was achieved in February 2017 while land acquisition award has also been done. As of September 2017, roughly 25 per cent civil works of the project with installed capacity of 720 MW has already been completed.\n\nSMEC International Pty Ltd is participating as Employer's Engineer, who is providing services for project management, design review and construction supervision for 720 MW Karot hydropower project\n\nTotal costs for the project are estimated to be $2 billion, and will be funded by International Finance Corporation, China's Silk Road Fund. The Export-Import Bank of China and China Development Bank will issue loans to the Karot Power Company, which is a subsidiary of China's Three Gorges Corporation. it will be built on a \"Build-Own-Operate-Transfer\" basis for 30 years, after which ownership will be turned over to the government of Pakistan.\n"}
{"id": "6697465", "url": "https://en.wikipedia.org/wiki?curid=6697465", "title": "LNG storage tank", "text": "LNG storage tank\n\nA liquefied natural gas storage tank or LNG storage tank is a specialized type of storage tank used for the storage of Liquefied Natural Gas. LNG storage tanks can be found in ground, above ground or in LNG carriers. The common characteristic of LNG Storage tanks is the ability to store LNG at the very low temperature of -162 °C (-260 °F). LNG storage tanks have double containers, where the inner contains LNG and the outer container contains insulation materials. The most common tank type is the full containment tank. Tanks vary greatly in size, depending on usage.\n\nIn LNG storage the pressure and temperature within the tank will continue to rise. LNG is a cryogen, and is kept in its liquid state at very low temperatures. The temperature within the tank will remain constant if the pressure is kept constant by allowing the boil off gas to escape from the tank. This is known as auto-refrigeration.\n\nThe world's largest above-ground tank (Delivered in 2000) is the 180 million liters full containment type for Osaka Gas Co., Ltd.\n(2) The world's largest tank (Delivered in 2001) is the 200 million liters Membrane type for Toho Gas Co., Ltd.\n"}
{"id": "19069168", "url": "https://en.wikipedia.org/wiki?curid=19069168", "title": "List of storms on the Great Lakes", "text": "List of storms on the Great Lakes\n\nEver since people have traveled the Great Lakes storms have taken lives and vessels. The first sailing vessel on the upper lakes, the \"Le Griffon\", was lost on its return from Green Bay in 1679. Since that time, memorable storms have swept the lakes, often in November taking men and ships to their death. With the advent of modern technology and sturdier vessels, fewer such losses have occurred. The large expanse of the lakes allows waves to build to substantial heights and the open water can alter weather systems (fog, lake effect snow). Storm winds can alter the lakes as well with large systems causing storm surges that lower lake levels several feet on one side while raising it even higher on the other. The shallowest lake, Lake Erie, sometimes sees storm surge rises of 8 or 10 feet. Seiches cause short-term irregular lake level changes, killing people swept off beaches and piers and even sometimes sinking boats The great tolls caused by Great Lakes storms in 1868 and 1869 were one of the main reasons behind establishing a national weather forecasting service, initially run by the U.S. Army Signal Corps using telegraphs to announce approaching storms in a few port cities.\n\nIt was September 1811 and Jacob Butler was headed to Sandusky, Ohio as the new Indian Agent. When he arrived in Buffalo, he found it to be a small town of 40-50 houses and little activity. There were but a few ships in the harbor. The \"Catherine\" was a new schooner that had set sail the day before, but was now anchored nine miles up the Canadian shore at Point Ebenew. As it had set sail, it encountered a west forcing it to seek shelter. Seeing an opportunity to avoid the long trip around the lake, he crossed the Niagara River and with the help of a guide came upon the ship at anchor after two hours. Soon they were underway with a steady breeze pushing them towards Sandusky.\n\nThe ship was packed and every possible space in which a person could find repose was occupied. All night they traveled westward, the ship pushed by the wind and the schooner rocking from side to side. With so many people, so closely packed, many became nauseated. The next day, they traveled westward. As night fell on their second day of travel, they expected to see Sandusky in the morning. Everyone had just settled down for the night, when a commotion arose and a gale blew out of the southwest, nearly tipping the vessel over. If the schooner had not been ‘hove to’ and resting quietly, it would have been capsized. (Without shore lights, lighthouse, or modern navigation equipment, Captains would ‘heave to’ at night if they anticipated approaching land/harbor soon. This prevented them from running aground in the dark.)\n\nQuickly the crew made the \"Catherine\" ready for the storm and let her drift before the winds. As daylight came, the captain was able to get his ship behind Presque Isle (Erie, Pennsylvania), where they rode out the storm for the next 24 hours. The winds persisted so fiercely that everything on deck was swept clear. The crew and passengers remained below deck in the dark, their supply of food gone. On the fourth day of his journey, the gale ended and they were able to resupply from shore. Setting sail for Sandusky, the hope was to make harbor by dark. Once again a gale of lesser force sprang up and pushed the vessel back to Presque Isle. Here, many of the passengers left the ship and hired a wagon for the two-week overland trip. On their next attempt to reach Sandusky, the \"Catherine\" made harbor without incident.\n\nIt was September 1825 when Henry Rowe Schoolcraft recorded a late fall storm on Lake Huron. He was returning from an Indian ‘Congress’ at Prairie du Chien, Wisconsin. It had been an uneventful six-day trip from the Mississippi River to Michilimackinac. From Mackinac, Schoolcraft was headed towards Detour Pass and up the St. Mary's River to Sault Ste. Marie. On the morning of 5 September, he arose, had breakfast and prepared to strike out in their canoes. The day was cloudy and threatening, so he decided to wait until the next day. Arising at three in the morning, he found the island lost in a fog. They waited until it began to clear at 6:30 a.m. and made their way to Goose Island, distance after three hours. From there, they made their way to Outard Point. Here, the headwind had increased so they hove to about noon and were able to pull into an inlet out of the wind and make camp. Eight hours later, the canoe party was still waiting for the wind to let up. The night brought a heavy rain, piercing the fabric of the tents, soaking everyone and everything. The morning of the 7th found the storm continuing. The increasing violence caused Schoolcraft to have his tent moved back into the trees for more protection. Around three, the sky seemed to be brightening and expectations were that the weather was clearing. But the rains and the wind came with renewed fury from the west and continued late into the night. As the morning of the 8th arrived, Schoolcraft determined to get on with his journey. As the wind was directly out of the west, he was headed east, he ordered the canoes readied and the sails. With sails reefed against the storm, the brigade set out into the lake at 10:00 a.m. Just under three and a half hours brought them 20 miles to the Isle St. Vital, behind which they took refuge from the wind. After a break, they once again set into the gale-force winds, driving for De Tour and the St. Mary’s Strait. Here they found the schooner \"Harriet\", down bound, waiting for the winds to subside. It was but another day and they were once again at Sault Ste. Marie.\n\nOn November 11, 1835, a southwest wind swept across the lakes, taking numerous vessels. This was still early in the life of commercial shipping on the Lakes, so most of the losses were on the lower lakes where settlements were greatest.\n\nBuffalo was a major port on Lake Erie and felt the force of the storm as water from the lake forced ships onto the piers and shoreline of the city. The creek rose 20 feet as the wind and the harbor front were swept away.\n\nThe Mataafa Storm of 1905 is the name of a storm that occurred on the Great Lakes on November 27–28, 1905. The system moved across the Great Basin with moderate depth on November 26 and November 27, then east-northeastward across the Great Lakes on November 28. Fresh east winds were forecast for the Great Lakes for the afternoon and evening of November 27, with storm warnings were in effect by the morning of November 28. Storm-force winds and heavy snows accompanied the cyclone's passage. The storm, named after the Mataafa wreck, ended up destroying or damaging about 29 vessels, killing 36 seamen and causing shipping losses of $3.567 million (1905 dollars) on Lake Superior.\n\nIn 1913, from the ninth of November through the twelfth, all five lakes were turned into cauldrons of rolling water by a unique combination of weather patterns. Before the four days ended, 13 ships went under and many more were driven ashore. Two hundred forty-four men lost their lives. The largest loss of ships was on Lake Huron (see Shipwrecks of Lake Huron)\n\nFriday, October 20, 1916 on Lake Erie. These were the years before there was ship to shore radio. Once out on the lakes, each ship had only itself to depend upon and the chance of meeting another ship. While only four ships were lost, nearly all the men of these crews were lost to the tempest of the storm-tossed lake. In all, Black Friday took the lives of 49 men.\n\nThe \"James B. Colgate\" had just finished loading coal and set sail from Buffalo, New York bound for Fort William, Ontario (now Thunder Bay). It was 1:10 in the morning as the \"Colgate\" dropped its hawsers and headed out into the open lake. Dawn found the \"Colgate\" off Long Point. All day they moved steadily westward, keeping the bow into the wind, waves crashing over the decks and beating on the hatch coverings. Slowing, water began to enter the cargo holds. The pumps could not keep up with the influx of water and she began to list at about eight o'clock that evening. No other ships had been seen and none could be found. The bow was riding low in the water. As the ten o'clock hour came around, the \"Colgate\" slid beneath the waves. The men all had life jackets, but nothing was floating which would help them get out of the cold water. One life raft was found and a coal passer, the engineer and the captain took refuge. In the middle of the night, the raft was flipped and the coal passer did not return. As the 21st dawned, the raft again spilled its occupants and the engineer returned but was too weak to hold on and he was lost. Night came on and Captain Walter J. Grashaw still hung on to the raft. A passenger steamer passed nearby, but he was unnoticed in the dark. As daylight dawned on the 22nd, Sunday, the Marquette & Bessemer No. 2 (II) came to his rescue.\n\n\"Marshall F. Butters\", a wooden lumber carrier down bound to Cleveland with a cargo of shingles and lumber, entered Lake Erie from the Detroit River. The wind rose and the waves grew in height. The \"Butters\" turned into Lake Erie heading towards the Southeast Shoals Light, off the tip of Point Pelee. The wooden ship could not take the pounding of the waves. Settling into the lake, soon the boilers were extinguished and the \"Butters\" was at the mercy of the storm. Ten men set sail in the lifeboat, leaving only the captain and two men on board the sinking vessel. The Pioneer Steamship Company’s \"Frank R. Billings\" and the \"F.G. Hartwell\" were nearby. The \"Billings\" approached to give aid. Pouring storm oil on the water, they were able to calm the seas enough to rescue Captain McClure and his two men. Meanwhile, the \"Hartwell\" rescued the men in the lifeboat.\n\nA third ship the \"D.L. Filer\", a wooden schooner of 45 years, was headed from Buffalo to Saugatuck, Michigan with a load of coal. For two days, the \"Filer\" beat into the wind headed for the Detroit River at the western end of Lake Erie. Just off Bar Point, within sight of the mouth of the Detroit River, the pumps could no longer move the volume of water rushing into the holds, and the seams began to open. In eighteen feet of water, she settled to the bottom. Six men climbed the foremast, while the captain climbed the after mast. It looked like all seven of the crew could cling to the mast and weather the night. But the weight of six men snapped the fore mast and five disappeared. Only one man made it to the after mast and climbed to safety. As dawn broke the horizon, the \"Western States\" came into sight and turned towards the two men clinging to the mast protruding from the shallows. As the steamer approached, one man slipped from the mast and was never seen again. Only the Captain John Mattison was rescued.\n\nMeanwhile, the Canadian steamer \"Merida\" disappeared that night. All 23 of her crew were found the next day floating in mid-lake, only identified by their life vests bearing the name \"Merida\".\n\nThe Armistice Day Blizzard was a winter storm that occurred on November 11–12, 1940 which brought heavy snow and winds up to 80 mph. The lake freighter SS \"William B. Davock\" sank with all 33 hands in Lake Michigan south of Pentwater, Michigan. The SS \"Anna C. Minch\" foundered, broke in two and sank nearby with the loss of all 24 crew. A third ship, the SS \"Novadoc\", wrecked on a reef in the same area. Two crew were lost and the rest were rescued two days later by the tug \"Three Brothers\". Two smaller boats also sank, bringing the total death toll on the Lakes to 66.\n\nOn April 30, 1967, a storm in Duluth, Minnesota took the lives of three teenage boys and Coast Guardsman Edgar Culbertson. Culbertson and two others were part of a rescue team searching for the missing boys on the Duluth Entry pier on Lake Superior. Meteorologists and Minnesota residents often refer to this day as \"Black Sunday\". There were reports of heavy rain as far north as Duluth that day. The waves on Lake Superior in Duluth that night were reportedly over high at times; the lake had water with gale-force winds gusting up to .\n\nOnce again it was a November storm that took the lives of men and their ship. It was November 9, 1975 that \"Edmund Fitzgerald\" was downbound to Detroit with a load of taconite. \"Arthur M. Anderson\" joined her on Lake Superior and was downbound for Gary, Indiana. As they were crossing Lake Superior the winter storm blew in. Winds were reported in excess of with waves running up to . The next day, Monday, November 10, eastern Lake Superior was still experiencing winds of 50 knots (57.5 mph). That afternoon \"Anderson\" reported being hit by a 75-knot gust (86.3 mph). By 3:30 pm the \"Fitzgerald\" reported a minor list and top-side damage, including the loss of radar. \"Fitzgerald\" was leading, but slowed to close the distance between ships so that it could be guided by \"Anderson\", who still had radar. Just after seven that night, the last radio contact from the \"Fitzgerald\" said that they were still managing. By 7:20 p.m. there was no more contact and \"Anderson\" no longer saw \"Fitzgerald\" on radar.\n\n\"Fitzgerald\" sank in Canadian waters deep, approximately 17 miles (15 nautical miles; 27 kilometers) from the entrance to Whitefish Bay near the twin cities of Sault Ste. Marie, Michigan, and Sault Ste. Marie, Ontario. Although \"Fitzgerald\" had reported being in difficulty earlier, no distress signals were sent before she sank. Her crew of 29 all perished, and no bodies were recovered.\n\nMany theories, books, studies and expeditions have examined the cause of the sinking. \"Fitzgerald\" may have fallen victim to the high waves of the storm, suffered structural failure, been swamped with water entering through her cargo hatches or deck, experienced topside damage, or shoaled in a shallow part of Lake Superior. The sinking of \"Edmund Fitzgerald\" is one of the best-known disasters in the history of Great Lakes shipping. Gordon Lightfoot made it the subject of his 1976 hit song \"The Wreck of the Edmund Fitzgerald\".\n\nThe 1996 Lake Huron cyclone was a unique storm for the Great Lakes, acquiring some tropical characteristics at its peak intensity.\n\nThe 2006 Northeastern Ontario Derecho formed on the Great Lakes. The Storm caused damage throughout Northern Ontario, and into Quebec.\n\nOn October 26, 2010, the USA recorded its lowest pressure ever in a continental, non-hurricane system, though its pressure was consistent with a category three hurricane. The powerful system was dubbed the \"Chiclone\" by the media as it hit the Chicago area particularly strongly, as well as Minnesota, Wisconsin and Michigan. It was also meteorologically referred to as a bombogenesis due to the rapid drop of barometric pressure experienced.\n\nIn Superior, Wisconsin, the storm managed a 28.38 inch reading—a new all-time low for Wisconsin at the time. Near International Falls on the U.S./Canada border, the system's reading established a new all-time Minnesota low pressure. Early Tuesday morning October 26, an F2 tornado rushed through Will County, south of Chicago, at 7:00 AM. Another tornado is said to have struck Racine, Wisconsin, to the north, but has not yet been confirmed. In Roscoe, IL, about 100 miles to the west of Chicago and 15 minutes north of Rockford, a woman was killed after being crushed under a large tree that fell in her neighborhood of Chickory Ridge.\n\nThe storm also produced some of the highest officially recorded waves by weather buoys stationed in Lakes Superior and Michigan. Specifically, on Wednesday, October 27, 2010, buoy no. 45136, operated by Environment Canada, in northern Lake Superior recorded a significant wave height of 26.6 feet (this is average height of 1/3 of the highest waves over an hour), and buoy no. 45002, operated by the National Data Buoy Center (NDBC), recorded a significant wave height of 21.7 feet in northern Lake Michigan. This would appear consistent with the NOAA forecast for northern Lake Michigan calling for 21–26-foot waves that day. The persistence and strength of the storm's westerly winds also piled the waters of Lake Michigan along the Michigan shoreline leading to declines in lake levels on the Illinois and Wisconsin side of the lake. Based on NOAA lake level sensors, an updated analysis of Wednesday, October 27, 2010 water levels on Lake Michigan revealed a two-day decrease of 42 inches at Green Bay, WI and 19 inches at Calumet Harbor, IL---while NOAA sensors at Ludington, MI and Mackinaw City, MI measured lake level rises of 7 and 19 inches respectively.\n\nA 78 mph gust was recorded the afternoon of October 27, 2010 at the Harrison-Dever Crib, three miles offshore of Chicago in Lake Michigan, with gusts reaching 63 mph at Chicago's Latin School and in Racine, Wisconsin, 61 mph at Buffalo Grove, Waukegan, Gary and Monroe, Wisconsin and 58 mph at Hinsdale. The storm further whitened sections of the Upper Midwest with the region's first significant snow Tuesday night and Wednesday. Snowfall reports from Minnesota and North Dakota indicate 9 inches fell at Twig, Minn.; 8.5 inches at Dunn Center, N.D.; 8 inches at Adolph, Minn. and Carrington, N.D.; 7.7 inches at Duluth; 4.1 inches Williston; 4 inches at Minot and 3.4 inches at Bismarck---all in North Dakota.\n\nThe cold front that merged with Hurricane Sandy at the end of October 2012 fueled Sandy's transition into a powerful extratropical cyclone, which brought strong winds and high waves across the Great Lakes. Lake Michigan recorded wave heights of 20 to 22 feet and wind gusts of 60 to 70 mph. The southern end of Lake Michigan experienced a lake level rise of 15 inches as the winds pushed water down the lake. Lake Huron experienced 23-foot waves and a wind gust of 74 mph was recorded at Fort Gratiot at the southern end of the lake. Most freighters stayed in harbor instead of trying to run through the storm.\n\nOn October 31, 2014, there was a powerful storm which impacted the Great Lakes area during times when people are traditionally celebrating Halloween. It was also called a \"great storm\".\n\n\n"}
{"id": "917817", "url": "https://en.wikipedia.org/wiki?curid=917817", "title": "Mast (sailing)", "text": "Mast (sailing)\n\nThe mast of a sailing vessel is a tall spar, or arrangement of spars, erected more or less vertically on the centre-line of a ship or boat. Its purposes include carrying sail, spars, and derricks, and giving necessary height to a navigation light, look-out position, signal yard, control position, radio aerial or signal lamp. Large ships have several masts, with the size and configuration depending on the style of ship. Nearly all sailing masts are guyed.\n\nUntil the mid-19th century all vessels' masts were made of wood formed from a single or several pieces of timber which typically consisted of the trunk of a conifer tree. From the 16th century, vessels were often built of a size requiring masts taller and thicker than could be made from single tree trunks. On these larger vessels, to achieve the required height, the masts were built from up to four sections (also called masts), known in order of rising height above the decks as the lower, top, topgallant and royal masts. Giving the lower sections sufficient thickness necessitated building them up from separate pieces of wood. Such a section was known as a made mast, as opposed to sections formed from single pieces of timber, which were known as pole masts.\n\nFor square-sail carrying ship, the masts, given their standard names in bow to stern (front to back) order, are:\n\nSome names given to masts in ships carrying other types of rig (where the naming is less standardised) are: \nMost types of vessels with two masts are supposed to have a main-mast and a smaller mizzen-mast, although both brigs and two-masted schooners carry a fore-mast and a main-mast instead. On a two-masted vessel with the main-mast forward and a much smaller second mast, such as a ketch, or particularly a yawl, the terms \"mizzen\" and \"jigger\" are synonymous.\n\nAlthough two-masted schooners may be provided with masts of identical size, the aftmost is still referred to as the main-mast, and normally has the larger course. Schooners have been built with up to seven masts in all, with several six-masted examples.\n\nOn square-rigged vessels, each mast carries several horizontal yards from which the individual sails are rigged.\n\nFolding mast ships use a \"tabernacle\" anchor point—\"the partly open socket or double post on the deck, into which a mast is fixed, with a pivot near the top so that the mast can be lowered\", \"large bracket attached firmly to the deck, to which the foot of the mast is fixed; it has two sides or cheeks and a bolt forming the pivot around which the mast is raised and lowered\", \"substantial fitting for mounting the mast on deck, so that it can be lowered easily for trailering or for sailing under bridges\", \"hinged device allowing for the easy folding of a mast 90 degrees from perpendicular, as for transporting the boat on a trailer, or passing under a bridge\" \n\nIn the West, the concept of a ship carrying more than one mast, to give it more speed under sail and to improve its sailing qualities, evolved in northern Mediterranean waters: The earliest foremast has been identified on an Etruscan pyxis from Caere, Italy, dating to the mid-7th century BC: a warship with a furled mainsail is engaging an enemy vessel, deploying a foresail. A two-masted merchant vessel with a sizable foresail rigged on a slightly inclined foremast is depicted in an Etruscan tomb painting from 475–450 BC. An \"artemon\" (Greek for foresail) almost the same size as the galley's mainsail can be found on a Corinthian krater as early as the late 6th century BC; apart from that Greek longships are uniformly shown without it until the 4th century BC.\n\nThe foremast became fairly common on Roman galleys, where, inclined at an angle of 45°, it was more akin to a bowsprit, and the foresail set on it, reduced in size, seems to be used rather as an aid to steering than for propulsion. While most of the ancient evidence is iconographic, the existence of foremasts can also be deduced archaeologically from slots in foremast-feets located too close to the prow for a mainsail.\n\n\"Artemon\", along with mainsail and topsail, developed into the standard rig of seagoing vessels in imperial times, complemented by a mizzen on the largest freighters. The earliest recorded three-masters were the giant \"Syracusia\", a prestige object commissioned by king Hiero II of Syracuse and devised by the polymath Archimedes around 240 BC, and other Syracusan merchant ships of the time. The imperial grain freighters travelling the routes between Alexandria and Rome also included three-masted vessels. A mosaic in Ostia (c. 200 AD) depicts a freighter with a three-masted rig entering Rome's harbour. Special craft could carry many more masts: Theophrastus (\"Hist. Plant.\" 5.8.2) records how the Romans imported Corsican timber by way of a huge raft propelled by as many as fifty masts and sails.\n\nThroughout antiquity, both foresail and mizzen remained secondary in terms of canvas size, although large enough to require full running rigging. In late antiquity, the foremast lost most of its tilt, standing nearly upright on some ships.\n\nBy the onset of the Early Middle Ages, rigging had undergone a fundamental transformation in Mediterranean navigation: the lateen which had long evolved on smaller Greco-Roman craft replaced the square rig, the chief sail type of the ancients, that practically disappeared from the record until the 14th century (while it remained dominant in northern Europe). The dromon, the lateen-rigged and oared bireme of the Byzantine navy, almost certainly had two masts, a larger foremast and one midships. Their length has been estimated at about 12 m and 8 m respectively, somewhat smaller than the Sicilian war galleys of the time.\n\nMultiple-masted \"sailing\" ships were reintroduced into the Mediterranean Sea by the Late Middle Ages. Large vessels were coming more and more into use and the need for additional masts to control these ships adequately grew with the increase in tonnage. Unlike in antiquity, the mizzen-mast was adopted on medieval two-masters earlier than the foremast, a process which can be traced back by pictorial evidence from Venice and Barcelona to the mid-14th century. To balance out the sail plan the next obvious step was to add a mast fore of the main-mast, which first appears in a Catalan ink drawing from 1409. With the three-masted ship established, propelled by square rig and lateen, and guided by the pintle-and-gudgeon rudder, all advanced ship design technology necessary for the great transoceanic voyages was in place by the beginning of the 15th century.\n\nThe first hollow mast was fitted on the American sloop Maria in 1845, 92 feet long and built of staves bound with iron hoops like a barrel. Other hollow masts were made from two tapered timbers hollowed and glued together. Nearly a century later, the simple box form of mast was arrived at.\n\nAlthough sailing ships were superseded by engine-powered ships in the 19th century, recreational sailing ships and yachts continue to be designed and constructed.\n\nIn the 1930s aluminium masts were introduced on large J-class yachts. An aluminium mast has considerable advantages over a wooden one: it is lighter and slimmer than a wooden one of the same strength, is impervious to rot, and can be produced as a single extruded length. During the 1960s wood was eclipsed by aluminium. Aluminium alloys, generally 6000 series, are commonly utilised.\n\nRecently some sailing yachts (particularly home-built yachts) have begun to use steel masts. Whilst somewhat heavier than aluminium, steel has its own set of advantages. It is significantly cheaper, and a steel mast of an equivalent strength can be smaller in diameter than an aluminium mast, allowing less turbulence and a better airflow onto the sail.\nFrom the mid-1990s racing yachts introduced the use of carbon fibre and other composite materials to construct masts with even better strength-to-weight ratios. Carbon fibre masts could also be constructed with more precisely engineered aerodynamic profiles.\n\nModern masts form the leading edge of a sail's airfoil and tend to have a teardrop-shaped cross-section. On smaller racing yachts and catamarans, the mast rotates to the optimum angle for the sail's \nairfoil. If the mast has a long, thin cross-section and makes up a significant area of the airfoil, it is called a wing-mast; boats using these have a smaller sail area to compensate for the larger mast area. There are many manufacturers of modern masts for sailing yachts of all sizes, a few notable companies are Hall Spars, Offshore Spars, and Southern Spars.\nAfter the end of the age of sail, warships retained masts, initially as observation posts and to observe fall of shot, also holding fire control equipment such as rangefinders, and later as a mounting point for radar and telecommunication antennas, which need to be mounted high up to increase range. Simple pole, lattice, and tripod masts have been used—also, on some past Japanese warships, complex pagoda masts.\n\n"}
{"id": "19622", "url": "https://en.wikipedia.org/wiki?curid=19622", "title": "Materials science", "text": "Materials science\n\nThe interdisciplinary field of materials science, also commonly termed materials science and engineering is the design and discovery of new materials, particularly solids. The intellectual origins of materials science stem from the Enlightenment, when researchers began to use analytical thinking from chemistry, physics, and engineering to understand ancient, phenomenological observations in metallurgy and mineralogy. Materials science still incorporates elements of physics, chemistry, and engineering. As such, the field was long considered by academic institutions as a sub-field of these related fields. Beginning in the 1940s, materials science began to be more widely recognized as a specific and distinct field of science and engineering, and major technical universities around the world created dedicated schools of the study, within either the Science or Engineering schools, hence the naming. \nMany of the most pressing scientific problems humans currently face are due to the limits of the materials that are available and how they are used. Thus, breakthroughs in materials science are likely to affect the future of technology significantly.\n\nMaterials scientists emphasize understanding how the history of a material (its \"processing\") influences its structure, and thus the material's properties and performance. The understanding of processing-structure-properties relationships is called the . This paradigm is used to advance understanding in a variety of research areas, including nanotechnology, biomaterials, and metallurgy. Materials science is also an important part of forensic engineering and failure analysis – investigating materials, products, structures or components which fail or do not function as intended, causing personal injury or damage to property. Such investigations are key to understanding, for example, the causes of various aviation accidents and incidents.\n\nThe material of choice of a given era is often a defining point. Phrases such as Stone Age, Bronze Age, Iron Age, and Steel Age are historic, if arbitrary examples. Originally deriving from the manufacture of ceramics and its putative derivative metallurgy, materials science is one of the oldest forms of engineering and applied science. Modern materials science evolved directly from metallurgy, which itself evolved from mining and (likely) ceramics and earlier from the use of fire. A major breakthrough in the understanding of materials occurred in the late 19th century, when the American scientist Josiah Willard Gibbs demonstrated that the thermodynamic properties related to atomic structure in various phases are related to the physical properties of a material. Important elements of modern materials science are a product of the space race: the understanding and engineering of the metallic alloys, and silica and carbon materials, used in building space vehicles enabling the exploration of space. Materials science has driven, and been driven by, the development of revolutionary technologies such as rubbers, plastics, semiconductors, and biomaterials.\n\nBefore the 1960s (and in some cases decades after), many \"materials science\" departments were named \"metallurgy\" departments, reflecting the 19th and early 20th century emphasis on metals. The growth of materials science in the United States was catalyzed in part by the Advanced Research Projects Agency, which funded a series of university-hosted laboratories in the early 1960s \"to expand the national program of basic research and training in the materials sciences.\" The field has since broadened to include every class of materials, including ceramics, polymers, semiconductors, magnetic materials, medical implant materials, biological materials, and nanomaterials, with modern materials classed within 3 distinct groups: Ceramic, Metal or Polymer. The prominent change in materials science during the last two decades is active usage of computer simulation methods to find new compounds, predict various properties.\n\nA material is defined as a substance (most often a solid, but other condensed phases can be included) that is intended to be used for certain applications. There are a myriad of materials around us—they can be found in anything from buildings to spacecraft. Materials can generally be further divided into two classes: crystalline and non-crystalline. The traditional examples of materials are metals, semiconductors, ceramics and polymers. New and advanced materials that are being developed include nanomaterials, biomaterials, and energy materials to name a few.\nThe basis of materials science involves studying the structure of materials, and relating them to their properties. Once a materials scientist knows about this structure-property correlation, they can then go on to study the relative performance of a material in a given application. The major determinants of the structure of a material and thus of its properties are its constituent chemical elements and the way in which it has been processed into its final form. These characteristics, taken together and related through the laws of thermodynamics and kinetics, govern a material's microstructure, and thus its properties.\n\nAs mentioned above, structure is one of the most important components of the field of materials science. Materials science examines the structure of materials from the atomic scale, all the way up to the macro scale. Characterization is the way materials scientists examine the structure of a material. This involves methods such as diffraction with X-rays, electrons, or neutrons, and various forms of spectroscopy and chemical analysis such as Raman spectroscopy, energy-dispersive spectroscopy (EDS), chromatography, thermal analysis, electron microscope analysis, etc. Structure is studied at various levels, as detailed below.\n\nThis deals with the atoms of the materials, and how they are arranged to give molecules, crystals, etc. Much of the electrical, magnetic and chemical properties of materials arise from this level of structure. The length scales involved are in angstroms(Å).\nThe way in which the atoms and molecules are bonded and arranged is fundamental to studying the properties and behavior of any material.\n\nNanostructure deals with objects and structures that are in the 1—100 nm range. In many materials, atoms or molecules agglomerate together to form objects at the nanoscale. This causes many interesting electrical, magnetic, optical, and mechanical properties.\n\nIn describing nanostructures it is necessary to differentiate between the number of dimensions on the nanoscale. Nanotextured surfaces have \"one dimension\" on the nanoscale, i.e., only the thickness of the surface of an object is between 0.1 and 100 nm. Nanotubes have \"two dimensions\" on the nanoscale, i.e., the diameter of the tube is between 0.1 and 100 nm; its length could be much greater. Finally, spherical nanoparticles have \"three dimensions\" on the nanoscale, i.e., the particle is between 0.1 and 100 nm in each spatial dimension. The terms nanoparticles and ultrafine particles (UFP) often are used synonymously although UFP can reach into the micrometre range. The term 'nanostructure' is often used when referring to magnetic technology. Nanoscale structure in biology is often called ultrastructure.\n\nMaterials which atoms and molecules form constituents in the nanoscale (i.e., they form nanostructure) are called nanomaterials. Nanomaterials are subject of intense research in the materials science community due to the unique properties that they exhibit.\n\nMicrostructure is defined as the structure of a prepared surface or thin foil of material as revealed by a microscope above 25× magnification. It deals with objects from 100 nm to a few cm. The microstructure of a material (which can be broadly classified into metallic, polymeric, ceramic and composite) can strongly influence physical properties such as strength, toughness, ductility, hardness, corrosion resistance, high/low temperature behavior, wear resistance, and so on. Most of the traditional materials (such as metals and ceramics) are microstructured.\n\nThe manufacture of a perfect crystal of a material is physically impossible. For example, any crystalline material will contain defects such as precipitates, grain boundaries (Hall–Petch relationship), vacancies, interstitial atoms or substitutional atoms. The microstructure of materials reveals these larger defects, so that they can be studied, with significant advances in simulation resulting in exponentially increasing understanding of how defects can be used to enhance material properties.\n\nMacro structure is the appearance of a material in the scale millimeters to meters—it is the structure of the material as seen with the naked eye.\n\nCrystallography is the science that examines the arrangement of atoms in crystalline solids. Crystallography is a useful tool for materials scientists. In single crystals, the effects of the crystalline arrangement of atoms is often easy to see macroscopically, because the natural shapes of crystals reflect the atomic structure. Further, physical properties are often controlled by crystalline defects. The understanding of crystal structures is an important prerequisite for understanding crystallographic defects. Mostly, materials do not occur as a single crystal, but in polycrystalline form, i.e., as an aggregate of small crystals with different orientations. Because of this, the powder diffraction method, which uses diffraction patterns of polycrystalline samples with a large number of crystals, plays an important role in structural determination.\nMost materials have a crystalline structure, but some important materials do not exhibit regular crystal structure. Polymers display varying degrees of crystallinity, and many are completely noncrystalline. Glass, some ceramics, and many natural materials are amorphous, not possessing any long-range order in their atomic arrangements. The study of polymers combines elements of chemical and statistical thermodynamics to give thermodynamic and mechanical, descriptions of physical properties.\n\nTo obtain a full understanding of the material structure and how it relates to its properties, the materials scientist must study how the different atoms, ions and molecules are arranged and bonded to each other. This involves the study and use of quantum chemistry or quantum physics. Solid-state physics, solid-state chemistry and physical chemistry are also involved in the study of bonding and structure.\n\nMaterials exhibit myriad properties, including the following.\nThe properties of a material determine its usability and hence its engineering application.\n\nSynthesis and processing involves the creation of a material with the desired micro-nanostructure. From an engineering standpoint, a material cannot be used in industry if no economical production method for it has been developed. Thus, the processing of materials is vital to the field of materials science.\n\nDifferent materials require different processing or synthesis methods. For example, the processing of metals has historically been very important and is studied under the branch of materials science named \"physical metallurgy\". Also, chemical and physical methods are also used to synthesize other materials such as polymers, ceramics, thin films, etc. As of the early 21st century, new methods are being developed to synthesize nanomaterials such as graphene.\n\nThermodynamics is concerned with heat and temperature and their relation to energy and work. It defines macroscopic variables, such as internal energy, entropy, and pressure, that partly describe a body of matter or radiation. It states that the behavior of those variables is subject to general constraints, that are common to all materials, not the peculiar properties of particular materials. These general constraints are expressed in the four laws of thermodynamics. Thermodynamics describes the bulk behavior of the body, not the microscopic behaviors of the very large numbers of its microscopic constituents, such as molecules. The behavior of these microscopic particles is described by, and the laws of thermodynamics are derived from, statistical mechanics.\n\nThe study of thermodynamics is fundamental to materials science. It forms the foundation to treat general phenomena in materials science and engineering, including chemical reactions, magnetism, polarizability, and elasticity. It also helps in the understanding of phase diagrams and phase equilibrium.\n\nChemical kinetics is the study of the rates at which systems that are out of equilibrium change under the influence of various forces. When applied to materials science, it deals with how a material changes with time (moves from non-equilibrium to equilibrium state) due to application of a certain field. It details the rate of various processes evolving in materials including shape, size, composition and structure. Diffusion is important in the study of kinetics as this is the most common mechanism by which materials undergo change.\n\nKinetics is essential in processing of materials because, among other things, it details how the microstructure changes with application of heat.\n\nMaterials science has received much attention from researchers. In most universities, many departments ranging from physics to chemistry to chemical engineering, along with materials science departments, are involved in materials research. Research in materials science is vibrant and consists of many avenues. The following list is in no way exhaustive. It serves only to highlight certain important research areas.\n\nNanomaterials describe, in principle, materials of which a single unit is sized (in at least one dimension) between 1 and 1000 nanometers (10 meter) but is usually 1—100 nm.\n\nNanomaterials research takes a materials science-based approach to nanotechnology, leveraging advances in materials metrology and synthesis which have been developed in support of microfabrication research. Materials with structure at the nanoscale often have unique optical, electronic, or mechanical properties.\n\nThe field of nanomaterials is loosely organized, like the traditional field of chemistry, into organic (carbon-based) nanomaterials such as fullerenes, and inorganic nanomaterials based on other elements, such as silicon. Examples of nanomaterials include fullerenes, carbon nanotubes, nanocrystals, etc.\n\nA biomaterial is any matter, surface, or construct that interacts with biological systems. The study of biomaterials is called \"bio materials science\". It has experienced steady and strong growth over its history, with many companies investing large amounts of money into developing new products. Biomaterials science encompasses elements of medicine, biology, chemistry, tissue engineering, and materials science.\n\nBiomaterials can be derived either from nature or synthesized in a laboratory using a variety of chemical approaches using metallic components, polymers, bioceramics, or composite materials. They are often used and/or adapted for a medical application, and thus comprises whole or part of a living structure or biomedical device which performs, augments, or replaces a natural function. Such functions may be benign, like being used for a heart valve, or may be bioactive with a more interactive functionality such as hydroxylapatite coated hip implants. Biomaterials are also used every day in dental applications, surgery, and drug delivery. For example, a construct with impregnated pharmaceutical products can be placed into the body, which permits the prolonged release of a drug over an extended period of time. A biomaterial may also be an autograft, allograft or xenograft used as an organ transplant material.\n\nSemiconductors, metals, and ceramics are used today to form highly complex systems, such as integrated electronic circuits, optoelectronic devices, and magnetic and optical mass storage media. These materials form the basis of our modern computing world, and hence research into these materials is of vital importance.\n\nSemiconductors are a traditional example of these types of materials. They are materials that have properties that are intermediate between conductors and insulators. Their electrical conductivities are very sensitive to impurity concentrations, and this allows for the use of doping to achieve desirable electronic properties. Hence, semiconductors form the basis of the traditional computer.\n\nThis field also includes new areas of research such as superconducting materials, spintronics, metamaterials, etc. The study of these materials involves knowledge of materials science and solid-state physics or condensed matter physics.\n\nWith the increase in computing power, simulating the behavior of materials has become possible. This enables materials scientists to discover properties of materials formerly unknown, as well as to design new materials. Up until now, new materials were found by time-consuming trial and error processes. But, now it is hoped that computational methods could drastically reduce that time, and allow tailoring materials properties. This involves simulating materials at all length scales, using methods such as density functional theory, molecular dynamics, etc.\n\nRadical materials advances can drive the creation of new products or even new industries, but stable industries also employ materials scientists to make incremental improvements and troubleshoot issues with currently used materials. Industrial applications of materials science include materials design, cost-benefit tradeoffs in industrial production of materials, processing methods (casting, rolling, welding, ion implantation, crystal growth, thin-film deposition, sintering, glassblowing, etc.), and analytic methods (characterization methods such as electron microscopy, X-ray diffraction, calorimetry, nuclear microscopy (HEFIB), Rutherford backscattering, neutron diffraction, small-angle X-ray scattering (SAXS), etc.).\n\nBesides material characterization, the material scientist or engineer also deals with extracting materials and converting them into useful forms. Thus ingot casting, foundry methods, blast furnace extraction, and electrolytic extraction are all part of the required knowledge of a materials engineer. Often the presence, absence, or variation of minute quantities of secondary elements and compounds in a bulk material will greatly affect the final properties of the materials produced. For example, steels are classified based on 1/10 and 1/100 weight percentages of the carbon and other alloying elements they contain. Thus, the extracting and purifying methods used to extract iron in a blast furnace can affect the quality of steel that is produced.\n\nAnother application of material science is the structures of ceramics and glass typically associated with the most brittle materials. Bonding in ceramics and glasses uses covalent and ionic-covalent types with SiO (silica or sand) as a fundamental building block. Ceramics are as soft as clay or as hard as stone and concrete. Usually, they are crystalline in form. Most glasses contain a metal oxide fused with silica. At high temperatures used to prepare glass, the material is a viscous liquid. The structure of glass forms into an amorphous state upon cooling. Windowpanes and eyeglasses are important examples. Fibers of glass are also available. Scratch resistant Corning Gorilla Glass is a well-known example of the application of materials science to drastically improve the properties of common components. Diamond and carbon in its graphite form are considered to be ceramics.\n\nEngineering ceramics are known for their stiffness and stability under high temperatures, compression and electrical stress. Alumina, silicon carbide, and tungsten carbide are made from a fine powder of their constituents in a process of sintering with a binder. Hot pressing provides higher density material. Chemical vapor deposition can place a film of a ceramic on another material. Cermets are ceramic particles containing some metals. The wear resistance of tools is derived from cemented carbides with the metal phase of cobalt and nickel typically added to modify properties.\n\n Filaments are commonly used for reinforcement in composite materials.\nAnother application of materials science in industry is making composite materials. These are structured materials composed of two or more macroscopic phases. Applications range from structural elements such as steel-reinforced concrete, to the thermal insulating tiles which play a key and integral role in NASA's Space Shuttle thermal protection system which is used to protect the surface of the shuttle from the heat of re-entry into the Earth's atmosphere. One example is reinforced Carbon-Carbon (RCC), the light gray material which withstands re-entry temperatures up to and protects the Space Shuttle's wing leading edges and nose cap. RCC is a laminated composite material made from graphite rayon cloth and impregnated with a phenolic resin. After curing at high temperature in an autoclave, the laminate is pyrolized to convert the resin to carbon, impregnated with furfural alcohol in a vacuum chamber, and cured-pyrolized to convert the furfural alcohol to carbon. To provide oxidation resistance for reuse ability, the outer layers of the RCC are converted to silicon carbide.\n\nOther examples can be seen in the \"plastic\" casings of television sets, cell-phones and so on. These plastic casings are usually a composite material made up of a thermoplastic matrix such as acrylonitrile butadiene styrene (ABS) in which calcium carbonate chalk, talc, glass fibers or carbon fibers have been added for added strength, bulk, or electrostatic dispersion. These additions may be termed reinforcing fibers, or dispersants, depending on their purpose.\n\nPolymers are chemical compounds made up of a large number of identical components linked together like chains. They are an important part of materials science. Polymers are the raw materials (the resins) used to make what are commonly called plastics and rubber. Plastics and rubber are really the final product, created after one or more polymers or additives have been added to a resin during processing, which is then shaped into a final form. Plastics which have been around, and which are in current widespread use, include polyethylene, polypropylene, polyvinyl chloride (PVC), polystyrene, nylons, polyesters, acrylics, polyurethanes, and polycarbonates and also rubbers which have been around are natural rubber, styrene-butadiene rubber, chloroprene, and butadiene rubber. Plastics are generally classified as \"commodity\", \"specialty\" and \"engineering\" plastics.\n\nPolyvinyl chloride (PVC) is widely used, inexpensive, and annual production quantities are large. It lends itself to a vast array of applications, from artificial leather to electrical insulation and cabling, packaging, and containers. Its fabrication and processing are simple and well-established. The versatility of PVC is due to the wide range of plasticisers and other additives that it accepts. The term \"additives\" in polymer science refers to the chemicals and compounds added to the polymer base to modify its material properties.\n\nPolycarbonate would be normally considered an engineering plastic (other examples include PEEK, ABS). Such plastics are valued for their superior strengths and other special material properties. They are usually not used for disposable applications, unlike commodity plastics.\n\nSpecialty plastics are materials with unique characteristics, such as ultra-high strength, electrical conductivity, electro-fluorescence, high thermal stability, etc.\n\nThe dividing lines between the various types of plastics is not based on material but rather on their properties and applications. For example, polyethylene (PE) is a cheap, low friction polymer commonly used to make disposable bags for shopping and trash, and is considered a commodity plastic, whereas medium-density polyethylene (MDPE) is used for underground gas and water pipes, and another variety called ultra-high-molecular-weight polyethylene (UHMWPE) is an engineering plastic which is used extensively as the glide rails for industrial equipment and the low-friction socket in implanted hip joints.\n\nThe study of metal alloys is a significant part of materials science. Of all the metallic alloys in use today, the alloys of iron (steel, stainless steel, cast iron, tool steel, alloy steels) make up the largest proportion both by quantity and commercial value. Iron alloyed with various proportions of carbon gives low, mid and high carbon steels. An iron-carbon alloy is only considered steel if the carbon level is between 0.01% and 2.00%. For the steels, the hardness and tensile strength of the steel is related to the amount of carbon present, with increasing carbon levels also leading to lower ductility and toughness. Heat treatment processes such as quenching and tempering can significantly change these properties, however. Cast Iron is defined as an iron–carbon alloy with more than 2.00% but less than 6.67% carbon. Stainless steel is defined as a regular steel alloy with greater than 10% by weight alloying content of Chromium. Nickel and Molybdenum are typically also found in stainless steels.\n\nOther significant metallic alloys are those of aluminium, titanium, copper and magnesium. Copper alloys have been known for a long time (since the Bronze Age), while the alloys of the other three metals have been relatively recently developed. Due to the chemical reactivity of these metals, the electrolytic extraction processes required were only developed relatively recently. The alloys of aluminium, titanium and magnesium are also known and valued for their high strength-to-weight ratios and, in the case of magnesium, their ability to provide electromagnetic shielding. These materials are ideal for situations where high strength-to-weight ratios are more important than bulk cost, such as in the aerospace industry and certain automotive engineering applications.\n\nThe study of semiconductors is a significant part of materials science. A semiconductor is a material that has a resistivity between a metal and insulator. Its electronic properties can be greatly altered through intentionally introducing impurities or doping. From these semiconductor materials, things such as diodes, transistors, light-emitting diodes (LEDs), and analog and digital electric circuits can be built, making them materials of interest in industry. Semiconductor devices have replaced thermionic devices (vacuum tubes) in most applications. Semiconductor devices are manufactured both as single discrete devices and as integrated circuits (ICs), which consist of a number—from a few to millions—of devices manufactured and interconnected on a single semiconductor substrate.\n\nOf all the semiconductors in use today, silicon makes up the largest portion both by quantity and commercial value. Monocrystalline silicon is used to produce wafers used in the semiconductor and electronics industry. Second to silicon, gallium arsenide (GaAs) is the second most popular semiconductor used. Due to its higher electron mobility and saturation velocity compared to silicon, it is a material of choice for high-speed electronics applications. These superior properties are compelling reasons to use GaAs circuitry in mobile phones, satellite communications, microwave point-to-point links and higher frequency radar systems. Other semiconductor materials include germanium, silicon carbide, and gallium nitride and have various applications.\n\nMaterials science evolved—starting from the 1960s—because it was recognized that to create, discover and design new materials, one had to approach it in a unified manner. Thus, materials science and engineering emerged at the intersection of various fields such as metallurgy, solid state physics, chemistry, chemical engineering, mechanical engineering and electrical engineering.\n\nThe field is inherently interdisciplinary, and the materials scientists/engineers must be aware and make use of the methods of the physicist, chemist and engineer. The field thus maintains close relationships with these fields. Also, many physicists, chemists and engineers also find themselves working in materials science.\n\nThe overlap between physics and materials science has led to the offshoot field of \"materials physics\", which is concerned with the physical properties of materials. The approach is generally more macroscopic and applied than in condensed matter physics. See important publications in materials physics for more details on this field of study.\n\nThe field of materials science and engineering is important both from a scientific perspective, as well as from an engineering one. When discovering new materials, one encounters new phenomena that may not have been observed before. Hence, there is a lot of science to be discovered when working with materials. Materials science also provides a test for theories in condensed matter physics.\n\nMaterials are of the utmost importance for engineers, as the usage of the appropriate materials is crucial when designing systems. As a result, materials science is an increasingly important part of an engineer's education.\n\n\n\n"}
{"id": "13178119", "url": "https://en.wikipedia.org/wiki?curid=13178119", "title": "Men of the Trees", "text": "Men of the Trees\n\nMen of the Trees is an international, non-profit, non-political, conservation organisation. It is involved in planting, maintenance and protection of trees. It was founded in Kenya on July 22, 1922, by Richard St. Barbe Baker. It was the precursor to another organisation also founded by St. Barbe Baker in 1924 as the International Tree Foundation, in the United Kingdom. In 1929, a branch was opened in Palestine.\n\nThere have been chapters in over 100 countries. By some estimates, organizations he founded or assisted have been responsible for planting at least 26 billion trees, internationally. In Australia there is currently still a small organization that goes by the name Men of the Trees, although in England, where the charity originated, the original charity has now been re-branded as the International Tree Foundation.\n\n"}
{"id": "18993882", "url": "https://en.wikipedia.org/wiki?curid=18993882", "title": "Overweight", "text": "Overweight\n\nBeing overweight or fat is having more body fat than is optimally healthy. Being overweight is especially common where food supplies are plentiful and lifestyles are sedentary.\n\n, excess weight reached epidemic proportions globally, with more than 1 billion adults being either overweight or obese. In 2013 this increased to more than 2 billion. Increases have been observed across all age groups.\n\nA healthy body requires a minimum amount of fat for proper functioning of the hormonal, reproductive, and immune systems, as thermal insulation, as shock absorption for sensitive areas, and as energy for future use. But the accumulation of too much storage fat can impair movement, flexibility, and alter the appearance of the body.\n\nThe degree to which a person is overweight is generally described by the body mass index (BMI). \"Overweight\" is defined as a BMI of 25 or more, thus it includes pre-obesity defined as a BMI between 25 and 30 and obesity as defined by a BMI of 30 or more. Pre-obese and overweight however are often used interchangeably, thus giving overweight a common definition of a BMI of between 25–30. There are, however, several other common ways to measure the amount of adiposity or fat present in an individual's body.\n\n\n\nThe most common method for discussing this subject and the one used primarily by researchers and advisory institutions is BMI. Definitions of what is considered overweight vary by ethnicity. The current definition proposed by the US National Institutes of Health (NIH) and the World Health Organization (WHO) designates whites, Hispanics and blacks with a BMI of 25 or more as overweight. For Asians, overweight is a BMI between 23 and 29.9 and obesity for all groups is a BMI of 30 or more.\n\nBMI, however, does not account extremes of muscle mass, some rare genetic factors, the very young, and a few other individual variations. Thus it is possible for an individuals with a BMI of less than 25 to have excess body fat, while others may have a BMI that is significantly higher without falling into this category. Some of the above methods for determining body fat are more accurate than BMI but come with added complexity.\n\nIf an individual is overweight and has excess body fat it can create or lead to health risks. Reports are surfacing, however, that being mildly overweight to slightly obese – BMI being between 24 and 31.9 – may be actually beneficial and that people with a BMI between 24 and 31.9 could actually live longer than normal weight or underweight persons.\n\nWhile the negative health outcomes associated with obesity are accepted within the medical community, the health implications of the overweight category are more controversial. The generally accepted view is that being overweight causes similar health problems to obesity, but to a lesser degree. A 2016 review estimated that the risk of death increases by seven percent among overweight people with a BMI of 25 to 27.5 and 20 percent among overweight people with a BMI of 27.5 to 30. The Framingham heart study found that being overweight at age 40 reduced life expectancy by three years. Being overweight also increases the risk of oligospermia and azoospermia in men.\n\nKatherine Flegal et al., however, found that the mortality rate for individuals who are classified as overweight (BMI 25 to 30) may actually be lower than for those with an \"ideal\" weight (BMI 18.5 to 25), noting that many studies show that the lowest mortality rate is at a BMI close to 25.\n\nBeing overweight has been identified as a cause of cancer, and is projected to overtake smoking as the primary cause of cancer in developed countries as cases of cancer linked to smoking dwindle.\n\nPsychological well-being is also at risk in the overweight individual due to social discrimination. However, children under the age of eight are normally not affected.\n\nBeing overweight has been shown not to increase mortality in older people: in a study of 70 to 75-year old Australians, mortality was lowest for \"overweight\" individuals (BMI 25 to 30), while a study of Koreans found that, among those initially aged 65 or more, an increase in BMI to above 25 was not associated with increased risk of death.\n\nBeing overweight is generally caused by the intake of more calories (by eating) than are expended by the body (by exercise and everyday activity). Factors that may contribute to this imbalance include:\n\n\nPeople who have insulin dependent diabetes and chronically overdose insulin may gain weight, while people who already are overweight may develop insulin tolerance, and in the long run develop type II diabetes.\n\nThe usual treatments for overweight individuals is diet and physical exercise.\n\nDietitians generally recommend eating several balanced meals dispersed through the day, with a combination of progressive, primarily aerobic, physical exercise.\n\nBecause these general treatments help most case of obesity, they are common in all levels of overweight individuals.\n\nAs much as 64% of the United States' adult population is considered either overweight or obese, and this percentage has increased over the last four decades.\n\n\n"}
{"id": "33775309", "url": "https://en.wikipedia.org/wiki?curid=33775309", "title": "Percobaltate", "text": "Percobaltate\n\nPercobaltates are chemical compounds where the oxidation state of cobalt is +5. This is the highest established oxidation state of cobalt. The simplest of these are bi-metallic Group 1 oxides such as sodium percobaltate (NaCoO); which may be produced by the reaction of cobalt(II,III) oxide and sodium oxide, using oxygen as the oxidant:\n\nThe potassium salt can be synthesized similarly; its magnetic moment has indicated the existence of cobalt(V). No crystallographic analysis has been reported for either material. Percobaltates can be stabilized by use of oxides or fluorides.\n\nA number of organometallic Co(V) complexes have also been reported.\n\n"}
{"id": "31032459", "url": "https://en.wikipedia.org/wiki?curid=31032459", "title": "Perifosine", "text": "Perifosine\n\nPerifosine (also KRX-0401) is a drug candidate being developed for a variety of cancer indications.\nIt is an alkyl-phospholipid structurally related to miltefosine. It acts as an Akt inhibitor and a PI3K inhibitor.\nIt was being developed by Keryx Biopharmaceuticals who have licensed it from Æterna Zentaris Inc.\n\nIn 2010 perifosine has reached phase II. In one phase II trial for metastatic colon cancer perifosine doubled time to progression.\n\nIt has orphan drug status in the U.S. for the treatment of multiple myeloma and neuroblastoma, and for multiple myeloma in the EU.\n\nIn 2011 it was in a phase III trial for colorectal cancer, and another for multiple myeloma. On April 2, 2012, it was announced that perifosine failed its phase III clinical trial for treatment of colon cancer. Detailed results were released in June 2012. On March 11, 2013 Aeterna Zentaris announced the discontinuing of Phase 3 clinical trial of perifosine for the treatment of relapsed and refractory multiple myeloma http://www.aezsinc.com/en/page.php?p=60&q=550.\n"}
{"id": "19441259", "url": "https://en.wikipedia.org/wiki?curid=19441259", "title": "Plasma Acoustic Shield System", "text": "Plasma Acoustic Shield System\n\nThe Plasma Acoustic Shield System, or PASS, is in the process of being developed by Stellar Photonics. The company received a $2.7 million contract from the U.S. Government to build the PASS. It is part of a project supervised by the United States Army Armament Research, Development and Engineering Center. The laser was first tested in 2008, and will continue to be tested into 2009, with the testing of turret-mounted PASS.\n\nThe device is able to disorient an enemy using a series of mid-air explosions, and may also use \"high-power speakers for hailing or warning, and a dazzler light source\" Its low power would mean that it would be unable to do significant damage to a specific enemy. While it would not be classified as a weapon, because of its inability to stun or disable a target, its distracting light or explosions are hoped to impede the progress of those in its path. \"The [PASS] creates a “mid-air plasma ball” that “basically ignites the air in front of the person...It creates fireworks right in front of you.”\n\nThe PASS uses Synchronized Photo-pulse Detonation (SPD), a technology researched by Stellar Photonics wherein two short but powerful laser pulses first create a ball of plasma, then a supersonic shockwave creates a flash and a loud bang. Pass is the first functioning SPD weapon system, and it may lead to the construction of a \"man-portable tuneable laser weapon that could be used in both non-lethal and lethal modes\".\n\n\n"}
{"id": "397388", "url": "https://en.wikipedia.org/wiki?curid=397388", "title": "Rankine–Hugoniot conditions", "text": "Rankine–Hugoniot conditions\n\nThe Rankine–Hugoniot conditions, also referred to as Rankine–Hugoniot jump conditions or Rankine–Hugoniot relations,\ndescribe the relationship between the states on both sides of a shock wave or a combustion wave (deflagration or detonation) in a one-dimensional flow in fluids or a one-dimensional deformation in solids. They are named in recognition of the work carried out by Scottish engineer and physicist William John Macquorn Rankine and French engineer Pierre Henri Hugoniot.\n\nIn a coordinate system that is moving with the discontinuity, the Rankine–Hugoniot conditions can be expressed as:\nwhere \"m\" is the mass flow rate per unit area, \"ρ\" and \"ρ\" are the mass density of the fluid upstream and downstream of the wave, \"u\" and \"u\" are the fluid velocity upstream and downstream of the wave, \"p\" and \"p\" are the pressures in the two regions, and \"h\" and \"h\" are the \"specific\" (with the sense of \"per unit mass\") enthalpies in the two regions. If in addition, the flow is reactive, then the species conservation equations demands that\n\nto vanish both upstream and downstream of the discontinuity. Here, formula_3 is the mass production rate of the \"i\"th species of total \"N\" species involved in the reaction. Combining conservation of mass and momentum gives us\n\nwhich defines a straight line known as the Rayleigh line with a negative slope (since formula_5 is always positive) in the formula_6 plane. Using the Rankine-Hugoniot equations for the conservation of mass and momentum to eliminate \"u\" and \"u\", the equation for the conservation of energy can be expressed as the Hugoniot equation:\nThe inverse of the density can also be expressed as the specific volume, formula_8. Along with these, one has to specify the relation between the upstream and downstream equation of state\nwhere formula_10 is the mass fraction of the species. Finally, the calorific equation of state formula_11 is assumed to be known, i.e.,\n\nThe following assumptions are made in order to simplify the Rankine-Hugoniot equations. The mixture is assumed to obey the ideal gas law, so that relation between the downstream and upstream equation of state can be written as\n\nwhere formula_14 is the universal gas constant and the mean molecular weight formula_15 is assumed to be constant (otherwise, formula_15 would depend on the mass fraction of the all species). If one assumes that the specific heat at constant pressure formula_17 is also constant across the wave, the change in enthalpies (calorific equation of state) can be simply written as\n\nwhere the first term in the above expression represents the amount of heat released per unit mass of the upstream mixture by the wave and the second term represents the sensible heating. Eliminating temperature using the equation of state and substituting the above expression for the change in enthalpies into the Hugoniot equation, one obtains a Hugoniot equation expressed only in terms of pressure and densities,\n\nwhere formula_20 is the specific heat ratio. Hugoniot curve without heat release (formula_21) is often called as Shock Hugoniot. Along with the Rayleigh line equation, the above equation completely determines the state of the system. These two equations can be written compactly by introducing the following non-dimensional scales,\n\nThe Rayleigh line equation and the Hugoniot equation then simplifies to\n\nGiven the upstream conditions, the intersection of above two equations in the formula_24 plane determine the downstream conditions. If there is no heat release occurs, for example, shock waves without chemical reaction, then formula_25. The Hugoniot curves asymptote to the lines formula_26 and formula_27, i.e., the pressure jump across the wave can take any values between formula_28, but the specific volume ratio is restricted to the interval formula_29 (the upper bound is derived for the case formula_30 because pressure cannot take negative values). The Chapman–Jouguet condition is where Rayleigh line is tangent to the Hugoniot curve.\n\nIf formula_31 (diatomic gas without the vibrational mode excitation), the interval is formula_32, in other words, the shock wave can increase the density at most by a factor of 6. For monoatomic gas, formula_33, therefore the density ratio is limited by the interval formula_34. For diatomic gases with vibrational mode excited, we have formula_35 leading to the interval formula_36. In reality, the specific heat ratio is not constant in the shock wave due to molecular dissociation and ionization, but even in these cases, density ratio in general do not exceed the factor formula_37.\n\nConsider gas in a one-dimensional container (e.g., a long thin tube).\nAssume that the fluid is inviscid\n(i.e., it shows no viscosity effects as for example\nfriction with the tube walls).\nFurthermore, assume that there is no heat transfer by conduction or radiation and that gravitational acceleration can be neglected.\nSuch a system can be described by the following\nsystem of conservation laws,\nknown as the 1D Euler equations, that in conservation form is:\n\nwhere\n\nAssume further that the gas is calorically ideal and that therefore a polytropic equation-of-state of the simple form\n\nis valid, where formula_20 is the constant ratio of specific heats formula_48.\nThis quantity also appears as the \"polytropic exponent\"\nof the polytropic process described by\n\nFor an extensive list of compressible flow equations, etc., refer to NACA Report 1135 (1953).\n\nNote: For a calorically ideal gas formula_50 is a constant and for a thermally ideal gas formula_50 is a function of temperature. In the latter case, the dependence of pressure\non mass density and internal energy might differ from that given by\nequation (4).\n\nBefore proceeding further it is necessary to introduce the concept of a \"jump condition\" – a condition that holds at a discontinuity or abrupt change.\n\nConsider a 1D situation where there is a jump in the scalar conserved physical quantity formula_52, which is governed by integral conservation law\nfor any formula_54, formula_55, formula_56, and, therefore, by partial differential equation \nfor smooth solutions.\n\nLet the solution exhibit a jump (or shock) at formula_58, where formula_59 and formula_60, then\n\nThe subscripts \"1\" and \"2\" indicate conditions \"just upstream\" and \"just downstream\" of the jump respectively, i.e. formula_63 and formula_64.\n\nNote, to arrive at equation (8) we have used the fact that formula_65 and formula_66.\n\nNow, let formula_67 and formula_68, when we have formula_69 and formula_70, and in the limit\n\nwhere we have defined formula_72 (the system \"characteristic\" or \"shock speed\"), which by simple division is given by\n\nEquation (9) represents the jump condition for conservation law (6). A shock situation arises in a system where its \"characteristics\" intersect, and under these conditions a requirement for a unique single-valued solution is that the solution should satisfy the \"admissibility condition\" or \"entropy condition\". For physically real applications this means that the solution should satisfy the \"Lax entropy condition\"\n\nwhere formula_75 and formula_76 represent \"characteristic speeds\" at upstream and downstream conditions respectively.\n\nIn the case of the hyperbolic conservation law (6), we have seen that the shock speed can be obtained by simple division. However, for the 1D Euler equations ( 1), ( 2) and ( 3), we have the vector state variable formula_77 and the jump conditions become\n\nEquations (12), (13) and (14) are known as the \"Rankine–Hugoniot conditions\" for the Euler equations and are derived by enforcing the conservation laws in integral form over a control volume that includes the shock. For this situation formula_81 cannot be obtained by simple division. However, it can be shown by transforming the problem to a moving co-ordinate system\n(setting formula_82, formula_83, formula_84\nto remove formula_85)\nand some algebraic manipulation\n(involving the elimination of formula_86\nfrom the transformed equation (13) using the transformed equation (12)),\nthat the shock speed is given by\n\nwhere formula_88 is the speed of sound in the fluid at upstream conditions.\n\nFor shocks in solids, a closed form expression such as equation (15) cannot be derived from first principles. Instead, experimental observations indicate that a linear relation can be used instead (called the shock Hugoniot in the \"u\"-\"u\" plane) that has the form\nwhere \"c\" is the bulk speed of sound in the material (in uniaxial compression), \"s\" is a parameter (the slope of the shock Hugoniot) obtained from fits to experimental data, and \"u\"=\"u\" is the particle velocity inside the compressed region behind the shock front. \n\nThe above relation, when combined with the Hugoniot equations for the conservation of mass and momentum, can be used to determine the shock Hugoniot in the \"p\"-\"v\" plane, where \"v\" is the specific volume (per unit mass):\nAlternative equations of state, such as the Mie–Gruneisen equation of state may also be used instead of the above equation.\n\nThe shock Hugoniot describes the locus of all possible thermodynamic states a material can exist in behind a shock, projected onto a two dimensional state-state plane. It is therefore a set of equilibrium states and does not specifically represent the path through which a material undergoes transformation.\n\nWeak shocks are isentropic and that the isentrope represents the path through which the material is loaded from the initial to final states by a compression wave with converging characteristics. In the case of weak shocks, the Hugoniot will therefore fall directly on the isentrope and can be used directly as the equivalent path. In the case of a strong shock we can no longer make that simplification directly. However, for engineering calculations, it is deemed that the isentrope is close enough to the Hugoniot that the same assumption can be made.\n\nIf the Hugoniot is approximately the loading path between states for an \"equivalent\" compression wave, then the jump conditions for the shock loading path can be determined by drawing a straight line between the initial and final states. This line is called the Rayleigh line and has the following equation:\n\nMost solid materials undergo plastic deformations when subjected to strong shocks. The point on the shock Hugoniot at which a material transitions from a purely elastic state to an elastic-plastic state is called the Hugoniot elastic limit (HEL) and the pressure at which this transition takes place is denoted \"p\". Values of \"p\" can range from 0.2 GPa to 20 GPa. Above the HEL, the material loses much of its shear strength and starts behaving like a fluid.\n\n"}
{"id": "25995", "url": "https://en.wikipedia.org/wiki?curid=25995", "title": "Reciprocating engine", "text": "Reciprocating engine\n\nA reciprocating engine, also often known as a piston engine, is typically a heat engine (although there are also pneumatic and hydraulic reciprocating engines) that uses one or more reciprocating pistons to convert pressure into a rotating motion. This article describes the common features of all types. The main types are: the internal combustion engine, used extensively in motor vehicles; the steam engine, the mainstay of the Industrial Revolution; and the niche application Stirling engine. Internal combustion engines are further classified in two ways: either a spark-ignition (SI) engine, where the spark plug initiates the combustion; or a compression-ignition (CI) engine, where the air within the cylinder is compressed, thus heating it, so that the heated air ignites fuel that is injected then or earlier.\n\nThere may be one or more pistons. Each piston is inside a cylinder, into which a gas is introduced, either already under pressure (e.g. steam engine), or heated inside the cylinder either by ignition of a fuel air mixture (internal combustion engine) or by contact with a hot heat exchanger in the cylinder (Stirling engine). The hot gases expand, pushing the piston to the bottom of the cylinder. This position is also known as the Bottom Dead Center (BDC), or where the piston forms the largest volume in the cylinder. The piston is returned to the cylinder top (Top Dead Centre) (TDC) by a flywheel, the power from other pistons connected to the same shaft or (in a double acting cylinder) by the same process acting on the other side of the piston. This is where the piston forms the smallest volume in the cylinder. In most types the expanded or \"exhausted\" gases are removed from the cylinder by this stroke. The exception is the Stirling engine, which repeatedly heats and cools the same sealed quantity of gas. The stroke is simply the distance between the TDC and the BDC, or the greatest distance that the piston can travel in one direction.\n\nIn some designs the piston may be powered in both directions in the cylinder, in which case it is said to be double-acting.\n\nIn most types, the linear movement of the piston is converted to a rotating movement via a connecting rod and a crankshaft or by a swashplate or other suitable mechanism. A flywheel is often used to ensure smooth rotation or to store energy to carry the engine through an un-powered part of the cycle. The more cylinders a reciprocating engine has, generally, the more vibration-free (smoothly) it can operate. The power of a reciprocating engine is proportional to the volume of the combined pistons' displacement.\n\nA seal must be made between the sliding piston and the walls of the cylinder so that the high pressure gas above the piston does not leak past it and reduce the efficiency of the engine. This seal is usually provided by one or more piston rings. These are rings made of a hard metal, and are sprung into a circular groove in the piston head. The rings fit closely in the groove and press lightly against the cylinder wall to form a seal, and more heavily when higher combustion pressure moves around to their inner surfaces.\n\nIt is common to classify such engines by the number and alignment of cylinders and total volume of displacement of gas by the pistons moving in the cylinders usually measured in cubic centimetres (cm³ or cc) or litres (l) or (L) (US: liter). For example, for internal combustion engines, single and two-cylinder designs are common in smaller vehicles such as motorcycles, while automobiles typically have between four and eight, and locomotives, and ships may have a dozen cylinders or more. Cylinder capacities may range from 10 cm³ or less in model engines up to thousands of liters in ships' engines.\n\nThe compression ratio affects the performance in most types of reciprocating engine. It is the ratio between the volume of the cylinder, when the piston is at the bottom of its stroke, and the volume when the piston is at the top of its stroke.\n\nThe bore/stroke ratio is the ratio of the diameter of the piston, or \"bore\", to the length of travel within the cylinder, or \"stroke\". If this is around 1 the engine is said to be \"square\", if it is greater than 1, i.e. the bore is larger than the stroke, it is \"oversquare\". If it is less than 1, i.e. the stroke is larger than the bore, it is \"undersquare\".\n\nCylinders may be aligned in line, in a V configuration, horizontally opposite each other, or radially around the crankshaft. Opposed-piston engines put two pistons working at opposite ends of the same cylinder and this has been extended into triangular arrangements such as the Napier Deltic. Some designs have set the cylinders in motion around the shaft, such as the Rotary engine.\n\nIn steam engines and internal combustion engines, valves are required to allow the entry and exit of gases at the correct times in the piston's cycle. These are worked by cams, eccentrics or cranks driven by the shaft of the engine. Early designs used the D slide valve but this has been largely superseded by Piston valve or Poppet valve designs. In steam engines the point in the piston cycle at which the steam inlet valve closes is called the cutoff and this can often be controlled to adjust the torque supplied by the engine and improve efficiency. In some steam engines, the action of the valves can be replaced by an oscillating cylinder.\n\nInternal combustion engines operate through a sequence of strokes that admit and remove gases to and from the cylinder. These operations are repeated cyclically and an engine is said to be 2-stroke, 4-stroke or 6-stroke depending on the number of strokes it takes to complete a cycle.\n\nIn some steam engines, the cylinders may be of varying size with the smallest bore cylinder working the highest pressure steam. This is then fed through one or more, increasingly larger bore cylinders successively, to extract power from the steam at increasingly lower pressures. These engines are called Compound engines.\n\nAside from looking at the power that the engine can produce, the Mean Effective Pressure (MEP), can also be used in comparing the power output and performance of reciprocating engines of the same size. The mean effective pressure is the fictitious pressure which would produce the same amount of net work that was produced during the power stroke cycle. This is shown by:\n\nand therefore:\n\nWhichever engine with the larger value of MEP produces more net work per cycle and performs more efficiently.\n\nAn early known example of rotary to reciprocating motion is the crank mechanism. The earliest hand-operated cranks appeared in China during the Han Dynasty (202 BC–220 AD). Several saw mills in Roman Asia and Byzantine Syria during the 3rd–6th centuries AD had a crank and connecting rod mechanism which converted the rotary motion of a water wheel into the linear movement of saw blades. In 1206, Arab engineer Al-Jazari invented a crankshaft.\n\nThe reciprocating engine developed in Europe during the 18th century, first as the atmospheric engine then later as the steam engine. These were followed by the Stirling engine and internal combustion engine in the 19th century. Today the most common form of reciprocating engine is the internal combustion engine running on the combustion of petrol, diesel, Liquefied petroleum gas (LPG) or compressed natural gas (CNG) and used to power motor vehicles and engine power plants.\n\nOne notable reciprocating engine from the World War II Era was the 28-cylinder, Pratt & Whitney R-4360 \"Wasp Major\" radial engine. It powered the last generation of large piston-engined planes before jet engines and turboprops took over from 1944 onward. It had a total engine capacity of , and a high power-to-weight ratio.\n\nThe largest reciprocating engine in production at present, but not the largest ever built, is the Wärtsilä-Sulzer RTA96-C turbocharged two-stroke diesel engine of 2006 built by Wärtsilä. It is used to power the largest modern container ships such as the Emma Mærsk. It is five stories high (), long, and weighs over in its largest 14 cylinders version producing more than 84.42 MW (114,800 bhp). Each cylinder has a capacity of , making a total capacity of for the largest versions.\n\nFor piston engines, an engine's capacity is the engine displacement, in other words the volume swept by all the pistons of an engine in a single movement. It is generally measured in litres (l) or cubic inches (c.i.d., cu in, \"or\" in³) for larger engines, and cubic centimetres (abbreviated cc) for smaller engines. All else being equal, engines with greater capacities are more powerful and consumption of fuel increases accordingly (although this is not true of every Reciprocating engine), although power and fuel consumption are affected by many factors outside of engine displacement.\n\nReciprocating engines that are powered by compressed air, steam or other hot gases are still used in some applications such as to drive many modern torpedoes or as pollution-free motive power. Most steam-driven applications use steam turbines, which are more efficient than piston engines.\n\nThe French-designed FlowAIR vehicles use compressed air stored in a cylinder to drive a reciprocating engine in a local-pollution-free urban vehicle.\n\nTorpedoes may use a working gas produced by high test peroxide or Otto fuel II, which pressurise without combustion. The Mark 46 torpedo, for example, can travel underwater at fuelled by Otto fuel without oxidant.\n\nQuantum heat engines are devices that generate power from heat that flows from a hot to a cold reservoir.\nThe mechanism of operation of the engine can be described by the laws of quantum mechanics. \nQuantum refrigerators are devices that consume power with the purpose to pump heat from a cold to a hot reservoir.\n\nIn a reciprocating quantum heat engine, the working medium is a quantum system such as spin systems or a harmonic oscillator.\nThe Carnot cycle and Otto cycle are the ones most studied.\nThe quantum versions obey the laws of thermodynamics. In addition, these models can justify the assumptions of\nendoreversible thermodynamics.\nA theoretical study has shown that it is possible and practical to build a reciprocating engine that is composed of a single oscillating atom. This is an area for future research and could have applications in nanotechnology.\n\nThere are a large number of unusual varieties of piston engines that have various claimed advantages, many of which see little if any current use:\n\n"}
{"id": "23340147", "url": "https://en.wikipedia.org/wiki?curid=23340147", "title": "Reisholz Rhine Powerline Crossing", "text": "Reisholz Rhine Powerline Crossing\n\nReisholz Rhine Powerline Crossing is a powerline crossing of Rhine River in Düsseldorf south of Holthausen substation, which was built in 1917 and originally part of the 110 kV-line between Reisholz Power Station and Goldenberg Power Station. This line was the first 110 kV-powerline of RWE. Later the line was transformed into a 220 kV line with 12 conductors arranged in 4 levels in an unusual double danube pylon mode.\nThe span width between the pylons, which are both tall, is . \nThe pylon on the east side of Rhine River has a unique feature as under its legs a normal-gauge railway connecting Holthausen substation with the railway grid passes.\n\n\n"}
{"id": "1656730", "url": "https://en.wikipedia.org/wiki?curid=1656730", "title": "Sequential hermaphroditism", "text": "Sequential hermaphroditism\n\nSequential hermaphroditism (called dichogamy in botany) is a type of hermaphroditism that occurs in many fish, gastropods, and plants. Sequential hermaphroditism occurs when the individual changes sex at some point in its life. \n\nIn animals, the different types of change are male to female (protandry), female to male (protogyny), female to hermaphrodite (protogynous hermaphroditism), and male to hermaphrodite (protandrous hermaphroditism). Those that change gonadal sex can have both female and male germ cells in the gonads or can change from one complete gonadal type to the other during their last life stage. \n\nIn plants, individual flowers are called dichogamous if their function has the two sexes separated in time, although the plant as a whole may have functionally male and functionally female flowers open at any one moment. A flower is protogynous if its function is first female, then male, and protandrous if its function is male then female. It used to be thought that this reduced inbreeding, but it may be a more general mechanism for reducing pollen-pistil interference.\n\nProtandrous hermaphrodites are animals that are born male and at some point in their lifespan change sex to female. Protandrous animals include clownfish. Clownfish have a very structured society. In the \"Amphiprion percula\" species, there are zero to four individuals excluded from breeding and a breeding pair living in a sea anemone. Dominance is based on size, the female being the largest and the male being the second largest. The rest of the group is made up of progressively smaller non-breeders, which have no functioning gonads. If the female dies, the male gains weight and becomes the female for that group. The largest non-breeding fish then sexually matures and becomes the male of the group.\n\nOther protandrous fishes can be found in the classes clupeiformes, siluriformes, stomiiformes, and within the perciform families pomacentridae and sparidae. Since these groups are distantly related and have many intermediate relatives that are not protandrous, it strongly suggests that protandry evolved multiple times. Other examples of protandrous animals include:\n\nProtogynous hermaphrodites are animals that are born female and at some point in their lifespan change sex to male. As the animal ages, based on internal or external triggers, it shifts sex to become a male animal. Male fecundity increases greatly with age, unlike female.\n\nProtogyny is the most common form of hermaphroditism in fish in nature. About 75% of the 500 known sequentially hermaphroditic fish species are protogynous. Common model organisms for this type of sequential hermaphroditism are wrasses. They are one of the largest families of coral reef fish and belong to the family Labridae. Wrasses are found around the world in all marine habitats and tend to bury themselves in sand at night or when they feel threatened. In wrasses, the larger of the two fish is the male, while the smaller is the female. In most cases, females and immature have a uniform color while the male has the terminal bicolored phase. Large males hold territories and try to pair spawn while small to mid-size initial-phase males live with females and group spawn. In other words, both the initial and terminal phase males can breed, but they differ in the way they do it.\n\nIn the California sheephead (\"Semicossyphus pulcher\"), a type of wrasse, when the female changes to male, the ovaries degenerate and spermatogenic crypts appear in the gonads. The general structure of the gonads remains ovarian after the transformation and the sperm is transported through a series of ducts on the periphery of the gonad and oviduct. Here sex change is age-dependent. For example, the California sheephead stays a female for four years before changing sex.\n\nBlue-headed wrasse begin life as males or females, but females can change sex and function as males. Young females and males start with a distinct coloration known as the \"Initial Phase\" before progressing into the \"Terminal Phase\" coloration, which has a change in intensity of color, stripes, and bars. Initial Phase males have larger testes than larger, terminal phase males, which enables the initial phase males to produce a large amount of sperm. This strategy is able to compete with that of the larger male, who is able to guard his own harem.\n\n\"Botryllus schlosseri\", a colonial tunicate, is a protogynous hermaphrodite. In a colony, eggs are released about two days before the peak of sperm emission. Although self-fertilization is avoided and cross-fertilization favored by this strategy, self-fertilization is still possible. Self-fertilized eggs develop with a substantially higher frequency of anomalies during cleavage than cross-fertilized eggs (23% vs. 1.6%). Also a significantly lower percentage of larvae derived from self-fertilized eggs metamorphose, and the growth of the colonies derived from their metamorphosis is significantly lower. These findings suggest that self-fertilization gives rise to inbreeding depression associated with developmental deficits that are likely caused by expression of deleterious recessive mutations.\n\nOther examples of protogynous organisms include:\n\n\nThe ultimate cause of a biological phenomenon concerns why that phenomenon makes organisms better adapted to their environment, and thus why evolution by natural selection has produced that phenomenon. This is in contrast to proximate causes, which concern the molecular and physiological mechanisms that produce the phenomenon. A number of ultimate causes of hermaphroditism have been proposed, of which two are most relevant to sequential hermaphroditism.\n\nThe 'size-advantage model' states that individuals of a given sex reproduce more effectively if they are a certain size or age. To create selection for sequential hermaphroditism, small individuals must have higher reproductive fitness as one sex and larger individuals must have higher reproductive fitness as the opposite sex. For example, eggs are larger than sperm, thus larger individuals are able to make more eggs, so individuals could maximize their reproductive potential by beginning life as male and then turning female upon achieving a certain size.\n\nSequential hermaphroditism can also protect against inbreeding in populations of organisms that have low enough motility and/or are sparsely distributed enough that there is a considerable risk of siblings encountering each other after reaching sexual maturity, and interbreeding. If siblings are all the same or similar ages, and if they all begin life as one sex and then transition to the other sex at about the same age, then siblings are highly likely to be the same sex at any given time. This should dramatically reduce the likelihood of inbreeding. Both protandry and protogyny are known to help prevent inbreeding in plants, and many examples of sequential hermaphroditism attributable to inbreeding prevention have been identified in a wide variety of animals.\n\nIn most ectotherms body size and female fecundity are positively correlated. This supports the size-advantage model. Kazancioglu and Alonzo (2010) performed the first comparative analysis of sex change in Labridae. Their analysis supports the size-advantage model and suggest that sequential hermaphroditism is correlated to the size-advantage. They determined that dioecy was less likely to occur when the size advantage is stronger than other advantages\nWarner suggests that selection for protandry may occur in populations where female fecundity is augmented with age and individuals mate randomly. Selection for protogyny may occur where there are traits in the population that depress male fecundity at early ages (territoriality, mate selection or inexperience) and when female fecundity is decreased with age, the latter seems to be rare in the field. An example of territoriality favoring protogyny occurs when there is a need to protect their habitat and being a large male is advantageous for this purpose. In the mating aspect, a large male has a higher chance of mating, while this has no effect on the female mating fitness. Thus, he suggests that female fecundity has more impact on sequential hermaphroditism than the age structures of the population.\n\nThe size-advantage model predicts that sex change would only be absent if the relationship between size/age with reproductive potential is identical in both sexes. With this prediction one would assume that hermaphroditism is very common, but this is not the case. Sequential hermaphroditism is very rare and according to scientists this is due to some cost that decreases fitness in sex changers as opposed to those who don’t change sex. Some of the hypotheses proposed for the dearth of hermaphrodites are the energetic cost of sex change, genetic and/or physiological barriers to sex change, and sex-specific mortality rates.\n\nIn 2009, Kazanciglu and Alonzo found that dioecy was only favored when the cost of changing sex was very large. This indicates that the cost of sex change does not explain the rarity of sequential hermaphroditism by itself.\n\nMany studies have focused on the proximate causes of sequential hermaphroditism. The role of aromatase has been widely studied in this area. Aromatase is an enzyme that controls the androgen/estrogen ratio in animals by catalyzing the conversion of testosterone into oestradiol, which is irreversible. It has been discovered that the aromatase pathway mediates sex change in both directions. Many studies also involve understanding the effect of aromatase inhibitors on sex change. One such study was performed by Kobayashi et al. In their study they tested the role of estrogens in male three-spot wrasses (\"Halichoeres trimaculatus\"). They discovered that fish treated with aromatase inhibitors showed decreased gonodal weight, plasma estrogen level and spermatogonial proliferation in the testis as well as increased androgen levels. Their results suggest that estrogens are important in the regulation of spermatogenesis in this protogynous hermaphrodite.\n\nSequential hermaphrodites almost always have a sex ratio biased towards the birth sex, and consequently experience significantly more reproductive success after switching sexes. In theory, this should decrease genetic diversity and effective population size (Ne). However, a study of two ecologically similar santer sea bream (gonochoric) and slinger sea bream (protogynous) in South African waters found that genetic diversities were similar in the two species, and while Ne was lower in the instant for the sex-changer, they were similar over a relatively short time horizion.\n\nIn the context of the plant sexuality of flowering plants (angiosperms), there are two forms of dichogamy: protogyny—female function precedes male function—and protandry—male function precedes female function.\n\nHistorically, dichogamy has been regarded as a mechanism for reducing inbreeding. However, a survey of the angiosperms found that self-incompatible (SI) plants, which are incapable of inbreeding, were as likely to be dichogamous as were self-compatible (SC) plants. This finding led to a reinterpretation of dichogamy as a more general mechanism for reducing the impact of pollen-pistil interference on pollen import and export.\nUnlike the inbreeding avoidance hypothesis, which focused on female function, this interference-avoidance hypothesis considers both reproductive functions.\n\nIn many hermaphroditic species, the close physical proximity of anthers and stigma makes interference unavoidable, either within a flower or between flowers on an inflorescence. Within-flower interference, which occurs when either the pistil interrupts pollen removal or the anthers prevent pollen deposition, can result in autonomous or facilitated self-pollination. Between-flower interference results from similar mechanisms, except that the interfering structures occur on different flowers within the same inflorescence and it requires pollinator activity. This results in geitonogamous pollination, the transfer of pollen between flowers of the same individual. In contrast to within-flower interference, geitonogamy necessarily involves the same processes as outcrossing: pollinator attraction, reward provisioning, and pollen removal. Therefore, between-flower interference not only carries the cost of self-fertilization (inbreeding depression), but also reduces the amount of pollen available for export (so-called \"pollen discounting\"). Because pollen discounting diminishes outcross siring success, interference avoidance may be an important evolutionary force in floral biology.\nDichogamy may reduce between-flower interference by minimizing the temporal overlap between stigma and anthers within an inflorescence. Large inflorescences attract more pollinators, potentially enhancing reproductive success by increasing pollen import and export. However, large inflorescences also increase the opportunities for both geitonogamy and pollen discounting, so that the opportunity for between-flower interference increases with inflorescence size. Consequently, the evolution of floral display size may represent a compromise between maximizing pollinator visitation and minimizing geitonogamy and pollen discounting (Barrett et al., 1994).\n\nProtandry may be particularly relevant to this compromise, because it often results in an inflorescence structure with female phase flowers positioned below male phase flowers. Given the tendency of many insect pollinators to forage upwards through inflorescences, protandry may enhance pollen export by reducing between-flower interference. Furthermore, this enhanced pollen export should increase as floral display size increases, because between-flower interference should increase with floral display size. These effects of protandry on between-flower interference may decouple the benefits of large inflorescences from the consequences of geitonogamy and pollen discounting. Such a decoupling would provide a significant reproductive advantage through increased pollinator visitation and siring success.\n\nHarder et al. (2000) demonstrated experimentally that dichogamy both reduced rates of self-fertilization and enhanced outcross siring success through reductions in geitonogamy and pollen discounting, respectively. Routley & Husband (2003) examined the influence of inflorescence size on this siring advantage and found a bimodal distribution with increased siring success with both small and large display sizes.\n\nThe length of stigmatic receptivity plays a key role in regulating the isolation of the male and female stages in dichogamous plants, and stigmatic receptivity can be influenced by both temperature and humidity. Another study by Jersakova and Johnson, studied the effects of protandry on the pollination process of the moth pollinated orchid, \"Satyrium longicauda\". They discovered that protandry tended to reduce the absolute levels of self-pollination and suggest that the evolution of protandry could be driven by the consequences of the pollination process for male mating success. Another study that indicated that dichogamy might increase male pollination success was by Dai and Galloway.\n\n"}
{"id": "408201", "url": "https://en.wikipedia.org/wiki?curid=408201", "title": "Sieve", "text": "Sieve\n\nA sieve, or sifter, is a device for separating wanted elements from unwanted material or for characterizing the particle size distribution of a sample, typically using a woven screen such as a mesh or net or metal. The word \"sift\" derives from \"sieve\". In cooking, a sifter is used to separate and break up clumps in dry ingredients such as flour, as well as to aerate and combine them. A strainer is a form of sieve used to separate solids from liquid.\n\nSome industrial strainers available are simplex basket strainers, duplex basket strainers, and Y strainers. Simple basket strainers are used to protect valuable or sensitive equipment in systems that are meant to be shut down temporarily. Some commonly used strainers are bell mouth strainers, foot valve strainers, basket strainers. Most processing industries (mainly pharmaceutical, coatings and liquid food industries) will opt for a self-cleaning strainer instead of a basket strainer or a simplex strainer due to limitations of simple filtration systems. The self-cleaning strainers or filters are more efficient and provide an automatic filtration solution.\n\nSieving is a simple technique for separating particles of different sizes. A sieve such as used for sifting flour has very small holes. Coarse particles are separated or broken up by grinding against one-another and screen openings. Depending upon the types of particles to be separated, sieves with different types of holes are used. Sieves are also used to separate stones from sand. Sieving plays an important role in food industries where sieves (often vibrating) are used to prevent the contamination of the product by foreign bodies. The design of the industrial sieve is here of primary importance.\n\nTriage sieving refers to grouping people according to their severity of injury.\n\nThe mesh in a wooden sieve might be made from wood or wicker. Use of wood to avoid contamination is important when the sieve is used for sampling. Henry Stephens, in his \"Book of the Farm\", advised that the withes of a wooden riddle or sieve be made from fir or willow with American elm being best. The rims would be made of fir, oak or, especially, beech.\n\nA sieve analysis (or gradation test) is a practice or procedure used (commonly used in civil engineering or sedimentology) to assess the particle size distribution (also called gradation) of a granular material. Sieve sizes used in combinations of four to eight sieves.\nDesignations and Nominal Sieve Openings\n\n"}
{"id": "44636797", "url": "https://en.wikipedia.org/wiki?curid=44636797", "title": "Silchester Ogham stone", "text": "Silchester Ogham stone\n\nThe Silchester Ogham stone is a pillar stone discovered in Silchester, Hampshire during excavations in 1893. Thus far it remains the only one of its kind found in England, and the only ogham inscription in England east of Cornwall and Devon.\n\nThe pillar stone was found upside down some five to six feet beneath the surface, apparently at the bottom of a former well. A white metal or pewter vessel \"of peculiar form\" lay underneath, crushed by it. Further investigation revealed an ogham inscription upon the pillar. According to Amanda Clarke and Michael Fulford of the University of Reading,\n\nRhys noted that the sandstone marker was \"rudely carved into what seems to me to be a phallic form\", though the upper part was broken off and missing. What was left \"may be described as the frustum of a cone, below which the stone narrows greatly, then widens into a moulded base.\" The inscription is in two lines beginning at the widest part of the frustum at its base.\n\nThe pewter vessel, when reconstructed, turned out to be \"a simple, biconical flagon similar to a few other examples found in southern Britain which can be dated broadly to the fourth and no earlier than the late third century.\" The most recent version of the translation on the stone is \"[The something] of Tebicatus, son of the tribe of N”. The missing word may be \"memorial\" or, less probably, \"land\", if the stone were a territorial marker rather than a grave marker. J. A. Baird argues that both archaeological and linguistic evidence suggest that the stone belongs to \"the earliest phases of the ogham script\", which emerges with contact between late Roman Latin and Old Irish.\n\nOn the basis of existing evidence, Reading University says that the \"inscription has been interpreted as an epitaph and it has been variously dated to the fifth and sixth centuries\". Baird says that it \"could well date to the late fourth or fifth century\". This would make it \"one of the earliest ogham stones anywhere.\"\n\nThe inscription was created during Sub-Roman Britain, a period of intense cross-cultural contact between Britain and Ireland at the time of the Anglo-Saxon invasions. Most ogham inscriptions are present in the British strongholds of Cornwall, Devon and Wales. The Silchester inscription is in Hampshire — east of Devon — and is presumed to be the work of Irish settlers occupying land further east than other known Irish migrants. The tradition of the expulsion of the Déisi and their migration from Ireland may be related to this pattern of settlement and British-Irish cultural integration.\n\n"}
{"id": "32321480", "url": "https://en.wikipedia.org/wiki?curid=32321480", "title": "Sweden National Renewable Energy Action Plan", "text": "Sweden National Renewable Energy Action Plan\n\nThe Sweden National Renewable Energy Action Plan is the National Renewable Energy Action Plan (NREAP) for Sweden. The plan was commissioned by the Directive 2009/28/EC which required Member States of the European Union to notify the European Commission with a road map. The report describes how Sweden planned to achieve its legally binding target of a 49% share of energy from renewable sources in gross final consumption of energy by 2020. \n\nIn the NREAP, the Federal Government estimates the share of renewable energies in gross final energy consumption to be 50.2% in 2020. The share of renewable energies in the electricity sector will thereby amount to 62.8%, the share in the heating/cooling sector will be 62%, while in the transport sector it will amount to 12.4%.\n\nThe Swedish licensing and legislative system is so structured that the Swedish Parliament and the Government govern through laws and regulations that are applied by national authorities that autonomously interpret and implement these laws and regulations. At a national level, there are regional government authorities and county administrative boards that have the task of coordinating and implementing the targets and mandates stipulated by the national authorities. \n\nSpecial for Wind Power \n\nThere are four wind power coordinators and a national network for wind power within the area of wind power. Furthermore, the Swedish Energy Agency has, on behalf of the Government, developed an Internet-based manual about wind power (Vindlov.se) for all of the information that is needed for licensing matters regarding wind power and information on most things from concept stage to the commissioning of the completed wind farm. \n\nSpecial for Solar Power \n\nSolar heating installations need to meet certain quality standards in order to benefit from solar heating funding.\nFinancial Support Systems in Sweden\n"}
{"id": "330981", "url": "https://en.wikipedia.org/wiki?curid=330981", "title": "Tau (particle)", "text": "Tau (particle)\n\nThe tau (τ), also called the tau lepton, tau particle, or tauon, is an elementary particle similar to the electron, with negative electric charge and a spin of. Together with the electron, the muon, and the three neutrinos, it is a lepton. Like all elementary particles with half-integer spin, the tau has a corresponding antiparticle of opposite charge but equal mass and spin, which in the tau's case is the antitau (also called the \"positive tau\"). Tau particles are denoted by and the antitau by .\n\nTau leptons have a lifetime of and a mass of (compared to for muons and for electrons). Since their interactions are very similar to those of the electron, a tau can be thought of as a much heavier version of the electron. Because of their greater mass, tau particles do not emit as much bremsstrahlung radiation as electrons; consequently they are potentially highly penetrating, much more so than electrons.\n\nBecause of their short lifetime, the range of the tau is mainly set by their decay length, which is too small for bremsstrahlung to be noticeable. Their penetrating power appears only at ultra-high velocity / ultra-high energy (above PeV energies), when time dilation extends their path-length.\n\nAs with the case of the other charged leptons, the tau has an associated tau neutrino, denoted by .\n\nThe tau was anticipated in a 1971 paper by Yung-Su Tsai. Providing the theory for this discovery, the tau was detected in a series of experiments between 1974 and 1977 by Martin Lewis Perl with his and Tsai's colleagues at the SLAC-LBL group. Their equipment consisted of SLAC's then-new – colliding ring, called SPEAR, and the LBL magnetic detector. They could detect and distinguish between leptons, hadrons and photons. They did not detect the tau directly, but rather discovered anomalous events:\n\nThe need for at least two undetected particles was shown by the inability to conserve energy and momentum with only one. However, no other muons, electrons, photons, or hadrons were detected. It was proposed that this event was the production and subsequent decay of a new particle pair:\n\nThis was difficult to verify, because the energy to produce the pair is similar to the threshold for D meson production. The mass and spin of the tau was subsequently established by work done at DESY-Hamburg with the Double Arm Spectrometer (DASP), and at SLAC-Stanford with the SPEAR Direct Electron Counter (DELCO), \n\nThe symbol τ was derived from the Greek \"τρίτον\" (\"triton\", meaning \"third\" in English), since it was the third charged lepton discovered.\n\nMartin Lewis Perl shared the 1995 Nobel Prize in Physics with Frederick Reines. The latter was awarded his share of the prize for experimental discovery of the neutrino. \n\nThe tau is the only lepton that can decay into hadrons – the other leptons do not have the necessary mass. Like the other decay modes of the tau, the hadronic decay is through the weak interaction.\n\nThe branching ratio of the dominant hadronic tau decays are:\n\nIn total, the tau lepton will decay hadronically approximately 64.79% of the time.\n\nSince the tauonic lepton number is conserved in weak decays, a tau neutrino is always created when a tau decays.\n\nThe branching ratio of the common purely leptonic tau decays are:\nThe similarity of values of the two branching ratios is a consequence of lepton universality.\n\nThe tau lepton is predicted to form exotic atoms like other charged subatomic particles. One of such, called tauonium by the analogy to muonium, consists of an antitauon and an electron: .\n\nAnother one is an onium atom called \"true tauonium\" and is difficult to detect due to tau's extremely short lifetime at low (non-relativistic) energies needed to form this atom. Its detection is important for quantum electrodynamics.\n\n\n"}
{"id": "24824041", "url": "https://en.wikipedia.org/wiki?curid=24824041", "title": "The Idiot Cycle", "text": "The Idiot Cycle\n\nThe Idiot Cycle is a 2009 French-Canadian documentary which alleges that six major chemical companies are responsible for decades of cancer causing chemicals and pollution, and also develop cancer treatments and drugs. It also argues these companies own the most patents on genetically modified crops that have never been tested for long term health impacts like cancer.\n\nThe film was shot in Canada (Sarnia, Ottawa, Toronto, ON and Saskatoon, SK), the United States (Memphis, TN, Midland, MI, and Washington DC), The Netherlands (Amsterdam), France (Agen, Paris), Italy (Bologna), Germany (Weilheim, Frankfurt, Bad Neihum, Burladingen, Tübingen), Belgium (Brussels), England (London, Hemel Hempstead), and Northern Ireland (Coleraine).\n\n\n\n"}
{"id": "38759902", "url": "https://en.wikipedia.org/wiki?curid=38759902", "title": "Tulsi Gowda", "text": "Tulsi Gowda\n\nTulsi Gowda environmentalist from Honnali village, Ankola taluk in Karnataka state. She has planted 100,000 saplings and looks after the nurseries of the Forest department. Though she is an illiterate, she has made immense contributions towards preserving the environment. Her work has been honoured by the government and various organisations.\n"}
{"id": "42867775", "url": "https://en.wikipedia.org/wiki?curid=42867775", "title": "United States Oil &amp; Gas Association", "text": "United States Oil &amp; Gas Association\n\nThe United States Oil & Gas Association, formerly the Mid-Continent Oil & Gas Association, is a trade association which promotes the well-being of the oil and natural gas industries in the United States. Primarily, the organization focuses on the production of these resources. Other organizations exist to deal with concerns of transportation, refining and processing, and other discrete functions of the fossil fuel industry.\n\nThe predecessor organization, Mid-Continent Oil & Gas Association, was founded on October 13, 1917, after the entry of the United States into World War I, in Tulsa, Oklahoma, which called itself \"The Oil Capital of the World\". At its creation, the association worked to provide petroleum to the Allied forces. In the decades since its establishment, the association is recognized as a leading advocate for producers of domestic oil and gas.\n\nBeginning in 1919, local divisions of the association were created in several states.The Oklahoma-Kansas Division was established that year under the leadership of Frank Phillips, a founder of Phillips Petroleum Company, as well as oil company entrepreneurs William G. Skelly of Tulsa and H. H. Champlin of Enid, Oklahoma. E. W. Marland, whose company became Conoco, Inc., was later the governor of Oklahoma from 1935-1939. Alfred M. Landon, later the governor of Kansas from 1935-1939 and the 1936 Republican presidential nominee, was also instrumental in the establishment of the Oklahoma-Kansas division. \n\nAs of June 2018, Bloomberg, LP, lists Mid-Continent Oil and Gas Association of Oklahoma, Inc. located at 6701 North Broadway, Suite 300 Oklahoma City, OK 73116, and states that its business is to, \"... support legislation for the energy industry at the Oklahoma State Capitol and to provide education programs and seminars.\nThe Louisiana Oil and Gas Association was founded in 1923, with emphasis in Louisiana and the Gulf Coast. Headed by its president, former U.S. Representative Chris John of Louisiana's 7th congressional district, since disbanded, LMOGA represents companies involved in exploration and production, refining, transportation, and marketing as well as other firms in the fields of engineering, environment, finance, law, and government relations. \n\nThe Texas association was also established in 1923 and renamed the Texas Oil & Gas Association in 1997.\n\n"}
{"id": "11316751", "url": "https://en.wikipedia.org/wiki?curid=11316751", "title": "Valdecaballeros Nuclear Power Plant", "text": "Valdecaballeros Nuclear Power Plant\n\nValdecaballeros Nuclear Power Plant is an abandoned nuclear power plant in the Valdecaballeros municipality, Badajoz Province, Extremadura, Spain. It was under construction in 1983 when the Spanish nuclear power expansion program was cancelled following a change of government. Its two BWRs, each of 975 MWe, were mothballed, one 60% complete and the other 70% complete. In 1994, the decision was taken that the plant would not be completed.\n\nThe abandoned nuclear power plant is 4 km north of the town, close to the Guadalupe River which has its mouth in the Guadiana at the Garcia de Sola Dam only 2 km downriver from the plant.\n\n"}
{"id": "12681709", "url": "https://en.wikipedia.org/wiki?curid=12681709", "title": "Velocity receiver", "text": "Velocity receiver\n\nA velocity receiver (velocity sensor) is a sensor that responds to velocity rather than absolute position. For example, dynamic microphones are velocity receivers. Likewise, many electronic keyboards used for music are velocity sensitive, and may be said to possess a velocity receiver in each key. Most of these function by measuring the time difference between switch closures at two different positions along the travel of each key.\n\nThere are two types of velocity receivers, moving coil and piezoelectric. The former contains a coil supported by springs and a permanently fixed magnet and require no output signal amplifiers. Movement causes the coil to move relative to the magnet, which in turn generates a voltage that is proportional to the velocity of that movement.\n\nPiezoelectric sensor velocity receivers are similar to a piezoelectric accelerometer, except that the output of the device is proportional to the velocity of the transducer. Unlike the moving coil variety, piezoelectric sensors will likely require an amplifier due to the small generated signal. \n"}
{"id": "182188", "url": "https://en.wikipedia.org/wiki?curid=182188", "title": "Volkswagen Golf", "text": "Volkswagen Golf\n\nThe Volkswagen Golf () is a compact car produced by the German automotive manufacturer Volkswagen since 1974, marketed worldwide across seven generations, in various body configurations and under various nameplates – such as the Volkswagen Rabbit in the United States and Canada (Mk1 and Mk5), and as the Volkswagen Caribe in Mexico (Mk1).\n\nThe original Golf Mk1 was a front-wheel drive, front-engined replacement for the air-cooled, rear-engined, rear-wheel drive Volkswagen Beetle. Historically, the Golf is Volkswagen's best-selling model and is among the world's top three best-selling models, with more than 30 million built by June 2013.\n\nInitially, most Golf production was in the 3-door hatchback style. Other variants include a 5-door hatchback, station wagon (Variant, from 1993), convertible (Cabriolet and Cabrio, 1979–2002, Cabriolet, 2011–present), and a Golf-derived notchback sedan, variously called Volkswagen Jetta, Volkswagen Vento (from 1992) or Volkswagen Bora (from 1999). The cars have filled many market segments, from being a basic, everyday car, to high-performance hot hatches.\n\nThe Volkswagen Golf has won many awards throughout its history. The Golf won the World Car of the Year in 2009, with the Volkswagen Golf Mk6 and in 2013 with the Volkswagen Golf Mk7. The VW Golf is one of only three cars, the others being the Renault Clio and Opel/Vauxhall Astra, to have been voted European Car of the Year twice, in 1992 and 2013. The Volkswagen Golf has made the annual Car and Driver 10Best list multiple times. The Golf Mk7 won the Motor Trend Car of the Year award in 2015, and the Mk1 GTI also won the award in 1985.\n\nIn May 1974, Volkswagen presented the first-generation Golf as a modern front-wheel-drive, long-range replacement for the Volkswagen Beetle. Later Golf variations included the Golf GTI (introduced in June 1976 with a fuel-injected 1.6-litre engine capable of ), a diesel-powered version (from September 1976), the Jetta notchback saloon version (from October 1979), the Volkswagen Golf Cabriolet (from January 1980) and a Golf-based pickup, the Volkswagen Caddy.\n\nThe Golf Mk1 was sold as the Volkswagen Rabbit in the United States and Canada and as the Volkswagen Caribe in Mexico.\n\nA facelifted version of the Golf Mk1 was produced in South Africa as the Citi Golf from 1984 to 2009.\n\nSeptember 1983 saw the introduction of the second-generation Golf (Mk2) that grew slightly in terms of wheelbase, exterior and interior dimensions, while retaining, in a more rounded form, the Mk1's overall look. Although it was available on the home market and indeed most other left-hand drive markets by the end of 1983, it was not launched in the UK until March 1984. \n\nThe Mk2 GTI featured a 1.8-litre 8-valve fuel-injected engine from its launch, with a 16-valve version capable of more than being introduced in 1985.\n\nIn 1985, the first Golfs with four-wheel-drive (Golf Syncro) went on sale with the same Syncro four-wheel-drive system being employed on the supercharged G60 models, exclusively released in continental Europe in 1989 with and anti-lock brakes (ABS).\n\nA Mk2-based second generation Jetta was unveiled in January 1984. There was no Mk2-based cabriolet model; instead, the Mk1 Cabriolet was continued over the Mk2's entire production run.\n\nThe third-generation Golf (Mk3) made its home-market début in August 1991 and again grew slightly in comparison with its immediate predecessor, while its wheelbase remained unchanged.\n\nNew engines included the first Turbocharged Direct Injection (TD) diesel engine in a Golf, and a narrow-angle 2.8-litre VR6 engine. US Environmental Protection Agency (EPA) fuel consumption estimates are (city) and (highway), with per tank (city) and per tank (highway). For the first time ever, a Golf estate (Golf Variant) joined the line-up in September 1993 (although most markets did not receive this model until early 1994). At the same time, a completely new Mk3-derived Cabriolet was introduced, replacing the 13-year-old Mk1-based version with one based on the Mk3 Golf platform from 1995 to early 1999. The Mk3 Golf Cabrio received a Mk4-style facelift in late 1999 and was continued until 2002.\n\nThe notchback version, called VW Vento (or Jetta in North America), was presented in January 1992.\n\nIt was European Car of the Year for 1992, ahead of the new Citroën ZX and General Motors' new Opel Astra model.\n\nThe Mk3 continued to be sold until 1999 in the United States, Canada and parts of South America, also in Mexico as a special edition called \"Mi\" (basically a Golf CL 4-door with added air conditioning, special interior, original equipment (OEM) black-tinted rear brake lights, and anti-lock brakes (ABS), but without a factory-fitted radio). The \"i\" in \"Mi\" is coloured red, which designates that multi-point fuel injection was equipped and the 1.8-litre engine was upgraded to 2.0-litres.\n\nThe Golf Mk4 was first introduced in August 1997, followed by a notchback version (VW Bora or, in North America, again VW Jetta) in August 1998 and a new Golf Variant (estate) in March 1999. There was no Mk4-derived Cabriolet, although the Mk3 Cabriolet received a facelift in late 1999 that consisted of bumpers, grill and headlights similar to those of the Mark IV models.\n\nAs with the earlier three versions of the Golf, the UK market received each version several months later than the rest of Europe. The hatchback version was launched there in the spring of 1998 and the estate some 12 months later (around the same time as the Bora).\n\nNew high-performance models included the 3.2-litre VR6-engined four-wheel-drive Golf \"R32\" introduced in 2002, its predecessor, the 2.8-litre VR6-engined \"Golf V6 4Motion\" (succeeding the 2.9-litre Mk3 \"Golf VR6 Syncro\"), as well as the famous 1.8T (turbo) 4-cylinder used in various Volkswagen Group models.\n\nAs of 2008, certain variants of the Golf/Bora Mk4 were still in production in Brazil, China, and Mexico. Revised versions of the Mk4 were sold in Canada marketed as the Golf City and Jetta City from 2007 to 2010. The two models were VW Canada's entry-level offerings. They received a significant refresh for the 2008 model year, including revised headlamps, taillamps, front and rear fascias, sound systems, and wheels. Both models were offered only with the 2.0-litre, 8-valve single over-head cam (SOHC) four-cylinder gasoline engine, rated at . They were the only entry-level offerings with an optional six-speed automatic transmission. Production of the European variant of the Golf Mk4 ceased at the end of the 2003 model year. Production of the U.S. version ended in 2006.\n\nWhen the Chinese market Bora received a July 2006 facelift, the Golf did too, becoming the \"Bora HS\" in the process.\n\nThe Mk4's popularity and low cost has allowed it to remain in production in several countries, including Brazil and Argentina, with minor cosmetic changes.\n\nThe Golf Mk5 was introduced in Europe in the autumn of 2003, reaching the UK market in early 2004. In North America, Volkswagen brought back the Rabbit nameplate when it introduced the vehicle in 2006. In Canada, the Golf is still the prevalent nameplate of the fifth generation (though both Rabbit and Golf have both been used historically). The North American base model is powered by a 2.5-litre five-cylinder engine, which produced in 2006 and 2007, but was upped to in the later models. A GTI version is powered by a turbocharged version of the 2.0-litre FSI engine, producing .\n\nVolkswagen also introduced the \"Fast\" marketing idea for the US market, \"dedicated to the 'fast' that lives inside every driver\". Drivers who purchased new GTI Mk5s from a dealership were shipped a model of said Fast (a plastic figurine), which employs GTI-like features. The GTI version is the only version on sale in Mexico.\n\nThe saloon/sedan version, again called Volkswagen Jetta in most markets, is assembled in Germany, South Africa, as well as Mexico. (In Mexico this car is known as Bora.) It was followed in 2004 by a new Golf Variant. The front ends of the car are the same, with the only difference being that the GLI is a sedan, while the GTI is a hatchback.\n\nLater models of the Mk5 introduced the 1.4-litre TSI turbocharged petrol engine with front-wheel drive.\n\nIn a comparison test conducted by \"Car and Driver\" Magazine, the Volkswagen Rabbit S was named the winner among eight small cars. While it was praised for its excellent driving position, fine instruments, and strong engine, it was criticized for having high levels of road noise, uncomfortable seats, and poor fuel economy. Though, the final verdict stated, \"This one is all about driving pleasure, so it wins.\" The Rabbit also placed first in their final comparison in December 2006.\n\nVolkswagen based the Golf Mk6 on the existing PQ35 platform from the Golf Mk5. This vehicle was debuted at the 2008 Paris Motor Show.\n\nThe Mk6 Golf was designed by Volkswagen's chief designer Walter de'Silva. The design is said to be more aerodynamic, helping fuel efficiency, and is quieter than its predecessor. Following criticism of the downgraded interior trim quality of the Mk5 Golf in comparison to the Mk4, Volkswagen opted to overhaul the interior to match the quality with the Mk4 Golf, while maintaining the same user friendliness from the Mk5. The car is also cheaper to build than its predecessor; Volkswagen claims it consequently will be able to pass these savings on to the customer.\n\nThe Mk6 Jetta was released in Mexico in mid-2010, and by late 2011 it was available in all markets.\nTurbocharged Direct Injection diesel engines which uses common rail injection technology replaced the longstanding Pumpe Düse (PD) Unit Injector system. New on the Golf is the optional Volkswagen Adaptive Chassis Control (not available in the North American market), which allows the driver to select between 'normal', 'comfort' and 'sports' modes, which will vary the suspension, steering and accelerator behavior accordingly.\n\nThe Mk6 Golf is available with both 5- and 6-speed manual transmission, and 6- or 7-speed Direct-Shift Gearbox (DSG)(with Dual Clutch) transmission options. In North America, the Mk5 version was originally sold as the Rabbit from 2006 to 2009. In 2010, Volkswagen brought back the Golf nameplate with the mid-cycle refresh. With it came a , 2.5-litre inline 5-cylinder with 240 N·m (177 lb·ft) of torque and a 2.0-litre, turbocharged inline 4-cylinder clean diesel engine that generates 320 N·m (236 lb·ft) of torque. The GTI version is equipped with a turbocharged inline 4-cylinder TSI gasoline engine while the Golf R has a turbocharged TFSI inline 4 engine. All three engines can be paired with a DSG dual-clutch 6-speed automatic or 6-speed manual transmission in either a 3- or 5-door configuration.\n\nThe car was introduced for sale in the UK in January 2009, and in North America in October 2009 as the 2010 Golf, rather than Rabbit. The Mk6 also reintroduced a diesel engine option to the North American market.\n\nThe Volkswagen Golf Mk6 was a 2012 Insurance Institute for Highway Safety (IIHS) Top Safety Pick.\n\nThe seventh-generation Golf had its début in September 2012 at the Paris Motor Show.\n\nThe Golf VII, Typ 5G uses the new MQB platform, shared with the third-generation Audi A3, SEAT León and Škoda Octavia. It is slightly larger than the Mk6 while managing to be approximately 100 kg lighter, depending on engine choice. The GTI will offer a turbocharged 2.0-litre four-cylinder with an available performance pack to raise the output to . The Golf R now has a turbocharged 2.0-litre four-cylinder with Haldex Traction all-wheel-drive\n\nA version of the GTI dubbed the GTI Clubsport making was released in 2016. A variant of the Clubsport called the Clubsport S held the record for the fastest front-wheel-drive car around the Nürburgring, until the 2017 Honda Civic Type-R took the record once again.\n\nThe Golf line is available in all the relevant drive systems: the Golf TSI, including GTI, is petrol-powered; Golf TDI diesel (Turbo Direct Injection), including GTD, is diesel-powered; the Golf TGI is powered by compressed natural gas (CNG); the e-Golf is powered by electricity; and the Golf GTE is a plug-in hybrid. The use of a modular transverse matrix assembly kit enables the manufacturing of Golf models with gasoline, diesel, natural gas, electric and hybrid drives from bumper to bumper at Volkswagen factories.\n\nIn November 2016, Volkswagen revealed a facelifted version (Golf 7.5) to the 3-door hatchback, 5-door hatchback, 5-door estate, GTI and GTE, in addition to a new \"R-Line\" Golf. With those models, comes a new economical engine: 1.5-litre TSI EVO which produces or and replaces the 1.4-litre TSI. The updated GTI version now features a 230 hp as standard (220 hp previously) or 247 hp in the optional performance pack (230 hp previously).\nIn terms of interior technology, the Golf now features a 12.3\" TFT display as an option that is similar to Audi models and known as \"Virtual Cockpit\", full LED lights, animated tail indicators as an option (also used in Audi models), etc.\n\nThe most powerful Golf in the range is the Golf R. Built as a 3 or 5 door hatchback, it is powered by a newly developed version of the turbocharged EA888 petrol FSI Inline-four engine used in the latest Golf GTI (and Audi S3), but in this application producing ( for \"hot climate\" markets such as Australia, South Africa, Japan, USA) from 5,500 to 6,200 rpm and from 1,800 to 5,500 rpm of torque. 0-62 mph (100 km/h) takes 5.1 seconds (versus 5.7 seconds for previous Golf R), or 4.9 seconds with optional DSG gearbox. In 3rd-party testing, it has been recorded at 4.5 seconds using Launch Control. The top speed is electronically limited to .\n\nThe VW Golf has had several generations made into electric \"CityStromer\" models. The first of these was in the 1970s, when VW took a standard Golf Mk1 and converted it to electric power. By the time the Golf Mk2 came into production a limited number of electric Golfs were made, using lead–acid battery packs and a custom-made motor and controller. VW continued with the production of limited numbers of CityStromer electric cars with the introduction of the Golf Mk3. The electric CityStromer Mk3 included a Siemens-based AC drive system, and lead–acid battery packs. They had a maximum speed of and a range of approximately . With a few exceptions, only left-hand drive Golfs were converted by VW into Citystromer models. These vehicles are still used today and have popularity in mainland Europe with only a few present in Great Britain. Only two right-hand drive Mk2 CityStromers were built for the UK market and it is believed only one remains today. It is owned by EV advocate and broadcaster Nikki Gordon-Bloomfield, host of Transport Evolved.\n\nAs part of the \"Fleet study in electric mobility\" project that began in 2008, VW developed 20 Golf Variant twinDRIVE plug-in hybrid electric vehicles. These research vehicles have an all-electric range of and the internal combustion engine provides for a total range of . The plug-in hybrid drive of the Golf Variant twinDRIVE is equipped with either an 11.2 kWh or a 13.2 kWh lithium-ion battery pack, as Volkswagen is testing packs from two vendors. Ten vehicles are equipped with batteries from the American-German manufacturer GAIA with cathode type nickel cobalt aluminium dioxide (NCA). The other ten are powered by lithium-ion batteries with nickel manganese cobalt (NMC) cathodes from the Korean-German joint venture SB LiMotive (Samsung and Bosch). These 10 vehicles have been in use since early 2011. Both battery systems offer high power and energy density. They each weigh about 150 kg. The gasoline engine is used to support the electric heating system when outdoor temperatures are low.\n\nUsing guidelines for determining the fuel consumption of plug-in hybrids, VW estimates a fuel consumption of 2.1 L/100 km (112 mpg US), which is equivalent to 49 g/km CO2. When the battery is fully charged, the Golf Variant twinDRIVE is designed to maximize the share of pure electrical energy used for driving, and only when longer distances are driven does the share of supplemental gasoline fuel increase. Top speed of the car is and it accelerates to 100 km/h (62 mph) in under 12 seconds. When operated in pure electric mode, the Golf Variant twinDRIVE can reach a top speed of .\n\nThe production version was expected to be based on Mk6 Golf featuring a 1.5 L turbodiesel engine and electric motor, with estimated arrival date of 2015. A SEAT León prototype with the Twin Drive system was also under development.\n\nThe Golf blue-e-motion concept has a range of . Volkswagen scheduled a field testing program with 500 units to begin in 2011. The first 10 units began field testing in Wolfsburg in May 2011. A second batch of 80 test cars began testing in June 2011 in Berlin, Hannover and Wolfsburg. In February 2012, the first e-Golf, as the production version was renamed, was delivered in Belmont, California. A total of 20 e-Golfs will be allocated to the U.S. field testing program.\n\nThe Golf blue-emotion concept has a 26.5 kWh lithium-ion battery pack and is powered by an 85 kW electric motor which drives the front wheels through a single speed transmission. It will accelerate to in 11.8 seconds and has a top speed of . Paddle shifters are used to adjust the amount of regenerative braking. The vehicle's PRNDL stick has an additional 'B' mode as found on some other electric vehicles to set the regenerative braking effort to the maximum for sustained downhill travelling.\n\nThe production version of the 2015 Volkswagen e-Golf was unveiled at the 2013 Frankfurt Motor Show. According to VW the e-Golf has a practical all-electric range of , with an official NEDC cycle of , and the winter range is expected to be . The 2015 e-Golf has an official EPA rated all-electric range of , and a combined fuel economy of 116 miles per gallon gasoline equivalent (MPGe) for an energy consumption of 29 kW-hrs/100 mi. The EPA rating for city driving is 126 MPGe and 105 MPGe in highway. Production of vehicles destined for retail customers began in March 2014.\n\nIn February 2017, Volkswagen announced an updated version of the 2017 model year e-Golf which includes improved range, better fuel economy, and more power than the outgoing model. With a new 35.8 kWh lithium-ion battery, the upgraded car is said to achieve an EPA-estimated range of , and have a combined 119 MPGe. The 2017 e-Golf also provides a faster charging time; SE and SEL Premium trim levels have a 7.2 kW unit that allows the battery to be fully charged in under six hours at a 240 V charging station. There is an optional (standard on SEL Premium) DC Fast Charging feature that allows the car to be charged to 80% in only an hour at a DC fast charging station. The DC fast charging function is notably limited to 2 consecutive uses, since the battery can otherwise overheat (the vehicle is designed with passive battery cooling only). This makes the vehicle unsuitable for convenient long distance highway journeys over 220 miles or so at normal highway speeds.\n\n\nOn 14 February 2014, Volkswagen launched sales of the e-Golf in Germany, with pricing starting at (~). On 11 March 2014, Volkswagen opened ordering for the e-Golf in the UK, and announced pricing of . UK deliveries began at the end of June.\n\nIn Norway, the e-Golf became available for pre-order on 25 February 2014 for delivery in June 2014. Over 1,300 cars were ordered that same day. By 3 March 2014, nearly 2,000 cars had been pre-ordered. Prices range from (~) for the basic model to (~) with all available options, comparable to the cheapest petrol and diesel models. The basic package includes equipment which is optional in other countries, such as a DAB+ radio receiver, heated front seats and a heated windshield. The VW e-Golf was the top selling plug-in electric car in July 2014 with 391 units sold and representing 34.4% of the Golf nameplate sales (1,136), which was Norway's top selling new car that month. The e-Golf was again the top selling electric car in August 2014 with 467 units sold, representing 43.4% of the Golf nameplate sales that month (1,075). In two months and a half a total of 925 Volkswagen e-Golf cars have been sold in Norway, surpassing initial Tesla Model S sales which delivered 805 units during its first two months in the Norwegian market. European sales totaled 3,328 units in 2014.\n\nIn April 2014, Volkswagen announced that the U.S. version of the 2015 e-Golf would not have a liquid-cooled battery pack because strict testing showed high ambient temperatures did not affect battery performance. U.S. sales were slated to start on selected markets in November 2014 at a price for the SEL Premium model starting at before any applicable government incentives, plus destination and delivery. However, the first delivery of an e-Golf actually happened on 31 October in California. During the month of November 2014, the first full month of sales of the vehicle, Volkswagen of America sold 119 units, and a total of 357 units were sold through December 2014. In January 2015, the e-Golf started to show up at dealerships throughout the Northeast.\n\nSales in Europe totaled 11,214 units in 2015. The e-Golf, with 8,943 units sold, was the best-selling plug-in electric car in Norway in 2015, representing 34.7% of the plug-in segment sales, ahead of the Tesla Model S (4,039) and the Nissan Leaf (3,189). The e-Golf variant represented 54.6% of total new VW Golf sales in Norway in 2015. , a total of 19,131 units have been sold worldwide, with 14,542 in Europe and 4,589 units in the U.S.\n\nThe Golf GTE is a plug-in hybrid version of the Golf hatchback unveiled at the 2014 Geneva Motor Show. The Golf GTE shares the basic powertrain hardware with the Audi A3 Sportback e-tron but the software controls are different. The Golf GTE also shares the same plug-in hybrid powertrain with the Volkswagen Passat GTE, but the Passat has a larger 9.9 kWh Li-ion battery pack.\n\nThe GTE is powered by a 1.4-litre (148 hp, 110 kW) TSI direct-injection gasoline engine combined with a 75 kW electric motor powered by a 8.8 kWh lithium-ion battery, enabling the plug-in hybrid to deliver an all-electric range of and a total range of . The all-electric mode can be activated at the push of a button. Under the New European Driving Cycle, combined fuel economy is equivalent. The Golf GTE has a top speed of and accelerates from in 7.6 seconds.\n\nThe GTE release to retail customers was scheduled for the fourth quarter of 2014. The first units were registered in Germany in August 2014. The Golf GTE, with 1,695 units sold, was the best-selling plug-in hybrid in France in 2015, representing 30.3% of the segment sales. With 17,300 units sold in Europe in 2015, the Golf GTE ranked as the second top selling plug-in hybrid after the Mitsubishi Outlander P-HEV (31,214). According to JATO Dynamics, a total of 18,397 units have been registered in Europe through December 2015.\n\nIn auto racing, APR Motorsport has led two MKV VW GTI's to victory in the Grand-Am KONI Sports Car Challenge Street Tuner (ST) class.\n\nVolkswagen Motorsport, the motorsport division of the brand, built the Golf TCR touring car in 2015 for use in various international and national competitions which use TCR regulations. In 2016 the car was updated and renamed Golf GTi TCR.\n\nVolkswagen Golfs are among the models included in the September 2015 Volkswagen emissions scandal in which Volkswagen manufactured and installed in their diesel vehicles a software program that manipulated the cars' vehicle emissions control during testing, thereby violating numerous countries' regulations. The program caused the vehicles' nitrogen oxide () output to meet US standards during regulatory testing but emit up to 40 times more in real-world driving.\n\n\n\n"}
{"id": "37617438", "url": "https://en.wikipedia.org/wiki?curid=37617438", "title": "Wagonette", "text": "Wagonette\n\nA wagonette (\"little wagon\") is a small horsecar with springs, which has two benches along the right and left side of the platform, people facing each other. The driver sits on a separate, front-facing bench. A wagonette may be open or have a tilt. A large horse-drawn enclosed vehicle with spring-suspension, a similar arrangement of the seats and obligatory roof is called an horsebus.\n\nThe 1914 book \"Motor Body-building in All Its Branches\" by Christopher William Terry, defined a shooting-brake as a wagonette provided with game and gun racks and accommodation for ammunition.\n\nCreated in 1810 in Argentina"}
