{"id": "20839004", "url": "https://en.wikipedia.org/wiki?curid=20839004", "title": "2008 Yevpatoria gas explosion", "text": "2008 Yevpatoria gas explosion\n\nThe 2008 Yevpatoria gas explosion took place on December 24, 2008 with an explosion in an apartment block in Yevpatoria, a Ukrainian Black Sea resort town. Within hours, the death toll stood at 22 with 10 missing. On December 26 the total number of deaths was 27 people. President Viktor Yushchenko declared December 26 to be a day of national mourning.\n\nOn 27 December 2008 the Ukrainian Cabinet of Ministers ordered the central government agencies and the Yevpatoria administration to provide the victims of the blast and the families of those killed with new housing and all financial compensations they are entitled to by January 1, 2009. The Education and Science Ministry has been instructed to make sure that the children from the families who suffered from the incident be provided with free school instruction.\n\nThe government also ordered that tougher control measures be taken in the gas supply industry to prevent new accidents.\n\nA similar gas explosion in Dnipropetrovsk on October 2007, killed 23 victims.\n\nGas explosions in crumbling apartment buildings are often caused by improper use or a poorly maintained infrastructure. They are common occurrences in former Soviet states, particularly in the winter, when residents use more heating.\n\n"}
{"id": "31827106", "url": "https://en.wikipedia.org/wiki?curid=31827106", "title": "A Pestering Journey", "text": "A Pestering Journey\n\nA Pestering Journey is a documentary film directed by K.R. Manoj. A voyage through two similar pesticide tragedies in post independent India, Pestering Journey interrogates the legitimate forms and technologies of killing available in a culture. In an atypical move it challenges and changes the idioms of pesticide and genocide. It is a journey which takes a pestering turn and blurs the boundaries of nature and culture, of self and other, of life and death and many other comfortable binaries.\n\nWon National Film Awards 2010\n"}
{"id": "2024285", "url": "https://en.wikipedia.org/wiki?curid=2024285", "title": "Able Archer 83", "text": "Able Archer 83\n\nAble Archer 83 is the codename for a command post exercise carried out in November 1983 by the North Atlantic Treaty Organization (NATO). As with Able Archer exercises from previous years, the purpose of the exercise was to simulate a period of conflict escalation, culminating in the US military attaining simulated DEFCON 1 coordinated nuclear attack. Coordinated from the Supreme Headquarters Allied Powers Europe (SHAPE) headquarters in Casteau, Belgium, it involved NATO forces throughout Western Europe, beginning on November 7, 1983, and lasting for five days.\n\nThe 1983 exercise introduced several new elements not seen in previous years, including a new, unique format of coded communication, radio silences, and the participation of heads of government. This increase in realism, combined with deteriorating relations between the United States and the Soviet Union and the anticipated arrival of Pershing II nuclear missiles in Europe, led some members of the Soviet Politburo and military to believe that Able Archer 83 was a ruse of war, obscuring preparations for a genuine nuclear first strike. In response, the Soviets readied their nuclear forces and placed air units in East Germany and Poland on alert. The apparent threat of nuclear war ended with the conclusion of the exercise on November 11.\n\nHistorians such as Thomas Blanton, Director of the National Security Archive, and Tom Nichols, a professor at the Naval War College, have since argued that Able Archer 83 was one of the times when the world has come closest to nuclear war since the Cuban Missile Crisis in 1962. Other incidents that also brought the world close to such a war include the Soviet nuclear false alarm incident that occurred a month earlier and the Norwegian rocket incident of 1995.\n\nThe greatest catalyst to the Able Archer war scare occurred more than two years earlier. In a May 1981 closed-session meeting of senior KGB officers and Soviet leaders, General Secretary Leonid Brezhnev and KGB chairman Yuri Andropov bluntly announced that the United States was preparing a secret nuclear attack on the USSR.\n\nTo combat this threat, Andropov announced, the KGB and GRU military foreign intelligence arm would begin \"Operation RYaN\". RYaN (РЯН) was a Russian acronym for \"Nuclear Missile Attack\" (Ракетное Ядерное Нападение); Operation RYaN was the largest, most comprehensive peacetime intelligence-gathering operation in Soviet history. Agents abroad were charged with monitoring the figures who would decide to launch a nuclear attack, the service and technical personnel who would implement the attack, and the facilities from which the attack would originate. It is possible that the goal of Operation RYaN was to discover the first \"intent\" of a nuclear attack and then preempt it.\n\nThe exact impetus for the implementation of Operation RYaN is not known for sure. Oleg Gordievsky, the highest-ranking KGB official ever to defect, attributed it to \"a potentially lethal combination of Reaganite rhetoric and Soviet paranoia.\" Gordievsky conjectured that Brezhnev and Andropov, who \"were very, very old-fashioned and easily influenced ... by Communist dogmas\", truly believed that an antagonistic Ronald Reagan would push the nuclear button and relegate the Soviet Union to the literal \"ash heap of history\". Central Intelligence Agency historian Benjamin B. Fischer lists several concrete occurrences that likely led to the birth of RYaN. The first of these was the use of psychological operations (PSYOP) that began soon after Reagan took office.\n\nIn his report, Fischer also writes that another CIA source was, at least partially, corroborating Gordievsky's reporting. This Czechoslovak intelligence officer—who worked closely with the KGB on RYaN—\"noted that his counterparts were obsessed with the historical parallel between 1941 and 1983. He believed this feeling was almost visceral, not intellectual, and deeply affected Soviet thinking.\"\n\nPsychological operations (PSYOP) by the United States began in mid-February 1981 and continued intermittently until 1983. These included a series of clandestine naval operations that stealthily accessed waters near the Greenland–Iceland–United Kingdom (GIUK) gap, and the Barents, Norwegian, Black, and Baltic seas, demonstrating how close NATO ships could get to critical Soviet military bases. American bombers also flew directly towards Soviet airspace, peeling off at the last moment, sometimes several times per week. These near-penetrations were designed to test Soviet radar vulnerability as well as demonstrate US capabilities in a nuclear war.\n\n\"It really got to them,\" said Dr. William Schneider, [former] undersecretary of state for military assistance and technology, who saw classified \"after-action reports\" that indicated U.S. flight activity. \"They didn't know what it all meant. A squadron would fly straight at Soviet airspace, and other radars would light up and units would go on alert. Then at the last minute the squadron would peel off and return home.\"\n\nIn April 1983, the United States Navy conducted FleetEx '83-1, the largest fleet exercise held to date in the North Pacific. The conglomeration of approximately 40 ships with 23,000 crewmembers and 300 aircraft was arguably one of the most powerful naval armadas ever assembled. U.S. aircraft and ships attempted to provoke the Soviets into reacting, allowing the U.S. Office of Naval Intelligence to study Soviet radar characteristics, aircraft capabilities, and tactical maneuvers. On April 4 at least six U.S. Navy aircraft flew over Zeleny Island, one of the Kurile Islands. In retaliation the Soviets ordered an overflight of the Aleutian Islands. The Soviet Union also issued a formal diplomatic note of protest, which accused the United States of repeated penetrations of Soviet airspace.\n\nOn September 1, 1983, the Korean Air Lines Flight 007 (KAL 007) was shot down by a Soviet interceptor over the Sea of Japan near Moneron Island (just west of Sakhalin island) while flying over prohibited Soviet airspace. All 269 passengers and crew aboard were killed, including Congressman Larry McDonald, a sitting member of the United States House of Representatives from Georgia and President of the anti-communist John Birch Society.\n\nFrom the start, the Reagan administration adopted a bellicose stance toward the Soviet Union, one that favored seriously constraining Soviet strategic and global military capabilities. The administration's rigorous focus on this objective resulted in the largest peacetime military buildup in the history of the United States. It also ushered in the final major escalation in rhetoric of the Cold War. On June 8, 1982, Reagan, in a speech to the British House of Commons, declared that, \"... Freedom and democracy will leave Marxism and Leninism on the ash heap of history.\"\n\nOn March 23, 1983, Reagan announced one of the most ambitious and controversial components to this strategy, the Strategic Defense Initiative (labeled \"Star Wars\" by the media and critics). While Reagan portrayed the initiative as a safety net against nuclear war, leaders in the Soviet Union viewed it as a definitive departure from the relative weapons parity of détente and an escalation of the arms race into space. Yuri Andropov, who had become General Secretary following Brezhnev's death in November 1982, criticised Reagan for \"inventing new plans on how to unleash a nuclear war in the best way, with the hope of winning it\".\n\nDespite the Soviet outcry over the Strategic Defense Initiative, the weapons plan that generated the most alarm among the Soviet Union's leadership during Able Archer 83 was NATO's planned deployment of intermediate-range Pershing II missiles in Western Europe. These missiles, deployed to counter Soviet SS-20 intermediate-range missiles on the USSR's western border, represented a major threat to the Soviets. The Pershing II was capable of destroying Soviet \"hard targets\" such as underground missile silos and command and control bunkers.\n\nThe missiles could be emplaced in and launched from any surveyed site in minutes, and because the guidance system was self-correcting, the missile system possessed a genuine \"first strike capability\". Furthermore, it was estimated that the missiles (deployed in West Germany) could reach targets in the western Soviet Union within four to six minutes of their launch. These capabilities led Soviet leaders to believe that the only way to survive a Pershing II strike was to preempt it. This fear of an undetected Pershing II attack, according to CIA historian Benjamin B. Fischer, was explicitly linked to the mandate of Operation RYaN: to detect a decision by the United States to launch a nuclear attack and to preempt it.\n\nOn the night of September 26, 1983, the Soviet orbital missile early warning system (SPRN), code-named Oko, reported a single intercontinental ballistic missile launch from the territory of the United States. Lieutenant Colonel Stanislav Petrov, who was on duty during the incident, correctly dismissed the warning as a computer error when ground early warning radars did not detect any launches. Part of his reasoning was that the system was new and known to have malfunctioned previously; also, a full-scale nuclear attack from the United States would involve thousands of simultaneous launches, not a single missile.\n\nLater, the system reported four more ICBM launches headed to the Soviet Union, but Petrov again dismissed the reports as false. The investigation that followed revealed that the system indeed malfunctioned and false alarms were caused by a rare alignment of sunlight on high-altitude clouds underneath the satellites' orbits.\n\nA scenario released by NATO details the hypothetical lead-up to the Able Archer exercise, which was used by the Joint Chiefs of Staff in Washington D.C. and the Ministry of Defence in London. Dr. Gregory Pedlow, a SHAPE historian explains the war game:\n\nThe exercise scenario began with Orange (the hypothetical opponent) opening hostilities in all regions of ACE on 4 November (three days before the start of the exercise) and Blue (NATO) declaring a general alert. Orange initiated the use of chemical weapons on 6 November and by the end of that day had used such weapons throughout ACE. All of these events had taken place prior to the start of the exercise and were simply part of the written scenario. There had thus been three days of fighting and a deteriorating situation prior to the start of the exercise. This was desired because—as previously stated—the purpose of the exercise was to test procedures for transitioning from conventional to nuclear operations. As a result of Orange advance, its persistent use of chemical weapons, and its clear intentions to rapidly commit second echelon forces, SACEUR requested political guidance on the use of nuclear weapons early on Day 1 of the exercise (7 November 1983)...\n\nThus, on November 7, 1983, as Soviet intelligence services were attempting to detect the early signs of a nuclear attack, NATO began to simulate one. The exercise, codenamed Able Archer, involved numerous NATO allies and simulated NATO's Command, Control, and Communications (C³) procedures during a nuclear war. Some Soviet leaders, because of the preceding world events and the exercise's particularly realistic nature, feared that the exercise was a cover for an actual attack. A KGB telegram of February 17 described one likely scenario:\n\nIn view of the fact that the measures involved in State Orange [a nuclear attack within 36 hours] have to be carried out with the utmost secrecy (under the guise of maneuvers, training etc) in the shortest possible time, without disclosing the content of operational plans, it is highly probable that the battle alarm system may be used to prepare a surprise RYaN [nuclear attack] in peacetime.\n\nAlso on February 17, KGB Permanent Operational Assignment assigned its agents to monitor several possible indicators of a nuclear attack. These included actions by \"A cadre of people associated with preparing and implementing decision about RYaN, and also a group of people, including service and technical personnel ... those working in the operating services of installations connected with processing and implementing the decision about RYaN, and communication staff involved in the operation and interaction of these installations.\"\n\nBecause Able Archer 83 simulated an actual release, it is likely that the service and technical personnel mentioned in the memo were active in the exercise. More conspicuously, British Prime Minister Margaret Thatcher and West German Chancellor Helmut Kohl participated (though not concurrently) in the nuclear drill. United States President Reagan, Vice President George H. W. Bush, and Secretary of Defense Caspar Weinberger were also intended to participate. Robert McFarlane, who had assumed the position of National Security Advisor just two weeks earlier, realized the implications of such participation early in the exercise's planning and rejected it.\n\nAnother illusory indicator likely noticed by Soviet analysts was a high rate of ciphered communications between the United Kingdom and the United States. Soviet intelligence was informed that \"so-called nuclear consultations in NATO are probably one of the stages of immediate preparation by the adversary for RYaN\". To the Soviet analysts, this burst of secret communications between the United States and the UK one month before the beginning of Able Archer may have appeared to be this \"consultation\". In reality, the burst of communication was about the US invasion of Grenada on October 25, 1983, which caused a great deal of diplomatic traffic as the sovereign of the island was Elizabeth II.\n\nA further startling aspect reported by KGB agents concerned the NATO communications used during the exercise. According to Moscow Centre's February 17 memo,\n\nIt [is] of the highest importance to keep a watch on the functioning of communications networks and systems since through them information is passed about the adversary's intentions and, above all, about his plans to use nuclear weapons and practical implementation of these. In addition, changes in the method of operating communications systems and the level of manning may in themselves indicate the state of preparation for RYaN.\n\nSoviet intelligence appeared to substantiate these suspicions by reporting that NATO was indeed using unique, never-before-seen procedures as well as message formats more sophisticated than previous exercises, which possibly indicated the proximity of nuclear attack.\n\nFinally, during Able Archer 83, NATO forces simulated a move through all alert phases, from DEFCON 5 to DEFCON 1. While these phases were simulated, alarmist KGB agents mistakenly reported them as actual. According to Soviet intelligence, NATO doctrine stated, \"\"Operational readiness No. 1\" is declared when there are obvious indications of preparation to begin military operations. It is considered that war is inevitable and may start at any moment.\"\n\nAccording to a 2013 analysis by the National Security Archive: \nThe Able Archer controversy has featured numerous descriptions of the exercise as so \"routine\" that it could not have alarmed the Soviet military and political leadership. Today's posting reveals multiple non-routine elements, including: a 170-flight, radio-silent air lift of 19,000 US soldiers to Europe, the shifting of commands from \"Permanent War Headquarters to the Alternate War Headquarters,\" the practice of \"new nuclear weapons release procedures,\" including consultations with cells in Washington and London, and the \"sensitive, political issue\" of numerous \"slips of the tongue\" in which B-52 sorties were referred to as nuclear \"strikes.\" These variations, seen through \"the fog of nuclear exercises,\" did in fact match official Soviet intelligence-defined indicators for \"possible operations by the USA and its allies on British territory in preparation for RYaN\"—the KGB code name for a feared Western nuclear missile attack.\n\nUpon learning that U.S. nuclear activity mirrored its hypothesized first strike activity, the Moscow Centre sent its residencies a flash telegram on November 8 or 9 (Oleg Gordievsky cannot recall which), incorrectly reporting an alert on American bases and frantically asking for further information regarding an American first strike. The alert precisely coincided with the seven- to ten-day period estimated between NATO's preliminary decision and an actual strike. This was the peak of the war scare.\n\nThe Soviet Union, believing its only chance of surviving a NATO strike was to preempt it, readied its nuclear arsenal. The CIA reported activity in the Baltic Military District and in Czechoslovakia, and it determined that nuclear-capable aircraft in Poland and East Germany were placed \"on high alert status with readying of nuclear strike forces\". Former CIA analyst Peter Vincent Pry goes further, saying he suspects that the aircraft were merely the tip of the iceberg. He hypothesizes that—in accordance with Soviet military procedure and history—ICBM silos, easily readied and difficult for the United States to detect, were also prepared for a launch. Lt. Gen. Leonard H. Perroots is credited with the decision not to place NATO forces on increased alert despite increased Soviet readiness, thereby reducing the possibility of a nuclear exchange.\n\nSoviet fears of the attack ended as the Able Archer exercise finished on November 11. Upon learning of the Soviet reaction to Able Archer 83 by way of the double agent Oleg Gordievsky, a British SIS asset, President Reagan commented, \"I don't see how they could believe that—but it's something to think about.\"\n\nThe double agent Oleg Gordievsky, whose highest rank was KGB resident in London, is the only Soviet source ever to have published an account of Able Archer 83. Oleg Kalugin and Yuri Shvets, who were KGB officers in 1983, have published accounts that acknowledge Operation RYaN, but they do not mention Able Archer 83. Gordievsky and other Warsaw Pact intelligence agents were extremely skeptical about a NATO first strike, perhaps because of their proximity to, and understanding of, the West. Nevertheless, agents were ordered to report their observations, not their analysis, and this critical flaw in the Soviet intelligence system—coined by Gordievsky as the \"intelligence cycle\"—fed the fear of US nuclear aggression.\n\nMarshal Sergei Akhromeyev, who at the time was chief of the main operations directorate of the Soviet General Staff, told Cold War historian Don Orbendorfer that he had never heard of Able Archer. The lack of public Soviet response over Able Archer 83 has led some historians, including Fritz W. Ermarth in his piece, \"Observations on the 'War Scare' of 1983 From an Intelligence Perch\", to conclude that the Soviet Union did not see Able Archer 83 as posing an immediate threat to the Soviet Union.\n\nIn May 1984, CIA Russian specialist Ethan J. Done drafted \"Implications of Recent Soviet Military-Political Activities\", which concluded: \"we believe strongly that Soviet actions are not inspired by, and Soviet leaders do not perceive, a genuine danger of imminent conflict with the United States.\" Robert Gates, Deputy Director for Intelligence during Able Archer 83, has published thoughts on the exercise that dispute this conclusion:\nInformation about the peculiar and remarkably skewed frame of mind of the Soviet leaders during those times that has emerged since the collapse of the Soviet Union makes me think there is a good chance—with all of the other events in 1983—that they really felt a NATO attack was at least possible and that they took a number of measures to enhance their military readiness short of mobilization. After going through the experience at the time, then through the postmortems, and now through the documents, I don't think the Soviets were crying wolf. They may not have believed a NATO attack was imminent in November 1983, but they did seem to believe that the situation was very dangerous. And US intelligence [SNIE 11–9-84 and SNIE 11–10–84] had failed to grasp the true extent of their anxiety.\n\nA report written by Nina Stewart for the President's Foreign Advisory Board concurs with Gates and refutes the previous CIA reports, concluding that further analysis shows that the Soviets were, in fact, genuinely fearful of US aggression. The decision of Gen. Perroots was described as \"fortuitous,\" noting \"[he] acted correctly out of instinct, not informed guidance,\" suggesting that had the depth of Soviet fear been fully realized, NATO may have responded differently.\n\nSome historians, including Beth A. Fischer in her book \"The Reagan Reversal\", pin Able Archer 83 as profoundly affecting President Reagan and his turn from a policy of confrontation towards the Soviet Union to a policy of rapprochement. The thoughts of Reagan and those around him provide important insight upon the nuclear scare and its subsequent ripples. On October 10, 1983, just over a month before Able Archer 83, President Reagan viewed a television film about Lawrence, Kansas, being destroyed by a nuclear attack titled \"The Day After\". In his diary, the president wrote that the film \"left me greatly depressed\".\n\nLater in October, Reagan attended a Pentagon briefing on nuclear war. During his first two years in office, he had refused to take part in such briefings, feeling it irrelevant to rehearse a nuclear apocalypse; finally, he consented to the Pentagon official requests. According to officials present, the briefing \"chastened\" Reagan. Weinberg said, \"[Reagan] had a very deep revulsion to the whole idea of nuclear weapons ... These war games brought home to anybody the fantastically horrible events that would surround such a scenario.\" Reagan described the briefing in his own words: \"A most sobering experience with <nowiki>[</nowiki>Caspar Weinberger<nowiki>]</nowiki> and Gen. Vessey in the Situation Room, a briefing on our complete plan in the event of a nuclear attack.\"\n\nThese two glimpses of nuclear war primed Reagan for Able Archer 83, giving him a very specific picture of what would occur had the situation further developed. After receiving intelligence reports from sources including Gordievsky, it was clear that the Soviets were unnerved. While officials were concerned with the Soviet panic, they were hesitant about believing the proximity of a Soviet attack. Secretary of State George P. Shultz thought it \"incredible, at least to us\" that the Soviets would believe the US would launch a genuine attack. In general, Reagan did not share the secretary's belief that cooler heads would prevail, writing:\nWe had many contingency plans for responding to a nuclear attack. But everything would happen so fast that I wondered how much planning or reason could be applied in such a crisis... Six minutes to decide how to respond to a blip on a radar scope and decide whether to unleash Armageddon! How could anyone apply reason at a time like that?\n\nAccording to McFarlane, the president responded with \"genuine anxiety\" in disbelief that a regular NATO exercise could have led to an armed attack. To the ailing Politburo—led from the deathbed of the terminally ill Andropov, a man with no firsthand knowledge of the United States, and the creator of Operation RYaN—it seemed \"that the United States was preparing to launch ... a sudden nuclear attack on the Soviet Union\". In his memoirs, Reagan, without specifically mentioning Able Archer 83, wrote of a 1983 realization:\n\nThree years had taught me something surprising about the Russians: Many people at the top of the Soviet hierarchy were genuinely afraid of America and Americans. Perhaps this shouldn't have surprised me, but it did...During my first years in Washington, I think many of us in the administration took it for granted that the Russians, like ourselves, considered it unthinkable that the United States would launch a first strike against them. But the more experience I had with Soviet leaders and other heads of state who knew them, the more I began to realize that many Soviet officials feared us not only as adversaries but as potential aggressors who might hurl nuclear weapons at them in a first strike...Well, if that was the case, I was even more anxious to get a top Soviet leader in a room alone and try to convince him we had no designs on the Soviet Union and Russians had nothing to fear from us.\"\n\n\n\n\n"}
{"id": "6264824", "url": "https://en.wikipedia.org/wiki?curid=6264824", "title": "Alliance to Save Energy", "text": "Alliance to Save Energy\n\nThe Alliance to Save Energy is a coalition consisting largely of industrial, technological, and energy corporations. The Alliance states that its mission is to \"support energy efficiency as a cost-effective energy resource under existing market conditions and advocate energy-efficiency policies that minimize costs to society and individual consumers, and that lessen greenhouse gas emissions and their impact on the global climate.\" The alliance's chief activities include public relations, research, and lobbying to change U.S. energy policy.\n\nThe creation of the Alliance was announced on February 10, 1977 with the support of the then U.S. President Jimmy Carter. It was the initiative of senators Charles Percy and Hubert Humphrey.\n\nThe alliance includes over 170 organizations committed to energy efficiency as a primary way to achieve the nation's environmental, economic, energy security and affordable housing goals.\n\nIn 2007, Alliance to Save Energy came out in support of the Energy Independence and Security Act of 2007, which phases out the use of inefficient forms of incandescent light bulbs in favor of more energy-efficient lights, such as CFL, halogen incandescents, and LED lights. The purchase price of more efficient lights is typically higher than the purchase price of legacy incandescents. However, the low energy consumption and long life span of high-efficiency lights result in lower life-cycle costs than their legacy incandescent counterparts. Several of the founding member corporations (including Philips, Panasonic, and Sylvania) are manufacturers of high-efficiency bulbs. However, all of these corporations are also manufacturers of the incandescent light bulbs that were phased out of production following passage of the Clean Energy Bill of 2007.\n\nCarbonCount is a metric developed by the Alliance to Save Energy that quantifies the impact of investments in U.S.-based energy-efficiency and renewable-energy projects given the expected reduction in carbon dioxide (CO) emissions resulting from each $1,000 of investment. In 2015, Bloomberg New Energy Finance honored CarbonCount with its Finance for Resilience (FiRE) award. FiRE is an open and action-oriented platform that collects, develops and helps implement powerful ideas to accelerate finance for clean energy, climate, sustainability and green growth. FiRe singles out ideas that have the potential for incremental finance of at least $1bn in clean energy in the first three years of implementation, that are achievable within 1–3 years. Hannon Armstrong's 2015 issuance of Sustainable Yield Bonds secured by a portion of its utility scale solar and wind real estate related assets was the first investment to be certified under the CarbonCount methodology, receiving a CarbonCount score of 0.39 metric tons of CO offset per $1000 of investment. In 2016, Deutsche Bank received a CarbonCount score of 0.18 metric tons of CO offset per $1000 of investment in a portfolio of rooftop solar PV systems.\n\nThe Alliance to Save Energy supported the bill To require the Secretary of Energy to prepare a report on the impact of thermal insulation on both energy and water use for potable hot water (H.R. 4801; 113th Congress), which would require the United States Secretary of Energy to prepare a report on the effects that thermal insulation has on both energy consumption and systems for providing potable water in . They called the bill a step \"in the right direction\" and saying that they \"are hopeful energy efficiency can once again be an area of common ground between parties.\"\n\nThey supported the EPS Service Parts Act of 2014 (H.R. 5057; 113th Congress), a bill that would exempt certain external power supplies from complying with standards set forth in a final rule published by the United States Department of Energy in February 2014. The United States House Committee on Energy and Commerce describes the bill as a bill that \"provides regulatory relief by making a simple technical correction to the 2007 Energy Independence and Security Act to exempt certain power supply (EPS) service and spare parts from federal efficiency standards.\"\n\nThe board includes CEOs, presidents and senior executives of companies, associations, consumer and environmental organizations, as well as officials from state government, universities and law firms.\n\nThe first board of directors and board of advisors were chaired by Senator Percy and Henry A. Kissinger, respectively. Honorary chairmen included Senators Daniel J. Evans, H. John Heinz III and Timothy E. Wirth.\n\nThe current board is chaired by Senator Mark Warner (D-VA) and co-chaired by Tom King, President of National Grid USA. Congressional vice-chairs include Senators Jeff Bingaman (D-NM), Susan M. Collins (R-ME), Chris Coons (D-DE), Richard Lugar (R–IN), Lisa Murkowski (R–AK), Rob Portman (R-OH), Mark Pryor (D-AR.), Jeanne Shaheen (D-NH), Mark Udall (D–CO). Congressional vice chairs also include Representatives Brian Bilbray (R-CA), Michael Burgess (R–TX), Ralph Hall (R– TX), Steve Israel (D–NY), Edward J. Markey (D–MA), and Paul Tonko (D–NY). Alliance President Kateri Callahan took office on January 1, 2004.\n\n\n"}
{"id": "11841096", "url": "https://en.wikipedia.org/wiki?curid=11841096", "title": "Ana María Cetto", "text": "Ana María Cetto\n\nAna Maria Cetto (born in Mexico City, 1946) is a Mexican physicist. \n\nShe has a bachelor's degree from the National Autonomous University of Mexico (UNAM) and graduate degrees from both, Harvard University and the National Autonomous University of Mexico. She is a researcher at the Physics Institute of the UNAM, and professor at the Faculty of Sciences of the same university. She is also the daughter of the renowned Mexican architect Max Cetto.\n\nHer speciality is Quantum Mechanics, Stochastic Electrodynamics and Biological Physics. \nShe has been the Technical Cooperation Leader and Deputy Director General of the International Atomic Energy Agency (IAEA) in Vienna, Austria. She has also been director of the Faculty of Sciences at UNAM, as well as a professor and researcher at the same institution. \n\nAna Maria Cetto was Council member of the Pugwash Conferences when the International organization received the Nobel Peace Prize in 1995. Being Deputy Technical Director of the International Atomic Energy Agency, she was also participant of the Nobel Peace Prize in 2003.\n\nIn 2003, she was named \"Woman of the Year\" and several other distinctions as member of the Third World Academy of Science (with headquarters in Italy), the Mexican Academy of Science, the Mexican Physics Academy and the American Physical Society. \n\nShe is the author of dozens of research articles and several books. She is also responsible for scientific information programs in Latin America, and programs for the promotion and participation of women in science as well. For these articles, she received the \"Sor Juana Inés de la Cruz\" distinction awarded by the UNAM in recognition to her work in the educational area.\n\nShe is also a member of the World Future Council.\n\n\n"}
{"id": "21538652", "url": "https://en.wikipedia.org/wiki?curid=21538652", "title": "Armand de Waele", "text": "Armand de Waele\n\nArmand Michel A. de Waele (17 November 1887 – December 1966) was a British chemist, noted for his contributions to rheology, and after whom the Ostwald-de Waele relationship for non-Newtonian fluids is named.\n\nDe Waele was born in Islington, London, in 1887, the son of a Belgian father and French mother. He held dual nationality until the age of 21, when he chose to be a British rather than Belgian. He obtained a BSc from the Regent Street Polytechnic then worked in the paint and linoleum industries. , In 1914, the same year was conscripted into the Royal Flying Corps. That same year, he married a Frenchwoman, Jeanne Thérèse Duvivier (1892–1971), in Staines. They had two sons, John and Peter\n\nAfter the First World War, he joined Gestetner as Chief Research Chemist, where he remained till 1957 when he retired. During this period he published 30 papers on rheology and patents on duplicating as well as a book. He was a Fellow of the Royal Institute of Chemistry and of the Institute of Physics.\n\nHe died in Enfield in December 1966.\n\n"}
{"id": "23340097", "url": "https://en.wikipedia.org/wiki?curid=23340097", "title": "Association of Power Producers of Ontario", "text": "Association of Power Producers of Ontario\n\nThe Association of Power Producers of Ontario (abbreviated APPrO) is a trade and professional body representing commercial electricity generators in Ontario, and the largest organization of its type in Canada. APPrO was established in 1986 as the Independent Power Producers' Society of Ontario (IPPSO) and changed its name to APPrO in 2003. It projects a unified voice of advocacy for Ontario-based generators of all types, addressing a range of public policy and regulatory issues of concern to the power industry. The organization also operates industry conferences and produces a number of publications, both hardcopy and electronic. The APPrO conference is the largest annual event of its type in Canada, and its magazine, IPPSO FACTO, is considered by many in the industry to be one of the most authoritative periodicals on electricity business and policy issues in Canada.\n\nAPPrO currently has about 100 corporate members including some of the most well-known names in the Canadian power business such as TransCanada Corporation, Bruce Power and Brookfield Renewable Power, along with many lesser-known companies. APPrO members produce electricity from a range of sources including natural gas, hydroelectricity (waterpower), cogeneration, windpower, solar energy, biomass (wood waste), biogas, nuclear energy, and other sources.\n\nThe organization has been a leading advocate for public policies and regulatory treatments that it believes would facilitate the development of power generation in the province and assist in the development of a more open and competitive market for power.\n\nAPPrO's mission statement cites the following as its top objective: The achievement of an economically and environmentally sustainable electricity sector in Ontario that supports the business interests of electricity generators including a reasonable rate of return.\nAPPrO's current advocacy work is focused on regulatory and policy issues affecting generators in Ontario including electricity market rules, power procurement processes, the regulation of the natural gas market, both provincially and federally, climate change rules and compliance mechanisms, approval requirements, transmission development, distributed generation, and a number of other issues.\n\nAPPrO has frequently put forward the view that the greatest benefits for consumers of electricity are likely to be achieved through the development of open and competitive markets for the production of electricity. Since it was established in 1986, APPrO has been one of the most vocal and consistent advocates for increasing the use of renewable energy in Ontario. Originally incorporated as the Independent Power Producers' Society of Ontario, the organization grew in scale and scope during the 1990s.\n\nAPPrO's predecessor IPPSO was one of the forces that helped convince the Ontario government to end the near-monopoly status of the former Ontario Hydro and introduce a competitive wholesale market for electricity in the province.\nThe organization is governed by a Board of 28 directors, and is operated on a day-to-day basis by a President, an Executive Director and support staff. 14 of the directors are appointed as direct representatives of major generator members or staff, and the remainder are elected from various categories of APPrO members. The first full-time (and current) president of APPrO is David Butters. The Executive Director is Jake Brooks.\n\nAPPrO describes its role as one in which it has \"raised awareness and understanding of its members' concerns with senior decision-makers in government, regulatory bodies and the public at large.\" One of its publications says \"Through its consistent record of insightful and constructive input, top-notch spokespeople, landmark conferences and widely respected publications, APPrO has earned status as a key stakeholder in Ontario's energy sector that ensures it will be consulted on any major developments of concern.\" APPrO has formally intervened in a large number of public hearings and policy development consultations.\n\nOver the years, APPrO's efforts have affected important decisions in the areas of ramp rates, natural gas supply services, net load billing for network transmission services, market design, and large number of technical rules and procedures. APPrO has worked closely with the Ontario government to ensure that the concerns and perspectives of its members are reflected in the contracting process for various procurement processes. APPrO says it intends to continue to advocate for \"fair access to the market for all generators, reasonable terms for interconnection to the electric grid, a more efficient system in the future, and lower costs for all users of the electric system in Ontario.\"\n\nA vision statement released by APPrO in 2006 cited the following as among its goals and as its \"preferred direction for evolution of the electricity sector\":\n\nGoals for Evolution of the Electricity Sector:\n\n\nPrinciples for Evolution:\n\n1. Decisions affecting the power sector should be made on an economically rational basis\n<br>\n2. Prices should reflect the true cost/value of production.\n<br>\n3. Robust competition among generators is a critical factor in achieving the optimal electricity cost for Ontario consumers.\n<br>\n4. Public and private sector entities should compete on a level playing field.\n<br>\n5. Markets with multiple sellers and buyers are the most effective way to harness the benefits of competition.\n<br>\n6. Stable and predictable public policy is critical to efficient investment and effective markets.\n\nAPPrO's annual conference, which celebrated its 25th anniversary in 2013, has become the largest annual event in the Canadian power generation industry.\n\nRegularly featuring the Ontario Minister of Energy as a keynote speaker, along with senior executives, regulators, and a range of experts from the provincial and national power sector, it has become a key place for important announcements and for the industry to discuss the critical concerns of the day. The 2014 event attracted nearly 1000 people and featured more than 60 speakers.\n\nIPPSO FACTO, APPrO's magazine (available in both hardcopy and online forms)\n\nPublic information initiative:\n\n\"Powering Ontario’s Future\", begun by APPrO in 2007\n\nA wide range of statements and releases, many of which can be seen on APPrO’s what’s new page.\n\n\n\n"}
{"id": "12026110", "url": "https://en.wikipedia.org/wiki?curid=12026110", "title": "Bald Hills Wind Farm", "text": "Bald Hills Wind Farm\n\nThe Bald Hills Wind Farm is an operating wind farm located approximately 10 km south east of Tarwin Lower in South Gippsland, Victoria, Australia. The Bald Hills Wind Farm site covers approximately 1,750ha of largely cleared cattle and sheep grazing farmland. The turbines are located in three distinct areas, one to the west and one to the east of Tarwin Lower Waratah Road, and one near the end of Bald Hills Road.\n\nThe Bald Hills Wind Farm has received planning approval from the Victorian Government and approval from the Federal Government under the Environmental Protection and Biodiversity Conservation (EPBC) Act. This followed extensive project feasibility studies and Environmental Effects Statements (EESs). The EESs were assessed by an independent panel.\n\nBald Hills Wind Farm comprises 52 turbines of 2.05 MW capacity each, giving it a total capacity of 106.6 MW. It is expected to produce 335,100 MWh of electricity per year, based on the long-term average forecast wind data. This is the equivalent of meeting the electricity requirements of over 62,000 homes—over four and a half times the homes in the South Gippsland Shire (based on 2006 census data).\n\nConstruction of the wind farm commenced in August 2012 and it became fully operational in May 2015.\n"}
{"id": "7023931", "url": "https://en.wikipedia.org/wiki?curid=7023931", "title": "Beta (plasma physics)", "text": "Beta (plasma physics)\n\nThe beta of a plasma, symbolized by \"β\", is the ratio of the plasma pressure (\"p\" = \"n\" \"k\" \"T\") to the magnetic pressure (\"p\" = \"B\"²/2\"μ\"). The term is commonly used in studies of the Sun and Earth's magnetic field, and in the field of fusion power designs.\n\nIn the fusion power field, plasma is often confined using strong magnets. Since the temperature of the fuel scales with pressure, reactors attempt to reach the highest pressures possible. The costs of large magnets roughly scales like \"β\". Therefore, beta can be thought of as a ratio of money out to money in for a reactor, and beta can be thought of (very approximately) as an economic indicator of reactor efficiency. For tokamaks, betas of larger than 0.05 or 5% are desired for economically viable electrical production.\n\nThe same term is also used when discussing the interactions of the solar wind with various magnetic fields. For example, beta in the corona of the Sun is about 0.01.\n\nNuclear fusion occurs when the nuclei of two atoms approach closely enough for the nuclear force to pull them together into a single larger nucleus. The strong force is opposed by the electrostatic force created by the positive charge of the nuclei's protons, pushing the nuclei apart. The amount of energy that is needed to overcome this repulsion is known as the Coulomb barrier. The amount of energy released by the fusion reaction when it occurs may be greater or less than the Coulomb barrier. Generally, lighter nuclei with a smaller number of protons and greater number of neutrons will have the greatest ratio of energy released to energy required, and the majority of fusion power research focusses on the use of deuterium and tritium, two isotopes of hydrogen.\n\nEven using these isotopes, the Coulomb barrier is large enough that the nuclei must be given great amounts of energy before they will fuse. Although there are a number of ways to do this, the simplest is to heat the gas mixture, which, according to the Maxwell–Boltzmann distribution, will result in a small number of particles with the required energy even when the gas as a whole is relatively \"cool\" compared to the Coulomb barrier energy. In the case of the D-T mixture, rapid fusion will occur when the gas is heated to about 100 million degrees.\n\nThis temperature is well beyond the physical limits of any material container that might contain the gasses, which has led to a number of different approaches to solving this problem. The main approach relies on the nature of the fuel at high temperatures. When the fusion fuel gasses are heated to the temperatures required for rapid fusion, they will be completely ionized into a plasma, a mixture of electrons and nuclei forming a globally neutral gas. As the particles within the gas are charged, this allows them to be manipulated by electric or magnetic fields. This gives rise to the majority of controlled fusion concepts.\n\nEven if this temperature is reached, the gas will be constantly losing energy to its surroundings (cooling off). This gives rise to the concept of the \"confinement time\", the amount of time the plasma is maintained at the required temperature. However, the fusion reactions might deposit their energy back into the plasma, heating it back up, which is a function of the density of the plasma. These considerations are combined in the Lawson criterion, or its modern form, the fusion triple product. In order to be efficient, the rate of fusion energy being deposited into the reactor would ideally be greater than the rate of loss to the surroundings, a condition known as \"ignition\".\n\nIn magnetic confinement fusion (MCF) reactor designs, the plasma is confined within a vacuum chamber using a series of magnetic fields. These fields are normally created using a combination of electromagnets and electrical currents running through the plasma itself. Systems using only magnets are generally built using the stellarator approach, while those using current only are the pinch machines. The most studied approach since the 1970s is the tokamak, where the fields generated by the external magnets and internal current are roughly equal in magnitude.\n\nIn all of these machines, the density of the particles in the plasma is very low, often described as a \"poor vacuum\". This limits its approach to the triple product along the temperature and time axis. This requires magnetic fields on the order of tens of Teslas, currents in the megaampere, and confinement times on the order of tens of seconds. Generating currents of this magnitude is relatively simple, and a number of devices from large banks of capacitors to homopolar generators have been used. However, generating the required magnetic fields is another issue, generally requiring expensive superconducting magnets. For any given reactor design, the cost is generally dominated by the cost of the magnets.\n\nGiven that the magnets are a dominant factor in reactor design, and that density and temperature combine to produce pressure, the ratio of the pressure of the plasma to the magnetic energy density naturally becomes a useful figure of merit when comparing MCF designs. In effect, the ratio illustrates how effectively a design confines its plasma. This ratio, beta, is widely used in the fusion field:\n\nformula_1 \n\nformula_2 is normally measured in terms of the total magnetic field. However, in any real-world design, the strength of the field varies over the volume of the plasma, so to be specific, the average beta is sometimes referred to as the \"beta toroidal\". In the tokamak design the total field is a combination of the external toroidal field and the current-induced poloidal one, so the \"beta poloidal\" is sometimes used to compare the relative strengths of these fields. And as the external magnetic field is the driver of reactor cost, \"beta external\" is used to consider just this contribution.\n\nIn a tokamak, for a stable plasma, formula_2 is always much smaller than 1 (otherwise it would collapse). Ideally, a MCF device would want to have as high beta as possible, as this would imply the minimum amount of magnetic force needed for confinement. In practice, most tokamaks operate at beta of order 0.01, or 1%. Spherical tokamaks typically operate at beta values an order of magnitude higher. The record was set by the START device at 0.4, or 40%.\n\nThese low achievable betas are due to instabilities in the plasma generated through the interaction of the fields and the motion of the particles due to the induced current. As the amount of current is increased in relation to the external field, these instabilities become uncontrollable. In early pinch experiments the current dominated the field components and the kink and sausage instabilities were common, today collectively referred to as \"low-n instabilities\". As the relative strength of the external magnetic field is increased, these simple instabilities are damped out, but at a critical field other \"high-n instabilities\" will invariably appear, notably the ballooning mode. For any given fusion reactor design, there is a limit to the beta it can sustain. As beta is a measure of economic merit, a practical tokamak based fusion reactor must be able to sustain a beta above some critical value, which is calculated to be around 5%.\n\nThrough the 1980s the understanding of the high-n instabilities grew considerably. Shafranov and Yurchenko first published on the issue in 1971 in a general discussion of tokamak design, but it was the work by Wesson and Sykes in 1983 and Francis Troyon in 1984 that developed these concepts fully. Troyon's considerations, or the \"Troyon limit\", closely matched the real-world performance of existing machines. It has since become so widely used that it is often known simply as \"the\" beta limit in tokamaks.\n\nThe Troyon limit is given as:\n\nformula_4\n\nWhere \"I\" is the plasma current, formula_5 is the external magnetic field, and a is the minor radius of the tokamak (see torus for an explanation of the directions). formula_6 was determined numerically, and is normally given as 0.028 if \"I\" is measured in megaamperes. However, it is also common to use 2.8 if formula_7 is expressed as a percentage.\n\nGiven that the Troyon limit suggested a formula_8 around 2.5 to 4%, and a practical reactor had to have a formula_8 around 5%, the Troyon limit was a serious concern when it was introduced. However, it was found that formula_6 changed dramatically with the shape of the plasma, and non-circular systems would have much better performance. Experiments on the DIII-D machine (the second D referring to the cross-sectional shape of the plasma) demonstrated higher performance, and the spherical tokamak design outperformed the Troyon limit by about 10 times.\n\nBeta is also sometimes used when discussing the interaction of plasma in space with different magnetic fields. A common example is the interaction of the solar wind with the magnetic fields of the Sun or Earth. In this case, the betas of these natural phenomena are generally much smaller than those seen in reactor designs; the Sun's corona has a beta around 1%. Active regions have much higher beta, over 1 in some cases, which makes the area unstable.\n\n\n"}
{"id": "3146340", "url": "https://en.wikipedia.org/wiki?curid=3146340", "title": "Bourke engine", "text": "Bourke engine\n\nThe Bourke engine was an attempt by Russell Bourke, in the 1920s, to improve the two-stroke engine. Despite finishing his design and building several working engines, the onset of World War II, lack of test results, and the poor health of his wife compounded to prevent his engine from ever coming successfully to market. The main claimed virtues of the design are that it has only two moving parts, is lightweight, has two power pulses per revolution, and does not need oil mixed into the fuel.\n\nThe Bourke engine is basically a two-stroke design, with one horizontally opposed piston assembly using two pistons that move in the same direction at the same time, so that their operations are 180 degrees out of phase. The pistons are connected to a Scotch Yoke mechanism in place of the more usual crankshaft mechanism, thus the piston acceleration is perfectly sinusoidal. This causes the pistons to spend more time at top dead center than conventional engines. The incoming charge is compressed in a chamber under the pistons, as in a conventional crankcase-charged two-stroke engine. The connecting-rod seal prevents the fuel from contaminating the bottom-end lubricating oil.\n\nThe operating cycle is very similar to that of a current production spark ignition two-stroke with crankcase compression, with two modifications:\n\nThe following design features have been identified:\n\n\n\n\n\nThe Bourke Engine has some interesting features, but the extravagant claims for its performance are unlikely to be borne out by real tests. Many of the claims are contradictory.\n\n\nRussell Bourke obtained British and Canadian patents for the engine in 1939: GB514842 and CA381959.\n\nHe also obtained a US Patent in 1939.\n\n"}
{"id": "37833942", "url": "https://en.wikipedia.org/wiki?curid=37833942", "title": "Cells Alive System", "text": "Cells Alive System\n\nThe Cells Alive System (CAS) is a line of commercial freezers manufactured by ABI Corporation, Ltd. of Chiba, Japan claimed to preserve food with greater freshness than ordinary freezing by using electromagnetic fields and mechanical vibrations to limit ice crystal formation that destroys food texture. They also are claimed to increase tissue survival without having its water replaced by cryogenically compatible fluids; whether they have any effect is unclear.\n\n"}
{"id": "15507", "url": "https://en.wikipedia.org/wiki?curid=15507", "title": "Compounds of carbon", "text": "Compounds of carbon\n\nCompounds of carbon are defined as chemical substances containing carbon. More compounds of carbon exist than any other chemical element except for hydrogen. Organic carbon compounds are far more numerous than inorganic carbon compounds. In general bonds of carbon with other elements are covalent bonds. Carbon is tetravalent but carbon free radicals and carbenes occur as short-lived intermediates. Ions of carbon are carbocations and carbanions are also short-lived. An important carbon property is catenation as the ability to form long carbon chains and rings.\n\nThe known inorganic chemistry of the allotropes of carbon (diamond, graphite, and the fullerenes) blossomed with the discovery of buckminsterfullerene in 1985, as additional fullerenes and their various derivatives were discovered. O\nderivatives is inclusion compounds, in which an ion is enclosed by the all-carbon shell of the fullerene. This inclusion is denoted by the \"@\" symbol in endohedral fullerenes. For example, an ion consisting of a lithium ion trapped within buckminsterfullerene would be denoted Li@C. As with any other ionic compound, this complex ion could in principle pair with a counterion to form a salt. Other elements are also incorporated in so-called graphite intercalation compounds.\n\nCarbides are binary compounds of carbon with an element that is less electronegative than it. The most important are\nAlC,\nBC,\nCaC,\nFeC,\nHfC,\nSiC,\nTaC,\nTiC, and\nWC.\n\nIt was once thought that organic compounds could only be created by living organisms. Over time, however, scientists learned how to synthesize organic compounds in the lab. The number of organic compounds is immense and the known number of defined compounds is close to 10 million. However, an indefinitely large number of such compounds are theoretically possible. \n\nBy definition, an organic compound must contain at least one atom of carbon, but this criterion is not generally regarded as sufficient. Indeed, the distinction between organic and inorganic compounds is ultimately a matter of convention, and there are several compounds that have been classified either way, such as:\nCOCl,\nCSCl,\nCS(NH),\nCO(NH).\nWith carbon bonded to metals the field of organic chemistry crosses over into organometallic chemistry.\n\nThere is a rich variety of carbon chemistry that does not fall within the realm of organic chemistry and is thus called inorganic carbon chemistry.\n\nThere are many oxides of carbon (oxocarbons), of which the most common are carbon dioxide (CO) and carbon monoxide (CO). Other less known oxides include carbon suboxide (CO) and mellitic anhydride (CO). There are also numerous unstable or elusive oxides, such as dicarbon monoxide (CO), oxalic anhydride (CO), and carbon trioxide (CO). \n\nThere are several oxocarbon anions, negative ions that consist solely of oxygen and carbon. The most common are the carbonate (CO) and oxalate (CO). The corresponding acids are the highly unstable carbonic acid (HCO) and the quite stable oxalic acid (HCO), respectively. These anions can be partially deprotonated to give the bicarbonate (HCO) and hydrogenoxalate (HCO). Other more exotic carbon–oxygen anions exist, such as acetylenedicarboxylate (OC–C≡C–CO), mellitate (CO), squarate (CO), and rhodizonate (CO). The anhydrides of some of these acids are oxides of carbon; carbon dioxide, for instance, can be seen as the anhydride of carbonic acid. \n\nSome important carbonates are\nAgCO,\nBaCO,\nCaCO,\nCdCO,\nCe(CO),\nCoCO,\nCsCO,\nCuCO,\nFeCO,\nKCO,\nLa(CO),\nLiCO,\nMgCO,\nMnCO,\n(NH)CO,\nNaCO,\nNiCO,\nPbCO,\nSrCO, and\nZnCO.\n\nThe most important bicarbonates include \nNHHCO,\nCa(HCO),\nKHCO, and\nNaHCO.\n\nThe most important oxalates include\nAgCO,\nBaCO,\nCaCO, \nCe(CO),\nKCO, and\nNaCO.\n\nCarbonyls are coordination complexes between transition metals and carbonyl ligands. Metal carbonyls are complexes that are formed with the neutral ligand CO. These complexes are covalent. Here is a list of some carbonyls:\nCr(CO),\nCo(CO),\nFe(CO),\nMn(CO),\nMo(CO),\nNi(CO),\nW(CO).\n\nImportant inorganic carbon-sulfur compounds are the carbon sulfides carbon disulfide (CS) and carbonyl sulfide (OCS). Carbon monosulfide (CS) unlike carbon monoxide is very unstable. Important compound classes are thiocarbonates, thiocarbamates, dithiocarbamates and trithiocarbonates. \nSmall inorganic carbon – nitrogen compounds are cyanogen, hydrogen cyanide, cyanamide, isocyanic acid and cyanogen chloride.\nParacyanogen is the polymerization product of cyanogen. Cyanuric chloride is the trimer of cyanogen chloride and 2-cyanoguanidine is the dimer of cyanamide. \n\nOther types of inorganic compounds include the inorganic salts and complexes of the carbon-containing cyanide, cyanate, fulminate, thiocyanate and cyanamide ions. Examples of cyanides are copper cyanide (CuCN) and potassium cyanide (KCN), examples of cyanates are potassium cyanate (KNCO) and silver cyanate (AgNCO), examples of fulminates are silver fulminate (AgOCN) and mercury fulminate (HgOCN) and an example of a thiocyanate is potassium thiocyanate (KSCN).\n\nThe common carbon halides are carbon tetrafluoride (CF), carbon tetrachloride (CCl), carbon tetrabromide (CBr), carbon tetraiodide (CI), and a large number of other carbon-halogen compounds.\n\nA carborane is a cluster composed of boron and carbon atoms such as HCBH.\n\nThere are hundreds of alloys that contain carbon. The most common of these alloys is steel, sometimes called \"carbon steel\" (see ). All kinds of steel contain some amount of carbon, by definition, and all ferrous alloys contain some carbon. \n\nSome other common alloys that are based on iron and carbon include anthracite iron, cast iron, pig iron, and wrought iron. \n\nIn more technical uses, there are also spiegeleisen, an alloy of iron, manganese, and carbon; and stellite, an alloy of cobalt, chromium, tungsten, and carbon. \n\nWhether it was placed there deliberately or not, some traces of carbon is also found in these common metals and their alloys: aluminum, chromium, magnesium, molybdenum, niobium, thorium, titanium, tungsten, uranium, vanadium, zinc, and zirconium. For example, many of these metals are smelted with coke, a form of carbon; and aluminum and magnesium are made in electrolytic cells with carbon electrodes. Some distribution of carbon into all of these metals is inevitable.\n"}
{"id": "40354380", "url": "https://en.wikipedia.org/wiki?curid=40354380", "title": "Concentrated solar still", "text": "Concentrated solar still\n\nA concentrated solar still is a system that uses the same quantity of solar heat input (same solar collection area) as a simple solar still but can produce a volume of freshwater that is many times greater. While a simple solar still is a way of distilling water by using the heat of the sun to drive evaporation from a water source and ambient air to cool a condenser film, a concentrated solar still uses a concentrated solar thermal collector to concentrate solar heat and deliver it to a multi-effect evaporation process for distillation, thus increasing the natural rate of evaporation. The concentrated solar still is capable of large-scale water production in areas with plentiful solar energy.\n\nThe concentrated solar still can produce as much as 20x more water than the theoretical maximum of a standard solar still and in practice, can produce as much as 30x the volume. For instance, with a solar collection area of 10 acres, a standard solar still operating at a typical 25% efficiency may produce as much as 27.8acre-ft/yr in a region with an average daily solar irradiation value of 21.6MJ/m². A concentrated solar still can produce more than 750acre-ft/yr in the same region with the same collection area.\n\nThe concentrated solar still implements a method for recovering the latent heat of the distillate vapor not captured and reused by a standard solar still. This is done by using multiple stages of evaporation in series (see multiple-effect evaporator). The latent heat of the distillate vapor produced in the n-1 stage (or effect) is recovered in the nth stage by boiling the leftover concentrated brine from the n-1 stage which produces distillate vapor whose latent heat will be recovered in the n+1 stage by boiling the leftover concentrated brine from the nth stage. Since brine is continuously concentrated in each stage, its boiling point will continue to rise under standard conditions. To overcome the boiling point elevation of the brine, each evaporator stage operates at a lower pressure than the previous stage, which effectively reduces the boiling point, allowing for sufficient heat transfer to take place in each stage. This process can be repeated until the distillate conditions are sufficiently degraded (i.e., pressure and temperature are very low and the distillate vapor volume is very large).\n\nThe final evaporation stage produces distillate vapor that is considered to be at very poor state conditions. This vapor can either be condensed in a final condenser, in which case its latent heat will be shed as waste, or it can be condensed by using a heat pump, in which case its latent heat (or a portion of it) can be recovered. In the latter case, the heat pump effectively “upgrades” the state conditions of the latent heat to more usable conditions (higher temperature and pressure) by performing work (e.g., compression). The conditions can be sufficiently upgraded such that the recovered heat can be used to provide additional heat for evaporation in the first effect.\n"}
{"id": "12298967", "url": "https://en.wikipedia.org/wiki?curid=12298967", "title": "Crêpe paper", "text": "Crêpe paper\n\nCrêpe paper is tissue paper that has been coated with sizing (a glue-like substance) and creped (creased in a way similar to party streamers) to create gathers, giving it a crinkly texture like that of crêpe.\n\nPaper that is creped is produced on a paper machine that has a single large steam-heated drying cylinder (\"yankee\") fitted with a hot-air hood. The raw material is paper pulp. The Yankee cylinder is sprayed with adhesives to make the paper stick. Crêping is done by the Yankee's \"doctor blade\" that is scraping the dry paper off the cylinder surface. The crinkle (crêping) is controlled by the strength of the adhesive, geometry of the doctor blade, speed difference between the yankee and final section of the paper machine and paper pulp characteristics.\n\nCrêpe paper and tissue are among the lightest papers and are normally below 35 g/m.\n\nThe crêpe ratio reflects how much the paper has shortened during crêping. The figure is normally between 10 - 30%. Crêping is used to adjust the paper's stretch and thickness, both of which have a marked effect on softness and absorbency.\n\nCrêping can also be applied to specialty papers, such as microcrêping in sack paper.\n\n\n"}
{"id": "53070578", "url": "https://en.wikipedia.org/wiki?curid=53070578", "title": "Cyanophages", "text": "Cyanophages\n\nCyanophages are viruses that infect cyanobacteria, also known as Cyanophyta or blue-green algae. Cyanobacteria are a phylum of bacteria that obtain their energy through the process of photosynthesis. Although cyanobacteria metabolize photoautotrophically like eukaryotic plants, they have prokaryotic cell structure. Cyanophages can be found in both freshwater and marine environments. Marine and freshwater cyanophages have icosahedral heads, which contain double-stranded DNA, attached to a tail by connector proteins. The size of the head and tail vary among species of cyanophages. Cyanophages infect a wide range of cyanobacteria and are key regulators of the cyanobacterial populations in aquatic environments, and may aid in the prevention of cyanobacterial blooms in freshwater and marine ecosystems. These blooms can pose a danger to humans and other animals, particularly in eutrophic freshwater lakes. Infection by these viruses is highly prevalent in cells belonging to \"Synechococcus\" spp. in marine environments, where up to 5% of cells belonging to marine cyanobacterial cells have been reported to contain mature phage particles.\n\nThe following three families of cyanophages have been recognized by the International Committee on Taxonomy of Viruses (ICTV): Myoviridae, Siphoviridae and Podoviridae; all contain double-stranded DNA. Initially, cyanophages were named after their hosts. However, the ability of cyanophages to infect multiple hosts and lack of a universal naming system can cause difficulties with their taxonomic classification. Many other classification systems used serological, morphological, or physiological properties. Currently, the suggested procedure of naming strains is as follows: Cyanophage Xx-YYZaa, where Xx is the first two letters of the genus and species names of the host that the type specimen phage is found in, YY is the origin of the specimen, Z is the virus family, and aa is the reference number of the virus.\n\nLike all other tailed bacteriophages cyanophages have a tail and a protein capsid surrounding genetic material. The double-stranded DNA is approximately 45 kbp long and in some cyanophages encodes photosynthetic genes, an integrase, or genes involved with phosphate metabolism (phosphate-inducible). The tail binds the virus to the host cell and transfers viral DNA to the host cell upon infection. Based on morphological characteristics, cyanophages are placed into the families Myoviridae, Podoviridaeand Siphoviridae, and although not formally recognized by the International Committee on Taxonomy of Viruses, historically cyanophages have been further classified into as a Cyanomyovirus, Cyanopodovirus or Cyanostylovirus based on which of the three families in which they are grouped.\n\nThe type species for Cyanomyovirus of the family \"Myoviridae\" is Cyanophage AS-1, which was isolated from a waste stabilization pond, and was also the first genus recognized. The tails have been observed as either contractile or noncontractile with lengths of 20 to 244 nm, widths of 15 to 23 nm, and a shrinking range of 93 nm. Cyanophages generally have isometric hexagonal heads with diameters ranging from 55 to 90 nm. There is large morphological variation in this group, which suggests that they infect a variety of host species. At the point of attachment between the long tail and the head there is a base plate where short pins are attached, a contractile sheath, and an internal core, similar to other bacteriophages in the Myoviridae.\n\nCyanopodovirus, within the \"Podoviridae\", are present in both fresh and marine water. The type specimen of cyanopodovirus is Cyanophage LPP-1, which infects \"Lyngbya\", \"Plectonema\" and \"Phormidium\". Their capsids are polyhedrons that appear hexagonal in 2-D. The tails are hollow with sixfold radial symmetry made of rings of six subunits with unknown orientation. Similar to cyanomyoviruses, they can be found in waste-stabilization ponds and have isometric capsids of similar size but shorter tails.\n\nCyanostylovirus belong to the family \"Siphoviridae\", where the type species is Cyanophage S-1, which is known to infect \"Synechococcus\". Cyanostyloviridae have smaller (50 nm in diameter) isometric capsids than the previous genera but longer tails (140 nm). Other genera in this family have tails that range from 200 to 300 nm in length.\n\nThe host range of cyanophages is very complex and is thought to play an important role in controlling cyanobacterial populations. Freshwater cyanophages have been reported to infect hosts in more than one genus although this may also reflect problems in the taxonomic classification of their hosts. Nonetheless, they have been classified into three major groups based on the taxonomy of their host organism.\n\nThe first group is LPP, which belongs to the cyanopodoviruses. This group of viruses includes the original cyanophage isolate that infected \"blue-green algae\". Cyanophages in this group are easy to isolate from the environment. They carry short non-contractile tails and cause lysis of several species within three genera of cyanobacteria: \"Lyngbya\", \"Plectonema\" and \"Phormidium\". Thus, the name LPP was derived from the three genera of hosts that they infect. LPP-1 and LPP-2 are two major types of LPP cyanhophages. This group of cyanophages has the same host same range; however, their serum and other body fluids are not the same.\n\nThe AS and SM groups represent the third group of cyanophages classified based on host range. This group of viruses is said to be the “new blue-green algae” and infects unicellular forms of cyanobacteria. The myovirus AS-1 infects \"Anacystis nidulans\", \"Synechococcus cedrorum\", \"Synechococcus elongatus\" and \"Microcystis aeruginosa\". Similarly, the unicellular blue-green algae \"Synechococcus elongatus\" and \"Microcystis aeruginosa\" are infected by the podovirus SM-1. There is a new SM-group of virus, known as SM-2, which also lyses \"Microcystis aeruginosa\"\".\"\n\nCyanophages classified in the groups A, AN, N and NP represent a second group of cyanophages classified based on host range. They play an important role in infecting and causing lysis of members of the genera \"Nostoc\", \"Anabaena\" and \"Plectonema\". The A-group of the virus causes lysis and infects \"Anabaena\" species. Similarly, the host range of the AN group includes both \"Anabaena\" and \"Nostoc\" species; whereas, the N group of viruses infects \"Nostoc\" species only and includes Cyanophage N-1. Cyanophage N-1 is remarkable in that it encodes a functional CRISPR array that may provide immunity to the host to infection by competing cyanophages. Lastly, cyanobacterial isolates of \"Nostoc\" and \"Plectonema\" species are infected by the NP group of viruses. These cyanobacterial isolates closely relate to the taxonomic group of \"Nostoc\". They all have a broad host range and mutations are noticeable in these groups of viruses.\n\nCyanophage replication has two dominant cycles: the lytic cycle and the lysogenic cycle. Viral nucleic-acid replication and immediate synthesis of virus-encoded protein is considered to be the lytic cycle. Phages are considered lytic if they only have the capacity to enter the lytic cycle; whereas, temperate phage can either enter the lytic cycle or become stabely integrated with the host genome and enter the lysogenic cycle. To meet the metabolic demand of replication, viruses recruit a multitude of strategies to sequester nutrients from their host. One such technique is to starve their host cell. This is done by inhibiting the host cells CO fixation, which enables the cyanophage to recruit photosynthetically formed redox and ATP from the host cell to meet their nucleotide and metabolic response. Many cyanophages contain genes known as viral-encoded auxiliary metabolic genes (AMGs), which encode critical, rate-limiting steps of the host organism. AMGs encode genes for the pentose phosphate pathway, phosphate acquisition, sulfur metabolism, and DNA/RNA processing; these genes interfere with the metabolism of the host cell. Metagenomic analysis highly supports the notion that these genes promote viral replication through the degradation of host DNA and RNA, as well as a shift in host-cell metabolism to nucleotide biosynthesis. Cyanophages also use these genes to maintain host photosynthesis through the progression of the infection, shuttling the energy away from carbon fixation to anabolism, which the virus takes advantage of. AMGs also code for proteins, which aid in the repair of the host photosystem, which is susceptible to photodegradation. One such example is the D1 proteins which replace the host cells D1 protein when it becomes damaged. The virus up-regulates photosynthesis, which leads to an increased rate of D1 protein degradation, the host cell alone can not efficiently replace these proteins so the cyanophage replaces them for the host cell, allowing it to continue providing energy for the cyanophage replication cycle.\n\nIt is evident that cyanophage replication is heavily dependent on the diel cycle. The first step in the infectious cycle is for the cyanophage to make contact and bind to the cyanobacteria, this adsorption process is heavily dependent on light intensity. Field studies also show that the infection and replication of cyanophages is directly or indirectly synchronized with the light-dark cycle.\n\nCyanophages like other bacteriophages rely on Brownian motion to collide with bacteria, and then use receptor binding proteins to recognize cell surface proteins, which leads to adherence. Viruses with contractile tails then rely on receptors found on their tails to recognize highly conserved proteins on the surface of the host cell. Cyanophages also have several surface proteins with Ig like domains, which are used for adherence. Some cyanophages also produce a horn like structure, which projects from the vertex opposite of the tail. The horn like structure is hypothesized to aid in attachment to cells in the natural environment; however, this has not been confirmed.\n\nCyanophages can undergo both the lytic and lysogenic cycles depending on the viruses and their environment. In one study on cyanomyoviruses infecting marine \"Synechococcus\" sp., the lytic phase was shown to last approximately 17 hours with the average number of viruses produced for each cell that was lysed (burst size) ranging from 328 under high light to 151 under low light. There is evidence supporting the premise that there is a correlation between light intensity and burst size. Studies show that cyanophage replication is driven by energy from photosynthetic metabolism of the host cell. Lysing of the host cell tends to occur after the completion of host DNA replication and immediately prior to cell division. This is due to the increased availability of resources for the replication of viral particles.\n\nCertain cyanophages infect and burst \"Prochlorococcus\", the world's smallest and most abundant primary producers. Marine cyanophages of the family \"Myoviridae\" help regulate primary production mainly through infection of \"Synechococcus\" spp. The other two families, \"Podoviridae\" and \"Siphoviridae\", are usually found in freshwater ecosystems. In coastal oceans, abundance of viruses infecting \"Synechococcus\" spp. can reach >10 mL and 10 g in sediments. An estimated 3% of \"Synechococcus\" are removed daily by cyanophages. Cyanophages are widely distributed both throughout the water column and geographically. Cyanophage populations have been found to inhabit microbial mats in the Arctic through metagenomic analysis and hypersaline lagoons. They can withstand temperatures ranging from 12-30 °C and salinities of 18-70 ppt. The DNA of cyanophages is susceptible to UV degradation but can be restored in host cells through a process called \"photoreactivation\". The viruses cannot move independently and must rely on currents, mixing, and host cells to transport them. Viruses cannot actively target their hosts and must wait to encounter them. The higher probability of collision may explain why cyanophages of the \"Myoviridae\" family primarily infect one of the most abundant cyanobacteria, \"Synechoccocus\". Evidence of seasonal co-variation between the phages and hosts, in addition to an increase in cyanophages above a threshold of 10 to 10 \"Synechococcus\" mL, may suggest a “kill-the-winner” dynamic.\n\nMembers of the genus \"Synechococcus\" contribute ~25% to photosynthetic primary productivity in the ocean, having significant bottom-up effect on higher trophic levels. The dissolved organic matter (DOM) released from viral lysis by cyanophages can be shunted into the microbial loop where it is recycled or rejected by heterotrophic bacteria to form recalcitrant matter that is eventually buried in sediment. This is an important step in atmospheric carbon sequestration, commonly referred to as the biological pump, and maintenance of other biogeochemical cycles.\n\nCyanobacteria perform oxygenic photosynthesis which is thought to be the origin of atmospheric oxygen approximately 2.5Ga ago. Population, and therefore, rate of oxygen evolution can be regulated by cyanophages. In certain species of cyanobacteria, such as \"Trichodesmium\" that perform nitrogen fixation, cyanophages are capable of increasing the supply rate of bioavailable organic nitrogen through lysis.\n\nCyanophages also infect bloom-forming cyanobacteria that can be toxic to health of humans and other animals through the production of microcystin and cause eutrophication, leading to oxygen minimum zones. Cyanophages can infect and kill four common bloom-forming cyanobacteria: \"Lyngbya birgei\", \"Anabaena circinalis\", \"Anabaena flosaquae\", and \"Microcystis aeruginosa\", and thus may be able to prevent harmful algal blooms under normal conditions. Blooms cause problems ecologically, economically, and in freshwater systems, adversely affect the quality of drinking water. Spikes in cyanobacteria populations are usually brought on by nutrient increases due to run-off from fertilizers, dust, and sewage. By killing hosts, cyanophages may help restore ecosystems to their natural balance.\n\nIn addition to regulating population size, cyanophages likely influence phylogenetic composition by allowing other phytoplankton normally inhibited by cyanobacteria to grow. The specificity with which cyanophages target various hosts also affects community structure. Due to the lysogenic phase of their replication cycle, cyanophages may behave as mobile genetic elements for genetic diversification of their hosts through horizontal gene transfer. Whether the lytic or lysogenic phase dominates in a given area has been hypothesized to depend on eutrophic or oligotrophic conditions, respectively. Increase in number of encounters is directly related to an increase in rate of infection providing more opportunity for selective pressure, making coastal \"Synechococcus\" more resistant to viral infection than their off-shore counterparts.\n\n"}
{"id": "11848344", "url": "https://en.wikipedia.org/wiki?curid=11848344", "title": "Department of Petroleum Engineering and Applied Geophysics, NTNU", "text": "Department of Petroleum Engineering and Applied Geophysics, NTNU\n\nThe Norwegian University of Science and Technology (NTNU) is the key university of science and technology in Norway. The Department of Petroleum Engineering and Applied Geophysics (IPT) was established in 1973, shortly after the start of production (Ekofisk field) from the Norwegian continental shelf. The department came to include Petroleum Engineering as well as Geophysics, which is seen as a major strength of the petroleum education at NTNU. The department has elected chairman and vice chairman, and 4 informal groups of professors; geophysics, drilling, production and reservoir engineering. The stated primary purpose of maintaining the informal groups is to take care of the teaching in their respective disciplines. Each group is responsible for offering a sufficient number of courses, semester projects and thesis projects at M.Sc. and Ph.D. levels in their discipline, and to make annual revisions of these in accordance with the needs of society and industry. The total number of professors, associate professors, assistant professors and adjunct professors is 32. The administrative staff is led by a department administrator, and consists of a total of 6 secretaries. The technical support staff reports to the department head, and consists of 8 engineers and technicians. Until 2000, the department was part of the Applied Earth Sciences faculty, together with the Geology-department. After that, the department is part of the Faculty of Engineering Science and Technology (one of a total of 10 departments).\n\nBrief historical statistics of the department:\n\nThe department focus research within the following 5 areas: Petroleum geophysics, Reservoir engineering, Production engineering, Subsea engineering, Drilling engineering and Integrated operations.\n\nPetroleum Geophysics:\n\nReservoir Engineering:\n\nProduction Engineering\n\nSubsea Engineering:\n\nDrilling engineering:\n\nIntegrated operations: The department hosts the Center for Integrated Operations in the Petroleum Industry. Key research areas are drilling, reservoir management, production optimization, operation and maintenance.\n\nThe department states that it intends to be strongly focused on the international profile with a friendly multi-cultural atmosphere. From the very beginning the international atmosphere existed at IPT in the form of teachers, researchers and students from various countries. IPT has been actively cooperating with countries like Angola, Aserbadjan, Australia, Austria, Bangladesh, Brazil, Canada, France, Germany, Italy, Iran, Mozambique, Netherlands, Russia, Spain, USA, Venezuela; altogether more than 50 countries. There are two 2-years international programs leading to Master's degrees, one in Petroleum Engineering and one in Petroleum Geoscience. Exchange students may take shorter term education within this program. In addition Ph.D-positions are open to qualified international candidates. These positions also constitute the basis for international research cooperation. Professors have individual scientific cooperation with various foreign institutions. The funding comes from Norwegian agencies SIU (NORAD, QUOTA), EnPe (NORAD, QUOTA), The Research Council of Norway, oil companies Statoil, Total, BP, and NTNU Scholarships; also from European Programs (Erasmus, Marie Curie, TIME, Socrates) and others. IPT cultivates personal international contacts as originators of new collaboration. Graduated Ph.D.s represent a particular bridging potential for new joint research.\n\nNew companies, on average one new company each year, are founded by professors and/or students, including: Agir Boosting Technology, Geoprobing Technology, DeepSeaAnchors, Markland Technology, Natural Gas Hydrate, PERA, Petrostreamz, Petreco, ResLab, Corrocean, Sensorlink, Seres, Technoguide, Verande, Voxelvision, Waptheweb.\n"}
{"id": "10541571", "url": "https://en.wikipedia.org/wiki?curid=10541571", "title": "Dixie Alley", "text": "Dixie Alley\n\nDixie Alley is a nickname sometimes given to areas of the southern United States that are particularly vulnerable to strong or violent tornadoes. This is distinct from the better known Tornado Alley and has a high frequency of strong, long-track tornadoes that move at higher speeds (50+ miles per hour). The term was coined by NSSFC Director Allen Pearson after witnessing a tornado outbreak which included more than 9 long-track, violent tornadoes that killed 121 on February 21, 1971. The specific characteristics of the Southeast led to VORTEX-SE, a field project studying tornadogenesis, diagnosis and forecasting, in addition to social science implications, and examines both supercellular tornadoes and those resulting from quasi-linear convective system (QLCS) thunderstorm structures.\n\nDixie Alley includes much of the area of the lower Mississippi Valley. It stretches from eastern Texas and Arkansas across Louisiana, Mississippi, Tennessee, Alabama, Georgia, to upstate South Carolina and western North Carolina; the area reaches as far north as southeast Missouri and southwest Kentucky. Another source places all of Arkansas within Dixie Alley.\n\nAlthough tornadoes are less frequent in these states than they are in the southern Plains, the southeastern states have had more tornado-related deaths than any of the Plains states (excluding Texas). This is in part due to the relatively high number of strong/violent long tracked tornadoes and higher population density of this region, as well as the Southern United States having the highest percentage of manufactured homes in the US, where 63% of the overall tornado-related fatalities occur. According to the National Climatic Data Center, for the period January 1, 1950 – October 31, 2006, Alabama and Kansas received the largest amount of F5 tornadoes. Complicating matters is that tornadoes are rarely visible in this area, as they are more likely to be rain-wrapped, embedded in shafts of heavy rain, and that the hilly topography and heavily forested landscape makes them difficult to see.\n\nDixie Alley is part of a region of enhanced tornadic activity extending between the Appalachian and Rocky Mountains, but tornadoes and outbreaks in the Dixie Alley region exhibit some statistically distinguishable characteristics than the more well known Tornado Alley. Tornadic storms in Dixie Alley are most often high precipitation supercells due to an increase of moisture from proximity to the nearby Gulf of Mexico. The Dixie Alley tornadoes accompanying the HP supercells are often partially or fully wrapped in rain visually impairing the tornadoes to storm spotters and chasers, law enforcement, and the public. Increases of warmth and instability in conjunction with strong wind shear in the Dixie Alley region impacts the times when tornadoes form. In the traditional Tornado Alley, tornadoes most often form from the mid afternoon to early evening. Dixie Alley's instability can be maintained long after sunset due to being adjacent to the Gulf, increasing the frequency of intense nighttime and early morning tornadoes. There is also a less focused tornado season which tends to be most active in early spring and late autumn but can continue throughout the winter and into late spring, which can lead to complacency among residents of the region. The region often is subject to tornadoes much earlier than the general national peak from May and June, usually from February to Mid-April, and several notorious outbreaks have struck during the late winter and early spring and also in late fall. The complacency situation was noted after the 2008 Super Tuesday tornado outbreak in February 2008 that hit the Dixie Alley killing 57 people, many people indicated that they had underestimated the threat of severe weather on that day since it was well before the peak of tornado season. \n\nVariations in climate patterns and teleconnections, such as the El Niño–Southern Oscillation (ENSO) can also have significant impacts on tornadic activity in the region from year to year. Climate change is also expected to affect tornado activity in the region.\n\nDixie Alley has been subject to numerous tornado outbreaks throughout history, including very intense outbreaks and those of very large spatial and temporal extent. Notorious outbreaks affecting the region include: the Great Natchez Tornado, the 1884 Enigma tornado outbreak, the April 1924 tornado outbreak, the 1932 Deep South tornado outbreak, the 1936 Tupelo-Gainesville tornado outbreak, the April 1957 Southeastern tornado outbreak, the 1984 Carolinas tornado outbreak, and the November 1992 tornado outbreak. The 1974 Super Outbreak also hit the area very hard, producing multiple F5 tornadoes in Alabama, and F4 tornadoes in North Georgia and the Appalachian southwest of North Carolina. More recently the region was hit by the 2008 Super Tuesday tornado outbreak followed by the tornado outbreak of April 14–16, 2011, the deadliest since the 2008 outbreak. Two weeks after the April 14–16 event, Dixie Alley was the epicenter of the 2011 Super Outbreak, which was the largest tornado outbreak ever recorded, as well as the fourth-deadliest outbreak in United States history, with over 300 people dead.\n\n"}
{"id": "46842491", "url": "https://en.wikipedia.org/wiki?curid=46842491", "title": "Efficient Dynamics", "text": "Efficient Dynamics\n\nEfficientDynamics is a set of technologies that are defined by BMW in its products to maximise performance, reduce CO2 emissions and reduce fuel consumption.\n\nBMW's common rail injection involves managing the fuel injection at every stage from pre injection to delivery with one common rail. This improves fuel delivery and power delivery.\n\nThe stop-start feature will automatically switch off the engine when the car is idling at a complete stop. This reduces fuel consumption while idling for extended periods. The engine automatically switch on when the brake pedal is released, turning your steering wheel (which activates power steering), or shifting from park.\n\nThe alternator is charging the battery only when the car is braking, coasting or decelerating\n\nThe air vents in front of the radiator grille will open up to allow for air to cool. They will automatically close if it is not in use, reducing air resistance and fuel consumption.\n\nThe power steering is only active when the steering wheel is turning. If it is kept in a constant location or place, the electric power is inactive.\n"}
{"id": "12947946", "url": "https://en.wikipedia.org/wiki?curid=12947946", "title": "Flexural modulus", "text": "Flexural modulus\n\nIn mechanics, the flexural modulus or bending modulus is an intensive property that is computed as the ratio of stress to strain in flexural deformation, or the tendency for a material to resist bending. It is determined from the slope of a stress-strain curve produced by a flexural test (such as the ASTM D790), and uses units of force per area.\nFor a 3-point test of a rectangular beam behaving as an isotropic linear material, where \"w\" and \"h\" are the width and height of the beam, \"I\" is the second moment of area of the beam's cross-section, \"L\" is the distance between the two outer supports, and \"d\" is the deflection due to the load \"F\" applied at the middle of the beam, the flexural modulus:\nFrom elastic beam theory \nand for rectangular beam \n\nthus formula_4 (Elastic modulus)\n\nIdeally, flexural or bending modulus of elasticity is equivalent to the tensile modulus (Young's modulus) or compressive modulus of elasticity. In reality, these values may be different, especially for polymers.\n"}
{"id": "44664482", "url": "https://en.wikipedia.org/wiki?curid=44664482", "title": "Flood Brothers Disposal", "text": "Flood Brothers Disposal\n\nFlood Brothers Disposal is a family-owned waste management and recycling service based in Oakbrook Terrace, IL with offices in Chicago, IL. Flood Brothers Disposal's service area extends to 150 or more communities in the Chicago area. The business is a member of the National Solid Waste Management Association, the Solid Waste Agency of Northern Cook County, the National Recycling Coalition, the Illinois Food Scrap Coalition, the National Waste & Recycling Association, and the Illinois Recycling Association.\n\nFlood Brothers Disposal started operations with one truck and one employee, at 139 N. Clark St. in Chicago, Illinois. In 1963, the main center of operations moved to 5435 W. Chicago Ave. From 1970 to 1977, the company continued expansion and moved to several additional locations, eventually settling at 4827 W. Harrison St. in Chicago, where the company is presently based.\n\nIn 1988, the company expanded to include a fully automated recycling center, and in 1990, became the first licensed special waste hauler in the Chicago community. In 1996, Flood Brothers Disposal opened an additional facility in Carol Stream, Illinois. As of today, Flood Brothers Disposal services more than 150 communities in the Chicago area.\n\nFlood Brothers Disposal provides service in 150 communities in the Chicago area as of present day\n\nIn 2015, Flood Brothers Disposal was named \"2015 Illinois Medium Family Business of the Year\" by Loyola University's Quinlan School of Business.\n"}
{"id": "3677471", "url": "https://en.wikipedia.org/wiki?curid=3677471", "title": "Fossil wood", "text": "Fossil wood\n\nFossil wood is wood that is preserved in the fossil record. Over time the wood will usually be the part of a plant that is best preserved (and most easily found). Fossil wood may or may not be petrified. The study of fossil wood is sometimes called palaeoxylology, with a \"palaeoxylologist\" somebody who studies fossil wood.\n\nThe fossil wood may be the only part of the plant that has been preserved, with the rest of the plant completely unknown: therefore such wood may get a special kind of botanical name. This will usually include \"xylon\" and a term indicating its presumed affinity, such as \"Araucarioxylon\" (wood of \"Araucaria\" or some related genus), \"Palmoxylon\" (wood of an indeterminate palm), or \"Castanoxylon\" (wood of an indeterminate chinkapin).\n\nPetrified wood are fossils of wood that have turned to stone through the process of permineralization. All organic materials are replaced with minerals while maintaining the original structure of the wood.\n\nThe most notable example is the petrified forest in Arizona.\n\nMummified wood are fossils of wood that have not permineralized. They are formed when trees are buried rapidly in dry cold or hot environments. They are valued in paleobotany because they retain original cells and tissues capable of being examined with the same techniques used with extant plants in dendrology.\n\nNotable examples include the mummified forests in Ellesmere Island and Axel Heiberg Island.\n\nSubmerged forests are remains of trees submerged by marine transgression. They are important in determining sea level rise since the last glacial period.\n\n"}
{"id": "6465284", "url": "https://en.wikipedia.org/wiki?curid=6465284", "title": "GASAG", "text": "GASAG\n\nGASAG (Berliner Gaswerke Aktiengesellschaft) is the main natural gas vendor in Berlin.\n\n"}
{"id": "13235968", "url": "https://en.wikipedia.org/wiki?curid=13235968", "title": "Global Forest Coalition", "text": "Global Forest Coalition\n\nThe Global Forest Coalition (GFC) is a coalition of NGOs and indigenous peoples organizations engaged in the global policy debate related to forests. The mission of the Global Forest Coalition is to advocate the rights of forest-dependent peoples as a basis for forest policy and addressing the direct and underlying causes of deforestation and forest degradation. To do so, the coalition facilitates effective and equitable participation of these groupsin global policy fora related to forests and monitors the implementation of agreed policy commitments. The three primary targets of the coalition are the United Nations Forum on Forests, the Framework Convention on Climate Change and the Convention on Biological Diversity.\n\nThe Global Forest Coalition was set up in 2000 by 19 groups from all over the world. It succeeds the NGO Forest Working Group, which was established in 1995 to advocate for social justice issues and the underlying causes of forest loss to be addressed in international forest policy debates.\n\nThe \"Forest Working Group\" was an informal network of 15-20 NGOs established in 1995, predating the Global Forest Coalition. It included NGOs from all regions that participated in international forest policy meetings and organized joint advocacy campaigns on issues such as indigenous peoples’ rights, socially-just forest policy, and underlying causes of forest loss.\n"}
{"id": "76236", "url": "https://en.wikipedia.org/wiki?curid=76236", "title": "Ground effect (aerodynamics)", "text": "Ground effect (aerodynamics)\n\nIn fixed-wing aircraft, ground effect is the increased lift (force) and decreased aerodynamic drag that an aircraft's wings generate when they are close to a fixed surface. \nWhen landing, ground effect can give the pilot the feeling that the aircraft is \"floating\". When taking off, ground effect may temporarily reduce the stall speed. The pilot can then fly just above the runway while the aircraft accelerates in ground effect until a safe climb speed is reached.\n\nWhen an aircraft flies at a ground level approximately at or below the length of the aircraft's wingspan or helicopter's rotor diameter, there occurs, depending on airfoil and aircraft design, an often noticeable ground effect. This is caused primarily by the ground interrupting the wingtip vortices and downwash behind the wing. When a wing is flown very close to the ground, wingtip vortices are unable to form effectively due to the obstruction of the ground. The result is lower induced drag, which increases the speed and lift of the aircraft.\n\nA wing generates lift by deflecting the oncoming airmass (relative wind) downward. The deflected or \"turned\" flow of air creates a resultant force on the wing in the opposite direction (Newton's 3rd law). The resultant force is identified as lift. Flying close to a surface increases air pressure on the lower wing surface, nicknamed the \"ram\" or \"cushion\" effect, and thereby improves the aircraft lift-to-drag ratio. The lower/nearer the wing is with regards to the ground, the more pronounced the ground effect becomes. While in the ground effect, the wing requires a lower angle of attack to produce the same amount of lift. If the angle of attack and velocity remain constant, an increase in the lift coefficient ensues, which accounts for the \"floating\" effect. Ground effect also alters thrust versus velocity, where reduced induced drag requires less thrust in order to maintain the same velocity.\n\nLow winged aircraft are more affected by ground effect than high wing aircraft. Due to the change in up-wash, down-wash, and wingtip vortices there may be errors in the airspeed system while in ground effect due to changes in the local pressure at the static source.\n\nAnother important issue regarding ground effect is that the makeup of the surface directly affects the intensity; this is to say that a concrete or other smooth hard surface will produce more effect than water or broken ground.\n\nMany vehicles have a design that makes use of the wing in ground effect. Although all airplanes fly through ground effect at some point, craft that do so in a dedicated manner are designed in such a way that their wings are normally unable to take them into flight out of ground effect (free flight). Those that can fly out of ground effect are often capable of only a short distance take-off into free flight. Because of this, these craft are often licensed as ships rather than as aircraft. These specially designed craft may use delta wings, ekranoplan wings, or tandem wings.\n\n\n"}
{"id": "14992929", "url": "https://en.wikipedia.org/wiki?curid=14992929", "title": "Hydrogen tank", "text": "Hydrogen tank\n\nA Hydrogen tank (other names- cartridge or canister) is used for hydrogen storage. The first type IV hydrogen tanks for compressed hydrogen at were demonstrated in 2001, the first fuel cell vehicles on the road with type IV tanks are the Toyota FCHV, Mercedes-Benz F-Cell and the GM HydroGen4.\n\nVarious applications have allowed the development of different H2 storage scenarios.\nRecently, the Hy-Can consortium has introduced a small one liter, format. Horizon Fuel Cells is now selling a refillable metal hydride form factor for consumer use called HydroStik.\n\n\n\n\n\n\nIn accordance with ISO/TS 15869 (revised):\n\nActual Standard EC 79/2009\n\nUsing magnesium for hydrogen storage, a safe but weighty reversible storage technology. Typically the pressure requirement are limited to .\nThe charging process generates heat whereas the discharge process will require some heat to release the H2 contained in the storage material. To activate those type of hydrides, at the current state of development you need to reach approximately . \n\nSee also sodium aluminium hydride\n\n\n\n"}
{"id": "52821559", "url": "https://en.wikipedia.org/wiki?curid=52821559", "title": "Italdesign GTZERO", "text": "Italdesign GTZERO\n\nThe Italdesign GTZERO is an electric shooting-brake concept produced by Italian design company Italdesign Giugiaro and showcased at the 2016 Geneva Motor Show.\n\nThe GTZERO is powered by three electric motors, two in the front and one in the rear, and is all-wheel drive. The front electric motors produce 148 hp (110 kW) each and the rear motor produces 188 hp (140 kW) for a total of 483 hp (360 kW) and an electronically limited top speed of 155 miles per hour. Italdesign says the car has a range of 310 miles and can recharge to 80 percent capacity in 30 minutes.\n\nThe GTZERO is built on a modular carbon fiber monocoque, which Italdesign says houses the batteries and allows for multiple body styles to be used. The chassis also features aluminum front and rear subframes. a lightweight composite body, butterfly doors and four wheel steering with 5 degrees of steering angle. The interior features configurable seating which can be changed from a 2+2 to a 3+1 arrangement as well as multiple touchscreens to control interior functions.\n"}
{"id": "41510584", "url": "https://en.wikipedia.org/wiki?curid=41510584", "title": "JPods", "text": "JPods\n\nJPods is a personal rapid transit concept which uses distributed collaborative computer networks to route transit in a manner similar to the data trafficking of the Internet. Developed by JPods, Inc, the vehicles consist of ultra-light pods controlled by on-board computers.\n\nPersonal rapid transit was defined in Congressional Office of Technology Assessment Study \"PB-244854\" as the solution for urban transport in response to the 1973 Oil Embargo. Morgantown, West Virginia installed a PRT system that has delivered 110 million oil-free, injury-free passenger miles, but where huge budget overruns crippled interest in PRT for several decades.\n\nIn the demonstration JPod, people get in, select a destination on a touch screen and the vehicle navigates to that address. In production models people and/or cargo will set destination and travel non-stop from origin to destination.\n\nJPods has signed letters-of-intent (see image of letter to the right) to build networks in 2014 with the cities of Secaucus ( network, US-NJ), Anshan (, China), and Linyi (, China). On June 25, 2014 the Town of Secaucus passed a Performance Standards Ordinance making it a law to grant rights of way access based on exceeding 120 passenger-miles per gallon.\n\n\nThe computer network is managed in three tiers:\n\n\n"}
{"id": "30366430", "url": "https://en.wikipedia.org/wiki?curid=30366430", "title": "Jinghong Dam", "text": "Jinghong Dam\n\nThe Jinghong Dam () is a gravity dam composed of roller-compacted concrete on the Lancang (Mekong) River near Jinghong in Yunnan Province, China. The main purpose of the dam is hydroelectric power production and it has an associated 1,750 MW power station. Part of the power generated is sold to Thailand under an agreement with China.\n\n"}
{"id": "4007815", "url": "https://en.wikipedia.org/wiki?curid=4007815", "title": "Jobber (fuel)", "text": "Jobber (fuel)\n\nA jobber, or petroleum marketer, is a person or company that purchases quantities of refined fuel from refining companies (e.g. BP, Shell, Exxon), either for sale to retailers (e.g., gasoline stations), or to sell directly to the users of those products (e.g., home heating oil to homeowners, lubricating oils to industrial operations or repair shops, jet fuel to FBOs, etc.). In essence, the jobber acts as the \"middleman\" between the company that refines the petroleum products and those that either use them or market them at retail prices. The jobber often owns the gasoline being sold, and the station it is being sold to, but allows an operator to lease the store.\n\nIn 2001, 44.3% of all gasoline in the U.S. was sold through jobbers. Approximately the same percentage was sold through integrated oil company owned and operated stores or franchise arrangements. The percentage of jobbers responsible for fuel sale in the USA in 2004 fell to 37.3%.\n\nJobbers are represented by trade associations such as the Association for Convenience & Fuel Retailing, National Association of Shell Marketers, Sigma: America's Leading Fuel Marketers, and the Petroleum Marketers Association of America.\n"}
{"id": "34943500", "url": "https://en.wikipedia.org/wiki?curid=34943500", "title": "Kwinana Oil Refinery", "text": "Kwinana Oil Refinery\n\nThe Kwinana Refinery, operated by BP, is located on the shore of Cockburn Sound at Kwinana, near Fremantle, Western Australia. It was constructed by the then Anglo-Iranian Oil Company and completed in 1955. It is the largest oil refinery in Australia, with a capacity of .\n\n"}
{"id": "26419679", "url": "https://en.wikipedia.org/wiki?curid=26419679", "title": "Mill Run Wind Energy Center", "text": "Mill Run Wind Energy Center\n\nThe Mill Run Wind Energy Center is a wind farm located in Fayette County, Pennsylvania with ten 1.5 MW Enron Wind TZs that began commercial operation in November 2001. The wind farm has a combined total nameplate capacity of 15 MW, but actually produces about 39,420 megawatt-hours of electricity annually. The wind farm was developed by Atlantic Renewable Energy and Horizon Wind Energy, and constructed and operated by NextEra Energy Resources, based in Florida.\n\n\n"}
{"id": "14572586", "url": "https://en.wikipedia.org/wiki?curid=14572586", "title": "Mutation Sensation", "text": "Mutation Sensation\n\nMutation Sensation is a recycling project based on reusing discarded “stuffed animals” toys.\n\nThe concept has become an evolutionary project that reconstructs the way people think about the market economy in reference to mass production (of toys, in this case) which symbolizes the early education of abundance as pleasure. It is also awakening an avenue that opens eyes to the possibilities of alternative equal rights relations between the producers and the consumers of the world. With respect to the inequality of the market, the 1st Mutation Sensation session occurred during a festival under various gamey stipulations such as: long hours, unfavourable work conditions and no pay. It was an intense lesson as to how much work is involved in sewing toys. The Mutations have continued to develop amongst recycling festivals and childhood ecological recycling projects.\n\nMutation Sensations serve an important demand for performing illusionary transformations and possible elimination of iconographic plush toys into futuristic idealism.\n\n"}
{"id": "23974290", "url": "https://en.wikipedia.org/wiki?curid=23974290", "title": "Ningde Nuclear Power Plant", "text": "Ningde Nuclear Power Plant\n\nNingde Nuclear Power Plant () is a nuclear power plant in Fujian province, China. \nThe site is located in Beiwan village in the town of Qinyu, Fuding, Ningde, Fujian. The plant will ultimately have six 1,080 megawatt (MWe) CPR-1000 pressurized water reactors (PWRs). \nThe first reactor began operation on 18 April 2013.The Ningde Nuclear Power project was approved by the National Development and Reform Commission (NDRC) in 2007.\nThe project is 51% funded by the Guangdong Nuclear Investment Company Ltd, with Datang International Power Generation Co and the Fujian Coal Group completing the shareholding. A total investment of 52 billion yuan (US$7.6 billion) should result in the completion of Ningde Phase I.\nIncluding the final two units of Phase II, the total cost will exceed 70 billion yuan. \nThe four units of Phase I will generate about 30 billion kilowatt hours per year, for which the plant will charge 0.37 yuan/kW·h (11 billion yuan/year).\n\nNingde marks a step in the development of China's domestic nuclear industry. \nShu Guogang, GM of China Guangdong Nuclear Power Project said, \"We built 55 percent of Ling Ao Phase 2, 70 percent of Hongyanhe, 80 percent of Ningde and 90 percent of Yangjiang Station.\"\nSite preparation at Ningde ran through 2007, with the first concrete for Ningde 1 poured in February 2008. \nNingde 2 followed nine months later. Construction of each unit is expected to take 58 months. \nNingde 1 was grid connected on 28 December 2012 and entered full commercial operation on 18 April 2013.\n\n\n"}
{"id": "37720926", "url": "https://en.wikipedia.org/wiki?curid=37720926", "title": "Nuclear power programme in Pakistan", "text": "Nuclear power programme in Pakistan\n\nThe Nuclear power programme in Pakistan started with the Karachi Nuclear Power Plant, also known as KANUPP, inaugurated November 28, 1972. Pakistan is the first Muslim country in the world to construct and operate civil nuclear power plants.\n\nThe \"Nuclear Power Programme 2050\" is the official nuclear energy policy of the Government of Pakistan to make usage of nuclear power to meet the existing electricity crises and to respond to the future requirements of a growing population and national economy. The program is envisaged to increase energy production from nuclear sources by the year of 2050. As part of the energy security strategy, the enactment of the program is aim to expand the self-sustaining nuclear power infrastructure all over the country by year 2050. It came in a strong response to U.S-India nuclear deal, as well as to counter the existing energy shortfalls and future requirements of a growing population and national economy. The primary focus of this program is to promote scientific and socio-economic development of the people as a \"foremost priority.\"\n\nThe policy was first stated by the former Prime minister Yousaf Raza Gillani during in the meeting of Nuclear Command Authority; at this meeting, the program was approved by Prime minister Gillani on 14 July 2011. This includes to regulate the nuclear facilities, waste storage, along with an increase in power plant building. Both legislative, military and bureaucratic regulations of nuclear energy in Pakistan have been shaped by the scientific research and the public opinion. The governmental nuclear regulatory authorities in Pakistan has projected to produce 4345 MW electricity from nuclear sources in 2022; and 8800 MW electricity by 2030.\n"}
{"id": "38568823", "url": "https://en.wikipedia.org/wiki?curid=38568823", "title": "Perleberg Solar Park", "text": "Perleberg Solar Park\n\nThe Perleberg Solar Park is a photovoltaic power station, with an installed capacity of 35 megawatts (MW). It uses 144,144 solar panels manufactured by Chinese company Yingli. The panels are mounted at a fixed angle on posts that are driven into the ground, at a former military airport.\n\n"}
{"id": "1182993", "url": "https://en.wikipedia.org/wiki?curid=1182993", "title": "Pomeron", "text": "Pomeron\n\nIn physics, the pomeron is a Regge trajectory — a family of particles with increasing spin — postulated in 1961 to explain the slowly rising cross section of hadronic collisions at high energies. It is named after Isaak Pomeranchuk.\n\nWhile other trajectories lead to falling cross sections, the pomeron can lead to logarithmically rising cross sections — which, experimentally, are approximately constant ones. The identification of the pomeron and the prediction of its properties was a major success of the Regge theory of strong interaction phenomenology. In later years, a BFKL pomeron was derived in further kinematic regimes from perturbative calculations in QCD, but its relationship to the pomeron seen in soft high energy scattering is still not fully understood.\n\nOne consequence of the pomeron hypothesis is that the cross sections of proton–proton and proton–antiproton scattering should be equal at high enough energies. This was demonstrated by the Soviet physicist Isaak Pomeranchuk by analytic continuation assuming only that the cross sections do not fall. The pomeron itself was introduced by Vladimir Gribov, and it incorporated this theorem into Regge theory. Geoffrey Chew and Steven Frautschi introduced the pomeron in the West. The modern interpretation of Pomeranchuk's theorem is that the pomeron has no conserved charges—the particles on this trajectory have the quantum numbers of the vacuum.\n\nThe pomeron was well accepted in the 1960s despite the fact that the measured cross sections of proton–proton and proton–antiproton scattering at the energies then available were unequal.\n\nThe pomeron carries no charges. The absence of electric charge implies that pomeron exchange does not lead to the usual shower of Cherenkov radiation, while the absence of color charge implies that such events do not radiate pions.\n\nThis is in accord with experimental observation. In high energy proton–proton and proton–antiproton collisions in which it is believed that pomerons have been exchanged, a \"rapidity gap\" is often observed: This is a large angular region in which no outgoing particles are detected.\n\nThe odderon, the counterpart of the pomeron that carries odd charge parity was introduced in 1973 by Leszek Łukaszuk and Basarab Nicolescu. It was potentially observed only in 2017 by the TOTEM experiment at the LHC. Odderon exists in QCD as compound state of 3 reggeized gluons.\n\nIn early particle physics, the 'pomeron sector' was what is now called the 'closed string sector' while what was called the 'reggeon sector' is now the 'open string theory'.\n\n\n"}
{"id": "49058608", "url": "https://en.wikipedia.org/wiki?curid=49058608", "title": "Prostanoic acid", "text": "Prostanoic acid\n\nProstanoic acid (7-[(1S,2S)-2-octylcyclopentyl]heptanoic acid) is a saturated fatty acid which contains a cyclopentane ring. Its derivatives are prostaglandins - physiologically active lipid substances. Prostanoic acid is not found in nature, but it can be synthesized \"in vitro\".\n\nFor the first time the synthesis of prostanoic acid from 1-formylcyclopentene was considered in detail in the scientific literature in 1975 by a group of French pharmacists. One year later, a group of Japanese scientists, who worked in the central research laboratory of the \"Sankyo Co., Ltd.\" company (Shinagawa, Tokyo), published another method for obtaining prostanoic acid from 2-[4-hydroxy-5-(methoxymethyl)cyclopent-2-en-1-yl] acetic acid. In 1986, a group of Japanese scientists from Kyushu University in Fukuoka proposed their own scheme for obtaining prostanoic acid from limonene.\n\n"}
{"id": "25602", "url": "https://en.wikipedia.org/wiki?curid=25602", "title": "Radium", "text": "Radium\n\nRadium is a chemical element with symbol Ra and atomic number 88. It is the sixth element in group 2 of the periodic table, also known as the alkaline earth metals. Pure radium is silvery-white, but it readily reacts with nitrogen (rather than oxygen) on exposure to air, forming a black surface layer of radium nitride (RaN). All isotopes of radium are highly radioactive, with the most stable isotope being radium-226, which has a half-life of 1600 years and decays into radon gas (specifically the isotope radon-222). When radium decays, ionizing radiation is a product, which can excite fluorescent chemicals and cause radioluminescence.\n\nRadium, in the form of radium chloride, was discovered by Marie and Pierre Curie in 1898. They extracted the radium compound from uraninite and published the discovery at the French Academy of Sciences five days later. Radium was isolated in its metallic state by Marie Curie and André-Louis Debierne through the electrolysis of radium chloride in 1911.\n\nIn nature, radium is found in uranium and (to a lesser extent) thorium ores in trace amounts as small as a seventh of a gram per ton of uraninite. Radium is not necessary for living organisms, and adverse health effects are likely when it is incorporated into biochemical processes because of its radioactivity and chemical reactivity. Currently, other than its use in nuclear medicine, radium has no commercial applications; formerly, it was used as a radioactive source for radioluminescent devices and also in radioactive quackery for its supposed curative powers. Today, these former applications are no longer in vogue because radium's toxicity has since become known, and less dangerous isotopes are used instead in radioluminescent devices.\n\nRadium is the heaviest known alkaline earth metal and is the only radioactive member of its group. Its physical and chemical properties most closely resemble its lighter congener barium.\n\nPure radium is a volatile silvery-white metal, although its lighter congeners calcium, strontium, and barium have a slight yellow tint. Its color rapidly vanishes in air, yielding a black layer of radium nitride (RaN). Its melting point is either or and its boiling point is . Both of these values are slightly lower than those of barium, confirming periodic trends down the group 2 elements. Like barium and the alkali metals, radium crystallizes in the body-centered cubic structure at standard temperature and pressure: the radium–radium bond distance is 514.8 picometers. Radium has a density of 5.5 g/cm, higher than that of barium, again confirming periodic trends; the radium-barium density ratio is comparable to the radium-barium atomic mass ratio, due to the two elements' similar crystal structures.\n\nRadium has 33 known isotopes, with mass numbers from 202 to 234: all of them are radioactive. Four of these – Ra (half-life 11.4 days), Ra (3.64 days), Ra (1600 years), and Ra (5.75 years) – occur naturally in the decay chains of primordial thorium-232, uranium-235, and uranium-238 (Ra from uranium-235, Ra from uranium-238, and the other two from thorium-232). These isotopes nevertheless still have half-lives too short to be primordial radionuclides and only exist in nature from these decay chains. Together with the artificial Ra (15 d), these are the five most stable isotopes of radium. All other known radium isotopes have half-lives under two hours, and the majority have half-lives under a minute. At least 12 nuclear isomers have been reported; the most stable of them is radium-205m, with a half-life of between 130 and 230 milliseconds, which is still shorter than twenty-four ground-state radium isotopes.\n\nIn the early history of the study of radioactivity, the different natural isotopes of radium were given different names. In this scheme, Ra was named actinium X (AcX), Ra thorium X (ThX), Ra radium (Ra), and Ra mesothorium 1 (MsTh). When it was realized that all of these are isotopes of the same element, many of these names fell out of use, and \"radium\" came to refer to all isotopes, not just Ra. Some of radium-226's decay products received historical names including \"radium\", ranging from radium A to radium G, with the letter indicating approximately how far they were down the chain from their parent Ra.\n\nRa is the most stable isotope of radium and is the last isotope in the (4\"n\" + 2) decay chain of uranium-238 with a half-life of over a millennium: it makes up almost all of natural radium. Its immediate decay product is the dense radioactive noble gas radon, which is responsible for much of the danger of environmental radium. It is 2.7 million times more radioactive than the same molar amount of natural uranium (mostly uranium-238), due to its proportionally shorter half-life.\n\nA sample of radium metal maintains itself at a higher temperature than its surroundings because of the radiation it emits – alpha particles, beta particles, and gamma rays. More specifically, natural radium (which is mostly Ra) emits mostly alpha particles, but other steps in its decay chain (the uranium or radium series) emit alpha or beta particles, and almost all particle emissions are accompanied by gamma rays.\n\nIn 2013 it was discovered that the nucleus of Radium-224 is pear-shaped. This was the first discovery of an asymmetric nucleus.\n\nRadium, like barium, is a highly reactive metal and always exhibits its group oxidation state of +2. It forms the colorless Ra cation in aqueous solution, which is highly basic and does not form complexes readily. Most radium compounds are therefore simple ionic compounds, though participation from the 6s and 6p electrons (in addition to the valence 7s electrons) is expected due to relativistic effects and would enhance the covalent character of radium compounds such as RaF and RaAt. For this reason, the standard electrode potential for the half-reaction Ra (aq) + 2e → Ra (s) is −2.916 V, even slightly lower than the value −2.92 V for barium, whereas the values had previously smoothly increased down the group (Ca: −2.84 V; Sr: −2.89 V; Ba: −2.92 V). The values for barium and radium are almost exactly the same as those of the heavier alkali metals potassium, rubidium, and caesium.\n\nSolid radium compounds are white as radium ions provide no specific coloring, but they gradually turn yellow and then dark over time due to self-radiolysis from radium's alpha decay. Insoluble radium compounds coprecipitate with all barium, most strontium, and most lead compounds.\n\nRadium oxide (RaO) has not been characterized well past its existence, despite oxides being common compounds for the other alkaline earth metals. Radium hydroxide (Ra(OH)) is the most readily soluble among the alkaline earth hydroxides and is a stronger base than its barium congener, barium hydroxide. It is also more soluble than actinium hydroxide and thorium hydroxide: these three adjacent hydroxides may be separated by precipitating them with ammonia.\n\nRadium chloride (RaCl) is a colorless, luminous compound. It becomes yellow after some time due to self-damage by the alpha radiation given off by radium when it decays. Small amounts of barium impurities give the compound a rose color. It is soluble in water, though less so than barium chloride, and its solubility decreases with increasing concentration of hydrochloric acid. Crystallization from aqueous solution gives the dihydrate RaCl·2HO, isomorphous with its barium analog.\n\nRadium bromide (RaBr) is also a colorless, luminous compound. In water, it is more soluble than radium chloride. Like radium chloride, crystallization from aqueous solution gives the dihydrate RaBr·2HO, isomorphous with its barium analog. The ionizing radiation emitted by radium bromide excites nitrogen molecules in the air, making it glow. The alpha particles emitted by radium quickly gain two electrons to become neutral helium, with builds up inside and weakens radium bromide crystals. This effect sometimes causes the crystals to break or even explode.\n\nRadium nitrate (Ra(NO)) is a white compound that can be made by dissolving radium carbonate in nitric acid. As the concentration of nitric acid increases, the solubility of radium nitrate decreases, an important property for the chemical purification of radium.\n\nRadium forms much the same insoluble salts as its lighter congener barium: it forms the insoluble sulfate (RaSO, the most insoluble known sulfate), chromate (RaCrO), carbonate (RaCO), iodate (Ra(IO)), tetrafluoroberyllate (RaBeF), and nitrate (Ra(NO)). With the exception of the carbonate, all of these are less soluble in water than the corresponding barium salts, but they are all isostructural to their barium counterparts. Additionally, radium phosphate, oxalate, and sulfite are probably also insoluble, as they coprecipitate with the corresponding insoluble barium salts. The great insolubility of radium sulfate (at 20 °C, only 2.1 mg will dissolve in 1 kg of water) means that it is one of the less biologically dangerous radium compounds. The large ionic radius of Ra (148 pm) results in weak complexation and poor extraction of radium from aqueous solutions when not at high pH.\n\nAll isotopes of radium have half-lives much shorter than the age of the Earth, so that any primordial radium would have decayed long ago. Radium nevertheless still occurs in the environment, as the isotopes Ra, Ra, Ra, and Ra are part of the decay chains of natural thorium and uranium isotopes; since thorium and uranium have very long half-lives, these daughters are continually being regenerated by their decay. Of these four isotopes, the longest-lived is Ra (half-life 1600 years), a decay product of natural uranium. Because of its relative longevity, Ra is the most common isotope of the element, making up about one part per trillion of the Earth's crust; essentially all natural radium is Ra. Thus, radium is found in tiny quantities in the uranium ore uraninite and various other uranium minerals, and in even tinier quantities in thorium minerals. One ton of pitchblende typically yields about one seventh of a gram of radium. One kilogram of the Earth's crust contains about 900 picograms of radium, and one liter of sea water contains about 89 femtograms of radium.\n\nRadium was discovered by Marie Sklodowska-Curie and her husband Pierre Curie on 21 December 1898, in a uraninite (pitchblende) sample. While studying the mineral earlier, the Curies removed uranium from it and found that the remaining material was still radioactive. They separated out an element similar to bismuth from pitchblende in July 1898, that turned out to be polonium. They then separated out a radioactive mixture consisting mostly of two components: compounds of barium, which gave a brilliant green flame color, and unknown radioactive compounds which gave carmine spectral lines that had never been documented before. The Curies found the radioactive compounds to be very similar to the barium compounds, except that they were more insoluble. This made it possible for the Curies to separate out the radioactive compounds and discover a new element in them. The Curies announced their discovery to the French Academy of Sciences on 26 December 1898. The naming of radium dates to about 1899, from the French word \"radium\", formed in Modern Latin from \"radius\" (\"ray\"): this was in recognition of radium's power of emitting energy in the form of rays.\n\nOn September 1910, Marie Curie and André-Louis Debierne announced that they had isolated radium as a pure metal through the electrolysis of a pure radium chloride (RaCl) solution using a mercury cathode, producing a radium–mercury amalgam. This amalgam was then heated in an atmosphere of hydrogen gas to remove the mercury, leaving pure radium metal. Later that same year, E. Eoler isolated radium by thermal decomposition of its azide, Ra(N). Radium metal was first industrially produced in the beginning of the 20th century by Biraco, a subsidiary company of Union Minière du Haut Katanga (UMHK) in its Olen plant in Belgium.\n\nThe common historical unit for radioactivity, the curie, is based on the radioactivity of Ra.\n\nRadium was formerly used in self-luminous paints for watches, nuclear panels, aircraft switches, clocks, and instrument dials. A typical self-luminous watch that uses radium paint contains around 1 microgram of radium. In the mid-1920s, a lawsuit was filed against the United States Radium Corporation by five dying \"Radium Girls\" – dial painters who had painted radium-based luminous paint on the dials of watches and clocks. The dial painters were instructed to lick their brushes to give them a fine point, thereby ingesting radium. Their exposure to radium caused serious health effects which included sores, anemia, and bone cancer. This is because radium is treated as calcium by the body, and deposited in the bones, where radioactivity degrades marrow and can mutate bone cells.\n\nDuring the litigation, it was determined that the company's scientists and management had taken considerable precautions to protect themselves from the effects of radiation, yet had not seen fit to protect their employees. Additionally, for several years the companies had attempted to cover up the effects and avoid liability by insisting that the Radium Girls were instead suffering from syphilis. This complete disregard for employee welfare had a significant impact on the formulation of occupational disease labor law.\n\nAs a result of the lawsuit, the adverse effects of radioactivity became widely known, and radium-dial painters were instructed in proper safety precautions and provided with protective gear. In particular, dial painters no longer licked paint brushes to shape them (which caused some ingestion of radium salts). Radium was still used in dials as late as the 1960s, but there were no further injuries to dial painters. This highlighted that the harm to the Radium Girls could easily have been avoided.\n\nFrom the 1960s the use of radium paint was discontinued. In many cases luminous dials were implemented with non-radioactive fluorescent materials excited by light; such devices glow in the dark after exposure to light, but the glow fades. Where long-lasting self-luminosity in darkness was required, safer radioactive promethium-147 (half-life 2.6 years) or tritium (half-life 12 years) paint was used; both continue to be used today. These had the added advantage of not degrading the phosphor over time, unlike radium. Tritium emits very low-energy beta radiation (even lower-energy than the beta radiation emitted by promethium) which cannot penetrate the skin, rather than the penetrating gamma radiation of radium and is regarded as safer.\n\nClocks, watches, and instruments dating from the first half of the 20th century, often in military applications, may have been painted with radioactive luminous paint. They are usually no longer luminous; however, this is not due to radioactive decay of the radium (which has a half-life of 1600 years) but to the fluorescence of the zinc sulfide fluorescent medium being worn out by the radiation from the radium. The appearance of an often thick layer of green or yellowish brown paint in devices from this period suggests a radioactive hazard. The radiation dose from an intact device is relatively low and usually not an acute risk; but the paint is dangerous if released and inhaled or ingested.\n\nRadium was once an additive in products such as toothpaste, hair creams, and even food items due to its supposed curative powers. Such products soon fell out of vogue and were prohibited by authorities in many countries after it was discovered they could have serious adverse health effects. (See, for instance, \"Radithor\" or \"Revigator\" types of \"Radium water\" or \"Standard Radium Solution for Drinking\".) Spas featuring radium-rich water are still occasionally touted as beneficial, such as those in Misasa, Tottori, Japan. In the U.S., nasal radium irradiation was also administered to children to prevent middle-ear problems or enlarged tonsils from the late 1940s through the early 1970s.\n\nRadium (usually in the form of radium chloride or radium bromide) was used in medicine to produce radon gas which in turn was used as a cancer treatment; for example, several of these radon sources were used in Canada in the 1920s and 1930s. However, many treatments that were used in the early 1900s are not used anymore because of the harmful effects radium bromide exposure caused. Some examples of these effects are anaemia, cancer, and genetic mutations. Safer gamma emitters such as Co, which is less costly and available in larger quantities, are usually used today to replace the historical use of radium in this application.\n\nEarly in the 1900s, biologists used radium to induce mutations and study genetics. As early as 1904, Daniel MacDougal used radium in an attempt to determine whether it could provoke sudden large mutations and cause major evolutionary shifts. Thomas Hunt Morgan used radium to induce changes resulting in white-eyed fruit flies.\nNobel-winning biologist Hermann Muller briefly studied the effects of radium on fruit fly mutations before turning to more affordable x-ray experiments.\n\nHoward Atwood Kelly, one of the founding physicians of Johns Hopkins Hospital, was a major pioneer in the medical use of radium to treat cancer. His first patient was his own aunt in 1904, who died shortly after surgery. Kelly was known to use excessive amounts of radium to treat various cancers and tumors. As a result, some of his patients died from radium exposure. His method of radium application was inserting a radium capsule near the affected area, then sewing the radium \"points\" directly to the tumor. This was the same method used to treat Henrietta Lacks, the host of the original HeLa cells, for cervical cancer. Currently, safer and more available radioisotopes are used instead.\n\nUranium had no large scale application in the late 19th century and therefore no large uranium mines existed. In the beginning the only large source for uranium ore was the silver mines in Joachimsthal, Austria-Hungary (now Jáchymov, Czech Republic). The uranium ore was only a byproduct of the mining activities.\n\nIn the first extraction of radium Curie used the residues after extraction of uranium from pitchblende. The uranium had been extracted by dissolution in sulfuric acid leaving radium sulfate, which is similar to barium sulfate but even less soluble in the residues. The residues also contained rather substantial amounts of barium sulfate which thus acted as a carrier for the radium sulfate. The first steps of the radium extraction process involved boiling with sodium hydroxide followed by hydrochloric acid treatment to remove as much as possible of other compounds. The remaining residue was then treated with sodium carbonate to convert the barium sulfate into barium carbonate carrying the radium, thus making it soluble in hydrochloric acid. After dissolution the barium and radium are reprecipitated as sulfates and this was repeated one or few times, for further purification of the mixed sulfate. Some impurities, that form insoluble sulfides, were removed by treating the chloride solution with hydrogen sulfide followed by filtering. When the mixed sulfate were pure enough they were once more converted to mixed chloride and barium and radium were separated by fractional crystallisation while monitoring the progress using a spectroscope (radium gives characteristic red lines in contrast to the green barium lines), and the electroscope.\n\nAfter the isolation of radium by Marie and Pierre Curie from uranium ore from Joachimsthal several scientists started to isolate radium in small quantities. Later small companies purchased mine tailings from Joachimsthal mines and started isolating radium. In 1904 the Austrian government nationalised the mines and stopped exporting raw ore. For some time radium availability was low.\n\nThe formation of an Austrian monopoly and the strong urge of other countries to have access to radium led to a worldwide search for uranium ores. The United States took over as leading producer in the early 1910s. The Carnotite sands in Colorado provide some of the element, but richer ores are found in the Congo and the area of the Great Bear Lake and the Great Slave Lake of northwestern Canada. Neither of the deposits is mined for radium but the uranium content makes mining profitable.\n\nThe Curies' process was still used for industrial radium extraction in 1940, but mixed bromides were then used for the fractionation. If the barium content of the uranium ore is not high enough it is easy to add some to carry the radium. These processes were applied to high grade uranium ores but may not work well with low grade ores.\n\nSmall amounts of radium were still extracted from uranium ore by this method of mixed precipitation and ion exchange as late as the 1990s, but today they are extracted only from spent nuclear fuel. In 1954, the total worldwide supply of purified radium amounted to about and it is still in this range today, while the annual production of pure radium compounds is only about 100 g in total today. The chief radium-producing countries are Belgium, Canada, the Czech Republic, Slovakia, the United Kingdom, and Russia. The amounts of radium produced were and are always relatively small; for example, in 1918, 13.6 g of radium were produced in the United States. The metal is isolated by reducing radium oxide with aluminium metal in a vacuum at 1200 °C.\n\nSome of the few practical uses of radium are derived from its radioactive properties. More recently discovered radioisotopes, such as cobalt-60 and caesium-137, are replacing radium in even these limited uses because several of these isotopes are more powerful emitters, safer to handle, and available in more concentrated form.\n\nThe isotope Ra (under the trade name Xofigo) was approved by the United States Food and Drug Administration in 2013 for use in medicine as a cancer treatment of bone metastasis. The main indication of treatment with Xofigo is the therapy of bony metastases from castration-resistant prostate cancer due to the favourable characteristics of this alpha-emitter radiopharmaceutical. Ra has also been used in experiments concerning therapeutic irradiation, as it is the only reasonably long-lived radium isotope which does not have radon as one of its daughters.\n\nRadium is still used today as a radiation source in some industrial radiography devices to check for flawed metallic parts, similarly to X-ray imaging. When mixed with beryllium, radium acts as a neutron source. Radium-beryllium neutron sources are still sometimes used even today, but other materials such as polonium are now more common: about 1500 polonium-beryllium neutron sources, with an individual activity of , have been used annually in Russia. These RaBeF-based (α, n) neutron sources have been deprecated despite the high number of neutrons they emit (1.84×10 neutrons per second) in favour of Am–Be sources. Today, the isotope Ra is mainly used to form Ac by neutron irradiation in a nuclear reactor.\n\nRadium is highly radioactive and its immediate daughter, radon gas, is also radioactive. When ingested, 80% of the ingested radium leaves the body through the feces, while the other 20% goes into the bloodstream, mostly accumulating in the bones. Exposure to radium, internal or external, can cause cancer and other disorders, because radium and radon emit alpha and gamma rays upon their decay, which kill and mutate cells. At the time of the Manhattan Project in 1944, the \"tolerance dose\" for workers was set at 0.1 micrograms of ingested radium.\n\nSome of the biological effects of radium were apparent from the start. The first case of so-called \"radium-dermatitis\" was reported in 1900, only 2 years after the element's discovery. The French physicist Antoine Becquerel carried a small ampoule of radium in his waistcoat pocket for 6 hours and reported that his skin became ulcerated. Pierre and Marie Curie were so intrigued by radiation that they sacrificed their own health to learn more about it. Pierre Curie attached a tube filled with radium to his arm for ten hours, which resulted in the appearance of a skin lesion, suggesting the use of radium to attack cancerous tissue as it had attacked healthy tissue. Handling of radium has been blamed for Marie Curie's death due to aplastic anemia. A significant amount of radium's danger comes from its daughter radon: being a gas, it can enter the body far more readily than can its parent radium.\n\nToday, Ra is considered to be the most toxic of the quantity radioelements, and it must be handled in tight glove boxes with significant airstream circulation that is then treated to avoid escape of its daughter Rn to the environment. Old ampoules containing radium solutions must be opened with care because radiolytic decomposition of water can produce an overpressure of hydrogen and oxygen gas.\n\n\n\n"}
{"id": "36054223", "url": "https://en.wikipedia.org/wiki?curid=36054223", "title": "Reclaimed Wood Council", "text": "Reclaimed Wood Council\n\nThe Reclaimed Wood Council was a trade association that promoted reclaimed wood from old buildings or from logs reclaimed from rivers. The Council was formed in May 2003 and dissolved in January 2008. \n"}
{"id": "55977783", "url": "https://en.wikipedia.org/wiki?curid=55977783", "title": "Separation energy", "text": "Separation energy\n\nIn nuclear physics, separation energy is the energy needed to remove one nucleon (or other specified particle or particles) from a nucleus.\n\nThe separation energy is different for each nuclide and particle to be removed. Values are stated as \"neutron separation energy\", \"two-neutron separation energy\", \"proton separation energy\", \"deuteron separation energy\", \"alpha separation energy\", and so on.\n\nThe lowest separation energy among stable nuclides is 1.67MeV, to remove a neutron from beryllium-9.\n\nThe energy can be added to the nucleus by an incident high-energy gamma ray. If the energy of the incident photon exceeds the separation energy, a photodisintegration might occur. Energy in excess of the threshold value becomes kinetic energy of the ejected particle.\n\nBy contrast, nuclear binding energy is the energy needed to completely disassemble a nucleus, or the energy released when a nucleus is assembled from nucleons. It is the sum of multiple separation energies, which should add to the same total regardless of the order of assembly or disassembly.\n\nElectron separation energy or electron binding energy, the energy required to remove one electron from a neutral atom or molecule (or cation) is called ionization energy. The reaction leads to photoionization, photodissociation, photoelectric effect, photovoltaics, etc.\n\nBond-dissociation energy is the energy required to break one bond of a molecule or ion, usually separating an atom or atoms.\n\n\n"}
{"id": "44588531", "url": "https://en.wikipedia.org/wiki?curid=44588531", "title": "Short fiber thermoplastics", "text": "Short fiber thermoplastics\n\nThermoplastics containing short fiber reinforcements were first introduced commercially in the 1960s. The most common type of fibers used in short fiber thermoplastics are glass fiber and carbon fiber \n. Adding short fibers to thermoplastic resins improves the composite performance for lightweight applications. In addition, short fiber thermoplastic composites are easier and cheaper to produce than continuous fiber reinforced composites. This compromise between cost and performance allows short fiber reinforced thermoplastics to be used in myriad applications.\n\nMechanical properties of short fiber reinforced composites depend critically on the fiber length distribution (FLD) and the fiber orientation distribution (FOD). In particular, the strength of short fiber reinforced composites increases with the increase of the mean fiber length and with the decrease of the mean fiber orientation angle (angle between the fiber axis and the loading direction).\nThe elastic modulus (E) of misaligned short fiber reinforced polymer composites depends on the distributions of fiber lengths and orientations within the composite structure. In general, the composite elastic modulus increases with the decrease of the mean fiber orientation angle and with the increase of the fiber orientation coefficient; and the elastic modulus increases with the increase of mean fiber length when the mean fiber length is small. When the mean fiber length is large, it has nearly no influence on the elastic modulus of short fiber reinforced composites.\n\nAn important characterizing parameter of short fiber composites is the aspect ratio (s) defined as the ratio between the length (l) and the diameter (d) of the fibers used as reinforcement:\nThe value of s can vary depending on fiber type and design, assuming values from approximately 50 to 500. Aspect ratios can affect properties such as the strain to failure and toughness. A higher aspect ratio will result in lower values of strain at failure and toughness, due to angular particles inducing crack formation.\n\nShort fiber reinforced composites are used increasingly as a structural material because they provide superior mechanical properties and can be easily produced by the rapid, low-cost injection molding process, by extrusion and with spray-up technique. An important issue for short fiber thermoplastic composites is void formation and growth during production processes. It has been shown that voids tend to nucleate at fiber ends, and their content depends on processing conditions, fiber concentration, and fiber length. For example, in an injection molding process bubble growth is suppressed by cooling the material under pressure. Density measurements confirm a much lower void content (-1%) in the injection-molded samples in comparison with the extrudates. \nAnother factor playing an important role in void formation is the cooling rate. While the melt is cooled external surface layers solidify first. These layers restrain the contraction of material within the melt. This leads to internal voiding. As a result, slower cooling rates decrease void content in the composite. Finally, in an extruded structure, longer fibers result in higher void contents. This unexpected behaviour is due to the overcoming of other factors like viscosity, extrusion pressure and shear rate, which make the analysis on this phenomenon very complicated.\n\nShort fiber thermoplastics can be modelled as a matrix with fiber inclusions. According to the inclusion model, the stress within the material is proportional to the product of inclusion volume fraction and the stress within a single inclusion. In other words, the stress within the composite is proportional to the fiber volume fraction and the stress on a single fiber. Using Mean Field Theory and the Mori-Tanaka model, the stresses within a short fiber thermoplastic can be modelled computationally.\nAssuming the matrix is a newtonian material, the creep from an applied shear stress can be approximated from equilibrium thermodynamics. This will yield information about the composite’s rheological response.\n\nShort fiber reinforced thermoplastics have a broad range of applications due to fiber reinforcement properties. Short fiber thermoplastics are able to withstand up to 30,000 psi of applied tensile load and have an elastic modulus on the order of 2 x 10 psi. They are ideal for applications for which toughness is of critical importance, high volume production is involved, and long shelf life and scrap recycling are important issues. With all of these performance capabilities, one of the greatest advantages to using short fiber reinforced thermoplastics is their ease of processing and reprocessability.\nEase of processing has been the key factor to the widespread use of short fiber reinforced thermoplastics. Effective processing techniques and the ability to recycle scrap offer significant cost reductions that compare to those of thermoset compounds and metals. Because of this, short fiber reinforced thermoplastics are desired in the electrical and electronic, automotive, oilfield, chemical process, and defense industries. Although short fiber thermoplastics have progressed considerably over the years and have a secured spot in a colossally-sized market, further refinement of compounding and process technology along with improvements in part design could allow the performance window of these materials to widen significantly, allowing them to be used for more applications in the future.\n\nInjection molding is a traditional cost effective method for manufacturing of short fiber thermoplastics. The processing conditions such as mold temperature and pressure as well as filling time, the part geometry, position and number of injection gates are main factors influencing distribution of fibers. As a result, depending on the total thickness of the manufactured parts as well as the distance from mold wall, different fiber orientation distributions can be observed. In a thin layer in mid-thickness fiber orientations are preferably perpendicular to the mold flow direction, while in two near wall thicknesses fibers are preferably in line with the mold flow direction.\n\nAn aspect of thermoplastics which distinguishes them from metallic materials is their time dependent properties as well as relatively low melting temperatures. As a result, the frequency at which a load is applied or rate of applied load is a determining factor on mechanical properties of such materials. Due to low thermal conductivity of thermoplastics, the generated heat due to energy dissipation under applying load results in self-heating or thermal degradation.In short fiber thermoplastics, the frictional heating between fiber and matrix as well as a higher intensity of stress near fiber ends increase the degree of self-heating.\n"}
{"id": "5538242", "url": "https://en.wikipedia.org/wiki?curid=5538242", "title": "Spot network substation", "text": "Spot network substation\n\nIn electricity distribution networks, spot network substations are used in interconnected distribution networks. They have the secondary network (also called a grid network) with all supply transformers bussed together on the secondary side at one location. Spot networks are considered the most reliable and most flexible arrangement of connecting power to all types of loads. Switching can be done without interrupting the power to the loads.\n\nElectricity distribution networks are typically of two types, radial or interconnected. A radial network arranges the station and branches like a tree with no connection to any other supply. This is typical of long rural lines with isolated load areas. In general, the radial distribution network has more power failures than the interconnected distribution networks.\n\nIn a secondary network the transformers are distributed across an area (e.g. in streets) and have multiple supplies. The transformers are wired together on the secondary side. The system is arranged so that nearby transformers do not use the same feeder. In case of an issue with a feeder (or transformer) the load is fed by nearby transformers, so there is no interruption, perhaps some voltage drop. Usually a fault on the secondary side is not a big problem since the conductors will destroy themselves.\n\nA spot network is basically a secondary network condensed to a point. Several transformers have multiple supplies and their secondaries are bussed together. Besides a region-wide blackout, they are vulnerable to a bus fault, which is extremely rare. The simplest case is where each transformer connects to one feeder and vice versa (\"unit system\"). High-voltage switching can be used to handle more cases, e.g., working transformer but faulty feeder or the reverse.\n\nUrban (spot) network substations can be used to make interconnected distribution networks to serve a single facility. Examples of such single facilities include airports, hospitals, major data processing centers (especially those using uninterruptible power supply), and sports arenas that regularly broadcast nationally televised events.\n\nIn large cities, many electric utility companies use grid feeders to make interconnected distribution networks to serve the downtown core. The interconnected network has multiple connections to the points of supply.\n\nNetwork protectors, (reverse current relays), are used to detect any open circuits that are letting the electrical current flow back towards its source.\n\nA local spot network of two to eight primary transformers can be connected to the same secondary bus to provide reliable power to a particular facility, like a large hospital or computer and major data processing center. St. Jude Children's Research Hospital in Memphis, Tennessee has eight primary transformers that are connected to the same secondary bus. The FedExForum (home of the NBA Grizzlies basketball team) in Memphis has a network of four primary transformers connected to the same secondary bus. In some arrangements with four transformers, any of the transformers can carry all of its connected loads. The Toronto Pearson International Airport is electrically fed by four grid feeders, each capable of carrying the entire 20+ MW load.\n\n"}
{"id": "1555955", "url": "https://en.wikipedia.org/wiki?curid=1555955", "title": "Stick (unit)", "text": "Stick (unit)\n\nThe stick may refer to several separate units, depending on the item being measured.\nIn typography, the stick, stickful, or was an inexact length based on the size of the various composing sticks used by newspaper editors to assemble pieces of moveable type. In English-language papers, it was roughly equal to 2 column inches or 100–150 words. In France, Spain, and Italy, sticks generally contained only between 1 and 4 lines of text each. A column was notionally equal to 10 sticks.\nIn American cooking, a is taken to be 4 ounces (about 113 g).\n\nIn American cooking, a stick of butter may also be understood as ½ cup or 8 tablespoons (about 125 mL).\n\n\n"}
{"id": "1868834", "url": "https://en.wikipedia.org/wiki?curid=1868834", "title": "Symmetrical components", "text": "Symmetrical components\n\nIn electrical engineering, the method of symmetrical components simplifies analysis of unbalanced three-phase power systems under both normal and abnormal conditions. The basic idea is that an asymmetrical set of \"N\" phasors can be expressed as a linear combination of \"N\" symmetrical sets of phasors by means of a complex linear transformation. \n\nIn the most common case of three-phase systems, the resulting \"symmetrical\" components are referred to as \"direct\" (or \"positive\"), \"inverse\" (or \"negative\") and \"zero\" (or \"homopolar\"). The analysis of power system is much simpler in the domain of symmetrical components, because the resulting equations are mutually linearly independent if the circuit itself is balanced.\n\nIn 1918 Charles Legeyt Fortescue presented a paper which demonstrated that any set of N unbalanced phasors (that is, any such \"polyphase\" signal) could be expressed as the sum of N symmetrical sets of balanced phasors, for values of N that are prime. Only a single frequency component is represented by the phasors. \n\nIn 1943 Edith Clarke published a textbook giving a method of use of symmetrical components for three-phase systems that greatly simplified calculations over the original Fortescue paper. In a three-phase system, one set of phasors has the same phase sequence as the system under study (positive sequence; say ABC), the second set has the reverse phase sequence (negative sequence; ACB), and in the third set the phasors A, B and C are in phase with each other (zero sequence, the common-mode signal). Essentially, this method converts three unbalanced phases into three independent sources, which makes asymmetric fault analysis more tractable.\n\nBy expanding a one-line diagram to show the positive sequence, negative sequence and zero sequence impedances of generators, transformers and other devices including overhead lines and cables, analysis of such unbalanced conditions as a single line to ground short-circuit fault is greatly simplified. The technique can also be extended to higher order phase systems.\n\nPhysically, in a three phase winding a positive sequence set of currents produces a normal rotating field, a negative sequence set produces a field with the opposite rotation, and the zero sequence set produces a field that oscillates but does not rotate between phase windings. Since these effects can be detected physically with sequence filters, the mathematical tool became the basis for the design of protective relays, which used negative-sequence voltages and currents as a reliable indicator of fault conditions. Such relays may be used to trip circuit breakers or take other steps to protect electrical systems.\n\nThe analytical technique was adopted and advanced by engineers at General Electric and Westinghouse and after World War II it became an accepted method for asymmetric fault analysis.\n\nAs shown in the figure to the above right, the three sets of symmetrical components (positive, negative, and zero sequence) add up to create the system of three unbalanced phases as pictured in the bottom of the diagram. The imbalance between phases arises because of the difference in magnitude and phase shift between the sets of vectors. Notice that the colors (red, blue, and yellow) of the separate sequence vectors correspond to three different phases (A, B, and C, for example). To arrive at the final plot, the sum of vectors of each phase is calculated. This resulting vector is the effective phasor representation of that particular phase. This process, repeated, produces the phasor for each of the three phases.\n\nSymmetrical components are most commonly used for analysis of three-phase electrical power systems. The voltage or current of a three-phase system at some point can be indicated by three phasors, called the three components of the voltage or the current. \n\nThis article discusses voltage, however, the same considerations also apply to current. In a perfectly balanced three-phase power system, the voltage phasor components have equal magnitudes but are 120 degrees apart. In an unbalanced system, the magnitudes and phases of the voltage phasor components are different. \n\nDecomposing the voltage phasor components into a set of symmetrical components helps analyze the system as well as visualize any imbalances. \nIf the three voltage components are expressed as phasors (which are complex numbers), a complex vector can be formed in which the three phase components are the components of the vector. A vector for three phase voltage components can be written as\n\nand decomposing the vector into three symmetrical components gives\nwhere the subscripts 0, 1, and 2 refer respectively to the zero, positive, and negative sequence components. The sequence components differ only by their phase angles, which are symmetrical and so are formula_3 radians or 120°. \n\nDefine a phasor rotation operator alpha, which rotates a phasor vector counterclockwise by 120 degrees:\nNote that α = 1 so that α = α.\n\nThe zero sequence components have equal magnitude and in phase with each other, therefore:\n\nand the other phase sequences have the same magnitude, but their phases differ by 120°:\n\nThus,\n\nwhere\n\nThe sequence components are derrived from the analysis equation\n\nwhere\n\nThe above two equations tell how to derive symmetrical components corresponding to an asymmetrical set of three phasors:\n\n\nVisually, if the original components are symmetrical, sequences 1 and 2 will each form a triangle, summing to zero, and sequence 0 components will sum to a straight line.\n\nThe phasors formula_11 form a closed triangle (e.g., outer voltages or line to line voltages). To find the synchronous and inverse components of the phases, take any side of the outer triangle and draw the two possible equilateral triangles sharing the selected side as base. These two equilateral triangles represent a synchronous and an inverse system. \n\nIf the phasors V were a perfectly synchronous system, the vertex of the outer triangle not on the base line would be at the same position as the corresponding vertex of the equilateral triangle representing the synchronous system. Any amount of inverse component would mean a deviation from this position. The deviation is exactly 3 times the inverse phase component. \n\nThe synchronous component is in the same manner 3 times the deviation from the \"inverse equilateral triangle\". The directions of these components are correct for the relevant phase. It seems counter intuitive that this works for all three phases regardless of the side chosen but that is the beauty of this illustration.\n\nThe illustration is from Napoleon's Theorem.\n\nIt can be seen that the transformation matrix above is a discrete Fourier transform, and as such, symmetrical components can be calculated for any poly-phase system. However, by Pontryagin duality, only certain groups have a unique inverse, which is necessary for use in fault analysis.\n\nHarmonics often occur in power systems as a consequence of non-linear loads. Each order of harmonics contributes to different sequence components. Harmonics of order formula_12 make no contribution. Harmonics of order formula_13 contribute to the zero sequence. Harmonics of order formula_14 contribute to the negative sequence. Harmonics of order formula_15 contribute to the positive sequence.\n\nNote that the rules above are only applicable if the phase values (or distortion) are exactly the same.\n\nThe zero sequence represents the component of the unbalanced phasors that is equal in magnitude and phase. Because they are in phase, zero sequence currents flowing through an n-phase network will sum to n times the magnitude of the individual zero sequence currents components. Under normal operating conditions this sum is small enough to be negligible. However, during large zero sequence events such as lightning strikes, this nonzero sum of currents can lead to a larger current flowing through the neutral conductor than the individual phase conductors. Because neutral conductors are typically not larger than individual phase conductors, and are often smaller than these conductors, a large zero sequence component can lead to overheating of neutral conductors and to fires.\n\nOne way to prevent large zero sequence currents is to use a delta connection, which appears as an open circuit to zero sequence currents. For this reason, most transmission, and much sub-transmission is implemented using delta. Much distribution is also implemented using delta, although \"old work\" distribution systems have occasionally been \"wyed-up\" (converted from delta to wye) so as to increase the line's capacity at a low converted cost, but at the expense of a higher central station protective relay cost.\n\n\n\n"}
{"id": "31016", "url": "https://en.wikipedia.org/wiki?curid=31016", "title": "Terrestrial Time", "text": "Terrestrial Time\n\nTerrestrial Time (TT) is a modern astronomical time standard defined by the International Astronomical Union, primarily for time-measurements of astronomical observations made from the surface of Earth.\nFor example, the Astronomical Almanac uses TT for its tables of positions (ephemerides) of the Sun, Moon and planets as seen from Earth. In this role, TT continues Terrestrial Dynamical Time (TDT or TD), which in turn succeeded ephemeris time (ET). TT shares the original purpose for which ET was designed, to be free of the irregularities in the rotation of Earth.\n\nThe unit of TT is the SI second, the definition of which is currently based on the caesium atomic clock, but TT is not itself defined by atomic clocks. It is a theoretical ideal, and real clocks can only approximate it.\n\nTT is distinct from the time scale often used as a basis for civil purposes, Coordinated Universal Time (UTC). TT indirectly underlies UTC, via International Atomic Time (TAI). Because of the historical difference between TAI and ET when TT was introduced, TT is approximately 32.184 s ahead of TAI.\n\nA definition of a terrestrial time standard was adopted by the International Astronomical Union (IAU) in 1976 at its XVI General Assembly, and later named \"Terrestrial Dynamical Time\" (TDT). It was the counterpart to Barycentric Dynamical Time (TDB), which was a time standard for Solar system ephemerides, to be based on a dynamical time scale. Both of these time standards turned out to be imperfectly defined. Doubts were also expressed about the meaning of 'dynamical' in the name TDT.\n\nIn 1991, in Recommendation IV of the XXI General Assembly, the IAU redefined TDT, also renaming it \"Terrestrial Time\". TT was formally defined in terms of Geocentric Coordinate Time (TCG), defined by the IAU on the same occasion. TT was defined to be a linear scaling of TCG, such that the unit of TT is the SI second on the geoid (Earth surface at mean sea level). This left the exact ratio between TT time and TCG time as something to be determined by experiment. Experimental determination of the gravitational potential at the geoid surface is a task in physical geodesy.\n\nIn 2000, the IAU very slightly altered the definition of TT by adopting an exact value for the ratio between TT and TCG time, as 1 − × 10. (As measured on the geoid surface, the rate of TCG is very slightly faster than that of TT, see below, Relativistic relationships of TT.)\n\nTT differs from Geocentric Coordinate Time (TCG) by a constant rate. Formally it is defined by the equation\n\nformula_1\n\nwhere TT and TCG are linear counts of SI seconds in Terrestrial Time and Geocentric Coordinate Time respectively, L is the constant difference in the rates of the two time scales, and E is a constant to resolve the epochs (see below). L is defined as exactly × 10. (In 1991 when TT was first defined, L was to be determined by experiment, and the best available estimate was × 10.)\n\nThe equation linking TT and TCG is more commonly seen in the form\n\nformula_2\n\nwhere JD is the TCG time expressed as a Julian date (JD). This is just a transformation of the raw count of seconds represented by the variable TCG, so this form of the equation is needlessly complex. The use of a Julian Date specifies the epoch fully. The above equation is often given with the Julian Date for the epoch, but that is inexact (though inappreciably so, because of the small size of the multiplier L). The value is exactly in accord with the definition.\n\nTime coordinates on the TT and TCG scales are conventionally specified using traditional means of specifying days, carried over from non-uniform time standards based on the rotation of Earth. Specifically, both Julian Dates and the Gregorian calendar are used. For continuity with their predecessor Ephemeris Time (ET), TT and TCG were set to match ET at around Julian Date (1977-01-01T00Z). More precisely, it was defined that TT instant 1977-01-01T00:00:32.184 exactly and TCG instant 1977-01-01T00:00:32.184 exactly correspond to the International Atomic Time (TAI) instant 1977-01-01T00:00:00.000 exactly. This is also the instant at which TAI introduced corrections for gravitational time dilation.\n\nTT and TCG expressed as Julian Dates can be related precisely and most simply by the equation\n\nwhere E is exactly.\n\nTT is a theoretical ideal, not dependent on a particular realization. For practical purposes, TT must be realized by actual clocks in the Earth system.\n\nThe main realization of TT is supplied by TAI. The TAI service, running since 1958, attempts to match the rate of proper time on the geoid, using an ensemble of atomic clocks spread over the surface and low orbital space of Earth. TAI is canonically defined retrospectively, in monthly bulletins, in relation to the readings that particular groups of atomic clocks showed at the time. Estimates of TAI are also provided in real time by the institutions that operate the participating clocks. Because of the historical difference between TAI and ET when TT was introduced, the TAI realization of TT is defined thus:\n\nBecause TAI is never revised once published, it is possible for errors in it to become known and remain uncorrected. It is thus possible to produce a better realization of TT based on reanalysis of historical TAI data. The BIPM has done this approximately annually since 1992. These realizations of TT are named in the form \"TT(BIPM08)\", with the digits indicating the year of publication. They are published in the form of table of differences from TT(TAI). The latest is TT(BIPM17).\n\nThe international communities of precision timekeeping, astronomy, and radio broadcasts have considered creating a new precision time scale based on observations of an ensemble of pulsars. This new pulsar time scale will serve as an independent means of computing TT, and it may eventually be useful to identify defects in TAI.\n\nSometimes times described in TT must be handled in situations where TT's detailed theoretical properties are not significant. Where millisecond accuracy is enough (or more than enough), TT can be summarized in the following ways:\n\n\nObservers in different locations, that are in relative motion or at different altitudes, can disagree about the rates of each other's clocks, owing to effects described by the theory of relativity. As a result, TT (even as a theoretical ideal) does not match the proper time of all observers.\n\nIn relativistic terms, TT is described as the proper time of a clock located on the geoid (essentially mean sea level).\nHowever,\nTT is now actually defined as a coordinate time scale.\nThe redefinition did not quantitatively change TT, but rather made the existing definition more precise. In effect it defined the geoid (mean sea level) in terms of a particular level of gravitational time dilation relative to a notional observer located at infinitely high altitude.\n\nThe present definition of TT is a linear scaling of Geocentric Coordinate Time (TCG), which is the proper time of a notional observer who is infinitely far away (so not affected by gravitational time dilation) and at rest relative to Earth. TCG is used so far mainly for theoretical purposes in astronomy. From the point of view of an observer on Earth's surface the second of TCG passes in slightly less than the observer's SI second. The comparison of the observer's clock against TT depends on the observer's altitude: they will match on the geoid, and clocks at higher altitude tick slightly faster.\n\n\n"}
{"id": "3774310", "url": "https://en.wikipedia.org/wiki?curid=3774310", "title": "The Zombie Survival Guide", "text": "The Zombie Survival Guide\n\nThe Zombie Survival Guide, written by American author Max Brooks and published in 2003, is a survival manual dealing with the fictional potentiality of a zombie attack. It contains detailed plans for the average citizen to survive zombie uprisings of varying intensity and reach, and describes \"cases\" of zombie outbreaks in history, including an interpretation of Roanoke Colony. \"The Zombie Survival Guide\" was also featured on \"The New York Times\" Best Seller's list. Brooks' inspiration for \"The Zombie Survival Guide\" was his childhood interest in zombies, sparked by the first zombie movie he ever saw, \"circa\" age 10: \"Revenge of the Zombies\" (1943).\n\nThe book is divided into six separate chapters, followed by a list of fictional attacks throughout history and an appendix. The first chapter, \"The Undead: Myths and Realities\", outlines Solanum, a fictional and incurable virus that creates a zombie, along with details on how it is spread (such as through an open wound, or contact with infected blood or saliva), and treatment of the infected (such as suicide or amputation of the injured limb, though the latter rarely works). The middle of this chapter explains the abilities and behavioral patterns of the undead, and the differences between \"voodoo\" zombies, movie zombies, and zombies created by Solanum.\n\nIn subsequent chapters, the book describes weapons and combat techniques, places of safety, and how to survive a zombie-infested world. In the section describing weapons, Brooks writes about the human body: \"If cared for and trained properly, is the greatest weapon on earth\". The guide concludes with a fictional list of documented zombie encounters throughout history. The oldest entry is 60,000 BC, in Katanda, Central Africa, although the author expresses doubt about its validity. Instead, he presents evidence from 3,000 B.C. in Ancient Egypt as the first verifiable instance of a zombie outbreak. The most recent entry is 2002, in Saint Thomas, U.S. Virgin Islands. Some of these encounters make reference to historical events, such as Roanoke Island.\n\nThis is a sample \"outbreak journal\": the author notes covered-up zombie outbreaks seen on the local news as well as the preparations he recommends in the event that the outbreak worsens. The following pages are blank entries, for the reader to use as a basis for his or her own notes on surviving zombies.\n\nRandom House released a deck of flash cards containing the same information as the book. It was released on July 22, 2008.\n\nRandom House published \"The Zombie Survival Guide: Recorded Attacks\", a tie-in comic written by Brooks. This book illustrates some of the recorded attacks, but not all. It was released on October 6, 2008. Brazilian artist Ibraim Roberson illustrated the book.\n\n\"The Zombie Survival Guide\", \"The Zombie Survival Guide: Recorded Attacks\", and \"World War Z\" have been confirmed to be produced as live-action films. Brad Pitt, who stars in \"World War Z\", also confirmed that the producing studio, Paramount, has also been given rights to \"The Zombie Survival Guide\" and \"The Zombie Survival Guide: Recorded Attacks\" and are planning on adapting both of them as movies after \"World War Z\" is released. \"World War Z\" was released in theatres on June 21, 2013, \"The Zombie Survival Guide\" and \"The Zombie Survival Guide: Recorded Attacks\" currently have no release dates set.\n\n\n"}
{"id": "11083753", "url": "https://en.wikipedia.org/wiki?curid=11083753", "title": "Turbo-compound engine", "text": "Turbo-compound engine\n\nA turbo-compound engine is a reciprocating engine that employs a turbine to recover energy from the exhaust gases. Instead of using that energy to drive a turbocharger as found in many high-power aircraft engines, the energy is instead sent to the output shaft to increase the total power delivered by the engine. The turbine is usually mechanically connected to the crankshaft, as on the Wright R-3350 Duplex-Cyclone, but electric and hydraulic power recovery systems have been investigated as well. \n\nAs this recovery process does not increase fuel consumption, it has the effect of reducing the specific fuel consumption, the ratio of fuel use to power. Turbo-compounding was used for commercial airliners and similar long-range, long-endurance roles before the introduction of high-bypass turbofan engines replaced them in this role. Examples using the Duplex-Cyclone include the Douglas DC-7B and Lockheed L-1049 Super Constellation, while other designs did not see production use.\n\nMost piston engines have a hot exhaust that still contains considerable undeveloped energy that could be used for propulsion if extracted. A turbine is often used to extract energy from such a stream of gases. A conventional gas turbine is fed high-pressure, high-velocity air, extracts energy from it, and leaves as a lower-pressure, slower-moving stream. This action has the side-effect of increasing the upstream pressure, which makes it undesirable for use with a piston engine as it increases the back-pressure in the engine, which decreases scavenging of the exhaust gas from the cylinders and thereby lowers the efficiency of the piston portion of a compound engine.\n\nThrough the late 1930s and early 1940s one solution to this problem was the introduction of \"jet stack\" exhaust manifolds. These were simply short sections of metal pipe attached to the exhaust ports, shaped so that they would interact with the airstream to produce a jet of air that produced forward thrust. Another World War II introduction was the use of the Meredith effect to recover heat from the radiator system to provide additional thrust.\n\nBy the late-war era, turbine development had improved dramatically and led to a new turbine design known as the \"blowdown turbine\" or \"power-recovery turbine\". This design extracts energy from the momentum of the moving air, but does not appreciably increase back-pressure. This means it does not have the undesirable effects of conventional designs when connected to the exhaust of a piston engine, and a number of manufacturers began studying the design.\n\nThe first aircraft engine to be tested with a power-recovery turbine was the Rolls-Royce Crecy. This was used primarily to drive a geared centrifugal supercharger, although it was also coupled to the crankshaft and gave an extra 15 to 35 percent fuel economy.\n\nBlowdown turbines became relatively common features in the late- and post-war era, especially for engines designed for long overwater flights. Turbo-compounding was used on several airplane engines after World War II, including the Napier Nomad and the Wright R-3350 being examples. The exhaust restriction imparted by the three blowdown turbines used on the Wright R-3350 is equal to a well-designed jet stack system used on a conventional radial engine, while recovering about at METO (maximum continuous except for take-off) power. In the case of the R-3350, maintenance crews sometimes nicknamed the turbine the \"parts recovery turbine\" due to its negative effect on engine reliability. Turbo-compound versions of the Napier Deltic, Rolls-Royce Crecy, Rolls-Royce Griffon, and Allison V-1710 were constructed but none was developed beyond the prototype stage. It was realized in many cases the power produced by the simple turbine was approaching that of the enormously complex and maintenance-intensive piston engine to which it was attached. As a result, turbo-compound aero engines were soon supplanted by turboprop and turbojet engines.\n\nSome modern heavy truck diesel manufacturers have incorporated turbo-compounding into their designs. Examples include the Detroit Diesel DD15 and Scania in production from 1991.\n\nStarting with the 2014 season, Formula 1 switched to a new 1.6 liter turbocharged V6 formula that uses turbo-compounding. The engines use a single turbocharger that is connected to an electric motor/generator called the MGU-H. The MGU-H uses a turbine to drive a generator, converting waste heat from the exhaust into electrical energy that is either stored in a battery or sent directly to an electric motor in the car's powertrain.\n\n\n\n"}
{"id": "23214758", "url": "https://en.wikipedia.org/wiki?curid=23214758", "title": "University of Petroleum and Energy Studies", "text": "University of Petroleum and Energy Studies\n\nThe University of Petroleum & Energy Studies (UPES) is an Indian University located in Dehradun, Uttarakhand known traditionally for its education and research in fields of energy & power. The University has been expanding its offerings in the sectors like Information Technology, Transportation, Public Policy, Design, Architecture, Business and Legal Studies.\n\nUPES Dehradun, Established in 2003, UPES is a leading multi-disciplinary university with campuses in the North Indian state of Uttarakhand. UPES gives its students a competitive advantage through strategic industry partnerships that transforms them into employable, agile, future-ready professionals. Given its industry-oriented programs and emphasis on holistic development, UPES graduates are a preferred choice for recruiters employers, ensuring a track record of 90% + placements over the last few years. UPES is committed to develop future leaders for diverse high-growth sectors in India and globally.\n\nThere is no campus named upes.\n\nThe campus is located in Dehradun\n\nIt is well connected and in proximity to popular Himalayan tourist destinations such as Mussoorie, Nainital, Auli and the Hindu holy cities of Haridwar and Rishikesh along with the Himalayan pilgrimage circuit of Char Dham.\n\nCultural activities like dramatics, Speaking, Literary Arts, Music and Fine Arts also mark an important feature in the life of a student of UPES. Student activities have been distributed into three broad categories, technical, cultural and sports.\n\nThe annual youth festival Uurja is held in the month of October. The most attractive feature of this 2-day event are the competitions, games and musical concerts.\n\nIgnite is the Annual Techno-Legal-Management festival, IGNITE aims at building a platform to bring students of all domains under a single umbrella to work together, learn together and grow together, IGNITE is first Techno-Legal-Management festival in India.\n\nJigyasa is the Annual fest organised by UPES FIPI Student Chapter(formally UPES Petrotech SC). It is the oldest FIPI student chapter in India operating since 2007 in UPES. It consists of various events covering paper and poster presentation, guest lectures from Industry experts. It provides platform to showcase one's knowledge and potential in Oil and Gas Industry.\n\nUPES SPE Fest is held in month of February every year since 2010 and is India's largest Student Organized-Petroleum Engineering Fest under Society of Petroleum Engineers. It comprises various events such as competitions, exhibits and talks from guest speakers which aims to provide a common platform for mutual sharing of knowledge, experience and to strengthen the bonds between the students and Oil and Gas Industry.\n\nGEMS team of UPES organized an instrumental competition- ‘GEMS INSTRU-BLITZ’ for the students on campus. Students enthralled the audience by playing wonderful tunes on Guitar (Acoustic & electric), Tabla and Keyboard. The event was sponsored by Resizone residency, USP and Mark industries. Mr. Vivek Aggarwal from Resizone group and Mr. Rahul Bhatnagar from USP were the guests of honor. The event was judged by Mr. Gaurav Dimang, a well-known vocalist from Dehradun and Ms. Namita Molasi, Head of the music Dept. at Raja Ram Mohan Roy Academy.\n\"Kalakriti\"- Your strokes, their smiles, was the event organized on Campus. The event is organized by UPES GOOGLE HUB. It focused on a creative way of spreading awareness about Google and its products with the fun of chart painting competition. This event had a massive participation of 75 teams from the various courses of College of engineering Studies, College of management Studies and College of Legal Studies.\n\nUPES has 5 schools and a Center in the campus. The university offers over 100 undergraduate and postgraduate programs.\nUPES is the one of the first university of India to provide UG, PG and PhD programs in the core sectors of petroleum and energy.\n\nSoE offers B.Tech., M.Tech. and Ph.D. programs across a wide spectrum of specialization areas. \nThese include Petroleum Engineering, Chemical Engineering, Petroleum Exploration, Energy Systems, Geoinformatics Engineering, Geo-Science Engineering, Automotive Design Engineering, Mining engineering, Instrumentation & Process Control, Aerospace Engineering, Disaster Management, Mechatronics Engineering, Robotics, Nanotechnology, Fire Safety Engineering, Electrical Engineering, Civil Engineering, Industrial Engineering, and several others.\n\nThe SoE has the following Departments:\n\n\nSoB is situated at Kandoli Campus of UPES which offers BBA, MBA & Ph.D. programs in varied domains such as Oil & Gas, Aerospace, Power, Infrastructure, Transportation & Logistics, Aviation, Information Technology, Information Systems and International Business.It allows the students to understand sector specific business and economic concepts; empowering them with management and leadership skills.\n\nSoB has following Departments.\n\n\nIt offers basic and specialized law courses in Corporate Law, Energy Specialization, Taxation Laws and Technical courses like Intellectual Property Rights and Cyber Law and integrated and specialized programs which are the first of their kind in the South Asian region. These include BA LLB with specialization in Energy Laws, BBA LLB (Hons) Corporate Law and B.Com + LLB with specialization in Taxation Laws and B.Tech Energy Technology + LLB with specialization in Intellectual Property Rights and B. Tech Computer Science & Engineering with specialization in Cyber Laws.\n\nSchool of Design provides creative design courses that help students understand and master visual design aspects of transportation, industrial and product development and build a successful career in the industry of various choices.\n\nThe School of Computer Science offers B.Tech. Computer Science & Engineering in association with IBM & XEBIA with specialization in:\nThe Centre for Continuing Education (CCE) and Centre for Aviation Studies (CAS) offer Executive programs through distance learning mode for the work professionals.\n\n\n\n\n\nhttp://upesfipi.org/\n"}
{"id": "2151656", "url": "https://en.wikipedia.org/wiki?curid=2151656", "title": "Wave tank", "text": "Wave tank\n\nA wave tank is a laboratory setup for observing the behavior of surface waves. The typical wave tank is a box filled with liquid, usually water, leaving open or air-filled space on top. At one end of the tank an actuator generates waves; the other end usually has a wave-absorbing surface. A similar device is the ripple tank, which is flat and shallow and used for observing patterns of surface waves from above.\n\nA wave basin is a wave tank which has a width and length of comparable magnitude, often used for testing ships, offshore structures and three-dimensional models of harbors (and their breakwaters).\n\nA wave flume (or wave channel) is a special sort of wave tank: the width of the flume is much less than its length. The generated waves are therefore – more or less – two-dimensional in a vertical plane (2DV), meaning that the orbital flow velocity component in the direction perpendicular to the flume side wall is much smaller than the other two components of the three-dimensional velocity vector. This makes a wave flume a well-suited facility to study near-2DV structures, like cross-sections of a breakwater. Also (3D) constructions providing little blockage to the flow may be tested, e.g. measuring wave forces on vertical cylinders with a diameter much less than the flume width.\n\nWave flumes may be used to study the effects of water waves on coastal structures, offshore structures, sediment transport and other transport phenomena.\n\nThe waves are most often generated with a mechanical wavemaker, although there are also wind–wave flumes with (additional) wave generation by an air flow over the water – with the flume closed above by a roof above the free surface. The wavemaker frequently consists of a translating or rotating rigid wave board. Modern wavemakers are computer controlled, and can generate besides periodic waves also random waves, solitary waves, wave groups or even tsunami-like wave motion. The wavemaker is at one end of the wave flume, and at the other end is the construction being tested, or a wave absorber (a beach or special wave absorbing constructions).\n\nOften, the side walls contain glass windows, or are completely made of glass, allowing for a clear visual observation of the experiment, and the easy deployment of optical instruments (e.g. by Laser Doppler velocimetry or particle image velocimetry).\n\nIn 2014, the first , circular, combined current and wave test basin, FloWaveTT was commissioned in The University of Edinburgh. This allows for \"true\" 360° waves to be generated to simulate rough storm conditions as well as scientific controlled waves in the same facility. It was designed by Edinburgh Designs.\n\n"}
