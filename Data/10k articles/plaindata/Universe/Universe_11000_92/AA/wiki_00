{"id": "46977372", "url": "https://en.wikipedia.org/wiki?curid=46977372", "title": "Adarme", "text": "Adarme\n\nThe adarme is an antiquated Spanish unit of mass, equal to three \"tomines\", equivalent to . The term derives from the Arabic درهم, parallel with \"drachem\" and the Greek δραχμἠ and persists in Spanish as an idiom for something insignificant or which exists in small quantity.\n"}
{"id": "1567306", "url": "https://en.wikipedia.org/wiki?curid=1567306", "title": "Backsaw", "text": "Backsaw\n\nA backsaw is any hand saw which has a stiffening rib on the edge opposite the cutting edge, enabling better control and more precise cutting than with other types of saws. Backsaws are normally used in woodworking for precise work, such as cutting dovetails, mitres, or tenons in cabinetry and joinery. Because of the stiffening rib, backsaws are limited in the depth to which they can cut. Backsaws usually have relatively closely spaced teeth, often with little or no set.\n\nBacksaws include the tenon saw, the dovetail saw, and the (United Kingdom) sash saw. Tenon and dovetail saws usually have a pistol grip style handle which may be open or closed at the bottom. \n\nDifferent types of backsaw include:\n\nThe work of a backsaw requires a thin, stiff blade. These two immediately incompatible requirements are resolved by swaging (and perhaps, spot welding) a stiffening cap over the length of the top edge of the blade. This thickening of the blade is the back and is generally made from some kind of metal, usually brass or steel. The stiffening back is carried through into the handle so that the blade does not bend between the two. The thickening of the back limits the depth to which the saw can cut but this is generally not a limitation in their typical uses.\n\n\n\n"}
{"id": "1872107", "url": "https://en.wikipedia.org/wiki?curid=1872107", "title": "Blind hole", "text": "Blind hole\n\nA blind hole refers to a hole that is reamed, drilled, or milled to a specified depth without breaking through to the other side of the workpiece. The etymology is that it is not possible to see through a \"blind\" hole. Other types of holes also include through holes, and clearance holes. In this instance blind may also refer to any feature that is taken to a specific depth, more specifically referring to internally threaded hole (tapped holes). Not considering the drill point, the depth of the blind hole, conventionally, may be slightly deeper than that of the threaded depth.\n\nThere are three accepted methods of threading blind holes:\n\nAt least two U.S. tool manufacturers have manufactured tools for thread milling in blind holes: Ingersoll Cutting Tools of Rockford, Illinois, and Tooling Systems of Houston, Texas, who introduced the ThredMil in 1977, a device that milled large internal threads in the blind holes of oil well blowout preventers. Today many CNC milling machines can run such a thread milling cycle (see a video of such a cut in the \"External links\" section).\n\n"}
{"id": "720221", "url": "https://en.wikipedia.org/wiki?curid=720221", "title": "Buick Regal", "text": "Buick Regal\n\nThe Buick Regal is an upscale mid-sized automobile that was first introduced by Buick for the 1973 model year. North American production ended in 2004 and began again in 2011. For the 2011 model year, Buick re-introduced the Regal to the North American market, positioned as an upscale sports sedan. Production and sales in China have continued since 1999.\n\nFor certain model years between 1973 and 2004, the Regal shared bodies and powertrains with the similar Buick Century. The two most recent generations (2011–present) are North American market versions of the Opel Insignia, known as the Vauxhall Insignia in the UK.\n\nBuick had been the first GM division to bring a personal luxury car to market with its full-size 1963 Riviera, but was otherwise slow to react to the developing lower-priced mid-size personal luxury market, which Pontiac created with the 1969 Grand Prix and Chevrolet with the Monte Carlo the following year, 1970. At the same time, Oldsmobile added a formal notchback coupé to its intermediate line, the Cutlass Supreme, in 1970 and that model soon became Olds' best selling intermediate. Buick did not get its own personal luxury coupe until the GM intermediates were redesigned in 1973, the so-called \"Colonnade\" cars that eliminated hardtop models completely. In a curious name swap, the Skylark name was dropped from Buick's intermediate line and instead the Century nameplate, last used in the 1950s, was revived for them.\n\nA highly trimmed, two-door coupe, the first Regal shared its front and rear styling with its Century parent with distinctions amounting to differing grilles and taillight lenses. The Regal shared the same \"Colonnade\" pillared hardtop roofline (a hardtop with B-pillars (center pillars) but frameless doors unlike a sedan body) and greenhouse (window area) with the Grand Prix, Monte Carlo, and Cutlass Supreme as well as the lower-priced Buick Century Luxus coupe. Like its corporate cousins, the Regal (and Luxus) featured the newly fashionable opera windows, which were small fixed rear-side windows surrounded by sheetmetal, instead of the traditional roll-down windows.\n\nFor the first model year in 1973, the Regal nameplate was only used for Buick's version of the GM intermediate personal luxury coupe, but the following year gained a sedan companion (there was no Regal station wagon).\n\nRegal interiors were generally more luxurious than lesser Century models with woodgrain trim on dashboard and door panels, along with door-pull straps and bench seats with center armrests with cloth, velour, or vinyl upholstery. Optionally available throughout the run was a 60/40 split bench seat with armrest. For 1976 and 1977, the Regal coupe was available with the S/R option that included reclining bucket seats with corduroy upholstery.\n\nThe model lasted five years with minimal changes, although there was a fairly substantial facelift in coupes for 1976 (sedans retained their original 1973 sheetmetal through 1977), which incorporated the recently-legalized square headlights (horizontally mounted on coupes, and vertically on sedans—much like the mid-1960s Pontiacs). The Regal coupe sold reasonably well, although it lagged behind the Monte Carlo and Cutlass Supreme which had become the best-selling cars in America by 1976.\n\nThe Regal was most commonly powered by Buick's 350 in³ V8, which was standard equipment on all models for 1973 and 1974, and optional on coupes but standard on sedans from 1975 to 1977, and the larger 455 cu in V8 was optional for 1973 and 1974 models only. Starting in 1975, Regal coupes came standard with Buick's resurrected V6 engine previously offered on the 1964–1967 Skylark; the engine's tooling had been sold to Kaiser Motors for use in Jeep models (Kaiser was purchased by American Motors in 1970 and Jeep became an AMC division) and sold back to GM by AMC in 1974. For 1975 and 1976, the Century and Regal were the only mid-sized cars in America to offer V6 engines. The bolt pattern for this vehicle is 5×.\n\nThe Century designation was quietly dropped from the Regal for 1975.\n\nA downsized Regal appeared for the 1978 model year with Buick's new V6 engine as standard equipment and a revised version of the venerable V6 as an option (which became standard for 1980). Initially, a 3-speed manual transmission was standard but this was later replaced by an automatic. This model lasted 10 years. The base model was equipped with softer-riding luxury suspension, and did not offer a manual transmission in later years.\n\nThe 1978 Regal could be equipped with a Turbocharged V6 engine with automatic transmission and the sedan model was dropped, once again leaving Regals exclusively as coupés with the Century nameplate applied to bread-and-butter sedans and station wagons. Non-turbo versions were offered with either a 2-bbl or a 4-bbl carburetor. The Buick LeSabre was also available with the turbocharged engine. The only other turbocharged cars available in the U.S. market in 1978 were imports from Mercedes-Benz, Porsche and Saab. The Turbo Regal also included a firm handling suspension with larger tires and sport wheels.\n\nA major facelift for 1981 gave the Regal a much more aerodynamic profile, helping make it possible for the car to compete on the NASCAR racing circuit. The sloping hood and nose of the car made it the favorite of several NASCAR teams. Richard Petty drove one to victory in the 1981 Daytona 500, and the car won a majority of the 1981 and 1982 seasons races and won the NASCAR manufacturers title in 1981 and 1982. V8s for street use were still available, but had shrunk to (1980 and 1981 only, Pontiac built), and the V6 was rapidly gaining popularity. In January 1982, a new Century appeared on the front-wheel drive A-body, but the former rear-wheel drive Century sedan and wagon were not discontinued. These models were simply rebadged as Regals, and for the first time the name appeared on a full model lineup. The wagon was discontinued after 1983, and the sedan dropped from the lineup the next year. From 1986 to 1987, the V8 was available as an option. The 2-bbl V6 was standard. The 200-4R overdrive transmission was an option with either engine.\n\nFor 1980, the Regal was offered in a special Somerset Limited Edition trim which featured unique tan and dark blue designer exterior paint, wire wheel covers, sport mirrors, and chrome Somerset badging. The interior had tan and blue plush velour upholstery, brushed chrome trim, and additional Somerset badging.\n\nA Somerset Limited Edition model was also offered on the restyled 1981 Regal. It had unique dark sandstone and camel exterior paint, sport mirrors, and turbine wheels. The interior plush velour upholstery was camel with dark brown piping.\n\nIn February 1982, the Regal Grand National debuted, which was named for the NASCAR Winston Cup Grand National Series (the \"Grand National\" term was part of the Cup series nomenclature until 1986). Buick had won the Manufacturers Cup in 1981 and 1982, and wanted to capitalize on its success: \"What wins on Sunday, sells on Monday\". These 1982 cars were not painted black, which may confuse those not familiar with them. All started out as charcoal gray Regals that were shipped off to a subcontractor for finishing.\n\nOriginally intended for a run of 100 units, Cars and Concepts of Brighton, Michigan, retrofitted 215 Regals with the GN package. Most obvious was the light silver gray firemist paint added to each side. Red pinstripes and billboard shadow lettering proclaiming \"Buick\" were applied. The wheel opening moldings and rocker panel moldings were blacked out using black vinyl tape. Finally, a front air dam and rear spoiler were installed. On the inside, special \"Lear-Siegler\" seats were installed. These seats are fully adjustable and were covered with silver brandon cloth with black vinyl inserts. The front seat had Buick's \"6\" emblem embroidered onto them. Also, a special clock delete plate was added to the instrument panel which contained the yellow and orange \"6\" logo and the words \"Grand National Buick Motor Division\".)\n\nThe 1982 GN came with a naturally aspirated V6 engine with at 4,000 rpm and of torque at 2,000 rpm. Of the 215 Regal Grand Nationals produced in 1982, at least 35 were based on the Buick Regal Sport coupe package with the turbocharged V6 engine with at 4,000 rpm and of torque at 2,600 rpm. There were only 2,022 Sport coupes produced in 1982, and the number of cars with both the GN and Sport coupe packages is estimated to be fewer than 50.\n\nFor 1983, there was no Grand National. The Sport coupe model was renamed the T-Type; 3,732 were produced ( at 1,600 rpm and of torque at 2,400 rpm). The T-Type had been used on other Buicks, starting with the Riviera in 1981 (in 1979 and 1980, it was the S Type). The 1983 Regal T-Type featured tube headers, Hydro-Boost II brakes, 200-4R 4-speed overdrive trans and 3.42 rear axle (7.5\").\n\nFor 1984, the Grand National returned, now in all black paint. The turbocharged became standard and was refined with sequential fuel injection, distributor-less computer controlled ignition, and boasted at 4,400 rpm and of torque at 2,400 rpm. Only 5,204 turbo Regals were produced that year, only 2,000 of which were Grand Nationals. Because this was the first year production of the computer controlled sequential fuel injection and distributor-less ignition, this is often considered the year and model that started the development of the legendary intercooled Grand Nationals. The performance of this package was well ahead of its time and the \"little V6\" easily kept up with the bigger V8s. performance was listed at 15.9 seconds at stock boost levels of , while for the same year, the Chevrolet Camaro was listed at 17.0 and the Chevrolet Corvette at 15.2 seconds. Soon, performance enthusiasts determined the modifications that worked and the Grand Nationals easily broke into the 13-second territory. All Grand Nationals had the Lear Siegler-made cloth/leather interior which was only available for this year. An estimated 200 of the 1984 Grand Nationals were produced with the T-Top option which makes these the rarest of the Grand Nationals.\n\nFor 1986, a modified engine design with air-air intercooling boosted the performance even further to a specified at 4,000 rpm and of torque at 2,400 rpm. The Grand Nationals (quantity 5,512) and T-Types (quantity 2,384) were both produced in 1986. For 1987, performance reached and of torque. Buick dropped the T-Type package for Regal for 1987 models and opted for a \"T\" sport package instead. There were only 7,896 turbo Regals produced in 1986. In 1987, when turbo Regals reached their peak in popularity, a total of 27,590 turbo Regals were produced through December, with those models produced between September and December of that year window stickered as \"1987½ Buick Grand National\" vehicles.\n\nFor 1987, a lightweight WE4 (turbo T) option was offered. Only 1,547 of this variant were produced. The differences between a WE4 and the Grand National were the interior trim package, wheels, exterior badging, aluminum bumper supports, and aluminum rear drum brakes as opposed to the Grand National's cast iron, making the WE4 a lighter and faster car. The rear spoiler was only available as a dealer installed option. Nineteen eighty-seven was the only year that the LC2 turbo option was available on any Regal, making it possible to even see a Limited with a vinyl landau roof and a power bulge turbo hood. Turbo Regal Limiteds were one of the rarest models of turbo Regals produced second only to the GNX at 1,035 turbo Limiteds. Turbo Regal Limiteds could be ordered with many options with most having chrome external trim but for $35 could have been built with the full black-out trim option making them extremely rare. Limiteds were treated to a very luxurious interior with plush carpeting and optional bench pillow seats and a column shift. The 1987 model would be the end of the manufacture of the RWD \"G-Body\" Regal, but GM had to extend the build of the Grand National to meet customer demand into December.\nFor the final year, 1987, Buick introduced the limited production GNX, for \"Grand National Experimental\", at $29,900. Made in partnership with McLaren Performance Technologies/ASC, Buick produced only 547 GNs with the interior trim package, that were then sent off to McLaren and upgraded into the Buick GNX. Buick underrated the GNX at @ 4,400 rpm and a very substantial @ 3,000 rpm of torque, although actual output is and . This was created to be the \"Grand National to end all Grand Nationals.\" Changes made included a special Garrett AiResearch T-3 turbocharger with a ceramic-impeller blowing through a more efficient and significantly larger capacity intercooler with a \"Cermatel (ceramic-aluminum) coated\" pipe connecting the intercooler to the engine. A GNX specific E-EPROM, low-restriction exhaust with dual mufflers, reprogrammed turbo Hydramatic 200-4R transmission with a custom torque converter and transmission cooler, and unique differential cover/Panhard bar included more of the performance modifications. Exterior styling changes include vents located on each front fender, 16 inch black mesh style wheels with VR-speed rated tires, and deletion of the hood and fender emblems. The interior changes of the GNX included a serial number on the dash plaque and a revised instrument cluster providing Stewart-Warner analog gauges, including an analog turbo boost gauge. Performance was measured faster than the Ferrari F40 and the Porsche 930 with a time of 12.7 seconds at (0.3 and 0.8 seconds quicker, 2.9 and faster) and a 0- time of 4.6 seconds (0.4 and 0.3 seconds quicker, respectively).\nGNX #001 is the 1986 prototype currently owned by Buick and sometimes makes appearances at car shows around the US. The GNX used a unique torque arm that was mounted to a special, GNX only, rear differential cover, for increased traction. The torque arm rear suspension alters the suspension geometry, making the body lift while planting the rear tires down, resulting in increased traction.\n\nThe stealthy appearance of the all-black GNX and Grand National (and the resemblance of its grill to his helmet's mouthpiece), coupled with the fact that the Grand National was initially released during the popularity of \"Star Wars\" movies, earned it the title \"Darth Vader's Car\". Car and Driver covered the GNX model's introduction with the headline \"Lord Vader, your car is ready.\" Due to the six cylinder engine, the Buick make, and the black paint Grand Nationals were sometimes referred to as the \"Dark Side\". The \"Dark Side\" contrasted with the more common V8 Mustangs and Camaros that were popular at the time.\n\nA new Regal appeared in October 1987 on the GM W platform as the first vehicle on that platform; this generation ran with few changes for nine years. Though the new Regal returned to Buick's original concept in being offered only as a coupe and in being aimed once again squarely at the personal luxury buyer, it departed from tradition in being the first front-wheel drive model, and in having no serious performance option or edition. Neither a V8 engine nor a turbocharged V6 was offered; the only engine available for 1988 was the Chevrolet 2.8 L V6, producing . From 1990, owing to the declining personal luxury car market, the Regal was again offered as a four-door sedan (as were the Cutlass Supreme and Grand Prix the same year, the latter offered as a sedan for the first time).\n\nThe Regal was initially offered in base \"Custom\" and upscale \"Limited\" trim lines. For 1989, the \"Gran Sport\" trim line was added, featuring aluminum wheels, body side cladding and a console mounted shifter attached to the 4-speed automatic. For 1990, the Regal gained the option of the Buick 3.8 L V6. The 3800 V6 was unique to the Regal, differentiating it from the mechanically similar Chevrolet Lumina, Oldsmobile Cutlass Supreme and Pontiac Grand Prix. Anti-lock brakes were made standard on all but the base \"Custom\" cars from 1992, and the grille was redesigned again for 1993. Along with the new look came an electronically controlled automatic transmission and LeSabre-like rear lights and bumper.\n\nFor 1993, a driver's side airbag was added, along with standard ABS on all models, standard power windows, and more in the base engine due to a revised intake manifold and cylinder head. The \"Limited\" coupe was deleted; only the \"Custom\" and \"Gran Sport (GS)\" coupes remained. Dual airbags were new for 1995 along with a new interior. For 1996, the larger V6 engine became the \"3800 Series II\" and gained . \n\nFor the 1997 model year, the Century and Regal once again rode upon the same platform; the revised W platform that was shared with the Oldsmobile Intrigue, the Pontiac Grand Prix, the Chevrolet Lumina and Chevrolet Monte Carlo. The Regal coupe was discontinued. Differences between the Regal and Century were mostly cosmetic. As the upmarket version, the Regal offered larger engines and fancier trim, and once again boasted a newer version of the 3.8 L V6. While the Century was mainly a reliable, economy-minded car based upon the W-body, the Regal was fitted with many amenities, including heated leather seats (optional on the Century), a Monsoon 8-speaker surround sound system, dual climate control, and expansive interior space. Few changes occurred during this version's seven-year run. It offered 5-passenger seating on all trim levels like the Pontiac Grand Prix and Oldsmobile Intrigue (formerly Cutlass Supreme), unlike their predecessors that had optional 6-passenger seating and the Buick Century (formerly built on the A platform) which had standard 6-passenger seating.\n\nThis period held the fastest Buick since the days of the 1991-1996 Roadmaster, the Buick Regal GS. This car was now supercharged instead of turbocharged, and produced and of torque. When introduced in January 1997, Buick advertised the Regal as the \"car for the supercharged family\". Buick also released two other model types, the LSE and the GSE. The LSE stayed with the engine with upgrades and the GSE stayed with the supercharged engine with upgrades. Also, in 2000 Buick came out with a concept GSX that had an intercooled 3.8 L, but was supercharged rather than turbocharged. It had \n\nThe North American Regal was replaced in 2005 by the Buick LaCrosse, also built on the W platform. The final 2004 Buick Regal rolled off the assembly line on June 1, 2004.\n\n\nThe Regal LS from the factory had a 1/4 mile (~400 m) elapsed time (ET) of 15.8 seconds and could do 0-60 mph in under 8 seconds. The supercharged Regal GS had a 1/4 mile ET of 14.9 seconds, and acceleration to took 6.7 seconds. The Regal GS, equipped with the supercharged 3.8 liter V6 engine (L67) produced & of torque. The Regal GS's PCM has programming that activates torque management to reduce wheel spin at launch. The Regal LS was EPA rated at 19/30 MPG city/freeway while the supercharged Regal GS was EPA rated at 18/27 MPG city/freeway.\n\nSLP Performance offered dealer-installed options and dealer supplied accessories for both LS and GS models.\n\nThe three power train packages, referred to as stages, added between 15-30 HP. Performance tires, handling and braking, and lowering options were offered along with interior and exterior cosmetic options.\n\nFrom 2001 to 2004 Buick offered a Joseph Abboud appearance package on both the GS and LS models. This package included either a solid taupe or two-tone taupe/chestnut leather seats, two-tone leather wrapped steering wheel, leather shifter handle and boot, 16\" aluminum wheels; and Joseph Abboud signature emblems on the front doors, floor mats, front seat head rests, and taillights.\n\nGeneral Motors and Shanghai Automotive Industry Corporation (SAIC) established a joint venture in 1997 called Shanghai GM, and had begun assembling the Buick Regal in Shanghai, China in April 1999.\n\nThe Regal has sold well in the Chinese market as a large, relatively luxurious model, despite its high price, costing more than the North American version. The Chinese market Regal has slightly different frontal sheetmetal compared to the North American version and different engines including the 2.0 L \"L34\", the 2.5 L \"LB8\" V6 and the 3.0 L \"LW9\" V6. Gearbox choices were a 4 speed automatic for V6 models while four cylinder variants are paired with a 5 speed manual gearbox. V6 models had a set of black dashboard gauges while the four cylinder models had white dashboard gauges. These models also had slightly different names: the entry-level model was the \"New Century\", with more upscale models carrying the \"GL\" and \"GLX\" names. Later, \"G\" and \"GS\" models were added. Production for this generation ended in November 2008 in China being replaced the Opel Insignia based Regal.\n\nThe fifth-generation Buick Regal is a four-door, five-passenger, midsized sedan with a front engine, front-wheel drive layout. GM says that the Regal is helping General Motors attract new, younger customers to the Buick brand. According to GM published information, more than 41% of Regal buyers in the US are coming from non-General Motors brands, and more than 60% of CXL Turbo buyers are under the age of 55.\n\nThe fifth generation Buick Regal rides on GM's Epsilon II platform and is mostly identical to the Opel Insignia. It first went into production in Rüsselsheim, Germany, in 2008. The Shanghai GM twin of the Insignia was introduced in China as Buick Regal in December, 2008 for the 2009 model year.\n\nGM originally planned to sell a modified version of the Opel Insignia in North America as the second generation Saturn Aura, but changed strategy after deciding to discontinue the Saturn brand.\n\nThe Regal utilizes unibody construction with galvanized steel front fenders, hood, roof and door panels and thermoplastic polyolefin (TPO) bumper covers.\n\nThe North American Regal weighs about 13 pounds more than an equivalent Opel Insignia due to increased structural support in the B-pillar necessary to meet U.S. rollover standards.\n\nGM revealed the fifth generation Regal to North American dealers on October 14, 2009, and introduced the Regal to the public in November, 2009 at the LA Auto Show. Sales of the Regal began in February 2010. In North America, the Regal is positioned below the larger, more expensive LaCrosse and above the Verano compact sedan which debuted late in calendar year 2011.\n\nProduction of the Shanghai GM variant of the Regal began in November 2008 and ended in July 2017.\n\nNorth American production at General Motors of Canada's Oshawa, Ontario, assembly plant was confirmed on November 25, 2009, and production began at Oshawa Car Assembly in February 2011. The initial production for the North American market was done together with its Opel twin the Opel Insignia in the Adam Opel AG's Rüsselsheim, Germany assembly plant from 1 March 2010 to 25 March 2011 (33,669 cars in 2010 and 12,637 in 2011).\n\nThe Regal debuted in North America with a single engine, the 2.4L DOHC I4 engine and was rated at and of torque. The 2.4L engine is mated to a GM Hydra-Matic 6T45 six-speed\n\nThe 2011 Regal produced in North America was offered in one trim level, called CXL, and was offered with two engine choices and seven option packages. Buick had planned to offer a lower-level trim called CX with cloth seats and a higher level trim called CXS, but those trim levels have not yet been offered.\n\nThe CXL Turbo comes equipped with a turbocharged 2.0L direct-injected Ecotec DOHC I4 rated at and of torque, mated to either an Aisin AF40 6-speed automatic or a 6-speed manual transmission, making the Regal Turbo the first Buick model to be offered with a manual transmission since the Buick Skyhawk ended production in 1989. The 2.0L turbo is the first direct-injected turbocharged production car capable of running on any blend of gasoline or E85 ethanol.\n\nAt the 2010 North American International Auto Show in Detroit, GM showed a concept GS version of the Regal based on the Opel Insignia OPC and the Vauxhall Insignia VXR. The concept featured a 2.0L, , high-output DOHC I4 turbocharged Ecotec engine, a 6-speed manual transmission and all-wheel drive.\n\nThe production GS leaves most of the concept specifications intact, but is front-wheel drive. The GS features Buick's Interactive Drive Control System with GS mode, a choice of an FGP Germany F40-6 six-speed manual or Aisin AF-40 (G2) six-speed automatic transmission, high performance brakes with Brembo front calipers and high performance strut (HiPerStrut) front suspension. 19 inch wheels will be standard and 20 inch forged aluminum wheels will be available. The GS is expected to accelerate from zero to 60 miles per hour in under seven seconds. The production version is equipped with GS-only high-output version of the Ecotec 2.0L turbo engine with and of torque. The GM LHU engine used in the GS trim makes 135 hp per liter - Buick’s highest specific output ever. The GS went on sale in fall 2011 as a 2012 model.\n\nThe Shanghai GM variant of Regal GS went on sale in China on September 15, 2011. This car has been localized. The output of the 2.0L Turbo SIDI engine is , which is about 40 kW less than the production US-model. The Torque is . and the top speed is claimed as 232 km/h. The car is only offered in this case as a front-wheel-drive. There exist also some interior and exterior differences between the American and Chinese models.\n\nFor 2016, the Regal added a new Sport Touring Edition which included unique 18\" aluminum black pocket wheels and a rear lip spoiler.\n\nBeginning in 2011, Buick began offering the eAssist system in the Regal, The 2012 model year Regal is the second GM vehicle to offer eAssist after the 2012 Buick LaCrosse. The eAssist system is standard in the LaCrosse, but the eAssist powertrain is optional in the Regal.\n\nThe eAssist system adds a lithium-ion battery housed in the trunk, along with regenerative braking, engine stop/start, fuel cut-off, grille louvres that close at speed, underbody panels and low-rolling resistance tires. The eAssist system adds up to to the standard 2.4L Ecotec engine during acceleration. Fuel economy for the Regal with eAssist is estimated at city, highway.\n\nGM revealed an updated 2014 Regal at the 2013 New York Auto Show. Changes include a revised interior and exterior, a boost in performance for the CXL Turbo up to 258 hp (192 kW) and 295 lb·ft (400 N·m) of torque, and an available all-wheel drive option offered for the 2.0L engine/6-speed automatic transmission equipped vehicles. Changes for the GS include revised interior and exterior, a drop in power to match that of the CXL Turbo, and available an all-wheel drive option offered for the 2.0L / six-speed automatic transmission equipped vehicles. The six-speed manual transmission is still offered, but only in the front-wheel drive variant. Buick's VentiPorts have reappeared starting with 2014 models, a styling feature unique to Buick that dates back to 1949.\n\nSeveral new safety features were added for the 2014 year, which include forward collision warning, lane-departure warning, blind-spot monitoring, rear cross-traffic alert, and a following-distance indicator. The foregoing are all part of a Driver Confidence package, while collision preparation, which pre-loads the brake system ahead of an imminent collision, and adaptive cruise control are available separately.\n\nOn April 4, 2017, an all-new sixth generation Buick Regal was introduced at GM's Design Dome in Warren, Michigan as a 2018 model in both liftback and station wagon versions. Buick Regal began sales in China on July 21, 2017. Similar to the previous generation, it is related to the Opel and Vauxhall Insignia sold in Europe and Holden ZB Commodore in Australia, and like the previous generation, the vehicles were developed and are produced in Germany.\n\nThe fastback version of the Regal, now branded as Regal Sportback, is the replacement for the Regal sedan. While the roofline is similar to that of its predecessor, the new liftback configuration provides added utility. It has grown larger in size, and is around 300 to 500 pounds lighter than the previous Regal. It carries two versions of the \"LTG\" 2.0 L Turbocharged inline-4 engine. Front-wheel drive models produce @ 5400 rpm and @ 2000 rpm, while the all-wheel drive model produces @ 5500 and @ 3000 rpm, respectively.\n\nThe station wagon variant, named Regal TourX, was also introduced and went on sale at the same time as the hatchback, marking the first time since the demise of the Roadmaster in 1996 that Buick had a station wagon in its lineup. The TourX competes against the Volvo V60 Cross Country, Audi A4 Allroad, Subaru Outback, and BMW 3 Series wagon, and offers up to 73.5 cubic feet of storage space behind the front seats, but only comes in all-wheel drive, along with a \"LTG\" turbocharged 2.0-liter four-cylinder producing an estimated and mated with an eight-speed automatic transmission. The TourX is not available in Canada.\n\nThe 2018 GS is the performance version of sixth generation Regal. This generation of GS sports a 3.6 liter LGX V6 engine producing 310 hp, signals return of V6 usage in a mid-size Buick after a hiatus of 13 years. This V6 engine makes GS the most powerful production Buick, (along with the third generation LaCrosse).\n\n\n"}
{"id": "11418039", "url": "https://en.wikipedia.org/wiki?curid=11418039", "title": "Clothing swap", "text": "Clothing swap\n\nA clothing swap is a type of swapmeet wherein participants exchange their valued but no longer used clothing for clothing they will use. Clothing swaps are considered not only a good way to refill one's wardrobe, but also are considered an act of environmentalism. It is also used to get rid of and obtain specialist clothing. Cloth swap can be specialized - they can cater for example for clothes swaps only for high end brands.\n\nThe notion of swapping is not a new concept; however, many groups and organizations exist that hold clothing swaps, in a variety of sizes, to raise money and clothes for charitable donations.\n\nClothes swaps originated in San Francisco California with events hosted by Suzanne Agasi in the late nineties.\n\nOnline clothes swapping has also become popular, with websites offering an environmentally friendly and frugal alternative to shopping or second-hand shops.\n\n"}
{"id": "44059936", "url": "https://en.wikipedia.org/wiki?curid=44059936", "title": "Coherent turbulent structure", "text": "Coherent turbulent structure\n\nTurbulent flows are complex multi-scale and chaotic motions that need to be classified into more elementary components, referred to coherent turbulent structures. Such a structure must have temporal coherence, i.e. it must persist in its form for long enough periods that the methods of time-averaged statistics can be applied. Coherent structures are typically studied on very large scales, but can be broken down into more elementary structures with coherent properties of their own, such examples include hairpin vortices. Hairpins and coherent structures have been studied and noticed in data since the 1930s, and have been since cited in thousands of scientific papers and reviews.\n\nFlow visualization experiments, using smoke and dye as tracers, have been historically used to simulate coherent structures and verify theories, but computer models are now the dominant tools widely used in the field to verify and understand the formation, evolution, and other properties of such structures. The kinematic properties of these motions include size, scale, shape, vorticity, energy, and the dynamic properties govern the way coherent structures grow, evolve, and decay. Most coherent structures are studied only within the confined forms of simple wall turbulence, which approximates the coherence to be steady, fully developed, incompressible, and with a zero pressure gradient in the boundary layer. Although such approximations depart from reality, they contain sufficient parameters needed to understand turbulent coherent structures in a highly conceptual degree.\n\nThe presence of organized motions and structures in turbulent shear flows was apparent for a long time, and has been additionally implied by mixing length hypothesis even before the concept was explicitly stated in literature. There were also early correlation data found by measuring jets and turbulent wakes, particularly by Corrsin and Roshko. Hama's hydrogen bubble technique, which used flow visualization to observe the structures, received wide spread attention and many researchers followed up including Kline. Flow visualization is a laboratory experimental technique that is used to visualize and understand the structures of turbulent shear flows.\nWith a much better understanding of coherent structures, it is now possible to discover and recognize many coherent structures in previous flow-visualization pictures collected of various turbulent flows taken decades ago. Computer simulations are now being the dominant tool for understanding and visualizing coherent flow structures. The ability to compute the necessary time-dependent Navier-Stokes equations produces graphic presentations at a much more sophisticated level, and can additionally be visualized at different planes and resolutions, exceeding the expected sizes and speeds previously generated in laboratory experiments. However, controlled flow visualization experiments are still necessary to direct, develop, and validate the numerical simulations now dominant in the field.\n\nA turbulent flow is a flow regime in fluid dynamics where fluid velocity varies significantly and irregularly in both position and time.Furthermore, a coherent structure is defined as a turbulent flow whose vorticity expression, which is usually stochastic, contains orderly components that can be described as being instantaneously coherent over the spatial extent of the flow structure. In other words, underlying the three-dimensional chaotic vorticity expressions typical of turbulent flows, there is an organized component of that vorticity which is phase-correlated over the entire space of the structure. The instantaneously space and phase correlated vorticity found within the coherent structure expressions can be defined as coherent vorticity, hence making coherent vorticity the main characteristic identifier for coherent structures. Another characteristic inherent in turbulent flows is their intermittency, but intermittency is a very poor identifier of the boundaries of a coherent structure, hence it is generally accepted that the best way to characterize the boundary of a structure is by identifying and defining the boundary of the coherent vorticity.\n\nBy defining and identifying coherent structure in this manner, turbulent flows can be decomposed into coherent structures and incoherent structures depending on their coherence, particularly their correlations with their vorticity. Hence, similarly organized events in an ensemble average of organized events can be defined as a coherent structure, and whatever events not identified as similar or phase and space aligned in the ensemble average is an incoherent turbulent structure. \n\nOther attempts at defining a coherent structure can be done through examining the correlation between their momenta or pressure and their turbulent flows. However, it often leads to false indications of turbulence, since pressure and velocity fluctuations over a fluid could be well correlated in the absence of any turbulence or vorticity. Some coherent structures, such as vortex rings, etc. can be large-scale motions comparable to the extent of the shear flow. There are also coherent motions at much smaller scales such as hairpin vortices and typical eddies, which are typically known as coherent substructures, as in coherent structures which can be broken up into smaller more elementary substructures.\n\nAlthough a coherent structure is by definition characterized by high levels of coherent vorticity, Reynolds stress, production, and heat and mass transportation, it does not necessarily require a high level of kinetic energy. In fact, one of the main roles of coherent structures is the large-scale transport of mass, heat, and momentum without requiring the high amounts of energy normally needed. Consequently, this implies that coherent structures are not the main production and cause of Reynolds stress, and incoherent turbulence can be similarly significant.\n\nCoherent structures cannot superimpose, i.e. they cannot overlap and each coherent structure has its own independent domain and boundary. Since eddies coexist as spatial superpositions, a coherent structure is not an eddy. For example, eddies dissipate energy by obtaining energy from the mean flow at large scales, and eventually dissipating it at the smallest scales. There is no such analogous exchange of energy between coherent structures, and any interaction such as tearing between coherent structures simply results in a new structure. However, two coherent structures can interact and influence each other. The mass of a structure change with time, with the typical case being that structures increase in volume via the diffusion of vorticity. \n\nOne of the most fundamental quantities of coherent structures is characterized by coherent vorticity, formula_1. Perhaps the next most critical measures of coherent structures are the coherent vs. incoherent Reynold's stresses, formula_2 and formula_3. These represent the transports of momentum, and their relative strength indicates how much momentum is being transported by coherent structures as compared to incoherent structures. The next most significant measures include contoured depictions of coherent strain rate and shear production. A useful property of such contours is that they are invariant under Galilean transformations, hence the contours of coherent vorticity constitute an excellent identifier to the structure's boundaries. The contours of these properties not only locate where exactly coherent structure quantities have their peaks and saddles, but also identify where the incoherent turbulent structures are when overlaid on their directional gradients. In addition, spatial contours can be drawn describe the shape, size, and strength of the coherent structures, depicting not only the mechanics but also the dynamical evolution of coherent structures. For example, in order for a structure to be evolving, and hence dominant, its coherent vorticity, coherent Reynolds stress, and production terms should be larger than the time averaged values of the flow structures.\n\nCoherent structures form due to some sort of instability, e.g. the Kelvin–Helmholtz instability. Identifying an instability, and hence the initial formation of a coherent structure, requires the knowledge of initial conditions of the flow structure. Hence, documentation of the initial condition is essential for capturing the evolution and interactions of coherent structures, since initial conditions are quite variable. Overlooking the initial conditions was common in early studies due to researchers overlooking their significance. Initial conditions include the mean velocity profile, thickness, shape, the probability densities of velocity and momentum, the spectrum of Reynolds stress values, etc. These measures of initial flow conditions can be organized and grouped into three broad categories: laminar, highly disturbed, and fully turbulent.\n\nOut of the three categories, coherent structures typically arise from instabilities in laminar or turbulent states. After an initial triggering, their growth is determined by evolutionary changes due to non-linear interactions with other coherent structures, or their decay onto incoherent turbulent structures. Observed rapid changes lead to the belief that there must be a regenerative cycle that takes place during decay. For example, after a structure decays, the result may be that the flow is now turbulent and becomes susceptible to a new instability determined by the new flow state, leading to a new coherent structure being formed. It is also possible that structures do not decay and instead distort by splitting into substructures or interacting with other coherent structures.\n\nLagrangian coherent structures (LCSs) are influential material surfaces that create clearly recognizable patterns in passive tracer distributions advected by an unsteady flow. LCSs can be classified as hyperbolic (locally maximally attracting or repelling material surfaces), elliptic (material vortex boundaries), and parabolic (material jet cores). These surfaces are generalizations of classical invariant manifolds, known in dynamical systems theory, to finite-time unsteady flow data. This Lagrangian perspective on coherence is concerned with structures formed by fluid elements, as opposed to the Eulerian notion of coherence, which considers features in the instantaneous velocity field of the fluid. Various mathematical techniques have been developed to identify LCSs in two- and three-dimenisonal data sets, and have been applied to laboratory experiments, numerical simulations and geophysical observations. \n\nHairpin vortices are found on top of turbulent bulges of the turbulent wall, wrapping around the turbulent wall in hairpin shaped loops, where the name originates. The hairpin-shaped vortices are believed to be one of the most important and elementary sustained flow patterns in turbulent boundary layers. Hairpins are perhaps the simplest structures, and models that represent large scale turbulent boundary layers are often constructed by breaking down individual hairpin vortices, which could explain most of the features of wall turbulence. Although hairpin vortices form the basis of simple conceptual models of flow near a wall, actual turbulent flows may contain a hierarchy of competing vortices, each with their own degree of asymmetry and disturbances.\n\nHairpin vortices resemble the horseshoe vortex, which exists because of perturbations of small upward motion due to differences in upward flowing velocities depending on the distance from the wall. These form multiple packets of hairpin vortices, where hairpin packets of different sizes could generate new vortices to add to the packet. Specifically, close to the surface, the tail ends of hairpin vortices could gradually converge resulting in provoked eruptions, producing new hairpin vortices. Hence, such eruptions are a regenerative process, in which they act to create vortices near the surface and eject them out onto the outer regions of the turbulent wall. Based on the eruptive properties, such flows can be inferred to be very efficient at heat transfer because of mixing. Specifically, eruptions carry hot fluids up while cooler flows are brought downwards during the converging of tails of the hairpin vortices before erupting.\n\nIt is believed that production and contributions to formula_4, the Reynolds stress, occur during strong interactions between the inner and outer walls of hairpins. During the production of this Reynold's stress term, the contributions come in sharp intermittent time segments when eruptions bring new vortices outward. \n\nFormation of hairpin vortices has been observed in experiments and numerical simulations of single hairpins, however observational evidence for them in nature is still limited. Theodorsen has been producing sketches that indicate the presence of hairpin vortices in his flow visualization experiments. These smaller elementary structures can be seen overlaying the main vortex in the sketch to the right (image of sketch to Theodorsen's steam experiment that exposes the presence of structures). The sketch was well advanced for the time, but with the advent of computers came better depictions. Robinson in 1952 isolated two types of flow structures that he named the \"horseshoe\", or arch, vortex and the \"quasi-streamwise\" vortex (classic figure shown to the right). \n\nSince the mass usage of computers, direct numerical simulations or DNS have been used widely, producing vast data sets describing the complex evolution of flow. DNS indicates many complicated 3-dimensional vortices are embedded in regions of high shear near the surface. Researchers look around this region of high shear for indications of individual vortex structures based on accepted definitions, like coherent vortices. Historically, a vortex has been thought of as a region in the flow where a group of vortex lines come together hence indicating the presence of a vortex core, with groups of instantaneous circular paths about the core. In 1991, Robinson defined a vortex structure to be a core consisting of convected low pressure regions, where instantaneous streamlines can form circles or spiral shapes relative to the plane normal to the vortex core plane. Although it is not possible to track the evolution of hairpins over long periods, it is possible to identify and trace their evolution over short time periods. Some of the key notable features of hairpin vortices are how they interact with the background shear flow, other vortices, and how they interact with the flow near the surface.\n"}
{"id": "8019165", "url": "https://en.wikipedia.org/wiki?curid=8019165", "title": "Deer stone", "text": "Deer stone\n\nDeer stones (also known as reindeer stones) are ancient megaliths carved with symbols found largely in Siberia and Mongolia. The name comes from their carved depictions of flying deer. There are many theories to the reasons behind their existence and the people who made them.\n\nDeer stones are usually constructed from granite or greenstone, depending on which is the most abundant in the surrounding area. They have varying heights; most are over 3 feet tall, but some reach a height of 15 feet. The tops of the stones can be flat, round or smashed, suggesting that perhaps the original top had been deliberately destroyed. The stones are usually oriented with the decorated face to the east.\n\nThe carvings and designs were most usually completed before the stone was erected, though some stones show signs of being carved in place. The designs were pecked or ground into the stone surface. Deep-grooved cuts and right-angle surfaces indicate the presence of metal tools. Stone tools were used to smooth the harsh cuts of some designs. Nearly all the stones were hand carved, but some unusual stones show signs that they could have been cut with a primitive type of mechanical drill.\n\nV. V. Volkov, in his thirty years of research, classified three distinct types of deer stones.\n\nThese stones are fairly detailed and more elegant in their depiction methods. They usually feature a belted warrior with a stylized flying reindeer on his torso. This type of stone is most prominent in southern Siberia and northern Mongolia. This concentration suggests that these stones were the origin of the deer stone tradition, and further types both simplified and elaborated on these.\n\nThese stones feature a central region of the stone, sectioned off by two \"belts\", horizontal lines. There are also \"earring hoops\", large circles, diagonal slashes in groups of two and three known as \"faces\", and \"necklaces\", collection of stone pits resembling their namesake.\n\nThe Sayan-Altai stones feature some of the West Asian-European markings, including free-floating, straight-legged animals, daggers and other tools. The appearance of deer motifs is markedly diminished, and those that do appear often do not emphasise the relationship between reindeer and flying. The Sayan-Altai stones can be sub-divided into two types:\n\nThere are many common images that appear in deer stones, as well as a multitude of ways they are presented.\n\nReindeer feature prominently in nearly all of the deer stones. Early stones have very simple images of reindeer, and as time progresses, the designs increase in detail. A gap of 500 years results in the appearance of the complicated flying reindeer depiction. Reindeer are depicted as flying through the air, rather than merely running on land. Piers Vitebsky writes, \"The reindeer is depicted with its neck outstretched and its legs flung out fore and aft, as if not merely galloping but leaping through the air.\" The antlers, sometimes appearing in pairs, have become extremely ornate, utilizing vast spiral designs that can encompass the entire deer. These antlers sometimes hold a sun disc or other sun-related image. Other artwork from the same period further emphasizes the connection between the reindeer and the sun, which is a very common association in Siberian shamanism. Tattoos on buried warriors contain deer tattoos, featuring antlers embellished with small birds' heads. This reindeer-sun-bird imagery perhaps symbolizes the shaman's spiritual transformation from the earth to the sky: the passage from earthly life to heavenly life. As these deer images also appear in warrior tattoos, it is possible that reindeer were believed to offer protection from dangerous forces. Another theory is that the deer spirit served as a guide to assist the warrior soul to heaven.\n\nParticularly in the Sayan-Altai stones, a multitude of other animals are present in deer stone imagery. One can see depictions of tigers, pigs, cows, horse-like creatures, frogs and birds. Unlike the reindeer, however, these animals are depicted in a more natural style. This lack of ornate detailing indicates the lack of supernatural importance of such animals, taking an obvious backseat against the reindeer. The animals are often paired off with one another in confrontation, e.g. a tiger confronting a horse in a much more earthly activity.\n\nWeapons and tools can be seen throughout all the stones, though weapons make a strong appearance in the Sayan-Altai stones. Bows and daggers crop up frequently, as well as typical Bronze-Age implements, such as fire-starters or chariot reins. The appearance of these tools helps date the stones to the Bronze Age.\n\nChevron patterns crop up occasionally, usually in the upper regions of the stone. These patterns can be likened to military shields, suggesting the stones' connection to armed conflict. It has also been suggested that chevron patterns could be a shamanic emblem representing the skeleton.\n\nHuman faces are a much rarer occurrence and are usually carved into the top of the stone. These faces are carved with an open mouth, as though singing. This also suggests a religious/shamanistic connection of the deer stone, as vocal expression is a common and important theme in shamanism.\n\nArchaeologists have found over 900 deer stones in Central Asia and South Siberia. Similar images are found in a wider area, as far west as Kuban, Russia; the Southern Bug in the Ukraine; Dobruja, Bulgaria; and the Elbe, which flows through the Czech Republic and Germany.\n\nDeer stones were probably originally erected by Bronze Age nomads around 1000 BCE though further research into the Cimmerian stone stelae-Kurgan stelae should be taken into much consideration. Later cultures have often reused the stones in their own burial mounds (known as \"kheregsüürs\") and for other purposes. Modern vandals have also defaced and even looted the stones.\n\nThere are several proposed theories for the purpose of the deer stones. There are different viewpoints about the origins of deer stone art. According to H.L. Chlyenova, the artistic deer image originated from the Sak tribe and its branches (Chlyenova 1962). Volkov believes that some of the methods of crafting deer stone art are closely related to Scythians (Volkov 1967), whereas Mongolian archaeologist D. Tseveendorj regards deer stone art as having originated in Mongolia during the Bronze Age and spread thereafter to Tuva and the Baikal area (Tseveendorj 1979). D. G. Savinov (1994) and M. H. Mannai-Ool (1970) have also studied deer stone art and have reached other conclusions. The stones do not occur alone, usually with several other stone monuments, sometimes carved, sometimes not. The soil around these gatherings often contains traces of animal remains, for example, horses. Such remains were placed underneath these auxiliary stones. Human remains, on the other hand, were not found at any of the sites, which discredits the theory that the stones could function as gravestones. The markings on the stones and the presence of sacrificial remains could suggest a religious purpose, perhaps a prime location for the occurrence of shamanistic rituals. Some stones include a circle at the top and stylised dagger and belt at the bottom, which has led some scholars, such as William Fitzhugh, to speculate that the stones could represent a spiritualized human body, particularly that of a prominent figure such as a warrior or leader. This theory is reinforced by the fact that the stones are all very different in construction and imagery, which could be because each stone tells a unique story for the individual it represents.\n\nIn 1892, V.V. Radlov published a collection of drawings of deer stones in Mongolia. Radlov's drawings showed the highly stylized images of deer on the stones, as well as the settings in which they were placed. Radlov showed that in some instances the stones were set in patterns suggesting the walls of a grave, and in other instances, the deer stones were set in elaborate circular patterns, suggesting use in rituals of unknown significance.\n\nIn 1954 A.P. Okladnikov published a study of a deer stone found in 1856 by D.P. Davydov near modern Ulan-Ude now known as the Ivolga stone, displayed in the Irkutsk State Historical Museum. Okladinkov identified the deer images as reindeer, dated the stone's carving to the 6th-7th Centuries BC, and concluded from its placement and other images that it was associated with funerary rituals, and was a monument to a warrior leader of high social prominence.\n\nA 1981 study by V.V. Volkov is the most extensive study of deer stones to date. It identified two cultural conditions behind the deer stones. The eastern deer stones appear to be associated with cemeteries composed of above-ground slab graves. The other cultural tradition is associated with the circular structures suggesting use as the center of rituals.\n\nIn 2006, the Deer Stone Project of the Smithsonian Institution and Mongolian Academy of Sciences began to record the stones digitally with 3-D laser scanning.\n\n\n\n"}
{"id": "11167093", "url": "https://en.wikipedia.org/wiki?curid=11167093", "title": "Diethylene glycol dinitrate", "text": "Diethylene glycol dinitrate\n\nDiethylene glycol dinitrate is a nitrated alcohol ester produced by the action of concentrated nitric acid, normally admixed with an excess of strong sulfuric acid as a dehydrating agent, upon diethylene glycol.\n\nDiethylene glycol dinitrate is a colorless, odorless, viscous, oily liquid, with specific gravity 1.4092 at 0 °C and 1.3846 at 20 °C; freezing point −11.5 °C under a standard atmosphere; the theoretical boiling point of approximately 197 °C difficult to confirm as the compound begins to decompose and spontaneously inflames at or slightly below this temperature. Partial pressure is reported as at 22.4 °C and . It is readily miscible in most non-polar solvents, methanol, and cold acetic acid. Solubility in water (4.1 g/L at 24 °C) and ethanol is very low. While chemically similar to a number of powerful high explosives, pure diethylene glycol dinitrate is extremely hard to initiate and will not propagate a detonation wave. It inflames only with difficulty (requiring localized heating to decomposition point) unless first atomized, and burns placidly even in quantity.\n\nMixed with nitrocellulose and extruded under pressure, diethylene glycol dinitrate forms a tough colloid whose characteristics (good specific impulse, moderate burn rate and temperature, great resistance to accidental ignition and casual handling) make it well suited as a smokeless powder for artillery and a solid propellant for rocketry. It was widely used in this capacity during World War II (e.g., by the Kriegsmarine). It also found use as a \"productive\" desensitizer (one that contributes to the overall power of the explosion rather than having a neutral or negative effect) in nitroglycerine and nitroglycol-based explosives such as dynamite and blasting gelatin. It is also used as plasticizer for energetic materials.\n\nIf ingested, like nitroglycerine, it produces rapid vasodilation through the release of nitrogen monoxide, NO. Popularly termed nitric oxide, NO is a physiological signaling molecule that relaxes smooth muscle. Consequently, diethylene glycol dinitrate has occasionally been used medically to relieve angina, substernal chest pain associated with impaired cardiac circulation. The rationale is that the concurrent headache it induces is somewhat less severe than other nitro compounds.\n\nAt present, interest in the chemical seems to be mostly historical: more potent perchlorate–metal mixtures have long since supplanted it as a solid propellant; safer explosives have replaced nitroglycerine, true dynamites (the term is often used generically, even by experienced field technicians, to refer to almost any explosive supplied in small, discrete packages) retaining only a few specialist uses. The medical application was never widespread, the standard nitroglycerine being faster acting and almost literally dirt cheap; oral nitrates in any case being only palliative, not an effective treatment.\n\nTriethylene glycol dinitrate, diethylene glycol dinitrate, and trimethylolethane trinitrate are being considered as replacements for nitroglycerin in propellants.\n\n\n"}
{"id": "2258083", "url": "https://en.wikipedia.org/wiki?curid=2258083", "title": "Dirac fermion", "text": "Dirac fermion\n\nIn physics, a Dirac fermion is spin  particle (a fermion) which is different from its antiparticle. The vast majority of particles – perhaps all – fall under this category.\n\nIn particle physics all fermions in the standard model have distinct antiparticles (\"perhaps\" excepting neutrinos) and hence are Dirac fermions. They are named for Paul Dirac, and can be modeled with the Dirac equation.\n\nA Dirac fermion is equivalent to two Weyl fermions. The counterpart to a Dirac fermion is a Majorana fermion, a particle that must be its own antiparticle.\n\nIn condensed matter physics, low-energy excitations in graphene and topological insulators, among others, are fermionic quasiparticles described by a pseudo-relativistic Dirac equation.\n\n"}
{"id": "5645320", "url": "https://en.wikipedia.org/wiki?curid=5645320", "title": "Ecalene", "text": "Ecalene\n\nEcalene is a trademarked mixture of alcohols, which may be used as fuel or as a fuel additive. The typical composition of Ecalene is as follows:\n"}
{"id": "1628599", "url": "https://en.wikipedia.org/wiki?curid=1628599", "title": "Ecology (disciplines)", "text": "Ecology (disciplines)\n\nEcology is a broad biological science and can be divided into many sub-disciplines using various criteria. Many of these fields overlap, complement and inform each other, and few of these disciplines exist in isolation. For example, the population ecology of an organism is a consequence of its behavioral ecology and intimately tied to its community ecology. Methods from molecular ecology might inform the study of the population, and all kinds of data are modeled and analyzed using quantitative ecology techniques. \n\nWhen discussing the study of a single species, a distinction is usually made between its \"biology\" and its \"ecology\". For example, \"polar bear biology\" might include the study of the polar bear's physiology, morphology, pathology and ontogeny, whereas \"polar bear ecology\" would include a study of its prey species, its population and metapopulation status, distribution, dependence on environmental conditions, etc. In that sense, there can be as many subdisciplines of ecology as there are species to study. \n\nEcology can also be classified on the basis of:\n\n\nSpecialized branches of ecology include, among others:\n\n\nEcology also plays important roles in many inter-disciplinary fields:\n\n\nEcology has also inspired (and lent its name to) other non-biological disciplines such as\n\n\nFinally, ecology is used to describe several philosophies or ideologies, such as \n\n"}
{"id": "24497801", "url": "https://en.wikipedia.org/wiki?curid=24497801", "title": "Emissions Reduction Currency System", "text": "Emissions Reduction Currency System\n\nEmissions Reduction Currency Systems (ERCS) are schemes that provide a positive economic and or social reward for reductions in greenhouse gas emissions, either through distribution or redistribution of national currency or through the publishing of coupons, reward points, local currency, or complementary currency.\n\nEmissions reduction currency is different from an emissions credit. The value of an emissions credit is determined by a national cap in emissions and the degree to which the credit confers a right to pollute. The ultimate value of an emissions credit is realised when it is surrendered to avoid punitive fines for emitting.\n\nEmissions reduction currency is also different from a voluntary carbon offset where a payment is made, typically to fund alternative energy or reforestation, the emissions reduction or sequestration resulting from which is used to reduce or cancel the payers responsibility for emissions produced by themselves. The value of an offset is in its being held by the purchaser and applies only for the period and purpose against which the offset applies.\n\nAn emissions reduction currency, by contrast, is purely an incentive for behaviour change by individuals or groups. As such the currency creates an additional economic benefit for emissions reductions separate from the cost imposed by national emissions caps or the voluntary cost assumed by the purchaser of a voluntary offset.\n\nEmissions reduction currencies are not exchangeable within national cap and trade systems and as such do not confer any right to pollute.\n\nWhile no emissions reduction currency system has achieved the scale of emissions crediting systems, there are a number of small scale schemes in operation or being set up. In addition there are a number of approaches that are currently hypothetical being promoted by a number of organisations, academic institutions and think tanks.\n\nEmissions reduction currency systems conceptually are inclusive of carbon currency systems but also include schemes that reduce emissions in incidental ways such as through waste reduction and community education.\n\nThe idea of a global wealth system based on alternative energy production was first suggested by Buckminster Fuller in his 1969 book \"Operating Manual for Spaceship Earth\". This idea was piloted by Garry Davis who distributed these \"kilowatt dollars\" at the 1992 Earth Summit held in Rio de Janeiro. Edgar Kempers and Rob Van Hilton launched the Kiwah (kilowatt hour) currency at the Copenhagen Climate Summit in 2009.\n\nEmission reduction currency systems can be designated as belonging to one or more of five categories:\n\nIntroduction of sustainable land management practices in tropical rainforest and other high carbon environments can lead to abatement of emissions from land clearing that might have otherwise occurred or from additional CO2 sequestration.\n\nLand purchased and managed for these purposes can be used to create an independently tradeable carbon right, which may or may not be recognised within an emissions credit scheme. For example, aboveground biomass resulting from land use changes can currently be converted to recognised emissions credits under the Kyoto Protocol Clean Development Mechanism (CDM). Increases in soil carbon for reasons other than reforestation either through changes to land management practices or through the burial of biochar are currently not included in emissions credit systems such as the CDM.\n\nThese certificates of legal title can be traded as a form of currency independently of their use as an offset, yielding additional economic benefits.\nThis use is suggested by The Carbon Currency Foundation.\n\nAnother emissions reduction currency system proposed on this basis is the ECO, a project of The Next Nature Lab which is an initiative of Eindhoven University of Technology in the Netherlands.\n\nAn emissions reduction currency system based on promotional discount is one where participants are rewarded for reducing their emissions by gaining points which can be redeemed for discounts from businesses advertising in the system.\n\nRecycleBank is one such scheme where participants weigh recycled materials in specially designed disposal bins that identify themselves to scales embedded in garbage collection vehicles. Recyclebank is also funded by municipal governments that purchase and operate the required equipment, allowing RecycleBank to operate as a private for profit company. Another similar scheme is Greenopolis that works through social media websites such as Facebook.\n\nEarthAid uses specialised software that publishes utility bills from companies in an online format that participants can share with family and friends. Reduced energy consumption earns reward points that can be redeemed for prizes at businesses in the EarthAid rewards network.\n\nAn emissions currency reduction system based on allocation is one where all individual participants are awarded an equal allotment of emissions currency. Participants then trade goods and services with one another to obtain enough of the currency to cover their actual emissions.\nThe objective of an allocation scheme is to obtain social parity between participants with regards to emissions reductions.\n\nTechnically an emissions crediting scheme,an allocation scheme is classed as an emissions reduction currency system because the trading of the currency between individuals as parity is sought can create a secondary market of trading where the currency can act as a medium of exchange, and this trading creates an additional positive economic value associated with emissions reductions.\n\nThe Global Resource Bank is one organisation advocating such a global allocation scheme.\n\nOtherwise known as personal carbon trading, an emissions reduction currency system based on rationing presumes a standard ration of emissions allowable for an average citizen that incrementally decreases over time.\n\nParticipants using less than the rationed amount receive a currency that can be traded with those emitting more than the allowed amount.\nAll participants pledge to in total remain below the average with a net positive value in the scheme.\n\nCarbon Rationing Action Groups (CRAG), started in the United Kingdom, has a global network of groups. CRAG participants use a standard average for the country as a basis for the rationed amount. Participants emitting at above rationed levels must pay those below it in national currency.\n\nNorfolk Island, Australia is in the process of implementing an island-wide voluntary personal carbon trading scheme designed by Southern Cross University Professor Garry Egger,\n\nA community based emissions reduction currency scheme is a C4 type local currency in which local currency issues are backed by the emissions reductions of the schemes members. The local currency, when accepted for trade by other members or local businesses, thereby rewards participants for their efforts at global warming prevention. These currencies may have various degrees of convertibility into carbon saved, renewable energy, or national currency.\n\nThe Edogawatt is a form of emissions reduction currency used in Edogawa, Tokyo that is an initiative of the local Jōdo Shinshū Jukou-in temple. In this scheme, the temple and devotees purchase solar panels and sell the excess power to the Tokyo Electric Power Company. The temple then takes the difference between the price paid by the Tokyo Electric Power Company and the price paid for natural energy in Germany and sells Green Power Certificates as a fund raiser for the temple. Purchasers of the Green Power Certificates are given 30 Edogawatt bills per certificate. \"These are currently being used among people ... as a certificate of debt or obligation in exchange for baby-sitting, carrying loads, translating and other small jobs. They have provided an incentive for creation of a mutual aid society within the community and we would like to make them a tool for deepening interpersonal relationships and trust.\"\n\nhttp://www.qoin.org/what-we-do/past-projects/kyoto4all/ Kyoto4All was a 2006 report written by Peter van Luttervelt, David Beatty and Edgar Kampers for the Dutch Ministry of Environment (then named VROM). The study described a series of monetary models to connect citizens-consumers to the climate change targets of the post-Kyoto period.\n\nThe Maia Maia Emissions Reduction Currency System, is a scheme developed in Western Australia. The system currency is known as a \"boya\", named after the indigenous Nyungar people's word for rock trading tokens used by them. Each boya is based on 10 kilograms of carbon dioxide equivalent global warming prevention which is equates to a $100 tonne CO2-e Social Cost of Carbon, which approximates a middle estimate from peer reviewed studies. The first issue of boya occurred on 30 January 2011 in Fremantle, Western Australia at an event hosted by the International Permaculture Service and the Gaia Foundation of Western Australia. Other issuers of Boya include the University of Vermont and in Australia, primary schools, non-profit organisations, and a neighbourhood association.\n\nThe Liquidity Network, an initiative of the Foundation for the Economics of Sustainability is proposing to introduce a community run emissions reduction currency in the County of Kilkenny in Ireland. The proposal is currently before council for consideration.\n\nA monetised emissions reduction currency is backed by the financial value of emissions credits or certified under a regulatory scheme or other financial products derived from them. These credits can be converted into fiat currency through transferring ownership of the underlying assets such as selling the emission credits into cap and trade markets.\n\nThe Ven is a virtual currency issued by the Hub Culture social media network. The value of Ven is determined on the financial markets from a basket of currencies and commodities. The Ven may be categorised as an emissions reduction currency because carbon futures are included as one of the commodities used to value the currency.\n\nCarbon Manna is a proposed scheme that will use proceeds from pre-selling credits from bundled emissions reduction projects to reimburse users directly or to enroll them in the successful mobile phone currency M-PESA being used in developing countries to reduce monetary transaction costs and hedge against currency fluctuations.\n\nEmission Reduction solutions for large fleets include the GRIP Idle Management System, which has saved the Columbus Police Department an average of $1.2 million annually.\n"}
{"id": "48934337", "url": "https://en.wikipedia.org/wiki?curid=48934337", "title": "Empty weight", "text": "Empty weight\n\nThe empty weight of an aircraft is based on its weight without any payload (cargo, passengers, usable fuel, etc.). Many different empty weight definitions exist, here are some of the more common ones used.\n\nIn 1975 the General Aviation Manufacturers Association (GAMA) standardized the definition of empty weight terms for Pilot Operating Handbooks as follows:\n\nStandard Empty Weight includes the following:\n\nOptional Equipment includes the following:\n\nPreviously the following were commonly used to define empty weights:\n\nIn this definition Empty Weight includes the following:\n\nNote that weight of oil must be added to Licensed Empty Weight for it to be equivalent to Basic Empty Weight\n\n"}
{"id": "23558678", "url": "https://en.wikipedia.org/wiki?curid=23558678", "title": "Environmental impact of the energy industry", "text": "Environmental impact of the energy industry\n\nThe environmental impact of the energy industry is diverse. Energy has been harnessed by human beings for millennia. Initially it was with the use of fire for light, heat, cooking and for safety, and its use can be traced back at least 1.9 million years. \nIn recent years there has been a trend towards the increased commercialization of various renewable energy sources.\n\nRapidly advancing technologies can potentially achieve a transition of energy generation, water and waste management, and food production towards better environmental and energy usage practices using methods of systems ecology and industrial ecology.\n\nThe scientific consensus on global warming and climate change is that it is caused by anthropogenic greenhouse gas emissions, the majority of which comes from burning fossil fuels with deforestation and some agricultural practices being also major contributors. A 2013 study showed that two thirds of the industrial greenhouse gas emissions are due to the fossil-fuel (and cement) production of just ninety companies around the world (between 1751 and 2010, with half emitted since 1986).\n\nAlthough there is a highly publicized denial of climate change, the vast majority of scientists working in climatology accept that it is due to human activity. The IPCC report \"Climate Change 2007: Climate Change Impacts, Adaptation and Vulnerability\" predicts that climate change will cause shortages of food and water and increased risk of flooding that will affect billions of people, particularly those living in poverty.\n\nOne measurement of greenhouse gas related and other Externality comparisons between energy sources can be found in the ExternE project by the Paul Scherrer Institut and the University of Stuttgart which was funded by the European Commission. According to that study, hydroelectric electricity produces the lowest CO2 emissions, wind produces the second-lowest, nuclear energy produces the third-lowest and solar photovoltaic produces the fourth-lowest.\n\nSimilarly, the same research study (ExternE, Externalities of Energy), undertaken from 1995 to 2005 found that the cost of producing electricity from coal or oil would double over its present value, and the cost of electricity production from gas would increase by 30% if external costs such as damage to the environment and to human health, from the airborne particulate matter, nitrogen oxides, chromium VI and arsenic emissions produced by these sources, were taken into account. It was estimated in the study that these external, downstream, fossil fuel costs amount up to 1%-2% of the EU’s entire Gross Domestic Product (GDP), and this was before the external cost of global warming from these sources was even included. The study also found that the environmental and health costs of nuclear power, per unit of energy delivered, was €0.0019/kWh, which was found to be lower than that of many renewable sources including that caused by biomass and photovoltaic solar panels, and was thirty times lower than coal at €0.06/kWh, or 6 cents/kWh, with the energy sources of the lowest external environmental and health costs associated with it being wind power at €0.0009/kWh.\n\nBiofuel is defined as solid, liquid or gaseous fuel obtained from relatively recently lifeless or living biological material and is different from fossil fuels, which are derived from long-dead biological material. Various plants and plant-derived materials are used for biofuel manufacturing.\n\nHigh use of bio-diesel leads to land use changes including deforestation.\n\nUnsustainable firewood harvesting can lead to loss of biodiversity and erosion due to loss of forest cover. An example of this is a 40-year study done by the University of Leeds of African forests, which account for a third of the world's total tropical forest which demonstrates that Africa is a significant carbon sink. A climate change expert, Lee White states that \"To get an idea of the value of the sink, the removal of nearly 5 billion tonnes of carbon dioxide from the atmosphere by intact tropical forests is at issue.\n\nAccording to the U.N. the African continent is losing forest twice as fast as the rest of the world. \"Once upon a time, Africa boasted seven million square kilometers of forest but a third of that has been lost, most of it to charcoal.\"\n\nThe three fossil fuel types are coal, petroleum and natural gas. It was estimated by the Energy Information Administration that in 2006 primary sources of energy consisted of petroleum 36.8%, coal 26.6%, natural gas 22.9%, amounting to an 86% share for fossil fuels in primary energy production in the world.\n\nIn 2013 the burning of fossil fuels produced around 32 billion tonnes (32 gigatonnes) of carbon dioxide and additional air pollution. This caused negative externalities of $4.9 trillion due to global warming and health problems (> 150 $/ton carbon dioxide). Carbon dioxide is one of the greenhouse gases that enhances radiative forcing and contributes to global warming, causing the average surface temperature of the Earth to rise in response, which climate scientists agree will cause major adverse effects.\n\nThe environmental impact of coal mining and burning is diverse. Legislation passed by the U.S. Congress in 1990 required the United States Environmental Protection Agency (EPA) to issue a plan to alleviate toxic pollution from coal-fired power plants. After delay and litigation, the EPA now has a court-imposed deadline of March 16, 2011, to issue its report.\n\nThe environmental impact of petroleum is often negative because it is toxic to almost all forms of life. The possibility of climate change exists. Petroleum, commonly referred to as oil, is closely linked to virtually all aspects of present society, especially for transportation and heating for both homes and for commercial activities.\n\nNatural gas is often described as the cleanest fossil fuel, producing less carbon dioxide per joule delivered than either coal or oil., and far fewer pollutants than other fossil fuels. However, in absolute terms it does contribute substantially to global carbon emissions, and this contribution is projected to grow. According to the IPCC Fourth Assessment Report, in 2004 natural gas produced about 5,300 Mt/yr of CO emissions, while coal and oil produced 10,600 and 10,200 respectively (Figure 4.4); but by 2030, according to an updated version of the SRES B2 emissions scenario, natural gas would be the source of 11,000 Mt/yr, with coal and oil now 8,400 and 17,200 respectively. (Total global emissions for 2004 were estimated at over 27,200 Mt.)\n\nIn addition, natural gas itself is a greenhouse gas far more potent than carbon dioxide when released into the atmosphere but is released in smaller amounts.\n\nThe environmental impact of electricity generation is significant because modern society uses large amounts of electrical power. This power is normally generated at power plants that convert some other kind of energy into electrical power. Each such system has advantages and disadvantages, but many of them pose environmental concerns.\n\nThe environmental impact of reservoirs is coming under ever increasing scrutiny as the world demand for water and energy increases and the number and size of reservoirs increases. Dams and the reservoirs can be used to supply drinking water, generate hydroelectric power, increasing the water supply for irrigation, provide recreational opportunities and for flood control. However, adverse environmental and sociological impacts have also been identified during and after many reservoir constructions. Whether reservoir projects are ultimately beneficial or detrimental—to both the environment and surrounding human populations— has been debated since the 1960s and probably long before that. In 1960 the construction of Llyn Celyn and the flooding of Capel Celyn provoked political uproar which continues to this day. More recently, the construction of Three Gorges Dam and other similar projects throughout Asia, Africa and Latin America have generated considerable environmental and political debate.\n\nThe environmental impact of nuclear power results from the nuclear fuel cycle, operation, and the effects of nuclear accidents.\n\nThe routine health risks and greenhouse gas emissions from nuclear fission power are smaller than those associated with coal, oil and gas. However, there is a \"catastrophic risk\" potential if containment fails, which in nuclear reactors can be brought about by over-heated fuels melting and releasing large quantities of fission products into the environment. The most long-lived radioactive wastes, including spent nuclear fuel, must be contained and isolated from humans and the environment for hundreds of thousands of years. The public is sensitive to these risks and there has been considerable public opposition to nuclear power. Despite this potential for disaster, normal fossil fuel related pollution is still considerably more harmful than any previous nuclear disaster.\n\nThe 1979 Three Mile Island accident and 1986 Chernobyl disaster, along with high construction costs, ended the rapid growth of global nuclear power capacity. A further disastrous release of radioactive materials followed the 2011 Japanese tsunami which damaged the Fukushima I Nuclear Power Plant, resulting in hydrogen gas explosions and partial meltdowns classified as a Level 7 event. The large-scale release of radioactivity resulted in people being evacuated from a 20 km exclusion zone set up around the power plant, similar to the 30 km radius Chernobyl Exclusion Zone still in effect.\n\nThe environmental impact of wind power when compared to the environmental impacts of fossil fuels, is relatively minor. According to the IPCC, in assessments of the life-cycle global warming potential of energy sources, wind turbines have a median value of between 12 and 11 (geq/kWh) depending, respectively, on if offshore or onshore turbines are being assessed. Compared with other low carbon power sources, wind turbines have some of the lowest global warming potential per unit of electrical energy generated.\n\nWhile a wind farm may cover a large area of land, many land uses such as agriculture are compatible with it, as only small areas of turbine foundations and infrastructure are made unavailable for use.\n\nThere are reports of bird and bat mortality at wind turbines as there are around other artificial structures. The scale of the ecological impact may or may not be significant, depending on specific circumstances. Prevention and mitigation of wildlife fatalities, and protection of peat bogs, affect the siting and operation of wind turbines.\n\nThere are anecdotal reports of negative health effects from noise on people who live very close to wind turbines. Peer-reviewed research has generally not supported these claims.\n\nAesthetic aspects of wind turbines and resulting changes of the visual landscape are significant. Conflicts arise especially in scenic and heritage protected landscapes.\n\nEnergy conservation refers to efforts made to reduce energy consumption. Energy conservation can be achieved through increased efficient energy use, in conjunction with decreased energy consumption and/or reduced consumption from conventional energy sources.\n\nEnergy conservation can result in increased financial capital, environmental quality, national security, personal security, and human comfort. Individuals and organizations that are direct consumers of energy choose to conserve energy to reduce energy costs and promote economic security. Industrial and commercial users can increase energy use efficiency to maximize profit.\n\nThe increase of global energy use can also be slowed by tackling human population growth, by using non-coercive measures such as better provision of family planning services and by empowering (educating) women in developing countries.\n\nEnergy policy is the manner in which a given entity (often governmental) has decided to address issues of energy development including energy production, distribution and consumption. The attributes of energy policy may include legislation, international treaties, incentives to investment, guidelines for energy conservation, taxation and other public policy techniques.\n\n"}
{"id": "1415829", "url": "https://en.wikipedia.org/wiki?curid=1415829", "title": "Fiat Idea", "text": "Fiat Idea\n\nThe Fiat Idea (Type 350) is a front-engine, front-wheel drive, five-door, five-passenger high-roof B-segment mini MPV manufactured and marketed by FCA for model years 2003-2012 — over a single generation with one intermediate facelift. \n\nAs FCA's first entry in the mini-MPV market, the Idea's exterior was designed by Giorgetto Giugiaro, its interior was designed by Fiat's Centro Stilo, and its platform is shared with the second-generation Fiat Punto. The Idea is noted for its centrally located instrument cluster, high H-point seating, and flexible seating — including reclining, sliding and folding rear seats. Its seating design uses biometric principles developed by Antonio Dal Monte at the Italian National Olympic Committee's sport medicine institute\n\nMarketed along with the Lancia Musa, an upscale, rebadged variant, the Idea was superseded by the Fiat 500L.\n\nThe \"Idea\" nameplate is an acronym for Intelligent Design Emotive Architecture.\n\nStandard features included six airbags, isofix attachments, automatic door locks, fire prevention system (fuel cut off), ABS braking with brake force distribution, an anti-slip regulation system to limit wheel slip in cases of reduced grip, traction control, electronic stability control, hill-holder, electric speed-sensitive power steering system, 40-20-40 split rear seating, and a headlight delay system marketed as \"Follow Me Home\".\n\nOptional features included a panoramic sunroof, parking sensors and dual-zone climate control.\n\nThe Fiat Idea engines are all Euro 4 compliant. The petrol engine is the 1.4-litre 16 valve , available with five- and six-speed gearboxes, and the diesel 1.3-litre 16 valve MultiJet units, with or . All of these engines can be matched with a clutchless 5-speed sequential manual shift with a selectable, fully automatic mode — as robotised manual gearbox marketed as Duologic.\n\nThe Brazilian version of the Idea was launched in late 2005. The engines available are the new 1.4-litre 8-valve Fire engine (the same as in the Grande Punto) and the latest version of the GM-sourced Powertrain 1.8-litre 8-valve engine. Both have flexible fuel technology, which lets the driver use either gasoline or ethanol.\n\nThis version has a longer wheelbase and height compared to the European version. The interior design is taken from the Fiat Palio Mk. III, and adapted to the South American market. It also has a panoramic glass roof as an option, known as the SkyDome.\n\nThe Idea is exported in two different trim levels (the ELX with the 1.4-litre engine and the HLX with the 1.8 engine) and it has four airbags, anti-lock braking system with EBD; parking, rain and lights sensor, security laminated glass, the SkyDome, 15\" alloys, Bluetooth phone system, and leather seats among other features.\n\nOver 170,000 Ideas have been sold in Brazil since its launch (2005).\nIn 2010 the Idea received another facelift with new front, rear and door handles.\n\nThe Idea Adventure is a mini crossover version that was launched in September 2006 in São Paulo, Brazil. It has a revised suspension, tires are Pirelli Scorpion 205/70 R15, a protective body kit, specially designed 15\" alloys, interior mods like the white instruments, the specially designed seats with leather, and a standard safety equipment which features double front and side airbags, ABS brakes with EBD. In 2009 the whole Adventure line (Idea, Doblò, Strada and Palio Weekend) was equipped with a locking differential. The line was rebadged as Adventure Locker.\nIdea Brazilian Facelift\n\nDuring his visit to Rio de Janeiro in July 2013, Pope Francis chose to ride in a Fiat Idea for his public appearances, instead of the expensive and secure vehicles usually used by dignitaries and popes.\n"}
{"id": "28910442", "url": "https://en.wikipedia.org/wiki?curid=28910442", "title": "Fleischmann–Pons experiment", "text": "Fleischmann–Pons experiment\n\nThe Fleischmann–Pons experiment was an investigation conducted in the 1980s by Martin Fleischmann of the University of Southampton and Stanley Pons of the University of Utah into whether electrolysis of heavy water on the surface of a palladium (Pd) electrode produces physical effects that defy chemical explanation. Of particular interest was evidence of \"excess\" (i.e. non-chemical) heat extracted from the deuterium fraction of common surface water which, if true, could have delivered the largest economic shock to the global energy industry since the Pennsylvania oil rush.\n\nOn March 23, 1989, Fleischmann (then one of the world's leading electrochemists) and Pons reported their work via a press release from the University of Utah (who asserted ownership of the technology) claiming that the table-top apparatus had produced anomalous heat (understood as \"excess\" heat) of a magnitude they asserted would defy explanation except in terms of nuclear processes, which later came to be referred to as \"cold fusion\". In addition to the results from calorimetry, they further reported measuring small amounts of nuclear reaction byproducts, including neutrons and tritium. The reported results received wide media attention, and raised hopes of a cheap and abundant source of energy.\n\nMany scientists tried to replicate the experiment with the few details available. Hopes faded due to the large number of negative replications, the withdrawal of many reported positive replications, the discovery of flaws and sources of experimental error in the original experiment, and finally the discovery that Fleischmann and Pons had not actually detected nuclear reaction byproducts. By late 1989, most scientists considered cold fusion claims dead.\n\nMartin Fleischmann of the University of Southampton and Stanley Pons of the University of Utah hypothesized that the high compression ratio and mobility of deuterium that could be achieved within palladium metal using electrolysis might result in nuclear fusion. To investigate, they conducted electrolysis experiments using a palladium cathode and heavy water within a calorimeter, an insulated vessel designed to measure process heat. Current was applied continuously for many weeks, with the heavy water being renewed at intervals. Some deuterium was thought to be accumulating within the cathode, but most was allowed to bubble out of the cell, joining oxygen produced at the anode. For most of the time, the power input to the cell was equal to the calculated power leaving the cell within measurement accuracy, and the cell temperature was stable at around 30 °C. But then, at some point (in some of the experiments), the temperature rose suddenly to about 50 °C without changes in the input power. These high temperature phases would last for two days or more and would repeat several times in any given experiment once they had occurred. The calculated power leaving the cell was significantly higher than the input power during these high temperature phases. Eventually the high temperature phases would no longer occur within a particular cell.\n\nIn 1988, Fleischmann and Pons applied to the United States Department of Energy for funding towards a larger series of experiments. Up to this point they had been funding their experiments using a small device built with $100,000 out-of-pocket. The grant proposal was turned over for peer review, and one of the reviewers was Steven Jones of Brigham Young University. Jones had worked for some time on muon-catalyzed fusion, a known method of inducing nuclear fusion without high temperatures, and had written an article on the topic entitled \"Cold nuclear fusion\" that had been published in \"Scientific American\" in July 1987. Fleischmann and Pons and co-workers met with Jones and co-workers on occasion in Utah to share research and techniques. During this time, Fleischmann and Pons described their experiments as generating considerable \"excess energy\", in the sense that it could not be explained by chemical reactions alone. They felt that such a discovery could bear significant commercial value and would be entitled to patent protection. Jones, however, was measuring neutron flux, which was not of commercial interest. To avoid future problems, the teams appeared to agree to simultaneously publish their results, though their accounts of their 6 March meeting differ.\n\nIn mid-March 1989, both research teams were ready to publish their findings, and Fleischmann and Jones had agreed to meet at an airport on 24 March to send their papers to \"Nature\" via FedEx. Fleischmann and Pons, however, pressured by the University of Utah, which wanted to establish priority on the discovery, broke their apparent agreement, submitting their paper to the \"Journal of Electroanalytical Chemistry\" on 11 March, and disclosing their work via a press release and press conference on 23 March. Jones, upset, faxed in his paper to \"Nature\" after the press conference.\n\nFleischmann and Pons' announcement drew wide media attention. But the 1986 discovery of high-temperature superconductivity had made the scientific community more open to revelations of unexpected scientific results that could have huge economic repercussions and that could be replicated reliably even if they had not been predicted by established theories. And many scientists were also reminded of the Mössbauer effect, a process involving nuclear transitions in a solid. Its discovery 30 years earlier had also been unexpected, though it was quickly replicated and explained within the existing physics framework.\n\nThe announcement of a new purported clean source of energy came at a crucial time: adults still remembered the 1973 oil crisis and the problems caused by oil dependence, anthropogenic global warming was starting to become notorious, the anti-nuclear movement was labeling nuclear power plants as dangerous and getting them closed, people had in mind the consequences of strip mining, acid rain, the greenhouse effect and the Exxon Valdez oil spill, which happened the day after the announcement. In the press conference, Chase N. Peterson, then the president of the University of Utah, Fleischmann and Pons, backed by the solidity of their scientific credentials, repeatedly assured the journalists that cold fusion would solve environmental problems and would provide a limitless inexhaustible source of clean energy using only seawater as fuel. They said the results had been confirmed dozens of times and they had no doubts about them. In the accompanying press release Fleischmann was quoted saying: \"What we have done is to open the door of a new research area, our indications are that the discovery will be relatively easy to make into a usable technology for generating heat and power, but continued work is needed, first, to further understand the science and secondly, to determine its value to energy economics.\"\n\nAlthough the experimental protocol had not been published, physicists in several countries attempted, and failed, to replicate the excess heat phenomenon. The first paper submitted to \"Nature\" reproducing excess heat, although it passed peer-review, was rejected because most similar experiments were negative and there were no theories that could explain a positive result; this paper was later accepted for publication by the journal \"Fusion Technology\". Nathan Lewis, professor of chemistry at the California Institute of Technology, led one of the most ambitious validation efforts, trying many variations on the experiment without success, while CERN physicist Douglas R. O. Morrison said that \"essentially all\" attempts in Western Europe had failed. Even those reporting success had difficulty reproducing Fleischmann and Pons' results. On 10 April 1989, a group at Texas A&M University published results of excess heat and later that day a group at the Georgia Institute of Technology announced neutron production—the strongest replication announced up to that point due to the detection of neutrons and the reputation of the lab. On 12 April Pons was acclaimed at an ACS meeting. But Georgia Tech retracted their announcement on 13 April, explaining that their neutron detectors gave false positives when exposed to heat. Another attempt at independent replication, headed by Robert Huggins at Stanford University, which also reported early success with a light water control, became the only scientific support for cold fusion in 26 April US Congress hearings. But when he finally presented his results he reported an excess heat of only one degree celsius, a result that could be explained by chemical differences between heavy and light water in the presence of lithium. He had not tried to measure any radiation and his research was derided by scientists who saw it later. For the next six weeks, competing claims, counterclaims, and suggested explanations kept what was referred to as \"cold fusion\" or \"fusion confusion\" in the news.\n\nIn April 1989, Fleischmann and Pons published a \"preliminary note\" in the \"Journal of Electroanalytical Chemistry\". This paper notably showed a gamma peak without its corresponding Compton edge, which indicated they had made a mistake in claiming evidence of fusion byproducts. Fleischmann and Pons replied to this critique, but the only thing left clear was that no gamma ray had been registered and that Fleischmann refused to recognize any mistakes in the data. A much longer paper published a year later went into details of calorimetry but did not include any nuclear measurements.\n\nNevertheless, Fleischmann and Pons and a number of other researchers who found positive results remained convinced of their findings. The University of Utah asked Congress to provide $25 million to pursue the research, and Pons was scheduled to meet with representatives of President Bush in early May.\n\nOn 30 April 1989, cold fusion was declared dead by the \"New York Times\". The \"Times\" called it a circus the same day, and the \"Boston Herald\" attacked cold fusion the following day.\n\nOn 1 May 1989, the American Physical Society held a session on cold fusion in Baltimore, including many reports of experiments that failed to produce evidence of cold fusion. At the end of the session, eight of the nine leading speakers stated that they considered the initial Fleischmann and Pons claim dead, with the ninth, Johann Rafelski, abstaining. Steven E. Koonin of Caltech called the Utah report a result of \"the incompetence and delusion of Pons and Fleischmann,\" which was met with a standing ovation. Douglas R. O. Morrison, a physicist representing CERN, was the first to call the episode an example of pathological science.\n\nOn 4 May, due to all this new criticism, the meetings with various representatives from Washington were cancelled.\n\nFrom 8 May only the A&M tritium results kept cold fusion afloat.\n\nIn July and November 1989, \"Nature\" published papers critical of cold fusion claims. Negative results were also published in several other scientific journals including \"Science\", \"Physical Review Letters\", and \"Physical Review C\" (nuclear physics).\n\nIn August 1989, in spite of this trend, the state of Utah invested $4.5 million to create the National Cold Fusion Institute.\n\nThe United States Department of Energy organized a special panel to review cold fusion theory and research. The panel issued its report in November 1989, concluding that results as of that date did not present convincing evidence that useful sources of energy would result from the phenomena attributed to cold fusion. The panel noted the large number of failures to replicate excess heat and the greater inconsistency of reports of nuclear reaction byproducts expected by established conjecture. Nuclear fusion of the type postulated would be inconsistent with current understanding and, if verified, would require established conjecture, perhaps even theory itself, to be extended in an unexpected way. The panel was against special funding for cold fusion research, but supported modest funding of \"focused experiments within the general funding system.\" Cold fusion supporters continued to argue that the evidence for excess heat was strong, and in September 1990 the National Cold Fusion Institute listed 92 groups of researchers from 10 different countries that had reported corroborating evidence of excess heat, but they refused to provide any evidence of their own arguing that it could endanger their patents. However, no further DOE nor NSF funding resulted from the panel's recommendation. By this point, however, academic consensus had moved decidedly toward labeling cold fusion as a kind of \"pathological science\".\n\nIn March 1990 Dr. Michael H. Salamon, a Utah physicist, and nine co-authors reported negative results. University faculty were then \"stunned\" when a lawyer representing Pons and Fleischmann demanded the Salamon paper be retracted under threat of a lawsuit. The lawyer later apologized; Fleischmann defended the threat as a legitimate reaction to alleged bias displayed by cold-fusion critics.\n\nIn early May 1990 one of the two A&M researchers, Kevin Wolf, acknowledged the possibility of spiking, but said that the most likely explanation was tritium contamination in the palladium electrodes or simply contamination due to sloppy work. In June 1990 an article in \"Science\" by science writer Gary Taubes destroyed the public credibility of the A&M tritium results when it accused its group leader John Bockris and one of his graduate students of spiking the cells with tritium. In October 1990 Wolf finally said that the results were explained by tritium contamination in the rods. An A&M cold fusion review panel found that the tritium evidence was not convincing and that, while they couldn't rule out spiking, contamination and measurements problems were more likely explanations, and Bockris never got support from his faculty to resume his research.\n\nOn 30 June 1991 the National Cold Fusion Institute closed after it ran out of funds; it found no excess heat, and its reports of tritium production were met with indifference.\n\nOn 1 January 1991, Pons left the University of Utah and went to Europe. In 1992, Pons and Fleischman resumed research with Toyota Motor Corporation's IMRA lab in France. Fleischmann left for England in 1995, and the contract with Pons was not renewed in 1998 after spending $40 million with no tangible results. The IMRA laboratory stopped cold fusion research in 1998 after spending £12 million. Pons has made no public declarations since, and only Fleischmann continued giving talks and publishing papers.\n\nMostly in the 1990s, several books were published that were critical of cold fusion research methods and the conduct of cold fusion researchers. Over the years, several books have appeared that defended them. Around 1998, the University of Utah had already dropped its research after spending over $1 million, and in the summer of 1997, Japan cut off research and closed its own lab after spending $20 million.\n\nIn the mid-1990s there was heated debate among scientists that promoted the room-temperature fusion and those who observed that isotopic shifts and heavy-element transmutations pointed not to fusion but to some sort of neutron-induced reaction.\n\nIn 1997, theorist Lewis Larsen looked at some of this data and suspected that a neutronization process was occurring in low-energy nuclear reactions (LENR). Physicist Allan Widom joined Larsen's team in 2004, and in 2006 they published a theory in the \"European Physical Journal C – Particles and Fields.\"\n\nThe Widom–Larsen theory is consistent with existing physics. Their theory explains nuclear reactions based on creation of ultra-low-momentum neutrons and their recapturing. The reaction rate are based on collective many-body interactions and not on few-body interactions that had been presumed for Fleischmann–Pons experiment.\n\nUltimately, although their experiment did not show what Fleischmann and Pons thought it did, it prompted new exciting venues of research that may highly expand applications of theoretical particle physics.\n\n"}
{"id": "38264942", "url": "https://en.wikipedia.org/wiki?curid=38264942", "title": "Gadoleic acid", "text": "Gadoleic acid\n\nGadoleic acid (20:1n-11) is an unsaturated fatty acid. It is a prominent component of some fish oils including cod liver oil. It is one of a number of eicosenoic acids.\n"}
{"id": "2211529", "url": "https://en.wikipedia.org/wiki?curid=2211529", "title": "Genset trailer", "text": "Genset trailer\n\nA genset trailer is a range extending device for use with battery electric vehicles consisting of an internal combustion engine and an electric generator (collectively called a genset). They run on traditional fuels such as gasoline or diesel and are sized to provide the continuous power requirements for the vehicle they are used with. Most small to midsized passenger vehicles would require for unlimited freeway travel using fuel. Larger vehicles could require or more of power depending on how heavy or un-aerodynamic they happen to be. This essentially converts an electric vehicle into a series-hybrid.\n\nOne such trailer was the AC Propulsion backtracking Long Ranger range extending gasoline-fueled trailer, making it a gasoline-electric series plug-in hybrid electric vehicle. This trailer used a Kawasaki engine with a fuel tank and achieved . It was rated at DC output and could maintain .\n\nThe disadvantages of genset trailers are gaseous emissions from the engine, system maintenance, the additional cost of fuel, and storage when it is not in use on the road. Also, since it is a trailer, it is impractical for some motorists. \n\nAdvantages of a genset trailer include improved efficiency over a dedicated series hybrid when the trailer is disconnected for local travel, additional cargo space and a simpler and more cost effective basic vehicle. Similar to a series hybrid such a combined vehicle would require fewer batteries to achieve an effectively longer range. \n\n\n"}
{"id": "8251931", "url": "https://en.wikipedia.org/wiki?curid=8251931", "title": "Hydrotrope", "text": "Hydrotrope\n\nA hydrotrope is a compound that solubilises hydrophobic compounds in aqueous solutions (by means other than micellar solubilization). Typically, hydrotropes consist of a hydrophilic part and a hydrophobic part (like surfactants) but the hydrophobic part is generally too small to cause spontaneous self-aggregation.<br>Hydrotropes do not have a critical concentration above which self-aggregation 'suddenly' starts to occur (as found for micelle- and vesicle-forming surfactants, which have a critical micelle concentration or cmc and a critical vesicle concentration or cvc, respectively). Instead, some hydrotropes aggregate in a step-wise self-aggregation process, gradually increasing aggregation size. However, many hydrotropes do not seem to self-aggregate at all, unless a solubilisate has been added. Hydrotropes are in use industrially. Examples of hydrotropes include urea, tosylate, cumenesulfonate and xylenesulfonate.\n\nThe term \"hydrotropy\" was originally put forward by Carl Neuberg to describe the increase in the solubility of a solute by the addition of fairly high concentrations of alkali metal salts of various organic acids. However, the term has been used in the literature to designate non-micelle-forming substances, either liquids or solids, organic or inorganic, capable of solubilizing insoluble compounds.\n\nThe chemical structure of the conventional Neuberg’s hydrotropic salts (proto-type, sodium benzoate) consists generally of two essential parts, an anionic group and a hydrophobic aromatic ring or ring system. The anionic group is involved in bringing about high aqueous solubility, which is a prerequisite for a hydrotropic substance. The type of anion or metal ion appeared to have a minor effect on the phenomenon. On the other hand, planarity of the hydrophobic part has been emphasized as an important factor in the mechanism of hydrotropic solubilization\n\nTo form a hydrotrope, an aromatic hydrocarbon solvent is sulfonated creating an aromatic sulfonic acid. It is then neutralized with a base.\n\nAdditives may either increase or decrease the solubility of a solute in a given solvent. These salts that increase solubility are said to ‘salt in’ the solute and those salts that decrease the solubility ‘salt out’ the solute. The effect of an additive depends very much on the influence, it has on the structure of water or its ability to compete with the solvent water molecules.\nA convenient quantitation of the effect of a solute additive on the solubility of another solute may be obtained by the\nSetschetow equation:\n\nHydrotropes are in use industrially and commercially in cleaning and personal care product formulations to allow more concentrated formulations of surfactants. About 29,000 metric tons are produced (i.e., manufactured and imported) annually in the US. Annual production (plus importation) in Europe and Australia is approximately 17,000 and 1,100 metric tons, respectively. \nCommon products containing a hydrotropes include laundry detergents, surface cleaners, dishwashing detergents, liquid soaps, shampoos and conditioners. They are coupling agents, used at concentrations from 0.1-15% to stabilize the formula, modify viscosity and cloud-point, reduce phase separation in low temperatures, and limit foaming.\n\nAdenosine triphosphate (ATP) has been shown to be a hydrotrope able to prevent aggregation of proteins at normal physiologic concentrations and to be approximately an order of magnitude more effective than sodium xylene sulfonate in a classic hydrotrope assay. The hydrotrope activity of ATP was shown to be independent of its activity as an \"energy currency\" in cells.\n\nHydrotropes have a low bioaccumulation potential, as the octanol:water partition coefficient is <1.0. Studies have found hydrotopes to be very slightly volatile, with vapour pressures <2.0x10-5 Pa. They are aerobically biodegradable. Removal via the secondary wastewater treatment process of activated sludge is >94%. Acute toxicity studies on fish show an LC50 >400 mg active ingredient (a.i.)/L. For Daphnia, the EC50 is >318 mg a.i./L. The most sensitive species is green algae with EC50 values in the range of 230–236 mg a.i./ L and No Observed Effect Concentrations (NOEC) in the range of 31–75 mg a.i./L. The aquatic Predicted No Effect Concentration (PNEC) was found to be 0.23 mg a.i./L. The Predicted Environmental Concentration (PEC)/PNEC ratio has been determined to be < 1 and, therefore, hydrotropes in household laundry and cleaning products have been determined to not be an environmental concern.\n\nAggregate exposures to consumers (direct and indirect dermal contact, ingestion, and inhalation) have been estimated to be 1.42 ug/Kg bw/day. \nAnimal studies do not suggest sensitization, but there are no reliable human data available. Calcium xylene sulfonate and sodium cumene sulfonate have been shown to cause temporary, slight eye irritation in animals. Studies have not found hydrotropes to be mutagenic, carcinogenic or have reproductive toxicity.\n"}
{"id": "32013958", "url": "https://en.wikipedia.org/wiki?curid=32013958", "title": "Kosovo A Power Station", "text": "Kosovo A Power Station\n\nKosovo A Power Station is a lignite power station with five units at Obilić, Kosovo. It is the second largest power station in Kosovo with capacity of 650 MW after Kosovo B Power Station. It is described as the worst single-point source of pollution in Europe and it is expected to be closed by 2017.\n\nKosovo A Power Station was opened in 1962. It was operated by EPS Surface Mining Kosovo and EPS TPP Kosovo until the end of Kosovo War. After UNMIK administration was established in Kosovo on 1 July 1999, Elektroprivreda Srbije (EPS) lost its access to the local coal mines and power plants, including Kosovo A and Kosovo B power plants.\n\nSince then, it is operated by Korporata Energjetike e Kosovës (KEK).\n\nIn December 2015, US-based company ContourGlobal signed a memorandum of understanding with the Government of Kosovo, to build a new $1.06 billion worth power plant to replace 45-year old Kosovo A Power Station. The construction is expected to start in 2018.\n\n\nOn 6 June 2014, the power station exploded killing two people and injuring 13 others. The station was then subsequently shut down. The cause of the explosion was due to the explosion of hydrogen tank located in a separate part of the power station from the generator.\n\n\n"}
{"id": "16921412", "url": "https://en.wikipedia.org/wiki?curid=16921412", "title": "Leaching (chemistry)", "text": "Leaching (chemistry)\n\nLeaching is the process of extracting substances from a solid by dissolving them in a liquid, either naturally or through an industrial process. In the chemical processing industry, leaching has a variety of commercial applications, including separation of metal from ore using acid, and sugar from sugar beets using hot water.\n\nAnother term for this is lixiviation, or the extraction of a soluble particle from its constituent parts.\n\nIn a typical leaching operation, the solid mixture to be separated consists of particles, inert insoluble carrier A and solute B. The solvent, C, is added to the mixture to selectively dissolve B. The overflow from the stage is free of solids and consists of only solvent C and dissolved B. The underflow consists of slurry of liquid of similar composition in the liquid overflow and solid carrier A. In an ideal leaching equilibrium stage, all the solute is dissolved by the solvent; none of the carrier is dissolved. The mass ratio of the solid to liquid in the underflow is dependent on the type of equipment used and properties of the two phases.\n\nLeaching is the process by which inorganic, organic contaminants or radionuclides are released from the solid phase into the water phase under the influence of mineral dissolution, desorption, complexation processes as affected by pH, redox, dissolved organic matter and (micro)biological activity. The process itself is universal, as any material exposed to contact with water will leach components from its surface or its interior depending on the porosity of the material considered.\n\nOne such reaction is:\n\nMany biological organic and inorganic substances occur in a mixture of different components in a solid. In order to separate the desired solute constituent or remove an undesirable solute component from the solid phase, the solid is brought into contact with a liquid. The solid and liquid are in contact, and the solute or solutes can diffuse from the solid into the solvent, resulting in separation of the components originally in the solid. This separation process is called \"liquid-solid leaching\" or simply \"leaching\". Because in leaching the solute is being extracted from the solid this is also called \"extraction\". In leaching, when an undesirable component is removed from a solid with water, the process is called \"washing\".\n\nIn the biological and food processing industries, many products are separated from their original natural structure by liquid-solid leaching. An important process for example is the leaching of sugar from sugar beets with hot water. In the production of vegetable oils, organic solvents such as hexane, acetone, or ether are used to extract oil from nuts, beans, and seeds.\n\nIn the pharmaceutical industry, many different pharmaceutical products are obtained by leaching plant roots, leaves, and stems.\n\nLeaching is extensively used in metal processing industries. The useful metal may occur in mixtures with very large amounts of undesirable constituents, and leaching is used to remove the metals as soluble salts. The use of acids is prevalent in the metal processing industry. Sulfates are normally used to remove metals from the solid phase. These processes result in harmful sulfate-rich environmental byproducts. Because acids solubilize metals, large-scale release of heavy-metal-rich acidic mine drainage (AMD) can thereby occur.\n\nAs mentioned above when leaching a solid with a liquid the desired solid goes to the liquid phase while undesired solid remains. The removal of the solid as the liquid dissolves into the particle leads to a diameter of unleached core that shrinks with time. A mathematical model can be used for this process, derived using Fick's law of diffusion taking pore diffusion as rate limiting step: \n\nSome recent work has been done to see if organic acids can be used to leach lithium and cobalt from spent batteries with some success. Experiments performed with varying temperatures and concentrations of malic acid show that the optimal conditions are 2.0 m/L of organic acid at a temperature of 90 °C. The reaction had an overall efficiency exceeding 90% with no harmful byproducts.\n\nThe same analysis with citric acid showed similar results with an optimal temperature and concentration of 90 °C and 1.5 molar solution of citric acid.\n\n"}
{"id": "53477203", "url": "https://en.wikipedia.org/wiki?curid=53477203", "title": "List of Fusor examples", "text": "List of Fusor examples\n\nFusors have been theoretically studied at multiple institutions, including: Kyoto University, and Kyushu University. Researchers meet annually at the US-Japan Workshop on Inertial Electrostatic Confinement Fusion. Listed here, are actual machines built.\n\n\nA number of amateurs have built working fusors and detected neutrons. Many fusor enthusiasts connect on forums and message boards online. Below are some examples of working fusors. \n"}
{"id": "26318057", "url": "https://en.wikipedia.org/wiki?curid=26318057", "title": "Louis Glavis", "text": "Louis Glavis\n\nLouis Russell Glavis (1883–1971) was an American lawyer and an employee of the United States Department of the Interior. He was a prominent figure in the 1910 Pinchot-Ballinger Controversy; \na political dispute between President Taft's Secretary of Interior, Richard Ballinger and conservationist Gifford Pinchot over Governmental conservation policies.\n\nIn 1909, Glavis was an agent of the Department's General Land Office's Field Division in the northwestern United States and was based in Portland, Oregon. He provided Pinchot with information about land deals in Alaska which he, Glavis, believed were illegal. Pinchot, in turn, accused Secretary Ballinger of providing Clarence Cunningham's syndicate of land claims that did not respect Roosevelt's conservation policies; policies Taft claimed to uphold in his administration. These accusations led to the controversy. After a Senate hearing, Ballinger was exonerated and Glavis was fired on the grounds of insubordination by President Taft who supported Ballinger's position. \n\nIn 1933, Glavis was rehired by incoming Secretary of Interior Harold L. Ickes as Chief of the Division of Investigation. This Division was responsible for investigating charges of corruption involving all the Department's many business contracts. However, after three years, Ickes found Glavis to have a generally insubordinate nature. Glavis would conduct unauthorized surveillance of Department employees he felt were disloyal, for example. Ultimately, Glavis resigned from the Department in the summer of 1936 and was transferred to a Senate investigating committee. Later, Ickes wrote in his diary that he felt disappointed in Glavis, whom Ickes had once admired for his honesty; and that he had been unfairly unjust toward Ballinger, someone he had opposed in 1910, as he had also had to act in a similar way with Glavis. In the biographical supplement that came with the Ickes diaries that were published in 1953, it is mentioned that Glavis had returned to a private law practice.\n\n"}
{"id": "17827134", "url": "https://en.wikipedia.org/wiki?curid=17827134", "title": "Measured depth", "text": "Measured depth\n\nIn the oil industry measured depth (commonly referred to as MD, or just the depth), is the length of the borehole. In conventional vertical wells, this coincides with the true vertical depth, but in directional or horizontal wells, especially those using extended reach drilling, the two can deviate greatly. For example, at the time of writing (2012) a borehole in Odoptu field, Sakhalin-I, has the greatest measured depth of any borehole at 12,345 m, but most of this is horizontal, giving it a true vertical depth of only 1,784 m. For comparison, the Kola Superdeep Borehole has a slightly shorter measured depth at 12,262 m, but since this is a vertical borehole, this is also equal to the true vertical depth, making the Kola Superdeep Borehole deeper by a factor of 6.9.\n"}
{"id": "37283348", "url": "https://en.wikipedia.org/wiki?curid=37283348", "title": "Methylpyridinium", "text": "Methylpyridinium\n\nMethylpyridinium is a chemical compound which is the quaternary ammonium compound derived from the \"N\"-methylation of pyridine. It is found in some coffee products. It is not present in unroasted coffee beans, but is formed during roasting from its precursor chemical, trigonelline. It is under investigation by scientists regarding its potential anti-carcinogenic properties, particularly an effect on colon cancer.\n\nThe chloride of N-methylpyridinium behaves as an ionic liquid in the molten state. Its properties with different mixtures of zinc chloride have been characterised by several authors in the temperature range 150 – 200 °C (423 – 473 K).\n\n"}
{"id": "50995575", "url": "https://en.wikipedia.org/wiki?curid=50995575", "title": "Molly R. Morris", "text": "Molly R. Morris\n\nMolly R. Morris is an American behavioral ecologist who has worked with treefrogs and swordtail fishes in the areas of alternative reproductive tactics and sexual selection.\n\nMorris received a Bachelor of Arts from Earlham College and a PhD from Indiana University. As a National Science Foundation postdoctoral fellow at the University of Texas at Austin, her work with Mike Ryan demonstrated equal fitnesses between alternative reproductive tactics in a species of swordtail fish. She joined the faculty at Ohio University in 1997, where she is now a professor in the Department of Biological Sciences. She is also the Associate Editor for the journal \"Behavior.\" Her publication credits include multiple papers on Animal behavior and Ecology\".\" Her current research relates to diabetes, as well as behavioral ecology, using the swordtail fish Xiphophorus as a model organism.\n\nMorris is married to Kevin de Queiroz, an evolutionary biologist at the Smithsonian Institution's National Museum of Natural History.\n\n"}
{"id": "3163925", "url": "https://en.wikipedia.org/wiki?curid=3163925", "title": "Niobium carbide", "text": "Niobium carbide\n\nNiobium carbide (NbC and NbC) is an extremely hard refractory ceramic material, commercially used in tool bits for cutting tools. It is usually processed by sintering and is a frequent additive as grain growth inhibitor in cemented carbides. It has the appearance of a brown-gray metallic powder with purple lustre. It is highly corrosion resistant.\n\nNiobium carbide is a frequent intentional product in microalloyed steels due to its extremely low solubility product in austenite, the lowest of all the refractory metal carbides. This means that micrometre-sized precipitates of NbC are virtually insoluble in steels at all processing temperatures and their location at grain boundaries helps prevent excessive grain growth in these steels. This is of enormous benefit, and the cornerstone of microalloyed steels, because it is their uniform, very fine grain size that ensures both toughness and strength.\nThe only commonly occurring compound with a lower solubility and hence, greater potential for restricting the grain growth of steels is titanium nitride.\n\nDepending on grain size, niobium carbide may burn at 200-800 °C in air. A layer of niobium carbide can be created by chemical vapor deposition. Zirconium carbide and niobium carbide can be used as refractory coatings in nuclear reactors.\n\nNiobium carbide can be produced by the heating of Niobium oxide in a vacuum at 1800 °C and adding coke.\n\nNiobium carbide has an Young's modulus of approximately 452GPa, and a shear modulus of 182GPa. It has a Poisson's ratio of 0.227.\n\nNiobocarbide - the naturally occurring form of niobium carbide - shall be regarded as an extremely rare mineral.\n"}
{"id": "26897895", "url": "https://en.wikipedia.org/wiki?curid=26897895", "title": "Nuclear power plant emergency response team", "text": "Nuclear power plant emergency response team\n\nA nuclear power plant emergency response team (ERT) is an incident response team composed of plant personnel and civil authority personnel specifically trained to respond to the occurrence of an accident at a nuclear power plant.\n\nEach nuclear power plant is required to have a detailed emergency plan. In the event of a potential accident (as defined by the International Nuclear Event Scale), the ERT personnel are notified by beeper and have a set time limit for reporting to their duty station.\n\nPotential duty stations include:\n\nIn the United States, ERT personnel are required to train twice a year and typically train four times. The Federal Emergency Management Agency (FEMA) (with support from the Nuclear Regulatory Commission (NRC) and other agencies) grades some of the drills. The drills normally are not announced in advance so as to simulate \"surprise\" conditions.\n\n\n"}
{"id": "28095034", "url": "https://en.wikipedia.org/wiki?curid=28095034", "title": "Online Electric Vehicle", "text": "Online Electric Vehicle\n\nThe Online Electric Vehicle (OLEV) is an electric vehicle that charges wirelessly while moving using electromagnetic induction (the wireless transfer of power through magnetic fields). It functions by using a segmented \"recharging\" road that induces a current in \"pick-up\" modules on the vehicle.\n\nOLEV is the first public transport system that used a \"recharging\" road and was first launched on March 9, 2010 by The Korea Advanced Institute of Science and Technology (KAIST).\n\nThe OLEV system is split into two main parts: the segmented \"recharging\" road and the \"pick-up\" modules on the vehicle.\n\nIn the \"recharging\" road, slim W-shaped ferrite cores (magnetic cores used in induction) are buried 30 cm underground in a fish bone like structure. Power cables are wrapped around the center of the fish bone structures to make the \"primary coils\". This design combines the magnetic fields of the two sides of the cables and shapes the fields in a way that maximizes induction. Moreover, the primary coils are placed in segments across certain spans of the road so that only about 5% to 15% of the road needs to be remodeled. To power the primary coils, the cables are attached to the South Korean national power grid through a power inverter. The inverter accepts 60Hz 3-phase 380 or 440 voltage from the grid to generate 20kHz of AC electricity into the cables. In turn, the cables create a 20 kHz magnetic field that sends flux through the slim ferrite cores to the pick-ups on the OLEV.\n\nAttached beneath the vehicle, are \"pick-up\" modules, or the secondary coils, that consist of wide W-shaped ferrite cores with wires wrapped around the center. When the pick-ups \"pick up\" the flux from the primary coils, each pick-up gains about 17 kW of power from the induced current. This power is sent to the electric motor and battery through a regulator (a managing device that can distribute power based on need), thereby charging the OLEV wirelessly.\n\nAs seen in the table above, the generation 1 OLEV lacks a realistic margin for error. The lower current means a smaller magnetic field and requires the secondary coil to be very close to the floor, which can be an issue while driving. Moreover, if the primary and secondary coils are vertically misaligned by a distance over 3mm, the power efficiency drops greatly.\n\nTo fix these issues, KAIST came up with the generation 2 OLEV. In the gen 2 OLEV, the current in the primary coil was doubled to create a stronger magnetic field that allows for a larger air gap. The ferrite cores in the primary coils were changed to a U shape and the cores in the secondary coil were changed a flat board shape to pick-up as much flux as possible. This design allows the vertical misalignment to be about 20 cm with a 50% power efficiency. However, the U shaped cores also require return cables which bumps up the cost of production. Overall, the gen 2 made up for the gen 1's margins but, was more costly.\n\nIn response to the cost issue of gen 2, the third generation OLEV was developed. The third generation OLEV, uses ultra-slim W-shaped ferrite cores in the primary coil to reduce the amount of ferrite used to 1/5 of gen 2 and to remove the need of return cables. The secondary coil uses a thicker variation of the w-shaped cores as a way to make up for the lesser area for the magnetic flux to flow through compared to gen 2. Overall, the gen 3 OLEV made up for the gen 1's small margins and gen 2's increased cost.\n\nBenefits\nIssues\n\nKAIST announced it has applied for more than 120 patents in connection with OLEV.\n\nIn November 2010, KAIST's Road-Embedded Rechargers was selected as \"Time\"'s \"The 50 Best Inventions of 2010\".\n\n"}
{"id": "52398237", "url": "https://en.wikipedia.org/wiki?curid=52398237", "title": "Ontario Power Company Generating Station", "text": "Ontario Power Company Generating Station\n\nThe Ontario Power Company Generating Station is a former generating station located along the Niagara River in Niagara Falls, Ontario, Canada, slightly upstream from the Rankine Generating Station.\n\nIn 1890, the US Niagara Falls Hydraulic Power and Manufacturing Company and its subsidiary Cataract Company formed the International Niagara Commission composed of experts, to analyze proposals to harness Niagara Falls on the US/Canada border to generate power. They settled on alternating current, or AC, electricity as being the preferred transmission method and, after going through many proposals. In 1893, they awarded the generating contract to Westinghouse Electric with further transmission lines and transformer contracts awarded to General Electric. Work began in 1893 and in November 1896, power generated from Niagara Falls at the Edward Dean Adams Power Plant was being sent to Buffalo, New York as well as the Niagara Falls plants of the Pittsburgh Reduction Company which needed large quantities of cheap electricity for smelting aluminum.\n\nA similar set of events were happening on the Canadian side of the falls. In June 1887, recognizing an opportunity, the Ontario Power Company of Niagara Falls was incorporated in Canada “to supply manufacturers, corporations, and persons with water, hydraulic, electric or other power.” While its operations were in Queen Victoria Park in Niagara Falls, Ontario, its executive office was in Buffalo with the following officers: Albright, president; Francis V. Greene, vice-president; and Robert C. Board, secretary and treasurer.\n\nIn 1903, the Company obtained an agreement with the Commissioners of the Queen Victoria Niagara Falls Park that allowed the company to develop at least 180,000 horsepower of electricity. The company built its hydroelectric generating plant, which opened in 1905, at the base of the Horseshoe Falls just above river level. The plant had 15 generators, which produced 203,000 horsepower of electric power, and was designed and built by L.L. Nunn. In 1904, Albright hired Buffalo architect E. B. Green to design the Beaux-Arts \"Ontario Power Company\" buildings, Murray Street at Buchanan Avenue, including the Entrance Pavilion, Spillway Building, Office & Transformer Station, Gate House, Screen House, and ‘’Ontario Power Company Generating Station’’ at river level.\n\nThe hydroelectric generating plant worked by allowing water to enter the generating station from an inlet located one mile upstream of Niagara Falls, near Dufferin Islands, and was then brought to the plant through buried conduit pipes and steel penstocks tunneled through the rock. The conduits, two steel and one wooden (bound with iron hoops and encased in cement), ran underground 6,180 feet (1,884 meters) to the top of the generating station. There, each conduit connected with six penstocks, six feet in diameter. At the point where the conduits and the penstocks join, there was a section which turned upwards into a spillway, called a surge tank. The surge tanks served to reduce fluctuations in heat and pressure during both the increase and decrease of loads. The open spillways sent any excess water to the Niagara River if the load suddenly reduced, which prevented any unwanted rise in pressure.\n\nFrom the distributing station, the transmission lines carried power at 60,000 volts each with a capacity of 40,000 kilowatts, running over a right of way that was 300 feet wide and 32,000 feet long. This ran north to an area down the Niagara River known as Devil's Hole, where they then crossed the Niagara River into New York State across a 1,300 feet long span. In addition to the high tension feeders, there were approximately 30 miles of lines serving Canadian customers at generator voltage.\n\nThe power that was transmitted to New York State was then sold in bulk to Niagara Lockport and Ontario Power Company, a New York company, which was then distributed to individual customers. The largest individual consumers of power from these lines included several entities with direct ties to Albright: The Lackawanna Steel Company, Empire State Railway, New York Central Railroad, the Shenandoah Steel Wire Company, the Syracuse Rapid Transit Railway Company, the Lockport Gas and Electric Light Company, the Auburn Light Heat and Power Company, the Erie Railroad Company, and the Genesee County Electric Light Power and Gas Company.\n\nThe plant continued to operate until 1999 when Ontario Power Generation (formerly Ontario Hydro) decommissioned the Ontario Power Company Generating Station from service in order to accommodate the construction of Niagara Fallsview Casino Resort, built on the former transformer building location.\n\nAs of 2015, the 1905 Generating Station along the Niagara River edge is owned by the Niagara Parks Commission, and sits abandoned.\n"}
{"id": "51351115", "url": "https://en.wikipedia.org/wiki?curid=51351115", "title": "Ozone Transport Commission", "text": "Ozone Transport Commission\n\nThe Ozone Transport Commission (OTC) is a multi-state organization founded in 1991 and created under the Clean Air Act. They are responsible for advising EPA on air pollution transport issues and for developing and implementing regional solutions to the ground-level ozone problem in the Northeast and Mid-Atlantic regions. OTC has no regulatory authority, but assists its members in developing model regulations for implementation at the state level. OTC also manages a regional planning organization MANE-VU (Mid-Atlantic Northeast Visibility Union), which is charged with regional multi-pollutant air quality planning.\n\n\n"}
{"id": "54305952", "url": "https://en.wikipedia.org/wiki?curid=54305952", "title": "Pentamethylantimony", "text": "Pentamethylantimony\n\nPentamethylantimony or pentamethylstiborane is an organometalllic compound containing five methyl groups bound to an antimony atom with formula Sb(CH). It is an example of a hypervalent compound. The molecular shape is trigonal bipyramid. Some other antimony(V) organometallic compounds include pentapropynylantimony (Sb(CCCH)) and pentaphenyl antimony (Sb(CH)). Other known pentamethyl-pnictides include pentamethylbismuth and pentamethylarsenic.\n\nPentamethylantimony can be made by reacting Sb(CH)Br with two equivalents of methyl lithium. Another production route is to convert trimethylstibine to the trimethyl antimony dichloride, and then replace the chlorine with methyl groups with methyl lithium.\n\nPentamethylantimony is colourless. At -143°V it crystallizes in the orthorhombic system with space group \"Ccmm\". Unit cell dimensions are a=6.630 Å b=11.004 Å c=11.090 Å. There are four formula per unit cell. Unit cell volume is 809.1 Å. The trigonal bipyramid shape has three equatorial positions for carbon, and two axial positions at the peaks of the pyramids. The length of the antimony-carbon bond is around 214 pm for equatorial methyl groups and 222 pm for the axial positions. The bond angles are 120° for ∠C-Sb-C across the equator, and 90° for ∠C-Sb-C between equator and axis. The molecules rapidly change carbon atom position, so that in NMR spectrum as low as −100°C, there is only one kind of hydrogen position.\n\nPentamethylantimony is more stable than pentamethylbismuth, because in lower energy trimethylbismuth, the non-bonding pair of electrons is more shielded due to the f-electrons and the lanthanoid contraction. Trimethylantimony is higher in energy, and thus less is released in a decomposition of pentamethylantimony. Pentamethylantimony can be stored as a liquid in clean glass at room temperature.\n\nPentamethylantimony melts at -19°C. Although it decomposes when boiling is attempted and can explode, it has a high vapour pressure at 8 mmHg at 25°C.\n\nThere are two absorption bands in the ultraviolet at 2380 and 2500 Å.\n\nPentamethylantimony reacts with methyl lithium to yield a colourless lithium hexamethylantimonate in tetrahydrofuran.\n\nPentamethylantimony reacts with silsesquioxanes to yield tetramethylstibonium silsesquioxanes. eg (\"cyclo\"-CH)SiO(OH) yields (\"cyclo\"-CH)SiO(OSB(CH)). The reaction happens quickly when there are more than two OH groups.\n\nPhosphonic acids and phosphinic acids combine with pentamethylantimony to yield compounds like (CH)SbOP(O)Ph, (CH)SbOP(O)(OH)Ph and (CH)SbOP(O)(OH), eliminating methane.\n\nStannocene Sn(CH) combines with pentamethylantimony to produce bis(tetramethylstibonium)tetracyclopentadienylstannate ([(CH)Sb]Sn(CH)).\n\nPentamethylantimony reacts with many very weak acids to form a tetramethylstibonium salt or tetramethylstibonium derivative with the acid. Such acids include water (HO), alcohols, thiols, phenol, carboxylic acids, hydrogen fluoride, thiocyanic acid, hydrazoic acid, difluorophosphoric acid, thiophosphinic acids, and alkylsilols.\n\nWith halogens, pentamethylantimony has one or two methyl groups replaced by the halogen atoms. Lewis acids also react to form tetramethyl stibonium salts, including [(CH)Sb]TlBr, [(CH)Sb][CHSbCl], \n\nPentamethylantimony reacts with the surface of silica to coat it with Si-O-Sb(CH) groups. Over 250°C this decomposes to Sb(CH) and leaves methyl groups attached to the silica surface.\n"}
{"id": "1007733", "url": "https://en.wikipedia.org/wiki?curid=1007733", "title": "Regenerative fuel cell", "text": "Regenerative fuel cell\n\nA regenerative fuel cell or reverse fuel cell (RFC) is a fuel cell run in reverse mode, which consumes electricity and chemical B to produce chemical A. By definition, the process of any fuel cell could be reversed. However, a given device is usually optimized for operating in one mode and may not be built in such a way that it can be operated backwards. Standard fuel cells operated backwards generally do not make very efficient systems unless they are purpose-built to do so as with high-pressure electrolysers, regenerative fuel cells, solid-oxide electrolyser cells and unitized regenerative fuel cells.\n\nA hydrogen fueled proton exchange membrane fuel cell, for example, uses hydrogen gas (H) and oxygen (O) to produce electricity and water (HO); a regenerative hydrogen fuel cell uses electricity and water to produce hydrogen and oxygen.\n\nWhen the fuel cell is operated in regenerative mode, the anode for the electricity production mode(fuel cell mode) becomes the cathode in the hydrogen generation mode (reverse fuel cell mode), and vice versa. When an external voltage is applied, water at the cathode side will undergo electrolysis to form hydrogen and oxide ions; oxide ions will be transported through the electrolyte to anode where it can be oxidised to form oxygen. In this reverse mode, the polarity of the cell is opposite to that for the fuel cell mode. \nThe following reactions describe the chemical process in the hydrogen generation mode:\n\nAt cathode: HO + 2e → H + O\n\nAt anode: O → 1/2O + 2e\n\nOverall: HO → 1/2O + H\n\nOne example of RFC is solid oxide regenerative fuel cell. Solid oxide fuel cell operates at high temperatures with high fuel-to-electricity conversion ratios and it is a good candidate for high temperature electrolysis. Less electricity is required for electrolysis process in SORFC due to high temperature.\n\nThe electrolyte can be O conducting and/or proton(H) conducting. The state of the art for O conducting yttria stabilized zirconia(YSZ) based SORFC using Ni–YSZ as the hydrogen electrode and LSM (or LSM–YSZ) as the oxygen electrode has been actively studied. Dönitz and Erdle reported on the operation of YSZ electrolyte cells with current densities of 0.3 A cm and 100% Faraday efficiency at only 1.07 V. The recent study by researchers from Sweden shows that ceria-based composite electrolytes, where both proton and oxide ion conductions exist, produce high current output for fuel cell operation and high hydrogen output for electrolysis operation. Zirconia doped with scandia and ceria (10Sc1CeSZ) is also investigated as potential electrolyte in SORFC for hydrogen production at intermediate temperatures(500-750 °C).It is reported that 10Sc1CeSZ shows good behavior and produces high current densities, with suitable electrodes.\n\nCurrent density–voltage ( j–V) curves and impedance spectra are investigated and recorded. Impedance\nspectra are realized applying an ac current of 1–2A RMS (root-mean-square) in the frequency range from 30 kHz\nto 10 Hz. Impedance spectra shows that the resistance is high at low frequencies(<10 kHz) and near zero at high frequencies(>10 kHz). Since high frequency corresponds to electrolyte activities, while low frequencies corresponds to electrodes process, it can be deduced that only a small fraction of the overall resistance is from the electrolyte and most resistance comes from anode and cathode. Hence, developing high performance electrodes are essential for high efficiency SORFC. Area specific resistance(ASR) can be obtained from the slope of j-V curve. Commonly used/tested electrodes materials are nickel/zirconia cermet (Ni/YSZ) and lanthanum-substituted strontium titanate/ceria composite for SORFC cathode, and lanthanum strontium manganite (LSM) for SORFC anode. Other anode materials can be lanthanum strontium ferrite\n(LSF), lanthanum strontium copper ferrite(LSCuF) and lanthanum strontium cobalt ferrite (LSCoF). Studies show that Ni/YSZ electrode was less active in reverse fuel cell operation than in fuel cell operation,and this can be attributed to a diffusion-limited process in the electrolysis direction, or its susceptibility to aging in a high-steam environment,\nprimarily due to coarsening of nickel particles. Therefore, alternative materials such as the titanate/ceria composite (La0.35Sr0.65TiO3–Ce0.5La0.5O2−δ)or (La0.75Sr0.25)0.95Mn0.5Cr0.5O3 (LSCM) have been proposed electrolysis cathodes.Both LSF and LSM/YSZ are reported as good anode candidates for electrolysis mode. \nFurthermore,higher operation temperature and higher absolute humidity ratio(AH) can result in lower ASR.\n\n\n"}
{"id": "1115299", "url": "https://en.wikipedia.org/wiki?curid=1115299", "title": "Safety testing of explosives", "text": "Safety testing of explosives\n\nThe safety testing of explosives involves the determination of various properties of the different energetic materials that are used in commercial, mining, and military applications. It is highly desirable to measure the conditions under which explosives can be set off for several reasons, including: safety in handling, safety in storage, and \nsafety in use.\n\nIt would be very difficult to provide an absolute scale for sensitivity with respect to the different properties of explosives. Therefore, it is generally required that one or more compounds be considered a standard for comparison to those compounds being tested. For example, PETN is considered to be a primary explosive by some individuals, and a secondary explosive by others. As a general rule, PETN is considered to be either a relatively insensitive primary explosive, or one of the most sensitive secondary explosives. PETN may be detonated by striking with a hammer on a hard steel surface (a very dangerous thing to do), and is generally considered the least sensitive explosive with which this may be done. For these facts and other reasons, PETN is considered one standard by which other explosives are gauged.\n\nAnother explosive that is used as a calibration standard is TNT, which was afforded the arbitrary Figure of Insensitivity of 100. Other explosives could then be compared against this standard.\n\nBecause there are different ways to set off explosives, there are several different components to the safety testing of explosives:\n\n\nUsed together, these numbers may be used to determine the potential threats afforded by energetic materials when employed in the field. It cannot be stressed enough that these figures are relative; when we determine that impact sensitivity of an explosive is lower for that of a tested explosive than PETN, for example, the number produced in the impact test is dimensionless, but it means that it is expected that it would take a greater impact to detonate it than PETN. Therefore, an experienced ordnance technician who works with raw PETN will know that the new explosive is not as sensitive with regards to impact. However, it could be more sensitive to friction, spark, or thermal issues. These conditions must be taken into account before any compound is to be stored, handled, or used in the field.\n\nIn the Netherlands, the Netherlands Organisation for Applied Scientific Research tests the safety of fireworks. According to a 2017 report by the Dutch Safety Board, 25% of all fireworks tested failed to meet safety standards and were banned from sale. Since 2010, safety testing of fireworks is required in the entire European Union, but companies are allowed to test their products in one member state before importing and selling them in another.\n"}
{"id": "601613", "url": "https://en.wikipedia.org/wiki?curid=601613", "title": "Sakuma, Shizuoka", "text": "Sakuma, Shizuoka\n\nAt the time of its merger, the town had an estimated population of 5,394 and a density of 32 persons per km. The total area was 168.53 km.\n\nOn July 1, 2005, Sakuma, along with the cities of Tenryū and Hamakita, the town of Haruno (from Shūchi District), the towns of Hosoe, Inasa and Mikkabi (all from Inasa District), the town of Misakubo, the village of Tatsuyama (all from Iwata District), and the towns of Maisaka and Yūtō (both from Hamana District), was merged into the expanded city of Hamamatsu, and is now part of Tenryū-ku, Hamamatsu City.\n\nSakuma is the location of the Sakuma Dam, an important hydroelectric power plant, and a frequency converter station that permits exchange of power between Japan's two different power systems. Japan's grids are also connected by two other stations, located at Higashi-Shimizu and Shin Shinano.\n\n\n"}
{"id": "1646450", "url": "https://en.wikipedia.org/wiki?curid=1646450", "title": "Savonius wind turbine", "text": "Savonius wind turbine\n\nSavonius wind turbines are a type of vertical-axis wind turbine (VAWT), used for converting the force of the wind into torque on a rotating shaft. The turbine consists of a number of aerofoils, usually—but not always—vertically mounted on a rotating shaft or framework, either ground stationed or tethered in airborne systems.\n\nThe Savonius wind turbine was invented by the Finnish engineer Sigurd Johannes Savonius in 1922. However, Europeans had been experimenting with curved blades on vertical wind turbines for many decades before this. The earliest mention is by the Italian Bishop of Czanad, Fausto Veranzio, who was also an engineer. He wrote in his 1616 book \"Machinae novae\" about several vertical axis wind turbines with curved or V-shaped blades. None of his or any other earlier examples reached the state of development made by Savonius. In his Finnish biography there is mention of his intention to develop a turbine-type similar to the Flettner-type, but autorotationary. He experimented with his rotor on small rowing vessels on lakes in his country. No results of his particular investigations are known, but the Magnus effect is confirmed by König. The two Savonius patents: US1697574, were filed in 1925 by Sigurd Johannes Savonius, and US1766765, in 1928.\n\nThe Savonius turbine is one of the simplest turbines. Aerodynamically, it is a drag-type device, consisting of two or three scoops. Looking down on the rotor from above, a two-scoop machine would look like an \"S\" shape in cross section. Because of the curvature, the scoops experience less drag when moving against the wind than when moving with the wind. The differential drag causes the Savonius turbine to spin. Because they are drag-type devices, Savonius turbines extract much less of the wind's power than other similarly-sized lift-type turbines. Much of the swept area of a Savonius rotor may be near the ground, if it has a small mount without an extended post, making the overall energy extraction less effective due to the lower wind speeds found at lower heights.\n\nAccording to Betz's law, the maximum power that is possible to extract from a rotor is formula_1, where formula_2 is the density of air, formula_3 and formula_4 are the height and radius of the rotor and formula_5 is the wind speed. However, in practice the extractable power is about half that (one can argue that only one half of the rotor — the scoop co-moving with the wind — works at each instant of time). Thus, one gets formula_6.\n\nThe angular frequency of a rotor is given by formula_7, where formula_8 is a dimensionless factor called the tip-speed ratio. \"λ\" is a characteristic of each specific windmill, and for a Savonius rotor \"λ\" is typically around unity.\n\nFor example, an oil-barrel sized Savonius rotor with \"h\"= and \"r\"= under a wind of \"v\"=, will generate a maximum power of and an angular speed of (190 revolutions per minute).\n\nSavonius turbines are used whenever cost or reliability is much more important than efficiency.\n\nMost anemometers are Savonius turbines for this reason, as efficiency is irrelevant to the application of measuring wind speed. Much larger Savonius turbines have been used to generate electric power on deep-water buoys, which need small amounts of power and get very little maintenance. Design is simplified because, unlike with horizontal axis wind turbines (HAWTs), no pointing mechanism is required to allow for shifting wind direction and the turbine is self-starting. Savonius and other vertical-axis machines are good at pumping water and other high torque, low rpm applications and are not usually connected to electric power grids. In the early 1980s Risto Joutsiniemi developed a version that does not require end plates, has a smoother torque profile and is self-starting in the same way a crossed pair of straight rotors is.\n\nThe most ubiquitous application of the Savonius wind turbine is the Flettner rotor, which is commonly seen on the roofs of vans and buses and is used as a cooling device. The ventilator was developed by the German aircraft engineer Anton Flettner in the 1920s. It uses the Savonius wind turbine to drive an extractor fan. The vents are still manufactured in the UK by Flettner Ventilator Limited.\n\nSmall Savonius wind turbines are sometimes seen used as advertising signs where the rotation helps to draw attention to the item advertised. They sometimes feature a simple two-frame animation.\n\n\n\n"}
{"id": "28865413", "url": "https://en.wikipedia.org/wiki?curid=28865413", "title": "Selenoyl fluoride", "text": "Selenoyl fluoride\n\nSelenoyl fluoride, selenoyl difluoride, selenium oxyfluoride, or selenium dioxydifluoride is a chemical compound with the formula SeOF.\n\nThe shape of the molecule is a distorted tetrahedron with the O-Se-O angle being 126.2°, the O-Se-F angle being 108.0° and F-Se-F being 94.1°. The Se-F bond length is 1.685 Å and the selenium to oxygen bond is 1.575 Å long.\n\nSelenoyl fluoride can be formed by the action of warm fluorosulfonic acid on barium selenate or selenic acid. SeO + SeF can give this gas along with other oxyfluorides.\n\nSelenoyl fluoride is more reactive than its analogon sulfuryl fluoride. It is easier to hydrolyse and to reduce. It may react violently upon contact with ammonia.\n\nSelenoyl fluoride reacting with xenon difluoride gives FXeOSeF.\n"}
{"id": "17653819", "url": "https://en.wikipedia.org/wiki?curid=17653819", "title": "SolarAid", "text": "SolarAid\n\nSolarAid is an international development charity which is working to create a sustainable market for solar lights in Africa. The organisation's aim is to reduce global poverty and climate change. SolarAid wholly owns an African social enterprise, SunnyMoney, the largest seller of solar lights in Africa. SolarAid was founded by Solarcentury, a solar energy company based in the UK.\n\nSolarAid aims to eradicate the kerosene lamp from Africa through the creation of a sustainable market for solar lights. The charity's social enterprise, SunnyMoney, operates in Uganda, Zambia and Malawi. A pilot project has also been conducted in Senegal in West Africa.\n\nSolarAid is the recipient of a 2013 Google Global Impact Award, a 2013 \"Guardian\" Sustainable Business Award. and the 2013 Ashden Gold Award.\n\nIn 2015, Indian Canadian singer Raghav released the single \"Until the Sun Comes Up\" in support of the SolarAid efforts. The single features also vocals from Indian film superstar actor Abhishek Bachchan and American rapper and singer Nelly.\n\n\n"}
{"id": "13456228", "url": "https://en.wikipedia.org/wiki?curid=13456228", "title": "Sweetwater Wind Farm", "text": "Sweetwater Wind Farm\n\nSweetwater phase 1 consists of 25 GE 1.5 megawatt S turbines, Sweetwater phase 2 consists of 61 GE 1.5 megawatt SLE turbines, Sweetwater 3 consists of 90 GE 1.5 megawatt XLE turbines. Sweetwater Stage 4 was financed by Epplament Energy, Lestis Group, NextEra and Lattner Energy.\n\nSweetwater stage 4 employs 135 Mitsubishi 1.0 megawatt wind turbines and 46 Siemens Wind Power 2.3 megawatt turbines. Its output is being sold to San Antonio’s CPS Energy under a 20-year purchase agreement.\n\nSweetwater 5 utilizes 35 Siemens 2.3 MW turbines.\n\n"}
{"id": "25204309", "url": "https://en.wikipedia.org/wiki?curid=25204309", "title": "TWISTEX", "text": "TWISTEX\n\nTWISTEX (an acronym for Tactical Weather-Instrumented Sampling in/near Tornadoes Experiment) is a tornado research experiment that was founded and led by Tim Samaras of Bennett, Colorado, US.\n\nThe project normally runs from mid-April through the end of June with a domain that covers the Great Plains and portions of the Midwestern United States. The project is normally at full strength for most of May and June with four vehicles, all equipped with roof-mounted mobile mesonet weather stations. One of the vehicles transports an array of in situ thermodynamic and video probes. Due to graduate and upper-division undergraduate student participant availability, a reduced vehicle compliment consisting of the in situ probe deployment truck and one support mesonet station vehicle is used in the first few weeks of the project.\n\nThe objectives of this research are to better understand tornado generation, maintenance and decay processes and to gain insight and knowledge of the seldom sampled near-surface internal tornado environment. Progress on these research fronts is aimed toward increasing tornado warning lead time while the internal tornado near-surface sampling provides essential ground truth data for structural engineering analysis of the interaction of tornadic winds with homes and buildings.\n\nTWISTEX is one of the featured teams in seasons 3, 4 and 5 of \"Storm Chasers\" on the Discovery Channel. The group has also been featured on National Geographic Channel's \"Disaster Labs\".\n\nOn May 31, 2013, Tim Samaras, his 24-year-old son Paul Samaras, and 45-year-old California native Carl Young lost their lives in the record wide EF3 multiple-vortex El Reno tornado. They were unable to escape along gravel roads as the funnel rapidly expanded to envelop them. Their Chevrolet Cobalt was caught by a subvortex, and Paul and Carl were ejected from the car. Tim was buckled in the passenger's seat, and was killed as the car was thrown approximately half a mile by the storm. Dan Robinson of St. Louis was a few hundred yards from them, but successfully escaped with minor injuries. Hinton resident Richard Henderson, who decided to follow the twister, lost his life in that same area. He snapped a picture of the storm from his cellular phone before it struck him. Several other storm chasers, including The Weather Channel's Mike Bettes, were also caught in the same sub-vortex but escaped with only minor injuries. Bettes and the Tornado Hunt crew were lifted up by the wedge tornado in their sport utility vehicle. That storm threw them two hundred yards off U.S. Route 81. The SUV was destroyed afterward. Five other people who were not involved with storm chasing also lost their lives in this tornado.\n\n\nStudents from the Atmospheric Sciences Department at Iowa State University rotate shifts into the mesonet vehicles during the project.\n\nKilled by the 2013 El Reno tornado:\n\nSeveral hardened instruments will be deployed in paths of tornadoes to collect the following datasets:\n\nThe thermodynamic probes are called Hardened In-situ Tornado Pressure Recorders (HITPR). All of the hardened instrumentation can collect/store the datasets. Measurements are recorded at 10 samples/second, and stored on non-volatile flash cards.\n\nTWISTEX will also have video probes that will provide visualization using 7 cameras each for a total of 14 cameras being deployed into the tornado core. Collectively the two camera probes will be used for photogrammetry purposes to visualize/measure tornado-driven debris and hydrometeors as well as for determining the tornado-relative location of the HITPRs.\n\nNew additional technologies will be used by deployment crew members to collect photogrammetric data from tornadoes as close as possible. One technique will be to record close tornado imagery using two digitally synchronized high-resolution high-speed cameras running at 500 frames per second for stereo photogrammetry techniques. This technique will provide excellent time resolution for velocity determination of low-level tornado core winds and lofted debris.\n\nWhile there are abundant kinematic datasets gathered by mobile radar of the tornadic region of supercells, the number of quality mobile mesonet or sticknet thermodynamic datasets of the flow field proximate to the tornadic region, generally within the supercell rear-flank downdraft (RFD) outflow, are comparatively rare. Even rarer are mesonet datasets reaching within about 1.5 km of tornadoes and datasets sampling the thermodynamic evolution of the RFD outflow.\n\nEach of the participating TWISTEX vehicles will have a mobile mesonet (MM) station mounted on the roof including the probe deployment truck. The mobile mesonet will be attempting to gather near-surface thermodynamic and kinematic data in as many quadrants of the RFD as possible. When coupled with the in-situ probe array data which represents another effective mesonet station, it is hoped to obtain thermodynamic and kinematic mapping that will describe characteristics of the flow reaching the tornado. Even if the hardened tornado probes do not take a direct hit, a peripheral tornado sampling is still very worthwhile.\n\nParticipants of the TWISTEX research project have contributed to many publications.\n\n\n\n\n\nTWISTEX was featured in seasons 3, 4, and 5 of Discovery Channel's \"Storm Chasers\".\n\n"}
{"id": "1484098", "url": "https://en.wikipedia.org/wiki?curid=1484098", "title": "Titanium carbide", "text": "Titanium carbide\n\nTitanium carbide, TiC, is an extremely hard (Mohs 9–9.5) refractory ceramic material, similar to tungsten carbide. It has the appearance of black powder with the sodium chloride (face-centered cubic) crystal structure. As found in nature its crystals range in size from 0.1 to 0.3mm.\n\nTitanium carbide is used in preparation of cermets, which are frequently used to machine steel materials at high cutting speed. It is also used as an abrasion-resistant surface coating on metal parts, such as tool bits and watch mechanisms. Titanium carbide is also used as a heat shield coating for atmospheric reentry of spacecraft.\n\nIt occurs in nature as a form of the very rare mineral \"\" () - (Ti,V,Fe)C. It was discovered in 1984 on Mount Arashan in the Chatkal District, USSR (modern Kyrgyzstan), near the Uzbek border. The mineral was named after Ibragim Khamrabaevich Khamrabaev, director of Geology and Geophysics of Tashkent, Uzbekistan.\n\nTitanium carbide has an elastic modulus of approximately 400 GPa and a shear modulus of 188 GPa.\n\nTool bits without tungsten content can be made of titanium carbide in nickel-cobalt matrix cermet, enhancing the cutting speed, precision, and smoothness of the workpiece.\n\nThe resistance to wear, corrosion, and oxidation of a tungsten carbide–cobalt material can be increased by adding 6–30% of titanium carbide to tungsten carbide. This forms a solid solution that is more brittle and susceptible to breakage.\n\nTitanium carbide can be etched with reactive-ion etching.\n\n"}
{"id": "38077315", "url": "https://en.wikipedia.org/wiki?curid=38077315", "title": "Tornado debris signature", "text": "Tornado debris signature\n\nA tornadic debris signature (TDS), often colloquially referred to as a debris ball, is an area of high reflectivity on weather radar caused by debris lofting into the air, usually associated with a tornado. A TDS may also be indicated by dual-polarization radar products, designated as a polarimetric tornado debris signature (PTDS). Polarimetric radar can discern meteorological and nonmeteorological hydrometeors and the co-location of a PTDS with the enhanced reflectivity of a debris ball are used by meteorologists as confirmation that a tornado is occurring.\n\nDebris balls can be a result of anthropogenic or biomass debris and are more likely to occur if a tornado crosses a \"target-rich\" environment such as a forest or populated area. A TDS is most likely to be observed when a tornado is closer to a radar site and the farther away from the radar that a TDS is observed the more likely that the tornado is stronger. As a result of the strong winds required to damage structures and loft debris into the air, debris balls are normally the result of EF3 or stronger tornadoes on the Enhanced Fujita Scale. Weaker tornadoes may also not cause debris balls due to their mostly short-lived nature and thus any debris may not be sampled by radar. However, not all tornadoes meeting such strength requirements exhibit debris balls, depending on their vicinity to sources of debris and distance from the radar site. A debris ball on radar images can verify tornadoes 70–80% of the time.\n\nDebris balls are seen on radar reflectivity images as a small, roundish area of high reflectivity values. Research conducted on debris balls that were noted during the 2011 Super Outbreak suggested that horizontal reflectivity from debris balls ranged from 51 to 72 dBZ during those outbreaks. Reflectivity values also decreased with increasing height. Due to the irregular and variable size, shapes, and tumbling nature of tornadic debris, debris balls typically produce a correlation coefficient (\"ρ\") less than 0.80. Differential reflectivity (\"Z\") values associated with debris balls are normally near or below 0 dB. Debris balls are almost always associated with a strong velocity couplet and the corresponding algorithm based detection, the tornado vortex signature (TVS) or tornado detection algorithm (TDA).\n\n"}
{"id": "1813863", "url": "https://en.wikipedia.org/wiki?curid=1813863", "title": "Trigonelline", "text": "Trigonelline\n\nTrigonelline is an alkaloid with chemical formula . It is a zwitterion formed by the methylation of the nitrogen atom of niacin (vitamin B). Trigonelline is a product of niacin metabolism that is excreted in urine of mammals.\n\nTrigonelline occurs in many plants. It has been isolated from fenugreek seeds (\"Trigonella foenum-graecum\", hence the name), garden peas, hemp seed, oats, potatoes, \"Stachys\" species, dahlia, \"Strophanthus\" species, and \"Dichapetalum cymosum\". Trigonelline is also found in coffee. Higher levels of trigonelline is found in arabica coffee.\n\nHoltz, Kutscher, and Theilmann have recorded its presence in a number of animals.\n\nTrigonelline crystallizes as a monohydrate from alcohol in hygroscopic prisms (m.p. 130 °C or 218 °C [\"dry, dec.\"]). It is readily soluble in water or warm alcohol, less so in cold alcohol, and slightly so in chloroform or ether. The salts crystallize well, the monohydrochloride, in leaflets, sparingly soluble in dry alcohol. The picrate forms shining prisms (m.p. 198−200 °C) soluble in water but sparingly soluble in dry alcohol or ether. The alkaloid forms several aurichlorides: the normal salt, B•HCl•AuCl, is precipitated when excess of gold chloride is added to the hydrochloride, and, after crystallization from dilute hydrochloric acid containing some gold chloride, has m.p. 198 °C. Crystallized from water or very dilute hydrochloric acid, slender needles of B•3 HAuCl (m.p. 186 °C) are obtained.\n\nWhen trigonelline is heated in closed tubes with barium hydroxide at 120 °C, it gives rise to methylamine, and, if treated similarly with hydrochloric acid at 260 °C creates chloromethane and nicotinic acid (a form of vitamin B). Trigonelline is a methyl betaine of nicotinic acid.\n"}
{"id": "23779446", "url": "https://en.wikipedia.org/wiki?curid=23779446", "title": "Undecylic acid", "text": "Undecylic acid\n\nUndecylic acid (systematically named undecanoic acid) is a naturally occurring carboxylic acid with chemical formula CH(CH)COOH. It is often used as an antifungal agent, to treat ringworm and athlete's foot, for example. Like decanoic acid, it has a distinctive, unpleasant odor.\n\n"}
{"id": "42168749", "url": "https://en.wikipedia.org/wiki?curid=42168749", "title": "Upper Paunglaung Dam", "text": "Upper Paunglaung Dam\n\nThe Upper Paunglaung Dam is a gravity dam on the Paunglaung River, about east of Pyinmana on the border of Naypyidaw Union Territory and Shan State, Burma. The primary purpose of the dam is hydroelectric power generation it will support a power station. Preliminary construction on the dam site began in January 2005 and roller-compacted concrete placement for the dam commenced in October 2010. The dam is expected to impound its reservoir and be complete in 2015. When complete, it is expected to regulate the river and improved power generation at the downstream Lower Paunglaung Dam.\n\nThe dam will force the relocation of some 15,000 residents which has drawn backlash from locals to international organizations. Many have already relocated but complain that their new land is of an insufficient size, has no power supply or natural resources to work.\n\n"}
{"id": "4316142", "url": "https://en.wikipedia.org/wiki?curid=4316142", "title": "Warm dark matter", "text": "Warm dark matter\n\nWarm dark matter (WDM) is a hypothesized form of dark matter that has properties intermediate between those of hot dark matter and cold dark matter, causing structure formation to occur bottom-up from above their free-streaming scale, and top-down below their free streaming scale. The most common WDM candidates are sterile neutrinos and gravitinos. The WIMPs (weakly interacting massive particles), when produced non-thermally could be candidates for warm dark matter. In general, however the thermally produced WIMPs are cold dark matter candidates.\n\nOne possible WDM candidate particle with a mass of a few keV comes from introducing two new, zero charge, zero lepton number fermions to the Standard Model of Particle Physics: \"keV-mass inert fermions\" (keVins) and \"GeV-mass inert fermions\" (GeVins). keVins are overproduced if they reach thermal equilibrium in the early universe, but in some scenarios the entropy production from the decays of unstable heavier particles can suppresses their abundance to the correct value. These particles are considered \"inert\" because they only have suppressed interactions with the Z boson. \nSterile neutrinos with masses of a few keV are possible candidates for keVins. \nAt temperatures below the electroweak scale their only interactions with standard model particles are weak interactions due to their mixing with ordinary neutrinos. Due to the smallness of the mixing angle they are not overproduced because they freeze out before reaching thermal equilibrium. Their properties are consistent with astrophysical bounds coming from structure formation and the Pauli principle if their mass is larger than 1-8 keV.\n\nIn February 2014, different analyses have extracted from the spectrum of X-ray emissions observed by XMM-Newton, a monochromatic signal around 3.5 keV. This signal is coming from different galaxy clusters (like Perseus and Centaurus) and several scenarios of warm dark matter can justify such a line. We can cite, for example, a 3.5 keV candidate annihilating into 2 photons, or a 7 keV dark matter particle decaying into a photon and a neutrino.\n\n\n"}
{"id": "49888528", "url": "https://en.wikipedia.org/wiki?curid=49888528", "title": "Wind power in South Korea", "text": "Wind power in South Korea\n\nWind power in South Korea is a form of renewable energy in South Korea. As of 2015 wind power capacity in South Korea was 835 MW and the wind energy share of total electricity consumption was far below 0,1%. Nevertheless, the Korean government plans to invest $8.2 billion into offshore wind farms in order to increase the total capacity to 2.5 GW by 2019\n\nInstalled wind power capacity in South Korea and generation in recent years is shown in the table below:\n"}
{"id": "2949555", "url": "https://en.wikipedia.org/wiki?curid=2949555", "title": "XENON", "text": "XENON\n\nThe XENON dark matter research project, operated at the Italian Gran Sasso National Laboratory, is a deep underground research facility featuring increasingly ambitious experiments aiming to detect dark matter particles. The experiments aim to detect particles in the form of weakly interacting massive particles (WIMPs) by looking for rare interactions via nuclear recoils in a liquid xenon target chamber. The current detector consists of a dual phase time projection chamber (TPC).\n\nThe experiment detects scintillation and ionization produced when particles interact in the liquid xenon volume, to search for an excess of nuclear recoil events over known backgrounds. The detection of such a signal would provide the first direct experimental evidence for dark matter candidate particles. The collaboration is currently led by Italian professor of physics Elena Aprile from Columbia University.\n\nThe XENON experiment operates a dual phase time projection chamber (TPC), which utilizes a liquid xenon target with a gaseous phase on top. Two arrays of photomultiplier tubes (PMTs), one at the top of the detector in the gaseous phase (GXe), and one at the bottom of the liquid layer (LXe), detect scintillation and electroluminescence light produced when charged particles interact in the detector. Electric fields are applied across both the liquid and gaseous phase of the detector. The electric field in the gaseous phase has to be sufficiently large to extract electrons from the liquid phase.\n\nParticle interactions in the liquid target produce scintillation and ionization. The prompt scintillation light produces 178 nm ultraviolet photons. This signal is detected by the PMTs, and is referred to as the S1 signal. This technique has proved sensitive enough to detect single photoelectrons. The applied electric field prevents recombination of all the electrons produced from a charged particle interaction in the TPC. These electrons are drifted to the top of the liquid phase by the electric field. The ionization is then extracted into the gas phase by the stronger electric field in the gaseous phase. The electric field accelerates the electrons to the point that it creates a proportional scintillation signal that is also collected by the PMTs, and is referred to as the S2 signal.\n\nThe detector allows for a full 3-D position determination of the particle interaction. Electrons in liquid xenon have a uniform drift velocity. This allows the interaction depth of the event to be determined by measuring the time delay between the S1 and S2 signal. The position of the event in the x-y plane can be determined by looking at the number of photons seen by each of the individual PMTs. The full 3-D position allows for the fiducialization of the detector, in which a low-background region is defined in the inner volume of the TPC. This fiducial volume has a greatly reduced rate of background events as compared to regions of the detector at the edge of the TPC, due to the self-shielding properties of liquid xenon. This allows for a much higher sensitivity when searching for very rare events.\n\nCharged particles moving through the detector are expected to either interact with the electrons of the xenon atoms producing electronic recoils, or with the nucleus, producing nuclear recoils. For a given amount of energy deposited by a particle interaction in the detector, the ratio of S2/S1 can be used as a discrimination parameter to distinguish electronic and nuclear recoil events. This ratio is expected to be greater for electronic recoils than for nuclear recoils. In this way backgrounds from electronic recoils can be suppressed by more than 99%, while simultaneously retaining 50% of the nuclear recoil events.\n\nThe XENON10 experiment was installed at the underground Gran Sasso laboratory in Italy during March 2006. The underground location of the laboratory provides 3100 m of water-equivalent shielding. The detector was placed within a shield to further reduce the background rate in the TPC. XENON10 was intended as a prototype detector, to prove the efficacy of the XENON design, as well as verify the achievable threshold, background rejection power and sensitivity. The XENON10 detector contained 15 kg of liquid xenon. The sensitive volume of the TPC measures 20 cm in diameter and 15 cm in height.\n\nAn analysis of 59 live days of data, taken between October 2006 and February 2007, produced no WIMP signatures. The number of events observed in the WIMP search region is statistically consistent with the expected number of events from electronic recoil backgrounds. This result excluded some of the available parameter space in minimal Supersymmetric models, by placing limits on spin independent WIMP-nucleon cross sections down to below for a WIMP mass.\n\nDue to nearly half of natural xenon having odd spin states (Xe has an abundance of 26% and spin-1/2; Xe has an abundance of 21% and spin-3/2), the XENON detectors can also be used to provide limits on spin dependent WIMP-nucleon cross sections for coupling of the dark matter candidate particle to both neutrons and protons. XENON10 set the world's most stringent restrictions on pure neutron coupling.\n\nThe second phase detector, XENON100, contains 165 kg of liquid xenon, with 62 kg in the target region and the remaining xenon in an active veto. The TPC of the detector has a diameter of 30 cm and a height of 30 cm. As WIMP interactions are expected to be extremely rare events, a thorough campaign was launched during the construction and commissioning phase of XENON100 to screen all parts of the detector for radioactivity. The screening was performed using high-purity Germanium detectors. In a few cases mass spectrometry was performed on low mass plastic samples. In doing so the design goal of <10 events/kg/day/keV was reached, realising the world's lowest background rate dark matter detector.\n\nThe detector was installed at the Gran Sasso National Laboratory in 2008 in the same shield as the XENON10 detector, and has conducted several science runs. In each science run, no dark matter signal was observed above the expected background, leading to the most stringent limit on the spin independent WIMP-nucleon cross section in 2012, with a minimum at for a WIMP mass. These results constrain interpretations of signals in other experiments as dark matter interactions, and rule out exotic models such as inelastic dark matter, which would resolve this discrepancy. XENON100 has also provided improved limits on the spin dependent WIMP-nucleon cross section. An axion result was published in 2014, setting a new best axion limit.\n\nXENON100 operated the then-lowest background experiment, for dark matter searches, with a background of 50 mDRU (1 mDRU=10 events/kg/day/keV).\n\nConstruction of the next phase, XENON1T, started in Hall B of the Gran Sasso National Laboratory in 2014. The detector contains 3.5 tons of ultra radio-pure liquid Xenon, and has a fiducial volume of about 2 tons. The detector is housed in a 10 m water tank that serves as a muon veto. The TPC is 1 m in diameter and 1 m in height. The predicted sensitivity at is . This is 100x lower than the current limit published for XENON100.\n\nIt is expected to explore/test supersymmetry candidates such as CMSSM.\n\nThe detector project team, called the XENON Collaboration, is composed of 135 investigators across 22 institutions from Europe, the Middle East, and the United States.\nThe first results from XENON1T were released by the XENON collaboration on May 18, 2017, based on 34 days of data-taking between November 2016 and January 2017. While no WIMPs or dark matter candidate signals were officially detected, the team did announce a record low reduction in the background radioactivity levels being picked up by XENON1T. The exclusion limits exceeded the previous best limits set by the LUX experiment, with an exclusion of cross sections larger than for WIMP masses of . Because some signals that the detector receives might be due to neutrons, reducing the radioactivity increases the sensitivity to WIMPs.\n\nXENONnT is the next upgrade step with a total xenon mass of 8 tonnes. Apart from a larger volume the experiment will feature even lower background radiation levels. It is designed to reach a sensitivity where neutrinos become a significant background. Commissioning is expected to start in 2019.\n\nGran Sasso National Laboratory (Italy),\nJohannes Gutenberg University, Mainz (Germany),\nColumbia University (USA),\nMax-Planck-Institut fur Kernphysik (Germany),\nRice University (USA),\nSUBATECH, Universite de Nantes (France),\nLAL, Laboratoire de l'Accélérateur Linéaire (France),\nUniversity of Bologna and INFN-Bologna (Italy),\nUniversity of California – Los Angeles (USA),\nUniversity of California – San Diego (USA),\nUniversity of Coimbra (Portugal),\nUniversity of Münster (Germany),\nUniversity of Freiburg (Germany),\nUniversity of Zurich (Switzerland),\nNikhef (Netherlands),\nWeizmann Institute of Science (Israel),\nPurdue University (USA),\nRensselaer Polytechnic Institute (USA)\nStockholm University (Sweden)\nNew York University Abu Dhabi (United Arab Emirates)\nUniversity of Chicago (USA)\n\nColumbia University (USA),\nJohannes Gutenberg University, Mainz (Germany),\nGran Sasso National Laboratory (Italy),\nMax-Planck-Institut fur Kernphysik (Germany),\nRice University (USA),\nSUBATECH, Universite de Nantes (France),\nUniversity of Bologna and INFN-Bologna (Italy),\nUniversity of California – Los Angeles (USA),\nUniversity of Coimbra (Portugal),\nUniversity of Münster (Germany),\nUniversity of Zurich (Switzerland),\nNikhef (Netherlands),\nWeizmann Institute of Science (Israel),\nPurdue University (USA),\nUniversity of Bern (Switzerland),\nShanghai Jiao Tong University (China)\nStockholm University (Sweden)\nNew York University Abu Dhabi (United Arab Emirates)\nUniversity of Chicago (USA)\n\nBrown (USA),\nCase Western Reserve (USA),\nColumbia University (USA),\nGran Sasso National Laboratory (Italy),\nLawrence Livermore National Laboratory (USA),\nRice University (USA),\nUniversity of Coimbra (Portugal),\nUniversity of Zurich (Switzerland),\nYale (USA)\n\n"}
{"id": "56108009", "url": "https://en.wikipedia.org/wiki?curid=56108009", "title": "Xingu-Estreito HVDC transmission line", "text": "Xingu-Estreito HVDC transmission line\n\nThe Xingu-Estreito HVDC transmission line is a 2076 km long 800 kV high-voltage direct current transmission line in Brazil between the Xingu substation at the city of Anapu in the Pará state, 17 km from the Belo Monte Dam, and the Estreito substation at the city of Ibiraci in the Minas Gerais state. It was inaugurated 21 December 2017.\n"}
