{"id": "15797705", "url": "https://en.wikipedia.org/wiki?curid=15797705", "title": "ASTM D6751", "text": "ASTM D6751\n\nASTM D6751 (American Society for Testing and Materials) details standards and specifications for biodiesels blended with middle distillate fuels. \nThis specification standard specifies various test methods to be used in the determination of certain properties for biodiesel blends. Some of the tests mentioned include flash point and kinematic viscosity.\n\nIt is generally comparable to European standard EN 14214 and National Standard of Canada CAN/CGSB-3.524.\n\n\n"}
{"id": "14319221", "url": "https://en.wikipedia.org/wiki?curid=14319221", "title": "African Conservation Centre", "text": "African Conservation Centre\n\nThe African Conservation Centre is a non-governmental organization based in Kenya. The group was founded in 1995. In 2007, it received a US$200,000 grant from the Ford Foundation. Their work has focused on capacity building \"to conserve wildlife through sound science, local initiatives and good governance.\" One of its projects, the Shompole Group Ranch.\", won the 2006 Equator Initiative Award for community-driven biodiversity-based business from the United Nations Development Programme.\nConservation of life's diversity for the well-being of the community and the environment \"\nThe mission is to conserve the wildlife and the environment in and beyond East Africa through the application of scientific and indigenous knowledge, enhanced livelihood and development of effective institutions.\" \nAfrican Conservation Center (ACC) began in the 1970s. Initially, it began as a research field for the issue of conserving wildlife. In 1995, ACC was registered \" as a nonprofit organization. Since then, the organization has grown to many accomplishments. In 2012, ACC collaborated with the South Rift Association of Landowners to create Kenya Rangeland Coalition. \" \n\n"}
{"id": "570172", "url": "https://en.wikipedia.org/wiki?curid=570172", "title": "Airspeed", "text": "Airspeed\n\nAirspeed is the speed of an aircraft relative to the air. Among the common conventions for qualifying airspeed are indicated airspeed (\"IAS\"), calibrated airspeed (\"CAS\"), equivalent airspeed (\"EAS\"), true airspeed (\"TAS\"), and density airspeed.\n\nIndicated airspeed is simply what is read off of an airspeed gauge connected to a pitot static system, calibrated airspeed is indicated airspeed adjusted for pitot system position and installation error, and equivalent airspeed is calibrated airspeed adjusted for compressibility effects. True airspeed is equivalent airspeed adjusted for air density, and is also the speed of the aircraft through the air in which it is flying. Calibrated airspeed is typically within a few knots of indicated airspeed, while equivalent airspeed decreases slightly from CAS as aircraft altitude increases or at high speeds.\n\nWith EAS constant, true airspeed increases as aircraft altitude increases. This is because air density decreases with higher altitude, but an aircraft's wing requires the same amount of air particles (i.e., mass of air) flowing around it to produce the same amount of lift for a given AOA; thus, a wing must move faster through thinner air than thicker air to obtain the same amount of lift.\n\nThe measurement and indication of airspeed is ordinarily accomplished on board an aircraft by an airspeed indicator (\"ASI\") connected to a pitot-static system. The pitot-static system comprises one or more pitot probes (or tubes) facing the on-coming air flow to measure pitot pressure (also called stagnation, total or ram pressure) and one or more static ports to measure the static pressure in the air flow. These two pressures are compared by the ASI to give an IAS reading.\n\nIndicated airspeed (IAS) is the airspeed indicator reading (ASIR) uncorrected for instrument, position, and other errors. From current EASA definitions: Indicated airspeed means the speed of an aircraft as shown on its pitot static airspeed indicator calibrated to reflect standard atmosphere adiabatic compressible flow at sea level uncorrected for airspeed system errors.\n\nOutside the former Soviet bloc, most airspeed indicators show the speed in knots (nautical miles per hour). Some light aircraft have airspeed indicators showing speed in statute miles per hour or kilometers per hour.\n\nAn airspeed indicator is a differential pressure gauge with the pressure reading expressed in units of speed, rather than pressure. The airspeed is derived from the difference between the ram air pressure from the pitot tube, or stagnation pressure, and the static pressure. The pitot tube is mounted facing forward; the static pressure is frequently detected at static ports on one or both sides of the aircraft. Sometimes both pressure sources are combined in a single probe, a pitot-static tube. The static pressure measurement is subject to error due to inability to place the static ports at positions where the pressure is true static pressure at all airspeeds and attitudes. The correction for this error is the position error correction (PEC) and varies for different aircraft and airspeeds. Further errors of 10% or more are common if the airplane is flown in “uncoordinated” flight.\n\nCalibrated airspeed (CAS) is indicated airspeed corrected for instrument errors, position error (due to incorrect pressure at the static port) and installation errors.\n\nCalibrated airspeed values less than the speed of sound at standard sea level (661.4788 knots) are calculated as follows:\n\nformula_1 minus position and installation error correction.\n\n\nUnits other than knots and inches of mercury can be used, if used consistently.\n\nThis expression is based on the form of Bernoulli's equation applicable to a perfect, incompressible gas. The values for formula_6 and formula_7 are consistent with the ISA i.e. the conditions under which airspeed indicators are calibrated.\n\nEquivalent airspeed (EAS) is defined as the airspeed at sea level in the International Standard Atmosphere at which the (incompressible) dynamic pressure is the same as the dynamic pressure at the true airspeed (TAS) and altitude at which the aircraft is flying. That is, it is defined by the equation\n\nformula_8\n\nwhere\n\nEAS is a measure of airspeed that is a function of incompressible dynamic pressure. Structural analysis is often in terms of incompressible dynamic pressure, so equivalent airspeed is a useful speed for structural testing. The significance of equivalent airspeed is that, at Mach numbers below the onset of wave drag, all of the aerodynamic forces and moments on an aircraft are proportional to the square of the equivalent airspeed. Thus, the handling and 'feel' of an aircraft, and the aerodynamic loads upon it, at a given equivalent airspeed, are very nearly constant and equal to those at standard sea level irrespective of the actual flight conditions.\n\nAt standard sea level pressure, CAS and EAS are equal. Up to about 200 knots CAS and 10,000 ft (3,000 m) the difference is negligible, but at higher speeds and altitudes CAS diverges from EAS due to compressibility.\n\nThe true airspeed (TAS; also KTAS, for \"knots true airspeed\") of an aircraft is the speed of the aircraft relative to the airmass in which it is flying. The true airspeed and heading of an aircraft constitute its velocity relative to the atmosphere.\n\nThe true airspeed is important information for accurate navigation of an aircraft. To maintain a desired ground track whilst flying in a moving airmass, the pilot of an aircraft must use knowledge of wind speed, wind direction, and true air speed to determine the required heading. See wind triangle.\n\nTAS is the true measure of aircraft performance in cruise, thus it is the speed listed in aircraft specifications, manuals, performance comparisons, pilot reports, and every situation when cruise or endurance performance needs to be measured. It is the speed normally listed on the flight plan, also used in flight planning, before considering the effects of wind.\n\nSince indicated airspeed is a better indicator of power used and lift available, true airspeed is not used for controlling the aircraft during taxiing, takeoff, climb, descent, approach or landing; for these purposes the Indicated airspeed – IAS or KIAS (knots indicated airspeed) – is used.\n\nTrue airspeed is related to the Mach number formula_11 and speed of sound formula_12 by\n\nformula_13\n\nBoth the Mach number and the speed of sound can be computed using measurements of impact pressure, static pressure and outside air temperature.\n\nAt sea level in the International Standard Atmosphere (ISA) and at low speeds where air compressibility is negligible (and so a constant air density may be assumed), TAS equals CAS. Above approximately , the compressibility error rises significantly.\n\nIn flight, it can be calculated using an E6B flight calculator or equivalent.\n\nSince temperature variations are of a smaller influence, the ASI error can be roughly estimated as indicating about 2% less than TAS per of altitude above sea level. For example, an aircraft flying at in the international standard atmosphere with an IAS of , is actually flying at TAS.\n\n\n\n"}
{"id": "5775707", "url": "https://en.wikipedia.org/wiki?curid=5775707", "title": "Alberta Fish and Game Association", "text": "Alberta Fish and Game Association\n\nThe Alberta Fish and Game Association (\"AFGA\") is a charitable organization dedicated to fish and wildlife conservation in the Canadian province of Alberta. The AFGA was founded in 1908 when a group of anglers and hunters first met in Calgary, Alberta. It now claims to have about 14,000 members province-wide.\n\nThe AFGA is a founding member of a number of sister conservation organizations. In 1962, along with representatives of the 9 other provinces, it helped found the Canadian Wildlife Federation (CWF). At the time it was felt that the organized hunters and anglers of Canada needed a national organization that would represent conservation. Today, it is still only the AFGA and its sister organizations from across Canada that constitute voting members of the CWF, although a significant amount of funds are raised from non-voting members.\n\nIn 1997, the AFGA was instrumental in the founding of the Alberta Conservation Association (ACA). The ACA acts as a delegated administration organization at arms-length from government, and administers funds received primarily from the sale of hunting and fishing licenses. At the time, the provincial government was going through a significant reorganization, and $18 million that were in a designated fund for conservation were being threatened to be absorbed by the provincial Treasurer. As a representative of the hunters and anglers who pay into this fund, the AFGA continues to be involved with the ACA.\n"}
{"id": "2022267", "url": "https://en.wikipedia.org/wiki?curid=2022267", "title": "Antimonide", "text": "Antimonide\n\nAntimonides (sometimes called stibnides) are compounds of antimony with more electropositive elements. The antimonide ion is Sb. \n\nSome antimonides are semiconductors, e.g. those of the boron group. Many antimonides are flammable or decomposed by oxygen when heated since the antimonide ion is a reducing agent.\n\n"}
{"id": "39301133", "url": "https://en.wikipedia.org/wiki?curid=39301133", "title": "Asbestos insulating board", "text": "Asbestos insulating board\n\nAsbestos insulating board, also known as \"AIB\" or by the trade name \"Asbestolux\", was an asbestos containing board used in the building trade because of its excellent fire resistance and heat insulating properties. These boards were commonly used in the UK from the 1930s up until production was terminated in 1980. AIB tended to contain 25-40% asbestos, with amosite being the most common form of asbestos used, although a mixture of amosite and chrysotile was also common, and crocidolite was sometimes used in older boards and some marine boards.\n\nAIB is softer, more porous and less dense than asbestos cement. This and the fact it contains between two and three times more asbestos than cement makes it far more friable and thus a greater health risk if the boards are disturbed or in poor condition.\n"}
{"id": "817175", "url": "https://en.wikipedia.org/wiki?curid=817175", "title": "Biological dispersal", "text": "Biological dispersal\n\nBiological dispersal refers to both the movement of individuals (animals, plants, fungi, bacteria, etc.) from their birth site to their breeding site ('natal dispersal'), as well as the movement from one breeding site to another ('breeding dispersal').\nDispersal is also used to describe the movement of propagules such as seeds and spores.\nTechnically, dispersal is defined as any movement that has the potential to lead to gene flow. \nThe act of dispersal involves three phases: departure, transfer, settlement and there are different fitness costs and benefits associated with each of these phases.\nThrough simply moving from one habitat patch to another, the dispersal of an individual has consequences not only for individual fitness, but also for population dynamics, population genetics, and species distribution. Understanding dispersal and the consequences both for evolutionary strategies at a species level, and for processes at an ecosystem level, requires understanding on the type of dispersal, the dispersal range of a given species, and the dispersal mechanisms involved.\n\nBiological dispersal may be contrasted with geodispersal, which is the mixing of previously isolated populations (or whole biotas) following the erosion of geographic barriers to dispersal or gene flow (Lieberman, 2005; Albert and Reis, 2011).\n\nDispersal can be distinguished from animal migration (typically round-trip seasonal movement), although within the population genetics literature, the terms 'migration' and 'dispersal' are often used interchangeably.\n\nSome organisms are motile throughout their lives, but others are adapted to move or be moved at precise, limited phases of their life cycles. This is commonly called the dispersive phase of the life cycle. The strategies of organisms' entire life cycles often are predicated on the nature and circumstances of their dispersive phases.\n\nIn general there are two basic types of dispersal:\nDue to population density, dispersal may relieve pressure for resources in an ecosystem, and competition for these resources may be a selection factor for dispersal mechanisms.\n\nDispersal of organisms is a critical process for understanding both geographic isolation in evolution through gene flow and the broad patterns of current geographic distributions (biogeography).\n\nA distinction is often made between natal dispersal where an individual (often a juvenile) moves away from the place it was born, and breeding dispersal where an individual (often an adult) moves away from one breeding location to breed elsewhere.\n\nIn the broadest sense, dispersal occurs when the fitness benefits of moving outweigh the costs.\n\nThere are a number of benefits to dispersal such as locating new resources, escaping unfavorable conditions, avoiding competing with siblings, and avoiding breeding with closely related individuals which could lead to inbreeding depression.\n\nThere are also a number of costs associated with dispersal, which can be thought of in terms of four main currencies: energy, risk, time and opportunity.\nEnergetic costs include the extra energy required to move as well as energetic investment in movement machinery (e.g. wings). Risks include increased injury and mortality during dispersal and the possibility of settling in an unfavorable environment.\nTime spent dispersing is time that often cannot be spent on other activities such as growth and reproduction.\nFinally dispersal can also lead to outbreeding depression if an individual is better adapted to its natal environment than the one it ends up in. In social animals (such as many birds and mammals) a dispersing individual must find and join a new group, which can lead to loss of social rank.\n\n\"Dispersal range\" refers to the distance a species can move from an existing population or the parent organism. An ecosystem depends critically on the ability of individuals and populations to disperse from one habitat patch to another. Therefore, biological dispersal is critical to the stability of ecosystems.\n\nFew species are ever evenly or randomly distributed within or across landscapes. In general, species significantly vary across the landscape in association with environmental features that influence their reproductive success and population persistence. Spatial patterns in environmental features (e.g. resources) permit individuals to escape unfavorable conditions and seek out new locations. This allows the organism to \"test\" new environments for their suitability, provided they are within animal's geographic range. In addition, the ability of a species to disperse over a gradually changing environment could enable a population to survive extreme conditions. (i.e. climate change).\n\nAs the climate changes, prey and predators have to adapt to survive. This poses a problem for many animals, for example the Southern Rockhopper Penguins. These penguins are able to live and thrive in a variety of climates due to the penguins' phenotypic plasticity. However, they are predicted to respond by dispersal, not adaptation this time. This is explained due to their long life spans and slow microevolution. Penguins in the subantarctic have very different foraging behavior than the subtropical waters, it would be very hard to survive and keep up with the fast changing climate because these behaviors took years to shape.\n\nA dispersal barrier may mean that the dispersal range of a species is much smaller than the species distribution. An artificial example is habitat fragmentation due to human land use. Natural barriers to dispersal that limit species distribution include mountain ranges and rivers. An example is the separation of the ranges of the two species of chimpanzee by the Congo River.\n\nOn the other hand, human activities may also expand the dispersal range of a species by providing new dispersal methods (e.g., ships). Many of them become invasive, like rats and stinkbugs, but some species also have a slightly positive effect to human settlers like honeybees and earthworms.\n\nMost animals are capable of locomotion and the basic mechanism of dispersal is movement from one place to another. Locomotion allows the organism to \"test\" new environments for their suitability, provided they are within the animal's range. Movements are usually guided by inherited behaviors.\n\nThe formation of barriers to dispersal or gene flow between adjacent areas can isolate populations on either side of the emerging divide. The geographic separation and subsequent genetic isolation of portions of an ancestral population can result in speciation.\n\nSeed dispersal is the movement or transport of seeds away from the parent plant. Plants have limited mobility and consequently rely upon a variety of dispersal vectors to transport their propagules, including both abiotic and biotic vectors. Seeds can be dispersed away from the parent plant individually or collectively, as well as dispersed in both space and time. The patterns of seed dispersal are determined in large part by the dispersal mechanism and this has important implications for the demographic and genetic structure of plant populations, as well as migration patterns and species interactions. There are five main modes of seed dispersal: gravity, wind, ballistic, water and by animals.\n\nThere are numerous animal forms that are non—motile, such as sponges, bryozoans, tunicates, sea anemones, corals, and oysters. In common, they are all either marine or aquatic. It may seem curious that plants have been so successful at stationary life on land, while animals have not, but the answer lies in the food supply. Plants produce their own food from sunlight and carbon dioxide—both generally more abundant on land than in water. Animals fixed in place must rely on the surrounding medium to bring food at least close enough to grab, and this occurs in the three-dimensional water environment, but with much less abundance in the atmosphere.\n\nAll of the marine and aquatic invertebrates whose lives are spent fixed to the bottom (more or less; anemones are capable of getting up and moving to a new location if conditions warrant) produce dispersal units. These may be specialized \"buds\", or motile sexual reproduction products, or even a sort of alteration of generations as in certain cnidaria.\n\nCorals provide a good example of how sedentary species achieve dispersion. Corals reproduce by releasing sperm and eggs directly into the water. These release events are coordinated by lunar phase in certain warm months, such that all corals of one or many species on a given reef will release on the same single or several consecutive nights. The released eggs are fertilized, and the resulting zygote develops quickly into a multicellular \"planula\". This motile stage then attempts to find a suitable substratum for settlement. Most are unsuccessful and die or are fed upon by zooplankton and bottom dwelling predators such as anemones and other corals. However, untold millions are produced, and a few do succeed in locating spots of bare limestone, where they settle and transform by growth into a polyp. All things being favorable, the single polyp grows into a coral head by budding off new polyps to form a colony.\n\nThe majority of all animals are motile. Although motile animals can, in theory, disperse themselves by their spontaneous and independent locomotive powers, a great many species utilize the existing kinetic energies in the environment, resulting in passive movement. Dispersal by water currents is especially associated with the physically small inhabitants of marine waters known as zooplankton. The term plankton comes from the Greek, πλαγκτον, meaning \"wanderer\" or \"drifter\".\n\nMany animal species, especially freshwater invertebrates, are able to disperse by wind or by transfer with an aid of larger animals (birds, mammals or fishes) as dormant eggs, dormant embryos or, in some cases, dormant adult stages. Tardigrades, some rotifers and some copepods are able to withstand desiccation as adult dormant stages. Many other taxa (Cladocera, Bryozoa, Hydra, Copepoda and so on) can disperse as dormant eggs or embryos. Freshwater sponges usually have special dormant propagules called gemmulae for such a dispersal. Many kinds of dispersal dormant stages are able to withstand not only desiccation and low and high temperature, but also action of digestive enzymes during their transfer through digestive tracts of birds and other animals, high concentration of salts and many kinds of toxicants. Such dormant-resistant stages made possible the long-distance dispersal from one water body to another and broad distribution ranges of many freshwater animals.\n\nDispersal is most commonly quantified either in terms of rate or distance.\n\nDispersal rate (also called migration rate in the population genetics literature) or probability describes the probability that any individual leaves an area or, equivalently, the expected proportion of individual to leave an area.\n\nThe dispersal distance is usually described by a dispersal kernel which gives the probability distribution of the distance traveled by any individual. A number of different functions are used for dispersal kernels in theoretical models of dispersal including the negative exponential distribution, extended negative exponential distribution, normal distribution, exponential power distribution, inverse power distribution, and the two-sided power distribution. The inverse power distribution and distributions with 'fat tails' representing long-distance dispersal events (called leptokurtic distributions) are though to best match empirical dispersal data.\n\nDispersal not only has costs and benefits to the dispersing individual (as mentioned above), but it also has consequences at the level of the population and species as well.\n\nMost populations have a patchy spatial distribution. Dispersal, by moving of individuals between different sub-populations, can increase the overall connectivity of the population, helping to minimize the risk of stochastic extinction, since if a sub-population goes extinct by chance, it is likely to be recolonized if the dispersal rate is high.\nIncreased connectivity can also decrease the degree of local adaptation.\n\n\n"}
{"id": "24954162", "url": "https://en.wikipedia.org/wiki?curid=24954162", "title": "Ceylon killifish", "text": "Ceylon killifish\n\nThe Ceylon killifish (\"Aplocheilus dayi\") is a species of killifish endemic to Sri Lanka. This species grows to a length of . Males and females have a black dot at the rear end of the base of the dorsal fin. The females lay 50–150 eggs.\n"}
{"id": "52713781", "url": "https://en.wikipedia.org/wiki?curid=52713781", "title": "Clacton Spear", "text": "Clacton Spear\n\nThe Clacton Spear, or Clacton Spear Point, is the tip of a wooden spear discovered in Clacton-on-Sea in 1911. It is 400,000 years old and the oldest known worked wooden implement.\nIt is made of yew wood, shaped into a point, and when found was 387 mm long, 39 mm diameter and straight. But drying out during the first decades of storage shrank it to 367 mm by 37 mm, and warped it slightly into a curve. Treatment by wax impregnation in 1952 has apparently stabilized it. At some time before this, the last 32 mm of the tip has broken off and had been re-attached by conservators. This again came off in 2013 and was re-attached. It is on display at the Natural History Museum, London where its age is stated as 420,000 years.\nTests to reproduce it suggested that it had been formed by scraping with a curved flint tool of the type found on the same site, known as the Clactonian notch.\n\nThe discoverer was Samuel Hazzledine Warren, an amateur pre-historian, who had been looking for simple stone tools in a known Palaeolithic sediment. He at first thought it to be an antler, but presented it to the Geological Society of London as a spear tip. This identification was generally accepted for some time. However, as it was not a whole spear and the great age meant that it was well before modern humans, many academics doubted that the amount of planning required (to manufacture a spear and use it for hunting) was within the cognitive capabilities of early hominids and argued it was a simpler tool such as a digging stick. However, the discovery of several more complete spears about 300,000 years old, the Schöningen Spears in 1995 in Germany demonstrated this capability, and the Clacton Spear is today generally regarded as a spear point.\n"}
{"id": "748731", "url": "https://en.wikipedia.org/wiki?curid=748731", "title": "Cooper (profession)", "text": "Cooper (profession)\n\nA cooper is a person trained to make wooden casks, barrels, vats, buckets, tubs, troughs and other staved containers, from timber that was usually heated or steamed to make it pliable. Journeymen coopers also traditionally made wooden implements, such as rakes and wooden-bladed shovels. Other materials, such as iron, were used as well as wood, in the manufacturing process.\n\nThe word \"cooper\" is derived from Middle Dutch or Middle Low German \"kūper\" 'cooper' from \"kūpe\" 'cask', in turn from Latin \"cupa\" 'tun, barrel'. Everything a cooper produces is referred to collectively as \"cooperage.\" A cask is any piece of cooperage containing a bouge, bilge, or bulge in the middle of the container. A barrel is a type of cask, so the terms \"barrel-maker\" and \"barrel-making\" refer to just one aspect of a cooper's work. The facility in which casks are made is also referred to as a cooperage.\n\nIn much the same way as the trade or vocation of smithing produced the common English surname Smith and the German name Schmidt, the cooper trade is also the origin of the English name Cooper; French Tonnelier and Tonnellier; Greek Varelas (); Danish Bødker; German Binder, Fassbender or Fassbinder (, literally 'cask-binder'), Böttcher ('tub-maker'), Scheffler, and Kübler; Dutch Kuiper and Cuypers; Lithuanian Kubilius; Latvian Mucenieks; Hungarian Kádár and Bodnár; Polish Bednarz, Bednarski, and Bednarczyk; Czech Bednář; Romanian Dogaru and Butnaru; Ukrainian Bodnar, Bodnaruk, and Bodnarchuk, and Bondarenko (); Russian and Ukrainian Bondarev () and Bocharov (); Yiddish Bodner; Portuguese Tanoeiro and Toneleiro; Spanish Cubero, Tonelero, and (via Greek) Varela; Bulgarian Bachvarov (); Macedonian Bacvarovski (); Croatian Bačvar; and Italian Bottai (from ).\n\nTraditionally, a cooper is someone who makes wooden, staved vessels, held together with wooden or metal hoops and possessing flat ends or heads. Examples of a cooper's work include casks, barrels, buckets, tubs, butter churns, hogsheads, firkins, tierces, rundlets, puncheons, pipes, tuns, butts, pins and breakers. Traditionally, a \"hooper\" was the man who fitted the wooden or metal hoops around the barrels or buckets that the cooper had made, essentially an assistant to the cooper. The English name Hooper is derived from that profession. With time, many Coopers took on the role of the Hooper themselves.\nThere were four divisions in the cooper's craft. The \"dry\" or \"slack\" cooper made containers that would be used to ship dry goods such as cereals, nails, tobacco, fruits, and vegetables. The \"dry-tight\" cooper made casks designed to keep dry goods in and moisture out. Gunpowder and flour casks are examples of a dry-tight cooper's work. The \"white\" cooper made straight-staved containers like washtubs, buckets, and butter churns, which would hold water and other liquids but did not allow shipping of the liquids. Usually there was no bending of wood involved in white cooperage. The \"wet\" or \"tight\" cooper made casks for long-term storage and transportation of liquids that could even be under pressure, as with beer. The \"general\" cooper worked on ships, on the docks, in breweries, wineries and distilleries, and in warehouses, and was responsible for cargo while in storage or transit.\nShips, in the age of sail, provided much work for coopers. They made water and provision casks, the contents of which sustained crew and passengers on long voyages. They also made barrels to contain high value commodities, such as wine and sugar. The proper stowage of casks on ships about to sail was an important stevedoring skill. Casks of various sizes were used to accommodate the sloping walls of the hull and make maximum use of limited space. Casks also had to be tightly packed, to ensure they did not move during the voyage and endanger the ship, crew and cask contents. Whaling ships in particular, featuring long voyages and large crews, needed many casks – for salted meat, other provisions and water – and to store the whale oil. Sperm whale oil was a particularly difficult substance to contain, due to its highly viscus nature, and oil coopers were perhaps the most skilled tradesmen in pre-industrial cooperage. Whaling ships usually carried a cooper on board, to assemble shooks and maintain casks.\n\nCoopers in Britain started to organise themselves as early as 1298. The Worshipful Company of Coopers, one of the oldest Livery Companies in London, still survives, although it is now largely a charitable organisation.\n\nPrior to the mid-20th century, the cooper's trade flourished in the United States; a dedicated trade journal was published, the \"National Cooper's Journal\", with advertisements from many different firms that hoped to supply anything from barrel staves to purpose-built machinery.\nPlastics, stainless steel, pallets, and corrugated cardboard replaced most wooden containers during the last half of the 20th century, and largely made the cooperage trade obsolete.\n\nIn the 21st century, coopers mostly operate barrel-making machinery and assemble casks for the wine and spirits industry.\n\nIn the United Kingdom, the trade of \"master cooper\" is dwindling; it is thought that the last remaining cooper company in England is a beer barrel manufacturer in Wetherby, West Yorkshire.\n\n\n\n"}
{"id": "30089154", "url": "https://en.wikipedia.org/wiki?curid=30089154", "title": "Deleni Wind Farm", "text": "Deleni Wind Farm\n\nThe Deleni Wind Farm is an under construction wind power project in Constanţa County, Romania. It will consist of an individual wind farm with 250 individual wind turbines with a nominal output of around 2 MW which will deliver up to 500 MW of power, enough to power over 327,000 homes, with a capital investment required of approximately €1.5 billion.\n"}
{"id": "506906", "url": "https://en.wikipedia.org/wiki?curid=506906", "title": "Density of air", "text": "Density of air\n\nThe density of air ρ (Greek: rho) (air density) is the mass per unit volume of Earth's atmosphere. Air density, like air pressure, decreases with increasing altitude. It also changes with variation in atmospheric pressure, temperature and humidity. At 101.325 kPa (abs) and 15°C, air has a density of approximately 1.225 kg/m³ (0.001225 g/cm³, 0.0023769 slug/(cu ft), 0.0765 lb/(cu ft)) according to ISA (International Standard Atmosphere).\n\nAir density is a property used in many branches of science, engineering, and industry, including aeronautics; gravimetric analysis; the air-conditioning industry; atmospheric research and meteorology; agricultural engineering (modeling and tracking of Soil-Vegetation-Atmosphere-Transfer (SVAT) models); and the engineering community that deals with compressed air.\n\nDepending on the measuring instruments used, different sets of equations for the calculation of the density of air can be applied. Air is a mixture of gases and the calculations always simplify, to a greater or lesser extent, the properties of the mixture.\n\nThe density of dry air can be calculated using the ideal gas law, expressed as a function of temperature and pressure:\n\nwhere:\n\nThe specific gas constant for dry air is 287.058 J/(kg·K) in SI units, and 53.35 (ft·lbf)/(lb·°R) in United States customary and Imperial units. This quantity may vary slightly depending on the molecular composition of air at a particular location.\n\nTherefore:\nThe following table illustrates the air density–temperature relationship at 1 atm or 101.325 kPa:\n\nThe addition of water vapor to air (making the air humid) reduces the density of the air, which may at first appear counter-intuitive.\nThis occurs because the molar mass of water (18 g/mol) is less than the molar mass of dry air (around 29 g/mol). For any ideal gas, at a given temperature and pressure, the number of molecules is constant for a particular volume (see Avogadro's Law). So when water molecules (water vapor) are added to a given volume of air, the dry air molecules must decrease by the same number, to keep the pressure or temperature from increasing. Hence the mass per unit volume of the gas (its density) decreases.\n\nThe density of humid air may be calculated by treating it as a mixture of ideal gases. In this case, the partial pressure of water vapor is known as the vapor pressure. Using this method, error in the density calculation is less than 0.2% in the range of −10 °C to 50 °C.\nThe density of humid air is found by:\n\nwhere:\n\nThe vapor pressure of water may be calculated from the saturation vapor pressure and relative humidity. It is found by:\n\nwhere:\n\nThe saturation vapor pressure of water at any given temperature is the vapor pressure when relative humidity is 100%.\nOne formula used to find the saturation vapor pressure is:\n\nwhere formula_4 is in degrees C.\n\nThe partial pressure of dry air formula_22 is found considering partial pressure, resulting in:\n\nWhere formula_24 simply denotes the observed absolute pressure.\n\nTo calculate the density of air as a function of altitude, one requires additional parameters. They are listed below, along with their values according to the International Standard Atmosphere, using for calculation the universal gas constant instead of the air specific constant:\n\nTemperature at altitude formula_31 meters above sea level is approximated by the following formula (only valid inside the troposphere, no more than ~18 km above Earth's surface (and lower away from Equator)):\n\nThe pressure at altitude formula_31 is given by:\n\nDensity can then be calculated according to a molar form of the ideal gas law:\nwhere:\n\n\n"}
{"id": "25714471", "url": "https://en.wikipedia.org/wiki?curid=25714471", "title": "Downhole oil–water separation technology", "text": "Downhole oil–water separation technology\n\nDownhole oil–water separation (DOWS) technology is an emerging technology that separates oil and gas from produced water at the bottom of the well, and re-injects most of the produced water into another formation which is usually deeper than the producing formation, while the oil and gas rich stream is pumped to the surface. DOWS effectively removes solids from the disposal fluid and thus avoids injectivity impairment caused by solids plugging. Simultaneous injection using DOWS minimizes the opportunity for the contamination of underground sources of drinking water (USDWs) through leaks in tubing and casing during the injection process.\n\nA DOWS system is installed at the bottom of an oil well, it separates oil and water in the wellbore. The oil rich stream is brought to the surface while the water rich stream is pumped into an injection formation without ever coming to the surface. A DOWS system includes many components but the two primary components are an oil/water separation system and a pumping/injection system used to lift oil to the surface and inject the water into a deeper formation. Two basic types of DOWS systems have been developed, one type uses hydrocyclones to mechanically separate oil and water and the other relies on gravity separation that takes place in the wellbore. Three basic types of pumping/injection systems are used with the DOWS technology. These include electrical submersible pumps, progressive cavity pumps and sucker rod pumps. Hydrocyclone separators are usually used with the electrical submersible pumps because of higher drawdown created with effective injection of water into the lower zone.\n\n"}
{"id": "42448992", "url": "https://en.wikipedia.org/wiki?curid=42448992", "title": "Foroska gas field", "text": "Foroska gas field\n\nThe Foroska gas field natural gas field located on the continental shelf of the Black Sea. It was discovered in 1974 and developed by Chornomornaftogaz. It started commercial production in 1975. The total proven reserves of the Foroska gas field are around , and production is slated to be around in 2018.\n"}
{"id": "13735033", "url": "https://en.wikipedia.org/wiki?curid=13735033", "title": "Helium atom scattering", "text": "Helium atom scattering\n\nHelium atom scattering (HAS) is a surface analysis technique used in materials science. HAS provides information about the surface structure and lattice dynamics of a material by measuring the diffracted atoms from a monochromatic helium beam incident on the sample.\n\nThe first recorded He diffraction experiment was completed in 1930 by Estermann and Stern [1] on the (100) crystal face of lithium fluoride. This experimentally established the feasibility of atom diffraction when the de Broglie wavelength, λ, of the impinging atoms is on the order of the interatomic spacing of the material. At the time, the major limit to the experimental resolution of this method was due to the large velocity spread of the helium beam. It wasn't until the development of high pressure nozzle sources capable of producing intense and strongly monochromatic beams in the 1970s that HAS gained popularity for probing surface structure. Interest in studying the collision of rarefied gases with solid surfaces was helped by a connection with aeronautics and space problems of the time. Plenty of studies showing the fine structures in the diffraction pattern of materials using helium atom scattering were published in the 1970s. However, it wasn't until a third generation of nozzle beam sources was developed, around 1980, that studies of surface phonons could be made by helium atom scattering. These nozzle beam sources were capable of producing helium atom beams with an energy resolution of less than 1meV, making it possible to explicitly resolve the very small energy changes resulting from the inelastic collision of a helium atom with the vibrational modes of a solid surface, so HAS could now be used to probe lattice dynamics. The first measurement of such a surface phonon dispersion curve was reported in 1981 [3], leading to a renewed interest in helium atom scattering applications, particularly for the study of surface dynamics.\n\nGenerally speaking, surface bonding is different from the bonding within the bulk of a material. In order to accurately model and describe the surface characteristics and properties of a material, it is necessary to understand the specific bonding mechanisms at work at the surface. To do this, one must employ a technique that is able to probe only the surface, we call such a technique \"surface-sensitive\". That is, the 'observing' particle (whether it be an electron, a neutron, or an atom) needs to be able to only 'see' (gather information from) the surface. If the penetration depth of the incident particle is too deep into the sample, the information it carries out of the sample for detection will contain contributions not only from the surface, but also from the bulk material. While there are several techniques that probe only the first few monolayers of a material, such as low-energy electron diffraction (LEED), helium atom scattering is unique in that it does not penetrate the surface of the sample at all! In fact, the scattering 'turnaround' point of the helium atom is 3-4 Angstroms above the surface plane of atoms on the material. Therefore, the information carried out in the scattered helium atom comes solely from the very surface of the sample. \nA visual comparison of helium scattering and electron scattering is shown below:\n\nHelium at thermal energies can be modeled classically as scattering from a hard potential wall, with the location of scattering points representing a constant electron density surface. Since single scattering dominates the helium-surface interactions, the collected helium signal easily gives information on the surface structure without the complications of considering multiple electron scattering events (such as in LEED).\n\nA qualitative sketch of the elastic one-dimensional interaction potential between the incident helium atom and an atom on the surface of the sample is shown here:\n\nThis potential can be broken down into an attractive portion due to Van der Waals forces, which dominates over large separation distances, and a steep repulsive force due to electrostatic repulsion of the positive nuclei, which dominates the short distances. To modify the potential for a two-dimensional surface, a function is added to describe the surface atomic corrugations of the sample. The resulting three-dimensional potential can be modeled as a corrugated Morse potential as [4]:\n\nThe first term is for the laterally-averaged surface potential - a potential well with a depth D at the minimum of z = z and a fitting parameter α, and the second term is the repulsive potential modified by the corrugation function, ξ(x,y), with the same periodicity as the surface and fitting parameter β.\n\nHelium atoms, in general, can be scattered either elastically (with no energy transfer to or from the crystal surface) or inelastically through excitation or deexcitation of the surface vibrational modes (phonon creation or annihilation). Each of these scattering results can be used in order to study different properties of a material's surface.\n\nThere are several advantages to using helium atoms as compared with x-rays, neutrons, and electrons to probe a surface and study its structures and phonon dynamics. As mentioned previously, the lightweight helium atoms at thermal energies do not penetrate into the bulk of the material being studied. This means that in addition to being strictly surface-sensitive they are truly non-destructive to the sample. Their de Broglie wavelength is also on the order of the interatomic spacing of materials, making them ideal probing particles. Since they are neutral, helium atoms are insensitive to surface charges. As a noble gas, the helium atoms are chemically inert. When used at thermal energies, as is the usual scenario, the helium atomic beam is an inert probe (chemically, electrically, magnetically, and mechanically). It is therefore capable of studying the surface structure and dynamics of a wide variety of materials, including those with reactive or metastable surfaces. A helium atom beam can even probe surfaces in the presence of electromagnetic fields and during ultra-high vacuum surface processing without interfering with the ongoing process. Because of this, helium atoms can be useful to make measurements of sputtering or annealing, and adsorbate layer depositions. Finally, because the thermal helium atom has no rotational and vibrational degrees of freedom and no available electronic transitions, only the translational kinetic energy of the incident and scattered beam need be analyzed in order to extract information about the surface.\n\nThe accompanying figure is a general schematic of a helium atom scattering experimental setup. It consists of a nozzle beam source, an Ultra High Vacuum scattering chamber with a crystal manipulator, and a detector chamber. Every system can have a different particular arrangement and setup, but most will have this basic structure. \n\nThe helium atom beam, with a very narrow energy spread of less than 1meV, is created through free adiabatic expansion of helium at a pressure of ~200bar into a low-vacuum chamber through a small ~5-10μm nozzle [5]. Depending on the system operating temperature range, typical helium atom energies produced can be 5-200meV. A conical aperture between A and B called the skimmer extracts the center portion of the helium beam. At this point, the atoms of the helium beam should be moving with nearly uniform velocity. Also contained in section B is a chopper system, which is responsible for creating the beam pulses needed to generate the time of flight measurements to be discussed later.\n\nThe scattering chamber, area C, generally contains the crystal manipulator and any other analytical instruments that can be used to characterize the crystal surface. Equipment that can be included in the main scattering chamber includes a LEED screen (to make complimentary measurements of the surface structure), an Auger analysis system (to determine the contamination level of the surface), a mass spectrometer (to monitor the vacuum quality and residual gas composition), and, for working with metal surfaces, an ion gun (for sputter cleaning of the sample surface). In order to maintain clean surfaces, the pressure in the scattering chamber needs to be in the range of 10 to 10 Pa. This requires the use of turbomolecular or cryogenic vacuum pumps.\n\nThe crystal manipulator allows for at least three different motions of the sample. The azimuthal rotation allows the crystal to change the direction of the surface atoms, the tilt angle is used to set the normal of the crystal to be in the scattering plane, and the rotation of the manipulator around the z-axis alters the beam incidence angle. The crystal manipulator should also incorporate a system to control the temperature of the crystal.\n\nAfter the beam scatters off the crystal surface, it goes into the detector area D. The most commonly used detector setup is an electron bombardment ion source followed by a mass filter and an electron multiplier. The beam is directed through a series of differential pumping stages that reduce the noise-to-signal ratio before hitting the detector. A time-of-flight analyzer can follow the detector to take energy loss measurements.\n\nUnder conditions for which elastic diffractive scattering dominates, the relative angular positions of the diffraction peaks reflect the geometric properties of the surface being examined. That is, the locations of the diffraction peaks reveal the symmetry of the two-dimensional space group that characterizes the observed surface of the crystal. The width of the diffraction peaks reflects the energy spread of the beam. The elastic scattering is governed by two kinematic conditions - conservation of energy and the energy of the momentum component parallel to the crystal:\n\nE = E => k² = k² = k² + k²\n\nk = k + G\n\nHere G is a reciprocal lattice vector, k and k are the final and initial (incident) wave vectors of the helium atom. The Ewald sphere construction will determine the diffracted beams to be seen and the scattering angles at which they will appear. A characteristic diffraction pattern will appear, determined by the periodicity of the surface, in a similar manner to that seen for Bragg scattering in electron and x-ray diffraction. Most helium atom scattering studies will scan the detector in a plane defined by the incoming atomic beam direction and the surface normal, reducing the Ewald sphere to a circle of radius R=k intersecting only reciprocal lattice rods that lie in the scattering plane as shown here:\n\nThe intensities of the diffraction peaks provide information about the static gas-surface interaction potentials. Measuring the diffraction peak intensities under different incident beam conditions can reveal the surface corrugation (the surface electron density) of the outermost atoms on the surface.\n\nNote that the detection of the helium atoms is much less efficient than for electrons, so the scattered intensity can only be determined for one point in k-space at a time. For an ideal surface, there should be no elastic scattering intensity between the observed diffraction peaks. If there is intensity seen here, it is due to a surface imperfection, such as steps or adatoms. From the angular position, width and intensity of the peaks, information is gained regarding the surface structure and symmetry, and the ordering of surface features.\n\nThe inelastic scattering of the helium atom beam reveals the surface phonon dispersion for a material. At scattering angles far away from the specular or diffraction angles, the scattering intensity of the ordered surface is dominated by inelastic collisions.\n\nIn order to study the inelastic scattering of the helium atom beam due only to single-phonon contributions, an energy analysis needs to be made of the scattered atoms. The most popular way to do this is through the use of time-of-flight (TOF) analysis. The TOF analysis requires the beam to be pulsed through the mechanical chopper, producing collimated beam 'packets' that have a 'time-of-flight' (TOF) to travel from the chopper to the detector. The beams that scatter inelastically will lose some energy in their encounter with the surface and therefore have a different velocity after scattering than they were incident with. The creation or annihilation of surface phonons can be measured, therefore, by the shifts in the energy of the scattered beam. By changing the scattering angles or incident beam energy, it is possible to sample inelastic scattering at different values of energy and momentum transfer, mapping out the dispersion relations for the surface modes. Analyzing the dispersion curves reveals sought-after information about the surface structure and bonding. A TOF analysis plot would show intensity peaks as a function of time. The main peak (with the highest intensity) is that for the unscattered helium beam 'packet'. A peak to the left is that for the annihilation of a phonon. If a phonon creation process occurred, it would appear as a peak to the right:\n\nThe qualitative sketch above shows what a time-of-flight plot might look like near a diffraction angle. However, as the crystal rotates away from the diffraction angle, the elastic (main) peak drops in intensity. The intensity never shrinks to zero even far from diffraction conditions, however, due to incoherent elastic scattering from surface defects. The intensity of the incoherent elastic peak and its dependence on scattering angle can therefore provide useful information about surface imperfections present on the crystal.\n\nThe kinematics of the phonon annihilation or creation process are extremely simple - conservation of energy and momentum can be combined to yield an equation for the energy exchange ΔE and momentum exchange q during the collision process. This inelastic scattering process is described as a phonon of energy ΔE=ћω and wavevector q. The vibrational modes of the lattice can then be described by the dispersion relations ω(q), which give the possible phonon frequencies ω as a function of the phonon wavevector q.\n\nIn addition to detecting surface phonons, because of the low energy of the helium beam, low-frequency vibrations of adsorbates can be detected as well, leading to the determination of their potential energy.\n\n"}
{"id": "20749128", "url": "https://en.wikipedia.org/wiki?curid=20749128", "title": "Ice jacking", "text": "Ice jacking\n\nIce jacking occurs when water invades a confined space in a structural support or geologic formation, and upon freezing causes structural fracture as the ice expands.\nIce jacking is a continuous process during the winter in areas that are located by lakes. The process starts when the ice begins to crack followed by water filling in those gaps and the process continues until there is a wall of ice surrounding the lakes shoreline, sometimes reaching up to three feet. The formation of ice jacking allows for nutrients to get trapped in the ice creating organic fertilizer to create a buffer (transition between land and aquatic organisms) which collaborates with already existing plants on the shore line that contribute to the development of future organisms leading to the expansion of fish. However ice jacking is unfeasible due to the lack of control over it. There are no methods that will prevent damages from ice jacking but there are a few defenses that can be used to fight against ice jacking such as building a riprap along the shoreline, attack its strengths/weaknesses, allow the natural process to take place and repair the damages later, or do nothing at all and let nature take its course.\n\n\nOne difference between ice jacking and ice heaving is that ice jacking occurs in the middle of winter while ice heaving occurs in the early spring. When there is snow, the ice stays at constant temperature. When there is no snow, nothing can protect or cover the ice from the changing ambient temperatures. When the temperature decreases, the ice will contract which causes cracks to occur in the ice which later are filled with the water from below. The ice expands once the temperature rises and the ice pushes up towards the shoreline since it does not have anywhere else to go. More pressure is put on the shoreline as the ice is jacked towards the shoreline. A solution to ice jacking should consist of sand shoreline. \n\nRock slope failures can occur due to the presence of water; ice jacking occurs when water between joint or fissure surfaces freezes and expands. This type of failure is progressive, resulting in incremental weakening over time, often requiring several cycles before failure. Ice jacking is one form of rock erosion. In a 2005 study, middle size rock falls (10 to 100,000 m3) in the French Subalpine Ranges in an elevation ranging from 200 meters and 2000 meters above sea level were observed. Statistical analysis studied the triggering factors of 46 rock falls, investigating the rainfall, freeze-thaw cycles, and earthquakes in the region. Correlation was found between failure and freeze-thaw cycles, suggesting that ice jacking is a main triggering factor in rock falls.\n\nFor example, on December 17, 2008 a Gondola tower on Blackcomb Mountain in Whistler, B.C collapsed as a result of ice jacking. The splice broke when the water entered into a section of the fourth tower and began to expand. This occurred around 2:30 pm, and left over fifty passengers stranded in sub-zero temperatures for hours. All of the passengers stuck in cabins on the Excalibur Gondola lift were rescued, with only twelve suffering from mild injuries. Many passengers recall watching in horror as the cars swung sideways, one dangling above the creek. It was apparent the first cabin to fall took the hardest hit, falling violently to the ground. Reports state none of the cabins actually came off of the cable, but that the strength of the cable had been significantly compromised. Following the incident, lift maintenance teams conducted inspections of all other towers to make sure there would be no other occurrence of ice jacking. After a second inspection by the British Columbia Safety Authority (BCSA), the lift were given approval to perform regularly again.\n\n"}
{"id": "3252568", "url": "https://en.wikipedia.org/wiki?curid=3252568", "title": "Inertia coupling", "text": "Inertia coupling\n\nIn aeronautics, inertia coupling is a potentially catastrophic phenomenon of high-speed flight in which the inertia of the heavier fuselage overpowers the aerodynamic stabilizing forces of the wing and empennage. The problem became apparent as single-engine jet fighter aircraft were developed with narrow wingspans, that had relatively low roll inertia relative to the pitch and yaw inertia dominated by the long slender high-density fuselage.\n\nInertia coupling occurs when an aircraft such as that described above is quickly put into a roll, resulting in violent pitching and yawing, and loss of control as the aircraft rotates on all three axes.\n\nThe phenomenon itself is not aerodynamic; it is caused by general conservation of angular momentum acting on mass whose radial distribution is not symmetric about the axis of rotation. It can be visualized by imagining a uniform long rod, at each end of which is a perpendicular extension, each pointing opposite the other. (If the rod is horizontal, one points up and the other points down.) At the end of each extension is a weight. The extensions and weights are identical, so the center of mass and the axis of rotation along the length of the rod are unaffected by the weights. If the rod is then spun about its axis, the centrifugal forces on the two weights will cause the entire assembly to tilt relative to its initial axis of rotation.\n\nAlthough a typical jet aircraft has most of its mass distributed close to its centerline, and the aerodynamics in planes designed to provide some stabilization (such that small fluctuations in control tend to return it to attitude equilibrium), it is important to remember that aircraft realistically always fly with a small non-zero random rate of yawing and pitching. Inertia coupling on an aircraft usually manifests itself as a downward pitching; rolling causes the tail mass to be flung upward and thus the nose to tip down. The pitching can in turn cause gyroscopic yawing.\n\nInertia coupling was essentially unknown before the introduction of high-speed jet aircraft. Prior to this time, aircraft tended to have greater width than length, and their mass was generally distributed closer to the center of mass. This was especially true for propeller aircraft, but equally true for early jet fighters as well. It was only when the aircraft began to sacrifice aerodynamic surface area in order to lower drag, and use longer fineness ratios that lowered supersonic drag, that the effect became obvious. In these cases the aircraft was generally much more tail-heavy, allowing its gyroscopic effect to overwhelm the small control surfaces.\n\nInertia coupling at Mach 3.2 killed pilot Captain Mel Apt in his first flight in the rocket-powered Bell X-2 on 27 September 1956, and had nearly killed Chuck Yeager in the X-1A three years earlier. It was also extremely obvious in the X-3 Stiletto (first flown in 1952), and flight test data on this aircraft were used to examine the problem. The first two production aircraft to overtly experience this phenomenon, the F-100 Super Sabre and F-102 Delta Dagger (both first flown in 1953), were modified to increase wing and tail areas and were fitted with augmented control systems. To enable pilot control during dynamic motion maneuvers, for instance, the tail area of the F-102A was increased 40%. In the case of the F-101 Voodoo (first flown in 1954), a stability augmentation system was retrofitted to the A models to help combat this problem. The Lockheed F-104 Starfighter (first flown in 1956) had its stabilator (horizontal tail surface) mounted atop its vertical rudder fin to reduce inertia coupling.\n"}
{"id": "27533722", "url": "https://en.wikipedia.org/wiki?curid=27533722", "title": "Infrared dark cloud", "text": "Infrared dark cloud\n\nAn infrared dark cloud (IRDC) is a cold, dense region of a giant molecular cloud. They can be seen in silhouette against the bright diffuse mid-infrared emission from the galactic plane.\n\nInfrared dark clouds have only been recently discovered in 1996 using the ISO and therefore are in need of further research.\n\nAstronomers believe that they represent the earliest stage in the formation of high-mass stars\n\n"}
{"id": "24871369", "url": "https://en.wikipedia.org/wiki?curid=24871369", "title": "Ionic liquid piston compressor", "text": "Ionic liquid piston compressor\n\nAn ionic liquid piston compressor, ionic compressor or ionic liquid piston pump is a hydrogen compressor based on an ionic liquid piston instead of a metal piston as in a piston-metal diaphragm compressor.\n\nAn ionic liquid compressor takes advantage of two properties of ionic liquids—their virtually non-measurable vapor pressures and large temperature window for the liquid phase—in combination with the low solubility of some gasses (\"e.g.\" hydrogen) in them. This insolubility is exploited by using the body of an ionic liquid to compress hydrogen up to 1000 bar (14,500 psi) in hydrogen filling stations. Linde's ionic liquid compressor reduced the number of moving parts from about 500 in a conventional reciprocating compressor down to 8.\n\nMany seals and bearings were removed in the design as the ionic liquid does not mix with the gas. Service life is about 10 times longer than a regular reciprocating compressor with reduced maintenance during use, energy costs are reduced by as much as 20%. The heat exchangers that are used in a normal piston compressor are removed as the heat is removed in the cylinder itself where it is generated. Almost 100% of the energy going into the process is being used with little energy wasted as reject heat.\n\nNot to be confused with the ion pump or the ionic liquid ring pump.\n\nAfter the renewed interest in ionic liquids, research was done by proionic, an enterprise in the spin-off center \"ZAT Center for applied Technology\" of the University of Leoben. The system was demonstrated at Zemships.\n\n"}
{"id": "6436402", "url": "https://en.wikipedia.org/wiki?curid=6436402", "title": "Jardines de la Reina", "text": "Jardines de la Reina\n\nJardines de la Reina () is an archipelago in the southern part of Cuba, in the provinces of Camagüey and Ciego de Ávila.\n\nIt was named by Christopher Columbus to honour the Queen of Spain, Isabella I of Castile. Since 1996 a marine reserve was established covering a large swath of the archipelago.In 2010, Jardines de la Reina was established as a national park (). With an area of , it is one of Cuba's largest protected areas.\n\nIt is located in the Caribbean Sea, between the Gulf of Ana Maria (north-west), Gulf of Guacanayabo (south) and Caballones Channel (west). It extends on a general north-west to south-east direction, paralleling the Cuban coast for from Cayo Breton to Cayos Mordazo. Cuba's second largest archipelago (smaller only than Jardines del Rey), it is formed by more than 600 cays and islands. Other cays in the archipelago include Caguamas, Cayos Cinco Balas, Cayo Anclitas, Cayo Algodon Grande, Cayos Pingues and Cayo Granada. Part of the archipelago is also known as Laberinto de las Doce Leguas (The Labyrinth of the Twelve Leagues)\n\nThe Islands area and population data retrieved from the 2012 census.\n\nThe archipelago is a popular destination for diving and sport fly-fishing. Only catch and release fly-fishing and a limited, well-regulated lobster fishery is allowed n the park, although many other fisheries occur suroounding the park and close to cays out of the park limits. It used to be one of Fidel Castro's favorite fishing spots. Species of fish found here include Cubera snapper, Bonefish, Yellowfin grouper, Black grouper, Atlantic goliath grouper as well as \"Strombus gigas\" (the large Caribbean conch) and Whale shark.\nBesides being an extraordinary site for fly fishing, one its main attractions for diving is the abundance of reef sharks.\n\nJardines de la Reina (The Gardens of the Queen) is one of the most popular scuba destinations of Cuba. The underwater landscapes include canyons, pinnacles and caves. Healthy mangroves, sponges and black corals cover the reef. The Jardines de la Reina also host numerous silky and Caribbean reef sharks. In the mangroves labyrinth it’s possible to find crocodiles and snorkel with them.\n\n\n"}
{"id": "36392276", "url": "https://en.wikipedia.org/wiki?curid=36392276", "title": "Karl Heinrich von Nassau-Siegen", "text": "Karl Heinrich von Nassau-Siegen\n\nKarl Heinrich von Nassau-Siegen (5 January 1743 – 10 April 1808), was a French-born fortune-seeker best known as Catherine II's least successful naval commander.\n\nCharles Henry, in Catherine II's own words, \"had everywhere the reputation of a crazy fellow\". He sailed around the world with Bougainville, \"fought tigers bare-handed\" in Central Africa and reportedly seduced the Queen of Tahiti. His tiger hunt is the subject of a vast canvas by Francesco Casanova.\n\nHe was the son of Maximilien Guillaume Adolphe of Nassau-Siegen (d. 1748) and his wife Amicie de Monchy (d. 1752). Charles Henry's family and title were disputed. His father was in 1756 posthumously recognized in France to be a legitimate son of Emmanuel Ignatius of Nassau-Siegen, in turn the youngest son of Prince Johan Franz of Nassau-Siegen by his third (and morganatic) wife, Isabella Clara du Puget de la Serre. Emmanuel Ignatius (d. 1735) had an unhappy union with the French noblewoman Charlotte de Mailly-Nesle (d. 1769) and became separated from her after a few years of marriage, during which they had two short-lived sons; years later (in 1722) they reconciled, and Emmanuel Ignatius recognized the third and only surviving son of his wife, the aforementioned Maximilien Guillaume Adolphe, as his own; however, shortly before his death (26 August 1734), he repudiated the child, declaring him adulterous.\n\nCiting descent from a princely dynasty in the male-line, Charles Henry claimed that under French law he was entitled to style himself \"Prince of Nassau-Siegen\", but neither title nor rank was recognized by the House of Nassau-Siegen or the Holy Roman Empire, which had declared his official grandfather, Emmanuel Ignatius de Nassau-Siegen, legitimate but born from a morganatic marriage.\n\nCharles Henry entered the French Navy at the age of 15, but led a dissolute life as a gambler at the royal courts of Vienna, Warsaw, Madrid and Versailles.\nProbably to escape his creditors, he joined the 1766 expedition of Louis Antoine de Bougainville to explore the South Pacific Ocean. On this expedition, he was able to develop his diplomatic skills in establishing contacts with the natives. For instance, in 1768 he was able to convince King Ereti of Tahiti of the peaceful intentions of the French.\n\nAs a French officer, Nassau-Siegen failed in his hastily prepared attack on the isle of Jersey (1779). For commanding the fireships at the siege of Gibraltar, Nassau-Siegen received from Spain three millions of francs and the dignity of Grandee of Spain. At Spa he met his future wife, a recently divorced Princess Sanguszko, the owner of a small estate in Podolia.\n\nIn 1786 Nassau-Siegen arrived in Russia, seeking to make an impression on the powerful Prince Potemkin. He accompanied the Empress in her journey through the provinces of New Russia and was put in charge of the Dnieper Flotilla. After routing the Turks at Ochakov, he wrote to his wife that the spectacle of the Turkish fleet was \"better than a ball in Warsaw\". \n\nAccording to John Paul Jones (who served under Nassau-Siegen's command), the putative prince sought to exaggerate his success to the utmost. He won the Order of St. Andrew after the Battle of Svensksund (1789) defeating the Swedes, but failed in stopping the Swedish fleet from breaking out in the Battle of Vyborg Bay in July 1790.\n\nHe was on the 9th of July 1790 decisively defeated by the Swedes at the second Battle of Svenskund. Despite this defeat, Nassau-Siegen was promoted to admiral by the Tsarina. Nassau-Siegen's military incompetence forced him to seek retirement. He left Russia in 1792 for the Rhine, to fight the French Revolution. But after the Peace of Amiens in 1802, he returned to France, where he solicited without success a position in Napoleon's army. He returned to Russia, to die at his estate at Tynna in Podolia in 1808.\n\nThe first plans of Russia's attack on India through Khiva and Bukhara are ascribed to Nassau-Siegen.\n\nHe had married in 1780 Polish Countess Karolina Gozdzka (1747-1807), but had no children.\n\n"}
{"id": "27196370", "url": "https://en.wikipedia.org/wiki?curid=27196370", "title": "List of installations for 15 kV AC railway electrification in Sweden", "text": "List of installations for 15 kV AC railway electrification in Sweden\n\nElectric railways in Sweden are powered from a single-phase AC supply of 15 kV at Hz, as used in Germany, Austria and Switzerland. Unlike these countries, the 132 kV traction current grid covers only part of the country, approximately north of Stockholm. This grid is fed by frequency converters and has no generation nor transmission lines from Norway, which uses the same traction current system. Formerly, the Porjus Hydroelectric Power Station provided electric power to the traction network. In the future the Älvkarleby Hydroelectric Power Station may be connected to the traction network, as this would increase stability of the power grid of the railway system.\nSeparate from the 132 kV grid is a 30 kV line connection between Mon and Varp substation north of Gothenburg.\n\nAlso the overhead wire system has some differences. It is common, that the pylons for the overhead wire also carry a three phase AC system. In order to reduce counts of substations, the voltage at the substations for the overhead wire is 32 kV at some lines (AT-System). However trains require 16 kV and therefore auto transformers are used, which have a central connection connected with the grounded railway, while the other connections are connected to the 32 kV output. The voltage between this central connection to the phase is 16 kV. The other pole to ground is also 16 kV, but with opposite polarity and runs along the track on the overhead wires until separation section. Other lines are fed directly with 16 kV from the substations. However differential transformers are used for feeding, in order to eliminate jamming signals.\n\nConverter Stations only feeding power into overhead wire.\n\nConverter Stations, which also feed power into 132 kV single phase AC grid.\n\nSubstations fed from 132 kV single phase AC grid.\n\nEndpoints of 30 kV-interconnection Mon-Varp.\n\nCrossing-point of traction current power line and HVDC Fenno-Skan 2 (only crossing point of HVDC overhead line and single-phase AC overhead powerline in the world): .\n\nThe railway on the Öresund bridge is fed with Danish standard voltage, 25 kV 50 Hz AC, along around 6 km within Swedish borders. The power is supplied from Denmark. The system limit is at Lernacken, near the bridge abutment.\n\nTwo local railways in Stockholm County, Roslagsbanan and Saltsjöbanan, as well as Stockholm Metro and all tramways are run on DC power.\n\n"}
{"id": "24436674", "url": "https://en.wikipedia.org/wiki?curid=24436674", "title": "Lucium", "text": "Lucium\n\nLucium was the proposed name for an alleged new element found by chemist Prosper Barrière in 1896 in the mineral monazite. Later, William Crookes confirmed that the new element was actually an impure sample of yttrium.\n\n"}
{"id": "15864430", "url": "https://en.wikipedia.org/wiki?curid=15864430", "title": "Maximon (particle)", "text": "Maximon (particle)\n\nMaximon – an hypothetical elementary particle of maximum mass in the mass spectrum of elementary particles having mass of 5 × 10 eV. Maximons can be electrically charged or neutral, they can have internal temperature of maximum or absolute zero, spin. They can look like black holes also.\n\nExistence of maximons was postulated by Soviet academician M. A. Markov in 1966.\n\n"}
{"id": "28900256", "url": "https://en.wikipedia.org/wiki?curid=28900256", "title": "Mean inter-particle distance", "text": "Mean inter-particle distance\n\nMean inter-particle distance (or mean inter-particle separation) is the mean distance between microscopic particles (usually atoms or molecules) in a macroscopic body.\n\nFrom the very general considerations, the mean inter-particle distance is proportional to the size of the per-particle volume formula_1, i.e., \nwhere formula_3 is the particle density. However, barring a few simple cases such as the ideal gas model, precise calculations of the proportionality factor are impossible analytically. Therefore, approximate expressions are often used. One such an estimation is the Wigner-Seitz radius\nwhich corresponds to the radius of a sphere having per-particle volume formula_1. Another popular definition is\ncorresponding to the length of the edge of the cube with the per-particle volume formula_1. The two definitions differ by a factor of approximately formula_8, so one has to exercise care if an article fails to define the parameter exactly. On the other hand, it is often used in qualitative statements where such a numeric factor is either irrelevant or plays an insignificant role, e.g.,\n\n\nWe want to calculate probability distribution function of distance to the nearest neighbor (NN) particle. (The problem was first considered by Paul Hertz; for a modern derivation see, e.g..) Let us assume formula_9 particles inside a sphere having volume formula_10, so that formula_3. Note that since the particles in the ideal gas are non-interacting, the probability to find a particle at a certain distance from another particle is the same as probability to find a particle at the same distance from any other point; we shall use the center of the sphere.\n\nAn NN particle at distance formula_12 means exactly one of the formula_9 particles resides at that distance while the rest \nformula_14 particles are at larger distances, i.e., they are somewhere outside the sphere with radius formula_12.\n\nThe probability to find a particle at the distance from the origin between formula_12 and formula_17 is\nformula_18, plus we have formula_9 kinds of way to choose which particle , while the probability to find a particle outside that sphere is formula_20. The sought-for expression is then\n\nwhere we substituted\nNote that formula_23 is the Wigner-Seitz radius. Finally, taking the formula_24 limit and using formula_25, we obtain\n\nOne can immediately check that\n\nThe distribution peaks at \n\nor, using the formula_30 substitution,\nwhere formula_32 is the gamma function. Thus,\n\nIn particular,\n\n"}
{"id": "300420", "url": "https://en.wikipedia.org/wiki?curid=300420", "title": "Mesophile", "text": "Mesophile\n\nA mesophile is an organism that grows best in moderate temperature, neither too hot nor too cold, typically between . The term is mainly applied to microorganisms. Organisms that prefer extreme environments are known as extremophiles. Mesophiles have diverse classifications, belonging to two domains: Bacteria, Archaea, and to kingdom Fungi of domain Eucarya. Mesophiles belonging to the domain Bacteria can either be gram-positive or gram-negative. Gram-positive bacteria have a cell layer made of peptidoglycan and stains purple. Gram-negative bacteria also contains peptidoglycan, yet the layer is extremely thin and stains red or pink. Oxygen requirements for mesophiles are not just confined to aerobic or anaerobic. There are three basic shapes of mesophiles: coccus, bacillus, and spiral.\n\nThe habitats of mesophiles can include cheese and yogurt. They are often included during fermentation of beer and wine making. Since normal human body temperature is 37 °C, the majority of human pathogens are mesophiles, as are most of the organisms comprising the human microbiome.\n\nMesophiles are the opposite of extremophiles. Extremophiles that prefer cold environments are termed psychrophilic, those preferring warmer temperatures are termed thermophilic or thermotropic and those thriving in extremely hot environments are hyperthermophilic.\nA genome-wide computational approach has been designed by Zheng, et al. to classify bacteria into mesophilic and thermophilic.\n\nAll bacteria have their own optimum environmental surroundings and temperatures in which they thrive. Many factors are responsible for a given organism's optimal temperature range, but evidence suggests that the expression of particular genetic elements (alleles) can alter the temperature-sensitive phenotype of the organism. A recently published study demonstrated that mesophilic bacteria could be genetically engineered to express certain alleles from psychrophilic bacteria, consequently shifting the restrictive temperature range of the mesophilic bacteria to closely match that of the psychrophilic bacteria.\n\nDue to the less stable structure of mesophiles, it has reduced flexibility for protein synthesis. Mesophiles are not able to synthesize proteins in low temperatures. It is more sensitive to temperature changes, and the fatty acid composition of the membrane does not allow for much fluidity. Decreasing the optimal temperature of 37 °C to 0 °C to 8 °C leads to a gradual decrease in protein synthesis. Cold-induced proteins (CIPs) are induced during low temperatures, which then allows cold-shock proteins (CSPs) to synthesize. The shift back to the optimal temperature sees an increase, indicating that mesophiles are highly dependent on temperature. Oxygen availability also affects microorganism growth.\n\nThere are two explanations for thermophiles being able to survive at such high temperatures whereas mesophiles can not. The most evident explanation is that thermophiles are believed to have cell components that are relatively more stable than the cell components of mesophiles which is why thermophiles are able to live at higher temperatures than mesophiles. \"A second school of thought, as represented by the writings of Gaughran (21) and Allen (3), believes that rapid resynthesis of damaged or destroyed cell constituents is the key to the problem of biological stability to heat.\"\n\nDue to the diversity of mesophiles, oxygen requirements greatly vary. Aerobic respiration requires the use of oxygen and anaerobic does not. There are three types of anaerobes. Facultative anaerobes grow in the absence of oxygen, using fermentation instead. During fermentation, sugars are converted to acids, alcohol, or gasses. If there is oxygen present, it will use aerobic respiration instead. Obligate anaerobes cannot grow in the presence of oxygen. Aerotolerant anaerobes can withstand oxygen.\n\nMicroorganisms play an important role in decomposition of organic matter and mineralization of nutrients. In aquatic environments, the diversity of the ecosystem allows for the diversity of mesophiles. The functions of each mesophile rely on the surroundings, most importantly temperature range. Bacteria such as mesophiles and thermophiles are used in the cheesemaking due to their role in fermentation. \"Traditional microbiologists use the following terms to indicate the general (slightly arbitrary) optimum temperature for the growth of bacteria: psychrophiles (15–20 °C), mesophiles (30–37 °C), thermophiles (50–60 °C) and extreme thermophiles (up to 122 °C)\". Both mesophiles and thermophiles are used in cheesemaking for the same reason; however, they grow, thrive and die at different temperatures. Psychrotrophic bacteria contribute to dairy products spoiling, getting mouldy or going bad due to their ability to grow at lower temperatures such as in a refrigerator.\n\nSome notable mesophiles include \"Listeria monocytogenes\", \"Staphylococcus aureus\", and \"Escherichia coli\". Other examples of species of mesophiles are \"Clostridium kluyveri\", \"Pseudomonas maltophilia\", \"Thiobacillus novellus\", \"Streptococcus pyogenes\", and \"Streptococcus pneumoniae\". Different types of diseases and infections typically have pathogens from mesophilic bacteria such as the ones listed above.\n\n\"Listeria monocytogenes\" is a gram-positive bacterium. It is closely related to \"Bacillus\" and \"Staphylococcus\". It is a rod-shaped, facultative anaerobe that is motile by peritrichous flagella. \"L. monocytogenes\" motility is limited from 20 °C to 25 °C. At the optimal temperature, it loses its motility. This bacterium is responsible for listeriosis which derives from contaminated food.\n\n\"Staphylococcus aureus\" was first identified in 1880. It is responsible for different infections stemming from an injury. The bacterium overcomes the body's natural mechanisms. Long lasting infections of \"S. aureus\" includes pneumonia, meningitis, and osteomyelitis. \"S. aureus\" is commonly contracted in hospital settings.\n\n\"Escherichia coli\" is a gram-negative, rod-shaped facultative anaerobic bacterium that does not produce spores. The bacterium is a member of Enterobacteriaceae. It is capable of producing enterotoxins which are thermolabile or thermostable. Other characteristics of \"E. coli\" are that it is oxidase-negative, citrate-negative, methyl-red positive, and Voges-Proskauer-negative. To sum up \"E. coli\", it is a coliform. It is able to use glucose and acetate as a carbon source for fermentation. \"E. coli\" is commonly found in the gut of living organisms. \"E. coli\" has many capabilities such as being a host for recombinant DNA and being a pathogen.\n\n"}
{"id": "20257727", "url": "https://en.wikipedia.org/wiki?curid=20257727", "title": "Muddy flood", "text": "Muddy flood\n\nA muddy flood is produced by an accumulation of run-off over agricultural land . Sediments are picked up by the run-off and carried as suspended matter or bed-load. Muddy floods are typically a hill-slope process, and should not be confused with mudflows produced by mass movements.\n\nMuddy floods can damage the road infrastructure and may deposit layers of mud blanket and may also clog sewers and damage private property.\n\nIt has been referred to 'muddy floods' since the 1980s. A similar designation appeared in French ('inondations boueuses') during the same period.\n\nMuddy runoff is generated on agricultural land when the soil surface is exposed or sparsely covered by vegetation. Large quantities of run-off usually generated by heavy storms is needed to start such a flood.\n\nMuddy floods have been observed in the entire European loess belt. Other affected areas include Normandy and Picardy (France), central Belgium and southern Limburg, the Netherlands.\n\nMuddy floods have also been observed in Slovakia and Poland.\n\nAn increase in muddy flood frequency has been observed during the last twenty years (e.g. in central Belgium,). This increase in their frequency may be due to a number of factors including:\n\n\nPreventive measures consist in limiting runoff generation and sediment production at the source. Alternative farming practices (e.g. reduced tillage) to increase runoff infiltration and limit erosion in their fields may assist.\n\nCurative measures generally consist in installing retention ponds at the boundary between cropland and inhabited areas.\n\nAn alternative is to apply other measures than can be referred to as intermediate measures. Grass buffer strips along or within fields, a grassed waterway (in the thalwegs of dry valleys) or earthen dams are good examples of this type of measures. They act as a buffer within landscape, retaining runoff temporarily and trapping sediments.\n\nImplementation of these measures is best coordinated at the catchment scale.\n\n"}
{"id": "9647686", "url": "https://en.wikipedia.org/wiki?curid=9647686", "title": "Newlincs EfW facility", "text": "Newlincs EfW facility\n\nThe Newlincs EfW facility is an incinerator which is located in Grimsby, North East Lincolnshire, England. The plant is operated by Cyclerval UK & TIRU Group under a PFI contract. The engineering of the facility is unusual as it consists of an oscillating kiln handling 56,000 tonnes of waste per year/7 tonnes per hour of waste. The facility is capable of generating 3.2 MW electricity.\n\n\n"}
{"id": "22756813", "url": "https://en.wikipedia.org/wiki?curid=22756813", "title": "Nicolas Antoine Boulanger", "text": "Nicolas Antoine Boulanger\n\nNicolas Antoine Boulanger (11 November 1722, Paris – 16 September 1759, Paris) was a French philosopher and man of letters during the Age of Enlightenment.\n\nBorn the son of a paper merchant in Paris, Boulanger studied first mathematics, and later ancient languages. He composed several philosophical works in which he sought to come up with naturalistic explanations for superstitions and religious practices, all of which were published posthumously. His major works were \"Research into the Origins of Oriental Despotism\" («Recherches sur l’origine du despotisme oriental», 1761) and \"Antiquity Unveiled\" («L’Antiquité dévoilée par ses usages», 1766). Boulanger's collected works were published in 1792.\n\nThe German-born Baron d'Holbach (Paul-Henri Thiry, 1723–1789) published his controversial anti-religious work \"Christianity Unveiled\" («Christianisme dévoilé», 1761), using Boulanger's name as his pseudonym, just two years after the philosopher's death. Boulanger also was one of the first modern critics of Paul.\n\nThe Koronian asteroid 7346 Boulanger, discovered in 1993, was named in his honor.\n"}
{"id": "21293847", "url": "https://en.wikipedia.org/wiki?curid=21293847", "title": "Nitrogen generator", "text": "Nitrogen generator\n\nNitrogen generators and stations are stationary or mobile air-to-nitrogen production complexes.\n\nThe adsorption gas separation process in nitrogen generators is based on the phenomenon of fixing various gas mixture components by a solid substance called an adsorbent. This phenomenon is brought about by the gas and adsorbent molecules' interaction.\n\nThe technology of air-to-nitrogen production with the use of adsorption processes in nitrogen generators is well studied and widely applied at industrial facilities for the recovery of high-purity nitrogen.\n\nThe operating principle of a nitrogen generator utilizing the adsorption technology is based upon the dependence of the adsorption rates featured by various gas mixture components upon pressure and temperature factors. Among nitrogen adsorption plants of various types, pressure swing adsorption (PSA) plants have found the broadest application world-wide.\n\nThe system's design is based on the regulation of gas adsorption and adsorbent regeneration by means of changing pressures in two adsorber–adsorbent-containing vessels. This process requires constant temperature, close to ambient. With this process, nitrogen is produced by the plant at the above-atmospheric pressure, while the adsorbent regeneration is accomplished at below-atmospheric pressure.\n\nThe swing adsorption process in each of the two adsorbers consists of two stages running for a few minutes. At the adsorption stage oxygen, HO and CO molecules diffuse into the pore structure of the adsorbent whilst the nitrogen molecules are allowed to travel through the adsorber–adsorbent-containing vessel. At the regeneration stage the adsorbed components are released from the adsorbent vented into the atmosphere. The process is then multiplely repeated.\n\n\n\n\nThe operation of membrane systems is based on the principle of differential velocity with which various gas mixture components permeate membrane substance. The driving force in the gas separation process is the difference in partial pressures on different membrane sides.\n\nStructurally, a hollow-fiber membrane represents a cylindrical cartridge functioning as a spool with specifically reeled polymer fibers. Gas flow is supplied under pressure into a bundle of membrane fibers. Due to the difference in partial pressures on the external and internal membrane surface gas flow separation is accomplished.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "1947680", "url": "https://en.wikipedia.org/wiki?curid=1947680", "title": "Nooksack Salmon Enhancement Association", "text": "Nooksack Salmon Enhancement Association\n\nNooksack Salmon Enhancement Association (NSEA) was formed in 1990 as a non-profit organization with an overall goal of seeing an increase in returning natural spawning salmon to the over 1,600 km of rivers and streams in Whatcom County, Washington in the United States. The association was formed as part of Washington State's Regional Fisheries Enhancement Groups Program established by the Washington State Legislature in 1990 to involve local communities, citizen volunteers, and landowners in the state’s salmon recovery efforts. NSEA is one of 14 groups in Washington State each with the common goal of restoring salmonid populations and habitat within their region.\n\nWhatcom County is home to the five species of Pacific salmon (chinook, chum, coho, pink, sockeye and kokanee, a lake resident sockeye), along with several other salmonids (bull trout, Dolly Varden, both sea-run and resident coastal cutthroat, and steelhead and rainbow trout) which rely heavily on the return of salmon each year.\n\nThe main focus of the NSEA is to restore lowland riparian areas. These areas are commonly impacted by human activities such as urban development, agriculture, and timber harvest. Riparian areas slow runoff, filter chemicals and excess nutrients caused by agriculture, shade the water, and slow the process of erosion to riverbanks. The NSEA deals with the issue of riparian destruction by leading many volunteer projects each year. The aim of these projects are to replant native trees and shrubs along stream banks, construct fences to keep livestock out, do an array of in-stream habitat improvement projects including adding large woody debris or gravel to streams, and stabilize eroding or undercut banks. Along with habitat restoration, the NSEA gives many free educational seminars to the local area, along with local schools, on how they can help with salmon management. The current executive director of NSEA is Rachel Vasak.\n\n\n"}
{"id": "56915324", "url": "https://en.wikipedia.org/wiki?curid=56915324", "title": "Occupational hazards of solar panel installation", "text": "Occupational hazards of solar panel installation\n\nThe introduction and rapid expansion of solar technology has brought with it a number of occupational hazards for workers responsible for panel installation. These hazards need to be addressed to ensure that workers are protected while we reap the benefits of this technology. Guidelines for safe solar panel installation exist, however the injuries related to panel installation are poorly quantified. There is concern for long term health effects acquired from prolonged ultraviolet radiation and from lifting heavy panels. The lack of data regarding these concerns makes increasing awareness for worker safety more challenging. Below is a discussion of occupational concerns including hazardous exposures to workers, accompanying health effects, the limitations of photovoltaic (PV) panels, challenges associated with this growing industry, as well as current and future directions for policy.\n\nWith regard to PV occupational safety, there are differing exposures depending on the stage of involvement in solar energy production. This can be broken down into four stages, including (1) Raw Material Mining and Processing; (2) Construction and Assembling; (3) Installation/Operation; and (4) End of Life Cycle and Recycling. Exposures and their impacts on worker health intricately depend on the PV life-cycle stage, as well as depth and duration of system involvement by the individual. There is a wide variety of tasks required by the PV industry. These include scientists and engineers for material development, workers who pilot manufacturing facilities, miners and millers, factory manufacturers, power electronics, planners, developers, installation crews who work with electrical grids, transportation jobs, and recycling industry.\n\nThe potential hazards are well documented for the first stage, solar PV cell production, and these are mainly chemical in nature. Although not exhaustive, a few of the hazardous materials of notable concern include crystalline silicon, amorphous silicon thin film, cadmium telluride thin film, copper indium selenide, copper indium gallium selenide, and gallium arsenide. The primary routes for occupational exposure include chemical burns, explosions, and inhalation of gaseous fumes, as a great deal of the materials used in manufacturing are highly toxic and flammable. Other routes can include hand-to-mouth contact or accidental ingestion. Most solar cells start as quartz, which is later refined into elemental silicon, thus extraction mines pose a risk to occupational health, including lung disease silicosis amongst miners.\n\nWith respect to crystalline silicon solar cell industrial development, workers may be exposed to hydrofluoric acid, or other acids and alkalis used for cleaning purposes, dopant gases and vapors (POCl, BH) due to improper ventilation, or the flammable nature of silane and byproducts from silicon nitride deposition and fabrication of x-Si layers. Other fire-hazard risks arise from the use of extremely flammable SiH gas. Exposure to cadmium compounds can be carcinogenic, as Cd is considered a lung carcinogen and is regulated by OSHA. For a complete list of chemical and material exposures due to photovoltaic production, please reference the Silicon Valley Toxics Coalition (SVTC).\n\nInstallation and operation presents a less-studied risk for occupational workers in the PV industry. Due to the nature of installing solar technology, falls from heights are a significant risk for concern. Health effects associated with falls from roofs include, but are not limited to: skeletomuscular injuries, brain or spine injuries, concussions, lacerations, bruising, swelling, long-term disability, and/or mortality. These can be exacerbated when roofing is old or damaged, or when panels are located close to edges, skylights, and vegetation. “Lack of fall protection, proximity to overhead power lines, and unguarded skylights” can also be contributing factors. One suggestion to prevent falls includes locating PV panels on the ground whenever possible, or incorporating PV technology into roof membranes, roof shingles, wall panels, or windows. Aside from falls, there is a risk for electrocution from solar PV installation or nearby power lines as well as ergonomic risks from heavy loads or a lack of lifting equipment such as ladder hoists, swing hoists, or truck-mounted cranes. Cold or heat stress and potential deleterious sun exposure can also occur. Installation ultimately combines the exposures of three separate industries into one, including roofing, carpentry, and electrical work, all of which carry high occupational risk.\nThere may also be a risk for infectious disease spread amongst PV employees. Solar farm (large-scale solar power facility) construction requires soil-disruptive work, which can precipitate exposure to soil-dwelling organisms. Such cases have been documented in San Luis Obispo County, California between 2011-2014, where 44 workers came down with coccidioidomycosis while constructing 2 solar power-generating facilities. Coccidioidomycosis (Valley Fever) is an infection caused by \"Coccidioides\" fungus spores and causes flu-like symptoms. However, complications can result, including meningitis, osteomyelitis, or cutaneous lesions. These can lead to fatality if not treated. In response to these isolated cases, the California Department of Public Health issued numerous recommendations, including improved worksite dust-control measures, equipping earth-moving equipment with high-efficiency particulate air (HEPA) filters, and respiratory protection for workers such as respirators with specialized particulate filters.\n\nIt is important to note that occupational hazards may be exacerbated in settings with fewer worker protection policies in place, disproportionately affecting vulnerable populations. If there are lesser or nonexistent standards for acceptable workplace environmental exposures as well as PPE, these chemical hazards could be exceptionally riskier for short and long-term health consequences. This concept of ‘exporting emissions’ should be of concern, given the propensity for solar panel production to be exported overseas. Manufacturing has moved from Europe, Japan, and the U.S. to countries such as China, Malaysia, the Philippines, and Taiwan, with nearly half of the world’s photovoltaics made in China as of 2014. However, some of the locations that produce the highest numbers of solar panels today may also lack proper worker protections.\n\nEnd-of-life recycling is also a large source of hazardous chemical exposures for workers. Green recycling requires increased material handling and manual separation, which can demand two to three times more individual handing of materials. This can present a risk for strains, sprains, and punctures. PV end-of-life materials can include lead from electronic circuits, as well as brominated flame retardants (BFRs), polybrominated biphenyls (PBBs), and polybrominated diphenylethers (PBDEs) used in circuit boards and solar panel inverters. These are considered toxic and potential estrogen disrupters, as PBDEs bioaccumulate in fatty tissues.\n\nThe popularity of solar photovoltaic (PV) panels has grown immensely and for good reason. PV panels provide us with the abundant clean energy needed to supply electricity. However, we still fall behind in terms of capturing the free, vast amount of energy produced by nature. The first, most obvious set back comes from the unpredictability of solar energy, i.e. cloudy or rainy weather. Solar radiation has a very broad spectrum which ranges from infrared to ultraviolet. It is challenging to design a device capable of utilizing all solar energy. While the cost of maintenance and operating are relatively low, solar panels are fragile and easily damaged. Further, in order to receive continuous supply of electric power the PV panels require inverters and storage batteries which increases costs. Most importantly, in comparison to other renewable energy systems, solar panels efficiency levels are relatively low, ~14%-25%. The UN Intergovernmental Panel on climate change estimates that the world will need the equivalent of 32,000 terawatt hours (TW h) of electrical energy by 2030. PV panels would only meet about 1%-2% of this demand (150-300 TW h). Although there are limitations to PV solar panels, we can continue to use them in tandem with other forms of renewable energy, such as wind.\n\nThere is no doubt that the use of solar energy has numerous benefits including using sustainable energy and reducing greenhouse gases. However, due to the rapid growth of the photo-voltaic (PV) industry, occupational hazards have increased for workers in the manufacturing, installation, and maintenance of solar energy. The occupational and health aspects of the PV industry have not been thoroughly examined. A few challenges facing workers are exposure to toxic chemicals, electrical shock, and risk of falling from dangerous heights.\n\nDuring the manufacturing process, workers are exposed to a variety of toxic chemicals. Some of these chemicals such as telluride, cadmium telluride, gallium, and germanium are still under study. Other chemicals like chlorosilanes and hydrogen chloride are not only toxic but highly volatile and explosive when mixed with water. Many solar PV technologies use extremely toxic material that have unknown health and environmental consequences including new nano-materials and processes. There is limited data on specific air emissions and liquid or solid effluents from PV cells and processing. This is not only problematic for occupational health but also for environmental health. A risk assessment for these chemicals would address hazard identification, risk of exposure, and an emphasis on dose-response relationships. The use of personal protective equipment is another resource that can be utilized to reduce exposures of hazardous chemicals.\n\nThe installation phase can also prove to be problematic and pose a serious risk for PV workers. According to “Photo-voltaic Industry on the Path to a Sustainable Future — Environmental and Occupational Health Issues,\" “The occupational injuries of PV workers have also been the subject of very few publications, although the safety aspect of solar panel installation has been highlighted”. Fatal falls from the installation phase have been reported from California and in France. Enforcement of standards, workforce education, and regulation are some ways to prevent falls during the installation of PV panels. The United States Department of Labor’s website https://www.osha.gov/dep/greenjobs/solar.html, lists different resources for preventing and controlling hazards; they promote safe work practices and provide workplace training requirements.\n\nPolicies addressing the hazards associated with solar panel manufacturing have the potential to create substantial improvement in worker safety. Research is currently underway exploring the possibility of replacing a number of the more hazardous chemicals workers are exposed to, such as cadmium and hydrofluoric acid, with less toxic chemicals. Once research is complete, enacting a policy that requires companies to make this change could greatly decrease the health risks associated with PV manufacturing.\n\nRegarding policies related to solar panel installation, OSHA requires employers to implement and provide safety training for workers, including information on how to assess the worksite for potential hazards, safely perform required action such as heavy lifting, and what to do should an accident occur. Examples of safety equipment include harnesses to protect from falls, covers for panels to protect from burns, and machinery to move the panels to protect from injuries due to heavy lifting. While companies are required to provide this training and equipment, it is ultimately the responsibility of the worker to carry out measures to protect themselves from injury. The inherent dangers that come with solar panel installation, work that combines the three dangerous professions of roofing, carpentry, and electrical work, are mitigated primarily by the workers’ adherence to correctly using protective equipment and following safety protocol at all times \n\nThis type of protection falls on the very tip of on the hierarchy of controls as the least effective measure to ensure the safety of workers. As the number of workers in this field grows, the number of work-related deaths and injuries is likely to grow as well. Policies related specifically to this field are extremely limited; further research is needed on how to best protect workers from potential harm associated with this occupation. In recent years The National Institute for Occupational Safety and Health launched the Prevention through Design (PtD) initiative. This initiative strives to improve the health and safety of workers by taking a comprehensive approach to eliminate hazards and control risks earlier in the process. This approach will create the safest work environment by further removing a hazard from the workers or reducing a hazard entirely; electrical safety and construction are already be researched through this initiative. By using this information and recognizing the unique nature of this occupation, solar panel installation safety can be greatly improved.\n\nLastly, in 2013, SEIA introduced the Solar Industry Environmental & Social Responsibility Commitment, a voluntary set of guidelines for the solar industry. Companies can choose to sign and agree to adhere to general best practices, including those for worker safety. Although not an official policy to protect those working with solar panel manufacturing and installation, it is a step in the right direction.\n"}
{"id": "47777731", "url": "https://en.wikipedia.org/wiki?curid=47777731", "title": "Peptitergents", "text": "Peptitergents\n\nPeptitergents (a portmanteau of peptide and detergent) are synthetic peptides designed to be lipophilic on one side and hydrophilic on the other upon folding to an α-helical conformation and were designed to solubilize integral membrane proteins in acqueous solution. They can be considered a sub-class of amphipols and are based on earlier fundamental explorations of amphiphilic secondary structures\n\n"}
{"id": "56551614", "url": "https://en.wikipedia.org/wiki?curid=56551614", "title": "Post-fire hillslope stabilization treatments", "text": "Post-fire hillslope stabilization treatments\n\nPost-fire hillslope stabilization treatments are treatments aimed at stabilizing fire-affected slopes by counteracting the negative impact of fire on vegetation and soil properties. The final objective of these treatments is reducing the risk of catastrophic runoff and erosion events and protecting valued resources downhill. Post-fire hillslope stabilization treatments are also called post-fire mitigation treatments and emergency stabilization treatments.\n\nVegetation fires usually partially or totally consume the canopy, above-ground organic residue (litter), fine roots, and soil organic compounds, reducing soil protection, enhancing soil water repellency, and compromising soil stability. The combined effect of fire and the occurrence of heavy rainfall in the post-fire season can lead to catastrophic hydrologic and erosion events with severe impacts on population and valued resources downhill such as housing, infrastructures, water supply systems, and critical habitats. \n\nFor example, high intensity rainfall after the Thomas Fire in California (USA) led to catastrophic floods and debris flows in 2018 with 20 people dead, 300 houses destroyed, and critical infrastructures severely affected. Stabilization treatments are part of the Burned Area Emergency Response (BAER) programme implemented by the US Forest Service and Department of Interior (USA) and similar programmes such as the Rapid Response Assessment Team (RRAT) in Australia conducted in many other fire-prone regions.\n\nThe design of the hillslope stabilization treatments are aimed at:\n\n\n\nThe selection of hillslope stabilization treatments should consider three key elements: (i) the effectiveness of the treatment, (iii) the cost of production and transport, and (i) the values-at-risk to be protected The most cost-effective treatment might not be adequate to protect critical values-at-risk whereas the treatment with the highest effectiveness and cost could not be suitable to treat large areas if the cost of repairing or replacing values-at-risk is low. Since the highest risk of catastrophic events happens during the first year after the fire, the implementation of any emergency treatment must be conducted quickly after the fire.\n\n\n\n\n\n"}
{"id": "238122", "url": "https://en.wikipedia.org/wiki?curid=238122", "title": "Potassium carbonate", "text": "Potassium carbonate\n\nPotassium carbonate (KCO) is a white salt, which is soluble in water (insoluble in ethanol) and forms a strongly alkaline solution. It can be made as the product of potassium hydroxide's absorbent reaction with carbon dioxide. It is deliquescent, often appearing a damp or wet solid. Potassium carbonate is used in the production of soap and glass.\n\nPotassium carbonate is the primary component of potash and the more refined pearl ash or salts of tartar. Historically, pearl ash was created by baking potash in a kiln to remove impurities. The fine, white powder remaining was the pearl ash. The first patent issued by the US Patent Office was awarded to Samuel Hopkins in 1790 for an improved method of making potash and pearl ash.\n\nIn late 18th century North America, before the development of baking powder, pearl ash was used as a leavening agent in quick breads.\n\nToday, potassium carbonate is prepared commercially by the electrolysis of potassium chloride. The resulting potassium hydroxide is then carbonated using carbon dioxide to form potassium carbonate, which is often used to produce other .\n\n\n\n"}
{"id": "11379174", "url": "https://en.wikipedia.org/wiki?curid=11379174", "title": "Protected areas of Slovakia", "text": "Protected areas of Slovakia\n\nProtected areas of Slovakia are areas that need protection because of their environmental, historical or cultural value to the nation. Protected areas in Slovakia are managed by institutions and organizations governed by the Ministry of the Environment.\n\nTypes of protected areas:\n\n\nFor the list of National Nature Reserves, Nature Reserves, National Nature Monuments, Nature Monuments, and Protected Sites, see the State Inventory of Specially Protected Parts of Nature and Landscape\n\nFor the list of Protected Trees, see enviroportal.sk .\n\n\nBetween the years 1994 – 2006 the following Protected Sites were cancelled: \nThe CHKO Malé Karpaty was reduced with the \"Sitina\" part no longer protected. In the same time-frame a new Protected Landscape Area was created, the Dunajské luhy Protected Landscape Area.\n\n"}
{"id": "676502", "url": "https://en.wikipedia.org/wiki?curid=676502", "title": "Rogue wave", "text": "Rogue wave\n\nRogue waves (also known as freak waves, monster waves, episodic waves, killer waves, extreme waves, and abnormal waves) are large, unexpected and suddenly appearing surface waves that can be extremely dangerous, even to large ships such as ocean liners.\n\nRogue waves present considerable danger for several reasons: they are rare, unpredictable, may appear suddenly or without warning, and can impact with tremendous force. A wave in the usual \"linear\" model would have a breaking pressure of . Although modern ships are designed to tolerate a breaking wave of , a rogue wave can dwarf both of these figures with a breaking pressure of .\n\nIn oceanography, rogue waves are more precisely defined as waves whose height is more than twice the significant wave height (\"H\" or SWH), which is itself defined as the mean of the largest third of waves in a wave record. Therefore, rogue waves are not necessarily the biggest waves found on the water; they are, rather, unusually large waves for a given sea state. Rogue waves seem not to have a single distinct cause, but occur where physical factors such as high winds and strong currents cause waves to merge to create a single exceptionally large wave.\n\nRogue waves can occur in media other than water. They appear to be ubiquitous in nature and have also been reported in liquid helium, in nonlinear optics and in microwave cavities. Recent research has focused on optical rogue waves which facilitate the study of the phenomenon in the laboratory. A 2015 paper studied the wave behavior around a rogue wave, including optical, and the Draupner wave, and concluded that \"rogue events do not necessarily appear without a warning, but are often preceded by a short phase of relative order\".. The physical origin of a rogue wave can be defined by the rogue wave theorem.\n\nRogue waves are an open water phenomenon, in which winds, currents, non-linear phenomena such as solitons, and other circumstances cause a wave to briefly form that is far larger than the \"average\" large occurring wave (the significant wave height or 'SWH') of that time and place. The basic underlying physics that makes phenomena such as rogue waves possible is that different waves can travel at different speeds, and so they can \"pile up\" in certain circumstances – known as \"constructive interference\". (In deep ocean the speed of a gravity wave is proportional to the square root of its wavelength—the distance peak-to-peak.) However other situations can also give rise to rogue waves, particularly situations where non-linear effects or instability effects can cause energy to move between waves and be concentrated in one or very few extremely large waves before returning to \"normal\" conditions.\n\nOnce considered mythical and lacking hard evidence for their existence, rogue waves are now proven to exist and known to be a natural ocean phenomenon. Eyewitness accounts from mariners and damage inflicted on ships have long suggested they occurred. The first scientific evidence of the existence of rogue waves came with the recording of a rogue wave by the Gorm platform in the central North Sea in 1984. A stand-out wave was detected with a wave height of in a relatively low sea state. However, the wave that caught the attention of the scientific community was the digital measurement of the \"Draupner wave\", a rogue wave at the Draupner platform in the North Sea on January 1, 1995, with a maximum wave height of (peak elevation of ) . During that event, minor damage was also inflicted on the platform, far above sea level, confirming that the reading was valid.\n\nTheir existence has also since been confirmed by video and photographs, and satellite imagery and radar of the ocean surface, by stereo wave imaging systems, by pressure transducers on the sea-floor and notably by oceanographic research vessel. In February 2000, a British oceanographic research vessel, the RRS \"Discovery\", sailing in the Rockall Trough west of Scotland encountered the largest waves ever recorded by scientific instruments in the open ocean, with a SWH of and individual waves up to . \"In 2004 scientists using three weeks of radar images from European Space Agency satellites found ten rogue waves, each or higher.\"\n\nA rogue wave is a natural ocean phenomenon that is not caused by land movement, only lasts briefly, occurs in a limited location, and most often happens far out at sea. Rogue waves are considered rare but potentially very dangerous, since they can involve the spontaneous formation of massive waves far beyond the usual expectations of ship designers, and can overwhelm the usual capabilities of ocean-going vessels which are not designed for such encounters. Rogue waves are therefore distinct from tsunamis. Tsunamis are caused by massive displacement of water, often resulting from sudden movement of the ocean floor, after which they propagate at high speed over a wide area. They are nearly unnoticeable in deep water and only become dangerous as they approach the shoreline and the ocean floor becomes shallower; therefore tsunamis do not present a threat to shipping at sea (the only ships lost in the 2004 Asian tsunami were in port). They are also distinct from megatsunamis, which are single massive waves caused by sudden impact, such as meteor impact or landslides within enclosed or limited bodies of water. They are also different from the waves described as \"hundred-year waves\", which is a purely statistical prediction of the highest wave likely to occur in a hundred-year period in a particular body of water.\n\nRogue waves have now been proven to be the cause of the sudden loss of some ocean-going vessels. Well documented instances include the freighter MS \"München\", lost in 1978 and the MV Derbyshire lost in 1980, the largest British ship ever lost at sea. A rogue wave has been implicated in the loss of other vessels including the Ocean Ranger which was a semi-submersible mobile offshore drilling unit that sank in Canadian waters on 15 February 1982. In 2007 the US National Oceanic and Atmospheric Administration compiled a catalogue of more than 50 historical incidents probably associated with rogue waves.\n\nIn 1826, French scientist and naval officer Captain Jules Dumont d'Urville reported waves as high as in the Indian Ocean with three colleagues as witnesses, yet he was publicly ridiculed by fellow scientist François Arago. In that era it was widely held that no wave could exceed . Author Susan Casey wrote that much of that disbelief came because there were very few people who had seen a rogue wave, and until the advent of steel double-hulled ships of the 20th century \"people who encountered 100-foot rogue waves generally weren't coming back to tell people about it.\"\n\nFor almost 100 years, oceanographers, meteorologists, engineers and ship designers have used a mathematical system commonly called the Gaussian function (or Gaussian Sea or standard linear model) to predict wave height. This model assumes that waves vary in a regular way around the average (so-called 'significant') wave height. In a storm sea with a significant wave height of , the model suggests there will hardly ever be a wave higher than . One of could indeed happen – but only once in ten thousand years (of wave height of ). This basic assumption was well accepted (and acknowledged to be an approximation). The use of a Gaussian form to model waves has been the sole basis of virtually every text on that topic for the past 100 years.\n\nThe first known scientific article on \"Freak waves\" was written by Professor Laurence Draper in 1964. In that paper which has been described as a 'seminal article' he documented the efforts of the National Institute of Oceanography in the early 1960s to record wave height and the highest wave recorded at that time which was about . Draper also described \"freak wave holes\".\n\nIn 1995, strong scientific evidence for the existence of rogue waves came with the recording of what has become known as the Draupner wave. The Draupner E is one structure in a gas pipeline support complex operated by Statoil about offshore and west by southwest from the southern tip of Norway. The Draupner E platform is the first major oil platform of the jacket-type attached to the seabed with a bucket foundation instead of pilings and a suction anchoring system. As a precaution, the operator (Statoil) fitted the platform with an extensive array of instrumentation. The instruments continuously check the platform's movements in particular any movement of the foundations during storm events. The state-of-the-art instrumentation fitted to the platform was able to continuously measure seven key parameters:\n\nThe rig was built to withstand a calculated 1 in 10,000 years wave with a predicted height of and was also fitted with state-of-the-art laser wave recorder on the platform’s underside. At 3 p.m. on 1 January 1995 it recorded an rogue wave (i.e. taller than the predicted 10,000 year wave) that hit the rig at . This was the first confirmed measurement of a freak wave, more than twice as tall and steep as its neighbors with characteristics that fell outside any known wave model. The wave was recorded by all of the sensors fitted to the platform and it caused enormous interest in the scientific community.\n\nStatoil researchers presented a paper in 2000 which collated evidence that freak waves were not the rare realizations of a typical or slightly non-gaussian sea surface population (\"Classical\" extreme waves) but rather they were the typical realizations of a rare and strongly non-gaussian sea surface population of waves (\"Freak\" extreme waves). A workshop of leading researchers in the world attended the first Rogue Waves 2000 workshop held in Brest in November 2000.\n\nIn 2000 the British oceanographic vessel RRS \"Discovery\" recorded a wave off the coast of Scotland near Rockall. This was a scientific research vessel and was fitted with high quality instruments. The subsequent analysis determined that under severe gale force conditions with wind speeds averaging a ship-borne wave recorder measured individual waves up to from crest to trough, and a maximum significant wave height of . These were some of the largest waves recorded by scientific instruments up to that time. The authors noted that modern wave prediction models are \"known\" to significantly under-predict extreme sea states for waves with a 'significant' height (H) above . The analysis of this event took a number of years, and noted that \"none of the state-of-the-art weather forecasts and wave models—the information upon which all ships, oil rigs, fisheries, and passenger boats rely—had predicted these behemoths.\" Put simply, a scientific model (and also ship design method) to describe the waves encountered did not exist. This finding was widely reported in the press which reported that \"according to all of the theoretical models at the time under this particular set of weather conditions waves of this size should not have existed\".\n\nMost popular texts on oceanography up until the mid 1990s such as that by Pirie made no mention of rogue or freak waves. The popular text on Oceanography by Gross (1996) only gave rogue waves a mention and stated that \"Under extraordinary circumstances unusually large waves called rogue waves can form\" without providing any further detail. From about 1997 most leading authors acknowledged the existence of rogue waves with the caveat that wave models had been unable to replicate rogue waves. The first scientific research which comprehensively proved that waves exist that are clearly outside the range of Gaussian waves was published in 1997. Some research confirms that observed wave height distribution in general follows well the Rayleigh distribution, but in shallow waters during high energy events, extremely high waves are more rare than this particular model predicts.\n\nIt is now proven via satellite radar studies that waves with crest to trough heights of to , occur far more frequently than previously thought. It is now known that rogue waves occur in all of the world's oceans many times each day. In 2004 the ESA MaxWave project identified more than ten individual giant waves above in height during a short survey period of three weeks in a limited area of the South Atlantic. The ESA's ERS satellites have helped to establish the widespread existence of these 'rogue' waves.\n\nThus acknowledgement of the existence of rogue waves (despite the fact that they cannot plausibly be explained by even state-of-the-art wave statistics) is a very modern scientific paradigm. It is now well accepted that rogue waves are a common phenomenon. Professor Akhmediev of the Australian National University, one of the world's leading researchers in this field, has stated that there are about 10 rogue waves in the world's oceans at any moment. Some researchers have speculated that approximately three of every 10,000 waves on the oceans achieve rogue status, yet in certain spots—like coastal inlets and river mouths—these extreme waves can make up three out of every 1,000 waves because wave energy can be focused.\n\nRogue waves may also occur in lakes. A phenomenon known as the \"Three Sisters\" is said to occur in Lake Superior when a series of three large waves forms. The second wave hits the ship's deck before the first wave clears. The third incoming wave adds to the two accumulated backwashes and suddenly overloads the ship deck with tons of water. The phenomenon is one of various theories as to the cause of the sinking of the SS Edmund Fitzgerald on Lake Superior in November 1975.\n\nSerious studies of the phenomenon of rogue waves only started about 20–30 years ago and have intensified since about 2005. One of the remarkable features of the rogue waves is that they always appear from nowhere and quickly disappear without a trace. Recent research has suggested that there could also be 'super-rogue waves' which are up to five times the average sea-state. Rogue waves has now become a near universal term given by scientists to describe isolated large amplitude waves, that occur more frequently than expected for normal, Gaussian distributed, statistical events. Rogue waves appear to be ubiquitous in nature and are not limited to the oceans. They appear in other contexts and have recently been reported in liquid helium, in nonlinear optics and in microwave cavities. It is now universally accepted by marine researchers that these waves belong to a specific kind of sea wave, not taken into account by conventional models for sea wind waves.\n\nResearchers at the Australian National University have also recently (2012) proven the existence of \"rogue wave holes\", an inverted profile of a rogue wave. In maritime folk-lore, stories of rogue holes are as common as stories of rogue waves. They follow from theoretical analysis but had never been proven experimentally. In 2012 the ANU published research confirming the existence of rogue wave holes on the water surface observed in a water wave tank.\n\nOn a smaller scale, kayakers call unpredictable 'exploding waves' caused by wave interaction \"clapotis\".\n\nThere are a number of research programmes currently underway focussed on rogue waves including:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBecause the phenomenon of rogue waves is still a matter of active research, it is premature to state clearly what the most common causes are or whether they vary from place to place. The areas of highest predictable risk appear to be where a strong current runs counter to the primary direction of travel of the waves; the area near Cape Agulhas off the southern tip of Africa is one such area; the warm Agulhas Current runs to the southwest, while the dominant winds are westerlies. However, since this thesis does not explain the existence of all waves that have been detected, several different mechanisms are likely, with localised variation. Suggested mechanisms for freak waves include the following:\n\n\n\n\nConstructive interference of elementary waves\n\nRogue waves can result from the constructive interference (dispersive and directional focusing) of elementary 3D waves enhanced by nonlinear effects.\n\n\nThe spatio-temporal focusing seen in the NLS equation can also occur when the nonlinearity is removed. In this case, focusing is primarily due to different waves coming into phase, rather than any energy transfer processes. Further analysis of rogue waves using a fully nonlinear model by R.H. Gibbs (2005) brings this mode into question, as it is shown that a typical wavegroup focuses in such a way as to produce a significant wall of water, at the cost of a reduced height.\n\nA rogue wave, and the deep trough commonly seen before and after it, may last only for some minutes before either breaking, or reducing in size again. Apart from one single rogue wave, the rogue wave may be part of a wave packet consisting of a few rogue waves. Such rogue wave groups have been observed in nature.\n\nThere are three categories of freak waves:\n\nThe possibility of the artificial stimulation of rogue wave phenomena has attracted research funding from DARPA, an agency of the United States Department of Defense. Bahram Jalali and other researchers at UCLA studied microstructured optical fibers near the threshold of soliton supercontinuum generation and observed rogue wave phenomena. After modelling the effect, the researchers announced that they had successfully characterized the proper initial conditions for generating rogue waves in any medium. Additional works carried out in optics have pointed out the role played by a nonlinear structure called Peregrine soliton that may explain those waves that appear and disappear without leaving a trace.\n\nIt should be noted that many of these encounters are only reported in the media, and are not examples of open ocean rogue waves. Often, in popular culture, an endangering huge wave is loosely denoted as a \"rogue wave\", while it has not been (and most often cannot be) established that the reported event is a rogue wave in the scientific sense — \"i.e.\" of a very different nature in characteristics as the surrounding waves in that sea state and with very low probability of occurrence (according to a Gaussian process description as valid for linear wave theory).\n\nThis section lists a limited selection of notable incidents.\n\n\n\n\nThe loss of the MS München in 1978 provided some of the first physical evidence of the existence of rogue waves. The MS München was a state-of-the-art cargo ship with multiple water-tight compartments, an expert crew and was considered unsinkable. She was lost with all crew and the wreck has never been found. The only evidence found was the starboard lifeboat which was recovered from floating wreckage some time later. The lifeboats hung from forward and aft blocks above the waterline. The pins had been bent back from forward to aft, indicating the lifeboat hanging below it had been struck by a wave that had run from fore to aft of the ship which had torn the lifeboat from the ship. To exert such force the wave must have been considerably higher than . At the time of the inquiry, the existence of rogue waves was considered so statistically unlikely as to be near impossible. Consequently, the Maritime Court investigation concluded that the severe weather had somehow created an 'unusual event' that had led to the sinking of the München.\n\nThe 1980 loss of the MV \"Derbyshire\" during Typhoon Orchid south of Japan with the loss of all crew marked a turning point for ship design. The \"Derbyshire\" was an ore-bulk-oil combination carrier built in 1976. At 91,655 gross register tons, she was—and remains—the largest British ship ever to have been lost at sea. The wreck was found in June 1994. The survey team deployed a remotely operated vehicle to photograph the wreck. A private report was published in 1998 which prompted the British government to reopen a formal investigation into the sinking. The British government investigation included a comprehensive survey by the Woods Hole Oceanographic Institution which took 135,774 pictures of the wreck during two surveys. The formal forensic investigation concluded that the ship sank because of structural failure and absolved the crew of any responsibility. Most notably, the report determined the detailed sequence of events that led to the structural failure of the vessel. A third comprehensive analysis was subsequently done by Douglas Faulkner, professor of marine architecture and ocean engineering at the University of Glasgow. His highly analytical and scientific report published in 2001 examined and linked the loss of the MV \"Derbyshire\" with what he called the emerging body of scientific evidence regarding the mechanics of freak waves. Professor Faulkner concluded that it was almost certain that \"Derbyshire\" would have encountered a wave of sufficient size to destroy her. Faulkner's conclusions have not been refuted in the more than 15 years since they were first presented (as of 2016). Indeed, subsequent analysis by others has corroborated his findings. Faulkner's finding that the \"Derbyshire\" was lost because of a rogue wave has had widespread implications on ship design. Faulkner has subsequently proposed the need for a paradigm shift in thinking for the design of ships and offshore installations to include what he calls a Survival Design approach additional to current design requirements. There is however no evidence that his recommendations have yet been adopted (as of 2016).\nIn 2004 an extreme wave was recorded impacting the Admiralty Breakwater, Alderney in the Channel Islands. This breakwater is exposed to the Atlantic Ocean. The peak pressure recorded by a shore-mounted transducer was 745 kPa which corresponds to a pressure of 74.5 tonnes/m2 or 74.5 Mt/m (metric tonnes per square metre). This pressure far exceeds almost any design criteria for modern ships and this wave would have destroyed almost any merchant vessel.\n\nMore recent work by Smith in 2007 confirmed prior forensic work by Faulkner in 1998 and determined that the MV \"Derbyshire\" was exposed to a hydrostatic pressure of a \"static head\" of water of about with a resultant static pressure of 201 kN/m. This is in effect 20 metres of green water (possibly a \"super rogue wave\") flowing over the vessel. The deck cargo hatches on the \"Derbyshire\" were determined to be the key point of failure when the rogue wave washed over the ship. The design of the hatches only allowed for a static pressure of less than two metres of water or 17.1 kN/m, in other words the typhoon load on the hatches was more than ten (10) times the design load. The forensic structural analysis of the wreck of the \"Derbyshire\" is now widely regarded as irrefutable.\n\nIn addition fast moving waves are now known to \"also\" exert extremely high dynamic pressure. It is known that plunging or breaking waves can cause short-lived impulse pressure spikes called Gifle peaks. These can reach pressures of 200 kN/m (or more) for milliseconds which is sufficient pressure to lead to brittle fracture of mild steel. Evidence of failure by this mechanism was also found on the \"Derbyshire\". Smith has documented scenarios where hydrodynamic pressure of up to 5,650 kN/m or over 500 metric tonnes per square metre could occur.\n\nVery few ship-wrecks have ever been fully investigated. The most recent bulk-carrier loss on the open seas to have been subjected to thorough investigation (as at March 2011) was the UK-owned M.V. \"Derbyshire\", which sank in 1980. Its entire crew of forty-four, all British citizens, perished. It took 14 years of pressure from the British public and a privately funded expedition to locate the wreck before a formal remote-camera search and investigation was done by the British government. At least a couple of hundred bulk carriers have been lost since 1980 and none have been properly investigated. A survey of 125 bulk carriers that sank between 1963 and 1996 found that seventy-six \"probably\" flooded, another four because of hatch-cover failure, the rest from \"unidentified\" causes. Nine other vessels broke completely in two. Causes of the remaining forty losses are unknown.\nMontgomery-Swan has outlined the generic mechanism of ship failure when encountering a rogue wave:\nThe scenario is very simple: the weight of the ship accelerates her down the back slope of the previous wave, the bow sticks into the lower part of the front of the giant incoming wave, and thousands of tons of green water fall onto the fore part of the ship. What happens next depends on the structure of the vessel.\nProfessor Faulkner who did the forensic independent analysis of the loss of the M.V. \"Derbyshire\" explains why this is such a problem for bulk carriers. He states that \"It is quite possible that some of the many unexplained heavy weather losses (of bulk carriers) may have been caused by hatch cover or coaming failures because fore end plunging due to flooding of large holds can be rapid.\" He noted in his report that \"because of their high inertias and natural pitch periods, these large ships do not rise to the waves, as appropriately experienced masters have confirmed. They tend to bury into them.\" Faulkner concluded that \"beyond any reasonable doubt, the direct cause of the loss of the M.V. \"Derbyshire\" was the quite inadequate strength of her cargo hatch covers to withstand the forces of Typhoon Orchid.\" He also noted that \"It is not possible to say which of the eighteen covers failed first, or from which direction the waves came; but evidence and other arguments suggest that the no. 1 hatch covers were probably the first to yield, probably from waves over the bow with the ship hove-to.\"\n\nIn November 1997 the International Maritime Organization (IMO) adopted new rules covering survivability and structural requirements for bulk carriers of and upwards. The bulkhead and double bottom must be strong enough to allow the ship to survive flooding in hold one unless loading is restricted.\n\nIt is now widely held that rogue waves present considerable danger for several reasons: they are rare, unpredictable, may appear suddenly or without warning, and can impact with tremendous force. A wave in the usual \"linear\" model would have a breaking force of . Although modern ships are designed to (typically) tolerate a breaking wave of 15 MT/m, a rogue wave can dwarf both of these figures with a breaking force far exceeding 100 MT/m. Smith has presented calculations using the International Association of Classification Societies (IACS) Common Structural Rules (CSR) for a typical bulk carrier which are consistent.\n\nPeter Challenor, a leading scientists in this field from the National Oceanography Centre in the United Kingdom was quoted in Casey's book in 2010 that \"We don’t have that random messy theory for nonlinear waves. At all\", he says. \"People have been working actively on this for the past 50 years at least. We don’t even have the start of a theory\".\n\nIn 2006 Smith proposed that the International Association of Classification Societies (IACS) recommendation 34 pertaining to standard wave data be modified so that the minimum design wave height be increased to . He presented analysis that there was sufficient evidence to conclude that high waves can be experienced in the 25-year lifetime of oceangoing vessels, and that high waves are less likely, but not out of the question. Therefore, a design criterion based on high waves seems inadequate when the risk of losing crew and cargo is considered. Smith has also proposed that the dynamic force of wave impacts should be included in the structural analysis.\n\nIt is notable that the Norwegian offshore standards now take into account extreme severe wave conditions and require that a 10,000-year wave does not endanger the ships integrity. Rosenthal notes that as at 2005 rogue waves were not explicitly accounted for in Classification Societies’ Rules for ships’ design. As an example, DNV GL, one of the world's largest international certification body and classification society with main expertise in technical assessment, advisory, and risk management publishes their Structure Design Load Principles which remain largely based on the 'Significant Wave height' and as at January 2016 still has not included any allowance for rogue waves.\n\nThe U.S. Navy historically took the design position that the largest wave likely to be encountered was 21.4 m (70 ft). Smith observed in 2007 that the navy now believes that larger waves can occur and the possibility of extreme waves that are steeper (i.e. do not have longer wavelengths) is now recognized. The navy has not had to make any fundamental changes in ship design as a consequence of new knowledge of waves greater than 21.4 m (70 ft) because they build to higher standards.\n\nA characteristic of the shipping industry is that there are no uniform codes or international standards. There are more than 50 classification societies worldwide, each has different rules. Ship design has historically largely been led by the ship insurers who inspected, classified and insured vessels. Hence the widespread adoption of new rules to allow for the existence of rogue waves is likely to take many years.\n\n\n\n\n"}
{"id": "26181149", "url": "https://en.wikipedia.org/wiki?curid=26181149", "title": "Sanitary garden", "text": "Sanitary garden\n\nThe Sanitary garden is an art exhibition consisting of artworks made from bathroom and toilet items such as lavatory seats and ballcocks.\n\nKala Sagar (Hindi word which means Ocean of Art) is the unique government museum where world’s largest collection of art created purely from damaged, discarded or waste sanitary ware items is exhibited. The whole collection of art pieces are created by only one artist Mr. Vijay Pal Goel (Age 50 years) who is creating this unique art since 1994 and is a non government employee.KALA SAGAR is situated in the sector 36-D, Chandigarh UT, INDIA.\n\nThe main objective of the artist behind this unique art is to generate mass awareness for reuse of waste materials for environment protection, treatment of the special children and established a tourist place for the special people such as mentally challenged people, visually impaired and physically handicapped.\n\nThis art is created by Mr. Vijay Pal Goel (Age 50 years). He was a private sanitary contractor. In the year 1994, an idea struck his mind and He started collecting waste sanitary material and converted it into marvelous art pieces. He is an uneducated person. He also never learned art making from any school, college or from any teacher.\nThe distinctiveness of the art is that whole collection of art pieces are shaped and created only from sanitary waste item. The sanitary waste items means discarded and damaged sanitary products used in bathroom, kitchen and public health such as water storage tanks, pipes, tapes, fountains, chinaware etc.\n\nAll the art pieces are created without using melting, molding, cementation, and carving techniques. This feature of the art makes it different from all other kind of art forms in the world.\n\nThe art pieces are created by arranging and assembling the different sanitary waste pieces and giving it a desired shape. For example: Airplanes, train, animal figures, telephones, human statues etc.\n\nThe size of the art pieces varies from 0.5 inch to 20 feet and depends upon of availability of the waste material. Presently the numbers of art pieces are around 300.\n\nThe other important fact of this art is that, this is the first kind of art form which is completely eco-friendly. Nothing gets wasted in teaching and creating this form of art. Here “nothing” means nothing. Every thing used in this art is only waste and discarded material. The same components of one art pieces can be easily converted into other and then again dismantled into the components.\n\nThe artist Mr. Vijay Pal Goel and his wife Mrs. Madhu Goel performed a study on the special children of the Government Institute for Mentally Retarded Children and 7 other special schools in the Chandigarh, INDIA.\n\nThe study covers around 500 Mentally Retarded Children for a time period 2001-2002. During this period these children were taught this unique art form.\n\nThe result of this study was very amazing. There is a noticeable change in the children behaviors. They became calmer and focused with enhancement in creativity skill and physical strength.\n\nIn year 2002, The Department of Social Welfare, Chandigarh Administration also appointed the Mr. Goel as full-time consultant for the rehabilitation of persons with disabilities on a very meager honorarium.\n\nThey also conducted a similar workshop in the Govt. Institute for Blind, Sector 26, Chandigarh.\n\nNow it is not very astounding to say that, this is the only art technique which can be enjoyed and also created by the Mentally Retarded and visually impaired children and Kala Sagar is the first museum for these special children.\n\nMr. and Mrs. Goel were also certified and awarded by the several govt. and other agencies for their innovative therapy and contribution in this field.\n\nBut without any funding and monetary support Mr. and Mrs. Goel were unable to continue their art therapy.\n\nIn the year 2000, the Health Minister, Punjab, INDIA impressed by the innovative art, requested the artist to construct a tableau on Pulse Polio for mass Awareness and the tableau was roamed in all the 17 districts covering almost all the cities and town and the remote areas of the Punjab. A large number of children were also vaccinated during this campaign.\n\nThe artist was honoured by The Governor of Punjab with the prestigious State Award on the Indian Independence Day (15 August 1998) for his tremendous contribution and excellent work in the field of art.\n\nOn 4th, September 1998, the Chandigarh administration persuaded the artist to donate whole his art collection to the government for the purpose of tourism promotion and a permanent exhibition of the art pieces was exhibited at the government museum Kala Sagar started in sector 36-D, Chandigarh UT. This museum is specially established only for the display of the unique art.\n\nThe Director, Government Institute for Mentally Retarded Children, Chandigarh honored him for his art therapy program.\n\nThe Director, Department of Environment, Chandigarh Administration also honored the artist for his unique way of waste management.\n\nThe Director, Department of Social Welfare, Chandigarh Administration UT honored him for his art therapy program.\n\nThe President, Parent Association of Mentally Retarded Children honored him for his art therapy program.\n\nThe artist is also certified by the several other prestigious awards (such as Rotary Club Midtown, etc.) for his tremendous social work.\n\n"}
{"id": "30874505", "url": "https://en.wikipedia.org/wiki?curid=30874505", "title": "Scalar field dark matter", "text": "Scalar field dark matter\n\nIn astrophysics and cosmology scalar field dark matter is a classical, minimally coupled, scalar field postulated to account for the inferred dark matter.\n\nThe universe may be accelerating, fueled perhaps by a cosmological constant or some\nother field possessing long range ‘repulsive’ effects. A model must predict the correct form for\nthe large scale clustering spectrum, account for cosmic microwave background anisotropies on large and intermediate angular scales, and provide agreement with the luminosity distance relation obtained from observations of high redshift supernovae. The modeled evolution of the universe includes a large amount of unknown matter in order to agree with such observations. This matter has two components cold dark matter and dark energy. Each contributes to the theory of the origination of galaxies and the expansion of the universe. The universe must have a critical density, a density not explained by baryonic matter (ordinary matter) alone.\n\nThe dark matter can be modeled as a scalar field using two fitted parameters, mass and self-interaction. In this picture the dark matter consists of an ultralight particle with a mass of O(10) eV when there is no self-interaction. If there is a self-interaction a wider mass range is allowed. The uncertainty in position of a particle is larger than its Compton wavelength, and for some reasonable estimates of particle mass and density of dark matter there is no point talking about the individual particle's position and momentum. The dark matter is more like a wave than a particle, and the galactic halos are giant systems of condensed bose liquid, possibly superfluid. The dark matter can be described as a Bose–Einstein condensate of the ultralight quanta of the field and as boson stars. The enormous Compton wavelength of these particles prevents structure formation on small subgalactic scales, which is a major problem in traditional cold dark matter models. The collapse of initial overdensities is studied in Refs.\n\nThis dark matter model is also known as BEC dark matter or wave dark matter. Fuzzy dark matter and ultra-light axion are examples of scalar field dark matter.\n\n\n"}
{"id": "11685729", "url": "https://en.wikipedia.org/wiki?curid=11685729", "title": "Seyed Hossein Mousavian", "text": "Seyed Hossein Mousavian\n\nSeyed Hossein Mousavian (born 1957 in Kashan) is an Iranian policymaker and scholar who served on Iran's nuclear diplomacy team in negotiations with the EU and International Atomic Energy Agency. He currently resides in the United States, where he is a visiting research scholar at Princeton University.\n\nMousavian was born in 1957 to a prosperous carpet-dealing family in Kashan, a major carpet-manufacturing center. His family had close ties to the Motalefeh, a religiously oriented revolutionary movement that dated back to the early 1960s and was eventually absorbed into the Islamic Republican Party (IRP). Mousavian studied at the Iran University of Science and Technology, as well as Sacramento City College and Sacramento State University in the United States.\n\nIn 1981 he was awarded a Bachelor of Science degree in Engineering from Sacramento State University of California. He was awarded an MA in International Relations from the University of Tehran in 1998 and a PhD in the same subject from the University of Kent at Canterbury in 2002.\n\nIt has been speculated that Mousavian's father's connections in Motalefeh helped him win access to leading IRP figures, as a consequence of which he was named editor-in-chief of the \"Tehran Times\", the revolution's English-language newspaper, which was established by IRP founder Mohammad Hosseini Beheshti. During his editorial tenure, from 1980 to 1990, Mousavian authored more than 2,000 articles. He also held a number of positions in the Iranian government in the 1980s, including a stint as Vice President of the Islamic Propagation Organization (1981–1983), worked with future-President Hashemi Rafsanjani as Chairman of the Parliament Administration Organization (1983–1986), and was Head and then Director General of the West Europe department of the Foreign Ministry (1986–1989). During the 1980s, Mousavian played a major role in what he later described, in a 2012 book, as “Iran's humanitarian intervention to secure the release of Western hostages in Lebanon.” However, the hostage-takers (Hezbollah), had acted on instructions from the government in Tehran.\n\nMousavian was Iran's Ambassador to Germany from 1990 to 1997. Four Iranian dissidents were murdered at Berlin's Mykonos restaurant in 1992, and five years later a German court concluded that Iran's Special Affairs Committee had ordered the murders, and that the supreme leader, president, foreign minister and intelligence minister of Iran were all active members of that committee. The court found an Iranian intelligence officer and three Lebanese men guilty and issued an arrest warrant for Iran's intelligence minister. Also, Germany expelled four Iranian diplomats and asked that Mousavian be recalled to Iran; he did return to Tehran shortly thereafter. After the issuance of the arrest warrant for the intelligence minister, which Mousavian described as an insult to the entire population of Iran, Iranian news agencies made veiled threats against Germans abroad. Mousavian echoed, saying that if European nations kept treating Iran as America and Israel did, then they would be treated the same way by Iran.\n\nIt was also during Mousavian's ambassadorial tenure that the Salman Rushdie affair took place, and Mousavian went on German radio to announce that Iran would not lift its fatwa against the writer.\n\nIn the same interview, he expressed skepticism that Germany would risk its trade relations with Iran by protesting the Rushdie fatwa. Because of this statement, Social Democratic politician Freimut Duve called for Mousavian's expulsion, saying that he had, in effect, publicly expressed agreement with the fatwa; politicians from other parties echoed Duve's call. The German Foreign Ministry, moreover, summoned Mousavian to a meeting.\n\nSubsequently, he headed the Foreign Relations Committee of the Supreme National Security Council (SNSC) of Iran during the eight years of Mohammad Khatami's presidency. Mousavian meanwhile continued his education in the late 1990s, receiving a master's degree from the University of Tehran in 1998 and a PhD from the University of Kent in the UK in international relations in 2002.\n\nHe later served as Foreign Policy Advisor to Ali Larijani, Secretary of the Supreme National Security Council (2005-7), and Vice President of the Center for Strategic Research and in a variety of capacities for the Expediency Discernment Council's Center for Strategic Research (CSR) from 2005 to 2008. During this period he was Editor-in-Chief or a member of the Editorial Board for numerous CSR publications, including the \"Journal of Human Rights Studies\", \"Journal of International Security and Terrorism\", \"Rahbord (Strategy) Magazine\", and \"Journal of Disarmament\".\n\nMousavian played a role in a number of key developments during his more than two decades working on Iranian foreign affairs. He helped to secure the release of two German hostages held by Hezbollah in Lebanon from 1990 to 1993 and American and other Western hostages held in Lebanon in 1998–1999, as well as contributing to the mediation of the largest-ever humanitarian exchange between Israel and Lebanese Hezbollah under Germany's auspices (1995–1996). Mousavian also played a role in Iran's cooperation with the US in Afghanistan against Al Qaeda and the Taliban in 2001. From 2003 to 2005, Mousavian was Spokesman of the Iranian nuclear negotiation team, which in 2003 agreed for Iran to provisionally suspend uranium enrichment and allow inspections by the International Atomic Energy Agency at its nuclear sites as confidence building measures. In 2004, while head of the negotiating team, Mousavian asserted Iran's sovereign right to pursue nuclear technology for civilian use and expressed satisfaction that the U.S. had been “isolated” at the IAEA in its attempt to pressure Iran. Mousavian's team was replaced shortly before the election of Mahmoud Ahmadinejad as President in 2005, in tandem with Supreme Leader Ali Khamenei's announcement that Iran would resume enrichment activities.\n\nMousavian was arrested and briefly jailed by the Ahmadinejad administration in 2007 and publicly accused by the president of espionage for allegedly providing classified information to Europeans, including the British Embassy, before being cleared by the judiciary. There was speculation that his arrest was part of a factional struggle between Ahmadinejad and a triumvirate of his opponents: Akbar Hashemi Rafsanjani, Mohammad Khatami, and Hassan Rouhani, of whom Mousavian was considered an ally. After a year, a spokesman of Iran's judiciary announced that Mousavian had been cleared of the espionage charge after an investigation by three different judges. However, the third of those judges had sentenced him to a suspended term of two years in prison and to a five-year ban from holding official diplomatic posts because of his confessed opposition to the foreign and nuclear policy of President Ahmadinejad.\n\nAccusations were renewed on August 22, 2010, when the Ministry of Intelligence issued a statement repeating the old claims publicly announced by Ahmadinejad in 2007. Mousavian's lawyer has questioned this development, arguing that the Ministry of Intelligence statement ran contrary to the verdict reached three years earlier by the Iranian judiciary.\n\nMousavian has been a visiting research scholar at Princeton University's Woodrow Wilson School of Public and International Affairs since 2009.\n\nWriting in the \"Weekly Standard\" in September 2012, Reuel Marc Gerecht argued that Mousavian's Western admirers, who see him as “a guide to a possible resolution” of the Iran crisis, “have been as naïve as he has been deceitful... To journalists and American officials, he has tried with conviction to make the case for Tehran’s 'peaceful' nuclear program. For him, the Iranian regime is 'misunderstood,' and the West, even under President Obama, has been too hostile and suspicious. Sufficient Western concessions and greater Western sensitivity are the keys to solving the nuclear contretemps.” Mousavian's “assertions that Khamenei’s intent isn’t threatening,” noted Gerecht, “are followed by hints that a bomb might, nonetheless, be logical for the Islamic Republic to develop, especially given the threatening behavior of the United States and Israel.” Gerecht further pointed out that Mousavian, in his writings and talks, is consistently “respectful towards Supreme Leader Khamenei.”\n\nGerecht noted that \"Mousavian at times claims to know a lot about Iran’s nuclear program, and then, when accusations from the IAEA are too cutting, not much at all.\" For example, \"Mousavian knew not a thing\" about the uranium-enrichment facility at Fordow until President Obama's press conference about it. Citing Mousavian's expression of trust in \"the supreme leader’s fatwa banning the use or production of nuclear weapons,\" Gerecht observes wryly that \"Mousavian is putting his trust in the man who demolished his world, incarcerated him, and forced him and his family into exile. It would be easy to say that Mousavian is just lying—and he is. But what is more intriguing is how hard it is for him to reflect critically on the revolution. In his eyes still, Iran’s faults are mostly American in origin—or tactical mistakes made by his archenemy, Ahmadinejad. Like battered Communists of old who just couldn’t stop loving the Soviet Union, Mousavian remains a party apparatchik who still loves the cause...There isn’t a word, even from the safety of Princeton, about the dark side of Khamenei and his Revolutionary Guards’ having the bomb. Little men like Mousavian—ideologues whose identities were created by the revolution—just can’t flip.\"\n\nMousavian expressed deep concern about developments in Iran after that country's 2009 presidential election, but continued “to press for the U.S. to engage Tehran in a bid to reduce regional tensions,” according to a 2010 article by Jay Solomon in the \"Wall Street Journal\". Mousavian believes that any future Iranian government will continue to pursue a nuclear fuel program, and that the U.S. should therefore improve relations with Tehran as a safeguard against an atomic weapons program. He also opposes UN sanctions against Iran, saying they “only serve to radicalize [Tehran’s] position.” Instead of imposing sanctions, the U.S. should “shape a comprehensive dialogue with Iran based on shared interests in stabilizing Iraq and Afghanistan” and “should develop with Tehran a broad security plan for the Persian Gulf that could prove crucial to securing the free flow of energy in and out of the strategic waterway.”\n\nMousavian dismissed as “politically motivated” the conclusions of a November 2011 report by the International Atomic Energy Agency (IAEA) that suggested “possible military dimensions” to Iran's nuclear program.\n\nWriting in \"National Interest\" in December 2012, Mousavian questioned the assumption by Western powers that “the best hope of altering Tehran’s nuclear policy and halting its enrichment activities” lies in “comprehensive international sanctions and a credible threat of military strike.” Mousavian maintained that \"such punitive pressures... will not change the Iranian leadership’s mindset, and that a military option would be catastrophic\" for everyone. Besides, he insisted, Iran has no interest in acquiring nuclear weapons. He offered ten reasons why this is the case, among them that developing nuclear weaponry would be considered “forbidden or haram” under Islam, would “trigger a regional nuclear arms race,” and would be contrary to Iran's goal of becoming a modern nation.\n\nIn another December 2012 article, Mousavian argued that \"U.S. strategists... should understand... the role of religion and clerics\" in Iran. Noting that the world's “most powerful ideological-political party... is the Shia Cleric Organization in Iran,” he complained that “Western policymakers and some of their ‘Iranian experts’... have yet to realize that the clerical system... is a collective decision-making process, with heated discussions and debates among about twenty of the most revered Grand Ayatollahs in the country.\" Washington, he emphasized, “needs to recognize that Islam is the main source of Iranian power, and the religious establishments will play a key role in the future developments of Iran and the region...Instead of changing Iran’s government, the United States should focus its regional policy on three key areas: addressing the Palestinian issue, engaging moderate Islamists, and pursuing a regional security pact.”\n\nIn a January 2013 \"New York Times\" op-ed headlined \"How to Talk to Iran,\" Mousavian and Mohammed Ali Shabani argued that there will be no “resolution of the nuclear standoff” between Iran and the West if Western leaders fail to grasp two key concepts: \"maslahat,\" meaning \"expediency, or self-interest\" and \"aberu,\" meaning \"face – as in, saving face.\" For millennia, they wrote, \"Persian culture has been distinguished by customs that revolve around honor and esteem...There are almost no instances in modern Iranian history when maslahat has trumped aberu. The West has poorly understood these concepts. This was particularly true under President Bush, who rewarded Iran’s tacit acceptance of the American invasion of Afghanistan by labeling Iran a member of an 'axis of evil.'\" Mousavian and Shabani expressed the belief that \"Iran would be open to new measures regarding the transparency of its nuclear program, and would agree not to pursue any capability to enrich uranium beyond that needed to fuel atomic power plants, if its legitimate right to enrichment under the Nuclear Nonproliferation Treaty was recognized and if an agreement to remove sanctions was reached.\"\nReviewing Mousavian's 2012 book, \"Iranian Nuclear Crisis: A Memoir\" in the \"Wall Street Journal\", Sohrab Ahmari said that although the book “is billed as a memoir,” it is more of “a diplomatic brief, complete with awkward bureaucratic prose and key sections stippled by bullet points.” Ahmari drew attention to “the endnotes, where an editor apparently felt obliged to acknowledge where the author parts company with the public record,” as in an assertion that Iran had fully disclosed the details of its nuclear activities to the IAEA.\n\n\n"}
{"id": "36594159", "url": "https://en.wikipedia.org/wiki?curid=36594159", "title": "Solar Liberty", "text": "Solar Liberty\n\nSolar Liberty is an American company that installs, sells and leases solar panel energy systems, known as photovoltaic (PV) systems, for homes, businesses, schools, universities, municipalities, non-profits, and other facilities and properties.\n\nSolar Liberty was founded in 2003 in Buffalo, New York by brothers Adam Rizzo and Nathan Rizzo. Solar Liberty was named #1 Largest Solar Installer in New York State in 2018 (http://www.solarliberty.com/news/335-solar-liberty-recognized-as-top-solar-contractor-in-new-york.html). In 2008 it was ranked number 92 by Inc. Magazine in its annual list of fastest growing private companies in the U.S. In 2012, it was ranked the number 6 fastest-growing private company in Western New York by the Business First publication.\n\nThe company headquarters is in Buffalo, NY—with offices in Rochester, Albany, Corning, Syracuse, Binghamton, and Plattsburgh (all in New York).\n\nSolar Liberty is currently the #1 Largest Solar Installer in New York State. Installations are predominantly located in Western, Central New York State, New York City area and Northern Pennsylvania. Notable installations include, but are not limited to: Buffalo City Mission (48 kW, Donated in 2010), University at Buffalo Interactive 'Solar Strand' (2014), Cummins Engines (2013), Rochester Institute of Technology (RIT, 2 MW, 2014), Life Storage (formerly Uncle Bobs Storage, 27 locations, 2015), Town of Holland (2017), City of Rochester (Landfill, 2017), Monroe County (2017), and St. John's Annex Cemetery in Long Island (10 MW, Acquired by PSEG Solar Source, 2017) which is the 'Second-Largest Solar System in New York State).\n\nThe Solar Liberty Foundation provides funding for renewable energy projects in developing nations. Solar Liberty contributes tremendously to the Solar Liberty Foundation. Completed projects include a health clinic, school and orphanage in Haiti, solar cookers in Kenya, and more solar donations to Liberia and Tanzania. During Fall 2017, a group of Solar Liberty volunteers traveled to Africa to provide solar energy to an all-girls school in Kitenga which is in Tanzania, Africa. This was the first time in history Kitenga has seen electricity. The Solar Liberty Foundation will continue to provide solar energy to underdeveloped communities throughout the world.\n\n\nhttp://www.solarliberty.com/news/335-solar-liberty-recognized-as-top-solar-contractor-in-new-york.html\nhttps://www.wnypapers.com/news/article/current/2018/07/26/133485/buffalo-based-solar-liberty-recognized-as-top-solar-contractor-in-new-york\n\n\n"}
{"id": "351036", "url": "https://en.wikipedia.org/wiki?curid=351036", "title": "Staple (fastener)", "text": "Staple (fastener)\n\nA staple is a type of two-pronged fastener, usually metal, used for joining or binding materials together. Large staples might be used with a hammer or staple gun for masonry, roofing, corrugated boxes and other heavy-duty uses. Smaller staples are used with a stapler to attach pieces of paper together; such staples are a more permanent and durable fastener for paper documents than the paper clip.\n\nThe word \"staple\" originated in the late thirteenth Century, from Old English \"stapol\", meaning \"post, pillar\". The word's first usage in the paper-fastening sense is attested from 1895.\n\n\nConstruction staples are commonly larger, have a more varied use, and are delivered by a staple gun or hammer tacker. Staple guns do not have backing anvils and are exclusively used for tacking (with the exception of outward-clinch staplers used for fastening duct insulation). They typically have staples made from thicker metal. Some staple guns use arched staples for fastening small cables, e.g. phone or cable TV, without damaging the cable. Devices known as hammer tackers or staple hammers operate without complex mechanics as a simple head loaded with a strip of staples drives them directly; this method requires a measure of skill. Powered electric staplers or pneumatic staplers drive staples easily and accurately; they are the simplest manner of applying staples, but are hindered by a cord or hose. Cordless electric staplers use a battery, typically rechargeable and sometimes replaceable.\n\nThe term \"stapling\" is used for both fastening sheets of paper together with bent legs or fastening sheets of paper to something solid with straight legs; however, when differentiating between the two, the term \"tacking\" is used for straight-leg stapling, while the term \"stapling\" is used for bent-leg stapling.\n\nModern staples for paper staplers are made from zinc-plated steel wires glued together and bent to form a long strip of staples. Staple strips are commonly available as \"full strips\" with 210 staples per strip. Both copper plated and more expensive stainless steel staples which do not rust are also available, but uncommon.\n\nSome staple sizes are used more commonly than others, depending on the application required. Some companies have unique staples just for their products. Staples from one manufacturer may or may not fit another manufacturers unit even if they look similar and serve the same purpose.\n\nStaples are often described as X/Y (e.g. 24/6 or 26/6), where the first number X is the gauge of the wire, and the second number Y is the length of the shank (leg) in millimeters. Some exceptions to this rule include staple sizes like No. 10.\n\nCommon sizes for the home and office include: 26/6, 24/6, 24/8, 13/6, 13/8 and No. 10 for mini staplers. Common sizes for heavy duty staplers include: 23/8, 23/12, 23/15, 23/20, 23/24, 13/10, and 13/14.\n\nStapleless staplers cut and bend paper without using metal fasteners.\n\nThere are few standards for staple size, length and thickness. This has led to many different incompatible staples and staplers systems, all serving the same purpose or applications.\n\n24/6 staples are described by the German DIN 7405 standard.\n\nIn the United States, the specifications for non-medical industrial staples are described in ASTM F1667-15, Standard Specification for Driven Fasteners: Nails, Spikes, and Staples. A heavy duty office staple might be designated as F1667 STFCC-04: ST indicates staple, FC indicates flat top crown, C indicates cohered (joined into a strip), and 04 is the dash number for a staple with a length of 0.250 inch (6 mm), a leg thickness of 0.020 inch (500 μm), a leg width of 0.030 inch (800 μm), and a crown width of 0.500 inch (13 mm).\n\nThe most common staples are used with paper. They are almost exclusively applied with a mechanical stapler which clinches the legs after they pass through the paper. Staples of this type are used with a desktop stapling machine.\n\nWhen stapling with a stapler the papers to be fastened are placed between the main body and the anvil. The papers are pinched between the body and the anvil, then a drive blade pushes on the crown of the staple on the end of the staple strip. The staple breaks from the end of the strip and the legs of the staple are forced through the paper. As the legs hit the grooves in the anvil they are bent to hold the pages together. Many staplers have an anvil in the form of a \"pinning\" or \"stapling\" switch. This allows a choice between bending in or out. The outward bent staples are easier to remove and are for temporary fastening or \"pinning\".\n\nMost staplers are capable of stapling without the anvil to drive straight leg staples for tacking.\n\nThere are various types of staples for paper, including heavy-duty staples, designed for use on documents 20, 50, or over 100 pages thick. There are also speedpoint staples, which have slightly sharper teeth so they can go through paper more easily.\n\nStaples are commonly considered to be a neat and efficient method of binding paperwork such as letters and documents in all areas of office business. This is predominantly because of the low cost and high availability of the staple, and because its small size does not detract from the content of the document.\n\nThe large staples found on corrugated cardboard boxes have folded legs, but they are applied from the outside and do not use an anvil; jaw-like appendages push through the cardboard alongside the legs and bend them from the outside.\n\nSaddle stitch staplers, also known as \"booklet staplers,\" feature a longer reach from the pivot point than general-purpose staplers and are used to bind pages into a booklet or \"signature\". Some, such as the Ring King, can also use \"loop-staples\" that enable the user to integrate folded matter into ring books and binders.\n\nOutward clinch staples are blind staples, i.e. there is no anvil, and they are applied with a staple gun. When applied, each staple leg forms a curve bending outwards. This is in part caused by the shape of the crown, which is like an inverted \"V\", and not flat as in ordinary staples. Also, the legs are sharpened with an inside bevel point, causing them to tend to go outwards when forced into the base material. These staples are used for upholstery work, especially in vehicles, where they are used for fastening fabric or leather to a foam base. These staples are also used when installing fiberglass insulation batts around air ducts- the FSK paper sheathing is overlapped, and the two layers are stapled together before sealing with tape.\n\nStaples are used in various types of packaging.\n\nSurgical staples are used for the closing of incisions and wounds, a function also performed by sutures.\n\nIn ancient times, the staple had different functions.\n\nLarge metal staples dating from the 6th century BC have been found in the masonry works of the Persian empire (ancient Iran). For the construction of the Pasargadae and later Ka'ba-ye Zartosht, these staples, which are known as \"dovetail\" or \"swallowtail\" staples, were used for tightening stones together.\n\nThe home stapling machine was developed by Henry Heyl in 1877 and registered under US Patent No. 195,603. Heyl's companies, American Paper-Box Machine Company, Novelty Paper Box Company, and Standard Box Company, all of Philadelphia, manufactured machinery using staples in paper packaging and for saddle stitching.\n\n\n"}
{"id": "806079", "url": "https://en.wikipedia.org/wiki?curid=806079", "title": "Subroto (politician)", "text": "Subroto (politician)\n\nSubroto (born 19 September 1923) is an Indonesian administrator and economist. He was a doctoral graduate and faculty member of University of Indonesia between 1956 and 1963, Minister of Energy and Natural Resources between 1978 and 1988, and Secretary General of OPEC between 1988 and 1994, nearly double the tenure of his longest-serving predecessor in that position. Like many Indonesians, Subroto is known by just a single name.\n\n\n"}
{"id": "679991", "url": "https://en.wikipedia.org/wiki?curid=679991", "title": "Synthetic crude", "text": "Synthetic crude\n\nSynthetic crude is the output from a bitumen/extra heavy oil upgrader facility used in connection with oil sand production. It may also refer to shale oil, an output from an oil shale pyrolysis. The properties of the synthetic crude depend on the processes used in the upgrading. Typically, it is low in sulfur and has an API gravity of around 30. It is also known as \"upgraded crude\".\n\nSynthetic crude is an intermediate product produced when an extra-heavy or unconventional oil source is upgraded into a transportable form. Synthetic crude is then shipped to oil refineries where it is further upgraded into finished products. Synthetic crude may also be mixed, as a diluent, with heavy oil to create synbit. Synbit is more viscous than synthetic crude, but can also be a less expensive alternative for transporting heavy oil to a conventional refinery.\n\nSyncrude Canada, Suncor Energy Inc., and Canadian Natural Resources Limited are the three largest worldwide producers of synthetic crude with a cumulative production of approximately . The NewGrade Energy Upgrader became operational in 1988, and was the first upgrader in Canada, now part of the CCRL Refinery Complex. \n\n\n"}
{"id": "679828", "url": "https://en.wikipedia.org/wiki?curid=679828", "title": "Toboggan", "text": "Toboggan\n\nA toboggan is a simple sled which is a traditional form of transport used by the Innu and Cree of northern Canada.\n\nIn modern times, it is used on snow to carry one or more people (often children) down a hill or other slope for recreation. Designs vary from simple, traditional models to modern engineered composites. A toboggan differs from most sleds or sleighs in that it has no runners or skis (or only low ones) on the underside. The bottom of a toboggan rides directly on the snow. Some parks include designated toboggan hills where ordinary sleds are not allowed and which may include toboggan runs similar to bobsleigh courses. \n\nToboggans can vary depending on the climate and geographical region. Such examples are Tangalooma (Australia) where Toboggans are made from Masonite boards and used for travelling down steep sand dunes at speeds up to 40km per hour.\n\nThe traditional toboggan is made of bound, parallel wood slats, all bent up and backwards at the front to form a recumbent 'J' shape. A thin rope is run across the edge of end of the curved front to provide rudimentary steering. The frontmost rider places their feet in the curved front space and sits on the flat bed; any others sit behind them and grasp the waist of the person before them.\n\nModern recreational toboggans are typically manufactured from wood or plastic or aluminum. Larger, more rugged models are made for commercial or rescue use. \n\nThe toboggan is a recurring prop in the Calvin and Hobbes comic. Comic author Bill Watterson uses it (or, alternatively, a wagon) as \"a simple device to add some physical comedy to the strip, and most often use[s] it when Calvin gets longwinded or philosophical.\"\n\n\n"}
{"id": "31627160", "url": "https://en.wikipedia.org/wiki?curid=31627160", "title": "Tornado preparedness", "text": "Tornado preparedness\n\nThe term \"tornado preparedness\" refers to safety precautions made before the arrival of and during a tornado. Historically, the steps taken have varied greatly, depending on location, or time remaining before a tornado was expected. For example, in rural areas, people might prepare to enter an external storm cellar, in case the main building collapses, and thereby allow exit without needing rescue from the main building as in urban areas. Because tropical storms have spawned many tornadoes, hurricane preparations also involve tornadoes. The term \"tornado preparedness\" has been used by government agencies, emergency response groups, schools, insurance companies, and others.\n\nPreparedness involves knowing the major dangers to avoid. Some tornadoes are the most violent storms in nature. Tornadoes have varied in strength, and some tornadoes have been mostly invisible due to a lack of loose dirt or debris in the funnel cloud. Spawned from severe thunderstorms, tornadoes have caused fatalities and devastated neighborhoods within seconds of arrival.\nA tornado operates as a rotating, funnel-shaped cloud that extends downward from a thunderstorm, to the ground, with swirling winds which have reached . The wind speed might be difficult to imagine: traveling the length of a U.S. football field within 1 second (over per second). Damage paths have been in excess of and .\n\nNot all tornadoes are easily seen. A tornado funnel can be transparent until reaching an area with loose dirt and debris. Also, some tornadoes have been seen against sunlit areas, but rain or nearby low-hanging clouds has obscured other tornadoes. Occasionally, tornadoes have developed so suddenly, so rapidly, that little, if any, advance warning was possible.\n\nBefore a tornado strikes an area, the wind has been known to die down and the air to become very still. A cloud of debris has sometimes marked the bottom of a tornado even when the funnel was not visible. Tornadoes typically occur along the trailing edge of a thunderstorm.\n\nThe following is a summary of typical tornadoes:\n\nThe U.S. Federal Emergency Management Agency (FEMA) has advised the following precautions before a storm reaches an area:\n\nUpon seeing an approaching storm or noticing any of the danger signs, they were advised to prepare to take shelter immediately, such as moving to a safe room, internal stairway, or other safe-haven area.\n\nAll individuals and families should have a disaster preparedness kit made prior to tornado. According to FEMA the kit should include items needed to shelter in place in the event of a disaster such as a tornado for up to 72 hours following impact. \n\nDuring August 2010, FEMA advised people to perform the following actions when a tornado struck.\n\nBecause some preparations vary, depending on location, people have been advised to consult their local area preparedness plans, rather than assume the plans are similar for all areas, such as which local buildings have been designated as storm shelters.\n\nA 2012 study of tornado injuries found that wearing a helmet such as those used for American football or bicycling, is an effective way to reduce injuries and deaths from head trauma. As of 2012, the CDC endorsed only general head protection, but recommended that if helmets are to be used, they be kept close by to avoid wasting time searching for them.\n\nAfter the 2013 Moore tornado, it became apparent that thousands of people attempt to flee major tornadoes, and this has been credited with reducing the death toll. However, during this event some people were killed as the tornado passed over the traffic jam caused by the impromptu evacuation. In addition to urban traffic, evacuation can also be hampered by flash flooding produced by associated thunderstorms, and the need to be certain about the position and direction of the tornado. Others who did not flee the Moore tornado were also killed because the buildings they were hiding in were completely destroyed, highlighting the need for storm shelters and safe rooms constructed specifically to withstand very high winds.\n\nDepending on location, various safe-haven areas have been prepared. The goal has been to avoid outer walls which might collapse when a roof section becomes airborne and the walls below lose their upper support: many interior rooms resist collapse longer, due to smaller walls interconnected to each other, while outer walls deflect the force of the winds. Because mobile homes typically lack foundation anchors and present a large surface-area sail (to catch wind), the advice has been to seek a safe haven elsewhere, such as in a stronger nearby building. When a mobile home begins to roll, people have been injured by hitting objects inside, or being crushed when a trailer suddenly hits the ground and begins to collapse around them.\n\nIn a multi-story building, an internal stairway (away from broken windows) often acts as a safe haven, due to the stairs reinforcing the walls and blocking any major debris falling from above. If a stairway is lined with windows, then there would be the danger of flying glass, so an interior stairway, or small inner room, would be preferable.\n\nIn private homes, some similar stairway rooms have been used, or an interior room/closet kept clear to quickly allow entry when a storm is seen or heard approaching (the wind roar intensifies, sounding like a swift \"freight train\" coming nearer, louder). With weeks or months to prepare, an interior safe room can be constructed, with space for emergency water, food and flashlights, and a telephone to call for rescue if the exit becomes blocked by falling debris. Some above-ground safe rooms have been built with steel-rebar rods in cement-filled cinder blocks, to withstand winds of . Rural homes might have an outside storm cellar, or other external bunker, to avoid being trapped within a collapsing house. In rural homes, generators are also helpful to maintain power with enough fuel for a few days.\n\nThere were no building codes requiring tornado shelters nor specifically designed to prevent tornado damage until the 2011 Joplin tornado prompted a local ordinance requiring hurricane ties or similar fasteners. The state of Oklahoma adopted the minimum U.S. standard that year for the first time, but did not add high-wind protections like those in Florida designed to protect against hurricanes. Other states in Tornado Alley have no statewide building codes. The chance of any given location in Tornado Alley getting hit by an F-2 tornado (strong enough to do major structural damage and exceeding the 90 mph guideline for straightline winds) is about 1 every 4,000-5,000 years; in other areas the annual probability is one in several million. The most stringent building codes only require earthquake strengthening for a 1 in every 500-1,000 year probability.\n\nThe U.S. Federal Emergency Management Agency has spent tens millions of dollars subsidizing the construction of shelters and safe rooms in both private and public buildings. Many buildings in Tornado Alley do not have basements, because unlike in more northern areas, there is no need for a deep foundation to get below the frost line, in some places the water table is high, and expansion and contraction of clay-heavy soils can produce additional pressure on buildings that can cause leaks if not reinforced.\n\nHaving a first aid kit in the safe haven is advised to help victims recover from minor injuries. People needing prescription medications could have a medicine bag ready to take to shelter. Some people have reported their \"ears popping\" due to the change in air pressure, but those effects seem to be temporary. Covering people with mattresses or cushions has helped avoid injury from flying debris, as walls collapsed nearby.\n\nInjuries sustained during a tornado vary in nature and in severity. The most common injuries experienced during a tornado are complex contaminated soft tissue wounds and account for more than 50% of the cases seen by emergency rooms following a tornado. These wounds will most likely be contaminated with soil and foreign bodies due to high wind speeds caused by tornadoes. Fractures are the second most common injury obtained after a tornado strikes and account for up to 30% of total injuries. Head injuries are also commonly reported during a tornado, but severe head injuries only account for less than 10% of the total. Even though only 10% of reported head injuries are severe, they are the most common cause of death following a tornado. Blunt trauma to the chest and abdomen are also injuries obtained following a tornado, but only account for less than 10% of overall injuries.\n\nTornado drills are an important element in tornado preparedness. Like any other safety drills, they increase chances of correct response to a real tornado threat.\n\nMost states in the midwestern and southern United States conduct a statewide tornado drill in late winter or early spring in preparation for the severe weather season. During these drills, the National Weather Service issues test tornado warnings, and local Emergency Alert Systems and/or NOAA Weather Radio (normally as a Required Weekly Test or Required Monthly Test; Live Tornado Warning Codes can only be used if a waiver from the FCC is granted since \"Live Code Testing\" is prohibited per regulations) are activated along with outdoor warning sirens. Schools and businesses may also conduct a tornado drill simultaneously.\n\nA tornado drill is a procedure of practicing to take cover in a specified location in the event that a tornado strikes an area. This is an important element of tornado preparedness.\n\nGenerally, a signal is given, such as a series of tones (ex. Continuous Tone), or a voice announcement. Upon receiving the signal, building occupants of schools, hospitals, factories, shopping centers, etc. proceed to a designated location, usually an interior room or corridor with no windows, and assume a protective position.\n\nIn homes and small buildings one must go to the basement or an interior room on the lowest floor (closet, bathroom), to stay away from glass.\n\nCars and mobile homes must be abandoned, if there is small chance to drive away.\n\nIn some jurisdictions, schools are required to conduct regular tornado drills, though generally less frequently than fire drills.\n\nIn many states tornado drills are part of the Severe Weather Awareness Week.\n\n\n\n"}
{"id": "33127165", "url": "https://en.wikipedia.org/wiki?curid=33127165", "title": "Yuhua Stone", "text": "Yuhua Stone\n\nThe Yuhua stone () is a special kind of stone found in Nanjing, China, due to the unique geology of the area. Yuhua stones, or pebbles, are of sedimentary origin and consist of minerals including quartz and other silicates. They are found in the Yangtze River, polished smooth by the action of water. Colors include red, rose, yellow, green, and white.\n\nThe origin of the Yuhua Stone is told through the legend of a Chinese monk named Yunguang (雲光) who lived during the Liang dynasty. Yunguang used to travel around the country to deliver sermons.One day, He travelled to Nanjing, there, he sat on the Guanghua Gate for three days, talking about his concept, without eating or drinking. According to the legend, his sincerity moved God deeply, so God rained thousands of colourful flowers from the sky to reward him. When the flowers touched the ground, they turned into the Yuhua Stone.\n\nThe stone is known for its beautiful patterns, often imagined to look like animals or flowers.\n\n\"\", means lots of flowers falling from the sky\n\n"}
