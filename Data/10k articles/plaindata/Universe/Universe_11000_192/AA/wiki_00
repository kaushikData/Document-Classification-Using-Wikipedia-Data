{"id": "24582041", "url": "https://en.wikipedia.org/wiki?curid=24582041", "title": "2001 Myrtle Beach tornadoes", "text": "2001 Myrtle Beach tornadoes\n\nThe Myrtle Beach tornadoes were F1 and F2 tornadoes which touched down in Myrtle Beach, South Carolina on Friday, July 6, 2001. At 4:05 PM EDT, the first tornado touched down. It crossed U.S. Route 17 in North Myrtle Beach while reaching a maximum width of 100 yards. It was estimated to be an F1 in strength, and was on the ground for 0.1 miles. The other tornado affected Myrtle Beach and four damage paths were found. The longest path, which could have been the parent tornado, spanned for 2 miles from 4:15 to 4:25 PM EDT and was estimated to have F2 strength. The tornado moved slowly southward along the beach, alternately moving on and offshore several times, and caused significant damage, including blown-out windows, tipped-over buses, and damaged roofs and utility poles. Thirty-nine people received minor injuries from this second tornado, which was rated F2. No serious injuries or fatalities were associated with this storm event.\n\nNear southern Myrtle Beach, evidence was found for three other damage paths that were estimated to have F0 and F1 strength. These tornadoes caused roof and tree damage.\nThe storm that produced these tornadoes occurred during the Fourth of July weekend as an estimated 400,000 vacationers were at Myrtle Beach. Damage from the tornadoes is estimated to be up to $8,000,000 - with an estimated $1,000,000 damages associated with vehicles. 4,000 residents were without power during the worst parts of the storm. \n\n"}
{"id": "21141486", "url": "https://en.wikipedia.org/wiki?curid=21141486", "title": "4-1000A", "text": "4-1000A\n\nThe 4-1000A/8166 is a radial-beam tetrode designed for use in radio transmitters. The 4-1000A is the largest of a series of tubes including the 4-65A, 4-125A, 4-250A, and the 4-400A. These tubes share a common naming convention in which the first number identifies the number of elements contained within the tube; \"i.e.\", the number “4” identifies the tube as a tetrode which contains four elements (filament, control grid, screen grid, and anode), and the second number indicates the maximum continuous power dissipation of the anode in Watts. The entire family of tubes can be used as oscillators, modulators, and amplifiers.\n\nThe 4-1000A is a relatively large glass tube with an overall height of 9.25 inches and a diameter of 5 inches. It is designed to operate with its plate (anode) at an orange-red color due to the \"getter\" being a zirconium compound on the anode structure which requires a great deal of heat to be effective. The cathode is a directly-heated, thoriated-tungsten filament rated at 7.5 volts at 21 amperes. Connections to the filament and grids are made via a special 5-pin socket, and the anode connection is at the top of the tube.\n\nThe tube may be operated as a class C amplifier in which a single tube can provide up to 3340 watts of RF power. A pair of tubes may be operated as an audio-frequency modulator for an AM transmitter; in this case a pair of tubes will provide up to 3,840 watts of audio power.\n\nThe 4-1000A is of radial construction; the most obvious feature is the large, roughly cylindrical, blackish anode suspended from the top of the tube. The anode, which may be constructed out of metal or graphite, is finned for increased heat dissipation. The filament and grids are supported from the lower section/base of the tube.\n\nThe 4-1000A was available from multiple manufacturers including RCA, EIMAC, AMPEREX, and Triton.\n\nA complete cooling system is required to operate the tube at rated values. A centrifugal fan pressurizes the equipment chassis and provides a continuous stream of cooling air. A specially designed socket is used to direct the air over the filament and grid connections, and a glass chimney (Pyrex®) then directs the air around the tube's glass envelope; chassis-mounted metal clips are used to center the chimney around the tube. Finally, a finned, cylindrical heat sink is attached to the anode connection (top of the tube) to provide additional cooling for the anode's glass-to-metal seal.\n\nThe 4-1000A was modified for operation in pulsed applications; the modified tube, identified as the 4PR1000A/8189, can be operated with anode voltages as high as 30 kV DC and peak plate current of 8.0 Amperes, but at a reduced duty cycle.\n"}
{"id": "53424670", "url": "https://en.wikipedia.org/wiki?curid=53424670", "title": "Avedøre Holme Offshore Wind Farm", "text": "Avedøre Holme Offshore Wind Farm\n\nAvedøre Holme Offshore Wind Farm is a nearshore wind farm right off the coast of Avedøre, Copenhagen. It was commissioned in 2009 with three 3.6 MW Siemens turbines as a demonstrator project for future offshore wind turbines.\n\n"}
{"id": "28639423", "url": "https://en.wikipedia.org/wiki?curid=28639423", "title": "Barry Stevens (technology developer)", "text": "Barry Stevens (technology developer)\n\nBarry Stevens (born 1949) is an American technology business developer, scientist, author, speaker and entrepreneur in technology-driven enterprises; Founder of TBD America Inc., a technology business development group (1997).\n\nStevens received his Ph.D. in Inorganic Chemistry from Rutgers University - Newark, an M.S. in Inorganic Chemistry from Rutgers University - New Brunswick, and a B.S. in Science (triple major: Biology, Chemistry, and Physics) from Fairleigh Dickinson University where he graduated Magna Cum Laude and Valedictorian.\n\nNamed in five U.S. patents, Barry Stevens' history in business, science and technology can be traced from 1978 involvement in developing the VideoDisc at RCA and CBS Records, through years at Eastman Kodak (1984–1989), receiving their Office of Innovation \"Recognition Award\" two years in a row (1986–1987), through current renewable energy efforts and scholarship in renewable and clean energy technologies, including involvement in biofuel development with DuBay Biofuel (DuBay Ingredients LLC) and renewable energy business development with TBD America, Inc.\n\n\n"}
{"id": "18714265", "url": "https://en.wikipedia.org/wiki?curid=18714265", "title": "Bucharest South Power Station", "text": "Bucharest South Power Station\n\nThe Bucharest South Power Station is a large thermal power plant located in Bucharest, having 2 generation groups of 50 MW each, 2 units of 100 MW and 2 units of 125 MW having a total electricity generation capacity of 550 MW.\n\nThe two chimneys of the power station are 140 metres tall.\n\n"}
{"id": "1029195", "url": "https://en.wikipedia.org/wiki?curid=1029195", "title": "Busbar", "text": "Busbar\n\nIn electric power distribution, a busbar (also bus bar) is a metallic strip or bar, typically housed inside switchgear, panel boards, and busway enclosures for local high current power distribution. They are also used to connect high voltage equipment at electrical switchyards, and low voltage equipment in battery banks. They are generally uninsulated, and have sufficient stiffness to be supported in air by insulated pillars. These features allow sufficient cooling of the conductors, and the ability to tap in at various points without creating a new joint. \n\nThe material composition and cross-sectional size of the busbar determine the maximum amount of current that can be safely carried. Busbars can have a cross-sectional area of as little as , but electrical substations may use metal tubes in diameter () or more as busbars. An aluminium smelter will have very large busbars used to carry tens of thousands of amperes to the electrochemical cells that produce aluminium from molten salts.\n\nBusbars are produced in a variety of shapes, such as flat strips, solid bars, or rods, and are typically composed of copper, brass, or aluminium as solid or hollow tubes. Some of these shapes allow heat to dissipate more efficiently due to their high surface area to cross-sectional area ratio. The skin effect makes 50–60 Hz AC busbars more than about thickness inefficient, so hollow or flat shapes are prevalent in higher current applications. A hollow section also has higher stiffness than a solid rod of equivalent current-carrying capacity, which allows a greater span between busbar supports in outdoor electrical switchyards. \n\nA busbar must be sufficiently rigid to support its own weight, and forces imposed by mechanical vibration and possibly earthquakes, as well as accumulated precipitation in outdoor exposures. In addition, thermal expansion from temperature changes induced by ohmic heating and ambient temperature variations, as well as magnetic forces induced by large currents, must be considered. In order to address these concerns, flexible bus bars, typically a sandwich of thin conductor layers, were developed. These require a structural frame or cabinet for their installation.\n\nDistribution boards split the electrical supply into separate circuits at one location. Busways, or bus ducts, are long busbars with a protective cover. Rather than branching from the main supply at one location, they allow new circuits to branch off anywhere along the route of the busway.\n\nA busbar may either be supported on insulators, or else insulation may completely surround it. Busbars are protected from accidental contact either by a metal earthed enclosure or by elevation out of normal reach. Power neutral busbars may also be insulated because it is not guaranteed that the potential between power neutral and safety grounding is always zero. Earthing (safety grounding) busbars are typically bare and bolted directly onto any metal chassis of their enclosure. Busbars may be enclosed in a metal housing, in the form of bus duct or busway, segregated-phase bus, or isolated-phase bus.\n\nBusbars may be connected to each other and to electrical apparatus by bolted, clamped, or welded connections. Often, joints between high-current bus sections have precisely-machined matching surfaces that are silver-plated to reduce the contact resistance. At extra high voltages (more than 300 kV) in outdoor buses, corona discharge around the connections becomes a source of radio-frequency interference and power loss, so special connection fittings designed for these voltages are used.\n\n\n"}
{"id": "2096699", "url": "https://en.wikipedia.org/wiki?curid=2096699", "title": "Carboxylate", "text": "Carboxylate\n\nA carboxylate is a salt or ester of a carboxylic acid.\n\"Carboxylate salts\" have the general formula M(RCOO), where M is a metal and \"n\" is 1, 2...; \"carboxylate esters\" have the general formula RCOOR′. R and R′ are organic groups; R′ ≠ H.\n\nA carboxylate ion is the conjugate base of a carboxylic acid, RCOO. It is an ion with negative charge.\n\nCarboxylic acids easily dissociate into a carboxylate anion and a positively charged hydrogen ion (proton), much more readily than alcohols do (into an alkoxide ion and a proton), because the carboxylate ion is stabilized by resonance. The negative charge that is left after deprotonation of the carboxyl group is delocalized between the two electronegative oxygen atoms in a resonance structure.\n\nThis delocalization of the electron cloud means that both of the oxygen atoms are less strongly negatively charged; the positive proton is therefore less strongly attracted back to the carboxylate group once it has left; hence, the carboxylate ion is more stable. In contrast, an alkoxide ion, once formed, would have a strong negative charge on the oxygen atom, which would make it difficult for the proton to escape. Carboxylic acids thus have a lower pH than alcohols: the higher the number of protons in solution, the lower the pH.\n\n"}
{"id": "33668371", "url": "https://en.wikipedia.org/wiki?curid=33668371", "title": "Ceresin", "text": "Ceresin\n\nCeresin (also cerin, cerasin, cerosin, ceresin wax or ceresine) is a wax derived from ozokerite by a purifying process.\n\nThe purifying process of the ozokerite commonly comprises a treatment with heat and sulfuric acid, but other processes are also in use.\n\nUses include:\n\n"}
{"id": "18152654", "url": "https://en.wikipedia.org/wiki?curid=18152654", "title": "Constant-energy surface", "text": "Constant-energy surface\n\nIn a Brillouin zone, the constant-energy surface represents the loci of all the formula_1-points (that is, all the crystal momentum values) that have the same energy. Fermi surface is a special constant-energy surface that separates the unfilled orbitals from the filled ones at zero kelvin.\n"}
{"id": "7096085", "url": "https://en.wikipedia.org/wiki?curid=7096085", "title": "Construction aggregate", "text": "Construction aggregate\n\nConstruction aggregate, or simply \"aggregate\", is a broad category of coarse to medium grained particulate material used in construction, including sand, gravel, crushed stone, slag, recycled concrete and geosynthetic aggregates. Aggregates are the most mined materials in the world. Aggregates are a component of composite materials such as concrete and asphalt concrete; the aggregate serves as reinforcement to add strength to the overall composite material. Due to the relatively high hydraulic conductivity value as compared to most soils, aggregates are widely used in drainage applications such as foundation and French drains, septic drain fields, retaining wall drains, and road side edge drains. Aggregates are also used as base material under foundations, roads, and railroads. In other words, aggregates are used as a stable foundation or road/rail base with predictable, uniform properties (e.g. to help prevent differential settling under the road or building), or as a low-cost extender that binds with more expensive cement or asphalt to form concrete.\n\nPreferred bitumenous aggregate sizes for road construction are given in EN 13043 as d/D (where the range shows the smallest and largest square mesh grating that the particles can pass). The same classification sizing is used for larger armour stone sizes in EN 13383, EN 12620 for concrete aggregate, EN 13242 for base layers of road construction and EN 13450 for railway ballast.\n\nThe American Society for Testing and Materials (ASTM) publishes an exhaustive listing of specifications including ASTM D 692 and ASTM D 1073 for various construction aggregate products, which, by their individual design, are suitable for specific construction purposes. These products include specific types of coarse and fine aggregate designed for such uses as additives to asphalt and concrete mixes, as well as other construction uses. State transportation departments further refine aggregate material specifications in order to tailor aggregate use to the needs and available supply in their particular locations.\n\nSources for these basic materials can be grouped into three main areas: Mining of mineral aggregate deposits, including sand, gravel, and stone; use of waste slag from the manufacture of iron and steel; and recycling of concrete, which is itself chiefly manufactured from mineral aggregates. In addition, there are some (minor) materials that are used as specialty lightweight aggregates: clay, pumice, perlite, and vermiculite.\n\nPeople have used sand and stone for foundations for thousands of years. Significant refinement of the production and use of aggregate occurred during the Roman Empire, which used aggregate to build its vast network of roads and aqueducts. The invention of concrete, which was essential to architecture utilizing arches, created an immediate, permanent demand for construction aggregates.\n\nVitruvius writes in :\nEconomy denotes the proper management of materials and of site, as well as a thrifty balancing of cost and common sense in the construction of works. This will be observed if, in the first place, the architect does not demand things which cannot be found or made ready without great expense. For example: it is not everywhere that there is plenty of pit-sand, rubble, fir, clear fir, and marble... Where there is no pit sand, we must use the kinds washed up by rivers or by the sea... and other problems we must solve in similar ways.\nThe advent of modern blasting methods enabled the development of quarries, which are now used throughout the world, wherever competent bedrock deposits of aggregate quality exist. In many places, good limestone, granite, marble or other quality stone bedrock deposits do not exist. In these areas, natural sand and gravel are mined for use as aggregate. Where neither stone, nor sand and gravel, are available, construction demand is usually satisfied by shipping in aggregate by rail, barge or truck. Additionally, demand for aggregates can be partially satisfied through the use of slag and recycled concrete. However, the available tonnages and lesser quality of these materials prevent them from being a viable replacement for mined aggregates on a large scale.\nLarge stone quarry and sand and gravel operations exist near virtually all population centers due to the high cost of transportation relative to the low value of the product. Trucking aggregate more than 40 kilometers is typically uneconomical. These are capital-intensive operations, utilizing large earth-moving equipment, belt conveyors, and machines specifically designed for crushing and separating various sizes of aggregate, to create distinct product stockpiles.\n\nAccording to the USGS, 2006 U.S. crushed stone production was 1.72 billion tonnes valued at $13.8 billion (compared to 1.69 billion tonnes valued at $12.1 billion in 2005), of which limestone was 1,080 million tonnes valued at $8.19 billion from 1,896 quarries, granite was 268 million tonnes valued at $2.59 billion from 378 quarries, traprock was 148 million tonnes valued at $1.04 billion from 355 quarries, and the balance other kinds of stone from 729 quarries. Limestone and granite are also produced in large amounts as dimension stone. The great majority of crushed stone is moved by heavy truck from the quarry/plant to the first point of sale or use. According to the USGS, 2006 U.S. sand and gravel production was 1.32 billion tonnes valued at $8.54 billion (compared to 1.27 billion tonnes valued at $7.46 billion in 2005), of which 264 million tonnes valued at $1.92 billion was used as concrete aggregates. The great majority of this was again moved by truck, instead of by electric train.\n\nCurrently, total U.S. aggregate demand by final market sector was 30%–35% for non-residential building (offices, hotels, stores, manufacturing plants, government and institutional buildings, and others), 25% for highways, and 25% for housing.\n\nThe largest-volume of recycled material used as construction aggregate is blast furnace and steel furnace slag. Blast furnace slag is either air-cooled (slow cooling in the open) or granulated (formed by quenching molten slag in water to form sand-sized glass-like particles). If the granulated blast furnace slag accesses free lime during hydration, it develops strong hydraulic cementitious properties and can partly substitute for portland cement in concrete. Steel furnace slag is also air-cooled. In 2006, according to the USGS, air-cooled blast furnace slag sold or used in the U.S. was 7.3 million tonnes valued at $49 million, granulated blast furnace slag sold or used in the U.S. was 4.2 million tonnes valued at $318 million, and steel furnace slag sold or used in the U.S. was 8.7 million tonnes valued at $40 million. Air-cooled blast furnace slag sales in 2006 were for use in road bases and surfaces (41%), asphaltic concrete (13%), ready-mixed concrete (16%), and the balance for other uses. Granulated blast furnace slag sales in 2006 were for use in cementitious materials (94%), and the balance for other uses. Steel furnace slag sales in 2006 were for use in road bases and surfaces (51%), asphaltic concrete (12%), for fill (18%), and the balance for other uses.\n\nGlass aggregate, a mix of colors crushed to a small size, is substituted for many construction and utility projects in place of pea gravel or crushed rock, often saving municipalities like the City of Tumwater, Washington Public Works, thousands of dollars (depending on the size of the project). Glass aggregate is not sharp to handle. In many cases, the state Department of Transportation has specifications for use, size and percentage of quantity for use. Common applications are as pipe bedding—placed around sewer, storm water or drinking water pipes to transfer weight from the surface and protect the pipe. Another common use would be as fill to bring the level of a concrete floor even with a foundation. Use of glass aggregate helps close the loop in glass recycling in many places where glass cannot be smelted into new glass.\n\nAggregates themselves can be recycled as aggregates. Unlike deposits of sand and gravel or stone suitable for crushing into aggregate, which can be anywhere and may require overburden removal and/or blasting, \"deposits\" of recyclable aggregate tend to be concentrated near urban areas, and production from them cannot be raised or lowered to meet demand for aggregates. Supply of recycled aggregate depends on physical decay of structures and their demolition. The recycling plant can be fixed or mobile; the smaller capacity mobile plant works best for asphalt-aggregate recycling. The material being recycled is usually highly variable in quality and properties.\n\nMany aggregate products of various types are recycled for other industrial purposes with economic and environmental benefits to the local and surrounding area. Contractors save on disposal costs and less aggregate is buried or piled and abandoned. In Bay City, Michigan, for example, a recycle program exists for unused products such as mixed concrete, block, brick, gravel, pea stone, and other used materials. The material is crushed to provide subbase for roads and driveways, among other purposes.\n\nAccording to the USGS in 2006, 2.9 million tonnes of Portland cement concrete (including aggregate) worth $21.9 million was recycled, and 1.6 million tonnes of asphalt concrete (including aggregate) worth $11.8 million was recycled, both by crushed stone operations. Much more of both materials are recycled by construction and demolition firms not in the USGS survey. For sand and gravel, the USGS survey for 2006 showed that 4.7 million tonnes of cement concrete valued at $32.0 million was recycled, and 6.17 million tonnes of asphalt concrete valued at $45.1 million was recycled. Again, more of both materials are recycled by construction and demolition firms not in this USGS survey. The Construction Materials Recycling Association indicates that there are 325 million tonnes of recoverable construction and demolition materials produced annually.\n\nMany geosynthetic aggregates are also made from recycled materials. Being polymer based, recyclable plastics can be reused in the production of these new age of aggregates. For example, Ring Industrial Group's EZflow product lines are produced with geosynthetic aggregate pieces that are more than 99.9% recycled polystyrene. This polystyrene, that would have otherwise been destined for a landfill, is instead gathered, melted, mixed, reformulated and expanded to create low density aggregates that maintain high strength properties while under compressive loads. Such geosynthetic aggregates replace conventional gravel while simultaneously increasing porosity, increasing hydraulic conductivity and eliminating the fine dust \"fines\" inherent to gravel aggregates which otherwise serve to clog and disrupt the operation of many drainage applications.\n\nRecycled aggregate in the UK is defined as aggregate resulting from the processing of inorganic material previously used in construction. To ensure the aggregate is inert, it is manufactured from material tested and characterised under European Waste Codes.\n\nIn 2008, 210 million tonnes of aggregate were produced in the UK of which 67 million tonnes was recycled product, according to the Quarry Products Association (QPA). The Waste and Resource Action Programme (WRAP) has produced a Quality Protocol for the regulated production of recycled aggregates. The recycled aggregate is delivered with documentation that states it has been produced using a quality assured system for the manufacturing process to ensure an aggregate that conforms to the relevant European standards.\n\nBy continuous effort of Mr.Shiv Prakash and k.j.anand Bituminous Concrete is designed with air cooled aggregate and same is under utilisation.\n\n\n"}
{"id": "3010408", "url": "https://en.wikipedia.org/wiki?curid=3010408", "title": "Dryrock", "text": "Dryrock\n\nDryrock is the term used for processed phosphate rock, used in the production of agricultural fertilizer. Dryrock requires protection from the weather, and so is always shipped by rail using covered hopper and gondola cars.\n"}
{"id": "9770", "url": "https://en.wikipedia.org/wiki?curid=9770", "title": "Eclipse", "text": "Eclipse\n\nAn eclipse is an astronomical event that occurs when an astronomical object is temporarily obscured, either by passing into the shadow of another body or by having another body pass between it and the viewer. This alignment of three celestial objects is known as a syzygy. Apart from syzygy, the term eclipse is also used when a spacecraft reaches a position where it can observe two celestial bodies so aligned. An eclipse is the result of either an occultation (completely hidden) or a transit (partially hidden).\n\nThe term eclipse is most often used to describe either a solar eclipse, when the Moon's shadow crosses the Earth's surface, or a lunar eclipse, when the Moon moves into the Earth's shadow. However, it can also refer to such events beyond the Earth–Moon system: for example, a planet moving into the shadow cast by one of its moons, a moon passing into the shadow cast by its host planet, or a moon passing into the shadow of another moon. A binary star system can also produce eclipses if the plane of the orbit of its constituent stars intersects the observer's position.\n\nFor the special cases of solar and lunar eclipses, these only happen during an \"eclipse season\", the two times of each year when the plane of the Earth's orbit around the Sun crosses with the plane of the Moon's orbit around the Earth. The type of solar eclipse that happens during each season (whether total, annular, hybrid, or partial) depends on apparent sizes of the Sun and Moon. If the orbit of the Earth around the Sun, and the Moon's orbit around the Earth were both in the same plane with each other, then eclipses would happen each and every month. There would be a lunar eclipse at every full moon, and a solar eclipse at every new moon. And if both orbits were perfectly circular, then each solar eclipse would be the same type every month. It is because of the non-planar and non-circular differences that eclipses are not a common event. Lunar eclipses can be viewed from the entire nightside half of the Earth. But solar eclipses, particularly a total eclipse, as occurring at any one particular point on the Earth's surface, is a rare event that can span many decades from one to the next.\n\nThe term is derived from the ancient Greek noun ('), which means \"the abandonment\", \"the downfall\", or \"the darkening of a heavenly body\", which is derived from the verb (') which means \"to abandon\", \"to darken\", or \"to cease to exist,\" a combination of prefix ('), from preposition ('), \"out,\" and of verb (\"\"), \"to be absent\".\n\nFor any two objects in space, a line can be extended from the first through the second. The latter object will block some amount of light being emitted by the former, creating a region of shadow around the axis of the line. Typically these objects are moving with respect to each other and their surroundings, so the resulting shadow will sweep through a region of space, only passing through any particular location in the region for a fixed interval of time. As viewed from such a location, this shadowing event is known as an eclipse.\n\nTypically the cross-section of the objects involved in an astronomical eclipse are roughly disk shaped. The region of an object's shadow during an eclipse is divided into three parts:\nA total eclipse occurs when the observer is within the umbra, an annular eclipse when the observer is within the antumbra, and a partial eclipse when the observer is within the penumbra. During a lunar eclipse only the umbra and penumbra are applicable. This is because Earth's apparent diameter from the viewpoint of the Moon is nearly four times that of the Sun. The same terms may be used analogously in describing other eclipses, e.g., the antumbra of Deimos crossing Mars, or Phobos entering Mars's penumbra.\n\nThe \"first contact\" occurs when the eclipsing object's disc first starts to impinge on the light source; \"second contact\" is when the disc moves completely within the light source; \"third contact\" when it starts to move out of the light; and \"fourth\" or \"last contact\" when it finally leaves the light source's disc entirely.\n\nFor spherical bodies, when the occulting object is smaller than the star, the length (\"L\") of the umbra's cone-shaped shadow is given by:\n\nwhere \"R\" is the radius of the star, \"R\" is the occulting object's radius, and \"r\" is the distance from the star to the occulting object. For Earth, on average \"L\" is equal to 1.384 km, which is much larger than the Moon's semimajor axis of 3.844 km. Hence the umbral cone of the Earth can completely envelop the Moon during a lunar eclipse. If the occulting object has an atmosphere, however, some of the luminosity of the star can be refracted into the volume of the umbra. This occurs, for example, during an eclipse of the Moon by the Earth—producing a faint, ruddy illumination of the Moon even at totality.\n\nOn Earth, the shadow cast during an eclipse moves very approximately at 1 km per sec. This depends on the location of the shadow on the Earth and the angle in which it is moving.\n\nAn eclipse cycle takes place when eclipses in a series are separated by a certain interval of time. This happens when the orbital motions of the bodies form repeating harmonic patterns. A particular instance is the saros, which results in a repetition of a solar or lunar eclipse every 6,585.3 days, or a little over 18 years. Because this is not a whole number of days, successive eclipses will be visible from different parts of the world.\n\nAn eclipse involving the Sun, Earth, and Moon can occur only when they are nearly in a straight line, allowing one to be hidden behind another, viewed from the third. Because the orbital plane of the Moon is tilted with respect to the orbital plane of the Earth (the ecliptic), eclipses can occur only when the Moon is close to the intersection of these two planes (the nodes). The Sun, Earth and nodes are aligned twice a year (during an eclipse season), and eclipses can occur during a period of about two months around these times. There can be from four to seven eclipses in a calendar year, which repeat according to various eclipse cycles, such as a saros.\n\nBetween 1901 and 2100 there are the maximum of seven eclipses in:\n\nExcluding penumbral lunar eclipses, there are a maximum of seven eclipses in:\n\nAs observed from the Earth, a solar eclipse occurs when the Moon passes in front of the Sun. The type of solar eclipse event depends on the distance of the Moon from the Earth during the event. A total solar eclipse occurs when the Earth intersects the umbra portion of the Moon's shadow. When the umbra does not reach the surface of the Earth, the Sun is only partially occulted, resulting in an annular eclipse. Partial solar eclipses occur when the viewer is inside the penumbra.\n\nThe eclipse magnitude is the fraction of the Sun's diameter that is covered by the Moon. For a total eclipse, this value is always greater than or equal to one. In both annular and total eclipses, the eclipse magnitude is the ratio of the angular sizes of the Moon to the Sun.\n\nSolar eclipses are relatively brief events that can only be viewed in totality along a relatively narrow track. Under the most favorable circumstances, a total solar eclipse can last for 7 minutes, 31 seconds, and can be viewed along a track that is up to 250 km wide. However, the region where a partial eclipse can be observed is much larger. The Moon's umbra will advance eastward at a rate of 1,700 km/h, until it no longer intersects the Earth's surface.\n\nDuring a solar eclipse, the Moon can sometimes perfectly cover the Sun because its apparent size is nearly the same as the Sun's when viewed from the Earth. A total solar eclipse is in fact an occultation while an annular solar eclipse is a transit.\n\nWhen observed at points in space other than from the Earth's surface, the Sun can be eclipsed by bodies other than the Moon. Two examples include when the crew of Apollo 12 observed the in 1969 and when the Cassini probe observed in 2006.\n\nLunar eclipses occur when the Moon passes through the Earth's shadow. This happens only during a full moon, when the Moon is on the far side of the Earth from the Sun. Unlike a solar eclipse, an eclipse of the Moon can be observed from nearly an entire hemisphere. For this reason it is much more common to observe a lunar eclipse from a given location. A lunar eclipse lasts longer, taking several hours to complete, with totality itself usually averaging anywhere from about 30 minutes to over an hour.\n\nThere are three types of lunar eclipses: penumbral, when the Moon crosses only the Earth's penumbra; partial, when the Moon crosses partially into the Earth's umbra; and total, when the Moon crosses entirely into the Earth's umbra. Total lunar eclipses pass through all three phases. Even during a total lunar eclipse, however, the Moon is not completely dark. Sunlight refracted through the Earth's atmosphere enters the umbra and provides a faint illumination. Much as in a sunset, the atmosphere tends to more strongly scatter light with shorter wavelengths, so the illumination of the Moon by refracted light has a red hue, thus the phrase 'Blood Moon' is often found in descriptions of such lunar events as far back as eclipses are recorded.\n\nRecords of solar eclipses have been kept since ancient times. Eclipse dates can be used for chronological dating of historical records. A Syrian clay tablet, in the Ugaritic language, records a solar eclipse which occurred on March 5, 1223 B.C., while Paul Griffin argues that a stone in Ireland records an eclipse on November 30, 3340 B.C. Positing classical-era astronomers' use of Babylonian eclipse records mostly from the 13th century BC provides a feasible and mathematically consistent explanation for the Greek finding all three lunar mean motions (synodic, anomalistic, draconitic) to a precision of about one part in a million or better. Chinese historical records of solar eclipses date back over 4,000 years and have been used to measure changes in the Earth's rate of spin.\n\nBy the 1600s, European astronomers were publishing books with diagrams explaining how lunar and solar eclipses occurred. In order to disseminate this information to a broader audience and decrease fear of the consequences of eclipses, booksellers printed broadsides explaining the event either using the science or via astrology.\n\nThe gas giant planets (Jupiter, Saturn, Uranus, and Neptune) have many moons and thus frequently display eclipses. The most striking involve Jupiter, which has four large moons and a low axial tilt, making eclipses more frequent as these bodies pass through the shadow of the larger planet. Transits occur with equal frequency. It is common to see the larger moons casting circular shadows upon Jupiter's cloudtops.\n\nThe eclipses of the Galilean moons by Jupiter became accurately predictable once their orbital elements were known. During the 1670s, it was discovered that these events were occurring about 17 minutes later than expected when Jupiter was on the far side of the Sun. Ole Rømer deduced that the delay was caused by the time needed for light to travel from Jupiter to the Earth. This was used to produce the first estimate of the speed of light.\n\nOn the other three gas giants, eclipses only occur at certain periods during the planet's orbit, due to their higher inclination between the orbits of the moon and the orbital plane of the planet. The moon Titan, for example, has an orbital plane tilted about 1.6° to Saturn's equatorial plane. But Saturn has an axial tilt of nearly 27°. The orbital plane of Titan only crosses the line of sight to the Sun at two points along Saturn's orbit. As the orbital period of Saturn is 29.7 years, an eclipse is only possible about every 15 years.\n\nThe timing of the Jovian satellite eclipses was also used to calculate an observer's longitude upon the Earth. By knowing the expected time when an eclipse would be observed at a standard longitude (such as Greenwich), the time difference could be computed by accurately observing the local time of the eclipse. The time difference gives the longitude of the observer because every hour of difference corresponded to 15° around the Earth's equator. This technique was used, for example, by Giovanni D. Cassini in 1679 to re-map France.\n\nOn Mars, only partial solar eclipses (transits) are possible, because neither of its moons is large enough, at their respective orbital radii, to cover the Sun's disc as seen from the surface of the planet. Eclipses of the moons by Mars are not only possible, but commonplace, with hundreds occurring each Earth year. There are also rare occasions when Deimos is eclipsed by Phobos. Martian eclipses have been photographed from both the surface of Mars and from orbit.\n\nPluto, with its proportionately largest moon Charon, is also the site of many eclipses. A series of such mutual eclipses occurred between 1985 and 1990. These daily events led to the first accurate measurements of the physical parameters of both objects.\n\nEclipses are impossible on Mercury and Venus, which have no moons. However, both have been observed to transit across the face of the Sun. There are on average 13 transits of Mercury each century. Transits of Venus occur in pairs separated by an interval of eight years, but each pair of events happen less than once a century. According to NASA, the next pair of transits will occur on December 10, 2117 and December 8, 2125. Transits on Mercury are much more common.\n\nA binary star system consists of two stars that orbit around their common centre of mass. The movements of both stars lie on a common orbital plane in space. When this plane is very closely aligned with the location of an observer, the stars can be seen to pass in front of each other. The result is a type of extrinsic variable star system called an eclipsing binary.\n\nThe maximum luminosity of an eclipsing binary system is equal to the sum of the luminosity contributions from the individual stars. When one star passes in front of the other, the luminosity of the system is seen to decrease. The luminosity returns to normal once the two stars are no longer in alignment.\n\nThe first eclipsing binary star system to be discovered was Algol, a star system in the constellation Perseus. Normally this star system has a visual magnitude of 2.1. However, every 2.867 days the magnitude decreases to 3.4 for more than nine hours. This is caused by the passage of the dimmer member of the pair in front of the brighter star. The concept that an eclipsing body caused these luminosity variations was introduced by John Goodricke in 1783.\n\n\nSun - Moon - Earth: Solar eclipse | annular eclipse | hybrid eclipse | partial eclipse\n\nSun - Earth - Moon: Lunar eclipse | penumbral eclipse | partial lunar eclipse | central lunar eclipse\n\nSun - Phobos - Mars: Transit of Phobos from Mars | Solar eclipses on Mars\n\nSun - Deimos - Mars: Transit of Deimos from Mars | Solar eclipses on Mars\n\nOther types: Solar eclipses on Jupiter | Solar eclipses on Saturn | Solar eclipses on Uranus | Solar eclipses on Neptune | Solar eclipses on Pluto\n\n\n"}
{"id": "32040997", "url": "https://en.wikipedia.org/wiki?curid=32040997", "title": "Fluid Equipment Development Company", "text": "Fluid Equipment Development Company\n\nFluid Equipment Development Company (FEDCO) is a Michigan-based designer and manufacturer of high-pressure feed pumps and brine energy recovery devices (ERDs) for brackish water reverse osmosis (BWRO) and seawater reverse osmosis (SWRO) systems. With over 3,500 units in service, FEDCO pumps and ERDs can be found on 6 continents, specifically in areas with little freshwater and rainfall or dense populations. Reverse osmosis (RO) applications including SWRO plants, boiler feedwater, oil platforms, ocean liners, military systems, hotels and resorts.\n\nRO is most commonly used to make potable water from sea-water by removing salt and other impurities. In other cases it is used to purify water for cleaning of sensitive materials/parts and electrical power generation.\n\nThe RO process requires one or several high-pressure pumps and an energy and dollar saving ERD. The FEDCO patented MSS and HPB (ERD) has propelled the company's remarkable growth.\n\nIn the early 1980s, Eli Oklejas was chief R&D hydraulic designer and head of the R&D lab at a major pump company. He was assigned the task of testing a piston-type RO energy recovery device. After dealing with several failures and being frustrated by the overall system complexity, he proposed development of a turbocharger device to boost the feed pressure using brine hydraulic energy, however, the idea was not pursued.\n\nMr. Oklejas left to co-found Pump Engineering, Inc. where he served as president and he led the effort to develop turbochargers for RO applications until his departure in 1996. FEDCO was founded in 1997 as a partnership between Osmonics, Inc. and Eli Oklejas. FEDCO’s goal was to develop innovative high-pressure pumps and energy recovery devices.\n\nGeneral Electric became FEDCO’s partner from 2003 through 2006.\n\nCurrently, FEDCO is 100% privately owned and has continued its focus on its growth in Michigan, innovation, quality and cost-effective solutions for pumping and energy recovery. This effort culminated with the introduction of the Hydraulic Pressure Booster (HPB) and the MSS series of high pressure feed pumps. Mr. Oklejas continues to serve as president of FEDCO as well as leading the development of several new pump and energy recovery product lines reaching new industry benchmarks.\n\nIn June 2012, FEDCO and Torishima Pump Manufacturing of Osaka, Japan announced the formation of Advanced Pumps International, a 50/50 joint venture company that will focus on developing and manufacturing pumping equipment for the oil and gas industries. The joint venture will be based in Monroe, Michigan, and plans to begin operations in the fall of 2012. Its products will be manufactured in accordance with API standards for a wide range of oil industry applications.\n\nThe success of FEDCO continues and its developments have been highlighted on the CBS’ Eye on America. The segment, on business and technology, focuses on the increasing need for potable water and describes how energy recovery is playing a significant role in the selection of process equipment.\n"}
{"id": "5217802", "url": "https://en.wikipedia.org/wiki?curid=5217802", "title": "Germanium telluride", "text": "Germanium telluride\n\nGermanium telluride (GeTe) is a chemical compound of germanium and tellurium and is a component of chalcogenide glasses. It shows semimetallic conduction and ferroelectric behaviour.\n\nGermanium telluride exists in three major crystalline forms, room-temperature α (rhombohedral) and γ (orthorhombic) structures and high-temperature β (cubic, rocksalt-type) phase; α phase being most phase for pure GeTe below the ferroelectric Curie temperature of approximately 670 K.\n\nDoped germanium telluride is a low temperature superconductor.\n"}
{"id": "30221673", "url": "https://en.wikipedia.org/wiki?curid=30221673", "title": "Global Centre for Nuclear Energy Partnership", "text": "Global Centre for Nuclear Energy Partnership\n\nGlobal Centre for Nuclear Energy Partnership is World's first nuclear energy partnership centre at Kheri Jasaur village of Bahadurgarh teshil in Jhajjar district of Haryana state in India. This center facilitates deliberation and discussions of international experts on various issues including innovation in nuclear reactors and the nuclear fuel cycle, development of proliferation-resistant reactors, security technologies and the effects of radiation exposure.\n\nOne among 6 research institutes of Department of Atomic Energy of Govt of India, this campus is located on 400 acre land in NCR Delhi.\n\n5 schools are:\n\nThe institute offers training courses and workshops in various areas through its schools.\n\n\nofficial website\n"}
{"id": "2982762", "url": "https://en.wikipedia.org/wiki?curid=2982762", "title": "Goatskin (material)", "text": "Goatskin (material)\n\nGoatskin refers to the skin of a goat, which by long term usage, is denoted by the term \"Morocco leather\". Kidskin, used for gloves, shoes and other accessories, is traditionally goatskin, although other leathers such as sheep and kangaroo can be used to make kid.\n\nTanned leather from goatskin is considered extremely durable and is commonly used to make rugs (for example in Indonesia) and carpet binding. It is often used for gloves, boots, and other products that require a soft hide. Kid gloves, popular in Victorian times, are still made today. It has been a major material for leather bookbindings for centuries, and the oldest European binding, that of the St Cuthbert Gospel in the British Library is in red goatskin. Goatskin is used for a traditional Spanish container for wine bota bag (or called goatskin). Traditional kefir was made in bags from goatskin.\n\nNon tanned goatskin is used for parchment or for drumheads or sounding boards of some musical instruments, e.g., mišnice in medieval Europe, bodhrán in Ireland, esraj in India and for instrumental drum skin named \"bedug\" in Indonesia.\n\nIn Roman mythology priests of god Lupercalia wore goatskins.\n\nA breed of goat that provides high-quality skin is for example the Black Bengal breed, native to Bangladesh.\n\nIn 1974, there was controversy in the United States surrounding goatskin products originating in Haiti. The U.S. Centers for Disease Control discovered that some of these products contained deadly anthrax spores. All Haitian goatskin products in the USA were recalled, although no fatalities were reported.\n\n"}
{"id": "47157000", "url": "https://en.wikipedia.org/wiki?curid=47157000", "title": "Hamilton Love", "text": "Hamilton Love\n\nHenry Hamilton Love (December 27, 1875 – May 2, 1922) was a Nashville lumberman and sportswriter. Known as the \"Daddy of the Nashville lumberman,\" he was the first president of the Nashville Lumberman's Club. He wrote the \"Hardwood Code\", a telegraphic code then used extensively in the trade, and urged by the Hardwood Manufacturer's Association of the United States.\n\nHe was also chair of the Nashville board of censorship of moving pictures, and active in the Rotary Club.\n\nHamilton Love was born on December 27, 1875 on his father's farm about three miles from Nashville, Tennessee, the youngest child of James Benton Love and Mary Elizabeth Plummer, named for his grandfather. Love's father James was a coal merchant, a member of the firm of Love & Randle. His mother's father James Ransom Plummer was the mayor of Columbia, Tennessee in 1832, 1833, 1834, 1836, and 1838. Love was thus a descendant of James Ransom, and a relative of Nathaniel Macon and Kemp Plummer Battle. His father's grandmother was a Gannaway, making him also a relative of William Gannaway Brownlow and William Trigg Gannaway.\n\nLove left school at the age of fifteen and worked as a reporter and newswriter for the \"Nashville Evening Herald\". He then got a job writing for the Sunday Times, and later wrote for the \"Nashville American\".\n\nLove contributed articles on sports in the South to \"The Sporting News\" and \"Sporting Life\". Love was chairman of the local baseball committee, and wrote several articles covering the Nashville Vols. On the 1908 team winning the Southern pennant after defeating New Orleans, Love wrote \"By one run, by one point, Nashville has won the Southern League pennant, nosing New Orleans out literally by an eyelash. Saturday's game, which was the deciding one, between Nashville and New Orleans was the greatest exhibition of the national game ever seen in the south and the finish in the league race probably sets a record in baseball history\".\n\nLove was called by some the \"Daddy of the Nashville lumbermen\", was vicegerent of the Concatenated Order of Hoo-Hoo, and was president of the Nashville Lumbermen's Club. He worked for his brother John Wheatley Love's firm Love, Boyd, & Co, which avoided losing and in fact made money during the Panic of 1893. Starting in 1895 or 1896, Hamilton Love initially worked in a minor capacity, but was given every opportunity for advancement and learned the trade. By 1899, he assumed charge of the Nashville office of the firm. There was also a Scottsville office, where John Boyd was from.\n\nLove was first president of the Nashville Lumberman's Club, in 1910. That same year he penned the \"Hardwood Code\", a telegraphic code used extensively in the trade, urged on by the Hardwood Manufacturer's Association of the United States, which became known as the Love code. That same year, he also wrote an article on the timber business for the \"Nashville American\" Anniversary Edition.\n\nLove was secretary of the Nashville Commercial Club. He was a charter member of the Rotary Club in Nashville in 1914. He was president of the club in 1915. Also in 1914, Love was appointed by mayor Hilary Ewing Howse as chair of his film censor board, and was named to a national board for such in 1917. Also in 1915, Love was director of the First and Fourth National Banks.\n\nOn November 30, 1901 Love married Bessie May Davis. Her father Leonard Fite Davis was a relative of Leonard B. Fite, and thus of the Fite sisters married by Vanderbilt football coach Dan McGugin and Michigan football coach Fielding Yost.\n\nHe had two sons, Henry Hamilton Love, Jr. and Robert Hamilton Love. Both were seamen. \"Ham\" Jr. attended the Naval Academy and married Louise McAlister, the daughter of governor Hill McAlister. He was a business man in Fort Lauderdale, Florida.\n\nHe died on May 2, 1922 of a revolver wound to the chest, ruled a suicide. He is buried in Mount Olivet Cemetery. The local Chamber of Commerce, in which he was also active, adopted a resolution in his memory. His \"memory is dear to the citizens of Nashville. His matchless bravery in the face of the passing years that smote his frail body with pain and suffering almost incessantly will always appeal to us as an example of fine, undaunted courage. He went to his Maker with head erect, unconquered by the long-continued and well-nigh intolerable blow of physical agony.\" He had apparently been suffering from rheumatism. His foot was also severely injured by falling boards in 1919. He still reviewed films from his bed. His poems were read at his funeral.\n"}
{"id": "25898053", "url": "https://en.wikipedia.org/wiki?curid=25898053", "title": "Hammer-headed tenon", "text": "Hammer-headed tenon\n\nHammer-headed tenon joints are one method that can be used to join curved members of joinery components.\n\nThe hammer-headed tenon is used to join a curved member to a straight member such as a curved head member to a jamb. The tenon is formed on the jamb and the mortise to receive the tenon is formed on the curved member. The mortise is increased in size to receive a pair of folding wedges each side of the tenon.\n\nThe hammer-headed key is used where there is no straight member to form the tenon. It is difficult to form a strong tenon on curved cut timber as the short grain there will weaken it, so two mortise sockets are formed one in each piece and a separate tenon piece called a key is formed to fit. As with the hammer-headed tenon the mortise sockets are increased in size to allow for the folding wedges each side of the tenon.\n\nWhen the joint is fitted and glued together it is the folding wedges that give the cramping effect that tightens the shoulder of the joint.\n\nBoth these hammer-headed joints need good attention to detail from the joiner as they are difficult to make completely by machine or power tool.\n\nOther joints that can be used on curved headed frames are:\nDraw dowelled bridle or mortise\nHandrail bolts and dowels\n\n"}
{"id": "1705788", "url": "https://en.wikipedia.org/wiki?curid=1705788", "title": "Hearn Generating Station", "text": "Hearn Generating Station\n\nThe Richard L. Hearn Generating Station (named after Richard Lankaster Hearn) is a decommissioned electrical generating station in Toronto, Ontario, Canada. The plant was originally fired by coal, but later converted to burn oil. It was never converted to run natural gas, rather it was replaced by a newer much smaller natural gas plant. It is still owned by Ontario Power Generation, a publicly owned electrical generation company. The plant has been described as \"Pharaonic in scale\", and encompasses cubic metres of space—large enough to fit 12 Parthenons inside.\n\nThe plant is located at 440 Unwin Avenue in Toronto's Port Lands area, directly south of the foot of Carlaw Avenue, across the shipping channel and next to the recently opened Portlands Energy Centre. The Richard L. Hearn Generating Station, together with the nearby Ashbridges Bay Wastewater Treatment Plant sewage sludge incinerator stack and the Commissioners Street waste incinerator stack, stand as towering landmarks of a bygone industrial era in the Portlands area of Toronto (all three facilities are no longer in operation, but their towering smokestacks still stand). The property has been leased to Studios of America since 2002.\n\nThe R. L. Hearn Generating Station was the site of Canada's first steam turbo-generator set. The station sits in what was once Ashbridge's Bay, a shallow marsh that was filled in with rubble from downtown construction sites from 1911 to 1950s. The station was officially opened on October 26, 1951 by Leslie Frost, Premier of Ontario, with the first two units in service. Four units were in operation by 1953. The plant originally burned coal which was transported on ships through the Saint Lawrence Seaway. The station was designed by Stone & Webster. The turbine generators were built by Parsons in England and the boilers were made in Canada by Babcock & Wilcox (Cambridge, Ontario) and Combustion Engineering (Montreal, Quebec).\n\nConstruction on the station was not even finished in the 1950s when Hydro officials and the government began to talk about phasing out the plant with nuclear power and closing it. The early years were marked by difficult labour relations and several near strikes. Several unions were involved in conflicts with management and each other during the life of the station. The R. L. Hearn station was one of the founding locals of the Canadian Union of Operating Engineers and General Workers (CUOE Local 100) in 1960.\n\nThe Richard L. Hearn plant reached full capacity of for the first time on March 22, 1961. At full load the boilers burned about 400 tonnes of coal per hour, and the turbines and other equipment required about gallons of cooling water from Lake Ontario per hour. Total construction cost was . The turbine hall was almost 300 metres long and was an impressive sight, viewed from the visitor gallery on the west side of the plant where the offices were located. Units 1–4 had one turbine-generator each. The units (5–8) had two turbine-generators per unit—an arrangement called tandem cross-compound—so there were a total of 12 turbine-generator sets in the turbine hall. At the peak of the R. L. Hearn's operation in the 1960s the station employed up to 600 people. Many Ontario Hydro (later Ontario Power Generation) operators, maintainers, technicians and professionals began their careers, and were trained at the station and then went on to work at other plants and Ontario's CANDU nuclear stations.\n\nThe station at first had four smaller chimneys, one for each of the four boilers. The construction of the four units added four more chimneys. The last three were a bit taller than the first five. The eight short chimneys were a source of air pollution in local neighborhoods and downtown Toronto and also fly ash and other particulates. The station contributed to Toronto's smog problem.\n\nThe eight chimneys were demolished and electrostatic precipitators were added for the units when the large smokestack was built. The new single tall smokestack was built in response to pressure to reduce smog in Toronto by the emerging environmental movement in the late 1960s. It stands 215 metres tall and was one of the tallest in the world, costing when it was completed in 1971. Air pollution in Toronto from the station was greatly reduced and the area around the plant became known as a good fishing and recreation spot.\n\nBy the end of 1971, the entire plant was converted to burn natural gas with four units retaining the option to burn coal. In December 1972, Alberta Premier Peter Lougheed called the billion cubic metres of Alberta natural gas the station was burning annually \"an appalling waste of natural gas\" at the price of about $0.035 per cubic metre, and he charged that Ontario was getting a \"cheap ride\" at Alberta's expense.\n\nThe station operated burning natural gas until the early 1980s and units 1–5 were mothballed between 1978–1979. Conversion to natural gas reduced pollution but increased operating costs and the plant's efficiency was much lower than today's combined cycle and cogeneration plants. The last three units at the plant resumed burning coal along with natural gas but they were phased out of operation in July 1983, due to concerns about increased air pollution in Toronto and an abundant energy supply in the province. The staff level had been reduced to around 180 when power production stopped in 1983. Many of the workers took early retirement and others were transferred to other sites. Some of the generators were operated as synchronous condensers to improve power quality in Toronto and the electrical control room and switchyard continued to operate until 1995, with a staff of about 10.\n\nIn October 1985, Premier David Peterson's Liberal government proposed the re-opening the station using natural gas. It was only the first of numerous proposals to restart the plant, involving cogeneration, tri-generation, garbage incineration\nand eventually gas turbine combined cycle plants as new technologies were developed.\n\nIn June 1987, Ontario Progressive Conservative Party (PC) energy critic Philip Andrewes pushed the governing Liberal government to have the Hearn re-opened as a \"non-polluting\" natural gas power plant. In October 1988, PC member of provincial parliament Donald Cousens called for the addition of scrubbers to the Hearn and proposed to return the station to service.\n\nOn March 16, 1990, Ontario Hydro announced the restart of two units (7 & 8) to meet demand for the winter of 1991. The restart had a projected cost of . Work on the restart was well underway when the new New Democratic Party government of Premier Bob Rae cancelled the project.\n\nThe site was designated as protected for future electricity development by the Mike Harris- and Ernie Eves-led Ontario Progressive Conservative Party. This was also done with all other existing publicly owned electrical generating stations during the deregulation of the Ontario electrical power system. The plant had all of the asbestos insulation removed and site remediation work was done in the 1990s. Former premier Mike Harris later mentioned his plans to build the Portlands Energy Centre on the site of the Hearn, but the actual plant, opened in 2008 sits next to Hearn.\n\nIn 2002, Ontario Power Generation announced that Studios of America and Comweb Group (headed by Paul Bronfman) would be leasing the property of the former generating station and had plans to construct a square metre multipurpose film production studio called Great Lakes Studios on the site. Most of the boilers and a large amount of other equipment were removed and sent to the scrapyard. This movie studio project was abandoned in 2006. Although the station did not become a movie studio, the R. L. Hearn interior and grounds were used in a number of movie productions over the years. , Studios of America still has long term lease obligations for the Hearn site (32.5 years, according to the City of Toronto’s Waterfront Secretariat, or 20 years, according to Studios of America). Subsequent reports indicated that the lease-holders are Paul Vaughan of Studios America and real estate developer Mario Cortellucci, with no reference to Comweb Group.\n\nThe Ontario government announced in April 2005 that the Portlands Energy Centre would not be part of the approved of new power production in Ontario coming online in the next few years. The Independent Electricity System Operator warned of rolling black-outs in Toronto if are not added by 2008, with an additional required by 2010. However, in February 2006 this decision was reversed, a new plan emerged proposing a new plant be built next to the Hearn site. Toronto mayor David Miller lobbied to have Hearn restored in some capacity to provide that power rather than build a second plant, while nearby residents opposed any kind of power generation plant in the area.\n\nA proposal was made by Enwave (the former Toronto District Heating Corporation) and Constellation Energy to install advanced gas turbines and cogeneration inside the station and restore the station's control rooms, turbine hall and building exterior as a historical, filmmaking and education centre. The Minister of Energy, Donna Cansfield rejected the proposal.\n\nOn September 18, 2006 an agreement was signed between the Provincial government, Ontario Power Generation and TransCanada Corp. to construct a gas-fired plant next to Hearn. The Portlands plant may eventually be co-generation, however it is being built as a combined cycle plant due to inability to negotiate contracts for cogeneration energy sales. The Portlands Energy Centre opened in June 2008 with simple cycle production and combined cycle operation scheduled for mid-2009. Extensive demolition of previously preserved areas of the station including the turbine hall began once Studios of America abandoned their plans for a film studio.\n\nIn 2006, proposal to build a transmission corridor from the Portlands plant to connect with higher voltage transmission lines north of Toronto was being discussed and opposed by resident and other groups. The transmission system that the R.L Hearn station supplied was a network of buried and overhead lines and transformer stations in Toronto. The city has been supplied by stepdown transformer stations from the east and west since the R. L. Hearn closed, which are becoming overloaded, especially in the summer.\n\nOn June 21, 2010, architecture firm Behnisch Architekten presented a proposal for converting the Hearn site into a three-pad arena. This plan did not proceed. The facility remains in the hands of the lease-holders with options that include extensions lasting until 2041. The property is not suitable for residential development because it is too close to the Portlands natural gas generating plant.\n\nThe nearly abandoned plant attracted photographers and \"urban explorers\" who published their work on websites and in photography exhibits in recent years. On June 15, 2008, Ryan Nyenhuis, an urban explorer trespassing in the plant fell three storeys into a coal chute, and was trapped for three hours when he became pinned under a steel plate. He suffered serious injuries and died two days later in hospital.\n\nDuring the 1990s the Hearn Generating Station doubled a filming location in many Canadian television productions, including Goosebumps (TV series) (it served as the iconic Dark Falls Chemical Factory on the Season 2 episode \"Welcome to Dead House\"), Once a Thief (TV series) and Animorphs (TV series). It was usually used to substitute as an industrial factory because of its basic industrial appearance and its iconic tall smokestack. This trend began to die down in the early 2000s but the generating station still appears on TV from time to time, most recently in 12 Monkeys from 2015-2018 as Raritan Valley National Laboratory, the home of a time machine in a post apocalyptic world.\n\nIn 2010, the station was used as the backdrop for the climactic scene of the movie RED, other scenes of which were shot in the Toronto area as well.\n\nOn June 5, 2014, the building was used for the Toronto Luminato Festival Big Bang Bash, their 2nd annual fundraising gala. It also featured the Yves Saint Laurent Opening Night Party later that evening.\n\nIn 2015 the building was used for UNSOUND hosted by Luminato Festival.\n\nFrom June 9–26, 2016 Luminato Festival used the building for its festival hub.\n"}
{"id": "39612470", "url": "https://en.wikipedia.org/wiki?curid=39612470", "title": "Heliophysics Science Division", "text": "Heliophysics Science Division\n\nThe Heliophysics Science Division of the Goddard Space Flight Center (NASA) conducts research on the Sun, its extended solar system environment (the heliosphere), and interactions of Earth, other planets, small bodies, and interstellar gas with the heliosphere. Division research also encompasses geospace—Earth's uppermost atmosphere, the ionosphere, and the magnetosphere—and the changing environmental conditions throughout the coupled heliosphere (solar system weather).\n\nScientists in the \"Heliophysics Science Division\" develop models, spacecraft missions and instruments, and systems to manage and disseminate heliophysical data. They interpret and evaluate data gathered from instruments, draw comparisons with computer simulations and theoretical models, and publish the results. The Division also conducts education and public outreach programs to communicate the excitement and social value of NASA heliophysics.\n\nGoddard's Heliophysics Science Division consists of four separate laboratories.\n\nThe \"Solar Physics Laboratory\" works to understand the Sun as a star and as the primary driver of activity throughout the solar system. Their research expands knowledge of the Earth-Sun system and helps to enable robotic and human exploration.\n\nThe \"Heliospheric Physics Laboratory\" develops instruments and models to investigate the origin and evolution of the solar wind, low-energy cosmic rays, and the interaction of the Sun's heliosphere with the local interstellar medium. The Laboratory designs and implements unique multi-mission and multidisciplinary data services to advance NASA's solar-terrestrial program and our understanding of the Sun-Earth system.\n\nThe \"Geospace Physics Laboratory\" focuses on processes occurring in the magnetospheres of magnetized planets and on the interaction of the solar wind with planetary magnetospheres. Researchers also study processes, such as magnetofluid turbulence, that permeate the heliosphere from the solar atmosphere to the edge of the solar system.\n\nThe \"Space Weather Laboratory\" performs research and analysis of the physical processes underlying space weather. It conducts space-based, ground-based, theoretical, and modeling studies of the chain of events that triggers space-weather effects of interest to NASA, other U.S. government agencies, and the general public. Laboratory staff lead the development of space environment projects and missions, and provide project scientists for NASA flight missions with space weather applications. The Laboratory communicates NASA research results to the scientific community, various space weather interests, and the general public.\nThe Space Weather Laboratory also includes the Community Coordinated Modeling Center, which is a multi-agency partnership to enable, support and perform the research and development for next-generation space science and space weather models.\n\nThis division of Goddard Space Flight Center has interests in various projects and missions. In addition to performing research based on NASA solar observatories in space, the division manages many heliophysics missions on behalf of the Science Mission Directorate at NASA headquarters. These include:\n\nThe Advanced Composition Explorer (ACE) observes and measures the composition of particles from the solar wind as well as galactic cosmic rays. Its prime objective is to improve measurements of the composition of diverse samples of matter associated with the sun, the interstellar medium, and the galaxy surrounding us. ACE is capable of providing near-real-time solar wind and magnetic field information that aids in forecasting space weather. Advance knowledge of solar wind disturbances heading toward Earth – of about half an hour – can help mitigate the effects of geomagnetic storms that can overload power grids and disrupt communications on Earth.\n\nThe ARTEMIS, or Acceleration, Reconnection, Turbulence and Electrodynamics of the Moon’s Interaction with the Sun, mission studies the moon's space environment, surface composition and magnetic field, and core structure. ARTEMIS uses two spacecraft from the THEMIS magnetosphere mission that were moved into place near the moon.\n\nThis division is also involved in the Balloon Array for Radiation-belt Relativistic Electron Losses (BARREL) study. Twenty balloons were launched during a January 2013 campaign in Antarctica to study a space weather phenomenon, during which electrons stream down toward the poles from the two Van Allen Belts, which surround Earth. It is a NASA-funded mission.\n\nThe Coupled Ion-Neutral Dynamics Investigations (CINDI) is a project to understand the dynamics of Earth's ionosphere. CINDI provides two instruments for the Communication/Navigation Outage Forecast System (C/NOFS) satellite, which is a United States Air Force project. CINDI helps predict the behavior of equatorial ionospheric irregularities, which can cause major problems for communications and navigation systems.\n\nCluster is a joint ESA/NASA mission that provides in-situ investigation of plasma processes in Earth's magnetosphere using four identical spacecraft. The four spacecraft make it possible to better observe three-dimensional and time-varying phenomena, as well as to distinguish between the two as it moves through space in its orbit around Earth.\n\nGeotail is a joint JAXA/NASA mission. Its primary objective is to study the dynamics of the entire length of Earth's magnetotail, from the near-Earth region to the distant tail.\n\nEngaging in solar and heliospheric science, the Interface Region Imaging Spectrograph (IRIS) mission is intended to study of the solar atmosphere, and in particular, of the interface between the photosphere and corona. The IRIS mission will accomplish this by tracing the flow of energy and plasma through the chromosphere and transition region into the corona using spectrometry and imaging. IRIS is designed to provide significant new information to increase the understanding of energy transport into the corona and solar wind and provide an archetype for all stellar atmospheres. The unique instrument capabilities, coupled with state of the art 3-D modeling, will fill a large gap in our knowledge of this dynamic region of the solar atmosphere. The mission will extend the scientific output of existing heliophysics spacecraft that follow the effects of energy release processes from the sun to Earth. The IRIS mission launched June 27, 2013.\n\nThe Interstellar Boundary Explorer, or IBEX, images the outer boundaries of the heliosphere, focusing on how the solar wind interacts with the interstellar medium and its magnetic fields at the very edges of our Solar System. IBEX maps the region by measuring the energetic neutral atoms that are created near the boundary, creating a new map every six months. After completing and analyzing the first maps, IBEX now monitors changes that correspond to variations in solar activity.\n\nThe Reuven Ramaty High Energy Solar Spectroscopic Imager, or RHESSI, combines high-resolution imaging in hard X-rays and gamma rays with high-resolution spectroscopy to explore the basic physics of particle acceleration and energy release in solar flares. Such information improves our understanding of the fundamental processes that are involved in generating solar flares and coronal mass ejections. These super-energetic solar eruptive events are the most extreme drivers of space weather and present significant dangers in space and on Earth.\n\nNASA’s Solar Dynamics Observatory (SDO) mission was launched in 2010 and is currently studying solar activity and how it causes space weather. Space weather affects not only our lives on Earth, but Earth itself, and everything outside its atmosphere (astronauts and satellites out in space and even the other planets). SDO is helping us understand where the sun's energy comes from, how the inside of the sun works, and how energy is stored and released in the sun's atmosphere. By better understanding the sun and how it works, we will be able to better predict and better forecast space weather events.\n\nA joint ESA/NASA mission, the Solar and Heliospheric Observatory, or SOHO, studies the sun, from deep inside its core to the outer corona and solar wind. SOHO has been capturing images of the dynamic flares and coronal mass ejections on the sun since 1996. The mission has provided an unprecedented breadth and depth of information about the sun, with a unique combination of instruments that study its interior through the hot and dynamic atmosphere to the solar wind and its interaction with the interstellar medium. Its coronagraphs – images that observe the sun's atmosphere by blocking out the bright sun in the middle – remain a key component for forecasting the speed, direction and strength of coronal mass ejections as they erupt from the sun. In addition to watching the sun, SOHO has become the most prolific discoverer of comets in astronomical history: as of 2012, over 2000 comets have been found by SOHO.\n\nThe Solar Terrestrial Relations Observatory, or STEREO, mission employs two nearly identical space-based observatories to provide the stereoscopic measurements to study the sun. With a pair of viewpoints, scientists are able to see the structure and evolution of solar storms as they blast from the sun and travel out through space. STEREO's instruments provide a unique combination of observations to help understand the causes and mechanisms of coronal mass ejections and to characterize how they propagate through the Solar System. STEREO also helps determine what powers the acceleration of energetic particles from the sun and provides information on the structure of the solar wind.\n\nTHEMIS answers fundamental questions concerning a type of space weather called a substorm that can abruptly and explosively release solar wind energy stored within Earth’s magnetotail. Substorms cause auroras at high latitudes, and THEMIS seeks to understand this process. Originally five spacecraft, THEMIS now consists of three, as two were repurposed to study the moon in the ARTEMIS mission. The mission also relies on a dedicated array of ground observatories located in Canada and the northern United States.\n\nThe Thermosphere Ionosphere Mesosphere Energetics and Dynamics, or TIMED, mission explores Earth's mesosphere and lower thermosphere (40–50 miles up), the least explored and understood region of the atmosphere. Solar events, as well as temperature changes in the stratosphere can perturb this region, but the overall structure of and responses to these effects are not understood. Advances in remote sensing technology employed by TIMED enable it to explore this region on a global basis from space.\n\nThe instruments on the Two Wide-Angle Imaging Neutral-Atom Spectrometers, or TWINS, provide stereo imaging of Earth's magnetosphere—the region surrounding the planet, controlled by Earth’s magnetic field and containing the Van Allen radiation belts and other energetic charged particles. TWINS enables three-dimensional global visualization of this region, leading to greatly enhanced understanding of the connections between different areas of the magnetosphere and their relation to the solar wind.\n\nThe Van Allen Probes consist of twin spacecraft studying the extreme and dynamic regions of space known as the Van Allen Radiation Belts that surround Earth. The radiation belts intensify or weaken over time as part of the much larger space weather system driven by the energy and material that erupt off the sun's surface and fill the entire Solar System.\n\nThe Voyager missions (Voyager 1 and Voyager 2) are a part of NASA's Heliophysics System Observatory, sponsored by the Heliophysics Division of the Science Mission Directorate at NASA Headquarters in Washington. The Voyager spacecraft were built and continue to be operated by NASA's Jet Propulsion Laboratory, in Pasadena, Calif. On December 4, 2012, eleven billion miles from Earth, NASA's Voyager 1 spacecraft has entered a \"magnetic highway\" that connects our Solar System to interstellar space. The \"magnetic highway\" is a place in the far reaches of the Solar System where the sun's magnetic field connects to the magnetic field of interstellar space. In this region, the sun's magnetic field lines are connected to interstellar magnetic field lines, allowing particles from inside the heliosphere to zip away and particles from interstellar space to zoom in. In recent years, the speed of the solar wind around Voyager 1 has slowed to zero, and the intensity of the magnetic field has increased.\n\nThe \"Space Physics Data Facility\" (SPDF) is a project of the Heliospheric Science Division (HSD) at NASA's Goddard Space Flight Center. SPDF consists of web-based services for survey and high resolution data and trajectories. The Facility supports data from most NASA Heliophysics missions to promote correlative and collaborative research across discipline and mission boundaries.\n\n"}
{"id": "32514093", "url": "https://en.wikipedia.org/wiki?curid=32514093", "title": "Hybrid Scorecard", "text": "Hybrid Scorecard\n\nThe Hybrid Scorecard was created by the Union of Concerned Scientists (UCS) to give consumers a comprehensive comparison of hybrid electric vehicles available in the U.S. market. The UCS Hybrid Scorecard ratings take into considerations fuel economy as rated by the U.S. Environmental Protection Agency (EPA); the environmental benefits as compared to its similar or closest conventional internal combustion engine counterpart; how cost-effectively a particular hybrid achieves its environmental performance; and premium features that are bundled on a hybrid as standard equipment raising its purchase price.\n\nThe Hybrid Scorecard is available to the public through the Hybridcenter.org website, created in 2005 by the Union of Concerned Scientists to provide consumer and technology information about hybrid vehicles. The first scorecard was published in January 2010, for 2009–10 model year hybrids. The 2011 Hybrid Scorecard was published in July 2011, and evaluated 34 hybrids from the 2011–12 model years available in the U.S.\n\nThe scorecard was retired in 2013 and was replaced in 2015 with an online tool that calculates electric vehicle emissions.\n\nAll hybrid vehicles are grouped into two broad categories, non-luxury and luxury models, and the Hybrid Scorecard score for each hybrid electric vehicle available in the U.S. market is assessed considering the following four variables: combined miles per gallon, environmental improvement score, hybrid value, and forced features.\n\nThis category considers the combined fuel economy rating for highway/city as estimated by the U.S. EPA and expressed in miles per gallon.\n\nThis category measures the smog-forming and global warming pollution performance of a hybrid against its similar or closest conventional gasoline counterpart, and thus, it does not reflect the vehicle’s overall greenhouse gas emissions performance relative to the average vehicle.\n\nThe greenhouse gas emission reductions are calculated as a percent reduction based on the combined city/highway fuel economy ratings for both the hybrid and non-hybrid model according to the ratings published in EPA's fuel economy guide available online at www.fueleconomy.gov. The score is measured in a scale from 0 to 10, where 10 is the best rating. The scores are assigned in relative terms, then a 10 is assigned to the hybrid with the largest reduction in greenhouse gas emissions, while a zero is assigned to the hybrid with the smallest reduction. The values assigned to the models are calculated through linear interpolation between the two end points. For purposes of updating the scorecard, if new hybrid models are introduced during the year, the scale has to be re-calculated to take account of the new arrivals. For the 2011 Scorecard, the Lincoln MKZ Hybrid was rated a 10 while the Volkswagen Touareg Hybrid was rated 0. The Toyota Prius was rated a 9.4.\n\nTailpipe emissions are considered through EPA’s air pollution score for each hybrid vehicle and not relatively to the improvement over the similar conventional model. Even though EPA rates air pollution scores for both California and federal standards, the Hybrid Scorecard ratings use only the California emissions certification level in order to introduce the maximum potential to reduce tailpipe emissions . The score is measured in a scale from 0 to 10, where 10 is the best rating and 0 is the worst. Then, using the legal definitions established by the California Air Resources Board (CARB), a zero emissions vehicle (ZEV) is assigned a 10, a partial zero emissions vehicle (PZEV) a 9, a super ultra low emission vehicle (SULEV II) an 8, and so forth.\n\nThe final Environmental Improvement Score is determined through the average (and rounded to the nearest tenth) of the values for global warming emissions reductions and tailpipe emissions. As an example, the 2011 Lincoln MKZ Hybrid achieved a 46% reduction in greenhouse gas emissions compared with its sibling gasoline-only model. Because this is the largest percent reduction achieved among all hybrids the MKZ Hybrid earns a global warming emissions score of 10. As a California certified Partial Zero Emission Vehicle, the MKZ Hybrid earns an EPA air pollution score of 9.0. When both scores are combined, the MKZ Hybrid earns an overall Environmental Improvement Score of 9.5.\n\nThis category measures how cost-effectively a particular hybrid achieves its environmental performance, and the scores vary from \"Very Poor\" to \"Superior.\" Each hybrid model is evaluated by dividing the estimated cost of hybrid technology used in the vehicle by its reduction in global warming emissions over the conventional model expressed as a percentage. The ratings are assigned to each model based on the resulting incremental cost according to the range values shown in the table in the right. When a hybrid achieves a better smog certification level than its conventional counterpart, the hybrid technology cost should include the cost of any additional emissions controls used in the hybrid model.\n\nThis category measures how many premium or upgraded features, called forced features, are included on a hybrid model as standard equipment in comparison to its sibling conventional model. UCS considers that these forced features make it less economical for consumers to buy a base hybrid model. UCS calculates the cost of forced features as the difference in MSRP between the base conventional model, with no upgrades, and the base hybrid model, and then the estimated cost of the hybrid technology is subtracted. This category is scored quantitatively from None to $$$$$. Vehicles with no forced features are assigned a rating of “None” meaning that the additional costs for the vehicle only cover the cost of the hybrid system. Vehicles that have or more of forced features are assigned the maximum of “$$$$$.” The other scores are defined in intermediate ranges of .\n\nThe U.C.S. evaluated 31 hybrids from the 2009–10 model years for the 2010 Hybrid Scorecard. The 2010 Toyota Prius scored top marks in the categories regarding environmental benefit and hybrid value. The UCS found that, compared to its closest conventional counterpart the Toyota Matrix, the Prius emits 44% less global warming pollution. The Prius score on forced features was rated relatively good, as the car came with worth of forced features. For the 2010 Hybrid Scorecard the evaluation found that the worst offender was the luxury Lexus LS 600h L hybrid which came with more than in forced features compared with the conventional, base model Lexus LS 460L. The 2010 Scorecard was updated on April 2010 to include new hybrids that have been launched in the U.S. market such as the Mercedes-Benz S400 and the BMW ActiveHybrid X6.\n\nThe 2011 Hybrid Scorecard evaluated 34 hybrids from the 2011–12 model years sold as of April 2011. The 2011 evaluation found that only 13 of the 34 hybrids available in the U.S. market reduce more than 25% of greenhouse emissions compared with their gasoline engine counterparts, but the 2011 scorecard now includes more top performers than it did in 2010.\nThe Toyota Prius remained the top nonluxury model in the environmental improvement category by emitting 40% less greenhouse emissions than comparable nonhybrid models. In the luxury category, two new additions, the Lincoln MKZ Hybrid and the Lexus CT200h accomplished similar performance, an achievement the UCS attributed to their relatively small gasoline engines, as the carmakers downsized these vehicles’ engines from six to four cylinders to maximize fuel economy. The evaluation found that the MKZ Hybrid reduces greenhouse emissions by 46.2% as compared to its sibling gasoline-only MKZ, and the Lexus CT200h a 42.9% reduction.\n\nBesides the Prius, the 2011 evaluation found that more automakers are using hybrid technology to increase fuel efficiency and decrease tailpipe emissions without imposing significant price premiums for those benefits, and other hybrids scoring high on environmental improvement include the Ford Fusion Hybrid, Honda Civic Hybrid, and Toyota Highlander Hybrid. In the non-luxury category, the worst offender is the Volkswagen Touareg Hybrid, which reduces less than a 10% of greenhouse emissions, an all-time low on the scorecard. According to the UCS, the \"\"Touareg was singled out as an example of a \"muscle\" hybrid that emphasized power over fuel efficiency.\"\" The UCS also singled out among the worst-rated hybrids for environmental improvement the new Porshe Cayenne S Hybrid, the BMW Active Hybrid 750i, and the ActiveHybrid X6.\n\nHybrid Scorecard for non-luxury models available in the U.S.(model year 2011–2012)\nHybrid Scorecard for luxury models available in the U.S.(model year 2011–2012)\nThe UCS explained that the Chevrolet Volt plug-in hybrid (PHEV) was not included in the evaluation even though the Volt could have been compared with the Chevrolet Cruze as the conventional similar. The reason for the exclusion is that PHEVs are affected by a variety of factors that affect their environmental performance, such as the type of fuel used (coal, wind, solar, etc.) for generating electricity to charge the vehicle, and also by the driving habits, including how often the vehicle is plugged in and the number of miles driven on electricity-only mode.\n\n\n"}
{"id": "11066704", "url": "https://en.wikipedia.org/wiki?curid=11066704", "title": "Internal Market in Electricity Directive", "text": "Internal Market in Electricity Directive\n\nInternal Market in Electricity Directive is the Directive 2003/54/EC of the European Parliament and the Council of 26 June 2003 concerning common rules for the internal market in electricity and repealing Directive 96/92/EC is based in the Treaty establishing the European Community, and in particular Article 47(2), Article 55 and Article 95 thereof. Note: The Directive 2003/54/EC has been replaced by the Directive 2009/72/EC.\n\nDirective 96/92/EC of the European Parliament and of the Council of 19 December 1996 concerning common\nrules for the internal market in electricity, has made significant contributions towards the creation of an\ninternal market for electricity. Experience in implementing this Directive shows the benefits that may result from the internal market in electricity, in terms of efficiency gains, price reductions, higher standards of service and increased competitiveness.\n\nHowever, important shortcomings and possibilities for improving the functioning of the market remained with the 96/92/EC directive, notably concrete provisions were needed to ensure a level playing field in generation and to reduce the risks of market dominance and predatory behaviour, ensuring non-discriminatory transmission and distribution tariffs, through access to the network based on third-party access rights and on the basis of tariffs published prior to their entry into force, and ensuring that the rights of small and vulnerable customers are protected and that information on energy sources for electricity generation is disclosed, as well as reference to sources, where available, giving information on their environmental impact.\n\nThe freedoms which the Treaty guarantees European citizens – free movement of goods, freedom to provide services and freedom of establishment – are only possible in a fully open market, which enables all consumers freely to choose their suppliers and all suppliers freely to deliver to their customers.\n\nTo ensure efficient and non-discriminatory network access, it is appropriate that distribution and transmission systems are operated through legally separate entities where vertically integrated undertakings exist. Independent management structures must be in place between the distribution system operators, the transmission system operators, and any generation/supply companies.\n\nIt is important however to distinguish between such legal separation and ownership unbundling. Legal separation does not imply a change of ownership of assets and nothing prevents similar or identical employment conditions applying throughout the whole of the vertically integrated undertakings. However, a non-discriminatory decision-making process should be ensured through organisational measures regarding the independence of the decision-makers responsible. \n\nTo facilitate the conclusion of contracts electricity undertaking established in a Member State the supply of electricity to eligible customers in Member State, Member States and, where appropriate, national regulatory authorities should work towards more homogeneous conditions and the same degree eligibility for the whole of the internal market.\n\nThe existence of effective regulation, carried out by one or more national regulatory authorities, is an important factor in guaranteeing non-discriminatory access to the network.\n\nAll Community industry and commerce, including small and medium-sized enterprises, and all Community citizens that enjoy the economic benefits of the internal market should also be able to enjoy high levels of consumer protection, and in particular households and, where Member States deem it appropriate, small enterprises should also be able to enjoy public service guarantees, in particular with regard to security of supply and reasonable tariffs, for reasons of fairness, competitiveness and indirectly to create employment.\n\nElectricity customers should be able to choose their supplier freely and ensure they have a real and effective right to choose their supplier.\n\nProgressive market opening towards full competition should as soon as possible remove differences between\nMember States. Transparency and certainty in the implementation of the Directive should be ensured.\n\nNearly all Member States have chosen to ensure competition in the electricity generation market through a transparent authorisation procedure. However, Member States should ensure the possibility to contribute to security of supply through the launching of a tendering procedure or an equivalent procedure in the event that sufficient electricity generation capacity is not built on the basis of the authorisation procedure. Member States should have the possibility, in the interests of environmental protection and the promotion of infant new technologies, of tendering for new capacity on the basis of published criteria. New capacity includes inter alia renewables and combined heat and power (CHP).\n\nThe construction and maintenance of the necessary network infrastructure, including interconnection capacity between areas and decentralised electricity generation, should contribute to ensuring a stable electricity supply.\n\nThe Commission has indicated its intention to take initiatives especially as regards the scope of the labelling provision and notably on the manner in which the information on the environmental impact in terms of at least emissions of CO and the radioactive waste resulting from electricity production from different energy sources, could be made available in a\ntransparent, easily accessible and comparable manner throughout the European Union and on the manner in which the measures taken in the Member States to control the accuracy of the information provided by suppliers could be streamlined (see no-carbon renewable energy economy – NCREE).\n\nThe respect of the public service requirements is a fundamental requirement of this Directive, and it is\nimportant that common minimum standards, respected by all Member States, are specified in this Directive.\n\n\n"}
{"id": "2762890", "url": "https://en.wikipedia.org/wiki?curid=2762890", "title": "Jointing (sharpening)", "text": "Jointing (sharpening)\n\nJointing refers to the process of filing or grinding the teeth or knives of cutting tools prior to sharpening. The purpose of jointing is to ensure that all surfaces to be sharpened are of a consistent size and all imperfections have been removed. \n\nJointing is usually the first step in the process of sharpening:\n\n\nJointing is usually carried out infrequently as it removes a lot of material from the edge of the blade.\n"}
{"id": "55289388", "url": "https://en.wikipedia.org/wiki?curid=55289388", "title": "Kilcoolaght East ogham stones", "text": "Kilcoolaght East ogham stones\n\nKilcoolaght East Ogham Stones (CIIC 206–213) are a collection of ogham stones forming a National Monument located in County Kerry, Ireland.\n\nKilcoolaght East Ogham Stones are located southeast of Killorglin, to the west of the Glasheenasheefree River.\n\nThe stones were carved in the 5th and 6th centuries AD and served as burial markers. This was a \"ceallurach\" (burial ground).\n\nAll the stones were found in a souterrain nearby.\n\nThe stones are sandstone pillars.\n\n"}
{"id": "8795472", "url": "https://en.wikipedia.org/wiki?curid=8795472", "title": "Kopenhagener Straße", "text": "Kopenhagener Straße\n\nThe Kopenhagener Straße in Berlin's Prenzlauer Berg district runs parallel to the Ringbahn tracks between busy \"Schönhauser Allee\" in the East all the way to the Mauerpark in the West, where the Berlin Wall separated the Soviet from the French sector. The street was named on 30 April 1899 after the Danish capital Copenhagen.\n\nAt a length of 800 metres (1 half mile) it features a nearly complete row of 61 apartment buildings built in the Jugendstil period with 4 modern houses and playgrounds in between. From East to West it is intersected by \"Rhinower Straße\", then crossed by \"Sonnenburger Straße\", by \"Ystader Straße\" and finally ends at \"Schwedter Straße\" at the northern end of the Mauerpark. At \"Sonnenburger Straße\" the \"Schönfließer Brücke\", built in 1908 according to plans by Alfred Grenander, crossed the Ringbahn railway towards the north up until the end of World War II, when it was intentionally detonated to provide a hindrance to the advancing Soviet troops. Three buildings in the Kopenhagener Straße were destroyed in the process. As a replacement, a 60 m (200 ft) long pedestrian bridge was installed right beside during the governance of the German Democratic Republic (GDR).\n\nA study prepared by the geography department at the Humboldt University of Berlin counted 45 different artists in the street, making this one of Berlin's significant creative functional clusters. Further one finds very many cafés, restaurants, bars and the infamous \"Mittwochsclub\" (Wednesday Club, located in the cellar under the \"Kohlenquelle\" Café in No. 16). The most obvious structure in the street is the former \"Humboldt\" transformer station at No. 61 (on the corner of \"Sonnenburger Straße\"), that was planned by the renowned German industrial architect Hans Heinrich Müller. An experimental urban children's farm (the \"Moritzhof\") is located at the western end of the Kopenhagener Straße, providing the surreal view of horses and goats against a backdrop of dense urban housing and the Fernsehturm.\n\nThe Kopenhagener Straße was inhabited by artists and bohémiens much before German reunification. Despite the renovation and revival of the majority of the buildings since the fall of the Berlin Wall in 1989, the cobblestone street at the northern rim of Berlin's city centre has maintained its pre-unification calm and creative charm. For this reason the Kopenhagener Straße was selected as the movie set for several movies that indulge in GDR Ostalgie, for example \"Der Rote Kakadu\" and \"Sommer vorm Balkon\". Aside from artists, many actors, authors and architects also have made the street their home or place of work. Approximately half of the current residents lived in the street before reunification. Newer inhabitants are mostly from former West Germany and other European countries.\n\n\n\n"}
{"id": "4210737", "url": "https://en.wikipedia.org/wiki?curid=4210737", "title": "LIGA", "text": "LIGA\n\nLIGA is a German acronym for \"Lithographie, Galvanoformung, Abformung\" (Lithography, Electroplating, and Molding) that describes a fabrication technology used to create high-aspect-ratio microstructures.\n\nThe LIGA consists of three main processing steps; lithography, electroplating and molding. \nThere are two main LIGA-fabrication technologies, X-Ray LIGA, which uses X-rays produced by a synchrotron to create high-aspect ratio structures, and UV LIGA, a more accessible method which uses ultraviolet light to create structures with relatively low aspect ratios.\n\nThe notable characteristics of X-ray LIGA-fabricated structures include:\n\nX-Ray LIGA is a fabrication process in microtechnology that was developed in the early 1980s\na team under the leadership of Erwin Willy Becker and Wolfgang Ehrfeld at the Institute for Nuclear Process Engineering\n(\"Institut für Kernverfahrenstechnik,\" IKVT) at the Karlsruhe Nuclear Research Center, since renamed to the Institute for Microstructure Technology (\"Institut für Mikrostrukturtechnik\", IMT) at the \"Karlsruhe Institute of Technology\" (KIT).\nLIGA was one of the first major techniques to allow on-demand manufacturing of high-aspect-ratio structures (structures that are much taller than wide) with lateral precision below one micrometer.\n\nIn the process, an X-ray sensitive polymer photoresist, typically PMMA, bonded to an electrically conductive substrate, is exposed to parallel beams of high-energy X-rays from a synchrotron radiation source through a mask partly covered with a strong X-ray absorbing material. Chemical removal of exposed (or unexposed) photoresist results in a three-dimensional structure, which can be filled by the electrodeposition of metal. The resist is chemically stripped away to produce a metallic mold insert. The mold insert can be used to produce parts in polymers or ceramics through injection molding.\n\nThe LIGA technique's unique value is the precision obtained by the use of deep X-ray lithography (DXRL). The technique enables microstructures with high aspect ratios and high precision to be fabricated in a variety of materials (metals, plastics, and ceramics). Many of its practitioners and users are associated with or are located close to synchrotron facilities.\n\nUV LIGA utilizes an inexpensive ultraviolet light source, like a mercury lamp, to expose a polymer photoresist, typically SU-8. Because heating and transmittance are not an issue in optical masks, a simple chromium mask can be substituted for the technically sophisticated X-ray mask. These reductions in complexity make UV LIGA much cheaper and more accessible than its X-ray counterpart. However, UV LIGA is not as effective at producing precision molds and is thus used when cost must be kept low and very high aspect ratios are not required.\n\nX-ray masks are composed of a transparent, low-Z carrier, a patterned high-Z absorber, and a metallic ring for alignment and heat removal. Due to extreme temperature variations induced by the X-ray exposure, carriers are fabricated from materials with high thermal conductivity to reduce thermal gradients. Currently, vitreous carbon and graphite are considered the best material, as their use significantly reduces side-wall roughness. Silicon, silicon nitride, titanium, and diamond are also in use as carrier substrates but not preferred, as the required thin membranes are comparatively fragile and titanium masks tend to round sharp features due to edge fluorescence. Absorbers are gold, nickel, copper, tin, lead, and other X-ray absorbing metals.\n\nMasks can be fabricated in several fashions. The most accurate and expensive masks are those created by electron beam lithography, which provides resolutions as fine as in resist thick and features in resist thick. An intermediate method is the plated photomask which provides resolution and can be outsourced at a cost on the order of $1000 per mask. The least expensive method is a direct photomask, which provides resolution in resist thick. In summary, masks can cost between $1000 and $20,000 and take between two weeks and three months for delivery. Due to the small size of the market, each LIGA group typically has its own mask-making capability. Future trends in mask creation include larger formats, from a diameter of to , and smaller feature sizes.\n\nThe starting material is a flat substrate, such as a silicon wafer or a polished disc of beryllium, copper, titanium, or other material. The substrate, if not already electrically conductive, is covered with a conductive plating base, typically through sputtering or evaporation.\n\nThe fabrication of high-aspect-ratio structures requires the use of a photoresist able to form a mold with vertical sidewalls. Thus the photoresist must have a high selectivity and be relatively free from stress when applied in thick layers. The typical choice, poly(methyl methacrylate) (PMMA) is applied to the substrate by a glue-down process in which a precast, high-molecular-weight sheet of PMMA is attached to the plating base on the substrate. The applied photoresist is then milled down to the precise height by a fly cutter prior to pattern transfer by X-ray exposure. Because the layer must be relatively free from stress, this glue-down process is preferred over alternative methods such as casting. Further, the cutting of the PMMA sheet by the fly cutter requires specific operating conditions and tools to avoid introducing any stress and crazing of the photoresist.\n\nA key enabling technology of LIGA is the synchrotron, capable of emitting high-power, highly collimated X-rays. This high collimation permits relatively large distances between the mask and the substrate without the penumbral blurring that occurs from other X-ray sources. In the electron storage ring or synchrotron, a magnetic field constrains electrons to follow a circular path and the radial acceleration of the electrons causes electromagnetic radiation to be emitted forward. The radiation is thus strongly collimated in the forward direction and can be assumed to be parallel for lithographic purposes. Because of the much higher flux of usable collimated X-rays, shorter exposure times become possible. Photon energies for a LIGA exposure are approximately distributed between 2.5 and .\n\nUnlike optical lithography, there are multiple exposure limits, identified as the top dose, bottom dose, and critical dose, whose values must be determined experimentally for a proper exposure. The exposure must be sufficient to meet the requirements of the bottom dose, the exposure under which a photoresist residue will remain, and the top dose, the exposure over which the photoresist will foam. The critical dose is the exposure at which unexposed resist begins to be attacked. Due to the insensitivity of PMMA, a typical exposure time for a thick PMMA is six hours. During exposure, secondary radiation effects such as Fresnel diffraction, mask and substrate fluorescence, and the generation of Auger electrons and photoelectrons can lead to overexposure.\n\nDuring exposure the X-ray mask and the mask holder are heated directly by X-ray absorption and cooled by forced convection from nitrogen jets. Temperature rise in PMMA resist is mainly from heat conducted from the substrate backward into the resist and from the mask plate through the inner cavity air forward to the resist, with X-ray absorption being tertiary. Thermal effects include chemistry variations due to resist heating and geometry-dependent mask deformation.\n\nFor high-aspect-ratio structures the resist-developer system is required to have a ratio of dissolution rates in the exposed and unexposed areas of 1000:1. The standard, empirically optimized developer is a mixture of tetrahydro-1,4-oxazine (), 2-aminoethanol-1 (), 2-(2-butoxyethoxy)ethanol (), and water (). This developer provides the required ratio of dissolution rates and reduces stress-related cracking from swelling in comparison to conventional PMMA developers. After development, the substrate is rinsed with deionized water and dried either in a vacuum or by spinning. At this stage, the PMMA structures can be released as the final product (e.g., optical components) or can be used as molds for subsequent metal deposition.\n\nIn the electroplating step, nickel, copper, or gold is plated upward from the metalized substrate into the voids left by the removed photoresist. Taking place in an electrolytic cell, the current density, temperature, and solution are carefully controlled to ensure proper plating. In the case of nickel deposition from NiCl in a KCl solution, Ni is deposited on the cathode (metalized substrate) and Cl evolves at the anode. Difficulties associated with plating into PMMA molds include voids, where hydrogen bubbles nucleate on contaminates; chemical incompatibility, where the plating solution attacks the photoresist; and mechanical incompatibility, where film stress causes the plated layer to lose adhesion. These difficulties can be overcome through the empirical optimization of the plating chemistry and environment for a given layout.\n\nAfter exposure, development, and electroplating, the resist is stripped. One method for removing the remaining PMMA is to flood expose the substrate and use the developing solution to cleanly remove the resist. Alternatively, chemical solvents can be used. Stripping of a thick resist chemically is a lengthy process, taking two to three hours in acetone at room temperature. In multilayer structures, it is common practice to protect metal layers against corrosion by backfilling the structure with a polymer-based encapsulant. At this stage, metal structures can be left on the substrate (e.g., microwave circuitry) or released as the final product (e.g., gears).\n\nAfter stripping, the released metallic components can be used for mass replication through standard means of replication such as stamping or injection molding.\n\nIn the 1990s, LIGA was a cutting-edge MEMS fabrication technology, resulting in the design of components showcasing the technique's unique versatility. Several companies that begin using the LIGA process later changed their business model (e.g., Steag microParts becoming Boehringer Ingelheim microParts, Mezzo Technologies). Currently, only two companies, HTmicro and microworks, continue their work in LIGA, benefiting from limitations of other competing fabrication technologies. UV LIGA, due to its lower production cost, is employed more broadly by several companies, such as Tecan, Temicon, and Mimotec in Switzerland, who supply the Swiss watch market with metal parts made of Nickel and Nickel-Phosphorus.\n\nBelow is a gallery of LIGA-fabricated structures arranged by date.\n\n\n\n"}
{"id": "16964677", "url": "https://en.wikipedia.org/wiki?curid=16964677", "title": "Larson–Miller parameter", "text": "Larson–Miller parameter\n\nThe Larson–Miller parameter is a means of predicting the lifetime of material vs. time and temperature using a correlative approach based on the Arrhenius rate equation. The value of the parameter is usually expressed as \"LMP\" = \"T\"(\"C\" + log \"t\"), where \"C\" is a material specific constant, often approximated as 20, \"t\" is the time in hours, and \"T\" is the temperature in kelvins.\n\nCreep-stress rupture data for high-temperature creep-resistant alloys are often plotted as log stress to rupture versus\na combination of log time to rupture and temperature. One of the most common time–temperature parameters used to present this kind of data is the Larson–Miller (L.M.) parameter, which in generalized form is\nwhere\n\nAccording to the L.M. parameter, at a given stress level the log time to stress rupture plus a constant of the order of 20 multiplied by the temperature in kelvins or degrees Rankine remains constant for a given material.\n\n\n"}
{"id": "30366560", "url": "https://en.wikipedia.org/wiki?curid=30366560", "title": "Lijiaxia Dam", "text": "Lijiaxia Dam\n\nThe Lijiaxia Dam (李家峡水库) is a concrete arch-gravity dam on the Yellow River in Jainca County, Qinghai Province, China. The dam houses a hydroelectric power station with 5 x 400 MW generators for a total installed capacity of 2,000 MW. Construction began in April 1988 and the reservoir began to fill on December 26, 1996. On January 26, 1997, the initial reservoir operating level was reached and the first generator was commissioned in February.\n\n"}
{"id": "22423074", "url": "https://en.wikipedia.org/wiki?curid=22423074", "title": "Linde–Frank–Caro process", "text": "Linde–Frank–Caro process\n\nThe Linde–Frank–Caro process is a method for hydrogen production by removing hydrogen and carbon dioxide from water gas by condensation. The process was invented in 1909 by Adolf Frank and developed with Carl von Linde and Heinrich Caro.\n\nWater gas is compressed to 20 bar and pumped into the Linde-Frank-Caro reactor. A water column removes most of the carbon dioxide and sulfur. Tubes with caustic soda then remove the remaining carbon dioxide, sulphur, and water from the gas stream. The gas enters a chamber and is cooled to −190 °C, resulting in the condensation of most of the gas to a liquid. The remaining gas is pumped to the next vessel where the nitrogen is liquefied by cooling to −205 °C, resulting in hydrogen gas as an end product.\n\n"}
{"id": "40040124", "url": "https://en.wikipedia.org/wiki?curid=40040124", "title": "List of largest oil and gas companies by revenue", "text": "List of largest oil and gas companies by revenue\n\n ** \"Revenue in 2012\"\n\n\n"}
{"id": "54191143", "url": "https://en.wikipedia.org/wiki?curid=54191143", "title": "Lithospheric mantle", "text": "Lithospheric mantle\n\nThe lithospheric mantle is the uppermost solid part of mantle.\n\nthe lithospheric mantle is subdvided into the subcontinental lithospheric mantle associated with the continental lithosphere and oceanic lithospheric mantle, associated with the oceanic lithosphere.\n"}
{"id": "1646586", "url": "https://en.wikipedia.org/wiki?curid=1646586", "title": "Machine taper", "text": "Machine taper\n\nA machine taper is a system for securing cutting tools or toolholders in the spindle of a machine tool or power tool. A male member of conical form (that is, with a taper) fits into the female socket, which has a matching taper of equal angle.\n\nAlmost all machine tool spindles, and many power tool spindles, have a taper as their primary method of attachment for tools. Even on many drill presses, handheld drills, and lathes, which have chucks (such as a drill chuck or collet chuck), the chuck is attached by a taper. On drills, drill presses, and milling machines, the male member is the tool shank or toolholder shank, and the female socket is integral with the spindle. On lathes, the male may belong to the tool or to the spindle; spindle noses may have male tapers, female tapers, or both.\n\nMachine tool operators must be able to install or remove tool bits quickly and easily. A lathe, for example, has a rotating spindle in its headstock, to which one may want to mount a spur drive or work in a collet. Another example is a drill press, to which an operator may want to mount a bit directly, or using a drill chuck.\n\nVirtually all milling machines, from the oldest manual machines up to the most modern CNC machines, utilize tooling that is piloted on a tapered surface.\n\nThe machine taper is a simple, low-cost, highly repeatable, and versatile tool mounting system. It provides indexability, as tools can be quickly changed but are precisely located both concentrically and axially by the taper. It also allows high power transmission across the interface, which is needed for milling.\n\nMachine tapers can be grouped into self-holding and self-releasing classes. With self-holding tapers, the male and female wedge together and bind to each other to the extent that the forces of drilling can be resisted without a drawbar, and the tool will stay in the spindle when idle. It is driven out with a wedge when a tool change is needed. Morse and Jacobs tapers are an example of the self-holding variety. With self-releasing tapers, the male will not stick in the female without a drawbar holding it there. However, with good drawbar force, it is very solidly immobile. NMTB/CAT, BT and HSK are examples of the self-releasing variety.\n\nFor light loads (such as encountered by a lathe tailstock or a drill press), tools with self-holding tapers are simply slipped onto or into the spindle; the pressure of the spindle against the workpiece drives the tapered shank tightly into the tapered hole. The friction across the entire surface area of the interface provides a large amount of torque transmission, so that splines or keys are not required.\n\nFor use with heavy loads (such as encountered by a milling machine spindle), there is usually a key to prevent rotation and/or a threaded section, which is engaged by a drawbar that engages either the threads or the head of a pull stud that is screwed into them. The drawbar is then tightened, drawing the shank firmly into the spindle. The draw-bar is important on milling machines as the transverse force component would otherwise cause the tool to wobble out of the taper.\n\nAll machine tapers are sensitive to chips, nicks (dents), and dirt. They will not locate accurately, and the self-holding variety will not hold reliably, if such problems interfere with the seating of the male into the female with firm contact over the whole conical surface. Machinists are trained on keeping tapers clean and handling them in ways that prevent them from being nicked by other tools. CNC tool-changing cycles usually include a compressed-air blast while one toolholder is being swapped with the next. The air blast tends to blow away chips that might otherwise end up interfering between the toolholder and spindle.\n\nTools with a tapered shank are inserted into a matching tapered socket and pushed or twisted into place. They are then retained by friction. In some cases, the friction fit needs to be made stronger, as with the use of a drawbar, essentially a long bolt that holds the tool into the socket with more force than is possible by other means.\n\nCaution needs to be exercised in the usual drilling machine or lathe situation, which provides no drawbar to pull the taper into engagement, if a tool is used requiring a high torque but providing little axial resistance. An example would be the use of a large diameter drill to slightly enlarge an existing hole. In this situation, there may be considerable rotary loading. In contrast, the cutting action will require very little thrust or feed force. Thrust helps to keep the taper seated and provides essential frictional coupling.\n\nThe tang is not engineered to withstand twisting forces which are sufficient to cause the taper to slip, and will frequently break off in this situation. This will allow the tool to spin in the female taper, which is likely to damage it. Morse taper reamers are available to alleviate minor damage.\n\nTapered shanks \"stick\" in a socket best when both the shank and the socket are clean. Shanks can be wiped clean, but sockets, being deep and inaccessible, are best cleaned with a specialized taper cleaning tool which is inserted, twisted, and removed.\n\nTapered shank tools are removed from a socket using different approaches, depending on the design of the socket. In drill presses and similar tools, the tool is removed by inserting a wedge shaped block of metal called a \"drift\" into a rectangular shaped cross hole through the socket and tapping it. As the cross section of the drift gets larger when the drift is driven further in, the result is that the drift, bearing against the foremost edge of the tang, pushes the tool out. In many lathe tailstocks, the tool is removed by fully withdrawing the quill into the tailstock, which brings the tool up against the end of the leadscrew or an internal stud, separating the taper and releasing the tool. Where the tool is retained by a drawbar, as in some mill spindles, the drawbar is partially unthreaded with a wrench and then tapped with a hammer, which separates the taper, at which point the tool can be further unthreaded and removed. Some mill spindles have a captive drawbar which ejects the tool when actively unscrewed past the loose stage; these do not require tapping. For simple sockets with open access to the back end, a drift punch is inserted axially from behind and the tool tapped out.\n\nThere are many standard tapers, which differ based on the following:\n\nThe standards are grouped into families that may include different sizes. The taper within a family may or may not be consistent. The Jarno and NMTB tapers are consistent, but the Jacobs and Morse families vary.\n\nThere are adaptors available to allow the use of one type of taper tooling, e.g. Morse, on a machine with a different taper, e.g. R8 or vice versa, and simpler adaptors consisting of an externally and internally tapered sleeve to allow a small Morse tool to be used in a machine of larger bore.\n\nOne of the first uses of tapers was to mount drill bits directly to machine tools, such as in the tailstock of a lathe, although later drill chucks were developed that held parallel shank drill bits.\n\nBrown & Sharpe tapers, standardized by the company of the same name, are an alternative to the more-commonly seen Morse taper. Like the Morse, these have a series of sizes, from 1 to 18, with 7, 9 and 11 being the most common. Actual taper on these lies within a narrow range close to .500 inches per foot.\n\nThe Jacobs Taper (abbreviated JT) is commonly used to secure drill press chucks to an arbor. The taper angles are not consistent varying from 1.41° per side for #0 (and the obscure #) to 2.33° per side for #2 (and #2 short).\n\nThere are also several sizes between #2 and #3: #2 short, #6 and #33.\n\nJarno tapers use a greatly simplified scheme. The rate of taper is 1:20 on diameter, in other words 0.600\" on diameter per foot, .050\" on diameter per inch.\nTapers range from a Number 2 to a Number 20. The diameter of the big end in inches is always the taper size divided by 8, the small end is always the taper size divided by 10 and the length is the taper size divided by 2. For example, a Jarno #7 measures 0.875\" (7/8) across the big end. The small end measures 0.700\" (7/10) and the length is 3.5\" (7/2).\n\nThe system was invented by Oscar J. Beale of Brown & Sharpe.\n\nThe Morse taper was developed by Stephen A. Morse, based in New Bedford Massachusetts, in the mid-1860s. Since then, it has evolved to encompass smaller and larger sizes and has been adopted as a standard by numerous organizations, including the International Organization for Standardization (ISO) as ISO 296 and the German Institute for Standardization (DIN) as DIN 228-1. It is one of the most widely used types, and is particularly common on the shank of taper-shank twist drills and machine reamers, in the spindles of industrial drill presses, and in the tailstocks of lathes. The taper angle of the Morse taper varies somewhat with size but is typically 1.49 degrees (around 3 degrees included).\n\nSome modular orthopedic total hip implants use a Morse taper to mate components together. Similarly, some dental implants use a Morse taper to connect components.\n\nMorse tapers come in eight sizes identified by whole numbers between 0 and 7, and one half-size (4 1/2 - very rarely found, and not shown in the table).\nOften the designation is abbreviated as MT followed by a digit, for example a Morse taper number 4 would be MT4. The MT2 taper is the size most often found in drill presses up to \" capacity. Stub (short) versions, the same taper angle but a little over half the usual length, are occasionally encountered for the whole number sizes from 1 through 5. There are standards for these, which among other things are sometimes used in lathe head stocks to preserve a larger spindle through-hole.\n\nMorse tapers are of the self-holding variety, and can have three types of ends:\n\nSelf holding tapers rely on a heavy preponderance of axial load over torsional load to transmit high torques. Problems may arise using large drills in relation to the shank, if the pilot hole is too large. The threaded style is essential for any sideloading, particularly milling. The only exception is that such unfavourable situations can be simulated to remove a jammed shank. Permitting chatter will help release the grip. The acute (narrow) taper angle can result in such jamming with heavy axial loads, or over long periods.\n\nEnd-milling cutters with a Morse taper shank with a tang are occasionally seen: for security these must be used with a C-collar or similar, fitting into the neck between cutter and shank, and pulling back against the large end of the taper\n\nThe taper itself is roughly 5/8\" per foot, but exact ratios and dimensions for the various sizes of tang type tapers are given below.\n\nB-series tapers are a DIN standard typically used for fitting chucks on their arbors, like the older Jacobs taper series. Each taper in the B-series is effectively the small or large end of a Morse taper:\n\nThe number after the B is the diameter of the large end of the taper to the nearest mm, and 'about' 1mm larger than the large end of the socket (~2mm in the case of B22 and B24) \n\nThe National Machine Tool Builders Association (now called the Association for Manufacturing Technology) defined a steep taper that is commonly used on milling machines. The taper is variously referred to as NMTB, NMT or NT. The taper is 3.500 inches per foot and is also referred to as \"7 in 24\" or 7/24; the computed angle is 16.5943 degrees. All NMTB tooling has this taper but the tooling comes in different sizes: NMTB-10, 15, 20, 25, 30, 35, 40, 45, 50 and 60. These tapers were apparently also specified in ASA (now ANSI) B5.10-1943.\n\nNMTB is a \"self releasing\" or \"fast\" taper. Unlike the more acute self holding tapers above, such tapers are not designed to transmit high torque; high torques are carried by driving keys engaging slots on the flange. The purpose is to allow a quick and easy change between different tools (either automatically or by hand) while ensuring the tool or toolholder will be tightly and rigidly connected to the spindle, and accurately coaxial with it. The larger end adjacent to the tool makes for more rigidity than is possible with Morse or R8 tapers fitted to comparable machines.\n\nPatent 1794361 (filed 25 March 1927) describes milling machine spindle and tool shapes using a steep taper. The patent was assigned to Kearney & Trecker Corporation, Brown & Sharpe, and Cincinnati Milling Machine Company. The patent wanted a taper that would freely release the tool and found that a taper of 3.5 in 12 had that property. The patent also used the keys and slots and a tail on the tool shank to prevent the tool shank from falling out of a horizontal mill's spindle while the operator connected the drawbar.\n\nANSI B5.18-1972 specifies some essential dimensions for milling machine spindles and tool shanks using taper sizes 30, 40, 45, 50, 60. The specifications describe the position of the driving key and flange and the thread of the draw-in bolt that holds the shank in the spindle.\n\nThe tooling is referred to as Quick Change; National Machine Tool Builders' Association, 1927; NMTB; American Standard Machine Taper, ANSI B5.18; DIN 2080 / IS 2340; ISO R 290-2583. There are slight variations in threads and flanges (JIS B 6339: MAS 403); and the European standards (e.g., ISO taper) use metric draw threads.\n\nThe NMTB tool shanks had the 7 in 24 taper, but they also had a constant diameter tail (pilot) at the end of the shank that was described in the 1927 patent. Subsequent design variations dropped the tail (making the shank shorter) and put a V-groove in the flange that aided automated tool changing. Modern designs started using power drawbars that gripped pull studs (also known as retention knobs) that were screwed into the tool shank rather than screw-in drawbars. The power drawbar would grip the pull stud rather than screwing into the tool shank.\n\nThe more modern toolholder designs became known as the Caterpillar \"V-Flange\", CAT, V-Flange, ANSI B5.50, SK, ISO, International (INT), BT, ISO 7388-1, DIN 69871, NFE 62540. Once again, there are slight variations in the tooling. Although the basic taper dimensions are the same, there are differences are in the flanges, draw-in thread sizes, and pull studs; the international versions use metric sizes.\n\nHSK toolholders were developed in the early 1990s. HSK stands for ; German for \"hollow shank tapers\".\n\nSteep tapers tend to loosen at high speed, as their solid shanks are stiffer than the spindles they fit into, so under high centrifugal force, the spindle expands more than the toolholder which changes the overall length: That is, as the spindle 'expands' the toolholder tends to move deeper into the spindle in the z-axis which can cause the production of parts that are out-of-tolerance. HSK's hollow shank is deliberately thin and flexible, so it expands more than the spindle and tightens when rotating at high speed. Furthermore, the HSK holder is dual contact: It engages with the spindle on both the taper and the top of the flange which prevents axial movement when thermal growth and/or centrifugal force of the spindle occurs.\n\nThe flexibility is also used to provide accurate axial location. An HSK toolholder has both a tapered shank, and a flange with a mating surface. The shank is short (about half as long as other machine tapers), with a shallow taper (a ratio of 1:10), and slightly too large to allow the flange to seat fully in the socket. The thin walls, short shank and shallow taper provide a large opening in the back of the tool. An expanding collet fits in there, and mates with 30° chamfer inside the shank. As the drawbar retracts, it expands the collet and pulls the shank back into the socket, compressing the shank until the flange seats against the front of the spindle. This provides a stiff, repeatable connection because it utilizes the centrifugal force inside the spindle. As centrifugal forces increase the expanding collet within the HSK forces the walls of the toolholder shank to stay in contact with the spindle wall.\n\nThe HSK design was developed as a nonproprietary standard. The working group that produced the HSK standard consisted of representatives from academia, the Association of German Tool Manufacturing and a group of international companies and end users. The results were the German DIN standards 69063 for the spindle and 69893 for the shank.\nThe HSK working group did not adopt a specific product design, but rather a set of standards that defined HSK toolholders for different applications. The group defined a total of six HSK shank forms, in 9 sizes.\n\nSizes are identified by the diameter of the shank’s flange in millimeters. These diameters are taken from the R10′ series of preferred numbers, from 25 to 160 mm.\n\nToday, the shank forms are designated by the letters A through F and T. The main differences between the forms are the positions of the drive slots, gripper-locating slots, coolant holes and the area of the flange.\n\nA is the basic form. The B-form shank is a variant for high-torque applications, and has a flange one size larger relative to its shaft diameter. (Thus, an A-40 shank will fit into a B-50 socket.)\n\nForms C and D are simplified variants of A and B for manual use, eliminating features to accommodate automatic tool changers like a V-groove and associated orientation slots, and a recess for an RFID chip.\n\nForms E and F flanges and tapers are similar to forms A and B, but designed for very high speed machining (20,000 rpm and up) of light materials by eliminating all asymmetric features to minimize imbalance and vibration.\n\nASME B5.62 \"Hollow Taper Tooling With Flange-Face Contact\" and ISO 12164-3:2014 \"Dimensions of shanks for stationary tools\" include an additional form T, which is bidirectionally compatible with form A, but has a much tighter tolerance on the widths of the keys and keyways used for angular alignment. This permits non-rotating lathe tooling to be held accurately.\n\nAn HSK connection depends on a combination of axial clamping forces and taper-shank interference. All these forces are generated and controlled by the mating components’ design parameters. The shank and spindle both must have precisely mating tapers and faces that are square to the taper’s axis. There are several HSK clamping methods. All use some mechanism to amplify the clamping action of equally spaced collet segments. When the toolholder is clamped into the spindle, the drawbar force produces a firm metal-to-metal contact between the shank and the ID of the clamping unit. An additional application of drawbar force positively locks the two elements together into a joint with a high level of radial and axial rigidity. As the collet segments rotate, the clamping mechanism gains centrifugal force. The HSK design actually harnesses centrifugal force to increase joint strength. Centrifugal force also causes the thin walls of the shank to deflect radially at a faster rate than the walls of the spindle. This contributes to a secure connection by guaranteeing strong contact between the shank and the spindle. The automotive and aerospace industries are the largest users of HSK toolholders. Another industry that is seeing increasing use is the mold and die industry.\n\nThis taper was designed by Bridgeport Machines, Inc. for use in its milling machines. R8 tapers are not self-holding, so they require a drawbar extending up through the spindle to the top of the machine to prevent loosening when lateral forces are encountered. They are also keyed (see image) to prevent rotation during insertion and removal, although it is the taper that transmits torque in use. The drawbar thread is typically ″–20 tpi (UNF). The angle of the cone is 16°51′ (16.85°) with an OD of 1.25″ and a length of ″. (source, Bridgeport Manufacturer) The diameter of the parallel locating portion is not a \"fractional inch\" size like the other dimensions and is 0.949″ to 0.9495″.\n\nTools with an R8 taper are inserted directly into the machine's spindle. R8 collets are typically used to hold tooling with round shanks, although any shape can be held if the collet has the corresponding shape cut in it. The collets have a precision bore with axial compression slots for holding cutting tools and are threaded for the drawbar. The R8 system is commonly used with collets ranging in size from ″ to ″ in diameter or tool holders with the same or slightly larger diameters. The collets or tool holders are placed directly into the spindle and the drawbar is tightened into the top of the collet or tool holder from above the spindle. Other tools such as drill chucks, fly cutters, indexable insert cutters, etc. may have an R8 taper shank built into or added to the tool.\n\nThe R8 taper is commonly encountered on Bridgeport and similar turret mills from the USA, or on (very common) copies of these mills from elsewhere. The popularity is due in large part to the success of Bridgeport and other mills that were closely modeled after it and produced throughout much of the 20th century.\n\n\nSources\n\n"}
{"id": "25866004", "url": "https://en.wikipedia.org/wiki?curid=25866004", "title": "March 18–21, 1958 nor'easter", "text": "March 18–21, 1958 nor'easter\n\nThe March 18–21, 1958 nor'easter was an unusual late-season winter storm that impacted the Mid-Atlantic and New England regions of the United States. Its snowfall extended from North Carolina through Maine.\n\n"}
{"id": "33925047", "url": "https://en.wikipedia.org/wiki?curid=33925047", "title": "Mountain leather", "text": "Mountain leather\n\nMountain leathers are flexible, sheet-like formations of asbestiform minerals, resembling leather. Minerals known to form mountain leather include:\n\nMindat.org\n"}
{"id": "33692407", "url": "https://en.wikipedia.org/wiki?curid=33692407", "title": "Nanoinverter", "text": "Nanoinverter\n\nA nanoinverter, also referred as nano inverter or solar nano inverter, converts direct current (DC) from a single solar cell or small solar panel to alternating current (AC). Nanoinverters contrast with microinverter devices, which are connected to larger than 100 Watt solar panels.\n\nNanoinverters have several advantages over microinverters. The main advantage is that, even small amounts of shading, debris or snow lines in any one solar cell, or a smaller panel failure, does not disproportionately reduce the output of an entire larger panel. Each nanoinverter obtains optimum power by performing maximum power point tracking for its connected panel.\n\nBIPV modules produce direct current power from range 10 watts to 100 watts.\nIntegrated circuit-based AC nanoinverter on each module increases harvest rate by 30% and substantially decreases the installation constraints.\n\nDC nanoconverter-on-chip is capable to harvest from microwatts power and as low as 80mV (0.08V) voltage. Harvest rate improves 30%-70% with integrated MPPT and DC-DC booster\n"}
{"id": "40848970", "url": "https://en.wikipedia.org/wiki?curid=40848970", "title": "Norman L. James", "text": "Norman L. James\n\nNorman Leslie James (November 29, 1840 - November 25, 1918) was a farmer, lumber manufacturer and hardware retailer from Richland Center, Wisconsin who served as a member of the Wisconsin State Assembly and the Wisconsin State Senate.\n\nJames was born November 29, 1840 in Deerfield, New Hampshire. He received a common school education, and came to Wisconsin with his family, eventually settling in Richland Center.\n\nWith his little brother David, James entered the United States Army in 1861, joining Company F of 16th Wisconsin Volunteer Infantry Regiment, upon the outbreak of the American Civil War. He participated in the Battle of Pittsburg Landing, and was discharged in 1862.\n\nJames served as a member of the town and village board, and village treasurer. He was first elected to the Assembly for Richland County's first Assembly district (the Towns of (Towns of Buena Vista, Henrietta, Ithaca, Orion, Richland, Rockbridge, Westford and Willow) in 1872 as a Republican; he did not seek re-election, and was succeeded by fellow Republican Joseph McGrew. He was elected again in 1874, with 776 votes to 659 for Democrat V. G. Harter. He was not a candidate for re-election in 1875, and was succeeded by Democrat J. L. R. McCollum.\n\nJames served as a delegate to the 1880 Republican National Convention. He was elected state senator for the 28th District (at that time consisting of Iowa and Richland Counties) in 1884 (Republican incumbent William Meffert was not a candidate), receiving 4,712 votes, against 4,291 for Democrat George Crawford and 573 for Prohibitionist John Lee. He served as chairman of the standing committee on railroads. He was not a candidate for re-election in 1888, and was succeeded by another Republican, Robert Joiner\n\nJames was described in the Wisconsin Blue Books as a merchant and a hardware merchant. He testified in a Congressional hearing that he manufactured lumber, and sold lumber at retail.\n\nIn 1911, he testified before the United States Senate about his involvement in the election of Isaac Stephenson to the Senate from Wisconsin. He described himself as a longtime friend and supporter of Stephenson, whom he had met when they were both delegates to the 1880 Republican National Convention, and with whom he often went fishing. He testified that the only money he had been paid from the campaign was reimbursement for expenditures he'd made in support of Stephenson's election. He testified that while he was not (as had been inquired) \"a man of some means,\" nonetheless \"... I have always taken an active part in politics. That is, I have always had some man as a candidate that I was interest in.\" \n\nJames' brother, David Goodrich James, was a member of the Senate from the same district from 1909 to 1912. His niece Ada James became a noted suffragist.\n\nHe died in Richland Center on November 25, 1918.\n"}
{"id": "21739551", "url": "https://en.wikipedia.org/wiki?curid=21739551", "title": "Oak forest", "text": "Oak forest\n\nAn oak forest is a plant community with a tree canopy dominated by oaks (\"Quercus spp.\"). In terms of canopy closure, oak forests contain the most closed canopy, compared to oak savannas and oak woodlands. \n\n\n"}
{"id": "6704127", "url": "https://en.wikipedia.org/wiki?curid=6704127", "title": "Oiler (occupation)", "text": "Oiler (occupation)\n\nAn oiler (also known as a \"greaser\") is a worker whose main job is to oil machinery. In previous eras there were oiler positions in various industries, including maritime work (naval and commercial), railroading, steelmaking, and mining. Today most such positions have been eliminated through technological change; lubrication tends to require less human intervention, so that workers seldom have oiling as a principal duty. In the days of ubiquitous plain bearings, oiling was often a job description in and of itself.\n\nToday, shipping is the economic segment that most thoroughly retains the notion of the oiler as a separate position. On a merchant ship, an oiler is an unlicensed rate of the engineering department. The position is of the junior rate in the engine room of a ship. The oiler is senior only to a wiper. Once a sufficient amount of sea time is acquired, the Oiler can apply to take a series of courses/examinations to become certified as an engineer.\n\nAs a member of the engineering department, the oiler operates and maintains the propulsion and other systems on board the vessel. Oilers also deal with the \"hotel\" facilities on board, notably the sewage, lighting, air conditioning, and water systems. They assist bulk fuel transfers and require training in firefighting and first aid. Moreover, oilers help facilitate operation of the ship's boats and other nautical tasks – especially with cargo loading/discharging gear and safety systems. However, the specific cargo discharge function remains the responsibility of deck officers and deck workers.\n\n\nUnder international conventions and agreements specifically the International Convention on Standards of Training, Certification and Watchkeeping for Seafarers (or STCW), all oilers who sail internationally are documented by their respective countries. Recent changes to the STCW no longer refer to specific rates such as oiler. The terms Able Seafarer-Engine and Able Seafarer-Deck are now used to refer to unlicensed positions on vessels engaged in international trade. \n\nIn the United States, Title 46 (Shipping) of the Code of Federal Regulations governs who is eligible to sail as an oiler in National trade. \n\nA person has to have a Merchant Mariner's Document issued by the United States Coast Guard in order to be employed as an oiler in the United States Merchant Marine. \n\nTo work as an oiler in the Canadian Coast Guard, it is required that the individual holds an engine room rating certificate. In order to obtain this certificate the applicant must first have no less than six months of documented sea time working under the supervision of a watch keeping engineer. The applicant must then go to a Transport Canada Office and have a written and oral exam before the certificate is issued. It is also required for all sea going personnel to have MED (marine emergency duty) certificates which can be obtained by taking courses through a college approved for training.\n\nOilers working for the Canadian Coast Guard have various duties, some of which include, performing regular maintenance of the ship's engines and other equipment, assisting the engineers with the repair and overhaul of the ship’s engines and equipment, making regular rounds of the engine room and designated spaces and alerting the engineer on watch of any problems noted. The oiler is also required to keep the engine room clean, tidy and freshly painted. They are the most junior crewmember of the engineering department in the Canadian Coast Guard.\n\n\n"}
{"id": "19230473", "url": "https://en.wikipedia.org/wiki?curid=19230473", "title": "PROX", "text": "PROX\n\nPROX is an acronym for PReferential OXidation, and refers to the preferential oxidation of a gas on a catalyst.\n\nThe catalyst preferentially oxidises carbon monoxide (CO) using a heterogeneous catalyst placed upon a ceramic support. Catalysts include metals such as platinum, platinum/iron, platinum/ruthenium, gold nanoparticles as well as novel copper oxide/ceramic conglomerate catalysts.\n\nThis reaction is a considerable subject area of research with implications for fuel cell design.\nIts main utility lies in the removal of carbon monoxide (CO) from the fuel cell's feed gas. CO poisons the catalyst of most low-temperature fuel cells.\n\nCarbon monoxide is often produced as a by-product from steam reforming of hydrocarbons, which produces hydrogen and CO.\nIt is possible to consume most of the CO by reacting it with steam in the water-gas shift reaction:\n\nThe water-gas shift reaction can reduce CO to 1% of the feed, with the added benefit of producing more hydrogen, but not eliminate it completely.\nTo be used in a fuel cell, feed gas must have CO below 10 ppm.\n\nThe PROX process allows for the reaction of CO with oxygen, reducing CO concentration from approximately 0.5–1.5% in the feed gas to less than 10 ppm.\n\nDue to the prevalent presence of hydrogen in the feed gas, the competing, undesired combustion of hydrogen will also occur to some degree:\n\nThe selectivity of the process is a measure of the quality of the reactor, and is defined as the ratio of consumed carbon monoxide to the total of consumed hydrogen and carbon monoxide.\n\nThe disadvantage of this technology is its very strong exothermic nature, coupled with a very narrow optimal operation temperature window, and is best operated between 353 and 450 kelvins, yielding a hydrogen loss of around one percent. Effective cooling is therefore required. In order to minimise steam generation, excessive dilution with nitrogen is used. Additionally the reaction is interrupted with an intermediary cooler before proceeding to a second stage.\n\nIn the first reaction an excess of oxygen is provided, at around a factor of two, and about 90% of the CO is transformed. In the second step a substantially higher oxygen excess is used, at approximately a factor of 4, which is then processed with the remaining CO, in order to reduce the CO concentration to less than 10 ppm. To also avoid excess CO-fraction loading, the transient operation of a CO adsorber may be important.\n\nThe instrumentation and process control complexity requirements are relatively high. The advantage of this technique over selective methanation is the higher space velocity, which reduces the required reactors size. For the case of strong temperature rises, the feed of air can simply be broken.\n\nThe technical origins for CO-PrOx lies in the synthesis of ammonia (Haber process). Ammonia synthesis also has a strict requirement of CO-free syngas, as CO is a strong catalyst poison for the usual catalysts used in this process.\n\n\n"}
{"id": "30383465", "url": "https://en.wikipedia.org/wiki?curid=30383465", "title": "Paper texture effects in calotype photography", "text": "Paper texture effects in calotype photography\n\nPaper texture effects in calotype photography limit the ability of this early process to record low contrast details and textures. A calotype is a photographic negative produced on uncoated paper. (See Paper negative.) An important feature is that a relatively short exposure in a camera produces a latent image that is subsequently made visible by development. Then positive images for viewing are obtained by contact printing. This technique was in use principally from 1840 into the 1850s, when it was displaced by photography on glass. Skilled photographers were able to achieve dramatic results with the calotype process, and the reason for its eclipse may not be evident from viewing reproductions of early work.\n\nPractical photography plausibly dates from the announcement of the Daguerreotype in 1839. This stimulated work by others: in 1840, the Englishman Talbot discovered what he called the calotype process for making photographic negatives on writing paper with the relatively short exposure time of a few minutes for subjects in bright sunlight. A positive print could subsequently be made by pressing a negative under glass against another piece of sensitive paper and exposing it to sunlight. For this, negatives were waxed to make them more transparent. Since many prints could be made from a single negative, travel photography became possible, and prints from calotypes were soon distributed in book form and sold in shops. The chemical operations used to make a calotype were not difficult to perform, and relatively few specialized supplies were required. But the process had a serious deficiency, and it was quickly replaced by photography on glass when this became practical.\n\nPaper texture limits what can be done with the calotype process. This texture can be seen by holding a piece of copier paper up to the light: the way fibers clump in the paper making process causes a relatively low contrast pattern that may remind one of bushes growing on a hillside. This pattern persists when paper is waxed, as similarly examining a piece of commercial waxed paper used for wrapping food will show. Since the contrast in the pattern is low, it does not interfere with tracing a drawing if the paper is thin enough for dark lines to show through. But it is very effective at obscuring low contrast patterns and textures and making them invisible. Consequently, calotype photographs can show bold outlines and high-contrast details clearly, but low-contrast details and textures tend to be lost because of non-uniform paper transmittance. Paper texture effects are limiting in nature photography, for example, where one expects to capture subtle patterns such as those produced by plants growing in close proximity or pebbles in a streambed. Early calotype photographers appear to have dealt with texture by composing with an eye for high contrasts and bold outlines. However, they sometimes resorted to inking in the sky areas of their negatives to eliminate distracting mottling. Information given in various works where calotype images are reproduced indicates that negative sizes of 20 x 25 cm (8 x 10 in) or greater were not unusual. This seems practical for a professional who planned to profit from the sale of contact prints that would be the same size as the original negative, but less so for an amateur because of the large amounts of silver nitrate required. The way paper texture effects become relatively more pronounced as negative size decreases can be demonstrated with a digital camera by photographing a sheet of paper lit from behind, first at a distance where the sheet just overfills the field of view, and then at a closer distance where the field of view is perhaps one third as wide. (A stack of the same paper will appear much more uniform if it is viewed by \"reflected\" light with diffuse illumination.) Anyone who wishes to make calotypes today should expect that otherwise successful negatives will suffer in comparison to the best surviving early work if a relatively small format such as 10 x 10 cm (4 x 4 in) is used. Despite the use of large formats and careful paper selection, continuing dissatisfaction with paper stimulated the invention of the collodion on glass process (see Collodion process) that quickly displaced calotypes after 1851, even though paper was the easier medium to work with. \n\nTo understand why the calotype process could sometimes produce attractive results but was found to be generally unsatisfactory, it would be useful to compare pictures of the same subject taken with first paper and then with glass (or clear plastic film) as the negative substrate. An easier way to gain insight into the destructive effect of low contrast spurious texture is to view a relatively large format (not 35 mm) photographic negative in contact with a piece of tracing paper by transmitted light. Positives can be made with a digital camera and digital image processing so that the two cases, with and without the tracing paper, can be compared side by side. Viewed from a distance, the image made through the tracing paper may seem satisfactory. But from closer up, the paper texture will appear more intrusive and begin to camouflage low contrast scene details. This comparison will also demonstrate that paper texture is relatively coarse and, unlike pixelation in a typical digital image, does not quickly become inconspicuous as the distance from which it is viewed increases. Here it is significant to note that the scale of mottling in paper correlates to the average fiber length, which is on the order of millimeters, rather than to the much smaller fiber width. \n\nSilver nitrate and other materials used by early photographers are hazardous. Consult references to learn about potential hazards involved in duplicating their methods. Safety goggles are essential for eye protection.\n"}
{"id": "11890087", "url": "https://en.wikipedia.org/wiki?curid=11890087", "title": "Pulsometer pump", "text": "Pulsometer pump\n\nThe Pulsometer steam pump is a pistonless pump which was patented in 1872 by American Charles Henry Hall. In 1875 a British engineer bought the patent rights of the Pulsometer and it was introduced to the market soon thereafter. The invention was inspired by the Savery steam pump invented by Thomas Savery. Around the turn of the century, it was a popular and effective pump for quarry pumping.\n\nThis extremely simple pump was made of cast iron, and had no pistons, rods, cylinders, cranks, or flywheels. It operated by the direct action of steam on water. The mechanism consisted of two chambers. As the steam condensed in one chamber, it acted as a suction pump, while in the other chamber, steam was introduced under pressure and so it acted as a force pump. At the end of every stroke, a ball valve consisting of a small brass ball moved slightly, causing the two chambers to swap functions from suction-pump to force-pump and vice versa. The result was that the water was first suction pumped and then force pumped.\n\nA good explanation can be found in the 1901 article referenced below: The operation of the pulsometer is as follows: The ball being at the entrance of the left-hand chamber, and the right-hand being full of water, steam enters, pressing on the surface of the water, and forcing it out through the discharge passage. A rapid condensation of steam occurs from contact with the water and with the walls of the chamber, previously cooled by the water. When the water level has reached the horizontal edge of the discharge passage, a large volume of steam suddenly escapes and is at once condensed by the relatively cold water between the chamber and the discharge valve. The pressure in the chamber quickly decreases; it cannot be sustained by steam from the boiler, for, in accordance with the inventor's first specifications, the steam pipe is small. If now the pressure in the left chamber is equal, or nearly equal, to that in the right, friction caused by the rapid flow of steam past the ball will draw the ball over and close the right-hand chamber. Cut off from further supply, the steam, in contact with water, begins to condense; a jet of cold water from the discharge pipe spurts up through the injection tube, and by breaking into spray against the side of the steam space, completes the condensation. The partial vacuum produced brings water through the suction valve to fill the chamber; but at the same time the air valve admits a little air, which passes up ahead of the water and forms an elastic cushion to prevent the water from striking violently against the steam ball. The air chamber is for the purpose of preventing water-hammer in the suction pipe.\n\nThe pump ran automatically without attendance. It was praised for its \"extreme simplicity of construction, operation, compact form, high efficiency, economy, durability, and adaptability\". Later designs were improved upon to enhance efficiency and to make the machine more accessible for inspection and repairs, thus reducing maintenance costs.\n\nIn the January 1901 issue of \"Technology Quarterly and Proceedings of the Society of Arts\", an article appeared by Joseph C. Riley describing key operational details and technical evaluation of the pulsometer pump's performance. Riley noted that although somewhat inefficient, the pulsometer's simplicity and robust construction made it well suited to pumping \"thick liquids or semi-fluids, such as heavy syrups, or even liquid mud\".\n\nPulsometer Engineering Company Limited was founded in Britain in 1875 after a British engineer bought the patent rights of the pulsometer pump from Thomas Hall. In 1901 the company moved from London to Reading, Berkshire. In 1961 Pulsometer merged with Sigmund Pumps of Gateshead to form Sigmund Pulsometer Pumps (SPP) and became one of the largest pump company's in Europe. SPP Ltd is now part of Kirloskar Brothers Ltd.\n\n"}
{"id": "22517607", "url": "https://en.wikipedia.org/wiki?curid=22517607", "title": "Ruthenium boride", "text": "Ruthenium boride\n\nRuthenium borides are compounds of ruthenium and boron. Their most remarkable property is potentially high hardness. Vickers hardness H = 50 GPa was reported for thin films composed of RuB and RuB phases. This value is significantly higher than those of bulk RuB or RuB, but it has to be confirmed independently, as measurements on superhard materials are intrinsically difficult. For example, note that the initial report on extreme hardness of related material rhenium diboride was probably too optimistic.\n\nRuthenium diboride was first thought to have a hexagonal structure, as in rhenium diboride, but it was later tentatively determined to possess an orthorhombic structure.\n"}
{"id": "22452250", "url": "https://en.wikipedia.org/wiki?curid=22452250", "title": "Salter's duck", "text": "Salter's duck\n\nSalter's duck, also known as the nodding duck or by its official name the Edinburgh duck, is a device that converts wave power into electricity. The wave impact induces rotation of gyroscopes located inside a pear-shaped \"duck\", and an electrical generator converts this rotation into electricity with an overall efficiency of up to 90%. The Salter's duck was invented by Stephen Salter in response to the oil shortage in the 1970s and was one of the earliest generator designs proposed to the Wave Energy program in the United Kingdom. The funding for the project was cut off in the early 1980s after oil prices rebounded and the UK government moved away from alternative energy sources. As of May 2018 no wave-power devices have ever gone into large-scale production.\n\nAs a result of the 1973 oil crisis, Salter set about creating a source of alternative energy. The idea for creating Salter's duck came about from his studies on a lavatory cistern while at Edinburgh University. He invented Salter's duck in 1974 and attempted to make it the main device of choice for the Wave Energy program in the United Kingdom. A prototype attempt to use the device was constructed in 1976 off Dores Beach. It was to be used to \"provide some 20 kw of power\". It was modified slightly from the original design and Coventry University, which helped with the design, went on to utilize a separate type that was called the Sea Clam.\n\nHowever, because of the 1980s oil glut, the perceived need for immediate alternative energy sources declined and, in 1982, the Wave Energy program was disbanded. This ended the hope of having Salter's duck become a mainstay in the alternative energy campaign. After later investigation, it was discovered that the Energy Technology Support Unit's cost determinations had mis-estimated the cost of building Salter's duck by more than double the actual cost. The Energy Technology Support Unit was set up in 1974 as an agency on behalf of the Department of Energy; though its function was to manage research programmes on renewable energy and energy conservation, it was operated by the United Kingdom Atomic Energy Authority. Cost considerations based on the findings were among the main factors in the duck's not being put into widespread production under the Wave Energy program in the late 1970s. The other major factor was that a consulting firm tasked with distributing government grants passed over the 9.5 million pounds that had been allocated to Salter's research and the improvement of Salter's duck, so the funds were never actually granted to Salter and his group. From this revelation and with the increase in research into alternative energy in the 2000s, Salter's duck has begun to be used as a part of wave energy research in the United Kingdom.\n\nThe original prototype of Salter's duck was made of \"a string of floating vanes of rudimentary duck cross-sections linked through a central spine\". The string itself had 12 ducks attached to it that were \"50 cm wide mounted on a spine 27 cm in diameter and 6 m long.\" It was made at Coventry University, with materials from Ready Made Concrete and Insituform. The final design worked by having 20 to 30 ducks connected together by the jointed spine, with each duck moving with the waves that hit it and transferring the energy of the impact to \"six to ten pumps\" for each duck. The pear shape of the ducks have them facing the waves due to the decided orientation of their spine so that they rock and turn over when a wave hits them. This causes four gyroscopes inside to move back and forth, creating hydraulic energy that is transferred to a turbine or generator.\n\nIn order to determine the efficiency of energy output from Salter's duck, in 1975, scientist Swift-Hook and others ran a series of tests. The optimum range of the ducks was determined according to the formula,\n\nformula_1\n\nThe use of a lowercase r in this formula indicates the back radius of the ducks. They also had to test for the incidence energy (R) given off by a submerged surface (s), the formula of which is,\n\nformula_2\n\nIn this formula, the v stands for body velocity and the u for unperturbed fluid velocity perpendicular to the surface. With this, they were able to then use the final formula that tested for the absorption efficiency, eta (n),\n\nformula_3\n\nThe use of these three formulas allowed Swift-Hook to determine that Salter's duck is able to convert \"90% of the wave energy into mechanical energy\". However, this percentage was lower when the duck was tested in a laboratory. In varying types of realistic conditions, the efficiency of the duck varies wildly and often drops to around 50%, as ducks are more often used in rough weather in order to convert enough wave power. Conversely, ducks are not useful in calm weather, as the waves would not have enough energy for there to be any efficiency in converting it.\n"}
{"id": "22033639", "url": "https://en.wikipedia.org/wiki?curid=22033639", "title": "Self-propagating high-temperature synthesis", "text": "Self-propagating high-temperature synthesis\n\nSelf-propagating high-temperature synthesis (SHS) is a method for producing both inorganic and organic compounds by combustion-like exothermic reactions in solids of different nature. A variant of this method is known as solid state metathesis (SSM). If the reactants, intermediates, and products are all solids, it is known as a solid flame. Since the process occurs at high temperatures, the method is ideally suited for the production of refractory materials with unusual properties, for example: powders, metallic alloys, or ceramics with high purity, corrosion–resistance at high–temperature or super-hardnessity.\n\nThe modern SHS process was reported and patented in 1971, although some SHS-like processes were known previously.\n\nIn its usual format, SHS is conducted starting from finely powdered reactants that are intimately mixed. In some cases, the reagents are finely powdered whereas in other cases, they are sintered to minimize their surface area and prevent uninitiated exothermic reactions, which can be dangerous. In other cases, the particles are mechanically activated through techniques such as high energy ball milling (e.g. in a planetary mill), which results in nanocomposite particles that contain both reactants within individual chemical cells. After reactant preparation, synthesis is initiated by point-heating of a small part (usually the top) of the sample. Once started, a wave of exothermic reaction sweeps through the remaining material. SHS has also been conducted with thin films, liquids, gases, powder–liquid systems, gas suspensions, layered systems, gas-gas systems, and others. Reactions have been conducted in a vacuum and under both inert or reactive gases. The temperature of the reaction can be moderated by the addition of inert salt that absorbs heat in the process of melting or evaporation, such as sodium chloride, or by adding \"chemical oven\"—a highly exothermic mixture—to decrease the ratio of cooling\n\nThe reaction of alkali metal chalcogenides (S, Se, Te) and pnictides (N, P, As) with other metal halides produce the corresponding metal chalcogenides and pnictides. The synthesis of gallium nitride from gallium triiodide and lithium nitride is illustrative:\nThe process is so exothermic (ΔH = -515 kJ/mol) that the LiI evaporates, leaving a residue of GaN. With GaCl in place of GaI, the reaction is so exothermic that the product GaN decomposes. Thus, the selection of the metal halide affects the success of the method.\n\nOther compounds prepared by this method include metal dichalcogenides such as MoS. The reaction is conducted in a stainless steel reactor with excess NaS.\n\nSelf-propagating high-temperature synthesis can also be conducted at artificial high gravity environment to control the phase composition of products.\n\n"}
{"id": "46811356", "url": "https://en.wikipedia.org/wiki?curid=46811356", "title": "Solar Energy Research Center", "text": "Solar Energy Research Center\n\nThe Solar Energy Research Center (SERC) is a research center dedicated to identifying methods for converting solar energy to renewable fuel sources. SERC opened on 25 May 2015 at the Lawrence Berkeley National Laboratory (LBL) in Berkeley, California. SERC is housed at the newly opened Chu Hall, named for Steven Chu.\n\n"}
{"id": "68353", "url": "https://en.wikipedia.org/wiki?curid=68353", "title": "Solar furnace", "text": "Solar furnace\n\nA solar furnace is a structure that uses concentrated solar power to produce high temperatures, usually for industry. Parabolic mirrors or heliostats concentrate light (Insolation) onto a focal point. The temperature at the focal point may reach , and this heat can be used to generate electricity, melt steel, make hydrogen fuel or nanomaterials.\n\nThe largest solar furnace is at Odeillo in the Pyrénées-Orientales in France, opened in 1970. It employs an array of plane mirrors to gather sunlight, reflecting it onto a larger curved mirror.\n\nThe ancient Greek / Latin term \"heliocaminus\" literally means \"solar furnace\" and refers to a glass-enclosed sunroom intentionally designed to become hotter than the outside air temperature.\n\nDuring the Second Punic War (218 - 202 BC), the Greek scientist Archimedes is said to have repelled the attacking Roman ships by setting them on fire with a \"burning glass\" that may have been an array of mirrors. An experiment to test this theory was carried out by a group at the Massachusetts Institute of Technology in 2005. It concluded that although the theory was sound for stationary objects, the mirrors would not likely have been able to concentrate sufficient solar energy to set a ship on fire under battle conditions.\n\nThe first modern solar furnace is believed to have been built in France in 1949 by Professor Félix Trombe. It is now still in place at Mont Louis, near Odeillo. The Pyrenees were chosen as the site because the area experiences clear skies up to 300 days a year.\n\nAnother solar furnace was built in Uzbekistan as a part of a Soviet Union \"Sun\" Complex Research Facility impulsed by Academician S.A. Asimov.\n\nThe rays are focused onto an area the size of a cooking pot and can reach , depending on the process installed, for example:\n\n\nIt has been suggested that solar furnaces could be used in space to provide energy for manufacturing purposes.\n\nTheir reliance on sunny weather is a limiting factor as a source of renewable energy on Earth but could be tied to thermal energy storage systems for energy production through these periods and into the night.\n\nThe solar furnace principle is being used to make inexpensive solar cookers and solar-powered barbecues, and for solar water pasteurization. A prototype Scheffler reflector is being constructed in India for use in a solar crematorium. This 50 m² reflector will generate temperatures of and displace 200–300 kg of firewood used per cremation.\n\n\n"}
{"id": "777147", "url": "https://en.wikipedia.org/wiki?curid=777147", "title": "Solar inverter", "text": "Solar inverter\n\nA solar inverter or PV inverter, is a type of electrical converter which converts the variable direct current (DC) output of a photovoltaic (PV) solar panel into a utility frequency alternating current (AC) that can be fed into a commercial electrical grid or used by a local, off-grid electrical network. It is a critical balance of system (BOS)–component in a photovoltaic system, allowing the use of ordinary AC-powered equipment. Solar power inverters have special functions adapted for use with photovoltaic arrays, including maximum power point tracking and anti-islanding protection.\n\nSolar inverters may be classified into three broad types:\n\nSolar inverters use \"maximum power point tracking\" (MPPT) to get the maximum possible power from the PV array. Solar cells have a complex relationship between solar irradiation, temperature and total resistance that produces a non-linear output efficiency known as the \"I-V curve\". It is the purpose of the MPPT system to sample the output of the cells and determine a resistance (load) to obtain maximum power for any given environmental conditions.\n\nThe fill factor, more commonly known by its abbreviation \"FF\", is a parameter which, in conjunction with the open circuit voltage (V) and short circuit current (I) of the panel, determines the maximum power from a solar cell. Fill factor is defined as the ratio of the maximum power from the solar cell to the product of V and I.\n\nThere are three main types of MPPT algorithms: perturb-and-observe, incremental conductance and constant voltage. The first two methods are often referred to as \"hill climbing\" methods; they rely on the curve of power plotted against voltage rising to the left of the maximum power point, and falling on the right.\n\nSolar micro-inverter is an inverter designed to operate with a single PV module. The micro-inverter converts the direct current output from each panel into alternating current. Its design allows parallel connection of multiple, independent units in a modular way.\n\nMicro-inverter advantages include single panel power optimization, independent operation of each panel, plug-and play installation, improved installation and fire safety, minimized costs with system design and stock minimization.\n\nA 2011 study at Appalachian State University reports that individual integrated inverter setup yielded about 20% more power in unshaded conditions and 27% more power in shaded conditions compared to string connected setup using one inverter. Both setups used identical solar panels.\n\nSolar grid-tie inverters are designed to quickly disconnect from the grid if the utility grid goes down. This is an NEC requirement that ensures that in the event of a blackout, the grid tie inverter will shut down to prevent the energy it produces from harming any line workers who are sent to fix the power grid.\n\nGrid-tie inverters that are available on the market today use a number of different technologies. The inverters may use the newer high-frequency transformers, conventional low-frequency transformers, or no transformer. Instead of converting direct current directly to 120 or 240 volts AC, high-frequency transformers employ a computerized multi-step process that involves converting the power to high-frequency AC and then back to DC and then to the final AC output voltage.\n\nHistorically, there have been concerns about having transformerless electrical systems feed into the public utility grid. The concerns stem from the fact that there is a lack of galvanic isolation between the DC and AC circuits, which could allow the passage of dangerous DC faults to the AC side. Since 2005, the NFPA's NEC allows transformer-less (or non-galvanically) inverters. The VDE 0126-1-1 and IEC 6210 also have been amended to allow and define the safety mechanisms needed for such systems. Primarily, residual or ground current detection is used to detect possible fault conditions. Also isolation tests are performed to ensure DC to AC separation.\n\nMany solar inverters are designed to be connected to a utility grid, and will not operate when they do not detect the presence of the grid. They contain special circuitry to precisely match the voltage, frequency and phase of the grid.\n\nAdvanced solar pumping inverters convert DC voltage from the solar array into AC voltage to drive submersible pumps directly without the need for batteries or other energy storage devices. By utilizing MPPT (maximum power point tracking), solar pumping inverters regulate output frequency to control the speed of the pumps in order to save the pump motor from damage.\n\nSolar pumping inverters usually have multiple ports to allow the input of DC current generated by PV arrays, one port to allow the output of AC voltage, and a further port for input from a water-level sensor.\n\nAs of 2014, conversion efficiency for state-of-the-art solar converters reached more than 98 percent. While string inverters are used in residential to medium-sized commercial PV systems, central inverters cover the large commercial and utility-scale market. Market-share for central and string inverters are about 50 percent and 48 percent, respectively, leaving less than 2 percent to micro-inverters.\n\n"}
{"id": "308411", "url": "https://en.wikipedia.org/wiki?curid=308411", "title": "Tuned mass damper", "text": "Tuned mass damper\n\nA tuned mass damper (TMD), also known as a harmonic absorber or seismic damper, is a device mounted in structures to reduce the amplitude of mechanical vibrations. Their application can prevent discomfort, damage, or outright structural failure. They are frequently used in power transmission, automobiles, and buildings.\n\nTuned mass dampers stabilize against violent motion caused by harmonic vibration. A tuned damper reduces the vibration of a system with a comparatively lightweight component so that the worst-case vibrations are less intense. Roughly speaking, practical systems are tuned to either move the main mode away from a troubling excitation frequency, or to add damping to a resonance that is difficult or expensive to damp directly. An example of the latter is a crankshaft torsional damper. Mass dampers are frequently implemented with a frictional or hydraulic component that turns mechanical kinetic energy into heat, like an automotive shock absorber.\n\nGiven a motor with mass formula_1 attached via motor mounts to the ground, the motor vibrates as it operates and the soft motor mounts act as a parallel spring and damper, formula_2 and formula_3. The force on the motor mounts is formula_4. In order to reduce the maximum force on the motor mounts as the motor operates over a range of speeds, a smaller mass, formula_5, is connected to formula_1 by a spring and a damper, formula_7 and formula_8. formula_9 is the effective force on the motor due to its operation.\n\nThe graph shows the effect of a tuned mass damper on a simple spring–mass–damper system, excited by vibrations with an amplitude of one unit of force applied to the main mass, formula_1. An important measure of performance is the ratio of the force on the motor mounts to the force vibrating the motor, formula_11. This assumes that the system is linear, so if the force on the motor were to double, so would the force on the motor mounts. The blue line represents the baseline system, with a maximum response of 9 units of force at around 9 units of frequency. The red line shows the effect of adding a tuned mass of 10% of the baseline mass. It has a maximum response of 5.5, at a frequency of 7. As a side effect, it also has a second normal mode and will vibrate somewhat more than the baseline system at frequencies below about 6 and above about 10. \n\nThe heights of the two peaks can be adjusted by changing the stiffness of the spring in the tuned mass damper. Changing the damping also changes the height of the peaks, in a complex fashion. The split between the two peaks can be changed by altering the mass of the damper (formula_5).\nThe Bode plot is more complex, showing the phase and magnitude of the motion of each mass, for the two cases, relative to F1.\n\nIn the plots at right, the black line shows the baseline response (formula_13). Now considering formula_14, the blue line shows the motion of the damping mass and the red line shows the motion of the primary mass. The amplitude plot shows that at low frequencies, the damping mass resonates much more than the primary mass. The phase plot shows that at low frequencies, the two masses are in phase. As the frequency increases formula_5 moves out of phase with formula_1 until at around 9.5 Hz it is 180° out of phase with formula_1, maximizing the damping effect by maximizing the amplitude of formula_18, this maximizes the energy dissipated into formula_8 and simultaneously pulls on the primary mass in the same direction as the motor mounts.\n\nThe tuned mass damper was introduced as part of the suspension system by Renault, on its 2005 F1 car (the Renault R25), at the 2005 Brazilian Grand Prix. It was deemed to be legal at first, and it was in use up to the 2006 German Grand Prix.\n\nAt Hockenheim, the mass damper was deemed illegal by the FIA, because the mass was not rigidly attached to the chassis and, due to the influence it had on the pitch attitude of the car, which in turn significantly affected the gap under the car and hence the ground effects of the car, to be a movable aerodynamic device and hence as a consequence, to be illegally influencing the performance of the aerodynamics.\n\nThe Stewards of the meeting deemed it legal, but the FIA appealed against that decision. Two weeks later, the FIA International Court of Appeal deemed the mass damper illegal.\n\nTuned mass dampers are widely used in production cars, typically on the crankshaft pulley to control torsional vibration and, more rarely, the bending modes of the crankshaft. They are also used on the driveline for gearwhine, and elsewhere for other noises or vibrations on the exhaust, body, suspension or anywhere else. Almost all modern cars will have one mass damper, some may have 10 or more.\n\nThe usual design of damper on the crankshaft consists of a thin band of rubber between the hub of the pulley and the outer rim. This device, often called a harmonic damper, is located on the other end of the crankshaft opposite of where the flywheel and the transmission is. An alternative design is the centrifugal pendulum absorber which is used to reduce the internal combustion engine's torsional vibrations on a few modern cars.\n\nAll four wheels of the Citroen 2cv incorporated a tuned mass damper (referred to as a \"Batteur\" in the original French) of very similar design to that used in the Renault F1 car, from the start of production in 1949 on all four wheels, before being removed from the rear and eventually the front wheels in the mid 1970s.\n\nOne proposal to reduce vibration on NASA's Ares solid fuel booster was to use 16 tuned mass dampers as part of a design strategy to reduce peak loads from 6g to 0.25 g, the TMDs being responsible for the reduction from 1 g to 0.25 g, the rest being done by conventional vibration isolators between the upper stages and the booster.\n\nHigh-tension lines often have small barbell-shaped Stockbridge dampers hanging from the wires to reduce the high-frequency, low-amplitude oscillation termed flutter.\n\nA standard tuned mass damper for wind turbines consists of an auxiliary mass which is attached to the main structure by means of springs and dashpot elements. The natural frequency of the tuned mass damper is basically defined by its spring constant and the damping ratio determined by the dashpot. The tuned parameter of the tuned mass damper enables the auxiliary mass to oscillate with a phase shift with respect to the motion of the structure. In a typical configuration an auxiliary mass hung below the nacelle of a wind turbine supported by dampers or friction plates.\n\nTypically, the dampers are huge concrete blocks or steel bodies mounted in skyscrapers or other structures, and moved in opposition to the resonance frequency oscillations of the structure by means of springs, fluid or pendulums.\n\nUnwanted vibration may be caused by environmental forces acting on a structure, such as wind or earthquake, or by a seemingly innocuous vibration source causing resonance that may be destructive, unpleasant or simply inconvenient.\n\nThe seismic waves caused by an earthquake will make buildings sway and oscillate in various ways depending on the frequency and direction of ground motion, and the height and construction of the building. Seismic activity can cause excessive oscillations of the building which may lead to structural failure. To enhance the building's seismic performance, a proper building design is performed engaging various seismic vibration control technologies.\nAs mentioned above, damping devices had been used in the aeronautics and automobile industries long before they were standard in mitigating seismic damage to buildings. In fact, the first specialized damping devices for earthquakes were not developed until late in 1950.\n\nMasses of people walking up and down stairs at once, or great numbers of people stomping in unison, can cause serious problems in large structures like stadiums if those structures lack damping measures.\n\nThe force of wind against tall buildings can cause the top of skyscrapers to move more than a meter. This motion can be in the form of swaying or twisting, and can cause the upper floors of such buildings to move. Certain angles of wind and aerodynamic properties of a building can accentuate the movement and cause motion sickness in people. A TMD is usually tuned to a certain building's frequency to work efficiently. However, during their lifetimes, high-rise and slender buildings may experience natural frequency changes under wind speed, ambient temperatures and relative humidity variations, among other factors, which requires a robust TMD design.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "18502508", "url": "https://en.wikipedia.org/wiki?curid=18502508", "title": "Y and Z Holes", "text": "Y and Z Holes\n\nThe Y and Z Holes are two rings of concentric (though irregular) circuits of 30 and 29 near identical pits cut around the outside of the Sarsen Circle at Stonehenge. The current view is that both circuits are contemporary. Radiocarbon dating of antlers deliberately placed in hole Y 30 provided a date of around 1600 BCE, a slightly earlier date was determined for material retrieved from Z 29. These dates make the Y and Z holes the last known structural activity at Stonehenge.\n\nThe holes were discovered in 1923 by William Hawley, who, on removing the topsoil over a wide area noted them as clearly visible patches of ‘humus’ against the chalk substrate. Hawley named them the ‘Y’ and ‘Z’ because for a short time he had earlier labeled the recently discovered Aubrey Holes the ‘X’ holes. 18 of the Y Holes have been excavated and 16 of the Z Holes. Further evidence of the Y and Z Holes being late in the sequence of events at Stonehenge is demonstrated by the fact that hole Z 7 was found to cut into the backfill of the construction ramp for stone 7 of the Sarsen Circle.\n\nThe outer Y ring consists of 30 holes averaging 1.7 m × 1.14 m tapering to a flat base typically close to 1 m × 0.5 m, the inner Z holes, of which only 29 holes are known (the missing hole Z 8 may lie beneath the fallen Sarsen stone 8), are slightly larger, on average by some 0.1 m. They can be best described morphologically as ‘wedge-shaped’. The diameter of the Y Hole circuit, i.e. the best-fit circle is some 54 m, that of the Z Hole series, around 39 m. \n\nThe fills of the holes was found to be largely stone-free, these deposits are thought to be the result of the gradual accumulation of wind-blown material. Examples of almost every material, both natural and artefactual, that have been found elsewhere at Stonehenge have been retrieved from their fills; this includes pottery of later periods (Iron Age, Romano-British, and Medieval) also coins, horseshoe nails, and even human remains.\n\nA new landscape investigation of the Stonehenge site was conducted in April 2009 and a shallow bank, little more than high, was identified between the two hole-circles. A further bank lies inside the Z Hole circle. These are interpreted as the spread of spoil from the original holes, or more speculatively as hedge banks from vegetation deliberately planted to screen the activities within.\n\nNeither Hawley, nor Richard Atkinson who investigated two of the holes (Y 16 and Z 16) in 1953 thought that there had ever been uprights of timber or stone in the holes. Atkinson however suggested that they had been intended to house bluestones, the question remains unresolved. Although unique in many ways, a similarity of form between these holes and the contemporary grave pits under the Bronze Age Barrow mounds has been pointed out. Attempts at interpreting the methods of construction used in building the stone monument sometimes show the Y and Z Holes used to locate temporary scaffold–like timber structures or ‘A’ frames. The fact that the stonework has been shown to be around 700 or 800 years earlier than the Y and Z Holes clearly precludes the possibility that the holes represent features cut for constructional purposes. For the same reason the Y and Z Holes cannot be logically introduced into any scheme that suggests they performed a structural function within the design of the stone monument.\n\nSome interpretations introduce the idea that the holes were deliberately laid out in a ‘spiral’ pattern. However their irregular pattern still retains an integrity that can be explained as reciprocal errors created by prehistoric surveyors using a cord (equal to the radius of each circuit) passed around the stone monument \n(the presence of the stones would have prevented an accurate circle from being scribed from the geometric centre of the site). The distances between the two circuits appears to have been established by the geometry of simple square and circle relationships (i.e. the Z Hole circuit is contained within a square inscribed within the Y Hole circuit).\n"}
