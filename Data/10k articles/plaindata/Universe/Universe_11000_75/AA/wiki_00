{"id": "32445400", "url": "https://en.wikipedia.org/wiki?curid=32445400", "title": "2011 Bohai Bay oil spill", "text": "2011 Bohai Bay oil spill\n\nThe 2011 Bohai bay oil spill () was a series of oil spills that began on June 4, 2011 at Bohai Bay. The spill itself however was not publicly disclosed until a month later. There were suspicions of official cover-ups by the State Oceanic Administration (SOA).\n\nThe oil field is 51% owned by China National Offshore Oil Corporation, and 49% owned by the United States company ConocoPhillips.\n\nOn June 4, 2011 the Penglai 19-3 oilfield caused an oil spill from a sea floor leak that lasted until June 7.\n\nOn June 17 a second oil spill that occurred at the Penglai 19-3 oilfield, but was contained within 48 hours. By the second leak, it was reported that a total of 840 square kilometers of first grade clean water in Bohai Bay was polluted.\n\nA third leak took place on July 12 with the Suizhong 36-1 oil field. This occurred just one day after the Huizhou refinery explosion incident. In total the leaks contaminated a total of 4,250 square kilometers. The media has described the spill to be six times the size of Singapore.\n\nThe oil spill was not publicly reported until 31 days later on July 5, 2011. It was only revealed because of a public microblog tip-off that appeared on June 21. The news of the oil spill was withheld by the State Oceanic Administration (SOA) for a month. In a statement by the SOA, the US company ConocoPhillips managing the platform was held responsible for the leak, and was fined 200,000 yuan (US$31,000). CNOOC however, said they informed government authorities from the start.\n\nOutside of the spill area, dead seaweed and rotting fish can be seen around Nanhuangcheng island (南隍城島) in Shandong province.\n\n\"The oil, containing toxic substances and heavy metals, will greatly affect the growth of marine lives that live on the seabed, such as clams, scallops and some kinds of crabs,\" Xinhua reported last week quoting Cui Wenlin, director of the environmental monitoring centre with the North China Sea branch of the SOA.\n\nBohai is a half-closed sea with comparatively low self-clean ability due to limited water exchange with the outside, he added.\n\nThe environmental monitoring centre Cui directs has been monitoring the impacts of the oil spills on the Bohai's water quality, seabed sediments and marine lives.\n\nThough the US firm claims that no oil sheen reached the shoreline after the spills, Xinhua reports that \"dead seaweed and rotting fish have been reported in the water around Nanhuangcheng Island, about 74 kilometres south from where the leaks originated\".\n\nFurther criticism followed that if the spilled oil were to flow into the Yellow Sea, this will damage both North Korea and South Korea. Korean media have complained about Beijing being as irresponsible as the Japanese's reluctant to share information about its nuclear disasters. ConocoPhillips said the spill was the equivalent of 1,500 barrels of oil.\n\nBut many Chinese environmental organisations questioned the credibility of the spill volume released by ConocoPhillips.\n\nZhong Yu, senior action coordinator of Greenpeace, an international environmental organisation, told Xinhua that “the amount is questionable” because, apart from ConocoPhillips and the SOA, ‘no third party attended the assessment’.\n\nIn addition, 11 environmental organisations sent an open letter to ConocoPhillips China and CNOOC Ltd on Thursday, asking the two companies to assist the organisations and other people concerned to visit the scene of the leak to investigate the incident and its aftermath.\n\nThe CNOOC has recently come under scrutiny for several accidents involving its facilities, the third such incident on the Bohai Sea in less than two months. Following an oil spill on Tuesday in its Suizhong 36-1 oilfield, it was to be shut temporarily, SOA announced in a statement. By Wednesday afternoon, CNOOC finished cleaning up an oil slick near the oilfield and gradually resumed production.\n"}
{"id": "6457048", "url": "https://en.wikipedia.org/wiki?curid=6457048", "title": "ArDM", "text": "ArDM\n\nThe ArDM (Argon Dark Matter) Experiment is a particle physics experiment based on a liquid argon detector, aiming at measuring signals from WIMPs (Weakly Interacting Massive Particles), which probably constitute the Dark Matter in the universe. Elastic scattering of WIMPs from argon nuclei is measurable by observing free electrons from ionization and photons from scintillation, which are produced by the recoiling nucleus interacting with neighbouring atoms. The ionization and scintillation signals can be measured with dedicated readout techniques, which constitute a fundamental part of the detector.\n\nIn order to get a high enough target mass the noble gas argon is used in the liquid phase as target material. Since the boiling point of argon is at 87 K at normal pressure, the operation of the detector requires a cryogenic system.\n\nThe ArDM detector aims at directly detecting signals from WIMPs via elastic scattering from argon nuclei. During the scattering, a certain recoil energy - typically lying between 1 keV and 100 keV - is transferred from the WIMP to the argon nucleus.\n\nIt is not known how frequently a signal from WIMP-argon interaction can be expected. This rate depends on the underlying model describing the properties of the WIMP. One of the most popular candidates for a WIMP is the Lightest Supersymmetric Particle (LSP) or neutralino from supersymmetric theories. Its cross section with nucleons presumably lies between 10 pb and 10 pb, making WIMP-nucleon interactions a rare event. The total event rate can be increased by optimizing the target properties, such as increasing the target mass. The ArDM detector is planned to contain approximately one ton of liquid argon. This target mass corresponds to an event rate of approximately 100 events per day at a cross section of 10 pb or 0.01 events per day at 10 pb.\n\nSmall event rates require a powerful background rejection. An important background comes from the presence of the unstable Ar isotope in natural argon liquefied from the atmosphere. Ar undergoes beta decay with a halflife of 269 years and an endpoint of the beta spectrum at 565 keV. The ratio of ionization over scintillation from electron and gamma interactions is different than WIMP scattering produces. The Ar background is therefore well distinguishable, with a precise determination of the ionization/scintillation ratio. As an alternative, the use of depleted argon from underground wells is being considered.\n\nNeutrons emitted by detector components and by materials surrounding the detector interact with argon in the same way as WIMPs. The neutron background is therefore nearly indistinguishable and has to be reduced as well as possible, as for example by carefully choosing the detector materials. Furthermore, an estimation or measurement of the remaining neutron flux is necessary.\n\nThe detector is planned to be run underground in order to avoid backgrounds induced by cosmic rays.\n\nThe ArDM detector was assembled and tested at CERN in 2006. Above ground studies of the equipment and detector performance were performed before it was moved underground in 2012 in the Canfranc Underground Laboratory in Spain. It was filled with was commissioned and tested at room temperature. During the April 2013 run underground, the light yield was improved compared to surface conditions.\n\nFuture cold argon gas runs are planned as well as continued detector development. Liquid argon results are planned for 2014.\n\nBeyond the one-ton version, the detector size can be increased without fundamentally changing its technology. A ten-ton liquid argon detector is a thinkable expansion possibility for ArDM. Current experiments for Dark Matter detection at a mass scale of 1 kg to 100 kg with negative results demonstrate the necessity of ton-scale experiments.\n\nDespite studying inherently 'dark' matter, the future seems bright for dark matter detector development. The \"Dark Side Program\" is a consortium that has conducted and continues to develop new experiments based on condensed atmospheric argon (LAr), instead of xenon, liquid. One recent Dark Side apparatus, the Dark Side-50 (DS-50), employs a method known as \"two-phase liquid argon time projection chambers (LAr TPCs),\" which allows for three-dimensional determination of collision event positions created by the electrolumnescence created by argon collisions with dark matter particles. The Dark Side program released its first results on its findings in 2015, so far being the most sensitive results for argon-based dark matter detection. LAr-based methods used for future apparatuses present an alternative to xenon-based detectors and could potentially lead to new, more sensitive multi-ton detectors in the near future.\n\n"}
{"id": "267909", "url": "https://en.wikipedia.org/wiki?curid=267909", "title": "Arachidonic acid", "text": "Arachidonic acid\n\nArachidonic acid (AA, sometimes ARA) is a polyunsaturated omega-6 fatty acid 20:4(ω-6), or 20:4(5,8,11,14). It is structurally related to the saturated arachidic acid found in cupuaçu butter (\"L. arachis\" – peanut).\n\nIn chemical structure, arachidonic acid is a carboxylic acid with a 20-carbon chain and four \"cis\"-double bonds; the first double bond is located at the sixth carbon from the omega end.\n\nSome chemistry sources define 'arachidonic acid' to designate any of the eicosatetraenoic acids. However, almost all writings in biology, medicine, and nutrition limit the term to all \"cis\"-5,8,11,14-eicosatetraenoic acid.\n\nArachidonic acid is a polyunsaturated fatty acid present in the phospholipids (especially phosphatidylethanolamine, phosphatidylcholine, and phosphatidylinositides) of membranes of the body's cells, and is abundant in the brain, muscles, and liver. Skeletal muscle is an especially active site of arachidonic acid retention, accounting for roughly 10-20% of the phospholipid fatty acid content typically.\n\nIn addition to being involved in cellular signaling as a lipid second messenger involved in the regulation of signaling enzymes, such as PLC-γ, PLC-δ, and PKC-α, -β, and -γ isoforms, arachidonic acid is a key inflammatory intermediate and can also act as a vasodilator. (Note separate synthetic pathways, as described in section below.)\n\nArachidonic acid is not one of the essential fatty acids. However, it does become essential if a deficiency in linoleic acid exists or if there an inability to convert linoleic acid to arachidonic acid occurs. \nSome mammals lack the ability or have a very limited capacity to convert linoleic acid to arachidonic acid, making it an essential part of their diets. Since little or no arachidonic acid is found in common plants, such animals are obligate carnivores; the cat is a common example having inability to desaturate essential fatty acids. A commercial source of arachidonic acid has been derived, however, from the fungus \"Mortierella alpina\".\n\nArachidonic acid is freed from a phospholipid molecule by the enzyme phospholipase A2 (PLA), which cleaves off the fatty acid, but can also be generated from DAG by diacylglycerol lipase.\n\nArachidonic acid generated for signaling purposes appears to be derived by the action of group IVA cytosolic phospholipase A2 (cPLA, 85 kDa), whereas inflammatory arachidonic acid is generated by the action of a low-molecular-weight secretory PLA (sPLA, 14-18 kDa).\n\nArachidonic acid is the precursor that is metabolized by various enzymes to a wide range of biologically and clinically important eicosanoids and metabolites of these eicosanoids:\n\nThe production of these derivatives and their actions in the body are collectively known as the \"arachidonic acid cascade\"; see essential fatty acid interactions and the enzyme and metabolite linkages given in the previous paragraph for more details.\n\nPLA, in turn, is activated by ligand binding to receptors, including:\n\nFurthermore, any agent increasing intracellular calcium may cause activation of some forms of PLA.\n\nAlternatively, arachidonic acid may be cleaved from phospholipids after phospholipase C (PLC) cleaves off the inositol trisphosphate group, yielding diacylglycerol (DAG), which subsequently is cleaved by DAG lipase to yield arachidonic acid.\n\nReceptors that activate this pathway include:\n\nPLC may also be activated by MAP kinase. Activators of this pathway include PDGF and FGF.\n\nArachidonic acid promotes the repair and growth of skeletal muscle tissue via conversion to prostaglandin PGF2alpha during and following physical exercise. PGF2alpha promotes muscle protein synthesis by signaling through the Akt/mTOR pathway, similar to leucine, β-hydroxy β-methylbutyric acid, and phosphatidic acid.\n\nArachidonic acid is one of the most abundant fatty acids in the brain, and is present in similar quantities to docosahexaenoic acid (DHA). The two account for about 20% of its fatty-acid content. Like DHA, neurological health is reliant upon sufficient levels of arachidonic acid. Among other things, arachidonic acid helps to maintain hippocampal cell membrane fluidity. It also helps protect the brain from oxidative stress by activating peroxisome proliferator-activated receptor gamma. ARA also activates syntaxin-3 (STX-3), a protein involved in the growth and repair of neurons.\n\nArachidonic acid is also involved in early neurological development. In one study, infants (18 months) given supplemental arachidonic acid for 17 weeks demonstrated significant improvements in intelligence, as measured by the Mental Development Index. This effect is further enhanced by the simultaneous supplementation of ARA with DHA.\n\nIn adults, the disturbed metabolism of ARA may contribute to neuropsychiatric disorders such as Alzheimer's disease and bipolar disorder. There is evidence of significant alterations in the conversion of arachidonic acid to other bioactive molecules (overexpression or disturbances in the ARA enzyme cascade) in these conditions.\n\nStudies on arachidonic acid and the pathogenesis of Alzheimer's disease is mixed, with one study of AA and its metabolites that suggests they are associated with the onset of Alzheimer's disease, whereas another study suggests that the supplementation of arachidonic acid during the early stages of this disease may be effective in reducing symptoms and slowing the disease progress. Additional studies on arachidonic acid supplementation for Alzheimer's patients are needed. Another study indicates that air pollution is the source of inflammation and arachidonic acid metabolites promote the inflammation to signal the immune system of the cell damage.\n\nArachidonic acid is marketed as an anabolic bodybuilding supplement in a variety of products. Supplementation of arachidonic acid (1,500 mg/day for 8 weeks) has been shown to increase lean body mass, strength, and anaerobic power in experienced resistance-trained men. This was demonstrated in a placebo-controlled study at the University of Tampa. Thirty men (aged 20.4 ± 2.1 years) took arachidonic acid or a placebo for 8 weeks, and participated in a controlled resistance-training program. After 8 weeks, lean body mass (LBM) had increased significantly, and to a greater extent, in the ARA group (1.62 kg) vs. placebo (0.09 kg) (p<0.05). The change in muscle thickness was also greater in the ARA group (.47 cm) than placebo (.25 cm) (p<0.05). Wingate anaerobic power increased to a greater extent in ARA group as well (723.01 to 800.66 W) vs. placebo (738.75 to 766.51 W). Lastly, the change in total strength was significantly greater in the ARA group (109.92 lbs.) compared to placebo (75.78 lbs.). These results suggest that ARA supplementation can positively augment adaptations in strength and skeletal muscle hypertrophy in resistance-trained men.\n\nAn earlier clinical study examining the effects of 1,000 mg/day of arachidonic acid for 50 days found supplementation to enhance anaerobic capacity and performance in exercising men. During this study, a significant group–time interaction effect was observed in Wingate relative peak power (AA: 1.2 ± 0.5; P: -0.2 ± 0.2 W•kg-1, p=0.015). Statistical trends were also seen in bench press 1RM (AA: 11.0 ± 6.2; P: 8.0 ± 8.0 kg, p=0.20), Wingate average power (AA:37.9 ± 10.0; P: 17.0 ± 24.0 W, p=0.16), and Wingate total work (AA: 1292 ± 1206; P: 510 ± 1249 J, p=0.087). AA supplementation during resistance training promoted significant increases in relative peak power with other performance-related variables approaching significance. These findings support the use of AA as an ergogenic.\n\nIncreased consumption of arachidonic acid will not cause inflammation during normal metabolic conditions unless lipid peroxidation products are mixed in. Arachidonic acid is metabolized to both proinflammatory and anti-inflammatory eicosanoids during and after the inflammatory response, respectively. Arachidonic acid is also metabolized to inflammatory and anti-inflammatory eicosanoids during and after physical activity to promote growth. However, chronic inflammation from exogenous toxins and excessive exercise should not be confused with acute inflammation from exercise and sufficient rest that is required by the inflammatory response to promote the repair and growth of the micro tears of tissues. However, the evidence is mixed. Some studies giving between 840 mg and 2,000 mg per day to healthy individuals for up to 50 days have shown no increases in inflammation or related metabolic activities. However, others show that increased arachidonic acid levels are actually associated with reduced pro-inflammatory IL-6 and IL-1 levels and increased anti-inflammatory tumor necrosis factor-beta. This may result in a reduction in systemic inflammation.\n\nArachidonic acid does still play a central role in inflammation related to injury and many diseased states. How it is metabolized in the body dictates its inflammatory or anti-inflammatory activity. Individuals suffering from joint pains or active inflammatory disease may find that increased arachidonic acid consumption exacerbates symptoms, presumably because it is being more readily converted to inflammatory compounds. Likewise, high arachidonic acid consumption is not advised for individuals with a history of inflammatory disease, or who are in compromised health. Of note, while ARA supplementation does not appear to have proinflammatory effects in healthy individuals, it may counter the anti-inflammatory effects of omega-3 fatty acid supplementation.\n\nArachidonic acid supplementation in daily doses of 1,000–1,500 mg for 50 days has been well tolerated during several clinical studies, with no significant side effects reported. All common markers of health, including kidney and liver function, serum lipids, immunity, and platelet aggregation appear to be unaffected with this level and duration of use. Furthermore, higher concentrations of ARA in muscle tissue may be correlated with improved insulin sensitivity. Arachidonic acid supplementation of the diets of healthy adults appears to offer no toxicity or significant safety risk.\n\nWhile studies looking at arachidonic acid supplementation in sedentary subjects have failed to find changes in resting inflammatory markers in doses up to 1,500 mg daily, strength-trained subjects may respond differently. One study reported a significant reduction in resting inflammation (via marker IL-6) in young men supplementing 1,000 mg/day of arachidonic acid for 50 days in combination with resistance training. This suggests that rather being pro-inflammatory, supplementation of ARA while undergoing resistance training may actually improve the regulation of systemic inflammation.\n\nA meta-analysis looking for associations between heart disease risk and individual fatty acids reported a significantly reduced risk of heart disease with higher levels of EPA and DHA (omega-3 fats), as well as the omega-6 arachidonic acid. A scientific advisory from the American Heart Association has also favorably evaluated the health impact of dietary omega-6 fats, including arachidonic acid. The group does not recommend limiting this essential fatty acid. In fact, the paper recommends individuals follow a diet that consists of at least 5–10% of calories coming from omega-6 fats, including arachidonic acid. It suggests dietary ARA is not a risk factor for heart disease, and may play a role in maintaining optimal metabolism and reduced heart disease risk. Maintaining sufficient intake levels of both omega-3 and omega-6 fatty acids, therefore, is recommended for optimal health.\n\nArachidonic acid is not carcinogenic, and studies show dietary level is not associated (positively or negatively) with risk of cancers. ARA remains integral to the inflammatory and cell growth process, however, which is disturbed in many types of disease including cancer. Therefore, the safety of arachidonic acid supplementation in patients suffering from cancer, inflammatory, or other diseased states is unknown, and supplementation is not recommended.\n\n\n"}
{"id": "22337270", "url": "https://en.wikipedia.org/wiki?curid=22337270", "title": "BTM Consult", "text": "BTM Consult\n\nBTM Consult is an independent consultancy company specializing in services pertaining to renewable energy commercialization. The company was founded 1989 by Per Krogsgaard and Birger Madsen. The Staff at BTM Consult have been working with wind power utilization since 1979.\n\nIn March 2010, BTM Consult released its 15th annual update report on the international wind power industry. This report showed that 2009 recorded the highest ever level of wind turbine installations.\n\nBTM Consult reports are widely cited in the media and other publications.\nIn 2011, BTM Consults was acquired by Navigant \nIn 2000, when wind power contributed just 0.25% of electricity, BTM forecasted that wind would contribute 1.78% in 2010, which was then regarded as overly optimistic. As of 2010, a decade later, wind power indeed contributes around 1.6%.\n\n"}
{"id": "51283178", "url": "https://en.wikipedia.org/wiki?curid=51283178", "title": "Castletimon Ogham Stone", "text": "Castletimon Ogham Stone\n\nCastletimon Ogham Stone (CIIC 047) is a ogham stone and National Monument located near Brittas Bay, County Wicklow, Ireland.\n\nCastletimon Ogham Stone lies prone by the roadside west of Ballynacarrig beach, which opens onto Brittas Bay. Potter's River flows to the south.\n\nCastletimon Ogham Stone was carved c. AD 350–550, and was rediscovered in 1854.\n\nLocal legend claims that the Ogham stone was once picked up by the Castletimon Giant and thrown down the hill; the scratches on it were left by his finger nails. Another says that a local man took the Ogham stone to use as a hob stone. The Aos Sí (fairies) got angry and made his cutlery dance and jiggle. After a week of this he returned the stone to its place.\n\nCastletimon Ogham Stone measures 150 × 48 × 20 cm and has Ogham carvings incised on one edge. (, perhaps \"Netacari, nephew of Cagi\"). Variant readings include , or .\n"}
{"id": "11989180", "url": "https://en.wikipedia.org/wiki?curid=11989180", "title": "Cellulosic ethanol commercialization", "text": "Cellulosic ethanol commercialization\n\nCellulosic ethanol commercialization is the process of building an industry out of methods of turning cellulose-containing organic matter into cellulosic ethanol for use as a biofuel. Companies, such as Iogen, POET, DuPont, and Abengoa, are building refineries that can process biomass and turn it into bioethanol. Companies, such as Diversa, Novozymes, and Dyadic, are producing enzymes that could enable a cellulosic ethanol future. The shift from food crop feedstocks to waste residues and native grasses offers significant opportunities for a range of players, from farmers to biotechnology firms, and from project developers to investors.\n\nAs of 2013, the first commercial-scale plants to produce cellulosic biofuels have begun operating. Multiple pathways for the conversion of different biofuel feedstocks are being used. In the next few years, the cost data of these technologies operating at commercial scale, and their relative performance, will become available. Lessons learnt will lower the costs of the industrial processes involved.\n\nCellulosic ethanol can be produced from a diverse array of feedstocks, such as wood pulp from trees or any plant matter. Instead of taking the grain from wheat and grinding that down to get starch and gluten, then taking the starch, cellulosic ethanol production involves the use of the whole crop. This approach should increase yields and reduce the carbon footprint because the amount of energy-intensive fertilisers and fungicides will remain the same, for a higher output of usable material.\n\nEthtec is building a pilot plant in Harwood, New South Wales, which uses wood residues as a feedstock.\n\nGranBio (formerly known as GraalBio) is building a facility projected to produce 82 million litres of cellulosic ethanol per year.\n\nIn Canada, Iogen Corp. is a developer of cellulosic ethanol process technology. Iogen has developed a proprietary process and operates a demonstration-scale plant in Ontario. The facility has been designed and engineered to process 40 tons of wheat straw per day into ethanol using enzymes made in an adjacent enzyme manufacturing facility. In 2004, Iogen began delivering its first shipments of cellulosic ethanol into the marketplace. In the near term, the company intends to commercialize its cellulose ethanol process by licensing its technology broadly through turnkey plant construction partnerships. The company is currently evaluating sites in the United States and Canada for its first commercial-scale plant.\n\nLignol Innovations has a pilot plant, which uses wood as a feedstock, in Vancouver.\n\nIn March 2009, KL Energy Corporation of South Dakota and Prairie Green Renewable Energy of Alberta announced their intention to develop a cellulosic ethanol plant near Hudson Bay, Saskatchewan. The Northeast Saskatchewan Renewable Energy Facility will use KL Energy’s modern design and engineering to produce ethanol from wood waste.\n\nCellulosic ethanol production currently exists at \"pilot\" and \"commercial demonstration\" scale, including a plant in China engineered by SunOpta Inc. and owned and operated by China Resources Alcohol Corporation that is currently producing cellulosic ethanol from corn stover (stalks and leaves) on a continuous, 24-hour-per-day basis.\n\nInbicon's bioethanol plant in Kalundborg, with the capacity to produce 5.4 million liters (1.4 million gallons) annually, was opened in 2009. Believed to be the world's largest cellulosic ethanol plant as of early 2011, the facility runs on about 30,000 metric tons (33,000 tons) of straw per year and the plant employs about 30 people. The plant also produces 13,000 metric tons of lignin pellets per year, used as fuel at combined-heat-and-power plants, and 11,100 metric tons of C5 molasses which is currently used for biomethane production via anaerobic digestion, and has been tested as a high carbohydrate animal feed supplement and potential bio-based feedstock for production of numerous commodity chemicals including diols, glycols, organic acids, and biopolymer precursors and intermediates.\n\nSince October 2010, an E5 blend of 95% gasoline and 5% cellulosic ethanol blend has been available at 100 filling stations across Denmark. Distributed by Statoil, the Bio95 2G mixture uses ethanol derived from wheat straw collected on Danish fields after harvest and produced by Inbicon (a div. of DONG Energy), using enzyme technology from Novozymes.\n\nThe biofuel company Butalco has recently signed a research and development contract with Hohenheim University. The Institute of Fermentation Technology within the Department of Food Science and Biotechnology at Hohenheim University has been concerned with questions on the production of bioethanol for almost 30 years. The focus in recent years has been on the improvement of the material, energy and life cycle assessment of the production of ethanol. Special interest to BUTALCO is the use of the newly built pilot plant, which is equipped with a safety class 1 approved fermentation room with 4 x 1.5 m³ fermenters. The concept of the plant allows both starch and lignocellulosic based raw materials to be processed. The collaboration will allow BUTALCO to optimise its C5 sugar fermenting and butanol producing yeast strains on a technical scale and produce first amounts of bioethanol from lignocellulose. The whole process of the production of biofuel from the choice of cellulosic biomass feedstock to the conversion into sugars and fermentation through to the purification will be optimised under industrial conditions.\n\nIn Straubing, the specialty chemicals company Clariant has been operating a precommercial plant based on its sunliquid process since 2012. The plant is able to produce up to 1000 tons of cellulosic ethanol from agricultural residues such as wheat straw, corn stover or sugarcane bagasse. The process technology uses enzymatic hydrolysis, followed by fermentation of C5 and C6 sugar into ethanol. The company plans to licence the technology worldwide.\n\nCellulosic ethanol production currently exists at \"pilot\" scale, with efforts being made on utilization of waste lignocellulosic biomass for ethanol production. Pilot scale studies for utilization of pine needles and Lantana weed undertaken at Cellulose and Paper Division, Forest Research Institute, Dehradun, India.\n\nItaly-based Mossi & Ghisolfi Group broke ground for its per year cellulosic ethanol facility in Crescentino in northwestern Italy on April 12, 2011. The project will be the largest cellulosic ethanol project in the world, 10 times larger than any of the currently operating demonstration-scale facilities. The plant is \"expected to become operational in 2012 and will use a variety of locally sourced feedstocks, beginning with wheat straw and Arundo donax, a perennial giant cane\".\n\nNippon Oil Corporation and other Japanese manufacturers including Toyota Motor Corporation plan to set up a research body to develop cellulose-derived biofuels. The consortium plans to produce 250,000 kilolitres (1.6 million barrels) per year of bioethanol by March 2014, and produce bioethanol at 40 yen ($0.437) per litre (about $70 a barrel) by 2015.\n\nIn March 2009, Honda Motor announced an agreement for the construction of a new cellulosic ethanol research facility in Japan. The new Kazusa-branch facility of the Honda Fundamental Technology Research Center will be built within the Kazusa Akademia Park, in Kisarazu, Chiba. Construction is scheduled to begin in April 2009, with the aim to begin operations in November 2009.\n\nIn October 2010, Norway-based cellulosic ethanol technology developer Weyland commenced production at its 200,000 liter (approximately 53,000 gallon) pilot-scale facility in Bergen, Norway. The plant will demonstrate the company’s acid hydrolysis production process, paving the way for a commercial-scale project. The company also plans to market its technology worldwide.\n\nA commercial factory converting wood (50% softwood + 50% hardwood) into Ethanol is in operation in Northern Russia, the city of Kirov, since 1972 and is still profitable. As side products the company, Kirov Biochemical Works, is offering dry fodder yeast (20 tons/month) and Lignin. To install equipment for drying and burning Lignin, both fresh and accumulated in the landfill, for steam and electricity, a bank loan of $200 million was recently secured.\n\nAbengoa continues to invest heavily in the necessary technology for bringing cellulosic ethanol to market. Utilizing process and pre-treatment technology from SunOpta Inc., Abengoa is building a cellulosic ethanol facility in Spain and have recently entered into a strategic research and development agreement with Dyadic International, Inc. (AMEX: DIL), to create new and better enzyme mixtures which may be used to improve both the efficiencies and cost structure of producing cellulosic ethanol.\n\nSEKAB has developed an industrial process for production of ethanol from biomass feed-stocks, including wood chips and sugar cane bagasse. The development work is being carried out at an advanced pilot plant in Örnsköldsvik, and has sparked international interest. The technology will be gradually scaled up to commercial production in a new breed of bio-refineries from 2013 to 2015.\n\nThe US government actively supports the development and commercialization of cellulosic ethanol through a variety of mechanisms. In the first decade of the 21st century, a lot of companies announced plans to build commercial cellulosic ethanol plants, but most of those plans eventually fell apart, and many of the small companies went bankrupt. Currently (2016), here are many demonstration plants throughout the country, and handful of commercial-scale plants which are in operation or close to it. With the market for cellulosic ethanol in the United States projected to continue growing in the coming years, the outlook for this industry is good.\n\nThe US Federal government is actively promoting the development of ethanol from cellulosic feedstocks as an alternative to conventional petroleum transportation fuels. For example, programs sponsored by U.S. Department of Energy (DOE) include research to develop better cellulose hydrolysis enzymes and ethanol-fermenting organisms, to engineering studies of potential processes, to co-funding initial ethanol from cellulosic biomass demonstration and production facilities. This research is conducted by various national laboratories, including the National Renewable Energy Laboratory (NREL), Oak Ridge National Laboratory (ORNL) and Idaho National Laboratory (INL), as well as by universities and private industry. Engineering and construction companies and operating companies are generally conducting the engineering work.\n\nIn May 2008, Congress passed a new farm bill that will accelerate the commercialization of advanced biofuels, including cellulosic ethanol. The \"Food, Conservation, and Energy Act of 2008\" provides for grants covering up to 30% of the cost of developing and building demonstration-scale biorefineries for producing \"advanced biofuels,\" which essentially includes all fuels that are not produced from corn kernel starch. It also allows for loan guarantees of up to $250 million for building commercial-scale biorefineries to produce advanced biofuels.\n\nUsing a newly developed tool known as the \"Biofuels Deployment Model\", Sandia researchers have determined that of cellulosic ethanol could be produced per year by 2022 without displacing current crops. The Renewable Fuels Standard, part of the 2007 Energy Independence and Security Act, calls for an increase in biofuels production to a year by 2022.\n\nIn January 2011, the USDA approved $405 million in loan guarantees through the 2008 Farm Bill to support the commercialization of cellulosic ethanol at three facilities owned by Coskata, Enerkem and INEOS New Planet BioEnergy. The projects represent a combined per year production capacity and will begin producing cellulosic ethanol in 2012. The USDA also released a list of advanced biofuel producers who will receive payments to expand the production of advanced biofuels. In July 2011, the US Department of Energy gave in $105 million in loan guarantees to POET for a commercial-scale plant to be built Emmetsburg, Iowa.\n\nThe cellulosic ethanol industry in the United States developed some new commercial-scale plants in 2008. Plants totaling 12 million liters (3.17 million gal) per year were operational, and an additional 80 million liters (21.13 million gal.) per year of capacity - in 26 new plants - was under construction. (For comparison the estimated US petroleum consumption for all uses was about 816 million gal/day in 2008.)\n\nCellulosic ethanol and grain-based ethanol are, in fact, the same product, but many scientists believe cellulosic ethanol production has distinct environmental advantages over grain-based ethanol production. On a life-cycle basis, ethanol produced from agricultural residues or dedicated cellulosic crops has significantly lower greenhouse gas emissions and a higher sustainability rating than ethanol produced from grain.\n\nAccording to US Department of Energy studies conducted by the Argonne National Laboratory of the University of Chicago, cellulosic ethanol reduces greenhouse gas emissions (GHG) by 85% over reformulated gasoline. By contrast, starch ethanol (e.g., from corn), which usually uses natural gas to provide energy for the process, reduces greenhouse gas emissions by 18% to 29% over gasoline.\n\nCritics such as Cornell University professor of ecology and agriculture David Pimentel and University of California at Berkeley engineer Tad Patzek question the likelihood of environmental, energy, or economic benefits from cellulosic ethanol technology from non-waste.\n\n\n"}
{"id": "865341", "url": "https://en.wikipedia.org/wiki?curid=865341", "title": "Chevrolet Silverado", "text": "Chevrolet Silverado\n\nThe Chevrolet Silverado, and its mechanically identical cousin the GMC Sierra, are a series of full-size and heavy-duty pickup trucks manufactured by General Motors and introduced in 1998 as the successor to the long-running Chevrolet C/K line. The Silverado name was taken from a trim level previously used on its predecessor, the Chevrolet C/K pickup truck from 1975 through 1998. General Motors continues to offer a GMC-badged variant of the Chevrolet full-size pickup under the GMC Sierra name, first used in 1987 for its variant of the GMT400 platform trucks.\n\nThe heavy-duty trucks are informally referred to as \"Silverado HD\" (and Sierra HD), while the light-duty version is referred simply to as \"Silverado\" (and Sierra).\n\nAlthough General Motors introduced its first pickup truck in 1930, the term \"Silverado\" was a designation used only to detail the trim for the Chevrolet C/K pickup trucks, Suburbans, and Tahoes from 1975 through 1999. GMC used a few variations of the \"Sierra\" name (e.g. Sierra, Sierra Classic, and High Sierra); however, Chevrolet still uses the CK and the CC in their current model codes.\nThe Chevrolet Silverado and GMC Sierra trucks have been essentially the same for their entire history. However, there are some trim and add-on option variations. Early models included variations in the engine and equipment, but the present differences are slight. The 1999 model year redesign included different grilles and interior trim, and certain features (e.g. Quadrasteer) were included at different times on the two trucks. The GMC Sierra has a luxury package known as \"Denali\" which adds additional creature comfort features and design changes. Chevrolet's equivalent to the Denali trim level is the High Country Silverado which was introduced in the 2014 model year. In 2018, at the Work Truck Show, another new Silverado will be released, being the 4500 and 5500; these vehicles will be exclusive to Chevrolet, as GMC has no plans to offer an equivalent. GM webpage\n\nThe GMT800 Silverado/Sierra 1500 and 2500 (non Heavy Duty) (light pickup trucks) were released in August 1998 as 1999 models. The \"classic\" light-duty GMT400 C/K trucks were continued in production for that first year alongside the new models, and the Heavy-Duty GMT400 pickups (alongside the GMT400 SUVs) were continued until 2000, with the new GMT800 Silverado/Sierra HD (Heavy Duty) released a year later. A small refresh for 2003 models was introduced in 2002, bringing slight design changes and an upgrade to the audio and HVAC controls. The latter 2006 and 2007 GMT800 production units used the name \"Classic\" to denote the difference between the first and second generation trucks.\n\nIn January 1993, GM began development on the GMT800 pickup program with numerous teams coming together. By the end of 1994, a final design was chosen and finalized for production in June 1995 at 36 months ahead of scheduled start in June 1998. Development sign-off was issued in late 1997, with pre-production and series production commencement in June 1998.\nThere are a number of models of light-duty Silverados and Sierras, including the half-ton, SS, and Hybrid.\n\nThe light-duty trucks use the 1500 name. They are available in three cab lengths, 2-door standard/regular cab, 4-door extended cab, and front-hinged 4-door crew cab. Three cargo beds are available: a short box, standard box, and a long box. The short box is only available with the crew cab.\n\nFor the first year, only the regular cab and an extended cab were available, along with the Vortec 4300 V6, Vortec 4800 V8, and the Vortec 5300 V8. In 2000, a driver's side door option became available for the extended cab, giving it four doors, and the crew-cab body was added to the lineup in 2004. Output on the 5.3 L engine also increased to and .\n\nThe 6.0 L Vortec 6000 V8 was standard on the 2500 and was added for the 2001 Heavy Duty models, rated at , with the GMC Sierra 1500 \"C3\" getting an uprated version of this engine. The Silverado \"Z71\" got an optional lighter composite box, with a suspension package for towing, but lacked the high-output engine. The \"C3\" became the \"Denali\" for 2002, and Quadrasteer was added.\n\nGM introduced a reworked version of the Silverado and Sierra in 2003, with a new front end and a slightly updated rear end. In 2006 the Silverado received another facelift, similar to the HD version introduced in 2005 HD models. In addition to that, Chevrolet has deleted the \"Chevrolet\" badge off the tailgate that was used from 1998-2005. Its SUV counterparts retained the use of the pre-facelift sheetmetal. During the 2005 model year, all light duty GMT800 pickups reverted to front disc/rear drum brakes as a cost cutting measure; heavy duty trucks and the SUVs retained their 4-wheel disc brakes.\n\nThe Insurance Institute for Highway Safety (IIHS) gave the Silverado an overall \"marginal\"\nscore on the frontal offset crash test for poor structural integrity and poor dummy control, although no injuries were recorded on the dummy's body regions.\n\nGMC created an upscale version of its Sierra 1500 in 2001 called the Sierra C3. It used all-wheel drive with a 3.73 final drive gear ratio and included the 6.0 L \"Vortec 6000\" LQ4 V8 rated at at 5000 rpm and 370 lb•ft (502 N·m) of torque at 4000 rpm coupled to a 4L60E-HD four-speed automatic transmission along with other upscale equipment. For 2002, the name was changed to Sierra Denali, but the specifications remained essentially the same except for the addition of Quadrasteer and GM changed from the 4L60E-HD to the 4L65E in conjunction with a 4.10 final drive gear ratio.\n\nThe Denali is rated for towing and hauling in the cargo box.\n\nThe Sierra Denali was initially equipped with Delphi's Quadrasteer system as standard equipment. It was a 4-wheel steering system that greatly reduced the truck's turning radius and improved lane changing while towing. General Motors dropped Quadrasteer from the Sierra Denali after the 2004 model year and its entire lineup after 2005 due to poor sales of this expensive option.\n\nLaunched in early 2003, the Silverado SS is a high-performance pickup truck built by Chevrolet. It is based on the 1500 Silverado Extended Cab with Fleetside Box and features upgrades in the drive train and both exterior and interior appearance. It was equipped standard with the 6.0 liter Vortec High-Output V8 rated at at 5200 rpm and of torque at 4000 rpm coupled to a 4L65E four-speed automatic transmission. This was the same engine used for the second generation Cadillac Escalade. Chevrolet and GMC advertised this engine as the Vortec High Output and later as the \"VortecMAX\", while Cadillac calls it the \"HO 6000\". The SS debuted in 2003 with a standard All Wheel Drive setup with a 4.10 final drive gear ratio and 4 wheel disc brakes. In 2005, in an attempt to increase sales, a 2-wheel drive version became available (the 2WD SS also lost its rear disc brakes in favor of drums, as did the rest of the 1/2-ton GMT800s). 2005 was also the first year the sunroof was available in the SS line up. In 2006, the AWD variant was dropped and the rear wheel drive was the only driveline layout available. In a further effort to reduce cost, you could also get cloth interior and/or a bench seat. The Silverado SS also comes with the Z60 performance suspension and 20 inch aluminum wheels. All the SS trucks in both 2 wheel drive and AWD used the torsion bar style front suspension for better handling. SS themed trucks were only available from the factory in \"Black Onyx\", \"Victory Red\", and \"Arrival Blue Metallic\" from 2003 to 2004. In 2005 \"Arrival Blue Metallic\" was dropped from the color choices and replaced with \"Silver Birch Metallic\".\n\nIn 2006, Chevrolet released a special edition Silverado SS under the name \"Intimidator SS\" (licensed by Dale Earnhardt, Inc.) to honor the late Dale Earnhardt. The truck came with several minor appearance upgrades (rear spoiler, embroidered headrests, Intimidator custom badging), but was essentially just a regular Silverado SS. Of the 1,033 scheduled trucks, only 933 were made (the remaining 100 were sold as 2007 Silverado SS \"classic\" body style trucks before the 2007.5 MY changeover. These trucks were only available in Black Onyx exterior but could be ordered with cloth or leather interior. Also features-\n\nThe Vortec High Output option was first introduced in 2004 to a limited market (mainly consisting of Texas and several surrounding areas); it was available nationwide for MY 2005. It was available for both the Chevy and GMC 1500 series trucks. This special edition package (under option code B4V) included several options previously not found on the standard 1500 model, most notably the LQ9 6.0 L V-8 engine (the same used for the Silverado SS, the 2005-2006 GMC Denalis and the Cadillac Escalade). The LQ9 motor was rated at at 5200 rpm and of torque at 4000 rpm, which was the same specifications shared in the SS models. The B4V package could only be ordered on extended cab standard box 2WD trucks. They were all built at the Canadian assembly plant and were equipped with the Z60 High Performance suspension package, in addition to the M32 = 4L65E transmission, GT4 = 3.73 rear gear, and G80 Gov Lock as standard equipment. The 2004 models were equipped with the standard 10 bolt 8.625 rear end. The 2005 models were upgraded with the larger 14 bolt 9.5 rear end under RPO option AXN. The package also included one style of the newly introduced GM 20 inch wheels installed from the factory. This marked the first time the LQ9 engine was available for a two-wheel drive application. Unlike the previous years with the SS Package, you could order the interior combination in anything from basic cloth to fully loaded. There were also more exterior color options available with this package.\n\nIn 2006, the Vortec Max trailering package became available. The Vortec Max package was added to the option list with an array of similar features and new badges, and at its core retained the LQ9/4L65-E powertrain. However, the Vortec Max package differed from the Performance edition in that it also came with a variant of the Z85 Handling/Trailering suspension, as well as 17-inch wheels and tires under the option code NHT on 4x4 models versus the Z60 High Performance Suspension and 20-inch wheel and tire package of the regular B4V 2 wheel drive Performance Edition. This was because the Vortec Max package was intended for max trailer towing, while the Performance Edition was intended more for customers who wanted the Silverado SS mechanicals without the visuals of the SS. It was also made available (in addition to the extended cab) in the light duty 4 door crew cab models. For the first time it was also available in 2 wheel drive or 4 wheel drive on both the Silverados and Sierras, with or without the Z71 package.\n\nThe newest Vortec Max performance package introduced in 2006 on the GMT900 includes a 10,800 pounds towing capacity.\n\nGM launched a hybrid version of the Silverado/Sierra in 2004, becoming the first ever GM hybrid passenger vehicle. Known within GM as the Parallel Hybrid Truck or PHT it is not actually a parallel hybrid by the current definition, but a type of micro hybrid design. The electric motor housed within the transmission flywheel housing, serves only to provide engine cranking/starting, battery charging, and powering accessories. The engine automatically shuts down as the truck comes to a stop and uses 42 Volt electric power to the starter/generator unit to restart the engine as the brake pedal is released. Besides the typical 12 V automotive battery the PHT uses three additional 12 V valve regulated lead acid (VRLA) batteries mounted under the rear seat to store and provide power. The truck uses a 5.3 L \"Vortec 5300\" V8 for primary propulsion power. These trucks were also purchased back from customers for more than what they were worth in the late 2000s.\n\nThe PHT features four 120 volt 20 amp AC outlets, two in the bed and two inside the cab under the rear seat. These are particularly interesting to the building/construction contractor market, since they often require AC power when on the job. Additionally, the extra reserves of power for the accessories make this truck well-suited to that market, where trucks often sit at idle for hours at a time.\n\nAvailability was extremely limited at first, with commercial buyers getting the first allotment. Later in 2005, the truck was offered at retail in Alaska, California, Florida, Nevada, Oregon, Washington and Canada. For 2006–07 the truck was generally available to retail buyers throughout North America. The Parallel Hybrid Truck was discontinued for the 2008 model year with the release of the GMT900 truck line. Starting in 2009, General Motors offers a second generation Chevrolet Silverado and GMC Sierra equipped with a Two-Mode Hybrid powertrain and CVT.\n\nThe HD variant is a heavy-duty light truck. It is a strengthened version of the Silverado/Sierra light-duty, and is available in the 1500HD, 2500HD, and 3500HD models. The 1500HD, introduced in 2000, offers a Vortec 6000 V8 with at 5200 rpm and of torque at 4000 rpm with a Hydra-Matic 4L80E four-speed automatic transmission. The 2500HD also offers the LB7 Duramax V8 with at 3100 rpm and of torque at 1800 rpm, the LLY Duramax V8 with 310 hp (231 kW) at 3000 rpm and 605 lb·ft (820 N·m) at 1600 rpm, and the LBZ Duramax V8 with 360 hp (268 kW) at 3200 rpm and 650 lb·ft (881 N·m) at 1600 rpm. Also available is the Vortec 8100 V8 with at 4200 rpm and of torque at 3200 rpm.\n\nThe 2500HD has an available five-speed (six-speed for 2006-2007 models) Allison 1000 transmission with the Vortec 8100 and Duramax 6.6. The Silverado 3500 offers the same engine/transmission features that the 2500HD does, however it is usually equipped with \"dually\" twin wheels at the rear and has a stronger suspension. The HD models are primarily used for towing and high-weight cargo. The Chevrolet Silverado 2500HD came in at #2 in a 2016 study by iSeeCars.com ranking the top 10 longest-lasting vehicles. The Silverado 2500HD had 5.7% of vehicles over 200,000 miles, according to the iSeeCars study.:)\n\nTowing capacity for the 1500HD is rated at and can haul in the bed depending on options.\nTowing capacity for the 2500HD is rated at with the gasoline 8.1L V8 with 3.73:1 rear and can haul in the bed depending on options.\nTowing capacity for the 3500HD is rated at and can haul in the bed depending on options. \nThe addition of 4 wheel drive tends to reduce the towing and carrying capacity by 200 to , depending upon year and model. Other factors, such as options, can also affect these numbers.\n\nThe all-new GMT900 generation of the Silverado/Sierra arrived in the last quarter of 2006 as a 2007 model. It features a redesigned exterior, interior, frame, and suspension as well as power increases on certain engines. It takes styling cues from the 2007 GMT900 SUVs and the Chevrolet Colorado pickups. Like the GMT900 SUVs, these pickups also have greatly improved aerodynamics over their predecessors like steeply raked windshields and tighter panel gaps which improve fuel economy. The GMT800 models were continued through 2007 badged as \"Classic\", just as the GMT400 models continued for two years after the GMT800's introduction.\n\nThe new Silverado earned the \"North American Truck of the Year\" award for 2007 and was \"Motor Trend\" magazine's Truck of the Year for 2007. Like its predecessors, the new Silverado offers buyers a choice of two door regular cabs, four door extended cabs (with rear doors that now open 170 degrees similar to the Nissan Titan) and four door crew cabs with the rear doors opening in the same direction as the front doors. GM also offers the trucks in the traditional two and four wheel drive configurations.\n\nFor the '07 model year, the Sierra Denali shares the same billet grille from the other Denali models, and also has the same dash as the '07 SUV's. The '07 Sierra Denali was initially the only half-ton pickup that had a 6.2 liter with and of torque coupled to a six-speed transmission. This truck is also an optional all-wheel drive vehicle and goes 0-60 mph in 6.3 seconds.\nThe Generation III small block V8 engines offered in the GMT 800 trucks were replaced in the GMT 900 series by the Generation IV small block V8 engine family, featuring upgrades such as increased power and Active Fuel Management on the 5.3 L and 6.0 L V8s. A new high performance 6.2 liter V8 (with and of torque) was introduced with the 2007 Cadillac Escalade and 2007 GMC Denali line, and is now available on the Silverado LTZ trim line. After skipping the 2008 model year, with 2007 being the last for the GMT800 hybrid line, a two-mode hybrid model was introduced in late 2008 as a 2009 model. General Motors discontinued the Silverado Hybrid due to poor sales along with the GMC Sierra Hybrid, Chevy Avalanche, Chevy Tahoe Hybrid, GMC Yukon Hybrid, Cadillac Escalade Hybrid, and Cadillac Escalade EXT after the 2013 model year even though it was the one of the two first hybrid pickup trucks ever manufactured. Available in either two- or four-wheel-drive, the Sierra 1500 Hybrid is powered by a 6.0-liter V8. It's joined by two 60-kilowatt electric motors supplied by a nickel–metal hydride battery pack under the rear seat. On its own, the V8 is rated at 332 horsepower and 367 pound-feet of torque. GM engineers say that combined output with the electric motors is 379 hp. The unique transmission houses the electric motors along with three different planetary gear sets and four traditional clutches.\n\nThere were two dash options offered in this model Silverado and Sierra: a luxury-inspired dash that closely mimics the dash in their GMT900 SUVs, and a more traditional upright dash to make room for a passenger seat in place of a center console.\n\nAs of 2008, General Motors full-size trucks were no longer sold in United States and Canada with manual transmissions; they were only offered in Mexico in the Silverado 1500 V6 engine and Silverado 3500.\n\nAll Silverado & Sierra 1/2-ton models received a revised bumper and shortened front fascia for the 2009 model year, and extended and crew cab models equipped with the Vortec 5300 V8 received a new six-speed 6L80 automatic transmission. The Vortec 6200 V8 was made available for LTZ and SLT models. Bluetooth was added to the equipment list, becoming standard on Denali, SLT, and LTZ and optional on SLE and LT, as was an optional rear vision camera. An integrated trailer brake controller, first available on the Silverado and Sierra HD for 2007, is now an option on 1500 series trucks. The XFE package was new for 2009, available only on Silverado 1500 Crew Cab LT 2 wheel drive models, it included the 5.3L Vortec V8, soft tonneau cover, aluminum wheels, and low rolling resistance tires. \n\nA full mid-cycle refresh followed with all 2010 models, including new interior door panels (which moved the handle forward and added an additional cup holder) and a six-speed automatic transmission on regular cab models with the 5.3L V8 was also made standard. The Vortec 6200 V8 was given wider availability, now being optional on LT and SLE extended and crew cabs. The new Z71 Appearance Package was optional on LT and LTZ, it included: body-color grille and front fascia, body color door handles and mirror caps, unique Z71 box side decals, chrome sill plates, and a unique Z71 gauge cluster. Two new exterior colors were added: Taupe Gray Metallic and Sheer Silver Metallic.\n\nFor 2012, the Sierra and Silverado 1500 received another mid-cycle refresh. This time the Silverado was given new grille and front fascia treatments for both LT and LTZ models. A newly redesigned touch-screen navigation radio was optional on LTZ, SLT, and Denali. Trailer sway control and hill start assist are now standard on all models. Cooled seats were made optional on LTZ and SLT and the woodgrain trim was replaced with brushed aluminum. \n\nThe 8.1 L big-block V8 is no longer offered on the Heavy Duty models, and no replacement has been announced. The 6L90 6-speed automatic transmission is standard in all Heavy Duty models. The Allison 1000 transmission is paired with the optional Duramax V8 diesel.\n\nAs of 2011 the GMC and Chevrolet heavy Duty's have being upgraded with a new fully boxed high strength steel frame from front to rear improving stiffness by 92% with bigger rear springs, larger engine and transmission mounts and new hydraulic body mounts to improve the ride.The front suspension incorporates new upper and lower control arms and new torsion bars tailored to one of five different gross axle weight ratings. Upper control arms are constructed from forged steel that is both stronger and lighter than the previous arms, while the new lower arms are cast iron to maximize load capacity. Using a unique torsion bar for each gross weight rating allows for better control over vehicle height, resulting in improved handling and better alignment for reduced tire wear. These improvements allow for up to a 6,000 pound front axle weight rating, allowing all 4wd trucks to accommodate a snow plow.\nAdditional front suspension enhancements come from new urethane bump stops, two per side. The upper shock mount has been changed from a single stem mount to a two-bolt design to eliminate the possibility of squeaks and thumps. \nThe rear suspension design uses asymmetrical leaf springs that are wider and capable of greater load handling. The design features 3-inch wide leaves, with front and rear spring sections of different lengths to reduce the twisting that can result in axle hop and loss of traction. The 2500HD use a two-stage design with a rating of 6,200 lbs, and 3500HD models have a three-stage design with 7,050 lb and 9,375 lb ratings on single and dual-wheel models respectively\n\nFor the 2011 model year of the Silverado/Sierra, the production of 1500 crew cab models partially moved from Silao, Mexico to Flint, Michigan.\n\nThe Silverado comes standard with four-wheel ABS. StabiliTrak and side curtain airbags are optional on certain trim levels.\n\n2007 NHTSA crash test:\n\n\nThe IIHS gave the Silverado a \"Good\" score in their frontal crash test, however 2007-09 models equipped with or without optional side curtain airbags received a \"Poor\" rating in the side impact test. For 2010 models the side structure was strengthened, side torso air bags were added, and side airbags became standard, with the upgrades the IIHS side impact overall rating improved to \"Acceptable\", while the overall side structure rating is improved from \"Poor\" to \"Acceptable\". The crew cab variant of the 2011-13 Silverado is also rated \"Marginal\" in the IIHS roof strength test.\n\nIn 2011 developer of the Chevrolet Volt and former vice chairman Bob Lutz joined VIA Motors in developing electric versions of the Chevrolet Silverado/GMC Sierra called VIA VTrux.\n\nOn December 13, 2012, the redesigned 2014 Chevrolet Silverado, along with the related 2014 GMC Sierra were introduced in Detroit, Michigan, later making their public debut at the North American International Auto Show. GM dropped the 900 platform and changed to K2XX. The third generation Silverado 1500 has three gas engine options: 4.3 L EcoTec3 V6, 5.3 L EcoTec3 V8, or 6.2 L EcoTec3 V8. Chevrolet's MyLink touch-screen multimedia interface system will be available on most models. It will have A2DP stereo streaming technologies, Bluetooth hands-free telephone, USB hookups, and an input for an Apple iPod or iPhone. When connected via the USB port, an iPhone 4/4S, iPhone 5/iPhone 5S, iPhone 6, or iPhone 6 Plus will be able to stream music from Pandora Radio. A Bose premium audio system, as well as a Bose surround sound audio system will be available on most models. OnStar will be standard on all models.\n\nUnderneath, the Silverado rides on a fully boxed high strength steel frame with hydroforming technology; truck cab's frame is built with high strength steel as well. The third generation Silverado uses aluminum on the hood, for the engine block and for the control arms in order to save mass. The truck's bed is made of roll-formed steel instead of stamped steel used by other manufacturers in order to save weight and gain strength. The third generation Silverado introduced the upmarket High Country edition which includes saddle brown leather interior, which is Chevrolet's first entry into the luxury market. A revised front end features styling cues from the 1980s-era Chevrolet C-Series Trucks, and, on Z-71 models, a Z-71 badge was added to the front grille. The first production Silverado completed assembly on April 29, 2013. The Silverado 1500 went on sale in May 2013 as a 2014 model, with the Silverado HD Series being available in early 2014 for the 2015 model year. On January 10, 2014 a recall was issued for 370,000 Silverado and Sierra pickups due to a fire risk.\n\nA rebadged version of the Silverado called the Chevrolet Cheyenne is sold in Mexico.\n\nThe American model version of the 2014 Chevrolet Silverado 1500/2500HD/3500HD and the 2015 Chevrolet Silverado High Country was introduced to the Philippines market by Chevrolet Motorama Show late November 2014 as a MY2015, along with the MY2015 Chevrolet Suburban, 2015 Chevrolet Tahoe, 2015 Chevrolet Impala, 2015 Chevrolet Express Van, 2015 Chevrolet Trax and 2015 Chevrolet Colorado.\n\nOn January 14, 2014, The Silverado, along with the Chevrolet Corvette Stingray, received the 2014 North American International Auto Show's Car and Truck of the Year awards.\n\nFor the 2015 model year, the 6.2L Ecotec3 was paired with the new 8-speed 8L90 transmission, offering a wider ratio spread with more closely spaced gears, quicker shifts, and improved acceleration and fuel economy. The Silverado 2500/3500 HD models continue to use the 6.0L L96 flex fuel capable Vortec engine combined with the 6l90e transmission, installed with engine oil and transmission fluid coolers for hauling/towing applications.\n\nFor that year, Chevrolet introduced Special Edition for the Silverado truck series. They are in the following series of Rally Edition 1 and 2, Midnight Edition, Custom Sport Edition, Custom Sport Plus Edition, Black Out Edition, and Texas Edition.\nFor 2016, the Chevrolet Silverado 1500 and GMC Sierra 1500 received its first mid-cycle refresh. The refreshed Silverado and Sierra received a new grille, new headlights and new front fascia, with design influences from the 2015 Chevrolet Colorado, as well as all new LED tail lights on the 2016 Silverado LTZ, HIGH COUNTRY, 2016 GMC Sierra SLT and DENALI trim levels while the 2016 Chevy Silverado LT trim levels and below still have the pre-facelifted taillights along with the 2016 GMC Sierra SLE trim levels and below. On October 1, 2015, HD Radio was added to the Silverado, Sierra, and their HD level models. The Silverado/Silverado HD will add HD Radio to their 8-inch MyLink system as a standard on the LT and LTZ trims, while the Sierra/Sierra HD will include the feature in its IntelliLink system as a standard on all its trims. 2016 Silverado and Sierra models optioned with the 8-inch touch screen feature Apple CarPlay and Android Auto capability. Trucks ordered with bucket seats and center console now feature a wireless charging pad on the console lid. \n\nFor the 2017 model year, Silverado and Sierra HD models equipped with the 6.6L Duramax Turbo Diesel V8 (L5P) received a new front hood with air intake vents. Gasoline-powered models now have a capless fuel fill. Sierra HD now features standard HID headlamps on all trim levels. New colors include \"Graphite Metallic\" and \"Pepperdust Metallic\" (Chevrolet) or \"Dark Slate Metallic\" and \"Pepperdust Metallic\" (GMC). Jet Black/Medium Ash Gray interior trim is added for Silverado High Country. Low-Speed Mitigation Braking is added to the Driver Alert Package for both Silverado and Sierra. \n\nStarting in 2016, GM offered an \"eAssist\" mild hybrid version of the 5.3L V8 engine in the Silverado LT and Sierra SLT, but only in the state of California. The engine came with an 8-speed automatic transmission and has the same horsepower and torque as the regular 5.3 V8. For 2017, it was also available in Hawaii, Oregon, Texas, and Washington. For 2018, it became available nationwide and was also offered in the Silverado LTZ.\n\nIn late 2014, Chevrolet released the 2015 Silverado SSV (Special Service Vehicle) to complement its lineup of law enforcement vehicles. The SSV Silverado is available in 1500 Crew Cab form with either the short (5.5') or standard (6.5') cargo box. The 5.3L EcoTec3 V8 is the only engine option, and the truck is based on WT trim. It features beefier brake rotors, upgraded oil coolers, high output alternator, auxiliary battery and an upfitter friendly interior. \n\nIn 2016, GMC introduced the Sierra All Terrain X, which is equipped with a 5.3-liter V8 engine with performance exhaust, bumping output up from 355 to 365 horsepower. On the exterior, the Sierra All Terrain X features back 18 inch rims fitted with Goodyear Wrangler DuraTrac MT tires, black bed-mounted sport bar, LED off-road lights, and blacked-out accents.\n\nAfter introducing the Sierra 1500 All Terrain X full-size pickup and the Canyon All Terrain X small pickup, in 2017 GMC extended the same formula to its heavy-duty pickup family with the Sierra HD All Terrain X.\n\nAvailable on four-wheel drive Sierra 2500 HD crew cab models in either Black Onyx or Summit White exterior colors, the Sierra HD All Terrain X package provides a unique, premium mix of specialized exterior trim and off-road special equipment, uniquely paired with GMC's high-level of refinement and technologies.\n\nAll Terrain X's customized appearance starts with a unique body-color grille surround, which flanks a distinctive grille insert that's unique to All Terrain. Body color door handles, front and rear bumpers, and bodyside moldings provide a distinctive monochromatic appearance, while black accents, including belt moldings, front bumper skid plate, and B-pillar accents, enhance its bold attitude.\n\nSierra HD All Terrain X models also include unique eighteen-inch black-painted aluminum wheels – fitted with rugged LT275/65R18 Goodyear Duratrac MT-rated tires – along with four-inch black sport side steps, black heated and power-folding trailering mirrors with integrated turn signals and LED guidance lamps, a spray-on bedliner, and a distinctive black bed-mounted sport bar, which is designed to support available GMC Accessories LED off-road driving lamps.\n\nInside, the 2017 Sierra 2500 HD All Terrain X also features GMC IntelliLink with an 8-inch-diagonal color touchscreen, Teen Driver, remote-locking tailgate, remote starting system, Rear Vision Camera, adjustable pedals, leather-appointed seats, heated front seats and wireless mobile device charging.\n\nSierra HD All Terrain X's strong, bold appearance is matched by equally strong engineering underneath its surface. A 6.0-liter gasoline engine is standard, but the all-new, next-generation 6.6-liter Duramax Diesel turbo-diesel V-8 is also available. More powerful and refined than ever before, this latest Duramax Diesel delivers 445 hp and 910 lb-ft of torque – providing plenty of power to trailer with confidence.\n\nFor added confidence off-road, the Sierra HD All Terrain X also includes the Z71 off-road suspension package, which adds front underbody and transfer case skid plates, twin-tube Rancho shocks, hill descent control, and off-road information graphics in the Driver Information Center. An Eaton automatically-locking rear differential (410:1) is also standard equipment.\n\nFor that year, Chevrolet has added and dropped some of the Special Editions for the Silverado truck series. They are in the following series of Rally Edition 1 and 2, Midnight HD Edition, Midnight Edition, Custom Sport HD Edition, Realtree Edition, Custom Sport HD Edition, Midnight Edition, and Special Ops Edition.\n\nThere were several addition to this year's Special Editions for the Silverado series shown in boldface. They are the following series of Midnight, Midnight HD, Rally 1, Rally 2, High Desert, Custom Sport HD, Realtree, Special Ops, Alaskan (Available on HD series only), Redline and Black Out. The remaining Special Edition listed in the 2017 Chevrolet Silverado brochure were All Star Edition and Texas Edition.\n\nSpecial edition models of the Chevrolet Silverado and Chevrolet Colorado were introduced to commemorate the 100th anniversary of Chevy Trucks.\n\nChevrolet introduced Centennial Edition Chevrolet Silverado for this year. In addition to the Centennial Edition, the remaining Silverado Special Editions were virtually unchanged from the last year model.\n\nIn January 2018, Chevrolet announced that it would continue production of the third-generation Silverado 1500 for 2019, and that it would be sold alongside the all-new, fourth-generation 2019 Chevrolet Silverado 1500. Renamed the Chevrolet Silverado LD (\"LD\" for Light Duty), production of the previous-generation truck will move from Silao, Mexico and Fort Wayne, Indiana to Oshawa Car Assembly, Ontario, Canada.\n\nThe first-generation Chevrolet Silverado 1500 was sold alongside the second-generation Chevrolet Silverado 1500 for the 2007 model year as the Chevrolet Silverado 1500 Classic, and 2500 and 3500 versions of the truck were also sold alongside their successors, the Chevrolet Silverado 2500 and 3500. GMC also offered versions of their Sierra trucks as the Sierra Classic for 2007 as well, and will do the same for 2019, offering the predecessor generation model as the 2019 GMC Sierra 1500 Limited.\n\nGM's chief financial officer Dhivya Suryadevara confirmed that the third-generation models of the Silverado and Sierra will be continuously built and sold to customers right next to the 2019 pickups until the year 2019. Crew cab models will continue production until early 2019 while regular and double cab variant production won't end until the early second half of next year.\n\nOn December 16, 2017, Chevrolet unveiled the all-new, fourth-generation 2019 Silverado 1500 at its Chevy Truck Centennial Celebration Weekend at Texas Motor Speedway in Fort Worth, Texas. The reveal of the all-new Silverado was not expected until early 2018. For its debut, the Silverado 1500 was airlifted via helicopter onto a stage, where it was introduced to a crowd of Chevrolet truck owners and enthusiasts, as well as to the automotive press. In addition to the reveal of the 2019 Silverado 1500, multiple Chevrolet trucks of different generations were on display as well.\n\nThe model shown at the reveal was the 2019 Chevrolet Silverado 1500 LT Trail Boss 4X4, which is a factory-modified version of the Silverado 1500 LT Z71. Distinguishing features of the Trail Boss from other Silverado 1500 trim levels are its gloss black front fascia, black rear step bumper, black Chevrolet \"bow-tie\" emblems on the front grille and rear tailgate, gloss black-finished aluminum-alloy wheels, large off-road tires, altered suspension, \"Trail Boss\" decals on the sides of the pickup box, front bumper-mounted center fog lamps, \"Z71\" emblems on each front fender, and red-painted front tow hooks.\n\nThe formal unveiling of the Silverado 1500 took place at the 2018 North American International Auto Show in Detroit, Michigan on January 13, 2018 exactly 100 years after Chevrolet delivered its first trucks to customers on January 13, 1918. At the unveiling, Chevrolet shown the commemoration of their 100th year of Chevy Trucks by a display of historic Chevrolet pickups temporarily shown at the stage before the launch and a presentation video showing how Chevy Trucks in 100 years have become the most dependable and longest lasting full-size pickups on the road. It shows that the company will not stop there and the next 100 years have just begun. The model was unveiled as a Silverado LT Trail Boss 4X4 model. After this, full details on the fourth-generation Chevrolet Silverado were announced.\n\nIt started availability to customers in August 2018 as an early 2019 model year vehicle with a base price of $32,200 MSRP.\n\nThe Silverado 1500 features a more sculpted exterior design, with front headlamps that integrate into the front grille, which also incorporate LED Daytime Running Accent Lamps (DRL's).\n\nThe Silverado 1500 will be available in eight distinct trim levels: WT, Custom, Custom Trail Boss, LT, RST, LT Trail Boss, LTZ, and High Country.\n\nChevrolet announced that 2 more Silverado trims will be exhibited at SEMA 2018 with the High Country and LTZ. They include the RST Off Road and RST Street which will be added into the Silverado trim lineup. \n\nEngine options include a carryover 4.3L gasoline V6, a revised 5.3L gasoline V8, a revised 6.2L gasoline V8, an all-new 3.0L Duramax turbocharged diesel inline six (I6), as well as an all-new 2.7L Turbocharged \"L3B\" 4 cylinder (I4) engine. The 6.2L gasoline V8 and 3.0L Duramax turbocharged diesel inline six (I6) engines will be mated to a ten-speed automatic transmission. The Silverado 1500 is the fourth full-size, half-ton pickup truck to offer a diesel engine option, following the 2018 Ford F-150, 2016 Nissan Titan XD, and 2014 Ram Truck 1500, and is the first to offer a 4-cylinder engine. Its engine has an improved version of Active Fuel Management called \"Dynamic Fuel Management\" that shuts off any number of cylinders in a variety of combinations, maximizing fuel economy and avoiding switching between banks of cylinders.\n\nAccording to EPA ratings, rear drive 2019 Silverado models with the 2.7-liter turbo-four engine can hit 20/23 mpg city/highway.\n\nStandard features include a next-generation Chevrolet infotainment system 3 with Apple CarPlay and Android Auto, OnStar with 4G LTE Wi Fi Connectivity. The Waze mobile app can now be compatible in its infotainment system through the use of CarPlay.\n\nTrailering features equipped for the 2019 model includes an industry's first trailering label and an in-vehicle towing app on its GM infotainment system. \n\nAdvanced, active safety features included in the 2019 Chevrolet Silverado are:\n\nChevrolet announced that they will release 3 All-new 2020 Silverado models including the Silverado 2500 HD and 3500 HD (2500 and 3500) models in the next 18 months. On November 21, 2018, Autoblog posted spy shots regarding the model getting a thinner disguise peeking the front headlights shown on Chevrolet's teaser photo of the vehicle.\n\nThe all-new 2019 GMC Sierra was unveiled in Detroit, Michigan on March 1, 2018. The Sierra 1500 will differentiate itself from its Chevrolet Silverado 1500 counterpart by offering unique features, such as a two-piece tailgate, a pickup bed constructed from carbon fiber, a 3\" x 7\" multi-color Heads-Up Display, a rear-view mirror backup camera system, and a luxury Denali trim level. The Sierra 1500 will also feature its own distinct exterior styling, though interior styling will be similar to that of the Chevrolet Silverado 1500.\n\nThere are 6 functions of GMC Sierra's Multi-Pro Tailgate including a primary tailgate load stop for large items. Its inner tailgate can be a load stopper, a full-width step, and a work area by dropping down the load stop or provide easy access to the bed just by folding down the inner tailgate.\n\nPowertrains include improved versions of the current 5.3L and 6.2L EcoTec3 V8 gasoline engines, as well as the same 3.0L Duramax Turbocharged Diesel I6 engine that will also be available in the 2019 Chevrolet Silverado 1500. An all-new, ten-speed automatic transmission will come as standard equipment on gasoline-powered Sierra 1500 models. Availability of the previous 4.3L EcoTec3 V6 gasoline engine was not announced at launch.\n\nThe 2019 GMC Sierra went on sale in September 2018 as an early 2019 model year vehicle at GMC dealerships nationwide.\n\nChevrolet introduced a medium duty version of its Silverado, the Silverado 4500HD and 5500HD, which will replace the discontinued Chevrolet Kodiak and GMC TopKick medium-duty trucks. The truck was unveiled at the 2018 Work Truck Show in Indianapolis, Indiana in March 2018, and will join the Chevrolet 4500HD/XD and 5500HD/XD cabover trucks. It will compete with the Ford Super Duty F-450 and F-550 and Ram 4500 and 5500. Chevrolet also plans to make the vehicles available for sale at least 160 selected dealerships, including the 240 that specializes in fleet and commercial truck sales.\n\nGMC has confirmed that they will not offer an equivalent, citing a lack of support for GMC to expand back into a medium duty market while making a push towards a premium market with their current lineup. The move leaves Chevrolet as the only brand in the GM truck division to have medium-duty vehicles in this segment as well as the only division to make its trucks and SUVs available for fleet sales.\n\nAllison Automatics started production of 1000 Series™ and 2000 Series™ fully automatic transmissions specifically designed for the Silverado 4500 HD, 5500 HD, and 6500 HD.\n\nThe medium-duty Silverado is a joint venture between GM and Navistar, it is built at Navistar's Springfield, Ohio plant. Navistar will also sell their own rebadged version, the International CV. \n\nThe prices for this model starts at $48,465.\n\nWhen production of the CUCV II ended in 2000, GM redesigned it to coincide with civilian truck offerings. The CUCV nomenclature was changed to Light Service Support Vehicle (LSSV) in 2001. In 2005, LSSV production switched to AM General, a unit of MacAndrews and Forbes Holdings. The LSSV is a GM-built Chevrolet Silverado 1500, Chevrolet Silverado 2500 HD, Chevrolet Tahoe, or Chevrolet Suburban that is powered by a Duramax 6.6 liter turbo diesel engine. The various Silverados, Tahoes, and Suburbans that are used provide numerous platforms for different kinds of vehicles. As GM has redesigned its civilian trucks and SUVs from 2001–present, LSSVs have also been updated cosmetically. \n\nThe Militarization of standard GM trucks/SUVs to become LSSVs includes exterior changes such as CARC paint (Forest Green, Desert Sand, or 3-color Camouflage), blackout lights, military bumpers, a brush guard, a NATO slave receptacle/NATO trailer receptacle, a pintle hook and tow shackles. The electrical system is changed to the 24/12 volt military standard. The dashboard has additional controls and dataplates. The truck also can be equipped with weapon supports in the cab, cargo tie down hooks, folding troop seats, pioneer tools, winches, and other military accessories.\n\nThe Enhanced Mobility Package (EMP) option adds enhanced suspension, 4-wheel anti-lock brakes, a locking differential, on/off-road beadlock tires, a tire pressure monitoring system and other upgrades. About 2,000 LSSV units were sold to U.S. and international military and law enforcement organizations.\n\n\nGeneral Motors relaunched GM Defense division in 2018 offering the ZH2 Silverado, an advanced technology Chevrolet Silverado with a hydrogen powered fuel cell and a heavy duty truck architecture modified for next generation military vehicle needs. There is also a ZH2 Chevrolet Colorado military version. \n\n\n\n\nChevrolet is represented in the NASCAR Camping World Truck Series by the Silverado. As of the 2016 season, 19 full-time teams use the Silverado. Chevrolet has won the NASCAR Camping World Truck Series Manufacturers Championship 8-times since the series inception in 1995 and Chevrolet drivers have won the Drivers Championship a combined 12-times. The Silverado is also the title sponsor for the Chevrolet Silverado 250 Truck Series race at Canadian Tire Motorsport Park.\n\nThe truck also won the Primm 300 off-road race in 2004, 2005 and 2006.\n\nAs of 2015, the Chevrolet Silverado is sold in the United States, Canada, Mexico, Venezuela, Chile, and the Middle East (except Iran), and the GMC Sierra is sold in the United States, Canada, Mexico, and the Middle East (except Israel and Iran).\n\nGM has also announced plans to introduce the Silverado, along with the smaller Colorado, to China.\n\nChevrolet will start selling the Silverado in Oceania in 2018 via its division Holden but will retain the Chevrolet brand and badge. The vehicles will be Right Hand Drive versions for that region.\n\nThere is a small gray market for both the Chevrolet Silverado and GMC Sierra trucks in some parts of the world, mostly in Scandinavian countries and Germany.\n\n\n"}
{"id": "44999247", "url": "https://en.wikipedia.org/wiki?curid=44999247", "title": "Chowilla Game Reserve", "text": "Chowilla Game Reserve\n\nChowilla Game Reserve is a protected area covering the floodplain on the north side of the River Murray in South Australia from about north-east of Renmark to the New South Wales border. It was proclaimed 8 April 1993 in conjunction with the Chowilla Regional Reserve. Its creation arises from a community consultation process which recommended that ‘hunting of waterfowl be a permitted activity in selected areas of the Chowilla floodplain’. The game reserve is classified as an IUCN Category VI protected area.\n\n\n"}
{"id": "8570947", "url": "https://en.wikipedia.org/wiki?curid=8570947", "title": "Clean Ocean Foundation", "text": "Clean Ocean Foundation\n\nThe Clean Ocean Foundation is an Australian environmental organisation that seeks to stop all forms of ocean pollution and restore our oceans to their former health. \n\nClean Ocean Foundation holds that the best way of minimising the impact of the waste from any land based activity is for the waste to be retained on land wherever possible so it’s impact can be properly monitored and minimised. It is dedicated to closure wherever possible of all ocean|sanitary sewer|sewerage|industrial outfalls. \n\nIf this is impossible Clean Ocean Foundation demands that rigorous, ongoing, transparent monitoring in conjunction in line with community standards and world’s best practice occur at each outfall.\n\nClean Ocean Foundation is currently involved in research under the auspices of the Marine Biodiversity Hub to produce the National Outfall Database. The database identifies the location and composition of domestic outfalls on the Australian coastline.\nClean Ocean Foundation is also developing community capability to monitor the impact of these outfalls on recreational users and the marine environment.\n\nClean Ocean Foundation was formed in 2000 by families, fisherman and surfers who became concerned at the high level of pollution at Mornington Peninsula surf beaches such as Gunnamatta.The Foundation was successful in convincing the Victorian State Government to commit over 400 million dollars to upgrade Eastern Treatment Plant that discharges to Boags Rock Outfall near Gunnamatta beach. \n\nFollowing this major victory, the Foundation is working towards the recycling of all wastewater from 254 outfalls around Australia to minimise the environmental effects of wastewater on the marine environment whilst also ensuring a vital source of water on a dry continent is not wasted.\n\nThe Foundation has also been successful in lobbying the NHMRC for Australia's recreational water guidelines to be raised to match World Health Organisation criteria.\n\nClean Ocean's mission is: \"\"To protect our ocean ecosystem and establish sustainable water management practices.\n\n"}
{"id": "67958", "url": "https://en.wikipedia.org/wiki?curid=67958", "title": "Copernicium", "text": "Copernicium\n\nCopernicium is a synthetic chemical element with symbol Cn and atomic number 112. It is an extremely radioactive element, and can only be created in a laboratory. The most stable known isotope, copernicium-285, has a half-life of approximately 29 seconds. Copernicium was first created in 1996 by the GSI Helmholtz Centre for Heavy Ion Research near Darmstadt, Germany. It is named after the astronomer Nicolaus Copernicus.\n\nIn the periodic table of the elements, copernicium is a d-block transactinide element and a group 12 element. During reactions with gold, it has been shown to be an extremely volatile metal, so much so that it is probably a gas at standard temperature and pressure.\n\nCopernicium is calculated to have several properties that differ from its lighter homologues in group 12, zinc, cadmium and mercury; due to relativistic effects, it may even give up its 6d electrons instead of its 7s ones. Copernicium has also been calculated to possibly show the oxidation state +4, while mercury shows it in only one compound of disputed existence and zinc and cadmium do not show it at all. It has also been predicted to be more difficult to oxidize copernicium from its neutral state than the other group 12 elements.\n\nCopernicium was first created on February 9, 1996, at the Gesellschaft für Schwerionenforschung (GSI) in Darmstadt, Germany, by Sigurd Hofmann, Victor Ninov et al. This element was created by firing accelerated zinc-70 nuclei at a target made of lead-208 nuclei in a heavy ion accelerator. A single atom (a second was reported but was found to have been based on data fabricated by Ninov) of copernicium was produced with a mass number of 277.\n\nIn May 2000, the GSI successfully repeated the experiment to synthesize a further atom of copernicium-277.\nThis reaction was repeated at RIKEN using the Search for a Super-Heavy Element Using a Gas-Filled Recoil Separator set-up in 2004 and 2013 to synthesize three further atoms and confirm the decay data reported by the GSI team. This reaction had also previously been tried in 1971 at the Joint Institute for Nuclear Research in Dubna, Russia to aim for Cn (produced in the 2n channel), but without success.\n\nThe IUPAC/IUPAP Joint Working Party (JWP) assessed the claim of copernicium's discovery by the GSI team in 2001 and 2003. In both cases, they found that there was insufficient evidence to support their claim. This was primarily related to the contradicting decay data for the known nuclide rutherfordium-261. However, between 2001 and 2005, the GSI team studied the reaction Cm(Mg,5n)Hs, and were able to confirm the decay data for hassium-269 and rutherfordium-261. It was found that the existing data on rutherfordium-261 was for an isomer, now designated rutherfordium-261m.\n\nIn May 2009, the JWP reported on the claims of discovery of element 112 again and officially recognized the GSI team as the discoverers of element 112. This decision was based on the confirmation of the decay properties of daughter nuclei as well as the confirmatory experiments at RIKEN.\n\nWork had also been done at the Joint Institute for Nuclear Research in Dubna, Russia from 1998 to synthesise the heavier isotope Cn in the hot fusion reaction U(Ca,3n)Cn; most observed atoms of Cn decayed by spontaneous fission, although an alpha decay branch to Ds was detected. While initial experiments aimed to assign the produced nuclide with its observed long half-life of 3 minutes based on its chemical behaviour, this was found to be not mercury-like as would have been expected (copernicium being under mercury in the periodic table), and indeed now it appears that the long-lived activity might not have been from Cn at all, but its electron capture daughter Rg instead, with a shorter 4-second half-life associated with Cn. (Another possibility is assignment to a metastable isomeric state, Cn.) While later cross-bombardments in the Pu+Ca and Cm+Ca reactions succeeded in confirming the properties of Cn and its parents Fl and Lv, and played a major role in the acceptance of the discoveries of flerovium and livermorium (elements 114 and 116) by the JWP in 2011, this work originated subsequent to the GSI's work on Cn and priority was assigned to the GSI.\n\nUsing Mendeleev's nomenclature for unnamed and undiscovered elements, copernicium should be known as \"eka-mercury\". In 1979, IUPAC published recommendations according to which the element was to be called \"ununbium\" (with the corresponding symbol of \"Uub\"), a systematic element name as a placeholder, until the element was discovered (and the discovery then confirmed) and a permanent name was decided on. Although widely used in the chemical community on all levels, from chemistry classrooms to advanced textbooks, the recommendations were mostly ignored among scientists in the field, who either called it \"element 112\", with the symbol of \"E112\", \"(112)\", or even simply \"112\".\n\nAfter acknowledging the GSI team's discovery, the IUPAC asked them to suggest a permanent name for element 112. On 14 July 2009, they proposed \"copernicium\" with the element symbol Cp, after Nicolaus Copernicus \"to honor an outstanding scientist, who changed our view of the world\".\n\nDuring the standard six-month discussion period among the scientific community about the naming,\nit was pointed out that the symbol \"Cp\" was previously associated with the name \"cassiopeium\" (cassiopium), now known as lutetium (Lu). For this reason, the IUPAC disallowed the use of Cp as a future symbol, prompting the GSI team to put forward the symbol Cn as an alternative. On 19 February 2010, the 537th anniversary of Copernicus' birth, IUPAC officially accepted the proposed name and symbol.\n\nCopernicium has no stable or naturally occurring isotopes. Several radioactive isotopes have been synthesized in the laboratory, either by fusing two atoms or by observing the decay of heavier elements. Seven different isotopes have been reported with atomic masses from 281 to 286, and 277. Most of these decay predominantly through alpha decay, but some undergo spontaneous fission, and copernicium-283 may have an electron capture branch.\n\nThe isotope copernicium-283 was instrumental in the confirmation of the discoveries of the elements flerovium and livermorium.\n\nAll copernicium isotopes are extremely unstable and radioactive; in general, heavier isotopes are more stable than the lighter. The most stable known isotope, Cn, has a half-life of 29 seconds; Cn has a half-life of 4 seconds, and the unconfirmed Cn has a half-life of about 8.45 seconds. Other isotopes have half-lives shorter than 0.1 seconds. Cn and Cn both have half-lives of 97 ms, and the other two isotopes have half-lives slightly under one millisecond. It is predicted that the heavy isotopes Cn and Cn may have half-lives longer than a few decades, and may have been produced in the r-process and be detectable in cosmic rays, though they would be about 10 times as abundant as lead.\n\nThe lightest isotopes of copernicium have been synthesized by direct fusion between two lighter nuclei and as decay products (except for Cn, which is not known to be a decay product), while the heavier isotopes are only known to be produced by decay of heavier nuclei. The heaviest isotope produced by direct fusion is Cn; the three heavier isotopes, Cn, Cn, and Cn, have only been observed as decay products of elements with larger atomic numbers.\n\nIn 1999, American scientists at the University of California, Berkeley, announced that they had succeeded in synthesizing three atoms of Og. These parent nuclei were reported to have successively emitted three alpha particles to form copernicium-281 nuclei, which were claimed to have undergone alpha decay, emitting alpha particles with decay energy 10.68 MeV and half-life 0.90 ms, but their claim was retracted in 2001. This isotope, however, was produced in 2010 by the same team. The new data contradicted the previous (fabricated) data.\n\nCopernicium is the tenth and last member of the 6d series and is the heaviest group 12 element in the periodic table, below zinc, cadmium and mercury. It is predicted to differ significantly from the lighter group 12 elements. The valence s-subshells of the group 12 elements and period 7 elements are expected to be relativistically contracted most strongly at copernicium. This and the closed-shell configuration of copernicium result in it probably being a very noble metal. Its metallic bonds should also be very weak, possibly making it extremely volatile, like the noble gases, and potentially making it gaseous at room temperature. However, it should be able to form metal–metal bonds with copper, palladium, platinum, silver, and gold; these bonds are predicted to be only about 15–20 kJ/mol weaker than the analogous bonds with mercury.\n\nOnce copernicium is ionized, its chemistry may present several differences from those of zinc, cadmium, and mercury. Due to the stabilization of 7s electronic orbitals and destabilization of 6d ones caused by relativistic effects, Cn is likely to have a [Rn]5f6d7s electronic configuration, using the 6d orbitals before the 7s one, unlike its homologues. The fact that the 6d electrons participate more readily in chemical bonding means that once copernicium is ionized, it may behave more like a transition metal than its lighter homologues, especially in the possible +4 oxidation state. In aqueous solutions, copernicium may form the +2 and perhaps +4 oxidation states. The diatomic ion , featuring mercury in the +1 oxidation state, is well-known, but the ion is predicted to be unstable or even non-existent. Copernicium(II) fluoride, CnF, should be more unstable than the analogous mercury compound, mercury(II) fluoride (HgF), and may even decompose spontaneously into its constituent elements. In polar solvents, copernicium is predicted to preferentially form the and anions rather than the analogous neutral fluorides (CnF and CnF, respectively), although the analogous bromide or iodide ions may be more stable towards hydrolysis in aqueous solution. The anions and should also be able to exist in aqueous solution. Nevertheless, more recent experiments have cast doubt on the possible existence of HgF, and indeed some calculations suggest that both HgF and CnF are actually unbound and of doubtful existence. Analogous to mercury(II) cyanide (Hg(CN)), copernicium is expected to form a stable cyanide, Cn(CN).\n\nCopernicium should be a very heavy metal with a density of around 23.7 g/cm in the solid state; in comparison, the most dense known element that has had its density measured, osmium, has a density of only 22.61 g/cm. This results from copernicium's high atomic weight, the lanthanide and actinide contractions, and relativistic effects, although production of enough copernicium to measure this quantity would be impractical, and the sample would quickly decay. However, some calculations predict copernicium to be a gas at room temperature, the first gaseous metal in the periodic table (the second being flerovium, eka-lead), due to the closed-shell electron configurations of copernicium and flerovium. The atomic radius of copernicium is expected to be around 147 pm. Due to the relativistic stabilization of the 7s orbital and destabilization of the 6d orbital, the Cn and Cn ions are predicted to give up 6d electrons instead of 7s electrons, which is the opposite of the behavior of its lighter homologues.\n\nIn addition to the relativistic contraction and binding of the 7s subshell, the 6d orbital is expected to be destabilized due to spin-orbit coupling, making it behave similarly to the 7s orbital in terms of size, shape, and energy. Calculations in 2007 expected that copernicium may therefore be a semiconductor with a band gap of around 0.2 eV, crystallizing in the hexagonal close-packed crystal structure. However, calculations in 2017 and 2018 suggested that copernicium should be a noble metal at standard conditions with a body-centered cubic crystal structure: it should hence have no band gap, like mercury, although the density of states at the Fermi level is expected to be lower for copernicium than for mercury. Like mercury, radon, and flerovium, but not oganesson (eka-radon), copernicium is calculated to have no electron affinity.\n\nInterest in copernicium's chemistry was sparked by predictions that it would have the largest relativistic effects in the whole of period 7 and group 12, and indeed among all 118 known elements. Copernicium is expected to have the ground state electron configuration [Rn]5f6d7s and thus should belong to group 12 of the periodic table, according to the Aufbau principle. As such, it should behave as the heavier homologue of mercury and form strong binary compounds with noble metals like gold. Experiments probing the reactivity of copernicium have focused on the adsorption of atoms of element 112 onto a gold surface held at varying temperatures, in order to calculate an adsorption enthalpy. Owing to relativistic stabilization of the 7s electrons, copernicium shows radon-like properties. Experiments were performed with the simultaneous formation of mercury and radon radioisotopes, allowing a comparison of adsorption characteristics.\n\nThe first chemical experiments on copernicium were conducted using the U(Ca,3n)Cn reaction. Detection was by spontaneous fission of the claimed parent isotope with half-life of 5 minutes. Analysis of the data indicated that copernicium was more volatile than mercury and had noble gas properties. However, the confusion regarding the synthesis of copernicium-283 has cast some doubt on these experimental results. Given this uncertainty, between April–May 2006 at the JINR, a FLNR–PSI team conducted experiments probing the synthesis of this isotope as a daughter in the nuclear reaction Pu(Ca,3n)Fl. (The Pu + Ca fusion reaction has a slightly larger cross-section than the U + Ca reaction, so that the best way to produce copernicium for chemical experimentation is as an overshoot product as the daughter of flerovium.) In this experiment, two atoms of copernicium-283 were unambiguously identified and the adsorption properties indicated that copernicium is a more volatile homologue of mercury, due to formation of a weak metal-metal bond with gold, placing it firmly in group 12. This agrees with general indications from relativistic calculations that copernicium is \"more or less\" homologous to mercury.\n\nIn April 2007, this experiment was repeated and a further three atoms of copernicium-283 were positively identified. The adsorption property was confirmed and indicated that copernicium has adsorption properties completely in agreement with being the heaviest member of group 12. These experiments also allowed the first experimental estimation of copernicium's boiling point: 84 °C, so that it may be a gas at standard conditions.\n\nBecause the lighter group 12 elements often occur as chalcogenide ores, experiments were conducted in 2015 to attempt to deposit copernicium atoms on a selenium surface to form copernicium selenide, CnSe. Reaction of copernicium atoms with trigonal selenium to form a selenide were observed, with Δ\"H\"(t-Se) > 48 kJ/mol, with the kinetic hindrance towards selenide formation being lower for copernicium than for mercury. This was unexpected as the stability of the group 12 selenides tends to decrease down the group from ZnSe to HgSe, while it increases down the group for the group 14 selenides from GeSe to PbSe.\n\n\n"}
{"id": "3595959", "url": "https://en.wikipedia.org/wiki?curid=3595959", "title": "Crimson Dawn", "text": "Crimson Dawn\n\nThe Crimson Dawn is a fictional mystical pocket dimension appearing in American comic books published by Marvel Comics. The dimension is depicted as existing beyond the shadows of the world and part of the Marvel Universe. Its first reference was in \"Uncanny X-Men #330\" (March 1996) where it was searched for the life-giving elixir called Ebony Vain.\n\nWhen Psylocke was left in a coma after being fatally wounded by Sabertooth, Archangel and Wolverine decided to journey to the realm of the Crimson Dawn. Their intention was to use the Elixer called the Ebony Vein to save Psylocke's life. Logan and Archangel went to New York’s China Town to find the Crimson Dawn, and Dr. Strange, sensing their intent, joined their quest, which brought them to a Chinese restaurant. Inside they met Gomurr, the ancient, an old acquaintance of Logan. The access to the Crimson Dawn was hidden in the mouth of a huge paper dragon, and their way was blocked by several Undercloaks, ninjas with the ability to travel through shadows, but the four allies got through. Face to face with Tar, the elixir’s Proctor, Gomurr kneeled and begged for them to take some of it, but Tar refused, rather annoyed that he was only visited when someone needed something of him. He spirited Gomurr away, but while Logan kept Tar busy, Dr. Strange and Archangel approached the Ebon Vein. Actually the Elixir could only be used on people present, but Strange reached into Archangel’s heart and pulled out the part of Psylocke’s soul that she had entrusted to him, resembled by a beautiful figurine. As he shoved the icon of their love into the glowing sphere containing the Crimson Dawn, back in Westchester, Betsy’s life signs stabilized and as a result Psylocke obtained a scarlet mark over her left eye. Tar angrily expelled all three heroes from his realm and they found themselves back on Earth, along with Gomurr.\n\nOver the following months it became quite clear that the experience changed Betsy more than on a skin-deep level. She grew much colder and distanced herself from her lover and their friends. Even when she manifested the ability to shadow-port, Psylocke didn’t bother much nor tell anyone, one day startling the X-Men by teleporting into the mansion without warning. Just like them, Betsy understood that something was amiss with her, but she couldn’t bring herself to talk with anyone about her concerns. When she found herself under attack by a group of Undercloaks, she managed to fight them off all on her own, and did keep the battle secret, not even telling Warren despite the dark warriors having attacked her in his penthouse.\n\nLater, it was revealed that their actions had also weakened the dimensional walls which allowed those who have been trapped in the Crimson Dawn, such as the Dragons of the Crimson Dawn, a group of monks from the last years of the Ming Dynasty, to escape the mystical realm. Journeying to the Wildways, another Magical dimension and the home of Spiral when not serving Mojo, they needed her help to teleport them to Earth, but when Spiral refused to aid them, she was threatened with torture that was even beyond her Body Shoppe’s capabilities and she reluctantly accepted to join them and received the mark of the Crimson Dawn. Actually Spiral thought she could outsmart the Dragons at one point, but the Crimson Dawn’s influence was too strong and she had no choice but to release them on Earth, namely in London, where they attacked by surprise Captain Britain. They took him prisoner to their native Hong Kong, where they wanted to channel his powers into breaching the dimensional wall to the Crimson Dawn, thus enslaving Earth.\n\nHowever, Spiral alerted Excalibur to the Dragons’ plan, and teleported the team to Hong Kong. Another one trying to spoil the plan was Xiandu’s spirit, a monk with ties to the Dragons of the Crimson Dawn, who told Meggan, the only one to sense him, that like him the Dragons of the Crimson Dawn were from the time China was overrun by the barbarian Li-Tzu Ch’eng. They had prayed that the kingdom would be spared from the endless bloodshed, but the Ming dynasty ended anyway. Frustrated that their prayers hadn’t been answered, Xiandu and three of his friends, Ra’al, Barak and A’yin, went down the darkest road they would ever travel. They sought out the \"Covenant of the Ebon Vein\", and were inducted on the promise that China would be freed from its oppressors. The four of them tasted the life-giving elixir and were given the scarlet mark over their left eyes, however, Xiandu underestimated the might of the elixir and during the night, A'yin, Barak and Ra'al were corrupted by its powers and murdered Xiandu. By killing Xiandu they were transformed into the Dragons of the Crimson Dawn, and Xiandu would roam Earth as a spirit for centuries to come, awaiting for their inevitable escape from the Crimson realm. The spirit revealed to Meggan that not only her abilities but also her love for Brain was needed to rescue him and stop the two worlds from merging into one. While the rest of Excalibur temporarily held the Dragons of the Crimson Dawn at bay, Meggan helped Brian redirect the power flow, releasing it safely into the atmosphere. While Brian survived, he was Captain Britain no more, his powers gone. The Dragons left by melting into the shadows, vowing that one day they would succeed and rule Earth.\n\nAnother creature to break through from the Crimson Dawn, was the demon Kuragari. He hunted down Tar, and magically wrested control over Tar’s servants from him, only proceeding to kill him to become the next Proctor of the Crimson Dawn. Gomurr found his old rival’s body, and gave him a decent burial. Knowing that Kuragari would come after Psylocke next, he tried to warn her and Archangel, but he happened to be too late. In the meantime, the two X-Men had once again been attacked by Undercloaks, this time leaving behind a strange ring. By the time Gomurr arrived, Betsy, somehow attracted to the piece of jewelry, was already wearing it on her finger, thus allowing Kuragari to abduct her to the Crimson Dawn realm. Gomurr also revealed to Warren that Kuragari was the one to send the Dragons of the Crimson Dawn in the first place.\n\nApparently one can not take from the elixir without giving something in return, and as Psylocke’s debt had never been paid, Kuragari now thought to force her into becoming his bride. Psylocke tried to resist as hard as she could, but by the time Archangel arrived, sent after her by Gomurr, Betsy had already been transformed into a shadow creature. While Betsy left with Kuragari, Warren had to face an entire army of Undercloaks. Back on Earth, Tar’s spirit visited Gomurr, thanking him for the burial and explaining that their past years of enmity had only served one purpose – training lessons for Gomurr to become the rightful next Proctor.\n\nAccepting the mantle passed on to him, Gomurr too crossed over to the Crimson Realm and helped Archangel. In order to release Betsy from the Dawn’s hold, Warren sacrificed an unknown amount of his own life-force to pay the debt. Now free of Kuragari’s influence, Betsy’s darkened skin vanished and together they managed to drive Kuragari away. While the lovers returned home, Gomurr remained behind in his new home.\n\nFurther changes awaited Psylocke, as when she found herself tricked by the Shadow King into creating a psionic energy wave that temporarily disabled all psionic abilities on a world-wide scale, she manifested another ability thanks to the Crimson Dawn. Actually the Shadow King had torn apart Betsy’s astral self, leaving her twisted and disfigured. Betsy, though, rebuilt herself in a new black form and found that this shadow astral self was nearly impossible to be detected, eventually defeating the Shadow King.\n\n"}
{"id": "1266658", "url": "https://en.wikipedia.org/wiki?curid=1266658", "title": "Crystallization", "text": "Crystallization\n\nCrystallization is the (natural or artificial) process by which a solid forms, where the atoms or molecules are highly organized into a structure known as a crystal. Some of the ways by which crystals form are precipitating from a solution, freezing, or more rarely deposition directly from a gas. Attributes of the resulting crystal depend largely on factors such as temperature, air pressure, and in the case of liquid crystals, time of fluid evaporation.\n\nCrystallization occurs in two major steps. The first is nucleation, the appearance of a crystalline phase from either a supercooled liquid or a supersaturated solvent. The second step is known as crystal growth, which is the increase in the size of particles and leads to a crystal state. An important feature of this step is that loose particles form layers at the crystal's surface lodge themselves into open inconsistencies such as pores, cracks, etc.\n\nThe majority of minerals and organic molecules crystallize easily, and the resulting crystals are generally of good quality, i.e. without visible defects. However, larger biochemical particles, like proteins, are often difficult to crystallize. The ease with which molecules will crystallize strongly depends on the intensity of either atomic forces (in the case of mineral substances), intermolecular forces (organic and biochemical substances) or intramolecular forces (biochemical substances).\n\nCrystallization is also a chemical solid–liquid separation technique, in which mass transfer of a solute from the liquid solution to a pure solid crystalline phase occurs. In chemical engineering, crystallization occurs in a crystallizer. Crystallization is therefore related to precipitation, although the result is not amorphous or disordered, but a crystal.\n\nThe crystallization process consists of two major events, \"nucleation\" and \"crystal growth\" which are driven by thermodynamic properties as well as chemical properties.\n\nIn crystallization \"Nucleation\" is the step where the solute molecules or atoms dispersed in the solvent start to gather into clusters, on the microscopic scale (elevating solute concentration in a small region), that become stable under the current operating conditions. These stable clusters constitute the nuclei. Therefore, the clusters need to reach a critical size in order to become stable nuclei. Such critical size is dictated by many different factors (temperature, supersaturation, etc.). It is at the stage of nucleation that the atoms or molecules arrange in a defined and periodic manner that defines the crystal structure — note that \"crystal structure\" is a special term that refers to the relative arrangement of the atoms or molecules, not the macroscopic properties of the crystal (size and shape), although those are a result of the internal crystal structure.\n\nThe \"crystal growth\" is the subsequent size increase of the nuclei that succeed in achieving the critical cluster size. Crystal growth is a dynamic process occurring in equilibrium where solute molecules or atoms precipitate out of solution, and dissolve back into solution. Supersaturation is one of the driving forces of crystallization, as the solubility of a species is an equilibrium process quantified by K. Depending upon the conditions, either nucleation or growth may be predominant over the other, dictating crystal size.\n\nMany compounds have the ability to crystallize with some having different crystal structures, a phenomenon called polymorphism. Each polymorph is in fact a different thermodynamic solid state and crystal polymorphs of the same compound exhibit different physical properties, such as dissolution rate, shape (angles between facets and facet growth rates), melting point, etc. For this reason, polymorphism is of major importance in industrial manufacture of crystalline products. Additionally, crystal phases can sometimes be interconverted by varying factors such as temperature.\n\nThere are many examples of natural process that involve crystallization.\n\nGeological time scale process examples include:\n\nHuman time scale process examples include:\n\nCrystal formation can be divided into two types, where the first type of crystals are composed of a cation and anion, also known as a salt, such as sodium acetate. The second type of crystals are composed of uncharged species, for example menthol.\n\nCrystal formation can be achieved by various methods, such as: cooling, evaporation, addition of a second solvent to reduce the solubility of the solute (technique known as antisolvent or drown-out), solvent layering, sublimation, changing the cation or anion, as well as other methods.\n\nThe formation of a supersaturated solution does not guarantee crystal formation, and often a seed crystal or scratching the glass is required to form nucleation sites.\n\nA typical laboratory technique for crystal formation is to dissolve the solid in a solution in which it is partially soluble, usually at high temperatures to obtain supersaturation. The hot mixture is then filtered to remove any insoluble impurities. The filtrate is allowed to slowly cool. Crystals that form are then filtered and washed with a solvent in which they are not soluble, but is miscible with the mother liquor. The process is then repeated to increase the purity in a technique known as recrystallization.\n\nFor biological molecules in which the solvent channels continue to be present to retain the three dimensional structure intact, microbatch crystallization under oil and vapor diffusion methods have been the common methods.\n\nEquipment for the main industrial processes for crystallization.\n\n\nThe crystallization process appears to violate the second principle of thermodynamics. Whereas most processes that yield more orderly results are achieved by applying heat, crystals usually form at lower temperatures—especially by supercooling. However, due to the release of the heat of fusion during crystallization, the entropy of the universe increases, thus this principle remains unaltered.\n\nThe molecules within a pure, \"perfect crystal\", when heated by an external source, will become liquid. This occurs at a sharply defined temperature (different for each type of crystal). As it liquifies, the complicated architecture of the crystal collapses. Melting occurs because the entropy (\"S\") gain in the system by spatial randomization of the molecules has overcome the enthalpy (\"H\") loss due to breaking the crystal packing forces:\nRegarding crystals, there are no exceptions to this rule. Similarly, when the molten crystal is cooled, the molecules will return to their crystalline form once the temperature falls beyond the turning point. This is because the thermal randomization of the surroundings compensates for the loss of entropy that results from the reordering of molecules within the system. Such liquids that crystallize on cooling are the exception rather than the rule.\n\nThe nature of a crystallization process is governed by both thermodynamic and kinetic factors, which can make it highly variable and difficult to control. Factors such as impurity level, mixing regime, vessel design, and cooling profile can have a major impact on the size, number, and shape of crystals produced.\nAs mentioned above, a crystal is formed following a well-defined pattern, or structure, dictated by forces acting at the molecular level. As a consequence, during its formation process the crystal is in an environment where the solute concentration reaches a certain critical value, before changing status. Solid formation, impossible below the solubility threshold at the given temperature and pressure conditions, may then take place at a concentration higher than the theoretical solubility level. The difference between the actual value of the solute concentration at the crystallization limit and the theoretical (static) solubility threshold is called supersaturation and is a fundamental factor in crystallization.\n\nNucleation is the initiation of a phase change in a small region, such as the formation of a solid crystal from a liquid solution. It is a consequence of rapid local fluctuations on a molecular scale in a homogeneous phase that is in a state of metastable equilibrium. Total nucleation is the sum effect of two categories of nucleation – primary and secondary.\n\nPrimary nucleation is the initial formation of a crystal where there are no other crystals present or where, if there are crystals present in the system, they do not have any influence on the process. This can occur in two conditions. The first is homogeneous nucleation, which is nucleation that is not influenced in any way by solids. These solids include the walls of the crystallizer vessel and particles of any foreign substance. The second category, then, is heterogeneous nucleation. This occurs when solid particles of foreign substances cause an increase in the rate of nucleation that would otherwise not be seen without the existence of these foreign particles. Homogeneous nucleation rarely occurs in practice due to the high energy necessary to begin nucleation without a solid surface to catalyse the nucleation.\n\nPrimary nucleation (both homogeneous and heterogeneous) has been modelled with the following:\n\nwhere\n\nSecondary nucleation is the formation of nuclei attributable to the influence of the existing microscopic crystals in the magma. Simply put, secondary nucleation is when crystal growth is initiated with contact of other existing crystals or \"seeds\". The first type of known secondary crystallization is attributable to fluid shear, the other due to collisions between already existing crystals with either a solid surface of the crystallizer or with other crystals themselves. Fluid-shear nucleation occurs when liquid travels across a crystal at a high speed, sweeping away nuclei that would otherwise be incorporated into a crystal, causing the swept-away nuclei to become new crystals. Contact nucleation has been found to be the most effective and common method for nucleation. The benefits include the following:\n\nThe following model, although somewhat simplified, is often used to model secondary nucleation:\nwhere\n\nOnce the first small crystal, the nucleus, forms it acts as a convergence point (if unstable due to supersaturation) for molecules of solute touching – or adjacent to – the crystal so that it increases its own dimension in successive layers. The pattern of growth resembles the rings of an onion, as shown in the picture, where each colour indicates the same mass of solute; this mass creates increasingly thin layers due to the increasing surface area of the growing crystal. The supersaturated solute mass the original nucleus may \"capture\" in a time unit is called the \"growth rate\" expressed in kg/(m*h), and is a constant specific to the process. Growth rate is influenced by several physical factors, such as surface tension of solution, pressure, temperature, relative crystal velocity in the solution, Reynolds number, and so forth.\n\nThe main values to control are therefore:\n\nThe first value is a consequence of the physical characteristics of the solution, while the others define a difference between a well- and poorly designed crystallizer.\n\nThe appearance and size range of a crystalline product is extremely important in crystallization. If further processing of the crystals is desired, large crystals with uniform size are important for washing, filtering, transportation, and storage, because large crystals are easier to filter out of a solution than small crystals. Also, larger crystals have a smaller surface area to volume ratio, leading to a higher purity. This higher purity is due to less retention of mother liquor which contains impurities, and a smaller loss of yield when the crystals are washed to remove the mother liquor. The theoretical crystal size distribution can be estimated as a function of operating conditions with a fairly complicated mathematical process called population balance theory (using population balance equations).\n\nSome of the important factors influencing solubility are:\nSo one may identify two main families of crystallization processes:\n\nThis division is not really clear-cut, since hybrid systems exist, where cooling is performed through evaporation, thus obtaining at the same time a concentration of the solution.\n\nA crystallization process often referred to in chemical engineering is the fractional crystallization. This is not a different process, rather a special application of one (or both) of the above.\n\nMost chemical compounds, dissolved in most solvents, show the so-called \"direct\" solubility that is, the solubility threshold increases with temperature.\n\nSo, whenever the conditions are favourable, crystal formation results from simply cooling the solution. Here \"cooling\" is a relative term: austenite crystals in a steel form well above 1000 °C. An example of this crystallization process is the production of Glauber's salt, a crystalline form of sodium sulfate. In the diagram, where equilibrium temperature is on the x-axis and equilibrium concentration (as mass percent of solute in saturated solution) in y-axis, it is clear that sulfate solubility quickly decreases below 32.5 °C. Assuming a saturated solution at 30 °C, by cooling it to 0 °C (note that this is possible thanks to the freezing-point depression), the precipitation of a mass of sulfate occurs corresponding to the change in solubility from 29% (equilibrium value at 30 °C) to approximately 4.5% (at 0 °C) – actually a larger crystal mass is precipitated, since sulfate entrains hydration water, and this has the side effect of increasing the final concentration.\n\nThere are limitations in the use of cooling crystallization:\n\nThe simplest cooling crystallizers are tanks provided with a mixer for internal circulation, where temperature decrease is obtained by heat exchange with an intermediate fluid circulating in a jacket. These simple machines are used in batch processes, as in processing of pharmaceuticals and are prone to scaling. Batch processes normally provide a relatively variable quality of product along the batch.\n\nThe \"Swenson-Walker\" crystallizer is a model, specifically conceived by Swenson Co. around 1920, having a semicylindric horizontal hollow trough in which a hollow screw conveyor or some hollow discs, in which a refrigerating fluid is circulated, plunge during rotation on a longitudinal axis. The refrigerating fluid is sometimes also circulated in a jacket around the trough. Crystals precipitate on the cold surfaces of the screw/discs, from which they are removed by scrapers and settle on the bottom of the trough. The screw, if provided, pushes the slurry towards a discharge port.\n\nA common practice is to cool the solutions by flash evaporation: when a liquid at a given T temperature is transferred in a chamber at a pressure P such that the liquid saturation temperature T at P is lower than T, the liquid will release heat according to the temperature difference and a quantity of solvent, whose total latent heat of vaporization equals the difference in enthalpy. In simple words, the liquid is cooled by evaporating a part of it.\n\nIn the sugar industry, vertical cooling crystallizers are used to exhaust the molasses in the last crystallization stage downstream of vacuum pans, prior to centrifugation. The massecuite enters the crystallizers at the top, and cooling water is pumped through pipes in counterflow.\n\nAnother option is to obtain, at an approximately constant temperature, the precipitation of the crystals by increasing the solute concentration above the solubility threshold. To obtain this, the solute/solvent mass ratio is increased using the technique of evaporation. This process is insensitive to change in temperature (as long as hydration state remains unchanged).\n\nAll considerations on control of crystallization parameters are the same as for the cooling models.\n\nMost industrial crystallizers are of the evaporative type, such as the very large sodium chloride and sucrose units, whose production accounts for more than 50% of the total world production of crystals. The most common type is the \"forced circulation\" (FC) model (see evaporator). A pumping device (a pump or an axial flow mixer) keeps the crystal slurry in homogeneous suspension throughout the tank, including the exchange surfaces; by controlling pump flow, control of the contact time of the crystal mass with the supersaturated solution is achieved, together with reasonable velocities at the exchange surfaces. The Oslo, mentioned above, is a refining of the evaporative forced circulation crystallizer, now equipped with a large crystals settling zone to increase the retention time (usually low in the FC) and to roughly separate heavy slurry zones from clear liquid. Evaporative crystallizers tend to yield larger average crystal size and narrows the crystal size distribution curve.\n\nWhichever the form of the crystallizer, to achieve an effective process control it is important to control the retention time and the crystal mass, to obtain the optimum conditions in terms of crystal specific surface and the fastest possible growth. This is achieved by a separation – to put it simply – of the crystals from the liquid mass, in order to manage the two flows in a different way. The practical way is to perform a gravity settling to be able to extract (and possibly recycle separately) the (almost) clear liquid, while managing the mass flow around the crystallizer to obtain a precise slurry density elsewhere. A typical example is the DTB (\"Draft Tube and Baffle\") crystallizer, an idea of Richard Chisum Bennett (a Swenson engineer and later President of Swenson) at the end of the 1950s. The DTB crystallizer (see images) has an internal circulator, typically an axial flow mixer – yellow – pushing upwards in a draft tube while outside the crystallizer there is a settling area in an annulus; in it the exhaust solution moves upwards at a very low velocity, so that large crystals settle – and return to the main circulation – while only the fines, below a given grain size are extracted and eventually destroyed by increasing or decreasing temperature, thus creating additional supersaturation. A quasi-perfect control of all parameters is achieved as DTF crystallizers offer superior control over crystal size and characteristics. This crystallizer, and the derivative models (Krystal, CSC, etc.) could be the ultimate solution if not for a major limitation in the evaporative capacity, due to the limited diameter of the vapour head and the relatively low external circulation not allowing large amounts of energy to be supplied to the system.\n\n\n"}
{"id": "569315", "url": "https://en.wikipedia.org/wiki?curid=569315", "title": "Dimethyl ether", "text": "Dimethyl ether\n\nDimethyl ether (DME, also known as methoxymethane) is the organic compound with the formula CHOCH, simplified to CHO. The simplest ether, it is a colorless gas that is a useful precursor to other organic compounds and an aerosol propellant that is currently being demonstrated for use in a variety of fuel applications. It is an isomer of ethanol.\n\nApproximately 50,000 tons were produced in 1985 in Western Europe by dehydration of methanol: \nThe required methanol is obtained from synthesis gas (syngas). Other possible improvements call for a dual catalyst system that permits both methanol synthesis and dehydration in the same process unit, with no methanol isolation and purification.\nBoth the one-step and two-step processes above are commercially available. The two-step process is relatively simple and start-up costs are relatively low. A one-step liquid-phase process is in development.\n\nDimethyl ether is a synthetic second generation biofuel (BioDME), which can be produced from lignocellulosic biomass. The EU is considering BioDME in its potential biofuel mix in 2030; It can also be made from biogas or methane from animal, food, and agricultural waste, or even from shale gas or natural gas.\n\nThe Volvo Group is the coordinator for the European Community Seventh Framework Programme project BioDME where Chemrec's BioDME pilot plant is based on black liquor gasification in Piteå, Sweden.\n\nThe largest use of dimethyl ether is as the feedstock for the production of the methylating agent, dimethyl sulfate, which entails its reaction with sulfur trioxide:\nDimethyl ether can also be converted into acetic acid using carbonylation technology related to the Monsanto acetic acid process:\n\nDimethyl ether is a low-temperature solvent and extraction agent, applicable to specialised laboratory procedures. Its usefulness is limited by its low boiling point (), but the same property facilitates its removal from reaction mixtures. Dimethyl ether is the precursor to the useful alkylating agent, trimethyloxonium tetrafluoroborate.\n\nA mixture of dimethyl ether and propane is used in some over-the-counter \"freeze spray\" products to treat warts, by freezing them. In this role, it has supplanted halocarbon compounds (Freon).\n\nDimethyl ether is also a component of certain high temperature \"MAP-plus\" blowtorch gas blends, supplanting the use of methyl acetylene and propadiene mixtures.\n\nDimethyl ether is also used as a propellant in aerosol products. Such products include hair spray, bug spray and some aerosol glue products.\n\nA potentially major use of dimethyl ether is as substitute for propane in LPG used as fuel in household and industry.\n\nIt is also a promising fuel in diesel engines, and gas turbines. For diesel engines, an advantage is the high cetane number of 55, compared to that of diesel fuel from petroleum, which is 40–53. Only moderate modifications are needed to convert a diesel engine to burn dimethyl ether. The simplicity of this short carbon chain compound leads during combustion to very low emissions of particulate matter. For these reasons as well as being sulfur-free, dimethyl ether meets even the most stringent emission regulations in Europe (EURO5), U.S. (U.S. 2010), and Japan (2009 Japan).\n\nAt the European Shell Eco Marathon, an unofficial World Championship for mileage, vehicle running on 100% dimethyl ether drove 589 km/liter, fuel equivalent to gasoline with a 50 cc 2-stroke engine. As well as winning they beat the old standing record of 306 km/liter, set by the same team in 2007.\n\nDimethyl ether is a refrigerant with ASHRAE refrigerant designation R-E170. It is also used in refrigerant blends with e.g. ammonia, carbon dioxide, butane and propene. \n\nUnlike other alkyl ethers, dimethyl ether resists autoxidation. Dimethyl ether is also relatively non-toxic, although it is highly flammable.\n\n"}
{"id": "34661896", "url": "https://en.wikipedia.org/wiki?curid=34661896", "title": "Douglas Archibald Campbell", "text": "Douglas Archibald Campbell\n\nDouglas Archibald Campbell (13 December 1906–7 March 1969) was a New Zealand teacher and soil conservator. He was born in Dunedin, New Zealand on 13 December 1906.\n"}
{"id": "475276", "url": "https://en.wikipedia.org/wiki?curid=475276", "title": "Downforce", "text": "Downforce\n\nDownforce is a downwards thrust created by the aerodynamic characteristics of a car. The purpose of downforce is to allow a car to travel faster through a corner by increasing the vertical force on the tires, thus creating more grip\".\n\nThe same principle that allows an airplane to rise off the ground by creating lift from its wings is used in reverse to apply force that presses the race car against the surface of the track. This effect is referred to as \"aerodynamic grip\" and is distinguished from \"mechanical grip\", which is a function of the car's mass, tires, and suspension. The creation of downforce by passive devices can be achieved only at the cost of increased aerodynamic drag (or friction), and the optimum setup is almost always a compromise between the two. The aerodynamic setup for a car can vary considerably between race tracks, depending on the length of the straights and the types of corners. Because it is a function of the flow of air over and under the car, downforce increases with the square of the car's speed and requires a certain minimum speed in order to produce a significant effect. Some cars have had rather unstable aerodynamics, such that a minor change in angle of attack or height of the vehicle can cause large changes in downforce. In the very worst cases this can cause the car to experience lift, not downforce; for example, by passing over a bump on a track or slipstreaming over a crest: this could have some disastrous consequences, such as Peter Dumbreck's Mercedes-Benz CLR in the 1999 24 Hours of Le Mans, which flipped spectacularly after closely following a competitor car over a hump.\n\nTwo primary components of a racing car can be used to create downforce when the car is travelling at racing speed:\n\n\nMost racing formulae have a ban on aerodynamic devices that can be adjusted during a race, except during pit stops.\n\nThe downforce exerted by a wing is usually expressed as a function of its lift coefficient: \n\nwhere:\n\nIn certain ranges of operating conditions and when the wing is not stalled, the lift coefficient has a constant value: the downforce is then proportional to the square of airspeed.\n\nIn aerodynamics, it is usual to use the top-view projected area of the wing as a reference surface to define the lift coefficient.\n\nThe rounded and tapered shape of the top of the car is designed to slice through the air and minimize wind resistance. Detailed pieces of bodywork on top of the car can be added to allow a smooth flow of air to reach the downforce-creating elements (i.e., wings or spoilers, and underbody tunnels).\n\nThe overall shape of a street car resembles an airplane wing. Almost all street cars have aerodynamic lift as a result of this shape. There are many techniques that are used to counterbalance a street car. Looking at the profile of most street cars, the front bumper has the lowest ground clearance followed by the section between the front and rear tires, and followed yet by a rear bumper usually with the highest clearance. Using this method, the air flowing under the front bumper will be constricted to a lower cross sectional area, and thus achieve a lower pressure. Additional downforce comes from the rake (or angle) of the vehicles' body, which directs the underside air up and creates a downward force, and increases the pressure on top of the car because the airflow direction comes closer to perpendicular to the surface. Volume does not affect the air pressure because it is not an enclosed volume, despite the common misconception. Race cars will exemplify this effect by adding a rear diffuser to accelerate air under the car in front of the diffuser, and raise the air pressure behind it to lessen the car's wake. Other aerodynamic components that can be found on the underside to improve downforce and/or reduce drag, include splitters and vortex generators.\n\nSome cars, such as the DeltaWing, do not have wings, and generate all of their downforce through their body.\n\nThe amount of downforce created by the wings or spoilers on a car is dependent primarily on two things:\n\nA larger surface area creates greater downforce and greater drag. The aspect ratio is the width of the airfoil divided by its chord. If the wing is not rectangular, aspect ratio is written AR=b/s, where AR=aspect ratio, b=span, and s=wing area. Also, a greater angle of attack (or tilt) of the wing or spoiler, creates more downforce, which puts more pressure on the rear wheels and creates more drag.\n\nThe function of the airfoils at the front of the car is twofold. They create downforce that enhances the grip of the front tires, while also optimizing (or minimizing disturbance to) the flow of air to the rest of the car. The front wings on an open-wheeled car undergo constant modification as data is gathered from race to race, and are customized for every characteristic of a particular circuit (see top photos). In most series, the wings are even designed for adjustment during the race itself when the car is serviced.\n\nThe flow of air at the rear of the car is affected by the front wings, front wheels, mirrors, driver's helmet, side pods and exhaust. This causes the rear wing to be less aerodynamically efficient than the front wing, Yet, because it must generate more than twice as much downforce as the front wings in order to maintain the handling to balance the car, the rear wing typically has a much larger aspect ratio, and often uses two or more elements to compound the amount of downforce created (see photo at left). Like the front wings, each of these elements can often be adjusted when the car is serviced, before or even during a race, and are the object of constant attention and modification.\n\nPartly as a consequence of rules aimed at reducing downforce from the front and rear wings of F1 cars, several teams have sought to find other places to position wings. Small wings mounted on the rear of the cars' sidepods began to appear in mid-1994, and were virtually standard on all F1 cars in one form or another, until all such devices were outlawed in 2009. Other wings have sprung up in various other places about the car, but these modifications are usually only used at circuits where downforce is most sought, particularly the twisty Hungary and Monaco racetracks.\n\nThe 1995 McLaren Mercedes MP4/10 was one of the first cars to feature a \"midwing\", using a loophole in the regulations to mount a wing on top of the engine cover. This arrangement has since been used by every team on the grid at one time or another, and in the 2007 Monaco Grand Prix all but two teams used them. These midwings are not to be confused either with the roll-hoop mounted cameras which each car carries as standard in all races, or with the bull-horn shaped flow controllers first used by McLaren and since by BMW Sauber, whose primary function is to smooth and redirect the airflow in order to make the rear wing more effective rather than to generate downforce themselves.\n\nA variation on this theme was \"X-wings\", high wings mounted on the front of the sidepods which used a similar loophole to midwings. These were first used by Tyrrell in 1997, and were last used in the 1998 San Marino Grand Prix, by which time Ferrari, Sauber, Jordan and others had used such an arrangement. However it was decided they would have to be banned in view of the obstruction they caused during refueling and the risk they posed to the driver should a car roll over. (It is rumored that Bernie Ecclestone saw them as being too ugly on television and therefore had them banned).\n\nVarious other extra wings have been tried from time to time, but nowadays it is more common for teams to seek to improve the performance of the front and rear wings by the use of various flow controllers such as the afore-mentioned \"bull-horns\" used by McLaren.\n\n\n\n"}
{"id": "23264409", "url": "https://en.wikipedia.org/wiki?curid=23264409", "title": "Dynamic structure factor", "text": "Dynamic structure factor\n\nIn condensed matter physics, the dynamic structure factor is a mathematical function that contains information about inter-particle correlations and their time evolution. It is a generalization of the structure factor that considers correlations in both space \"and\" time. Experimentally, it can be accessed most directly by inelastic neutron scattering or X-ray Raman scattering.\n\nThe dynamic structure factor is most often denoted formula_1, where formula_2 (sometimes formula_3) is a wave vector (or wave number for isotropic materials), and formula_4 a frequency (sometimes stated as energy, formula_5). It is defined as:\n\nHere formula_7, is called the intermediate scattering function and can be measured by neutron spin echo spectroscopy. The intermediate scattering function is the spatial Fourier transform of the van Hove function formula_8:\n\nThus we see that the dynamical structure factor is the spatial \"and\" temporal Fourier transform of van Hove's time-dependent pair correlation function. It can be shown (see below), that the intermediate scattering function is the correlation function of the Fourier components of the density formula_10: \n\nThe dynamic structure is exactly what is probed in coherent inelastic neutron scattering. The differential cross section is : \n\nwhere formula_13 is the scattering length. \n\nThe van Hove Function for a spatially uniform system containing formula_14 point particles is defined as:\nIt can be rewritten as: \n\nIn an isotropic sample (with scalar \"r\"), \"G(r,t)\" is a time dependent radial distribution function.\n\n"}
{"id": "53668390", "url": "https://en.wikipedia.org/wiki?curid=53668390", "title": "El Arrayán Wind Farm", "text": "El Arrayán Wind Farm\n\nThe 115 MW El Arrayán is the largest wind farm in Chile. At the time of its inauguration in 2014, it was the largest in Latin America.\n\nThe site is approximately 400 km north of Santiago in the coastal town of Ovalle, which is the capital of the Limarí Province, in the Coquimbo Region. The project consists of 50 Siemens 2.3 MW wind turbines. It completed in June 2014 and officially went into service in August 2014. Pattern Energy owns 70% of the facility, which it also operates. Antofagasta Minerals SA owns the remaining 30% minority stake.\n"}
{"id": "9728715", "url": "https://en.wikipedia.org/wiki?curid=9728715", "title": "Energy audit", "text": "Energy audit\n\nAn energy audit is an inspection survey an analysis of energy flows, for energy conservation in a building, process or system to reduce the amount of energy input into the system without negatively affecting the output(s). In commercial and industrial real estate, an energy audit is the first step in identifying opportunities to reduce energy expense and carbon footprints.\n\nWhen the object of study is an occupied building then reducing energy consumption while maintaining or improving human comfort, health and safety are of primary concern. Beyond simply identifying the sources of energy use, an energy audit seeks to prioritize the energy uses according to the greatest to least cost effective opportunities for energy savings.\n\nA \"home energy audit\" is a service where the energy efficiency of a house is evaluated by a person using professional equipment (such as blower doors and infrared cameras), with the aim to suggest the best ways to improve energy efficiency in heating and cooling the house.\n\nAn energy audit of a home may involve recording various characteristics of the building envelope including the walls, ceilings, floors, doors, windows, and skylights. For each of these components the area and resistance to heat flow (R-value) is measured or estimated. The leakage rate or infiltration of air through the building envelope is of concern, both of which are strongly affected by window construction and quality of door seals such as weatherstripping. The goal of this exercise is to quantify the building's overall thermal performance. The audit may also assess the efficiency, physical condition, and programming of mechanical systems such as the heating, ventilation, air conditioning equipment, and thermostat.\n\nA home energy audit may include a written report estimating energy use given local climate criteria, thermostat settings, roof overhang, and solar orientation. This could show energy use for a given time period, say a year, and the impact of any suggested improvements per year. The accuracy of energy estimates are greatly improved when the homeowner's billing history is available showing the quantities of electricity, natural gas, fuel oil, or other energy sources consumed over a one or two-year period.\n\nSome of the greatest effects on energy use are user behavior, climate, and age of the home. An energy audit may therefore include an interview of the homeowners to understand their patterns of use over time. The energy billing history from the local utility company can be calibrated using heating degree day and cooling degree day data obtained from recent, local weather data in combination with the thermal energy model of the building. Advances in computer-based thermal modeling can take into account many variables affecting energy use.\n\nA home energy audit is often used to identify cost effective ways to improve the comfort and efficiency of buildings. In addition, homes may qualify for energy efficiency grants from central government.\n\nRecently, the improvement of smartphone technology has enabled homeowners to perform relatively sophisticated energy audits of their own homes. This technique has been identified as a method to accelerate energy efficiency improvements.\n\nIn the United States, this kind of service can often be facilitated by:\n\nUtility companies may provide this service, as well as loans and other incentives to insulate. They also often provide incentives to switch, for example, if you are an oil customer considering switching to natural gas.\n\nWhere to look for insulation recommendations:\n\nResidential energy auditors are accredited by the Building Performance Institute (BPI) or the Residential Energy Services Network (RESNET).\n\nThere are also some simplified tools available, with which a homeowner can quickly assess energy improvement potential. Often these are supplied for free by state agencies or local utilities, who produce a report with estimates of usage by device/area (since they have usage information already). Examples include the Energy Trust of Oregon program and the Seattle Home Resource Profile. Such programs may also include free compact fluorescent lights.\n\nA simple do-it-yourself home energy audit can be performed without using any specialized tools. With an attentive and planned assessment, a homeowner can spot many problems that cause energy losses and make decisions about possible energy efficiency upgrades. During a home energy audit it is important to have a checklist of areas that were inspected as well as problems identified. Once the audit is completed, a plan for suggested actions needs to be developed.\n\nIn New York City, local laws such as Local Law 87 require buildings larger than to have an energy audit once every ten years, as assigned by its parcel number. Energy auditors must be certified to perform this work, although there is no oversight to enforce the rule. Because Local Law 87 requires a licensed Professional Engineer to oversee the work, choosing a well-established engineering firm is the safest route.\n\nThese laws are the results of New York City's PlaNYC to reduce energy used by buildings, which is the greatest source of pollution in New York City. Some engineering firms provide free energy audits for facilities committed to implementing the energy saving measures found.\n\nSince 2002, The Lebanese Center for Energy Conservation (LCEC) initiated a nationwide program on energy audits for medium and large consuming facilities. By the end of 2008, LCEC has financed and supervised more than 100 audits.\n\nLCEC launched an energy audit program to assist Lebanese energy consuming tertiary and public buildings and industrial plants in the management of their energy through this program.\n\nThe long-term objective of LCEC is to create a market for ESCOs, whereby any beneficiary can contact directly a specialized ESCO to conduct an energy audit, implement energy conservation measures and monitor energy saving program according to a standardized energy performance contract.\n\nCurrently, LCEC is helping in the funding of the energy audit study and thus is linking both the beneficiary and the energy audit firm. LCEC also targets the creation of a special fund used for the implementation of the energy conservation measures resulting from the study.\n\nLCEC set a minimum standard for the ESCOs qualifications in Lebanon and published a list of qualified ESCOs on its website.\n\nIncreasingly in the last several decades, industrial energy audits have exploded as the demand to lower increasingly expensive energy costs and move towards a sustainable future have made energy audits greatly important. Their importance is magnified since energy spending is a major expense to industrial companies (energy spending accounts for ~ 10% of the average manufacturer's expenses). This growing trend should only continue as energy costs continue to rise.\n\nWhile the overall concept is similar to a home or residential energy audit, industrial energy audits require a different skillset. Weatherproofing and insulating a house are the main focus of residential energy audits. For industrial applications, it is the HVAC, lighting, and production equipment that use the most energy, and hence are the primary focus of energy audits.\n\nThe term energy audit is commonly used to describe a broad spectrum of energy studies ranging from a quick walk-through of a facility to identify major problem areas to a comprehensive analysis of the implications of alternative energy efficiency measures sufficient to satisfy the financial criteria of sophisticated investors.\nNumerous audit procedures have been developed for non-residential (tertiary) buildings (ASHRAE; IEA-EBC Annex 11; Krarti, 2000). Audit is required to identify the most efficient and cost-effective Energy Conservation Opportunities (ECOs) or Measures (ECMs). Energy conservation opportunities (or measures) can consist in more efficient use or of partial or global replacement of the existing installation.\n\nWhen looking to the existing audit methodologies developed in IEA EBC Annex 11, by ASHRAE and by Krarti (2000), it appears that the main issues of an audit process are:\n\nCommon types/levels of energy audits are distinguished below, although the actual tasks performed and level of effort may vary with the consultant providing services under these broad headings. The only way to ensure that a proposed audit will meet your specific needs is to spell out those requirements in a detailed scope of work. Taking the time to prepare a formal solicitation will also assure the building owner of receiving competitive and comparable proposals.\n\nGenerally, four levels of analysis can be outlined (ASHRAE):\n\nThe impossibility of describing all possible situations that might be encountered during an audit means that it is necessary to find a way of describing what constitutes good, average and bad energy performance across a range of situations. The aim of benchmarking is to answer this question. Benchmarking mainly consists in comparing the measured consumption with reference consumption of other similar buildings or generated by simulation tools to identify excessive or unacceptable running costs. As mentioned before, benchmarking is also necessary to identify buildings presenting interesting energy saving potential.\nAn important issue in benchmarking is the use of performance indexes to characterize the building.\n\nThese indexes can be:\n\nTypically, benchmarks are established based on the energy outlets (loads) within the building and are then further parsed into \"base loads\" and \"weather sensitive loads\". These are established through a simple regression analysis of energy consumption and demand (if metered) correlated to weather (temperature and degree - day) data during the period for which utility data is available. Aggregate base loads will represent as the intercept of this regression and the slope will typically represent the combination of building envelope conduction and infiltration losses less losses or gains from the base loads themselves. For example, while lighting is typically a base load, the heat generated from that lighting must be subtracted from the weather sensitive cooling load derived from the slope to gain an accurate picture of the true contribution of the building envelope on cooling energy use and demand. \n\nThe preliminary audit (alternatively called a simple audit, screening audit or walk-through audit) is the simplest and quickest type of audit. It involves minimal interviews with site-operating personnel, a brief review of facility utility bills and other operating data, and a walk-through of the facility to become familiar with the building operation and to identify any glaring areas of energy waste or inefficiency.\n\nTypically, only major problem areas will be covered during this type of audit. Corrective measures are briefly described, and quick estimates of implementation cost, potential operating cost savings, and simple payback periods are provided. A list of energy conservation measures (ECMs, or energy conservation opportunities, ECOs) requiring further consideration is also provided.\nThis level of detail, while not sufficient for reaching a final decision on implementing proposed measure, is adequate to prioritize energy-efficiency projects and to determine the need for a more detailed audit.\n\nThe general audit (alternatively called a mini-audit, site energy audit or detailed energy audit or complete site energy audit) expands on the preliminary audit described above by collecting more detailed information about facility operation and by performing a more detailed evaluation of energy conservation measures. Utility bills are collected for a 12- to 36-month period to allow the auditor to evaluate the facility's energy demand rate structures and energy usage profiles. If interval meter data is available, the detailed energy profiles that such data makes possible will typically be analyzed for signs of energy waste. Additional metering of specific energy-consuming systems is often performed to supplement utility data. In-depth interviews with facility operating personnel are conducted to provide a better understanding of major energy consuming systems and to gain insight into short- and longer-term energy consumption patterns.\nThis type of audit will be able to identify all energy-conservation measures appropriate for the facility, given its operating parameters. A detailed financial analysis is performed for each measure based on detailed implementation cost estimates, site-specific operating cost savings, and the customer's investment criteria. Sufficient detail is provided to justify project implementation.\nThe evolution of cloud-based energy auditing software platforms is enabling the managers of commercial buildings to collaborate with general and specialty trades contractors in performing general and energy system-specific audits. The benefit of software-enabled collaboration is the ability to identify the full range of energy efficiency options that may be applicable to the specific building under study with \"live time\" cost and benefit estimates supplied by local contractors.\n\nIn most corporate settings, upgrades to a facility's energy infrastructure must compete for capital funding with non-energy-related investments. Both energy and non-energy investments are rated on a single set of financial criteria that generally stress the expected return on investment (ROI). The projected operating savings from the implementation of energy projects must be developed such that they provide a high level of confidence. In fact, investors often demand guaranteed savings.\nThe investment-grade audit expands on the detailed audit described above and relies on a complete engineering study in order to detail technical and economical issues necessary to justify the investment related to the transformations.\n\nA complete audit procedure, very similar to the ones proposed by ASHRAE and Krarti (2000), has been proposed in the frame of the AUDITAC and HARMONAC projects to help in the implementation of the EPB (“Energy Performance of Buildings”) directive in Europe and to fit to the current European market.\n\nThe following procedure proposes to make an intensive use of modern BES tools at each step of the audit process, from benchmarking to detailed audit and financial study:\n\nThe advent of high-resolution thermography has enabled inspectors to identify potential issues within the building envelope by taking a thermal image of the various surfaces of a building. For purposes of an energy audit, the thermographer will analyze the patterns within the surface temperatures to identify heat transfer through convection, radiation, or conduction. It is important to note that the thermography \"only\" identifies \"surface\" temperatures, and analysis must be applied to determine the reasons for the patterns within the surface temperatures. Thermal analysis of a home generally costs between 300 and 600 dollars.\n\nFor those who cannot afford a thermal inspection, it is possible to get a general feel for the heat loss with a non-contact infrared thermometer and several sheets of reflective insulation. The method involves measuring the temperatures on the inside surfaces of several exterior walls to establish baseline temperatures. After this, reflective barrier insulation is taped securely to the walls in by strips and the temperatures are measured in the center of the insulated areas at 1-hour intervals for 12 hours (the reflective barrier is pulled away from the wall to measure the temperature in the center of the area which it has covered). The best manner in which to do this is when the temperature differential (Delta T) between the inside and outside of the structure is at least 40 degrees. A well-insulated wall will commonly change approximately 1 degree per hour if the difference between external and internal temperatures is an average of 40 degrees. A poorly insulated wall can drop as much as 10 degrees in an hour.\n\nWith increases in carbon dioxide emissions or other greenhouse gases, pollution audits are now a prominent factor in most energy audits. Implementing energy efficient technologies help prevent utility generated pollution.\n\nOnline pollution and emission calculators can help approximate the emissions of other prominent air pollutants in addition to carbon dioxide.\n\nPollution audits generally take electricity and heating fuel consumption numbers over a two-year period and provide approximations for carbon dioxide, VOCs, nitrous oxides, carbon monoxide, sulfur dioxide, mercury, cadmium, lead, mercury compounds, cadmium compounds and lead compounds.\n\nEnergy audits initially became popular in response to the energy crisis of 1973 and later years. Interest in energy audits has recently increased as a result of growing understanding of human impact upon global warming and climate change. Energy audits are also popular due to financial incentives for homeowners.\n\n\n\n\n"}
{"id": "28744332", "url": "https://en.wikipedia.org/wiki?curid=28744332", "title": "Fangchenggang Nuclear Power Plant", "text": "Fangchenggang Nuclear Power Plant\n\nFangchenggang Nuclear Power Plant is a nuclear power plant under construction in Fangchenggang, near Hongshacun Village ( 红沙村 ), autonomous region of Guangxi (Guangxi Zhuang Autonomous Region) in the People's Republic of China. A total of six reactors are planned to operate at the Fangchenggang site. Units 1 and 2 are both CPR-1000s, units 3–6 are planned to be based on Hualong One reactors. Fangchenggang 3 and 4 will be the reference plant for the proposed Bradwell B plant in the UK.\n\nThe plant is located about 45 kilometres from the border with Vietnam. It is a project of Guangxi Fangchenggang Nuclear Power Group, a joint venture between China Guangdong Nuclear Power Co (CGNPC) and Guangxi Investment Group.\n\nUnit 1 was connected to the electricity grid on 25 October 2015. Unit 1 is commercially operating starting on 1 January 2016.\nConstruction works for Unit 3 started in December 2015. Unit 3 first concrete pour occurred on 24 December 2015. \nFirst concrete for Unit 4 followed one year later, on 23 December 2016. \n"}
{"id": "49630668", "url": "https://en.wikipedia.org/wiki?curid=49630668", "title": "Fine electronic structure", "text": "Fine electronic structure\n\nIn solid state physics and physical chemistry, the fine electronic structure of a solid are the features of the electronic bands induced by intrinsic interactions between charge carriers. Valence and conduction bands split slightly compared to the difference between the various bands. Some mechanisms that allow it are angular momentum couplings, spin-orbit coupling, lattice distortions (Jahn–Teller effect), and other interactions described by crystal field theory. \n\nThe name comes from the fine structure of atoms, where energy levels suffer from a similar effect from the non-relativistic calculation due to effects like spin–orbit interaction, zitterbewegung, and corrections to the kinetic energy. \n\n"}
{"id": "928832", "url": "https://en.wikipedia.org/wiki?curid=928832", "title": "Forestation", "text": "Forestation\n\nForestation is the establishment of forest growth on areas that either had forest or lacked it naturally. In the first case, the process is called reforestation, while the second is called afforestation. \n"}
{"id": "5955857", "url": "https://en.wikipedia.org/wiki?curid=5955857", "title": "Garwood Load Packer", "text": "Garwood Load Packer\n\nThe Garwood Load Packer was a refuse collection vehicle built by Garwood Industries in Detroit, Michigan. Engineered by Melvin Donald Silvey, the Packer brought significant changes in the mode and automation of garbage collection in the United States.\n\nThe Garwood Load Packer was one of the first vehicles to utilize a compactor, increasing the truck's hauling capacity and reducing the costs of larger payloads. The Packer was introduced in 1938, but significant numbers weren't manufactured until after World War II. By 1949, over 2500 of these trucks were in use across the US and Canada. Almost all waste collection vehicles today utilize some type of compaction mechanism.\n\n"}
{"id": "38153826", "url": "https://en.wikipedia.org/wiki?curid=38153826", "title": "Generator interlock kit", "text": "Generator interlock kit\n\nA generator interlock kit (or just interlock kit) is a device designed to allow safe backfeeding of a home through a portable generator during power outages, thereby eliminating illegal and/or unsafe generator backfeeding situations that could potentially electrocute power linemen or destroy the generator.\n\nA generator interlock kit is installed on the front cover of the home's breaker panel. It consists of two sliding steel or plastic (depending on the brand) plates held together by three bolts. When moved down, the plate blocks the generator backfeed circuit breaker and allows the main breaker to remain on; when moved up, the generator backfeed circuit breaker may be turned on, but the main circuit breaker is blocked and will remain off. The generator backfeed circuit breaker is connected to a generator inlet installed (preferably) on the outside of the structure. A short cord connects the generator to the house inlet, usually through the use of twistlock plugs and sockets.\n\n\n\n\n"}
{"id": "24763544", "url": "https://en.wikipedia.org/wiki?curid=24763544", "title": "Green sport event", "text": "Green sport event\n\nA green sport event is a sporting event that stresses utilizing greener resources. Fans can do their part by riding public transportation to the event. People are being more aware of many things, like using earth friendly paper products, fertilizer and pesticides. Also see sustainable event management or event greening.\n\nOne week after the International Olympic Committee (IOC) awarded the XVII Olympic Winter Games to Lillehammer, national, local and regional governments decided to make the Lillehammer Games a showcase for GREEN mega-events. More than 200 different projects with environmental aspects were carried out. And at IOC's 100 years celebration in Paris in 1994, environment was added as the Third Dimension to the Olympics in addition to Sport and Culture.\n\nThe legacy from Lillehammer has, in a global perspective, followed two paths:\n- World Conferences on sport and environment, the first in Lillehammer in 1996\n- Environmental requirements which have to be met by future bidders and organisers of Olympic Games.\n\nThe American Airlines Center home of the Miami Heat (NBA) has been awarded LEED Certification for Existing Buildings: Operations & Maintenance by the U.S. Green Building Council (USGBC), being one of two the first two to receive this award. The Center received the award on the same day as the Philips Arena in Atlanta. This center has committed to saving energy and water savings. They wish that they could be a lead to other sports venues becoming more \"green\" too.\n\nAmerica Recycles Day (ARD) is the only nationally recognized day dedicated to encouraging Americans to recycle and buy recycled products. ARD is celebrated November 15. Hundreds of events are held across the U.S. to raise awareness about the importance of recycling and to encourage Americans to sign personal pledges to recycle and buy recycled products.\n\nSource\n\n"}
{"id": "12737", "url": "https://en.wikipedia.org/wiki?curid=12737", "title": "Gunpowder", "text": "Gunpowder\n\nGunpowder, also known as black powder to distinguish it from modern smokeless powder, is the earliest known chemical explosive. It consists of a mixture of sulfur (S), charcoal (C), and potassium nitrate (saltpeter, KNO). The sulfur and charcoal act as fuels while the saltpeter is an oxidizer. Because of its incendiary properties and the amount of heat and gas volume that it generates, gunpowder has been widely used as a propellant in firearms, artillery, rockets, and fireworks and as a blasting powder in quarrying, mining, and road building.\n\nGunpowder was invented in 9th-century China and spread throughout most parts of Eurasia by the end of the 13th century. Originally developed by the Taoists for medicinal purposes, gunpowder was first used for warfare about 1000 AD.\n\nGunpowder is classified as a low explosive because of its relatively slow decomposition rate and consequently low brisance. Low explosives deflagrate (i.e., burn) at \"subsonic\" speeds, whereas high explosives detonate, producing a supersonic wave.\n\nIgnition of gunpowder packed behind a projectile generates enough pressure to force the shot from the muzzle at high speed, but usually not enough force to rupture the gun barrel. Gunpowder thus makes a good propellant, but is less suitable for shattering rock or fortifications with its low-yield explosive power. However, by transferring enough energy (from the burning gunpowder to the mass of the cannonball, and then from the cannonball to the opposing fortifications by way of the impacting ammunition) eventually a bombardier may wear down an opponent's fortified defenses.\n\nGunpowder was widely used to fill fused artillery shells (and used in mining and civil engineering projects) until the second half of the 19th century, when the first high explosives were put into use. Gunpowder is no longer used in modern weapons nor is it used for industrial purposes due to its relatively inefficient cost compared to newer alternatives such as dynamite and ammonium nitrate/fuel oil. Today gunpowder firearms are limited primarily to hunting, target shooting, and bulletless historical reenactments.\n\nBased on a 9th-century Taoist text, the invention of gunpowder by Chinese alchemists was likely an accidental byproduct from experiments seeking to create elixir of life. This experimental medicine origin of gunpowder is reflected in its Chinese name \"huoyao\", which means “fire medicine”. The first military applications of gunpowder were developed around 1000 AD. The earliest chemical formula for gunpowder appeared in the 11th century Song dynasty text, \"Wujing Zongyao\", however gunpowder had already been used for fire arrows since at least the 10th century. In the following centuries various gunpowder weapons such as bombs, fire lances, and the gun appeared in China.\n\nSaltpeter was known to the Chinese by the mid-1st century AD and was primarily produced in the provinces of Sichuan, Shanxi, and Shandong. There is strong evidence of the use of saltpeter and sulfur in various medicinal combinations. A Chinese alchemical text dated 492 noted saltpeter burnt with a purple flame, providing a practical and reliable means of distinguishing it from other inorganic salts, thus enabling alchemists to evaluate and compare purification techniques; the earliest Latin accounts of saltpeter purification are dated after 1200.\n\nThe first reference to the incendiary properties of such mixtures is the passage of the \"Zhenyuan miaodao yaolüe\", a Taoist text tentatively dated to the mid-9th century: \"Some have heated together sulfur, realgar and saltpeter with honey; smoke and flames result, so that their hands and faces have been burnt, and even the whole house where they were working burned down.\" The Chinese word for \"gunpowder\" is , which literally means \"Fire Medicine\"; however this name only came into use some centuries after the mixture's discovery. In the following centuries a variety of gunpowder weapons such as rockets, bombs, and land mines appeared before the first metal barrel firearms were invented. Explosive weapons such as bombs have been discovered in a shipwreck off the shore of Japan dated from 1281, during the Mongol invasions of Japan.\n\nThe Chinese \"Wujing Zongyao\" (\"Complete Essentials from the Military Classics\"), written by Zeng Gongliang between 1040 and 1044, provides encyclopedia references to a variety of mixtures that included petrochemicals—as well as garlic and honey. A slow match for flame throwing mechanisms using the siphon principle and for fireworks and rockets is mentioned. The mixture formulas in this book do not contain enough saltpeter to create an explosive however; being limited to at most 50% saltpeter, they produce an incendiary. The \"Essentials\" was however written by a Song dynasty court bureaucrat, and there is little evidence that it had any immediate impact on warfare; there is no mention of gunpowder use in the chronicles of the wars against the Tanguts in the 11th century, and China was otherwise mostly at peace during this century.\n\nHowever, by 1083 the Song court was producing hundreds of thousands of fire arrows for their garrisons. Bombs and the first proto-guns, known as \"fire lances\", became prominent during the 12th century and were used by the Song during the Jin-Song Wars. Fire lances were first recorded to have been used at the Siege of De'an in 1132 by Song forces against the Jin. In the early 13th century the Jin utilized iron-casing bombs. Projectiles were added to fire lances, re-usable fire lance barrels were developed, first out of hardened paper, and then metal. By 1257 some fire lances were firing wads of bullets. In the late 13th century metal fire lances became 'eruptors', proto-cannons firing co-viative projectiles, and by 1287 at the latest, had become true guns, the hand cannon.\n\nThe earliest Western accounts of gunpowder appear in texts written by English philosopher Roger Bacon in the 13th century.\nSeveral sources mention Chinese firearms and gunpowder weapons being deployed by the Mongols against European forces at the Battle of Mohi in 1241. Professor Kenneth Warren Chase credits the Mongols for introducing into Europe gunpowder and its associated weaponry. However, there is no clear route of transmission, and while the Mongols are often pointed to as the likeliest vector, Timothy May points out that \"there is no concrete evidence that the Mongols used gunpowder weapons on a regular basis outside of China.\"\n\nIn Europe, one of the first mentions of gunpowder use appears in a passage found in Roger Bacon's \"Opus Maius\" of 1267 and \"Opus Tertium\" in what has been interpreted as being firecrackers. The most telling passage reads: \"We have an example of these things (that act on the senses) in [the sound and fire of] that children's toy which is made in many [diverse] parts of the world; i.e., a device no bigger than one's thumb. From the violence of that salt called saltpeter [together with sulfur and willow charcoal, combined into a powder] so horrible a sound is made by the bursting of a thing so small, no more than a bit of parchment [containing it], that we find [the ear assaulted by a noise] exceeding the roar of strong thunder, and a flash brighter than the most brilliant lightning.\" In the early 20th century, British artillery officer Henry William Lovett Hime proposed that another work tentatively attributed to Bacon, \"Epistola de Secretis Operibus Artis et Naturae, et de Nullitate Magiae\" contained an encrypted formula for gunpowder. This claim has been disputed by historians of science including Lynn Thorndike, John Maxson Stillman and George Sarton and by Bacon's editor Robert Steele, both in terms of authenticity of the work, and with respect to the decryption method. In any case, the formula claimed to have been decrypted (7:5:5 saltpeter:charcoal:sulfur) is not useful for firearms use or even firecrackers, burning slowly and producing mostly smoke. However, if Bacon's recipe is taken as measurements by volume rather than weight, a far more potent and serviceable explosive powder is created suitable for firing hand-cannons, albeit less consistent due to the inherent inaccuracies of measurements by volume. One example of this composition resulted in 100 parts saltpeter, 27 parts charcoal, and 45 parts sulfur, by weight.\n\nThe \"Liber Ignium\", or \"Book of Fires\", attributed to Marcus Graecus, is a collection of incendiary recipes, including some gunpowder recipes. Partington dates the gunpowder recipes to approximately 1300. One recipe for \"flying fire\" (\"ignis volatilis\") involves saltpeter, sulfur, and colophonium, which, when inserted into a reed or hollow wood, \"flies away suddenly and burns up everything.\" Another recipe, for artificial \"thunder\", specifies a mixture of one pound native sulfur, two pounds linden or willow charcoal, and six pounds of saltpeter. Another specifies a 1:3:9 ratio.\n\nSome of the gunpowder recipes of \"De Mirabilibus Mundi\" of Albertus Magnus are identical to the recipes of the \"Liber Ignium\", and according to Partington, \"may have been taken from that work, rather than conversely.\" Partington suggests that some of the book may have been compiled by Albert's students, \"but since it is found in thirteenth century manuscripts, it may well be by Albert.\" Albertus Magnus died in 1280.\n\nA major advance in manufacturing began in Europe in the late 14th century when the safety and thoroughness of incorporation was improved by wet grinding; liquid, such as distilled spirits was added during the grinding-together of the ingredients and the moist paste dried afterwards. The principle of wet mixing to prevent the separation of dry ingredients, invented for gunpowder, is used today in the pharmaceutical industry.\n\nIt was also discovered that if the paste was rolled into balls before drying the resulting gunpowder absorbed less water from the air during storage and traveled better. The balls were then crushed in a mortar by the gunner immediately before use, with the old problem of uneven particle size and packing causing unpredictable results. If the right size particles were chosen, however, the result was a great improvement in power. Forming the damp paste into \"corn\"-sized clumps by hand or with the use of a sieve instead of larger balls produced a product after drying that loaded much better, as each tiny piece provided its own surrounding air space that allowed much more rapid combustion than a fine powder. This \"corned\" gunpowder was from 30% to 300% more powerful. An example is cited where 34 pounds of serpentine was needed to shoot a 47-pound ball, but only 18 pounds of corned powder. The optimum size of the grain depended on its use; larger for large cannon, finer for small arms. Larger cast cannons were easily muzzle-loaded with corned powder using a long-handled ladle. Corned powder also retained the advantage of low moisture absorption, as even tiny grains still had much less surface area to attract water than a floury powder.\n\nDuring this time, European manufacturers also began regularly purifying saltpeter, using wood ashes containing potassium carbonate to precipitate calcium from their dung liquor, and using ox blood, alum, and slices of turnip to clarify the solution.\n\nDuring the Renaissance, two European schools of pyrotechnic thought emerged, one in Italy and the other at Nuremberg, Germany. The German printer and publisher Christiaan Egenolff adapted an earlier work on pyrotechnics from manuscript to print form, publishing his \"Büchsenmeysterei\" in 1529 and reprinting it in 1531. Now extremely rare, the book discusses the manufacturing of gunpowder, the operation of artillery and the rules of conduct for the gunsmith.\n\nIn Italy, Vannoccio Biringuccio, born in 1480, was a member of the guild \"Fraternita di Santa Barbara\" but broke with the tradition of secrecy by setting down everything he knew in a book titled \"De la pirotechnia\", written in vernacular. It was published posthumously in 1540, with 9 editions over 138 years, and also reprinted by MIT Press in 1966.\n\nBy the mid-17th century fireworks were used for entertainment on an unprecedented scale in Europe, being popular even at resorts and public gardens. With the publication of \"Deutliche Anweisung zur Feuerwerkerey\" (1748), methods for creating fireworks were sufficiently well-known and well-described that \"Firework making has become an exact science.\" In 1774 Louis XVI ascended to the throne of France at age 20. After he discovered that France was not self-sufficient in gunpowder, a Gunpowder Administration was established; to head it, the lawyer Antoine Lavoisier was appointed. Although from a bourgeois family, after his degree in law Lavoisier became wealthy from a company set up to collect taxes for the Crown; this allowed him to pursue experimental natural science as a hobby.\n\nWithout access to cheap saltpeter (controlled by the British), for hundreds of years France had relied on saltpetermen with royal warrants, the \"droit de fouille\" or \"right to dig\", to seize nitrous-containing soil and demolish walls of barnyards, without compensation to the owners. This caused farmers, the wealthy, or entire villages to bribe the petermen and the associated bureaucracy to leave their buildings alone and the saltpeter uncollected. Lavoisier instituted a crash program to increase saltpeter production, revised (and later eliminated) the \"droit de fouille\", researched best refining and powder manufacturing methods, instituted management and record-keeping, and established pricing that encouraged private investment in works. Although saltpeter from new Prussian-style putrefaction works had not been produced yet (the process taking about 18 months), in only a year France had gunpowder to export. A chief beneficiary of this surplus was the American Revolution. By careful testing and adjusting the proportions and grinding time, powder from mills such as at Essonne outside Paris became the best in the world by 1788, and inexpensive.\n\nGunpowder production in Britain appears to have started in the mid 14th century with the aim of supplying the English Crown. Records show that, in England, gunpowder was being made in 1346 at the Tower of London; a powder house existed at the Tower in 1461; and in 1515 three King's gunpowder makers worked there. Gunpowder was also being made or stored at other Royal castles, such as Portchester. By the early 14th century, according to N.J.G. Pounds's study \"The Medieval Castle in England and Wales,\" many English castles had been deserted and others were crumbling. Their military significance faded except on the borders. Gunpowder had made smaller castles useless.\n\nHenry VIII of England was short of gunpowder when he invaded France in 1544 and England needed to import gunpowder via the port of Antwerp in what is now Belgium.\n\nThe English Civil War (1642–1645) led to an expansion of the gunpowder industry, with the repeal of the Royal Patent in August 1641.\n\nOne of the most notable uses of gunpowder in Great Britain was the Gunpowder Plot of 1605: a failed assassination attempt on King James I and VI. The plot was foiled when Guy Fawkes was found under the House of Lords with hidden barrels of gunpowder. All assailants who had a role in the plot escaped but were eventually caught. King James later decreed that November 5th become a day of celebration, which is a tradition that carries on today known as Bonfire Night. \n\nTwo British physicists, Andrew Noble and Frederick Abel, worked to improve the properties of black powder during the late 19th century. This formed the basis for the Noble-Abel gas equation for internal ballistics.\n\nThe introduction of smokeless powder in the late 19th century led to a contraction of the gunpowder industry. After the end of World War I, the majority of the United Kingdom gunpowder manufacturers merged into a single company, \"Explosives Trades limited\"; and a number of sites were closed down, including those in Ireland. This company became Nobel Industries Limited; and in 1926 became a founding member of Imperial Chemical Industries. The Home Office removed gunpowder from its list of \"Permitted Explosives\"; and shortly afterwards, on 31 December 1931, the former Curtis & Harvey's Glynneath gunpowder factory at Pontneddfechan, in Wales, closed down, and it was demolished by fire in 1932.\n\nThe last remaining gunpowder mill at the Royal Gunpowder Factory, Waltham Abbey was damaged by a German parachute mine in 1941 and it never reopened. This was followed by the closure of the gunpowder section at the Royal Ordnance Factory, ROF Chorley, the section was closed and demolished at the end of World War II; and ICI Nobel's Roslin gunpowder factory, which closed in 1954.\n\nThis left the sole United Kingdom gunpowder factory at ICI Nobel's Ardeer site in Scotland; it too closed in October 1976. Since then gunpowder has been imported into the United Kingdom. In the late 1970s/early 1980s gunpowder was bought from eastern Europe, particularly from what was then the German Democratic Republic and former Yugoslavia.\n\nThe Muslims acquired knowledge of gunpowder some time between 1240 and 1280, by which point the Syrian Hasan al-Rammah had written, in Arabic, recipes for gunpowder, instructions for the purification of saltpeter, and descriptions of gunpowder incendiaries. It is implied by al-Rammah's usage of \"terms that suggested he derived his knowledge from Chinese sources\" and his references to saltpeter as \"Chinese snow\" ( '), fireworks as \"Chinese flowers\" and rockets as \"Chinese arrows\" that knowledge of gunpowder arrived from China. However, because al-Rammah attributes his material to \"his father and forefathers\", al-Hassan argues that gunpowder became prevalent in Syria and Egypt by \"the end of the twelfth century or the beginning of the thirteenth\". In Persia saltpeter was known as \"Chinese salt\" () \"namak-i chīnī\") or \"salt from Chinese salt marshes\" ( ').\n\nHasan al-Rammah included 107 gunpowder recipes in his text \"al-Furusiyyah wa al-Manasib al-Harbiyya\" (\"The Book of Military Horsemanship and Ingenious War Devices\"), 22 of which are for rockets. If one takes the median of 17 of these 22 compositions for rockets (75% nitrates, 9.06% sulfur, and 15.94% charcoal), it is nearly identical to the modern reported ideal gunpowder recipe of 75% potassium nitrate, 10% sulfur, and 15% charcoal.\n\nAl-Hassan claims that in the Battle of Ain Jalut of 1260, the Mamluks used against the Mongols in \"the first cannon in history\" gunpowder formula with near-identical ideal composition ratios for explosive gunpowder. Other historians urge caution regarding claims of Islamic firearms use in the 1204–1324 period as late medieval Arabic texts used the same word for gunpowder, \"naft\", that they used for an earlier incendiary, naphtha.\n\nKhan claims that it was invading Mongols who introduced gunpowder to the Islamic world and cites Mamluk antagonism towards early musketeers in their infantry as an example of how gunpowder weapons were not always met with open acceptance in the Middle East. Similarly, the refusal of their Qizilbash forces to use firearms contributed to the Safavid rout at Chaldiran in 1514.\n\nThe state-controlled manufacture of gunpowder by the Ottoman Empire through early supply chains to obtain nitre, sulfur and high-quality charcoal from oaks in Anatolia contributed significantly to its expansion between the 15th and 18th century. It was not until later in the 19th century when the syndicalist production of Turkish gunpowder was greatly reduced, which coincided with the decline of its military might.\n\nGunpowder and gunpowder weapons were transmitted to India through the Mongol invasions of India. The Mongols were defeated by Alauddin Khalji of the Delhi Sultanate, and some of the Mongol soldiers remained in northern India after their conversion to Islam. It was written in the \"Tarikh-i Firishta\" (1606–1607) that Nasir ud din Mahmud the ruler of the Delhi Sultanate presented the envoy of the Mongol ruler Hulegu Khan with a dazzling pyrotechnics display upon his arrival in Delhi in 1258. Nasir ud din Mahmud tried to express his strength as a ruler and tried to ward off any Mongol attempt similar to the Siege of Baghdad (1258). Firearms known as \"top-o-tufak\" also existed in many Muslim kingdoms in India by as early as 1366. From then on the employment of gunpowder warfare in India was prevalent, with events such as the \"Siege of Belgaum\" in 1473 by Sultan Muhammad Shah Bahmani.\n\nThe shipwrecked Ottoman Admiral Seydi Ali Reis is known to have introduced the earliest type of matchlock weapons, which the Ottomans used against the Portuguese during the Siege of Diu (1531). After that, a diverse variety of firearms, large guns in particular, became visible in Tanjore, Dacca, Bijapur, and Murshidabad. Guns made of bronze were recovered from Calicut (1504)- the former capital of the Zamorins\n\nThe Mughal emperor Akbar mass-produced matchlocks for the Mughal Army. Akbar is personally known to have shot a leading Rajput commander during the Siege of Chittorgarh. The Mughals began to use bamboo rockets (mainly for signalling) and employ sappers: special units that undermined heavy stone fortifications to plant gunpowder charges.\n\nThe Mughal Emperor Shah Jahan is known to have introduced much more advanced matchlocks, their designs were a combination of Ottoman and Mughal designs. Shah Jahan also countered the British and other Europeans in his province of Gujarāt, which supplied Europe saltpeter for use in gunpowder warfare during the 17th century. Bengal and Mālwa participated in saltpeter production. The Dutch, French, Portuguese, and English used Chhapra as a center of saltpeter refining.\n\nEver since the founding of the Sultanate of Mysore by Hyder Ali, French military officers were employed to train the Mysore Army. Hyder Ali and his son Tipu Sultan were the first to introduce modern cannons and muskets, their army was also the first in India to have official uniforms. During the Second Anglo-Mysore War Hyder Ali and his son Tipu Sultan unleashed the Mysorean rockets at their British opponents effectively defeating them on various occasions. The Mysorean rockets inspired the development of the Congreve rocket, which the British widely utilized during the Napoleonic Wars and the War of 1812.\n\nThe Javanese Majapahit Empire was arguably able to encompass much of modern-day Indonesia due to its unique mastery of bronze-smithing and use of a central arsenal fed by a large number of cottage industries within the immediate region. Documentary and archeological evidence indicate that Arab traders introduced gunpowder, gonnes, muskets, blunderbusses, and cannons to the Javanese, Acehnese, and Batak via long established commercial trade routes around the early to mid 14th century. Portuguese and Spanish invaders were unpleasantly surprised and even outgunned on occasion. The resurgent Singhasari Empire overtook Sriwijaya and later emerged as the Majapahit whose warfare featured the use of fire-arms and cannonade. Circa 1540, the Javanese, always alert for new weapons found the newly arrived Portuguese weaponry superior to that of the locally made variants. Javanese bronze breech-loaded swivel-guns, known as meriam, or erroneously as lantaka, was used widely by the Majapahit navy as well as by pirates and rival lords. The demise of the Majapahit empire and the dispersal of disaffected skilled bronze cannon-smiths to Brunei, modern Sumatra, Malaysia and the Philippines lead to widespread use, especially in the Makassar Strait.\n\nSaltpeter harvesting was recorded by Dutch and German travelers as being common in even the smallest villages and was collected from the decomposition process of large dung hills specifically piled for the purpose. The Dutch punishment for possession of non-permitted gunpowder appears to have been amputation. Ownership and manufacture of gunpowder was later prohibited by the colonial Dutch occupiers. According to a colonel McKenzie quoted in Sir Thomas Stamford Raffles, \"The History of Java\" (1817), the purest sulfur was supplied from a crater from a mountain near the straits of Bali.\n\nOn the origins of gunpowder technology, historian Tonio Andrade remarked, \"Scholars today overwhelmingly concur that the gun was invented in\nChina.\" Gunpowder and the gun are widely believed by historians to have originated from China because there is a large body of evidence that documents the evolution of the gun from the Chinese fire lance to a metal gun and the evolution of gunpowder from a medicine to an incendiary and an explosive, whereas similar records do not exist in Europe. As Andrade explains, the large amount of variation in gunpowder recipes in China relative to Europe is \"evidence of experimentation in China, where gunpowder was at first used as an incendiary and only later became an explosive and a propellant... in contrast, formulas in Europe diverged only very slightly from the ideal proportions for use as an explosive and a propellant, suggesting that gunpowder was introduced as a mature technology.\"\n\nHowever, the history of gunpowder is not without controversy. A major problem confronting the study of early gunpowder history is ready access to sources close to the events described. Often the first records potentially describing use of gunpowder in warfare were written several centuries after the fact, and may well have been colored by the contemporary experiences of the chronicler. Translation difficulties have led to errors or loose interpretations bordering on artistic licence. Ambiguous language can make it difficult to distinguish gunpowder weapons from similar technologies that do not rely on gunpowder. A commonly cited example is a report of the Battle of Mohi in Eastern Europe that mentions a \"long lance\" sending forth \"evil-smelling vapors and smoke\", which has been variously interpreted by different historians as the \"first-gas attack upon European soil\" using gunpowder, \"the first use of cannon in Europe\", or merely a \"toxic gas\" with no evidence of gunpowder. It is difficult to accurately translate original Chinese alchemical texts, which tend to explain phenomena through metaphor, into modern scientific language with rigidly defined terminology in English. Early texts potentially mentioning gunpowder are sometimes marked by a linguistic process where semantic change occurred. For instance, the Arabic word \"naft\" transitioned from denoting naphtha to denoting gunpowder, and the Chinese word \"pào\" changed in meaning from catapult to referring to a cannon. This has led to arguments on the exact origins of gunpowder based on etymological foundations. Science and technology historian Bert S. Hall makes the observation that, \"It goes without saying, however, that historians bent on special pleading, or simply with axes of their own to grind, can find rich material in these terminological thickets.\"\n\nAnother major area of contention in modern studies of the history of gunpowder is regarding the transmission of gunpowder. While the literary and archaeological evidence supports a Chinese origin for gunpowder and guns, the manner in which gunpowder technology was transferred from China to the West is still under debate. It is unknown why the rapid spread of gunpowder technology across Eurasia took place over several decades whereas other technologies such as paper, the compass, and printing did not reach Europe until centuries after they were invented in China.\n\nFor the most powerful black powder, meal powder, a wood charcoal, is used. The best wood for the purpose is Pacific willow, but others such as alder or buckthorn can be used. In Great Britain between the 15th and 19th centuries charcoal from alder buckthorn was greatly prized for gunpowder manufacture; cottonwood was used by the American Confederate States. The ingredients are reduced in particle size and mixed as intimately as possible. Originally, this was with a mortar-and-pestle or a similarly operating stamping-mill, using copper, bronze or other non-sparking materials, until supplanted by the rotating ball mill principle with non-sparking bronze or lead. Historically, a marble or limestone edge runner mill, running on a limestone bed, was used in Great Britain; however, by the mid 19th century this had changed to either an iron-shod stone wheel or a cast iron wheel running on an iron bed. The mix was dampened with alcohol or water during grinding to prevent accidental ignition. This also helps the extremely soluble saltpeter to mix into the microscopic nooks and crannies of the very high surface-area charcoal.\n\nAround the late 14th century, European powdermakers first began adding liquid during grinding to improve mixing, reduce dust, and with it the risk of explosion. The powder-makers would then shape the resulting paste of dampened gunpowder, known as mill cake, into corns, or grains, to dry. Not only did corned powder keep better because of its reduced surface area, gunners also found that it was more powerful and easier to load into guns. Before long, powder-makers standardized the process by forcing mill cake through sieves instead of corning powder by hand.\n\nThe improvement was based on reducing the surface area of a higher density composition. At the beginning of the 19th century, makers increased density further by static pressing. They shoveled damp mill cake into a two-foot square box, placed this beneath a screw press and reduced it to its volume. \"Press cake\" had the hardness of slate. They broke the dried slabs with hammers or rollers, and sorted the granules with sieves into different grades. In the United States, Eleuthere Irenee du Pont, who had learned the trade from Lavoisier, tumbled the dried grains in rotating barrels to round the edges and increase durability during shipping and handling. (Sharp grains rounded off in transport, producing fine \"meal dust\" that changed the burning properties.)\n\nAnother advance was the manufacture of kiln charcoal by distilling wood in heated iron retorts instead of burning it in earthen pits. Controlling the temperature influenced the power and consistency of the finished gunpowder. In 1863, in response to high prices for Indian saltpeter, DuPont chemists developed a process using potash or mined potassium chloride to convert plentiful Chilean sodium nitrate to potassium nitrate.\n\nThe following year (1864) the Gatebeck Low Gunpowder Works in Cumbria (Great Britain) started a plant to manufacture potassium nitrate by essentially the same chemical process. This is nowadays called the ‘Wakefield Process’, after the owners of the company. It would have used potassium chloride from the Staßfurt mines, near Magdeburg, Germany, which had recently become available in industrial quantities.\n\nDuring the 18th century, gunpowder factories became increasingly dependent on mechanical energy. Despite mechanization, production difficulties related to humidity control, especially during the pressing, were still present in the late 19th century. A paper from 1885 laments that \"Gunpowder is such a nervous and sensitive spirit, that in almost every process of manufacture it changes under our hands as the weather changes.\" Pressing times to the desired density could vary by a factor of three depending on the atmospheric humidity.\n\nThe term \"black powder\" was coined in the late 19th century, primarily in the United States, to distinguish prior gunpowder formulations from the new smokeless powders and semi-smokeless powders. Semi-smokeless powders featured bulk volume properties that approximated black powder, but had significantly reduced amounts of smoke and combustion products. Smokeless powder has different burning properties (pressure vs. time) and can generate higher pressures and work per gram. This can rupture older weapons designed for black powder. Smokeless powders ranged in color from brownish tan to yellow to white. Most of the bulk semi-smokeless powders ceased to be manufactured in the 1920s.\n\nBlack powder is a granular mixture of\nPotassium nitrate is the most important ingredient in terms of both bulk and function because the combustion process releases oxygen from the potassium nitrate, promoting the rapid burning of the other ingredients. To reduce the likelihood of accidental ignition by static electricity, the granules of modern black powder are typically coated with graphite, which prevents the build-up of electrostatic charge.\n\nCharcoal does not consist of pure carbon; rather, it consists of partially pyrolyzed cellulose, in which the wood is not completely decomposed. Carbon differs from ordinary charcoal. Whereas charcoal's autoignition temperature is relatively low, carbon's is much greater. Thus, a black powder composition containing pure carbon would burn similarly to a match head, at best.\n\nThe current standard composition for the black powders that are manufactured by pyrotechnicians was adopted as long ago as 1780. Proportions by weight are 75% potassium nitrate (known as saltpeter or saltpetre), 15% softwood charcoal, and 10% sulfur. These ratios have varied over the centuries and by country, and can be altered somewhat depending on the purpose of the powder. For instance, power grades of black powder, unsuitable for use in firearms but adequate for blasting rock in quarrying operations, are called blasting powder rather than gunpowder with standard proportions of 70% nitrate, 14% charcoal, and 16% sulfur; blasting powder may be made with the cheaper sodium nitrate substituted for potassium nitrate and proportions may be as low as 40% nitrate, 30% charcoal, and 30% sulfur. In 1857, Lammot du Pont solved the main problem of using cheaper sodium nitrate formulations when he patented DuPont \"B\" blasting powder. After manufacturing grains from press-cake in the usual way, his process tumbled the powder with graphite dust for 12 hours. This formed a graphite coating on each grain that reduced its ability to absorb moisture.\n\nNeither the use of graphite nor sodium nitrate was new. Glossing gunpowder corns with graphite was already an accepted technique in 1839, and sodium nitrate-based blasting powder had been made in Peru for many years using the sodium nitrate mined at Tarapacá (now in Chile). Also, in 1846, two plants were built in south-west England to make blasting powder using this sodium nitrate. The idea may well have been brought from Peru by Cornish miners returning home after completing their contracts. Another suggestion is that it was William Lobb, the planthunter, who recognised the possibilities of sodium nitrate during his travels in South America. Lammot du Pont would have known about the use of graphite and probably also knew about the plants in south-west England. In his patent he was careful to state that his claim was for the combination of graphite with sodium nitrate-based powder, rather than for either of the two individual technologies.\n\nFrench war powder in 1879 used the ratio 75% saltpeter, 12.5% charcoal, 12.5% sulfur. English war powder in 1879 used the ratio 75% saltpeter, 15% charcoal, 10% sulfur. The British Congreve rockets used 62.4% saltpeter, 23.2% charcoal and 14.4% sulfur, but the British Mark VII gunpowder was changed to 65% saltpeter, 20% charcoal and 15% sulfur. The explanation for the wide variety in formulation relates to usage. Powder used for rocketry can use a slower burn rate since it accelerates the projectile for a much longer time—whereas powders for weapons such as flintlocks, cap-locks, or matchlocks need a higher burn rate to accelerate the projectile in a much shorter distance. Cannons usually used lower burn rate powders, because most would burst with higher burn rate powders.\n\nThe original dry-compounded powder used in 15th-century Europe was known as \"Serpentine\", either a reference to Satan or to a common artillery piece that used it. The ingredients were ground\ntogether with a mortar and pestle, perhaps for 24 hours, resulting in a fine flour. Vibration during transportation could cause the components to separate again, requiring remixing in the field. Also if the quality of the saltpeter was low (for instance if it was contaminated with highly hygroscopic calcium nitrate), or if the powder was simply old (due to the mildly hygroscopic nature of potassium nitrate), in humid weather it would need to be re-dried. The dust from \"repairing\" powder in the field was a major hazard.\n\nLoading cannons or bombards before the powder-making advances of the Renaissance was a skilled art. Fine powder loaded haphazardly or too tightly would burn incompletely or too slowly. Typically, the breech-loading powder chamber in the rear of the piece was filled only about half full, the serpentine powder neither too compressed nor too loose, a wooden bung pounded in to seal the chamber from the barrel when assembled, and the projectile placed on. A carefully determined empty space was necessary for the charge to burn effectively. When the cannon was fired through the touchhole, turbulence from the initial surface combustion caused the rest of the powder to be rapidly exposed to the flame.\n\nThe advent of much more powerful and easy to use \"corned\" powder changed this procedure, but serpentine was used with older guns into the 17th century.\n\nFor propellants to oxidize and burn rapidly and effectively, the combustible ingredients must be reduced to the smallest possible particle sizes, and be as thoroughly mixed as possible. Once mixed, however, for better results in a gun, makers discovered that the final product should be in the form of individual dense grains that spread the fire quickly from grain to grain, much as straw or twigs catch fire more quickly than a pile of sawdust.\n\nBecause the dry powdered ingredients must be mixed and bonded together for extrusion and cutting into grains to maintain the blend, size reduction and mixing is done while the ingredients are damp, usually with water. After 1800, instead of forming grains by hand or with sieves, the damp \"mill-cake\" was pressed in molds to increase its density and extract the liquid, forming \"press-cake\". The pressing took varying amounts of time, depending on conditions such as atmospheric humidity. The hard, dense product was broken again into tiny pieces, which were separated with sieves to produce a uniform product for each purpose: coarse powders for cannons, finer grained powders for muskets, and the finest for small hand guns and priming. Inappropriately fine-grained powder often caused cannons to burst before the projectile could move down the barrel, due to the high initial spike in pressure. \"Mammoth\" powder with large grains, made for Rodman's 15-inch cannon, reduced the pressure to only 20 percent as high as ordinary cannon powder would have produced.\n\nIn the mid-19th century, measurements were made determining that the burning rate within a grain of black powder (or a tightly packed mass) is about 6 cm/s (0.20 feet/s), while the rate of ignition propagation from grain to grain is around 9 m/s (30 feet/s), over two orders of magnitude faster.\n\nModern corning first compresses the fine black powder meal into blocks with a fixed density (1.7 g/cm³). In the United States, gunpowder grains were designated F (for fine) or C (for coarse). Grain diameter decreased with a larger number of Fs and increased with a larger number of Cs, ranging from about for 7F to for 7C. Even larger grains were produced for artillery bore diameters greater than about . The standard DuPont \"Mammoth\" powder developed by Thomas Rodman and Lammot du Pont for use during the American Civil War had grains averaging in diameter with edges rounded in a glazing barrel. Other versions had grains the size of golf and tennis balls for use in Rodman guns. In 1875 DuPont introduced \"Hexagonal\" powder for large artillery, which was pressed using shaped plates with a small center core—about diameter, like a wagon wheel nut, the center hole widened as the grain burned. By 1882 German makers also produced hexagonal grained powders of a similar size for artillery.\n\nBy the late 19th century manufacturing focused on standard grades of black powder from Fg used in large bore rifles and shotguns, through FFg (medium and small-bore arms such as muskets and fusils), FFFg (small-bore rifles and pistols), and FFFFg (extreme small bore, short pistols and most commonly for priming flintlocks). A coarser grade for use in military artillery blanks was designated A-1. These grades were sorted on a system of screens with oversize retained on a mesh of 6 wires per inch, A-1 retained on 10 wires per inch, Fg retained on 14, FFg on 24, FFFg on 46, and FFFFg on 60. Fines designated FFFFFg were usually reprocessed to minimize explosive dust hazards. In the United Kingdom, the main service gunpowders were classified RFG (rifle grained fine) with diameter of one or two millimeters and RLG (rifle grained large) for grain diameters between two and six millimeters. Gunpowder grains can alternatively be categorized by mesh size: the BSS sieve mesh size, being the smallest mesh size, which retains no grains. Recognized grain sizes are Gunpowder G 7, G 20, G 40, and G 90.\n\nOwing to the large market of antique and replica black-powder firearms in the US, modern gunpowder substitutes like Pyrodex, Triple Seven and Black Mag3 pellets have been developed since the 1970s. These products, which should not be confused with smokeless powders, aim to produce less fouling (solid residue), while maintaining the traditional volumetric measurement system for charges. Claims of less corrosiveness of these products have been controversial however. New cleaning products for black-powder guns have also been developed for this market.\n\nBesides black powder, there are other historically important types of gunpowder. \"Brown gunpowder\" is cited as composed of 79% nitre, 3% sulfur, and 18% charcoal per 100 of dry powder, with about 2% moisture. Prismatic Brown Powder is a large-grained product the Rottweil Company introduced in 1884 in Germany, which was adopted by the British Royal Navy shortly thereafter. The French navy adopted a fine, 3.1 millimeter, not prismatic grained product called \"Slow Burning Cocoa\" (SBC) or \"cocoa powder\". These brown powders reduced burning rate even further by using as little as 2 percent sulfur and using charcoal made from rye straw that had not been completely charred, hence the brown color.\n\nLesmok powder was a product developed by DuPont in 1911, one of several semi-smokeless products in the industry containing a mixture of black and nitrocellulose powder. It was sold to Winchester and others primarily for .22 and .32 small calibers. Its advantage was that it was believed at the time to be less corrosive than smokeless powders then in use. It was not understood in the U.S. until the 1920s that the actual source of corrosion was the potassium chloride residue from potassium chlorate sensitized primers. The bulkier black powder fouling better disperses primer residue. Failure to mitigate primer corrosion by dispersion caused the false impression that nitrocellulose-based powder caused corrosion. Lesmok had some of the bulk of black powder for dispersing primer residue, but somewhat less total bulk than straight black powder, thus requiring less frequent bore cleaning. It was last sold by Winchester in 1947.\n\nThe development of smokeless powders, such as cordite, in the late 19th century created the need for a spark-sensitive priming charge, such as gunpowder. However, the sulfur content of traditional gunpowders caused corrosion problems with Cordite Mk I and this led to the introduction of a range of sulfur-free gunpowders, of varying grain sizes. They typically contain 70.5 parts of saltpeter and 29.5 parts of charcoal. Like black powder, they were produced in different grain sizes. In the United Kingdom, the finest grain was known as \"sulfur-free mealed powder\" (\"SMP\"). Coarser grains were numbered as sulfur-free gunpowder (SFG n): 'SFG 12', 'SFG 20', 'SFG 40' and 'SFG 90', for example; where the number represents the smallest BSS sieve mesh size, which retained no grains.\n\nSulfur's main role in gunpowder is to decrease the ignition temperature. A sample reaction for sulfur-free gunpowder would be\n\nGunpowder does not burn as a single reaction, so the byproducts are not easily predicted. One study showed that it produced (in order of descending quantities) 55.91% solid products: potassium carbonate, potassium sulfate, potassium sulfide, sulfur, potassium nitrate, potassium thiocyanate, carbon, ammonium carbonate and 42.98% gaseous products: carbon dioxide, nitrogen, carbon monoxide, hydrogen sulfide, hydrogen, methane, 1.11% water.\n\nHowever, simplified equations have been cited.\n\nA simple, commonly cited, chemical equation for the combustion of black powder is\n\nA balanced, but still simplified, equation is\n\nBoth previous equation are based on the assumption that charcoal is pure carbon, while in real life charcoal's chemical formula varies, but it can be summed up by its empirical formula: CHO . Therefore, a more accurate equation of the decomposition of regular black powder with sulfur is:\n\nLikewise, black powder without sulfur gives:\n\nBlack powder made with less-expensive and more plentiful sodium nitrate (in appropriate proportions) works just as well, and previous equations apply, with sodium instead of potassium. However, it is more hygroscopic than powders made from potassium nitrate—popularly known as saltpeter. Because \"corned\" black powder grains made with saltpeter are less affected by moisture in the air, they can be stored unsealed without degradation by humidity. Muzzleloaders have been known to fire after hanging on a wall for decades in a loaded state, provided they remained dry. By contrast, black powder made with sodium nitrate must be kept sealed to remain stable.\n\nThe matchlock musket or pistol (an early gun ignition system), as well as the flintlock would often be unusable in wet weather, due to powder in the pan being exposed and dampened.\n\nGunpowder releases 3 megajoules per kilogram and contains its own oxidant. This is lower than\nTNT (4.7 megajoules per kilogram), or gasoline (47.2 megajoules per kilogram, but gasoline requires an oxidant, so an optimized gasoline and O mixture contains 10.4 megajoules per kilogram).\nBlack powder also has a low energy density compared to modern \"smokeless\" powders, and thus to achieve high energy loadings, large amounts of black powder are needed with heavy projectiles.\n\nGunpowder is a low explosive, that is, it does not detonate but rather deflagrates (burns quickly). This is an advantage in a propeller device, where one does not desire a shock that would shatter the gun and potentially harm the operator, however it is a drawback when some explosion is wanted. In that case, gunpowder (and most importantly, gases produced by its burning) must be confined. Since it contains its own oxidizer and additionally burns faster under pressure, its combustion is capable of bursting containers such as shell, grenade, or improvised \"pipe bomb\" or \"pressure cooker\" casings to form shrapnel.\n\nIn quarrying, high explosives are generally preferred for shattering rock. However, because of its low brisance, black powder causes fewer fractures and results in more usable stone compared to other explosives, making black powder useful for blasting monumental stone such as granite and marble. Black powder is well suited for blank rounds, signal flares, burst charges, and rescue-line launches. Black powder is also used in fireworks for lifting shells, in rockets as fuel, and in certain special effects.\n\nAs seen above, combustion converts less than half the mass of black powder to gas, most of it turns into particulate matter. Some of it is ejected, wasting propelling power, fouling the air, and generally being a nuisance (giving away a soldier's position, generating fog that hinders vision, etc.). Some of it ends up as a thick layer of soot inside the barrel, where it also is a nuisance for subsequent shots, and a cause of jamming an automatic weapon. Moreover, this residue is hygroscopic, and with the addition of moisture absorbed from the air forms a corrosive substance. The soot contains potassium oxide or sodium oxide that turns into potassium hydroxide, or sodium hydroxide, which corrodes wrought iron or steel gun barrels. Black powder arms must therefore be well cleaned after use, both inside and out, to remove the residue.\n\nThe United Nations Model Regulations on the Transportation of Dangerous Goods and national transportation authorities, such as United States Department of Transportation, have classified gunpowder (black powder) as a \"Group A: Primary explosive substance\" for shipment because it ignites so easily. Complete manufactured devices containing black powder are usually classified as \"Group D: Secondary detonating substance, or black powder, or article containing secondary detonating substance\", such as firework, class D model rocket engine, etc., for shipment because they are harder to ignite than loose powder. As explosives, they all fall into the category of Class 1.\n\nBesides its use as a propellant in firearms and artillery, black powder's other main use has been as a blasting powder in quarrying, mining, and road construction (including railroad construction). During the 19th century, outside of war emergencies such as the Crimean War or the American Civil War, more black powder was used in these industrial uses than in firearms and artillery. But dynamite gradually replaced it for those uses. Today industrial explosives for such uses are still a huge market, but most of the market is in newer explosives rather than black powder.\n\nBeginning in the 1930s, gunpowder or smokeless powder was used in rivet guns, stun guns for animals, cable splicers and other industrial construction tools. The \"stud gun\" drove nails or screws into solid concrete, a function not possible with hydraulic tools. Today powder-actuated tools are still an important part of various industries, but the cartridges usually use smokeless powders. Industrial shotguns have been used to eliminate persistent material rings in operating rotary kilns (such as those for cement, lime, phosphate, etc.) and clinker in operating furnaces, and commercial tools make the method more reliable.\n\nGunpowder has occasionally been employed for other purposes besides weapons, mining, and construction:\n\n\n\n\n"}
{"id": "54634552", "url": "https://en.wikipedia.org/wiki?curid=54634552", "title": "HEPBS", "text": "HEPBS\n\nHEPBS (N-(2-Hydroxyethyl)piperazine-N'-(4-butanesulfonic acid)) is a zwitterionic organic chemical buffering agent; one of Good's buffers. HEPBS and HEPES have very similar structures and properties, HEPBS also having a pH in the physiological range (7.6-9.0 useful range). This makes it possible to use it for cell culture work.\n"}
{"id": "40470875", "url": "https://en.wikipedia.org/wiki?curid=40470875", "title": "Hsu diffusion", "text": "Hsu diffusion\n\nHow the plasma transport is reduced by the strength of the external magnetic field is of great concern in studying magnetic confinement of fusion plasma. The plasma diffusion may be classified by the classical diffusion of B scaling, the Bohm diffusion conjectured to follow the B scaling, and the Hsu diffusion of B scaling. Here, B is the external magnetic field.\n\nThe low-frequency fluctuating electric fields can cause particles to execute the ExB drift. Due to the long range nature of Coulomb interaction, the electric field coherence time is long enough to allow virtually free streaming of particles across the field lines. Thus, when no other decoherence mechanism exists, the transport would be the only mechanism to limit the run of its own course and to result in the Bohm diffusion of 1/B scaling in a 2D like plasma.\n\nIn a 3D plasma, the parallel decoherence (the decoherence along the field line) is significant enough to reduce the transport of ExB drifts to only the classical diffusion. There are, however, cyclotron harmonics that can cause resonance diffusion in the velocity space leading to an unbounded Larmor radius enlargement and particle diffusion. Hsu, Wu, Agarwal, and Ryu in 2013 proposed this effective diffusion mechanism by the combined effects from the ExB drift and the cyclotron resonance.\n\nSince the cyclotron harmonic is in tune with the particle gyration, it is effectively stationary as seen by the particles, but weakened by the finite Larmor radius (FLR) effect, i. e., \"I\"(λ)e~λ≡kρ«1 in the thermal fluctuation spectrum, where k is the wave number perpendicular to the magnetic field and ρ≡v/Ω is the plasma gyroradius, v the thermal velocity and Ω the gyrofrequecy. When the parallel decoherence, characterized by 1/kv , and the perpendicular diffusive damping, characterized by kD, are on the same time scale, namely, Ω»kD~kv»ν, it results in a diffusion coefficient\n\nThe electric field energy of thermal fluctuations is a fraction of the particle thermal energy given by δE~εnkT, where ε is the plasma parameter. Therefore, the renormalized D value gives the Hsu diffusion of the 1/B scaling .\n\n"}
{"id": "10940032", "url": "https://en.wikipedia.org/wiki?curid=10940032", "title": "Hydrogen anion", "text": "Hydrogen anion\n\nThe hydrogen anion, H, is a negative ion of hydrogen, that is, a hydrogen atom that has captured an extra electron. The hydrogen anion is an important constituent of the atmosphere of stars, such as the Sun. In chemistry, this ion is called hydride. The ion has two electrons bound by the electromagnetic force to a nucleus containing one proton.\n\nThe binding energy of H equals the binding energy of an extra electron to a hydrogen atom, called electron affinity of hydrogen. It is measured to be or (see Electron affinity (data page)). The total ground state energy thus becomes .\n\nThe hydrogen anion is an important species in the photosphere of the Sun. It absorbs energies in the range 0.75–4.0 eV, which ranges from the infrared into the visible spectrum (, ). It also occurs in the Earth's ionosphere (), and can be produced in particle accelerators.\n\nIts existence was first proven theoretically by Hans Bethe in 1929 (). H is unusual because, in its free form, it has no bound excited states, as was finally proven in 1977 (). It has been studied experimentally using particle accelerators ().\n\nIn chemistry, the hydride anion is hydrogen that has the formal oxidation state −1.\n\nThe term hydride is probably most often used to describe compounds of hydrogen with other elements in which the hydrogen is in the formal −1 oxidation state. In most such compounds the bonding between the hydrogen and its nearest neighbor is covalent. An example of a hydride is the borohydride anion ().\n\n\n"}
{"id": "413091", "url": "https://en.wikipedia.org/wiki?curid=413091", "title": "List of national parks of China", "text": "List of national parks of China\n\nIn China, () is the exact equivalent of the term 'national park' () applied to the rest of the world, as specified in the National Standard of the People's Republic of China GB50298-1999: \"Code for Scenic Area Planning\", and in the Green Paper: \"Situation and Prospects of China's Scenic Areas\" published by the Ministry of Construction in 1994. National parks in China were officially approved and declared by the State Council. The Ministry of Housing and Urban-Rural Development is in charge of the supervision and administration of national and provincial parks throughout the country.\n\nTo date, China has 244 national parks. The ranges and boundaries of these national parks are often extended beyond what the official names might suggest. For example, the Taihu National Park (; overall size [scenic zones + transitional zones]：3,091 km²), stretching across Suzhou and Wuxi, is composed of 13 scenic zones (with a number of scenic spots in each SZ): Mudu, Shihu/石湖, Guangfu/光福, Dongshan/东山, Xishan/西山, Luzhi, Tongli, Mount Yushan, Meiliang Lake, Lake Lihu, Xihui, Mashan/马山, Yangxian/阳羡, plus 2 isolated scenic spots (falling outside the scenic zone): Taibo's Shrine and Tomb.\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "1207207", "url": "https://en.wikipedia.org/wiki?curid=1207207", "title": "Little–Parks effect", "text": "Little–Parks effect\n\nThe Little–Parks effect was discovered in 1962 by William A. Little and Roland D. Parks in experiments with empty and thin-walled superconducting cylinders subjected to a parallel magnetic field. It was one of the first experiments that indicate the importance of Cooper-pairing principle in BCS theory.\n\nThe essence of Little–Parks (LP) effect is slight suppression of the superconductivity by persistent current.\n\nResults schematically shown in the Fig., where we see periodical oscillations of critical temperature (\"T\"), below which an object becomes superconductive, superimposed on the parabolic background.\n\nThe electrical resistance of such cylinders shows a periodic oscillation with the magnetic flux piercing the cylinder, the period being\n\nwhere \"h\" is the Planck constant and \"e\" is the electron charge. The explanation provided by Little and Parks is that the resistance oscillation reflects a more fundamental phenomenon, i.e. periodic oscillation of the superconducting \"T\".\n\nThe LP effect consists in a periodic variation of the \"T\" with the magnetic flux, which is the product of the magnetic field (coaxial) and the cross sectional area of the cylinder. \"T\" depends on the kinetic energy (KE) of the superconducting electrons. More precisely, the \"T\" is such temperature at which the free energies of normal and superconducting electrons are equal, for a given magnetic field. To understand the periodic oscillation of the \"T\", which constitutes the LP effect, one needs to understand the periodic variation of the kinetic energy. The KE oscillates because the applied magnetic flux increases the KE while superconducting vortices, periodically entering the cylinder, compensate for the flux effect and reduce the KE. Thus, the periodic oscillation of the kinetic energy and the related periodic oscillation of the critical temperature occur together.\n\nThe LP effect is a result of collective quantum behavior of superconducting electrons. It reflects the general fact that it is the fluxoid rather than the flux which is quantized in superconductors.\n\nThe LP effect can be seen as a result of the requirement that quantum physics be invariant with respect to the gauge choice for the electromagnetic potential, of which the magnetic vector potential A forms part.\n\nElectromagnetic theory implies that a particle with electric charge \"q\" travelling along some path \"P\" in a region with zero magnetic field B, but non-zero A (by formula_1), acquires a phase shift formula_2, given in SI units by\n\nIn a superconductor, the electrons form a quantum superconducting condensate, called a Bardeen–Cooper–Schrieffer (BCS) condensate. In the BCS condensate all electrons behave coherently, i.e. as one particle. Thus the phase of the collective BCS wavefunction behaves under the influence of the vector potential A in the same way as the phase of a single electron. Therefore the BCS condensate flowing around a closed path in a multiply connected superconducting sample acquires a phase difference Δ\"φ\" determined by the magnetic flux \"Φ\" through the area enclosed by the path (via Stokes' theorem and formula_4), and given by:\n\nThis phase effect is responsible for the quantized-flux requirement and the LP effect in superconducting loops and empty cylinders. The quantization occurs because the superconducting wave function must be single valued in a loop or an empty superconducting cylinder: its phase difference Δ\"φ\" around a closed loop must be an integer multiple of 2π, with the charge for the BCS electronic superconducting pairs.\n\nIf the period of the Little–Parks oscillations is 2π with respect to the superconducting phase variable, from the formula above it follows that the period with respect to the magnetic flux is the same as the magnetic flux quantum, namely\n\nLittle–Parks oscillations is a widely used proof mechanism of Cooper pairing. One of the good example is the study of the Superconductor Insulator Transition.\n\nThe challenge here is to separate LP oscillations from weak (anti-)localization (Altshuler et al. results, where authors observed the Aharonov–Bohm effect in a dirty metallic films).\n\nFritz London predicted that the fluxoid is quantized in a multiply connected superconductor. Experimentally has been shown, that the trapped magnetic flux existed only in discrete quantum units \"h\"/2\"e\". Deaver and Fairbank were able to achieve the accuracy 20–30% because of the wall thickness of the cylinder.\n\nLittle and Parks examined a \"thin-walled\" (Materials: Al, In, Pb, Sn and Sn–In alloys) cylinder (diameter was about 1 micron) at \"T\" very close to the transition temperature in an applied magnetic field in the axial direction. They found magnetoresistance oscillations with the period consistent with \"h\"/2\"e\".\n\nWhat they actually measured was an infinitely small changes of resistance versus temperature for (different) constant magnetic field, as it shown in Fig.\n"}
{"id": "24235618", "url": "https://en.wikipedia.org/wiki?curid=24235618", "title": "Metaklett", "text": "Metaklett\n\nMetaklett (from German \"Metall\", metal + \"Klettband\", Velcro ribbon) is a fastening material made of steel that acts on a similar principle to conventional hook and loop fasteners. It was developed by Reinz-Dichtungs-GmbH, Technische Universität München, Hölzel Stanz- und Feinwerktechnik GmbH & Co. KG and Koenig Verbindungstechnik GmbH. Metaklett is claimed to support shear strength 35 t/m² at temperatures up to 800 °C . It consists of two complementary strips of 0.2 mm thick perforated steel with catcher and holes. A second variant consists of two strips with protruding brushes and hooks. There is also a hybrid variant with one metal and one synthetic fleece ribbon.\n\nIn the context of the development Dr. Christoph Karl Hein compiled his dissertation \"Systematic analysis to metallic hook-and-loop bondings\".\n\n"}
{"id": "54291216", "url": "https://en.wikipedia.org/wiki?curid=54291216", "title": "Micropound", "text": "Micropound\n\nThe micropound (abbreviation μlb) is a small unit of avoirdupois weight and mass in the US and imperial systems of measurement, equal to one-millionth () pound. It is equal to exactly kg or about 453.6μg.\n\n"}
{"id": "7863793", "url": "https://en.wikipedia.org/wiki?curid=7863793", "title": "Molecular solid", "text": "Molecular solid\n\nA molecular solid is a solid consisting of discrete molecules. The cohesive forces that bind the molecules together are van der Waals forces, dipole-dipole interactions, quadrupole interactions, π-π interactions, hydrogen bonding, halogen bonding, London dispersion forces, and in some molecular solids, coulombic interactions. Van der Waals, dipole interactions, quadrupole interactions, π-π interactions, hydrogen bonding, and halogen bonding (2-127 kJ mol) are typically much weaker than the forces holding together other solids: metallic (metallic bonding, 400-500 kJ mol), ionic (Coulomb’s forces, 700-900 kJ mol), and network solids (covalent bonds, 150-900 kJ mol). Intermolecular interactions, typically do not involve delocalized electrons, unlike metallic and certain covalent bonds. Exceptions are charge-transfer complexes such as the tetrathiofulvane-tetracyanoquinodimethane (TTF-TCNQ), a radical ion salt. These differences in the strength of force (i.e. covalent vs. van der Waals) and electronic characteristics (i.e. delocalized electrons) from other types of solids give rise to the unique mechanical, electronic, and thermal properties of molecular solids.\n\nFor instance, molecular solids such as coronene have low conductivity (ρ = 1 x 10 to 1 x 10 Ω cm) making them poor electrical conductors. As mentioned there are exceptions such as TTF-TCNQ (ρ = 5 x 10 Ω cm), but it still substantially less than the conductivity of copper (ρ = 6 x 10 Ω cm). Molecular solids tend to have lower fracture toughness (sucrose, K = 0.08 MPa m) than metal (iron, K = 50 MPa m), ionic (sodium chloride, K = 0.5 MPa m), and covalent solids (diamond, K = 5 MPa m). Molecular solids have low melting (T) and boiling (T) points compared to metal (iron), ionic (sodium chloride), and covalent solids (diamond). Examples of molecular solids with low melting and boiling temperatures include argon, water, naphthalene, nicotine, and caffeine (see table below). The constituents of molecular solids range in size from condensed monatomic gases to small molecules (i.e. naphthalene and water) to large molecules with tens of atoms (i.e. fullerene with 60 carbon atoms).\nMolecular solids may consist of single atoms, diatomic, and/or polyatomic molecules. The intermolecular interactions between the constituents dictate how the crystal lattice of the material is structured. All atoms and molecules can partake in van der Waals and London dispersion forces (sterics). It is the lack or presence of other intermolecular interactions based on the atom or molecule that affords materials unique properties.\n\nArgon, is a noble gas that has a full octet, no charge, and is nonpolar. These characteristics make it unfavorable for Argon to partake in metallic, covalent, and ionic bonds as well as most intermolecular interactions. It can though partake in van der Waals and London dispersion forces. These weak self-interactions are isotropic and result in the long-range ordering of the atoms into face centered cubic packing when cooled below -189.3. Similarly iodine, a linear diatomic molecule has a net dipole of zero and can only partake in van der Waals interactions that are fairly isotropic. This results in the bipyramidal symmetry.\n\nFor acetone dipole-dipole interactions are a major driving force behind the structure of its crystal lattice. The negative dipole is caused by oxygen. Oxygen is more electronegative than carbon and hydrogen, causing a partial negative (δ-) and positive charge (δ+) on the oxygen and remainder of the molecule, respectively. The δ- orienttowards the δ+ causing the acetone molecules to prefer to align in a few configurations in a δ- to δ+ orientation (pictured left). The dipole-dipole and other intermolecular interactions align to minimize energy in the solid state and determine the crystal lattice structure.\n\nA quadrupole, like a dipole, is a permanent pole but the electric field of the molecule is not linear as in acetone, but in two dimensions. Examples of molecular solids with quadrupoles are octafluoronaphthalene and naphthalene. Naphthalene consists of two joined conjugated rings. The electronegativity of the atoms of this ring system and conjugation cause a ring current resulting in a quadrupole. For naphthalene, this quadrupole manifests in a δ- and δ+ accumulating within and outside the ring system, respectively. Naphthalene assembles through the coordination of δ- of one molecules to the δ+ of another molecule. This results in 1D columns of naphthalene in a herringbone configuration. These columns then stack into 2D layers and then 3D bulk materials. Octafluoronaphthalene follows this path of organization to build bulk material except the δ- and δ+ are on the exterior and interior of the ring system, respectively.\n\nA hydrogen bond is a specific dipole where a hydrogen atom has a partial positive charge (δ+) to due a neighboring electronegative atom or functional group. Hydrogen bonds are amongst the strong intermolecular interactions know other than ion-dipole interactions. For intermolecular hydrogen bonds the δ+ hydrogen interacts with a δ- on an adjacent molecule. Examples of molecular solids that hydrogen bond are water, amino acids, and acetic acid. For acetic acid, the hydrogen (δ+) on the alcohol moiety of the carboxylic acid hydrogen bonds with other the carbonyl moiety (δ-) of the carboxylic on the adjacent molecule. This hydrogen bond leads a string of acetic acid molecules hydrogen bonding to minimize free energy. These strings of acetic acid molecules then stack together to build solids.\n\nA halogen bond is when an electronegative halide participates in a noncovalent interaction with a less electronegative atom on an adjacent molecule. Examples of molecular solids that halogen bond are hexachlorobenzene and a cocrystal of bromine 1,4-dioxane. For the second example, the δ- bromine atom in the diatomic bromine molecule is aligning with the less electronegative oxygen in the 1,4-dioxane. The oxygen in this case is viewed as δ+ compared to the bromine atom. This coordination results in a chain-like organization that stack into 2D and then 3D.\n\nCoulombic interactions are manifested in some molecular solids. A well-studied example is the radical ion salt TTF-TCNQ with a conductivity of 5 x 10 Ω cm, much closer to copper (ρ = 6 x 10 Ω cm) than many molecular solids (e.g. coronene, ρ = 1 x 10 to 1 x 10 Ω cm). The coulombic interaction in TTF-TCNQ stems from the large partial negative charge (δ = -0.59) on the cyano- moiety on TCNQ at room temperature. For reference, a completely charged molecule δ = ±1. This partial negative charge leads to a strong interaction with the thio- moiety of the TTF. The strong interaction leads to favorable alignment of these functional groups adjacent to each other in the solid state. While π-π interactions cause the TTF and TCNQ to stack in separate columns.\n\nOne form of an element may be a molecular solid, but another form of that same element may not be a molecular solid. For example, solid phosphorus can crystallize as different allotropes called \"white\", \"red\", and \"black\" phosphorus. White phosphorus forms molecular crystals composed of tetrahedral P molecules. Heating at ambient pressure to 250 °C or exposing to sunlight converts white phosphorus to red phosphorus where the P tetrahedra are no longer isolated, but connected by covalent bonds into polymer-like chains. Heating white phosphorus under high (GPa) pressures converts it to black phosphorus which has a layered, graphite-like structure.\n\nThe structural transitions in phosphorus are reversible: upon releasing high pressure, black phosphorus gradually converts into the red phosphorus, and by vaporizing red phosphorus at 490 °C in an inert atmosphere and condensing the vapor, covalent red phosphorus can be transformed into the molecular solid, white phosphorus.\n\nSimilarly, yellow arsenic is a molecular solid composed of As units. Some forms of sulfur and selenium are composed of S (or Se) units and are molecular solids at ambient conditions, but converted into covalent allotropes having atomic chains extending throughout the crystal.\n\nSince molecular solids are held together by relatively weak forces they tend to have low melting and boiling points, low mechanical strength, low electrical conductivity, and poor thermal conductivity. Also, depending on the structure of the molecule the intermolecular forces may have directionality leading to anisotropy of certain properties.\n\nThe characteristic melting point of metals and ionic solids is ~ 1000 °C and greater, while molecular solids typically melt closer to 300 °C (see table), thus many corresponding substances are either liquid (ice) or gaseous (oxygen) at room temperature. This is due to the elements involved, the molecules they form, and the weak intermolecular interactions of the molecules.\nAllotropes of phosphorus are useful to further demonstrate this structure-property relationship. White phosphorus, a molecular solid, has a relatively low density of 1.82 g/cm and melting point of 44.1 °C; it is a soft material which can be cut with a knife. When it is converted to the covalent red phosphorus, the density goes to 2.2–2.4 g/cm and melting point to 590 °C, and when white phosphorus is transformed into the (also covalent) black phosphorus, the density becomes 2.69–3.8 g/cm and melting temperature ~200 °C. Both red and black phosphorus forms are significantly harder than white phosphorus.\n\nMolecular solids can be either ductile or brittle, or a combination depending on the crystal face stressed. Both ductile and brittle solids undergo elastic deformation till they reach the yield stress. Once the yield stress is reached ductile solids undergo a period of plastic deformation, and eventually fracture. Brittle solids fracture promptly after passing the yield stress. Due to the asymmetric structure of most molecules, many molecular solids have directional intermolecular forces. This phenomenon can lead to anisotropic mechanical properties. Typically a molecular solid is ductile when it has directional intermolecular interactions. This allows for dislocation between layers of the crystal much like metals.\n\nOne example of a ductile molecular solid, that can be bent 180°, is hexachlorobenzene (HCB). In this example the π-π interactions between the benzene cores are stronger than the halogen interactions of the chlorides. This difference leads to its flexibility. This flexibility is anisotropic; to bend HCB to 180° you must stress the [001] face of the crystal. Another example of a flexible molecular solid is 2-(methylthio)nicotinic acid (MTN). MTN is flexible due to its strong hydrogen bonding and π-π interactions creating a rigid set of dimers that dislocate along the alignment of their terminal methyls. When stressed on the [010] face this crystal will bend 180°. Note, not all ductile molecular solids bend 180° and some may have more than one bending faces.\n\nMany molecular solids have a large band gap making them insulators. For example, coronene has band gap of 2.4 eV. This large band gap (compared to Germanium at 0.7 eV) is due to the discrete nature of the molecules and relatively weak intermolecular interactions. These factors result in low charge carrier mobility and thus conductivity. There are cases though in which molecular solids can be relatively good conductors: 1) when the molecules partake in ion-radical chemistry and 2) when the solids are doped with atoms, molecules, or materials. A well-known example of such an ion radical salt is TTF-TCNQ. TTF-TCNQ (ρ = 5 x 10 Ω cm) is more conductive than other molecular solids (i.e. coronene, ρ = 1 x 10 to 1 x 10 Ω cm)) because the TCNQ charge donor has such a strong partial negative charge (δ = 0.59) making the intermolecular interactions more coulombic in electronic character. This partial charge increase with decreasing temperature. The coulombic major component of the lattice energy causing the electrical conduction of the crystal to be anisotropic. Fullerenes are an example of how a molecular solid can be doped to become a conductor. A solid purely consisting of fullerenes is an insulator because the valence electrons of the carbon atoms are primarily involved in the covalent bonds within the individual carbon molecules. However, inserting (intercalating) alkali metal atoms between the fullerene molecules provides extra electrons, which can be easily ionized from the metal atoms and make the material conductive.\n\nMolecular solids have many thermal properties: specific heat capacity, thermal expansion, and thermal conductance to name a few. These thermal properties are determined by the intra- and intermolecular vibrations of the atoms and molecules of the molecular solid. While transitions of an electron do contribute to thermal properties, their contribution is negligible compared to the vibrational contribution.\n\n\n"}
{"id": "30251550", "url": "https://en.wikipedia.org/wiki?curid=30251550", "title": "NASA wind turbines", "text": "NASA wind turbines\n\nStarting in 1975, NASA managed a program for the United States Department of Energy and the United States Department of Interior to develop utility-scale wind turbines for electric power, in response to the increase in oil prices.\nA number of the world's largest wind turbines were developed and tested under this pioneering program. The program was an attempt to leap well beyond the then-current state of the art of wind turbine generators, and developed a number of technologies later adopted by the wind turbine industry. The development of the commercial industry however was delayed by a significant decrease in competing energy prices during the 1980s.\n\nIn 1974, partially in response to the increase in oil price after the 1973 oil crisis, the Energy Research and Development Administration (ERDA), later part of United States Department of Energy, appointed a department under the direction of Louis Divone to fund research into utility-scale wind turbines. NASA, through its Lewis Research Center in Sandusky Ohio (now the Glenn Research Center) was assigned the task of coordination of development by large contractors such as General Electric, Westinghouse, United Technologies and Boeing.\n\nIn 1975 NASA designed and built its first prototype wind turbine, the 100 kW Mod-0 in Sandusky Ohio, with funding from the National Science Foundation and ERDA. The Mod-0 was modeled after the light weight two-bladed research turbine by Austrian Ulrich Hütter. The two-bladed wind turbine with flexible or teetered rotor hubs characterized the NASA-led program. NASA and its contractors found that two blades can produce essentially equivalent energy as three blades but at a savings of the cost and weight of a blade. Two-blade rotors turn faster than equivalent three-blade rotors, reducing the ratio in the gearbox. Flexibility in the rotor minimizes the transfer of bending loads into the drive train; none of the NASA wind turbines experienced gearbox failures that are often a problem for rigid rotor systems in use today.\n\nThe NASA program hosted technical conferences, inviting international partners. NASA even helped refurbish and operate the Danish three-bladed Gedser wind turbine between 1977 and 1979, so that its operation and characteristics could be studied as a model for larger units. This 1957 unit designed by Johannes Juul generated 200 kW for 11 years, and used a three-bladed upwind rotor with a lattice tower and blades supported partly by internal guy wires. The effort produce research data on its aerodynamic, electrical, and mechanical characteristics. An important result of this effort was the development of an engineering design model used by the industry for passive power control.\n\nLarger wind turbine units achieve economies of scale. NASA research and prototypes demonstrated that there were considerable scaling challenges in structural strength, fatigue, speed control, and aerodynamics. In the 1980s most wind turbines were small units up to 25 kW rating. Studies carried out by NASA's contractors suggested that much larger units would be required, on the order of 1 MW or more, for economic production of electricity by utilities. Although the largest-diameter sets of propeller blades then in use were for helicopters, which spanned only 46 feet, it was projected that large blade sets, covering 200 to 300 feet in diameter, would be feasible to build and would produce the lowest cost of energy.\n\nThe first design was MOD-0, built near the Lewis Research Center in Sandusky, Ohio and operational in September 1975. It served as a test bed for development of many concepts for use in larger units. This design had a 38-metre diameter downwind two-bladed rotor, coupled to a synchronous generator, with a power rating of 100 kW at 8 m/s wind speed. A speed increaser stepped up the 40 r/min of the turbine to drive an 1800 r/min generator. The power output of the machine was regulated by pitching the rotor blades.\n\nThe initial MOD-0 blades were made by Lockheed, out of aluminum. Structural problems surfaced almost immediately at the root end of the blades. Several significant changes and efforts were performed to address this. An investigation revealed that unexpectedly high cyclic loads were the result of a significant blockage of the wind by the complex truss tower structure. This caused the aerodynamic loads on the downwind rotor rapidly change. To correct this blockage, the access stairs were removed from the center of the tower. A major blade material program was started that assessed fiberglass composite, steel, wood and even concrete. NASA approached the Gougeon Brothers, Inc. of Michigan to apply their boat material technology to wind turbines. The resulting wood and composite blades replaced the aluminum blades on th Mod-0 (and later Mod-0A), eliminating the blade root structural problems. Gougeon Brothers successfully commercialized their products into the wind turbine industry with sales around the world.\n\nMany experiments were done with MOD-0, including brief operation with the rotor blades upwind of the tower, and a trial of a single blade for the turbine rotor. It tested the first variable speed generators as well prior to their use in the 3.2 MW Mod-5B and later throughout the industry. The Mod-0 was also used to test the first steel shell towers, now the dominant tower design. The design challenge was to take weight and cost out of the tower while safely passing through a resonance of the soft structure during startup.\n\nOperating experience with the prototype MOD-0 provided the basis for construction of several demonstration units designated the MOD-0A. These were similar to the prototype with the same rotor size, but rated at 200 kW at slightly higher wind speed. Westinghouse was appointed as prime contractor responsible for the overall construction. Units were installed at Clayton New Mexico in 1977, Culebra, Puerto Rico in 1978, Block Island, Rhode Island in 1979 and the fourth at Kahaku Point Hawaii in 1980.\n\nNASA contracted with General Electric in 1978 to scale up from the MOD-0A with a 10-fold increase in power. The Mod-1 was the first wind turbine in the world to produce 2 megawatts and also General Electric's first wind turbine. The Danish with a hub height of 46 meters above the ground, a larger rotor and a rating at a higher wind speed, had a capacity of 2000 kW but never achieved 2 MW power output of the Mod-1. The Mod-1's design weight prevented it from becoming a competitive commercial product, but a prototype was installed and run at Howard's Knob near Boone, North Carolina. The quick design cycle to multi-megawatt size based on the first generation Mod-0 caused technical and operational challenges. Low-frequency noise from the heavy truss tower blocking the wind to the downwind rotor caused problems to residences located close by. With additional pressure of a reduction in federal program funding, the turbine was dismantled in 1983.\n\nIn 1977 Boeing won the NASA and US-DOE contract for the design, fabrication, construction, installation and testing of several 2.5-megawatt wind turbine models in the United States. The first four MOD-2 models went into operation during the early 1980s. The dedication ceremony for the first three turbines was held on April 17, 1980, at Goodnoe Hills, Washington. On September 2, 1982, a fourth began operating at Medicine Bow, Wyoming. The Bonneville Power Administration bought the generated electricity of the Goodnoe Hills turbines and integrated it into the regional power grid. During the periods of May 1981, the three turbines at the Goodnoe Hills site formed the first wind farm in the world. The Goodnoe Hills site was primarily a research project for Boeing, Bonneville Power Administration, NASA and the Battelle Memorial Institute. The Solar Energy Research Institute also evaluated the suitability of megawatt-class wind turbines as a source of electricity. During 1986, the MOD-2 wind turbines of Goodnoe Hills were dismantled. In 1985, the last full year of operation, the combined electrical output of the three turbines was 8,251 megawatt-hours. The Medicine Bow MOD-2 wind turbine was sold for scrap in 1987. In 2008, the Goodnoe Hills Wind Farm opened on the same site with 47 REpower 2.0 MW wind turbines for a combined nameplate capacity of 94 MW. enXco/Power Holdings owns the wind farm, with PacifiCorp as the power purchaser.\n\nThe WTS-4 (4 megawatt) wind turbine in Wyoming was designed by United Technologies (Hamilton Standard Division), under the technical management of NASA and with funding from the United States Department of Interior. The WTS-4 was placed into operation in Medicine Bow, Wyoming in 1982. It featured a \"soft\" steel tube tower, fiberglass blades, torsional springs and dashpots in the drivetrain, and a flexible teetered hub. To this day, the WTS-4 is the most powerful wind turbine to have operated in the US and it held the world record for power output for over 20 years. A second commercial prototype with a smaller generator (3 megawatts) designated the WTS-3 was constructed and operated in Sweden. Glidden Doman was Hamilton Standard's System Design Manager for the WTS-4 project and was hired by the Swedish government to initiate the WTS-3 project.\n\nThe MOD-5B wind turbine, built in 1987, was the largest operating wind turbine in the world in the 1990s. The contract to build the Mod-5B was awarded to Boeing in 1980 and it was installed on Oahu in 1987. With a rated capacity of 3.2 megawatts, it weighed and had a diameter two-blade rotor on a steel tower. Early operation of the Mod-5B demonstrated a good availability of 95 percent for the new first-unit wind turbine. Early in 1988, operation of the turbine was transferred to Hawaiian Electric Industries, then to the Makani Uwila Power Corporations (MUPC), and kept in service intermittently until late in 1996. Because of financial difficulties, the wind turbine was shut down, along with the rest of MUPC, and passed to the property owner, Campbell Estates. Campbell Estates decided to disassemble the unit and sell it for scrap. The DOE salvaged the drive train gearbox and generator in July 1998.\n\nNone of the NASA prototypes became commonly produced as commercial generators because the purpose of the program was to develop the technology and support the emerging industry. Many of the technologies such as doubly fed variable speed generators, light weight tubular towers, and the engineering design tools used in the wind industry today were developed and pioneered by the NASA program. Total cost of the program between 1974 and 1992 was $330 million. For reference, the global wind market had reached $47 billion annually by 2008. General Electric, Boeing Engineering and Construction, Westinghouse and United Technologies were the commercial partners on the program, some of whom are involved in the wind industry today. Although it has widely been stated that no commercial designs were produced, NASA's industry partners did indeed produce commercial turbines during this program such as the Boeing Mod-2 (described earlier) and Westinghouse 600 kW turbines at Kahuku wind farm in Hawaii. When oil prices declined by a factor of three from 1980 through the early 1990s during what came to be known as the \"1980s oil glut\", many turbine manufacturers, both large and small, left the business. The commercial sales of the NASA/Boeing Mod-5B, for example, came to an end in 1987 when Boeing Engineering and Construction announced they were \"planning to leave the market because low oil prices are keeping windmills for electricity generation uneconomical.\" A summary of the DOE/NASA large wind turbine program was published in 1984.\n\n\n\n"}
{"id": "11202225", "url": "https://en.wikipedia.org/wiki?curid=11202225", "title": "NUR Reactor", "text": "NUR Reactor\n\nNUR Reactor is an open pool research reactor at Centre de Dévéloppement des Techniques Nucléaires in Algiers. \n\nBuilt by INVAP, it is 1MWt and based on RA-6. It achieved first criticality in April 1989.\n\n"}
{"id": "3051710", "url": "https://en.wikipedia.org/wiki?curid=3051710", "title": "Nano-thermite", "text": "Nano-thermite\n\nNano-thermite or super-thermite is a metastable intermolecular composite (MICs) characterized by a particle size of its main constituents, a metal and a metal oxide, under 100 nanometers. This allows for high and customizable reaction rates. Nano-thermites contain an oxidizer and a reducing agent, which are intimately mixed on the nanometer scale. MICs, including nano-thermitic materials, are a type of reactive materials investigated for military use, as well as for general applications involving propellants, explosives, and pyrotechnics.\n\nWhat distinguishes MICs from traditional thermites is that the oxidizer and a reducing agent, normally iron oxide and aluminium, are in the form of extremely fine powders (nanoparticles). This dramatically increases the reactivity relative to micrometre-sized powder thermite. As the mass transport mechanisms that slow down the burning rates of traditional thermites are not so important at these scales, the reaction proceeds much more quickly.\n\nHistorically, pyrotechnic or explosive applications for traditional thermites have been limited due to their relatively slow energy release rates. Because nanothermites are created from reactant particles with proximities approaching the atomic scale, energy release rates are far greater.\n\nMICs or Super-thermites are generally developed for military use, propellants, explosives, incendiary devices, and pyrotechnics. Research into military applications of nano-sized materials began in the early 1990s. Because of their highly increased reaction rate, nanosized thermitic materials are being studied by the U.S. military with the aim of developing new types of bombs several times more powerful than conventional explosives. Nanoenergetic materials can store more energy than conventional energetic materials and can be used in innovative ways to tailor the release of this energy. Thermobaric weapons are one potential application of nanoenergetic materials.\n\nThere are many possible thermodynamically stable fuel-oxidizer combinations. Some of them are:\n\nIn military research, aluminium-molybdenum oxide, aluminium-Teflon and aluminium-copper(II) oxide have received considerable attention. Other compositions tested were based on nanosized RDX and with thermoplastic elastomers. PTFE or other fluoropolymer can be used as a binder for the composition. Its reaction with the aluminium, similar to magnesium/teflon/viton thermite, adds energy to the reaction. Of the listed compositions, that with potassium permanganate has the highest pressurization rate.\n\nA method for producing nano scale, or ultra fine grain (UFG) aluminum powders, a key component of most nano-thermitic materials, is the dynamic gas-phase condensation method, pioneered by Wayne Danen and Steve Son at Los Alamos National Laboratory. A variant of the method is being used at the Indian Head Division of the Naval Surface Warfare Center. The powders made by both processes are indistinguishable.\nA critical aspect of the production is the ability to produce particles of sizes in the tens of nano-meter range, as well as with a limited distribution of particle sizes. In 2002, the production of nano-sized aluminum particles required considerable effort, and commercial sources for the material were limited. Current production levels are now beyond 100 kg/month.\n\nAn application of the sol-gel method, developed by Randall Simpson, Alexander Gash and others at the Lawrence Livermore National Laboratory, can be used to make the actual mixtures of nano-structured composite energetic materials. Depending on the process, MICs of different density can be produced. Highly porous and uniform products can be achieved by super-critical extraction.\n\nAs with all explosives, research into control yet simplicity has been a goal of research into nanoscale explosives. Some can be ignited with laser pulses. \n\nMICs have been investigated as a possible replacement for lead (e.g. lead styphnate, lead azide) in percussion caps and electric matches. Compositions based on Al-BiO tend to be used. PETN may be optionally added.\n\nAluminium powder can be added to nano explosives. Aluminium has a relatively low combustion rate and a high enthalpy of combustion.\n\nThe products of a thermite reaction, resulting from ignition of the thermitic mixture, are usually metal oxides and elemental metals. At the temperatures prevailing during the reaction, the products can be solid, liquid or gaseous, depending on the components of the mixture.\n\nLike conventional thermite, super thermite reacts at very high temperature and is difficult to extinguish. The reaction produces dangerous ultra-violet (UV) light requiring that the reaction not be viewed directly, or that special eye protection (for example, a welder's mask) be worn.\n\nIn addition, super thermites are very sensitive to electrostatic discharge (ESD). Surrounding the metal oxide particles with carbon nanofibers may make nanothermites safer to handle.\n\n\n"}
{"id": "4365438", "url": "https://en.wikipedia.org/wiki?curid=4365438", "title": "National Spherical Torus Experiment", "text": "National Spherical Torus Experiment\n\nThe National Spherical Torus Experiment (NSTX) is a magnetic fusion device based on the \"spherical tokamak\" concept. It was constructed by the Princeton Plasma Physics Laboratory (PPPL) in collaboration with the Oak Ridge National Laboratory, Columbia University, and the University of Washington at Seattle.\n\nThe spherical tokamak (ST) is an offshoot of the conventional tokamak design. Proponents claim that it has a number of practical advantages over these devices, some of them dramatic. For this reason the ST has seen considerable interest since it was proposed in the late 1980s. However, development remains effectively one generation behind mainline efforts such as JET. Other major experiments in the field include the pioneering START and MAST at Culham in the UK.\n\nNSTX studies the physics principles of spherically shaped plasmas—hot ionized gases in which nuclear fusion will occur under the appropriate conditions of temperature and density, which are produced by confinement in a magnetic field.\n\nFirst plasma was obtained on NSTX on Friday, February 12, 1999 at 6:06 p.m.\n\nMagnetic fusion experiments use plasmas composed of one or more hydrogen isotopes. For example, in 1994, PPPL's Tokamak Fusion Test Reactor (TFTR) produced a world-record 10.7 megawatts of fusion power from a plasma composed of equal parts of deuterium and tritium, a fuel mix likely to be used in commercial fusion power reactors. NSTX was a \"proof of principle\" experiment and therefore employed deuterium plasmas only. If successful it was to be followed by similar devices, eventually including a demonstration power reactor (e.g. ITER), burning deuterium-tritium fuel.\n\nNSTX produced a spherical plasma with a hole through its center (a \"cored apple\" profile; see MAST), different from the doughnut-shaped (toroidal) plasmas of conventional tokamaks. The low aspect ratio \"A\" (that is, an \"R\"/\"a\" of 1.31, with the major radius \"R\" of 0.85 m and the minor radius \"a\" of 0.65 m) experimental NSTX device had several advantages including plasma stability through improved confinement. Design challenges include the toroidal and poloidal field coils, vacuum vessels and plasma-facing components. This plasma configuration can confine a higher pressure plasma than a doughnut tokamak of high aspect ratio for a given, confinement magnetic field strength. Since the amount of fusion power produced is proportional to the square of the plasma pressure, the use of spherically shaped plasmas could allow the development of smaller, more economical and more stable fusion reactors. NSTX's attractiveness may be further enhanced by its ability to trap a high \"bootstrap\" electric current. This self-driven internal plasma current would reduce the power requirements of externally driven plasma currents required to heat and confine the plasma.\n\nThe NSTX-U (Upgrade) was completed in 2015. It doubles the toroidal field (to 1 Tesla), plasma current (to 2 MA) and heating power. It increases the pulse duration by a factor of five. To achieve this the central stack (solenoid) was widened.\n\n"}
{"id": "3385681", "url": "https://en.wikipedia.org/wiki?curid=3385681", "title": "Oil terminal", "text": "Oil terminal\n\nAn oil depot (sometimes called a tank farm, tankfarm, installation or oil terminal) is an industrial facility for the storage of oil and/or petrochemical products and from which these products are usually transported to end users or further storage facilities. An oil depot typically has tankage, either above ground or below ground, and gantries (framework) for the discharge of products into road tankers or other vehicles (such as barges) or pipelines. \n\nOil depots are usually situated close to oil refineries or in locations where marine tankers containing products can discharge their cargo. Some depots are attached to pipelines from which they draw their supplies and depots can also be fed by rail, by barge and by road tanker (sometimes known as \"bridging\").\n\nMost oil depots have road tankers operating from their grounds and these vehicles transport products to petrol stations or other users.\n\nAn oil depot is a comparatively unsophisticated facility in that (in most cases) there is no processing or other transformation on site. The products which reach the depot (from a refinery) are in their final form suitable for delivery to customers. In some cases additives may be injected into products in tanks, but there is usually no manufacturing plant on site. Modern depots comprise the same types of tankage, pipelines and gantries as those in the past and although there is a greater degree of automation on site, there have been few significant changes in depot operational activities over time.\n\nOne of the key imperatives is Health, Safety and Environment (HSE) and the operators of a depot must ensure that products are safely stored and handled. There must be no leakages (etc.) which could damage the soil or the water table. \nFire protection is a primary consideration, especially for the more flammable products such as petrol (gasoline) and Aviation Fuel.\n\nThe ownership of oil depots falls into three main categories:\n\n\nIn all cases the owners may also provide \"hospitality\" or \"pick up rights\" at the facility to other companies.\n\nMost airports also have their own dedicated oil depots (usually called \"fuel farms\") where aviation fuel (Jet A or 100LL) is stored prior to being discharged into aircraft fuel tanks. Fuel is transported from the depot to the aircraft either by road tanker or via a hydrant system.\n\nThe world's third largest oil consumer had national reserves of 113 days of oil demand under the government's storage and 85 days held by the private sector at the end of December 2010. In this respect, the total oil stored in Japan in December stood at 587.4 million barrels. Japan requires the private sector to hold 70 days as oil reserves, but is making the period shorter by three days to 67 days. As such it will allow oil companies to release 8.9 million barrels of crude oil from mandatory stockpiles.\n\n"}
{"id": "22305", "url": "https://en.wikipedia.org/wiki?curid=22305", "title": "Oxide", "text": "Oxide\n\nAn oxide is a chemical compound that contains at least one oxygen atom and one other element in its chemical formula. \"Oxide\" itself is the dianion of oxygen, an O atom. Metal oxides thus typically contain an anion of oxygen in the oxidation state of −2. Most of the Earth's crust consists of solid oxides, the result of elements being oxidized by the oxygen in air or in water. Hydrocarbon combustion affords the two principal carbon oxides: carbon monoxide and carbon dioxide. Even materials considered pure elements often develop an oxide coating. For example, aluminium foil develops a thin skin of AlO (called a passivation layer) that protects the foil from further corrosion. Individual elements can often form multiple oxides, each containing different amounts of the element and oxygen. In some cases these are distinguished by specifying the number of atoms as in carbon monoxide and carbon dioxide, and in other cases by specifying the element's oxidation number, as in iron(II) oxide and iron(III) oxide. Certain elements can form many different oxides, such as those of nitrogen.\n\nDue to its electronegativity, oxygen forms stable chemical bonds with almost all elements to give the corresponding oxides. Noble metals (such as gold or platinum) are prized because they resist direct chemical combination with oxygen, and substances like gold(III) oxide must be generated by indirect routes.\n\nTwo independent pathways for corrosion of elements are hydrolysis and oxidation by oxygen. The combination of water and oxygen is even more corrosive. Virtually all elements burn in an atmosphere of oxygen, or an oxygen rich environment. In the presence of water and oxygen (or simply air), some elements— sodium—react rapidly, to give the hydroxides. In part for this reason, alkali and alkaline earth metals are not found in nature in their metallic, i.e., native, form. Caesium is so reactive with oxygen that it is used as a getter in vacuum tubes, and solutions of potassium and sodium, so-called NaK are used to deoxygenate and dehydrate some organic solvents. The surface of most metals consists of oxides and hydroxides in the presence of air. A well-known example is aluminium foil, which is coated with a thin film of aluminium oxide that passivates the metal, slowing further corrosion. The aluminium oxide layer can be built to greater thickness by the process of electrolytic anodising. Though solid magnesium and aluminium react slowly with oxygen at STP—they, like most metals, burn in air, generating very high temperatures. Finely grained powders of most metals can be dangerously explosive in air. Consequently, they are often used in solid-fuel rockets.\nIn dry oxygen, iron readily forms iron(II) oxide, but the formation of the hydrated ferric oxides, FeO(OH), that mainly comprise rust, typically requires oxygen \"and\" water. Free oxygen production by photosynthetic bacteria some 3.5 billion years ago precipitated iron out of solution in the oceans as FeO in the economically important iron ore hematite.\n\nOxides have a range of different structures, from individual molecules to polymeric and crystalline structures. At standard conditions, oxides may range from solids to gases.\n\nOxides of most metals adopt polymeric structures. The oxide typically links three metal atoms (e.g., rutile structure) or six metal atoms (carborundum or rock salt structures). Because the M-O bonds are typically strong and these compounds are crosslinked polymers, the solids tend to be insoluble in solvents, though they are attacked by acids and bases. The formulas are often deceptively simple. Many are nonstoichiometric compounds.\n\nAlthough most metal oxides are polymeric, some oxides are molecules. Examples of molecular oxides are carbon dioxide and carbon monoxide. All simple oxides of nitrogen are molecular, e.g., NO, NO, NO and NO. Phosphorus pentoxide is a more complex molecular oxide with a deceptive name, the real formula being PO. Some polymeric oxides depolymerize when heated to give molecules, examples being selenium dioxide and sulfur trioxide. Tetroxides are rare. The more common examples: ruthenium tetroxide, osmium tetroxide, and xenon tetroxide.\n\nMany oxyanions are known, such as polyphosphates and polyoxometalates. Oxycations are rarer, some examples being nitrosonium (NO), vanadyl (VO), and uranyl (). Of course many compounds are known with both oxides and other groups. In organic chemistry, these include ketones and many related carbonyl compounds. For the transition metals, many oxo complexes are known as well as oxyhalides.\n\nConversion of a metal oxide to the metal is called reduction. Reduction can be induced with many reagents. Many metal oxides convert to metals simply by heating.\n\nMetals are \"won\" from their oxides by chemical reduction, i.e. by the addition of a chemical reagent. A common and cheap reducing agent is carbon in the form of coke. The most prominent example is that of iron ore smelting. Many reactions are involved, but the simplified equation is usually shown as:\n\nMetal oxides can be reduced by organic compounds. This redox process is the basis for many important transformations in chemistry, such as the detoxification of drugs by the P450 enzymes and the production of ethylene oxide, which is converted to antifreeze. In such systems the metal centre transfers an oxide ligand to the organic compound followed by regeneration of the metal oxide, often by oxygen in air.\n\nMetals that are lower in the reactivity series can be reduced by heating alone. For example, silver oxide decomposes at 200 °C:\n\nMetals that are more reactive displace the oxide of the metals that are less reactive. For example, zinc is more reactive than copper, so it displaces copper (II) oxide to form zinc oxide:\n\nApart from metals, hydrogen can also displace metal oxides to form hydrogen oxide, also known as water:\n\nSince metals that are reactive form oxides that are stable, some metal oxides must be electrolyzed to be reduced. This includes sodium oxide, potassium oxide, calcium oxide, magnesium oxide, and aluminium oxide. The oxides must be molten before immersing graphite electrodes in them:\n\nOxides typically react with acids or bases, sometimes both. Those reacting only with acids are labeled basic oxides. Those reacting only by bases are called \"acidic oxides\". Oxides that react with both are amphoteric. Metals tend to form basic oxides, non-metals tend to form acidic oxides, and amphoteric oxides are formed by elements near the boundary between metals and non-metals (metalloids). This reactivity is the basis of many practical processes, such as the extraction of some metals from their ores in the process called hydrometallurgy.\n\nOxides of more electropositive elements tend to be basic. They are called \"basic anhydrides\". Exposed to water, they may form basic hydroxides. For example, sodium oxide is basic—when hydrated, it forms sodium hydroxide. Oxides of more electronegative elements tend to be acidic. They are called \"acid anhydrides\"; adding water, they form oxoacids. For example, dichlorine heptoxide is an acid anhydride; perchloric acid is its fully hydrated form. Some oxides can act as both acid and base. They are amphoteric. An example is aluminium oxide. Some oxides do not show behavior as either acid or base.\n\nThe oxide ion has the formula O. It is the conjugate base of the hydroxide ion, OH and is encountered in ionic solids such as calcium oxide. O is unstable in aqueous solution − its affinity for H is so great (p\"K\" ~ −38) that it abstracts a proton from a solvent HO molecule:\n\nThe equilibrium constant of aforesaid reactions is pK ~ −22\n\nIn the 18th century, oxides were named calxes or calces after the calcination process used to produce oxides. \"Calx\" was later replaced by \"oxyd.\" \n\nThe reductive dissolution of a transition metal oxide occurs when dissolution is coupled to a redox event. For example, ferric oxides dissolve in the presence of reductants, which can include organic compounds. or bacteria Reductive dissolution is integral to geochemical phenomena such as the iron cycle.\n\nReductive dissolution does not necessarily occur at the site where the reductant adsorbs. Instead, the added electron travel through the particle, causing reductive dissolution elsewhere on the particle.\n\nSometimes, metal-oxygen ratios are used to name oxides. Thus, NbO would be called niobium monoxide and TiO is titanium dioxide. This naming follows the Greek numerical prefixes. In the older literature and continuing in industry, oxides are named by adding the suffix \"-a\" to the element's name. Hence alumina, magnesia and chromia, are, respectively, AlO, MgO and CrO.\n\nSpecial types of oxides are peroxide, O, and superoxide, O. In such species, oxygen is assigned higher oxidation states than oxide. \n\nThe chemical formulas of the oxides of the chemical elements in their highest oxidation state are predictable and are derived from the number of valence electrons for that element. Even the chemical formula of O, tetraoxygen, is predictable as a group 16 element. One exception is copper, for which the highest oxidation state oxide is copper(II) oxide and not copper(I) oxide. Another exception is fluoride, which does not exist as one might expect—as FO—but as OF.\n\nSince fluorine is more electronegative than oxygen, oxygen difluoride (OF) does not represent an oxide of fluorine, but instead represents a fluoride of oxygen.\n\nThe following table gives examples of commonly encountered oxides. Only a few representatives are given, as the number of polyatomic ions encountered in practice is very large.\n\n"}
{"id": "56738396", "url": "https://en.wikipedia.org/wiki?curid=56738396", "title": "Peter Gottfried Kremsner", "text": "Peter Gottfried Kremsner\n\nPeter Gottfried Kremsner (born 16 May 1961 in Wiener Neustadt, Austria) is a specialist in tropical medicine and Full Professor at the University of Tübingen, Germany. Since 1992 he has been leading the Centre de Recherches Médicales de Lambaréné (CERMEL), Gabon, now as president. For about three decades Kremsner has played an important role in the worldwide research of tropical infectious diseases.\n\nKremsner grew up in Sigleß, Austria. He studied medicine at the University of Vienna and graduated as a Doctor of Medicine in 1985. He started his academic career as a medical researcher at the Institute of Specific Prophylaxis and Tropical Medicine, University of Vienna. 1987 he went to Brazil working there for the Superintendencia de Campanhas de Saude Publica at Rio Branco. From 1988 to 1996 he held the position of a group leader at the Institute of Tropical Medicine in Berlin. 1990 he earned his habilitation for Tropical Medicine and Specific Prophylaxis at the University of Vienna and 1992 an additional habilitation for Tropical Medicine and Parasitology at the Humboldt University of Berlin. \n1992 he started the medical research center in Lambaréné, now Centre de Recherches Médicales de Lambaréné (CERMEL), and made it one of the premier research and training centers in Africa. 1996 Kremsner was appointed Professor for Parasitology at the University of Tübingen. Since 2008 he has been Chairman and Professor for Tropical Medicine, Travel Medicine and Parasitology at Tübingen University as well as Director of the Institute for Tropical Medicine. In 2014 he was additionally appointed CEO of the Comprehensive Infectious Disease Center of University Hospital Tübingen. 2016 he became also Adjunct Professor at the Medical University of Vienna.\n\nKremsner led numerous studies on tropical infectious diseases and is author of more than 600 scientific publications. Since 20 years he is one of the most cited scientists in the field of parasitology and malaria in Europe and worldwide. He has been principal investigator on key studies for the development of atovaquone/proguanil, artesunate/amodiaquine, artesunate/pyronaridine and parenteral artesunate for malaria therapy and prophylaxis. \nKremsner and his team developed a simplified method for assessment of severity of malaria. Outcome can now be predicted and therapy focused by health care providers using the \"Lambaréné Score\", which uses two clinical characteristics, coma and deep breathing without laboratory assessment. This was accomplished by using data from a study in 26,000 children with severe malaria in Africa led by him. \nKremsner took part in the phase 3 testing of the malaria vaccine, RTS,S/AS01, as a member of the governing clinical trial partnership committee and coordinator of clinical trials in Lambaréné. Since 2011, he and his team are working on development of other malaria vaccines, notably PfSPZ-based vaccines in cooperation with Sanaria Inc. A study carried out by his team in Tübingen of a PfSPZ vaccine showed 100 percent protection, a level consistent with WHO guidelines for worldwide use. Kremsner and his teams in Tübingen and Lambaréné also established a Plasmodium falciparum controlled human malaria infection (CHMI) model. Use of this CHMI model dramatically reduces the time needed for clinical development of malaria vaccine and drug candidates.\n\nKremsner is married to the lawyer Inge Thomforde. They have three children (Helene, Gottfried and Ferdinand). He is interested in classical music, arts and literature.\nIn addition to his citizenship of the European Union (Austrian), Kremsner is also a Gabonese citizen.\n\n\n"}
{"id": "53218", "url": "https://en.wikipedia.org/wiki?curid=53218", "title": "Pionium", "text": "Pionium\n\nPionium is an exotic atom consisting of one and one meson. It can be created, for instance, by interaction of a proton beam accelerated by a particle accelerator and a target nucleus. Pionium has a short lifetime, predicted by chiral perturbation theory to be . It decays mainly into two mesons, and to a smaller extent into two photons.\n\nPionium is currently under investigation at CERN to measure its lifetime. The Dimeson Relativistic Atomic Complex (DIRAC) experiment at the Proton Synchrotron was able to detect 21227 atomic pairs from a total of events, which allows the pionium lifetime to be determined to within statistical errors of 9%.<ref name=\"hep-ph/9808407\">\n</ref>\n\nIn 2006, the NA48/2 collaboration at CERN published an evidence for pionium production and decay in decays of charged kaons, studying mass spectra of daughter pion pairs in the events with three pions in the final state K → π(ππ) → πππ.<ref name=\"hep-ex/0511056\">\n</ref> This was followed by a precision measurement of the S-wave pion scattering length, published by the collaboration in 2009.\n\nThe results of the above experiments will provide crucial tests of low-energy QCD predictions.\n\n"}
{"id": "3909371", "url": "https://en.wikipedia.org/wiki?curid=3909371", "title": "Radiation flux", "text": "Radiation flux\n\nRadiation flux is a measure of the amount of radiation received by an object from a given source. This can be any type of radiation, including electromagnetic, sound, and particles from a radioactive source.\n\nΦ = is the radiation flux, L is the luminosity, or total power output of the source, and r is the distance from the radiation source. The units of radiation flux are W·m, or kg·s.\n\nRadiation flux density is a related measure that takes into account the area the radiation flux passes through, and is defined as the flux divided by the area it passes through. The Radiation flux density is also known as Intensity where I = \n"}
{"id": "17166338", "url": "https://en.wikipedia.org/wiki?curid=17166338", "title": "Richard M. Brett", "text": "Richard M. Brett\n\nRichard M. Brett (September 3, 1903 – September 7, 1989) was an American conservationist and author.\n\nBrett was born in Darien, Connecticut and spent most of his life in Woodstock, Vermont, and Fairfield, Connecticut. Brett was a graduate of the Taft School, Williams College, and the Yale School of Forestry.\n\nBrett served as treasurer (appointed 1926) and general manager of Macmillan Publishing. After serving in World War II, Brett was the business manager of the New York Public Library from 1947 until 1953.\n\nAfter retirement in 1953, Brett moved to Vermont, where he set up a tree farm with habitats for wildlife at Hawk's Hill in East Barnard. He served as a trustee of the Vermont Natural Resources Council. Brett later donated his Hawk's Hill tree farm to the New England Forestry Foundation.\n\nBrett served in the Army Air Corps during World War II.\n\n\n"}
{"id": "39197820", "url": "https://en.wikipedia.org/wiki?curid=39197820", "title": "Saxion", "text": "Saxion\n\nThe Saxion is the scalar superpartner of the axion, and part of a chiral superfield.\n"}
{"id": "35592056", "url": "https://en.wikipedia.org/wiki?curid=35592056", "title": "Sibirskaya Nuclear Power Plant", "text": "Sibirskaya Nuclear Power Plant\n\nThe Siberian Nuclear Power Plant (Sibirskaya Nuclear Power Plant) was built in the city of Seversk (then known as Tomsk-7), Tomsk Oblast. It was the second nuclear power plant in the USSR and the first industrial-scale nuclear power plant in the country (the first NPP, built in Obninsk, had a capacity of only 6 MW).\n"}
{"id": "30047", "url": "https://en.wikipedia.org/wiki?curid=30047", "title": "Thulium", "text": "Thulium\n\nThulium is a chemical element with symbol Tm and atomic number 69. It is the thirteenth and third-last element in the lanthanide series. Like the other lanthanides, the most common oxidation state is +3, seen in its oxide, halides and other compounds; because it occurs so late in the series, however, the +2 oxidation state is also stabilized by the nearly full 4f shell that results. In aqueous solution, like compounds of other late lanthanides, soluble thulium compounds form coordination complexes with nine water molecules.\n\nIn 1879, the Swedish chemist Per Teodor Cleve separated from the rare earth oxide erbia another two previously unknown components, which he called holmia and thulia; these were the oxides of holmium and thulium, respectively. A relatively pure sample of thulium metal was first obtained in 1911.\n\nThulium is the second-least abundant of the lanthanides, after radioactively unstable promethium which is only found in trace quantities on Earth. It is an easily workable metal with a bright silvery-gray luster. It is fairly soft and slowly tarnishes in air. Despite its high price and rarity, thulium is used as the radiation source in portable X-ray devices, and in some solid-state lasers. It has no significant biological role and is not particularly toxic.\n\nPure thulium metal has a bright, silvery luster, which tarnishes on exposure to air. The metal can be cut with a knife, as it has a Mohs hardness of 2 to 3; it is malleable and ductile. Thulium is ferromagnetic below 32K, antiferromagnetic between 32 and 56K, and paramagnetic above 56K.\n\nThulium has two major allotropes: the tetragonal α-Tm and the more stable hexagonal β-Tm.\n\nThulium tarnishes slowly in air and burns readily at 150°C to form thulium(III) oxide:\n\nThulium is quite electropositive and reacts slowly with cold water and quite quickly with hot water to form thulium hydroxide:\n\nThulium reacts with all the halogens. Reactions are slow at room temperature, but are vigorous above 200°C:\n\nThulium dissolves readily in dilute sulfuric acid to form solutions containing the pale green Tm(III) ions, which exist as [Tm(OH)] complexes:\n\nThulium reacts with various metallic and non-metallic elements forming a range of binary compounds, including TmN, TmS, TmC, TmC, TmH, TmH, TmSi, TmGe, TmB, TmB and TmB. In those compounds, thulium exhibits valence states +2 and +3, however, the +3 state is most common and only this state has been observed in thulium solutions. Thulium exists as a Tm ion in solution. In this state, the thulium ion is surrounded by nine molecules of water. Tm ions exhibit a bright blue luminescence.\n\nThulium's only known oxide is TmO. This oxide is sometimes called \"thulia\". Reddish-purple thulium(II) compounds can be made by the reduction of thulium(III) compounds. Examples of thulium(II) compounds include the halides (except the fluoride). Some hydrated thulium compounds, such as TmCl·7HO and Tm(CO)·6HO are green or greenish-white. Thulium dichloride reacts very vigorously with water. This reaction results in hydrogen gas and Tm(OH) exhibiting a fading reddish color. Combination of thulium and chalcogens results in thulium chalcogenides.\n\nThulium reacts with hydrogen chloride to produce hydrogen gas and thulium chloride. With nitric acid it yields thulium nitrate, or Tm(NO).\n\nThe isotopes of thulium range from Tm to Tm. The primary decay mode before the most abundant stable isotope, Tm, is electron capture, and the primary mode after is beta emission. The primary decay products before Tm are element 68 (erbium) isotopes, and the primary products after are element 70 (ytterbium) isotopes.\n\nThulium-169 is thulium's longest-lived and most abundant isotope. It is the only isotope of thulium that is thought to be stable, although it is predicted to undergo alpha decay to holmium-165 with a very long half-life. After thulium-169, the next-longest-lived isotopes are thulium-171, which has a half-life of 1.92 years, and thulium-170, which has a half-life of 128.6 days. Most other isotopes have half-lives of a few minutes or less. Thirty-five isotopes and 26 nuclear isomers of thulium have been detected. Most isotopes of thulium lighter than 169 atomic mass units decay via electron capture or beta-plus decay, although some exhibit significant alpha decay or proton emission. Heavier isotopes undergo beta-minus decay.\n\nThulium was discovered by Swedish chemist Per Teodor Cleve in 1879 by looking for impurities in the oxides of other rare earth elements (this was the same method Carl Gustaf Mosander earlier used to discover some other rare earth elements). Cleve started by removing all of the known contaminants of erbia (ErO). Upon additional processing, he obtained two new substances; one brown and one green. The brown substance was the oxide of the element holmium and was named holmia by Cleve, and the green substance was the oxide of an unknown element. Cleve named the oxide thulia and its element thulium after Thule, an Ancient Greek place name associated with Scandinavia or Iceland. Thulium's atomic symbol was once Tu, but this was changed to Tm.\n\nThulium was so rare that none of the early workers had enough of it to purify sufficiently to actually see the green color; they had to be content with spectroscopically observing the strengthening of the two characteristic absorption bands, as erbium was progressively removed. The first researcher to obtain nearly pure thulium was Charles James, a British expatriate working on a large scale at New Hampshire College in Durham. In 1911 he reported his results, having used his discovered method of bromate fractional crystallization to do the purification. He famously needed 15,000 purification operations to establish that the material was homogeneous.\n\nHigh-purity thulium oxide was first offered commercially in the late 1950s, as a result of the adoption of ion-exchange separation technology. Lindsay Chemical Division of American Potash & Chemical Corporation offered it in grades of 99% and 99.9% purity. The price per kilogram has oscillated between US$4,600 and $13,300 in the period from 1959 to 1998 for 99.9% purity, and it was second highest for lanthanides behind lutetium.\n\nThe element is never found in nature in pure form, but it is found in small quantities in minerals with other rare earths. Thulium is often found with minerals containing yttrium and gadolinium. In particular, thulium occurs in the mineral gadolinite. However, thulium also occurs in the minerals monazite, xenotime, and euxenite. Thulium has not been found in prevalence over the other rare earths in any mineral yet. Its abundance in the Earth's crust is 0.5 mg/kg by weight and 50 parts per billion by moles. Thulium makes up approximately 0.5 parts per million of soil, although this value can range from 0.4 to 0.8 parts per million. Thulium makes up 250 parts per quadrillion of seawater. In the solar system, thulium exists in concentrations of 200 parts per trillion by weight and 1 part per trillion by moles. Thulium ore occurs most commonly in China. However, Australia, Brazil, Greenland, India, Tanzania, and the United States also have large reserves of thulium. Total reserves of thulium are approximately 100,000 tonnes. Thulium is the least abundant lanthanide on earth except for promethium.\n\nThulium is principally extracted from monazite ores (~0.007% thulium) found in river sands, through ion-exchange. Newer ion-exchange and solvent-extraction techniques have led to easier separation of the rare earths, which has yielded much lower costs for thulium production. The principal sources today are the ion adsorption clays of southern China. In these, where about two-thirds of the total rare-earth content is yttrium, thulium is about 0.5% (or about tied with lutetium for rarity). The metal can be isolated through reduction of its oxide with lanthanum metal or by calcium reduction in a closed container. None of thulium's natural compounds are commercially important. Approximately 50 tonnes per year of thulium oxide are produced. In 1996, thulium oxide cost US$20 per gram, and in 2005, 99%-pure thulium metal powder cost US$70 per gram.\n\nThulium has a few applications:\n\nHolmium-chromium-thulium triple-doped yttrium aluminum garnet (Ho:Cr:Tm:YAG, or Ho,Cr,Tm:YAG) is an active laser medium material with high efficiency. It lases at 2080 nm and is widely used in military applications, medicine, and meteorology. Single-element thulium-doped YAG (Tm:YAG) lasers operate at 2,01 μm. The wavelength of thulium-based lasers is very efficient for superficial ablation of tissue, with minimal coagulation depth in air or in water. This makes thulium lasers attractive for laser-based surgery.\n\nDespite its high cost, portable X-ray devices use thulium that has been bombarded in a nuclear reactor as a radiation source. These sources have a useful life of about one year, as tools in medical and dental diagnosis, as well as to detect defects in inaccessible mechanical and electronic components. Such sources do not need extensive radiation protection – only a small cup of lead.\n\nThulium-170 is gaining popularity as an X-ray source for cancer treatment via brachytherapy. This isotope has a half-life of 128.6 days and five major emission lines of comparable intensity (at 7.4, 51.354, 52.389, 59.4 and 84.253 keV). Thulium-170 is one of the four most popular radioisotopes for use in industrial radiography.\n\nThulium has been used in high-temperature superconductors similarly to yttrium. Thulium potentially has use in ferrites, ceramic magnetic materials that are used in microwave equipment. Thulium is also similar to scandium in that it is used in arc lighting for its unusual spectrum, in this case, its green emission lines, which are not covered by other elements. Because thulium fluoresces with a blue color when exposed to ultraviolet light, thulium is put into euro banknotes as a measure against counterfeiting. The blue fluorescence of Tm-doped calcium sulfate has been used in personal dosimeters for visual monitoring of radiation. Tm-doped halides which Tm is in its 2+ valence state, are promising luminescent materials that can make efficient electricity generating windows based on the principle of a luminescent solar concentrator, possible.\n\nSoluble thulium salts are mildly toxic, but insoluble thulium salts are completely nontoxic. When injected, thulium can cause degeneration of the liver and spleen and can also cause hemoglobin concentration to fluctuate. Liver damage from thulium is more prevalent in male mice than female mice. Despite this, thulium has a low level of toxicity. In humans, thulium occurs in the highest amounts in the liver, kidneys and bones. Humans typically consume several micrograms of thulium per year. The roots of plants do not take up thulium, and the dry weight of vegetables usually contains one part per billion of thulium. Thulium dust and powder are toxic upon inhalation or ingestion and can cause explosions. Radioactive thulium isotopes can cause radiation poisoning.\n\n"}
{"id": "1906938", "url": "https://en.wikipedia.org/wiki?curid=1906938", "title": "To See Every Bird on Earth", "text": "To See Every Bird on Earth\n\nTo See Every Bird on Earth: A Father, a Son, and a Lifetime Obsession is a book by Dan Koeppel first published in 2005. It is about the author's relationship with his father Richard Koeppel, an obsessive \"Big Lister\" birdwatcher who had spotted over 7000 different species of birds at the time the book was written. The book focuses on Dan Koeppel's attempts to understand the obsession that ruled his father's life. It also examines the culture on highly competitive birders who travel the world making lists of their sightings, and discusses the history and rules of listing. Richard Koeppel was diagnosed with cancer in 2000, which curtailed his birding and forced him to switch to butterflies found locally near his home on Long Island, New York. He died of cancer-related causes on August 2, 2012.\n\n"}
{"id": "4006962", "url": "https://en.wikipedia.org/wiki?curid=4006962", "title": "Whip (tree)", "text": "Whip (tree)\n\nA whip is a slender, unbranched shoot or plant. This term is used in forestry to refer to unbranched young tree seedlings of approximately 0.5-1.0 m (1 ft 7 in-3 ft 3 in) in height and 2–3 years old that have been grown for planting out.\n"}
{"id": "42174448", "url": "https://en.wikipedia.org/wiki?curid=42174448", "title": "Wood car racing", "text": "Wood car racing\n\nWood car racing is a racing event for youth who build small cars from wood, usually from kits containing a block of pine, plastic wheels and metal axles. Kids from all over the world participate in events related to wood car racing.\n\nThe pinewood derby was created as an event for Cub Scouts in the Boy Scouts of America. The first pinewood derby was held on May 15, 1953 at the Scout House in Manhattan Beach, California by Cub Scout Pack 280C (the present Pack 713). The concept was created by the Pack’s Cubmaster Don Murphy, and sponsored by the Management Club at North American Aviation.\n\nMurphy's son was too young to participate in the popular Soap Box Derby races, so he came up with the idea of racing miniature wood cars. The cars had the same gravity-powered concept as the full-size Soap Box Derby cars, but were much smaller and easier to build.\n\nThe pinewood derby had a sensational first year. The originator of the idea—Don Murphy and the Management Club of North American Aviation—sent out thousands of brochures to anyone who requested more information. The idea spread rapidly, and competitions were held across the country, mainly with recreation departments and nonprofit organizations including the Los Angeles County Department of Recreation. Of all that early enthusiasm, however, only the Boy Scouts of America made it part of an official program.\n\nAs the popularity of the pinewood derby grew, other organizations adopted the concept. Pinewood derby is a registered trademark of the BSA, so most use different names. Each derby has slightly different rules for making and racing their cars.\n"}
{"id": "33325400", "url": "https://en.wikipedia.org/wiki?curid=33325400", "title": "Ōitagawa Dam", "text": "Ōitagawa Dam\n"}
