{"id": "45064230", "url": "https://en.wikipedia.org/wiki?curid=45064230", "title": "1888 Sundsvall fire", "text": "1888 Sundsvall fire\n\nThe 1888 Sundsvall fire () was a fire in Sundsvall, Sweden on 25 June 1888. The fire occurred during a storm, allowing the fire to spread fast among the wooden houses in town. The same day, a fire also occurred in Umeå.\n\nThe spark from the steam boat \"Selånger\" traveling on Selångersån landed in the brewhouse of the widow Märta Charlotta Styf on Stora Nygatan. The fire was quick - and devastating. At 12:25 o'clock all of the city's bells rang to warn people that a fire had broken out. But the fire was overpowering. Strong winds to the northwest and the dry hot air made did that the wooden houses one by one soon was devoured by the flames. 9,000 people became homeless in just 9 hours. In addition to thousands of people left homeless the property damage was estimated to SEK 30 million, corresponding to almost SEK 2 billion (USD 241.42 million) in 2015. In the wake of the fire the city was looted on what was left. When evening came on 25 June the city of Sundsvall was a smoking ruin.\n\nA major investigation into the cause of the fire was started, which included hearing of the captain of \"Selånger\". He stated that he saw smoke rising up through the bridge cabin chimney when passing the Styfska yard, a claim which, however, contested by all the witnesses. There was no other reasonable explanation than that the fire was started by sparks from one of the steamboats \"Selånger\", or possibly \"Högom\". Four people died in the fire: the workers Mikael Olof Norvall and Charlotta Eufrosina Askling, the maritime pilot C.E. Carlsson and a man so severely burned that he could not be identified.\n\nThe coppersmith journeyman Arvid Göhle from Hudiksvall without regard to himself saved several lives, including the wife of tailor Otzén and her newborn child that was born during the morning of that day. Without considering to first save his own belongings he went into the house and carried out the bed with the wife, the newborn baby and a little girl on the farm. With the bed set on a cart, he pulled the whole equipage through the burning city via Norrmalm and on to Heffners, away from the flames. When he returned to his own home everything he had owned was burnt.\n\n"}
{"id": "39532555", "url": "https://en.wikipedia.org/wiki?curid=39532555", "title": "Alpaslan-2 Dam", "text": "Alpaslan-2 Dam\n\nThe Alpaslan-2 Dam is an embankment dam currently under construction on the Murat River in Muş Province, Turkey. The dam is located about north of the provincial capital, Muş. The primary purpose of the dam is hydroelectric power production and irrigation. Its power station will have an installed capacity of 280 MW and the reservoir will help irrigate of land. Enerjisa Power Generation Inc. acquired the license for the power station from Özışık İnsaat & Enerji in April 2011 and Yenigün Construction started excavating the diversion tunnels May 2012. The diversion tunnels are expected to be complete by the end of 2012. In late May 2013 Pöyry was awarded the detailed design of the dam, power station, spillway and switch yard. Enerjisa expects the project to be complete in 2016.\n\n"}
{"id": "2908539", "url": "https://en.wikipedia.org/wiki?curid=2908539", "title": "Ash Wednesday Storm of 1962", "text": "Ash Wednesday Storm of 1962\n\nThe Ash Wednesday Storm of 1962 occurred on March 5–9, 1962 along the mid-Atlantic coast of the United States. Also known as the Great March Storm of 1962, it was considered by the U.S. Geological Survey to be one of the most destructive storms ever to affect the mid-Atlantic states. Classified as a level 5 or Extreme Nor'easter by the Dolan-Davis scale for classification of Atlantic Nor'easters it was one of the ten worst storms in the United States in the 20th century. It lingered through five high tides over a three-day period, killing 40 people, injuring over 1,000, and causing hundreds of millions in property damage in six states. The storm also deposited significant snowfall over the Southeast, with a regional snowfall index of 12.663.\n\nOn March 4, 1962, a large low pressure area developed along a cold front off the southeast coast of the United States, with several ill-defined circulation centers. At the same time, a large ridge was over Atlantic Canada, and a powerful upper-level low was over the Ohio Valley. The upper-level low reached the North Carolina coast on March 6, which aided in the intensification of the frontal system, reaching a minimum barometric pressure of . From March 6–8, the storm drifted northeastward, but quicker movement was blocked by the ridge. The interaction between the storm and the ridge produced a steep pressure gradient that produced hurricane-force wind gusts along the Mid-Atlantic coast. The storm also produced a long fetch of flow of winds from the Atlantic Ocean, building the height of the waves as high as . Storm conditions diminished along the coast on March 9. The storm was unusual because of its slow movement, contrary to typical nor'easters.\n\nIn most areas, the peak of the storm in most areas occurred on March 7, which was the Christian holiday of Ash Wednesday, the first day of Lent that year. Outer Banks writer Aycock Brown named the coastal storm as the \"Ash Wednesday Storm\". In their Mariners Weather Log summary, the U.S. Weather Bureau referred to the weather system as the \"Great Atlantic Coastal Storm\". The storm is also known as the \"Five High Storm\" because it lingered off the Atlantic Coast of the northeast United States over a period of five high tides. \n\nThe Weather Bureau described the storm as \"one of the most damaging extratropical cyclones to hit the United States coastline,\" with damage estimated around $200 million. The storm destroyed 1,793 houses, and damaged another 16,782. According to the American Red Cross, the storm killed 40 people in the United States, with 1,252 people injured.\n\nAt the same time of the storm reaching its maturity, the moon was at its perigee – its closest point to Earth during its 28-day elliptical orbit) – while the moon was also in alignment with the sun and moon, a set of conditions known as the Perigean spring tide. High tides lasted over five successive high tides. The large storm dropped heavy snowfall as far south as Alabama, with the heaviest accumulations in western Virginia and Maryland. Little precipitation occurred to the storm's north over New England.\n\nDuring the storm, several ships came into danger amid the high waves. The British vessel \"Arthur Albright\" was blown ashore at Port Tampa, Florida. About 200 mi (320 km) southeast of Charleston, three people were rescued when the yacht \"Guinevere\" sank during the storm. Offshore Cape Hatteras, a tanker broke in two; one member of the crew died while attempting to launch lifeboats, but the remaining crew were rescued by a passing cruise ship and the Navy. Two ships – one off Cape Hatteras and another east of Virginia – sustained damage to their rudders. A wave damaged the Chesapeake lightship east of Cape Henry, Virginia; the lightship had to evacuate, and the Coast Guard sent another ship in its place. Two freighters were washed ashore New Jersey, and two fishing trawlers with nine people onboard went missing.\n\nThe weather system in the southeastern United States dropped snow as far south as Alabama, and dropped the temperature in Miami, Florida to . Cold, northerly winds first affected Florida on March 5, with peak gusts of in Daytona Beach. Tides reached above normal in Vero Beach. The high tides caused minor beach erosion and extensive drifting of sand. Flooding entered oceanfront properties, damaging docks, streets, and cabanas. Monetary damage was estimated around $1 million. Impacts were minimal in Georgia. In neighboring South Carolina, the high tides eroded beaches, with Folly Beach losing up to of sand. Charleston, South Carolina reported wind gusts of and tides above normal. The storm wrecked a few beachside cottages, and one person drowned along the Santee-Cooper Lakes. \n\nA gale warning was issued for North Carolina on March 6. Along the Atlantic side of the Outer Banks of North Carolina, high waves eroded sand dunes and created a new inlet on Hatteras Island, about 2 mi (3 km) north of Buxton. Several sections of North Carolina Highway 12 were washed out or covered with sand, covering cars with sand.\n\nHeavy snowfall occurred in North Carolina and into Virginia, reaching in the Blue Ridge Mountains of western Virginia. Winchester, Virginia reported of snowfall, a city record at the time.\n\nIn Norfolk, Virginia, the storm produced the third highest tide on record, and highest unrelated to a tropical cyclone, reaching . The tides flooded thousands of cars and damaged thousands of houses, especially near the coast. Wind gusts in the Hampton Roads area reached , although even stronger winds occurred over Chesapeake Bay, reaching gusts and producing waves. Construction of the Chesapeake Bay Bridge–Tunnel was disrupted when the waves knocked over the world's largest pile driver, which stood on four legs. The $1.5 million machine was buried in sand, and the two workers onboard were rescued by helicopter. Along the Eastern Shore of Virginia, high waves damaged installations at the Wallops Flight Facility. Thousands of people had to be evacuated to the mainland during the storm. On Chincoteague Island, the storm damaged boats and homes and also killed several livestock. Several Chincoteague Ponies died during the floods, but Misty – the subject of a book and movie – survived. Damage throughout Virginia reached over $30 million, with half in Virginia Beach, and there were five deaths in the state.\n\nAt the Town of Chincoteague on Virginia's Eastern Shore near the border with Maryland, six feet (2 m) of water covered parts of Main Street, and most of the island was flooded to various depths. On adjacent Assateague Island, the Chincoteague Fire Company lost a portion of its herd of wild Chincoteague Ponies. Misty, the local pony made famous by Marguerite Henry's award-winning children's book \"Misty of Chincoteague\" and the 1961 movie \"Misty\", survived by being brought inside a house. Also along the Delmarva Peninsula, at Wallops Island, a million dollars in damage was done to NASA's Wallops Flight Facility.\n\nAlong the Eastern Shore of Maryland and in eastern Delaware, the high waves eroded beaches and damaged boardwalks. In Ocean City, Maryland, wind gusts reached , while tides were estimated at above normal. The tides washed away dunes and beaches Flooding closed down roads and entered houses along the coast, causing significant damage to seaside resorts. Farther inland, the floods killed over 1.2 million broiler chickens and many incubating eggs due to power outages. Saltwater intrusion damaged fields in northern Delaware. At the Delaware Breakwater, winds reached , and the highest tide was above normal before the tidal gauge failed; the high tide was estimated at . Damage in the region was estimated at $50 million, and ten people were killed – seven fatalities were in Delaware and the other three were in Maryland.\n\nFurther north, 7.6 m (25 ft) waves struck Ocean City, Maryland and the resort developments beginning on Assateague Island were destroyed. Waves more than 12 m (40 ft) high occurred at Rehoboth Beach, Delaware destroying the boardwalk and beach front homes. The flow of water flooded waterfront areas of Philadelphia and Camden, New Jersey. \n\nThe Weather Bureau forecast for Atlantic City failed to anticipate the severity of the storm, and there was little warning for the flooding along the coasts. High waves battered the Jersey Shore and the Delaware Bay, along with strong wind gusts, reaching in Long Branch. Damage was estimated at $80 million statewide, one of the most damaging storms on record for the coast. The highest tide in the state was in Harrison. High waves and tides changed the New Jersey coastline. Floodwaters breached Long Beach Island in five locations, and about half of Harvey Cedars was wrecked during the storm. In coastal towns, about 4,000 houses were destroyed and another 40,000 were severely damaged after being inundated with of water. In Atlantic City, the waves knocked a barge into Steel Pier, which washed out a quarter-mile section, including the tank used for the diving horse. Also lost was the wave sensor, which last reported a tide of on March 6. The estimated high tide was above normal. Stranded residents along the coast attempted to evacuate by boat, but were impeded by icy waters. About 2,000 people were evacuated by army trucks and helicopters. Boardwalks were damaged in several towns, and rail lines were shut down. The high tides cut power and phone lines, which sparked fires that were unable to be reached by fire trucks until the floods subsided. There 14 fatalities, as well as 12 people missing and presumed killed. Avalon, New Jersey lost 6 blocks. \n\nIn New York, high waves and tides affected Long Island and the New York metro. Willets Point reported a high tide of before the gauge failed. Waves up to in height washed away about 100 houses in the state, including 35 on Fire Island, and flooded coastal roads, forcing families to evacuate. The Battery reported wind gusts of during the storm. The winds knocked down power lines, trees, and signs. Damage in New York was estimated at over $10 million.\n\nHigh winds and waves affected southern New England, with a peak wind gust of on Block Island, Rhode Island; the same station recorded sustained winds of . The highest tide in New England was in Boston, Massachusetts. High waves flooded coastal and low-lying areas, inundating roads and houses' cellars, and washing over seawalls as far north as Portland, Maine. Near New Haven, Connecticut, a barge and an oyster boat sank. Little precipitation occurred in New England, and the Weather Bureau referred to the storm as a \"dry nor'easter\". Flights in and around the region were canceled during the storm. Damage in New England was estimated at $1.3 million. Extensive damage to trees and structures and beach erosion was also reported along the southern New England coast.\n\nAfter the storm, stranded residents in the Outer Banks had food and emergency supplies delivered by ferry. The governor of Virginia declared a state of emergency for the Hampton Roads area and the Eastern Shore. President John F. Kennedy declared a disaster area for the affected areas in New Jersey. Following the storm, the police and National Guard patrolled the damaged barrier islands to keep order. Shore towns cleared sand and restored road access ahead of the summer tourist season. The Army Corps of Engineers made emergency beach replenishments to shore towns to protect from further storms.\n\nPerhaps a fitting memorial to what was lost in the storm is Assateague Island National Seashore, a unit of the National Park Service. In the 1950s, some 5,000 private lots comprising what is now National Park Service land were zoned and sold for resort development. The Ash Wednesday Storm halted the plans for development, as it destroyed the few existing structures on the island and ripped roads apart. Instead, in 1965, Assateague Island became a National Seashore.\n\nShortly after the storm subsided, Misty, the famous horse from Chincoteague who spent the storm in the family's kitchen (her barn was flooded) gave birth to a foal. The family named her \"Stormy\", laying the basis for another book in Marguerite Henry's award-winning \"Misty of Chincoteague\" series. The new book was named \"Stormy, Misty's Foal\".\n\n"}
{"id": "2022252", "url": "https://en.wikipedia.org/wiki?curid=2022252", "title": "Bismuthide", "text": "Bismuthide\n\nThe bismuthide ion is Bi.\n\nBismuthides are compounds of bismuth with more electropositive elements. They are intermetallic compounds, containing partially metallic and partially ionic bonds.\n\n"}
{"id": "8391417", "url": "https://en.wikipedia.org/wiki?curid=8391417", "title": "Carbon dioxide equivalent", "text": "Carbon dioxide equivalent\n\nCarbon dioxide equivalent (CDE) and equivalent carbon dioxide (e and eq) are two related but distinct measures for describing how much global warming a given type and amount of greenhouse gas may cause, using the functionally equivalent amount or concentration of carbon dioxide () as the reference.\n\nCarbon dioxide equivalency is a quantity that describes, for a given mixture and amount of greenhouse gas, the amount of that would have the same global warming potential (GWP), when measured over a specified timescale (generally, 100 years). Carbon dioxide equivalency thus reflects the time-integrated radiative forcing of a quantity of \"emissions\" or rate of greenhouse gas emission—a \"flow\" into the atmosphere—rather than the instantaneous value of the radiative forcing of the \"stock\" (concentration) of greenhouse gases \"in the atmosphere\" described by e.\n\nThe carbon dioxide equivalency for a gas is obtained by multiplying the mass and the GWP of the gas. The following units are commonly used:\n\nFor example, the GWP for methane over 100 years is 34 and for nitrous oxide 298. This means that emissions of 1 million metric tonnes of methane and nitrous oxide respectively is equivalent to emissions of 34 and 298 million metric tonnes of carbon dioxide.\n\nEquivalent (e) is the concentration of that would cause the same level of radiative forcing as a given type and concentration of greenhouse gas. Examples of such greenhouse gases are methane, perfluorocarbons, and nitrous oxide. e is expressed as parts per million by volume, ppmv.\n\n\n"}
{"id": "143133", "url": "https://en.wikipedia.org/wiki?curid=143133", "title": "Center of pressure (fluid mechanics)", "text": "Center of pressure (fluid mechanics)\n\nThe center of pressure is the point where the total sum of a pressure field acts on a body, causing a force to act through that point. The total force vector acting at the center of pressure is the value of the integrated vectorial pressure field. The resultant force and center of pressure location produce equivalent force and moment on the body as the original pressure field. Pressure fields occur in both static and dynamic fluid mechanics. Specification of the center of pressure, the reference point from which the center of pressure is referenced, and the associated force vector allows the moment generated about any point to be computed by a translation from the reference point to the desired new point. It is common for the center of pressure to be located on the body, but in fluid flows it is possible for the pressure field to exert a moment on the body of such magnitude that the center of pressure is located outside the body.\n\nSince the forces of water on a dam are hydrostatic forces, they vary linearly with depth. The total force on the dam is then the integral of the pressure multiplied by the width of the dam as a function of the depth. The center of pressure is located at the centroid of the triangular shaped pressure field 2/3 from the top of the water line. The hydrostatic force and tipping moment on the dam about some point can be computed from the total force and center of pressure location relative to the point of interest.\n\nCenter of pressure is used in sailboat design to represent the position on a sail where the aerodynamic force is concentrated.\n\nThe relationship of the aerodynamic center of pressure on the sails to the hydrodynamic center of pressure (referred to as the center of lateral resistance) on the hull determines the behavior of the boat in the wind. This behavior is known as the \"helm\" and is either a weather helm or lee helm. A slight amount of weather helm is thought by some sailors to be a desirable situation, both from the standpoint of the \"feel\" of the helm, and the tendency of the boat to head slightly to windward in stronger gusts, to some extent self-feathering the sails. Other sailors disagree and prefer a neutral helm.\n\nThe fundamental cause of \"helm\", be it weather or lee, is the relationship of the center of pressure of the sail plan to the center of lateral resistance of the hull. If the center of pressure is astern of the center of lateral resistance, a weather helm, the tendency of the vessel is to want to turn into the wind.\n\nIf the situation is reversed, with the center of pressure forward of the center of lateral resistance of the hull, a \"lee\" helm will result, which is generally considered undesirable, if not dangerous. Too much of either helm is not good, since it forces the helmsman to hold the rudder deflected to counter it, thus inducing extra drag beyond what a vessel with neutral or minimal helm would experience.\n\nA stable configuration is desirable not only in sailing, but in aircraft design as well. Aircraft design therefore borrowed the term center of pressure. And like a sail, a rigid non-symmetrical airfoil not only produces lift, but a moment.\nThe center of pressure of an aircraft is the point where all of the aerodynamic pressure field may be represented by a single force vector with no moment. A similar idea is the aerodynamic center which is the point on an airfoil where the pitching moment produced by the aerodynamic forces is constant with angle of attack.\n\nThe aerodynamic center plays an important role in analysis of the longitudinal static stability of all flying machines. It is desirable that when the pitch angle and angle of attack of an aircraft are disturbed (by, for example turbulence) that the aircraft returns to its original trimmed pitch angle and angle of attack without a pilot or autopilot changing the control surface deflection. For an aircraft to return towards its trimmed attitude, without input from a pilot or autopilot, it must have positive longitudinal static stability.\n\nMissiles typically do not have a preferred plane or direction of maneuver and thus have symmetric airfoils. Since the center of pressure for symmetric airfoils is relatively constant for small angle of attack, missile engineers typically speak of the complete center of pressure of the entire vehicle for stability and control analysis. In missile analysis, the center of pressure is typically defined as the center of the additional pressure field due to a change in the angle of attack off of the trim angle of attack.\n\nFor unguided rockets the trim position is typically zero angle of attack and the center of pressure is defined to be the center of pressure of the resultant flow field on the entire vehicle resulting from a very small angle of attack (that is, the center of pressure in the limit as angle of attack goes to zero). For positive stability in missiles, the total vehicle center of pressure defined as given above must be further from the nose of the vehicle than the center of gravity. In missiles at lower angles of attack, the contributions to the center of pressure are dominated by the nose, wings, and fins. The normalized normal force coefficient derivative with respect to the angle of attack of each component multiplied by the location of the center of pressure can be used to compute a centroid representing the total center of pressure. The center of pressure of the added flow field is behind the center of gravity and the additional force \"points\" in the direction of the added angle of attack; this produces a moment that pushes the vehicle back to the trim position.\n\nIn guided missiles where the fins can be moved to trim the vehicles in different angles of attack, the center of pressure is the center of pressure of the flow field at that angle of attack for the undeflected fin position. This is the center of pressure of any small change in the angle of attack (as defined above). Once again for positive static stability, this definition of center of pressure requires that the center of pressure be further from the nose than the center of gravity. This ensures that any increased forces resulting from increased angle of attack results in increased restoring moment to drive the missile back to the trimmed position. In missile analysis, positive static margin implies that the complete vehicle makes a restoring moment for any angle of attack from the trim position.\n\nThe center of pressure on a symmetric airfoil typically lies close to 25% of the chord length behind the leading edge of the airfoil. (This is called the \"quarter-chord point\".) For a symmetric airfoil, as angle of attack and lift coefficient change, the center of pressure does not move. It remains around the quarter-chord point for angles of attack below the stalling angle of attack. The role of center of pressure in the control characterization of aircraft takes a different form than in missiles.\n\nOn a cambered airfoil the center of pressure does not occupy a fixed location. For a conventionally cambered airfoil, the center of pressure lies a little behind the quarter-chord point at maximum lift coefficient (large angle of attack), but as lift coefficient reduces (angle of attack reduces) the center of pressure moves toward the rear. When the lift coefficient is zero an airfoil is generating no lift but a conventionally cambered airfoil generates a nose-down pitching moment, so the location of the center of pressure is an infinite distance behind the airfoil.\n\nFor a reflex-cambered airfoil, the center of pressure lies a little ahead of the quarter-chord point at maximum lift coefficient (large angle of attack), but as lift coefficient reduces (angle of attack reduces) the center of pressure moves forward. When the lift coefficient is zero an airfoil is generating no lift but a reflex-cambered airfoil generates a nose-up pitching moment, so the location of the center of pressure is an infinite distance ahead of the airfoil. This direction of movement of the center of pressure on a reflex-cambered airfoil has a stabilising effect.\n\nThe way the center of pressure moves as lift coefficient changes makes it difficult to use the center of pressure in the mathematical analysis of longitudinal static stability of an aircraft. For this reason, it is much simpler to use the aerodynamic center when carrying out a mathematical analysis. The aerodynamic center occupies a fixed location on an airfoil, typically close to the quarter-chord point.\n\nThe aerodynamic center is the conceptual starting point for longitudinal stability. The horizontal stabilizer contributes extra stability and this allows the center of gravity to be a small distance aft of the aerodynamic center without the aircraft reaching neutral stability. The position of the center of gravity at which the aircraft has neutral stability is called the neutral point.\n\n\n"}
{"id": "4932763", "url": "https://en.wikipedia.org/wiki?curid=4932763", "title": "Characterization (materials science)", "text": "Characterization (materials science)\n\nCharacterization, when used in materials science, refers to the broad and general process by which a material's structure and properties are probed and measured. It is a fundamental process in the field of materials science, without which no scientific understanding of engineering materials could be ascertained. The scope of the term often differs; some definitions limit the term's use to techniques which study the microscopic structure and properties of materials, while others use the term to refer to any materials analysis process including macroscopic techniques such as mechanical testing, thermal analysis and density calculation. The scale of the structures observed in materials characterization ranges from angstroms, such as in the imaging of individual atoms and chemical bonds, up to centimeters, such as in the imaging of coarse grain structures in metals.\n\nWhile many characterization techniques have been practiced for centuries, such as basic optical microscopy, new techniques and methodologies are constantly emerging. In particular the advent of the electron microscope and Secondary ion mass spectrometry in the 20th century has revolutionized the field, allowing the imaging and analysis of structures and compositions on much smaller scales than was previously possible, leading to a huge increase in the level of understanding as to why different materials show different properties and behaviors. More recently, atomic force microscopy has further increased the maximum possible resolution for analysis of certain samples in the last 30 years.\n\nMicroscopy is a category of characterization techniques which probe and map the surface and sub-surface structure of a material. These techniques can use photons , electrons , ions or physical cantilever probes to gather data about a sample's structure on a range of length scales. Some common examples of microscopy instruments include:\n\n\nThis group of techniques use a range of principles to reveal the chemical composition, composition variation, crystal structure and photoelectric properties of materials. Some common instruments include:\n\n\n\n\n\nA huge range of techniques are used to characterize various macroscopic properties of materials, including:\n\n\n"}
{"id": "22863481", "url": "https://en.wikipedia.org/wiki?curid=22863481", "title": "Composite fermion", "text": "Composite fermion\n\nA composite fermion is the topological bound state of an electron and an even number of quantized vortices, sometimes visually pictured as the bound state of an electron and, attached, an even number of magnetic flux quanta. Composite fermions were originally envisioned in the context of the fractional quantum Hall effect, but subsequently took on a life of their own, exhibiting many other consequences and phenomena.\n\nVortices are an example of topological defect, and also occur in other situations. Quantized vortices are found in type II superconductors, called Abrikosov vortices. Classical vortices are relevant to the Berezenskii–Kosterlitz–Thouless transition in two-dimensional XY model.\n\nWhen electrons are confined to two dimensions, cooled to very low temperatures, and subjected to a strong magnetic field, their kinetic energy is quenched due to Landau level quantization. Their behavior under such conditions is governed by the Coulomb repulsion alone, and they produce a strongly correlated quantum liquid. Experiments have shown that electrons minimize their interaction by capturing quantized vortices to become composite fermions. The interaction between composite fermions themselves is often negligible to a good approximation, which makes them the physical quasiparticles of this quantum liquid.\n\nThe signature quality of composite fermions, which is responsible for the otherwise unexpected behavior of this system, is that they experience a much smaller magnetic field than electrons. The magnetic field seen by composite fermions is given by\n\nwhere formula_2 is the external magnetic field, formula_3 is the number of vortices bound to composite fermion (also called the vorticity or the vortex charge of the composite fermion), formula_4 is the particle density in two dimensions, and formula_5 is called the “flux quantum” (which differs from the superconducting flux quantum by a factor of two). The effective magnetic field is a direct manifestation of the existence of composite fermions, and also embodies a fundamental distinction between electrons and composite fermions.\n\nSometimes it is said that electrons \"swallow\" formula_3 flux quanta each to transform into composite fermions, and the composite fermions then experience the residual magnetic field formula_7 More accurately, the vortices bound to electrons produce their own geometric phases which partly cancel the Aharonov–Bohm phase due to the external magnetic field to generate a net geometric phase that can be modeled as an Aharonov–Bohm phase in an effective magnetic field formula_8\n\nThe behavior of composite fermions is similar to that of electrons in an effective magnetic field formula_7 Electrons form Landau levels in a magnetic field, and the number of filled Landau levels is called the filling factor, given by the expression formula_10 Composite fermions form Landau-like levels in the effective magnetic field formula_11 which are called composite fermion Landau levels or formula_12 levels. One defines the filling factor for composite fermions as formula_13 This gives the following relation between the electron and composite fermion filling factors\n\nThe minus sign occurs when the effective magnetic field is antiparallel to the applied magnetic field, which happens when the geometric phase from the vortices overcompensate the Aharonov–Bohm phase.\n\nThe central statement of composite fermion theory is that the strongly correlated electrons at a magnetic field formula_15 (or filling factor formula_16) turn into weakly interacting composite fermions at a magnetic field formula_17 (or composite fermion filling factor formula_18). This allows an effectively single-particle explanation of the otherwise complex many-body behavior, with the interaction between electrons manifesting as an effective kinetic energy of composite fermions. Here are some of the phenomena arising from composite fermions:\n\nThe effective magnetic field for composite fermions vanishes for formula_19, where the filling factor for electrons is formula_20. Here, composite fermions make a Fermi sea. This Fermi sea has been observed at half filled Landau level in a number of experiments, which also measure the Fermi wave vector.\n\nAs the magnetic field is moved slightly away from formula_21, composite fermions execute semiclassical cyclotron orbits. These have been observed by coupling to surface acoustic waves, resonance peaks in antidot superlattice, and magnetic focusing. The radius of the cyclotron orbits is consistent with the effective magnetic field formula_21 and is sometimes an order of magnitude or more larger than the radius of the cyclotron orbit of an electron at the externally applied magnetic field formula_15. Also, the observed direction of trajectory is opposite to that of electrons when formula_17 is anti-parallel to formula_15.\n\nIn addition to the cyclotron orbits, cyclotron resonance of composite fermions has also been observed by photoluminescence.\n\nAs the magnetic field is moved further away from formula_21, quantum oscillations are observed that are periodic in formula_27 These are Shubnikov–de Haas oscillations of composite fermions. These oscillations arise from the quantization of the semiclassical cyclotron orbits of composite fermions into composite fermion Landau levels. From the analysis of the Shubnikov–de Haas experiments, one can deduce the effective mass and the quantum lifetime of composite fermions.\n\nWith further increase in formula_28 or decrease in temperature and disorder, composite fermions exhibit integer quantum Hall effect. The integer fillings of composite fermions, formula_29, correspond to the electrons fillings\n\nCombined with\n\nwhich are obtained by attaching vortices to holes in the lowest Landau level, these constitute the prominently observed sequences of fractions. Examples are\n\nThe fractional quantum Hall effect of electrons is thus explained as the integer quantum Hall effect of composite fermions. It results in fractionally quantized Hall plateaus at\n\nwith formula_36 given by above quantized values. These sequences terminate at the composite fermion Fermi sea. Note that the fractions have odd denominators, which follows from the even vorticity of composite fermions.\n\nThe above sequences account for most, but not all, observed fractions. Other fractions have been observed, which arise from a weak residual interaction between composite fermions, and are thus more delicate. A number of these are understood as fractional quantum Hall effect of composite fermions. For example, the fractional quantum Hall effect of composite fermions at formula_37 produces the fraction 4/11, which does not belong to the primary sequences.\n\nAn even denominator fraction, formula_38 has been observed. Here the second Landau level is half full, but the state cannot be a Fermi sea of composite fermions, because the Fermi sea is gapless and does not show quantum Hall effect. This state is viewed as a \"superconductor\" of composite fermion, arising from a weak attractive interaction between composite fermions at this filling factor. The pairing of composite fermions opens a gap and produces a fractional quantum Hall effect.\n\nThe neutral excitations of various fractional quantum Hall states are excitons of composite fermions, that is, particle hole pairs of composite fermions. The energy dispersion of these excitons has been measured by light scattering and phonon scattering.\n\nAt high magnetic fields the spin of composite fermions is frozen, but it is observable at relatively low magnetic fields. The fan diagram of the composite fermion Landau levels has been determined by transport, and shows both spin-up and spin-down composite fermion Landau levels. The fractional quantum Hall states as well as composite fermion Fermi sea are also partially spin polarized for relatively low magnetic fields.\n\nThe effective magnetic field of composite fermions has been confirmed by the similarity of the fractional and the integer quantum Hall effects, observation of Fermi sea at half filled Landau level, and measurements of the cyclotron radius.\n\nThe mass of composite fermions has been determined from the measurements of: the effective cyclotron energy of composite fermions; the temperature dependence of Shubnikov–de Haas oscillations; energy of the cyclotron resonance; spin polarization of the Fermi sea; and quantum phase transitions between states with different spin polarizations. Its typical value in GaAs systems is on the order of the electron mass in vacuum. (It is unrelated to the electron band mass in GaAs, which is 0.07 of the electron mass in vacuum.)\n\nMuch of the experimental phenomenology can be understood from the qualitative picture of composite fermions in an effective magnetic field. In addition, composite fermions also lead to a detailed and accurate microscopic theory of this quantum liquid. Two approaches have proved useful.\n\nThe following trial wave functions embody the composite fermion physics:\n\nformula_39\n\nHere formula_40 is the wave function of interacting electrons at filling factor formula_16; formula_42 is the wave function for weakly interacting electrons at formula_18; formula_44 is the number of electrons or composite fermions; formula_45 is the coordinate of the formula_46th particle; and formula_47 is an operator that projects the wave function into the lowest Landau level. This provides an explicit mapping between the integer and the fractional quantum Hall effects. Multiplication by formula_48 attaches formula_49 vortices to each electron to convert it into a composite fermion. The right hand side is thus interpreted as describing composite fermions at filling factor formula_18. The above mapping gives wave functions for both the ground and excited states of the fractional quantum Hall states in terms of the corresponding known wave functions for the integral quantum Hall states. The latter do not contain any adjustable parameters for formula_29, so the FQHE wave functions do not contain any adjustable parameters at formula_52.\n\nComparisons with exact results show that these wave functions are quantitatively accurate. They can be used to compute a number of measurable quantities, such as the excitation gaps and exciton dispersions, the phase diagram of composite fermions with spin, the composite fermion mass, etc. For formula_53 they reduce to the Laughlin wavefunction at fillings formula_54.\n\nAnother formulation of the composite fermion physics is through a Chern–Simons field theory, wherein flux quanta are attached to electrons by a singular gauge transformation. At the mean field approximation the physics of free fermions in an effective field is recovered. Perturbation theory at the level of the random phase approximation captures many of the properties of composite fermions.\n\n\n"}
{"id": "53079421", "url": "https://en.wikipedia.org/wiki?curid=53079421", "title": "Dissimilatory nitrate reduction to ammonium", "text": "Dissimilatory nitrate reduction to ammonium\n\nDissimilatory nitrate reduction to ammonium (DNRA), also known as nitrate/nitrite ammonification, is the result of anaerobic respiration by chemoorganoheterotrophic microbes using nitrate (NO) as an electron acceptor for respiration. In anaerobic conditions microbes which undertake DNRA oxidise organic matter and use nitrate (rather than oxygen) as an electron acceptor, reducing it to nitrite, then ammonium (NO→NO→NH).\n\nDissimilatory nitrate reduction to ammonium is more common in prokaryotes but may also occur in eukaryotic microorganisms. DNRA is a component of the terrestrial and oceanic nitrogen cycle. Unlike denitrification, it acts to conserve bioavailable nitrogen in the system, producing soluble ammonium rather than unreactive dinitrogen gas.\n\nDissimilatory nitrate reduction to ammonium is a two step process, reducing NO to NO then NO to NH, though the reaction may begin with NO directly. Each step is mediated by a different enzyme, the first step of dissimilatory nitrate reduction to ammonium is usually mediated by a periplasmic nitrate reductase. The second step (respiratory NO reduction to NH) is mediated by cytochrome c nitrite reductase, occurring at the periplasmic membrane surface. Despite DNRA not producing NO as an intermediate during nitrate reduction (as denitrification does) NO may still be released as a byproduct, thus DNRA may also act as a sink of fixed, bioavailable nitrogen. DNRA's production of NO may be enhanced at higher pH levels.\n\nDissimilatory nitrate reduction to ammonium is similar to the process of denitrification, though NO is reduced farther to NH rather than to N, transferring eight electrons. Both denitrifiers and nitrate ammonifiers are competing for NO in the environment. Despite the redox potential of dissimilatory nitrate reduction to ammonium being lower than denitrification and producing less Gibbs free energy, energy yield of denitrification may not be efficiently conserved in its series of enzymatic reactions and nitrate ammonifiers may achieve higher growth rates and outcompete denitrifiers. This is may be especially pronounced when NO is limiting compared to organic carbon, as organic carbon is oxidised more 'efficiently' per NO (as each molecule NO is reduced farther). The balance of denitrification and DNRA is important to the nitrogen cycle of an environment as both use NO but, unlike denitrification, which produces gaseous, non-bioavailable N (a sink of nitrogen), DNRA produces bioavailable, soluble NH.\n\nAs dissimilatory nitrate reduction to ammonium is an anaerobic respiration process, marine microorganisms capable of performing DNRA are most commonly found in environments low in O, such as oxygen minimum zones (OMZs) in the water column, or sediments with steep O gradients.\n\nDNRA has been documented in prokaryotes inhabiting the upper layer of marine sediments. For example,Benthic sulfur bacteria in genera such as \"Beggiatoa\" and \"Thioploca\" inhabit anoxic sediments on continental shelves and obtain energy by oxidizing sulfide via DNRA. These bacteria are able to carry out DNRA using intracellular nitrate stored in vacuoles. The direct reduction of nitrate to ammonium via dissimilatory nitrate reduction, coupled with the direct conversion of ammonium to dinitrogen via Anammox, has been attributed to significant nitrogen loss in certain parts of the ocean; this DNRA-Anammox coupling by DNRA and Anammox bacteria can account for nitrate loss in areas with no detectable denitrification, such as in OMZs off the coast of Chile, Peru, and Namibia, as well as OMZs over the Omani Shelf in the Arabian Sea. While denitrification is more energetically favourable than DNRA, there is evidence that bacteria using DNRA conserve more energy than denitrifiers, allowing them to grow faster. Thus, via DNRA-Anammox coupling, bacteria using DNRA and Anammox may be stronger competitors for substrates than denitrifiers.\n\nWhile dissimilatory nitrate reduction to ammonium is more commonly associated with prokaryotes, recent research has found increasing evidence of DNRA in various eukaryotic microorganisms. Of the known DNRA-capable fungal species, one is found in marine ecosystems; an isolate of ascomycete \"Aspergillus terreus\" from an OMZ of the Arabian Sea has been found to be capable of performing DNRA under anoxic conditions. Evidence of DNRA has also been found in marine foraminifers.\n\nMore recently, it has been discovered that using intracellular nitrate stores, diatoms can carry out dissimilatory nitrate reduction to ammonium, likely for short-term survival or for entering resting stages, thereby allowing them to persist in dark and anoxic conditions. However, their metabolism is probably not sustained by DNRA for long-term survival during resting stages, as these resting stages often can be much longer than their intracellular nitrate supply would last. The use of DNRA by diatoms is a possible explanation for how they can survive buried in dark, anoxic sediment layers on the ocean floor, without being able to carry out photosynthesis or aerobic respiration. Currently, DNRA is known to be carried out by the benthic diatom \"Amphora coffeaeformis\", as well as the pelagic diatom \"Thalassiosira weissflogii\". As diatoms are a significant source of oceanic primary production, the ability for diatoms to perform DNRA has major implications on their ecological role, as well as their role in the marine nitrogen cycle.\n\nUnlike denitrification, which removes reactive nitrogen from the system, dissimilatory nitrate reduction to ammonium conserves nitrogen within the system. Since DNRA takes nitrate and converts it into ammonium, it does not produce N or NO. Consequently, DNRA recycles nitrogen rather than causing N-loss, which leads to more sustainable primary production and nitrification.\n\nWithin an ecosystem, denitrification and DNRA can occur simultaneously. Usually DNRA is about 15% of the total nitrate reduction rate, which includes both DNRA and denitrification. However, the relative importance of each process is influenced by environmental variables. For example, DNRA is found to be three to seven times higher in sediments under fish cages than nearby sediments due to the accumulation of organic carbon.\n\nConditions where dissimilatory nitrate reduction to ammonium is favoured over denitrification in coastal ecosystems include the following:\n\nHigh sulfide concentration can inhibit the processes of nitrification and denitrification. Meanwhile, it can also enhance dissimilatory nitrate reduction to ammonium since high sulfide concentration provides more electron donors.\n\nEcosystems where DNRA is dominant have less nitrogen loss, resulting in higher levels of preserved nitrogen in the system. Within sediments, the total dissimilatory nitrate reduction to ammonium rate is higher in spring and summer compared to autumn. Prokaryotes are the major contributors for DNRA during summer, while eukaryotes and prokaryotes contribute similarly to DNRA during spring and autumn.\n\nPotential benefits of using dissimilatory nitrate reduction to ammonium for individual organisms may include the following:\n\nThe balance of dissimilatory nitrate reduction to ammonium and denitrification alters the accuracy of f-ratio calculations. The f-ratio is used to quantify the efficiency of the biological pump, which reflects sequestering of carbon from the atmosphere to the deep sea. The f-ratio is calculated using estimates of 'new production' (primary productivity stimulated by nutrients entering the photic zone from outside the photic zone, for example from the deep ocean) and 'regenerated production' (primary productivity stimulated by nutrients already in the photic zone, released by remineralisation). Calculations of the f-ratio use the nitrogen species stimulating primary productivity as a proxy for the type of production occurring; productivity stimulated by NH rather than NO is 'regenerated production'. DNRA also produces NH (in addition to remineralisation) but from organic matter which has been exported from the photic zone; this may be subsequently reintroduced by mixing or upwelling of deeper water back to the surface, thereby, stimulating primary productivity; thus, in areas where high amounts of DNRA is occurring, f-ratio calculations will not be accurate.\n"}
{"id": "11327397", "url": "https://en.wikipedia.org/wiki?curid=11327397", "title": "Electrocar", "text": "Electrocar\n\nElectrocar, Electrocart is an electric vehicle, typically a small cart with an electrical driving gear and a storage battery.\n\nIn the United Kingdom, similar small electric vehicles were known as electric drays.\n\nA typical Soviet/Belarusian electrocar EC-1.00 (and also its modifications, including EC-1.00-1 designed and manufactured () at Mogilev Automobile Plant (MoAZ) is frequently used in factories for the transportation of not so heavy loads, because it does not emit harmful exhaust.\n\n\n"}
{"id": "17904141", "url": "https://en.wikipedia.org/wiki?curid=17904141", "title": "Eolica Beidaud Wind Farm", "text": "Eolica Beidaud Wind Farm\n\nThe Eolica Beidaud Wind Farm is a proposed wind power project in Beidaud, Tulcea County, Romania. It will consist of four individual wind farms connected together. It will have 64 individual wind turbines with a nominal output of around 2 MW which will deliver up to 128 MW of power, enough to power over 75,800 homes, with a capital investment required of approximately US$150 million.\n\nhttp://www.maths-starters.co.uk/"}
{"id": "53932", "url": "https://en.wikipedia.org/wiki?curid=53932", "title": "Euclidean distance", "text": "Euclidean distance\n\nIn mathematics, the Euclidean distance or Euclidean metric is the \"ordinary\" straight-line distance between two points in Euclidean space. With this distance, Euclidean space becomes a metric space. The associated norm is called the Euclidean norm. Older literature refers to the metric as the Pythagorean metric. A generalized term for the Euclidean norm is the L norm or L distance.\n\nThe Euclidean distance between points p and q is the length of the line segment connecting them (formula_1).\n\nIn Cartesian coordinates, if p = (\"p\", \"p\"..., \"p\") and q = (\"q\", \"q\"..., \"q\") are two points in Euclidean \"n\"-space, then the distance (d) from p to q, or from q to p is given by the Pythagorean formula:\n\nThe position of a point in a Euclidean \"n\"-space is a Euclidean vector. So, p and q may be represented as Euclidean vectors, starting from the origin of the space (initial point) with their tips (terminal points) ending at the two points. The Euclidean norm, or Euclidean length, or magnitude of a vector measures the length of the vector:\nwhere the last expression involves the dot product.\n\nDescribing a vector as a directed line segment from the origin of the Euclidean space (vector tail), to a point in that space (vector tip), its length is actually the distance from its tail to its tip. The Euclidean norm of a vector is seen to be just the Euclidean distance between its tail and its tip.\n\nThe relationship between points p and q may involve a direction (for example, from p to q), so when it does, this relationship can itself be represented by a vector, given by\n\nIn a two- or three-dimensional space (\"n\" = 2, 3), this can be visually represented as an arrow from p to q. In any space it can be regarded as the position of q relative to p. It may also be called a displacement vector if p and q represent two positions of some moving point. \n\nThe Euclidean distance between p and q is just the Euclidean length of this displacement vector:\nwhich is equivalent to equation 1, and also to:\n\nIn the context of Euclidean geometry, a metric is established in one dimension by fixing two points on a line, and choosing one to be the origin. The length of the line segment between these points defines the unit of distance and the direction from the origin to the second point is defined as the \"positive direction\". This line segment may be translated along the line to build longer segments whose lengths correspond to multiples of the unit distance. In this manner real numbers can be associated to points on the line (as the distance from the origin to the point) and these are the Cartesian coordinates of the points on what may now be called the real line. As an alternate way to establish the metric, instead of choosing two points on the line, choose one point to be the origin, a unit of length and a direction along the line to call positive. The second point is then uniquely determined as the point on the line that is at a distance of one positive unit from the origin.\n\nThe distance between any two points on the real line is the absolute value of the numerical difference of their coordinates. It is common to identify the name of a point with its Cartesian coordinate. Thus if \"p\" and \"q\" are two points on the real line, then the distance between them is given by:\n\nIn one dimension, there is a single homogeneous, translation-invariant metric (in other words, a distance that is induced by a norm), up to a scale factor of length, which is the Euclidean distance. In higher dimensions there are other possible norms.\n\nIn the Euclidean plane, if p = (\"p\", \"p\") and q = (\"q\", \"q\") then the distance is given by\n\nThis is equivalent to the Pythagorean theorem.\n\nAlternatively, it follows from () that if the polar coordinates of the point p are (\"r\", θ) and those of q are (\"r\", θ), then the distance between the points is\n\nIn three-dimensional Euclidean space, the distance is\n\nIn general, for an \"n\"-dimensional space, the distance is\n\nThe standard Euclidean distance can be squared in order to place progressively greater weight on objects that are farther apart. In this case, the equation becomes\n\nSquared Euclidean distance is not a metric, as it does not satisfy the triangle inequality; however, it is frequently used in optimization problems in which distances only have to be compared.\n\nIt is also referred to as quadrance within the field of rational trigonometry.\n\n\n"}
{"id": "44948384", "url": "https://en.wikipedia.org/wiki?curid=44948384", "title": "Formula Sun Grand Prix", "text": "Formula Sun Grand Prix\n\nThe Formula Sun Grand Prix (FSGP) is an annual solar-car race that takes place on closed-loop race tracks. In the race, teams from colleges and universities throughout North America design, build, test, and race solar-powered vehicles.\n\nEvery two years the race serves as a qualifier for the American Solar Challenge road race.\n\nFSGP 2017 took place between July 6 and 8 at Circuit of the Americas in Austin, Texas.\n\n\nFormula Sun Grand Prix is governed by the Innovators Educational Foundation, and was started in summer 2000 at Heartland Park race track in Topeka, Kansas, in conjunction with a solar-bike race.\n\nIt has served as a qualifying race for the biennial American Solar Challenge road race.\n\nThe race was held there every year through 2005 until the 2007 American Solar Challenge was canceled due to funding issues. It resumed in 2009 and has been held every year since then at a few different venues.\n\nThe inaugural race was held at Heartland Park Topeka race track in Topeka, Kansas. It was won by Rose-Hulman Institute of Technology's \"Solar Phantom V\". The stock class was won by the University of Missouri-Rolla's \"Solar Miner II\".\n\nFSGP 2001 served as a qualifier for the 2001 American Solar Challenge and was the only year when multiple events were held.\n\n\n\nFSGP 2002 was held at Heartland Park Topeka and was won by the University of Missouri-Rolla's \"Solar Miner III\", while the stock class was won by Kansas State University.\n\nFSGP 2003 was held at Heartland Park Topeka and served as a qualifier for the 2003 American Solar Challenge. It was won by the University of Minnesota's \"Borealis II\", while Kansas State finished just one lap behind in second place. The stock class was won by North Dakota State University.\n\nFSGP 2004 was held at Heartland Park Topeka and the University of Missouri-Rolla's \"Solar Miner IV\" took first place overall.\n\nFSGP 2005 was held at Heartland Park Topeka and served as a qualifier for the 2005 North American Solar Challenge. It was won by Minnesota's \"Borealis III\".\n\nAfter a three-year hiatus due to a lack of funding, FSGP 2009 was held at the Motorsports Ranch in Cresson, Texas. The University of Minnesota's \"Centaurus\" took first place overall.\n\nFSGP 2010 was again held at the Cresson Motorsports Ranch and served as a qualifier for the 2010 American Solar Challenge. It was won by the University of Michigan.\n\nFSGP 2011 was held at the Indianapolis Motor Speedway in Indianapolis, Indiana, as part of the Indianapolis 500's 100th-anniversary celebrations. Minnesota's \"Centaurus II\" took first place overall.\n\nFSGP 2012 was held at the Monticello Motor Club in Monticello, New York, and served as a qualifier for the 2012 American Solar Challenge. It was won by Michigan.\n\nFSGP 2013 was held at Circuit of the Americas in Austin, Texas. Oregon State University's \"Phoenix\" took first place overall, while Illinois State University's \"Mercury IV\" finished one lap behind in second place, and Iowa State University's \"Hyperion\" finished one lap behind them in third place.\n\nFSGP 2014 was again held at Circuit of the Americas and served as a qualifier for the 2014 American Solar Challenge. It was won by Michigan. The race was marred by an electrical fire in a garage causing many team's cars and equipment to be damaged by water from the complex's sprinkler system.\n\nFSGP 2015 was held again at Circuit of the Americas on July 29–31. It was won by Iowa State University with 223 total laps and a fast lap of 4:30.444, both track records for solar cars. Hot weather forced most of the teams to have to pull into the pits at times to cool their batteries. The University of Calgary became the first Cruiser team to compete in FSGP.\n<nowiki>*</nowiki> Teams with a Cruiser Class vehicle tied below the next conventional team.\nFSGP 2016 took place at Pitt Race in Wampum, Pennsylvania on July 26-28, 2016. It served as a qualifier for the 2016 American Solar Challenge. It was won by Michigan.\n\nFSGP 2017 was held again at Circuit of the Americas on July 6–8. It was won by the CalSol team of UC Berkeley with 228 total laps. \n\nFSGP 2018 was held at Motorsports Park Hastings in Hastings, Nebraska on July 10–12. This year featured the first ever Multi-Occupant Vehicle (MOV) class at FSGP, which was won by University of Minnesota with triple the score of the runner-up, setting a record as the first team to win FSGP 5 times. The Single-Occupant Vehicle (SOV) class was won by Polytechnique Montreal with 403 laps. \n\n\n\n"}
{"id": "2398706", "url": "https://en.wikipedia.org/wiki?curid=2398706", "title": "Friends of the Earth (HK)", "text": "Friends of the Earth (HK)\n\nFriends of the Earth (HK) Charity Limited (FoE (HK)) is a Hong Kong based environmental organisation founded in 1983. Commonly known as Friends of the Earth (HK)or FoE (HK), it has a membership of more than 12,000 individuals.\n\nAs FoE (HK) receives no regular funding from governments, it relies on donations from the public and volunteer work.\n\nThe organisation is active in environmental campaigns and environmental education. It is not a member group of Friends of the Earth International, owing to disagreements over the latter's policy against commercial sponsorship.\n\nThe former Director, Mrs Mei Ng, achieved recognition of her work with the organisation when she was elected to the United Nations Global 500 Roll of Honor on World Environment Day in 2000.\n\nSince 1992, FoE (HK)'s China team has been networking in mainland China with women's groups, students, academics, NGOs, cadres and governmental agencies. Hundreds of people in China are enlisted as advisors and honorary members. FoE (HK) has also given an \"Earth Award\" to people dedicated to environmental issues.\n\n\n"}
{"id": "3280211", "url": "https://en.wikipedia.org/wiki?curid=3280211", "title": "Goncalo alves", "text": "Goncalo alves\n\nGonçalo alves is a hardwood (from the Portuguese name, Gonçalo Alves). It is sometimes referred to as tigerwood — a name that underscore the wood's often dramatic, contrasting color scheme, that some compare to rosewood.\n\nWhile the sapwood is very light in color, the heartwood is a sombre brown, with dark streaks that give it a unique look. The wood's color deepens with exposure and age and even the plainer-looking wood has a natural luster.\n\nTwo species are usually listed as sources for gonçalo alves: \"Astronium fraxinifolium\" and \"Astronium graveolens\", although other species in the genus may yield similar wood; the amount of striping that is present may vary. All trees grow in neotropical forests; Brazil is a major exporter of these woods.\n"}
{"id": "26859186", "url": "https://en.wikipedia.org/wiki?curid=26859186", "title": "Graphalloy", "text": "Graphalloy\n\nGraphalloy is a group of metal impregnated graphite materials. The materials are commonly used for self-lubricating plain bearings or electrical contacts. They are proprietary materials owned by the Graphite Metallizing Corp.\n\nWhen the metal is impregnated in the graphite it forms long continuous filaments. These are what gives the material its ductility, strength, and heat dissipation properties.\n\nThere are many types of Graphalloy because the graphite can be impregnated with many different metals.\n\nGraphalloy is used in applications where high and low temperatures are encountered, grease or oil is not feasible, expulsion of wear particles is prohibited, or in dusty, submerged, or corrosive environments. It is non-corrosive in gasoline, jet fuel, solvents, bleaches, caustics, dyes, liquefied gases, acids, and many more chemicals. It is not used in highly abrasive applications. Common applications include pumps, bleaching and washing tanks, ovens, industrial dryers, steam turbines, kilns, cryogenics.\n\nIt is also used as bearing in applications where electrical conduction is necessary. It is used in when high frequency current degrades ball or needle bearings. Examples of applications include packaging machines, radar joints, and welding equipment.\n"}
{"id": "24813911", "url": "https://en.wikipedia.org/wiki?curid=24813911", "title": "Guided-rotor compressor", "text": "Guided-rotor compressor\n\nThe guided-rotor compressor (GRC) is a positive-displacement rotary gas compressor. The compression volume is defined by the trochoidally rotating rotor mounted on an eccentric drive shaft with a typical 80 to 85% adiabatic efficiency.\n\nThe development of the GRC started in 1990 to minimize the use of compressor valve plates and springs by using simple inlet/discharge ports.\n\nThe guided-rotor compressor is under research as a hydrogen compressor for hydrogen stations and hydrogen pipeline transport.\n\n"}
{"id": "1739087", "url": "https://en.wikipedia.org/wiki?curid=1739087", "title": "Hafslund Nycomed", "text": "Hafslund Nycomed\n\nHafslund Nycomed is a defunct company that existed between 1986 and 1996 after the power and industry company Hafslund had bought the pharmaceutical company Nycomed. The company was listed on the Oslo Stock Exchange. In 1996 it was demerged and Nycomed merged with the British pharmaceutical company Amersham, while Hafslund took the power division.\n"}
{"id": "22726552", "url": "https://en.wikipedia.org/wiki?curid=22726552", "title": "Heated hose", "text": "Heated hose\n\nHeated hoses are used in bonding technology, filling and dosing systems, medical technology, chemical, pharmaceutical and food industry, extruder applications, and research & development. The heated hoses are used wherever a liquid, viscous or melted medium has to be transported from one place to another, e.g. chocolate, jelly or hotmelt. In most applications, the temperature of the medium needs to remain constant at a specified value irrespective of variations in ambient temperature.\n\nHeated hoses are suitable for environments from -40°C to 80°C and can be used in explosion-proof zones 1/21 and 2/22, if required.\n\nA heated hose consists of a flexible hose, through which the media is pumped. This hose determines the resistance against temperature and chemicals. A heating element is wrapped on the hose and then it is covered with insulation material. Possible insulation materials are Polyamid, steel wire or silicone. The heating element contains a heat sensor. Fittings and fixtures can vary with the application.\n\nHeated hose are generally specified by required inside diameter, working pressure, operating conditions, voltage, temperature sensor and hose fitting.\n\nHeated hoses are now available for use around the home and farm. Commonly used on farms to get water to livestock in northern climates during the winter.\n\n\n"}
{"id": "52729578", "url": "https://en.wikipedia.org/wiki?curid=52729578", "title": "Icelink", "text": "Icelink\n\nIcelink is a proposed electricity interconnector between Iceland and Great Britain. As of 2017, the project is still at the feasibility stage. According to current plans, IceLink may become operational in 2027.\n\nAt 1000–1200 km, the 1000 MW HVDC link would be the longest sub-sea power interconnector in the world.\n\nThe project partners are National Grid plc in the UK, and Landsvirkjun, the state-owned generator in Iceland, and Landsnet, the Icelandic Transmission System Operator (TSO).\n\n\n"}
{"id": "33714853", "url": "https://en.wikipedia.org/wiki?curid=33714853", "title": "Jess Moore", "text": "Jess Moore\n\nJess Moore is a founding member and spokesperson of Stop CSG Illawarra, an independent community group campaigning for a freeze on coal seam gas development in NSW until a Royal Commission can investigate the industry. She's been involved in a range of other community campaigns for social & ecological justice including renewable energy development, rights for Aboriginal communities, marriage equality and more.\n\nShe was President of the Wollongong Undergraduate Students' Association in 2006 and was the national co-ordinator of Resistance, in 2010. In 2011, Moore won the Nature Conservation Council of New South Wales “Rising Star” award for being the “most outstanding campaigner under the age of 30” for her role in the campaign to stop coal seam gas mining and other campaigns to protect the environment and water resources.\n"}
{"id": "17523133", "url": "https://en.wikipedia.org/wiki?curid=17523133", "title": "Lithium-ion capacitor", "text": "Lithium-ion capacitor\n\nA lithium-ion capacitor (LIC) is a hybrid type of capacitor classified as a type of supercapacitor. Activated carbon is typically used as the cathode. The anode of the LIC consists of carbon material which is pre-doped with lithium ions. This pre-doping process lowers the potential of the anode and allows a relatively high output voltage compared with other supercapacitors.\n\nIn 1981, Dr. Yamabe of Kyoto University, in collaboration with Dr. Yata of Kanebo Co., created a material known as PAS (polyacenic semiconductive) by pyrolyzing phenolic resin at 400–700 °C. This amorphous carbonaceous material performs well as the electrode in high-energy-density rechargeable devices. Patents were filed in the early 1980s by Kanebo Co., and efforts to commercialize PAS capacitors and lithium-ion capacitors (LICs) began. The PAS capacitor was first used in 1986, and the LIC capacitor in 1991.\n\nA lithium-ion capacitor is a hybrid electrochemical energy storage device which combines the intercalation mechanism of a lithium-ion battery anode with the double-layer mechanism of the cathode of an electric double-layer capacitor (EDLC). The packaged energy density of an LIC is approximately 20 Wh/kg, roughly four times higher than an EDLC and five times lower than a lithium-ion battery. The power density, however, has been shown to match that of EDLCs, as it is able to completely discharge in seconds. At the negative electrode (cathode), for which activated carbon is often used, charges are stored in an electric double layer that develops at the interface between the electrode and the electrolyte.\n\nThe positive electrode (anode) was originally made from lithium titanate oxide, but is now more commonly made from graphitic carbon to maximize energy density. The graphitic electrode potential initially at -0.1 V versus SHE (standard hydrogen electrode) is lowered further to -2.8 V by intercalating lithium ions. This step is referred to as \"doping\" and often takes place in the device between the anode and a sacrificial lithium electrode. The pre-doping process is critical to the device functioning as it can significantly affect the development of the solid electrolyte interphase (SEI) layer. Doping the anode lowers the anode potential and leads to a higher output voltage of the capacitor. Typically, output voltages for LICs are in the range of 3.8–4.0 V but are limited to minimum allowed voltages of 1.8–2.2 V. If the voltage drops any lower than this lithium ions will deintercalate more rapidly than they can be restored during normal use. Like EDLCs, LIC voltages vary linearly adding to complications integrating them into systems which have power electronics that expect the more stable voltage of batteries. As a consequence, LICs have a high energy density, which varies with the square of the voltage. The capacitance of the anode is several orders of magnitude larger than that of the cathode. As a result, the change of the anode potential during charge and discharge is much smaller than the change in the cathode potential.\n\nThe electrolyte used in an LIC is a lithium-ion salt solution that can be combined with other organic components and is generally identical to that used in lithium-ion batteries.\n\nA separator prevents direct electrical contact between anode and cathode.\n\nTypical properties of an LIC are\n\n\nBatteries, EDLC and LICs each have different strengths and weaknesses, making them useful for different categories of applications.\nLICs have higher power densities than batteries, and are safer than lithium-ion batteries (LIBs), in which thermal runaway reactions may occur.\nCompared to the electric double-layer capacitor (EDLC), the LIC has a higher output voltage. Although they have similar power densities, the energy density of an LIC is much higher than those of the others.\n\nThe Ragone plot in figure 1 shows that LICs combine the high energy of LIBs with the high power density of EDLCs.\n\nCycle life performance of LICs is much better than batteries and is similar to EDLCs.\n\nLithium-ion capacitors are quite suitable for applications which require a high energy density, high power densities and excellent durability. Since they combine high energy density with high power density, there is no need for additional electrical storage devices in various kinds of applications, resulting in reduced costs.\n\nPotential applications for lithium-ion capacitors are, for example, in the fields of wind power generation systems, uninterruptible power source systems (UPS), voltage sag compensation, photovoltaic power generation, energy recovery systems in industrial machinery, and transportation systems.\n\n"}
{"id": "12557741", "url": "https://en.wikipedia.org/wiki?curid=12557741", "title": "London tornado of 1091", "text": "London tornado of 1091\n\nThe London Tornado of 1091 is reckoned by modern assessment of the reports as possibly a T8 tornado (roughly equal to an F4 tornado) which occurred in London in the Kingdom of England and was the earliest reported tornado in that area, occurring on Friday, 17 October 1091. The wooden London Bridge was demolished, and the church of St. Mary-le-Bow in the city of London was badly damaged; four rafters long were driven into the ground with such force that only protruded above the surface. Other churches in the area were demolished, as were over 600 (mostly wooden) houses. For all the damage inflicted, the tornado claimed just two known victims from a population of about 18,000.\n"}
{"id": "55831305", "url": "https://en.wikipedia.org/wiki?curid=55831305", "title": "Ministry of Oil Industry (Soviet Union)", "text": "Ministry of Oil Industry (Soviet Union)\n\nThe Ministry of Oil Industry (Minnefteprom; ) was a government ministry in the Soviet Union.\n\nThe Ministry of the Petroleum Industry was created by the 28 December 1948 ukase of the Presidium of the Supreme Soviet USSR merging the Ministry of the Petroleum Industry of the Southern and Western Regions, the Ministry of the Petroleum Industry of the Eastern Regions, Glavgaztoppron (Main Administration of Synthetic Liquid Fuel and Gas) and Glavneftegazstroy (Main Administration for the Construction of the Petroleum and Gas Industry) under the Council of Ministers USSR, and Glavneftesnab (Main Administration for the Supply of Petroleum Products to the National Economy) under Gossnab USSR (State Committee of the Council of Ministers USSR for Material and Technical Supply to the National Economy).\n\nThe Ministry of Petroleum Industry (People's Commissariat of Petroleum Industry prior to 15 March 1946) was established 12 October 1939 by ukase of the Presidium, Supreme Soviet USSR, and was subdivided on 4 March 1946 into the Ministry of the Petroleum Industry of the Southern ard Western Regions of the USSR and the Ministry of the Petroleum Industry of the Eastern Region of the USSR.\n\nPrior to 1939 it had been a part of the All-Union People's Commissariat of the Fuel Industry, established 24 January 1939 by ukase of the Presidium, Supreme Soviet USSR, as a result of the subdivision of the People's Commissariat of Heavy Industry USSR.\n\nThe Ministry of the Petroleum Industry was an all-Union ministry which administered the petroleum extraction, petroleum refining, and gas industries, the production of liquid fuels, the construction of installations for the petroleum and gas industries, the construction of petroleum-drilling machinery, and the marketing of petroleum products.\n\nThe Ministry of the Petroleum Industry was headed by a minister who directed the entire work of the Ministry of the Petroleum Industry and of the enterprises and organizations under its jurisdiction. The Minister of the Petroleum Industry issued, within the limits of his competence, orders and directives based on, and in execution of, existing laws, as well as of decrees and regulations of the Council of Ministers USSR, and he checked on their execution.\n\nThe Minister of the Petroleum Industry appointed directors of main administrations, administrations, and divisions of the Ministry and of the enterprises and organizations under its jurisdiction; he organized enterprises and organizations in accordance with established procedures; he approved statutes on main administrations, administrations, and divisions of the Ministry as well as the statutes and charters of organizations and enterprises under the jurisdiction of the Ministry.\n\nA collegium was formed within the Ministry of the Petroleum Industry, consisting of the Minister (chairman), his deputies, and the supervisory personnel of the Ministry. The membership of the collegium was approved by the Council of Ministers USSR, upon recommendation of the Minister of the Petroleum Industry.\n\nThe collegium of the Ministry of the Petroleum Industry, at its regular sessions, considered questions of practical supervision, of checking on the execution of decisions, and of selection of personnel, as well as the most important orders and directives of the Ministry; it hears the reports of the directors of main administrations, administrations, and divisions of the Ministry and of organizations and enterprises under its jurisdiction.\n\nThe decisions of the collegium are carried out by orders of the Minister. The basic task, for the carrying out of which the Ministry of the Petroleum Industry is responsible, is to assure the development of the petroleum industry in accordance with plans approved by the USSR government, with the goal of completely satisfying the requirement of the rational economy for petroleum products and gas.\n\nThis goal was to be achieved by the introduction of the most modern equipment, by mechanizing and automatizing production processes in the extraction and refining of petroleum and gas, by the construction of oil fields, refineries, gas plants, plants for the production of synthetic liquid fuels, pipelines and petroleum tanks by the development of petroleum machinery building, by the creation of permanent cadres of qualified workers, engineers, and technicians, by increasing labor productivity, and by improving the quality of production and reducing its costs. The Ministry of the Petroleum Industry prepares production plans (prospective, yearly, quarterly), plans for capital construction, marketing, and railroad and water transport, as well as for balancing income and expenditures.\n\nIt submitted these plans for ratification by the Council of Ministers USSR in accordance with established procedure, and it takes measures toward the fulfillment of plans that have been ratified.\n\nThe Ministry of the Petroleum Industry organized the prospecting and surveying of new petroleum, gas, and ozocerite deposits by geological, geophysical, and geochemical surveying methods. It takes measures to increase labor productivity, to improve the quality of production, and to reduce the cost of production and construction. It exercises technical and production supervision over enterprises of the Ministry, and introduces the most modern equipment, technical improvements and inventions, mechanization and automatization of production processes,and established norms for the consumption of materials, power and fuel. \n\nIt drafts plans for production standards in branches of industry under the jurisdiction of the Ministry- and submits them for ratification according to established procedure. The Ministry directs the construction of enterprises and installations of the petroleum industry and, in accordance with established procedure, ratifies planned quotas, technical plans, and estimates for capital construction. It organizes and supervises the material and technical supply of enterprises, organizations, and installations. It supervises the operations of enterprises which produce industrial and construction materials required by branches of industry under the jurisdiction of the Ministry.\n\nThe Ministry handled the sales of petroleum products produced by enterprises of the Ministry of the Petroleum Industry through a system of administrations and petroleum bases of Glavneftesbyt (Main Administration of Petroleum Sales).\n\nIt selected the personnel of the Ministry, takes measures to provide enterprises and construction projects of the Ministry with personnel and to utilize them properly, at the same time providing for their living conditions.\n\nIt draws up regulations on the wages of workers, engineers, technicians, and employees of the enterprises, submits them for ratification by the Council of Ministers USSR, in accordance with established procedure, and supervises their application. It determines technical norms and the revision of output norms (except for the basic revision of norms carried out with the permission of the government), and checks on their fulfillment.\n\nIt supervised the observance of labor legislation and rules for accident prevention. It directs the conclusion of collective contracts and checks on their fulfillment. The Ministry directs socialist competition to develop the creative initiative of workers, engineers, and technicians, to increase labor productivity further, to fulfill production plans ahead of schedule, to improve the quality of production, to increase profits of the enterprises, and to acquire above-plan accumulations.\n\nIt financed the enterprises and organizations of the Ministry in accordance with established procedure, supervises their financial activity, and takes measures to speed up the turnover of working capital and to increase the profits of the enterprises. It supervises the organization and system of accounting, approves balances and reports of main administrations and organizations under the direct jurisdiction of the Ministry, and draws up periodic and yearly accounting records for all types of industrial and administrative activity of the Ministry.\n\nIt exercised financial control and makes documentary revisions in the administration of the Ministry. It administers scientific research institutes under the jurisdiction of the Ministry, planning organizations, and educational establishments and directs the training of workers in the mass professions. It directs the activity of administrations and divisions of labor supply and subsidiary economies.\n\nIt takes measures for the protection of state socialist, property in enterprises, establishments, and organizations under the Ministry. It publishes literature on the technology, economics, and organization of the petroleum and gas industries. The structure of the Ministry of the Petroleum Industry is determined by the Council of Ministers USSR to utilize to the best advantage outstanding experiences of directors, ordinary workers, and Stakhanovites, the Ministry of the Petroleum Industry, its chain administrations associations, trusts and enterprises summon meetings at which reports are heard and discussed dealing with the most important party and government decisions as well as administrative directives of the Ministry. To develop criticism and self-criticism, questions on the activity of production and management are also discussed.\n\n\"Source\":\n"}
{"id": "26799731", "url": "https://en.wikipedia.org/wiki?curid=26799731", "title": "Nigerian Gas Association", "text": "Nigerian Gas Association\n\nThe Nigerian Gas Association is the professional body responsible for the promotion and protection of the interests of the gas industry in Nigeria.\n\nFormed in 1999, the NGA's initial membership came from the primary gas production and utilization companies in Nigeria. The earliest members include the Nigerian National Petroleum Corporation (NNPC), the Shell Petroleum Development Company (SPDC), the Nigeria Liquefied Natural Gas Limited (NLNG), Chevron Nigeria Limited, the Nigerian Gas Company (NGC), Elf Petroleum, Mobil Producing Nigeria, the Nigerian Agip Oil Company (NAOC) and Conoco Energy Nigeria Limited.\n\nThe NGA has since grown to encompass the entire gamut of stakeholders and operators in the Nigerian gas industry – from gas producers, to transmitters, gas industry service providers, students, members of the academia and government. The association presently has over 40 corporate members, 800 individual professional industry members, and more than 100 student members. The NGA is also a full chartered member of the International Gas Union (IGU), representing Nigeria on the Council of the global gas body.\n\nThe aims of the Nigerian Gas Association include:\n\nThe Nigerian Gas Association (NGA) is run by a council of elected officers who serve for a single term of two years. The Association’s elections usually hold just before the commencement of its Annual General Meeting (AGM). The Association also has a functional secretariat that is staffed by competent administrative personnel to provide members and non-members with information and other support activities as required.\n\nThe Nigerian Gas Association's Council comprises the following officers:\n\nThe Nigerian Gas Association usually organizes events and activities through which it drives to achieve its agenda-setting and industry engagement activities. Some of these events include:\n\nBusiness leaders and other prespected parties in the Nigerian gas business/industry are usually invited to the NGA Business Fora, where they speak and share their thoughts on agreed themes. The quarterly fora are free to all NGA members and serve as a pint for the gathering of the latest thoughts on the Nigerian gas industry.\n\nHeld every two years, the NGA Conference and Exhibition serve as the highlights of the NGA's calendar. The events are usually attended by experts, members of the academia from within and outside Nigeria. The theme for the conference is set from the end of the previous conference, and papers are delivered by subject-matter experts. The event is also characterised by syndicate sessions and an exhibition, where all gas industry players exhibit their products, services and projects to attendees.\n\nTo be launched in June 2010, The NGA Training Series are a set of training programmes organized by the NGA to address certain skill and competency gaps identified in the Nigerian gas industry. The training programmes will be contextualised to fit and meet with the requirements of the Nigerian market, and will prosecuted by both professional experts with real industry experience.\n\n"}
{"id": "33085446", "url": "https://en.wikipedia.org/wiki?curid=33085446", "title": "Oil Insurance Limited", "text": "Oil Insurance Limited\n\nOil Insurance Limited (OIL) is a Bermuda–based mutual insurance company. It was founded in 1972 by 16 oil companies.\n\nThe founding shareholders were:\n\n"}
{"id": "7103278", "url": "https://en.wikipedia.org/wiki?curid=7103278", "title": "Patch cut", "text": "Patch cut\n\nPatch cuts are logging cuts too small to be considered clearcuts, and are instead considered a form of selection cut. A typical patch cut might be 2-3 tree lengths. Below a certain size, seedling regeneration advantage shifts from the shade intolerant species favored in clearcuts to the shade tolerant species favored by selection harvests.\n\nAn areas of patch cuts of different shapes were tried in the Northern hemisphere to see where shade tolerant and shade intolerant species might regenerate. It was found in small patches (1/10 acre) N. 44° that there was a limited solar exposure of only a few hours (3-4) and this would lead to strongly shade tolerant and moderately shade tolerant trees dominating over time. Yet at slightly larger (1/2 acre) patches much more sunlight for longer was reaching the forest floor. This would warm the floor more, increase nutrient release and thus more light demanding species could grow faster and might then reach maturity in the stand.\n\nAs well as light other factors to be considered are wind to avoid windthrow, pathogens and fire risk. Positive benefits can also be gained if patches keep horizontal biodiversity in a forest stand and for an owner with not so much forest this may be useful for keeping non-timber forest products in usable amounts.\n\nPatch cuts can have less visual impact than clearcuts, and can mimic some stand disturbance processes. Patch cuts can also be simpler to implement than trying to selectively log individual trees within a selection cut.\n\nPatch cuts are distinct from the group selection method. A patch would pick several trees to be big enough to allow regeneration of a specific species, perhaps that has problems growing due to shade intolerance or another factor, as several together. Whereas a group would be only a few mature trees (3-5) and would not necessarily favor a shade intolerant species to establish into a group. In a patch the edge effect would be overcome but in a group most likely not.\n\nPatch cuts may be used in a hybrid system with the single tree selection cut method or the group tree selection cut method.\n\n\n"}
{"id": "8861618", "url": "https://en.wikipedia.org/wiki?curid=8861618", "title": "Patterson power cell", "text": "Patterson power cell\n\nThe Patterson power cell is an electrolysis device invented by chemist James A. Patterson, which he said created 200 times more energy than it used, and neutralize radioactivity without emitting any harmful radiation. It is one of several cells that some observers classified as cold fusion; cells which were the subject of an intense scientific controversy in 1989, before being discredited in the eyes of mainstream science.\n\nThe Patterson power cell is given little credence by scientists. Physicist Robert L. Park describes the device as fringe science in his book \"Voodoo Science\".\n\nThe cell has a non-conductive housing. The cathode is composed of thousands of sub-millimeter microspheres (co-polymer beads), with a flash coat of copper and multiple layers of electrolytically deposited thin film (650 Angstrom) nickel and palladium. The beads are submerged in water with a lithium sulfate (LiSO) electrolyte solution. The cell uses lithium sulphate for electrolyte and nano nickel or nickel spheres. It splits water into hydrogen and oxygen. I refer you to the U.S. Patent.\n\nIn 1995, Clean Energy Technologies Inc. was formed to produce and promote the power cell. Patterson died in 2008.\n\nPatterson variously said it produced a hundred or two hundred times more power than it used. Clean Energy Technologies, Inc. (CETI) representatives promoting the device at the Power-Gen '95 Conference said that an input of 1 watt would generate more than 1,000 watts of excess heat. This supposedly happens as hydrogen or deuterium nuclei fuse together to produce heat through a form of low energy nuclear reaction. The byproducts of nuclear fusion, e.g. a tritium nucleus and a proton or an He nucleus and a neutron, have not been detected in a reliable way, leading a vast majority of experts to think that no such fusion is taking place.\n\nIt is further claimed that if radioactive isotopes such as uranium are present, the cell enables the hydrogen nuclei to fuse with these isotopes, transforming them into stable elements and thus neutralizing the radioactivity; and this would be achieved without releasing any radiation to the environment and without expending any energy. A televised demonstration on June 11, 1997, on \"Good Morning America\" was not conclusive because there was no measurement of the radioactivity of the beads after the test, thus it cannot be discarded that the beads had simply absorbed the uranium ions and become radioactive themselves. In 2002, the neutralization of radioactive isotopes has only been achieved through intense neutron bombardment in a nuclear reactor or large scale high energy particle accelerator, and at a large expense of energy.\n\nWhen asked about reliability in 1998, Gabe Collins, a chemical engineer at CETI, stated: \"When they don't work, it's mostly due to contamination. If you get any sodium in the system it kills the reaction – and since sodium is one of the more abundant elements, it's hard to keep it out.\"\n\nPatterson has carefully distanced himself from the work of Fleischmann and Pons and from the label of \"cold fusion\", due to the negative connotations associated to them since 1989. Ultimately, this effort was unsuccessful, and not only did it inherit the label of pathological science, but it managed to make cold fusion look a little more pathological in the public eye. Some cold fusion proponents view the cell as a confirmation of their work, while critics see it as \"the fringe of the fringe of cold fusion research\", since it attempts to commercialize cold fusion on top of making bad science.\n\nIn 2002, John R. Huizenga, professor of nuclear chemistry at the University of Rochester, who was head of a government panel convened in 1989 to investigate the cold fusion claims of Fleischmann and Pons, and who wrote a book about the controversy, said \"I would be willing to bet there's nothing to it\", when asked about the Patterson Power Cell.\n\nIn 2006, Hideo Kozima, professor emeritus of physics at Shizuoka University, has suggested that the byproducts are consistent with cold fusion.\n\n2016 report http://lenr-canr.org/acrobat/MosierBossinvestigat.pdf\n\nGeorge H. Miley is a professor of nuclear engineering and a cold fusion researcher who claims to have replicated the Patterson Power Cell. During the 2011 World Green Energy Symposium, Miley stated that his device continuously produces several hundred watts of energy. Earlier results by Miley have not convinced mainstream researchers, who believe that they can be explained by contamination or by misinterpretation of data. \n\nOn the television show \"Good Morning America\", Quintin Bowles, professor of mechanical engineering at the University of Missouri–Kansas City, claimed in 1996 to have successfully replicated the Patterson power cell. In the book \"Voodoo Science\", Bowles is quoted as having stated: \"It works, we just don't know how it works\".\n\nA replication has been attempted at Earthtech, using a CETI supplied kit. They were not able to replicate the excess heat. They looked for cold fusion products, but only found traces of contamination in the electrolyte.\n\n"}
{"id": "1898867", "url": "https://en.wikipedia.org/wiki?curid=1898867", "title": "Petrobras 36", "text": "Petrobras 36\n\nPetrobras 36 (P-36) was at the time the largest floating semi-submersible oil platform in the world prior to its sinking on 20 March 2001. It was owned by Petrobras, a semi-public Brazilian oil company headquartered in Rio de Janeiro. The cost of the platform was US$350 million (currently US$).\n\nThe vessel was built at the Fincantieri shipyard in Genoa, Italy in 1995 as a drilling rig. She was owned then by Società Armamento Navi Appoggio S.p.A. The rig was converted by Davie Industry, Levis, Canada to the world's largest oil production platform.\n\n\"P-36\" was operating for Petrobras on the Roncador Oil Field, off the Brazilian coast, producing about of crude per day.\n\nP-36 was replaced by FPSO-Brasil which is a leased vessel from SBM Offshore. The FPSO-Brasil started its lease contract with Petrobras in December 2002.\n\nIn the early hours of March 15, 2001 there were two explosions in the aft starboard column at or around the emergency drain tank. The first explosion was caused by an overpressure event, the second by ignition of leaking hydrocarbon vapor. At the time there were 175 people on the rig; 11 were killed. Following the explosions, the rig developed a 16° list, sufficient to allow down-flooding from the submerged fairlead boxes.\n\nMarine salvage teams tried over the weekend to save the platform by pumping nitrogen and compressed air into the tanks to expel the water, but they abandoned the rig after bad weather.\n\nThe platform sank five days after the explosions (March 20), in of water with an estimated of crude oil remaining on board.\n\n\n"}
{"id": "1991352", "url": "https://en.wikipedia.org/wiki?curid=1991352", "title": "Polyhydroxyalkanoates", "text": "Polyhydroxyalkanoates\n\nPolyhydroxyalkanoates or PHAs are polyesters produced in nature by numerous microorganisms, including through bacterial fermentation of sugar or lipids. When produced by bacteria they serve as both a source of energy and as a carbon store. More than 150 different monomers can be combined within this family to give materials with extremely different properties. These plastics are biodegradable and are used in the production of bioplastics.\n\nThey can be either thermoplastic or elastomeric materials, with melting points ranging from 40 to 180 °C.\n\nThe mechanical properties and biocompatibility of PHA can also be changed by blending, modifying the surface or combining PHA with other polymers, enzymes and inorganic materials, making it possible for a wider range of applications.\n\nTo produce PHA, a culture of a micro-organism such as \"Cupriavidus necator\" is placed in a suitable medium and fed appropriate nutrients so that it multiplies rapidly. Once the population has reached a substantial level, the nutrient composition is changed to force the micro-organism to synthesize PHA. The yield of PHA obtained from the intracellular granule inclusions can be as high as 80% of the organism's dry weight.\n\nThe biosynthesis of PHA is usually caused by certain deficiency conditions (e.g. lack of macro elements such as phosphorus, nitrogen, trace elements, or lack of oxygen) and the excess supply of carbon sources.\n\nPolyesters are deposited in the form of highly refractive granules in the cells. Depending upon the microorganism and the cultivation conditions, homo- or copolyesters with different hydroxyalkanic acids are generated. PHA granules are then recovered by disrupting the cells. Recombinant \"Bacillus subtilis\" str. pBE2C1 and \"Bacillus subtilis\" str. pBE2C1AB were used in production of polyhydroxyalkanoates (PHA) and it was shown that they could use malt waste as carbon source for lower cost of PHA production.\n\nPHA synthases are the key enzymes of PHA biosynthesis. They use the coenzyme A - thioester of (r)-hydroxy fatty acids as substrates. The two classes of PHA synthases differ in the specific use of hydroxy fatty acids of short or medium chain length.\n\nThe resulting PHA is of the two types:\n\n\nA few bacteria, including \"Aeromonas hydrophila\" and \"Thiococcus pfennigii\", synthesize copolyester from the above two types of hydroxy fatty acids, or at least possess enzymes that are capable of part of this synthesis.\n\nAnother even larger scale synthesis can be done with the help of soil organisms. For lack of nitrogen and phosphorus they produce a kilogram of PHA per three kilograms of sugar.\n\nThe simplest and most commonly occurring form of PHA is the fermentative production of poly-beta-hydroxybutyrate (poly-3-hydroxybutyrate, P3HB), which consists of 1000 to 30000 hydroxy fatty acid monomers.\n\nIn the industrial production of PHA, the polyester is extracted and purified from the bacteria by optimizing the conditions of microbial fermentation of sugar or glucose.\n\nIn the 1980s, Imperial Chemical Industries developed poly(3-hydroxybutyrate-\"co\"-3-hydroxyvalerate) obtained via fermentation that was named \"Biopol\". It was sold under the name \"Biopol\" and distributed in the U.S. by Monsanto and later Metabolix.\n\nAs raw material for the fermentation, carbohydrates such as glucose and sucrose can be used, but also vegetable oil or glycerine from biodiesel production. Researchers in industry are working on methods with which transgenic crops will be developed that express PHA synthesis routes from bacteria and so produce PHA as energy storage in their tissues. Several companies are working to develop methods of producing PHA from waste water, including start-up Micromidas and Veolia subsidiary Anoxkaldnes.\n\nPHAs are processed mainly via injection molding, extrusion and extrusion bubbles into films and hollow bodies.\n\nPHA polymers are thermoplastic, can be processed on conventional processing equipment, and are, depending on their composition, ductile and more or less elastic. They differ in their properties according to their chemical composition (homo-or copolyester, contained hydroxy fatty acids).\n\nThey are UV stable, in contrast to other bioplastics from polymers such as polylactic acid, partial ca. temperatures up to , and show a low permeation of water. The crystallinity can lie in the range of a few to 70%. Processability, impact strength and flexibility improves with a higher percentage of valerate in the material. PHAs are soluble in halogenated solvents such chloroform, dichloromethane or dichloroethane.\n\nPHB is similar in its material properties to polypropylene (PP), has a good resistance to moisture and aroma barrier properties. Polyhydroxybutyric acid synthesized from pure PHB is relatively brittle and stiff. PHB copolymers, which may include other fatty acids such as beta-hydroxyvaleriate acid, may be elastic.\n\nDue to its biodegradability and potential to create bioplastics with novel properties, much interest exists to develop the use of PHA-based materials. PHA fits into the green economy as a means to create plastics from non-fossil fuel sources. Furthermore, active research is being carried out for the biotransformation \"upcycling\" of plastic waste (e.g., polyethylene terephthalate and polyurethane) into PHA using \"Pseudomonas putida\" bacteria.\n\nA PHA copolymer called PHBV (poly(3-hydroxybutyrate-co-3-hydroxyvalerate)) is less stiff and tougher, and it may be used as packaging material.\n\nIn June 2005, a US company (Metabolix, Inc.) received the US Presidential Green Chemistry Challenge Award (small business category) for their development and commercialisation of a cost-effective method for manufacturing PHAs.\n\nThere are potential applications for PHA produced by micro-organisms within the medical and pharmaceutical industries, primarily due to their biodegradability.\n\nFixation and orthopaedic applications have included sutures, suture fasteners, meniscus repair devices, rivets, tacks, staples, screws (including interference screws), bone plates and bone plating systems, surgical mesh, repair patches, slings, cardiovascular patches, orthopedic pins (including bone.lling augmentation material), adhesion barriers, stents, guided tissue repair/regeneration devices, articular cartilage repair devices, nerve guides, tendon repair devices, atrial septal defect repair devices, pericardial patches, bulking and filling agents, vein valves, bone marrow scaffolds, meniscus regeneration devices, ligament and tendon grafts, ocular cell implants, spinal fusion cages, skin substitutes, dural substitutes, bone graft substitutes, bone dowels, wound dressings, and hemostats.\n\n"}
{"id": "48161271", "url": "https://en.wikipedia.org/wiki?curid=48161271", "title": "Radiation exposure", "text": "Radiation exposure\n\nRadiation exposure is a measure of the ionization of air due to ionizing radiation from photons; that is, gamma rays and X-rays. It is defined as the electric charge freed by such radiation in a specified volume of air divided by the mass of that air. \n\nThe SI unit of exposure is the coulomb per kilogram (C/kg), which has largely replaced the roentgen (R). One roentgen equals ; an exposure of one coulomb per kilogram is equivalent to 3876 roentgens.\n\nAs a measure of radiation damage exposure has been superseded by the concept of absorbed dose which takes into account the absorption characteristic of the target material. \n\nDose is the measure of energy per unit mass deposited by ionizing radiation. For a given radiation field, the absorbed dose will depend on the type of matter which absorbs the radiation. For instance, for an exposure of 1 roentgen by gamma rays with an energy of 1 MeV, the dose in air will be 0.877 rad, the dose in water will be 0.975 rad, the dose in silicon will be 0.877 rad, and the dose in averaged human tissue will be 1 rad. A table giving the exposure to dose conversion for these four materials for a variety of gamma ray energies can be found in the reference.\n\nThe gamma ray field can be characterized by the exposure rate (in units of, for instance, roentgen per hour). For a point source, the exposure rate will be linearly proportional to the source's radioactivity and inversely proportional to the square of the distance,\n\nwhere \"F\" is the exposure rate, \"r\" is the distance, \"α\" is the source activity, and \"Γ\" is the exposure rate constant, which is dependent on the particular radionuclide used as the gamma ray source.\n\nBelow is a table of exposure rate constants for various radionuclides. They give the exposure rate in roentgens per hour for a given activity in millicuries at a distance in centimeters.\n\nThe following table shows radiation quantities in SI and non-SI units:\n\nAlthough the United States Nuclear Regulatory Commission permits the use of the units curie, rad, and rem alongside SI units, the European Union European units of measurement directives required that their use for \"public health ... purposes\" be phased out by 31 December 1985.\n\n"}
{"id": "50961436", "url": "https://en.wikipedia.org/wiki?curid=50961436", "title": "Renewable portfolio standard", "text": "Renewable portfolio standard\n\nA renewable portfolio standard (RPS) is a regulation that requires the increased production of energy from renewable energy sources, such as wind, solar, biomass, and geothermal. Other common names for the same concept include Renewable Electricity Standard (RES) at the United States federal level and Renewables Obligation in the UK.\n\nThe RPS mechanism places an obligation on electricity supply companies to produce a specified fraction of their electricity from renewable energy sources. Certified renewable energy generators earn certificates for every unit of electricity they produce and can sell these along with their electricity to supply companies. Supply companies then pass the certificates to some form of regulatory body to demonstrate their compliance with their regulatory obligations. RPS can rely on the private market for its implementation. In jurisdictions such as California, minimum RPS requirements are legislated. California Senate Bill 350 passed in October 2015 requires retail sellers and publicly owned utilities to procure 50 percent of their electricity from eligible renewable energy resources by 2030. RPS programs tend to allow more price competition between different types of renewable energy, but can be limited in competition through eligibility and multipliers for RPS programs. Those supporting the adoption of RPS mechanisms claim that market implementation will result in competition, efficiency, and innovation that will deliver renewable energy at the lowest possible cost, allowing renewable energy to compete with cheaper fossil fuel energy sources.\n\nRPS-type mechanisms have been adopted in several countries, including the United Kingdom, Italy, Poland, Sweden, Belgium, and Chile, as well as in 29 of 50 U.S. states, and the District of Columbia.\n\n\"Renewable Energy (Electricity) Act 2000\" (Cth)\n\nChina adopted a renewable energy target in 2006 and modified it in 2009 to the following targets:\n\n\nThe European Union passed the Directive on Electricity Production from Renewable Energy Sources in 2001 and expanded it in 2007 to the following EU-wide targets (although member states are free to pass more aggressive targets):\n\n\nThe German Renewable Energy Act, since its adoption in 2000, is producing strong growth in renewable power capacity by encouraging private investors through guaranteed Feed-in tariffs.\nGermany adopted targets more aggressive than EU mandated targets on September 2010:\n\n\nBased on the 1997 Act on the Promotion of New Energy Usage, 118 million KWh was targeted in 2012 (METI).\n\nThe Republic of Korea adopted the \"Act on the Promotion of the Development, Use, and Diffusion of New and Renewable Energy\" since 2012.\n\nThe Renewables Obligation (RO) is designed to encourage generation of electricity from eligible renewable sources in the United Kingdom. It was introduced in England and Wales and in a different form (the Renewables Obligation (Scotland)) in Scotland in April 2002 and in Northern Ireland in April 2005, replacing the Non-Fossil Fuel Obligation which operated from 1990.\n\nThe RO places an obligation on licensed electricity suppliers in the United Kingdom to source an increasing proportion of electricity from renewable sources, similar to a renewable portfolio standard. In 2010/11 it is 11.1% (4.0% in Northern Ireland). This figure was initially set at 3% for the period 2002/03 and under current political commitments will rise to 15.4% (6.3% in Northern Ireland) by the period 2015/16 and then it runs until 2037 (2033 in Northern Ireland). The extension of the scheme from 2027 to 2037 was declared on 1 April 2010 and is detailed in the National Renewable Energy Action Plan. Since its introduction the RO has more than tripled the level of eligible renewable electricity generation (from 1.8% of total UK supply to 7.0% in 2010).\n\nThe Public Utility Regulatory Policies Act is a law, passed in 1978 by the United States Congress as part of the National Energy Act. It was meant to promote greater use of renewable energy.\n\nIn 2009, the US Congress considered Federal level RPS requirements. The American Clean Energy and Security Act reported out of committee in July by the Senate Committee on Energy & Natural Resources includes a Renewable Electricity Standard that called for 3% of U.S. electrical generation to come from non-hydro renewables by 2013, but the full Senate did not pass the bill.\n\nDifferent state RPS programs issue a different number of Renewable Energy Credits depending on the generation technology; for example, solar generation counts for twice as much as other renewable sources in Michigan and Virginia. \n\nThe Lawrence Berkeley National Laboratory claims that RPS requirements were responsible for 60% of the total increase in American renewable electricity generation since the year 2000. However, the LBNL also reports that RPSs' role has been declining in recent years from 71% of the annual American renewables builds in the year 2013 to 46% just two years later, in 2015. \n"}
{"id": "8495722", "url": "https://en.wikipedia.org/wiki?curid=8495722", "title": "Roaring 40s", "text": "Roaring 40s\n\nRoaring 40s is an electricity generator formed in 2005 as a joint venture between Hydro Tasmania, Australia and Hong Kong-based China Light & Power (CLP). Since the beginning in 2005, Roaring 40s has had 13 sites in operation or in planning in Australia, India, Hong Kong and mainland China. Cathedral Rocks, Woolnorth, Waterloo Wind Farm and Musselroe are four notable power plants that the company owns. \n\nRoaring 40s partnership will be splitting in June 2011 with the projects being divided between the two original partners, CLP and Hydro Tasmania.\n\n"}
{"id": "47225270", "url": "https://en.wikipedia.org/wiki?curid=47225270", "title": "Running on Climate", "text": "Running on Climate\n\nRunning On Climate is a feature documentary by Robert Alstead and Joanna Clarke of icycle.ca productions Ltd. The film focuses on the election campaign in 2013 of climate scientist Andrew J. Weaver as the first Green Party Member of the Legislative Assembly in British Columbia, Canada.\n\n\"Running on Climate\" premiered at the DOXA Documentary Film Festival in Vancouver in May, 2015.\n\n"}
{"id": "29866802", "url": "https://en.wikipedia.org/wiki?curid=29866802", "title": "Sandolo", "text": "Sandolo\n\nThe sandolo is a traditional, flat-bottomed Venetian rowing boat designed for the generally shallow waters of the Venetian Lagoon. The plural is sandoli.\nA sandolo is of a much simpler build than a gondola but also has a pointed, decorated metal nose. It is also lighter and smaller than a gondola, and can be recognized at a glance, as it always lacks the benches and high steel prow (called \"ferro\") which is seen on a gondola. The sandolo, like the larger craft, is rowed while standing up. It can be fitted with a sail, and also with an outboard motor.\n\nIn the past, the police used an extant variant of the sandolo called vipera, which differed in having no stem, being sharply pointed at both ends and constructed so that it can be rowed from either end.\n\nSpace in the sandolo is limited, with enough room for one oarsman, aft, two passengers on the main seat, and two more passengers sitting on small stools towards the bow. The traditional use of the sandolo is for recreation and racing, and it is considered one of the four principal types of boat used in and around Venice. Rather less stable than a gondola, it has a rocking motion all of its own.\n\nAlthough not often used for fishing, as such, the craft is used for collecting crabs and mussels, while an early 20th-century writer noted that he had heard the sandolo called \"the donkey cart of Venice\".\nThe boat has also been called \"without doubt one of if not the most graceful of all Venetian craft\". Less manoevrable but lighter than a gondola, it was in the past used especially by boys, artists, and women.\n\nIn \"Gondola Days\" (1897), Francis Hopkinson Smith (1838–1915) stated that the sandolo was \"the only boat of really modern design, and this is rarely used as a fishing-boat\". He went on to describe it as \"a shallow skiff drawing but a few inches of water, and with bow and stern sharp and very low\", and considered that it was originally intended for greater speed in boat racing.\n\nHoratio Brown said in his \"Life on the Lagoons\" (1884), \"The Venetians are not good boat-builders. The only boats they make successfully are gondolas and sandoli. In a later book he wrote, \"The pleasantest way to go to Malamocco is to take a sandolo, if you can.\n\nAlexander Robertson said of Venice in 1898, \"Their streets are canals, their carriages are gondolas and sandolos...\"\n"}
{"id": "37403014", "url": "https://en.wikipedia.org/wiki?curid=37403014", "title": "Schweizerische Zeitschrift für Forstwesen", "text": "Schweizerische Zeitschrift für Forstwesen\n\nSchweizerische Zeitschrift für Forstwesen (the \"Swiss Forestry Journal\") is one of the oldest forestry journals still in print in the world. It was established in 1850.\n\n"}
{"id": "9591516", "url": "https://en.wikipedia.org/wiki?curid=9591516", "title": "Seer (unit)", "text": "Seer (unit)\n\nA Seer (also sihr) is a traditional unit of mass and volume used in large parts of Asia prior to the middle of the 20th century. It remains in use only in a few countries such as Afghanistan and Iran, although in the latter it indicates a smaller unit of weight than the one used in India.\n\nIn India, the seer (Government seer) was defined by the Standards of Weights and Measures Act (No. 89 of 1956, amended in 1960 and 1964) as being exactly equal to 1.25 kg (1.792 lb). However, there were many local variants of the seer in India.\n\nIn Aden (Oman), Nepal, and Pakistan a seer was approximately 0.93310 kg (2.057 lb) derived from the Government seer of British colonial days.\n\nIn Afghanistan, it was a unit of mass, approximately 7.066 kg (15.58 lb).\n\nIn Persia (and later Iran), it was and remains in two units. \n\nThe smaller weight is now part of the national weight system in Iran and is used on daily basis for small measures of delicate foodstuff and choice produce.\n\nIn Sri Lanka, it was a measure of capacity, approximately 1.86 pint (1.024 litres)\n\n"}
{"id": "40697521", "url": "https://en.wikipedia.org/wiki?curid=40697521", "title": "Silje Lundberg", "text": "Silje Lundberg\n\nSilje Ask Lundberg (born 13 April 1988 in Oslo) is a Norwegian environmentalist and chairman of Norwegian Society for the Conservation of Nature (Friends of the Earth Norway). She is a former leder of Nature and Youth. She grew up in Harstad in Northern Norway. Prior to her leadership she has been deputy chairman and been working for other environmental organisations, such as ZERO and Bellona Foundation.\n"}
{"id": "37988477", "url": "https://en.wikipedia.org/wiki?curid=37988477", "title": "The long tailpipe", "text": "The long tailpipe\n\nThe long tailpipe is an argument stating that usage of electric vehicles does not always result in fewer emissions (e.g. greenhouse gases) compared to those from non-electric vehicles. While the argument acknowledges that plug-in electric vehicles operating in all-electric mode have no greenhouse gas emissions from the onboard source of power, it claims that these emissions are shifted from the vehicle tailpipe to the location of the electrical generation plants. From the point of view of a well-to-wheel assessment, the extent of the actual carbon footprint depends on the fuel and technology used for electricity generation.\n\nPlug-in electric vehicles operating in all-electric mode do not emit greenhouse gases from the onboard source of power but emissions are shifted to the location of the generation plants. From the point of view of a well-to-wheel assessment, the extent of the actual carbon footprint depends on the fuel and technology used for electricity generation. From the perspective of a full life cycle analysis, the electricity used to recharge the batteries must be generated from renewable or clean sources such as wind, solar, hydroelectric, or nuclear power for PEVs to have almost none or zero well-to-wheel emissions. On the other hand, when PEVs are recharged from coal-fired plants, they usually produce slightly more greenhouse gas emissions than internal combustion engine vehicles and higher than hybrid electric vehicles.\n\nBecause plug-in electric vehicles do not produce emissions at the point of operation are often perceived as being environmentally friendlier than vehicles driven through internal combustion. Assessing the validity of that perception is difficult due to the greenhouse gases generated by the power plants that provide the electricity to charge the vehicles' batteries. For example, the New York Times reported that a Nissan Leaf driving in Los Angeles would have the same environmental impact as a gasoline-powered car with compared to the same trip in Denver would only have the equivalent of . The U.S. Department of Energy published a concise description of the problem: \"Electric vehicles (EVs) themselves emit no greenhouse gases (GHGs), but substantial emissions can be produced 'upstream' at the electric power plant.\"\n\nA study published in the UK in April 2013 assessed the carbon footprint of plug-in electric vehicles in 20 countries. As a baseline the analysis established that manufacturing emissions account for 70 g CO/km. The study found that in countries with coal-intensive generation, PEVs are no different from conventional petrol-powered vehicles. Among these countries are China, Indonesia, Australia, South Africa and India. A pure electric car in India generates emissions comparable to a petrol car. The country ranking was led by Paraguay, where all electricity is produced from hydropower, and Iceland, where electricity production relies on renewable power, mainly hydro and geothermal power. Resulting carbon emissions from an electric car in both countries are 70 g CO/km, which is equivalent to a petrol car, and correspond to manufacturing emissions. Next in the ranking are other countries with similar low carbon electricity generation, including Sweden (mostly hydro and nuclear power ), Brazil (mainly hydropower) and France (predominantly nuclear power). Countries ranking in the middle include Japan, Germany, the UK and the United States.\n\nThe following table shows the emissions intensity estimated in the study for each of the 20 countries, and the corresponding emissions equivalent in miles per US gallon of a petrol-powered car:\nIn the case of the United States, the Union of Concerned Scientists (UCS) conducted a study in 2012 to assess average greenhouse gas emissions resulting from charging plug-in car batteries from the perspective of the full life-cycle (well-to-wheel analysis) and according to fuel and technology used to generate electric power by region. The study used the Nissan Leaf all-electric car to establish the analysis baseline, and electric-utility emissions are based on EPA's 2007 estimates. The UCS study expressed the results in terms of miles per gallon instead of the conventional unit of grams of greenhouse gases or carbon dioxide equivalent emissions per year in order to make the results more friendly for consumers. The study found that in areas where electricity is generated from natural gas, nuclear, hydroelectric or renewable sources, the potential of plug-in electric cars to reduce greenhouse emissions is significant. On the other hand, in regions where a high proportion of power is generated from coal, hybrid electric cars produce less CO equivalent emissions than plug-in electric cars, and the best fuel efficient gasoline-powered subcompact car produces slightly less emissions than a PEV. In the worst-case scenario, the study estimated that for a region where all energy is generated from coal, a plug-in electric car would emit greenhouse gas emissions equivalent to a gasoline car rated at a combined city/highway driving fuel economy of . In contrast, in a region that is completely reliant on natural gas, the PEV would be equivalent to a gasoline-powered car rated at.\n\nThe following table shows a representative sample of cities within each of the three categories of emissions intensity used in the UCS study, showing the corresponding miles per gallon equivalent for each city as compared to the greenhouse gas emissions of a gasoline-powered car:\n\nAn analysis of EPA power plant data from 2016 showed improvement in mpg-equivalent ratings of electric cars for nearly all regions, with a national weighted average of 80 mpg for electric vehicles. The regions with the highest ratings include upstate New York, New England, and California at over 100 mpg, while only Oahu, Wisconsin, and part of Illinois and Missouri are below 40 mpg, though still higher than nearly all gasoline cars. \n\nThe long tailpipe has been the target of criticism, ranging from claims that many estimates are methodologically flawed to estimates that state that electricity generation in the United States will become less carbon-intensive over time. Tesla Motors CEO Elon Musk published his own criticism of the long tailpipe.\nThe extraction and refining of carbon based fuels and its distribution is in itself an energy intensive industry contributing to CO emissions. In 2007 U.S. refineries consumed 39353 million kWh, 70769 million lbs of steam and 697593 million cubic feet of Natural Gas. And the refining energy efficiency for gasoline is estimated to be, at best, 87.7%.\n\n"}
{"id": "2243964", "url": "https://en.wikipedia.org/wiki?curid=2243964", "title": "Titanium oxide", "text": "Titanium oxide\n\nTitanium oxide may refer to:\n\nOften used as an active ingredient in sunscreens combined with oxybenzone and octyl methoxycinnamate.\n\nAlso used in making of vehicle exhaust pipes due to its aesthetic value of the purple color.\n"}
{"id": "10427046", "url": "https://en.wikipedia.org/wiki?curid=10427046", "title": "Total pressure", "text": "Total pressure\n\nIn physics, the term total pressure may indicate two different quantities, both having the dimensions of a pressure:\n\n"}
{"id": "3587813", "url": "https://en.wikipedia.org/wiki?curid=3587813", "title": "Tropylium cation", "text": "Tropylium cation\n\nIn organic chemistry, the tropylium ion or cycloheptatrienyl cation is an aromatic species with a formula of [CH]. Its name derives from the molecule tropine (itself named for the molecule atropine). Salts of the tropylium cation can be stable, even with nucleophiles of moderate strength e.g., tropylium tetrafluoroborate and tropylium bromide (\"see below\"). Its bromide and chloride salts can be made from cycloheptatriene (tropylidene) and bromine or phosphorus pentachloride, respectively.\n\nIt is a regular heptagonal, planar, cyclic ion; as well, it has 6 π-electrons (4\"n\" + 2, where \"n\" = 1), which fulfills Hückel's rule of aromaticity. It can coordinate as a ligand to metal atoms.\n\nThe structure shown is a composite of seven resonance contributors in which each carbon atom carries part of the positive charge.\n\nIn 1891 G. Merling obtained a water-soluble bromine containing compound from a reaction of cycloheptatriene and bromine. Unlike most hydrocarbyl bromides, this compound, later named tropylium bromide, is a water-soluble solid and is insoluble in hydrocarbons, chloroform, and ether. It is purified by crystallization from hot ethanol. Reaction with aqeuous silver nitrate immediately gave a precipitate of silver bromide. The structure of tropylium bromide was deduced to be a salt, CHBr, by Doering and Knox in 1954 by analysis of its infrared and ultraviolet spectra. The ionic structures of tropylium perchlorate (CHClO) and tropylium iodide (CHI) in the solid state have been confirmed by X-ray crystallography. The bond length of the carbon-carbon bonds were found to be longer (147 pm) than those of benzene (140 pm) but still shorter than those of a typical single-bonded species like ethane (154 pm). \n\nThe tropylium ion is an acid in aqueous solution (i.e., an Arrhenius acid) as a consequence of its Lewis acidity: it first acts as a Lewis acid to form an adduct with water, which can then donate a proton to another molecule of water: CH + 2HO ⇌ CHOH + HO. (Boric acid gives acidic aqueous solutions in much the same way.) The equilibrium constant is 1.8 × 10, making it about as acidic in water as acetic acid. \n\nThe tropylium ion is frequently encountered in mass spectrometry in the form of a signal at \"m\"/\"z\" = 91 and is used in mass spectrum analysis. This fragment is often found for aromatic compounds containing a benzyl unit. Upon ionization, the benzyl fragment forms a cation (), which rearranges to the highly stable tropylium cation ().\n\nThe tropylium cation reacts with nucleophiles to form substituted cycloheptatrienes, for example:\n\nReduction by lithium aluminum hydride yields cycloheptatriene.\n\nReaction with a cyclopentadienide salt of sodium or lithium yields 7-cyclopentadienylcyclohepta-1,3,5-triene:\n\nWhen treated with oxidising agents such as chromic acid, the tropylium cation undergoes rearrangement into benzaldehyde:\n\nMany metal complexes of tropylium ion are known. One example is [Mo(η-CH)(CO)], which is prepared by hydride abstraction from cycloheptatrienemolybdenum tricarbonyl.\n\n"}
{"id": "34775719", "url": "https://en.wikipedia.org/wiki?curid=34775719", "title": "Velocity triangle", "text": "Velocity triangle\n\nIn turbomachinery, a velocity triangle or a velocity diagram is a triangle representing the various components of velocities of the working fluid in a turbomachine. Velocity triangles may be drawn for both the inlet and outlet sections of any turbomachine. The vector nature of velocity is utilized in the triangles, and the most basic form of a velocity triangle consists of the tangential velocity, the absolute velocity and the relative velocity of the fluid making up three sides of the triangle.\n\nA general velocity triangle consists of the following vectors:\n\nThe following angles are encountered during the analysis:\n"}
{"id": "610530", "url": "https://en.wikipedia.org/wiki?curid=610530", "title": "Windstar Foundation", "text": "Windstar Foundation\n\nThe Windstar Foundation was an environmental education and humanitarian organization founded by John Denver and Thomas Crum in 1976 to conserve of land in Snowmass, Colorado, where it had its headquarters.\n\nWindstar educated all age groups about the environment on a world-wide level. \n\nThe Foundation closed its doors in October 2012, voted to dissolve, and sold its property for $8.5 million in early 2013. All but 30 acres of the land is subject to a conservation easement which prohibits development. Rocky Mountain Institute, which had its headquarters on the unprotected 30 acres and owned a 50% interest in the property, plans to move to Basalt, Colorado. Windstar's $4-million portion of the proceeds was donated to an Aspen charity.\n"}
{"id": "4262506", "url": "https://en.wikipedia.org/wiki?curid=4262506", "title": "Wintergreen (book)", "text": "Wintergreen (book)\n\nWintergreen, written in 1987, is a book by Robert Michael Pyle. It describes the devastation caused by unrestrained logging in Washington's Willapa Hills. It was also the winner of the John Burroughs Medal for Distinguished Nature Writing.\n"}
{"id": "1677810", "url": "https://en.wikipedia.org/wiki?curid=1677810", "title": "Woodchopping", "text": "Woodchopping\n\nWoodchopping (also spelled wood-chopping or wood chopping), called woodchop for short, is a sport that has been around for hundreds of years in several cultures. In woodchopping competitions, skilled contestants attempt to be the first to cut or saw through a log or other block of wood. It is often held at state fairs and agricultural shows. Participants (especially men) are often referred to as axemen.\nThe modern sport of woodchopping is said to have had its genesis in 1870 in Ulverstone, Tasmania, as the result of a £25 ($50) bet between two axemen as to who could first fell a tree. An alternative origin story comes from 16th century Basque Country, in which a man ran a marathon and chop ten logs to be allowed to propose to his future wife.\n\nThe world's first woodchopping championship was held in 1891, at Bell's Parade, Latrobe, Tasmania. This event was celebrated and commemorated with the selection of the site to be the home of the Australian Axemen's Hall of Fame and Timberworks.\n\nWoodchopping is practiced in regions where forestry is or has been an important part of the economy:\n\nMany woodchopping events are handicap events, where the axemen start at different times, depending on how fast they are expected to chop through the log. In New Zealand and parts of Australia, each axeman's individual handicap is recorded in performance books which are graded on how many events they win and how many events they enter. Championship events are scratch events with no handicap, and typically use larger diameter logs (375 mm).\n\nHandicap events may use logs of 250 mm to 350 mm, depending on the skill of the competitors. All competitors have the same size log; the handicap is based purely on time.\n\nThis event is done by an individual cutting a scarf in one side. Once the first side has been completed the individual starts cutting another scarf on the opposite side, slightly higher than the first, generally about two inches higher but can vary with each axeman’s individual preference.\n\nIn this event, the axeman stands on the top of the log and uses a downwards motion to chop the log in two as fast as possible. This is done by cutting a scarf in the front side and then turning around on the block and completing it from the other side. These scarfs are generally offset from each other, the degree of offset depending on the size of the log and the axeman’s preference.\n\nIn this event the axeman cuts a small pocket in the side of a pole and jams a wooden jigger board with a metal shoe on the end of it into the hole. The shoe is designed to grip into the wood when pressure is put on it from the top. After the axeman has climbed onto his first board he then cuts another pocket and so on. Once up on his top board he proceeds to cut the block on the top of the pole.\n\nThere are two distinct versions of tree felling:\n\nThis event is often considered the hardest discipline in woodchopping. The competitor pulls and pushes a razor sharp saw specifically designed for the event. The saws vary in length from five foot six inches to six foot four inches. The saws cost between $1500 and $2000.\n\nThis event consists of two people pulling and pushing a saw to cut a log. It is far faster than the single saw event as there are two people using the saw yet times for this event can be two or three times faster in the same size wood. The saws used in double tend to be a lot hungrier, that is, they cut and draw more wood out with each stroke. This, however, makes it far harder to push and pull the saw.\n\nIn this event the axemen use identically tuned and sharpened chainsaws to cut through a log, once downwards and once upwards, within a 3-inch space of wood. The competitor starts with their hands on top of the log. On a buzzer the axeman picks up the saw and pulls the starting cord and then makes his first cut downward, then his second cut upward. If the saw does not start that is just bad luck and they get a slow time. If the axeman takes over more than the allocated wood then they are disqualified and no time is recorded.\n\nThis event is often the crowd's favourite , and certainly the loudest. It uses a large homemade methanol-run chainsaw. The saws used by top competitors are typically snowmobile engines cut in half and are far heavier than regular chainsaws. The start for this event is exactly the same as the stock saw except the log is bigger and the axeman has to do three cuts: the first in a downwards motion, the second upwards, and the third down. This event is the fastest by far, lasting between five and seven seconds.\n\n\nMany different types of wood are used in the sport and they vary between countries. Common woods used in competition in Australia are gum, mountain ash, woolley butt and poplar. The most common woods cut in New Zealand are radiata pine (\"Pinus radiata\"), poplar and \"Pinus strobus\". Woods cut in America include white pine, alder, aspen frozen wood and cotton wood.\n\nThe rules of the sport vary from country to country.\n\nWoodchopping events in Australia are generally run in conjunction with agricultural shows. Competitions can run for up to 10 days, with over 100 competitors at each show.\n\nIn the Jack Pollard's 1968 or 1969 editions of the \"Ampol's Australian Sporting Records\" woodchopping records appear to run from the 1920s \n\nThe Axeman's Hall of Fame is located in Latrobe, Tasmania. The peak body for the sport in Australia is the Australian Axemen's Association.\n\nThe sport is called \"aizkolaritza\" in Basque from \"aizkolari\" \"wood-chopper\". The sport is very popular and competitions are common at most festivals.\n\nNew Zealand is a leading country in the sport of woodchopping, having had the world's top two competitors; Jason Wynyard, and David Bolstad who died in November 2011. Competitions are generally held at A & P shows, but there are also shows dedicated to wood chopping.\n\n\n\n\n\n"}
