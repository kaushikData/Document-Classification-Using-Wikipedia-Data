{"id": "21344716", "url": "https://en.wikipedia.org/wiki?curid=21344716", "title": "African Journal of Ecology", "text": "African Journal of Ecology\n\nThe African Journal of Ecology (formerly \"East African Wildlife Journal\") is a quarterly scientific journal focused on the ecology and conservation of the animals and plants of Africa. It is published by Blackwell Publishing in association with the East African Wildlife Society.\n\n"}
{"id": "13973930", "url": "https://en.wikipedia.org/wiki?curid=13973930", "title": "Akershus Energi", "text": "Akershus Energi\n\nAkershus Energi is a Norwegian power company that produces hydroelectricity.\n\nThe company is wholly owned by Akershus County Municipality. In 2016, Akershus Energi bought half of the Nittedal property after Aller Trykk. In June 2017, the Swedish company Infranode bought one third of Akershus Energi Varme AS.\n\nAnnual production is 2.3 TWh. There are five plants in Glomma, three in Haldensvassdraget, and two in Skiensvassdraget.\n\nThe company was founded in 1922. Its first managing director was Augustin Paus.\n"}
{"id": "2408", "url": "https://en.wikipedia.org/wiki?curid=2408", "title": "Analytical chemistry", "text": "Analytical chemistry\n\nAnalytical chemistry studies and uses instruments and methods used to separate, identify, and quantify matter. In practice, separation, identification or quantification may constitute the entire analysis or be combined with another method. Separation isolates analytes. Qualitative analysis identifies analytes, while quantitative analysis determines the numerical amount or concentration.\n\nAnalytical chemistry consists of classical, wet chemical methods and modern, instrumental methods. Classical qualitative methods use separations such as precipitation, extraction, and distillation. Identification may be based on differences in color, odor, melting point, boiling point, radioactivity or reactivity. Classical quantitative analysis uses mass or volume changes to quantify amount. Instrumental methods may be used to separate samples using chromatography, electrophoresis or field flow fractionation. Then qualitative and quantitative analysis can be performed, often with the same instrument and may use light interaction, heat interaction, electric fields or magnetic fields. Often the same instrument can separate, identify and quantify an analyte.\n\nAnalytical chemistry is also focused on improvements in experimental design, chemometrics, and the creation of new measurement tools. Analytical chemistry has broad applications to forensics, medicine, science and engineering.\n\nAnalytical chemistry has been important since the early days of chemistry, providing methods for determining which elements and chemicals are present in the object in question. During this period significant contributions to analytical chemistry include the development of systematic elemental analysis by Justus von Liebig and systematized organic analysis based on the specific reactions of functional groups.\n\nThe first instrumental analysis was flame emissive spectrometry developed by Robert Bunsen and Gustav Kirchhoff who discovered rubidium (Rb) and caesium (Cs) in 1860.\n\nMost of the major developments in analytical chemistry take place after 1900. During this period instrumental analysis becomes progressively dominant in the field. In particular many of the basic spectroscopic and spectrometric techniques were discovered in the early 20th century and refined in the late 20th century.\n\nThe separation sciences follow a similar time line of development and also become increasingly transformed into high performance instruments. In the 1970s many of these techniques began to be used together as hybrid techniques to achieve a complete characterization of samples.\n\nStarting in approximately the 1970s into the present day analytical chemistry has progressively become more inclusive of biological questions (bioanalytical chemistry), whereas it had previously been largely focused on inorganic or small organic molecules. Lasers have been increasingly used in chemistry as probes and even to initiate and influence a wide variety of reactions. The late 20th century also saw an expansion of the application of analytical chemistry from somewhat academic chemical questions to forensic, environmental, industrial and medical questions, such as in histology.\n\nModern analytical chemistry is dominated by instrumental analysis. Many analytical chemists focus on a single type of instrument. Academics tend to either focus on new applications and discoveries or on new methods of analysis. The discovery of a chemical present in blood that increases the risk of cancer would be a discovery that an analytical chemist might be involved in. An effort to develop a new method might involve the use of a tunable laser to increase the specificity and sensitivity of a spectrometric method. Many methods, once developed, are kept purposely static so that data can be compared over long periods of time. This is particularly true in industrial quality assurance (QA), forensic and environmental applications. Analytical chemistry plays an increasingly important role in the pharmaceutical industry where, aside from QA, it is used in discovery of new drug candidates and in clinical applications where understanding the interactions between the drug and the patient are critical.\n\nAlthough modern analytical chemistry is dominated by sophisticated instrumentation, the roots of analytical chemistry and some of the principles used in modern instruments are from traditional techniques, many of which are still used today. These techniques also tend to form the backbone of most undergraduate analytical chemistry educational labs.\n\nA qualitative analysis determines the presence or absence of a particular compound, but not the mass or concentration. By definition, qualitative analyses do not measure quantity.\n\nThere are numerous qualitative chemical tests, for example, the acid test for gold and the Kastle-Meyer test for the presence of blood.\n\nInorganic qualitative analysis generally refers to a systematic scheme to confirm the presence of certain, usually aqueous, ions or elements by performing a series of reactions that eliminate ranges of possibilities and then confirms suspected ions with a confirming test. Sometimes small carbon containing ions are included in such schemes. With modern instrumentation these tests are rarely used but can be useful for educational purposes and in field work or other situations where access to state-of-the-art instruments are not available or expedient.\n\nQuantitative analysis is the measurement of the quantities of particular chemical constituents present in a substance.\n\nGravimetric analysis involves determining the amount of material present by weighing the sample before and/or after some transformation. A common example used in undergraduate education is the determination of the amount of water in a hydrate by heating the sample to remove the water such that the difference in weight is due to the loss of water.\n\nTitration involves the addition of a reactant to a solution being analyzed until some equivalence point is reached. Often the amount of material in the solution being analyzed may be determined. Most familiar to those who have taken chemistry during secondary education is the acid-base titration involving a color changing indicator. There are many other types of titrations, for example potentiometric titrations.\nThese titrations may use different types of indicators to reach some equivalence point.\n\nSpectroscopy measures the interaction of the molecules with electromagnetic radiation. Spectroscopy consists of many different applications such as atomic absorption spectroscopy, atomic emission spectroscopy, ultraviolet-visible spectroscopy, x-ray fluorescence spectroscopy, infrared spectroscopy, Raman spectroscopy, dual polarization interferometry, nuclear magnetic resonance spectroscopy, photoemission spectroscopy, MÃ¶ssbauer spectroscopy and so on.\n\nMass spectrometry measures mass-to-charge ratio of molecules using electric and magnetic fields. There are several ionization methods: electron impact, chemical ionization, electrospray, fast atom bombardment, matrix assisted laser desorption ionization, and others. Also, mass spectrometry is categorized by approaches of mass analyzers: magnetic-sector, quadrupole mass analyzer, quadrupole ion trap, time-of-flight, Fourier transform ion cyclotron resonance, and so on.\n\nElectroanalytical methods measure the potential (volts) and/or current (amps) in an electrochemical cell containing the analyte. These methods can be categorized according to which aspects of the cell are controlled and which are measured. The four main categories are potentiometry (the difference in electrode potentials is measured), coulometry (the transferred charge is measured over time), amperometry (the cell's current is measured over time), and voltammetry (the cell's current is measured while actively altering the cell's potential).\n\nCalorimetry and thermogravimetric analysis measure the interaction of a material and heat.\n\nSeparation processes are used to decrease the complexity of material mixtures. Chromatography, electrophoresis and Field Flow Fractionation are representative of this field.\n\nCombinations of the above techniques produce a \"hybrid\" or \"hyphenated\" technique. Several examples are in popular use today and new hybrid techniques are under development. For example, gas chromatography-mass spectrometry, gas chromatography-infrared spectroscopy, liquid chromatography-mass spectrometry, liquid chromatography-NMR spectroscopy. liquid chromagraphy-infrared spectroscopy and capillary electrophoresis-mass spectrometry.\n\nHyphenated separation techniques refers to a combination of two (or more) techniques to detect and separate chemicals from solutions. Most often the other technique is some form of chromatography. Hyphenated techniques are widely used in chemistry and biochemistry. A slash is sometimes used instead of hyphen, especially if the name of one of the methods contains a hyphen itself.\n\nThe visualization of single molecules, single cells, biological tissues and nanomaterials is an important and attractive approach in analytical science. Also, hybridization with other traditional analytical tools is revolutionizing analytical science. Microscopy can be categorized into three different fields: optical microscopy, electron microscopy, and scanning probe microscopy. Recently, this field is rapidly progressing because of the rapid development of the computer and camera industries.\n\nDevices that integrate (multiple) laboratory functions on a single chip of only millimeters to a few square centimeters in size and that are capable of handling extremely small fluid volumes down to less than picoliters.\n\nError can be defined as numerical difference between observed value and true value.\n\nIn error the true value and observed value in chemical analysis can be related with each other by the equation\nwhere \nError of a measurement is an inverse measure of accurate measurement i.e. smaller the error greater the accuracy of the measurement. \n\nErrors can be expressed relatively. Given the relative error(formula_5):\n\nThe percent error can also be calculated:\n\nIf we want to use these values in a function, we may also want to calculate the error of the function. Let formula_8 be a function with formula_9 variables. Therefore, the propagation of uncertainty must be calculated in order to know the error in formula_8:\n\nA general method for analysis of concentration involves the creation of a calibration curve. This allows for determination of the amount of a chemical in a material by comparing the results of unknown sample to those of a series of known standards. If the concentration of element or compound in a sample is too high for the detection range of the technique, it can simply be diluted in a pure solvent. If the amount in the sample is below an instrument's range of measurement, the method of addition can be used. In this method a known quantity of the element or compound under study is added, and the difference between the concentration added, and the concentration observed is the amount actually in the sample.\n\nSometimes an internal standard is added at a known concentration directly to an analytical sample to aid in quantitation. The amount of analyte present is then determined relative to the internal standard as a calibrant. An ideal internal standard is isotopically-enriched analyte which gives rise to the method of isotope dilution.\n\nThe method of standard addition is used in instrumental analysis to determine concentration of a substance (analyte) in an unknown sample by comparison to a set of samples of known concentration, similar to using a calibration curve. Standard addition can be applied to most analytical techniques and is used instead of a calibration curve to solve the matrix effect problem.\n\nOne of the most important components of analytical chemistry is maximizing the desired signal while minimizing the associated noise. The analytical figure of merit is known as the signal-to-noise ratio (S/N or SNR).\n\nNoise can arise from environmental factors as well as from fundamental physical processes.\n\nThermal noise results from the motion of charge carriers (usually electrons) in an electrical circuit generated by their thermal motion. Thermal noise is white noise meaning that the power spectral density is constant throughout the frequency spectrum.\n\nThe root mean square value of the thermal noise in a resistor is given by\n\nwhere \"k\" is Boltzmann's constant, \"T\" is the temperature, \"R\" is the resistance, and formula_13 is the bandwidth of the frequency formula_14.\n\nShot noise is a type of electronic noise that occurs when the finite number of particles (such as electrons in an electronic circuit or photons in an optical device) is small enough to give rise to statistical fluctuations in a signal.\n\nShot noise is a Poisson process and the charge carriers that make up the current follow a Poisson distribution. The root mean square current fluctuation is given by\n\nwhere \"e\" is the elementary charge and \"I\" is the average current. Shot noise is white noise.\n\nFlicker noise is electronic noise with a 1/\"Æ\" frequency spectrum; as \"f\" increases, the noise decreases. Flicker noise arises from a variety of sources, such as impurities in a conductive channel, generation and recombination noise in a transistor due to base current, and so on. This noise can be avoided by modulation of the signal at a higher frequency, for example through the use of a lock-in amplifier.\n\nEnvironmental noise arises from the surroundings of the analytical instrument. Sources of electromagnetic noise are power lines, radio and television stations, wireless devices, Compact fluorescent lamps and electric motors. Many of these noise sources are narrow bandwidth and therefore can be avoided. Temperature and vibration isolation may be required for some instruments.\n\nNoise reduction can be accomplished either in computer hardware or software. Examples of hardware noise reduction are the use of shielded cable, analog filtering, and signal modulation. Examples of software noise reduction are digital filtering, ensemble average, boxcar average, and correlation methods.\n\nAnalytical chemistry has applications including in forensic science, bioanalysis, clinical analysis, environmental analysis, and materials analysis. Analytical chemistry research is largely driven by performance (sensitivity, detection limit, selectivity, robustness, dynamic range, linear range, accuracy, precision, and speed), and cost (purchase, operation, training, time, and space). Among the main branches of contemporary analytical atomic spectrometry, the most widespread and universal are optical and mass spectrometry. In the direct elemental analysis of solid samples, the new leaders are laser-induced breakdown and laser ablation mass spectrometry, and the related techniques with transfer of the laser ablation products into inductively coupled plasma. Advances in design of diode lasers and optical parametric oscillators promote developments in fluorescence and ionization spectrometry and also in absorption techniques where uses of optical cavities for increased effective absorption pathlength are expected to expand. The use of plasma- and laser-based methods is increasing. An interest towards absolute (standardless) analysis has revived, particularly in emission spectrometry.\n\nGreat effort is being put in shrinking the analysis techniques to chip size. Although there are few examples of such systems competitive with traditional analysis techniques, potential advantages include size/portability, speed, and cost. (micro total analysis system (ÂµTAS) or lab-on-a-chip). Microscale chemistry reduces the amounts of chemicals used.\n\nMany developments improve the analysis of biological systems. Examples of rapidly expanding fields in this area are genomics, DNA sequencing and related research in genetic fingerprinting and DNA microarray; proteomics, the analysis of protein concentrations and modifications, especially in response to various stressors, at various developmental stages, or in various parts of the body, metabolomics, which deals with metabolites; transcriptomics, including mRNA and associated fields; lipidomics - lipids and its associated fields; peptidomics - peptides and its associated fields; and metalomics, dealing with metal concentrations and especially with their binding to proteins and other molecules.\n\nAnalytical chemistry has played critical roles in the understanding of basic science to a variety of practical applications, such as biomedical applications, environmental monitoring, quality control of industrial manufacturing, forensic science and so on.\n\nThe recent developments of computer automation and information technologies have extended analytical chemistry into a number of new biological fields. For example, automated DNA sequencing machines were the basis to complete human genome projects leading to the birth of genomics. Protein identification and peptide sequencing by mass spectrometry opened a new field of proteomics.\n\nAnalytical chemistry has been an indispensable area in the development of nanotechnology. Surface characterization instruments, electron microscopes and scanning probe microscopes enables scientists to visualize atomic structures with chemical characterizations.\n\n\n"}
{"id": "7543420", "url": "https://en.wikipedia.org/wiki?curid=7543420", "title": "Bradvek", "text": "Bradvek\n\nBradvek was a form of Tyvek polymer, produced by Du Pont. It was used for printing one of the first polymer banknotes in 1983 for the Isle of Man by the American Banknote Company.\n\n"}
{"id": "42734038", "url": "https://en.wikipedia.org/wiki?curid=42734038", "title": "COMPASS tokamak", "text": "COMPASS tokamak\n\nTokamak COMPASS (COMPact ASSembly) is the main experimental facility of Tokamak department of Institute of Plasma Physics of the Academy of Sciences of the Czech Republic since 2006. It was designed in the 1980s in the British Culham Science Centre as a flexible research facility dedicated mostly to plasma physics studies in circular and D shaped plasmas.\n\nThe first plasma in COMPASS \"broke down\" in 1989 in a C-shaped vacuum vessel, i.e., in a simpler vessel with a circular cross-section. Pioneering experiments followed, including for example the ITER-relevant tests of magnetic field correction with saddle coils for RMPs experiment (Resonant magnetic perturbations) or experiments with non-inductive current drive in plasma.\n\nThe operation of tokamak restarted with a D-shaped vacuum vessel in 1992. The operation mode with high plasma confinement (H-mode) was achieved, which represents a reference operation (\"standard scenario\") for the ITER tokamak. The COMPASS tokamak with its size (major radius 0.6 m and height of the vessel approx. 0.7 m) ranks to smaller tokamaks capable of the H-mode operation. Importantly, due to its size and shape the COMPASS plasmas correspond to one tenth (in the linear scale) of the ITER plasmas. At present, besides COMPASS there are only two operational tokamaks in Europe with ITER-like configuration capable of regime with the high plasma confinement. It is the Joint European Torus (JET) and the German tokamak ASDEX Upgrade (Institut fÃ¼r Plasmaphysik, Garching, Germany). JET is the biggest experimental device of this type in the world.\n\nIn 2002, British scientists started alternative research on larger, spherical tokamak MAST. Operation of COMPASS was discontinued due to insufficient resources for operation of both tokamaks, however, the research program foreseen for the latter tokamak was not concluded. Due to its important and not completely realised opportunities - and, in particular, due to its direct relevance to the ITER project - the facility was offered for free by the European Commission and UKAEA to the Institute of Plasma Physics AS CR in Prague in autumn 2004. The Prague institute has been coordinating research in thermonuclear fusion in the Czech Republic in the framework of EURATOM since 1999. Team of physicists from the institute has a long-time experience in this field of research including operation of a small tokamak CASTOR. The European Commission has declared that the institute is fully competent to operate the tokamak COMPASS.\n\nMovie: COMPASS discharge using fast - visible camera: \n\n\n"}
{"id": "1453778", "url": "https://en.wikipedia.org/wiki?curid=1453778", "title": "Cenosphere", "text": "Cenosphere\n\nA cenosphere is a lightweight, inert, hollow sphere made largely of silica and alumina and filled with air or inert gas, typically produced as a byproduct of coal combustion at thermal power plants. The color of cenospheres varies from gray to almost white and their density is about , which gives them a great buoyancy. \"Cf.\" glass microspheres.\n\nCenospheres are hard and rigid, light, waterproof, innoxious, and insulative. This makes them highly useful in a variety of products, notably fillers. Cenospheres are now used as fillers in cement to produce low-density concrete. Recently, some manufacturers have begun filling metals and polymers with cenospheres to make lightweight composite materials with higher strength than other types of foam materials. Such composite materials are called syntactic foam. Aluminum-based syntactic foams are finding applications in the automotive sector.\n\nSilver-coated cenospheres are used in conductive coatings, tiles and fabrics. Another use is in conductive paints for antistatic coatings and electromagnetic shielding.\n\nThe word \"cenosphere\" is derived from two Greek words, \"ÎºÎµÎ½ÏÏ\" (\"kenos\": hollow, empty) and \"ÏÏÎ±Î¯ÏÎ±\" (\"sphaera\": sphere), literally meaning \"hollow sphere.\"\n\nThe process of burning coal in thermal power plants produces fly ash containing ceramic particles made largely of alumina and silica. They are produced at temperatures of through complicated chemical and physical transformation. Their chemical composition and structure varies considerably depending on the composition of coal that generated them.\n\nThe ceramic particles in fly ash have three types of structures. The first type of particles are solid and are called precipitator. The second type of particles are hollow and are called cenospheres. The third type of particles are called plerospheres, which are hollow particles of large diameter filled with smaller size precipitator and cenospheres. \n\nThe definition of cenosphere has changed over the last 30 years. Up until the 1990s it was limited to a largely carbonaceous sphere caused by the oxygen-deficient combustion of a liquid fuel droplet that was cooled below before it was consumed. These fuel cenospheres indicated a combustion source using injected droplets of fuel or the open burning of heavy liquid fuels such as asphalt or a thermoplastic material that were bubbling as they burned; the bursting of the bubbles created airborne droplets of fuel. This is still a common definition used in environmental microscopy to differentiate between the inefficient combustion of liquid fuels and the high temperature fly ash resulting from the efficient combustion of fuels with inorganic contaminants. Fuel cenospheres are always black.\n\nThe refractory cenosphere as defined above is synonymous with microballoons or glass microspheres and excludes the traditional fuel cenospheres definition. The use of the term \"cenosphere\" in place of \"microballoons\" is widespread, and it has become an additional definition.\n\n\n"}
{"id": "4390136", "url": "https://en.wikipedia.org/wiki?curid=4390136", "title": "Chemical classification", "text": "Chemical classification\n\nChemical classification systems attempt to classify elements or compounds according to certain chemical functional or structural properties. Whereas the structural properties are largely intrinsic, functional properties and the derived classifications depend to a certain degree on the type of chemical interaction partners on which the function is exerted. Sometimes other criteria like purely physical ones (e.g. molecular weight) or - on the other hand - functional properties above the chemical level are also used for building chemical taxonomies.\n\nSome systems mix the various levels, resulting in hierarchies where the domains are slightly confused, for example having structural and functional aspects end up on the same level. Whereas chemical function is closely dependent on chemical structure, the situation becomes more involved when e.g. pharmacological function is integrated, because the QSAR can usually not be directly computed from structural qualities.\n\n\n\nMostly appropriate only for large biological molecules (as at least one interacting partner), in particular enzymes, depends on chemical functions of their constituent amino acids.\n\n\n\"See also:\" biological activity\n\n\n"}
{"id": "4297956", "url": "https://en.wikipedia.org/wiki?curid=4297956", "title": "Clockwork universe", "text": "Clockwork universe\n\nIn the history of science, the clockwork universe compares the universe to a mechanical clock. It continues ticking along, as a perfect machine, with its gears governed by the laws of physics, making every aspect of the machine predictable.\n\nThis idea was very popular among deists during the Enlightenment, when Isaac Newton derived his laws of motion, and showed that alongside the law of universal gravitation, they could explain the behaviour of both terrestrial objects and the solar system.\n\nA similar concept goes back, to John of Sacrobosco's early 13th-century introduction to astronomy: \"On the Sphere of the World\". In this widely popular medieval text, Sacrobosco spoke of the universe as the \"machina mundi\", the machine of the world, suggesting that the reported eclipse of the Sun at the crucifixion of Jesus was a disturbance of the order of that machine.\n\nResponding to Gottfried Leibniz, a prominent supporter of the theory, in the LeibnizâClarke correspondence, Samuel Clarke wrote:\n\nIn 2009 artist Tim Wetherell created a large wall piece for Questacon (The National Science and Technology centre in Canberra, Australia) representing the concept of the clockwork universe. This steel artwork contains moving gears, a working clock, and a movie of the lunar terminator.\n\n\n\n"}
{"id": "1539785", "url": "https://en.wikipedia.org/wiki?curid=1539785", "title": "Dark matter halo", "text": "Dark matter halo\n\nA dark matter halo is a theoretical component of a galaxy that envelops the galactic disc and extends well beyond the edge of the visible galaxy. The halo's mass dominates the total mass. Thought to consist of dark matter, halos have not been observed directly. Their existence is inferred through their effects on the motions of stars and gas in galaxies. Dark matter halos play a key role in current models of galaxy formation and evolution. The dark matter halo is not fully explained by the presence of massive compact halo objects (MACHOs).\n\nThe presence of dark matter (DM) in the halo is inferred from its gravitational effect on a spiral galaxy's rotation curve. Without large amounts of mass throughout the (roughly spherical) halo, the rotational velocity of the galaxy would decrease at large distances from the galactic center, just as the orbital speeds of the outer planets decrease with distance from the Sun. However, observations of spiral galaxies, particularly radio observations of line emission from neutral atomic hydrogen (known, in astronomical parlance, as HI), show that the rotation curve of most spiral galaxies flattens out, meaning that rotational velocities do not decrease with distance from the galactic center. The absence of any visible matter to account for these observations implies either that unobserved (\"dark\") matter, first proposed by Ken Freeman in 1970, exist, or that the theory of motion under gravity (General Relativity) is incomplete. Freeman noticed that the expected decline in velocity was not present in NGC 300 nor M33, and considered an undetected mass to explain it. The DM Hypothesis has been reinforced by several studies.\n\nThe formation of dark matter halos is believed to have played a major role in the early formation of galaxies. During initial galactic formation, the temperature of the baryonic matter should have still been much too high for it to form gravitationally self-bound objects, thus requiring the prior formation of dark matter structure to add additional gravitational interactions. The current hypothesis for this is based on cold dark matter (CDM) and its formation into structure early in the universe.\n\nThe hypothesis for CDM structure formation begins with density perturbations in the Universe that grow linearly until they reach a critical density, after which they would stop expanding and collapse to form gravitationally bound dark matter halos. These halos would continue to grow in mass (and size), either through accretion of material from their immediate neighborhood, or by merging with other halos. Numerical simulations of CDM structure formation have been found to proceed as follows: A small volume with small perturbations initially expands with the expansion of the Universe. As time proceeds, small-scale perturbations grow and collapse to form small halos. At a later stage, these small halos merge to form a single virialized dark matter halo with an ellipsoidal shape, which reveals some substructure in the form of dark matter sub-halos.\n\nThe use of CDM overcomes issues associated with the normal baryonic matter because it removes most of the thermal and radiative pressures that were preventing the collapse of the baryonic matter. The fact that the dark matter is cold compared to the baryonic matter allows the DM to form these initial, gravitationally bound clumps. Once these subhalos formed, their gravitational interaction with baryonic matter is enough to overcome the thermal energy, and allow it to collapse into the first stars and galaxies. Simulations of this early galaxy formation matches the structure observed by galactic surveys as well as observation of the Cosmic Microwave Background.\n\nA commonly used model for galactic dark matter halos is the pseudo-isothermal halo:\n\nformula_1\n\nwhere formula_2 denotes the finite central density and formula_3 the core radius. This provides a good fit to most rotation curve data. However, it cannot be a complete description, as the enclosed mass fails to converge to a finite value as the radius tends to infinity. The isothermal model is, at best, an approximation. Many effects may cause deviations from the profile predicted by this simple model. For example, (i) collapse may never reach an equilibrium state in the outer region of a dark matter halo, (ii) non-radial motion may be important, and (iii) mergers associated with the (hierarchical) formation of a halo may render the spherical-collapse model invalid.\n\nNumerical simulations of structure formation in an expanding universe lead to the theoretical prediction of the NFW (Navarro-Frenk-White) profile:\n\nformula_4\n\nwhere formula_5 is a scale radius, formula_6 is a characteristic (dimensionless) density, and formula_7 = formula_8 is the critical density for closure. The NFW profile is called 'universal' because it works for a large variety of halo masses, spanning four orders of magnitude, from individual galaxies to the halos of galaxy clusters. This profile has a finite gravitational potential even though the integrated mass still diverges logarithmically. It has become conventional to refer to the mass of a halo at a fiducial point that encloses an overdensity 200 times greater than the critical density of the universe, though mathematically the profile extends beyond this notational point. It was later deduced that the density profile depends on the environment, with the NFW appropriate only for isolated halos. NFW halos generally provide a worse description of galaxy data than does the pseudo-isothermal profile, leading to the cuspy halo problem.\n\nHigher resolution computer simulations are better described by the Einasto profile:\n\nformula_9\n\nwhere r is the spatial (i.e., not projected) radius. The term formula_10 is a function of n such that formula_11 is the density at the radius formula_12 that defines a volume containing half of the total mass. While the addition of a third parameter provides a slightly improved description of the results from numerical simulations, it is not observationally distinguishable from the 2 parameter NFW halo, and does nothing to alleviate the cuspy halo problem.\n\nThe collapse of overdensities in the cosmic density field is generally aspherical. So, there is no reason to expect the resulting halos to be spherical. Even the earliest simulations of structure formation in a CDM universe emphasized that the halos are substantially flattened. Subsequent work has shown that halo equidensity surfaces can be described by ellipsoids characterized by the lengths of their axes.\n\nBecause of uncertainties in both the data and the model predictions, it is still unclear whether the halo shapes inferred from observations are consistent with the predictions of ÎCDM cosmology.\n\nUp until the end of the 1990s, numerical simulations of halo formation revealed little substructure. With increasing computing power and better algorithms, it became possible to use greater numbers of particles and obtain better resolution. Substantial amounts of substructure are now expected. When a small halo merges with a significantly larger halo it becomes a subhalo orbiting within the potential well of its host. As it orbits, it is subjected to strong tidal forces from the host, which cause it to lose mass. In addition the orbit itself evolves as the subhalo is subjected to dynamical friction which causes it to lose energy and angular momentum to the dark matter particles of its host. Whether a subhalo survives as a self-bound entity depends on its mass, density profile, and its orbit.\n\nAs originally pointed out by Hoyle and first demonstrated using numerical simulations by Efstathiou & Jones, asymmetric collapse in an expanding universe produces objects with significant angular momentum.\n\nNumerical simulations have shown that the spin parameter distribution for halos formed by dissipation-less hierarchical clustering is well fit by a log-normal distribution, the median and width of which depend only weakly on halo mass, redshift, and cosmology:\n\nformula_13\n\nwith formula_14 and formula_15. At all halo masses, there is a marked tendency for halos with higher spin to be in denser regions and thus to be more strongly clustered.\n\nThe visible disk of the Milky Way Galaxy is embedded in a much larger, roughly spherical halo of dark matter. The dark matter density drops off with distance from the galactic center. It is now believed that about 95% of the Galaxy is composed of dark matter, a type of matter that does not seem to interact with the rest of the Galaxy's matter and energy in any way except through gravity. The luminous matter makes up approximately 9 x 10 solar masses. The dark matter halo is likely to include around 6 x 10 to 3 x 10 solar masses of dark matter.\n\n\n\n"}
{"id": "36359685", "url": "https://en.wikipedia.org/wiki?curid=36359685", "title": "Desay Madu Jhya", "text": "Desay Madu Jhya\n\nDesay Madu JhyÄ (Devanagari: à¤¦à¥à¤¸à¤¯ à¤®à¤¦à¥ à¤à¥à¤¯à¤¾:) is a traditional wooden window in Kathmandu which is celebrated for its uniqueness. The name means \"window without equal in the country\" in Nepal Bhasa. The window is set into the facade of a residential house in central Kathmandu.\n\nDesay Madu JhyÄ is a specimen of the woodcarving heritage of the Newar people of Nepal which goes back more than a thousand years. Newar architecture is characterised by artistic windows and doors set into bare brick walls. The intricate carvings mostly depict religious motifs, ritual objects, mythical beasts and birds. The level of design and carving of the Newar window reached its peak in the mid-18th century. They are found on palaces, private residences and sacred houses across Nepal Mandala.\n\nDesay Madu JhyÄ is famed for being the only one of its kind. While most traditional windows are bay windows carved with elaborate details, Desay Madu Jhya is a latticed window with multiple frames. Its design looks like the bellows in an old folding camera.\n\nThe unique window is set into a house at YatkhÄ, a street to the north of Kathmandu Durbar Square, the old royal palace complex. The street forms part of the ceremonial circuit in the historic section of Kathmandu through which chariot processions and festival parades pass. The window is a tourist attraction and is part of the itinerary on sightseeing tours of the city.\n\nDesay Madu JhyÄ is more than a century old and has been declared a national treasure. In 1978, the Postal Service Department of the Nepal government issued a postage stamp depicting the window.\n"}
{"id": "25836919", "url": "https://en.wikipedia.org/wiki?curid=25836919", "title": "Edinburgh to Glasgow Improvement Programme", "text": "Edinburgh to Glasgow Improvement Programme\n\nThe Edinburgh Glasgow Improvement Programme or EGIP is an initiative funded by Transport Scotland on behalf of the Scottish Government to increase capacity on the main railway line between Edinburgh and Glasgow, with new, longer electric trains running by 2017 and full completion in 2019. It is expected to cost Â£742 million and will be delivered by Network Rail.\n\nThe programme was initially announced by the Labour-Liberal Democrat coalition government in 2006, and was continued, although cut back from the original scheme, by the subsequent Scottish National Party governments.\n\nThe project will deliver the infrastructure to enable 8 car, electric trains to operate (an increase from the previous 6 car maximum) on Edinburgh - Glasgow services with a fastest journey time of 42 minutes.\n\nThe additional train carriages will allow a total increase in capacity of 30%.\n\nIt will also deliver electrification of 94 miles of track including diversionary routes through Cumbernauld and Falkirk.\n\n\nAs originally announced in 2006, the project would have cost Â£1 billion and aimed to increase capacity by increasing service frequency to 6tph rather than by lengthening trains.\n\nTransport Scotland commissioned Jacobs to examine the project for potential savings and they identified that the a similar capacity increase could be achieved by increasing train length while maintaining a 4tph frequency. This had previously been thought to be impossible due to restrictions at Glasgow Queen Street station. The redevelopment of Buchanan Galleries allowed this option to be developed instead with a major rebuild and the lengthening of platforms at Queen St high level station.\n\nThis enabled several major schemes to be dropped from the scope of EGIP as they were only required if an increase in train frequency took place:\n\nAnother proposed scheme the Garngad Chord, near had already been dropped from the scheme. It was intended to allow Glasgow-Cumbernauld services to use the North Clyde Line into Queen Street Low Level, thus freeing up capacity on the High Level station. Instead services started on this route in 2014 with a reversal in Springburn, creating a slightly longer journey time.\n\nElectrification to Stirling, and was originally reported to have been dropped as part of the changes to EGIP. Electrification is continuing and is expected to be complete by 2018; it has been carried out under the rolling programme of electrification, rather than as part of the EGIP programme.\n\nThe new proposals were criticised by business and environmental groups. Sustainable transport campaign group Transform Scotland described the cuts as 'a major step backwards' and suggested that the government's concurrent decision to bring forward a Â£3 billion project to dual the A9 road demonstrated a 'perverse set of priorities'.\n\nThe fastest journey times between Glasgow and Edinburgh under the scheme will now be 42 minutes compared to the just over half an hour previously promised.\n\nOne of the most disruptive elements of the project was the complete closure of Glasgow Queen Street (High Level) for a 20-week period between March and August 2016, to allow replacement of the track in the Cowlairs Tunnel and the lowering of the track bed to accommodate the installation of overhead electrification equipment in the tunnel. ScotRail warned that the diversion works created a potential single point of failure which could paralyse Glasgow's rail network in the event of a train failure or other incident.\n"}
{"id": "31357965", "url": "https://en.wikipedia.org/wiki?curid=31357965", "title": "Electricity Authority (New Zealand)", "text": "Electricity Authority (New Zealand)\n\nThe New Zealand Electricity Authority () is an independent Crown entity responsible for the regulation of the New Zealand electricity market. The Authority was established in November 2010, following a government review of the electricity industry, and replaced the Electricity Commission. The Authority has a narrower focus on industry competition, reliability and efficiency than the Electricity Commission had.\n\nThe key functions performed by the Authority are:\n\nFunctions that were performed by the Commission, but which are undertaken by agencies other than the Authority include:\n\n\n"}
{"id": "2103226", "url": "https://en.wikipedia.org/wiki?curid=2103226", "title": "Energy and Environmental Research Center", "text": "Energy and Environmental Research Center\n\nThe Energy and Environmental Research Center (EERC) in Grand Forks, North Dakota, United States is a research, development, demonstration, and commercialization facility for energy and environment technologies development. The center is a nonprofit division of the University of North Dakota.\n\nThe center was founded in 1951 as the Robertson Lignite Research Laboratory, a federal facility under the United States Bureau of Mines. It became a federal energy technology center under the United States Department of Energy in 1977 and was defederalized in 1983. The center employs approximately 210 employees.\n\nThe EERC conducts research, development, demonstration, and commercialization activities involving zero-emissions coal conversion; CO capture and sequestration; energy and water sustainability; hydrogen and fuel cells; advanced air emission control technologies, emphasizing SO, NO, air toxics, fine particulate, CO, and mercury control; renewable energy; wind energy; water management; flood prevention; global climate change; waste utilization; energy efficiency; and contaminant cleanup.\n\nThe EERC is located on more than of land on the southeast corner of the UND campus in Grand Forks, North Dakota, and houses of laboratories, fabrication facilities, technology demonstration facilities, and offices. The EERC has a current contract portfolio of over $208.4 million and the EERC's estimated regional economic impact is $78.1 million. Since 1987, the EERC has had more than 1,315 clients in 50 states and 52 countries.\n\nThe EERC's eleven centers of excellence include the following:\n\nThe National Center for Hydrogen Technology does research in hydrogen and fuel cell technology. In 2006, hydrogen-related contracts at the NCHT totaled more than $20 million. Groundbreaking on the NCHT building on the EERC campus took place on April 17, 2006.\n"}
{"id": "25828507", "url": "https://en.wikipedia.org/wiki?curid=25828507", "title": "Explosive-driven ferromagnetic generator", "text": "Explosive-driven ferromagnetic generator\n\nAn explosive-driven ferromagnetic generator (EDFMG, explosively pumped ferromagnetic generator, EPFMG, or FMG) is a compact pulsed power generator, a device used for generation of short high-voltage high-current pulse by releasing energy stored in a permanent magnet. It is suited for delivering high-current pulses (kiloamperes) to low-impedance loads.\n\nThe FMGs consist of a permanent magnet (usually a neodymium magnet), a high explosive charge, and a pickup coil. They are a kind of phase transition generators, utilizing pressure-induced magnetic phase transition effect. By adjusting the number of turns of the coil, which can be as low as a single turn, the generator can be designed for delivery of high-current low-voltage pulses or, with more turns, low-current high-voltage pulses.\n\nThe shock wave generated by explosion destroys the magnetic domains in the magnet, cause loss of the magnetic field, and the very sudden change induces a high-peak electric current in the surrounding coil. Both the shock wave directions parallel to the vector of magnetization (longitudal) and perpendicular (transverse) are possible to be used. One of the possible configurations is a ring magnet with the explosive charge in its center.\n\nEDFMGs are especially well suited as seed power sources for explosively pumped flux compression generators and can be used for charging capacitor banks.\n\nA generator coupling an EDFMG containing an 8.75Â cm of magnetic material with a spiral vector inversion generator yielded a pulse of amplitude over 40 kilovolts with a rise time of 6.2 nanoseconds. Generators delivering pulses over 50 kV and 5 kA were demonstrated.\n\nUltra-compact generators with diameter less than 50Â mm were developed.\n\n\n"}
{"id": "26779693", "url": "https://en.wikipedia.org/wiki?curid=26779693", "title": "Float (horse-drawn)", "text": "Float (horse-drawn)\n\nA float is a form of two-wheeled horse-drawn cart with a dropped axle to give an especially low loadbed.\n\nThey were intended for carrying heavy or unstable items such as milk churns. The name survives today in that of the milkfloat.\n\nThe axle passes beneath the loadbed of the cart but was cranked upwards at each side. This allows the load to be carried low, for stability and ease of loading with heavy objects such as churns or barrels. The high position of the stub axles allow large wheels, giving a smooth ride. The box body is open at the rear, with no more than a chain across for safety. Rather than a driving seat or box, they are driven from a standing position, alongside the load. Floats were drawn by a single horse or, especially in Ireland, a donkey.\n\n"}
{"id": "33330773", "url": "https://en.wikipedia.org/wiki?curid=33330773", "title": "Global Map", "text": "Global Map\n\nGlobal Map is a set of digital maps that accurately cover the whole globe to express the status of global environment. It is developed through the cooperation of National Geospatial Information Authorities (NGIAs) in the world. An initiative to develop Global Map under international cooperation, the Global Mapping Project, was advocated in 1992 by Ministry of Construction, Japan (MOC) at the time (the current Ministry of Land, Infrastructure, Transport and Tourism, Japan-MLIT).\n\nGlobal Map is digital geospatial information in 1Â km resolution which satisfies the following conditions:\nThe global geospatial information developed as Global Map mainly consists of the following thematic layers:\n\nSince the United Nations Conference on the Human Environment in 1972, global environmental challenges have been recognized as an issue which is common to humankind. âThe United Nations Conference on Environment and Development (the Earth Summit)â in Brazil in 1992 adopted âAn action plan of humankind for sustainable development: Agenda 21.â Agenda 21 describes in many parts the importance of information for decision-making to appropriately cope with global environmental challenges. Especially geospatial information is regarded to be critical.\n\nIn response to the objectives of Agenda 21 and in recognition of the need for further contribution to the development of geospatial information, the MOC at the time (the current MLIT) advocated, in the same year, the Global Mapping Project, an international cooperation initiative to develop global geospatial information to understand the present status and changes of global environment. This concept was presented at the Fifth United Nations Regional Cartographic Conference for the Americas in New York in 1993. At the same time, resolution\ncalling for the promotion of the development of global geospatial data was adopted at this conference. Following this conference, a similar resolution\nwas adopted at Thirteenth United Nations Regional Cartographic Conference for Asia and the Pacific in Beijing in 1994.\nIn 1996, International Steering Committee for Global Mapping (ISCGM), which consists of heads or its equivalents of NGIAs, was established to promote the Global Mapping Project. Thus the mechanism for the international promotion was formed. The Geospatial Information Authority of Japan (GSI) has been serving as the secretariat of the ISCGM. In the following year, in 1997, which was five years after the Earth Summit, the nineteenth special session of the United Nations General Assembly (19th UNGASS) was held. At the paragraph 112 of the resolution adopted by the 19th UNGASS, importance of a supportive environment to enhance national capacities and capabilities for information collection and processing, especially in developing countries, to facilitate public access to information on global environmental issues was mentioned along with the description mentioning the significance of international cooperation, including global mapping, as a means to develop the supportive environment.\n\nAs a result of such movement, In 1998, a recommendatory letter to participate in the Global Mapping Project was sent from the United Nations to NGIAs of respective countries in the world.\n\nFurther, at the World Summit on Sustainable Development (Johannesburg Summit) held in 2002, global mapping was included in the Plan of Implementation.\n\n"}
{"id": "19286202", "url": "https://en.wikipedia.org/wiki?curid=19286202", "title": "Glossary of fuel cell terms", "text": "Glossary of fuel cell terms\n\nThe Glossary of fuel cell terms lists the definitions of many terms used within the fuel cell industry. The terms in this fuel cell glossary may be used by fuel cell industry associations, in education material and fuel cell codes and standards to name but a few.\n\nAn alkali anion exchange membrane (AAEM) is a semipermeable membrane generally made from ionomers and designed to conduct anions while being impermeable to gases such as oxygen or hydrogen.\n\nHydrogen purity or hydrogen quality is the lack of impurities in hydrogen as a fuel gas.\n\nA hydrogen sulfide sensor or HS sensor is a gas sensor for the measurement of hydrogen sulfide in a gas stream.\n\nA solid oxide electrolyser cell (SOEC) is a solid oxide fuel cell set in regenerative mode for the electrolysis of water with a solid oxide, or ceramic, electrolyte to produce oxygen and hydrogen gas.\n\nA thermoplastic is a plastic that melts to a liquid when heated and freezes to a brittle, very glassy state when cooled sufficiently.\n\nA transfer switch allows switching from a primary power source to a secondary or tertiary power source and are employed in some electrical power distribution systems.\n\nA unitized regenerative fuel cell (URFC) is a fuel cell based on the proton exchange membrane which can do the electrolysis of water in regenerative mode and function in the other mode as a fuel cell recombining oxygen and hydrogen gas to produce electricity.\n"}
{"id": "32229558", "url": "https://en.wikipedia.org/wiki?curid=32229558", "title": "Goose Island Conservation Park", "text": "Goose Island Conservation Park\n\nThe conservation park was proclaimed in 1972 to âconserve an offshore breeding and refuge area for sea-birds and the Australian sea lion (Neophoca cinerea).â The conservation park consists of the following islands: Goose Island, Little Goose Island, Seal Rocks and White Rocks located to the immediate north of Wardang Island with Beatrice Rock, Island Point and Rocky Island all located to the east of Wardang Island, and Boat Rock and Bikini Islets being located on the west side of Wardang Island. \n\nThe conservation park is classified as an IUCN Category III protected area.\n\nThe Goose Island Conservation Park have been identified by BirdLife International as an Important Bird Area because it is considered to support over 1% of the world population of black-faced cormorants, holding up to 750 breeding pairs. It is also a frequently used site for fairy terns which have been recorded as breeding there.\n\n"}
{"id": "31331392", "url": "https://en.wikipedia.org/wiki?curid=31331392", "title": "Hydrant coupler", "text": "Hydrant coupler\n\nA hydrant coupler is a type of aircraft fueling equipment which opens and allows fuel to flow through the hydrant cart into the aircraft. A hydrant coupler needs 60lbs of air pressure to open up the pit valve to allow the flow of fuel. Hydrant couplers run off a pressure fueling system consisting of fuel and air pressure.\n\n"}
{"id": "25085526", "url": "https://en.wikipedia.org/wiki?curid=25085526", "title": "Indian Oil Institute of Petroleum Management", "text": "Indian Oil Institute of Petroleum Management\n\nThe Indian Oil Institute of Petroleum Management (IiPM, stylised IndianOil Institute of Petroleum Management) is a training institute of Indian Oil Corporation set up in 1995 at Gurgaon, a city in the state of Haryana, India. \nIt conducts advanced management education programmes in collaboration with premier business schools and top line professionals. IiPM has been conducting global standard international business management programmes for executive along with various management development programmes.\n\nIiPM is an ISO 9001-2000 certificate institute and has been awarded the Golden Peacock National Award for 'Innovative Training Practices' by the Institute of Directors (IOD), for 1998, 2000, 2005, 2006 and 2007. IIPM received the \"Best Innovation in Teaching\" award by the Association of Indian Management Schools.\n\nIIPM also claims that it received the Indian Society of Training & Development's 'Innovative Training Practices' award for 2006-07.\n\nIIPM has institutional membership of the Delhi Library Network.\n\nIIPM officials, faculty and researchers also undertake participation in seminars at other educational institutions documenting the annual advances in petroleum refining. These seminars are not just within India but internationally too.\n\nIIPM offers a 1-year MBA programme in Petroleum Management in collaboration with the International Center for Promotion of Enterprises an intergovernmental organization based in Ljubljana in Slovenia. But IIPM allows only middle-rank officials of Public Sector Undertaking, not just from oil sector companies, to take admission in the programme.\n\nIIPM announced in 2006 that the first pilot batch of 45 students (all employees of Indian Oil Corporation) had graduated in a joint programme with the University of British Columbia, qualifying to receive the 'Hybrid Certificate Programme in Project Management'. Since then, IIPM planed to train over 1000 of IOC's employees under this programme.\n\n\n"}
{"id": "39115254", "url": "https://en.wikipedia.org/wiki?curid=39115254", "title": "Kangite", "text": "Kangite\n\nKangite is an exceedingly rare scandium mineral, a natural form of impure scandium oxide (ScO), with the formula (Sc,Ti,Al,Zr,Mg,Ca,â¡)O. It crystalizes in the cubic crystal system diploidal class. In terms of chemistry it scandium-analogue of tistarite. Both kangite and tistarite were discovered in the Allende meteorite.\n"}
{"id": "10150234", "url": "https://en.wikipedia.org/wiki?curid=10150234", "title": "Kill A Watt", "text": "Kill A Watt\n\nThe Kill A Watt (a pun on \"kilowatt\") is an electricity usage monitor manufactured by P3 International. It measures the energy used by devices plugged directly into the meter, as opposed to in-home energy use displays, which display the energy used by an entire household. The LCD shows voltage; current; true, reactive, and apparent power; power factor (for sinusoidal waveform); energy consumed in kWh; and hours connected. Some models display estimated cost.\n\nHaving a NEMA 5-15 plug and receptacle, and rated for 115Â VAC (maximum 125Vac), the Kill A Watt is sold for the North American market. The unit is manufactured by the Taiwanese company Prodigit, which also makes 230Â Vac models of similar appearance and functionality for European Schuko, U.K. BS 1363 and Australian AS 3112 receptacles, and a model compatible with 100Â VAC for the Japanese market (2022-04, marketed there as the Watt Checker [ã¯ãããã§ãã«ã¼] Plus by other companies). The basic models support current up to 15 amperes, power up to 1,875Â VA (the 230Vac equivalents also allow up to 15A, corresponding to 3,750Â VA).\n\nThe device can give an indication of the standby power used by appliances.\n\nThere are several models of Kill A Watt meters:\n\nThis is the original, most basic version, based on the Prodigit 2000M. From the time it is plugged in, it measures:\nThe power setting displays instantaneous power, and the kilowatt-hour setting displays energy consumed since last reset. When electricity is disconnected, the P4400's measurements and meters are reset.\n\nThis is an enhanced version, based on the Prodigit 2022, which includes a backup battery to store measurements even when disconnected from electricity. It has the same capabilities as the P4400, and can be programmed with electricity cost information, which enables it to display the cost of the electricity consumed since reset. From this, it can calculate cost per hour, day, week, month, or year.\n\nThis model, based on the Prodigit 2024, is integrated into an eight-outlet power strip. Unlike the other models, it does not display frequency or apparent power. It protects against surges and EMI, has a configurable overcurrent shutdown limit, and also measures earth leakage current; one version acts as an earth leakage circuit breaker (ELCB). It switches power on or off at an AC zero crossing, minimizing current surges and interference.\n\nAlthough identical externally, there have been several different versions of the Kill A Watt and Prodigit equivalents, all essentially of the same design, but with different PCB layout, packaging, and components\n\nOne shortcoming of the Kill-a-Watt range of devices is that they do not have the ability to store, transmit or transfer the readings, thus limiting their usage for any ongoing monitoring purposes. To counter this shortcoming, a couple of openly available modifications have been published on the Web, to enable these devices send data wirelessly to a receiver.\n\nA circuit diagram has been drawn up.\n\nThe Tweet-a-watt is a hacked version of the standard Kill-A-Watt Plug in Power Meter. By piggybacking on the device's on-board LM2902N op-amp chip, the creator was able to get readings for voltage and current and transmit to a computer, which then sent this to Twitter via handle @tweetawatt. At the time it gained quite a lot of interest on the Web, but interest waned after some time. The last tweet from this handle was in March 2010.\n\nFollowing the usefulness of the Tweet-a-Watt, designs for the WattMote were released on the Web by another hobbyist, Felix Rusu at LowPowerLab. The modifications use a customized clone of the Arduino chip known as the Moteino, making this version much cheaper, and requires much less soldering than the original design. Further optimizations on the design were done by Mike Tranchemontage, his designs featured a more robust power supply unit to the moteino chip, avoid problems capacitors which discharged too slowly with the original design.\n\nMost plug-in wattmeters are not useful for measuring standby power, also called vampire power if the device in standby is not doing anything useful such as being prepared to wake under timer control. Many meters only have a resolution of 1W when reading power; the Kill-a-Watts read down to 0.1W, but this is still too coarse for measuring low standby power. Modification to read standby power has been described and discussed in detail (with oscilloscope waveforms and measurements). Essentially, the meter's shunt resistor, used to generate a voltage proportional to load current, is replaced by a much larger value, typically 100 times larger, with protective diodes. Readings of the modified meter have to be divided by the resistance factor (e.g. 100), and maximum measurable power is reduced by the same factor.\n\n\n"}
{"id": "171216", "url": "https://en.wikipedia.org/wiki?curid=171216", "title": "Lanolin", "text": "Lanolin\n\nLanolin (from Latin âwoolâ, and âoilâ), also called wool wax or wool grease, is a wax secreted by the sebaceous glands of wool-bearing animals. Lanolin used by humans comes from domestic sheep breeds that are raised specifically for their wool. Historically, many pharmacopoeias have referred to lanolin as wool fat (\"adeps lanae\"); however, as lanolin lacks glycerides (glycerol esters), it is not a true fat. Lanolin primarily consists of sterol esters instead. Lanolin's waterproofing property aids sheep in shedding water from their coats. Certain breeds of sheep produce large amounts of lanolin.\n\nLanolin's role in nature is to protect wool and skin from climate and the environment; it also plays a role in skin (integumental) hygiene. Lanolin and its derivatives are used in the protection, treatment and beautification of human skin.\n\nA typical high-purity grade of lanolin is composed predominantly of long chain waxy esters (approximately 97% by weight) the remainder being lanolin alcohols, lanolin acids and lanolin hydrocarbons.\n\nAn estimated 8,000 to 20,000 different types of lanolin esters are present in lanolin, resulting from combinations between the 200 or so different lanolin acids and the 100 or so different lanolin alcohols identified so far.\n\nLanolinâs complex composition of long-chain esters, hydroxyesters, diesters, lanolin alcohols, and lanolin acids means in addition to it being a valuable product in its own right, it is also the starting point for the production of a whole spectrum of lanolin derivatives, which possess wide-ranging chemical and physical properties. The main derivatisation routes include hydrolysis, fractional solvent crystallisation, esterification, hydrogenation, alkoxylation and quaternisation. Lanolin derivatives obtained from these processes are used widely in both high-value cosmetics and skin treatment products.\n\nHydrolysis of lanolin yields lanolin alcohols and lanolin acids. Lanolin alcohols are a rich source of cholesterol (an important skin lipid) and are powerful water-in-oil emulsifiers; they have been used extensively in skincare products for over 100 years. Notably, approximately 40% of the acids derived from lanolin are alpha-hydroxy acids (AHAs). The use of AHAs in skin care products has attracted a great deal of attention in recent years. Details of the AHAs isolated from lanolin can be seen in the table below.\n\nIn addition to general purity requirements, lanolin must meet official requirements for the permissible levels of pesticide residues. The Fifth Supplement of the United States Pharmacopoeia XXII published in 1992 was the first to specify limits for 34 named pesticides. A total limit of 40Â ppm (i.e. 40Â mg/kg) total pesticides was stipulated for lanolin of general use, with no individual limit greater than 10Â ppm.\n\nA second monograph also introduced into the US Pharmacopoeia XXII in 1992 was entitled âModified Lanolinâ. Lanolin conforming to this monograph is intended for use in more exacting applications, for example on open wounds. In this monograph, the limit of total pesticides was reduced to 3Â ppm total pesticides, with no individual limit greater than 1Â ppm.\n\nIn 2000, the European Pharmacopoeia introduced pesticide residue limits into its lanolin monograph. This requirement, which is generally regarded as the new quality standard, extends the list of pesticides to 40 and imposes even lower concentration limits.\n\nSome very high-purity grades of lanolin surpass monograph requirements. New products obtained using complex purification techniques produce lanolin esters in their natural state, removing oxidative and environmental impurities resulting in white, odourless, hypoallergenic lanolin. These ultra-high-purity grades of lanolin are ideally suited to the treatment of dermatological disorders such as eczema and on open wounds.\n\nLanolin attracted attention owing to a misunderstanding concerning its sensitising potential. A study carried out at New York University Hospital in the early 1950s had shown about 1% of patients with dermatological disorders were allergic to the lanolin being used at that time. By one estimate, this simple misunderstanding of failing to differentiate between the general healthy population and patients with dermatological disorders exaggerates the sensitising potential of lanolin by 5,000â6,000 times.\n\nThe European Cosmetics Directive, introduced in July 1976, contained a stipulation that cosmetics which contained lanolin should be labelled to that effect. This ruling was challenged immediately, and in the early 1980s, it was overturned and removed from the directive. Despite only being in force for a short period of time, this ruling did harm both to the lanolin industry and to the reputation of lanolin in general. The Cosmetics Directive ruling only applied to the presence of lanolin in cosmetic products; it did not apply to the many hundreds of its different uses in dermatological products designed for the treatment of compromised skin conditions.\n\nModern analytical methods have revealed lanolin possesses a number of important chemical and physical similarities to human stratum corneum lipids; the lipids which help regulate the rate of water loss across the epidermis and govern the hydration state of the skin.\n\nCryogenic scanning electron microscopy has shown that lanolin, like human stratum corneum lipids, consists of a mass of liquid crystalline material. Cross-polarised light microscopy has shown the multilamellar vesicles formed by lanolin are identical to those formed by human stratum corneum lipids. The incorporation of bound water into the stratum corneum involves the formation of multilamellar vesicles.\n\nSkin bioengineering studies have shown the durational effect of the emollient (skin smoothing) action produced by lanolin is very significant and lasts for many hours. Lanolin applied to the skin at 2Â mg/cm has been shown to reduce roughness by about 35% after one hour and 50% after two hours, with the overall effect lasting for considerably more than eight hours. Lanolin is also known to form semiocclusive (breathable) films on the skin. When applied daily at around 4Â mg/cm for five consecutive days, the positive moisturising effects of lanolin were detectable until 72Â hours after final application. Lanolin may achieve some of its moisturising effects by forming a secondary moisture reservoir within the skin.\n\nThe barrier repair properties of lanolin have been reported to be superior to those produced by both petrolatum and glycerol. In a small clinical study conducted on volunteer subjects with terribly dry (xerotic) hands, lanolin was shown to be superior to petrolatum in reducing the signs and symptoms of dryness and scaling, cracks and abrasions, and pain and itch. In another study, a high purity grade of lanolin was found to be significantly superior to petrolatum in assisting the healing of superficial wounds.\n\nLanolin and its many derivatives are used extensively in both the personal care (e.g., high value cosmetics, facial cosmetics, lip products) and health care sectors such as topical liniments. Lanolin is also found in lubricants, rust-preventive coatings, shoe polish, and other commercial products.\n\nLanolin is a relatively common allergen and is often misunderstood as a wool allergy. However, allergy to a lanolin-containing product is difficult to pinpoint and often other products containing lanolin may be fine for use. Patch testing can be done if a lanolin allergy is suspected.\n\nIt is frequently used in protective baby skin treatment and for sore nipples in breastfeeding mothers.\n\nLanolin is used commercially in many industrial products ranging from rustproof coatings to lubricants. Some sailors use lanolin to create slippery surfaces on their propellers and stern gear to which barnacles cannot adhere. Commercial products (e.g. Lanocote) containing up to 85% lanolin are used to prevent corrosion in marine fasteners, especially when two different metals are in contact with each other and saltwater. The water-repellent properties make it valuable in many applications as a lubricant grease where corrosion would otherwise be a problem.\n\n7-Dehydrocholesterol from lanolin is used as a raw material for producing vitamin D by irradiation with ultraviolet light.\n\nBaseball players often use it to soften and break in their baseball gloves (shaving cream with lanolin is popularly used for this).\n\nAnhydrous liquid lanolin, combined with parabens, has been used in trials as artificial tears to treat dry eye.<br>\nAnhydrous lanolin is also used as a lubricant for brass instrument tuning slides.\n\nLanolin can also be restored to woollen garments to make them water and dirt repellent, such as for cloth diaper covers.\n\nLanolin is also used in lip balm products such as Carmex. For some people, it can irritate the lips.\n\nLanolin is sometimes used by people on continuous positive airway pressure therapy to reduce irritation with masks, particular nasal pillow masks that can often create sore spots in the nostrils.\n\nLanolin is a popular additive to moustache wax, particularly 'extra-firm' varieties.\n\nLanolin is used as a primary lubricating component in aerosol-based brass lubricants in the ammunition reloading process. Mixed warm 1:12 with highly concentrated ethanol (usually 99%), the ethanol acts as a carrier which evaporates quickly after application, leaving a fine film of lanolin behind to prevent brass seizing in resizing dies.\n\nLanolin, when mixed with ingredients such as neatsfoot oil, beeswax and glycerol, is used in various leather treatments, for example in some saddle soaps and in leather care products.\n\nCrude lanolin constitutes about 5â25% of the weight of freshly shorn wool. The wool from one Merino sheep will produce about 250â300Â ml of recoverable wool grease. Lanolin is extracted by washing the wool in hot water with a special wool scouring detergent to remove dirt, wool grease (crude lanolin), suint (sweat salts), and anything else stuck to the wool. The wool grease is continuously removed during this washing process by centrifuge separators, which concentrate it into a waxlike substance melting at approximately .\n"}
{"id": "49851311", "url": "https://en.wikipedia.org/wiki?curid=49851311", "title": "List of wind farms in Lithuania", "text": "List of wind farms in Lithuania\n\nAs of 2016, there were over 160 operational wind farms in Lithuania, but the majority of them consisted of small farms generating less than 2 MW.\n\nOnly farms with bigger than 3 MW capacity are listed.\n\n"}
{"id": "27488923", "url": "https://en.wikipedia.org/wiki?curid=27488923", "title": "MV Waily", "text": "MV Waily\n\nMV \"Waily\" is a Saint Vincent and the Grenadines-flagged bulk carrier owned by Treasure Marine Ltd. The vessel was involved in a collision with the Malaysian-flagged oil tanker \"MT Bunga Kelana 3\" on 25 May 2010 in the Singapore Strait that caused a release of 2,000 tonnes of crude oil.\n"}
{"id": "19593227", "url": "https://en.wikipedia.org/wiki?curid=19593227", "title": "Magnetic diffusivity", "text": "Magnetic diffusivity\n\nThe magnetic diffusivity is a parameter in plasma physics which appears in the magnetic Reynolds number. It has SI units of mÂ²/s and is defined as:\nwhile in Gaussian units it can be defined as\nIn the above, formula_3 is the permeability of free space, formula_4 is the speed of light, and formula_5 is the electrical conductivity of the material in question. In case of a plasma, this is the conductivity due to Coulomb or neutral collisions: formula_6, where\n"}
{"id": "15363525", "url": "https://en.wikipedia.org/wiki?curid=15363525", "title": "Microlith (catalytic reactor)", "text": "Microlith (catalytic reactor)\n\nMicrolith is a brand of catalytic reactor invented by the prize-winning engineer William C. Pfefferle and sold by Precision Combustion. Microlith's advantages include its weight, size, efficiency, and fast thermal response.\n\nA catalyst is a substance that speeds a reaction but that itself is left in its original state after the reaction, so that it can assist in the reaction of a large quantity of material over a long period of time. A Microlith reactor is constructed with a very thin metal substrate coated with a variety of materials including catalysts to speed reactions, and adsorbent materials for use in filters. The substrate has short channels (0.001â0.020Â in) which resemble screens or meshes. This results in a lower pressure drop than other reactors and allows for high cell density and low thermal mass. Mass and heat transfer are significantly increased, allowing faster reactor response to gas temperatures and improved rates of reactant contact with the surface. By passing an electric current through the metal substrate, the Microlith can be heated rapidly and efficiently. Over 12 Microlith related US patents have been issued.\n\n\n"}
{"id": "12697430", "url": "https://en.wikipedia.org/wiki?curid=12697430", "title": "Nunobiki Plateau Wind Farm", "text": "Nunobiki Plateau Wind Farm\n\nThe is a wind farm operated by the electric utility company J-Power alongside Lake Inawashiro in KÅriyama, Fukushima, Japan. It is approximately 1,080 meters above sea level.\n\nThe wind farm produces 65.98 megawatts of power using a total of 33 type Enercon E-70, wind turbines, and generates enough power for approximately 35,000 houses.\n\nThe facility began operating in February 2007. Each wind turbine is about 100 meters high, and each turbine blade is about 30 meters long.\n\n"}
{"id": "48037063", "url": "https://en.wikipedia.org/wiki?curid=48037063", "title": "October 2015 North American storm complex", "text": "October 2015 North American storm complex\n\nThe OctoberÂ 2015 North American storm complex was an extratropical storm that triggered a high precipitation event, which caused historic flash flooding across North and South Carolina. The incipient cold front traversed the Eastern United States on SeptemberÂ 29â30, producing heavy rain in multiple states. The system subsequently stalled just offshore. Tapping into moisture from the nearby Hurricane Joaquin, a developing surface low brought heavy, continuous rain to southeastern States, with the worst effects concentrated in South Carolina where catastrophic flooding occurred. The event culminated in South Carolina on OctoberÂ 4 when numerous rivers burst their banks, washing away roads, bridges, vehicles, and homes. Hundreds of people required rescue and the state's emergency management department urged everyone in the state not to travel. Some areas of the state saw rainfall equivalent to a 1-in-1000-year event.\n\nAt least 25Â deaths have been attributed to the weather complex: 19 in South Carolina, 2 in New York, 2 in North Carolina, 1 in Florida, and 1 in New Brunswick. Damage from the storm reached $2Â billion dollars.\n\nOn SeptemberÂ 29, 2015, a cold front moved southeast across the Eastern United States and produced widespread heavy rain. By OctoberÂ 2, the frontal system stalled offshore and a 1000Â mbar (hPa; 29.53Â inHg) surface low developed just east of the FloridaâGeorgia border. The cyclone interacted with Hurricane Joaquinâsituated over the Bahamas at the timeâwith moisture streaming north from the hurricane into the Southeastern United States. This moisture interacted with the surface low, frontal boundary, and a strong upper-level low to produce prolonged, heavy rains over the region with training bands situated over South Carolina. A strengthening ridge to the northeast created a tighter pressure gradient, resulting in a large area of onshore gales.\n\nOn SeptemberÂ 30, Virginia Governor Terry McAuliffe declared a state of emergency for the entire state owing to heavy rains and the threat of Hurricane Joaquin. The City of Norfolk also declared an emergency. On OctoberÂ 1, Governors Larry Hogan, Chris Christie, Pat McCrory, and Nikki Haley declared a state of emergency for Maryland, New Jersey, North Carolina, and South Carolina respectively.\n\nBy OctoberÂ 3, approximately 22Â million people were under flood warnings or watches. The storm prompted the cancellation of 145Â flights nationwide.\n\nOne person was killed in North Carolina on OctoberÂ 1 when a tree fell on her car. Flooding in Brunswick County, North Carolina prompted the evacuation of 400â500Â people. More than 10,000Â people were without power in the state. A second death was confirmed on OctoberÂ 5.\n\nOn OctoberÂ 4, a 9-year-old drowned after being pulled out to sea by rip currents near St. Pete Beach, Florida.\n\nRainfall across parts of South Carolina reached 500-year event levels, with areas near Columbia experiencing a 1-in-1000 year event. Accumulations reached near Boone Hall by 11:00Â a.m. EDT (15:00Â UTC) on OctoberÂ 4. Charleston International Airport saw a record 24-hour rainfall of on OctoberÂ 3. Nearly 30,000Â people were without power in the state. One woman drowned in Spartanburg on OctoberÂ 1 after her car was overwhelmed by flooding in an underpass. On OctoberÂ 2, a plane crashed along the South Carolina side of Lake Hartwell, killing all four occupants. The cause is currently unknown though there was light rain at the time of the incident. On OctoberÂ 3, the Charleston Historic District was brought to a virtual standstill with most roads closed because of flooding. Three deaths were confirmed in the state on OctoberÂ 2 and 3. Through the evening of OctoberÂ 3, highway patrol reported 500Â traffic accidents and 104Â flooded roads.\n\nEarly on OctoberÂ 4, the National Weather Service issued flash flood emergencies for Berkeley, Charleston, and Dorchester counties. From 4:00â7:30Â a.m. EDT (08:00â11:30Â UTC), Gills Creek in Columbia rapidly rose to before the river gauge stopped reporting; this shattered the previous record crest of in JulyÂ 1997. The state's Emergency Management Division issued a statement later that morning via Twitter at 6:59Â a.m. EDT, stating: \"... remain where you are if you are safely able to do so.\" They reiterated this at 8:20Â a.m., stressing that residents should not travel: \"Remain. Where. You. Are. Dangerous flooding conditions through the state for most of the day.\" The State's Emergency Management division also issued a statement not to move or drive around barricades blocking flooded roads, yet drivers still moved and/or drove around barricades. Three subjects died after someone else removed a barricade from a road; in the darkness, the three were unable to see that the road had been washed out, and they drove into a chasm. A dam along Semmes Lake at Fort Jackson collapsed. More than 140Â rescues were made during the overnight hours; the United States Coast Guard was deployed to assist in rescue missions.\nAt 10:54Â a.m. EDT (14:54Â UTC) on October 4, 211Â state roads and 43Â bridges were closed. On the same day, Georgetown County Emergency Management closed all roadways in the county because of severe flooding; the South Carolina Emergency Management Division announced the closure of Interstate 95 between Interstate 20 and Interstate 26, a stretch. A mandatory curfew was put in place for Columbia beginning at 6:00Â p.m. EDT (22:00Â UTC). All residents in the city were also advised to boil water as water lines suffered damage. One person died in the city after her car was swept away. Multiple school districts and colleges across the state were closed the week of October 5th, including Horry County Schools, Georgetown County Schools, Williamsburg County Schools, Sumter County Schools, Charleston County Schools, University of South Carolina, Coastal Carolina University and College of Charleston.\n\nEighteen dams were breached or collapsed across the state. A mandatory evacuation was issued for areas downstream of the Overcreek Dam on OctoberÂ 5 after the structure was breached. The head of the South Carolina National Guard compared damage from the floods to Hurricane Hugo in 1989, which caused $9.5Â billion in economic losses. Reinsurance company Aon Benfield indicated losses from the floods would be well in excess of $1Â billion, with a large portion coming from uninsured homeowners.\n\nAt least 19Â deaths were confirmed in relation to the storm, by the evening of OctoberÂ 9, 2015.\n\nThe fallout from the flooding forced the South Carolina Gamecocks to move their October 10 home game against the LSU Tigers to Tiger Stadium in Baton Rouge. While Williams-Brice Stadium in Columbia was not heavily damaged, school officials felt the damage to the area's infrastructure was too severe to host the game.\n\nIn Virginia, heavy rains resulted in numerous traffic accidents; state police responded to 375Â incidents on OctoberÂ 3. Police received more than 1,200Â calls that day. Power outages affected 7,300Â customers at the height of the storm. The James River approached flood levels, and hundreds evacuated low-lying areas in Lancaster County on Virginia's Northern Neck.\n\nTidal flooding in Ocean City, Maryland prompted road closures. Rainfall in the state peaked at near Bishopville. In Delaware, the storm caused coastal flooding, with Delaware Route 1 between Bethany Beach and Dewey Beach closed on OctoberÂ 2 due to flooding and not reopened until OctoberÂ 4.\n\nSeveral days of continuous onshore flow caused significant coastal flooding and beach erosion in New Jersey. The worst erosion took place in Ocean County, specifically around Mantoloking, where of sand was washed away; Ortley Beach saw up to . Wind gusts up to âthe highest observed winds in relation to the nor'easterâwere measured at Cape May. Coastal flooding in New Jersey destroyed at least one home. Stone Harbor sustained millions of dollars in damage to the beach. At least 3,600Â residences lost power in the state. Despite severe coastal erosion, structural damage was limited.\n\nOn OctoberÂ 2, a fishing boat with five crew capsized amid swells in Jamaica Bay, near Floyd Bennett Field along the south coast of Long Island, New York. Two people were able to swim to shore and signal rescue for the other three; two later died in the hospital.\n\nFlooding in Portland, Maine stranded several vehicles.\n\nHeavy rains associated with the incipient frontal boundary extended into Atlantic Canada, with of rain observed in parts of New Brunswick. Widespread flooding washed out roads and bridges, impairing travel; Hoyt was rendered inaccessible. One person died in Berwick after a retaining wall collapsed on him.\n\nPresident Barack Obama declared parts of South Carolina a disaster area, making federal aid available in Charleston, Berkeley, Dorchester, Georgetown, Horry, Lexington, Orangeburg, Richland, and Williamsburg Counties. Calhoun, Clarendon, Kershaw, Lee, and Sumter counties were later added to the list of federal disaster areas. More than 1,300Â National Guard soldiers and 250Â state troopers were mobilized across the state. The United States Department of Transportation released $5Â million in emergency funds to the South Carolina Department of Transportation on OctoberÂ 6. On October 16, the following counties were added to the list of federal disaster areas: Abbeville, Anderson, Bamberg, Colleton, Darlington, Fairfield, Florence, Laurens, McCormick, and Newberry counties.\n\nLooting was reported in some areas of Columbia that had been evacuated.\n\nOn October 5, 541 roads were closed.\n\nIt was announced October 9, that 18 bridges along 13 miles of Interstate 95 had foundation damage that still needed repairs, which would start October 10. Until the repairs were done, drivers had to make a detour of 94 extra miles by Columbia. Southbound lanes on the final 16 miles of Interstate 95 opened October 12 and all of the interstate reopened by 8 A.M. EDT on October 13, after structural repairs to 13 bridges.\n\nBy November 25, 2015, 69 roads were closed. 26 of those needed repair or replacement of private dams to take place. A South Carolina Department of Transportation report said 221 bridges were affected and 18 would have to be replaced. Workers removed 2000 truckloads of debris from roads. The estimated cost of road repairs was $137 million.\n\nAs of August 2017, almost 2 years to the date, there are still several roads and private dams that are in disrepair.\n\n"}
{"id": "12477677", "url": "https://en.wikipedia.org/wiki?curid=12477677", "title": "Old field (ecology)", "text": "Old field (ecology)\n\nOld field is a term used in ecology to describe lands formerly cultivated or grazed but later abandoned. The dominant flora include perennial grasses, heaths and herbaceous plants. Old fields are canonically defined as an intermediate stage found in ecological succession in an ecosystem advancing towards its climax community, a concept which has been debated by contemporary ecologists for some time.\n\nOld field sites are often marginal lands with soil quality unsuitable for crops or pasture. Examples include abandoned farmlands in central Ontario, along the edge of the Canadian Shield.\n\nStress tolerant species with wide seed dispersal ranges are able to colonize cultivated fields after their initial abandonment, usually followed by perennial grasses. The succession of old fields culminates in takeover by trees and shrubs.\n\n\n"}
{"id": "22886704", "url": "https://en.wikipedia.org/wiki?curid=22886704", "title": "Paint recycling", "text": "Paint recycling\n\nPaint is a recyclable item. Latex paint is collected at collection facilities in many countries and shipped to paint-recycling facilities.\n\nThere are many ways that paint can be recycled. Most often, the highest quality of latex paint is sorted out and turned back into recycled paint that can be used. Recycled paint is environmentally preferable to new paint, while still maintaining comparable quality. In many cases, reusable paints of the same color are pumped into a tank where the material is mixed and tested. The paint is adjusted with additives and colorants as necessary. Finally, the paint is fine filtered and packaged for sale.\n\nPaint that cannot be reused has other environmentally friendly uses. Non-reusable paint can be made into a product used in cement manufacturing, thereby recycling virtually 100% of the original paint.\n\nRecycling one gallon of paint could save 13 gallons of water, 1 quart of oil, and 250,000 gallons of water pollution, 13.74 pounds of , save enough energy to power the average home for 3 hours, or cook 6 meals in a microwave oven, or blow dry someone's hair 27 times.\n\nIn Ontario, Stewardship Ontario oversees the collection of waste paint from consumers and diversion from landfill to meet targets approved by the Ministry of the Environment through a program called the Orange Drop Program. The Orange Drop program is an extensive and growing network of collection sitesâdrop-off locations for paint leftovers and other special materials that canât go in the Blue Box or the garbage.\nAs an Orange Drop-approved transporter and processor, Loop Recycled Products Inc. takes leftover paint, collected through Stewardship Ontario, and turns it into 12 shades of premium, affordable and environmentally friendly recycled paint. Reusing top-quality residual paint (on average, the original retail value of a gallon of incoming paint is approximately $30) enables Loop to create premium products without the raw material costs and energy consumption needed to make paint from scratch.\nSince 2012, Loop Recycled Products Inc. has diverted over 6 million litres of paint from disposal in Ontarioâs landfills, incineration and waterways and is committed to innovation and solving Canadaâs waste paint problem.\n\nIn February 2015 Waste Diversion Ontario approved Product Care as the new Ontario waste paint stewardship operator effectively replacing Stewardship Ontario.\n\nIn March 2017, Colortech ECO Paints introduced its line of recycled wall and floor paints to specific retail markets consisting of a large network of liquidation and discount stores across Canada and the United States., as well as exporting large quantities to West Africa and South America.\n\nAlbertaâs paint recycling program started accepting leftover, unwanted paint on April 1, 2008. It is estimated that about 30 million liters of paint is sold in Alberta each year. On average, 5 to 10 percent of this ends up as waste, which can pose environmental and health risks if disposed of improperly. Paint contains many components that have great potential for reuse, recycling and recovery. The Paint Recycling Alberta program enables these products to be handled and recycled in an environmentally safe manner, reducing their impact on the environment. The program is funded through environmental fees charged on the sale of new paint in Alberta. The fees are put into a dedicated fund that can only be used to manage the paint recycling program. \nThe paint is sorted into different streams and sent to registered processors to be recycled into new paint, used in other products or in energy recovery, or sent for proper disposal if necessary. Any processor that receives paint must be registered with the Paint Recycling Program and meet all applicable environmental, transportation, health & safety, and local requirements. \n\nCalibre Environmental LTD. (CEL) located in Calgary, Alberta, became a key part in 2008 of the new Alberta Paint Stewardship program which significantly increased the recycling of unused latex paint from across the province of Alberta. Calibre Environmental Ltd. currently processes about 1.6 million kilograms of latex paint annually, which equates to the successful recycling of one million litres of quality latex paint per year.\n\nIn the UK reusable leftover paint can be donated to Community RePaint, a national network of paint reuse schemes. The network comprises local schemes run by not-for-profit organisations, local authorities or waste management companies, in the Community RePaint network. The schemes collect surplus paint from trade sources i.e. painters, decorators, retailers, manufacturers, and/or leftover paint donated by householders at council household waste and recycling centres (also known as tips). The paint is then sorted by staff and volunteers before being redistributed to local charities, community groups, families and individuals in need. The Community RePaint network, is sponsored by Dulux (part of AkzoNobel), managed by an environmental consultancy, Resource Futures and has been cited as an example of best practice for the management of surplus paint in a report by the European Commission and by DEFRA in Guidance on Applying the Waste Hierarchy. \n\nThere are also a handful of companies recycling paint in the UK, but only two reprocessing waste paint, back into the quality of virgin paint; Newlife Paints. Newlife Paints was formed in 2008 after Keith Harrison, an industrial chemist, developed a process that converted waste emulsion paint back into full quality, commercial grade paint. Castle RePaint, part of the social enterprise company Castle Furniture also consolidates unwanted emulsion paint into brand new 'RePaint' in a range of colours.\n\nConcerns about the life cycle of paint have led to the creation of PaintCare, a non-profit 501(c)(3) organization established to represent paint manufacturers (paint producers) to plan and operate paint stewardship programs in the United States in those states that pass paint stewardship laws. \n\nPaint stewardship law aims to enable the paint industry to implement a collection program that allows consumers to take their leftover, unwanted paint to a collection site to be collected and recycled. Legislation mandating the creation of the PaintCare program has been enacted in eight states since 2009: Oregon, California, Connecticut, Rhode Island, Vermont, Minnesota, Maine, and Colorado. Legislation has also been passed for the District of Columbia; PaintCare anticipates beginning the District's paint stewardship program in September 2016. PaintCare is responsible for promoting the reuse of post-consumer architectural paint (leftover paint) and providing for the collection, transport, and processing of this paint using the hierarchy of \"reduce, reuse, recycle,\" and proper disposal. Most PaintCare locations are at paint retailers who volunteer to take back paint. These retailers take back paint during regular business hours, making paint recycling and disposal much more convenient for the public.\n\nPaint is shipped to companies such as Amazon Environmental, American Paint Recyclers (Ohio), GDB International, Metro Paint (Oregon), UCI Environmental (Nevada) and Kelly Moore, Visions Paint Recycling, Inc (California)& Williams Paint Recycling Company. In the Southern California area, Acrylatex Coatings & Recycling, Inc. accepts unused/unwanted latex paints for reprocessing into a viable resource of recycled paints in 20-standard colors. In the southeastern United States Atlanta Paint Disposal has a paint recycling program with drop off locations in Atlanta, Georgia. In the northeast The Paint Exchange, LLC recycles latex paint under the brand recolorÂ®.\n\nA new charitable organization known as The Global Paint for Charity incorporated in Georgia, US, has as its mission to collect leftover paint from residents and businesses nationwide and use it for global housing rehabilitation projects, including homes, schools, hospitals, jails and churches for vulnerable families in developing countries. They partner with non-profit organizations with existing operations in these continents for paint distribution. Through the support of their donors and partners they are able to improve communities, increase access to quality paints and protect the environment.\n\nThe Environmental Protection Agency (EPA) estimates that every homeowner in the US has 3 to 4 gallons of leftover paints in their basement, and 10 percent of those paints ends up in landfills. \n\nOne gallon of improperly disposed paint has the ability to pollute up to 250,000 gallons of water. \n\nBy participating in the program, individuals and businesses will take greater steps to protect the environment, and improve living conditions for vulnerable populations throughout the world. If you would like to support the Global Paint for Charity, they encourage you to take action today.\n\nImprove access of high-quality paints to vulnerable populations around the world. Nearly 2.5 billion people in developing countries live on less than $2 a day. For them, paint is very expensive. In these settings, it is very difficult for families to secure sufficient income for their basic needs (including but not limited to: food, medicine, water, clothes, school supplies, and shelter). When making consumption choices that involve spending on their basic needs there is nothing left to spend on paint. The paint shortage affects many other areas of the world, where communities lack even the most basic need and materials to uplift their people. For the worldâs poorest communities, home isnât just where the heart is. Dirt walls and neglected communities are not attractive to tourists, putting those who cannot afford paint, not only at the greatest risk of life-threatening of bad germs but also lack of economic opportunities.\n\nSince it started years ago, as many as 500â6000 gallons of paint have been shipped at a time to developing countries, including Kenya, Uganda, Haiti, Dominican Republic, Honduras, El Salvador, Guyana, Guinea, Ghana, Jamaica and Mexico, 240 volunteers have painted 459 family homes and 40 schools and orphanages with over 150,000 gallons of donated paint from businesses and residences.\n\nGlobal Paint for Charity continues to expand and impact our communities around the world. They do this through working directly with their volunteers, donors, and partners. From developing paint projects that engage their employees in the beautification of the community; to run frequent local paint drives to support their program.\n\nGlobal Paint for Charity has recently won the National Energy Globe Award of United States 2017. With more than 178 participating countries and over 2,000 project submissions annually the Energy Globe Award is today's most prestigious environmental prize worldwide. The Austrian Honorary Consul General, Mr. Ferdinand C. Seefried hosted an exclusive ceremony to present the National Energy Globe Award 2017 for the United States to Mr. Rony Delgarde, Global Paint for Charity.\n\n\n\n"}
{"id": "33682469", "url": "https://en.wikipedia.org/wiki?curid=33682469", "title": "Paul GrÃ¸stad", "text": "Paul GrÃ¸stad\n\nPaul M. GrÃ¸stad (21 September 1933 â 4 April 2011) was a Norwegian businessperson.\n\nHe took the cand.jur. degree, and worked in Norske Shell (Royal Dutch Shell in Norway) from 1960 to 1993. He is best known as the chairman of the company, from 1997 to 2003.\n\nDuring his time in Norske Shell, he was involved in both building up and closing down the oil refinery in Sola. The company also became involved in the Ormen Lange gas field, eventually becoming the field operator. GrÃ¸stad also handled a boycott of Shell, which started because of alleged corporation with apartheid-era South Africa.\n"}
{"id": "18460403", "url": "https://en.wikipedia.org/wiki?curid=18460403", "title": "Perusahaan Gas Negara", "text": "Perusahaan Gas Negara\n\nPT Perusahaan Gas Negara (Persero) Tbk (National Gas Company in English) is an Indonesian natural gas transportation and distribution company in Indonesia\n. The total length of distribution pipelines of the company is 3,187Â km that serve around 84 million customers.\n\nThe gas exploitation in Indonesia was managed by a Dutch private gas company named \"IJN Eindhoven & Co.\", which was established in 1859. The company introduced the use of gas in Indonesia, which was at the time made from coal.\n\nThe transfer of power processes occurs at the end of World War II in August 1945, when Japan surrendered to the Allies. This opportunity was used by the youth and electrical workers (through Electricity and Gas Labour/Employee delegation), which together with the chairman of the Central Indonesian National Committee (KNIP) initiated to meet President Sukarno for take over the company to the Government of Indonesia.\n\nOn 27 October 1945, President Sukarno formed \"Jawatan Listrik dan Gas\" (Bureau of Electricity and Gas) under the Ministry of Public Works and Energy with a power generating capacity of 157.5 MW. In 1958, I.J.N. Eindhoven & Co nationalised and converted into \"PN Gas\".\n\nOn 1 January 1961, the Bureau of Electricity and Gas converted into BPU-PLN (\"Badan Pimpinan Umum Perusahaan Listrik Negara\", General Governing Agency of the State Electric Company). The agency, which managed the electricity and gas, then dissolved on 1 January 1965. At the same time, two state-owned companies was initiated: the Perusahaan Listrik Negara (PLN) as the manager of electric power and Perusahaan Gas Negara (PGN) as the manager of gas. PGN was officially established on 13 May 1965.\n\nThe company owns and operates four transmission pipelines:\n\nIn 2011 the company distributed 795 MMSCFD and transmitted 845 MMSCFD of natural gas\n"}
{"id": "33053601", "url": "https://en.wikipedia.org/wiki?curid=33053601", "title": "Radioactive contamination from the Rocky Flats Plant", "text": "Radioactive contamination from the Rocky Flats Plant\n\nThe Rocky Flats Plant was a former U.S. nuclear weapons production facility located about 15 miles northwest of Denver.\n\nHistorical releases caused radioactive (plutonium, americium) contamination within and outside its boundaries. The contamination primarily resulted from releases from the 903 Pad drum storage area â wind-blown plutonium that leaked from barrels of radioactive waste â and two major plutonium fires in 1957 and 1969 (plutonium is pyrophoric and shavings can spontaneously combust). Much lower concentrations of radioactive isotopes were released throughout the operational life of the plant from 1952 to 1992, from smaller accidents and from normal operational releases of plutonium particles too small to be filtered. Prevailing winds from the plant carried airborne contamination south and east, into populated areas northwest of Denver.\n\nThe contamination of the Denver area by plutonium from the fires and other sources was not publicly reported until the 1970s. According to a 1972 study coauthored by Edward Martell, \"In the more densely populated areas of Denver, the Pu contamination level in surface soils is several times fallout\", and the plutonium contamination \"just east of the Rocky Flats plant ranges up to hundreds of times that from nuclear tests.\" As noted by Carl Johnson in Ambio, \"Exposures of a large population in the Denver area to plutonium and other radionuclides in the exhaust plumes from the plant date back to 1953.\"\n\nWeapons production at the plant was halted after a combined FBI and EPA raid in 1989 and years of protests. The plant has since been shut down, with its buildings demolished and completely removed from the site. The Rocky Flats Plant was declared a Superfund site in 1989 and began its transformation to a cleanup site in February 1992. Removal of the plant and surface contamination was completed in the late 1990s and early 2000s. Nearly all underground contamination was left in place, and measurable radioactive environmental contamination in and around Rocky Flats will probably persist for thousands of years. The land formerly occupied by the plant is now the Rocky Flats National Wildlife Refuge. Plans to make this refuge accessible for recreation have been repeatedly delayed due to lack of funding and protested by citizen organizations.\n\nWhile some residual contamination remains above background levels, environmental data shows these levels are below health-based regulations that would trigger further action. Offsite areas and the lands now comprising the Rocky Flats National Wildlife Refuge were studied as part of the cleanup. The Refuge lands did not require remediation because levels were so low, below action levels. Calculated external radiation doses to an adult or child Refuge visitor in the part of the Refuge with the highest detected levels of plutonium are 0.07 millirems (mrems) per year and 0.2 mrem per year, respectively, based on an assumption of no ingestion., These levels are well below the Colorado Standards for Protection Against Radiation acceptable dose of 25 mrem per year. For comparison, a single medical x-ray can provide a dose of 10 mrem. The average annual radiation dose to an American is about 620 mrem per year from both naturally-occurring and commercial, industrial, and medical sources of radioactivity. \n\nThe Department of Energy continues to monitor the site. Surface water sampling is ongoing and groundwater samples are regularly collected and analyzed. Some private groups and researchers remain concerned about the extent and long-term public health consequences of the contamination. Estimates of the public health risk caused by the contamination vary significantly, with accusations that the United States government is being too secretive and that citizen activists are being alarmist. The Comprehensive Risk Assessment for the site found the post-cleanup risks posed by the site to be very low and within EPA guidelines. A 1998 independent study by the Colorado Department of Public Health and Environment on cancer rates in communities surrounding Rocky Flats also found no pattern of increased cancers tied to Rocky Flats. In 2016, this study was updated with 25 years of additional cancer data; the data supported the same conclusion.\n\nIn September 2017, CDPHE's Cancer Registry released a supplement to this study in response to public interest. The supplement looked at the incidence of thyroid and all rare cancers in communities around Rocky Flats. Data showed no evidence of higher than expected rates of thryoid cancer. In addition, the overall incidence of all rare cancers was not higher than expected. However, the supplement identified a statistically higher rate of pancreatic cancer in men in Wheat Ridge; the supplement noted that pancreatic cancer has many risk factors associated with it: being overweight or obese, diabetes, a family history of the disease, smoking, and heavy alcohol use. Over 2/3 of these pancreatic cancer cases had a history of smoking and/or alcohol use.\n\nThe Rocky Flats Plant was located south of Boulder, Colorado and northwest of Denver. Originally under management of the Dow Chemical Company, management was transferred to Rockwell in 1975. Initially having an area of , the site was expanded with a buffer zone in 1972.\n\nConstruction of the first buildings was started on the site on July 10, 1951. Production of parts for nuclear weapons began in 1953. At the time, the precise nature of the work at Rocky Flats was a closely guarded secret. The plant produced fission cores for nuclear weapons, used to \"ignite\" fusion and fissionable fuel in all modern nuclear weapons. Fission cores resemble miniaturized versions of the Fat Man nuclear bomb detonated above Nagasaki. They are often referred to as \"triggers\" in official and news documents to obfuscate their function. For much of its operational lifetime, Rocky Flats was the sole mass-producer of plutonium components for America's nuclear stockpile.\n\nManagement of the site passed to EG&G in 1990, which did not reapply for the contract in 1994. Management of the site then passed to the Kaiser-Hill Company as of July 1, 1995. The Department of Energy now manages the central portion of the site, where production buildings were once located, while the Fish and Wildlife Service has taken over management of the Peripheral Outer Unit.\n\nMost of the radioactive contamination from Rocky Flats came from three sources: a catastrophic fire in 1957, leaking barrels in an outdoor storage area in 1964-1968, and another less severe fire in 1969. Plutonium, used to construct the weapons' fissile components, can spontaneously combust at room temperatures in air. Additional sources of actinide contamination include inadequate pondcrete vitrification attempts and routine releases during the decades of plant operations.\n\nOn the evening of September 11, 1957, plutonium shavings in a glove box located in building 771 (the Plutonium Recovery and Fabrication Facility) spontaneously ignited. The fire spread to the flammable glove box materials, including plexiglas windows and rubber gloves. The fire rapidly spread through the interconnected glove boxes and ignited the large bank of High-efficiency particulate air (HEPA) filters located in a plenum downstream. Within minutes the first filters had burned out, allowing plutonium particles to escape from the building exhaust stacks. The building exhaust fans stopped operating due to fire damage at 10:40 PM, which ended the majority of the plutonium release. Fire fighters initially used carbon dioxide fire extinguishers because water can act as a moderator and cause plutonium to go critical. They resorted to water hoses when the dry fire extinguishers proved ineffective.\n\nThe 1957 fire released 11-36 Ci () of plutonium, much of which contaminated off-site areas as microscopic particles entrained in smoke from the fire. Isopleth diagrams from studies show portions of the city of Denver included in the area where surface sampling detected plutonium. The fact that the fire had resulted in significant plutonium contamination of surrounding populated areas remained secret. News reports at the time reported, per the Atomic Energy Commission's briefing, that there was slight risk of light contamination and that no fire fighters had been contaminated. No abnormal radioactivity was reported by the Colorado Public Health Service.\n\nPlutonium milling operations produced large quantities of toxic cutting fluid contaminated with particles of plutonium and uranium. Thousands of 55-gallon drums of the waste were stored outside in an unprotected earthen area called the 903 pad storage area, where they corroded and leaked radionuclides over years into the soil and water. An estimated 5,000 gallons of plutonium-contaminated oil leached into the soil between 1964 and 1967. Portions of this waste, mixed with dust that composed Pad 903, became airborne in the heavy winds of the Front Range and contaminated offsite areas to the south and east.\n\nLeaking storage barrels at Pad 903 released 1.4-15 Ci () of plutonium as airborne dust during the storage and subsequent attempts at cleanup. Much more remains interred under the Pad 903 area, which has been paved over with asphalt.\n\nAnother major fire occurred on May 11, 1969 in building 776/777 (the Plutonium Processing Facility), again starting due to spontaneous combustion of plutonium shavings in a glove box. Fire fighters again resorted to fighting the fire with water after dry extinguishers proved ineffective. Despite recommendations after the 1957 fire, suppression systems were not built into the glove boxes.\n\nWhile the fire bore marked similarities to the 1957 fire, the level of contamination was less severe because the HEPA filters in the exhaust system did not burn through (After the 1957 fire, the filter material was changed from cellulose to nonflammable fiberglass). Had the filters failed or the roof (which sustained heavy fire damage) been breached, the release could have been more severe than the 1957 fire. About of plutonium was in the storage area where the fire occurred, and about total plutonium was in building 776/777.\n\nThe 1969 fire released 13-62 mCi () of plutonium, about 1000th as much as was released in the 1957 fire. The 1969 fire, however, led local health officials to perform independent tests of the area surrounding Rocky Flats to determine the extent of the contamination. This resulted in the first releases of information to the public that populated areas southeast of Rocky Flats had been contaminated.\n\nRockwell workers mixed hazardous and other wastes with concrete to create one-ton solid blocks called pondcrete. These were stored in the open under tarps on asphalt pads. The pondcrete turned out to be weak storage, an outcome that had been predicted by Rockwell's own engineers. Relatively unprotected from the elements, the blocks began to leak and sag. Nitrates, cadmium and low-level radioactive waste began to leach into the ground and run downhill toward Walnut Creek and Woman Creek.\n\nMost of the plutonium from Rocky Flats was oxidized plutonium, which does not readily dissolve in water. A large portion of the plutonium released into the creeks sank to the bottom and is now found in the streambeds of Walnut and Woman Creeks, and on the bottom of local public reservoirs just outside Rocky Flats: Great Western Reservoir, (no longer used for city of Broomfield drinking water consumption as of 1997 but still used for irrigation), and Standley Lake, a drinking water supply for the cities of Westminster, Thornton, Northglenn and some residents of Federal Heights. As one of several forms of remediation and once the extent of the lapses at Rocky Flats became public knowledge, several streams that were formed by drainage through the contaminated areas of the Rocky Flats Plant were diverted such that they would no longer flow directly into some of the local reservoirs, such as Mower Reservoir and Standley Lake. Also, a surface water control system was built to allow runoff from contaminated creeks to collect in holding ponds and thus reduce or prevent direct runoff into Standley Lake. Proposals to remove or breach some of these dams to reduce the cost of maintenance have been protested by the cities downstream.\n\nNo radioactivity warning, advisement or cleanup was provided to the public in the 1957 fire, the worse of the two major fires. At the time of the 1957 fire, AEC officials told the \"Denver Post\" that the fire \"resulted in no spread of radioactive contamination of any consequence.\" The public was not informed of substantial contamination from the 1957 plutonium fire until after the highly visible 1969 fire, when civilian monitoring teams confronted government officials with measurements made outside the plant of radioactive contamination suspected to be from the 1969 fire, which consumed hundreds of pounds of plutonium (850Â kg).\n\nThe 1969 fire raised public awareness of potential hazards posed by the plant and led to years of increasing citizen protests and demands for plant closure. Releases from previous years had not been reported publicly prior to the fire; airborne-become-groundborne radioactive contamination extending well beyond the Rocky Flats plant was not publicly reported until the 1970s.\n\nIn 2002, the U.S. Fish and Wildlife Service surveyed tissues harvested from deer that lived at Rocky Flats for plutonium and other actinides. Isotopes of plutonium, americium, and uranium were detected, with the highest measured activity being 0.0125 pCi/g (2360 seconds per disintegration) for uranium-233 or uranium-234. The increased cancer risk, as reported by the study, to an individual who ate of Rocky Flats deer meat per year over a 70-year lifetime was estimated to be as high as 1 in 210,000. This is near the conservative end of the EPA's acceptable risk range.\n\nPlutonium-239 and 240 emit ionizing radiation in the form of alpha particles. Inhalation is the primary pathway by which plutonium enters the body, though plutonium can also enter the body through a wound. Once inhaled, plutonium increases the risk of lung cancer, liver cancer, bone cancer, and leukemia. Once absorbed into the body, the biological half life of plutonium is about 200 years.\n\nFollowing the public 1969 fire, surveys were taken of the land outside the boundaries of Rocky Flats to quantify the amount of plutonium contamination. Researchers noted that plutonium contamination from the plant was present, but did not match the wind conditions of the 1969 fire. The 1957 fire and leaking barrels on Pad 903 have since been confirmed to be the main sources of plutonium contamination. Authors Krey and Hardy estimated the total quantity of plutonium contamination outside of Rocky Flats's boundaries to be 2.6 Ci (), while Poet and Martell estimated the value to be 6.6 Ci (). The study also noted that plutonium levels just outside the boundaries of the plant were hundreds of times higher than the background level caused by global fallout from nuclear testing, and that contamination to the north of the plant was probably caused by normal operations rather than accidental releases.\n\nFrom September 1947 to April 1969 there were 5 or more accidental surface water releases of tritium. Tritium, a radioactive element which was found in scrap material from Rocky Flats, was therefore directed in to the Great Western Reservoir. This was uncovered in 1973 and following this, urine samples were taken from people living or working near Broomfield who could have drunk water from the reservoir. The findings of the samples showed that those who were exposed to contaminated water had tritium concentrations near seven times higher than normal (4,300 picocuries per liter versus 600 picocuries per liter). However, when the same group underwent urine sampling three years later, their tritium concentrations had returned to the standard.\n\nIn a 1981 study by Dr. Carl Johnson, health director for Jefferson County, showed a 45 percent increase in congenital birth defects in Denver suburbs downwind of Rocky Flats compared to the rest of Colorado. Moreover, he found a 16% increase in cancer rates for those living closest to the plant as compared to those on the outer perimeter of the area, and he estimated 491 excess cancer cases whereas the DOE estimated one. A 1987 study by Crump and others did not find the cancer rates in the northwestern portion of Denver to be significantly higher than other parts of the city and attributed variance in cancer rates to the population density of urban areas. Crump's conclusions were contested by Johnson in a letter to the journal editor. In a 1992 survey of radiation risk analysis, the authors concluded, \"Johnson failed to describe an effective and complete model for the cause of the cancers and its relationship to other knowledge as Crump et al. have done. Therefore, Crump et al.'s explanation must be preferred.\"\n\nIn 1983, Colorado University Medical School professor John C. Cobb and the EPA reported plutonium concentrations from about 500 persons who had died in Colorado. A comparison study was done of those who lived near Rocky Flats with those who lived far from this nuclear weapons production site. The ratio of Pu-240 to Pu-239 was \"minutely lower\" for persons who lived within 50Â km of Rocky Flats, but was more strongly correlated to age, gender, and smoking habits than proximity to the plant.\n\nIn 1991, the Department of Energy's public affairs group published a pamphlet stating that the inhalation of sediments that become resuspended in the air is considered the most significant pathway that could expose human beings to plutonium from the contaminated local reservoirs, but also stated that the airborne plutonium concentrations as measured by downwind air monitors remained below the DOE standard. In a 1999 analysis, it was found that \"the major event contributing the highest individual risk from plutonium released from Rocky Flats was the 1957 fire,\" with wind distribution of plutonium from the 903 Pad Storage Area being the next greatest source of health risk. In this analysis, health risk estimates for off-site humans had a variance of four orders of magnitude, from \"between 2.0 Ã 10 (95th percentile) and 2.2 Ã 10 (5th percentile), with a median risk estimate of 2.3 Ã 10.\" The DOE maintains a list of Rocky Flats epidemiological studies.\n\nIn 1995, a report over 8,000 pages long was released by the Plutonium Working Group Report on Environmental, Safety and Health Vulnerabilities Associated with the Department's Plutonium Storage. This report listed Rocky Flats as having 5 of the 14 most vulnerable facilities based on plutonium environmental, safety, and health vulnerability at all Department Of Energy facilities.\n\nDuring the early 1990s, an independent Health Advisory Panel - appointed by then-Governor Roy Romer - oversaw a series of reports, called the Historical Public Exposure Studies. The 12-member Health Advisory Panel included multiple medical doctors, scientists, PhDs, and local officials. The Rocky Flats Historical Public Exposure Studies involved nine years of research. The Studies had three main objectives: (1) create a public record of plant operations and accidents that contributed to contaminant releases from the Rocky Flats Plant between 1952 and 1989; (2) assess public exposures to contaminants and potential risks from past releases; and, (3) determine the need for future studies. The Studies' research included identification and assessment of chemicals and radioactive materials from past releases; estimates of risk to residents living or working in surrounding communities during the Plant's operation from 1952 to 1989; an evaluation of possible exposure pathways; and, dose assessments for historical releases.\n\nIn 2003, Dr. James Ruttenber led a study on the health effects of plutonium. Conducted by the University of Colorado Health Sciences Center and the Colorado Department of Public Health and Environment, the study concluded that lung cancer is linked to plutonium inhalation. \"We have supporting evidence from other studies that, along with our findings, support the hypothesis that plutonium exposure causes lung cancer,\" Ruttenber said. His group's findings were part of a broader study that tracked 16,303 people who worked at the Rocky Flats plant between 1952 and 1989. Their research also found that these workers were 2.5 times more likely to develop brain tumors than other people.\n\nMany findings linking workers and other cancer development are muddled due to the \"strong healthy worker effect\" (that workers tend to have lower overall death rates than general population because those that are ill or disabled are restricted from working). Also the standard mortality rates for cancers of stomach and rectum were found to be much higher than other studies of nuclear workers which indicates the necessity for further study since inhalation of plutonium can distribute to these areas.\n\nIn February 2006, the Rocky Flats Stewardship Council was formed to address post-closure management of Rocky Flats. The council includes elected officials from nine municipal governments neighboring Rocky Flats and four skilled/experienced organizations and/or individuals. Information about the council is available on their website.\n\nIn 2016, the Colorado Department of Public Health and Environment announced its Cancer Registry was preparing a follow up cancer study to its original 1998 report on cancer incidence in the vicinity of the former Rocky Flats Plant. The original report and 2016 report found no pattern of increased cancers in communities around Rocky Flats. In 2017, a follow-up cancer study was conducted by the Cancer Registry, which specifically found no pattern of increased thyroid or rare cancers in communities around Rocky Flats. \n\nSubsequent to reports of environmental crimes being committed at Rocky Flats, the United States Department of Justice sponsored an FBI raid dubbed \"Operation Desert Glow,\" which began at 9 a.m. on June 6, 1989. The FBI entered the premises under the ruse of providing a terrorist threat briefing, and served its search warrant to Dominick Sanchini, Rockwell International's manager of Rocky Flats.\n\nThe FBI raid led to the formation of Colorado's first special grand jury, the juried testimony of 110 witnesses, reviews of 2,000 exhibits and ultimately a 1992 plea agreement in which Rockwell admitted to 10 federal environmental crimes and agreed to pay $18.5 million in fines out of its own funds. This amount was less than the company had been paid in bonuses for running the plant as determined by the GAO, and yet was also by far the highest hazardous-waste fine ever; four times larger than the previous record. Due to DOE indemnification of its contractors, without some form of settlement being arrived at between the U.S. Justice Department and Rockwell the cost of paying any civil penalties would ultimately have been borne by U.S. taxpayers. While any criminal penalties allotted to Rockwell would not have been covered by U.S. taxpayers, Rockwell claimed that the Department of Energy had specifically exempted them from most environmental laws, including hazardous waste.\n\nAs forewarned by the prosecuting U.S. Attorney, Ken Fimberg (later Ken Scott), the Department of Justice's stated findings and plea agreement with Rockwell were heavily contested by its own, 23-member special grand jury. Press leaks by both members of the DOJ and the grand jury occurred in violation of secrecy Rule 6(e) regarding Grand Jury information. The public contest led to U.S. Congressional oversight committee hearings chaired by Congressman Howard Wolpe, which issued subpoenas to DOJ principals despite several instances of the DOJ's refusal to comply. The hearings, whose findings include that the Justice Department had \"bargained away the truth,\" ultimately still did not fully reveal the special grand jury's report to the public, which remains sealed by the court.\n\nThe special grand jury report was nonetheless leaked to \"Westword\" and excerpts published in its September 29, 1992 issue. According to its subsequent publications, the Rocky Flats special grand jury had compiled indictments charging three DOE officials and five Rockwell employees with environmental crimes. The grand jury also wrote a report, intended for the public's consumption per their charter, lambasting the conduct of DOE and Rocky Flats contractors for \"engaging in a continuing campaign of distraction, deception and dishonesty\" and noted that Rocky Flats, for many years, had discharged pollutants, hazardous materials and radioactive matter into nearby creeks and Broomfield's and Westminster's water supplies.\n\nThe DOE itself, in a study released in December of the year prior to the FBI raid, called Rocky Flats' ground water the single greatest environmental hazard at any of its nuclear facilities. From the grand jury's report: \"The DOE reached this conclusion because the groundwater contamination was so extensive, toxic, and migrating toward the drinking water supplies for the Cities of Broomfield and Westminster, Colorado.\"\n\nA class action lawsuit, \"Cook v. Rockwell International Corp.\", was filed in January 1990 against Rockwell and Dow Chemical (due to the indemnity of nuclear contractors, the award would have been paid by the federal government). Sixteen years later, the plaintiffs were awarded $926 million in economic damages, punitive damages\nIn May 2016, U.S. District Judge John L. Kane gave preliminary approval for a $375 million settlement against the Rockwell International Corp. and Dow Chemical Co. Nearly 26 years later, approximately 13,000 to 15,000 eligible property owners could receive monetary payments for damages and decreased property values. Property and homeowners who owned property on June 7, 1989, the day the FBI raided the plant, are eligible to file a claim for property devaluation. The deadline to file a claim is June 1, 2017.\n\nCarl Johnson sued Jefferson County for unlawful termination, after he was forced to resign from his position as Director of the Jefferson County Health Department. He alleged that his termination was due to concerns by the board members that his reports of contamination would lower property values. The suit was settled out of court for $150,000. \n\nIn May 2018, local activists sued the U.S. Fish & Wildlife Service and filed a motion for a preliminary injunction, asking a federal court to stop the planned opening of Refuge access points. On August 9, 2018, the court denied the activists' motion, explaining that \"plaintiffs failed to meet their burden to show that they will likely suffer irreparable harm.\" In addition, the court later rejected activists' motion to add documents to the administrative record. The court observed that plaintiffs statements in support of this motion were \"conclusory.\" See Civil Action No. 18-cv-01017-PB. The activists had previously sued in 2017. The court dismissed this lawsuit and awarded costs to the U.S. Fish & Wildlife Service. In September 2018, Rocky Flats National Wildlife Refuge opened to the public.\n\nDenver's automotive beltway does not include a component in the northwest sector, partly due to concerns over unremediated plutonium contamination.\n\nAccording to the Rocky Flats National Wildlife Refuge Act of 2001, the land transferred from DOE to the US Fish and Wildlife Service was to be used as a wildlife refuge once Rocky Flats cleanup was complete. In order to help guide the future of Rocky Flats care and management, the Rocky Flats Stewardship Council was formed in 2006 after the US Congress, DOE and previous organization created the new council. \n\nCleanup of Rocky Flats was finished in 2005 and verified by the EPA and CDPHE in 2007 after ten years and almost $7 billion. However, residual contamination below levels of regulatory concern remain. Plutonium 239, with a 24,000 year half life, will persist in the environment hundreds of thousands of years. Volatile organic compounds have a much shorter lifespan. Heavy metals will persist in perpetuity.\n\nIn 2006, according to DOE, \"The selected remedy/corrective action for the Peripheral OU is no action. The RI/FS report (RCRA Facility Investigation-Remedial Investigation/Corrective Measures Study- Feasibility Study) concludes that the Peripheral OU is already in a state protective of human health and the environment.\"\n\nIn 2007, the \"Peripheral Operable Unit\" (Peripheral OU) land area of Rocky Flats was transferred from DOE to FWS for use and preservation as the Rocky Flats National Wildlife Refuge. During the environmental investigation and sampling, it had been determined that levels of residual contamination were so low, no remediation was required; the Refuge land was already in a state suitable for any use. In 2017, a statutorily-required review confirmed the Refuge was suitable for any use, prior to its formal opening. In contrast, the DOE-retained \"Central Operable Unit\" of Rocky Flats remains under DOE control, and is subject to ongoing monitoring and sampling and groundwater treatment. \n\nMultiple assessments of Rocky Flats indicate that the long-term health risk to citizens living outside the boundaries of Rocky Flats is negligible, but citizen organizations feel that the remediation of the site was inadequate, despite the achievement of legal and regulatory requirements. An independent Public Health Assessment, completed by the Agency for Toxic Substances and Disease Registry (ATSDR), concluded that \"the available sampling data, epidemiological studies, exposure investigations and other relevant reports paint a consistent picture of the public health implications of environmental contamination\": \"past, current and future exposures are below levels associated with adverse health effects.\" ATSDR specifically considered children's health when evaluating exposures and their public health implications. Overall, ATSDR did not identify any environmental exposures at levels of public health concern for past and current exposures. Notably, past and current inhalation exposures to site-related air emissions presented no apparent public health hazard.\n\nIn March 2006, the Rocky Flats Stewardship Council was formed to address post-closure management of Rocky Flats and provide a forum for public discussion. This organization was the successor organization to the Rocky Flats Coalition of Local Governments, which advocated for stakeholders during the site cleanup. The Council includes elected officials from nine municipal governments neighboring Rocky Flats and four skilled/experienced organizations and/or individuals. Information and Council meetings minutes and reports are available on its website. Members of the public are welcome to attend Council meetings and make public comments.\n\nIn 2014, the U.S. Fish and Wildlife Services proposed a controlled burn on 701 acres of the Wildlife Refuge. In 2015, they reported that they will postpone those burns until 2017. In 2015, there was a \"soft opening\" of the Rocky Flats Wildlife Refuge where small groups of people could reserve space on a three-mile guided nature walk. The Rocky Flats National Wildlife Refuge opened to the public on September 15, 2018. \n\nIn 2015 Rocky Mountain Downwinders was founded to study health effects in people who lived east of the facility while it was operational. The group set up an online health survey conducted by Metropolitan State University of Denver. Nicolas Hansen, a Denver litigation attorney, founded this group . To date, no final survey report has been published by the Downwinders.\n\nOn the weekend of April 28, 1979, more than 15,000 people demonstrated against the Rocky Flats Nuclear Weapons Plant. The protest was coordinated with other anti-nuclear demonstrations across the country. Daniel Ellsberg and Allen Ginsberg were among the 284 people who were arrested. The demonstration followed more than six months of continuous protests that included an attempted blockade of the railroad tracks leading to the site. Large pro-nuclear counter demonstrations were also staged that year.\n\nIn 1983 the Rocky Mountain Peace and Justice Center was founded with a goal of closing the Rocky Flats plant. The Center has since set goals of keeping the Rocky Flats National Wildlife Refuge closed to the public, preventing construction of highways in or near the site of the former plant, and preventing new housing construction in the area. The Center is a 501(c)(3) non-profit corporation an, as of 2014, has one full-time employee.\n\nOn October 15, 1983, about 10,000 demonstrators turned out for protest at the Rocky Flats Nuclear Weapons Plant (well short of the 21,000 hoped for by protest organizers). No arrests were made. On August 10, 1987 (the 42nd anniversary of the atomic bombing of Nagasaki), 320 demonstrators were arrested after they tried to force a one-day shutdown of the plant. A similar protest with a turnout of about 3,500 was staged on August 6, 1989 (the anniversary of the nuclear bombing of Hiroshima).\n\nThough public demonstrations against plant operations ceased with the decommissioning of the plant, activists continue to protest disposal of nuclear waste from the site and the scale and scope of cleanup operations. Since 2013, opposition has focused on the Candelas development located along the southern border of the former Plant site.\n\nWith the establishment of the Rocky Flats National Wildlife Refuge Act in 2001, a 300Â ft strip on the eastern edge of the refuge was allocated to Jefferson County for construction of the Jefferson County Parkway. In May 2008, the Jefferson Parkway Public Highway Authority was established to complete this last portion of the Denver metro beltway. Opponents of the parkway are concerned about the disruption of plutonium laden soil from excavation of the area to build the parkway. In April 2015, the WestConnect Corridor Coalition was formed with the hopes of bringing about the end of a decades long dispute to the completion of the Jefferson County Parkway. However, by October 2015, the WestConnect Corridor had withdrawn its support from the parkway, determining that the decision to build the parkway should be made outside of the coalition's process. As of 2015, the Jefferson Parkway Public Authority was still searching for private and public funding to complete the beltway.\n\nA group named Candelas Glows is opposed to a large housing and commercial development planned in the area, which the group calls a \"plutonium dust bowl.\" The Department of Energy responded by saying that studies show more risk from naturally occurring radioactive elements than from very low-level amounts of plutonium remaining around the former plant. Candelas Glows argued that a July 2015 radiation report from the Rocky Flats Stewardship Council shows plutonium levels at 1.02 pCi/L, compared to the regulatory standard of 0.15 pCi/L.\n\n\n"}
{"id": "4512595", "url": "https://en.wikipedia.org/wiki?curid=4512595", "title": "Rocking stone", "text": "Rocking stone\n\nRocking stones (also known as logan stones or logans) are large stones that are so finely balanced that the application of just a small force causes them to rock. Typically, rocking stones are residual corestones formed initially by spheroidal weathering and have later been exposed by erosion or glacial erratics left by retreating glaciers. Natural rocking stones are found throughout the world. A few rocking stones might be man-made megaliths.\n\nThe word \"logan\" is probably derived from the word \"log\", which in an English dialect means to rock. In fact, in some parts of the UK, rocking stones or logan stones are called logging stones. The word \"log\" might be connected with the Danish word \"logre\", which means to \"wag a tail\".\n\nSome have suggested that the word \"logan\" comes from a Cornish expression for the movement that someone makes when inebriated. Davies Gilbert writes:\nSuch stones are common in Britain and other places around the world. For example, in Galicia, rocking stones are called \"pedras de abalar\".\n\nPliny the Elder (23â79) wrote about a rock near Harpasa (in Caria, Asia Minor) \"that can be moved with one finger, but that also resists a push made with the whole body.\" Ptolemy (\"circa\" 90â168) wrote about the Gygonian rock, which he wrote \"can only be moved with an asphodel and remains immovable by force.\"\n\nThere are stones in Iona called \"na clachan-brÃ th\", within the precincts of a burial ground, and placed on the pedestal of a cross, and have been according to Pennant, the supports of a tomb.\nA massive 90- to 95-ton glacial erratic boulder near Halifax, Nova Scotia, can still be rocked with a lever, but used to move quite easily, before a band of sailors from the nearby Halifax garrison rocked it into a more stable configuration in the 1890s, and before its base was worn down by excessive rocking in the 1980s and '90s when a park was developed around it at Kidston Lake, in the Spryfield area of the municipality. It used to be a popular picnic destination; in Victorian times, people would travel from Halifax, climb upon it and spread their lunches, while enjoying the sensation of rocking gently while seated upon the huge rock.\n\nThe Pontypridd Rocking Stone in Wales is set within the middle of a Druidic stone circle.\nBosistow Logan Rock is at the head of Pendower Cove (sometimes written as Pendour Cove) near Zennor, Cornwall. It apparently was discovered by an employee of the lord of the local manor whose duty it was to watch the coast. A ship had been wrecked in the cove, and while watching ensuing activity, the employee leaned against a boulder. Suddenly, a gust of wind occurred, and the boulder shifted, or \"logged\". The longest side of this mass of stone is about , and the circumference of its biggest end is about . It is thought to weigh about 20 tons.\n\nA rocking stone is recorded near the site of Saint Bride's Chapel. The Witch's or Boarstone stands on top of the Craigs of Kyle near Coylton in Ayrshire. It weighs around 30 tons and rests upon two or three stones. A large standing stone known as Wallace's stone is recorded to have stood nearby.\n\nA rocking stone is found near Loch Riecawr in South Ayrshire.\n\nIn the parish of North Carrick in the Straiton District in South Ayrshire, about a quarter of a mile to the west of the White Laise, and near the March Dyke, a rocking stone named the Logan Stone exists. The Logan Stone is a grey granite rock and rests on greywacke, and can easily be moved with one hand. It is by , by high.\n\nThere is a famous pair of rocking stones on the Faroese island of Eysturoy in the village of OyndarfjÃ¸rÃ°ur. These are known as the Rinkusteinar. Local legend states that an old sorceress cursed two pirate ships that were menacing the village, turning them to stone. A chain connected to the mainland makes it easier to see the rockâs movements.\nNear Lugar in the Parish of Auchinleck in Ayrshire, Scotland, is the Lamargle or Lamargee rocking stone in a hollow by the Bello or Bellow Water near its junction with the Glenmore Water from which point the name Lugar Water is applied. It is made of two vertical stones, and a horizontal stone about 6 ft long, 3 ft broad, and 4 ft high. It was regarded as a Druidic monument or the grave of a Caledonian hero. This stone has often been overlooked, as the OS maps give the wrong location; it sits beside the Bellow Water above its confluence with the Glenmore Water and not on Lamargle Hill.\n\nThe Kyaiktiyo Pagoda in Burma is a religious shrine built on top of a huge granite boulder that is also a rocking stone.\n\nSome masses shaped by humans also exhibit similar behaviour (sometimes unintentionally). For example, in the ruins of the Roman temples at Jerash in Jordan (the \"city of 1000 pillars\"), some massive pillars move back and forth in the slightest breeze.\n\nA stone used to rock on a gritstone outcrop on Warley Moor near Halifax in West Yorkshire. It had already ceased to rock when described by John Watson in 1775.\n\nAyrshire in Southwest Scotland apparently is endowed with a geology that lends itself towards the formation of rocking stones. Several rocking stones, or stones that used to rock at one time, are there.\n\nA rocking stone that some associate with the Druids is on Cuff Hill in Hessilhead, near Beith in North Ayrshire. It no longer rocks due to people digging beneath to ascertain its fulcrum. It is in a small wood and surrounded by a circular drystone wall. An article was published in the \"Cumnock Chronicle\" in 1907 on the reason for the stone being dislodged.\n\nThe Ogrestone or Thurgartstone near Dunlop in East Ayrshire is thought to have been a rocking stone. However, soil has built up around the base of the Thurgatstone over the years, which now prevents any rocking motion.\n\nA rocking stone existed in 1913â1919 at Sannox on Arran, on a nearly horizontal platform next to the seashore.\n\nThe Lamagee or Lamargle stone in Lugar as previously mentioned is in the center of a loose collection of stones in the village of Lugar in East Ayrshire. The Lamargle stone rests on two stones. Local legend has it that the Lamargle stone used to rock, but it no longer does. The Witch's Stone or Boarstone on the Craigs o'Kyle is recorded by the author John Smith as having rocked.\nThe Clochoderick Stone near Howwood and Kilbarchan in Renfrewshire used to rock, and it is said that the Druids used it to judge people. The accused was made to sit on the stone and by the way it moved, the Druids judged the innocence or guilt of the individual. It is also said to be the burial place of Rhydderch Hael, King of Strathclyde, who was the victor at the Battle of Arderydd near Arthuret in the Borders. His victory brought Christianity to Strathclyde. This stone is very unusual and is a SSSI for Geology in its own right.\n\nOften wear, erosion, or human intervention has resulted in the dislodging of rocking stones.\n\nA well-known rocking stone or logan stone was located at Sharpitor near Lustleigh on Dartmoor. It was also called the Nutcrackers Stone, sometimes seen on Ordnance Survey maps. The huge stone once lay overhanging Lustleigh Cleave until 1951, when vandals pushed the stone down the valley. A failed attempt to rescue the stone by pulling it back up the valley resulted in it breaking into pieces.\n\nAround 1900 there were rocking stones at Rippon tor, and Sittaford tor on Dartmoor.\n\nA 150-ton glacial erratic occurs on Rockingstone Avenue in Larchmont, New York, that was so perfectly balanced that just a small touch would allow it to rock back and forth. Unfortunately in the 1920s, blasting for a new sewer system in the neighbourhood dislodged the rock, so it no longer balances.\n\nA rocking stone in Pembrokeshire is described in Gibsonâs edition of Camden's \"Britannia\", from a manuscript account by George Owen:\n\nCromwellâs soldiers rendered the rocking stone of Pembrokeshire immovable after Mr. Owen had described it. They reportedly destroyed it because they felt it encouraged superstition.\n\nAnother rocking stone was at Golcar Hill, near Halifax in Yorkshire. However, the Golcar Hill rocking stone will no longer easily rock because some masons wanted to find out how such a large weight could move so easily, so they chopped at it until they destroyed its balance.\n\nA very sensitive rocking stone called Men Amber (sometimes written as Men-Amber or Menamber) was on a high ridge in the parish of Sithney, near Pendennis, Cornwall. It is long, deep, and wide. It was toppled by Shrubsall, the governor of Pendennis, and his men about 1650 during Cromwellâs Commonwealth. One rumoured motivation for the dislodging was a purported prophecy of Merlin, who supposedly said that Men Amber would stand until England had no king.\n\nRev. Dr. William Stukeley wrote:\n\nWilliam Borlase in his 1754 book \"Antiquities of Cornwall\", claimed that Men Amber was dislodged because:\n\nAnother well-known example of a rocking or logan stone is Logan Rock of Treen in Cornwall. This huge stone weighs about 80 or 90 tons. It is one of the best-known rocking stones for several reasons. For example, Modred, in William Mason's dramatic poem \"Caractacus\" addressing the characters Vellinus and Elidurus, says of the Logan Rock:\n\nHowever, another reason that the Logan Rock of Treen is remembered is that it was the center of a famous drama. In April 1824, Lieutenant Hugh Goldsmith, R. N. (nephew of the famous poet Oliver Goldsmith), and 10 or 12 of his crew of the cutter HMS \"Nimble\", armed with bars and levers, rocked the huge granite boulder until it fell from its cliff-top perch. Goldsmith was apparently motivated to disprove the claim of Dr. Borlase, who wrote in \"Antiquities of Cornwall\" in 1754 that:\n\nGoldsmith was determined to demonstrate that nothing was impossible when the courage and skill of British seamen were engaged. The Logan Rock fell and was caught in a narrow chasm.\n\nThis upset the local residents considerably, since Logan Rock had been used to draw tourists to the area. Sir Richard Vyvyan (1800â1879) was particularly unhappy. They demanded that the British Admiralty strip Lieutenant Goldsmith of his Royal Navy commission unless he restored the boulder to its previous position at his own expense. However, Mr. Davies Gilbert persuaded the Lords of the Admiralty to lend Lieutenant Goldsmith the required apparatus for replacing the Logan Rock. The Admiralty sent 13 captains with blocks and chains from the dock yard at Plymouth, and contributed Â£25 towards expenses. Gilbert also raised more funds.\n\nAfter months of effort, at 4:20 pm on 2 November 1824, in front of thousands of spectators and with the help of more than 60 men and block and tackle, the Logan Rock was finally repositioned and returned to \"rocking condition\". Apparently, the total final cost of this enterprise was Â£130 8s 6d. However, it is not clear how much of the remaining Â£105 Goldsmith had to make up out of his own pocket.\n\nFor some time after, the rock was kept chained and padlocked, but eventually these restrictions were removed, and the rock was set free. However, it apparently no longer vibrates or \"logs\" as easily as it did before.\n\nTourism dropped, and this was blamed on the condition of Logan Rock. For a while, Treen was nicknamed 'Goldsmith's Deserted Village'.\n\nAnother famous rock structure, Lanyon Cromlech, was knocked down during a thunderstorm in 1815. The same machinery that was used to restore the Logan Rock in Treen was successful in repositioning Lanyon Cromlech.\n\nA wide variety of beliefs are associated with rocking stones. Because of their strange nature, rocking stones were sometimes associated with witchcraft, or Druids.\n\nThe rocking stone near Nancledrea in Cornwall was said to only move at midnight when witches were out. People claimed that if one touched the rocking stone nine times at midnight, one would turn into a witch.\n\nThe Brimham rocking stone in Yorkshire is said to rock only for the efforts of an honest man.\n\nThe rocking stone at Land's End was said to have been placed there by a giant who used it to rock himself to sleep.\n\nIt was claimed that the Logan Stone in Treen could cure childhood diseases. The children were rocked on the Logan Stone in certain seasons. People say that the charm was broken when Lieutenant Goldsmith dislodged the Logan Stone.\n\nIt is a Cornish tradition to make a vow and then attempt to move a rocking stone, or logan rock. It was said that no persons with treachery in their hearts could make a rocking stone move.\n\n\n\n"}
{"id": "275701", "url": "https://en.wikipedia.org/wiki?curid=275701", "title": "Scaphe", "text": "Scaphe\n\nThe Scaphe () was a sundial said to have been invented by Aristarchus (3rd century BC). There are no original works still in existence by Aristarchus, but the adjacent image is an accurate image of what it might have looked like, only his would have been made of stone. It consisted of a hemispherical bowl which had a vertical gnomon placed inside it, with the top of the gnomon level with the edge of the bowl. Twelve gradations inscribed perpendicular to the hemisphere indicated the hour of the day. Using this measuring instrument, Eratosthenes of Cyrene (c. 220 BC) measured the length of Earth's meridian arc.\nThe scaphe is also known as a skaphe, scaphion (diminutive) or .\n\nAristarchus of Samos (; , \"Aristarkhos\"; c. 310 â c. 230 BC) was an ancient Greek astronomer and mathematician who presented the first known model that placed the Sun at the center of the known universe with the Earth revolving around it (see Solar system). He was influenced by Philolaus of Croton, but he identified the \"central fire\" with the Sun, and put the other planets in their correct order of distance around the Sun. His astronomical ideas were often rejected in favor of the geocentric theories of Aristotle and Ptolemy.\n\nGreeks and Romans used large stone sundials based on \"a partial sphere or scaphe,â the shadow of the tip of the gnomon was the time-telling index. These dials could in theory tell time accurately if carved to a true sphere and correctly calibrated for a given site.\nIt took a skilled stone worker and a great deal of time and money to create a sundial. So only wealthy citizens could afford this elaborate contraption, and it was often for their villas or as donations for erection in the town forum.\nThere was a need for cheaper dials that ordinary laborer could construct. But even if it were easier to make the question of calibrating the Scaphe still posed a problem.\n\nThe problem of projecting the three- dimensional scaphe dial upon a vertical or horizontal plane, was addressed by a number of distinguished nineteenth-century mathematicians, each of whom presumably solved it to his own personal satisfaction. Unfortunately, their publications are so complex, long-winded, and obscure that they were virtually inaccessible to antiquaries andâit would appear from the replication of effort----even to their own (unacknowledged) colleagues. The spherical trigonometry required for the latter endeavor is really quite basic, and the calculations tedious rather than difficult. The availability of computer-generated graphics has, of course, completely altered the situation. A Fortran program was written for the VAX computer at the University of Leicester that enabled vertical or horizontal dials to be plotted for any latitude.\n\nMade famous by Eratosthenes of Cyrene, he calculated the circumference of the Earth without leaving Egypt. Eratosthenes knew that at local noon on the summer solstice in the Ancient Egyptian city of Swenet (known in ancient Greek as Syene, and now as Aswan) on the Tropic of Cancer, the sun would appear at the zenith, directly overhead. He knew this because he had been told that the shadow of someone looking down a deep well in Syene would block the reflection of the Sun at noon off the water at the bottom of the well. Using a gnomon, he measured the sun's angle of elevation at noon on the solstice in Alexandria, and found it to be 1/50th of a circle (7Â°12') south of the zenith. He may have used a compass to measure the angle of the shadow cast by the sun. Assuming that the Earth was spherical, and that Alexandria was due north of Syene, he concluded that the meridian arc distance from Alexandria to Syene must therefore be 1/50th of a circle's circumference, or 7Â°12'/360Â°.\n\nHis knowledge of the size of Egypt after many generations of surveying trips for the Pharaonic bookkeepers gave a distance between Alexandria and Syene of 5,000 stadia. This distance was corroborated by inquiring about the time that it took to travel from Syene to Alexandria by camel. He rounded the result to a final value of 700 stadia per degree, which implies a circumference of 252,000 stadia. Some claim Eratosthenes used the Egyptian stade of 157.5 meters, which would imply a circumference of 39,690Â km, an error of 1.6%, but the 185 meter Attic stade is the most commonly accepted value for the length of the stade used by Eratosthenes in his measurements of the Earth, which implies a circumference of 46,620Â km, an error of 16.3%.\n\n\n"}
{"id": "5491283", "url": "https://en.wikipedia.org/wiki?curid=5491283", "title": "Sealant", "text": "Sealant\n\nSealant is a substance used to block the passage of fluids through the surface or joints or openings in materials, a type of mechanical seal. In building construction \"sealant\" is sometimes synonymous with \"caulking\" and also serve the purposes of blocking dust, sound and heat transmission. Sealants may be weak or strong, flexible or rigid, permanent or temporary. Sealants are not adhesives but some have adhesive qualities and are called \"adhesive-sealants\" or \"structural sealants\".\n\nSealants were first used in prehistory in the broadest sense as mud, grass and reeds to seal dwellings from the weather such as the daub in wattle and daub and thatching. Natural sealants and adhesive-sealants included plant resins such as pine pitch and birch pitch, bitumen, wax, tar, natural gum, clay (mud) mortar, lime mortar, lead, blood and egg. In the 17th century glazing putty was first used to seal window glass made with linseed oil and chalk, later other drying oils were also used to make oil-based putties which were often referred to as \"caulks\". In the 1920s polymers such as acrylic polymers, butyl polymers and silicone polymers were first developed and used in sealants. By the 1960s synthetic-polymer-based sealants were widely available.\n\nSealants, despite not having great strength, convey a number of properties. They seal top structures to the substrate, and are particularly effective in waterproofing processes by keeping moisture out (or in) the components in which they are used. They can provide thermal and acoustical insulation, and may serve as fire barriers. They may have electrical properties, as well. Sealants can also be used for simple smoothing or filling. They are often called upon to perform several of these functions at once.\n\nA caulking sealant has three basic functions: It fills a gap between two or more substrates; it forms a barrier through the physical properties of the sealant itself and by adhesion to the substrate; and, it maintains sealing properties for the expected lifetime, service conditions, and environments. The sealant performs these functions by way of correct formulation to achieve specific application and performance properties. Other than adhesives, however, there are few functional alternatives to the sealing process. Soldering or welding can perhaps be used as alternatives in certain instances, depending on the substrates and the relative movement that the substrates will see in service. However, the simplicity and reliability offered by organic elastomers usually make them the clear choice for performing these functions.\n\nA sealant may be viscous material that has little or no flow characteristics and which stay where they are applied; or they can be thin and runny so as to allow it to penetrate the substrate by means of capillary action. Anaerobic acrylic sealants (generally referred to as impregnants) are the most desirable, as they are required to cure in the absence of air, unlike surface sealants that require air as part of the cure mechanism that changes state to become solid, once applied, and is used to prevent the penetration of air, gas, noise, dust, fire, smoke, or liquid from one location through a barrier into another. Typically, sealants are used to close small openings that are difficult to shut with other materials, such as concrete, drywall, etc. Desirable properties of sealants include insolubility, corrosion resistance, and adhesion. Uses of sealants vary widely and sealants are used in many industries, for example, construction, automotive and aerospace industries.\nSealants can be categorized in accordance with varying criteria, e. g. in accordance with the reactivity of the product in the ready-to-use condition or on the basis of its mechanical behavior after installation.\nOften the intended use or the chemical basis is used to classify sealants, too. A typical classification system for most commonly used sealants is shown below.\nTypes of sealants fall between the higher-strength, adhesive-derived sealers and coatings at one end, and extremely low-strength putties, waxes, and caulks at the other. Putties and caulks serve only one function â i.e., to take up space and fill voids. Silicone is an example of a sealant - and has a proven long life and is unaffected by UV or extremes of weather or temperature.\n\nSee below for other common types of sealants - \n\nThe main difference between adhesives and sealants is that sealants typically have lower strength and higher elongation than adhesives do. When sealants are used between substrates having different thermal coefficients of expansion or differing elongation under stress, they need to have adequate flexibility and elongation. Sealants generally contain inert filler material and are usually formulated with an elastomer to give the required flexibility and elongation. They usually have a paste consistency to allow filling of gaps between substrates. Low shrinkage after application is often required. Many adhesive technologies can be formulated into sealants.\n"}
{"id": "5594272", "url": "https://en.wikipedia.org/wiki?curid=5594272", "title": "Slush", "text": "Slush\n\nSlush, also called slush ice, is a slurry mixture of small ice crystals (e.g., snow) and liquid water.\n\nIn the natural environment, slush forms when ice or snow melts. This often mixes with dirt and other materials, resulting in a gray or muddy brown color. Often, solid ice or snow can block the drainage of fluid water from slushy areas, so slush often goes through multiple freeze/thaw cycles before completely disappearing.\n\nIn areas where road salt is used to clear roadways, slush forms at lower temperatures than it would ordinarily in salted areas. This can produce a number of different consistencies over the same geographical area.\n\nSlushfall or slushing is the action of a wet snow falling from the sky.\n\nBecause slush behaves like a Non-Newtonian fluid, which means it behaves like a solid mass until its inner shear forces rise beyond a specific threshold and beyond can very suddenly become fluid, it is very difficult to predict its behavior. This is the underlying mechanism of Avalanches and their unpredictability and thus hidden potential to become a natural hazard.\n\nSlush can also be a problem on an aircraft runway since the effect of excess slush acting on the aircraft's wheels can have a resisting effect during take off, which can cause an accident such as the Munich air disaster. Slush on roads can also increase the braking distances for cars and trucks, increasing the possibility of rear end crashes and other accidents.\n\nSlush refreezing in overnight frost can turn to dangerous slippery ice underfoot.\n\nIn some cases though, slush can be beneficial. When snow hits the slush, it melts on contact. This prevents roads from becoming too congested with snow or sleet.\n"}
{"id": "14140594", "url": "https://en.wikipedia.org/wiki?curid=14140594", "title": "Solar car", "text": "Solar car\n\nA solar car is a solar vehicle used for land transport. Solar cars usually run on only power from the sun, although some models will supplement that power using a battery, or use solar panels to recharge batteries or run auxiliary systems for a car that mainly uses battery power.\n\nSolar cars combine technology typically used in the aerospace, bicycle, alternative energy and automotive industries. The design of a solar vehicle is severely limited by the amount of energy input into the car. Most solar cars have been built for the purpose of solar car races. Some prototypes have been designed for public use, although no cars primarily powered by the sun are available commercially.\n\nSolar cars depend on a solar array that uses photovoltaic cells (PV cells) to convert sunlight into electricity. Unlike solar thermal energy which converts solar energy to heat, PV cells directly convert sunlight into electricity. When sunlight (photons) strike PV cells, they excite electrons and allow them to flow, creating an electric current. PV cells are made of semiconductor materials such as silicon and alloys of indium, gallium and nitrogen. Crystalline silicon is the most common material used and has an efficiency rate of 15-20%. \n\nThe first model solar car invented was a tiny 15-inch vehicle created by General Motors employee, William G. Cobb. Called it the Sunmobile, he displayed it in 1955 at the Chicago, Powerama convention. It was made up of 12 selenium photovoltaic cells and a small electric motor. \n\nThe solar array consists of hundreds of solar cells converting sunlight into electricity. In order to construct an array, PV cells are placed together to form modules which are placed together to form an array. The larger arrays in use can produce over 2 kilowatts (2.6Â hp).\n\nThe solar array can be mounted in six ways: \n\nThe choice of solar array geometry involves an optimization between power output, aerodynamic resistance and vehicle mass, as well as practical considerations. For example, a free horizontal canopy gives 2-3 times the surface area of a vehicle with integrated cells but offers better cooling of the cells and shading of the riders. There are also thin flexible solar arrays in development.\n\nSolar arrays on solar cars are mounted and encapsulated very differently from stationary solar arrays. Solar arrays on solar cars are usually mounted using industrial grade double-sided adhesive tape right onto the car's body. The arrays are encapsulated using thin layers of Tedlar.\n\nSome solar cars use gallium arsenide solar cells, with efficiencies around thirty percent. Other solar cars use silicon solar cells, with efficiencies around twenty percent.\n\nThe battery pack in a typical solar car is sufficient to allow the car to go 250 miles (400Â km) without sun, and allow the car to continuously travel at speeds of .\n\nThe motors used in solar cars typically about 2 or 3 horsepower, yet experimental light solar cars may attain the same speed as a typical family car ().\n\nTo keep the car running smoothly, the driver must monitor multiple gauges to spot possible problems. Cars without gauges almost always feature wireless telemetry, which allows the driver's team to monitor the car's energy consumption, solar energy capture and other parameters and thereby freeing the driver to concentrate on driving.\n\nTwo solar car races are the World Solar Challenge and the American Solar Challenge, overland road rally-style competitions contested by a variety of university and corporate teams.\n\nThe World Solar Challenge features a field of competitors from around the world who race to cross the Australian continent, over a distance of . Speeds of the vehicles have steadily increased. So, for example, the high speeds of 2005 race participants led to the rules being changed for solar cars starting in the 2007 race and 2014 also.\n\nThe American Solar Challenge, previously known as the 'North American Solar Challenge' and 'Sunrayce USA', features mostly collegiate teams racing in timed intervals in the United States and Canada. This race also changed rules for the most recent race due to teams reaching the regulated speed limits. The most recent American Solar Challenge took place on July 21â28, 2014 from Austin, Texas to Minneapolis, Minnesota.\n\nThe Dell-Winston School Solar Car Challenge is an annual solar-powered car race for high school students. The event attracts teams from around the world, but mostly from American high schools. The race was first held in 1995. Each event is the end product of a two-year education cycle launched by the Winston Solar Car Team. In odd-numbered years, the race is a road course that starts at the Dell Diamond in Round Rock, Texas; the end of the course varies from year to year. In even-numbered years, the race is a track race around the Texas Motor Speedway. Dell has sponsored the event since 2002.\nThe South African Solar Challenge is an epic, bi-annual, two-week race of solar-powered cars through the length and breadth of South Africa. Teams will have to build their own cars, design their own engineering systems and race those same machines through the most demanding terrain that solar cars have ever seen. The 2008 race proved that this event can attract the interest of the public, and that it has the necessary international backing from the FIA. Late in September, all entrants will take off from Pretoria and make their way to Cape Town via the N1, then drive along the coast to Durban, before climbing the escarpment on their way back to the finish line in Pretoria 10 days later. In 2008 the event was endorsed by International Solarcar Federation (ISF), FÃ©dÃ©ration Internationale de l'Automobile (FIA), World Wildlife Fund (WWF) making it the first Solar Race to receive endorsement from these 3 organizations.\n\nThere are other distance races, such as Suzuka, Phaethon, WGC (WSR/JISFC/WSBR) and the World Solar Rally in Taiwan. Suzuka and WGC is a yearly track race in Japan and Phaethon was part of the Cultural Olympiad in Greece right before the 2004 Olympics.\n\nGuinness World Records recognize a land speed record for vehicles powered only by solar panels. This record is currently held by the Sky Ace TIGA from the Ashiya University. The record of 91.332Â km/h (56.75Â mph) was set on 20 August 2014 at the Shimojishima Airport, in Miyakojima, Okinawa, Japan. The previous record was held by the University of New South Wales with the car Sunswift IV. Its battery was removed so the vehicle was powered only by its solar panels. The record of was set on 7 January 2011 at the naval air base in Nowra, breaking the record previously held by the General Motors car Sunraycer of . The record takes place over a flying stretch, and is the average of two runs in opposite directions.\n\nThe first solar family car was built in 2013. Researchers at Case Western Reserve University, have also developed a better solar car which can recharge more quickly, due to better materials used in the solar panels.\n\nChinese solar panel manufacturer Hanergy plans to build and sell solar cars equipped with lithium-ion batteries to consumers in China. Hanergy says that five to six hours of sunlight should allow the carsâ thin-film solar cells to generate between 8-10Â kWh of power a day, allowing the car to travel about 80Â km (50Â mi) on solar power alone. Maximum range is about 350Â km (217Â mi).\n\n"}
{"id": "14617609", "url": "https://en.wikipedia.org/wiki?curid=14617609", "title": "South-West Indian Ocean tropical cyclone", "text": "South-West Indian Ocean tropical cyclone\n\nIn the south-west Indian Ocean, tropical cyclones form south of the equator and west of 90Â°Â E to the coast of Africa.\n\nEach year, the MÃ©tÃ©o-France office (MFR) based on RÃ©union island issues warnings on tropical cyclones within the basin, which is defined as the waters of the Indian Ocean from the coast of Africa to 90Â°Â E, south of the equator. The agency issues the warnings as part of its role as a Regional Specialized Meteorological Center, designated as such in 1993 by the World Meteorological Organization. Intensities are estimated through the Dvorak technique, which utilizes images from satellites by the American National Oceanic and Atmospheric Administration.\n\nThe Joint Typhoon Warning Center â a joint United States NavyÂ â United States Air Force task force â also issues tropical cyclone warnings for the region. Wind estimates from MÃ©tÃ©o-France and most other basins throughout the world are sustained over 10Â minutes, while estimates from the United States-based Joint Typhoon Warning Center are sustained over 1Â minute. 1-minute winds are about 1.12 times the amount of 10-minute winds.\n\nIf a tropical storm in the basin strengthens to attain 10Â minute sustained winds of at least 118Â km/h (74Â mph), the MFR classifies it as a tropical cyclone, equivalent to a hurricane or typhoon (a use of \"tropical cyclone\" which is more restrictive than the usual definition).\n\nThe first storm in the database for MFR originated on JanuaryÂ 11, 1848. In JanuaryÂ 1960, the first named storm was Alex, and each subsequent year had a list of storm names. Beginning in 1967, satellites helped locate cyclones in the basin, and in the following year, the MFR began estimating storm intensities from the satellite images. By 1977, the agency was using the Dvorak technique on an unofficial basis, but officially adopted it in 1981. Originally, the basin only extended to 80Â°Â E, and while it was extended eastward to the current 90Â°Â E, a lack of satellite imagery initially made data uncertain east of 80Â°Â E. The World Meteorological Organization designated the MFR as a Regional Tropical Cyclones Advisory Centre in 1988, and upgraded it to a Regional Specialized Meteorological Center in 1993. In May 1998, two Europe-based Meteosat satellites began providing complete coverage of the basin. On JulyÂ 1, 2002, the MFR shifted the cyclone year to begin on this date and end on JuneÂ 30 of the following year; previously, the cyclone year began on AugustÂ 1 and ended on the subsequent JulyÂ 31. In 2003, the MFR extended their area of warning responsibility to 40Â°Â S, having previously been limited at 30Â°Â S. During 2011, MFR started a reanalysis project of all tropical systems between 1978â98, with methods such as a Dvorak technique reanalysis and use of microwave imagery. Preliminary results from this reanalysis project include correcting an increasing trend in the number of very intense tropical cyclones in the basin since 1978. This also revealed a seemingly systematic underestimation of tropical cyclone intensities in the past.\n\nFrom the 1980â81 to the 2010â11 season, there was an average of 9.3Â tropical storms each year in the basin. A tropical storm has 10Â minute winds of at least 65Â km/h (40Â mph). There are an average of five storms that become tropical cyclones, which have 10\nÂ minute winds of at least 120Â km/h (75Â mph). As of 2002, there was an average of 54Â days when tropical systems were active in the basin, of which 20 had tropical cyclones active, or a system with winds of over 120Â km/h (75Â mph). The median start date for the season was NovemberÂ 17, and the median end date was AprilÂ 20.\n\nGenerally, the monsoon does not cross into the Mozambique Channel until December; as a result, storms rarely form there before that time. From 1948 to 2010, 94Â tropical systems developed in the small body of water, of which about half made landfall. Occasionally, small storms form in the Mozambique Channel that resemble Mediterranean tropical cyclones or storms in the northeastern Atlantic Ocean; these systems are well-organized but have weaker convection than typical tropical cyclones, and originate over cooler than normal water temperatures of less than . A survey in 2004 conducted by weather expert Gary Padgett found meteorologists split whether these storms should be classified as tropical or subtropical.\n\nIn an average year, ten tropical depressions or storms strike Madagascar, and most generally do not cause much damage. Occasionally, storms or their remnants enter the interior of southeastern Africa, bringing heavy rainfall to Zimbabwe.\n\n\n"}
{"id": "5730974", "url": "https://en.wikipedia.org/wiki?curid=5730974", "title": "Stability derivatives", "text": "Stability derivatives\n\nStability derivatives, and also control derivatives, are measures of how particular forces and moments on an aircraft change as other parameters related to stability change (parameters such as airspeed, altitude, angle of attack, etc.). For a defined \"trim\" flight condition, changes and oscillations occur in these parameters. \"Equations of motion\" are used to analyze these changes and oscillations. Stability and control derivatives are used to linearize (simplify) these equations of motion so the stability of the vehicle can be more readily analyzed.\n\nStability and control derivatives change as flight conditions change. The collection of stability and control derivatives as they change over a range of flight conditions is called an aero model. Aero models are used in engineering flight simulators to analyze stability, and in real-time flight simulators for training and entertainment.\n\n\"Stability\" derivatives and \"control\" derivatives are related because they both are measures of forces and moments on a vehicle as other parameters change. Often the words are used together and abbreviated in the term \"S&C derivatives\". They differ in that stability derivatives measure the effects of changes in flight conditions while control derivatives measure effects of changes in the control surface positions:\n\n\nStability and control derivatives change as flight conditions change. That is, the forces and moments on the vehicle are seldom simple (linear) functions of its states. Because of this, the dynamics of atmospheric flight vehicles can be difficult to analyze. The following are two methods used to tackle this complexity.\n\n\n\nIn addition to engineering simulators, aero models are often used in \"real time flight simulators\" for home use and professional flight training.\n\nAir vehicles use a coordinate system of axes to help name important parameters used in the analysis of stability. All the axes run through the center of gravity (called the \"CG\"):\n\n\nTwo slightly different alignments of these axes are used depending on the situation: \"body-fixed axes\", and \"stability axes\".\n\nBody-fixed axes, or \"body axes\", are defined and fixed relative to the body of the vehicle.:\n\n\nAircraft (usually not missiles) operate at a nominally constant \"trim\" angle of attack. The angle of the nose (the X Axis) does not align with the direction of the oncoming air. The difference in these directions \"is\" the \"angle of attack\". So, for many purposes, parameters are defined in terms of a slightly modified axis system called \"stability axes\". The stability axis system is used to get the X axis aligned with the oncoming flow direction. Essentially, the body axis system is rotated about the Y body axis by the trim angle of attack and then \"re-fixed\" to the body of the aircraft:\n\n\nForces on the vehicle along the body axes are called \"Body-axis Forces\":\n\n\nThe use of stability derivatives is most conveniently demonstrated with missile or rocket configurations, because these exhibit greater symmetry than aeroplanes, and the equations of motion are correspondingly simpler. If it is assumed that the vehicle is roll-controlled, the pitch and yaw motions may be treated in isolation. It is common practice to consider the yaw plane, so that only 2D motion need be considered. Furthermore, it is assumed that thrust equals drag, and the longitudinal equation of motion may be ignored.\n\nThe body is oriented at angle formula_1 (psi) with respect to inertial axes. The body is oriented at an angle formula_2 (beta) with respect to the velocity vector, so that the components of velocity in body axes are:\nwhere formula_5 is the speed.\n\nThe aerodynamic forces are generated with respect to body axes, which is not an inertial frame. In order to calculate the motion, the forces must be referred to inertial axes. This requires the body components of velocity to be resolved through the heading angle formula_6 into inertial axes.\n\nResolving into fixed (inertial) axes:\nThe acceleration with respect to inertial axes is found by differentiating these components of velocity with respect to time:\nFrom Newton's Second Law, this is equal to the force acting divided by the mass. Now forces arise from the pressure distribution over the body, and hence are generated in body axes, and not in inertial axes, so the body forces must be resolved to inertial axes, as Newton's Second Law does not apply in its simplest form to an accelerating frame of reference.\n\nResolving the body forces:\nNewton's Second Law, assuming constant mass:\nwhere \"m\" is the mass.\nEquating the inertial values of acceleration and force, and resolving back into body axes, yields the equations of motion:\nThe sideslip, formula_2, is a small quantity, so the small perturbation equations of motion become:\nThe first resembles the usual expression of Newton's Second Law, whilst the second is essentially the centrifugal acceleration.\nThe equation of motion governing the rotation of the body is derived from the time derivative of angular momentum:\nwhere C is the moment of inertia about the yaw axis.\nAssuming constant speed, there are only two state variables; formula_2 and formula_22, which will be written more compactly as the yaw rate r.\nThere is one force and one moment, which for a given flight condition will each be functions of formula_2, r and their time derivatives. For typical missile configurations the forces and moments depend, in the short term, on formula_2 and r. The forces may be expressed in the form:\nwhere formula_26 is the force corresponding to the equilibrium condition (usually called the trim) whose stability is being investigated.\nIt is common practice to employ a shorthand:\nThe partial derivative formula_28 and all similar terms characterising the increments in forces and moments due to increments in the state variables are called stability derivatives. \nTypically, formula_29 is insignificant for missile configurations, so the equations of motion reduce to:\n\nEach stability derivative is determined by the position, size, shape and orientation of the missile components. In aircraft, the directional stability determines such features as dihedral of the main planes, size of fin and area of tailplane, but the large number of important stability derivatives involved precludes a detailed discussion within this article. The missile is characterised by only three stability derivatives, and hence provides a useful introduction to the more complex aeroplane dynamics.\n\nConsider first formula_32, a body at an angle of attack formula_2 generates a lift force in the opposite direction to the motion of the body. For this reason formula_32 is always negative.\n\nAt low angles of attack, the lift is generated primarily by the wings, fins and the nose region of the body. The total lift acts at a distance formula_35 ahead of the centre of gravity (it has a negative value in the figure), this, in missile parlance, is the centre of pressure . If the lift acts ahead of the centre of gravity, the yawing moment will be negative, and will tend to increase the angle of attack, increasing both the lift and the moment further. It follows that the centre of pressure must lie aft of the centre of gravity for static stability. formula_35 is the static margin and must be negative for longitudinal static stability. Alternatively, positive angle of attack must generate positive yawing moment on a statically stable missile, i.e. formula_37 must be positive. It is common practice to design manoeuvrable missiles with near zero static margin (i.e. neutral static stability).\n\nThe need for positive formula_37 explains why arrows and darts have flights and unguided rockets have fins.\n\nThe effect of angular velocity is mainly to decrease the nose lift and increase the tail lift, both of which act in a sense to oppose the rotation. formula_39 is therefore always negative. There is a contribution from the wing, but since missiles tend to have small static margins (typically less than a calibre), this is usually small. Also the fin contribution is greater than that of the nose, so there is a net force formula_40, but this is usually insignificant compared with formula_32 and is usually ignored.\n\nManipulation of the equations of motion yields a second order homogeneous linear differential equation in the angle of attack formula_2:\n\nThe qualitative behavior of this equation is considered in the article on directional stability. Since formula_32 and formula_39 are both negative, the damping is positive. The stiffness does not only depend on the static stability term formula_37, it also contains a term which effectively determines the angle of attack due to the body rotation. The distance of the center of lift, including this term, ahead of the centre of gravity is called the maneuver margin. It must be negative for stability.\n\nThis damped oscillation in angle of attack and yaw rate, following a disturbance, is called the 'weathercock' mode, after the tendency of a weathercock to point into wind.\n\nThe state variables were chosen to be the angle of attack formula_2 and the yaw rate r, and have omitted the speed perturbation u, together with the associated derivatives e.g. formula_48. This may appear arbitrary. However, since the timescale of the speed variation is much greater than that of the variation in angle of attack, its effects are negligible as far as the directional stability of the vehicle is concerned. Similarly, the effect of roll on yawing motion was also ignored, because missiles generally have low aspect ratio configurations and the roll inertia is much less than the yaw inertia, consequently the roll loop is expected to be much faster than the yaw response, and is ignored. These simplifications of the problem based on \"a priori\" knowledge, represent an engineer's approach. Mathematicians prefer to keep the problem as general as possible and only simplify it at the end of the analysis, if at all.\n\nAircraft dynamics is more complex than missile dynamics, mainly because the simplifications, such as separation of fast and slow modes, and the similarity between pitch and yaw motions, are not obvious from the equations of motion, and are consequently deferred until a late stage of the analysis. Subsonic transport aircraft have high aspect ratio configurations, so that yaw and roll cannot be treated as decoupled. However, this is merely a matter of degree; the basic ideas needed to understand aircraft dynamics are covered in this simpler analysis of missile motion.\n\nDeflection of control surfaces modifies the pressure distribution over the vehicle, and these are dealt with by including perturbations in forces and moments due to control deflection. The fin deflection is normally denoted formula_49 (zeta). Including these terms, the equations of motion become:\nIncluding the control derivatives enables the response of the vehicle to be studied, and the equations of motion used to design the autopilot.\n\n\n\n"}
{"id": "19137584", "url": "https://en.wikipedia.org/wiki?curid=19137584", "title": "State-of-the-Art Reactor Consequence Analyses", "text": "State-of-the-Art Reactor Consequence Analyses\n\nThe State-of-the-Art Reactor Consequence Analyses (SOARCA) is a study of nuclear power plant safety conducted by the Nuclear Regulatory Commission. The purpose of the SOARCA is assessment of possible impact on population caused by major radiation accidents that might occur at NPPs. This new study updates older studies with the latest state-of-the-art computer models and incorporates new plant safety and security enhancements.\n\n"}
{"id": "2159669", "url": "https://en.wikipedia.org/wiki?curid=2159669", "title": "Stuffing box", "text": "Stuffing box\n\nA stuffing box is an assembly which is used to house a gland seal. It is used to prevent leakage of fluid, such as water or steam, between sliding or turning parts of machine elements.\n\nA stuffing box of a sailboat will have a stern tube that's slightly bigger than the prop shaft. It will also have packing nut threads or a gland nut. The packing is inside the gland nut and creates the seal. The shaft is wrapped by the packing and put in the gland nut. Through tightening it onto the stern tube, the packing is compressed, creating a seal against the shaft. Creating a proper plunger alignment is critical for correct flow and a long wear life. Stuffing box components are of stainless steel, brass or other application-specific materials.\n\nA gland is a general type of stuffing box, used to seal a rotating or reciprocating shaft against a fluid. The most common example is in the head of a tap (faucet) where the gland is usually packed with string which has been soaked in tallow or similar grease. The gland nut allows the packing material to be compressed to form a watertight seal and prevent water leaking up the shaft when the tap is turned on. The gland at the rotating shaft of a centrifugal pump may be packed in a similar way and graphite grease used to accommodate continuous operation. The linear seal around the piston rod of a double acting steam piston is also known as a gland, particularly in marine applications. Likewise the shaft of a handpump or wind pump is sealed with a gland where the shaft exits the borehole.\n\nOther types of sealed connections without moving parts are also sometimes called glands; for example, a cable gland or fitting that connects a flexible electrical conduit to an enclosure, machine or bulkhead facilitates assembly and prevents liquid or gas ingress.\n\nOn a boat having an inboard motor that turns a shaft attached to an external propeller, the shaft passes though a stuffing box, also called a \"packing box\" or \"stern gland\" in this application. The stuffing box prevents sea water from entering the boat's hull. In many small fiberglass boats, for example, the stuffing box is mounted inboard near the point the shaft exits the hull. The \"box\" is a cylindrical assembly, typically of bronze, comprising a sleeve threaded on one end to accept adjusting and locking nuts. A special purpose heavy-duty rubber hose attaches the stuffing box to a stern tube, also called a shaft log, that projects inward from the hull. Marine-duty hose clamps secure the hose to the stern tube and the aft portion of the stuffing box sleeve. A sound stuffing box installation is critical to safety because failure can admit a catastrophic volume of water into the boat.\n\nIn a common type of stuffing box, rings of braided fiber, known as shaft packing or gland packing, form a seal between the shaft and the stuffing box. A traditional variety of shaft packing comprises a square cross-section rope made of flax or hemp impregnated with wax and lubricants. A turn of the adjusting nut compresses the shaft packing. Ideally, the compression is just enough to make the seal both watertight when the shaft is stationary and drip slightly when the shaft is turning. The drip rate must be at once sufficient to lubricate and cool the shaft and packing, but not so much as could sink an unattended boat.\n\nThe market offers improved shaft packing materials that aim to be drip-less when the shaft is turning as well as when stationary. There are also pack-less sealing systems that employ engineered materials such as carbon composites and PTFE (e.g. Teflon).\n\nIn a steam engine, where the piston rod reciprocates through the cylinder cover, a stuffing box provided in the cylinder cover prevents the leakage of steam from the cylinder.\n\n\n\n"}
{"id": "12298953", "url": "https://en.wikipedia.org/wiki?curid=12298953", "title": "TAM Airlines Flight 3054", "text": "TAM Airlines Flight 3054\n\nTAM Airlines Flight 3054 (JJ3054/TAM3054) was a regularly-scheduled domestic passenger flight from Porto Alegre to SÃ£o Paulo, Brazil. On July 17, 2007, the Airbus A320-233 executing the flight, registration PR-MBK, overran runway 35L at SÃ£o Paulo during moderate rain and crashed into a nearby TAM Express warehouse adjacent to a Shell filling station which exploded as a result of the impact. All 187 passengers and crew aboard the Airbus A320 died, along with 12 people on the ground. The crash surpassed Gol Transportes AÃ©reos Flight 1907 as the deadliest aviation accident in South American history, and remains the deadliest aviation accident involving the A320 family in general worldwide. and the second air disaster involving the A320 family, surpassed by the bombing of Metrojet Flight 9268, an A321-231, which crashed in Egypt in October 2015 with 224 fatalities. \n\nThe accident was investigated by the Brazilian Air Force's Aeronautical Accidents Investigation and Prevention Center ( (CENIPA)), with a final report issued in September 2009. CENIPA concluded that the accident was caused by errors committed by the pilots during the landing at SÃ£o Paulo.\n\nThe aircraft serving Flight 3054 was an Airbus A320-233, serial number 789, registration PR-MBK; it was powered by two IAE V2527E-A5 engines. It was built in 1998 and had been operated by several other airlines before entering service with TAM in January 2007. The aircraft was owned by Pegasus Aviation and had flown more than 21,000 hours over 10,000 cycles before the crash.\n\nThe aircraft was dispatched with the thrust reverser on the starboard engine deactivated, as it had jammed. TAM said in a statement a fault in a reverser \"does not jeopardize landings\" and no mechanical problem had been recorded on July 16, the day before the accident. The aircraft had no difficulty braking on the same runway a day before the fatal accident.\n\nThe flight was under the command of an experienced cockpit crew, consisting of Captain Henrique Stefanini Di Sacco (53) and First Officer Kleyber Aguiar Lima (54). Both pilots had been flying for over 30 years. The Captain had logged almost 13,700 flight hours in his career, the First Officer nearly 14,800.\n\nMost of the victims were Brazilian; there were also two French nationals, two Argentinean, one Portuguese, one American, three South Africans and one Peruvian. \n\nThe plane departed from Salgado Filho International Airport in Porto Alegre at 17:18 local time (20:18 UTC). At 18:54 local time (21:54 UTC), the flight made its landing at Congonhas-SÃ£o Paulo Airport.\n\nFlight 3054 was cleared to land at Congonhas' Runway 35L. Reviews by government officials of the surveillance videos showed that despite the aircraft touching down without incident, it did not slow down normally, departing the far end at around , bearing to its left. The runway is elevated above the surrounding area, and the aircraft's momentum carried it over traffic on the adjacent \"Avenida\" Washington LuÃ­s, a major thoroughfare, and into a four-story TAM Express facility, resulting in a large fire. The TAM Express building contained offices and a warehouse, and was located adjacent to a Shell gas station. All 187 passengers and crew aboard died and the aircraft was destroyed.\n\nThe runway had recently been resurfaced, and did not yet have water-channeling grooves cut into it to reduce the danger of hydroplaning.\n\nFlight Data Recorder (FDR) information recovered after the crash and released by Brazilian authorities showed that immediately prior to touchdown, both thrust levers were in CL (or \"climb\") position, with engine power being governed by the flight computer's autothrottle system. Two seconds prior to touchdown, an aural warning, \"retard, retard\", was issued by the flight's computer system, advising the pilots to retard the thrust lever to the recommended idle lever position. This would disengage the autothrottle, with engine power then governed directly by the thrust levers.\n\nAt the moment of touchdown, the spoiler lever was in the \"ARMED\" position. According to the system logic of the A320's flight controls, in order for the spoilers to automatically deploy upon touchdown, not only must the spoiler lever be in the \"ARMED\" position, but both thrust levers must be at or close to the \"idle\" position. The FDR transcript shows that immediately after the warning, the flight computer recorded the left thrust lever being retarded to the rear-most position, activating the thrust reverser on the left engine, while the right thrust lever (controlling the engine with the disabled thrust reverser) remained in the CL position. One theory put forth by CENIPA is that the pilots may not have noticed that the right engine remained at CL because the Airbus autothrottle system, unlike other aircraft manufacturers, does not automatically move the levers when the autothrust controller changes engine settings. Therefore, the pilots may have thought that the right engine was at idle power without realizing that Airbus autothrust logic dictates that, when one or more of the thrust levers is pulled to the idle position, the autothrust is automatically disengaged. Thus, when the pilot pulled the left engine thrust lever to idle, it disconnected the autothrust system and the computer did not retard the right engine power to idle. The A320's spoilers did not deploy during the landing run, as the right thrust lever was above the \"idle\" setting required for automatic spoiler deployment. Since the right engine thrust lever was still in the \"climb\" detent at that time, the right engine accelerated to climb power while the left engine deployed its thrust reverser. The resulting asymmetric thrust condition resulted in a loss of control and a crash ensued.\n\nAviation safety in Brazil had been under increased scrutiny following the mid-air collision in September 2006 over the Amazon of Gol Transportes AÃ©reos Flight 1907 and an Embraer Legacy 600 (see Brazil's 2006-2007 aviation crisis). Congonhas was singled out for having safety issues relating to operations in wet weather due to its location and runway characteristics for the traffic it serves.\n\nThe 35L runway at Congonhas is long. Congonhas' counterpart in Rio de Janeiro, the Santos Dumont Airport, has an even shorter runway, at . Both airports receive the same type of traffic â ranging from small private planes to Boeing 737s and A320s. Many variables affect the landing distance of an aircraft, such as approach speed, weight and the presence of either a tailwind or a headwind. For an Airbus A320, a landing speed of higher than normal can result in as much as a 25% increase in the runway length needed to stop an aircraft. Wet weather can also significantly reduce the braking performance of aircraft, leading to an increase in the minimum runway length requirement.\n\nPilots have called Congonhas airport the \"aircraft carrier,\" because of the runway's short length and because pilots are told to go-around if they overshoot the first of runway.\n\nIn February 2007, a Brazilian judge briefly banned flights using Fokker 100, Boeing 737-700 and Boeing 737-800 aircraft in and out of the airport. The Airbus A320 was not among the aircraft banned, due to its manufacturer-stated braking distance being shorter than those of the banned aircraft. Pilots had complained that water had been accumulating on the runway, reducing aircraft braking performance and occasionally causing planes to hydroplane. The judge claimed the runway needed to be longer for these aircraft to operate safely. At the time, a spokeswoman from Brazil's National Civil Aviation Agency claimed \"The safety conditions of the runway and the airport as a whole are adequate.\" TAM also objected to the decision, with a spokesman stating \"If the injunction stands, it will cause total chaos,\" claiming over 10,000 passengers per day would be inconvenienced.\n\nThe airport reopened on July 19, 2007 using an alternative runway.\n\nMany flights, including all OceanAir and BRA Transportes AÃ©reos, were transferred to Guarulhos International Airport, the major airport in SÃ£o Paulo, due to the closure of the main runway at Congonhas and the ongoing investigation of the accident.\n\nOn July 20, Presidency Chief of Staff Dilma Rousseff announced plans to significantly reduce the number of flights operating at Congonhas. The plan included banning, within 60 days, all connection, stopover, charter, and international flights and the reduction in the number of private jets. The airport would only operate direct flights to certain cities in Brazil. The plan also called for a study of the expansion of SÃ£o Paulo's two current airports and the construction of a third airport in the metropolitan area.\n\nState crime scene investigators terminated the search for remains on July 28, 2007; as of that date, 114 bodies recovered from the site had been identified by the SÃ£o Paulo Medical Examiner's Office as those of passengers.\n\nThe investigation was carried out by Brazil's Aeronautical Accidents Investigation and Prevention Center (\"Centro de InvestigaÃ§Ã£o e PrevenÃ§Ã£o de Acidentes AeronÃ¡uticos\", CENIPA). Data from the flight data recorder and cockpit voice recorder (CVR) were downloaded by the National Transportation Safety Board in the United States commencing July 20 and 23 respectively. Based on preliminary data from the FDR, on July 25 Airbus cautioned A320 operators to ensure that both thrust levers are set to idle during flare. The transcript of the CVR was released on August 1. It shows that the pilots were aware of the wet runway conditions and the deactivated thrust reverser. The pilots' comments suggest that the spoilers did not deploy and that they were unable to slow the aircraft. Crew error has not been ruled out.\n\nAn investigation by the Brazilian Public Safety Ministry released in November 2008 concluded that the pilots mistakenly left the lever for the right engine to climb upon landing, due to a mistake in landing procedures with the right thrust reverser being disabled from a prior maintenance, when in fact it was necessary to retard both engines in order for the spoilers to work. They also said that the National Civil Aviation Agency should have closed the airport on the night the plane landed because of heavy rains; that Congonhas airport authorities shared the blame because its runway had not been properly constructed with grooves to drain away excess rainwater, contributing to the crash; that the plane's manufacturer, Airbus, should have provided alarms warning the pilots that the braking system was failing; and that TAM failed to properly train its pilots, who did not act correctly in the emergency.\n\nIn September 2009, more than two years after the accident, CENIPA announced the results of official investigations. The report shows that one of the thrust levers, which control engines, was in position to accelerate when it should be in idle, but it was not proved if there was mechanical or human failure as the cause of the accident.\n\nThe report suggests two hypotheses for the accident. In the first, there was a flaw in the power control of the plane's engines, which would have kept one of the thrust levers into acceleration, regardless of their actual position. In such circumstances, there was mechanical failure of the aircraft. The occurrence of this failure is one in 400 billion hours of flight and therefore highly improbable. In the second hypothesis, the pilot has performed a procedure different from that provided in the manual, and put the thrust lever in an irregular position, a configuration of human error for the accident.\n\nIn addition to the positions of the thrust levers, the report points to several factors that may have contributed to the accident, such as a high volume of rain on the day, with the formation of puddles on the runway, as well as the absence of grooving. The report does not blame the length of the runway for the accident. The BEA also cleared Airbus of any misdoing because they had proposed a system warning modification regarding the incorrect thrust lever positions which TAM had rejected.\n\nAfter the crash, President Luiz InÃ¡cio Lula da Silva ordered three days of national mourning.\n\nDuring the 2007 Pan American Games in Rio de Janeiro, the Brazilian athletes wore a black armband in remembrance of the victims. The flags of all participating countries were flown at half mast on July 18. Matches involving a Brazilian athlete or team started with a minute of silence.\n\nAll matches of the Campeonato Brasileiro 2007 started with a minute of silence, while all players wore black armbands. Brazilian Formula One driver Felipe Massa had a black stripe on top of his helmet during the 2007 European Grand Prix, to commemorate the victims. Rubens Barrichello also had stripes on his helmet, and the two Red Bull Racing drivers David Coulthard and Mark Webber had small Brazilian flags on their helmets referring to the accident.\n\nMore than 5,000 Brazilians marched to the crash site on July 29, 2007, blaming their government's failure to invest in airport infrastructure for the crash. Many of the protesters also demanded the ousting of President Luiz Inacio Lula da Silva.\n\nOn November 19, 2008, the 13,600 page police investigation was completed, which took 16 months of research to produce, during which 336 people were heard. For federal prosecutors, the former director of ANAC Denise Abreu and then flight safety officer of the company, Marco Aurelio dos Santos de Miranda, should be convicted of attempt on air transport security in willful mode. In March 2006, Denise Abreu took over the management of air services of the newly established National Civil Aviation Agency (ANAC). The investigation of the Airbus TAM accident, which killed 199 people in Congonhas, revealed that in February 2007, the SÃ£o Paulo federal judge Cecilia Marcondes, who saw action restricting the landing planes in Congonhas on rainy days, received Denise Abreu one of ANAC document like a standard, but it was only a technical study. According to the complaint, the study presented as standard guarantee, in theory, security in landing operations at Congonhas airport in SÃ£o Paulo. The study indicated that takeoffs and landings were forbidden in Congonhas in case the track had a water depth greater than 3Â mm. After the accident with the TAM plane in July 2007, it revealed that the study was not standard and thus there was no obligation to follow it. According to the testimony of federal judge Cecilia Marcondes to the Federal Public Ministry, the document was fundamental to the Federal Court to release the runway for takeoffs and landings of all equipment.\n\nIn 2011, the Brazilian Federal Public Ministry (MinistÃ©rio PÃºblico FederalâMPF) laid criminal charges against , the director of the Brazilian National Civil Aviation Agency (ANAC) at the time of the disaster, as well as two former TAM directorsâMarco AurÃ©lio dos Santos de Miranda, director of flight safety, and Alberto Fajerman, vice president of operations. They were accused of neglecting air transport safety by allowing the aircraft to land in heavy rain on the notoriously short, recently resurfaced runway before cutting of grooves to channel away excess rainwater. The trial began in SÃ£o Paulo in 2013. In 2014, MPF withdrew the charges against Fajerman, for lack of evidence. A second charge against Abreu of \"documentary falsehood\" was dismissed in November, 2014. As of March 2015, no judgement had been handed down on the other charges.\n\nIn 2014, TAM's insurer ItaÃº Seguros, the company responsible for paying compensation for the tragedy, launched a lawsuit in Brazil against Airbus for R$350 million (US$156.2 million), according to Folha de S. Paulo. Attorneys representing Airbus responded in a Brazilian court filing that Airbus accepts no responsibility, laying the blame for the disaster with the cockpit crew, the airline and the poor state of the runway.\n\nAmong the victims were:\n\nThe Discovery Channel Canada / National Geographic TV series \"Mayday\" featured the crash and investigation in a Season 11 episode titled \"Deadly Reputation\", which included interviews with accident investigators and a dramatic recreation of the accident.\n\n\n"}
{"id": "34247806", "url": "https://en.wikipedia.org/wiki?curid=34247806", "title": "Tomiyamichthys russus", "text": "Tomiyamichthys russus\n\nTomiyamichthys russus, also known as the Ocellated shrimpgoby, is a species of goby native to the South China Sea where it can be found on silty bottoms at depths of from , often near the mouths of streams.\n"}
{"id": "57873303", "url": "https://en.wikipedia.org/wiki?curid=57873303", "title": "Tree caliper", "text": "Tree caliper\n\nA tree caliper is a special caliper to measure the diameter of a tree.\n\nThere is a considerable number of designs of the tool. \n"}
{"id": "51189063", "url": "https://en.wikipedia.org/wiki?curid=51189063", "title": "UNNExT", "text": "UNNExT\n\nThe United Nations Network of Experts for Paperless Trade and Transport in Asia and the Pacific (UNNExT) is a community of trade facilitation specialists and practitioners focusing on simplifying import, export and transit procedures by enabling traders and governments to exchange information electronically and through automated and integrated systems, including national and regional Single window. The Network has made significant contributions to the development of Trade facilitation and Paperless trade in the region.\n\nLaunched by the United Nations Economic and Social Commission for Asia and the Pacific in cooperation with United Nations Economic Commission for Europe in 2009, early work of the Community focused on building capacity of developing countries on fundamental issues associated with single window and paperless trade system development, such as business process analysis of trade procedures, data harmonization, and legal framework development.\n\nInitially referred to as the UN Network of Experts for Paperless Trade in Asia-Pacific âTransportâ was added to the full name of the network in 2014 in recognition of the importance of transit facilitation and transport-related procedures for the many landlocked developing countries members of ESCAP, particularly those in Central Asia. Taking into account the importance of small and medium-sized enterprises and agriculture for the sustainable and inclusive development of the region, recent work of the Network has increasingly focused on these two sectors since 2012.\n\nWith the growing interest and investment of Governments in the development of national single window and related paperless trade systems, addressing the issue of system interoperability and the need to enable electronic exchange and legal recognition of data and documents across borders has become more pressing. A UNNExT Advisory Group dedicated to this issue was set up in 2013 and provided significant support in the development of the âFramework Agreement on Facilitation of Cross-Border Paperless Trade in Asia and the Pacificâ, a new United Nations treaty open for signature to 53 ESCAP member states starting 1 October 2016.\n\nThe continuous demand for capacity building on paperless trade and trade facilitation has led the network to develop online training and certification programmes, with a first e-learning series on \"Business Process Analysis for Trade Facilitation\" launched in 2015.\n\nAs emphasized in the UNNExT-supported 7th Asia-Pacific Trade Facilitation Forum held in Wuhan, China in 2015, there are strong linkages between paperless trade and e-commerce and future work of the network is expected to address procedural bottlenecks to cross-border e-commerce.\n"}
{"id": "5163454", "url": "https://en.wikipedia.org/wiki?curid=5163454", "title": "Waste heat", "text": "Waste heat\n\nWaste heat is heat that is produced by a machine, or other process that uses energy, as a byproduct of doing work. All such processes give off some waste heat as a fundamental result of the laws of thermodynamics. Waste heat has lower utility (or in thermodynamics lexicon a lower exergy or higher entropy) than the original energy source. Sources of waste heat include all manner of human activities, natural systems, and all organisms, for example, a refrigerator warms the room air, an internal combustion engine generates high-temperature exhaust gases, and electronic components get warm when in operation.\n\nInstead of being \"wasted\" by release into the ambient environment, sometimes waste heat (or cold) can be utilized by another process (such as using hot engine coolant to heat a vehicle), or a portion of heat that would otherwise be wasted can be reused in the same process if make-up heat is added to the system (as with heat recovery ventilation in a building).\n\nThermal energy storage, which includes technologies both for short- and long-term retention of heat or cold, can create or improve the utility of waste heat (or cold). One example is waste heat from air conditioning machinery stored in a buffer tank to aid in night time heating. Another is seasonal thermal energy storage (STES) at a foundry in Sweden. The heat is stored in the bedrock surrounding a cluster of heat exchanger equipped boreholes, and is used for space heating in an adjacent factory as needed, even months later. An example of using STES to utilize natural waste heat is the Drake Landing Solar Community in Alberta, Canada, which, by using a cluster of boreholes in bedrock for interseasonal heat storage, obtains 97 percent of its year-round heat from solar thermal collectors on the garage roofs. Another STES application is storing winter cold underground, for summer air conditioning.\n\nOn a biological scale, all organisms reject waste heat as part of their metabolic processes, and will die if the ambient temperature is too high to allow this.\n\nAnthropogenic waste heat is thought by some to contribute to the urban heat island effect. The biggest point sources of waste heat originate from machines (such as electrical generators or industrial processes, such as steel or glass production) and heat loss through building envelopes. The burning of transport fuels is a major contribution to waste heat.\n\nMachines converting energy contained in fuels to mechanical work or electric energy produce heat as a by-product.\n\nIn the majority of energy applications, energy is required in multiple forms. These energy forms typically include some combination of: heating, ventilation, and air conditioning, mechanical energy and electric power. Often, these additional forms of energy are produced by a heat engine, running on a source of high-temperature heat. A heat engine can never have perfect efficiency, according to the second law of thermodynamics, therefore a heat engine will always produce a surplus of low-temperature heat. This is commonly referred to as waste heat or \"secondary heat\", or \"low-grade heat\". This heat is useful for the majority of heating applications, however, it is sometimes not practical to transport heat energy over long distances, unlike electricity or fuel energy.\nThe largest proportions of total waste heat are from power stations and vehicle engines. The largest single sources are power stations and industrial plants such as oil refineries and steelmaking plants.\n\nThe electrical efficiency of thermal power plants is defined as the ratio between the input and output energy. It is typically only 30%.\nThe images show cooling towers which allow power stations to maintain the low side of the temperature difference essential for conversion of heat differences to other forms of energy. Discarded or \"Waste\" heat that is lost to the environment may instead be used to advantage.\n\nIndustrial processes, such as oil refining, steel making or glass making are major sources of waste heat.\n\nAlthough small in terms of power, the disposal of waste heat from microchips and other electronic components, represents a significant engineering challenge. This necessitates the use of fans, heatsinks, etc. to dispose of the heat. \n\nFor example, data centers use electronic components that consume electricity for computing, storage and networking. The French CNRS explains a data center is like a resistance and most of the energy it consumes is transformed into heat and requires cooling systems.\n\nAnimals, including humans, create heat as a result of metabolism. In warm conditions, this heat exceeds a level required for homeostasis in warm-blooded animals, and is disposed of by various thermoregulation methods such as sweating and panting. Fiala \"et al.\" modelled human thermoregulation.\n\nLow temperature heat contains very little capacity to do work (Exergy), so the heat is qualified as waste heat and rejected to the environment. Economically most convenient is the rejection of such heat to water from a sea, lake or river. If sufficient cooling water is not available, the plant has to be equipped with a cooling tower to reject the waste heat into the atmosphere.\nIn some cases it is possible to use waste heat, for instance in heating homes by cogeneration. However, by slowing the release of the waste heat, these systems always entail a reduction of efficiency for the primary user of the heat energy.\n\nWaste of the by-product heat is reduced if a cogeneration system is used, also known as a Combined Heat and Power (CHP) system. Limitations to the use of by-product heat arise primarily from the engineering cost/efficiency challenges in effectively exploiting small temperature differences to generate other forms of energy. Applications utilizing waste heat include swimming pool heating and paper mills. In some cases, cooling can also be produced by the use of absorption refrigerators for example, in this case it's called trigeneration or CCHP (combined cooling, heat and power).\n\nWaste heat can be forced to heat incoming fluids and objects before being highly heated. For instance outgoing water can give its waste heat to incoming water in a heat exchanger before heating in homes or power plants.\n\nThere are many different approaches to transfer thermal energy to electricity, and the technologies to do so have existed for several decades. The organic Rankine cycle, offered by companies such as Ormat, is a very known approach, whereby an organic substance is used as working medium instead of water. The benefit is that this process can reject heat at lower temperatures for the production of electricity than the regular water steam cycle. An example of use of the steam Rankine cycle is the Cyclone Waste Heat Engine.\nAnother established approach is by using a thermoelectric, such as those offered by Alphabet Energy, where a change in temperature across a semiconductor material creates a voltage through a phenomenon known as the Seebeck effect. A related approach is the use of thermogalvanic cells, where a temperature difference gives rise to an electric current in an electrochemical cell.\n\nAnthropogenic heat is heat generated by humans and human activity. The American Meteorological Society defines it as \"Heat released to the atmosphere as a result of human activities, often involving combustion of fuels. Sources include industrial plants, space heating and cooling, human metabolism, and vehicle exhausts. In cities this source typically contributes 15â50 W/m to the local heat balance, and several hundred W/m in the center of large cities in cold climates and industrial areas.\"\n\nEstimates of anthropogenic heat generation can be made by totaling all the energy used for heating and cooling, running appliances, transportation, and industrial processes, plus that directly emitted by human metabolism.\n\nAnthropogenic heat is a small influence on rural temperatures, and becomes more significant in dense urban areas. It is one contributor to urban heat islands. Other human-caused effects (such as changes to albedo, or loss of evaporative cooling) that might contribute to urban heat islands are not considered to be anthropogenic heat by this definition.\n\nAnthropogenic heat is a much smaller contributor to global warming than are greenhouse gases. In 2005, although anthropogenic waste heat flux was significantly high in certain urban areas (and can be high regionally. For example, waste heat flux was +0.39 and +0.68 W/m for the continental United States and western Europe, respectively) globally it accounted for only 1% of the energy flux created by anthropogenic greenhouse gases. Global forcing from waste heat was 0.028 W/m in 2005. This statistic is predicted to rise as urban areas become more widespread.\n\nAlthough waste heat has been shown to have influence on regional climates, climate forcing from waste heat is not normally calculated in state-of-the-art global climate simulations. Equilibrium climate experiments show statistically significant continental-scale surface warming (0.4â0.9Â Â°C) produced by one 2100 AHF scenario, but not by current or 2040 estimates. Simple global-scale estimates with different growth rates of anthropogenic heat that have been actualized recently show noticeable contributions to global warming, in the following centuries. For example, a 2% p.a. growth rate of waste heat resulted in a 3 degree increase as a lower limit for the year 2300. Meanwhile, this has been confirmed by more refined model calculations.\n\nOne research showed that if antropogenic heat emissions continue to rise in current rate, they will became a source of warming as strong as GHG emmisions in 21 century.\n\n"}
{"id": "41554784", "url": "https://en.wikipedia.org/wiki?curid=41554784", "title": "World Soil Museum", "text": "World Soil Museum\n\nThe World Soil Museum (WSM) displays physical examples of soil profiles (monoliths) representing major soil types of the world, from the volcanic ash soils from Indonesia to the red, strongly weathered soils from the Amazon region. The museum is managed by ISRIC - World Soil Information, legally registered as the International Soil Reference and Information Centre (ISRIC), an independent, science-based foundation. Physically, the museum is located on the campus of Wageningen University and Research Centre in Wageningen, The Netherlands. \n\nThe WSM (originally known as International Soil Museum) was created in 1996 at the request of the United Nations Educational, Scientific and Cultural Organization (UNESCO) and the International Society of Soil Science (ISSS) (now IUSS). with a view to underpin the development of the FAO-UNESCO 'Soil Map of the World' . The initial ISM building was located at the University of Utrecht.\n\nSome 80 soil monoliths are on display in the WSM, with a much larger collection (some 1000 from over 70 countries) stored and maintained in our repository. For each soil monolith, there is supplemental information about the site of sampling (e.g. landscape, land use, parent material and climate), a detailed profile description for each soil horizon or layer, and data on chemical compositions and physical features. The museum displays examples of the main (32) WRB Soil Reference Groups of the World. A special section is devoted to the major soil types of Netherlands. Further, it showcases soils that have changed significantly under the influence of long-term human activity. Much of this information can also be viewed online. The WSM plays an important role in ISRIC's educational and outreach programme, and is an important component of ISRIC's Annual Spring School on worls soils and teir assessment..\n\nRecent developments at ISRIC are succinctly described in a series of Annual Highlights.\n\n"}
