{"id": "41309023", "url": "https://en.wikipedia.org/wiki?curid=41309023", "title": "1961 President Airlines Douglas DC-6 crash", "text": "1961 President Airlines Douglas DC-6 crash\n\nThe 1961 President Airlines Douglas DC-6 crash occurred on the night of September 10, 1961, when a President Airlines Douglas DC-6B named \"Theodore Roosevelt\" outbound from Shannon, Ireland crashed into the nearby River Shannon shortly after takeoff, killing all 83 people on board. To date, the crash remains the deadliest one in Irish territory, as well as the third-deadliest involving a DC-6 after LAN Chile Flight 107 and Olympic Airways Flight 954.\n\nThe aircraft involved in the accident was a Douglas DC-6B registered N90773. It first flew in 1953 and was powered by four Pratt & Whitney R-2800 engines. The aircraft's occupants on the accident flight consisted of 77 passengers and six crew members.\n\nThe aircraft was on a non-scheduled international passenger flight from Düsseldorf, Germany to Chicago with stopovers in Shannon and Gander, Newfoundland for refueling. Shortly after takeoff from Shannon Airport's runway 24, the pilots were cleared for a right-hand turn, but they instead turned left and kept turning until the aircraft had reached a bank angle of about 90 degrees or more. Unable to recover, the aircraft plummeted into the River Shannon 5,000 ft. from the end of the runway. There were no survivors among the 83 passengers and crew. Subsequent investigations indicate that the crash probably resulted from a malfunctioning attitude indicator, a fault in the starboard ailerons, or both. Poor weather conditions and crew fatigue were also cited as possible contributing factors. Irish State Papers released in 1994 indicated that the pilot had alcohol in his blood and that the airline was insolvent at the time of the accident.\n\n\n"}
{"id": "1621815", "url": "https://en.wikipedia.org/wiki?curid=1621815", "title": "Accessory cloud", "text": "Accessory cloud\n\nAn accessory cloud is a cloud which is dependent on a larger cloud system for its development and continuance. It is often an appendage but also can be adjacent to the parent cloud system. \n\nThe arcus and roll clouds, shelf cloud, wall cloud, and scud are examples of low level or vertical accessory clouds whilst the anvil, and overshooting top, are examples of high level accessory clouds. The condensation funnel of funnel clouds and tornadoes are also accessory clouds. They are associated with deep moist convection and especially cumulonimbus, the primary cloud producing thunderstorms. The pileus and mammatus types can form at various altitude ranges depending on the main clouds with which they are associated. The World Meteorological Organization classifies most accessory clouds as \"supplementary features\". The height range classification of a supplementary feature is the same as the parent cloud. As an example, the anvil cloud (supplementary feature incus) forms at high altitude but is not classified by the WMO as a high cloud because of its association with the genus cumulonimbus.\n\nIt is \"very rare\" for any accessory cloud to generate its own precipitation. However, the parent cloud may generate precipitation. Precipitation from the parent cloud is often confused with the accessory cloud and observers think that the precipitation is actually falling from the accessory cloud.\n\n"}
{"id": "44263754", "url": "https://en.wikipedia.org/wiki?curid=44263754", "title": "Aeromedical Biological Containment System", "text": "Aeromedical Biological Containment System\n\nThe Aeromedical Biological Containment System (ABCS) is an aeromedical evacuation capability devised by the U.S. Centers for Disease Control and Prevention (CDC) in collaboration with the U.S. Department of Defense (DoD) and government contractor Phoenix Air between 2007 and 2010. Its purpose is to safely air-transport a highly contagious patient; it comprises a transit isolator (a tent-like plastic structure provided with negative air pressure to prevent escape of airborne-contagious pathogens) and an appropriately configured supporting aircraft. Originally developed to support CDC staff who might become infected while investigating avian flu and SARS in East Asia, it was never used until the 2014 Ebola virus epidemic in West Africa, transporting 36 Ebola patients out of West Africa.\n\nThe CDC experienced difficulties and embarrassment relating to safe patient air-transport during an international tuberculosis scare in 2007. Additionally, the memory of the severe acute respiratory syndrome (SARS) epidemic of 2003-2004 and the then-current avian influenza threat (both potentially requiring transport of sick patients back to the U.S. from the Far East) prompted CDC officials to initiate the program that became the ABCS. The Cartersville, Georgia-based military airlift provider Phoenix Air was contracted; the CDC provided medical expertise to the collaboration, while the DoD provided some of the protective technology through its Edgewood Chemical Biological Center.\n\nWith the anticipated decommissioning of the U.S. Army’s Aeromedical Isolation Team (AIT) in December 2010, the need for a new system to support transport of highly contagious patients became even more urgent. By that time, the ABCS had been extensively tested and was certified for its mission by the Federal Aviation Administration (FAA). The U.S. Air Force had also approved it for ferrying military personnel. The dedicated air platform for the ABCS was a 32-year-old Gulfstream III jet that had once been owned and operated by the Royal Danish Air Force as military tailcode “F-313”. (F-313 had been sold to Phoenix Air in January 2005. Rechristened as air ambulance jet “N173PA”, it was utilized in support of the 2007 TB incident.)\n\nThe international swine flu pandemic of 2009-10 did not occasion the need for air-transport under isolation of any CDC personnel. Consequently, the ABCS—which comprised three isolation units (but only one aircraft) by late-2011—was to some extent mothballed. That changed abruptly in late-July 2014 when Phoenix Air was asked if they could support a patient with a “bloodborne pathogen” (namely Ebola virus) as well as it could an “airborne pathogen” (such as TB or flu, for which it had been designed). Physicians from Phoenix, along with CDC experts, spent a day and a half reviewing the system, after which they all agreed it was suitable for Ebola patients. \"In fact, it's probably over-engineered for Ebola, because it's designed for airborne pathogens,\" the Phoenix director stated. On 2 August 2014, the ABCS carried the first Ebola patient (Dr. Kent Brantly, an employee of Samaritan's Purse) ever to be evacuated to the United States; three days later, it transported another, Nancy Writebol. To date, Phoenix Air and the ABCS have flown five Ebola patients out of West Africa (four Americans—Brantly, Writebol, Dr Rick Sacra, and an unidentified American on September 9 -- and a German doctor from Sierra Leone to Hamburg, Germany).\nAfter the first two missions, Phoenix Air—which says it considered them successful \"proof-of-concept flights\"—decided that it would only undertake future missions under the aegis of the U.S. government. Challenges such as dealing with U.S. customs officials, obtaining permission to transit foreign airspace, and the selection of specific destination medical centers had become too onerous. Since then, the U.S. Department of State has coordinated all such flights, including those for foreigners returning to their own countries. U.S. taxpayers pick up the tab for official government patients, but reimbursement is required for all others. (Brantly and Writebol each cost about $200,000, including the cost of equipment decontamination, which their organization paid.)\n\nPhoenix Air currently keeps only one plane on standby for transporting Ebola victims.\n\nThe modular ABCS utilizes a transit isolator consisting of a metal frame supporting numerous items of medical equipment (heart and pulse oxygen monitors, etc.). Most of airframe’s cabin space is taken up by the isolator. The framework is encased in a transparent plastic sheathing, or “tent”. Only one patient—attended by one doctor and two nurses—can be accommodated in one isolator in one aircraft. The patient area is provided with continuous negative air pressure and air filtration to keep pathogens from entering the rest of the cabin. An antechamber to the patient area is used by medical personnel for donning and doffing personal protective equipment as well as for decontamination. Next to the isolator is a toilet for the patient’s use.\n\nAfter transport of an infected patient is completed, Phoenix Air performs a decontamination process stipulated by the CDC. This consists of spraying a powerful disinfectant inside the module for 24 hours and sending all contaminated contents — including the plastic casing, patient stretchers, and even walkie-talkies — to an incinerator (operated by a federally licensed hazardous-materials disposal team) for burning.\n\n"}
{"id": "14688273", "url": "https://en.wikipedia.org/wiki?curid=14688273", "title": "Allotropes of phosphorus", "text": "Allotropes of phosphorus\n\nElemental phosphorus can exist in several allotropes, the most common of which are white and red solids. Solid violet and black allotropes are also known. Gaseous phosphorus exists as diphosphorus and atomic phosphorus.\n\nWhite phosphorus, yellow phosphorus or simply tetraphosphorus (P) exists as molecules made up of four atoms in a tetrahedral structure. The tetrahedral arrangement results in ring strain and instability. The molecule is described as consisting of six single P–P bonds. Two different crystalline forms are known. The α form is defined as the standard state of the element, but is actually metastable under standard conditions. It has a body-centered cubic crystal structure, and transforms reversibly into the β form at 195.2 K. The β form is believed to have a hexagonal crystal structure.\n\nWhite phosphorus is a translucent waxy solid that quickly becomes yellow when exposed to light. For this reason it is also called yellow phosphorus. It glows greenish in the dark (when exposed to oxygen) and is highly flammable and pyrophoric (self-igniting) upon contact with air. It is toxic, causing severe liver damage on ingestion and phossy jaw from chronic ingestion or inhalation. The odour of combustion of this form has a characteristic garlic smell, and samples are commonly coated with white \"diphosphorus pentoxide\", which consists of PO tetrahedral with oxygen inserted between the phosphorus atoms and at their vertices. White phosphorus is only slightly soluble in water and can be stored under water. Indeed, white phosphorus is safe from self-igniting only when it is submerged in water. It is soluble in benzene, oils, carbon disulfide, and disulfur dichloride.\n\nThe white allotrope can be produced using several different methods. In the industrial process, phosphate rock is heated in an electric or fuel-fired furnace in the presence of carbon and silica. Elemental phosphorus is then liberated as a vapour and can be collected under phosphoric acid. An idealized equation for this carbothermal reaction is shown for calcium phosphate (although phosphate rock contains substantial amounts of fluoroapatite):\nWhite phosphorus has an appreciable vapour pressure at ordinary temperatures. The vapour density indicates that the vapour is composed of P molecules up to about 800 °C. Above that temperature, dissociation into P molecules occurs.\n\nIt ignites spontaneously in air at about , and at much lower temperatures if finely divided. This combustion gives phosphorus (V) oxide:\nBecause of this property, white phosphorus is used as a weapon.\n\nAlthough white phosphorus converts to the thermodynamically more stable red allotrope, the formation of the cubic P molecule is not observed in the condensed phase. Analogs of this hypothetical molecule have been prepared from phosphaalkynes.\n\nRed phosphorus may be formed by heating white phosphorus to in the absence of air or by exposing white phosphorus to sunlight. Red phosphorus exists as an amorphous network. Upon further heating, the amorphous red phosphorus crystallizes. Red phosphorus does not ignite in air at temperatures below , whereas pieces of white phosphorus ignite at about . Ignition is spontaneous at room temperature with finely divided material.\n\nUnder standard conditions it is more stable than white phosphorus, but less stable than the thermodynamically stable black phosphorus. The standard enthalpy of formation of red phosphorus is -17.6 kJ/mol.\n\nRed phosphorus can be used as a very effective flame retardant, especially in thermoplastics (e.g. polyamide) and thermosets (e.g. epoxy resins or polyurethanes). The flame retarding effect is based on the formation of polyphosphoric acid. Together with the organic polymer material, this acid creates a char which prevents the propagation of the flames. The safety risks associated with phosphine generation and friction sensitivity of red phosphorus can be effectively reduced by stabilization and micro-encapsulation. For easier handling, red phosphorus is often used in form of dispersions or masterbatches in various carrier systems. However, for electronic/electrical systems, red phosphorus flame retardant has been effectively banned by major OEMs due to its tendency to induce premature failures. There have been two issues over the years: the first was red phosphorus in epoxy molding compounds inducing elevated leakage current in semiconductor devices and the second was acceleration of hydrolysis reactions in PBT insulating material.\n\nRed phosphorus can also be used in the illicit production of narcotics, including some recipes for methamphetamine.\n\nMonoclinic phosphorus, or violet phosphorus, is also known as Hittorf's metallic phosphorus. In 1865, Johann Wilhelm Hittorf heated red phosphorus in a sealed tube at 530 °C. The upper part of the tube was kept at 444 °C. Brilliant opaque monoclinic, or rhombohedral, crystals sublimed as a result. Violet phosphorus can also be prepared by dissolving white phosphorus in molten lead in a sealed tube at 500 °C for 18 hours. Upon slow cooling, Hittorf's allotrope crystallises out. The crystals can be revealed by dissolving the lead in dilute nitric acid followed by boiling in concentrated hydrochloric acid. In addition, a fibrous form exists with similar phosphorus cages.\n\nIt does not ignite in air until heated to 300 °C and is insoluble in all solvents. It is not attacked by alkali and only slowly reacts with halogens. It can be oxidised by nitric acid to phosphoric acid.\n\nIf it is heated in an atmosphere of inert gas, for example nitrogen or carbon dioxide, it sublimes and the vapour condenses as white phosphorus. If it is heated in a vacuum and the vapour condensed rapidly, violet phosphorus is obtained. It would appear that violet phosphorus is a polymer of high relative molecular mass, which on heating breaks down into P molecules. On cooling, these would normally dimerize to give P molecules (i.e. white phosphorus) but, in vacuo, they link up again to form the polymeric violet allotrope.\n\nBlack phosphorus is the thermodynamically stable form of phosphorus at room temperature and pressure, with a heat of formation of -39.3 kJ/mol (relative to white phosphorus which is defined as the standard state). It is obtained by heating white phosphorus under high pressures (12,000 atmospheres). In appearance, properties, and structure, black phosphorus is very much like graphite with both being black and flaky, a conductor of electricity, and having puckered sheets of linked atoms. Phonons, photons, and electrons in layered black phosphorus structures behave in a highly anisotropic manner within the plane of layers, exhibiting strong potential for applications to thin film electronics and infrared optoelectronics.\n\nBlack phosphorus has an orthorhombic structure and is the least reactive allotrope, a result of its lattice of interlinked six-membered rings where each atom is bonded to three other atoms. Black and red phosphorus can also take a cubic crystal lattice structure. The first high-pressure synthesis of black phosphorus crystals was made by the physicist Percy Williams Bridgman in 1914. A recent synthesis of black phosphorus using metal salts as catalysts has been reported.\n\nThe similarities to graphite also include the possibility of scotch-tape delamination (exfoliation), resulting in phosphorene, a graphene-like 2D material with excellent charge and thermal transport properties.Highly anisotropic thermal conductivity has been measured in three major principal crystal orientations. Exfoliated black phosphorus sublimes at 400 °C in vacuum. It gradually oxidizes when exposed to water in the presence of oxygen, which is a concern when contemplating it as a material for the manufacture of transistors, for example.\n\nSingle-layer blue phosphorus was first produced in 2016 by the method of molecular beam epitaxy from black phosphorus as precursor.\n\nThe diphosphorus allotrope (P) can normally be obtained only under extreme conditions (for example, from P at 1100 kelvin). In 2006, the diatomic molecule was generated in homogenous solution under normal conditions with the use of transition metal complexes (for example, tungsten and niobium).\n\nDiphosphorus is the gaseous form of phosphorus, and the thermodynamically stable form between 1200 °C and 2000 °C. The dissociation of tetraphosphorus () begins at lower temperature: the percentage of at 800 °C is ≈ 1%. At temperatures above about 2000 °C, the diphosphorus molecule begins to dissociate into atomic phosphorus.\n\nP nanorod polymers were isolated from CuI-P complexes using low temperature treatment.\n\nRed/brown phosphorus was shown to be stable in air for several weeks and have significantly different properties from red phosphorus. Electron microscopy showed that red/brown phosphorus forms long, parallel nanorods with a diameter between 3.4 Å and 4.7 Å.\n\n"}
{"id": "897047", "url": "https://en.wikipedia.org/wiki?curid=897047", "title": "Arc-fault circuit interrupter", "text": "Arc-fault circuit interrupter\n\nAn arc-fault circuit interrupter (AFCI) also known as an arc-fault detection device (AFDD) is a circuit breaker that breaks the circuit when it detects an electric arc in the circuit it protects to prevent electrical fires. An AFCI selectively distinguishes between a harmless arc (incidental to normal operation of switches, plugs, and brushed motors), and a potentially dangerous arc (that can occur, for example, in a lamp cord which has a broken conductor).\n\nAFCI breakers have been required for circuits feeding electrical outlets in residential bedrooms by the electrical codes of Canada and the United States since the beginning of the 21st century; the U.S. \"National Electrical Code\" has required them to protect most residential outlets since 2014, and the \"Canadian Electrical Code\" has since 2015. In parts of the world using 230 V, where the higher voltage implies lower currents, specifically Western Europe and the UK, adoption is slower, and their use is optional, except in high risk cases.\n\nIn the USA, arc faults are one of the leading causes for residential electrical fires. Each year in the United States, over 40,000 fires are attributed to home electrical wiring. These fires result in over 350 deaths and over 1,400 injuries each year.\n\nConventional circuit breakers only respond to overloads and short circuits, so they do not protect against arcing conditions that produce erratic, and often reduced current. An AFCI is selective so that normal arcs do not cause it to trip. The AFCI circuitry continuously monitors the current and discriminates between normal and unwanted arcing conditions. Once detected, the AFCI opens its internal contacts, thus de-energizing the circuit and reducing the potential for a fire to occur.\n\nThe electronics inside an AFCI breaker detect electrical current alternating at characteristic frequencies, usually around 100 kHz, known to be associated with wire arcing, which are sustained for more than a few milliseconds. A \"combination AFCI breaker\" provides protection against parallel arcing (line to neutral), series arcing (a loose, broken, or otherwise high resistance segment in a single line), ground arcing (from line, or neutral, to ground), overload protection and short circuit protection.\n\nAFCI receptacles contain electronic components to monitor a circuit for the presence of dangerous arcing conditions. Based upon an established threshold in the sine wave, the AFCI can be triggered to quickly react and de-power a circuit if dangerous arcing is detected.\n\nWhen installed as the first outlet on a branch circuit, AFCI receptacles provide series arc protection for the entire branch circuit. They also provide parallel arc protection for the branch circuit starting at the AFCI receptacle. Unlike AFCI breakers, AFCI receptacles may be used on any wiring system regardless of the panel.\n\nStarting with the 1999 version of the \"National Electrical Code\" in the United States, and the 2002 version of the \"Canadian Electrical Code\" in Canada, the national codes require AFCIs in all circuits that feed outlets in bedrooms of dwelling units. As of the 2014 NEC, AFCI protection is required on all branch circuits supplying outlets or devices installed in dwelling unit kitchens, along with the 2008 NEC additions of family rooms, dining rooms, living rooms, parlors, libraries, dens, bedrooms, sunrooms, recreation rooms, closets, hallways, laundry areas, and similar rooms and areas. They are also required in dormitory units. This requirement may be accomplished by using a \"combination type\" breaker—a specific kind of circuit-breaker defined by UL 1699—in the breaker panel that provides combined arc-fault and overcurrent protection or by using an AFCI receptacle for modifications/extensions, as replacement receptacles or in new construction, at the first outlet on the branch. Not all U.S. jurisdictions have adopted the NEC's AFCI requirements so it is important to check local code requirements.\n\nThe AFCI is intended to prevent fire from arcs. AFCI circuit breakers are designed to meet one of two standards as specified by UL 1699: \"branch\" type or \"combination\" type (note: the Canadian Electrical Code uses different terminology but similar technical requirements). A branch type AFCI trips on 75 amperes of arcing current from the line wire to either the neutral or ground wire. A combination type adds series arcing detection to branch type performance. Combination type AFCIs trip on 5 amperes of series arcing.\n\nAFCI receptacles are an alternative solution to AFCI breakers. These receptacles are designed to address the dangers associated with both types of potentially hazardous arcing: parallel and series. AFCI receptacles offer the benefit of localized \"test\" and \"reset\" with such buttons located on the face of the device. This is very convenient and saves a journey to the breaker panel.\n\nIn 2002, the NEC removed the word \"receptacle\", leaving \"outlets\", with the effect that lights and other wired-in devices such as ceiling fans within bedrooms were added to the requirement. The 2005 code made it clearer that all outlets must be protected despite discussion in the code-making panel about excluding bedroom smoke detectors from the requirement. \"Outlets\" as defined in the NEC includes receptacles, light fixtures and smoke alarms, among other things. Basically, any point where electricity is used to power something is an outlet.\n\nAs of January 2008, only \"combination type\" AFCIs meet the NEC requirement. The 2008 NEC requires the installation of combination-type AFCIs in all 15 and 20 ampere residential circuits with the exception of laundries, kitchens, bathrooms, garages, and unfinished basements, though many of these require GFCI protection. The 2014 NEC adds kitchens and laundry rooms to the list of rooms requiring AFCI circuitry, as well as any devices (such as lighting) requiring protection.\n\nIn the UK the 2018 edition of the wiring regulations is the first edition to make any mention of arc fault devices, and indicate they may be installed if the design has an unusually high risk of fire from arc faults. The annexes relating to testing indicate than when AFDDs are installed, their correct operation must be verified before completion, but the method of testing is not described. This is in contrast to RCDs where a number of trip times at different fault current levels must be verified. \nThe German Wiring rules VDE 100, recommend AFDDs for high risk situations, and lists old peoples care homes, community centres and Kindergartens as examples.\n\nAFCIs are designed to protect against fires caused by electrical arc faults. While the sensitivity of the AFCIs helps in the detection of arc faults, these breakers can also indicate false positives by identifying normal circuit behaviors as arc faults. For instance, lightning strikes provide voltage and current profiles that resemble arc faults, and vacuum cleaners and some laser printers trip AFCIs. This nuisance tripping reduces the overall effectiveness of AFCIs. Research into advancements in this area is being pursued.\n\nAlso, AFCIs provide no specific protection against \"glowing\" connections also known as a High Resistance Connection, excess current, high line voltages or low line voltages. AFCI circuit breakers include a standard inverse-time circuit breaker. Glowing connections occur when relatively high current exists in a relatively large resistance object. Heat comes from power dissipation. This energy, when dissipated in a small junction area, can generate temperatures above 1000 °C (1800 °F) and can ignite most flammable materials.\n\nBad wiring junctions can occur in utilization equipment, cords, or in-situ wiring and especially in a defective switch, socket, plug, wiring connection and even at the circuit breaker or fuse panels. Terminal screws loosened by vibration, improper tightening or other causes offer increased resistance to the current, with consequent heating and potential thermal creep, which will cause the termination to loosen further and exacerbate the heating effect. In North America, high resistance junctions are sometimes observed at the terminations of aluminum wire circuits, where oxidation has caused increased resistance, resulting in thermal creep. No technology located in a circuit breaker or fuse panel could detect a high-resistance wiring fault as no measurable characteristic exists that differentiates a glow fault from normal branch circuit operation. Power Fault Circuit Interrupters (PFCI) located in receptacles are designed to prevent fires caused by glowing connections in wiring or panels. From the receptacle a PFCI can detect the voltage drop when high current exists in a high resistance junction. In a properly designed and maintained circuit, substantial voltage drops should never occur. Proper wire terminations inside utilization equipment, such as appliances, and cords prevent high-resistance connections that could lead to fires.\n\nAn AFCI does not detect high line voltage due to an open neutral in a multiwire branch circuit. A multiwire branch circuit has both energized wires of a 120-240 V split phase service. If the neutral is broken, devices connected from a 120 V leg to the neutral may experience excess voltage, up to twice normal.\n\nAFCIs do not detect low line voltage. Low line voltage can cause electromechanical relays to repeatedly turn off and on. If current is flowing through the load contacts it causes arcing across the contacts as they open. The arcing can oxidize, pit and melt the contacts. This process can increase the contact resistance, superheat the relay and lead to fires. Power fault circuit interrupters are designed to prevent fires from low voltage across loads.\n\nAFCIs are also known to be sensitive (false tripping) to the presence of radio frequency energy, especially within the so-called \"high frequency\" spectrum (3-30 MHz) which include legitimate Citizens Band and Amateur Radio operations. Sensitivities and mitigation have been known since 2013.\n\nAFCIs may interfere with the operation of some power line communication technologies.\n\n\n"}
{"id": "20345231", "url": "https://en.wikipedia.org/wiki?curid=20345231", "title": "BioEthanol for Sustainable Transport", "text": "BioEthanol for Sustainable Transport\n\nBioEthanol for Sustainable Transport (BEST) was a four-year project financially supported by the European Union for promoting the introduction and market penetration of bioethanol as a vehicle fuel, and the introduction and wider use of flexible-fuel vehicles and ethanol-powered vehicles on the world market. The project began in January 2006 and continued until the end of 2009, and had nine participating regions or cities in Europe, Brazil, and China.\n\nThe BEST project targets included the introduction of more than 10,000 flex-fuel or ethanol cars and 160 ethanol buses; to promote the opening of 135 E85 and 13 ED95 public fuel stations; and to promote the development and testing of hydrous E15 and anhydrous low ethanol blends with gasoline and diesel.\n\nThere were ten participating cities and regions, and several commercial partners. Stockholm (Sweden) was the coordinating city, and other participants were Basque Country and Madrid (Spain), the Biofuel Region in Sweden, Brandenburg (Germany), La Spezia (Italy), Nanyang (China), Rotterdam (Netherlands), São Paulo (Brazil), and Somerset (UK). The commercial partners were Ford Europe, Saab Automobile and several bioethanol suppliers.\n\nA major activity in BEST was the promotion of E85 flexifuel vehicles (FFVs). During the project nine BEST sites introduced over 77,000 FFVs, far exceeding the original project's target of 10,000 vehicles. In 2008, out of the 170,000 flexifuel vehicles in operation in Europe, 45% of the vehicles operated at BEST sites; and out of 2,200 E85 pumps installed in the EU, 80% are found in the BEST countries. Sweden stands out with 70% of all flexifuel vehicles operating in the EU. BEST sites also evaluated both dedicated E85 pumps and flexifuel pumps and found very few problems.\n\nThe project included demonstration of two types of bioethanol-powered buses, a diesel engine Scania bus running on ED95 (sugarcane ethanol plus an ignition improver) and a Dongfeng bus capable of running on both E100 and petrol (flexible-fuel bus). Fuel pumps were also installed at bus depots in the five participating cities.\n\nBEST demonstrated more than 138 bioethanol ED95 buses and 12 ED95 pumps at five sites, three in Europe, one in China and one in Brazil. These trials helped increase knowledge about bioethanol buses in the participating cities. An innovation within BEST was the demonstration of two dual-tank E100 buses developed by the Chinese vehicle producer Dongfeng Motor. All BEST sites will continue to drive their bioethanol buses in regular traffic and some cities are already planning to expand their fleets.\n\nThe trial demonstrations showed that ethanol-powered ED95 buses:\n\nUnder the auspices of the BEST project, the first ED95 bus began operations in São Paulo city on December 2007 as a trial project. The bus is a Scania with a modified diesel engine capable of running with 95% hydrous ethanol with 5% ignition improver. Scania adjusted the compression ratio from 18:1 to 28:1, added larger fuel injection nozzles, and altered the injection timing.\n\nDuring the first year trial period performance and emissions were monitored by the National Reference Center on Biomass (CENBIO - ) at the Universidade de São Paulo, and compared with similar diesel models, with special attention to carbon monoxide and particulate matter emissions. Performance is also important as previous tests have shown a reduction in fuel economy of around 60% when E95 is compared to regular diesel.\n\nIn November 2009, a second ED95 bus began operating in São Paulo city. The bus was a Swedish Scania engine and chassis with a CAIO bus body. The second bus was scheduled to operate between Lapa and Vila Mariana, passing through Avenida Paulista, one of the main business centers of São Paulo city.\n\nCENBIO laboratory tests found that as compared to diesel, carbon dioxide emissions are 80% lower with ED95, particulate drops by 90%, nitrogen oxide emissions are 62% lower, and there are no sulphur emissions. During the trial was observed that the first bus began presenting sudden halts of the engine in slow running. The problem manifested more frequently in hot days, when the ambient temperature reached 26 °C or more and on top of long grades. After analyzing carefully the problem in the engine's fuel power line, it was discovered that the bus was developed for the European temperate climate, where average temperatures are lower than in tropical climate. In hotter days, the temperature of the fuel line reached up to 58 °C, a temperature that could increase even more when the engine would be slow running. The excessive heating was causing the vaporization of the fuel in the power line of the engine. The solution found was to deviate the fuel return from the engine straight to the tank, and thus, adapting the engine to Brazilian climate conditions.\n\nBased on the satisfactory results obtained during the 3-year trial operation of the two buses, in November 2010 the municipal government of São Paulo city signed an agreement with UNICA, Cosan, Scania and \"Viação Metropolitana\"\", a local bus operator, to introduce a fleet of 50 ethanol-powered ED95 buses by May 2011. The city's government objective is to reduce the carbon footprint of the city's bus fleet which is made of 15,000 diesel-powered buses, and the final goal is for the entire bus fleet to use only renewable fuels by 2018 . The first ethanol-powered buses were delivered in May 2011, and the 50 ethanol-powered ED95 buses will begin regular service in June 2011.\n\nIn Nanyang, Henan, a new type of bioethanol flexible-fuel bus capable of running on petrol or neat ethanol fuel (E100) was developed by Dongfeng Motor. The buses look like conventional buses and have two fuel tanks, one for petrol and one for E100. Two buses were demonstrated by local bioethanol producer Tianguan, who also supplied E100 for the buses. One fuel pump was set up for the trial. One of the buses uses a modified petrol-engine and the other uses a modified natural gas engine. The new bus types were developed to overcome import duties and are a low-cost alternative for Chinese cities seeking to introduce bioethanol to their public transport systems. Each E100 bus developed by Dongfeng costs around , which is more expensive than a conventional petrol bus.\n\nThree ED95 buses and one fuel pump was installed in La Spezia.\n\nFive ED95 buses operated in Madrid and one fuel pump was installed.\n\nIn Stockholm a total of 127 ED95 buses and five ED95 ethanol fuel stations were funded within the BEST project.\n\n\n"}
{"id": "23561715", "url": "https://en.wikipedia.org/wiki?curid=23561715", "title": "Carbon profiling", "text": "Carbon profiling\n\nCarbon profiling is a mathematical process that calculates how much carbon dioxide is put into the atmosphere per m of space in a building over one year.\n\nThe analysis is in two parts which are then added together to produce an overall figure which is termed the ‘Carbon Profile’:\n\nEmbodied carbon emissions relate to the amount of carbon dioxide emitted into the atmosphere from creating and maintaining the materials that form the building e.g. the carbon dioxide released from the baking of bricks or smelting or iron. In the Carbon Profiling Model these emissions are measured as Embodied Carbon Efficiency (ECE), measured as kg of CO/m/year.\n\nOccupational Carbon Emissions relate to the amount of Carbon Dioxide emitted into the atmosphere from the direct use of energy to run the building e.g. the heating or electricity used by the building over the year. In the Carbon Profiling Model these emissions are measured in BER’s (Building Emission Rate) in kg of /m/year.\n\nThe BER is a United Kingdom government accepted unit of measurement that comes from an approved calculation process called sBEM (Simplified Building Emission Model)\n\nThe purpose of Carbon Profiling is to provide a method of analyzing and comparing both operational and embodied carbon emissions at the same time. With this information it is then possible to allocate a projects resources in such a way to minimize the total amount of Carbon Dioxide emitted into the atmosphere through the use of a given piece of space.\n\nA secondary benefit is that having quantified the Carbon Profiling of different buildings it is then possible to make comparisons and rank buildings in term of their performance. This allows investors and occupiers to identify which building are good and bad carbon investments.\n\nSimon Sturgis and Gareth Roberts of Sturgis Associates in the United Kingdom originally developed ‘Carbon Profiling’ in December 2007.\n"}
{"id": "396928", "url": "https://en.wikipedia.org/wiki?curid=396928", "title": "China Airlines Flight 642", "text": "China Airlines Flight 642\n\nChina Airlines Flight 642 (operated by Mandarin Airlines) was a flight that crashed at Hong Kong International Airport on 22 August 1999. It was operating from Bangkok (Bangkok International Airport, now renamed Don Mueang International Airport) to Taipei with a stopover in Hong Kong.\n\nThe plane, a McDonnell Douglas MD-11 (registration , was operated by subsidiary Mandarin Airlines on behalf of China Airlines. While landing during a typhoon, it touched down hard, flipped over and caught fire. Of the 315 people on board, 312 survived and three were killed.\n\nThe route continues to operate today with the flight no longer originating in Bangkok and is strictly a Hong Kong-Taipei route. Bangkok-Hong Kong service ended on 31 October 2010. The route is now flown by a Boeing 747-400 and Airbus A330-300.\n\nFlight 642 was one of only two hull losses of MD-11s with passenger configuration, the other being Swissair Flight 111, which crashed in 1998 with 229 fatalities. All other hull losses of MD-11s have been when the aircraft has been serving as a cargo aircraft.\n\nAt about 6:43 P.M. local time (1043 UTC) on 22 August 1999, the MD-11 was making its final approach to runway 25L when Typhoon Sam was 50 km NE of the airport. At an altitude of 700 feet prior to touchdown a further wind check was reported to the crew: 320 deg/28 knots gusting to 36 knots. This resulted in a crosswind vector of 26 knots gusting to 33 knots, while the tested limit for the aircraft was .\n\nDuring the final flare to land, the plane rolled to the right, landed hard on its right main gear and the No. 3 engine touched the runway. The right wing separated from the fuselage. The aircraft continued to roll over and skidded off the runway in flames. When it stopped, it was on its back and the rear of the plane was on fire, coming to rest on a grass area next to the runway, 3500 ft from the runway threshold. The right wing was found on a taxiway 280ft from the nose of the plane. As shown in photos of the aircraft at rest, the fire caused significant damage to the rear section of the aircraft but was quickly extinguished due to the heavy rain and quick response from rescue teams in the airport.\n\nThree of the 300 passengers died; all 15 crew members survived.\n\nThe final report of the accident blamed it mainly on pilot error; specifically the inability to arrest the high rate of descent existing at 50 ft altitude on the radar altimeter. The descent rate at touch down was 18–20 ft/s.\n\nThe flight data stored in the volatile memory of the aircraft's Quick Access Recorder (QAR) during the last 500 ft of the approach could not be recovered due to the interruption of the power supply at impact. Probable wind variations and the loss of headwind component, together with the early retardation of thrust levers, led to a 20 kt loss in indicated airspeed just prior to touchdown.\n\nDue to the severe weather conditions forecast for Hong Kong, the flight crew had prepared to divert the flight to Taipei if the situation at Hong Kong was deemed unsuitable for landing. Extra fuel was carried for this possibility, resulting in a landing weight of 429,557 lbs, or 99.897% maximum landing weight (mlw), which is below its mlw of . Based on the initial weather and wind check which was passed along to the crew from Hong Kong during the flight, they believed they could land there and decided against a diversion to Taipei. However four earlier flights had carried out missed approaches at Hong Kong and five had diverted.\n\nDuring the final approach, the plane descended along the ILS glideslope until at about 700 ft, the crew visually acquired the runway. They disengaged the autopilot but left the autothrottle on. During the flare, the rate of descent was not arrested, the plane landed with the right wing slightly lower. The right landing gear touched down first, the right engine impacted the runway and the right wing was detached from the fuselage. Since the left wing was still attached, the lift from that wing rolled the fuselage onto its right side, and the plane came to rest inverted in the grass strip next to the runway. The spilled fuel caught fire.\n\nSeveral suggestions were given to China Airlines concerning its training.\n\nThe landing and crash of Flight 642 was recorded by nearby occupants in a car. The video which also captured reactions from the witnesses has been uploaded to websites such as YouTube and Airdisaster.com.\n\nA photo showing another Mandarin Airlines MD-11 taxiing past the remains of Flight 642 was circulated.\n\nThis disaster was also aired on RTHK's \"Elite Brigade II\" Episode 2 in 2012.\n\n"}
{"id": "27420956", "url": "https://en.wikipedia.org/wiki?curid=27420956", "title": "Clean Air Cab", "text": "Clean Air Cab\n\nClean Air Cab was an Arizona-based carbon-neutral taxi service provider, and the second taxi fleet in the U.S. to use only hybrid vehicles. Based in Mesa, Arizona, it served the Phoenix area with environmentally friendly transportation service by using clean automotive technology. Clean Air cab ceased operations November 2016. \n\nThe main goal of the operation was to be an environmentally aware business, and to take action to mitigate the damage done by pollution while providing a reliable and competitive transportation service. Launched in October 2009, it was the first subsidiary company of Beyond Green, L.L.C.\n\nThe taxi fleet once consisted of 29 Toyota Prius vehicles, whose hybrid design helped cut emissions by 66% compared to typical taxi fleets of Ford Crown Victorias. Only vehicles that qualified as Super Ultra Low Emission Vehicles, which are hybrid vehicles that work to minimize air pollution, were used. Using hybrid vehicles helped the state of Arizona do its part in reducing greenhouse gas emissions in compliance with the Western Climate Initiative, which is \"a collaboration of independent jurisdictions who commit to work together to identify, evaluate, and implement policies to tackle climate change at a regional level.\"\n\nThe company purchased carbon offsets and supported reforestation by subsidizing the planting of 10 rain forest trees monthly for each cab in service. Their atmospheric carbon reduction partners were Trees for Tempe at the local level, and Trees for the Future and Carbonfund.org on a global level. . Since October 19, 2009 over 15,000 trees had been planted.\n\nClean Air Cab had a \"Pink Cab\", where some of the fares are donated to Susan G. Komen for the Cure. Three more cabs were added in 2011, for the Phoenix Children's hospital, United Way, and Arizona State University. In December 2009, Clean Air Cab sponsored the Pro Players Classic charity golf tournament. Clean Air Cab actively participated in the Valley Metro's Adopt-A-Station program to maintain cleanliness at light rail stations, promoting the use of environmentally friendly transportation choices.\n\nClean Air Cab was a member of both the Phoenix Green Chamber of Commerce and the Greater Phoenix Gay & Lesbian Chamber of Commerce. It was also a member of Valley Forward, a Phoenix-based non-profit organization focusing on environmental protection and sustainability. In February 2013 Clean air Cab joined One Community and signed the Unity Pledge decreeing equality for all individuals regardless of orientation in the work place.\n\n\n"}
{"id": "3272494", "url": "https://en.wikipedia.org/wiki?curid=3272494", "title": "Cloughmore", "text": "Cloughmore\n\nCloughmore or Cloghmore (), known locally as \"The Big Stone\", is a huge granite boulder perched on a mountainside almost above the village of Rostrevor, County Down, Northern Ireland. It sits on the slopes of Slieve Martin, overlooking Rostrevor Forest, Carlingford Lough and the Cooley Peninsula. It is popular destination for visitors, and is part of a National Nature Reserve and Area of Special Scientific Interest.\n\nThe granite boulder, which has a calculated mass of 50 tonnes, is a glacial erratic, thought to have been transported from Scotland (from an island in Strathclyde bay) and deposited about 10,000 years ago by retreating ice during the last Ice Age. It sits on a relatively flat area of Silurian metasedimentary rock.\n\nLocal legend has it that the stone was thrown from the Cooley Mountains, on the other side of Carlingford Lough, by the giant Fionn mac Cumhaill.\n"}
{"id": "3093672", "url": "https://en.wikipedia.org/wiki?curid=3093672", "title": "Common beta emitters", "text": "Common beta emitters\n\nStrontium-90 is a commonly used beta emitter used in industrial sources. It decays to Yttrium-90, which is itself a beta emitter. It is also used as a thermal power source in radioisotope thermoelectric generator power packs. These use heat produced by radioactive decay of strontium-90 to generate heat, which can be converted to electricity using a thermocouple. Strontium-90 has a shorter half-life, produces less power, and requires more shielding than Plutonium-238, but is cheaper as it is a fission product and is present in a high concentration in nuclear waste and can be relatively easily chemically extracted. Strontium-90 based RTGs have been used to power remote lighthouses. \n\nStrontium-89 is a short lived beta emitter which has been used as a treatment for bone tumors, this is used in palliative care in terminal cancer cases. Both strontium-89 and strontium-90 are fission products.\n\nTritium is a low-energy beta emitter commonly used as a radiotracer in research and in traser self-powered lightings. The half-life of tritium is 12.3 years. The electrons from beta emission from tritium are so low in energy (average decay energy 5.7 keV) that a Geiger counter cannot be used to detect them. An advantage of the low energy of the decay is that it is easy to shield, since the low energy electrons penetrate only to shallow depths, reducing the safety issues in deal with the isotope. Note that one of the first-aid treatments for the intake of tritium (as tritiated water) in a human is to give the human plenty of water to drink. This means that the tritated water will be mixed with normal water and will therefore be flushed out by the body more rapidly.\n\nTritium can also be found in metal work in the form of a tritiated rust, this can be treated by heating the steel in a furnace to drive off the tritium-containing water.\n\nTritium can be made by the neutron irradiation of lithium.\n\nCarbon-14 is also commonly used as a beta source in research, it is commonly used as a radiotracer in organic compounds. While the energy of the beta particles is higher than those of tritium they are still quite low in energy. For instance the walls of a glass bottle are able to absorb it. Carbon-14 is made by the np reaction of nitrogen-14 with neutrons. It is generated in the atmosphere by the action of cosmic rays on nitrogen. Also a large amount was generated by the neutrons from the air bursts during nuclear weapons testing conducted in the 20th century. The specific activity of atmospheric carbon increased as a result of the nuclear testing but due to the exchange of carbon between the air and other parts of the carbon cycle it has now returned to a very low value. For small amounts of carbon-14 one of the favoured disposal methods is to burn the waste in a medical incinerator, the idea is that by dispersing the radioactivity over a very wide area the threat to any one human is very small.\n\nPhosphorus-32 is a short-lived high energy beta emitter, which is used in research in radiotracers. It has a half-life of 14 days. It can be used in DNA research. Phosphorus-32 can be made by the neutron irradiation (np reaction) of sulfur-32 or from Phosphorus-31 by neutron capture.\n\nNickel-63 is a special isotope of nickel that can be used as an energy source in Radioisotope Piezoelectric Generators. It has a half-life of 100.1 years. It can be created by irradiating Nickel-62 with neutrons in a nuclear reactor.\n\n\n"}
{"id": "2096333", "url": "https://en.wikipedia.org/wiki?curid=2096333", "title": "Cooperative Institute for Arctic Research", "text": "Cooperative Institute for Arctic Research\n\nThe Cooperative Institute for Arctic Research is designed to be a focal point for interactions between the National Oceanic and Atmospheric Administration (NOAA)/Office of Oceanic and Atmospheric Research (OAR) and the Arctic research community through the University of Alaska for research related to the Western Arctic/Bering Sea region. \n\n(CIFAR) was established through a Memorandum of Understanding between NOAA and the University of Alaska. CIFAR is exclusively concerned with Arctic research. They work closely with NOAA's Arctic Research Office and the Pacific Marine Environmental Laboratory (PMEL). Partnerships with NOAA also include the National Marine Fisheries Service (NMFS), the National Ocean Service (NOS), and an emerging relationship with the National Weather Service.\n\n"}
{"id": "47384181", "url": "https://en.wikipedia.org/wiki?curid=47384181", "title": "Cowardin classification system", "text": "Cowardin classification system\n\nThe Cowardin classification system is a system for classifying wetlands, devised by Lewis M. Cowardin \"et al.\" in 1979 for the United States Fish and Wildlife Service. The system includes five main types of wetlands:\nThe primary purpose of this ecological classification system was to establish consistent terms and definitions used in inventory of wetlands and to provide standard measurements for mapping these lands.\n\n\n"}
{"id": "14662209", "url": "https://en.wikipedia.org/wiki?curid=14662209", "title": "Darro Solar Park", "text": "Darro Solar Park\n\nDarro Solar Park, also referred to as Solarpark Darro, is a 5.8 MWp photovoltaic power plant located in Darro, Spain. There are a total of 29,964 solar panels on 710 dual-axis trackers, which follows the sun throughout the day. The solar panels, or modules, are Conergy C180M and SunPower STM210, and the trackers are SolarOptimus by Conergy.\n\nThe plant was built by EPURON GmbH, and SunTechnics GmbH, and completed in 2007.\n\n"}
{"id": "1212255", "url": "https://en.wikipedia.org/wiki?curid=1212255", "title": "Eastern Guinean forests", "text": "Eastern Guinean forests\n\nThe Eastern Guinean forests are a tropical moist broadleaf forest ecoregion of West Africa. The ecoregion includes the lowland forests extending from the Gulf of Guinea a few hundred kilometers inland, from western Côte d'Ivoire to the western shore of Lake Volta in Ghana. A few small enclaves lie further east and inland in Togo and Benin. The Sassandra River of Côte d'Ivoire separates the Eastern Guinean forests from the Western Guinean forests which lie to the west. Inland and to the east, the Eastern Guinean forests transition to the Guinean forest-savanna mosaic.\n\nThe Eastern Guinean forests, together with the other tropical moist broadleaf forests of West Africa, is included within Conservation International's Guinean Forests of West Africa biodiversity hotspot.\n\n"}
{"id": "23296121", "url": "https://en.wikipedia.org/wiki?curid=23296121", "title": "Energy in Benin", "text": "Energy in Benin\n\nEnergy in Benin is derived from multiple sources, including, oil and natural gas.\n\nProduction from the Sémé off shore oil field began in October 1982, operated by Saga Petroleum, a Norwegian company working under a service contract. The field yielded of oil in 1991. In 1990, Benin exported an estimated of crude oil. In 1986, the contract was transferred to Pan Ocean Oil (Panoco), a Swiss-based US company, but loans to Benin from international development agencies were frozen because the company could not furnish satisfactory financial and capability statements; it withdrew, forcing Benin to take over production. Reserves, estimated at , were considered sufficient to meet domestic needs. However, Benin has no refinery; consequently, refined petroleum products have to be re-imported. In 2002, imports of refined petroleum products amounted to . The company responsible for petroleum imports is Société Nationale de Commercialisation des Produits Pétroliers (SONACOP).\n\nBenin ceased petroleum production from its Seme oilfield in 1998. America's Kosmos Energy LLC explored for petroleum in 2006.\n\nAlthough Benin has natural gas reserves, natural gas production is not active. The company responsible for gas imports is Société Beninoise de Gaz. Natural gas is supplied to Benin by the West African Gas Pipeline.\n\nElectrical generating capacity in 2002, totaled 122 MW. Total domestic power output in that same year was 55 GWh, of which hydropower accounted for 2 GWh and fossil fuels for the rest. Electricity consumption in 2002 was 488 GWh. An agreement was signed with Togo and Ghana in 1967 under which Benin receives low-cost electric power from Akosombo Dam on the Volta River in Ghana. Total electricity imports for 2002 were estimated at 437 GWh. Togo and Benin are constructing a dam on the Mono River, along the Togo border, that will feed a power station to supply the southern regions of both countries. Benin also imports electricity from Nigeria through the CEB-NEPA Power Interconnection, commissioned in 2007.\n"}
{"id": "5356128", "url": "https://en.wikipedia.org/wiki?curid=5356128", "title": "Ethanol fuel energy balance", "text": "Ethanol fuel energy balance\n\nIn order to create ethanol, all biomass needs to go through some of these steps: it needs to be grown, collected, dried, fermented, and burned. All of these steps require resources and an infrastructure. The ratio of the energy released by burning the resulting ethanol fuel to the energy used in the process, is known as the ethanol fuel energy balance (sometimes called \"Net energy gain\") and studied as part of the wider field of energy economics. Figures compiled in a 2007 National Geographic Magazine article point to modest results for corn ethanol produced in the US: 1 unit of energy input equals 1.3 energy units of corn ethanol energy. The energy balance for sugarcane ethanol produced in Brazil is much more favorable, 1 to 8. Over the years, however, many reports have been produced with contradicting energy balance estimates. A 2006 University of California Berkeley study, after analyzing six separate studies, concluded that producing ethanol from corn uses marginally less petroleum than producing gasoline.\n\nIn 1995 the USDA released a report stating that the net energy balance of corn ethanol in the United States was an average of 1.24. It was previously considered to have a negative net energy balance. However, due to increases in corn crop yield and more efficient farming practices corn ethanol had gained energy efficiency \n\nKen Cassman, a professor of agronomy at the University of Nebraska–Lincoln, said in 2008 that ethanol has a substantial net positive direct energy balance—1.5 to 1.6 more units of energy are derived from ethanol than are used to produce it. Comparing 2008 to 2003, Alan Tiemann of Seward, a Nebraska Corn Board member, said that ethanol plants produce 15 percent more ethanol from a bushel of corn and use about 20 percent less energy in the process. At the same time, corn growers are more efficient, producing more corn per acre and using less energy to do so.\n\nOpponents of corn ethanol production in the U.S. often quote the 2005 paper of David Pimentel, a retired Entomologist, and Tadeusz Patzek, a Geological Engineer from UC Berkeley. Both have been exceptionally critical of ethanol and other biofuels. Their studies contend that ethanol, and biofuels in general, are \"energy negative\", meaning they take more energy to produce than is contained in the final product.\n\nA 2006 article in Science offers the consensus opinion that current corn ethanol technologies had similar greenhouse gas emissions to gasoline, but was much less petroleum-intensive than gasoline. Fossil fuels also require significant energy inputs which have seldom been accounted for in the past.\n\nEthanol is not the only product created during production. By-products also have energy content. Corn is typically 66% starch and the remaining 33% is not fermented. This unfermented component is called distillers grain, which is high in fats and proteins, and makes a good animal feed supplement.\n\nIn 2000, Dr. Michael Wang, of Argonne National Laboratory, wrote that these ethanol by-products are the most contentious issue in evaluating the energy balance of ethanol. He wrote that Pimentel assumes that corn ethanol entirely replaces gasoline and so the quantity of by-products is too large for the market to absorb, and they become waste. At lower quantities of production, Wang finds it appropriate to credit corn ethanol based on the input energy requirement of the feed product or good that the ethanol by-product displaces. In 2004, a USDA report found that co-products accounting made the difference between energy ratios of 1.06 and 1.67. In 2006, MIT researcher Tiffany Groode came to similar conclusions about the co-product issue.\n\nIn Brazil where sugar cane is used, the yield is higher, and conversion to ethanol is more energy efficient than corn. Recent developments with cellulosic ethanol production may improve yields even further.\n\nIn 2006 a study from the University of Minnesota found that corn-grain ethanol produced 1.25 units of energy per unit put in.\n\nA 2008 study by the University of Nebraska found a 5.4 energy balance for ethanol derived specifically from switchgrass. This estimate is better than in previous studies and according to the authors partly due to the larger size of the field trial (3-9 ha) on 10 farms.\n\nAccording to DoE, to evaluate the net energy of ethanol four variables must be considered:\n\nMuch of the current academic discussion regarding ethanol currently revolves around issues of system borders. This refers to how complete of a picture is drawn for energy inputs. There is debate on whether to include items like the energy required to feed the people tending and processing the corn, to erect and repair farm fences, even the amount of energy a tractor represents.\n\nIn addition, there is no consensus on what sort of value to give the rest of the corn (such as the stalk), commonly known as the 'coproduct.' Some studies leave it on the field to protect the soil from erosion and to add organic matter, while others take and burn the coproduct to power the ethanol plant, but do not address the resulting soil erosion (which would require energy in the form of fertilizer to replace). Depending on the ethanol study you read, net energy returns vary from .7-1.5 units of ethanol per unit of fossil fuel energy consumed. For comparison, that same one unit of fossil fuel invested in oil and gas extraction (in the lower 48 States) will yield 15 units of gasoline, a yield an order of magnitude better than current ethanol production technologies, ignoring the energy quality arguments above and the fact that the gain (14 units) is both declining and not carbon neutral.\n\nIn this regard, geography is the decisive factor. In tropical regions with abundant water and land resources, such as Brazil and Colombia, the viability of production of ethanol from sugarcane is no longer in question; in fact, the burning of sugarcane residues (bagasse) generates far more energy than needed to operate the ethanol plants, and many of them are now selling electric energy to the utilities. However, while there may be a positive net energy return at the moment, recent research suggests that the sugarcane plantations are not sustainable in the long run, as they are depleting the soil of nutrients and carbon matter On the other hand, productivity of sugar cane per land area in Brazil has consistently grown over the decades; sugar cane has been shown to be less depleting to the soil than cattle and yearly cultures; and there are many regions in the country where sugar cane has been cultivated for centuries. Those facts suggest that related soil depletion processes are very slow and therefore ethanol from sugar cane may be far more sustainable in the long run than common fossil fuel alternatives. Besides, since the energy surplus is high in the case of sugar cane ethanol, conceivably part of that energy can be used to synthesize fertilizers and replenish soil depletion a long time, therefore making the process indefinitely sustainable.\n\nThe picture is different for other regions, such as most of the United States, where the climate is too cool for sugarcane. In the U.S., agricultural ethanol is generally obtained from grain, chiefly corn. But it can also be obtained from cellulose, a more energy balanced bioethanol.\n\nClean production bioethanol is a biofuel obtained by maximizing non-greenhouse gas emitting (renewable) resources: \nUsing ethanol returns carbon to the atmosphere whereas burning gasoline adds carbon to the atmosphere. Thus the effects of gasoline burning increase over time.\n\n"}
{"id": "2078801", "url": "https://en.wikipedia.org/wiki?curid=2078801", "title": "Ettingshausen effect", "text": "Ettingshausen effect\n\nThe Ettingshausen Effect (named for Albert von Ettingshausen) is a thermoelectric (or thermomagnetic) phenomenon that affects the electric current in a conductor when a magnetic field is present.\n\nEttingshausen and his PhD student Walther Nernst were studying the Hall effect in bismuth, and noticed an unexpected perpendicular current flow when one side of the sample was heated. This is also known as the Nernst effect. Conversely, when applying a current (along the y-axis) and a perpendicular magnetic field (along the z-axis) a temperature gradient appears along the x-axis. Because of the Hall effect, electrons are forced to move perpendicular to the applied current. Due to the accumulation of electrons on one side of the sample, the number of collisions increases and a heating of the material occurs.\nThis effect is quantified by the Ettingshausen coefficient \"P\", which is defined as:\n\nwhere \"dT/dx\" is the temperature gradient that results from the \"y\"-component \"J\" of an electric current density and the \"z\"-component \"B\" of a magnetic field.\n\nIn most metals like copper, silver and gold \"P\" is on the order of and thus difficult to observe in common magnetic fields. In bismuth the Ettingshausen coefficient is several orders of magnitude larger because of its poor thermal conductivity.\n\n"}
{"id": "6947256", "url": "https://en.wikipedia.org/wiki?curid=6947256", "title": "Evolution (TV series)", "text": "Evolution (TV series)\n\nEvolution is a 2001 documentary series by the American broadcaster Public Broadcasting Service (PBS) and WGBH on evolutionary biology, from the producers of \"NOVA\".\nThe spokespeople for the series were Jane Goodall (overall spokesperson), Kenneth R. Miller and Stephen Jay Gould (science spokespeople), Eugenie C. Scott (education spokesperson), Arthur Peacocke and Arnold Thomas (religious spokespeople). The series was narrated by the Irish actor Liam Neeson.\n\nThe series was accompanied by a book by the popular science writer Carl Zimmer \"\". An extensive website provides teaching resources for each episode's material, including \"The Mating Game\", further looks at Charles Darwin, and an interactive history of speciation in the invented \"pollencreeper\" birds.\n\nThe episode \"What about God?\" features discussion of the issues of evolution and creationism at Wheaton College, an Evangelical Protestant college that teaches evolution but has in the past restricted professors from taking a stance on the literal versus the allegorical interpretations of Adam and Eve in the Genesis account of creation.\n\n\nTV critic Julie Salamon, writing in \"The New York Times\", said that \"[a] powerful sense of drama, discovery and intellectual enthusiasm runs through this rich eight-hour series ... The series covers an enormous amount of ground but doesn't leave you feeling swamped.\"\n\nBeing made and broadcast in the country where creation-evolution controversy is strongest, the last episode \"What About God?\" focused on religion, and \"through personal stories of students and teachers, it offers the view that they are compatible\". Phina Borgeson, Faith Network Director of the National Center for Science Education, provided a Congregational Study Guide for Evolution. Conversely, the Discovery Institute's Center for the Renewal of Science and Culture produced a website to refute the documentary and started a petition it called A Scientific Dissent From Darwinism to show that there were \"scientists that dispute the claims\".\n\n"}
{"id": "27639129", "url": "https://en.wikipedia.org/wiki?curid=27639129", "title": "Fessenden oscillator", "text": "Fessenden oscillator\n\nA Fessenden oscillator is an electro-acoustic transducer invented by Reginald Fessenden, with development starting in 1912 at the Submarine Signal Company of Boston. It was the first successful acoustical echo ranging device. Similar in operating principle to a dynamic voice coil loudspeaker, it was an early kind of transducer, capable of creating underwater sounds and of picking up their echoes.\n\nThe creation of this device was motivated by the RMS \"Titanic\" disaster of 1912, which highlighted the need to protect ships from collisions with icebergs, obstacles, and other ships. Because of its relatively low operating frequency, it has been replaced in modern transducers by piezoelectric devices.\n\nThe \"oscillator\" in the name referred to the fact that the device vibrated and moved water in response to a driving AC current. It was not an electronic oscillator but a mechanical one in that it generated repetitive mechanical vibrations. Electronic oscillators did not yet exist when this device was created. Because the design of the device does not depend on a resonant response, it should not be considered an harmonic oscillator.\n\nThe Fessenden oscillator somewhat resembled a modern dynamic microphone or dynamic loudspeaker in overall construction. A circular metal plate, clamped at its edge, in contact with the water on one side, was attached on the other side to a copper tube, which was free to move in the circular gap of a magnet system. The magnet system had a direct-current winding to provide a polarizing magnetic field in the gap, and an alternating current winding that induced currents in the copper tube. These induced currents produced a magnetic field that reacted against the polarizing field. The resulting force was communicated to the membrane and in turn provided acoustic vibrations into the water. \n\nUnlike previous underwater sound sources such as underwater bells, the Fessenden oscillator was reversible; the AC winding could be connected to a head set and underwater sounds and echoes could be heard. Using this device Fessenden was able to detect icebergs at a distance of about 2 miles, and occasionally detected echoes from the sea floor. \n\nThe device could also be used as an underwater telegraph, sending Morse code through the water. The \"Fessenden underwater signalling apparatus\", or more usually just \"The Fessenden\", was fitted to Royal Navy submarines in World War I.\nBritish K-series submarines were equipped with Fessenden oscillators starting in 1915. However, a submarine signalling the surface could be heard by any nearby (enemy) hydrophone, so the system had restricted utility during wartime patrols.\n\nDuring the First World War the Fessenden oscillator was applied to detection of submarines, but its rather low operating frequency of around 1 kilohertz gave it a very broad beam, unsuitable for detecting and localising small targets. In peacetime, the oscillator was used for depth finding, where the lack of directionality was not a concern, and Fessenden designed a commercial fathometer using a carbon microphone as receiver, for the Submarine Signal Company.\n\n\n"}
{"id": "25268229", "url": "https://en.wikipedia.org/wiki?curid=25268229", "title": "Final cover", "text": "Final cover\n\nFinal cover is a multilayered system of various materials which are primarily used to reduce the amount of storm water that will enter a landfill after closing. Proper final cover systems will also minimize the surface water on the liner system, resist erosion due to wind or runoff, control the migrations of landfill gases, and improve aesthetics.\n\nA final cover system can include a top soil layer composed of nutrient rich soil, a protective layer to reduce the effects of freeze/thaw, a drainage layer which moves storm water, a barrier layer, and a grading layer.\n\nFor a final cover system consisting of a geomembrane, an analysis of the mechanical properties of the geomembrane should be conducted to ensure cover integrity is not jeopardized by localized subsidence, bending, and cover slope stability.\n\nLocalized subsidence induces tensile stresses in geomembranes, which can threaten the final cover integrity. The magnitudes of the tensile stress can be viewed as a function of the dimensions of the subsidence zone and the properties of the cover soil. Allowable tensile stress of a geomembrane is usually known; in order to ensure the stability of final cover, the tensile stress allowable by a geomembrane should exceed the calculated value for tensile stress induced by waste.\n\nGeomembrane bending, both from its self-weight and soil cover may also induce tensile stress. Tensile stress from bending should not exceed the allowable tensile stress of the geomembrane.\n\nCover slope stability analysis involves evaluation of the interface strengths under static and seismic conditions. In order to perform an evaluation of the effects of a final cover system placed on a refuse side slope, slope stability analyses are required. A slope stability analysis assumes that the driving forces causing movement are due to the weight of the materials and the forces governing resistance are due to material strength. Most engineers design the permanent slope to have a minimum Factor of Safety of 1.5 at the designed inclination (typically 3H:1V) for static loading. Steepening of a slope past the design inclination increases the driving forces thus decreasing the Factor of Safety. Decreasing the Factor of Safety past 1.0 could possibly push the final cover system components beyond their limits of stability.\n\n"}
{"id": "47319123", "url": "https://en.wikipedia.org/wiki?curid=47319123", "title": "Forestry literature", "text": "Forestry literature\n\nForestry literature is the books, journals and other publications about forestry.\n\nThe first major works about forestry in the English language included Roger Taverner's \"Booke of Survey\" (1565), John Manwood's \"A Brefe Collection of the Lawes of the Forrest\" (1592) and John Evelyn's \"Sylva\" (1662).\n\n"}
{"id": "18890741", "url": "https://en.wikipedia.org/wiki?curid=18890741", "title": "Frank McMahon (oilman)", "text": "Frank McMahon (oilman)\n\nFrancis Murray Patrick \"Frank\" McMahon (2 October 1902 - 20 May 1986) was a Canadian oilman best known as the founder and first chairman of Westcoast Transmission Co. Ltd. whom the July 15, 1957 issue of \"TIME\" magazine called \"The man who did the most to open up northwest Canada's wilderness—and convince oilmen of its treasures.\"\n\nIn addition, McMahon was a major racehorse owner/breeder whose Thoroughbreds competed in North America and Europe and who won the 1969 Kentucky Derby and Preakness Stakes with the U.S. Racing Hall of Fame colt, Majestic Prince.\n\nFrank McMahon was born in the village of Moyie in the East Kootenays of British Columbia, Canada, the son of a hard-rock miner.\n\nHe attended Gonzaga University in Spokane, Washington, where he was a campus mate of Bing Crosby. As a young man, he worked as a driller for British Columbia mining companies until 1927 when he founded his own diamond-drilling contracting business which he expanded into drilling for oil and natural gas.\n\nFrank McMahon and two brothers established West Turner Petroleums to explore and develop oil deposits in the Turner Valley Oilfields in Alberta. Their major find generated sufficient revenues and income to allow for expansion through acquisitions and exploration. He later merged two smaller companies with West Turner and formed a holding company, Pacific Petroleums Ltd.\n\nIn 1945 McMahon founded Atlantic Oil Company and acquired rights to a part of the Leduc field, near Leduc, Alberta. After Imperial Oil's discovery of oil at Leduc in 1947, the Atlantic No. 3 well discovered oil on March 8, 1948. It was well-publicized due to the oil blowout that took six months to contain and a well fire that started in the week prior to the well being contained. The well propelled McMahon's wealth. Pacific's headquarters were set up in Calgary, Alberta.\n\nIn December 1947, McMahon's operations began oil and gas exploration in the Peace River Region after the B.C. government opened the area to exploration.\n\nIn November 1951, Pacific's Fort St. John No. 1 well found significant quantities of good quality oil, B.C.'s first oil discovery. In 1952 the company drilled the first of many high-producing gas wells at Fort St. John.\n\nIn about 1961 McMahon and his brother George sold their controlling interest in Pacific Petroleums to Phillips Petroleum Company. In 1977 Pacific Petroleums was purchased by Petro-Canada.\n\nMcMahon saw an enormous opportunity to supply natural gas to the huge United States market. In 1949 he incorporated Westcoast Transmission Co. Ltd. whose business plan included the construction of a 650-mile gas pipeline from Taylor in north-eastern British Columbia to the United States.\n\nMcMahon personally began lobbying the Canadian and American governments to remove their restrictions on the export and import of natural gas. In December 1954, he signed a $400-million contract with Pacific Northwest Pipeline Corp. to sell natural gas into their pipeline system in the United States. In 1955 Westcoast was awarded permission from the U.S. Federal Power Commission to export gas. Construction started the same year on the $170-million, 650-mile Westcoast Pipeline from the Peace River area to the U.S. border, to hook into the Pacific Northwest Pipeline Corp.'s six-state gas grid and to supply gas to Vancouver. The pipeline was Canada's first \"big-inch\" pipeline. Along with its gathering system, the processing plants and compressor stations were completed in the fall of 1957.\n\nIn 1964, Westcoast Transmission built another processing plant at Fort Nelson, British Columbia, in support of an additional 250-mile line to the company's new discoveries in the Canadian Northwest.\n\nAfter McMahon's death in 1986, Westcoast Transmission Co. Ltd. was renamed Westcoast Energy Inc. and in 2002 Duke Energy of Charlotte, North Carolina, paid $8 billion for the company.\n\nIn the 1950s McMahon had an interest in Alberta Distillers Ltd. In October 1957, Time Magazine estimated his worth at $50-million and that he controlled assets in partnership with others that totalled about $500-million.\n\nFrank McMahon's significant contribution to Canada's economic prosperity was recognized by his election to the Canadian Business Hall of Fame.\n\nIn the 1950s Frank McMahon backed a number of Broadway plays in New York, including The Pajama Game and Damn Yankees.\n\nFrank McMahon, a founding member of the Jockey Club of Canada, raced thoroughbreds on his own as well as in partnership with others.\n\nHis Frank McMahon Stable Inc. won numerous races at racetracks across Canada including the 1966 British Columbia Derby at Exhibition Park Racetrack in Vancouver and the 1970 Canadian Derby at Edmonton's Northlands Park. His most famous horse was Majestic Prince.\n\nFrank McMahon was inducted in the British Columbia Horse Racing Hall of Fame in 1995 in the breeders/owners category.\n\nHe set up a racing stable in California with U.S. Racing Hall of Fame jockey-turned-trainer Johnny Longden. Among the horses they raced was the great Majestic Prince whose important stakes include the 1969 Kentucky Derby and Preakness Stakes. Majestic Prince was inducted into the National Museum of Racing and Hall of Fame in 1988 and was listed at No. 46. in The Blood-Horse magazine ranking of the top 100 U.S. thoroughbred champions of the 20th Century. In 1975, McMahon had another Triple Crown contender with Diabolo who finished third in both the Kentucky Derby and Preakness Stakes, and was fourth in the Belmont Stakes.\n\nIn the 1950s he was part owner of Alberta Ranches, Ltd. which won the 1953 Hollywood Gold Cup with Royal Serenade.\n\nTogether with Calgary newspaper publisher Max Bell, Frank McMahon founded the Golden West Farms thoroughbred breeding operation at Okotoks, Alberta. Among its notable wins, Golden West Farms' racing stable won the 1968 Queen's Plate with Merger. In partnership with American singer, Bing Crosby, McMahon and Bell owned Meadow Court who raced in Europe where he won the 1965 Irish Derby and the King George VI and Queen Elizabeth Stakes.\n\nMcMahon also teamed up with Kentucky horseman Leslie Combs II of Spendthrift Farm to breed Crowned Prince, a sibling of Majestic Prince, who in 1970 became the first yearling to be sold at auction for half a million dollars. McMahon won the bidding for Crowned Prince and sent him to race in England where he won the Dewhurst and Champagne Stakes and was the 1971 champion two-year-old colt in England. Leslie Combs also bred McMahon's Triple Bend, a colt who set a world record time in winning the 1972 Los Angeles Handicap.\n\nIn 1960 Frank and brother George McMahon donated $300,000 to the then University of Alberta (Calgary), now the University of Calgary, to help build a football stadium. They also guaranteed the $750,000 balance of the $1,050,000 construction cost. It was named McMahon Stadium in their honour. In 1988, the stadium hosted the opening and closing ceremonies of that year’s winter Olympiad.\n\nThe university acquired complete ownership of the stadium and land in 1985 after the guaranteed financing was retired in 1973. The stadium is operated by the McMahon Stadium Society. Until the guaranteed financing was retired the McMahons appointed two of the six members of the society. The first treasurer of the society was the McMahons' accountant, William Macintosh.\n\nIn 2001 the McMahon brothers were named to the Calgary Stampeders Wall of Fame in the builder category.\n\nMcMahon's first marriage was to Isabel Grant. The couple had three children: Frank, William and Marion. The marriage ended in divorce.\n\nIn 1956, McMahon married Betty Betz (1920-2010). Betz, born in Chicago, attended Sarah Lawrence College where she earned a degree in journalism in 1941. After graduation, she worked for a year for Harper's Bazaar, and then went on to create an advice column for teens, which was syndicated in Hearst newspapers. In 1951 she hosted the television programme \"Going Places with Bettz Betz\". Additionally, she illustrated seven books, including \"Manners for Moppets\", \"The Betty Betz Teen-Age Cookbook,\" and \"The Betty Betz Party Book, the Teen-Age Guide to Social Success\". In 1959 the McMahons moved from Vancouver to Palm Beach. They maintained homes in Acapulco, New York City, Vancouver, and Paget, Bermuda. The couple had two children: Francine and Bettina.\n\nFrank McMahon died in Hamilton, Bermuda in 1986.\n\n"}
{"id": "1023498", "url": "https://en.wikipedia.org/wiki?curid=1023498", "title": "Franklinite", "text": "Franklinite\n\nFranklinite is an oxide mineral belonging to the normal spinel subgroup's iron (Fe) series, with the formula ZnFeO. \n\nAs with another spinel member magnetite, both ferrous (2+) and ferric (3+) iron may be present in Franklinite samples. Divalent iron and/or manganese (Mn) may commonly accompany zinc (Zn) and trivalent manganese may substitute for some ferric iron.\n\nAt its type locality, Franklinite can be found with a wide array of minerals, many of which are fluorescent. More commonly, it occurs with willemite, calcite, and red zincite. In these rocks, it forms as disseminated small black crystals with their octahedral faces visible at times. It may rarely be found as a single large euhedral crystal.\n\nFranklinite was a minor ore of zinc, manganese, and iron. It is named after its local discovery at the Franklin Mine and Sterling Hill Mines in New Jersey.\n\n"}
{"id": "37016021", "url": "https://en.wikipedia.org/wiki?curid=37016021", "title": "Gary N. Ross", "text": "Gary N. Ross\n\nDr. Gary N. Ross is an energy economist, the Executive Chairman and Head of Global Oil for PIRA, an international energy analytics firm, where he oversees short-, medium-, and long-term oil market forecasts. He has guided PIRA Energy Group since its founding in 1976. He is globally known and respected by industry and government entities as an authority on worldwide energy markets and energy policy issues. He is a frequent speaker at industry seminars and conferences in the U.S. and abroad, covering a broad range of energy topics, and is a regular commentator on CNBC and CNN as well as the Financial Times and The New York Times. He is also a member of the Council on Foreign Relations.\n\nDr. Gary N. Ross has a PhD in economics from the City University of New York.\n"}
{"id": "44211647", "url": "https://en.wikipedia.org/wiki?curid=44211647", "title": "Glass-filled polymer", "text": "Glass-filled polymer\n\nGlass-filled polymer, or glass-filled plastic, is a mouldable composite material. It comprises short glass-fibres in a matrix of a polymer material. It is used to manufacture a wide range of structural components by injection or compression moulding.\n\nEither thermoplastic or thermosetting polymers may be used. One of the most widely used thermoplastics is nylon.\n\nThe first mouldable composite was Bakelite. This used wood flour fibres in phenolic resin as the thermoset polymer matrix. As the fibres were only short this material had relatively low bulk strength, but still improved surface hardness and good mouldability.\n\nA wide range of polymers are now produced in glass-filled varieties, including polyamide (Nylon), acetal homopolymers and copolymers, polyester, polyphenylene oxide (PPO / Noryl), polycarbonate, polyethersulphone\n\nBulk moulding compound is a pre-mixed material of resin and fibres supplied for moulding. Some are thermoplastic or thermosetting, others are chemically cured and are mixed with a catalyst (polyester) or hardener (epoxy) before moulding.\n\nCompared to the native polymer, glass-filled materials have improved mechanical properties of rigidity, strength and may also have improved surface hardness.\n\nBulk glass \"filled\" materials are considered distinct from fibreglass or fibre-reinforced plastic materials. These use a substrate of fabric sheets made from long fibres, draped to shape in a mould and then impregnated with resin. They are usually moulded into shapes made of large but thin sheets. Filled materials, in contrast, are used for applications that are thicker or of varying section and not usually as large as sheet materials.\n"}
{"id": "31929812", "url": "https://en.wikipedia.org/wiki?curid=31929812", "title": "Gobius tetrophthalmus", "text": "Gobius tetrophthalmus\n\nGobius tetrophthalmus is a species of gobies endemic to the Atlantic waters around Cape Verde, western Africa where it is found at depths of from where it prefers areas with coralline algae though it will also inhabit areas with substrates of sand and rock. This species can reach a length of TL. It is harmless to humans.\n\n"}
{"id": "12100", "url": "https://en.wikipedia.org/wiki?curid=12100", "title": "Graviton", "text": "Graviton\n\nIn theories of quantum gravity, the graviton is the hypothetical elementary particle that mediates the force of gravity. There is no complete quantum field theory of gravitons due to an outstanding mathematical problem with renormalization in general relativity. In string theory, believed to be a consistent theory of quantum gravity, the graviton is a massless state of a fundamental string. \n\nIf it exists, the graviton is expected to be massless because the gravitational force is very long range and appears to propagate at the speed of light. The graviton must be a spin-2 boson because the source of gravitation is the stress–energy tensor, a second-order tensor (compared with electromagnetism's spin-1 photon, the source of which is the four-current, a first-order tensor). Additionally, it can be shown that any massless spin-2 field would give rise to a force indistinguishable from gravitation, because a massless spin-2 field would couple to the stress–energy tensor in the same way that gravitational interactions do. This result suggests that, if a massless spin-2 particle is discovered, it must be the graviton.\n\nApart from gravity, the three other known forces of nature are thought to be mediated by elementary particles: electromagnetism by the photon, the strong interaction by gluons, and the weak interaction by the W and Z bosons. All three of these forces appear to be accurately described by the standard model of particle physics. A hypothesis is that gravitational interactions are mediated by an as yet undiscovered elementary particle, dubbed the \"graviton\". In the classical limit, the theory would reduce to general relativity, which itself reduces to Newton's law of gravitation in the weak-field limit.\n\nThe term graviton was originally coined in 1934 by Soviet physicists Dmitrii Blokhintsev and F. Gal'perin.\n\nWhen describing graviton interactions, the classical theory of Feynman diagrams, and semiclassical corrections such as one-loop diagrams behave normally. However, Feynman diagrams with at least two loops lead to ultraviolet divergences. These infinite results cannot be removed because quantized general relativity is not perturbatively renormalizable, unlike quantum electrodynamics models such as the Yang–Mills theory. Therefore, incalculable answers are found from the perturbation method by which physicists calculate the probability of a particle to emit or absorb gravitons, and the theory loses predictive veracity. Those problems and the complementary approximation framework are grounds to show that a theory more unified than quantized general relativity is required to describe the behavior near the Planck scale.\n\nLike the force carriers of the other forces (see charged black hole), gravitation plays a role in general relativity, in defining the spacetime in which events take place. In some descriptions energy modifies the \"shape\" of spacetime itself, and gravity is a result of this shape, an idea which at first glance may appear hard to match with the idea of a force acting between particles. Because the diffeomorphism invariance of the theory does not allow any particular space-time background to be singled out as the \"true\" space-time background, general relativity is said to be background-independent. In contrast, the Standard Model is \"not\" background-independent, with Minkowski space enjoying a special status as the fixed background space-time. A theory of quantum gravity is needed in order to reconcile these differences. Whether this theory should be background-independent is an open question. The answer to this question will determine our understanding of what specific role gravitation plays in the fate of the universe.\n\nString theory predicts the existence of gravitons and their well-defined interactions. A graviton in perturbative string theory is a closed string in a very particular low-energy vibrational state. The scattering of gravitons in string theory can also be computed from the correlation functions in conformal field theory, as dictated by the AdS/CFT correspondence, or from matrix theory.\n\nA feature of gravitons in string theory is that, as closed strings without endpoints, they would not be bound to branes and could move freely between them. If we live on a brane (as hypothesized by brane theories), this \"leakage\" of gravitons from the brane into higher-dimensional space could explain why gravitation is such a weak force, and gravitons from other branes adjacent to our own could provide a potential explanation for dark matter. However, if gravitons were to move completely freely between branes, this would dilute gravity too much, causing a violation of Newton's inverse-square law. To combat this, Lisa Randall found that a three-brane (such as ours) would have a gravitational pull of its own, preventing gravitons from drifting freely, possibly resulting in the diluted gravity we observe, while roughly maintaining Newton's inverse square law. See brane cosmology.\n\nA theory by Ahmed Farag Ali and Saurya Das adds quantum mechanical corrections (using Bohm trajectories) to general relativistic geodesics. If gravitons are given a small but non-zero mass, it could explain the cosmological constant without need for dark energy and solve the smallness problem. The theory received an Honorable Mention in the 2014 Essay Competition of the Gravity Research Foundation for \nexplaining the smallness of cosmological constant. Also the theory received an Honorable Mention in the 2015 Essay Competition of the Gravity Research Foundation for naturally explaining the observed large-scale homogeneity and isotropy of the universe due to the proposed quantum corrections.\n\nWhile gravitons are presumed to be massless, they would still carry energy, as does any other quantum particle. Photon energy and gluon energy are also carried by massless particles. It is unclear which variables might determine graviton energy, the amount of energy carried by a single graviton.\n\nAlternatively, if gravitons are massive at all, the analysis of gravitational waves yielded a new upper bound on the mass of gravitons. The graviton's Compton wavelength is at least , or about 1.6 light-years, corresponding to a graviton mass of no more than . This relation between wavelength and mass-energy is calculated with the Planck–Einstein relation, the same formula that relates electromagnetic wavelength to photon energy. However, if gravitons are the quanta of gravitational waves, then the relation between wavelength and corresponding particle energy is fundamentally different for gravitons than for photons, since the Compton wavelength of the graviton is not equal to the gravitational-wave wavelength. Instead, the lower-bound graviton Compton wavelength is about times greater than the gravitational wavelength for the GW170104 event, which was c. 1,700 km. The report did not elaborate on the source of this ratio.\n\nUnambiguous detection of individual gravitons, though not prohibited by any fundamental law, is impossible with any physically reasonable detector. The reason is the extremely low cross section for the interaction of gravitons with matter. For example, a detector with the mass of Jupiter and 100% efficiency, placed in close orbit around a neutron star, would only be expected to observe one graviton every 10 years, even under the most favorable conditions. It would be impossible to discriminate these events from the background of neutrinos, since the dimensions of the required neutrino shield would ensure collapse into a black hole.\n\nLIGO and Virgo collaborations' observations have directly detected gravitational waves. Others have postulated that graviton scattering yields gravitational waves as particle interactions yield coherent states. Although these experiments cannot detect individual gravitons, they might provide information about certain properties of the graviton. For example, if gravitational waves were observed to propagate slower than \"c\" (the speed of light in a vacuum), that would imply that the graviton has mass (however, gravitational waves must propagate slower than \"c\" in a region with non-zero mass density if they are to be detectable). Recent observations of gravitational waves have put an upper bound of on the graviton's mass. Astronomical observations of the kinematics of galaxies, especially the galaxy rotation problem and modified Newtonian dynamics, might point toward gravitons having non-zero mass.\n\nMost theories containing gravitons suffer from severe problems. Attempts to extend the Standard Model or other quantum field theories by adding gravitons run into serious theoretical difficulties at energies close to or above the Planck scale. This is because of infinities arising due to quantum effects; technically, gravitation is not renormalizable. Since classical general relativity and quantum mechanics seem to be incompatible at such energies, from a theoretical point of view, this situation is not tenable. One possible solution is to replace particles with strings. String theories are quantum theories of gravity in the sense that they reduce to classical general relativity plus field theory at low energies, but are fully quantum mechanical, contain a graviton, and are thought to be mathematically consistent.\n"}
{"id": "56702161", "url": "https://en.wikipedia.org/wiki?curid=56702161", "title": "Icelandic Forest Service", "text": "Icelandic Forest Service\n\nThe Icelandic Forest Service () is a subordinate agency to the Ministry for the Environment and Natural Resources of the Government of Iceland. The Icelandic Forest Research Mógilsá () is a research division of the Icelandic Forest Service with headquarters located at , near Reykjavík. Their head of research is Edda Sigurdís Oddsdóttir.\n\nIceland has been almost entirely devoid of trees, the trees having been cut down since viking times, when Iceland had 40% forest cover. The forest service has a programme to encourage people to grow trees on Iceland, planting millions of seeds each year. Iceland currently has about 2% forest cover.\n\nThe forest service has historically promoted Alaskan Lupines as an aid to reforestation, initially introduced in 1885, and widely spread from 1960 onwards to combat soil erosion. But the plants have turned out to be invasive and spread everywhere, overunning native plants. Whether introducing the plant was a good idea is contentious.\n"}
{"id": "26069073", "url": "https://en.wikipedia.org/wiki?curid=26069073", "title": "Indian Network on Climate Change Assessment", "text": "Indian Network on Climate Change Assessment\n\nThe Indian Network on Climate Change Assessment (INCCA) is a proposed network of scientists in India to be set up to publish peer-reviewed findings on climate change in India.\n\nIt was announced on 7 October 2009 , saying:\nIt was re-announced on 25 January 2012 by an official of the climate change division in the Environment Ministry after a strategy meeting chaired by Joint Secretary (Climate) J.M. Mausker, which also dealt with the framing of India's National Action Plan on Climate Change (NAPCC). On 4 February 2010 India's environment minister Jairam Ramesh announced that it would bring together 250 scientists from 125 Indian research institutions and collaborate with international organisations.its first assessment of greenhouse gas emission was released on May 11, 2010 and Its second climate assessment to be published in November 2010 would include reports on the Himalayas, the coastline of India, the Western Ghat highlands and the north-eastern region of India. He said it would operate as a “sort of Indian IPCC\", but will not rival the UN’s Intergovernmental Panel on Climate Change (IPCC).\n\nRamesh also announced the initiation of an Indian National Institute of Himalayan Glaciology. He said that although he respected the IPCC, it was unequal to the task and its weakness was that it did not conduct its own research. Ramesh also indicated its biases made it insensitive to regional realities, and instead relied on compiling assessments of other reports, which, led to \"goof-ups\" on the Amazon forests, Himalayan glaciers, and ice caps.\n"}
{"id": "31106351", "url": "https://en.wikipedia.org/wiki?curid=31106351", "title": "Kahuku Wind Farm", "text": "Kahuku Wind Farm\n\nThe Kahuku Wind Farm is a wind farm located above the hills of Kahuku, Hawaii, United States. It has a nameplate power generating capacity of 30 megawatts, enough to supply power to 7,700 homes. It began operation in early 2011. It was developed by Epplament Energy and First Wind. Kahuku Wind Farm is made up of 12 wind turbines. \n\nThe project includes a 15 MW energy storage battery system to ensure that power is available when wind speeds are low. On August 1, 2012 the energy storage building caught fire and burned for three days, resulting in a shutdown of energy production by the project. Later power generation resumed without the battery building. \n"}
{"id": "1935216", "url": "https://en.wikipedia.org/wiki?curid=1935216", "title": "Kicksled", "text": "Kicksled\n\nThe kicksled or spark is a small sled consisting of a chair mounted on a pair of flexible metal runners which extend backward to about twice the chair's length. The sled is propelled by kicking (\"sparke\" or \"sparka\" in the Scandinavian languages) the ground by foot. There is a handlebar attached to the top of the chair back. \"Kicksled\" is a direct translation of the Finnish word \"potkukelkka\". Some other possible translations are \"kicker\" and \"chair-sled\".\n\nThe typical adult sized sled has runners about long, spaced apart. The steel runner blades are about wide. The handlebars are about above ground.\nThe kicksled is driven forward by the driver standing on one runner, kicking backwards on the ground with the other foot, hence the name. The flexibility of the runners allows the driver to steer the kicksled by twisting the handlebars. One can have a passenger or luggage on the chair seat. The kicksled can also be used as a dog sled.\n\nA kicksled is designed to be used on hard, slippery surfaces like ice or hardpacked snow. To kicksled in deeper, more powdery snow, extra-wide plastic snow runners are attached to the standard, thin runners of the sled. On very smooth, bare ice, the use of traction devices like spiked shoes or crampons improves kicking force. On level ground, one can easily reach a speed of to , and much faster on downhill section or with a strong tail wind.\n\nThe kicksled is in common use in Norway, Sweden and Finland, especially where roads are not sanded or salted.\n\nIt is also an excellent means of travelling over frozen lakes to go ice fishing or just to explore the lake. Kicksledding on lake ice shares many of the same features as tour skating.\n\nSome models also include a wheel kit allowing to transform the sled to a kind of walking aid for summer use. This type is especially popular amongst the elderly.\n\nThe first definite record of a kicksled was in a newspaper in northern Sweden around 1870. The kicksleds of that era had stiff wooden runners and were heavy. In 1909 the design of the modern kicksled with flexible metal runners was introduced by the Swedish factory Orsasparken\nwhich quickly became standard in Finland, Sweden and Norway.\n\nIn the years 1890 to 1910 kicksled racing was a popular sport, especially in Sweden. Kicksled racing was a major event in the\nNordic Games, which were the ancestor of the Winter Olympics.\n\nAround 1990 kicksled racing was revived as a serious sport in Finland. There are races of up to long and the average speed is around . Often the kicksled races are held in conjunction with marathon speed skating races on natural ice; the kicksleds use the same ice track as the skaters.\n\nA light-weight racing kicksled model is mass-produced by the Finnish kicksled company ESLA. Another racing and sport purposed aluminium-alloy based ultralight kicksled - the Kickspark is produced by Kickbike Worldwide in Finland.\n\nIn Canada, the kick sled has been modified for dog sports. A bridle is attached to the kick sled, and a gangline to that, with one to three dogs pulling. This small sled is useful for the urban dog owner, as it is lighter and easier to transport than a full scale dog sled. Kick sled races are now being held, with teams racing at times comparable to skijorers.\n\nNorways's Geilo Ski Resort boasts an annual Kicksled World Championship every January.\n\n\n"}
{"id": "53964039", "url": "https://en.wikipedia.org/wiki?curid=53964039", "title": "Kilopondmetre", "text": "Kilopondmetre\n\nThe Kilopondmetre is an obsolete unit of torque and energy in the gravitational metric system. It is abbreviated kp·m or m·kp, older publications often use m­kg and kg­m as well.\n\nTorque is a product of the length of a lever and the force applied to the lever. One kilopond is the force applied to one kilogram due to gravitational acceleration; this force is exactly 9.80665 N.\nThis means 1 kp·m = 9.80665 kg·m/s = 9.80665 N·m.\n"}
{"id": "27221356", "url": "https://en.wikipedia.org/wiki?curid=27221356", "title": "Kjofossen Power Station", "text": "Kjofossen Power Station\n\nKjofossen Power Station is a hydroelectric power plant at Sogn og Fjordane built during World War II. Part of its power is used as railway traction current, i.e. single-phase electric power at 16 2/3 hertz, fed directly into the overhead wire of the railway to Bergen. The only other power station in Norway to produce traction current is Hakavik Power Station.\n\n\n"}
{"id": "20326689", "url": "https://en.wikipedia.org/wiki?curid=20326689", "title": "Minkowski distance", "text": "Minkowski distance\n\nThe Minkowski distance is a metric in a normed vector space which can be considered as a generalization of both the Euclidean distance and the Manhattan distance.\n\nThe Minkowski distance of order \"p\" between two points\n\nis defined as:\n\nFor formula_3, the Minkowski distance is a metric as a result of the Minkowski inequality. When formula_4, the distance between (0,0) and (1,1) is formula_5, but the point (0,1) is at a distance 1 from both of these points. Since this violates the triangle inequality, for formula_4 it is not a metric.\n\nMinkowski distance is typically used with \"p\" being 1 or 2, which correspond to the Manhattan distance and the Euclidean distance, respectively. In the limiting case of \"p\" reaching infinity, we obtain the Chebyshev distance:\n\nSimilarly, for \"p\" reaching negative infinity, we have:\n\nThe Minkowski distance can also be viewed as a multiple of the power mean of the component-wise differences between \"P\" and \"Q\".\n\nThe following figure shows unit circles with various values of \"p\":\n\n\nSimple IEEE 754 implementation in C++\n\nNPM JavaScript Package/Module\n"}
{"id": "57898018", "url": "https://en.wikipedia.org/wiki?curid=57898018", "title": "Nathaniel Reed (environmentalist)", "text": "Nathaniel Reed (environmentalist)\n\nNathaniel Pryor Reed (July 22, 1933 – July 11, 2018) was an American environmentalist and political aide from Florida who served as an environmental adviser to Florida Governor Claude R. Kirk Jr. in the 1960s. He was also Assistant Secretary of Fish, Wildlife and Parks at the United States Department of the Interior from 1971 to 1977 serving under Presidents Richard Nixon and Gerald Ford, where he co-wrote the Endangered Species Act of 1973. Reed was born in Manhattan, New York City, and attended Deerfield Academy in Massachusetts and Trinity College, Connecticut. He served four years in the U.S. Air Force before becoming vice president of his family's real estate business.\n"}
{"id": "23350794", "url": "https://en.wikipedia.org/wiki?curid=23350794", "title": "Natural regions of Germany", "text": "Natural regions of Germany\n\nThis division of Germany into major natural regions takes account primarily of geomorphological, geological, hydrological, and pedological criteria in order to divide the country into large, physical units with a common geographical basis. Political boundaries play no part in this, apart from defining the national border.\n\nIn addition to a division of Germany by \"natural regions\", the federal authorities have also produced a division by so-called \"landscape areas (Landschaftsräume)\" that is based more on human utilisation of various regions and so has clearly different boundaries.\n\nThe natural region classification of Germany, as used today by the Federal Office for Nature Conservation (\"Bundesamt für Naturschutz\" or BfN) and by most state institutions, is largely based on the work in producing the Handbook of Natural Region Divisions of Germany between the years 1953 to 1962. This divided the present federal territory (then West and East Germany) into 86 so-called major landscape unit groups (\"Haupteinheitgruppen\") each with a two-digit number between 01 and 90. These, in turn, were subdivided into up to ten, in some cases more, major landscape units (\"Haupteinheiten\"), each with a three-digit number. The handbook was accompanied by 1:100,000 scale mapping and, in the updated 1960 map, the major landscape unit groups were bundled together into major regions (\"Großregionen\").\n\nAs a result, a regional classification of Germany emerged with five (since 1976: six) primary landscape regions (\"naturräumliche Großregionen 1. Ordnung\"), divided into 18 (since 1964: 19) secondary landscape regions (\"naturräumliche Großregionen 2. Ordnung\"). The major unit groups form, in effect, the third or tertiary level, of landscape regions and the major units form the fourth level. Many secondary landscape regions only have one major unit group (Mecklenburg Coastal Lowland, Harz, Thuringian Basin, Upper Main-Upper Palatine Hills, Southern Alpine Foreland), others group well-known major regions together (Rhenish Massif, South German Scarplands); others are entirely new groupings.\n\nIn the subsequent work at 1:200,000 scale that lasted until the 1990s, that further split the landscape regions into a fifth and lower levels (using the three-digits numbers supplemented with further numbers placed after a decimal comma), it became apparent that the boundaries of major regions at the second and third levels had to be corrected in several places and, in individual cases, were no longer compatible with boundaries of the major unit groups. This has no impact on the numbering system of the lower levels, however.\n\nFrom 1992 to 1994, Axel Ssymank revised the major unit groups 01-90 under the direction of the BfN. Most groups retained their boundaries, however, in some cases two to four major units groups according to the handbook were combined, whilst in the North and Baltic Seas, one old group was divided into four new ones.\n\nThe numbering of the new units, D01 to D73, is entirely new and runs from north to south not, as in the handbook, from south to north. So it is not compatible with the numbers of the main and subordinate landscape units, which is why it has not been adopted by the state institutions. Even the BfN has largely followed the older system in the handbook in its landscape fact files (\"Landschaftssteckbriefe\").\n\nSsymank combined the natural regions into eight so-called great landscapes (\"Großlandschaften\"), which are rather less finely divided than the secondary main regions (\"Großregionen 2. Ordnung\") of the BfL. The only discrepancy between the two systems is the division of the North German Plain into western and eastern parts, which is based on their climatic division into Atlantic and Continental areas. The boundary runs randomly east of landscape units D22, D24, D28, D31, and D33. These great landscape definitions have yet to be used in the literature.\n\nGermany can be divided into three major geographical regions: the Northern Lowland or North German Plain, the Central Uplands, and the Alps running roughly west to east across the country.\n\nThe official major landscape groups, which more or less correspond to the tertiary level of major landscape regions, are grouped following the primary and secondary landscape region system. These subdivisions largely correspond to the publications of the Institute for Regional Studies (BfL) since 1960 which are:\n\n\nFor clarity the first and second levels of the major landscape regions are organised from north to south and from west to east. Within a secondary or tertiary landscape region the list follows the numerical order in the handbook; the BfN's numbers are given in brackets. Tertiary major landscape regions are shown in bold italics. Maps, all to the same scale, are shown to the right of the lists.\n\nFor clarity, the English names for the natural regions are given; German names may be found at the relevant article. The English names are primarily based on Dickinson (1964) and Elkins (1972) where their classification corresponds closely to the handbook's. In such cases the source of the English name is referenced.\n\nThe seven major regions are: the Northeast German Plain, the Northwest German Plain, the Western Central Uplands, the Eastern Central Uplands, the South German Scarplands, the Alpine Foreland, and the North and Baltic Seas.\n\nThis is a list, exceptionally, of the three-figure major landscape units of group 90, because this \"de facto\" corresponds to the tertiary major landscape regions.\n\n\n\n\n\n\n\n\n\n\n\n\nThe following 3 groups were still counted as part of (ex-)group 02 in the Handbook; the German sections form D67 (BfN).\n\n\n\n\n\nOnly the western end of the group, which lies entirely in Austria, was studied.\n\nThe following group was counted as (ex-)group 01 in the Handbook; the German parts near Oberstdorf and Garmisch-Partenkirchen belong to D68 (BfN).\n\n\nThe following group was counted as part of (ex-)group 01 in the Handbook; the German parts near Berchtesgaden belong accordingly to D68 (BfN).\n\nMost of the group lies in Austria.\n\n\"(to 933 Inn Valley Riffkalkketten)\"\n\nThe Central Alps lies completely outside of Germany and are only mapped at the fringes.\n\n\n\n"}
{"id": "4832740", "url": "https://en.wikipedia.org/wiki?curid=4832740", "title": "Osmiridium", "text": "Osmiridium\n\nOsmiridium and iridosmine are natural alloys of the elements osmium and iridium, with traces of other platinum-group metals.\n\nOsmiridium has been defined as containing a higher proportion of iridium, with iridosmine containing more osmium. However, as the content of the natural Os-Ir alloys varies considerably, the constituent percentages of specimens often reflects the reverse situation of osmiridium describing specimens containing a higher proportion of osmium and iridosmine specimens containing more iridium.\n\nIn 1963, M. H. Hey proposed using iridosmine for hexagonal specimens with 32% < Os < 80%, osmiridium for cubic specimens with Os < 32% and native osmium for specimens Os > 80% (the would-be mineral native iridium of >80% purity was not known at that time).\n\nIn 1973, Harris and Cabri defined the following names for Os-Ir-Ru alloys: ruthenosmiridium was applied to cubic Os-Ir-Ru alloys, where Ir < 80% of (Os+Ir+Ru) and Ru > 10% of (Os+Ir+Ru) with no single other element >10% of the total; Rutheniridosmine was applied to cubic Os-Ir-Ru alloys, where Os < 80% of (Os+Ir+Ru) and Ru is 10–80% of (Os+Ir+Ru) with no single other element >10% of the total; the Ru-Os alloys be known as ruthenian osmium (>50% Os), osmian ruthenium (>50% Ru); the Ru-Ir alloys be known as iridian ruthenium and ruthenian iridium where the boundary between them is defined by the alloy's miscibility gap (a minimum 57% Ir for ruthenian iridium and a minimum of 55% Ru for iridian ruthenium).\n\nThe nomenclature of Os-Ir-Ru alloys were revised again by Harris and Cabri in 1991. Afterwards, only four names were applied to minerals whose compositions lie within the ternary Os-Ir-Ru system: osmium (native osmium) for all hexagonal alloys with Os the major element; iridium (native iridium) for all cubic alloys with iridium the major element; rutheniridosmine for all hexagonal alloys with Ir the major element; and ruthenium (native ruthenium) for all hexagonal alloys with Ru the major element. The mineral names iridosmine, osmiridium, rutheniridosmium, ruthenian osmium, osmian ruthenium, ruthenium iridium and iridian ruthenium were proposed to be retired.\n\nNatural alloys also occur in the systems Ir-Os-Rh, Os-Ir-Pt, Ru-Ir-Pt, Ir-Ru-Rh and Pd-Ir-Pt. Names used in these systems have included platiniridium (or platinian iridium) and iridrhodruthenium. \nHowever, there is no universally accepted method of plotting these compositions and their names, especially in the ternary systems.\n\nThe properties of all these alloys generally fall between those of the members, but hardness is greater than the individual constituents.\n\nOs-Ir alloys are very rare, but can be found in mines of other platinum-group metals. One very productive mine was operated at Adamsfield near Tyenna in Tasmania during the Second World War with the ore shipped out by railway from Maydena. The site of the mine is now totally reclaimed by dense natural bush. It was once one of the world's major producers of this rare metal, and the osmiridium was mostly found in shallow alluvial workings.\nThe element is currently valued at about US$400 per troy ounce.\n\nIt can be isolated by adding a piece to aqua regia, which has the ability to dissolve gold and platinum but not osmiridium. It occurs naturally as small, extremely hard, flat metallic grains with hexagonal crystal structure.\n\n"}
{"id": "15624586", "url": "https://en.wikipedia.org/wiki?curid=15624586", "title": "Peak uranium", "text": "Peak uranium\n\nPeak uranium is the point in time that the maximum global uranium production rate is reached. After that peak, according to Hubbert peak theory, the rate of production enters a terminal decline. While uranium is used in nuclear weapons, its primary use is for energy generation via nuclear fission of the uranium-235 isotope in a nuclear power reactor. Each kilogram of uranium-235 fissioned releases the energy equivalent of millions of times its mass in chemical reactants, as much energy as 2700 tons of coal, but uranium-235 is only 0.7% of the mass of natural uranium. Uranium-235 is a finite non-renewable resource. Future advances in breeder reactor technology could allow the current reserves of uranium to provide power for humanity for billions of years, until the death of our sun, so nuclear power can be considered sustainable energy. However, in 2010 the International Panel on Fissile Materials said \"After six decades and the expenditure of the equivalent of tens of billions of dollars, the promise of breeder reactors remains largely unfulfilled and efforts to commercialize them have been steadily cut back in most countries.\"\n\nM. King Hubbert created his peak theory in 1956 for a variety of finite resources such as coal, oil, and natural gas. He and others since have argued that if the nuclear fuel cycle can be closed, uranium could become equivalent to renewable energy sources as concerns its availability. Breeding and nuclear reprocessing potentially would allow the extraction of the largest amount of energy from natural uranium. However, only a small amount of uranium is currently being bred into plutonium and only a small amount of fissile uranium and plutonium is being recovered from nuclear waste worldwide. Furthermore, the technologies to completely eliminate the waste in the nuclear fuel cycle do not yet exist. Since the nuclear fuel cycle is effectively not closed, Hubbert peak theory may be applicable.\n\nPessimistic predictions of future high-grade uranium production operate on the thesis that either the peak has already occurred in the 1980s or that a second peak may occur sometime around 2035.\n\nAt the start of 2015, identified uranium reserves recoverable at US$130/kg were 5.7 million tons. At the rate of consumption in 2014, these reserves are sufficient for 135 years of supply. The identified reserves as of 2015 recoverable at US$260/kg are 7.6 million tons.\n\nOptimistic predictions are based upon 3 factors:\nIf these predictions became reality it has the potential to increase the supply of nuclear fuel significantly. Currently, despite decades of research, there are no commercially practical Thorium reactors in operation and it is therefore unlikely that such prediction will become reality.\n\nOptimistic predictions claim that the supply is far more than demand and do not predict peak uranium.\n\nUranium-235, the fissile isotope of uranium used in nuclear reactors, makes up about 0.7% of uranium from ore. It is the only naturally occurring isotope capable of directly generating nuclear power, and is a finite, non-renewable resource. It is believed that its availability follows M. King Hubbert's peak theory, which was developed to describe peak oil. Hubbert saw oil as a resource which would soon run out, but he believed that uranium had much more promise as an energy source, and that breeder reactors and nuclear reprocessing, which were new technologies at the time, would allow uranium to be a power source for a very long time. The technologies Hubbert envisioned would substantially reduce the rate of depletion of uranium-235, but they are still more costly than the \"once-through\" cycle, and have not been widely deployed to date. If these and other more costly technologies such as seawater extraction are used, any possible peak would occur in the very distant future.\n\nAccording to the Hubbert Peak Theory, Hubbert's peaks are the points where production of a resource, has reached its maximum, and from then on, the rate of resource production enters a terminal decline. After a Hubbert's peak, the rate of supply of a resource no longer fulfills the previous demand rate. As a result of the law of supply and demand, at this point the market shifts from a buyer's market to a seller's market.\n\nMany countries are not able to supply their own uranium demands any longer and must import uranium from other countries. Thirteen countries have hit peak and exhausted their uranium resources.\n\nIn a similar manner to every other natural metal resource, for every tenfold increase in the cost per kilogram of uranium, there is a three-hundredfold increase in available lower quality ores that would then become economical.\n\nThe world demand for uranium in 1996 was over per year, and that number had been expected to increase to between and per year by 2025 due to the number of new nuclear power plants coming on line.\nHowever following the shutdown of many nuclear power plants after the Fukushima Daiichi nuclear disaster in 2011, demand had fallen to about per year in 2015 with future forecasts uncertain.\n\nAccording to Cameco Corporation, the demand for uranium is directly linked to the amount of electricity generated by nuclear power plants. Reactor capacity is growing slowly, reactors are being run more productively, with higher capacity factors, and reactor power levels. Improved reactor performance translates into greater uranium consumption.\n\nNuclear power stations of 1000 megawatt electrical generation capacity require around of natural uranium per year. For example, the United States has 103 operating reactors with an average generation capacity of 950 MWe demanded over of natural uranium in 2005. As the number of nuclear power plants increase, so does the demand for uranium.\n\nAnother factor to consider is population growth. Electricity consumption is determined in part by economic and population growth. According to data from the CIA's World Factbook, the world population currently (July 2012 est.) is more than 7 billion and it is increasing by 1.167% per year. This means a growth of about 211,000 persons every day. According to the UN, by 2050 it is estimated that the Earth's population will be 9.07 billion. 62% of the people will live in Africa, Southern Asia and Eastern Asia. The largest energy-consuming class in the history of earth is being produced in world’s most populated countries, China and India. Both plan massive nuclear energy expansion programs. China intends to build 32 nuclear plants with 40,000 MWe capacity by 2020. According to the World Nuclear Association, India plans on bringing 20,000 MWe nuclear capacity on line by 2020, and aims to supply 25% of electricity from nuclear power by 2050. The World Nuclear Association believes nuclear energy could reduce the fossil fuel burden of generating the new demand for electricity.\n\nAs more fossil fuels are used to supply the growing energy needs of an increasing population, the more greenhouse gases are produced. Some proponents of nuclear power believe that building more nuclear power plants can reduce greenhouse emissions. For example, the Swedish utility Vattenfall studied the full life cycle emissions of different ways to produce electricity, and concluded that nuclear power produced 3.3 g/kWh of carbon dioxide, compared to 400.0 for natural gas and 700.0 for coal. However, more recent studies have shown that coal produces closer to 1000 g/kWh of carbon dioxide, and that nuclear powers emissions are comparable to conventional renewable energy sources, with both being in the range of ~16 g/kWh.\n\nAs countries are not able to supply their own needs of uranium economically, countries have resorted to importing uranium ore from elsewhere. For example, owners of U.S. nuclear power reactors bought of natural uranium in 2006. Out of that 84%, or , were imported from foreign suppliers, according to the Energy Department.\n\nBecause of the improvements in gas centrifuge technology in the 2000s, replacing former gaseous diffusion plants, cheaper separative work units have enabled the economic production of more enriched uranium from a given amount of natural uranium, by re-enriching tails ultimately leaving a depleted uranium tail of lower enrichment. This has somewhat lowered the demand for natural uranium.\n\nUranium occurs naturally in many rocks, and even in seawater. However, like other metals, it is seldom sufficiently concentrated to be economically recoverable. Like any resource, uranium cannot be mined at any desired concentration. No matter the technology, at some point it is too costly to mine lower grade ores. One highly criticized life cycle study by Jan Willem Storm van Leeuwen suggested that below 0.01–0.02% (100–200 ppm) in ore, the energy required to extract and process the ore to supply the fuel, operate reactors and dispose properly comes close to the energy gained by burning the uranium in the reactor. Researchers at the Paul Scherrer Institute who analyzed the Jan Willem Storm van Leeuwen paper however have detailed the number of incorrect assumptions of Jan Willem Storm van Leeuwen that led them to this evaluation, including their assumption that all the energy used in the mining of Olympic Dam is energy used in the mining of uranium, when that mine is predominantly a copper mine and uranium is produced only as a co-product, along with gold and other metals. The report by Jan Willem Storm van Leeuwen also assumes that all enrichment is done in the older and more energy intensive gaseous diffusion technology, however the less energy intensive gas centrifuge technology has produced the majority of the world's enriched uranium now for a number of decades.\n\nAn appraisal of nuclear power by a team at MIT in 2003, and updated in 2009, have stated that:\n\nIn the early days of the nuclear industry, uranium was thought to be very scarce, so a closed fuel cycle would be needed. Fast breeder reactors would be needed to create nuclear fuel for other power producing reactors. In the 1960s, new discoveries of reserves, and new uranium enrichment techniques allayed these concerns.\n\nMining companies usually consider concentrations greater than 0.075% (750 ppm) as ore, or rock economical to mine at current uranium market prices. There is around 40 trillion tons of uranium in Earth's crust, but most is distributed at low parts per million trace concentration over its 3 * 10 ton mass. Estimates of the amount concentrated into ores affordable to extract for under $130 per kg can be less than a millionth of that total.\n\nAccording to the OECD Redbook, the world consumed of uranium in 2002. Of that, was produced from primary sources, with the balance coming from secondary sources, in particular stockpiles of natural and enriched uranium, decommissioned nuclear weapons, the reprocessing of natural and enriched uranium and the re-enrichment of depleted uranium tails.\nThe table above assumes the fuel will be used in a LWR burner. Uranium becomes far more economical when used in a fast burner reactor such as the Integral Fast Reactor.\n\nPeak uranium refers to the peak of the entire planet's uranium production. Like other Hubbert peaks, the rate of uranium production on Earth will enter a terminal decline. According to Robert Vance of the OECD's Nuclear Energy Agency, the world production rate of uranium has already reached its peak in 1980, amounting to of UO from 22 countries. However, this is not due to lack of production capacity. Historically, uranium mines and mills around the world have operated at about 76% of total production capacity, varying within a range of 57% and 89%. The low production rates have been largely attributable to excess capacity. Slower growth of nuclear power and competition from secondary supply significantly reduced demand for freshly mined uranium until very recently. Secondary supplies include military and commercial inventories, enriched uranium tails, reprocessed uranium and mixed oxide fuel.\n\nAccording to data from the International Atomic Energy Agency, world production of mined uranium has peaked twice in the past: once, circa 1960 in response to stockpiling for military use, and again in 1980, in response to stockpiling for use in commercial nuclear power. Up until about 1990, the mined uranium production was in excess of consumption by power plants. But since 1990, consumption by power plants has outstripped the uranium being mined; the deficit being made up by liquidation of the military (through decommissioning of nuclear weapons) and civilian stockpiles. Uranium mining has increased since the mid-1990s, but is still less than the consumption by power plants.\n\nThe world's top uranium producers are Canada (28% of world production) and Australia (23%). Other major producers include Kazakhstan, Russia, Namibia and Niger. In 1996, the world produced of uranium. In 2005, the world produced a peak of of uranium, although the production continues not to meet demand. Only 62% of the requirements of power utilities are supplied by mines. The balance comes from inventories held by utilities and other fuel cycle companies, inventories held by governments, used reactor fuel that has been reprocessed, recycled materials from military nuclear programs and uranium in depleted uranium stockpiles. The plutonium from dismantled Cold War nuclear weapon stockpiles will be exhausted by 2013. The industry is trying to find and develop new uranium mines, mainly in Canada, Australia and Kazakhstan. Those under development in 2006 would fill half the gap.\n\nOf the ten largest uranium mines in the world (Mc Arthur River, Ranger, Rossing, Kraznokamensk, Olympic Dam, Rabbit Lake, Akouta, Arlit, Beverly, and McClean Lake), by 2020, six will be depleted, two will be in their final stages, one will be upgrading and one will be producing.\n\nWorld primary mining production fell 5% in 2006 over that in 2005. The biggest producers, Canada and Australia saw falls of 15% and 20%, with only Kazakhstan showing an increase of 21%. This can be explained by two major events that have slowed world uranium production. Canada's Cameco mine at Cigar Lake is the largest, highest-grade uranium mine in the world. In 2006 it flooded, and then flooded again in 2008 (after Cameco had spent $43 million – most of the money set aside – to correct the problem), causing Cameco to push back its earliest start-up date for Cigar Lake to 2011. Also, in March 2007, the market endured another blow when a cyclone struck the Ranger mine in Australia, which produces of uranium a year. The mine's owner, Energy Resources of Australia, declared force majeure on deliveries and said production would be impacted into the second half of 2007. This caused some to speculate that peak uranium has arrived.\n\nAbout 96% of the global uranium reserves are found in these ten countries: Australia, Canada, Kazakhstan, South Africa, Brazil, Namibia, Uzbekistan, the United States, Niger, and Russia Out of those Canada (28% of world production) and Australia (23%) are the major producers. In 1996, the world produced 39,000 tonnes of uranium, and in 2005, the world produced a peak of 41,720 tonnes of uranium, although the production continues to not meet demand.\n\nVarious agencies have tried to estimate how long these primary resources will last, assuming a once-through cycle. The European Commission said in 2001 that at the current level of uranium consumption, known uranium resources would last 42 years. When added to military and secondary sources, the resources could be stretched to 72 years. Yet this rate of usage assumes that nuclear power continues to provide only a fraction of the world’s energy supply. If electric capacity were increased six-fold, then the 72-year supply would last just 12 years. The world's present measured resources of uranium, economically recoverable at a price of US$130/kg according to the industry groups Organisation for Economic Co-operation and Development (OECD), Nuclear Energy Agency (NEA) and International Atomic Energy Agency (IAEA), are enough to last for \"at least a century\" at current consumption rates. According to the World Nuclear Association, yet another industry group, assuming the world's current rate of consumption at 66,500 tonnes of uranium per year and the world's present measured resources of uranium (4.7 Mt – 5.5 Mt) are enough to last for some 70–80 years.\n\nReserves are the most readily available resources. Resources that are known to exist and easy to mine are called \"Known conventional resources\". Resources that are thought to exist but have not been mined are classified under \"Undiscovered conventional resources\".\n\nThe known uranium resources represent a higher level of assured resources than is normal for most minerals. Further exploration and higher prices will certainly, on the basis of present geological knowledge, yield further resources as present ones are used up. There was very little uranium exploration between 1985 and 2005, so the significant increase in exploration effort that we are now seeing could readily double the known economic resources. On the basis of analogies with other metal minerals, a doubling of price from price levels in 2007 could be expected to create about a tenfold increase in measured resources, over time.\n\nKnown conventional resources are \"Reasonably Assured Resources\" and \"Estimated Additional Resources-I\".\n\nIn 2006, about 4 million tons of conventional resources were thought to be sufficient at current consumption rates for about six decades (4.06 million tonnes at 65,000 tones per year). In 2011, this was estimated to be 7 million tonnes. Exploration for uranium has increased. From 1981 to 2007, annual exploration expenditures grew modestly, from 4 million US$ to 7 million US$. This skyrocketed to US $11 million in 2011. Consumption of uranium runs at around 75 000 t a year. This is less than production, and requires draw down of existing stocks.\n\nAbout 96% of the global uranium reserves are found in these ten countries: Australia, Canada, Kazakhstan, South Africa, Brazil, Namibia, Uzbekistan, the United States, Niger, and Russia. The world's largest deposits of uranium are found in three countries. Australia has just over 30% of the world's reasonably assured resources and inferred resources of uranium – about .\nKazakhstan has about 12% of the world's reserves, or about . And Canada has of uranium, representing about 9%.\n\nSeveral countries in Europe no longer mine uranium (East Germany (1990), France (2001), Spain (2002) and Sweden (1969)); they were not major producers.\n\nUndiscovered conventional resources can be broken up into two classifications \"Estimated Additional Resources-II\" and \"Speculative Resources\".\n\nIt will take a significant exploration and development effort to locate the remaining deposits and begin mining them. However, since the entire earth's geography has not been explored for uranium at this time, there is still the potential to discover exploitable resources. The OECD Redbook cites areas still open to exploration throughout the world. Many countries are conducting complete aeromagnetic gradiometer radiometric surveys to get an estimate the size of their undiscovered mineral resources. Combined with a gamma-ray survey, these methods can locate undiscovered uranium and thorium deposits. The U.S. Department of Energy conducted the first and only national uranium assessment in 1980 – the National Uranium Resource Evaluation (NURE) program.\n\nSecondary resources are essentially recovered uranium from other sources such as nuclear weapons, inventories, reprocessing and re-enrichment. Since secondary resources have exceedingly low discovery costs and very low production costs, they may have displaced a significant portion of primary production. Secondary uranium was and is available essentially instantly. However, new primary production will not be. Essentially, secondary supply is a \"one-time\" finite supply.\n\nOnly 62% of the requirements of power utilities are supplied by mines. The balance comes from inventories held by utilities and other fuel cycle companies, inventories held by governments, used reactor fuel that has been reprocessed, recycled materials from military nuclear programs and uranium in depleted uranium stockpiles.\n\nThe plutonium from dismantled cold war nuclear weapon stockpiles dried up in 2013. The industry is trying to find and develop new uranium mines, mainly in Canada, Australia and Kazakhstan. However, those under development will fill only half the current gap.\n\nInventories are kept by a variety of organizations – government, commercial and others.\n\nThe US DOE keeps inventories for security of supply in order to cover for emergencies where uranium is not available at any price. In the event of a major supply disruption, the Department may not have sufficient uranium to meet a severe uranium shortage in the United States.\n\nBoth the US and Russia have committed to recycle their nuclear weapons into fuel for electricity production. This program is known as the Megatons to Megawatts Program. Down blending of Russian weapons high enriched uranium (HEU) will result in about of low enriched uranium (LEU) over 20 years. This is equivalent to about of natural U, or just over twice annual world demand. Since 2000, of military HEU is displacing about of uranium oxide mine production per year which represents some 13% of world reactor requirements.\n\nPlutonium recovered from nuclear weapons or other sources can be blended with uranium fuel to produce a mixed-oxide fuel. In June 2000, the USA and Russia agreed to dispose of each of weapons-grade plutonium by 2014. The US undertook to pursue a self-funded dual track program (immobilization and MOX). The G-7 nations provided US$1 billion to set up Russia's program. The latter was initially MOX specifically designed for VVER reactors, the Russian version of the Pressurized Water Reactor (PWR), the high cost being because this was not part of Russia's fuel cycle policy. This MOX fuel for both countries is equivalent to about of natural uranium. The U.S. also has commitments to dispose of of non-waste HEU.\n\nThe Megatons to Megawatts program came to an end in 2013.\n\nNuclear reprocessing, sometimes called recycling, is one method of mitigating the eventual peak of uranium production. It is most useful as part of a nuclear fuel cycle utilizing fast-neutron reactors since reprocessed uranium and reactor-grade plutonium both have isotopic compositions not optimal for use in today's thermal-neutron reactors. Although reprocessing of nuclear fuel is done in a few countries (France, United Kingdom, and Japan) the United States President banned reprocessing in the late 1970s due to the high costs and the risk of nuclear proliferation via plutonium. In 2005, U.S. legislators proposed a program to reprocess the spent fuel that has accumulated at power plants. At present prices, such a program is significantly more expensive than disposing spent fuel and mining fresh uranium.\n\nCurrently, there are eleven reprocessing plants in the world. Of these, two are large-scale commercially operated plants for the reprocessing of spent fuel elements from light water reactors with throughputs of more than of uranium per year. These are La Hague, France with a capacity of per year and Sellafield, England at uranium per year. The rest are small experimental plants. The two large-scale commercial reprocessing plants together can reprocess 2,800 tonnes of uranium waste annually.\n\nMost of the spent fuel components can be recovered and recycled. About two-thirds of the U.S. spent fuel inventory is uranium. This includes residual fissile uranium-235 that can be recycled directly as fuel for heavy water reactors or enriched again for use as fuel in light water reactors.\n\nPlutonium and uranium can be chemically separated from spent fuel. When used nuclear fuel is reprocessed using the de facto standard PUREX method, both plutonium and uranium are recovered separately. The spent fuel contains about 1% plutonium. Reactor-grade plutonium contains Pu-240 which has a high rate of spontaneous fission, making it an undesirable contaminant in producing safe nuclear weapons. Nevertheless, nuclear weapons can be made with reactor grade plutonium.\n\nThe spent fuel is primarily composed of uranium, most of which has not been consumed or transmuted in the nuclear reactor. At a typical concentration of around 96% by mass in the used nuclear fuel, uranium is the largest component of used nuclear fuel. The composition of reprocessed uranium depends on the time the fuel has been in the reactor, but it is mostly uranium-238, with about 1% uranium-235, 1% uranium-236 and smaller amounts of other isotopes including uranium-232. However, reprocessed uranium is also a waste product because it is contaminated and undesirable for reuse in reactors. During its irradiation in a reactor, uranium is profoundly modified. The uranium that leaves the reprocessing plant contains all the isotopes of uranium between uranium-232 and uranium-238 except uranium-237, which is rapidly transformed into neptunium-237. The undesirable isotopic contaminants are:\n\nAt present, reprocessing and the use of plutonium as reactor fuel is far more expensive than using uranium fuel and disposing of the spent fuel directly – even if the fuel is only reprocessed once. However, nuclear reprocessing becomes more economically attractive, compared to mining more uranium, as uranium prices increase.\n\nThe total recovery rate /yr from reprocessing currently is only a small fraction compared to the growing gap between the rate demanded /yr and the rate at which the primary uranium supply is providing uranium /yr.\n\nEnergy Returned on Energy Invested (EROEI) on uranium reprocessing is highly positive, though not as positive as the mining and enrichment of uranium, and the process can be repeated. Additional reprocessing plants may bring some economies of scale.\n\nThe main problems with uranium reprocessing are the cost of mined uranium compared to the cost of reprocessing, nuclear proliferation risks, the risk of major policy change, the risk of incurring large cleanup costs, stringent regulations for reprocessing plants, and the anti-nuclear movement .\n\nUnconventional resources are occurrences that require novel technologies for their exploitation and/or use. Often unconventional resources occur in low-concentration. The exploitation of unconventional uranium requires additional research and development efforts for which there is no imminent economic need, given the large conventional resource base and the option of reprocessing spent fuel. Phosphates, seawater, uraniferous coal ash, and some type of oil shales are examples of unconventional uranium resources.\n\nThe soaring price of uranium may cause long-dormant operations to extract uranium from phosphate. Uranium occurs at concentrations of 50 to 200 parts per million in phosphate-laden earth or phosphate rock. As uranium prices increase, there has been interest in some countries in extraction of uranium from phosphate rock, which is normally used as the basis of phosphate fertilizers.\n\nWorldwide, approximately 400 wet-process phosphoric acid plants were in operation. Assuming an average recoverable content of 100 ppm of uranium, and that uranium prices do not increase so that the main use of the phosphates are for fertilizers, this scenario would result in a maximum theoretical annual output of UO.\n\nHistorical operating costs for the uranium recovery from phosphoric acid range from $48–$119/kg UO. In 2011, the average price paid for UO in the United States was $122.66/kg.\n\nThere are 22 million tons of uranium in phosphate deposits. Recovery of uranium from phosphates is a Mature technology; it has been utilized in Belgium and the United States, but high recovery costs limit the utilization of these resources, with estimated production costs in the range of US$60–100/kgU including capital investment, according to a 2003 OECD report for a new 100 tU/year project.\n\nUnconventional uranium resources include up to of uranium contained in sea water. Several technologies to extract uranium from sea water have been demonstrated at the laboratory scale.\n\nIn the mid-1990s Extraction costs were estimated at 260 USD/kgU (Nobukawa, et al., 1994) but scaling up laboratory-level production to thousands of tonnes is unproven and may encounter unforeseen difficulties.\n\nOne method of extracting uranium from seawater is using a uranium-specific nonwoven fabric as an absorbent. The total amount of uranium recovered in an experiment in 2003 from three collection boxes containing 350 kg of fabric was >1 kg of yellow cake after 240 days of submersion in the ocean.\nAccording to the OECD, uranium may be extracted from seawater using this method for about US$300/kgU.\n\nIn 2006 the same research group stated: \"If 2g-U/kg-adsorbent is submerged for 60 days at a time and used 6 times, the uranium cost is calculated to be 88,000 JPY/kgU, including the cost of adsorbent production, uranium collection, and uranium purification. When an extraction 6g of U per kg of adsorbent and 20 repetitions or more becomes possible, the uranium cost reduces to 15,000 yen. This price level is equivalent to that of the highest cost of the minable uranium. The lowest cost attainable now is 25,000 yen with 4g-U/kg-adsorbent used in the sea area of Okinawa, with 18 repetition uses. In this case, the initial investment to collect the uranium from seawater is 107.7 billion yen, which is 1/3 of the construction cost of a one million-kilowatt class nuclear power plant.\"\n\nIn 2012, ORNL researchers announced the successful development of a new absorbent material dubbed HiCap, which vastly outperforms previous best adsorbents, which perform surface retention of solid or gas molecules, atoms or ions. \"We have shown that our adsorbents can extract five to seven times more uranium at uptake rates seven times faster than the world's best adsorbents\", said Chris Janke, one of the inventors and a member of ORNL's Materials Science and Technology Division. HiCap also effectively removes toxic metals from water, according to results verified by researchers at Pacific Northwest National Laboratory.\n\nAmong the other methods to recover uranium from sea water, two seem promising: algae bloom to concentrate uranium\nand nanomembrane filtering.\n\nSo far, no more than a very small amount of uranium has been recovered from sea water in a laboratory.\n\n In particular, nuclear power facilities produce about 200,000 metric tons of low and intermediate level waste (LILW) and 10,000 metric tons of high level waste (HLW) (including spent fuel designated as waste) each year worldwide.\n\nAlthough only several parts per million average concentration in coal before combustion (albeit more concentrated in ash), the theoretical maximum energy potential of trace uranium and thorium in coal (in breeder reactors) actually exceeds the energy released by burning the coal itself, according to a study by Oak Ridge National Laboratory.\n\nFrom 1965 to 1967 Union Carbide operated a mill in North Dakota, United States burning uraniferous lignite and extracting uranium from the ash. The plant produced about 150 metric tons of UO before shutting down.\n\nAn international consortium has set out to explore the commercial extraction of uranium from uraniferous coal ash from coal power stations located in Yunnan province, China. The first laboratory scale amount of yellowcake uranium recovered from uraniferous coal ash was announced in 2007. The three coal power stations at Xiaolongtang, Dalongtang and Kaiyuan have piled up their waste ash. Initial tests from the Xiaolongtang ash pile indicate that the material contains (160–180 parts per million uranium), suggesting a\ntotal of some UO could be recovered from that ash pile alone.\n\nSome oil shales contain uranium, which may be recovered as a byproduct. Between 1946 and 1952, a marine type of Dictyonema shale was used for uranium production in Sillamäe, Estonia, and between 1950 and 1989 alum shale was used in Sweden for the same purpose.\n\nA breeder reactor produces more nuclear fuel than it consumes and thus can extend the uranium supply. It typically turns the dominant isotope in natural uranium, uranium-238, into fissile plutonium-239. This results in hundredfold increase in the amount of energy to be produced per mass unit of uranium, because U-238, which constitute 99.3% of natural uranium, is not used in conventional reactors which instead use U-235 which only represent 0.7% of natural uranium. In 1983, physicist Bernard Cohen proposed that the world supply of uranium is effectively inexhaustible, and could therefore be considered a form of renewable energy. He claims that fast breeder reactors, fueled by naturally-replenished uranium-238 extracted from seawater, could supply energy at least as long as the sun's expected remaining lifespan of five billion years., making them as sustainable in fuel availability terms as renewable energy sources. Despite this hypothesis there is no known economically viable method to extract sufficient quantities from sea water. Experimental techniques are under investigation.\n\nThere are two types of breeders: Fast breeders and thermal breeders.\n\nA fast breeder, in addition to consuming U-235, converts fertile U-238 into Pu-239, a fissile fuel. Fast breeder reactors are more expensive to build and operate, including the reprocessing, and could only be justified economically if uranium prices were to rise to pre-1980 values in real terms. About 20 fast-neutron reactors have already been operating, some since the 1950s, and one supplies electricity commercially. Over 300 reactor-years of operating experience have been accumulated. In addition to considerably extending the exploitable fuel supply, these reactors have an advantage in that they produce less long-lived transuranic wastes, and can consume nuclear waste from current light water reactors, generating energy in the process. Several countries have research and development programs for improving these reactors. For instance, one scenario in France is for half of the present nuclear capacity to be replaced by fast breeder reactors by 2050. China, India, and Japan plan large scale utilization of breeder reactors during the coming decades. (Following the crisis at Japan's Fukishima Daiichi nuclear power plant in 2011, Japan is revising its plans regarding future use of nuclear power. (\"See:\" Fukushima Daiichi nuclear disaster: Energy policy implications.))\n\nThe breeding of plutonium fuel in Fast Breeder Reactors (FBR), known as the plutonium economy, was for a time believed to be the future of nuclear power. But many of the commercial breeder reactors that have been built have been riddled with technical and budgetary problems. Some sources critical of breeder reactors have gone so far to call them the Supersonic Transport of the '80s.\n\nUranium turned out to be far more plentiful than anticipated, and the price of uranium declined rapidly (with an upward blip in the 1970s). This is why the US halted their use in 1977 and the UK abandoned the idea in 1994.\n\nFast Breeder Reactors, are called fast because they have no moderator slowing down the neutrons (light water, heavy water or graphite) and breed more fuel than they consume. The word 'fast' in fast breeder thus refers to the speed of the neutrons in the reactor's core. The higher the energy the neutrons have, the higher the breeding ratio or the more uranium that is changed into plutonium.\n\nSignificant technical and materials problems were encountered with FBRs, and geological exploration showed that scarcity of uranium was not going to be a concern for some time. By the 1980s, due to both factors, it was clear that FBRs would not be commercially competitive with existing light water reactors. The economics of FBRs still depend on the value of the plutonium fuel which is bred, relative to the cost of fresh uranium. Research continues in several countries with working prototypes \"Phénix\" in France, the BN-600 reactor in Russia, and the Monju scheduled to be restarted in 2009.\n\nOn February 16, 2006 the United States, France and Japan signed an arrangement to research and develop sodium-cooled fast breeder reactors in support of the Global Nuclear Energy Partnership. Breeder reactors are also being studied under the Generation IV reactor program.\n\nEarly prototypes have been plagued with problems. The liquid sodium coolant is highly flammable, bursting into flames if it comes into contact with air and exploding if it comes into contact with water. Japan's fast breeder Monju Nuclear Power Plant has been scheduled to re-open in 2008, 13 years after a serious accident and fire involving a sodium leak. In 1997 France shut down its Superphenix reactor, while the Phenix, built earlier, closed as scheduled in 2009.\n\nAt higher uranium prices breeder reactors may be economically justified. Many nations have ongoing breeder research programs. China, India, and Japan plan large scale utilization of breeder reactors during the coming decades. 300 reactor-years experience has been gained in operating them.\n\nAs of June 2008 there are only two running commercial breeders and the rate of reactor-grade plutonium production is very small (20 tonnes/yr). The reactor grade plutonium is being processed into MOX fuel. However, next to the rate at which uranium is being mined (46,403 tonnes/yr), this is not enough to stave off peak uranium.\n\nThorium is an alternate fuel cycle to uranium. Thorium is three times more plentiful than uranium. Thorium-232 is in itself not fissile, but fertile. It can be made into fissile uranium-233 in a breeder reactor. In turn, the uranium-233 can be fissioned, with the advantage that smaller amounts of transuranics are produced by neutron capture, compared to uranium-235 and especially compared to plutonium-239.\n\nDespite the thorium fuel cycle having a number of attractive features, development on a large scale can run into difficulties:\n\nAdvocates for liquid core and molten salt reactors such as LFTR claim that these technologies negate the abovementioned thorium's disadvantages present in solid fueled reactors.\n\nThe first successful commercial reactor at the Indian Point power station in Buchanan, New York (Indian Point Unit 1) ran on Thorium. The first core did not live up to expectations.\n\nIndian interest in thorium is motivated by their substantial reserves. Almost a third of the world's thorium reserves are in India. India's Department of Atomic Energy (DAE) says that it will construct a 500 MWe prototype reactor in Kalpakkam. There are plans for four breeder reactors of 500 MWe each - two in Kalpakkam and two more in a yet undecided location.\n\nChina has initiated a research and development project in thorium molten-salt breeder reactor technology. It was formally announced at the Chinese Academy of Sciences (CAS) annual conference in January 2011. Its ultimate target is to investigate and develop a thorium based molten salt breeder nuclear system in about 20 years. A 5 MWe research MSR is apparently under construction at Shanghai Institute of Applied Physics (under the Academy) with 2015 target operation.\n\nDue to reduction in nuclear weapons stockpiles, a large amount of former weapons uranium was released for use in civilian nuclear reactors. As a result, starting in 1990, a significant portion of uranium nuclear power requirements were supplied by former weapons uranium, rather than newly mined uranium. In 2002, mined uranium supplied only 54 percent of nuclear power requirements. But as the supply of former weapons uranium has been used up, mining has increased, so that in 2012, mining provided 95 percent of reactor requirements, and the OCED Nuclear Energy Agency and the International Atomic Energy Agency projected that the gap in supply would be completely erased in 2013.\n\nEleven countries, Germany, the Czech Republic, France, DR Congo, Gabon, Bulgaria, Tajikistan, Hungary, Romania, Spain, Portugal\nand Argentina, have seen uranium production peak, and rely on imports for their nuclear programs. Other countries have reached their peak production of uranium and are currently on a decline.\n\n\n\n\n\nUranium mining declined with the last open pit mine shutting down in 1992 (Shirley Basin, Wyoming). United States production occurred in the following states (in descending order): New Mexico, Wyoming, Colorado, Utah, Texas, Arizona, Florida, Washington, and South Dakota. The collapse of uranium prices caused all conventional mining to cease by 1992. \"In-situ\" recovery or ISR has continued primarily in Wyoming and adjacent Nebraska as well has recently restarted in Texas.\n\n\nIn 1943, Alvin M. Weinberg et al. believed that there were serious limitations on nuclear energy if only U-235 were used as a nuclear power plant fuel. They concluded that breeding was required to usher in the age of nearly endless energy.\n\nIn 1956, M. King Hubbert declared world fissionable reserves adequate for at least the next few centuries, assuming breeding and reprocessing would be developed into economical processes.\n\nIn 1975 the US Department of the Interior, Geological Survey, distributed the press release \"Known US Uranium Reserves Won't Meet Demand\". It was recommended that the US not depend on foreign imports of uranium.\n\nAll the following sources predict peak uranium:\n\nEdward Steidle, Dean of the School of Mineral Industries at Pennsylvania State College, predicted in 1952 that supplies of fissionable elements were too small to support commercial-scale energy production.\n\nRobert Vance, while looking back at 40 years of uranium production through all of the Red Books, found that peak global production was achieved in 1980 at from 22 countries. In 2003, uranium production totaled from 19 countries.\n\nMichael Meacher, the former environment minister of the UK 1997–2003, and UK Member of Parliament, reports that peak uranium happened in 1981. He also predicts a major shortage of uranium sooner than 2013 accompanied with hoarding and its value pushed up to the levels of precious metals.\n\nDay projected that uranium reserves could run out as soon as 1989, but, more optimistically, would be exhausted by 2015.\n\nJan Willem Storm van Leeuwen, an independent analyst with Ceedata Consulting, contends that supplies of the high-grade uranium ore required to fuel nuclear power generation will, at current levels of consumption, last to about 2034. Afterwards, the cost of energy to extract the uranium will exceed the price the electric power provided.\n\nThe Energy Watch Group has calculated that, even with steep uranium prices, uranium production will have reached its peak by 2035 and that it will only be possible to satisfy the fuel demand of nuclear plants until then.\n\nVarious agencies have tried to estimate how long these resources will last.\n\n\nThe European Commission said in 2001 that at the current level of uranium consumption, known uranium resources would last 42 years. When added to military and secondary sources, the resources could be stretched to 72 years. Yet this rate of usage assumes that nuclear power continues to provide only a fraction of the world’s energy supply. If electric capacity were increased six-fold, then the 72-year supply would last just 12 years.\n\n\nThe world's present measured resources of uranium, economically recoverable at a price of US$130/kg according to the industry groups OECD, NEA and IAEA, are enough to last for 100 years at current consumption.\n\n\nAccording to the Australian Uranium Association, yet another industry group, assuming the world's current rate of consumption at 66,500 tonnes of uranium per year and the world's present measured resources of uranium (4.7 Mt) are enough to last for 70 years.\n\nAll the following references claim that the supply is far more than demand. Therefore, they do not predict peak uranium.\n\nIn his 1956 landmark paper, M. King Hubbert wrote \"There is promise, however, provided mankind can solve its international problems and not destroy itself with nuclear weapons, and provided world population (which is now expanding at such a rate as to double in less than a century) can somehow be brought under control, that we may at last have found an energy supply adequate for our needs for at least the next few centuries of the 'foreseeable future.'\" Hubbert's study assumed that breeder reactors would replace light water reactors and that uranium would be bred into plutonium (and possibly thorium would be bred into uranium). He also assumed that economic means of reprocessing would be discovered. For political, economic and nuclear proliferation reasons, the plutonium economy never materialized. Without it, uranium is used up in a once-through process and will peak and run out much sooner. However, at present, it is generally found to be cheaper to mine new uranium out of the ground than to use reprocessed uranium, and therefore the use of reprocessed uranium is limited to only a few nations.\n\nThe OECD estimates that with the world nuclear electricity generating rates of 2002, with LWR, once-through fuel cycle, there are enough conventional resources to last 85 years using known resources and 270 years using known and as yet undiscovered resources. With breeders, this is extended to 8,500 years.\n\nIf one is willing to pay $300/kg for uranium, there is a vast quantity available in the ocean. It is worth noting that since fuel cost only amounts to a small fraction of nuclear energy total cost per kWh, and raw uranium price also constitutes a small fraction of total fuel costs, such an increase on uranium prices wouldn’t involve a very significant increase in the total cost per kWh produced.\n\nIn 1983, physicist Bernard Cohen proposed that uranium is effectively inexhaustible, and could therefore be considered a renewable source of energy. He claims that fast breeder reactors, fueled by naturally replenished uranium extracted from seawater, could supply energy at least as long as the sun's expected remaining lifespan of five billion years. While uranium is a finite mineral resource within the earth, the hydrogen in the sun is finite too – thus, if the resource of nuclear fuel can last over such time scales, as Cohen contends, then nuclear energy is every bit as sustainable as solar power or any other source of energy, in terms of sustainability over the time scale of life surviving on this planet.\n\nHis paper assumes extraction of uranium from seawater at the rate of per year of uranium. The current demand for uranium is near per year; however, the use of breeder reactors means that uranium would be used at least 60 times more efficiently than today.\n\nA nuclear engineer writing for American Energy Independence in 2004 believes that there is a several hundred years' supply of recoverable uranium even for standard reactors. For breeder reactors, \"it is essentially infinite\".\nAll the following references claim that the supply is far more than demand. Therefore, they believe that uranium will not deplete in the foreseeable future.\n\nThe IAEA estimates that using only known reserves at the current rate of demand and assuming a once-through nuclear cycle that there is enough uranium for at least 100 years. However, if all primary known reserves, secondary reserves, undiscovered and unconventional sources of uranium are used, uranium will be depleted in 47,000 years.\n\nKenneth S. Deffeyes estimates that if one can accept ore one tenth as rich then the supply of available uranium increased 300 times. His paper shows that uranium concentration in ores is log-normal distributed. There is relatively little high-grade uranium and a large supply of very low grade uranium.\n\nErnest J. Moniz, a professor at the Massachusetts Institute of Technology and the current United States Secretary of Energy, testified in 2009 that an abundance of uranium had put into question plans to reprocess spent nuclear fuel. The reprocessing plans dated from decades previous, when uranium was thought to be scarce. But now, \"roughly speaking, we’ve got uranium coming out of our ears, for a long, long time,\" Professor Moniz said.\n\nAs uranium production declines, uranium prices would be expected to increase. However, the price of uranium makes up only 9% of the cost of running a nuclear power plant, much lower than the cost of coal in a coal-fired power plant (77%), or the cost of natural gas in a gas-fired power plant (93%).\n\nUranium is different from conventional energy resources, such as oil and coal, in several key aspects. Those differences limit the effects of short-term uranium shortages, but most have no bearing on the eventual depletion. Some key features are:\n\nFast neutron reactors (breeder reactors) could utilize large amounts of Uranium-238 indirectly by conversion to Plutonium-239, rather than fissioning primarily just Uranium-235 (which is 0.7% of original mined uranium), for approximately a factor of 100 increase in uranium usage efficiency. Intermediate between conventional estimates of reserves and the 40 trillion tons total of uranium in Earth's crust (trace concentrations adding up over its 3 * 10 ton mass), there are ores of lower grade than otherwise practical but of still higher concentration than the average rock. Accordingly, resource figures depend on economic and technological assumptions.\n\nThe uranium spot price has increased from a low in Jan 2001 of US$6.40 per pound of UO to a peak in June 2007 of US$135. The uranium prices have dropped substantially since. Currently (15 July 2013) the uranium spot is US$38.\n\nThe high price in 2007 resulted from shrinking weapons stockpiles and a flood at the Cigar Lake Mine, coupled with expected rises in demand due to more reactors coming online, leading to a uranium price bubble. Miners and Utilities are bitterly divided on uranium prices.\n\nAs prices go up, production responds from existing mines, and production from newer, harder to develop or lower quality uranium ores begins. Currently, much of the new production is coming from Kazakhstan. Production expansion is expected in Canada and in the United States. However, the number of projects waiting in the wings to be brought online now are far less than there were in the 1970s. There have been some encouraging signs that production from existing or planned mines is responding or will respond to higher prices. The supply of uranium has recently become very inelastic. As the demand increases, the prices respond dramatically.\n\nUnlike other metals such as gold, silver, copper or nickel, uranium is not widely traded on an organized commodity exchange such as the London Metal Exchange. It is traded on the NYMEX but on very low volume. Instead, it is traded in most cases through contracts negotiated directly between a buyer and a seller. The structure of uranium supply contracts varies widely. The prices are either fixed or base on referenced to economic indices such as GDP, inflation or currency exchange. Contracts traditionally are based on the uranium spot price and rules by which the price can escalate. Delivery quantities, schedules, and prices vary from contract to contract and often from delivery to delivery within the term of a contract.\n\nSince the number of companies mining uranium is small, the number of available contracts is also small. Supplies are running short due to flooding of two of the world's largest mines and a dwindling amount of uranium salvaged from nuclear warheads being removed from service. While demand for the metal has been steady for years, the price of uranium is expected to surge as a host of new nuclear plants come online.\n\nRising uranium price entices draws investment into new uranium mining projects. Mining companies are returning to abandoned uranium mines with new promises of hundreds of jobs and millions in royalties. Some locals want them back. Others say the risk is too great, and will try to stop those companies \"until there's a cure for cancer.\"\n\nSince many utilities have extensive stockpiles and can plan many months in advance, they take a wait-and-see approach on higher uranium costs. In 2007, spot prices rose significantly due to announcements of planned reactors or new reactors coming online. Those trying to find uranium in a rising cost climate are forced to face the reality of a seller’s market. Sellers remain reluctant to sell significant quantities. By waiting longer, sellers expect to get a higher price for the material they hold. Utilities on the other hand, are very eager to lock up long-term uranium contracts.\n\nAccording to the NEA, the nature of nuclear generating costs allows for significant increases in the costs of uranium before the costs of generating electricity significantly increase. A 100% increase in uranium costs would only result in a 5% increase in electric cost. This is because uranium has to be converted to gas, enriched, converted back to yellow cake and fabricated into fuel elements. The cost of the finished fuel assemblies are dominated by the processing costs, not the cost of the raw materials. Furthermore, the cost of electricity from a nuclear power plant is dominated by the high capital and operating costs, not the cost of the fuel. Nevertheless, any increase in the price of uranium is eventually passed on to the consumer either directly or through a fuel surcharge.\n\nAn alternative to uranium is thorium which is three times more common than uranium. Fast breeder reactors are not needed. Compared to conventional uranium reactors, thorium reactors using the thorium fuel cycle may produce some 40 times the amount of energy per unit of mass.\n\nIf nuclear power prices rise too quickly, or too high, power companies may look for substitutes in fossil energy (coal, oil, and gas) and/or renewable energy, such as hydro, bio-energy, solar thermal electricity, geothermal, wind, tidal energy. Both fossil energy and some renewable electricity sources (e.g. hydro, bioenergy, solar thermal electricity and geothermal) can be used as base-load.\n\nPrediction\n\nTechnology\n\nEconomics\n\nOthers\n\n\n"}
{"id": "2740328", "url": "https://en.wikipedia.org/wiki?curid=2740328", "title": "Peukert's law", "text": "Peukert's law\n\nPeukert's law, presented by the German scientist in 1897, expresses approximately the change in capacity of rechargeable lead–acid batteries at different rates of discharge. As the rate of discharge increases, the battery's available capacity decreases, approximately according to Peukert's law.\n\nManufacturers specify the capacity of a battery at a specified discharge rate. For example, a battery might be rated at 100 A·h when discharged at a rate that will fully discharge the battery in 20 hours (at 5 amperes for this example). If discharged at a faster rate the delivered capacity is less. Peukert's law describes a power relationship between the discharge current (normalized to some base rated current) and delivered capacity (normalized to the rated capacity) over some specified range of discharge currents. If Peukert's constant formula_1, the exponent, were equal to unity, the delivered capacity would be independent of the current. For a real battery the exponent is greater than unity, and capacity decreases as discharge rate increases. For a lead–acid battery formula_1 is typically between 1.1 and 1.3. For different lead-acid rechargeable battery technologies it generally ranges from 1.05 to 1.15 for VRSLAB AGM batteries, from 1.1 to 1.25 for gel, and from 1.2 to 1.6 for flooded batteries. The Peukert constant varies with the age of the battery, generally increasing (getting worse) with age. Application at low discharge rates must take into account the battery self-discharge current. At very high currents, practical batteries will give less capacity than predicted with a fixed exponent. The equation does not take into account the effect of temperature on battery capacity.\n\nFor a one-ampere discharge rate, Peukert's law is often stated as\n\nwhere:\n\nThe capacity at a one-ampere discharge rate is not usually given for practical cells. As such, it can be useful to reformulate the law to a known capacity and discharge rate:\n\nwhere:\n\nUsing the above example, if the battery has a Peukert constant of 1.2 and is discharged at a rate of 10 amperes, it would be fully discharged in time formula_14, which is approximately 8.7 hours. It would therefore deliver only 87 ampere-hours rather than 100.\n\nPeukert's law can be written as\n\ngiving formula_16, which is the effective capacity at the discharge rate formula_5.\n\nPeukert's law, taken literally, would imply that the total charge delivered by the battery (formula_18) goes to infinity as the rate of discharge goes to zero. This is of course impossible, because the battery will still self-discharge internally with or without zero discharge through a load. The self discharge rate depends on the chemistry and ambient temperature.\n\nIf the capacity is listed for two discharge rates, the Peukert exponent can be determined algebraically:\n\nAnother commonly used form of the Peukert's law is:\n\nwhere:\n\nSeveral representative examples of different α and corresponding k are tabulated below:\n\nPeukert's law becomes a key issue in a battery electric vehicle, where batteries rated, for example, at a 20-hour discharge time are used at a much shorter discharge time of about 1 hour.\nAt high load currents the internal resistance of a real battery dissipates significant power, reducing the power (watts) available to the load in addition to the Peukert reduction, delivering less capacity than the simple power law equation predicts.\n\nA 2006 critical study concluded that Peukert's equation could not be used to predict the state of charge of a battery accurately unless it is discharged at a constant current and constant temperature.\n\nPeukert's law was developed for Lead-Acid batteries, and works well in that application. \n\nIt does not necessarily apply to other battery chemistries, especially Lithium-Ion batteries.\nLithium-Ion batteries tend to self-heat during rapid discharge, and the Nernst Equation predicts battery voltage will increase with temperature.\nThus, the effect of increased resistance is offset by the self-heating effect.\nThis advantage of Lithium-Ion batteries is a well-known advertised feature, see .\nIn a research paper, a 50Ah lithium-ion battery tested was found to give about the same capacity at 5A and 50A; this was attributed to possible Peukert loss in capacity being countered by the increase in capacity due to the 30◦C temperature rise due to self-heating, with the conclusion that the Peukert equation is not applicable. \n\nPeukert's law brings a certain degree of fire-safety to many battery designs. It limits the maximum output power of the battery.\nA good example of this is lead-acid batteries, which will not catch fire via excessive discharge currents.\nAs such, starting a car is safe even if the lead-acid battery dies.\nThe primary fire-danger with lead-acid batteries occurs during over-charging when hydrogen gas is produced. This danger is easily controlled by limiting the available charge voltage, and ensuring ventilation is present during charging to vent any excess hydrogen gas. A secondary danger exists when broken plates inside the battery short out the battery, or reconnect inside the battery causing an internal spark, igniting the hydrogen and oxygen generated inside the battery during very fast discharge. \n\nOn the other hand, Lithium-Ion batteries self-heat, do not follow Peukert's law, and have a flammable electrolyte. The combination results in their catching fire when discharged at rapid rates. In particular, if the cell develops an internal short, it tends to overheat, release electrolyte, and catch fire. A fire generates additional heat, which can melt adjacent cells and result in additional leakage of the flammable electrolyte. Additionally, a fire can also increase cell temperatures in adjacent cells, and this further increase the available fault currents (and heat). The resultant runaway reactions can be spectacular.\n\n\n"}
{"id": "652531", "url": "https://en.wikipedia.org/wiki?curid=652531", "title": "Photovoltaics", "text": "Photovoltaics\n\nPhotovoltaics (PV) is the conversion of light into electricity using semiconducting materials that exhibit the photovoltaic effect, a phenomenon studied in physics, photochemistry, and electrochemistry.\n\nA photovoltaic system employs solar panels, each comprising a number of solar cells, which generate electrical power. PV installations may be ground-mounted, rooftop mounted or wall mounted. The mount may be fixed, or use a solar tracker to follow the sun across the sky.\n\nSolar PV has specific advantages as an energy source: once installed, its operation generates no pollution and no greenhouse gas emissions, it shows simple scalability in respect of power needs and silicon has large availability in the Earth’s crust.\n\nPV systems have the major disadvantage that the power output works best with direct sunlight, so about 10-25% is lost if a tracking system is not used. Dust, clouds, and other obstructions in the atmosphere also diminish the power output. Another important issue is the concentration of the production in the hours corresponding to main insolation, which do not usually match the peaks in demand in human activity cycles. Unless current societal patterns of consumption and electrical networks adjust to this scenario, electricity still needs to be stored for later use or made up by other power sources, usually hydrocarbons.\n\nPhotovoltaic systems have long been used in specialized applications, and stand-alone and grid-connected PV systems have been in use since the 1990s. They were first mass-produced in 2000, when German environmentalists and the Eurosolar organization got government funding for a ten thousand roof program.\n\nAdvances in technology and increased manufacturing scale have in any case reduced the cost, increased the reliability, and increased the efficiency of photovoltaic installations. \nNet metering and financial incentives, such as preferential feed-in tariffs for solar-generated electricity, have supported solar PV installations in many countries. More than 100 countries now use solar PV.\n\nAfter hydro and wind powers, PV is the third renewable energy source in terms of global capacity. At the end of 2016, worldwide installed PV capacity increased to more than 300 gigawatts (GW), covering approximately two percent of global electricity demand. China, followed by Japan and the United States, is the fastest growing market, while Germany remains the world's largest producer, with solar PV providing seven percent of annual domestic electricity consumption. With current technology (as of 2013), photovoltaics recoups the energy needed to manufacture them in 1.5 years in Southern Europe and 2.5 years in Northern Europe.\n\nThe term \"photovoltaic\" comes from the Greek φῶς (\"phōs\") meaning \"light\", and from \"volt\", the unit of electro-motive force, the volt, which in turn comes from the last name of the Italian physicist Alessandro Volta, inventor of the battery (electrochemical cell). The term \"photo-voltaic\" has been in use in English since 1849.\n\nPhotovoltaics are best known as a method for generating electric power by using solar cells to convert energy from the sun into a flow of electrons by the photovoltaic effect.\n\nSolar cells produce direct current electricity from sunlight which can be used to power equipment or to recharge a battery. The first practical application of photovoltaics was to power orbiting satellites and other spacecraft, but today the majority of photovoltaic modules are used for grid connected power generation. In this case an inverter is required to convert the DC to AC. There is a smaller market for off-grid power for remote dwellings, boats, recreational vehicles, electric cars, roadside emergency telephones, remote sensing, and cathodic protection of pipelines.\n\nPhotovoltaic power generation employs solar panels composed of a number of solar cells containing a photovoltaic material. Copper solar cables connect modules (module cable), arrays (array cable), and sub-fields. Because of the growing demand for renewable energy sources, the manufacturing of solar cells and photovoltaic arrays has advanced considerably in recent years.\n\nSolar photovoltaic power generation has long been seen as a clean energy technology which draws upon the planet’s most plentiful and widely distributed renewable energy source – the sun. Cells require protection from the environment and are usually packaged tightly in solar panels.\n\nPhotovoltaic power capacity is measured as maximum power output under standardized test conditions (STC) in \"W\" (watts peak). The actual power output at a particular point in time may be less than or greater than this standardized, or \"rated\", value, depending on geographical location, time of day, weather conditions, and other factors. Solar photovoltaic array capacity factors are typically under 25%, which is lower than many other industrial sources of electricity.\n\nFor best performance, terrestrial PV systems aim to maximize the time they face the sun. Solar trackers achieve this by moving PV panels to follow the sun. The increase can be by as much as 20% in winter and by as much as 50% in summer. Static mounted systems can be optimized by analysis of the sun path. Panels are often set to latitude tilt, an angle equal to the latitude, but performance can be improved by adjusting the angle for summer or winter. Generally, as with other semiconductor devices, temperatures above room temperature reduce the performance of photovoltaics.\n\nA number of solar panels may also be mounted vertically above each other in a tower, if the zenith distance of the Sun is greater than zero, and the tower can be turned horizontally as a whole and each panels additionally around a horizontal axis. In such a tower the panels can follow the Sun exactly. Such a device may be described as a ladder mounted on a turnable disk. Each step of that ladder is the middle axis of a rectangular solar panel. In case the zenith distance of the Sun reaches zero, the \"ladder\" may be rotated to the north or the south to avoid a solar panel producing a shadow on a lower solar panel. Instead of an exactly vertical tower one can choose a tower with an axis directed to the polar star, meaning that it is parallel to the rotation axis of the Earth. In this case the angle between the axis and the Sun is always larger than 66 degrees. During a day it is only necessary to turn the panels around this axis to follow the Sun. Installations may be ground-mounted (and sometimes integrated with farming and grazing) or built into the roof or walls of a building (building-integrated photovoltaics).\n\nAnother recent development involves the makeup of solar cells. Perovskite is a very inexpensive material which is being used to replace the expensive crystalline silicon which is still part of a standard PV cell build to this day. Michael Graetzel, Director of the Laboratory of Photonics and Interfaces at EPFL says, \"Today, efficiency has peaked at 18 percent, but it's expected to get even higher in the future.\" This is a significant claim, as 20% efficiency is typical among solar panels which use more expensive materials.\n\nElectrical efficiency (also called conversion efficiency) is a contributing factor in the selection of a photovoltaic system. However, the most efficient solar panels are typically the most expensive, and may not be commercially available. Therefore, selection is also driven by cost efficiency and other factors.\n\nThe electrical efficiency of a PV cell is a physical property which represents how much electrical power a cell can produce for a given insolation. The basic expression for maximum efficiency of a photovoltaic cell is given by the ratio of output power to the incident solar power (radiation flux times area)\n\nThe efficiency is measured under ideal laboratory conditions and represents the maximum achievable efficiency of the PV material. Actual efficiency is influenced by the output Voltage, current, junction temperature, light intensity and spectrum.\n\nThe most efficient type of solar cell to date is a multi-junction concentrator solar cell with an efficiency of 46.0% produced by Fraunhofer ISE in December 2014. The highest efficiencies achieved without concentration include a material by Sharp Corporation at 35.8% using a proprietary triple-junction manufacturing technology in 2009, and Boeing Spectrolab (40.7% also using a triple-layer design). The US company SunPower produces cells that have an efficiency of 21.5%, well above the market average of 12–18%.\n\nThere is an ongoing effort to increase the conversion efficiency of PV cells and modules, primarily for competitive advantage. In order to increase the efficiency of solar cells, it is important to choose a semiconductor material with an appropriate band gap that matches the solar spectrum. This will enhance the electrical and optical properties. Improving the method of charge collection is also useful for increasing the efficiency. There are several groups of materials that are being developed. Ultrahigh-efficiency devices (η>30%) are made by using GaAs and GaInP2 semiconductors with multijunction tandem cells. High-quality, single-crystal silicon materials are used to achieve high-efficiency, low cost cells (η>20%).\n\nRecent developments in Organic photovoltaic cells (OPVs) have made significant advancements in power conversion efficiency from 3% to over 15% since their introduction in the 1980s. To date, the highest reported power conversion efficiency ranges from 6.7% to 8.94% for small molecule, 8.4%–10.6% for polymer OPVs, and 7% to 21% for perovskite OPVs. OPVs are expected to play a major role in the PV market. Recent improvements have increased the efficiency and lowered cost, while remaining environmentally-benign and renewable.\n\nSeveral companies have begun embedding power optimizers into PV modules called smart modules. These modules perform maximum power point tracking (MPPT) for each module individually, measure performance data for monitoring, and provide additional safety features. Such modules can also compensate for shading effects, wherein a shadow falling across a section of a module causes the electrical output of one or more strings of cells in the module to decrease.\n\nOne of the major causes for the decreased performance of cells is overheating. The efficiency of a solar cell declines by about 0.5% for every 1 degree Celsius increase in temperature. This means that a 100 degree increase in surface temperature could decrease the efficiency of a solar cell by about half. Self-cooling solar cells are one solution to this problem. Rather than using energy to cool the surface, pyramid and cone shapes can be formed from silica, and attached to the surface of a solar panel. Doing so allows visible light to reach the solar cells, but reflects infrared rays (which carry heat).\n\nSolar photovoltaics is growing rapidly and worldwide installed capacity reached about 300 gigawatts (GW) by the end of 2016. Since 2000, installed capacity has seen a growth factor of about 57. The total power output of the world’s PV capacity in a calendar year in 2014 is now beyond 200 TWh of electricity. This represents 1% of worldwide electricity demand. More than 100 countries use solar PV. China, followed by Japan and the United States is now the fastest growing market, while Germany remains the world's largest producer, contributing more than 7% to its national electricity demands. Photovoltaics is now, after hydro and wind power, the third most important renewable energy source in terms of globally installed capacity.\n\nSeveral market research and financial companies foresee record-breaking global installation of more than 50 GW in 2015. China is predicted to take the lead from Germany and to become the world's largest producer of PV power by installing another targeted 17.8 GW in 2015. India is expected to install 1.8 GW, doubling its annual installations. By 2018, worldwide photovoltaic capacity is projected to doubled or even triple to 430 GW. Solar Power Europe (formerly known as EPIA) also estimates that photovoltaics will meet 10% to 15% of Europe's energy demand in 2030.\n\nIn 2017 a study in Science estimated that by 2030 global PV installed capacities will be between 3,000 and 10,000 GW. The EPIA/Greenpeace Solar Generation Paradigm Shift Scenario (formerly called Advanced Scenario) from 2010 shows that by the year 2030, 1,845 GW of PV systems could be generating approximately 2,646 TWh/year of electricity around the world. Combined with energy use efficiency improvements, this would represent the electricity needs of more than 9% of the world's population. By 2050, over 20% of all electricity could be provided by photovoltaics.\n\nMichael Liebreich, from Bloomberg New Energy Finance, anticipates a tipping point for solar energy. The costs of power from wind and solar are already below those of conventional electricity generation in some parts of the world, as they have fallen sharply and will continue to do so. He also asserts, that the electrical grid has been greatly expanded worldwide, and is ready to receive and distribute electricity from renewable sources. In addition, worldwide electricity prices came under strong pressure from renewable energy sources, that are, in part, enthusiastically embraced by consumers.\n\nDeutsche Bank sees a \"second gold rush\" for the photovoltaic industry to come. Grid parity has already been reached in at least 19 markets by January 2014. Photovoltaics will prevail beyond feed-in tariffs, becoming more competitive as deployment increases and prices continue to fall.\n\nIn June 2014 Barclays downgraded bonds of U.S. utility companies. Barclays expects more competition by a growing self-consumption due to a combination of decentralized PV-systems and residential electricity storage. This could fundamentally change the utility's business model and transform the system over the next ten years, as prices for these systems are predicted to fall.\n\nTypes of impacts\n\nWhile solar photovoltaic (PV) cells are promising for clean energy production, their deployment is hindered by production costs, material availability, and toxicity. Data required to investigate their impact are sometimes affected by a rather large amount of uncertainty. The values of human labor and water consumption, for example, are not precisely assessed due to the lack of systematic and accurate analyses in the scientific literature.\n\nLife cycle assessment (LCA) is one method of determining environmental impacts from PV. Many studies have been done on the various types of PV including first generation, second generation, and third generation. Usually these PV LCA studies select a cradle to gate system boundary because often at the time the studies are conducted, it is a new technology not commercially available yet and their required balance of system components and disposal methods are unknown.\n\nA traditional LCA can look at many different impact categories ranging from global warming potential, eco-toxicity, human toxicity, water depletion, and many others.\n\nMost LCAs of PV have focused on two categories: carbon dioxide equivalents per kWh and energy pay-back time (EPBT). The EPBT is defined as \" the time needed to compensate for the total renewable- and non-renewable- primary energy required during the life cycle of a PV system\". A 2015 review of EPBT from first and second generation PV suggested that there was greater variation in embedded energy than in efficiency of the cells implying that it was mainly the embedded energy that needs to reduce to have a greater reduction in EPBT. One difficulty in determining impacts due to PV is to determine if the wastes are released to the air, water, or soil during the manufacturing phase. Research is underway to try to understand emissions and releases during the lifetime of PV systems.\n\nImpacts from first-generation PV\n\nCrystalline silicon modules are the most extensively studied PV type in terms of LCA since they are the most commonly used. Mono-crystalline silicon photovoltaic systems (mono-si) have an average efficiency of 14.0%. The cells tend to follow a structure of front electrode, anti-reflection film, n-layer, p-layer, and back electrode, with the sun hitting the front electrode. EPBT ranges from 1.7 to 2.7 years. The cradle to gate of CO-eq/kWh ranges from 37.3 to 72.2 grams.\n\nTechniques to produce multi-crystalline silicon (multi-si) photovoltaic cells are simpler and cheaper than mono-si, however tend to make less efficient cells, an average of 13.2%. EPBT ranges from 1.5 to 2.6 years. The cradle to gate of CO-eq/kWh ranges from 28.5 to 69 grams. Some studies have looked beyond EPBT and GWP to other environmental impacts. In one such study, conventional energy mix in Greece was compared to multi-si PV and found a 95% overall reduction in impacts including carcinogens, eco-toxicity, acidification, eutrophication, and eleven others.\n\nImpacts from second generation\n\nCadmium telluride (CdTe) is one of the fastest-growing thin film based solar cells which are collectively known as second generation devices. This new thin film device also shares similar performance restrictions (Shockley-Queisser efficiency limit) as conventional Si devices but promises to lower the cost of each device by both reducing material and energy consumption during manufacturing. Today the global market share of CdTe is 5.4%, up from 4.7% in 2008. This technology’s highest power conversion efficiency is 21%. The cell structure includes glass substrate (around 2 mm), transparent conductor layer, CdS buffer layer (50–150 nm), CdTe absorber and a metal contact layer.\n\nCdTe PV systems require less energy input in their production than other commercial PV systems per unit electricity production. The average CO-eq/kWh is around 18 grams (cradle to gate). CdTe has the fastest EPBT of all commercial PV technologies, which varies between 0.3 and 1.2 years.\n\nCopper Indium Gallium Diselenide (CIGS) is a thin film solar cell based on the copper indium diselenide (CIS) family of chalcopyrite semiconductors. CIS and CIGS are often used interchangeably within the CIS/CIGS community. The cell structure includes soda lime glass as the substrate, Mo layer as the back contact, CIS/CIGS as the absorber layer, cadmium sulfide (CdS) or Zn (S,OH)x as the buffer layer, and ZnO:Al as the front contact. CIGS is approximately 1/100th the thickness of conventional silicon solar cell technologies. Materials necessary for assembly are readily available, and are less costly per watt of solar cell. CIGS based solar devices resist performance degradation over time and are highly stable in the field.\n\nReported global warming potential impacts of CIGS range from 20.5 – 58.8 grams CO-eq/kWh of electricity generated for different solar irradiation (1,700 to 2,200 kWh/m/y) and power conversion efficiency (7.8 – 9.12%). EPBT ranges from 0.2 to 1.4 years, while harmonized value of EPBT was found 1.393 years. Toxicity is an issue within the buffer layer of CIGS modules because it contains cadmium and gallium. CIS modules do not contain any heavy metals.\n\nImpacts from third generation\n\nThird-generation PVs are designed to combine the advantages of both the first and second generation devices and they do not have Shockley-Queisser limit, a theoretical limit for first and second generation PV cells. The thickness of a third generation device is less than 1 µm.\n\nOne emerging alternative and promising technology is based on an organic-inorganic hybrid solar cell made of methylammonium lead halide perovskites. Perovskite PV cells have progressed rapidly over the past few years and have become one of the most attractive areas for PV research. The cell structure includes a metal back contact (which can be made of Al, Au or Ag), a hole transfer layer (spiro-MeOTAD, P3HT, PTAA, CuSCN, CuI, or NiO), and absorber layer (CHNHPbIxBr-x, CHNHPbIxCl-x or CHNHPbI), an electron transport layer (TiO, ZnO, AlO or SnO) and a top contact layer (fluorine doped tin oxide or tin doped indium oxide).\n\nThere are a limited number of published studies to address the environmental impacts of perovskite solar cells. The major environmental concern is the lead used in the absorber layer. Due to the instability of perovskite cells lead may eventually be exposed to fresh water during the use phase. These LCA studies looked at human and ecotoxicity of perovskite solar cells and found they were surprisingly low and may not be an environmental issue. Global warming potential of perovskite PVs were found to be in the range of 24–1500 grams CO-eq/kWh electricity production. Similarly, reported EPBT of the published paper range from 0.2 to 15 years. The large range of reported values highlight the uncertainties associated with these studies. Celik et al. (2016) critically discussed the assumptions made in perovskite PV LCA studies.\n\nTwo new promising thin film technologies are copper zinc tin sulfide (CuZnSnS or CZTS), zinc phosphide (ZnP) and single-walled carbon nano-tubes (SWCNT). These thin films are currently only produced in the lab but may be commercialized in the future. The manufacturing of CZTS and (ZnP) processes are expected to be similar to those of current thin film technologies of CIGS and CdTe, respectively. While the absorber layer of SWCNT PV is expected to be synthesized with CoMoCAT method. by Contrary to established thin films such as CIGS and CdTe, CZTS, ZnP, and SWCNT PVs are made from earth abundant, nontoxic materials and have the potential to produce more electricity annually than the current worldwide consumption. While CZTS and ZnP offer good promise for these reasons, the specific environmental implications of their commercial production are not yet known. Global warming potential of CZTS and ZnP were found 38 and 30 grams CO-eq/kWh while their corresponding EPBT were found 1.85 and 0.78 years, respectively. Overall, CdTe and ZnP have similar environmental impacts but can slightly outperform CIGS and CZTS. Celik et al. performed the first LCA study on environmental impacts of SWCNT PVs, including a laboratory-made 1% efficient device and an aspirational 28% efficient four-cell tandem device and interpreted the results by using mono-Si as a reference point. the results show that compared to monocrystalline Si (mono-Si), the environmental impacts from 1% SWCNT was ∼18 times higher due mainly to the short lifetime of three years. However, even with the same short lifetime, the 28% cell had lower environmental impacts than mono-Si.\n\nOrganic and polymer photovoltaic (OPV) are a relatively new area of research. The tradition OPV cell structure layers consist of a semi-transparent electrode, electron blocking layer, tunnel junction, holes blocking layer, electrode, with the sun hitting the transparent electrode. OPV replaces silver with carbon as an electrode material lowering manufacturing cost and making them more environmentally friendly. OPV are flexible, low weight, and work well with roll-to roll manufacturing for mass production. OPV uses \"only abundant elements coupled to an extremely low embodied energy through very low processing temperatures using only ambient processing conditions on simple printing equipment enabling energy pay-back times\". Current efficiencies range from 1–6.5%, however theoretical analyses show promise beyond 10% efficiency.\n\nMany different configurations of OPV exist using different materials for each layer. OPV technology rivals existing PV technologies in terms of EPBT even if they currently present a shorter operational lifetime. A 2013 study analyzed 12 different configurations all with 2% efficiency, the EPBT ranged from 0.29–0.52 years for 1 m² of PV. The average CO-eq/kWh for OPV is 54.922 grams.\n\nThere have been major changes in the underlying costs, industry structure and market prices of solar photovoltaics technology, over the years, and gaining a coherent picture of the shifts occurring across the industry value chain globally is a challenge. This is due to: \"the rapidity of cost and price changes, the complexity of the PV supply chain, which involves a large number of manufacturing processes, the balance of system (BOS) and installation costs associated with complete PV systems, the choice of different distribution channels, and differences between regional markets within which PV is being deployed\". Further complexities result from the many different policy support initiatives that have been put in place to facilitate photovoltaics commercialisation in various countries.\n\nThe PV industry has seen dramatic drops in module prices since 2008. In late 2011, factory-gate prices for crystalline-silicon photovoltaic modules dropped below the $1.00/W mark. The $1.00/W installed cost, is often regarded in the PV industry as marking the achievement of grid parity for PV. Technological advancements, manufacturing process improvements, and industry re-structuring, mean that further price reductions are likely in coming years. As of 2017 power-purchase agreement prices for solar farms below $0.05/kWh are common in the United States and the lowest bids in several international countries were about $0.03/kWh.\n\nFinancial incentives for photovoltaics, such as feed-in tariffs, have often been offered to electricity consumers to install and operate solar-electric generating systems. Government has sometimes also offered incentives in order to encourage the PV industry to achieve the economies of scale needed to compete where the cost of PV-generated electricity is above the cost from the existing grid. Such policies are implemented to promote national or territorial energy independence, high tech job creation and reduction of carbon dioxide emissions which cause climate change. Due to economies of scale solar panels get less costly as people use and buy more—as manufacturers increase production to meet demand, the cost and price is expected to drop in the years to come.\n\nSolar cell efficiencies vary from 6% for amorphous silicon-based solar cells to 44.0% with multiple-junction concentrated photovoltaics. Solar cell energy conversion efficiencies for commercially available photovoltaics are around 14–22%. Concentrated photovoltaics (CPV) may reduce cost by concentrating up to 1,000 suns (through magnifying lens) onto a smaller sized photovoltaic cell. However, such concentrated solar power requires sophisticated heat sink designs, otherwise the photovoltaic cell overheats, which reduces its efficiency and life. To further exacerbate the concentrated cooling design, the heat sink must be passive, otherwise the power required for active cooling would reduce the overall efficiency and economy.\n\nCrystalline silicon solar cell prices have fallen from $76.67/Watt in 1977 to an estimated $0.74/Watt in 2013. This is seen as evidence supporting Swanson's law, an observation similar to the famous Moore's Law that states that solar cell prices fall 20% for every doubling of industry capacity.\n\nAs of 2011, the price of PV modules has fallen by 60% since the summer of 2008, according to Bloomberg New Energy Finance estimates, putting solar power for the first time on a competitive footing with the retail price of electricity in a number of sunny countries; an alternative and consistent price decline figure of 75% from 2007 to 2012 has also been published, though it is unclear whether these figures are specific to the United States or generally global. The levelised cost of electricity (LCOE) from PV is competitive with conventional electricity sources in an expanding list of geographic regions, particularly when the time of generation is included, as electricity is worth more during the day than at night. There has been fierce competition in the supply chain, and further improvements in the levelised cost of energy for solar lie ahead, posing a growing threat to the dominance of fossil fuel generation sources in the next few years. As time progresses, renewable energy technologies generally get cheaper, while fossil fuels generally get more expensive:\n\nAs of 2011, the cost of PV has fallen well below that of nuclear power and is set to fall further. The average retail price of solar cells as monitored by the Solarbuzz group fell from $3.50/watt to $2.43/watt over the course of 2011.\nFor large-scale installations, prices below $1.00/watt were achieved. A module price of 0.60 Euro/watt ($0.78/watt) was published for a large scale 5-year deal in April 2012.\n\nBy the end of 2012, the \"best in class\" module price had dropped to $0.50/watt, and was expected to drop to $0.36/watt by 2017.\n\nIn many locations, PV has reached grid parity, which is usually defined as PV production costs at or below retail electricity prices (though often still above the power station prices for coal or gas-fired generation without their distribution and other costs). However, in many countries there is still a need for more access to capital to develop PV projects. To solve this problem securitization has been proposed and used to accelerate development of solar photovoltaic projects. For example, SolarCity offered, the first U.S. asset-backed security in the solar industry in 2013.\n\nPhotovoltaic power is also generated during a time of day that is close to peak demand (precedes it) in electricity systems with high use of air conditioning. More generally, it is now evident that, given a carbon price of $50/ton, which would raise the price of coal-fired power by 5c/kWh, solar PV will be cost-competitive in most locations. The declining price of PV has been reflected in rapidly growing installations, totaling about 23 GW in 2011. Although some consolidation is likely in 2012, due to support cuts in the large markets of Germany and Italy, strong growth seems likely to continue for the rest of the decade. Already, by one estimate, total investment in renewables for 2011 exceeded investment in carbon-based electricity generation.\n\nIn the case of self consumption payback time is calculated based on how much electricity is not brought from the grid. Additionally, using PV solar power to charge DC batteries, as used in Plug-in Hybrid Electric Vehicles and Electric Vehicles, leads to greater efficiencies. Traditionally, DC generated electricity from solar PV must be converted to AC for buildings, at an average 10% loss during the conversion. An additional efficiency loss occurs in the transition back to DC for battery driven devices and vehicles, and using various interest rates and energy price changes were calculated to find present values that range from $2,057 to $8,213 (analysis from 2009).\n\nFor example, in Germany with electricity prices of 0.25 euro/kWh and Insolation of 900 kWh/kW one kW will save 225 euro per year and with installation cost of 1700 euro/kW means that the system will pay back in less than 7 years.\n\nOverall the\nmanufacturing process of creating solar photovoltaics is simple in that it does\nnot require the culmination of many complex or moving parts. Because of the\nsolid state nature of PV systems they often have relatively long lifetimes,\nanywhere from 10 to 30 years. To increase electrical output of a PV\nsystem, the manufacturer must simply add more photovoltaic components and\nbecause of this economies of scale are important for manufacturers as costs\ndecrease with increasing output.\n\nWhile there are many types of PV systems known to be effective, crystalline silicon PV accounted for around 90% of the worldwide production of PV in 2013. Manufacturing silicon PV systems has several steps. First, polysilicon is processed from mined quartz until it is very pure (semi-conductor grade). This is melted down when small amounts of boron, a group III element, are added to make a p-type semiconductor rich in electron holes. Typically using a seed crystal, an ingot of this solution is grown from the liquid polycrystalline. The ingot may also be cast in a mold. Wafers of this semiconductor material are cut from the bulk material with wire saws, and then go through surface etching before being cleaned. Next, the wafers are placed into a phosphorus vapor deposition furnace which lays a very thin layer of phosphorus, a group V element, which creates an n-type semiconducting surface. To reduce energy losses, an anti-reflective coating is added to the surface, along with electrical contacts. After finishing the cell, cells are connected via electrical circuit according to the specific application and prepared for shipping and installation.\n\nCrystalline silicon photovoltaics are only one type of PV, and while they represent the majority of solar cells produced currently there are many new and promising technologies that have the potential to be scaled up to meet future energy needs. As of 2018, crystalline silicon cell technology serves as the basis for several PV module types, including monocrystalline, multicrystalline, mono PERC, and bifacial. \n\nAnother newer technology, thin-film PV, are manufactured by depositing semiconducting layers on substrate in vacuum. The substrate is often glass or stainless-steel, and these semiconducting layers are made of many types of materials including cadmium telluride (CdTe), copper indium diselenide (CIS), copper indium gallium diselenide (CIGS), and amorphous silicon (a-Si). After being deposited onto the substrate the semiconducting layers are separated and connected by electrical circuit by laser-scribing. Thin-film photovoltaics now make up around 20% of the overall production of PV because of the reduced materials requirements and cost to manufacture modules consisting of thin-films as compared to silicon-based wafers.\n\nOther emerging PV technologies include organic, dye-sensitized, quantum-dot, and Perovskite photovoltaics. OPVs fall into the thin-film category of manufacturing, and typically operate around the 12% efficiency range which is lower than the 12–21% typically seen by silicon based PVs. Because organic photovoltaics require very high purity and are relatively reactive they must be encapsulated which vastly increases cost of manufacturing and meaning that they are not feasible for large scale up. Dye-sensitized PVs are similar in efficiency to OPVs but are significantly easier to manufacture. However these dye-sensitized photovoltaics present storage problems because the liquid electrolyte is toxic and can potentially permeate the plastics used in the cell. Quantum dot solar cells are quantum dot sensitized DSSCs and are solution processed meaning they are potentially scalable, but currently they peak at 12% efficiency. Perovskite solar cells are a very efficient solar energy converter and have excellent optoelectric properties for photovoltaic purposes, but they are expensive and difficult to manufacture.\n\nA photovoltaic system, or solar PV system is a power system designed to supply usable solar power by means of photovoltaics. It consists of an arrangement of several components, including solar panels to absorb and directly convert sunlight into electricity, a solar inverter to change the electric current from DC to AC, as well as mounting, cabling and other electrical accessories. PV systems range from small, roof-top mounted or building-integrated systems with capacities from a few to several tens of kilowatts, to large utility-scale power stations of hundreds of megawatts. Nowadays, most PV systems are grid-connected, while stand-alone systems only account for a small portion of the market.\n\n\n\n\n\n\n\n\n\n\n\n\nThe 122 PW of sunlight reaching the Earth's surface is plentiful—almost 10,000 times more than the 13 TW equivalent of average power consumed in 2005 by humans. This abundance leads to the suggestion that it will not be long before solar energy will become the world's primary energy source. Additionally, solar electric generation has the highest power density (global mean of 170 W/m) among renewable energies.\n\nSolar power is pollution-free during use, which enables it to cut down on pollution when it is substituted for other energy sources. For example, MIT estimated that 52,000 people per year die prematurely in the U.S. from coal-fired power plant pollution and all but one of these deaths could be prevented from using PV to replace coal. Production end-wastes and emissions are manageable using existing pollution controls. End-of-use recycling technologies are under development and policies are being produced that encourage recycling from producers.\n\nPV installations can operate for 100 years or even more with little maintenance or intervention after their initial set-up, so after the initial capital cost of building any solar power plant, operating costs are extremely low compared to existing power technologies.\n\nGrid-connected solar electricity can be used locally thus reducing transmission/distribution losses (transmission losses in the US were approximately 7.2% in 1995).\n\nCompared to fossil and nuclear energy sources, very little research money has been invested in the development of solar cells, so there is considerable room for improvement. Nevertheless, experimental high efficiency solar cells already have efficiencies of over 40% in case of concentrating photovoltaic cells and efficiencies are rapidly rising while mass-production costs are rapidly falling.\n\nIn some states of the United States, much of the investment in a home-mounted system may be lost if the home-owner moves and the buyer puts less value on the system than the seller. The city of Berkeley developed an innovative financing method to remove this limitation, by adding a tax assessment that is transferred with the home to pay for the solar panels. Now known as PACE, Property Assessed Clean Energy, 30 U.S. states have duplicated this solution.\n\nThere is evidence, at least in California, that the presence of a home-mounted solar system can actually increase the value of a home. According to a paper published in April 2011 by the Ernest Orlando Lawrence Berkeley National Laboratory titled An Analysis of the Effects of Residential Photovoltaic Energy Systems on Home Sales Prices in California:\n\nPV has been a well-known method of generating clean, emission free electricity. PV systems are often made of PV modules and inverter (changing DC to AC). PV modules are mainly made of PV cells, which has no fundamental difference to the material for making computer chips. The process of producing PV cells (computer chips) is energy intensive and involves highly poisonous and environmental toxic chemicals. There are few PV manufacturing plants around the world producing PV modules with energy produced from PV. This measure greatly reduces the carbon footprint during the manufacturing process. Managing the chemicals used in the manufacturing process is subject to the factories' local laws and regulations.\n\nWith the increasing levels of rooftop photovoltaic systems, the energy flow becomes 2-way. When there is more local generation than consumption, electricity is exported to the grid. However, electricity network traditionally is not designed to deal with the 2- way energy transfer. Therefore, some technical issues may occur. For example, in Queensland Australia, there have been more than 30% of households with rooftop PV by the end of 2017. The famous Californian 2020 duck curve appears very often for a lot of communities from 2015 onwards. An over-voltage issue may come out as the electricity flows from these PV households back to the network. There are solutions to manage the over voltage issue, such as regulating PV inverter power factor, new voltage and energy control equipment at electricity distributor level, re-conductor the electricity wires, demand side management, etc. There are often limitations and costs related to these solutions.\n\nThere is no silver bullet in electricity or energy demand and bill management, because customers (sites) have different specific situations, e.g. different comfort/convenience needs, different electricity tariffs, or different usage patterns. Electricity tariff may have a few elements, such as daily access and metering charge, energy charge (based on kWh, MWh) or peak demand charge (e.g. a price for the highest 30min energy consumption in a month). PV is a promising option for reducing energy charge when electricity price is reasonably high and continuously increasing, such as in Australia and Germany. However, for sites with peak demand charge in place, PV may be less attractive if peak demands mostly occur in the late afternoon to early evening, for example residential communities. Overall, energy investment is largely an economical decision and it is better to make investment decisions based on systematical evaluation of options in operational improvement, energy efficiency, onsite generation and energy storage.\n\n"}
{"id": "44818203", "url": "https://en.wikipedia.org/wiki?curid=44818203", "title": "Sarpler", "text": "Sarpler\n\nSarpler, Sarplier or (in Scotland) Serplathe was a UK weight for wool.\n\nThe \"Oxford English Dictionary\" defines a sarpler as 80 tods, where a tod is usually 28lbs thus usually 80 x 28 lbs, or 160 stone, = \n\nThis definition is supported by Cowell's 1607 book:\nA different and apparently arithmetically confused definition is given in \"The Life and Works of Arthur Hall of Grantham\", where he states:\n"}
{"id": "27114", "url": "https://en.wikipedia.org/wiki?curid=27114", "title": "Silicon", "text": "Silicon\n\nSilicon is a chemical element with symbol Si and atomic number 14. It is a hard and brittle crystalline solid with a blue-grey metallic lustre; and it is a tetravalent metalloid and semiconductor. It is a member of group 14 in the periodic table: carbon is above it; and germanium, tin, and lead are below it. It is relatively unreactive. Because of its large chemical affinity for oxygen, it was not until 1823 that Jöns Jakob Berzelius was first able to prepare it and characterize it in pure form. Its melting and boiling points of 1414 °C and 3265 °C respectively are the second-highest among all the metalloids and nonmetals, being only surpassed by boron. Silicon is the eighth most common element in the universe by mass, but very rarely occurs as the pure element in the Earth's crust. It is most widely distributed in dusts, sands, planetoids, and planets as various forms of silicon dioxide (silica) or silicates. More than 90% of the Earth's crust is composed of silicate minerals, making silicon the second most abundant element in the Earth's crust (about 28% by mass) after oxygen.\n\nMost silicon is used commercially without being separated, and often with little processing of the natural minerals. Such use includes industrial construction with clays, silica sand, and stone. Silicates are used in Portland cement for mortar and stucco, and mixed with silica sand and gravel to make concrete for walkways, foundations, and roads. They are also used in whiteware ceramics such as porcelain, and in traditional quartz-based soda-lime glass and many other specialty glasses. Silicon compounds such as silicon carbide are used as abrasives and components of high-strength ceramics. Silicon is the basis of the widely used synthetic polymers called silicones.\n\nElemental silicon also has a large impact on the modern world economy. Most free silicon is used in the steel refining, aluminium-casting, and fine chemical industries (often to make fumed silica). Even more visibly, the relatively small portion of very highly purified elemental silicon used in semiconductor electronics (< 10%) is essential to integrated circuits — most computers, cell phones, and modern technology depend on it.\n\nSilicon is an essential element in biology, although only traces are required by animals. However, various sea sponges and microorganisms, such as diatoms and radiolaria, secrete skeletal structures made of silica. Silica is deposited in many plant tissues.\n\nIn 1787 Antoine Lavoisier suspected that silica might be an oxide of a fundamental chemical element, but the chemical affinity of silicon for oxygen is high enough that he had no means to reduce the oxide and isolate the element. After an attempt to isolate silicon in 1808, Sir Humphry Davy proposed the name \"silicium\" for silicon, from the Latin \"silex\", \"silicis\" for flint, and adding the \"-ium\" ending because he believed it to be a metal. Most other languages use transliterated forms of Davy's name, sometimes adapted to local phonology (e.g. German \"Silizium\", Turkish \"silisyum\"). A few others use instead a calque of the Latin root (e.g. Russian \"кремний\", from \"кремень\" \"flint\"; Greek \"πυριτιο\" from \"πυρ\" \"fire\"; Finnish \"pii\" from \"piikivi\" \"flint\").\n\nGay-Lussac and Thénard are thought to have prepared impure amorphous silicon in 1811, through the heating of recently isolated potassium metal with silicon tetrafluoride, but they did not purify and characterize the product, nor identify it as a new element. Silicon was given its present name in 1817 by Scottish chemist Thomas Thomson. He retained part of Davy's name but added \"-on\" because he believed that silicon was a nonmetal similar to boron and carbon. In 1823, Jöns Jacob Berzelius prepared amorphous silicon using approximately the same method as Gay-Lussac (reducing potassium fluorosilicate with molten potassium metal), but purifying the product to a brown powder by repeatedly washing it. As a result, he is usually given credit for the element's discovery. The same year, Berzelius became the first to prepare silicon tetrachloride; silicon tetrafluoride had already been prepared long before in 1771 by Carl Wilhelm Scheele by dissolving silica in hydrofluoric acid.\n\nSilicon in its more common crystalline form was not prepared until 31 years later, by Deville. By electrolyzing a mixture of sodium chloride and aluminium chloride containing approximately 10% silicon, he was able to obtain a slightly impure allotrope of silicon in 1854. Later, more cost-effective methods have been developed to isolate several allotrope forms, the most recent being silicene in 2010. Meanwhile, research on the chemistry of silicon continued; Friedrich Wöhler discovered the first volatile hydrides of silicon, synthesising trichlorosilane in 1857 and silane itself in 1858, but a detailed investigation of the silanes was only carried out in the early 20th century by Alfred Stock, despite early speculation on the matter dating as far back as the beginnings of synthetic organic chemistry in the 1830s. Similarly, the first organosilicon compound, tetraethylsilane, was synthesised by Charles Friedel and James Crafts in 1863, but detailed characterisation of organosilicon chemistry was only done in the early 20th century by Frederic Kipping.\n\nStarting in the 1920s, the work of William Lawrence Bragg on X-ray crystallography successfully elucidated the compositions of the silicates, which had previously been known from analytical chemistry but had not yet been understood, together with Linus Pauling's development of crystal chemistry and Victor Goldschmidt's development of geochemistry. The middle of the 20th century saw the development of the chemistry and industrial use of siloxanes and the growing use of silicone polymers, elastomers, and resins. In the late 20th century, the complexity of the crystal chemistry of silicides was mapped, along with the solid-state chemistry of doped semiconductors.\n\nBecause silicon is an important element in high-technology semiconductor devices, many places in the world bear its name. For example, Santa Clara Valley in California acquired the nickname Silicon Valley, as the element is the base material in the semiconductor industry there. Since then, many other places have been dubbed similarly, including Silicon Forest in Oregon, Silicon Hills in Austin, Texas, Silicon Slopes in Salt Lake City, Utah, Silicon Saxony in Germany, Silicon Valley in India, Silicon Border in Mexicali, Mexico, Silicon Fen in Cambridge, England, Silicon Roundabout in London, Silicon Glen in Scotland, and Silicon Gorge in Bristol, England.\n\nA silicon atom has fourteen electrons. In the ground state, they are arranged in the electron configuration [Ne]3s3p. Of these, four are valence electrons, occupying the 3s orbital and two of the 3p orbitals. Like the other members of its group, the lighter carbon and the heavier germanium, tin, and lead, it has the same number of valence electrons as valence orbitals: hence, it can complete its octet and obtain the stable noble gas configuration of argon by forming sp hybrid orbitals, forming tetrahedral SiX derivatives where the central silicon atom shares an electron pair with each of the four atoms it is bonded to. The first four ionisation energies of silicon are 786.3, 1576.5, 3228.3, and 4354.4 kJ/mol respectively; these figures are high enough to preclude the possibility of simple cationic chemistry for the element. Following periodic trends, its single-bond covalent radius of 117.6 pm is intermediate between those of carbon (77.2 pm) and germanium (122.3 pm). The hexacoordinate ionic radius of silicon may be considered to be 40 pm, although this must be taken as a purely notional figure given the lack of a simple Si cation in reality.\n\nAt standard temperature and pressure, silicon is a shiny semiconductor with a bluish-grey metallic lustre; as typical for semiconductors, its resistivity drops as temperature rises. This arises because silicon has a small energy gap between its highest occupied energy levels (the valence band) and the lowest unoccupied ones (the conduction band). The Fermi level is about halfway between the valence and conduction bands and is the energy at which a state is as likely to be occupied by an electron as not. Hence pure silicon is an insulator at room temperature. However, doping silicon with a pnictogen such as phosphorus, arsenic, or antimony introduces one extra electron per dopant and these may then be excited into the conduction band either thermally or photolytically, creating an n-type semiconductor. Similarly, doping silicon with a group 13 element such as boron, aluminium, or gallium results in the introduction of acceptor levels that trap electrons that may be excited from the filled valence band, creating a p-type semiconductor. Joining n-type silicon to p-type silicon creates a p-n junction with a common Fermi level; electrons flow from n to p, while holes flow from p to n, creating a voltage drop. This p-n junction thus acts as a diode that can rectify alternating current that allows current to pass more easily one way than the other. A transistor is an n-p-n junction, with a thin layer of weakly p-type silicon between two n-type regions. Biasing the emitter through a small forward voltage and the collector through a large reverse voltage allows the transistor to act as a triode amplifier.\n\nSilicon crystallises in a giant covalent structure at standard conditions, specifically in a diamond cubic lattice. It thus has a high melting point of 1414 °C, as a lot of energy is required to break the strong covalent bonds and melt the solid. It is not known to have any allotropes at standard pressure, but several other crystal structures are known at higher pressures. The general trend is one of increasing coordination number with pressure, culminating in a hexagonal close-packed allotrope at about 40 gigapascals known as Si–VII (the standard modification being Si–I). Silicon boils at 3265 °C: this, while high, is still lower than the temperature at which its lighter congener carbon sublimes (3642 °C) and silicon similarly has a lower heat of vaporisation than carbon, consistent with the fact that the Si–Si bond is weaker than the C–C bond.\n\nNaturally occurring silicon is composed of three stable isotopes, Si (92.23%), Si (4.67%), and Si (3.10%). Out of these, only Si is of use in NMR and EPR spectroscopy, as it is the only one with a nuclear spin (\"I\" = ). All three are produced in stars through the oxygen-burning process, with Si being made as part of the alpha process and hence the most abundant. The fusion of Si with alpha particles by photodisintegration rearrangement in stars is known as the silicon-burning process; it is the last stage of stellar nucleosynthesis before the rapid collapse and violent explosion of the star in question in a type II supernova.\n\nTwenty radioisotopes have been characterized, the two stablest being Si with a half-life of about 150 years, and Si with a half-life of 2.62 hours. All the remaining radioactive isotopes have half-lives that are less than seven seconds, and the majority of these have half-lives that are less than one tenth of a second. Silicon does not have any known nuclear isomers. Si undergoes low-energy beta decay to P and then stable S. Si may be produced by the neutron activation of natural silicon and is thus useful for quantitative analysis; it can be easily detected by its characteristic beta decay to stable P, in which the emitted electron carries up to 1.48 MeV of energy.\n\nThe known isotopes of silicon range in mass number from 22 to 44. The most common decay mode of the isotopes with mass numbers lower than the three stable isotopes is inverse beta decay, primarily forming aluminium isotopes (13 protons) as decay products. The most common decay mode for the heavier unstable isotopes is beta decay, primarily forming phosphorus isotopes (15 protons) as decay products.\n\nCrystalline bulk silicon is rather inert, but becomes more reactive at high temperatures. Like its neighbour aluminium, silicon forms a thin, continuous surface layer of silicon dioxide (SiO) that protects the metal from oxidation. Thus silicon does not measurably react with the air below 900 °C, but formation of the vitreous dioxide rapidly increases between 950 °C and 1160 °C and when 1400 °C is reached, atmospheric nitrogen also reacts to give the nitrides SiN and SiN. Silicon reacts with gaseous sulfur at 600 °C and gaseous phosphorus at 1000 °C. This oxide layer nevertheless does not prevent reaction with the halogens; fluorine attacks silicon vigorously at room temperature, chlorine does so at about 300 °C, and bromine and iodine at about 500 °C. Silicon does not react with most aqueous acids, but is oxidised and fluorinated by a mixture of concentrated nitric acid and hydrofluoric acid; it readily dissolves in hot aqueous alkali to form silicates. At high temperatures, silicon also reacts with alkyl halides; this reaction may be catalysed by copper to directly synthesise organosilicon chlorides as precursors to silicone polymers. Upon melting, silicon becomes extremely reactive, alloying with most metals to form silicides, and reducing most metal oxides because the heat of formation of silicon dioxide is so large. As a result, containers for liquid silicon must be made of refractory, unreactive materials such as zirconium dioxide or group 4, 5, and 6 borides.\n\nTetrahedral coordination is a major structural motif in silicon chemistry just as it is for carbon chemistry. However, the 3p subshell is rather more diffuse than the 2p subshell and does not hybridise so well with the 3s subshell. As a result, the chemistry of silicon and its heavier congeners shows significant differences from that of carbon, and thus octahedral coordination is also significant. For example, the electronegativity of silicon (1.90) is much less than that of carbon (2.55), because the valence electrons of silicon are further from the nucleus than those of carbon and hence experience smaller electrostatic forces of attraction from the nucleus. The poor overlap of 3p orbitals also results in a much lower tendency toward catenation (formation of Si–Si bonds) for silicon than for carbon, due to the concomitant weakening of the Si–Si bond compared to the C–C bond: the average Si–Si bond energy is approximately 226 kJ/mol, compared to a value of 356 kJ/mol for the C–C bond. This results in multiply bonded silicon compounds generally being much less stable than their carbon counterparts, an example of the double bond rule. On the other hand, the presence of 3d orbitals in the valence shell of silicon suggests the possibility of hypervalence, as seen in five and six-coordinate derivatives of silicon such as and . Lastly, because of the increasing energy gap between the valence s and p orbitals as the group is descended, the divalent state grows in importance from carbon to lead, so that a few unstable divalent compounds are known for silicon; this lowering of the main oxidation state, in tandem with increasing atomic radii, results in an increase of metallic character down the group. Silicon already shows some incipient metallic behavior, particularly in the behavior of its oxide compounds and its reaction with acids as well as bases (though this takes some effort), and is hence often referred to as a metalloid rather than a nonmetal. However, metallicity does not become clear in group 14 until germanium and dominant until tin, with the growing importance of the lower +2 oxidation state.\n\nSilicon shows clear differences from carbon. For example, organic chemistry has very few analogies with silicon chemistry, while silicate minerals have a structural complexity unseen in oxocarbons. Silicon tends to resemble germanium far more than it does carbon, and this resemblance is enhanced by the d-block contraction, resulting in the size of the germanium atom being much closer to that of the silicon atom than periodic trends would predict. Nevertheless, there are still some differences because of the growing importance of the divalent state in germanium compared to silicon, which result in germanium being significantly more metallic than silicon. Additionally, the lower Ge–O bond strength compared to the Si–O bond strength results in the absence of \"germanone\" polymers that would be analogous to silicone polymers.\n\nMany metal silicides are known, most of which have formulae that cannot be explained through simple appeals to valence: their bonding ranges from metallic to ionic and covalent. Some known stoichiometries are MSi, MSi, MSi, MSi, MSi, MSi, MSi, MSi, MSi, MSi, MSi, MSi, MSi, and MSi. They are structurally more similar to the borides than the carbides, in keeping with the diagonal relationship between boron and silicon, although the larger size of silicon than boron means that exact structural analogies are few and far between. The heats of formation of the silicides are usually similar to those of the borides and carbides of the same elements, but they usually melt at lower temperatures. Silicides are known for all stable elements in groups 1–10, with the exception of beryllium: in particular, uranium and the transition metals of groups 4–10 show the widest range of stoichiometries. Except for copper, the metals in groups 11–15 do not form silicides. Instead, most form eutectic mixtures, although the heaviest post-transition metals mercury, thallium, lead, and bismuth are completely immiscible with liquid silicon.\n\nUsually, silicides are prepared by direct reaction of the elements. For example, the alkali metals and alkaline earth metals react with silicon or silicon oxide to give silicides. Nevertheless, even with these highly electropositive elements true silicon anions are not obtainable, and most of these compounds are semiconductors. For example, the alkali metal silicides contain pyramidal tricoordinate silicon in the anion, isoeelctronic with white phosphorus, P. Metal-rich silicides tend to have isolated silicon atoms (e. g. CuSi); with increasing silicon content, catenation increases, resulting in isolated clusters of two (e. g. USi) or four silicon atoms (e. g. [K][Si]) at first, followed by chains (e. g. CaSi), layers (e. g. CaSi), or three-dimensional networks of silicon atoms spanning space (e. g. α-ThSi) as the silicon content rises even higher.\n\nThe silicides of the group 1 and 2 metals usually are more reactive than the transition metal silicides. The latter usually do not react with aqueous reagents, except for hydrofluoric acid; however, they do react with much more aggressive reagents such as liquid potassium hydroxide, or gaseous fluorine or chlorine when red-hot. The pre-transition metal silicides instead readily react with water and aqueous acids, usually producing hydrogen or silanes:\nProducts often vary with the stoichiometry of the silicide reactant. For example, CaSi is polar and non-conducting and has the anti-PbCl structure with single isolated silicon atoms, and reacts with water to produce calcium hydroxide, hydrated silicon dioxide, and hydrogen gas. CaSi with its zigzag chains of silicon atoms instead reacts to give silanes and polymeric SiH, while CaSi with its puckered layers of silicon atoms does not react with water, but will react with dilute hydrochloric acid: the product is a yellow polymeric solid with stoichiometry SiHO.\n\nSpeculation on silicon hydride chemistry started in the 1830s, contemporary with the development of synthetic organic chemistry. Silane itself, as well as trichlorosilane, were first synthesised by Friedrich Wöhler and Heinrich Buff in 1857 by reacting aluminium–silicon alloys with hydrochloric acid, and characterised as SiH and SiHCl by Charles Friedel and Albert Ladenburg in 1867. Disilane (SiH) followed in 1902, when it was first made by Henri Moissan and Samuel Smiles by the protonolysis of magnesium silicides. Further investigation had to wait until 1916 because of the great reactivity and thermal instability of the silanes; it was then that Alfred Stock began to study silicon hydrides in earnest with new greaseless vacuum techniques, as they were found as contaminants of his focus, the boron hydrides. The names \"silanes\" and \"boranes\" are his, based on analogy with the alkanes. The Moissan and Smiles method of preparation of silanes and silane derivatives via protonolysis of metal silicides is still used, although the yield is lowered by the hydrolysis of the products that occurs simultaneously, so that the preferred route today is to treat substituted silanes with hydride reducing agents such as lithium aluminium hydride in etheric solutions at low temperatures. Direct reaction of HX or RX with silicon, possibly with a catalyst such as copper, is also a viable method of producing substituted silanes.\n\nThe silanes comprise a homologous series of silicon hydrides with a general formula of SiH. They are all strong reducing agents. Unbranched and branched trains are known up to \"n\"=8, and the cycles SiH and SiH are also known. The first two, silane and disilane, are colourless gases; the heavier members of the series are volatile liquids. All silanes are very reactive and catch fire or explode spontaneously in air. They become less thermally stable with room temperature, so that only silane is indefinitely stable at room temperature, although disilane does not decompose very quickly (only 2.5% of a sample decomposes after the passage of eight months). They decompose to form polymeric polysilicon hydride and hydrogen gas. As expected from the difference in atomic weight, the silanes are less volatile than the corresponding alkanes and boranes, but more so than the corresponding germanes. They are much more reactive than the corresponding alkanes, because of the larger radius of silicon compared to carbon facilitating nucleophilic attack at the silicon, the greater polarity of the Si–H bond compared to the C–H bond, and the ability of silicon to expand its octet and hence form adducts and lower the reaction's activation energy.\n\nSilane pyrolysis gives polymeric species and finally elemental silicon and hydrogen; indeed ultrapure silicon is commercially produced by the pyrolysis of silane. While the thermal decomposition of alkanes starts by the breaking of a C–H or C–C bond and the formation of radical intermediates, polysilanes decompose by eliminating silylenes :SiH or :SiHR, as the activation energy of this process (~210 kJ/mol) is much less than the Si–Si and Si–H bond energies. While pure silanes do not react with pure water or dilute acids, traces of alkali catalyse immediate hydrolysis to hydrated silicon dioxide. If the reaction is carried out in methanol, controlled solvolysis results in the products SiH(OMe), SiH(OMe), and Si(OMe). The Si–H bond also adds to alkenes, a reaction which proceeds slowly and speeds up with increasing substitution of the silane involved. At 450 °C, silane participates in an addition reaction with acetone, as well as a ring-opening reaction with ethylene oxide. Direct reaction of the silanes with chlorine or bromine results in explosions at room temperature, but the reaction of silane with bromine at −80 °C is controlled and yields bromosilane and dibromosilane. The monohalosilanes may be formed by reacting silane with the appropriate hydrogen halide with an AlX catalyst, or by reacting silane with a solid silver halide in a heated flow reactor:\nAmong the derivatives of silane, iodosilane (SiHI) and potassium silanide (KSiH) are very useful synthetic intermediates in the production of more complicated silicon-containing compounds: the latter is a colourless crystalline ionic solid containing K cations and anions in the NaCl structure, and is made by the reduction of silane by potassium metal. Additionally, the reactive hypervalent species is also known. With suitable organic substituents it is possible to produce stable polysilanes: they have surprisingly high electric conductivities, arising from sigma delocalisation of the electrons in the chain.\n\nSilicon and silicon carbide readily react with all four stable halogens, forming the colourless, reactive, and volatile silicon tetrahalides. Silicon tetrafluoride also may be made by fluorinating the other silicon halides, and is produced by the attack of hydrofluoric acid on glass. Heating two different tetrahalides together also produce a random mixture of mixed halides, which may also be produced by halogen exchange reactions. The melting and boiling points of these species usually rise with increasing atomic weight, though there are many exceptions: for example, the melting and boiling points drop as one passes from SiFBr through SiFClBr to SiFClBr. The shift from the hypoelectronic elements in group 13 and earlier to the group 14 elements is illustrated by the change from an infinite ionic structure in aluminium fluoride to a lattice of simple covalent silicon tetrafluoride molecules, as dictated by the lower electronegativity of aluminium than silicon, the stoichiometry (the +4 oxidation state being too high for true ionicity), and the smaller size of the silicon atom compared to the aluminium atom. Silicon tetrachloride is manufactured on a huge scale as a precursor to the production of pure silicon, silicon dioxide, and some silicon esters. The silicon tetrahalides hydrolyse readily in water, unlike the carbon tetrahalides, again because of the larger size of the silicon atom rendering it more open to nucleophilic attack and the ability of the silicon atom to expand its octet which carbon lacks. The reaction of silicon fluoride with excess hydrofluoric acid produces the octahedral hexafluorosilicate anion .\n\nAnalogous to the silanes, halopolysilanes SiX also are known. While catenation in carbon compounds is maximised in the hydrogen compounds rather than the halides, the opposite is true for silicon, so that the halopolysilanes are known up to at least SiF, SiCl, and SiBr. A suggested explanation for this phenomenon is the compensation for the electron loss of silicon to the more electronegative halogen atoms by pi backbonding from the filled p orbitals on the halogen atoms to the empty d orbitals on silicon: this is similar to the situation of carbon monoxide in metal carbonyl complexes and explains their stability. These halopolysilanes may be produced by comproportionation of silicon tetrahalides with elemental silicon, or by condensation of lighter halopolysilanes (trimethylammonium being a useful catalyst for this reaction).\n\nSilicon dioxide (SiO), also known as silica, is one of the best-studied compounds, second only to water. Twelve different crystal modifications of silica are known, the most common being α-quartz, a major constituent of many rocks such as granite and sandstone. It also is known to occur in a pure form as rock crystal; impure forms are known as rose quartz, smoky quartz, morion, amethyst, and citrine. Some poorly crystalline forms of quartz are also known, such as chalcedony, chrysoprase, carnelian, agate, onyx, jasper, heliotrope, and flint. Other modifications of silicon dioxide are known in some other minerals such as tridymite and cristobalite, as well as the much less common coesite and stishovite. Biologically generated forms are also known as kieselguhr and diatomaceous earth. Vitreous silicon dioxide is known as tektites, and obsidian, and rarely as lechatelierite. Some synthetic forms are known as keatite and W-silica. Opals are composed of complicated crystalline aggregates of partially hydrated silicon dioxide.\n\nMost crystalline forms of silica are made of infinite arrangements of {SiO} tetrahedra (with Si at the center) connected at their corners, with each oxygen atom linked to two silicon atoms. In the thermodynamically stable room-temperature form, α-quartz, these tetrahedra are linked in intertwined helical chains with two different Si–O distances (159.7 and 161.7 pm) with a Si–O–Si angle of 144°. These helices can be either left- or right-handed, so that individual α-quartz crystals are optically active. At 537 °C, this transforms quickly and reversibly into the similar β-quartz, with a change of the Si–O–Si angle to 155° but a retention of handedness. Further heating to 867 °C results in another reversible phase transition to β-tridymite, in which some Si–O bonds are broken to allow for the arrangement of the {SiO} tetrahedra into a more open and less dense hexagonal structure. This transition is slow and hence tridymite occurs as a metastable mineral even below this transition temperature; when cooled to about 120 °C it quickly and reversibly transforms by slight displacements of individual silicon and oxygen atoms to α-tridymite, similarly to the transition from α-quartz to β-quartz. β-tridymite slowly transforms to cubic β-cristobalite at about 1470 °C, which once again exists metastably below this transition temperature and transforms at 200–280 °C to α-cristobalite via small atomic displacements. β-cristobalite melts at 1713 °C; the freezing of silica from the melt is quite slow and vitrification, or the formation of a glass, is likely to occur instead. In vitreous silica, the {SiO} tetrahedra remain corner-connected, but the symmetry and periodicity of the crystalline forms are lost. Because of the slow conversions between these three forms, it is possible upon rapid heating to melt β-quartz (1550 °C) or β-tridymite (1703 °C). Silica boils at approximately 2800 °C. Other high-pressure forms of silica are known, such as coesite and stishovite: these are known in nature, formed under the shock pressure of a meteorite impact and then rapidly quenched to preserve the crystal structure. Similar melting and cooling of silica occurs following lightning strikes, forming glassy lechatelierite. W-silica is an unstable low-density form involving {SiO} tetrahedra sharing opposite edges instead of corners, forming parallel chains similarly to silicon disulfide (SiS) and silicon diselenide (SiSe): it quickly returns to forming amorphous silica with heat or traces of water.\nSilica is rather inert chemically. It is not attacked by any acids other than hydrofluoric acid. However, it slowly dissolves in hot concentrated alkalis, and does so rather quickly in fused metal hydroxides or carbonates, to give metal silicates. Among the elements, it is attacked only by fluorine at room temperature to form silicon tetrafluoride: hydrogen and carbon also react, but require temperatures over 1000 °C to do so. Silica nevertheless reacts with many metal and metalloid oxides to form a wide variety of compounds important in the glass and ceramic industries above all, but also have many other uses: for example, sodium silicate is often used in detergents due to its buffering, saponifying, and emulsifying properties.\n\nAdding water to silica drops its melting point by around 800 °C due to the breaking of the structure by replacing Si–O–Si linkages with terminating Si–OH groups. Increasing water concentration results in the formation of hydrated silica gels and colloidal silica dispersions. Many hydrates and silicic acids exist in the most dilute of aqueous solutions, but these are rather insoluble and quickly precipitate and condense and cross-link to form various polysilicic acids of variable combinations following the formula [SiO(OH)], similar to the behaviour of boron, aluminium, and iron, among other elements. Hence, although some simple silicic acids have been identified in dilute solutions, such as orthosilicic acid Si(OH) and metasilicic acid SiO(OH), none of these are likely to exist in the solid state.\n\nAbout 95% of the Earth's crustal rocks are made of silica or silicate and aluminosilicate minerals, as reflected in oxygen, silicon, and aluminium being the three most common elements in the crust (in that order). Measured by mass, silicon makes up 27.7% of the Earth's crust. Pure silicon crystals are very rarely found in nature, but notable exceptions are crystals as large as to 0.3 mm across found during sampling gases from the Kudriavy volcano on Iturup, one of the Kuril Islands.\n\nSilicate and aluminosilicate minerals have many different structures and varying stoichiometry, but they may be classified following some general principles. Tetrahedral {SiO} units are common to almost all these compounds, either as discrete structures, or combined into larger units by the sharing of corner oxygen atoms. These may be divided into \"neso\"-silicates (discrete {SiO} units) sharing no oxygen atoms, \"soro\"-silicates (discrete {SiO} units) sharing one, \"cyclo\"-silicates (closed ring structures) and \"ino\"-silicates (continuous chain or ribbon structures) both sharing two, \"phyllo\"-silicates (continuous sheets) sharing three, and \"tecto\"-silicates (continuous three-dimensional frameworks) sharing four. The lattice of oxygen atoms that results is usually close-packed, or close to it, with the charge being balanced by other cations in various different polyhedral sites according to size.\n\nThe orthosilicates MSiO (M = Be, Mg, Mn, Fe, Zn) and ZrSiO are \"neso\"-silicates. BeSiO (phenacite) is unusual as both Be and Si occupy tetrahedral four-coordinated sites; the other divalent cations instead occupy six-coordinated octahedral sites and often isomorphously replace each other as in olivine, (Mg,Fe,Mn)SiO. Zircon, ZrSiO, demands eight-coordination of the Zr cations due to stoichiometry and because of their larger ionic radius (84 pm). Also significant are the garnets, [MM(SiO)], in which the divalent cations (e.g. Ca, Mg, Fe) are eight-coordinated and the trivalent ones are six-coordinated (e.g. Al, Cr, Fe). Regular coordination is not always present: for example, it is not found in CaSiO, which mixes six- and eight-coordinate sites for Ca. \"Soro\"-silicates, involving discrete double or triple tetrahedral units, are quite rare: metasilicates involving cyclic \"[(SiO)]\" units of corner-abutting tetrahedra forming a polygonal ring are also known.\n\nChain metasilicates, {SiO}, form by corner-sharing of an indefinite chain of linked {SiO} tetrahedra. Many differences arise due to the differing repeat distances of conformation across the line of tetrahedra. A repeat distance of two is most common, as in most pyroxene minerals, but repeat distances of one, three, four, five, six, seven, nine, and twelve are also known. These chains may then link across each other to form double chains and ribbons, as in the asbestos minerals, involving repeated chains of cyclic tetrahedron rings.\nLayer silicates, such as the clay minerals and the micas, are very common, and often are formed by horizontal cross-linking of metasilicate chains or planar condensation of smaller units. An example is kaolinite [Al(OH)SiO]; in many of these minerals cation and anion replacement is common, so that for example tetrahedral Si may be replaced by Al, octahedral Al by Mg, and OH by F. Three-dimensional framework aluminosilicates are structurally very complex; they may be conceived of as starting from the SiO structure, but having replaced up to one-half of the Si atoms with Al, they require more cations to be included in the structure to balance charge. Examples include feldspars (the most abundant minerals on the Earth), zeolites, and ultramarines. Many feldspars can be thought of as forming part of the ternary system NaAlSiO–KAlSiO–CaAlSiO. Their lattice is destroyed by high pressure prompting Al to undergo six-coordination rather than four-coordination, and this reaction destroying feldspars may be a reason for the Mohorovičić discontinuity, which would imply that the crust and mantle have the same chemical composition, but different lattices, although this is not a universally held view. Zeolites have many polyhedral cavities in their frameworks (truncated cuboctahedra being most common, but other polyhedra also are known as zeolite cavities), allowing them to include loosely bound molecules such as water in their structure. Ultramarines alternate silicon and aluminium atoms and include a variety of other anions such as Cl, , and , but are otherwise similar to the feldspars.\n\nSilicon disulfide (SiS) is formed by burning silicon in gaseous sulfur at 100 °C; sublimation of the resulting compound in nitrogen results in white, flexible long fibers reminiscent of asbestos with a structure similar to W-silica. This melts at 1090 °C and sublimes at 1250 °C; at high temperature and pressure this transforms to a crystal structure analogous to cristobalite. However, SiS lacks the variety of structures of SiO, and quickly hydrolyses to silica and hydrogen sulfide. It is also ammonoloysed quickly and completely by liquid ammonia as follows to form an imide:\nIt reacts with the sulfides of sodium, magnesium, aluminium, and iron to form metal thiosilicates: reaction with ethanol results in tetraethylsilicate Si(OEt) and hydrogen sulfide. Ethylsilicate is useful as its controlled hydrolysis produces adhesive or film-like forms of silica. Reacting hydrogen sulfide with silicon tetrahalides yields silicon thiohalides such as S(SiCl), cyclic ClSi(μ-S)SiCl, and crystalline (SiSCl). Despite the double bond rule, stable organosilanethiones RR'Si=S have been made thanks to the stabilising mechanism of intermolecular coordination via an amine group.\n\nSilicon nitride, SiN, may be formed by directly reacting silicon with nitrogen above 1300 °C, but a more economical means of production is by heating silica and coke in a stream of nitrogen and hydrogen gas at 1500 °C. It would make a promising ceramic if not for the difficulty of working with and sintering it: chemically, it is near-totally inert, and even above 1000 °C it keeps its strength, shape, and continues to be resistant to wear and corrosion. It is very hard (9 on the Mohs hardness scale), dissociates only at 1900 °C at 1 atm, and is quite dense (density 3.185 g/cm), because of its compact structure similar to that of phenacite (BeSiO). A similar refractory material is SiNO, formed by heating silicon and silica at 1450 °C in an argon stream containing 5% nitrogen gas, involving 4-coordinate silicon and 3-coordinate nitrogen alternating in puckered hexagonal tilings interlinked by non-linear Si–O–Si linkages to each other.\n\nReacting silyl halides with ammonia or alkylammonia derivatives in the gaseous phase or in ethanolic solution produces various volatile silylamides, which are silicon analogues of the amines:\n\nMany such compounds have been prepared, the only known restriction being that the nitrogen is always tertiary, and species containing the SiH–NH group are unstable at room temperature. The stoichiometry around the nitrogen atom in compounds such as N(SiH)is planar, which has been attributed to a p–d interaction between a lone pair on nitrogen and an empty d orbital on silicon. Similarly, trisilylamines are weaker as ligands than their carbon analogues, the tertiary amines, although substitution of some SiH groups by CH groups mitigates this weakness. For example, N(SiH) does not form an adduct with BH at all, while MeN(SiH) and MeNSiH form adducts at low temperatures that decompose upon warming. Some silicon analogues of imines, with a Si=N double bond, are known: the first found was BuSi=N–SiBu, which was discovered in 1986.\nSilicon carbide (SiC) was first made by Edward Goodrich Acheson in 1891, who named it carborundum to reference its intermediate hardness and abrasive power between diamond (an allotrope of carbon) and corundum (aluminium oxide). He soon founded a company to manufacture it, and today about one million tonnes are produced each year. Silicon carbide exists in about 250 crystalline forms. The polymorphism of SiC is characterized by a large family of similar crystalline structures called polytypes. They are variations of the same chemical compound that are identical in two dimensions and differ in the third. Thus they can be viewed as layers stacked in a certain sequence. It is made industrially by reduction of quartz sand with excess coke or anthracite at 2000–2500 °C in an electric furnace:\n\nIt is the most thermally stable binary silicon compound, only decomposing through loss of silicon starting from around 2700 °C. It is resistant to most aqueous acids, phosphoric acid being an exception. It forms a protective layer of silicon dioxide on the surface and hence only oxidises appreciably in air above 1000 °C; removal of this layer by molten hydroxides or carbonates leads to quick oxidation. Silicon carbide is rapidly attacked by chlorine gas, which forms SiCl and carbon at 100 °C and SiCl and CCl at 1000 °C. It is mostly used as an abrasive and a refractory material, as it is chemically stable and very strong, and it fractures to form a very sharp cutting edge. It is also useful as an intrinsic semiconductor, as well as an extrinsic semiconductor upon being doped. In its diamond-like behavior it serves as an illustration of the chemical similarity between carbon and silicon.\n\nBecause the Si–C bond is close in strength to the C–C bond, organosilicon compounds tend to be markedly thermally and chemically stable. For example, tetraphenylsilane (SiPh) may be distilled in air even at its boiling point of 428 °C, and so may its substituted derivatives PhSiCl and PhSiCl, which boil at 378 °C and 305 °C respectively. Furthermore, since carbon and silicon are chemical congeners, organosilicon chemistry shows some significant similarities with carbon chemistry, for example in the propensity of such compounds for catenation and forming multiple bonds. However, significant differences also arise: since silicon is more electropositive than carbon, bonds to more electronegative elements are generally stronger with silicon than with carbon, and vice versa. Thus the Si–F bond is significantly stronger than even the C–F bond and is one of the strongest single bonds, while the Si–H bond is much weaker than the C–H bond and is readily broken. Furthermore, the ability of silicon to expand its octet is not shared by carbon, and hence some organosilicon reactions have no organic analogues. For example, nucleophilic attack on silicon does not proceed by the S2 or S1 processes, but instead goes through a negatively charged true pentacoordinate intermediate and appears like a substitution at a hindered tertiary atom. This works for silicon, unlike for carbon, because the long Si–C bonds reduce the steric hindrance and the d-orbital of silicon is geometrically unconstrained for nucleophilic attack, unlike for example a C–O σ* antibonding orbital. Nevertheless, despite these differences, the mechanism is still often called \"S2 at silicon\" for simplicity.\n\nOne of the most useful silicon-containing groups is trimethylsilyl, MeSi–. The Si–C bond connecting it to the rest of the molecule is reasonably strong, allowing it to remain while the rest of the molecule undergoes reactions, but is not so strong that it cannot be removed specifically when needed, for example by the fluoride ion, which is a very weak nucleophile for carbon compounds but a very strong one for organosilicon compounds. It may be compared to acidic protons; while trisilylmethyl is removed by hard nucleophiles instead of bases, both removals usually promote elimination. As a general rule, while saturated carbon is best attacked by nucleophiles that are neutral compounds, those based on nonmetals far down on the periodic table (e.g. sulfur, selenium, or iodine), or even both, silicon is best attacked by charged nucleophiles, particularly those involving such highly electronegative nonmetals as oxygen, fluorine, or chlorine. For example, enolates react at the carbon in haloalkanes, but at the oxygen in silyl chlorides; and when trimethylsilyl is removed from an organic molecule using hydroxide as a nucleophile, the product of the reaction is not the silanol as one would expect from using carbon chemistry as an analogy, because the siloxide is strongly nucleophilic and attacks the original molecule to yield the silyl ether hexamethyldisiloxane, (MeSi)O. Conversely, while the S2 reaction is mostly unaffected by the presence of a partial positive charge (δ+) at the carbon, the analogous \"S2\" reaction at silicon is so affected. Thus, for example, the silyl triflates are so electrophilic that they react 10 to 10 times faster than silyl chlorides with oxygen-containing nucleophiles. Trimethylsilyl triflate is in particular a very good Lewis acid and is used to convert carbonyl compounds to acetals and silyl enol ethers, reacting them together analogously to the aldol reaction.\n\nSi–C bonds are commonly formed in three ways. In the laboratory, preparation is often carried out in small quantities by reacting tetrachlorosilane with organolithium, Grignard, or organoaluminium reagents, or by catalytic addition of Si–H across C=C double bonds. The second route has the drawback of not being applicable to the most important silanes, the methyl and phenyl silanes. Organosilanes are made industrially by directly reacting alkyl or aryl halides with silicon with 10% by weight metallic copper as a catalyst. Standard organic reactions suffice to produce many derivatives; the resulting organosilanes are often significantly more reactive than their carbon congeners, readily undergoing hydrolysis, ammonolysis, alcoholysis, and condensation to form cyclic oligomers or linear polymers.\n\nThe word \"silicone\" was first used by Frederic Kipping in 1901. He invented the word to illustrate the similarity of chemical formulae between PhSiO and benzophenone, PhCO, although he also stressed the lack of chemical resemblance due to the polymeric structure of PhSiO, which is not shared by PhCO.\n\nSilicones may be considered analogous to mineral silicates, in which the methyl groups of the silicones correspond to the isoelectronic O of the silicates. They are quite stable to extreme temperatures, oxidation, and water, and have useful dielectric, antistick, and antifoam properties. Furthermore, they are resistant over long periods of time to ultraviolet radiation and weathering, and are inert physiologically. They are fairly unreactive, but do react with concentrated solutions bearing the hydroxide ion and fluorinating agents, and occasionally, may even be used as mild reagents for selective syntheses. For example, (MeSi)O is valuable for the preparation of derivatives of molybdenum and tungsten oxyhalides, converting a tungsten hexachloride suspension in dichloroethane solution quantitatively to WOCl in under an hour at room temperature, and then to yellow WOCl at 100 °C in light petroleum at a yield of 95% overnight.\n\nIn the universe, silicon is the seventh most abundant element, coming after hydrogen, helium, carbon, nitrogen, oxygen, and neon. These abundances are not replicated well on Earth due to substantial separation of the elements taking place during the formation of the Solar System. Silicon makes up 27.2% of the Earth's crust by weight, second only to oxygen at 45.5%, with which it always is associated in nature. Further fractionation took place in the formation of the Earth by planetary differentiation: Earth's core, which makes up 31.5% of the mass of the Earth, has approximate composition FeNiCoS; the mantle makes up 68.1% of the Earth's mass and is composed mostly of denser oxides and silicates, an example being olivine, (Mg,Fe)SiO; while the lighter siliceous minerals such as aluminosilicates rise to the surface and form the crust, making up 0.4% of the Earth's mass.\n\nThe crystallisation of igneous rocks from magma depends on a number of factors; among them are the chemical composition of the magma, the cooling rate, and some properties of the individual minerals to be formed, such as lattice energy, melting point, and complexity of their crystal structure. As magma is cooled, olivine appears first, followed by pyroxene, amphibole, biotite mica, orthoclase feldspar, muscovite mica, quartz, zeolites, and finally, hydrothermal minerals. This sequence shows a trend toward increasingly complex silicate units with cooling, and the introduction of hydroxide and fluoride anions in addition to oxides. Many metals may substitute for silicon. After these igneous rocks undergo weathering, transport, and deposition, sedimentary rocks like clay, shale, and sandstone are formed. Metamorphism also may occur at high temperatures and pressures, creating an even vaster variety of minerals.\n\nSilicon of 96–99% purity is made by reducing quartzite or sand with highly pure coke. The reduction is carried out in an electric arc furnace, with an excess of SiO used to stop silicon carbide (SiC) from accumulating:\nThis reaction, known as carbothermal reduction of silicon dioxide, usually is conducted in the presence of scrap iron with low amounts of phosphorus and sulfur, producing ferrosilicon. Ferrosilicon, an iron-silicon alloy that contains varying ratios of elemental silicon and iron, accounts for about 80% of the world's production of elemental silicon, with China, the leading supplier of elemental silicon, providing 4.6 million tonnes (or 2/3 of world output) of silicon, most of it in the form of ferrosilicon. It is followed by Russia (610,000 t), Norway (330,000 t), Brazil (240,000 t), and the United States (170,000 t). Ferrosilicon is primarily used by the iron and steel industry (see below) with primary use as alloying addition in iron or steel and for de-oxidation of steel in integrated steel plants. Another reaction, sometimes used, is aluminothermal reduction of silicon dioxide, as follows:\n\nLeaching powdered 96–97% pure silicon with water results in ~98.5% pure silicon, which is used in the chemical industry. However, even greater purity is needed for semiconductor applications, and this is produced from the reduction of tetrachlorosilane or trichlorosilane. The former is made by chlorinating scrap silicon and the latter is a byproduct of silicone production. These compounds are volatile and hence can be purified by repeated fractional distillation, followed by reduction to elemental silicon with very pure zinc metal as the reducing agent. The spongy pieces of silicon thus produced are melted and then grown to form cylindrical single crystals, before being purified by zone refining. Other routes use the thermal decomposition of silane or tetraiodosilane. Another process used is the reduction of sodium hexafluorosilicate, a common waste product of the phosphate fertilizer industry, by metallic sodium: this is highly exothermic and hence requires no outside fuel source. Hyperfine silicon is made at a higher purity than almost every other material: transistor production requires impurity levels in silicon crystals less than 1 part per 10, and in special cases impurity levels below 1 part per 10 are needed and attained.\n\nMost silicon is used industrially without being purified, and indeed, often with comparatively little processing from its natural form. More than 90% of the Earth's crust is composed of silicate minerals, which are compounds of silicon and oxygen, often with metallic ions when negatively charged silicate anions require cations to balance the charge. Many of these have direct commercial uses, such as clays, silica sand, and most kinds of building stone. Thus, the vast majority of uses for silicon are as structural compounds, either as the silicate minerals or silica (crude silicon dioxide). Silicates are used in making Portland cement (made mostly of calcium silicates) which is used in building mortar and modern stucco, but more importantly, combined with silica sand, and gravel (usually containing silicate minerals such as granite), to make the concrete that is the basis of most of the very largest industrial building projects of the modern world.\n\nSilica is used to make fire brick, a type of ceramic. Silicate minerals are also in whiteware ceramics, an important class of products usually containing various types of fired clay minerals (natural aluminium phyllosilicates). An example is porcelain, which is based on the silicate mineral kaolinite. Traditional glass (silica-based soda-lime glass) also functions in many of the same ways, and also is used for windows and containers. In addition, specialty silica based glass fibers are used for optical fiber, as well as to produce fiberglass for structural support and glass wool for thermal insulation.\n\nSilicones often are used in waterproofing treatments, molding compounds, mold-release agents, mechanical seals, high temperature greases and waxes, and caulking compounds. Silicone is also sometimes used in breast implants, contact lenses, explosives and pyrotechnics. Silly Putty was originally made by adding boric acid to silicone oil. Other silicon compounds function as high-technology abrasives and new high-strength ceramics based upon silicon carbide. Silicon is a component of some superalloys.\n\nElemental silicon is added to molten cast iron as ferrosilicon or silicocalcium alloys to improve performance in casting thin sections and to prevent the formation of cementite where exposed to outside air. The presence of elemental silicon in molten iron acts as a sink for oxygen, so that the steel carbon content, which must be kept within narrow limits for each type of steel, can be more closely controlled. Ferrosilicon production and use is a monitor of the steel industry, and although this form of elemental silicon is grossly impure, it accounts for 80% of the world's use of free silicon. Silicon is an important constituent of electrical steel, modifying its resistivity and ferromagnetic properties.\n\nThe properties of silicon may be used to modify alloys with metals other than iron. \"Metallurgical grade\" silicon is silicon of 95–99% purity. About 55% of the world consumption of metallurgical purity silicon goes for production of aluminium-silicon alloys (silumin alloys) for aluminium part casts, mainly for use in the automotive industry. Silicon's importance in aluminium casting is that a significantly high amount (12%) of silicon in aluminium forms a eutectic mixture which solidifies with very little thermal contraction. This greatly reduces tearing and cracks formed from stress as casting alloys cool to solidity. Silicon also significantly improves the hardness and thus wear-resistance of aluminium.\n\nMost elemental silicon produced remains as a ferrosilicon alloy, and only approximately 20% is refined to metallurgical grade purity (a total of 1.3–1.5 million metric tons/year). An estimated 15% of the world production of metallurgical grade silicon is further refined to semiconductor purity. This typically is the \"nine-9\" or 99.9999999% purity, nearly defect-free single crystalline material.\n\nMonocrystalline silicon of such purity is usually produced by the Czochralski process, is used to produce silicon wafers used in the semiconductor industry, in electronics, and in some high-cost and high-efficiency photovoltaic applications. Pure silicon is an intrinsic semiconductor, which means that unlike metals, it conducts electron holes and electrons released from atoms by heat; silicon's electrical conductivity increases with higher temperatures. Pure silicon has too low a conductivity (i.e., too high a resistivity) to be used as a circuit element in electronics. In practice, pure silicon is doped with small concentrations of certain other elements, which greatly increase its conductivity and adjust its electrical response by controlling the number and charge (positive or negative) of activated carriers. Such control is necessary for transistors, solar cells, semiconductor detectors, and other semiconductor devices used in the computer industry and other technical applications. In silicon photonics, silicon may be used as a continuous wave Raman laser medium to produce coherent light.\n\nIn common integrated circuits, a wafer of monocrystalline silicon serves as a mechanical support for the circuits, which are created by doping and insulated from each other by thin layers of silicon oxide, an insulator that is easily produced on Si surfaces by processes of Thermal Oxidation or Local Oxidation (LOCOS), which involve exposing the element to oxygen under the proper conditions that can be predicted by the Deal–Grove model. Silicon has become the most popular material for both high power semiconductors and integrated circuits because it can withstand the highest temperatures and greatest electrical activity without suffering avalanche breakdown (an electron avalanche is created when heat produces free electrons and holes, which in turn pass more current, which produces more heat). In addition, the insulating oxide of silicon is not soluble in water, which gives it an advantage over germanium (an element with similar properties which can also be used in semiconductor devices) in certain fabrication techniques.\n\nMonocrystalline silicon is expensive to produce, and is usually justified only in production of integrated circuits, where tiny crystal imperfections can interfere with tiny circuit paths. For other uses, other types of pure silicon may be employed. These include hydrogenated amorphous silicon and upgraded metallurgical-grade silicon (UMG-Si) used in the production of low-cost, large-area electronics in applications such as liquid crystal displays and of large-area, low-cost, thin-film solar cells. Such semiconductor grades of silicon are either slightly less pure or polycrystalline rather than monocrystalline, and are produced in comparable quantities as the monocrystalline silicon: 75,000 to 150,000 metric tons per year. The market for the lesser grade is growing more quickly than for monocrystalline silicon. By 2013, polycrystalline silicon production, used mostly in solar cells, was projected to reach 200,000 metric tons per year, while monocrystalline semiconductor grade silicon was expected to remain less than 50,000 tons per year.\n\nAlthough silicon is readily available in the form of silicates, very few organisms use it directly. Diatoms, radiolaria, and siliceous sponges use biogenic silica as a structural material for their skeletons. In more advanced plants, the silica phytoliths (opal phytoliths) are rigid microscopic bodies occurring in the cell; some plants, for example rice, need silicon for their growth. Silicon has been shown to improve plant cell wall strength and structural integrity in some plants.\n\nThere is some evidence that silicon is important to human health for their nail, hair, bone, and skin tissues, for example, in studies that demonstrate that premenopausal women with higher dietary silicon intake have higher bone density, and that silicon supplementation can increase bone volume and density in patients with osteoporosis. Silicon is needed for synthesis of elastin and collagen, of which the aorta contains the greatest quantity in the human body, and has been considered an essential element; nevertheless, it is difficult to prove its essentiality, because silicon is very common, and hence, deficiency symptoms are difficult to reproduce.\n\nSilicon is currently under consideration for elevation to the status of a \"plant beneficial substance by the Association of American Plant Food Control Officials (AAPFCO).\" \n\nPeople may be exposed to elemental silicon in the workplace by breathing it in, swallowing it, or having contact with the skin or eye. In the latter two cases, silicon poses a slight hazard as an irritant. It is hazardous if inhaled. The Occupational Safety and Health Administration (OSHA) has set the legal limit (Permissible exposure limit) for silicon exposure in the workplace as 15 mg/m total exposure and 5 mg/m respiratory exposure over an 8-hour workday. The National Institute for Occupational Safety and Health (NIOSH) has set a Recommended exposure limit (REL) of 10 mg/m total exposure and 5 mg/m respiratory exposure over an 8-hour workday. Inhalation of crystalline silica dust may lead to silicosis, an occupational lung disease marked by inflammation and scarring in the form of nodular lesions in the upper lobes of the lungs.\n\n\n\n"}
{"id": "10885364", "url": "https://en.wikipedia.org/wiki?curid=10885364", "title": "Somerset Wind Farm", "text": "Somerset Wind Farm\n\nThe Somerset Wind Farm is a wind farm in Somerset County, Pennsylvania with six GE 1.5 MW Wind Turbines that began commercial operation in October 2001. The wind farm has a combined total nameplate capacity of 9 MW, and produces about 25,000 megawatt-hours of electricity annually, which is roughly a 30% capacity factor. The wind farm was constructed by NextEra Energy Resources, based in Florida.\n\nThe wind farm is just one-half mile () south of the Pennsylvania Turnpike, which makes its towers easily visible to turnpike travelers.\n\n\n"}
{"id": "21201515", "url": "https://en.wikipedia.org/wiki?curid=21201515", "title": "State Electricity Regulatory Commission", "text": "State Electricity Regulatory Commission\n\nState Electricity Regulatory Commission (SERC, ) was a government agency responsible for the administration and regulation of the electricity and power industry in the People's Republic of China. This includes regulating the development of electricity markets, advising the National Development Reform Commission on the setting tariffs, while NDRC actually sets the tariffs, transmission, distribution, safety standards, technical standards, business licenses, environmental laws and development of the industry. Its functions were later folded into the National Energy Administration.\n\n\n\n"}
{"id": "39134490", "url": "https://en.wikipedia.org/wiki?curid=39134490", "title": "Tigo Energy", "text": "Tigo Energy\n\nTigo Energy is an American private corporation, headquartered in Los Gatos, California, United States. It provides products, technologies, software and services to installers, distributors, and original equipment manufacturers within the photovoltaic industry. It specializes in module-level power optimizers and smart module power electronics.\n\nTigo® is a Silicon Valley company founded in 2007 by a team of experienced technologists. Combining a unique systems-level approach with expertise in semiconductors, power electronics, and solar energy, the Tigo team developed the first-generation Smart Module Optimizer technology for the solar industry. Tigo's vision is to leverage integrated and retrofitted Flex MLPE (module-level power electronics) and communications technology to drive the cost of solar electricity down. By partnering with tier 1 module and inverter manufacturers in the industry, Tigo is able to focus on its key innovation with the smartest TS4 modular platform and leverage the broader ecosystem. Tigo has operations in the USA, across Europe, Latin America, Japan, China, Australia and the Middle East.\n\nTigo Energy is not affiliated with Millicom International Cellular, also known as Tigo.\n\nThe company was founded by Sam Arditi and Ron Hadar in 2007. As of May 2017, it employs ~75 people in ~10 offices worldwide.\n\nIn December 2011, the company announced its fourth round of funding, an $18 million investment led by Bessemer Ventures.\n\nIn June 2012, Tigo Energy announced an award from the United States Department of Energy as part of its SunShot incubator projects worth $3.5 million. The projects are designed to reduce the ongoing costs and prevent safety hazards from electric arcs in photovoltaic arrays.\n\nIn 2012 Tigo Energy announced the market launch of its newest generation of technology designed to be built directly into solar panels during manufacturing. These smart modules included all the benefits of traditional power optimizers, but without requiring installers to add extra boxes to solar modules. Tigo Energy's launch partners included Trina Solar, Hanwha SolarOne, DelSolar, Astronergy, Upsolar, and Luxor Solar.\n\nIn September 2012 IMS Research identified Tigo Energy as one of three suppliers accounting for 90% of the micro-inverter and power optimizer market.\n\nTigo Energy uses impedance matching to provide power control, data monitoring and safety mechanisms for every module in a solar array to increase performance and safety over traditional system-level technologies. The company also hosts customer data and provides detailed analytics with a software as a service business model.\n\nThe company was the first power optimizer to successfully certify embedding their electronics directly into the junction box of a solar module, reducing the component and labor costs of optimized photovoltaic arrays and eliminating the need for additional hardware.\n"}
{"id": "601254", "url": "https://en.wikipedia.org/wiki?curid=601254", "title": "Tritonal", "text": "Tritonal\n\nTritonal is a mixture of 80% TNT and 20% aluminium powder, used in several types of ordnance such as air-dropped bombs. The aluminium improves the total heat output and hence impulse of the TNT — the length of time during which the blast wave is positive. Tritonal is approximately 18% more powerful than TNT alone.\n\nThe 87 kg of tritonal in a Mark 82 bomb has the potential to produce approximately 863 MJ of energy when detonated.\n\n"}
