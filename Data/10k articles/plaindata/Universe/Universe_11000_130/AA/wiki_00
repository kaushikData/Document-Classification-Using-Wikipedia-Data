{"id": "21151773", "url": "https://en.wikipedia.org/wiki?curid=21151773", "title": "8974", "text": "8974\n\n8974 / X-2159 is a power tetrode designed for megawatt power levels in industrial and broadcast applications.\n\nThe 8974 is an external anode tube made of metal and ceramic. It has an overall height of 23.75 inches (603 mm), a diameter of 17.03 inches (433 mm), and weighs 175 pounds (80 kg). It contains a directly heated thoriated-tungsten filament rated at 16.3 volts at approx. 625 amperes. The anode is designed to dissipate 1.25 megawatts. The tube may be operated as a class C amplifier, where a single tube with an anode voltage of 22.5 kV DC can provide up to 2,158 kilowatts of RF power.\n\nThe thoriated-tungsten filament consists of two independent sections mounted on water-cooled supports. The two filaments may be excited in quadrature to reduce hum contributed by an ac power source. Each filament is rated at a nominal value of 16.3 Volts at approx. 625 Amperes for a total of over 20 kilowatts of filament power required when the tube is in operation.\n\nExperimental type X2159 was assigned to the development of a power tetrode by Eimac (then a division of Varian Associates) on May 28, 1970, and the design engineer was Sterling G. McNees. This electron tube was intended for use in very high power medium-frequency broadcast service and VLF communications equipment and as a pulse modulator (as a switch tube). The EIA (Electronic Industries Alliance) designation 8974 was assigned to this as it became a standard product. The initial technical data sheet was printed in July 1971.\n\nContinental Electronics of Dallas, Texas, designed two 8974s (one as PA and the other as modulator) into their D323 series 1 megawatt AM transmitters which were sold primarily into the middle East, where stations were built to cover large geographical areas. The 8974 is still used in the same transmitters , over 30 years later. The 8974 was also used as the basis for the development of several large tetrode switch tubes capable of operating at anode voltage up to 175 kilovolts dc.\n\nThe anode is cooled by circulated water via two flanges located at the top of the tube. The heat is transferred to the outside environment using a radiator, or to a secondary cooling system using a heat exchanger. Controlling the purity of the water is important to prevent the formation of copper oxide which would reduce the cooling efficiency. Impurities could also cause electrolysis which could destroy the cooling passages. The plumbing connections for the water inlet and outlet are made at the anode, where a high DC voltage (up to 22 kV DC) is present. Forced air is used to provide additional cooling to the filament and grid connections at the bottom of the tube.\n\n"}
{"id": "54443743", "url": "https://en.wikipedia.org/wiki?curid=54443743", "title": "Active power filter", "text": "Active power filter\n\nActive power filters (APF) are filters, which can perform the job of harmonic elimination. Active power filters can be used to filter out harmonics in the power system which are significantly below the switching frequency of the filter. The active power filters are used to filter out both higher and lower order harmonics in the power system.\n\nThe main difference between active power filters and passive power filters is that APFs mitigate harmonics by injecting active power with the same frequency but with reverse phase to cancel that harmonic, where passive power filters use combinations of resistors (R), inductors (L) and capacitors (C) and does not require an external power source or active components such as transistors. This difference, make it possible for APFs to mitigate a wide range of harmonics.\n\n"}
{"id": "45312904", "url": "https://en.wikipedia.org/wiki?curid=45312904", "title": "Aplocheilus werneri", "text": "Aplocheilus werneri\n\nAplocheilus werneri (Werner's killifish) is a species of killifish endemic to Sri Lanka. This species grows to a length of . Its natural habitats are small, shallow, slow-flowing, heavily shaded streams and rivulets with a silt or clay substrate. They are largely use as an aquarium fish.\n"}
{"id": "58304101", "url": "https://en.wikipedia.org/wiki?curid=58304101", "title": "Arik Levinson", "text": "Arik Levinson\n\nArik Mark Levinson is a professor of economics at Georgetown University. He is known for his research in the fields of energy economics and environmental economics. He was a senior economist on the Council of Economic Advisers in the Obama administration from 2010 to 2011.\n\nLevinson received his A.B. from Harvard University in 1986 and his Ph.D. from Columbia University in 1993, the latter under the supervision of David E. Bloom.\n\n"}
{"id": "259962", "url": "https://en.wikipedia.org/wiki?curid=259962", "title": "Avoirdupois system", "text": "Avoirdupois system\n\nThe avoirdupois system (; abbreviated avdp) is a measurement system of weights which uses pounds and ounces as units. It was first commonly used in the 13th century and was updated in 1959.\n\nIn 1959, by international agreement, the definitions of the pound and ounce became standardized in countries which use the pound as a unit of mass. The International Avoirdupois Pound was then created. It is the everyday system of weights used in the United States. It is still used, in varying degrees, in everyday life in the United Kingdom, Canada, and some other former British colonies, despite their official adoption of the metric system.\nThe avoirdupois weight system's general attributes were originally developed for the international wool trade in the Late Middle Ages, when trade was in recovery. It was historically based on a physical standardized pound or \"prototype weight\" that could be divided into 16 ounces. There were a number of competing measures of mass, and the fact that the avoirdupois pound had three even numbers as divisors (half and half and half again) may have been a cause of much of its popularity, so that the system won out over systems with 12 or 10 or 15 subdivisions. The use of this unofficial system gradually stabilized and evolved, with only slight changes in the reference standard or in the prototype's actual mass.\nOver time, the desire not to use too many different systems of measurement allowed the establishment of \"value relationships\", with other commodities metered and sold by weight measurements such as bulk goods (grains, ores, flax) and smelted metals; so the avoirdupois system gradually became an accepted standard through much of Europe.\n\nIn England, Henry VII authorized its use as a standard, and Queen Elizabeth I acted three times to enforce a common standard, thus establishing what became the Imperial system of weights and measures. Late in the 19th century various governments acted to redefine their base standards on a scientific basis and establish ratio-metric equations to SI metric system standards. They did not always pick the same equivalencies, though the pound remained very similar; these independent legal actions led to small differences in certain quantities, such as the American and Imperial pounds. \n\nAn alternative system of mass, the troy system, is generally used for precious materials. The modern definition of the avoirdupois pound (1 lb) is exactly kilograms.\n\n is from Anglo-Norman French (later \"avoir du pois\"), literally \"goods of weight\" (Old French , as verb meaning \"to have\" and as noun meaning \"property, goods\", comes from the Latin , \"to have, to hold, to possess something\"; = \"from\"/\"of\", cf. Latin; = \"weight\", from Latin ). This term originally referred to a class of merchandise: , \"goods of weight\", things that were sold in bulk and were weighed on large steelyards or balances. \nOnly later did the term become identified with a particular \"system of units\" used to weigh such merchandise. The warfare-impacted orthography of the day has left many variants of the term, such as and . (The Norman became the Parisian . In the 17th century \"de\" was replaced with \"du\".)\n\nThe rise in use of the measurement system corresponds to the regrowth of trade during the High Middle Ages after the early crusades, when Europe experienced a growth in towns, turned from the chaos of warlordism to long distance trade, and began annual fairs, tournaments and commerce, by land and sea. There are two major hypotheses regarding the origins of the avoirdupois system. The older hypothesis is that it originated in France. A newer hypothesis is that it is based on the weight system of Florence.\n\nThe avoirdupois weight system is thought to have come into use in England circa 1300. It was originally used for weighing wool. In the early 14th century several other specialized weight systems were used, including the weight system of the Hanseatic League with a 16-ounce pound of grains and an 8-ounce mark. However, the main weight system, used for coinage and for everyday use, was based on the 12-ounce tower pound of grains. From the 14th century until the late 16th century, the systems basis, the avoirdupois pound, the prototype for today's international pound was also known as the wool pound or the avoirdupois wool pound. \nThe earliest known version of the avoirdupois weight system had the following units: a pound of grains, a stone of 14 pounds, a woolsack of 26 stone, an ounce of pound, and finally, the ounce was divided into 16 \"parts\".\n\nThe earliest known occurrence of the word \"avoirdupois\" (or some variant thereof) in England is from a document entitled \"Tractatus de Ponderibus et Mensuris\" (\"Treatise on Weights and Measures\"). This document is listed in early statute books under the heading \"31 Edward I\" dated 2 February 1303. More recent statute books list it among statutes of uncertain date. Scholars nowadays believe that it was probably written between 1266 and 1303. Initially a royal memorandum, it eventually took on the force of law and was recognized as a statute by King Henry VIII and Queen Elizabeth I.\nIt was repealed by the Weights and Measures Act 1824. In the \"Tractatus\", the word \"avoirdupois\" refers not to a weight system, but to a class of goods, specifically heavy goods sold by weight, as opposed to goods sold by volume, count, or some other method. Since it is written in Anglo-Norman French, this document is not the first occurrence of the word in the English language.\n\nThree major developments occurred during the reign of Edward III (r. 1327-77). First, a statute known as 14 Edward III. st. 1. Cap. 12 (1340) \"Bushels and Weights shall be made and sent into every County.\"\n\nThe second major development is the statute 25 Edward III. st. 5. Cap. 9. (1350) \"The Auncel Weight shall be put out, and Weighing shall be by equal Balance.\"\n\nThe third development is a set of 14th-century bronze weights at the Westgate Museum in Winchester, England. The weights are in denominations of 7 pounds (corresponding to a unit known as the clip or wool-clip), 14 pounds (stone), 56 pounds (4 stone) and 91 pounds ( sack or woolsack). The 91-pound weight is thought to have been commissioned by Edward III in conjunction with the statute of 1350, while the other weights are thought to have been commissioned in conjunction with the statutes of 1340. The 56-pound weight was used as a reference standard as late as 1588.\n\nA statute of Henry VIII (24 Henry VIII. Cap. 3) made avoirdupois weights mandatory.\n\nIn 1588 Queen Elizabeth increased the weight of the avoirdupois pound to grains and added the troy grain to the avoirdupois weight system. Prior to 1588, the \"part\" () was the smallest unit in the avoirdupois weight system. In the 18th century, the \"part\" was renamed \"drachm\".\n\nThese are the units in their original Anglo-Norman French forms:\n\nIn the United Kingdom, 14 avoirdupois pounds equals one stone. The quarter, hundredweight, and ton equal respectively, 28 lb, 112 lb, and 2,240 lb in order for masses to be easily converted between them and stones. The following are the units in the British or imperial version of the avoirdupois system:\n\n\"Note:\" The plural form of the unit \"stone\" is either \"stone\" or \"stones\", but \"stone\" is most frequently used.\n\nThe 13 British colonies in North America used the avoirdupois system, but continued to use the British system as it was, without the evolution that was occurring in Britain in the use of the stone unit. In 1824 there was landmark new weights and measures legislation in the United Kingdom that the United States did not adopt.\n\nIn the United States, quarters, hundredweights, and tons remain defined as 25, 100, and respectively. The quarter is now virtually unused, as is the hundredweight outside of agriculture and commodities. If disambiguation is required, then they are referred to as the smaller \"short\" units in the United States, as opposed to the larger British \"long\" units. Grains are used worldwide for measuring gunpowder and smokeless powder charges. Historically, the dram has also been used worldwide for measuring gunpowder charges, particularly for shotguns and large black-powder rifles.\n\n\n"}
{"id": "1515708", "url": "https://en.wikipedia.org/wiki?curid=1515708", "title": "Brown note", "text": "Brown note\n\nThe brown note is a hypothetical infrasonic frequency that would cause humans to lose control of their bowels due to resonance. Attempts to demonstrate the existence of a \"brown note\" using sound waves transmitted through air have failed.\n\nThe name is a metonym for the common color of human feces. Frequencies supposedly involved are between 5 and 9 Hz, which is below the lower frequency limit of human hearing. High power sound waves below 20 Hz are felt in the body, not heard by the ear as sound.\n\nJürgen Altmann of the Dortmund University of Technology, an expert on sonic weapons, has said that there is no reliable evidence for nausea and vomiting caused by infrasound.\n\nHigh volume levels at concerts from subwoofer arrays have been cited as causing lung collapse in individuals who are very close to the subwoofers, especially for smokers who are particularly tall and thin.\n\nIn September 2009, London student Tom Reid died of sudden arrhythmic death syndrome (SADS) after complaining that \"loud bass notes\" were \"getting to his heart\". The inquest recorded a verdict of natural causes, although some experts commented that the bass could have acted as a trigger.\n\nAir is a very inefficient medium for transferring low frequency vibration from a transducer to the human body. Mechanical connection of the vibration source to the human body, however, provides a potentially dangerous combination. The U.S. space program, worried about the harmful effects of rocket flight on astronauts, ordered vibration tests that used cockpit seats mounted on vibration tables to transfer \"brown note\" and other frequencies directly to the human subjects. Very high power levels of 160 dB were achieved at frequencies of 2–3 Hz. Test frequencies ranged from 0.5 Hz to 40 Hz. Test subjects suffered motor ataxia, nausea, visual disturbance, degraded task performance and difficulties in communication. These tests are assumed by researchers to be the nucleus of the current urban myth.\n\nThe report \"A Review of Published Research on Low Frequency Noise and its Effects\" contains a long list of research about exposure to high-level infrasound among humans and animals. For instance, in 1972, Borredon exposed 42 young men to tones at 7.5 Hz at 130 dB for 50 minutes. This exposure caused no adverse effects other than reported drowsiness and a slight blood pressure increase. In 1975, Slarve and Johnson exposed four male subjects to infrasound at frequencies from 1 to 20 Hz, for eight minutes at a time, at levels up to 144 dB SPL. There was no evidence of any detrimental effect other than middle ear discomfort. Tests of high-intensity infrasound on animals resulted in measurable changes, such as cell changes and ruptured blood vessel walls.\n\nIn February 2005 the television show \"MythBusters\" used twelve Meyer Sound 700-HP subwoofers—a model and quantity that has been employed for major rock concerts. Normal operating frequency range of the selected subwoofer model was 28 Hz to 150 Hz but the 12 enclosures at \"MythBusters\" had been specially modified for deeper bass extension. Roger Schwenke and John Meyer directed the Meyer Sound team in devising a special test rig that would produce very high sound levels at infrasonic frequencies. The subwoofers' tuning ports were blocked and their input cards were altered. The modified cabinets were positioned in an open ring configuration: four stacks of three subwoofers each. Test signals were generated by a SIM 3 audio analyzer, with its software modified to produce infrasonic tones. A Brüel & Kjær sound level analyzer, fed with an attenuated signal from a model 4189 measurement microphone, displayed and recorded sound pressure levels. The hosts on the show tried a series of frequencies as low as 5 Hz, attaining a level of 120 decibels of sound pressure at 9 Hz and up to 153 dB at frequencies above 20 Hz, but the rumored physiological effects did not materialize. The test subjects all reported some physical anxiety and shortness of breath, even a small amount of nausea, but this was dismissed by the hosts, noting that sound at that frequency and intensity moves air rapidly in and out of one's lungs. The show declared the brown note myth \"busted.\"\n\nIn the comic book series \"Transmetropolitan\", the main character, Spider Jerusalem, wields a \"Bowel Disruptor\" that operates using ultrasonic waves, with varying settings of intensity.\n\nIn the season 3 episode \"World Wide Recorder Concert\" (episode 17) of the animated show \"South Park\" in 2000, the brown note myth is featured prominently; the boys rewrite the music for a worldwide recorder concert to include the brown note (for kids from New York who kept insulting them), with the unintended result that everyone in the world defecates almost simultaneously.\n\nIn the season 4 episode \"The Curse of Shiva\" (episode 13) of \"The League\", Taco (Jon Lajoie) has found a use for the thousands of discarded vuvuzela instruments left over from the 2010 FIFA World Cup hosted by South Africa. The instruments were tinkered with to produce the brown note with hopes that it could then be sold to the military as a weapon.\n\nIn the season 1 episode \"Angel Boy\" (episode 4) of the Adult Swim show \"Tim and Eric's Bedtime Stories\", the character Scotty sings a note that causes attendees at a birthday party to have intense gastrointestinal distress and defecate uncontrollably.\n\nIn the season 6 episode \"Edie's Wedding\" (episode 4) of \"Archer\", \"Doctor\" Algernop Krieger builds an ultrasonic weapon which hypothetically produces the brown note.\n\n"}
{"id": "43948096", "url": "https://en.wikipedia.org/wiki?curid=43948096", "title": "C.D.V agar", "text": "C.D.V agar\n\nC.D.V. agar is a nutrient medium prepared using Cicer arietinum (Bengal gram), Daucus carota (carrot) and Vigna radiata (green gram) by Mr. Vinay J. Rao and Mr. Keshava Murthy P. at Surana College, Bangalore, India under the guidance of their botany lecturers, Prof. B.R Chandrashekarappa (H.O.D.- Dept. Of Botany), Mrs. Sharada H.C., and with the aid of their classmates from Vijaya College, Bangalore.\n\nSo far, CDV agar has been tried only on E. coli. Attempts will be made to try the medium on other organisms as well. The nutrient has been prepared in order to counter the discomfort of using materials such as peptone, beef extract, and yeast extract.\n\nAs with higher organisms, lower organisms also depend on nutrition for their survival. As a result, many microbiologists have proposed various sources of nutrition for proper microbial growth. Most nutrient media consist of animal products, the use of which is considered unethical by some. Here, an attempt has been made to substitute for animal products with some plant products.\n\nThe medium was prepared in two sets; one was autoclaved before pouring, while the other was heated to around 60C to conserve the proteins, which might have denatured at a higher temperature. The results obtained were very satisfactory; solitary colonies were observed in the plates with the autoclaved agar, while the heated agar gave a larger surface area of E. coli.\n\nThe above-mentioned plants were taken for this experiment due to their biochemical contents and the contents of ordinary agar. In this recipe, the chick pea replaces peptone; for a protein source, green gram replaces beef extract; for another protein source, carrot replaces yeast extract. All these plants contain proteins and carbohydrates, along with other essential biochemicals in sufficient amounts.\n\nTEQIP II Sponsored National Conference, organized by Dept. Of Biotechnology, PES Institute Of Technology, Bangalore on 12th-13th Sept 2013.\n"}
{"id": "2908558", "url": "https://en.wikipedia.org/wiki?curid=2908558", "title": "Capacitor types", "text": "Capacitor types\n\nCapacitors are manufactured in many forms, styles, lengths, girths, and from many materials. They all contain at least two electrical conductors (called \"plates\") separated by an insulating layer (called the dielectric). Capacitors are widely used as parts of electrical circuits in many common electrical devices.\n\nCapacitors, together with resistors, and inductors, belong to the group of \"passive components\" used in electronic equipment. Although, in absolute figures, the most common capacitors are integrated capacitors (e.g. in DRAMs or flash memory structures), this article is concentrated on the various styles of capacitors as discrete components.\n\nSmall capacitors are used in electronic devices to couple signals between stages of amplifiers, as components of electric filters and tuned circuits, or as parts of power supply systems to smooth rectified current. Larger capacitors are used for energy storage in such applications as strobe lights, as parts of some types of electric motors, or for power factor correction in AC power distribution systems. Standard capacitors have a fixed value of capacitance, but adjustable capacitors are frequently used in tuned circuits. Different types are used depending on required capacitance, working voltage, current handling capacity, and other properties.\n\nIn a conventional capacitor, the electric energy is stored statically by charge separation, typically electrons, in an electric field between two electrode plates. The amount of charge stored per unit voltage is essentially a function of the size of the plates, the plate material's properties, the properties of the dielectric material placed between the plates, and the separation distance (i.e. dielectric thickness). The potential between the plates is limited by the properties of the dielectric material and the separation distance.\n\nNearly all conventional industrial capacitors except some special styles such as \"feed-through capacitors\", are constructed as \"plate capacitors\" even if their electrodes and the dielectric between are wound or rolled. The capacitance formula for plate capacitors is:\n\nThe capacitance C increases with the area A of the plates and with the permittivity ε of the dielectric material and decreases with the plate separation distance d. The capacitance is therefore greatest in devices made from materials with a high permittivity, large plate area, and small distance between plates.\n\nAnother type – the electrochemical capacitor – makes use of two other storage principles to store electric energy. In contrast to ceramic, film, and electrolytic capacitors, supercapacitors (also known as electrical double-layer capacitors (EDLC) or ultracapacitors) do not have a conventional dielectric. The capacitance value of an electrochemical capacitor is determined by two high-capacity storage principles. These principles are:\n\n\nThe ratio of the storage resulting from each principle can vary greatly, depending on electrode design and electrolyte composition. Pseudocapacitance can increase the capacitance value by as much as an order of magnitude over that of the double-layer by itself.\n\nCapacitors are divided into two mechanical groups: Fixed capacitors with fixed capacitance values and variable capacitors with variable (trimmer) or adjustable (tunable) capacitance values.\n\nThe most important group is the fixed capacitors. Many got their names from the dielectric. For a systematic classification these characteristics can't be used, because one of the oldest, the electrolytic capacitor, is named instead by its cathode construction. So the most-used names are simply historical.\n\nThe most common kinds of capacitors are:\n\n\nIn addition to the above shown capacitor types, which derived their name from historical development, there are many individual capacitors that have been named based on their application. They include:\n\n\nOften, more than one capacitor family is employed for these applications, e.g. interference suppression can use ceramic capacitors or film capacitors.\n\nOther kinds of capacitors are discussed in the #Special capacitors section.\n\nThe most common dielectrics are:\n\n\nAll of them store their electrical charge statically within an electric field between two (parallel) electrodes.\n\nBeneath this conventional capacitors a family of electrochemical capacitors called Supercapacitors was developed. Supercapacitors don't have a conventional dielectric. They store their electrical charge statically in Helmholtz double-layers and faradaically at the surface of electrodes\n\n\nThe most important material parameters of the different dielectrics used and the appr. Helmholtz-layer thickness are given in the table below.\n\nThe capacitor's plate area can be adapted to the wanted capacitance value. The permittivity and the dielectric thickness are the determining parameter for capacitors. Ease of processing is also crucial. Thin, mechanically flexible sheets can be wrapped or stacked easily, yielding large designs with high capacitance values. Razor-thin metallized sintered ceramic layers covered with metallized electrodes however, offer the best conditions for the miniaturization of circuits with SMD styles.\n\nA short view to the figures in the table above gives the explanation for some simple facts:\n\n\nCapacitance ranges from picofarad to more than hundreds of farad. Voltage ratings can reach 100 kilovolts. In general, capacitance and voltage correlates with physical size and cost.\n\nAs in other areas of electronics, volumetric efficiency measures the performance of electronic function per unit volume. For capacitors, the volumetric efficiency is measured with the \"CV product\", calculated by multiplying the capacitance (C) by the maximum voltage rating (V), divided by the volume. From 1970 to 2005, volumetric efficiencies have improved dramatically.\n\nThese individual capacitors can perform their application independent of their affiliation to an above shown capacitor type, so that an overlapping range of applications between the different capacitor types exists.\n\nA ceramic capacitor is a non-polarized fixed capacitor made out of two or more alternating layers of ceramic and metal in which the ceramic material acts as the dielectric and the metal acts as the electrodes. The ceramic material is a mixture of finely ground granules of paraelectric or ferroelectric materials, modified by mixed oxides that are necessary to achieve the capacitor's desired characteristics. The electrical behavior of the ceramic material is divided into two stability classes:\n\n\nThe great plasticity of ceramic raw material works well for many special applications and enables an enormous diversity of styles, shapes and great dimensional spread of ceramic capacitors. The smallest discrete capacitor, for instance, is a \"01005\" chip capacitor with the dimension of only 0.4 mm × 0.2 mm.\n\nThe construction of ceramic multilayer capacitors with mostly alternating layers results in single capacitors connected in parallel. This configuration increases capacitance and decreases all losses and parasitic inductances. Ceramic capacitors are well-suited for high frequencies and high current pulse loads.\n\nBecause the thickness of the ceramic dielectric layer can be easily controlled and produced by the desired application voltage, ceramic capacitors are available with rated voltages up to the 30 kV range.\n\nSome ceramic capacitors of special shapes and styles are used as capacitors for special applications, including RFI/EMI suppression capacitors for connection to supply mains, also known as safety capacitors, X2Y® and three-terminal capacitors for bypassing and decoupling applications, feed-through capacitors for noise suppression by low-pass filters and ceramic power capacitors for transmitters and HF applications.\n\nFilm capacitors or plastic film capacitors are non-polarized capacitors with an insulating plastic film as the dielectric. The dielectric films are drawn to a thin layer, provided with metallic electrodes and wound into a cylindrical winding. The electrodes of film capacitors may be metallized aluminum or zinc, applied on one or both sides of the plastic film, resulting in metallized film capacitors or a separate metallic foil overlying the film, called film/foil capacitors.\n\nMetallized film capacitors offer self-healing properties. Dielectric breakdowns or shorts between the electrodes do not destroy the component. The metallized construction makes it possible to produce wound capacitors with larger capacitance values (up to 100 µF and larger) in smaller cases than within film/foil construction.\n\nFilm/foil capacitors or metal foil capacitors use two plastic films as the dielectric. Each film is covered with a thin metal foil, mostly aluminium, to form the electrodes. The advantage of this construction is the ease of connecting the metal foil electrodes, along with an excellent current pulse strength.\n\nA key advantage of every film capacitor's internal construction is direct contact to the electrodes on both ends of the winding. This contact keeps all current paths very short. The design behaves like a large number of individual capacitors connected in parallel, thus reducing the internal ohmic losses (ESR) and ESL. The inherent geometry of film capacitor structure results in low ohmic losses and a low parasitic inductance, which makes them suitable for applications with high surge currents (snubbers) and for AC power applications, or for applications at higher frequencies.\n\nThe plastic films used as the dielectric for film capacitors are Polypropylene (PP), Polyester (PET), Polyphenylene sulfide (PPS), Polyethylene naphthalate (PEN), and Polytetrafluoroethylene or Teflon (PTFE). Polypropylene film material with a market share of something about 50% and Polyester film with something about 40% are the most used film materials. The rest of something about 10% will be used by all other materials including PPS and paper with roughly 3%, each.\n\nSome film capacitors of special shapes and styles are used as capacitors for special applications, including RFI/EMI suppression capacitors for connection to the supply mains, also known as safety capacitors, Snubber capacitors for very high surge currents, Motor run capacitors, AC capacitors for motor-run applications\n\nA related type is the power film capacitor. The materials and construction techniques used for large power film capacitors mostly are similar to those of ordinary film capacitors. However, capacitors with high to very high power ratings for applications in power systems and electrical installations are often classified separately, for historical reasons. The standardization of ordinary film capacitors is oriented on electrical and mechanical parameters. The standardization of power capacitors by contrast emphasizes the safety of personnel and equipment, as given by the local regulating authority.\n\nAs modern electronic equipment gained the capacity to handle power levels that were previously the exclusive domain of \"electrical power\" components, the distinction between the \"electronic\" and \"electrical\" power ratings blurred. Historically, the boundary between these two families was approximately at a reactive power of 200 volt-amperes.\n\nFilm power capacitors mostly use polypropylene film as the dielectric. Other types include metallized paper capacitors (MP capacitors) and mixed dielectric film capacitors with polypropylene dielectrics. MP capacitors serve for cost applications and as field-free carrier electrodes (soggy foil capacitors) for high AC or high current pulse loads. Windings can be filled with an insulating oil or with epoxy resin to reduce air bubbles, thereby preventing short circuits.\n\nThey find use as converters to change voltage, current or frequency, to store or deliver abruptly electric energy or to improve the power factor. The rated voltage range of these capacitors is from approximately 120 V AC (capacitive lighting ballasts) to 100 kV.\n\nElectrolytic capacitors have a metallic anode covered with an oxidized layer used as dielectric. The second electrode is a non-solid (wet) or solid electrolyte. Electrolytic capacitors are polarized. Three families are available, categorized according to their dielectric.\n\n\nThe anode is highly roughened to increase the surface area. This and the relatively high permittivity of the oxide layer gives these capacitors very high capacitance per unit volume compared with film- or ceramic capacitors.\n\nThe permittivity of tantalum pentoxide is approximately three times higher than aluminium oxide, producing significantly smaller components. However, permittivity determines only the dimensions. Electrical parameters, especially conductivity, are established by the electrolyte's material and composition. Three general types of electrolytes are used:\n\n\nInternal losses of electrolytic capacitors, prevailing used for decoupling and buffering applications, are determined by the kind of electrolyte.\n\nThe large capacitance per unit volume of electrolytic capacitors make them valuable in relatively high-current and low-frequency electrical circuits, e.g. in power supply filters for decoupling unwanted AC components from DC power connections or as coupling capacitors in audio amplifiers, for passing or bypassing low-frequency signals and storing large amounts of energy. The relatively high capacitance value of an electrolytic capacitor combined with the very low ESR of the polymer electrolyte of polymer capacitors, especially in SMD styles, makes them a competitor to MLC chip capacitors in personal computer power supplies.\n\nBipolar aluminum electrolytic capacitors (also called Non-Polarized capacitors) contain two anodized aluminium foils, behaving like two capacitors connected in series opposition.\n\nElectrolytic capacitors for special applications include motor start capacitors, flashlight capacitors and audio frequency capacitors.\n\nSupercapacitors (SC), comprise a family of electrochemical capacitors. Supercapacitor, sometimes called ultracapacitor is a generic term for electric double-layer capacitors (EDLC), pseudocapacitors and hybrid capacitors. They don't have a conventional solid dielectric. The capacitance value of an electrochemical capacitor is determined by two storage principles, both of which contribute to the total capacitance of the capacitor:\n\n\nThe ratio of the storage resulting from each principle can vary greatly, depending on electrode design and electrolyte composition. Pseudocapacitance can increase the capacitance value by as much as an order of magnitude over that of the double-layer by itself.\n\nSupercapacitors are divided into three families, based on the design of the electrodes:\n\n\nSupercapacitors bridge the gap between conventional capacitors and rechargeable batteries. They have the highest available capacitance values per unit volume and the greatest energy density of all capacitors. They support up to 12,000 farads/1.2 volt, with capacitance values up to 10,000 times that of electrolytic capacitors. While existing supercapacitors have energy densities that are approximately 10% of a conventional battery, their power density is generally 10 to 100 times greater. Power density is defined as the product of energy density, multiplied by the speed at which the energy is delivered to the load. The greater power density results in much shorter charge/discharge cycles than a battery is capable, and a greater tolerance for numerous charge/discharge cycles. This makes them well-suited for parallel connection with batteries, and may improve battery performance in terms of power density.\n\nWithin electrochemical capacitors, the electrolyte is the conductive connection between the two electrodes, distinguishing them from electrolytic capacitors, in which the electrolyte only forms the cathode, the second electrode.\n\nSupercapacitors are polarized and must operate with correct polarity. Polarity is controlled by design with asymmetric electrodes, or, for symmetric electrodes, by a potential applied during the manufacturing process.\n\nSupercapacitors support a broad spectrum of applications for power and energy requirements, including:\n\nSupercapacitors are rarely interchangeable, especially those with higher energy densities. IEC standard 62391-1 \"Fixed electric double layer capacitors for use in electronic equipment\" identifies four application classes:\n\nExceptional for electronic components like capacitors are the manifold different trade or series names used for supercapacitors like: \"APowerCap, BestCap, BoostCap, CAP-XX, DLCAP, EneCapTen, EVerCAP, DynaCap, Faradcap, GreenCap, Goldcap, HY-CAP, Kapton capacitor, Super capacitor, SuperCap, PAS Capacitor, PowerStor, PseudoCap, Ultracapacitor\" making it difficult for users to classify these capacitors.\n\nMany safety regulations mandate that Class X or Class Y capacitors must be used whenever a \"fail-to-short-circuit\" could put humans in danger,\nto guarantee galvanic isolation even when the capacitor fails.\n\nLightning strikes and other sources cause high voltage surges in mains power.\nSafety capacitors protect humans and devices from high voltage surges by shunting the surge energy to ground.\n\nIn particular, safety regulations mandate a particular arrangement of Class X and Class Y mains filtering capacitors.\nIn principle, any dielectric could be used to build Class X and Class Y capacitors;\nperhaps by including an internal fuse to improve safety.\nIn practice, capacitors that meet Class X and Class Y specifications are typically\nceramic RFI/EMI suppression capacitors or\nplastic film RFI/EMI suppression capacitors.\n\nBeneath the above described capacitors covering more or less nearly the total market of discrete capacitors some new developments or very special capacitor types as well as older types can be found in electronics.\n\n\n\n\nSpecialized devices such as built-in capacitors with metal conductive areas in different layers of a multi-layer printed circuit board and kludges such as twisting together two pieces of insulated wire also exist.\n\nCapacitors made by twisting 2 pieces of insulated wire together are called gimmick capacitors.\nGimmick capacitors were used in commercial and amateur radio receivers.\n\n\nVariable capacitors may have their capacitance changed by mechanical motion. Generally two versions of variable capacitors has to be to distinguished\n\n\nVariable capacitors include capacitors that use a mechanical construction to change the distance between the plates, or the amount of plate surface area which overlaps. They mostly use air as dielectric medium.\n\nSemiconductive variable capacitance diodes are not capacitors in the sense of passive components but can change their capacitance as a function of the applied reverse bias voltage and are used like a variable capacitor. They have replaced much of the tuning and trimmer capacitors.\n\nDiscrete capacitors deviate from the ideal capacitor. An ideal capacitor only stores and releases electrical energy, with no dissipation. Capacitor components have losses and parasitic inductive parts. These imperfections in material and construction can have positive implications such as linear frequency and temperature behavior in class 1 ceramic capacitors. Conversely, negative implications include the non-linear, voltage-dependent capacitance in class 2 ceramic capacitors or the insufficient dielectric insulation of capacitors leading to leakage currents.\n\nAll properties can be defined and specified by a series equivalent circuit composed out of an idealized capacitance and additional electrical components which model all losses and inductive parameters of a capacitor. In this series-equivalent circuit the electrical characteristics are defined by:\n\n\nUsing a series equivalent circuit instead of a parallel equivalent circuit is specified by IEC/EN 60384-1.\n\nThe rated capacitance C or nominal capacitance C is the value for which the capacitor has been designed. Actual capacitance depends on the measured frequency and ambient temperature. Standard measuring conditions are a low-voltage AC measuring method at a temperature of 20 °C with frequencies of\n\n\nFor supercapacitors a voltage drop method is applied for measuring the capacitance value. .\n\nCapacitors are available in geometrically increasing preferred values (E series standards) specified in IEC/EN 60063. According to the number of values per decade, these were called the E3, E6, E12, E24 etc. series. The range of units used to specify capacitor values has expanded to include everything from pico- (pF), nano- (nF) and microfarad (µF) to farad (F). Millifarad and kilofarad are uncommon.\n\nThe percentage of allowed deviation from the rated value is called tolerance. The actual capacitance value should be within its tolerance limits, or it is out of specification. IEC/EN 60062 specifies a letter code for each tolerance.\n\nThe required tolerance is determined by the particular application. The narrow tolerances of E24 to E96 are used for high-quality circuits such as precision oscillators and timers. General applications such as non-critical filtering or coupling circuits employ E12 or E6. Electrolytic capacitors, which are often used for filtering and bypassing capacitors mostly have a tolerance range of ±20% and need to conform to E6 (or E3) series values.\n\nCapacitance typically varies with temperature. The different dielectrics express great differences in temperature sensitivity. The temperature coefficient is expressed in parts per million (ppm) per degree Celsius for class 1 ceramic capacitors or in % over the total temperature range for all others.\n\nMost discrete capacitor types have more or less capacitance changes with increasing frequencies. The dielectric strength of class 2 ceramic and plastic film diminishes with rising frequency. Therefore, their capacitance value decreases with increasing frequency. This phenomenon for ceramic class 2 and plastic film dielectrics is related to dielectric relaxation in which the time constant of the electrical dipoles is the reason for the frequency dependence of permittivity. The graphs below show typical frequency behavior of the capacitance for ceramic and film capacitors.\n\nFor electrolytic capacitors with non-solid electrolyte, mechanical motion of the ions occurs. Their movability is limited so that at higher frequencies not all areas of the roughened anode structure are covered with charge-carrying ions. As higher the anode structure is roughned as more the capacitance value decreases with increasing frequency. Low voltage types with highly roughened anodes display capacitance at 100 kHz approximately 10 to 20% of the value measured at 100 Hz.\n\nCapacitance may also change with applied voltage. This effect is more prevalent in class 2 ceramic capacitors. The permittivity of ferroelectric class 2 material depends on the applied voltage. Higher applied voltage lowers permittivity. The change of capacitance can drop to 80% of the value measured with the standardized measuring voltage of 0.5 or 1.0 V. This behavior is a small source of non-linearity in low-distortion filters and other analog applications. In audio applications this can cause distortion (measured using THD).\n\nFilm capacitors and electrolytic capacitors have no significant voltage dependence.\n\nThe voltage at which the dielectric becomes conductive is called the breakdown voltage, and is given by the product of the dielectric strength and the separation between the electrodes. The dielectric strength depends on temperature, frequency, shape of the electrodes, etc. Because a breakdown in a capacitor normally is a short circuit and destroys the component, the operating voltage is lower than the breakdown voltage. The operating voltage is specified such that the voltage may be applied continuously throughout the life of the capacitor.\n\nIn IEC/EN 60384-1 the allowed operating voltage is called \"rated voltage\" or \"nominal voltage\". The rated voltage (UR) is the maximum DC voltage or peak pulse voltage that may be applied continuously at any temperature within the rated temperature range.\n\nThe voltage proof of nearly all capacitors decreases with increasing temperature. Some applications require a higher temperature range. Lowering the voltage applied at a higher temperature maintains safety margins. For some capacitor types therefore the IEC standard specify a second \"temperature derated voltage\" for a higher temperature range, the \"category voltage\". The category voltage (UC) is the maximum DC voltage or peak pulse voltage that may be applied continuously to a capacitor at any temperature within the category temperature range.\n\nThe relation between both voltages and temperatures is given in the picture right.\n\nIn general, a capacitor is seen as a storage component for electric energy. But this is only one capacitor function. A capacitor can also act as an AC resistor. In many cases the capacitor is used as a decoupling capacitor to filter or bypass undesired biased AC frequencies to the ground. Other applications use capacitors for capacitive coupling of AC signals; the dielectric is used only for blocking DC. For such applications the AC resistance is as important as the capacitance value.\n\nThe frequency dependent AC resistance is called impedance formula_2 and is the complex ratio of the voltage to the current in an AC circuit. Impedance extends the concept of resistance to AC circuits and possesses both magnitude and phase at a particular frequency. This is unlike resistance, which has only magnitude.\n\nThe magnitude formula_4 represents the ratio of the voltage difference amplitude to the current amplitude, formula_5 is the imaginary unit, while the argument formula_6 gives the phase difference between voltage and current.\n\nIn capacitor data sheets, only the impedance magnitude |Z| is specified, and simply written as \"Z\" so that the formula for the impedance can be written in Cartesian form\n\nwhere the real part of impedance is the resistance formula_8 (for capacitors formula_9) and the imaginary part is the reactance formula_10.\n\nAs shown in a capacitor's series-equivalent circuit, the real component includes an ideal capacitor formula_11, an inductance formula_12 and a resistor formula_13. The total reactance at the angular frequency formula_14 therefore is given by the geometric (complex) addition of a capacitive reactance (Capacitance) formula_15 and an inductive reactance (Inductance): formula_16.\n\nTo calculate the impedance formula_2 the resistance has to be added geometrically and then formula_18 is given by\n\nto calculate either the peak or the effective value of the current or the voltage.\n\nIn the special case of resonance, in which the both reactive resistances\n\nhave the same value (formula_22), then the impedance will only be determined by formula_23.\nThe impedance specified in the datasheets often show typical curves for the different capacitance values. With increasing frequency as the impedance decreases down to a minimum. The lower the impedance, the more easily alternating currents can be passed through the capacitor. At the apex, the point of resonance, where XC has the same value than XL, the capacitor has the lowest impedance value. Here only the ESR determines the impedance. With frequencies above the resonance the impedance increases again due to the ESL of the capacitor. The capacitor becomes an inductance.\n\nAs shown in the graph, the higher capacitance values can fit the lower frequencies better while the lower capacitance values can fit better the higher frequencies.\n\nAluminum electrolytic capacitors have relatively good decoupling properties in the lower frequency range up to about 1 MHz due to their large capacitance values. This is the reason for using electrolytic capacitors in standard or switched-mode power supplies behind the rectifier for smoothing application.\n\nCeramic and film capacitors are already out of their smaller capacitance values suitable for higher frequencies up to several 100 MHz. They also have significantly lower parasitic inductance, making them suitable for higher frequency applications, due to their construction with end-surface contacting of the electrodes. To increase the range of frequencies, often an electrolytic capacitor is connected in parallel with a ceramic or film capacitor.\n\nMany new developments are targeted at reducing parasitic inductance (ESL). This increases the resonance frequency of the capacitor and, for example, can follow the constantly increasing switching speed of digital circuits. Miniaturization, especially in the SMD multilayer ceramic chip capacitors (MLCC), increases the resonance frequency. Parasitic inductance is further lowered by placing the electrodes on the longitudinal side of the chip instead of the lateral side. The \"face-down\" construction associated with multi-anode technology in tantalum electrolytic capacitors further reduced ESL. Capacitor families such as the so-called MOS capacitor or silicon capacitors offer solutions when capacitors at frequencies up to the GHz range are needed.\n\nESL in industrial capacitors is mainly caused by the leads and internal connections used to connect the capacitor plates to the outside world. Large capacitors tend to have higher ESL than small ones because the distances to the plate are longer and every mm counts as an inductance.\n\nFor any discrete capacitor, there is a frequency above DC at which it ceases to behave as a pure capacitor. This frequency, where formula_24 is as high as formula_25, is called the self-resonant frequency. The self-resonant frequency is the lowest frequency at which the impedance passes through a minimum. For any AC application the self-resonant frequency is the highest frequency at which capacitors can be used as a capacitive component.\n\nThis is critically important for decoupling high-speed logic circuits from the power supply. The decoupling capacitor supplies transient current to the chip. Without decouplers, the IC demands current faster than the connection to the power supply can supply it, as parts of the circuit rapidly switch on and off. To counter this potential problem, circuits frequently use multiple bypass capacitors—small (100 nF or less) capacitors rated for high frequencies, a large electrolytic capacitor rated for lower frequencies and occasionally, an intermediate value capacitor.\n\nThe summarized losses in discrete capacitors are ohmic AC losses. DC losses are specified as \"leakage current\" or \"insulating resistance\" and are negligible for an AC specification. AC losses are non-linear, possibly depending on frequency, temperature, age or humidity. The losses result from two physical conditions:\n\n\nThe largest share of these losses in larger capacitors is usually the frequency dependent ohmic dielectric losses. For smaller components, especially for wet electrolytic capacitors, conductivity of liquid electrolytes may exceed dielectric losses. To measure these losses, the measurement frequency must be set. Since commercially available components offer capacitance values cover 15 orders of magnitude, ranging from pF (10 F) to some 1000 F in supercapacitors, it is not possible to capture the entire range with only one frequency. IEC 60384-1 states that ohmic losses should be measured at the same frequency used to measure capacitance. These are:\n\n\nA capacitor's summarized resistive losses may be specified either as ESR, as a dissipation factor(DF, tan δ), or as quality factor (Q), depending on application requirements.\n\nCapacitors with higher ripple current formula_26 loads, such as electrolytic capacitors, are specified with equivalent series resistance ESR. ESR can be shown as an ohmic part in the above vector diagram. ESR values are specified in datasheets per individual type.\n\nThe losses of film capacitors and some class 2 ceramic capacitors are mostly specified with the dissipation factor tan δ. These capacitors have smaller losses than electrolytic capacitors and mostly are used at higher frequencies up to some hundred MHz. However the numeric value of the dissipation factor, measured at the same frequency, is independent on the capacitance value and can be specified for a capacitor series with a range of capacitance. The dissipation factor is determined as the tangent of the reactance (formula_27) and the ESR, and can be shown as the angle δ between imaginary and the impedance axis.\n\nIf the inductance formula_28 is small, the dissipation factor can be approximated as:\n\nCapacitors with very low losses, such as ceramic Class 1 and Class 2 capacitors, specify resistive losses with a quality factor (Q). Ceramic Class 1 capacitors are especially suitable for LC resonant circuits with frequencies up to the GHz range, and precise high and low pass filters. For an electrically resonant system, Q represents the effect of electrical resistance and characterizes a resonator's bandwidth formula_30 relative to its center or resonant frequency formula_31. Q is defined as the reciprocal value of the dissipation factor.\n\nA high Q value is for resonant circuits a mark of the quality of the resonance.\n\nA capacitor can act as an AC resistor, coupling AC voltage and AC current between two points. Every AC current flow through a capacitor generates heat inside the capacitor body. These dissipation power loss formula_33 is caused by formula_34 and is the squared value of the effective (RMS) current formula_35\n\nThe same power loss can be written with the dissipation factor formula_37 as\n\nThe internal generated heat has to be distributed to the ambient. The temperature of the capacitor, which is established on the balance between heat produced and distributed, shall not exceed the capacitors maximum specified temperature. Hence, the ESR or dissipation factor is a mark for the maximum power (AC load, ripple current, pulse load, etc.) a capacitor is specified for.\n\nAC currents may be a:\n\n\nRipple and AC currents mainly warms the capacitor body. By this currents internal generated temperature influences the breakdown voltage of the dielectric. Higher temperature lower the voltage proof of all capacitors. In wet electrolytic capacitors higher temperatures force the evaporation of electrolytes, shortening the life time of the capacitors. In film capacitors higher temperatures may shrink the plastic film changing the capacitor's properties.\n\nPulse currents, especially in metallized film capacitors, heat the contact areas between end spray (schoopage) and metallized electrodes. This may reduce the contact to the electrodes, heightening the dissipation factor.\n\nFor safe operation, the maximal temperature generated by any AC current flow through the capacitor is a limiting factor, which in turn limits AC load, ripple current, pulse load, etc.\n\nA \"ripple current\" is the RMS value of a superimposed AC current of any frequency and any waveform of the current curve for continuous operation at a specified temperature. It arises mainly in power supplies (including switched-mode power supplies) after rectifying an AC voltage and flows as charge and discharge current through the decoupling or smoothing capacitor. The \"rated ripple current\" shall not exceed a temperature rise of 3, 5 or 10 °C, depending on the capacitor type, at the specified maximum ambient temperature.\n\nRipple current generates heat within the capacitor body due to the ESR of the capacitor. The ESR, composed out of the dielectric losses caused by the changing field strength in the dielectric and the losses resulting out of the slightly resistive supply lines or the electrolyte depends on frequency and temperature. For ceramic and film capacitors in generally ESR decreases with increasing temperatures but heighten with higher frequencies due to increasing dielectric losses. For electrolytic capacitors up to roughly 1 MHz ESR decreases with increasing frequencies and temperatures.\n\nThe types of capacitors used for power applications have a specified rated value for maximum ripple current. These are primarily aluminum electrolytic capacitors, and tantalum as well as some film capacitors and Class 2 ceramic capacitors.\n\nAluminium electrolytic capacitors, the most common type for power supplies, experience shorter life expectancy at higher ripple currents. Exceeding the limit tends to result in explosive failure.\n\nTantalum electrolytic capacitors with solid manganese dioxide electrolyte are also limited by ripple current. Exceeding their ripple limits tends to shorts and burning components.\n\nFor film and ceramic capacitors, normally specified with a loss factor tan δ, the ripple current limit is determined by temperature rise in the body of approximately 10 °C. Exceeding this limit may destroy the internal structure and cause shorts.\n\nThe rated pulse load for a certain capacitor is limited by the rated voltage, the pulse repetition frequency, temperature range and pulse rise time. The \"pulse rise time\" formula_39, represents the steepest voltage gradient of the pulse (rise or fall time) and is expressed in volts per μs (V/μs).\n\nThe rated pulse rise time is also indirectly the maximum capacity of an applicable peak current formula_40. The peak current is defined as:\n\nwhere: formula_40 is in A; formula_11 in µF; formula_39 in V/µs\n\nThe permissible pulse current capacity of a metallized film capacitor generally allows an internal temperature rise of 8 to 10 K.\n\nIn the case of metallized film capacitors, pulse load depends on the properties of the dielectric material, the thickness of the metallization and the capacitor's construction, especially the construction of the contact areas between the end spray and metallized electrodes. High peak currents may lead to selective overheating of local contacts between end spray and metallized electrodes which may destroy some of the contacts, leading to increasing ESR.\n\nFor metallized film capacitors, so-called pulse tests simulate the pulse load that might occur during an application, according to a standard specification. IEC 60384 part 1, specifies that the test circuit is charged and discharged intermittently. The test voltage corresponds to the rated DC voltage and the test comprises 10000 pulses with a repetition frequency of 1 Hz. The pulse stress capacity is the pulse rise time. The rated pulse rise time is specified as 1/10 of the test pulse rise time.\n\nThe pulse load must be calculated for each application. A general rule for calculating the power handling of film capacitors is not available because of vendor-related internal construction details. To prevent the capacitor from overheating the following operating parameters have to be considered:\n\n\nHigher pulse rise times are permitted for pulse voltage lower than the rated voltage.\n\nExamples for calculations of individual pulse loads are given by many manufactures, e.g. WIMA and Kemet.\n\nAn AC load only can be applied to a non-polarized capacitor. Capacitors for AC applications are primarily film capacitors, metallized paper capacitors, ceramic capacitors and bipolar electrolytic capacitors.\n\nThe rated AC load for an AC capacitor is the maximum sinusoidal effective AC current (rms) which may be applied continuously to a capacitor within the specified temperature range. In the datasheets the AC load may be expressed as\n\nThe rated AC voltage for film capacitors is generally calculated so that an internal temperature rise of 8 to 10 °K is the allowed limit for safe operation. Because dielectric losses increase with increasing frequency, the specified AC voltage has to be derated at higher frequencies. Datasheets for film capacitors specify special curves for derating AC voltages at higher frequencies.\n\nIf film capacitors or ceramic capacitors only have a DC specification, the peak value of the AC voltage applied has to be lower than the specified DC voltage.\n\nAC loads can occur in AC motor run capacitors, for voltage doubling, in snubbers, lighting ballast and for power factor correction PFC for phase shifting to improve transmission network stability and efficiency, which is one of the most important applications for large power capacitors. These mostly large PP film or metallized paper capacitors are limited by the rated reactive power VAr.\n\nBipolar electrolytic capacitors, to which an AC voltage may be applicable, are specified with a rated ripple current.\n\nThe resistance of the dielectric is finite, leading to some level of DC \"leakage current\" that causes a charged capacitor to lose charge over time. For ceramic and film capacitors, this resistance is called \"insulation resistance R\". This resistance is represented by the resistor R in parallel with the capacitor in the series-equivalent circuit of capacitors.\nInsulation resistance must not be confused with the outer isolation of the component with respect to the environment.\n\nThe time curve of self-discharge over insulation resistance with decreasing capacitor voltage follows the formula\n\nWith stored DC voltage formula_46 and self-discharge constant\n\nThus, after formula_48 voltage formula_46 drops to 37% of the initial value.\n\nThe self-discharge constant is an important parameter for the insulation of the dielectric between the electrodes of ceramic and film capacitors. For example, a capacitor can be used as the time-determining component for time relays or for storing a voltage value as in a sample and hold circuits or operational amplifiers.\n\nClass 1 ceramic capacitors have an insulation resistance of at least 10 GΩ, while class 2 capacitors have at least 4 GΩ or a self-discharge constant of at least 100 s. Plastic film capacitors typically have an insulation resistance of 6 to 12 GΩ. This corresponds to capacitors in the uF range of a self-discharge constant of about 2000–4000 s.\n\nInsulation resistance respectively the self-discharge constant can be reduced if humidity penetrates into the winding. It is partially strongly temperature dependent and decreases with increasing temperature. Both decrease with increasing temperature.\n\nIn electrolytic capacitors, the insulation resistance is defined as leakage current.\n\nFor electrolytic capacitors the insulation resistance of the dielectric is termed \"leakage current\". This DC current is represented by the resistor R in parallel with the capacitor in the series-equivalent circuit of electrolytic capacitors. This resistance between the terminals of a capacitor is also finite. R is lower for electrolytics than for ceramic or film capacitors.\n\nThe leakage current includes all weak imperfections of the dielectric caused by unwanted chemical processes and mechanical damage. It is also the DC current that can pass through the dielectric after applying a voltage. It depends on the interval without voltage applied (storage time), the thermic stress from soldering, on voltage applied, on temperature of the capacitor, and on measuring time.\n\nThe leakage current drops in the first minutes after applying DC voltage. In this period the dielectric oxide layer can self-repair weaknesses by building up new layers. The time required depends generally on the electrolyte. Solid electrolytes drop faster than non-solid electrolytes but remain at a slightly higher level.\n\nThe leakage current in non-solid electrolytic capacitors as well as in manganese oxide solid tantalum capacitors decreases with voltage-connected time due to self-healing effects. Although electrolytics leakage current is higher than current flow over insulation resistance in ceramic or film capacitors, the self-discharge of modern non solid electrolytic capacitors takes several weeks.\n\nA particular problem with electrolytic capacitors is storage time. Higher leakage current can be the result of longer storage times. These behaviors are limited to electrolytes with a high percentage of water. Organic solvents such as GBL do not have high leakage with longer storage times.\n\nLeakage current is normally measured 2 or 5 minutes after applying rated voltage.\n\nAll ferroelectric materials exhibit piezoelectricity a piezoelectric effect. Because Class 2 ceramic capacitors use ferroelectric ceramics dielectric, these types of capacitors may have electrical effects called microphonics. Microphonics (microphony) describes how electronic components transform mechanical vibrations into an undesired electrical signal (noise). The dielectric may absorb mechanical forces from shock or vibration by changing thickness and changing the electrode separation, affecting the capacitance, which in turn induces an AC current. The resulting interference is especially problematic in audio applications, potentially causing feedback or unintended recording.\n\nIn the reverse microphonic effect, varying the electric field between the capacitor plates exerts a physical force, turning them into an audio speaker. High current impulse loads or high ripple currents can generate audible sound from the capacitor itself, draining energy and stressing the dielectric.\n\nDielectric absorption occurs when a capacitor that has remained charged for a long time discharges only incompletely when briefly discharged. Although an ideal capacitor would reach zero volts after discharge, real capacitors develop a small voltage from time-delayed dipole discharging, a phenomenon that is also called dielectric relaxation, \"soakage\" or \"battery action\".\n\nIn many applications of capacitors dielectric absorption is not a problem but in some applications, such as long-time-constant integrators, sample-and-hold circuits, switched-capacitor analog-to-digital converters, and very low-distortion filters, the capacitor must not recover a residual charge after full discharge, so capacitors with low absorption are specified.\nThe voltage at the terminals generated by the dielectric absorption may in some cases possibly cause problems in the function of an electronic circuit or can be a safety risk to personnel. In order to prevent shocks most very large capacitors are shipped with shorting wires that need to be removed before they are used.\n\nThe capacitance value depends on the dielectric material (ε), the surface of the electrodes (A) and the distance (d) separating the electrodes and is given by the formula of a plate capacitor:\n\nThe separation of the electrodes and the voltage proof of the dielectric material defines the breakdown voltage of the capacitor. The breakdown voltage is proportional to the thickness of the dielectric.\n\nTheoretically, given two capacitors with the same mechanical dimensions and dielectric, but one of them have half the thickness of the dielectric. With the same dimensions this one could place twice the parallel-plate area inside. This capacitor has theoretically 4 times the capacitance as the first capacitor but half of the voltage proof.\n\nSince the energy density stored in a capacitor is given by:\n\nthus a capacitor having a dielectric half as thick as another has 4 times higher capacitance but ½ voltage proof, yielding an equal maximum energy density.\n\nTherefore, dielectric thickness does not affect energy density within a capacitor of fixed overall dimensions. Using a few thick layers of dielectric can support a high voltage, but low capacitance, while thin layers of dielectric produce a low breakdown voltage, but a higher capacitance.\n\nThis assumes that neither the electrode surfaces nor the permittivity of the dielectric change with the voltage proof. A simple comparison with two existing capacitor series can show whether reality matches theory. The comparison is easy, because the manufacturers use standardized case sizes or boxes for different capacitance/voltage values within a series.\n\nIn reality modern capacitor series do not fit the theory. For electrolytic capacitors the sponge-like rough surface of the anode foil gets smoother with higher voltages, decreasing the surface area of the anode. But because the energy increases squared with the voltage, and the surface of the anode decreases lesser than the voltage proof, the energy density increases clearly. For film capacitors the permittivity changes with dielectric thickness and other mechanical parameters so that the deviation from the theory has other reasons.\n\nComparing the capacitors from the table with a supercapacitor, the highest energy density capacitor family. For this, the capacitor 25  F/2.3 V in dimensions D × H = 16 mm × 26 mm from Maxwell HC Series, compared with the electrolytic capacitor of approximately equal size in the table. This supercapacitor has roughly 5000 times higher capacitance than the 4700/10 electrolytic capacitor but ¼ of the voltage and has about 66,000 mWs (0.018 Wh) stored electrical energy, approximately 100 times higher energy density (40 to 280 times) than the electrolytic capacitor.\n\nElectrical parameters of capacitors may change over time during storage and application. The reasons for parameter changings are different, it may be a property of the dielectric, environmental influences, chemical processes or drying-out effects for non-solid materials.\n\nIn ferroelectric Class 2 ceramic capacitors, capacitance decreases over time. This behavior is called \"aging\". This aging occurs in ferroelectric dielectrics, where domains of polarization in the dielectric contribute to the total polarization. Degradation of polarized domains in the dielectric decreases permittivity and therefore capacitance over time. The aging follows a logarithmic law. This defines the decrease of capacitance as constant percentage for a time decade after the soldering recovery time at a defined temperature, for example, in the period from 1 to 10 hours at 20 °C. As the law is logarithmic, the percentage loss of capacitance will twice between 1 h and 100 h and 3 times between 1 h and 1,000 h and so on. Aging is fastest near the beginning, and the absolute capacitance value stabilizes over time.\n\nThe rate of aging of Class 2 ceramic capacitors depends mainly on its materials. Generally, the higher the temperature dependence of the ceramic, the higher the aging percentage. The typical aging of X7R ceramic capacitors is about 2.5% per decade. The aging rate of Z5U ceramic capacitors is significantly higher and can be up to 7% per decade.\n\nThe aging process of Class 2 ceramic capacitors may be reversed by heating the component above the Curie point.\n\nClass 1 ceramic capacitors and film capacitors do not have ferroelectric-related aging. Environmental influences such as higher temperature, high humidity and mechanical stress can, over a longer period, lead to a small irreversible change in the capacitance value sometimes called aging, too.\n\nThe change of capacitance for P 100 and N 470 Class 1 ceramic capacitors is lower than 1%, for capacitors with N 750 to N 1500 ceramics it is ≤ 2%. Film capacitors may lose capacitance due to self-healing processes or gain it due to humidity influences. Typical changes over 2 years at 40 °C are, for example, ±3% for PE film capacitors and ±1% PP film capacitors.\n\nElectrolytic capacitors with non-solid electrolyte age as the electrolyte evaporates. This evaporation depends on temperature and the current load the capacitors experience. Electrolyte escape influences capacitance and ESR. Capacitance decreases and the ESR increases over time. In contrast to ceramic, film and electrolytic capacitors with solid electrolytes, \"wet\" electrolytic capacitors reach a specified \"end of life\" reaching a specified maximum change of capacitance or ESR. End of life, \"load life\" or \"lifetime\" can be estimated either by formula or diagrams or roughly by a so-called \"10-degree-law\". A typical specification for an electrolytic capacitor states a lifetime of 2,000 hours at 85 °C, doubling for every 10 degrees lower temperature, achieving lifespan of approximately 15 years at room temperature.\n\nSupercapacitors also experience electrolyte evaporation over time. Estimation is similar to wet electrolytic capacitors. Additional to temperature the voltage and current load influence the life time. Lower voltage than rated voltage and lower current loads as well as lower temperature extend the life time.\n\nCapacitors are reliable components with low failure rates, achieving life expectancies of decades under normal conditions. Most capacitors pass a test at the end of production similar to a \"burn in\", so that early failures are found during production, reducing the number of post-shipment failures.\n\nReliability for capacitors is usually specified in numbers of Failures In Time (FIT) during the period of constant random failures. FIT is the number of failures that can be expected in one billion (10) component-hours of operation at fixed working conditions (e.g. 1000 devices for 1 million hours, or 1 million devices for 1000 hours each, at 40 °C and 0.5 U). For other conditions of applied voltage, current load, temperature, mechanical influences and humidity the FIT can recalculated with terms standardized for industrial or military contexts.\n\nCapacitors may experience changes to electrical parameters due to environmental influences like soldering, mechanical stress factors (vibration, shock) and humidity. The greatest stress factor is soldering. The heat of the solder bath, especially for SMD capacitors, can cause ceramic capacitors to change contact resistance between terminals and electrodes; in film capacitors, the film may shrink, and in wet electrolytic capacitors the electrolyte may boil. A recovery period enables characteristics to stabilize after soldering; some types may require up to 24 hours. Some properties may change irreversibly by a few per cent from soldering.\n\nElectrolytic capacitors with non-solid electrolyte are \"aged\" during manufacturing by applying rated voltage at high temperature for a sufficient time to repair all cracks and weaknesses that may have occurred during production. Some electrolytes with a high water content react quite aggressively or even violently with unprotected aluminum. This leads to a \"storage\" or \"disuse\" problem of electrolytic capacitors manufactured before the 1980s. Chemical processes weaken the oxide layer when these capacitors are not used for too long. New electrolytes with \"inhibitors\" or \"passivators\" were developed during the 1980s to solve this problem.\nAs of 2012 the standard storage time for electronic components of two years at room temperature substantiates (cased) by the oxidation of the terminals will be specified for electrolytic capacitors with non-solid electrolytes, too. Special series for 125 °C with organic solvents like GBL are specified up to 10 years storage time ensure without pre-condition the proper electrical behavior of the capacitors.\n\nFor antique radio equipment, \"pre-conditioning\" of older electrolytic capacitors may be recommended. This involves applying the operating voltage for some 10 minutes over a current limiting resistor to the terminals of the capacitor. Applying a voltage through a safety resistor repairs the oxide layers.\n\nThe tests and requirements to be met by capacitors for use in electronic equipment for approval as standardized types are set out in the generic specification IEC/EN 60384-1 in the following sections.\n\nGeneric specification\nCeramic capacitors\nFilm capacitors\nElectrolytic capacitors\nSupercapacitors\n\nCapacitors, like most other electronic components and if enough space is available, have imprinted markings to indicate manufacturer, type, electrical and thermal characteristics, and date of manufacture. If they are large enough the capacitor is marked with:\n\n\nPolarized capacitors have polarity markings, usually \"−\" (minus) sign on the side of the negative electrode for electrolytic capacitors or a stripe or \"+\" (plus) sign, see #Polarity marking. Also, the negative lead for leaded \"wet\" e-caps is usually shorter.\n\nSmaller capacitors use a shorthand notation. The most commonly used format is: XYZ J/K/M VOLTS V, where XYZ represents the capacitance (calculated as XY × 10 pF), the letters J, K or M indicate the tolerance (±5%, ±10% and ±20% respectively) and VOLTS V represents the working voltage.\n\nExamples:\n\n\nCapacitance, tolerance and date of manufacture can be indicated with a short code specified in IEC/EN 60062. Examples of short-marking of the rated capacitance (microfarads): µ47 = 0,47 µF, 4µ7 = 4,7 µF, 47µ = 47 µF\n\nThe date of manufacture is often printed in accordance with international standards.\n\n\nFor very small capacitors like MLCC chips no marking is possible. Here only the traceability of the manufacturers can ensure the identification of a type.\n\n Capacitors do not use color coding.\n\nAluminum e-caps with \"non-solid\" electrolyte have a polarity marking at the cathode (\"minus\") side. Aluminum, tantalum, and niobium e-caps with \"solid\" electrolyte have a polarity marking at the anode (\"plus\") side. Supercapacitors are marked at the \"minus\" side.\nDiscrete capacitors today are industrial products produced in very large quantities for use in electronic and in electrical equipment. Globally, the market for fixed capacitors was estimated at approximately US$18 billion in 2008 for 1,400 billion (1.4 × 10) pieces. This market is dominated by ceramic capacitors with estimate of approximately one trillion (1 × 10) items per year.\n\nDetailed estimated figures in value for the main capacitor families are:\n\n\nAll other capacitor types are negligible in terms of value and quantity compared with the above types.\n\n\n"}
{"id": "9504741", "url": "https://en.wikipedia.org/wiki?curid=9504741", "title": "Copiotroph", "text": "Copiotroph\n\nA copiotroph is an organism found in environments rich in nutrients, particularly carbon. They are the opposite to oligotrophs, which survive in much lower carbon concentrations.\n\nCopiotrophic organisms tend to grow in high organic substrate conditions. For example, copiotrophic organisms grow in Sewage lagoons. They grow in organic substrate conditions up to 100x higher than oligotrophs.\n"}
{"id": "807570", "url": "https://en.wikipedia.org/wiki?curid=807570", "title": "Cryogenic fuel", "text": "Cryogenic fuel\n\nCryogenic fuels are fuels that require storage at extremely low temperatures in order to maintain them in a liquid state. These fuels are used in machinery that operates in space (e.g. rocket ships and satellites) because ordinary fuel cannot be used there, due to absence of an environment that supports combustion (on Earth, oxygen is abundant in the atmosphere, whereas in human-explorable space, oxygen is virtually non-existent) and space is a vacuum. Cryogenic fuels most often constitute liquefied gases such as liquid hydrogen.\n\nSome rocket engines use regenerative cooling, the practice of circulating their cryogenic fuel around the nozzles before the fuel is pumped into the combustion chamber and ignited. This arrangement was first suggested by Eugen Sänger in the 1940s. The Saturn V rocket that sent the first manned missions to the Moon used this design element, which is still in use today.\n\nQuite often, liquid oxygen is mistakenly called cryogenic \"fuel\", though it is actually an oxidizer and not a fuel.\n\nRussian aircraft manufacturer Tupolev developed a version of its popular Tu-154 design but with a cryogenic fuel system, designated the Tu-155. Using a fuel referred to as liquefied natural gas (LNG), its first flight was in 1989.\n\nCryogenic fuels can be placed into two categories: inert and flammable or combustible. Both types exploit the large liquid to gas volume ratio that occurs when liquid transitions to gas phase. The feasibility of cryogenic fuels is associated with what is known as a high mass flow rate. With regulation, the high-density energy of cryogenic fuels is utilized to produce thrust in rockets and controllable consumption of fuel. The following sections provide further detail.\n\nThese types of fuels typically use the regulation of gas production and flow to power pistons in an engine. The large increases in pressure are controlled and directed toward the engine's pistons. The pistons move due to the mechanical power transformed from the monitored production of gaseous fuel. A notable example can be seen in Peter Dearman's liquid air vehicle. Some common inert fuels include:\n\nThese fuels utilize the beneficial liquid cryogenic properties along with the flammable nature of the substance as a source of power. These types of fuel are well known primarily for their use in rockets including the Intercontinental ballistic missile. Some common combustible fuels include:\n\nCombustible cryogenic fuels offer much more utility than most inert fuels can. Liquefied natural gas, as with any fuel, will only combust when properly mixed with right amounts of air. As for LNG, the bulk majority of efficiency depends on the methane number, which is the gas equivalent of the octane number. This is determined based on the methane content of the liquefied fuel and any other dissolved gas, and varies as a result of experimental efficiencies. Maximizing efficiency in combustion engines will be a result of determining the proper fuel to air ratio and utilizing the addition other hydrocarbons for added optimal combustion.\n\nGas liquefying processes have been improving over the past decades with the advent of better machinery and control of system heat losses. Typical techniques take advantage of the temperature of the gas dramatically cooling as the controlled pressure of a gas is released. Enough pressurization and then subsequent depressurization can liquefy most gases, as exemplified by the Joule-Thomson effect.\n\nWhile it is cost effective to liquefy natural gas for storage, transport, and use, roughly 10 to 15 percent of the gas gets consumed during the process. The optimal process contains four stages of propane refrigeration and two stages of ethylene refrigeration. There can be the addition of an additional refrigerant stage, but the additional costs of equipment are not economically justifiable. Efficiency can be tied to the pure component cascade processes which minimize the overall source to sink temperature difference associated with refrigerant condensing. The optimized process incorporates optimized heat recovery along with the use of pure refrigerants. All process designers of liquefaction plants using proven technologies face the same challenge: to efficiently cool and condense a mixture with a pure refrigerant. In the optimized Cascade process, the mixture to be cooled and condensed is the feed gas. In the propane mixed refrigerant processes, the two mixtures requiring cooling and condensing are the feed gas and the mixed refrigerant. The chief source of inefficiency lies in the heat exchange train during the liquefaction process.\n\n\n\n"}
{"id": "55641006", "url": "https://en.wikipedia.org/wiki?curid=55641006", "title": "Cyaphide", "text": "Cyaphide\n\nCyaphide, P≡C, is the phosphorus analogue of cyanide. It is not known as a discrete salt, however, \"In silico\" measurements reveal that the -1 charge in this ion is location mainly on carbon (0.65), as opposed to phosphorus. \n\nOrganometallic complexes of cyaphide were first reported in 1992. More recent preparations use two other routes:\n\nTreatment of the ɳ-coordinated phosphaalkyne complex \"trans\"-[RuH(P≡CSiPh)(dppe)] with an alkoxide resulted in desilylation, followed by subsequent rearrangement to the corresponding carbon-bound cyaphide complex. Cyaphide-alkynyl complexes are prepared similarly.\n\nAn actinide cyaphide complex can be prepared by C-O bond cleavage of the phosphaethynolate anion, the phosphorus analogue of cyanate. Reaction of the uranium complex [((ArO)N)U(DME)] with [Na(OCP)(dioxane)] in the presence of 2.2.2-cryptand results in the formation of a dinuclear, oxo-bridged uranium complex featuring a C≡P ligand.\n\n"}
{"id": "2602098", "url": "https://en.wikipedia.org/wiki?curid=2602098", "title": "Degree of ionization", "text": "Degree of ionization\n\nThe degree of ionization (also known as \"ionization yield\" in the literature) refers to the proportion of neutral particles, such as those in a gas or aqueous solution, that are ionized to charged particles. For electrolytes, it could be understood as a capacity of acid/base to ionize itself. A low degree of ionization is sometimes called \"partially ionized\" (also \"weakly ionized\"), and a high degree of ionization as \"fully ionized\".\n\nIonization refers to the process whereby an atom or molecule loses one or several electrons from its atomic orbital, or conversely gains an additional one, from an incoming free electron (electron attachement). In both cases, the atom or molecule is no more a neutral particle and becomes a charge carrier. If the species has lost one or several electrons, it becomes positively charged and is called a positive ion, or cation. On the contrary, if the species has gained one or several additional electrons, it becomes negatively charged and is called a negative ion, or anion. Individual free electrons and ions in a plasma have very short lives typically inferior to the microsecond, as ionization and recombination, excitation and relaxation are collective continuous processes.\n\nThe degree of dissociation \"α\" (also known as degree of ionization), is a way of representing the strength of an acid. It is defined as the ratio of the number of ionized molecules and the number of molecules dissolved in water. It can be represented as a decimal number or as a percentage. One can classify strong acids as those having ionization degrees above 30%, weak acids as those with \"α\" below 30%, and the rest as moderate acids, at a specified molar concentration.\n\nIn gases, the degree of ionization formula_1 refers to the proportion of neutral particles that are ionized into charged particles:\n\nwhere formula_3 is the ion density and formula_4 the neutral density (in particles per cubic meter). It is a dimensionless number, most of the time expressed as a percentage.\n\nIn a plasma, the electron-ion collision frequency formula_5 is much greater than the electron-neutral collision frequency formula_6. Therefore, with a weak degree of ionization formula_1, the electron-ion collision frequency can equal the electron-neutral collision frequency: formula_8 is the limit separating a plasma from being partially or fully ionized.\n\nThe term \"fully ionized gas\" introduced by Lyman Spitzer does not mean the degree of ionization is unity, but only that the plasma is in a \"Coulomb-collision dominated regime\", i.e. when formula_9, which can correspond to a degree of ionization as low as 0.01%.\n\nA \"partially\" or \"weakly ionized gas\" means the plasma is not dominated by Coulomb collisions, i.e. when formula_10.\n\nA particular case of fully ionizes gases are very hot thermonuclear plasmas, such as plasmas artificially produced in nuclear explosions or naturally formed in our Sun and all stars in the universe. Stars contain largely hydrogen and helium gases, that are \"fully ionized\" into electrons, protons (H) and helium ions (He). Equations to characterize such very hot, fully ionized thermonuclear plasmas in the presence of stellar magnetic fields can be approximated to ideal magnetohydrodynamics laws with high magnetic Reynolds number.\n\nIonized matter was first identified in a discharge tube (or Crookes tube), and so described by Sir William Crookes in 1879 (he called it \"radiant matter\"). The nature of the Crookes tube \"cathode ray\" matter was subsequently identified by English physicist Sir J.J. Thomson in 1897, and dubbed \"plasma\" by Irving Langmuir in 1928, perhaps because it reminded him of a blood plasma.\n\n"}
{"id": "1823952", "url": "https://en.wikipedia.org/wiki?curid=1823952", "title": "Diffusion capacitance", "text": "Diffusion capacitance\n\nDiffusion Capacitance is the capacitance due to transport of charge carriers between two terminals of a device, for example, the diffusion of carriers from anode to cathode in forward bias mode of a diode or from emitter to baseforward-biased junction for a transistor. In a semiconductor device with a current flowing through it (for example, an ongoing transport of charge by diffusion) at a particular moment there is necessarily some charge in the process of transit through the device. If the applied voltage changes to a different value and the current changes to a different value, a different amount of charge will be in transit in the new circumstances. The change in the amount of transiting charge divided by the change in the voltage causing it is the diffusion capacitance. The adjective \"diffusion\" is used because the original use of this term was for junction diodes, where the charge transport was via the diffusion mechanism. See Fick's laws of diffusion.\n\nTo implement this notion quantitatively, at a particular moment in time let the voltage across the device be formula_1. Now assume that the voltage changes with time slowly enough that at each moment the current is the same as the DC current that would flow at that voltage, say formula_2 (the \"quasistatic approximation\"). Suppose further that the time to cross the device is the forward transit time formula_3. In this case the amount of charge in transit through the device at this particular moment, denoted formula_4, is given by\n\nConsequently, the corresponding diffusion capacitance:formula_6. is\n\nIn the event the quasi-static approximation does not hold, that is, for very fast voltage changes occurring in times shorter than the transit time formula_8, the equations governing time-dependent transport in the device must be solved to find the charge in transit, for example the Boltzmann equation. That problem is a subject of continuing research under the topic of non-quasistatic effects. See Liu \n, and Gildenblat \"et al.\"\n\n"}
{"id": "1648512", "url": "https://en.wikipedia.org/wiki?curid=1648512", "title": "Downcycling", "text": "Downcycling\n\nDowncycling, or cascading, is the recycling of waste where the recycled material is of lower quality and functionality than the original material. Often, this is due the accumulation of tramp elements in secondary metals, which may exclude the latter from high-quality applications. For example, steel scrap from end-of-life vehicles is often contaminated with copper from wires and tin from coating. This contaminated scrap yields a secondary steel that does not meet the specifications for automotive steel and therefore, it is mostly applied in the construction sector.\n\nDowncycling can help to keep materials in use, reduce consumption of raw materials, and avoid the energy usage, greenhouse gas emissions, air pollution, and water pollution of primary production and resource extraction. \nThe term \"downcycling\" was first used by Reiner Pilz in an interview by Thornton Kay in SalvoNEWS in 1994. We talked about the impending EU Demolition Waste Streams directive. \"Recycling, he said, \"I call it downcycling. They smash bricks, they smash everything. What we need is upcycling where old products are given more value not less.\" He despairs of the German situation and recalls the supply of a large quantity of reclaimed woodblock from an English supplier for a contract in Nuremberg while just down the road a load of similar blocks was scrapped. It was a pinky looking aggregate with pieces of handmade brick, old tiles and discernible parts of useful old items mixed with crushed concrete. Is this the future for Europe?\nThe term \"downcycling\" was also used by William McDonough and Michael Braungart in their 2002 book \"\".\n\nDowncycling is related to but different from 'open-loop recycling'. While downcycling implies quality loss the term open-loop recycling denotes a situation where the secondary material is used in a different product system than the original material; it thus comprises both upcycling and downcycling. A detailed discussion on the relation between downcycling, open loop recycling and their environmental impact is provided by Geyer et al. (2015). They write that \"Poor product design and EOL management can lead to recycled materials of poor quality, which, in turn, limits the applications these materials can be used in.\" They also argue that \"closed-loop recycling neither intrinsically displaces more primary material owing to multiple loops (quantity argument) nor per se generates higher environmental benefits on a unit basis (quality argument).\" The reason for their argument lies in the necessity to include the product system of the target application, in which the recycled material is used or not, into the assessment of overall primary material demand and environmental impact.\n\n"}
{"id": "28367985", "url": "https://en.wikipedia.org/wiki?curid=28367985", "title": "Dulcinea Solar Plant", "text": "Dulcinea Solar Plant\n\nThe Dulcinea Solar Plant () is a photovoltaic power station in Cuenca, Spain. It consists of 300 photovoltaic generating units with a total capacity of . The solar power station covers area of . It is equipped with 82,896 Kyocera KC-200-GHT2 photovoltaic modules, 6,078 Kyocera KD-210-GHP2 modules, and 66,286 Suntech STP-210/18Ud modules. 6,600 strings of 24 photovoltaic panels linked in series 300 SMA SC100-Outdoor solar inverters. The estimated available radiation of 1,810 kWh/m2 per year is 1,497 peak sunlight hours.\n\n"}
{"id": "13702071", "url": "https://en.wikipedia.org/wiki?curid=13702071", "title": "Energy Matters", "text": "Energy Matters\n\nEnergy Matters was the title of a magazine published by students at the University of Cambridge between November 1980 and June 1984. Its objective, outlined in the editorial to the first edition, was to provide facts, details and opinions relating to energy, in a way accessible to interested students.\n\n\"Energy Matters\" was notable in a number ways. Its dispassionate and technical approach to this controversial topic was possibly unprecedented at a British University, at a time when many student publications were highly partisan on the issue. It was an independent undergraduate student magazine wholly funded (initially) by a University department, an unusual and possibly unique arrangement in the UK. It received endorsement from the British Royal Family, a significant and unusual gesture at the time for a magazine dealing with political matters. It was where a number of subsequently famous journalists and academics showed their first public work.\n\nThe magazine was founded at a time of high public interest in energy issues, due to the second oil crisis and the Three Mile Island accident the previous year, and to a wave of public interest in energy conservation stimulated by environmental concerns. The topic had become highly political; for example a debate took place in the Cambridge Union Society (the University's debating club) in November 1980 under the title \"This House Believes Britain's Dependence on Nuclear Power will Result in Disaster\", opposed by Bernard Jenkin.\n\nIn this context, the magazine was founded by two undergraduates at the Cambridge University Engineering Department, Richard Davies and Andrew Bud. They secured funding from the Engineering Department which agreed to supply materials and printing services. The first edition ran to 52 pages, and 500 copies were distributed free throughout the University immediately after the Union Society debate. Thanks to the support of one of the Department's professors, Sir William Hawthorne, an expert in energy conservation, the magazine carried an introduction specially provided by HRH the Duke of Edinburgh (see image).\n\nThe magazine was well received and a team of volunteer student journalists created the second edition, published in April 1981, catalogued as . This ran to 100 pages, and the printing of 1000 copies entirely blocked the Engineering Department's reprographic department for three weeks, driving the in-house printer to the edge of despair. Regretfully, the Department withdrew its logistical support.\n\nAs a result, commercial sponsorship was sought and successfully raised from the energy industry. Funding came from the oil industry, the electricity and nuclear sectors and several University departments, and the magazine was commercially printed. Under new editors Helen Field and Anne Pudsey-Dawson, the third edition appeared in November 1981.\n\nAlthough interest in the magazine amongst students beyond the science and engineering faculties was limited, it attracted considerable attention in Whitehall and within the industry. Dozens of copies were requested by these institutions. As a result, the magazine's editors were able to secure interviews with cabinet ministers and heads of the nationalised energy industries and trades unions. As a spin-off, a number of branded seminars were organised at the University, featuring leaders of the energy industry.\n\nThe magazine's editorial line was rigorously neutral, and substantial space was given to the thoughts of leading anti-nuclear campaigners, proponents of conservation and evangelists of combined heat and power.\n\nBy 1984, the magazine's editor Roger Tredre merged it with the activities of the Cambridge student magazine Cantab, and its main function became to generate advertising revenue to cross-subsidise Cantab. It ceased publication after its seventh edition in summer 1984, its final editors being Richard Penty (now Professor of Photonics at the University of Cambridge) and John Crowther.\n\nAmongst those who worked on the magazine a number subsequently became influential in the fields of journalism or energy policy. The 1983 edition was edited by Vanessa Houlder (currently a leading business journalist on the Financial Times of London), and included contributions from Mike Grubb (now a senior academic working for the Carbon Trust).\n\n"}
{"id": "11648886", "url": "https://en.wikipedia.org/wiki?curid=11648886", "title": "Eugene Green Energy Standard", "text": "Eugene Green Energy Standard\n\nThe Eugene Green Energy Standard was an international standard to which national or international green electricity labelling schemes could be accredited to confirm that they provide genuine environmental benefits. It was designed to encourage the generation and use of additional renewable energy sources for electricity generation, although the limited use of additional natural gas-fired cogeneration plant was also supported. Initially funded in part through the EU's clean-e programme, but also including some participants from outside Europe, the Eugene standard was formally discontinued after February 2009. \n\nOn 2 February 2009, EUGENE’s General Assembly voted in favour of its dismantling. It was agreed that the members and board would continue to work together to promote green energy in Europe, but that the EUGENE standard and the association (ASBL) under Belgian law would cease to exist.\n\nThe standard confirmed that energy supplied under the accredited schemes:\n\nTwo variations of the standard, 'gold' and 'silver', differentiated between schemes depending on the additionality of new renewable energy supplied. The development of the standard was aided by the European Union's CLEAN-E initiative during 2005 and 2006\n\nNational energy labels formerly accredited by EUGENE included:\n\nThe organisation also recommends certain other national schemes that are progressing towards accreditation, including:\n\nThere was no Eugene accredited scheme in the United Kingdom. The UK's energy regulator (Ofgem) published guidelines in February 2009 that broadly followed the EUGENE standard, requiring matching and additionality through carbon savings, although allowing the latter to be met through energy efficiency or offsetting, as well as through new renewables schemes. These were implemented in February 2010 in the UK's Green Energy Supply Certification Scheme.\n\nThe Eugene Standard had also been adopted in Chile, while a pilot scheme is in progress in France.\n\nThe standard was managed by the Eugene Network (formerly the European Green Electricity Network), an international membership-based non-profit organization. The Network aimed to coordinate and harmonise green energy labelling nationally and internationally, promote the adoption of the Eugene Standard as the basis for national and international green energy markets, and encourage consumers and suppliers to choose credible green energy products. Formal discussions on the Eugene standard first took place in 2000, led by the World Wide Fund for Nature, and it was officially launched on June 24, 2002. The Eugene Network was legally established in 2003 and the first national energy labels were accredited in 2004.\n\nFull voting membership of the Network was open to 'citizen organisations pursuing not for profit activities with the objectives of promoting green electricity but with no direct interests in the generation and supply of energy services'. Organisations outside this scope but which do 'have a commitment and interest in creating a viable green energy market' were able to become non-voting associate members or supporters.\n\nAs of June 2007, the members of the Eugene Network were:\n\n\n\n"}
{"id": "32671103", "url": "https://en.wikipedia.org/wiki?curid=32671103", "title": "Federal Energy Administration", "text": "Federal Energy Administration\n\nThe Federal Energy Administration (FEA) was a United States government organization created in 1974 to address the 1970s energy crisis, and specifically the 1973 oil crisis. It was merged in 1977 with the newly created United States Department of Energy.\n\nIn 1973, the Organization of Petroleum Exporting Countries placed an oil embargo on nations perceived as assisting Israel in the Yom Kippur War. To combat the embargo, President Nixon established the Federal Energy Office (FEO) in December 1973, which was tasked with coordinating the American response to the embargo. In June 1974, the FEO was superseded by the FEA under the \"Federal Energy Administration Act\" of 1974 and . The FEA was tasked with managing fuel allocation, pricing regulation, and energy data collection and analysis. The Energy Research and Development Administration (ERDA) was created by the Energy Reorganization Act of 1974 and managed the energy research and development, nuclear weapons, and naval reactors programs.\n\nIn December 1975, Energy Policy and Conservation Act directed the FEA to change petroleum pricing regulations such that crude oil prices would rise gradually. Additionally, subject to congressional review, the FEA could now remove refined petroleum products from pricing controls. By June 1967, fuel oil, middle distillates, naphtha, and gas oils were no longer under pricing controls.\n\nThe Federal Energy Administration Act created the first U.S. agency with the primary focus on energy and mandated it to collect, assemble, evaluate, and analyze energy information. It also provided FEA with data collection enforcement authority for gathering data from energy producing and major consuming firms. Section 52 of the FEA Act mandated establishment of the National Energy Information System to \"contain such energy information as is necessary to carry out the Administration’s statistical and forecasting activities.\"\n\nThe Department of Energy Organization Act of 1977 created the United States Department of Energy (USDOE), which merged ERDA and FEA under USDOE. It also created the Energy Information Administration as the primary Federal Government authority on energy statistics and analysis.\n\n\n\n\n\nAndrew Gibson was nominated to succeed Sawhill but was withdrawn before the Senate had a chance to act on it.\n\n"}
{"id": "837880", "url": "https://en.wikipedia.org/wiki?curid=837880", "title": "Fire clay", "text": "Fire clay\n\nFire clay is a range of refractory clays used in the manufacture of ceramics, especially fire brick. The United States Environmental Protection Agency defines fire clay very generally as a \"mineral aggregate composed of hydrous silicates of aluminium (AlO·2SiO·2HO) with or without free silica.\"\n\nHigh-grade fire clays can withstand temperatures of 1,775 °C (3,227 °F), but to be referred to as a \"fire clay\" the material must withstand a minimum temperature of . Fire clays range from \"flint clays\" to \"plastic fire clays\", but there are \"semi-flint\" and \"semi-plastic\" fire clays as well. Fire clays consist of natural argillaceous materials, mostly Kaolinite group clays, along with fine-grained micas and quartz, and may also contain organic matter and sulphur compounds.\n\nFire clay is resistant to high temperatures, having fusion points higher than ; therefore it is suitable for lining furnaces, as fire brick, and for manufacture of utensils used in the metalworking industries, such as crucibles, saggars, retorts and glassware. Because of its stability during firing in the kiln, it can be used to make complex items of pottery such as pipes and sanitary ware.\n\nThe chemical composition typical for fire clays are 23-34% AlO, 50-60% SiO and 6-27% loss on ignition together with various amounts of FeO, CaO, MgO, KO, NaO and TiO. Chemical analyses from two 19th-century sources, shown in table below, are somewhat lower in alumina\nalthough a more contemporary source quotes analyses that are closer.\n\nUnlike conventional brick-making clay, some fire clays (especially flint clays) are mined at depth, found as a seatearth, the underclay associated with coal measures.\n"}
{"id": "10153961", "url": "https://en.wikipedia.org/wiki?curid=10153961", "title": "Friction modifier", "text": "Friction modifier\n\nFriction modifiers are added to lubricants in order to reduce friction and wear in machine components. They are particularly important in the boundary lubrication regime, where they can prevent solid surfaces from coming into direct contact, substantially reducing friction and wear.\n\nSeveral classes of friction modifier additives exist, the main examples being organic friction modifiers (OFMs), oil-soluble organo-molybdenum additives, functionalized polymers, and dispersed nanoparticles. \nReduction of frictional losses and through more efficient lubrication is a key target in order to reduce carbon dioxide emissions. One approach has been to progressively reduce lubricant viscosity to minimize hydrodynamic shear, churning and pumping losses. However, this means that an increased number of components operate under boundary lubrication conditions. This has led to a resurgence in interest in friction modifier additives, particularly OFMs. For example, recent tribology experiments and molecular dynamics simulations have given new insights into their behaviour under boundary lubrication conditions.\n\n"}
{"id": "43371588", "url": "https://en.wikipedia.org/wiki?curid=43371588", "title": "Gandao Dam", "text": "Gandao Dam\n\nGandao Dam is a gravity dam under construction near town of Ghalanai in Mohmand Agency of FATA, Pakistan.\n\nConstruction of dam started in January, 2013 and is expected to complete in 2015 with projected cost of PKR 449 Millions. The dam was attacked by militants in September 2013 and construction did not resume until May 2014.\nThe dam has a height of 105 feet and lengths 250 feet. It will have total water storage capacity of around 810 acre feet.\n\n"}
{"id": "15370837", "url": "https://en.wikipedia.org/wiki?curid=15370837", "title": "Global Warming: The Signs and The Science", "text": "Global Warming: The Signs and The Science\n\nGlobal Warming: The Signs and The Science is a 2005 documentary film on global warming made by ETV, the PBS affiliate in South Carolina, and hosted by Alanis Morissette. The documentary examines the science behind global warming and pulls together segments filmed in the United States, Asia and South America and shows how people in these different locales are responding in different ways to the challenges of global warming to show some of the ways that the world can respond.\n\n"}
{"id": "2476425", "url": "https://en.wikipedia.org/wiki?curid=2476425", "title": "Gérard Mestrallet", "text": "Gérard Mestrallet\n\nGérard Mestrallet (born 1 April 1949 in) is the chairman of the board of directors of Engie and former CEO (from July 2008 to May 2016). He is also the chairman of Suez.\n\nHe received degrees from Sciences Po Toulouse, the Ecole polytechnique, École nationale de l'aviation civile and the École nationale d'administration.\n\nAfter his studies, he joined the Treasury. From September 1982 and July 1984 he was the technical advisor in charge of industrial affairs under the Minister of Economics and Finance (during the time of Jacques Delors)\n\nIn 1984, he started working as a special advisor for Suez, known today just as Suez. He has been its Chairman and CEO since 2001.He was named Deputy Delegate General for Industrial Affairs in June 1986. In July 1987, he was General Director of the European Court of Human Rights, an affiliate of the Compagnie financière de Suez. In January 1991 he started as the Deputy Director General of the company, but a month later he became the Managing Director and Chairman of the Management Committee of the Société General de Belgique, to ensure its refocusing and development.\n\nHe was appointed as a member of the Hong Kong Chief Executive's Council of International Advisers in the years of 1998-2005. He is the President of . He is also the President of the Fondation Agir Contre l'Exclusion (FACE).\n\nIn 1995 he took a position as the head of Suez and since then he has worked for Suez while focusing on three businesses: energy, water, and waste He was also the instigator of the reconciliation between Suez and the Lyonnaise des Eaux. The Suez-Lyonnaise des Eaux was started in 1997, at which time Mestrallet became the chairman of its executive board while Jérôme Monod, chairman of Lyonnaise, became the Chairman of the Supervisory Board. However, the group soon went back to its original name, Suez and in 2003 its leaders made a proposition to shareholders to change the association of the company by becoming an LP with a board of directors and Gerard Mestrallet as the CEO.\n\nin July 2008, he was appointed CEO of Engie.\n\nIn 2014, his retirement cap of 21 million euros was a controversy in the media due to GDF-Suez’s record losses for 2013 and its effort to prepare a savings plan.\n\nOn May 3, 2016, he became chairman of the board of directors of the group, with Isabelle Kocher appointed as CEO, as planned for months\n\nHe also serves on the Boards of International Power, Aguas de Barcelona, Electrabel, Saint-Gobain, and Pargesa Holding S.A..\n\n\nMestrallet reached 3,005,079 in remuneration in 2012 (down 2.7% from 2011). It was the tenth compensation from SBF 120 and the eighth from CAC 40.\n\nIn May 2016, Engie announced that Gérard Mestrallet's 350,000 euro chairman salary will go to the Engie foundation charity and that he will carry out his new job for free.\n\nHe is Commander of the National Order of the Legion of Honour and is a Knight of the National Order of Merit.\n\nHe is the son of Georges Mestrallet and Paule Mestrallet-Besnard. He has two brothers, Michael and Patrick Mestrallet. He married Joëlle Emillienne Renée Arcens in 1974 and has three children.\n"}
{"id": "47507667", "url": "https://en.wikipedia.org/wiki?curid=47507667", "title": "Institut Néel", "text": "Institut Néel\n\nInstitut Néel is a research laboratory in condensed matter physics located on Polygone Scientifique in Grenoble, France. It is named after scientist Louis Néel.\n\nThe institute is an independent research unit (UPR2940) of the French Centre national de la recherche scientifique created in 2007 as a reorganization of four research laboratories: the center for research in very low temperatures (Centre de Recherches sur les très basses températures (CRTBT)), the laboratory for the study of electronic properties of solids (laboratoire d’étude des propriétés électroniques des solides (LEPES)), the Louis Néel laboratory (laboratoire Louis Néel (LLN)), and the Laboratory of crystallography (Laboratoire de cristallographie (LdC)).\n\n"}
{"id": "2265029", "url": "https://en.wikipedia.org/wiki?curid=2265029", "title": "Ion wind", "text": "Ion wind\n\nIon wind, ionic wind, coronal wind or electric wind is the airflow induced by electrostatic forces linked to corona discharge arising at the tips of some sharp conductors (such as points or blades) subjected to high voltage relative to ground. Ion wind is an electrohydrodynamic phenomenon. Ion wind generators can also be considered electrohydrodynamic thrusters.\n\nThe term “ionic wind” is considered a misnomer due to misconceptions that only positive and negative ions were primarily involved in the phenomenon. A 2018 study found that electrons play a larger role than the negative ions during the negative voltage period. As a result, the term “electric wind” has been suggested as a more accurate terminology.\n\nThis phenomenon is now used in an MIT ionic wind plane, the first solid state plane, developed in 2018.\n\nB. Wilson in 1750 demonstrated the recoil force associated to the same corona discharge and precursor to the ion thruster was the corona discharge pinwheel. The corona discharge from the freely rotating pinwheel arm with ends bent to sharp points gives the air a space charge which repels the point because the polarity is the same for the point and the air.\n\nFrancis Hauksbee, curator of instruments for the Royal Society of London, made the earliest report of electric wind in 1709. Myron Robinson completed an extensive bibliography and literature review during the 1950s resurgence of interest in the phenomena.\n\nIn 2018, researchers from South Korea and Slovenia used Schlieren photography to experimentally determine that electrons, in addition to ions, play an important role in generating ionic wind. The study was the first to provide direct evidence that the electrohydrodynamic force responsible for the ionic wind is caused by a charged particle drag that occur as the electrons and ions push the neutral particles away.\n\nIn 2018, a team of MIT researchers built and successfully flew the first-ever prototype plane propelled by ionic wind.\n\nNet electric charges on conductors, including local charge distributions associated with dipoles, reside entirely on their external surface (see Faraday cage), and tend to concentrate more around sharp points and edges than on flat surfaces. This means that the electric field generated by charges on a sharp conductive point is much stronger than the field generated by the same charge residing on a large smooth spherical conductive shell. When this electric field strength exceeds what is known as the corona discharge inception voltage (CIV) gradient, it ionizes the air about the tip, and a small faint purple jet of plasma can be seen in the dark on the conductive tip. Ionization of the nearby air molecules result in generation of ionized air molecules having the same polarity as that of the charged tip. Subsequently, the tip repels the like-charged ion cloud, and the ion cloud immediately expands due to the repulsion between the ions themselves. This repulsion of ions creates an electric \"wind\" that emanates from the tip, which is usually accompanied by a hissing noise due to the change in air pressure at the tip. An opposite force acts on the tip that may recoil if not tight to ground.\n\nA vaneless ion wind generator performs the inverse function, using ambient wind to move ions, which are collected yielding electrical energy.\n\n"}
{"id": "22463012", "url": "https://en.wikipedia.org/wiki?curid=22463012", "title": "Joseph Barker Stearns", "text": "Joseph Barker Stearns\n\nJoseph Barker Stearns was the inventor of the duplex system of telegraphy.\n\nStearns was the son of Edward Ray and Eliza Tyler Barker Stearns of Weld, Maine. As a youth, he worked on a farm. He studied telegraphy at Newburyport, Massachusetts, where he became manager of the office. From 1855 to 1869, he was superintendent of the Fire Alarm Telegraph Company of Boston, Massachusetts and was the first to take out patents on the use of reversed currents in connection with the fire alarm signal system.\n\nHe was president of Franklin Telegraph Co., from 1869 to 1871, during which time he invented the first practical system of duplex telegraphy which was successfully applied to the English, French and Belgian lines. Two years later this system was used for the Atlantic cables. He sold rights under his duplex patents to the Western Union Telegraph and Cable Companies, receiving large royalties for the use of his inventions from governments in England, France, Italy, Spain, Belgium, Russia and India, and several submarine cable companies. From 1879 to 1880, he was employed as engineer by the Mexican Telegraph Company in making, laying, and putting into operation the cables of that company between Galveston, Texas, and Veracruz, Mexico. In 1881, he performed a similar service for the Central and South American Telegraph Company, whose cables extended from the Isthmus of Tehuantepec in Mexico to Callao, Peru, in all between 4,000 and 5,000 miles. This work he completed in 1882.\n\nHe retired from active business in 1885 and settled in Camden, Maine. There he had a library of 10,000 volumes, and a collection of Chiriquí Province pottery, which was exhibited at the Smithsonian Institution in Washington, D.C.. He also had a collection of carved ivories which was exhibited at the Metropolitan Museum of Art in New York City. He died in Camden.\n\nHe had eight children. He got married on January 8, 1853, to Lois M. Brooks of Putney, Vermont (born June 4, 1827; died July 29, 1861, in South Boston), then married Frances Amanda Edmonds of Portsmouth, New Hampshire (born January 16, 1838) on June 6, 1866.\n\nIn 1872, the American Institute of New York awarded him the Great Medal of Honor for the invention of the duplex telegraph.\n\n"}
{"id": "747591", "url": "https://en.wikipedia.org/wiki?curid=747591", "title": "Kammback", "text": "Kammback\n\nA Kammback — also known as \"Kamm tail\" or \"K-tail\" — is an where the rear of the car slopes downwards before being abruptly cut off with a vertical surface. The purpose of a Kammback is to minimise aerodynamic drag while maintaining a practical shape for a vehicle. The Kammback is named after German aerodynamicist Wunibald Kamm for his work developing the design in the 1930s.\n\nThe Kammback was originally used on sports cars and racing cars to improve performance at high speeds, and has been more recently used by hybrid vehicles to reduce fuel consumption.\n\nAs the speed of cars increased during the 1920s and 1930s, designers began to pay more attention to automotive aerodynamics. In 1922, Paul Jaray patented a car based on a tear-drop profile (i.e. a rounded nose and long tapering tail) to minimise the aerodynamic drag created at higher speeds. The streamliner vehicles of the mid 1930s — such as the Tatra 77, Chrysler Airflow and Lincoln-Zephyr — were designed according to this philosophy.\n\nHowever, the long tail was not a practical shape for a car, so automotive designers were looking for other solutions. In 1935, the German aircraft designer Georg Hans Madelung showed alternatives to minimise drag without a long tail. In 1936, a similar theory was applied to cars, when Baron Reinhard Koenig-Fachsenfeld developed a smooth roofline but with an abrupt end at a vertical surface, which was effective in achieving low amounts of drag similar to a fully streamlined body. He worked on an aerodynamic design for a bus, and Koenig-Fachsenfeld patented the idea. Koenig-Fachsenfeld began working under Wunibald Kamm at Stuttgart University, investigating vehicle shapes that would \"provide a good compromise between everyday utility (e.g. vehicle length and interior dimensions) and an attractive drag coefficient\". In addition to aerodynamic efficiency, Wunibald Kamm also emphasized vehicle stability in his design. He proved mathematically and empirically the effectiveness of the design. \n\nIn 1938 Kamm produced the first prototype to use a Kammback, based on a BMW 328. The Kammback, along with other aerodynamic modifications, gave the prototype a drag coefficient of 0.25.\n\nThe earliest mass-produced cars that used Kammback principles were the 1949–1951 Nash Airflyte in the U.S. and the 1952–1955 Borgward Hansa 2400 in Europe.\n\nThe ideal shape to minimize drag is a teardrop. However researchers including Kamm found that abruptly cutting off the tail resulted in minimal increase in drag. The reason for this is that a turbulent wake region forms behind the vertical surface at the rear of the car. This wake region mimics the effect of the tapered tail in that air in the free stream does not enter this region (avoiding boundary layer separation), therefore smooth airflow is maintained which minimises drag.\n\nKamm's design is based on the tail being truncated at the point where the cross section area is 50% of the car's maximum cross section, which Kamm found represented a good compromise—by that point the turbulence typical of flat-back vehicles had been mostly eliminated at typical speeds.\n\nInitially, the Kamm tail was used on sportscars and racing cars. The design had a resurgence in the early 2000s as a method to reduce fuel consumption in hybrid electric vehicles.\n\nSeveral cars have been marketed as Kammbacks, despite the profile not adhering to aerodynamic philosophy of a Kammback. These models include the 1971–1977 Chevrolet Vega Kammback wagon, the 1981–1982 AMC Eagle Kammback, the AMC AMX-GT and the Pontiac Firebird–based \"Type K\" concept cars.\n\nCars that have a Kammback include:\n"}
{"id": "31780248", "url": "https://en.wikipedia.org/wiki?curid=31780248", "title": "Land for Wildlife", "text": "Land for Wildlife\n\nLand for Wildlife is a program sponsored by the Department of Sustainability and Environment in the state of Victoria, Australia. It was established in November 1981 to support private landholders and managers who voluntarily provide and enhance habitat for native wildlife on their properties within the state. Many non-landholder volunteers also participate in the scheme, which is coordinated by departmental extension officers. By doing so they are contributing to the maintenance and restoration of native biodiversity. The scheme was largely instigated by Ellen McCulloch and Reg Johnson, two prominent members of the community group Bird Observation & Conservation Australia (then the Bird Observers Club - now merged with BirdLife Australia), with which it continues to have a cooperative relationship.\n\nBenefits of full registration of a property in the scheme include on-site visits to provide advice and answer questions about how to manage the land to contribute to biodiversity conservation, participation in field and neighbourhood days, open-properties and information sessions, a regular newsletter, specialist information and a Land for Wildlife sign. It does not alter the legal status of a property in any way and landholders may withdraw from the scheme at any time.\n\nAs of 2012 there were over 5,700 participating properties, with about 15,000 people involved in the program. The area of wildlife habitat being managed on the properties totals more than 560,000 hectares (4% of private land in Victoria), and includes grasslands, heaths, woodlands, forests and freshwater wetlands.\n\nThe Victorian Land For Wildlife program allows interstate agencies (government and non-government) to deliver the scheme under an agreement in which the original aims and objectives are maintained.\n\n\n"}
{"id": "21094122", "url": "https://en.wikipedia.org/wiki?curid=21094122", "title": "Lao Holding State Enterprise", "text": "Lao Holding State Enterprise\n\nLao Holding State Enterprise \"(LHSE)\" is a state corporation of Laos that is primarily involved with the financing of the energy industry, including the Nam Theun 2 Power Company, of whose stock it controls 25%. LHSE is involved with other projects of Laos' energy infrastructure, including Nam Ngum Dam, Nam Ngiep Dam and the Hongsa Lignite Power Plant. The company is part of the Ministry of Finance.\n\n\n"}
{"id": "6753193", "url": "https://en.wikipedia.org/wiki?curid=6753193", "title": "MV Erika", "text": "MV Erika\n\nErika was the name of a tanker built in 1975 and last chartered by Total-Fina-Elf. It sank off the coast of France in 1999, causing a major environmental disaster.\n\nErika was one of eight sister ships built in Japan. Despite having 10% less steel than many other tankers of similar size, Erika was very popular amongst shipping companies because of its relative inexpensiveness.\n\nOn December 8, 1999, she sailed out of Dunkerque, bound for Livorno and with a cargo of around 31,000 tons of heavy fuel oil.\n\nAs she entered the Bay of Biscay, the Erika ran into a heavy storm. On December 12, 1999, she broke in two and sank, releasing thousands of tons of oil into the sea, killing marine life and polluting shores around Brittany, France.\n\nAccording to the official inquiry by the Dunkerque Tribunal, the Principal Shareholder of Tevere Shipping is Giuseppe Savarese, owner of the Erika since 1996. Savarese lives in London and was personally responsible for finance, administration, legal, commercial, hull and machinery insurance and P&I insurance matters.\n\nThe Erika's technical and maritime management company was Panship, a Ravenna-based corporation incorporated in 1997. The Pollara and Vitiello families each own 50 percent. The company did not employ a specialist in naval architecture or vessel strength which is typical for such companies. With regards to maintenance, Panship defined the scope and nature of maintenance work in addition to creating and evaluating calls for bids for such work. All decisions were submitted to Giuseppe Savarese. Erika's was registered under a Maltese flag.The Classification Society for classed the Erika was RINA or the Foundation Registro Italiano Navale ed Aeronautica, based in Genoa. Malta like most Flag States delegates compliance with International Safety Management Code of International Maritime Organization to Classification Societies such as RINA. RINA issued all safety certificates for the Erika.\n\nList of Certificates issued for the Erika by RINA:\n\n\nTotal said that the classification society, Registro Italiano Navale had reported that the tanker was in good condition, and that it routinely requires certificates of good condition for vessels more than 20 years old.\n\nThe accident triggered new EU-legislation as regard to transport by sea.\n\nOn January 16, 2008, Total SA, Giuseppe Savarese (the shipowner), Antonio Pollara (the handler) and RINA (the expert company) were sentenced in solidum to pay indemnities of €192 million ($280 million USD), plus individual penalties. The judgement, while recognizing the risks inherent to oceangoing vessels, reckons Total SA was \"guilty of imprudence\", from the fact that Total SA did not take into account \"the age of the ship\", (nearly 25 years), and \"the discontinuity of its technical handling and maintenance\".\n\nOn March 30, 2010, Total SA lost their appeal to overturn the court's decision.\n\n"}
{"id": "14006400", "url": "https://en.wikipedia.org/wiki?curid=14006400", "title": "Magic number (oil)", "text": "Magic number (oil)\n\nThe magic number is a term in economics that denotes the price of crude oil (measured in dollars per barrel) at which a crude oil exporting economy runs a deficit.\n\nSome countries support almost all spending from income derived from oil exports. As the price of oil drops, these countries take in less revenue from oil. The magic number denotes the point at which the revenue from oil is no longer sufficient to pay for spending. Mathematically, this can be expressed by the inequality:\n\nwhere Q is the quantity of oil exported, P is the price, and S is spending. The magic number is the value of P at which this inequality no longer holds true - that is, that the economy runs a deficit. \n\nPFC Energy publishes the magic number for all the OPEC nations. \n\n\"Qatar is at $21 a barrel, because it brings in much more oil money than it spends. Saudi Arabia's break-even point is at $49 a barrel. And Venezuela is at $58, second only to Nigeria's $65.\"\n\n"}
{"id": "30325571", "url": "https://en.wikipedia.org/wiki?curid=30325571", "title": "Margaret Jacobsohn", "text": "Margaret Jacobsohn\n\nMargaret Jacobsohn is a Namibian environmentalist. She was awarded the Goldman Environmental Prize in 1993, jointly with Garth Owen-Smith, for their efforts on conservation of wildlife in rural Namibia.\n\nShe was born in South Africa, in Pretoria. She became an NGO worker in community-based natural resource management, in Namibia. Since 1983, in the northeast of Namibia, with Garth Owen-Smith, they have been fighting against endemic illegal hunting, which has decimated species such as black rhinos and desert elephants, and for the economic and social development of local populations. Through their actions, poaching is better controlled. Game guards are designated by the rural community. Other natural resources, such as palm trees, thatch grass, plant dyes and water lilies, are monitored. She became interested in semi-nomadic Himba people, devoting a book published in 2003 to them, \"Himba, nomads of Namibia\". They are one of the few African groups that use red ochre, as a full-body make-up called otjize. The Himba originally belonged to the group of the Herero. \n\nShe was awarded the Goldman Environmental Prize in 1993, jointly with Garth Owen-Smith, and the Global 500 Roll of Honour in 1994. In 1996, following their initiatives, the Namibian government has adopted what is known as the Communal Areas Conservation Act. This amendment allows rural communities living on state-owned land to manage and benefit from their own wildlife in the same way as farmers on private farms.\n"}
{"id": "19830", "url": "https://en.wikipedia.org/wiki?curid=19830", "title": "Maxwell–Boltzmann distribution", "text": "Maxwell–Boltzmann distribution\n\nIn physics (in particular in statistical mechanics), the Maxwell–Boltzmann distribution is a particular probability distribution named after James Clerk Maxwell and Ludwig Boltzmann. \n\nIt was first defined and used for describing particle speeds in idealized gases, where the particles move freely inside a stationary container without interacting with one another, except for very brief collisions in which they exchange energy and momentum with each other or with their thermal environment. \nThe term \"particle\" in this context refers to gaseous particles only (atoms or molecules), and the system of particles is assumed to have reached thermodynamic equilibrium.\nThe energies of such particles follow what is known as Maxwell-Boltzmann statistics, and the statistical distribution of speeds is derived by equating particle energies with kinetic energy.\n\nMathematically, the Maxwell–Boltzmann distribution is the chi distribution with three degrees of freedom (the components of the velocity vector in Euclidean space), with a scale parameter measuring speeds in units proportional to the square root of formula_8 (the ratio of temperature and particle mass).\n\nThe Maxwell–Boltzmann distribution is a result of the kinetic theory of gases, which provides a simplified explanation of many fundamental gaseous properties, including pressure and diffusion. \nThe Maxwell–Boltzmann distribution applies fundamentally to particle velocities in three dimensions, but turns out to depend only on the speed (the magnitude of the velocity) of the particles. \nA particle speed probability distribution indicates which speeds are more likely: a particle will have a speed selected randomly from the distribution, and is more likely to be within one range of speeds than another.\nThe kinetic theory of gases applies to the classical ideal gas, which is an idealization of real gases. In real gases, there are various effects (e.g., van der Waals interactions, vortical flow, relativistic speed limits, and quantum exchange interactions) that can make their speed distribution different from the Maxwell–Boltzmann form. \nHowever, rarefied gases at ordinary temperatures behave very nearly like an ideal gas and the Maxwell speed distribution is an excellent approximation for such gases. \nIdeal plasmas, which are ionized gases of sufficiently low density, frequently also have particle distributions that are partially or entirely Maxwellian.\n\nThe distribution was first derived by Maxwell in 1860 on heuristic grounds. \nBoltzmann later, in the 1870s, carried out significant investigations into the physical origins of this distribution.\n\nAssuming the system of interest contains a large number of particles, the fraction of the particles within an infinitesimal element of three-dimensional velocity space, formula_9, centered on a velocity vector of magnitude formula_10, is formula_11, in which \n\nwhere formula_13 is the particle mass and formula_14 is the product of Boltzmann's constant and thermodynamic temperature.\nOne can write the element of velocity space as dformula_15 = dformula_16dformula_17dformula_18, for velocities in a standard Cartesian coordinate system, or as dformula_15 = formula_20dformula_10dformula_22 in a standard spherical coordinate system, where dformula_22 is an element of solid angle. Here formula_24 is given as a probability distribution function, properly normalized so that formula_25dformula_15 over all velocities equals one. In plasma physics, the probability distribution is often multipled by the particle density, so that the integral of the resulting distribution function equals the density. \n\nUndergraduate students are likely to work with the Maxwellian distribution function for particles moving in only one direction. If this direction is formula_27, then one has \nwhich can be obtained by integrating the three-dimensional form given above over formula_17 and formula_18.\n\nRecognizing the symmetry of formula_24, one can integrate over solid angle and write a probability distribution of speeds as the function\n\nThis probability density function gives the probability, per unit speed, of finding the particle with a speed near formula_10. This equation is simply the Maxwell-Boltzmann distribution (given in the infobox) with distribution parameter formula_34. \nThe Maxwell–Boltzmann distribution is equivalent to the chi distribution with three degrees of freedom and scale parameter formula_34.\n\nThe simplest ordinary differential equation satisfied by the distribution is:\n\nor in unitless presentation:\n\nWith the Darwin–Fowler method of mean values the Maxwell–Boltzmann distribution is obtained as an exact result.\n\nThe mean speed formula_40, \nmost probable speed (mode) , \nand root-mean-square speed formula_41 \ncan be obtained from properties of the Maxwell distribution.\n\nThis works well for nearly ideal, monatomic gases like helium, but also for molecular gases like diatomic oxygen. \nThis is because despite the larger heat capacity (larger internal energy at the same temperature) due to their larger number of degrees of freedom, their translational kinetic energy (and thus their speed) is unchanged.\n\nwith the solution:\n(the second solution formula_44 representing the \"least probable speed\").\n\nFor diatomic nitrogen (N, the primary component of air) \nat room temperature (), this gives \n\nIn summary, the typical speeds are related as follows:\n\nThe root mean square speed is directly related to the speed of sound in the gas, by\nwhere formula_61 is the adiabatic index, \nFor the example above, diatomic nitrogen (approximating air) at , formula_62 and\nthe true value for air can be approximated by using the average molar weight of air (), yielding \n\nThe original derivation in 1860 by James Clerk Maxwell was an argument based on molecular collisions of the Kinetic theory of gases as well as certain symmetries in the speed distribution function; Maxwell also gave an early argument that these molecular collisions entail a tendency towards equilibrium. After Maxwell, Ludwig Boltzmann in 1872 also derived the distribution on mechanical grounds and argued that gases should over time tend toward this distribution, due to collisions (see H-theorem). He later (1877) derived the distribution again under the framework of statistical thermodynamics. The derivations in this section are along the lines of Boltzmann's 1877 derivation, starting with result known as Maxwell–Boltzmann statistics (from statistical thermodynamics). Maxwell–Boltzmann statistics gives the average number of particles found in a given single-particle microstate, under certain assumptions, the logarithm of the fraction of particles in a given microstate is proportional to the ratio of the energy of that state to the temperature of the system:\nThe assumptions of this equation are that the particles do not interact, and that they are classical; this means that each particle's state can be considered independently from the other particles' states. Additionally, the particles are assumed to be in thermal equilibrium.\n\nThis relation can be written as an equation by introducing a normalizing factor:\nwhere:\nThe denominator in Equation () is simply a normalizing factor so that the ratios formula_65 add up to unity — in other words it is a kind of partition function (for the single-particle system, not the usual partition function of the entire system).\n\nBecause velocity and speed are related to energy, Equation () can be used to derive relationships between temperature and the speeds of gas particles. All that is needed is to discover the density of microstates in energy, which is determined by dividing up momentum space into equal sized regions.\n\nThe potential energy is taken to be zero, so that all energy is in the form of kinetic energy.\nThe relationship between kinetic energy and momentum for massive non-relativistic particles is\n\nwhere \"p\" is the square of the momentum vector \np = [\"p\", \"p\", \"p\"]. We may therefore rewrite Equation () as:\n\nwhere \"Z\" is the partition function, corresponding to the denominator in Equation (). Here \"m\" is the molecular mass of the gas, \"T\" is the thermodynamic temperature and \"k\" is the Boltzmann constant. This distribution of formula_65 is proportional to the probability density function \"f\" for finding a molecule with these values of momentum components, so:\n\nThe normalizing constant can be determined by recognizing that the probability of a molecule having \"some\" momentum must be 1. \nIntegrating the exponential in () over all \"p\", \"p\", and \"p\" yields a factor of \n\nSo that the normalized distribution function is:\nThe distribution is seen to be the product of three independent normally distributed variables formula_68, formula_69, and formula_70, with variance formula_71. Additionally, it can be seen that the magnitude of momentum will be distributed as a Maxwell–Boltzmann distribution, with formula_72.\nThe Maxwell–Boltzmann distribution for the momentum (or equally for the velocities) can be obtained more fundamentally using the H-theorem at equilibrium within the Kinetic theory of gases framework.\n\nThe energy distribution is found imposing\n\nwhere formula_73 is the infinitesimal phase-space volume of momenta corresponding to the energy interval formula_74.\nMaking use of the spherical symmetry of the energy-momentum dispersion relation formula_75,\nthis can be expressed in terms of formula_74 as\n\nUsing then () in (), and expressing everything in terms of the energy formula_77, we get\nand finally\n\nSince the energy is proportional to the sum of the squares of the three normally distributed momentum components, this distribution is a gamma distribution; in particular, it is a chi-squared distribution with three degrees of freedom.\n\nBy the equipartition theorem, this energy is evenly distributed among all three degrees of freedom, so that the energy per degree of freedom is distributed as a chi-squared distribution with one degree of freedom:\n\nwhere formula_80 is the energy per degree of freedom. At equilibrium, this distribution will hold true for any number of degrees of freedom. For example, if the particles are rigid mass dipoles of fixed dipole moment, they will have three translational degrees of freedom and two additional rotational degrees of freedom. The energy in each degree of freedom will be described according to the above chi-squared distribution with one degree of freedom, and the total energy will be distributed according to a chi-squared distribution with five degrees of freedom. This has implications in the theory of the specific heat of a gas.\n\nThe Maxwell–Boltzmann distribution can also be obtained by considering the gas to be a type of quantum gas for which the approximation \"ε » k T\" may be made.\n\nRecognizing that the velocity probability density \"f\" is proportional to the momentum probability density function by\n\nand using p = mv we get\n\nwhich is the Maxwell–Boltzmann velocity distribution. The probability of finding a particle with velocity in the infinitesimal element [\"dv\", \"dv\", \"dv\"] about velocity v = [\"v\", \"v\", \"v\"] is\n\nLike the momentum, this distribution is seen to be the product of three independent normally distributed variables formula_16, formula_17, and formula_18, but with variance formula_86. \nIt can also be seen that the Maxwell–Boltzmann velocity distribution for the vector velocity\n[\"v\", \"v\", \"v\"] is the product of the distributions for each of the three directions:\n\nwhere the distribution for a single direction is\n\nEach component of the velocity vector has a normal distribution with mean formula_89 and standard deviation formula_90, so the vector has a 3-dimensional normal distribution, a particular kind of multivariate normal distribution, with mean formula_91 and standard deviation formula_92.\n\nThe Maxwell–Boltzmann distribution for the speed follows immediately from the distribution of the velocity vector, above. Note that the speed is\n\nand the volume element in spherical coordinates\n\nwhere formula_95 and formula_96 are the spherical coordinate angles of the velocity vector. Integration of the probability density function of the velocity over the solid angles formula_97 yields an additional factor of formula_98.\nThe speed distribution with substitution of the speed for the sum of the squares of the vector components:\n\n\n"}
{"id": "40278001", "url": "https://en.wikipedia.org/wiki?curid=40278001", "title": "Mississippi Valley Conservation Authority", "text": "Mississippi Valley Conservation Authority\n\nThe Mississippi Valley Conservation Authority (MVCA) (formerly Mississippi Valley Conservation (MVC)) is a conservation authority in the province of Ontario. It is headquartered in Carleton Place, Ontario and serves a 4450 km watershed located across eleven municipalities.\n\n\n"}
{"id": "30396075", "url": "https://en.wikipedia.org/wiki?curid=30396075", "title": "Nick Carter (environmentalist)", "text": "Nick Carter (environmentalist)\n\nNick Carter (died 2000) was a Zambian environmentalist.\n\nHe was awarded the Goldman Environmental Prize in 1997, for his efforts on documenting pirate whaling and global wildlife crime, and his contributions to organizing activities leading to the \"Lusaka Agreement\" between six African countries in September 1994, aiming to enforce regulations such as the Convention on International Trade in Endangered Species.\n"}
{"id": "14655670", "url": "https://en.wikipedia.org/wiki?curid=14655670", "title": "Oil megaprojects (2003)", "text": "Oil megaprojects (2003)\n\nThis page summarizes projects that brought more than of new liquid fuel capacity to market with the first production of fuel beginning in 2003. This is part of the Wikipedia summary of Oil Megaprojects—see that page for further details. 2003 saw 30 projects come on stream with an aggregate capacity of when full production was reached (which may not have been in 2003).\n\n"}
{"id": "9149344", "url": "https://en.wikipedia.org/wiki?curid=9149344", "title": "Pennyland project", "text": "Pennyland project\n\nThe Pennyland project was one of a series of low-energy building experiments sparked by the 1973 oil crisis. It involved the construction of an estate of 177 houses in the Pennyland area of Milton Keynes, Buckinghamshire, United Kingdom. It compared possible future UK building efficiency standards with newly introduced Danish ones.\n\nAlthough identical externally, half the Pennyland houses (Area 1) were built to the proposed 1982 UK Building Regulations energy efficiency standards. The other half (Area 2) were built to the much more demanding 1977 Danish \"BR77\" standard. (In Europe, BR77 and the Swedish \"SBN-80\" standards set the benchmark for low-energy housing at the time.)\n\nSponsored by the UK Department of Energy, Department of Environment and Milton Keynes Development Corporation, and using the technical expertise of the Open University Energy Research Group, the houses were constructed during 1979 and 1980.\n\nFor comparison, two additional groups of control houses, already built in 1978, were studied on an adjoining site at Neath Hill. A further group, the Linford low energy houses were built to the Pennyland Area 2 standards and used for more detailed tests.\n\nThe Pennyland estate was laid out to take advantage of solar gain. The majority of houses faced south with the main living rooms located on the south side. They were designed with an insulated cavity wall with a poured concrete inner leaf to provide thermal mass. This had the added benefit of increasing airtightness. Nearly all were fitted with conventional radiator central heating systems.\n\nSeveral levels of insulation, heating system efficiency and passive solar design were compared:\n\nAt Neath Hill most houses featured \n\nAbout 20 of the Neath Hill houses were retrofitted with foam cavity fill wall insulation.\n\nThe Pennyland Area 1 houses featured:\n\nThe Pennyland Area 2 houses (almost Danish BR77 standards) featured:\nThe houses were monitored over the winters of 1981 and 1982 with energy and internal temperatures being measured in a large sample of houses on a weekly average basis. More detailed monitoring on an hourly basis was carried out on the Linford houses.\n\nAmong the lessons learned were:\n\nThe short payback times encouraged Milton Keynes Development Corporation to introduce its own Building Regulation standards for new housing within the city leading to the major 1986 Energy World demonstration project and exhibition.\n\nThe issue of mould growth in air-tight houses has since been tackled by Building Regulation requirements for specific ventilation in kitchens, bathrooms and toilets.\n\nThe levels of insulation tested at Pennyland and Great Linford in the early 1980s were only surpassed in the UK Building Regulation requirements in 2002.\n\n\n\n"}
{"id": "54364080", "url": "https://en.wikipedia.org/wiki?curid=54364080", "title": "Personal transporter", "text": "Personal transporter\n\nA personal transporter (also electric rideable, personal light electric vehicle, personal mobility device, etc) is a class of compact, electric or internal combustion vehicle for transporting an individual at speeds that do not normally exceed . They include electric skateboards, kick scooters, self-balancing unicycles and Segways, as well as gasoline-fueled motorized scooters or skateboards, typically using two-stroke engines of less than displacement. Many newer versions use recent advances in vehicle battery and motor-control technologies. They are growing in popularity, and legislators are in the process of determining how these devices should be classified, regulated and accommodated during a period of rapid innovation. \n\nExcluded from this legal category are electric bicycles (that are considered to be a type of bicycle), electric motorbikes and scooters (that are treated as a type of scooter) and powered mobility aids with 3 or 4 wheels on which the rider sits (which fall within regulations covering powered mobility scooters). \n\nThe first personal transporter was the 1915 Autoped, a stand up scooter with a gasoline engine made from 1915 to 1922. Engine powered scooters and skate boards reappeared in the 1970s and 1980s. \n\nWith the rapid improvements in lithium batteries in the late 1990s and early 2000s, a range of new types of personal transporters appeared, and began to spread into use in urban settings for both recreation and practical transportation. \n\nDean Kamen applied for his first patent for a 'human transporter', the Segway PT, in 1994. This was followed by other patent applications prior to its product launch in late 2001 and first deliveries to customers early in 2002.\n\nTrevor Blackwell demonstrated a self-balancing unicycle based on the control-mechanism from a Segway PT in 2004 for which he published open source designs (see Eunicycle). Focus Designs released the first commercially available self-balancing unicycle (which had a seat) in 2008 and in 2010 Shane Chen, an American businessman and founder of Inventist, filed a patent for the more familiar and compact seatless device which his company, Inventis launched in 2011.\n\nChen then went on to file a patent for a self-balancing scooter in February 2013, and launched a Kickstarter fund-raising campaign in May 2013 with multiple companies, mainly in China releasing similar products. 500,000 units from 10 suppliers were recalled from the US market alone in July 2016.\n\nLouie Finkle of California is credited with creating the first commercial electric skateboards, offering his first wireless electric skateboard in 1997 and he filed for a patent in April 1999, though it was not until 2004 that electric motors and batteries had sufficient torque and efficiency to power boards effectively. In 2012 ZBoard raised nearly 30 times their target for a balance controlled electric skateboard on Kickstarter, which was well received at the Consumer Electronics Show in Las Vegas in January 2013.\n\nIn December 2016 \"The Verge\" magazine suggested that 2017 would be an \"important year\" for personal electric vehicles of all sizes.\n\nThe terminology for these devices is not yet stable () as the media and legislators discuss a rapidly emerging potential class of motor vehicle and its relationship to laws relating to other transport devices, including electric bicycles and mobility aids such as mobility scooters. Commonly used terms are used for these new devices include:\n\nMedia: \"rideable\", \"electric rideable\", \"electric personal transporter\", \"personal electric vehicle\", \"personal transporter\" \"portable electric vehicle\" \"portable personal vehicle\" and \"portable transporter\".\n\nLegislative: \"personal mobility device\" (Singapore, Australia - Victoria Transport Policy Unit) \"personal e-mobility device\" (Underwriters Laboratory), \"electrically motorized board\" (California USA), \"personal light electric vehicles\" (European Union), \"electric personal assistive mobility device\" (Washington State USA), \"powered transporters\" (UK).\n\nOther languages: \"Engins de déplacement personnel\" (French)\n\nThe earliest example of a motorized scooter, or standing scooter with an internal combustion engine, was the 1915 Autoped, made in the US until 1919 and in Germany until 1922.\n\nAn electric standing scooter with a small platform with two or more wheels driven by an electric motor which fold for portablity. \n\nAn electric skateboard is an electrically powered skateboard controlled by the rider shifting their weight and in some cases also a hand-held throttle.\n\nThe self-balancing scooter is a category of personal transporter which includes all self-balancing powered portable devices with two parallel wheels; these include the Segway PT, the Segway miniPRO and self-balancing hoverboards.\n\nA self-balancing unicycle is a single-rider electrically powered unicycle that automatically keeps itself upright using computer-controlled gyroscopes, accelerometers.\n\nThe Onewheel has elements of an electric skateboard (it is powered) and a self-balancing unicycle (it has one wheel).\n\nThe Honda UNI-CUB and its predecessor the Honda U3-X are concept seated devices that are fully stable that can travel sideways as well as in the forwards/backwards axis.\n\nMost devices are powered by rechargeable lithium-ion vehicle batteries, and often 18650-size batteries controlled by complex battery management systems. Lithium polymer batteries are being tested for higher performance.\n\nMany devices now contain one, or sometimes two, batteries in the range, which fall within the sizes that can be carried on an airline. Airlines may restrict carrying some devices due to the earlier product defects. As a rule, every 100 WHours of capacity will provide 6-7 miles of range.\n\nThe price of lithium batteries fell 80% between 2011 and 2017. These batteries, which have good energy density, energy-to-mass ratio provide the range, torque, operational life required, unlike the previously available lead–acid, NiMH and NiCad technologies.\n\nMany of these devices use brushless DC electric motors with permanent magnets attached to the moving hub which turns around a fixed armature which offer high efficiency, good speed-torque characteristics and low weight. This motor is often built into the wheel itself, eliminating gears and drive belts. Many devices have a motor in the 250-500 watts range which provides good performance for an adult rider on the flat and on an incline, with sportier models using motors in excess of 1500 Watts.\n\nBrushless DC motors, which often have regenerative braking, also need complex motor controllers.\n\nIn Hong Kong, the Transport Department issued a statement saying that under the Road Traffic Ordinance, these self-balancing hoverboards are classified as motor vehicles, since they are mechanically propelled: \"Registration and licence is required before any motor vehicle is used on the roads, including private roads. However, since the construction and operation of these motor-driven devices could pose a danger to the users themselves and other road users, they are not appropriate to use on roads, hence they cannot be registered and licensed.\"\n\nIn 2006, the Segway PT was approved for use on sidewalks and other pedestrian designated locations, and on roads without sidewalks, with obstructed sidewalks or sidewalks that lack curb cuts. The user must be over 16 years old. No license is required. The maximum allowed speed is , enforced by electronic restriction put in place by the importer.\n\nIn a court, Segway PT was classified as a motorcycle, owing to the power output; however, there is no report of registration. Segway Japan, an authorized dealer, sells Segways only to corporations to use in facilities.\n\nIn Mecca they were banned after a video of a pilgrim, using it during hajj on a hoverboard was posted on social media.\n\nDevices can be ridden on the sidewalk at speeds of up to and cycleways at speeds of up to whilst providing pedestrians with the right of way. Devices must be fitted with lights or alternatively the rider must wear a luminous jacket. They are not allowed on the roadway. In December 2016 the Land Transport Authority started a 6-month trial where devices were allowed on trains and buses at all times.\n\nThe European Committee for Standardization (CEN) has been in the process of defining a standard for personal transporters, referred to as 'personal light electric vehicle', including both self-balancing vehicles and standing vehicles with maximum speeds of up to and is expected to complete its work by the end of 2017. In the mean time the some countries have allowed personal transporters to be used on public roads with certain conditions:\n\nA law revision by the Government of Åland concerning \"small electrically powered vehicles\" means the Segway PT and all other mainly one person electrical vehicles have been classified as bicycles since March 14, 2012.\n\nThe type Segway i2 is (width 63 cm) narrower than the width limit and has a low-enough maximum speed to come under laws relating to electric bicycles and therefore has to use cycle lanes and paths, otherwise street lanes. The type Segway x2 reaches with its bigger wheels 84 cm width and is therefore an electric vehicle, that needs a license and insurance. Neither type may use sidewalks (lengthwise) or pedestrian zones (unless exemption stated).\n\nIn Belgium the law was recently adjusted allowing electrical motorized devices to the public road. Art 2.15.2.\nDevices with a max speed of can ride on the cycle path. One can also use these devices on sidewalks at walking pace. Devices with a higher maximum speed are subject under the existing rules for motorised vehicles. An insurance and protective wear will be required in any cases.\n\nUse of a Segway PT is allowed within city limits wherever pedestrian and bicycles are allowed, i.e., sidewalks, bicycle paths, parks, etc. Segways can be rented for city tours in cities of Zagreb, Split and Dubrovnik.\n\n devices are allowed on pavements at a speed not exceeding that of pedestrians and on cycling lanes at up to cycling speed. A new clearer framework is being developed by the road police in major cities.\n\nAt least since the autumn of 2010, the Ministry of Transport enforced the interpretation that a rider on the Segway PT is considered as a pedestrian. The amendment act 48/2016 Sb., in force since 20 February 2016, defined a new strange term \"osobní technický prostředek\" (= personal technical device/medium) for \"personal transporter with selfbalancing device\" and \"other similar devices\". However, the text of the act uses a term \"osobní přepravník\" (\"personal transporter\") in that sense instead. Municipality can restrict their traffic by municipal decree, but such a restriction needs to be marked by road signs. Since 21 March 2016, a new ordinance of the Ministry of Transport, 84/2016 Sb., which introduced several new road signs, is in force:\n\nOn 19 July 2016, the Prague Council approved a decree (in force since 3 August 2016) that Segways (strictly speaking all \"personal transporters\" as defined by law) are forbidden in the Prague Conservation Area, Prague 7 and part of Prague 4\n\nThe Segway PT is classified as a moped (\"knallert\"). As such vehicles must be fitted with lights, license plates and mechanical brakes, the Segway is effectively banned from public roads. A trial where the Segway would be classified as a bicycle has been announced running from June 1, 2010 to April 1, 2011. The trial was extended to December 1, 2011, and later to the end of 2014.\n\nIn September 2015 authorities in Finland recommended that personal transporters should be made legal for use on roads, making a distinction between devices with a maximum speed of which would be treated as pedestrians and ones with a maximum speed of which would be treated as bicycles.\n\nSegway PTs are classified as low-power mopeds and therefore require license plates, effectively banning the use on public roads. On March 31, 2015, The Ministry of Transport and Communications of Finland started progress to propose changes to law to allow Segways under 25 km/h on sidewalks and reclassifying them as bicycles. Like bicycles, Segways would be required to include safety reflectors and a bell to alert pedestrians and the driver is required to wear a bicycle helmet.\n\nSegway PTs, also named \"gyropode\", are equivalent to pedestrians and obey the same rules and laws.\nIn Germany self-balancing hoverboards are not allowed on public streets.\n\nIt is not legal to ride solowheels on public roads (includes sidewalks, parks, forest tracks, etc.) in Germany as of June 2017. Because it is considered as a type of motor vehicle the rider would need a test certificate from the Technical Inspection Agency (Technischer Überwachungsverein) to get an insurance. Additionally, the driver would have to pay taxes according to the certificate. However, the Inspection Agency has no valid classification for it, no certificate can be obtained. Hence, riding a solowheel on public road would mean to ride without certificate, without insurance and to evade taxes. It may have severe penalties (up to one year in prison ) when a solowheel rider is caught by the police. In contrast, for the Seqway as a two-wheeled vehicle with handlebar, there is a classification which allows to get a certificate and thus, the compulsory insurance.\n\nThe Segway PT i2 is generally allowed on bicycle paths and public roads within city limits since July 25, 2009. Outside city limits, the Segway may not be used on federal motorways, federal highways, state roads, and district roads. Bicycle lanes must be used if present. Riding a Segway on sidewalks and in pedestrian zones for city tours requires a special permit. The Segway is classified as an \"electronic mobility aid\", a new class of vehicle defined specifically for the Segway PT. Segways used on public roads must be equipped with front and rear lighting, reflectors, a bell, and an insurance plate. The driver must have procured a vehicle insurance and hold at least an \"M type\" (moped) license.\n\nThe \"Központi Közlekedési Főfelügyelet\" (Central Traffic Authority Board) does not consider Segways to be vehicle, and considers skateboarders, and people moving luggage trolleys pedestrians. Segway riders may use sidewalks and follow rules for pedestrians.\n\nSegway PTs are permitted in most public places. They are permitted in certain areas on bicycle paths around Dublin and Cork.\n\nUse of a Segway PT is allowed within city limits wherever pedestrians or bicycles are allowed, i.e., sidewalks, bicycle paths, parks, etc.\n\nSegway PTs are legal on bicycle trails and roads. They are the equivalent to electric bicycles and obey the same rules and laws. \n\nIn the Netherlands the use of self-balancing hoverboards is illegal on all public roads, it is only allowed on private property. The main reason given is that the vehicle is motorized but has no steering wheel and no place to sit. Therefore, the vehicle does not fall in any category allowed on public roads.\n\nIn The Netherlands, any motorised skateboard is not permitted on public roads, including those driven by an electric motor.\n\nIn April 2008, the Dutch Government announced that it would ease the ban it had imposed in January 2007 that made it illegal to use a Segway PT on public roads in the Netherlands. Until recently, a tolerance policy was in place due to the inability of the authorities to classify the Segway as a vehicle. However, certain handicapped people, primarily heart and lung patients, are allowed to use the Segway, but only on the pavement. From 1 July 2008, anyone over the age of 16 is permitted to use a Segway on Dutch roads but users need to buy custom insurance. Amsterdam police officers are testing the Segway. In Rotterdam the Segway is used regularly by police officers and city watches.\n\nBecause of the top speed of 20 km/h, the Segway was classified as a moped in Norway. Prior to 2014, there were requirements for registration, insurance, age limit, drivers licenses and helmets to operate a Segway in the country. Therefore, Segways were not originally able to be used legally on public or private roads or on private property in Norway. Segways became legal in Norway on July 1, 2014 on all public roads with speed limits 20 km/h or less, sidewalks and bicycle lanes for ages 16 and older without requiring registration or insurance.\n\nSegway PTs are legal on public paths from age 18 (and below, when accompanied by adults) as an equivalent to pedestrian traffic and are used by local police forces, and by Polícia Marítima] (a Navy unit), for beach patrolling. They are also used (rented) by tour operators across the country, and by shopping security guards.\n\nIt was unlawful to use a Segway PT on any public road or pavement in Sweden until 18 December 2008 when the Segway was re-classified as a \"cykel klass II\" (class 2 bicycle). On 1 October 2010 the Segway and similar one person electrical vehicles were re-classified as bicycles.\n\nIn Switzerland, devices with a maximum speed of have an age limit of age 14 years with a licence, and 16 years without a licence.\n\nThe Segway PT is classified as a moped with usage of all bicycle circulation areas. Only the PT i2 and x2 (SE) has been approved for use in Switzerland, no NineBot Elite or mini Pro. Every selfbalancing vehicle must be fullredundant. The PT may be used on roads provided that it is equipped with a Swiss Road Kit and a license plate. The Swiss Road Kit has front and back lighting, a battery source, and a license plate holder. Use on sidewalks and pedestrian zones is prohibited. An exception is made for handicapped individuals, who must obtain in advance a special authorization from the Swiss Federal Roads Office. The Segway PT i180 may also be registered for use on specific request. However, the PT i180 must be equipped with a left/right turn indicator system before it may be admitted for road use.\n\nIn England and Wales use of these devices on a sidewalk is banned under Section 72 of the Highway Act 1835. With reference to its use of the carriageway it probably falls into the category of 'motor vehicle' (defined as 'a mechanically propelled vehicle, intended or adapted for use on roads' by section 136 of the Road Traffic Regulation Act 1984) and as such would be covered by the Road Vehicles (Construction & Use) Regulations 1986 and hence approval through European Community Whole Vehicle Type Approval. The government has been petitioned to allow these devices on the road. While in opposition in 2008, the Conservatives and Liberal Democrats lobbied the Labour Government to change the law to allow Segways to use public cycle lanes. In July 2010, a man was charged under the Highway Act 1835 in Barnsley for riding his Segway on the pavement, and was prosecuted and fined £75 in January 2011. His conviction was upheld by the High Court on appeal.\n\nIn Scotland, it is illegal to ride on public pavements (sidewalks) under the Roads Act, 1984.\n\nIn Toronto motorized vehicles are not allowed on sidewalks, except for mobility scooters for people who need them.\n\nRestrictions on motorized vehicle use are set by provinces individually. In Alberta, Segway PTs cannot legally be driven on public roads including sidewalks abutting public roads. Segways cannot legally be driven on city-owned bicycle paths in Calgary. Segways are allowed on private land with the landowner's permission. In British Columbia, Segways cannot legally be operated on B.C. roads or on sidewalks because they cannot be licensed or insured as a vehicle in B.C. In Ontario, the Ministry of Transportation started a pilot program allowing Segways to be used by people 14 years or older with a disability, Canada Post door-to-door delivery personnel, and police officers. It was originally planned to end on October 19, 2011, but was extended by two years, and then extended again an additional five years (to October 19, 2018), due to limited participation. Prior to the end of the pilot program, the Ministry of Transportation will assess the data and information gathered from the pilot decide whether to allow Segways and how to legislate them.\n\nIn California, as of 1 January 2016 'electrically motorized boards' can be used by those over 16 years old at speeds of up to on streets where the speed limit is under as long as they wear a helmet and comply with drive/drug laws. Boards must be speed limited to , be designed for the transport of one person and have a power of less than 1000 watts. Use of these devices on the sidewalk is left to cities and counties to decide. Having monitored this new law for 5 years, California Highway Patrol will submit a final report to the legislature in 2021. University of California, Los Angeles included Hoverboards in a general restriction on the use of bicycles, scooters and skateboards using walkways and hallways in November 2015.\n\nIn New York City, self-balancing hoverboards are banned under existing legislation; however, community advocates are working with lawmakers to legalize their use but there is no current explanation from the lawmakers relating to electric skateboards.\n\nThe Segway PT has been banned from use on sidewalks and in public transportation in a few municipalities and the company has challenged bans and sought exemption from sidewalk restrictions in over 30 states. Advocacy groups for pedestrians and the blind in the US have been critical of Segway PT use: America Walks and the American Council of the Blind oppose allowing people, even those with disabilities, to drive the Segway PT on sidewalks and have actively lobbied against any such legislation. Today, Segways are allowed on sidewalks in most states, though local municipalities may forbid them. Many states also allow them on bicycle lanes or on roads with speed limits of up to .\n\nIn 2011, the U.S. government Department of Justice—amending regulations that implement title II of the Americans with Disabilities Act (ADA)—ruled that the Segway is an \"other power-driven mobility device\" and its use must be permitted unless the covered entity can demonstrate that users cannot operate the class of devices in accordance with legitimate safety requirements.\n\nA fact sheet published by the US Justice Department states: \"People with mobility, circulatory, respiratory, or neurological disabilities use many kinds of devices for mobility. Some use walkers, canes, crutches, or braces. Some use manual or power wheelchairs or electric scooters. In addition, advances in technology have given rise to new devices, such as Segways that some people with disabilities use as mobility devices, including many veterans injured while serving in the military. And more advanced devices will inevitably be invented, providing more mobility options for people with disabilities.\" There is some allowance in only some very specific circumstances where usage would be considered unsafe. Semi-ambulatory Americans have previously benefitted from Segway use, even in New York City. Segs4Vets provides Segway PTs to permanently injured military veterans.\n\nSan Francisco banned the Segway PT from sidewalks over safety concerns in 2002. The District of Columbia categorizes Segways as a \"personal mobility device\" which means Segway users follow D.C.'s bicycle laws, which do not require Segway users to wear helmets and other protective gear. Users are not allowed to wear headphones with the exception of hearing aids or other devices that only require the use of one ear.\n\nIn Mexico there is no regulation that limits Segway use in public spaces.\n\nThe authorities stated in late 2015 that self-balancing hoverboards must not be ridden on the carriageway or sidewalk in the state of New South Wales since they are categorised as motor vehicles but don't comply with any existing vehicle class. They did also say that \"'our road safety experts in the Centre for Road Safety are currently working with their counterparts across the country on national laws and safety standards for these personal electric transport devices, so we can figure out how and where people can use them safely\"\". Other states in Australia have yet to make a clear decision or announcement on legality and enforcement, and are relying on existing laws in place. They are free to use on private property.\n\nIn Australia laws are determined at the state & territory level, each differing in their adoption of the Australian Road Rules. It is generally illegal to use Segway PTs in public places and on roads throughout Australia.\n\nIn the Australian Capital Territory, use of Segways is illegal on roads and other public places, but, , was permitted around Canberra's Lake Burley Griffin and other tourist attractions, subject to training, safety equipment and speed limit requirements.\n\nIn New South Wales, the Segway has been confirmed by the Roads and Traffic Authority as being illegal on both roads and footpaths. \"In simple terms, riders are way too exposed to mix with general traffic on a road and too fast, heavy and consequently dangerous to other users on footpaths or cycle paths.\" Although this does not render them totally illegal (they may still, for example, be used on private property), their uses are limited enough that they are not sold to the general public.\n\nIn Queensland, the use of the Segway became legal on the 1st of August 2013. Queensland transport Minister Scott Emerson noted that it makes sense for Segways to be allowed on public paths across Queensland, given users wear helmets.\n\nIn Western Australia, the law enables Electric Personal Transporters (EPT) (Segways) to be used as part of a supervised commercial tour, being run by an operator that holds the appropriate approvals. You may use an EPT on private property. Tour operators should approach the Local Authority where they wish to operate the tour. Local authorities have ultimate responsibility for approving tour operators within their respective areas.\n\nIn New Zealand the Segway PT is classed as a mobility device, in the same category as a mobility scooter or electric wheelchair. Mobility Devices must be ridden on footpaths where possible, at a speed that does not endanger others, and give way to pedestrians. This ruling might not be consistently applied: in 2011, police in Taupo had to stop using Segways because there is no separate vehicle classification that applies to them, requiring their registration as roadworthy in the same manner as cars.\n\n\n"}
{"id": "651360", "url": "https://en.wikipedia.org/wiki?curid=651360", "title": "Photoemission spectroscopy", "text": "Photoemission spectroscopy\n\nPhotoemission spectroscopy (PES), also known as photoelectron spectroscopy, refers to energy measurement of electrons emitted from solids, gases or liquids by the photoelectric effect, in order to determine the binding energies of electrons in a substance. The term refers to various techniques, depending on whether the ionization energy is provided by an X-ray photon, an EUV photon, or an ultraviolet photon. Regardless of the incident photon beam, however, all photoelectron spectroscopy revolves around the general theme of surface analysis by measuring the ejected electrons.\n\nX-ray photoelectron spectroscopy (XPS) was developed by Kai Siegbahn starting in 1957 and is used to study the energy levels of atomic core electrons, primarily in solids. Siegbahn referred to the technique as \"electron spectroscopy for chemical analysis\" (ESCA), since the core levels have small chemical shifts depending on the chemical environment of the atom that is ionized, allowing chemical structure to be determined. Siegbahn was awarded the Nobel Prize in 1981 for this work. XPS is sometimes referred to as PESIS (photoelectron spectroscopy for inner shells), whereas the lower-energy radiation of UV light is referred to as PESOS (outer shells) because it cannot excite core electrons.\n\nIn the ultraviolet and visible region, the method is usually referred to as photoelectron spectroscopy for the study of gases, and photoemission spectroscopy for solid surfaces.\n\nUltraviolet photoelectron spectroscopy (UPS) is used to study valence energy levels and chemical bonding, especially the bonding character of molecular orbitals. The method was developed originally for gas-phase molecules in 1961 by Feodor I. Vilesov and in 1962 by David W. Turner, and other early workers included David C. Frost, J. H. D. Eland and K. Kimura. Later, Richard Smalley modified the technique and used a UV laser to excite the sample, in order to measure the binding energy of electrons in gaseous molecular clusters.\n\nTwo-photon photoelectron spectroscopy (2PPE) extends the technique to optically excited electronic states through the introduction of a pump-and-probe scheme.\n\nExtreme-ultraviolet photoelectron spectroscopy (EUPS) lies in between XPS and UPS. It is typically used to assess the valence band structure. Compared to XPS, it gives better energy resolution, and compared to UPS, the ejected electrons are faster, resulting in less space charge and mitigated final state effects.\n\nThe physics behind the PES technique is an application of the photoelectric effect. The sample is exposed to a beam of UV or XUV light inducing photoelectric ionization. The energies of the emitted photoelectrons are characteristic of their original electronic states, and depend also on vibrational state and rotational level. For solids, photoelectrons can escape only from a depth on the order of nanometers, so that it is the surface layer which is analyzed.\n\nBecause of the high frequency of the light, and the substantial charge and energy of emitted electrons, photoemission is one of the most sensitive and accurate techniques for measuring the energies and shapes of electronic states and molecular and atomic orbitals. Photoemission is also among the most sensitive methods of detecting substances in trace concentrations, provided the sample is compatible with ultra-high vacuum and the analyte can be distinguished from background.\n\nTypical PES (UPS) instruments use helium gas sources of UV light, with photon energy up to 52 eV (corresponding to wavelength 23.7 nm). The photoelectrons that actually escaped into the vacuum are collected, energy resolved, slightly retarded and counted, which results in a spectrum of electron intensity as a function of the measured kinetic energy. Because binding energy values are more readily applied and understood, the kinetic energy values, which are source dependent, are converted into binding energy values, which are source independent. This is achieved by applying Einstein's relation formula_1. The formula_2 term of this equation is due to the energy (frequency) of the UV light that bombards the sample. Photoemission spectra are also measured using synchrotron radiation sources.\n\nThe binding energies of the measured electrons are characteristic of the chemical structure and molecular bonding of the material. By adding a source monochromator and increasing the energy resolution of the electron analyzer, peaks appear with full width at half maximum (FWHM) less than 5–8 meV.\n\n\n"}
{"id": "18952324", "url": "https://en.wikipedia.org/wiki?curid=18952324", "title": "Pitaya", "text": "Pitaya\n\nA pitaya () or pitahaya () is the fruit of several different cactus species indigenous to the Americas. Pitaya usually refers to fruit of the genus \"Stenocereus\", while pitahaya or dragon fruit refers to fruit of the genus \"Hylocereus\", both in the Cactaceae family. The dragon fruit is cultivated in Southeast Asia, Florida, the Caribbean, Australia, and throughout tropical and subtropical world regions.\n\nThese fruits are commonly known in English as \"dragon fruit\", reflecting its vernacular Asian names. The names \"pitahaya\" and \"pitaya\" derive from Mexico, and \"pitaya roja\" in Central America and northern South America, possibly relating to pitahaya for names of tall cacti species with flowering fruit.\n\nPitahaya-producing cacti of the genus \"Hylocereus\", originally native to a region including Mexico, Guatemala, Nicaragua, Costa Rica, El Salvador, and northern South America. The dragon fruit is cultivated in Southeast Asia, Florida, the Caribbean, Australia, and throughout tropical and subtropical world regions.\n\n\"Stenocereus\" fruit (sour pitayas) are a variety that is commonly eaten in the arid regions of the Americas. They are more sour and refreshing, with juicier flesh and a stronger taste. The sour pitaya or \"pitaya agria\" (\"S. gummosus\") in the Sonoran Desert has been an important food source for indigenous peoples of the Americas. The Seri people of northwestern Mexico still harvest the fruit, and call the plant \"ziix is ccapxl\" \"thing whose fruit is sour\". The fruit of related species, such as \"S. queretaroensis\" and the dagger cactus (\"S. griseus\"), are also locally important foods. The fruit of the organ pipe cactus (\"S. thurberi\", called \"ool\" by the Seris) is the \"pitaya dulce\" \"sweet pitaya\". It still has a more tart aroma than \"Hylocereus\" fruit, described as somewhat reminiscent of watermelon; it has some uses in traditional medicine.\n\nSweet pitahayas come in three types, all with leathery, slightly leafy skin:\n\nEarly imports from Colombia to Australia were designated \"Hylocereus ocampensis\" (supposedly, the red fruit) and \"Cereus triangularis\" (supposedly, the yellow fruit). It is not quite certain to which species these taxa refer, though the former is probably the red pitaya.\n\nThe fruit normally weighs from ; some may reach .\n\nAfter thorough cleaning of the seeds from the pulp of the fruit, the seeds may be stored when dried. Ideally, the fruit is unblemished and overripe.\n\nSeeds grow well in a compost or potting soil mix – even as a potted indoor plant. Pitaya cacti usually germinate after between 11 and 14 days after shallow planting. As they are cacti, overwatering is a concern for home growers. As their growth continues, these climbing plants will find something to climb on, which can involve putting aerial roots down from the branches in addition to the basal roots. Once the plant reaches a mature 10 pounds in weight, the plant may flower.\nCommercial plantings can be done at high density with between 1100 and 1350 plants per hectare. Plants can take up to five years to come into full commercial production, at which stage yields of 20 to 30 tons per hectare can be expected.\n\nPitaya flowers bloom overnight and usually wilt by the morning. They rely on nocturnal pollinators such as bats or moths for fertilization. Self-fertilization will not produce fruit in some species, and while cross-breeding has resulted in several \"self-fertile\" varieties, cross-pollinating with a second plant species generally increases fruit set and quality. This limits the capability of home growers to produce the fruit. However, the plants can flower between three and six times in a year depending on growing conditions. Like other cacti, if a healthy piece of the stem is broken off, it may take root in soil and become its own plant.\n\nThe plants can endure temperatures up to and very short periods of frost, but will not survive long exposure to freezing temperatures. The cacti thrive most in USDA zones 10–11, but may survive outdoors in zone 9a or 9b.\n\n\"Hylocereus\" has adapted to live in dry tropical climates with a moderate amount of rain. The dragon fruit sets on the cactus-like trees 30–50 days after flowering and can sometimes have 5-6 cycles of harvests per year. In numerous regions, it has escaped cultivation to become a weed and is classified as an invasive weed in some countries.\n\nOverwatering or excessive rainfall can cause the flowers to drop and fruit to rot. Also, extended over-watering can cause maturing fruit to split on the branch. Birds can be a nuisance. The bacterium \"Xanthomonas campestris\" causes the stems to rot. \"Dothiorella\" fungi can cause brown spots on the fruit, but this is not common. Other fungi known to infect pitaya include \"Botryosphaeria dothidea\", \"Colletotrichum gloesporioides\" and \"Bipolaris cactivora\".\n\nThe fruit's texture is sometimes likened to that of the kiwifruit because of its black, crunchy seeds. The flesh is bland, mildly sweet and low in calories. The seeds have a nutty taste. The seeds are rich in lipids. Dragon fruit is also used to flavor (and color) juices and alcoholic beverages, such as \"Dragon's Blood Punch\" and the \"Dragotini\". The flowers can be eaten or steeped as tea.\n\nThe red and purple colors of \"Hylocereus\" fruits are due to betacyanins, a family of pigments that includes betanin, the same substance that gives beets, Swiss chard, and amaranth their red color.\nAs the nutrient content of raw pitaya has not been thoroughly analyzed as of 2018, the US Department of Agriculture reports one limited entry from a manufacturer, showing that a 100 gram amount of pitaya contains 268 calories, 82% carbohydrates, 4% protein, and 11% of the Daily Value each for vitamin C and calcium (table). \n\nThe fatty acid compositions of two pitaya seed oils were determined as follows:\n"}
{"id": "51114654", "url": "https://en.wikipedia.org/wiki?curid=51114654", "title": "Plyscraper", "text": "Plyscraper\n\nA plyscraper is a skyscraper made of wood. They may alternatively be known as mass timber buildings. The use of bulk timber gives high fire resistance, as a char layer forms in the event of fire. They lock up carbon during construction. They are inherently resistant to ground movement, due to the properties of the timber.\n\n, the tallest habitable building made of timber is the Brock Commons on the campus of the University of British Columbia near Vancouver, Canada. Several proposals for taller timber buildings have been made, including a tower in Tokyo.\n\n"}
{"id": "296636", "url": "https://en.wikipedia.org/wiki?curid=296636", "title": "Power outage", "text": "Power outage\n\nA power outage (also called a power cut, a power out, a power blackout, power failure or a blackout) is a short-term or a long-term loss of the electric power to a particular area.\n\nThere are many causes of power failures in an electricity network. Examples of these causes include faults at power stations, damage to electric transmission lines, substations or other parts of the distribution system, a short circuit, or the overloading of electricity mains.\n\nPower failures are particularly critical at sites where the environment and public safety are at risk. Institutions such as hospitals, sewage treatment plants, mines, shelters and the like will usually have backup power sources such as standby generators, which will automatically start up when electrical power is lost. Other critical systems, such as telecommunication, are also required to have emergency power. The battery room of a telephone exchange usually has arrays of lead–acid batteries for backup and also a socket for connecting a generator during extended periods of outage.\n\nPower outages are categorized into three different phenomena, relating to the duration and effect of the outage:\n\nIn power supply networks, the power generation and the electrical load (demand) must be very close to equal every second to avoid overloading of network components, which can severely damage them. Protective relays and fuses are used to automatically detect overloads and to disconnect circuits at risk of damage.\n\nUnder certain conditions, a network component shutting down can cause current fluctuations in neighboring segments of the network leading to a cascading failure of a larger section of the network. This may range from a building, to a block, to an entire city, to an entire electrical grid.\n\nModern power systems are designed to be resistant to this sort of cascading failure, but it may be unavoidable (see below). Moreover, since there is no short-term economic benefit to preventing rare large-scale failures, researchers have expressed concern that there is a tendency to erode the resilience of the network over time, which is only corrected after a major failure occurs. In a 2003 publication, Carreras and co-authors claimed that reducing the likelihood of small outages only increases the likelihood of larger ones. In that case, the short-term economic benefit of keeping the individual customer happy increases the likelihood of large-scale blackouts.\n\nThe Senate Committee on Energy and Natural Resources held a hearing in October 2018 to examine \"black start\", the process of restoring electricity after a system-wide power loss. The hearing's purpose was for Congress to learn about what the backup plans are in the electric utility industry in the case that the electric grid is damaged. Threats to the electrical grid include cyberattacks, solar storms, and severe weather, among others. For example, the \"Northeast Blackout of 2003\" was caused when overgrown trees touched high-voltage power lines. Around 55 million people in the U.S. and Canada lost power, and restoring it cost around $6 billion.\n\nComputer systems and other electronic devices containing logic circuitry are susceptible to data loss or hardware damage that can be caused by the sudden loss of power. These can include data networking equipment, video projectors, alarm systems as well as computers. To protect computer systems against this, the use of an uninterruptible power supply or 'UPS' can provide a constant flow of electricity if a primary power supply becomes unavailable for a short period of time. To protect against surges (events where voltages increase for a few seconds), which can damage hardware when power is restored, a special device called a surge protector that absorbs the excess voltage can be used.\n\nRestoring power after a wide-area outage can be difficult, as power stations need to be brought back on-line. Normally, this is done with the help of power from the rest of the grid. In the total absence of grid power, a so-called black start needs to be performed to bootstrap the power grid into operation. The means of doing so will depend greatly on local circumstances and operational policies, but typically transmission utilities will establish localized 'power islands' which are then progressively coupled together. To maintain supply frequencies within tolerable limits during this process, demand must be reconnected at the same pace that generation is restored, requiring close coordination between power stations, transmission and distribution organizations.\n\nIt has been argued on the basis of historical data and computer modeling that power grids are self-organized critical systems. These systems exhibit unavoidable disturbances of all sizes, up to the size of the entire system. This phenomenon has been attributed to steadily increasing demand/load, the economics of running a power company, and the limits of modern engineering. While blackout frequency has been shown to be reduced by operating it further from its critical point, it generally isn’t economically feasible, causing providers to increase the average load over time or upgrade less often resulting in the grid moving itself closer to its critical point. Conversely, a system past the critical point will experience too many blackouts leading to system-wide upgrades moving it back below the critical point. The term critical point of the system is used here in the sense of statistical physics and nonlinear dynamics, representing the point where a system undergoes a phase transition; in this case the transition from a steady reliable grid with few cascading failures to a very sporadic unreliable grid with common cascading failures. Near the critical point the relationship between blackout frequency and size follows a power-law distribution. \n\nOther leaders are dismissive of system theories that conclude that blackouts are inevitable, but do agree that the basic operation of the grid must be changed. The Electric Power Research Institute champions the use of smart grid features such as power control devices employing advanced sensors to coordinate the grid. Others advocate greater use of electronically controlled high-voltage direct current (HVDC) firebreaks to prevent disturbances from cascading across AC lines in a wide area grid.\n\nCascading failure becomes much more common close to this critical point. The power-law relationship is seen in both historical data and model systems. The practice of operating these systems much closer to their maximum capacity leads to magnified effects of random, unavoidable disturbances due to aging, weather, human interaction etc. While near the critical point, these failures have a greater effect on the surrounding components due to individual components carrying a larger load. This results in the larger load from the failing component having to be redistributed in larger quantities across the system, making it more likely for additional components not directly affected by the disturbance to fail, igniting costly and dangerous cascading failures. These initial disturbances causing blackouts are all the more unexpected and unavoidable due to actions of the power suppliers to prevent obvious disturbances (cutting back trees, separating lines in windy areas, replacing aging components etc.). The complexity of most power grids often makes the initial cause of a blackout extremely hard to identify.\nIn 2002, researchers at Oak Ridge National Laboratory (ORNL), Power System Engineering Research Center of the University of Wisconsin (PSerc), and the University of Alaska Fairbanks proposed a mathematical model for the behavior of electrical distribution systems.\nThis model has become known as the OPA model, a reference to the names of the authors' institutions.\nOPA is a cascading failure model. Other cascading failure models include Manchester, Hidden failure, CASCADE, and Branching.\n\nThe effects of trying to mitigate cascading failures near the critical point in an economically feasible fashion are often shown to not be beneficial and often even detrimental. Four mitigation methods have been tested using the \"OPA\" blackout model:\n\nIn addition to the finding of each mitigation strategy having a cost-benefit relationship with regards to frequency of small and large blackouts, the total number of blackout events was not significantly reduced by any of the above-mentioned mitigation measures.\n\nA complex network-based model to control large cascading failures (blackouts) \"using local information only\" was proposed by A. E. Motter.\n\nOne of the proposed solutions proposed to reduce the impact of power outage was introduced by M. Saleh.\n\nUtilities are measured on three specific performance measures:\n\n"}
{"id": "11464020", "url": "https://en.wikipedia.org/wiki?curid=11464020", "title": "Rainer Nõlvak", "text": "Rainer Nõlvak\n\nRainer Nõlvak is an Estonian entrepreneur and nature protector who is\nChairman of the board of Estonian Nature Fund.\n\nRainer Nõlvak has advocated for the Estonian energy industry to move away from oil shale\nand move towards renewable energy systems. He has published the \"Green Energy\" program.\n\nHe was among the organizers of Let's Do It 2008, a civic action with 50 000 volunteers participating in cleaning up the countryside of Estonia in one day. Because of this he received the 2008 Estonian Volunteer of the Year national award. The movement has initiated the global Let's Do It! World action.\n\nHe founded the following companies: Microlink Baltics, Curonia Research, Celecure.\n\n"}
{"id": "29420608", "url": "https://en.wikipedia.org/wiki?curid=29420608", "title": "Renault Symbol", "text": "Renault Symbol\n\nThe Renault Symbol, or Thalia in some markets, is a subcompact car produced by the French automobile manufacturer Renault. It was introduced in late 1999, under the Clio Symbol name, as the sedan version of the second generation Renault Clio, and unlike the hatchback it was marketed only in those countries where saloons were traditionally preferred over hatchbacks, while it was not available in Western Europe.\n\nThe second generation has a different design than the third generation Clio and is built on the platform of the first generation car. A third generation has been introduced in late 2012, as a rebadged version of the second generation Dacia Logan.\n\nIts key markets are Central and Eastern Europe, Latin America, the Maghreb and the Persian Gulf states, most notably countries like Turkey, Brazil, Romania, Russia, Algeria, Colombia and Tunisia. \n\nIn late 1999, the Clio Symbol began production in Turkey, as the sedan version of the Clio II. It was subsequently launched in other countries, under different names, depending of the market: Clio Symbol, Thalia, Clio Sedan, Clio \"4 Puertas\", Symbol, or Clio Classic. The car was intended for sale in developing countries, where saloons were traditionally preferred over hatchbacks, most notably in Eastern Europe. In some Latin American markets, like Chile and Mexico, the facelifted model was offered as Nissan Platina, with slight changes at the front of the car to make it resemble the Nissan Altima. It is longer by than the hatchback and has a larger boot of .\n\nThe main production site of the model has been the Oyak-Renault plant in Bursa, Turkey, where manufacturing started in 1999. Worldwide, it was also built since 2000 in Argentina at the plant in Santa Isabel, Córdoba, in Brazil at the Ayrton Senna complex in Sao José dos Pinhais, near Curitiba, and since 2001 in Colombia by Sofasa. In 2002, it began assembly in Mexico, rebadged as Nissan Platina, at the Nissan Mexicana plant in Aguascalientes. For a short time, between the end of 2002 and 2004, it was also assembled in Russia by Avtoframos. The Platina was dropped following the 2010 model year, and was replaced by the March in its Mexican assembly plant and the line-up.\n\nAt the beginning it was available with only two engine options in Europe: a 1.4 litre 8-valve (75 hp, 114 Nm) and a 1.4 litre 16-valve (98 hp, 127 Nm), both of them with multi-point fuel injection. In Argentina it was manufactured with a 1.6 litre 16-valve petrol (100 hp; 74 kW) engine and a 1.9 litre diesel (65 hp; non-turbo), in Colombia with the 1.4 litre 8-valve, and in Brazil either with the 1.6 litre 16-valve or with a 1.0 16-valve petrol (70 hp; 52 kW). Initial trim designations were \"RN\" (\"RNA, RND\") and \"RT \" (\"RTE, RTD\"), but later they received names of their own: \"Authentique\", \"Expression\", \"Dynamique\", \"Alizé\", \"Privilège\" etc.\n\nIt was facelifted in March 2002, gaining the new front end of the hatchback Clio, black moldings on the bumpers, slightly revised interior (electric window buttons moved from the central console onto the doors, the steering wheel got a new design) and improved safety levels. New engines were also added: a 1.5 litre diesel (65 hp, 160 Nm), and the 1.6 litre 16-valve petrol (105 hp, 148 Nm) in Europe, which was only available with the \"Dynamique\" trim level. Throughout the following years, two other versions of the dCi engine were featured in the range: the 65 hp version was upgraded to 70 hp (following the introduction of the Euro IV emission standards from January 2005) and a more powerful version of 80 hp and 185 Nm (in the second half of 2004). A new 1.2 litre 16-valve engine (75 hp, 105 Nm) was introduced the first half of 2006. An automatic transmission version was available, but only with the 1.4 16-valve petrol engine.\n\nThe new front end was adopted in South America from 2003. In Brazil two engines were adapted to run with flexible-fuel: the 1.0 litre 16-valve (76 hp on petrol, 77 hp on alcohol) and the 1.6 litre 16-valve (110 hp on petrol, 115 hp on alcohol); while in Argentina a CNG version was introduced towards the end of the year. The interior of the pre-facelifted model was kept in the South American facelift version. In Colombia, it was added the automatic transmission in 2004 to the Symbol \"Expression\", and the new interior of the facelift model was added in 2008, with some differences from the European version, including the location of the electric window controls, the seats and the radio, and it kept the exterior of the phase II Symbol. The \"Alizé\" model included front electric windows and driver airbag; the \"Expression\" included electric mirrors, digital clock, passenger airbag, ABS and foglights. The 2008 model also included a 1.6 litre petrol engine, replacing the 1.4 litre one.\nThe Nissan Platina was available only with the 1.6 litre 16-valve petrol engine (110 hp; 82 kW). It had four trim levels, baptized \"Grado Q\", \"Grado K\", \"Grado K plus\" and \"Grado A\", the same as the top cards in a playing deck. From the 2008 model year, they changed to \"Custom\", \"Emotion\", \"Premium\" and \"Premium A\". It was sold either with a manual gearbox or with an automatic one with overdrive system.\n\nIn the first half of 2005, minor changes were done to the Renault models. These included clear tail lights and side repeaters, a slightly modified grille and some of the display units passing form analogue to digital.\n\nIn the beginning of 2006, an improved version of the sedan was offered in Argentina and Brazil, and from fall in Europe. At the exterior, it now had body coloured bumpers, without moldings and regardless of the trim level. The shape of the grille was revised and the boot got a new handle with the Renault logo integrated, similar to the one used on Laguna, as well as the new styling of the Renault word. Four new colours were made available and two distinctive new rim designs. For the European model and the Platina, the interior was upgraded to the one used in the facelifted Clio II, with very minor parts commonality with Mégane II, as well as new standard and optional equipment, such as automatic air conditioning and a CD player.\n\nThis model was offered in three equipment levels: \"Authentique\", \"Expression\" and \"Dynamique\". \n\"Expression\" included driver airbag, air conditioning, trip computer, electric mirrors, electric front windows, CD-player and height adjustable steering wheel. \"Dynamique\" added passenger airbag, ABS, rear electric windows, body coloured door handles and alloy rims, although the ABS and passenger airbag were optionals that could be added to the lower levels too. Automatic air conditioning was available as an extra feature.\n\nIt scored 12 points out of 16 in a frontal crash test conducted by the Russian magazine Autoreview in 2002, which was considered equivalent to the result scored by the hatchback version in the Euro NCAP testing. Over 600,000 units were sold worldwide since the release of this model.\n\nThe second generation uses the running gear of the previous generation car and has a different design than the third generation Clio. The name Clio was dropped in favour to just Symbol, or Thalia in those markets where this name was used on the previous generation.\n\nIt was revealed for the first time in August 2008 at the Moscow International Motor Show and went on sale between September and November. The second generation car is longer than the previous by and has a boot capacity of .\n\nThe new Symbol was designed jointly by Renault's engineering teams in France, Turkey and Romania, for countries in the Eastern and Central Europe, Russia, Turkey, North Africa and as well the Persian Gulf States. Development of the project (codenamed \"L35\") took 26 months. The main production site remains the Oyak-Renault plant in Bursa, Turkey, and from 2009 it is also manufactured in Santa Isabel, Argentina, for the South American market and only with the 1.6 litre 16-valve engine.\n\nIt is available in three equipment levels: \"Authentique\", \"Expression\" and \"Privilège\". Features included by \"Authentique\" are driver airbag, rev-counter or body-coloured front and rear bumpers. \"Expression\" adds hydraulic power steering, trip computer, electric front windows, height adjustable steering wheel and front seats, folding rear bench seat, rear headrests and body-coloured mirrors and electric central locking with remote control. \"Privilège\" adds electric mirrors, leather-trimmed steering wheel, foglamps, rear electric windows and radio-CD player with MP3 playback.\n\nIn terms of safety, the new Symbol offers driver airbag, passenger airbag, two lateral airbags (depending on version), anti-lock braking system (ABS) and electronic brakeforce distribution (EBD), anti-intrusion strengthening in front and rear doors, height-adjustable front seatbelts, power steering and rear parking sensors.\n\nIn December 2008, the second generation of the Renault Symbol was awarded the \"Autobest 2009\" prize, by a jury made up of journalists from fifteen countries, mainly emerging markets in Eastern and Central Europe.\n\nThe Turkish police forces use second generation Renault Symbol cars.\n\nAccording to Renault, the engines used on this model are of the latest generation at the date of release.\n\nThe third generation was revealed at the 2012 Istanbul Motor Show, and it is a rebadged version of the second generation Dacia Logan. It is manufactured in Bursa, Turkey, and went sale on the Turkish market at the beginning of 2013, also being released in Algeria and Tunisia.\n\nIt is also manufactured and sold in Brazil since November 2013, as the Renault Logan. In markets such as Chile it is marketed as the Renault Symbol.\n\nIn November 2014, the Renault Symbol began assembly from CKD kits in a new plant in Oran, Algeria. Its production capacity is 25,000 units per year, which may eventually be increased to 75,000 vehicles per year.\n\nThe model produced for South America (where it is marketed mainly as the Renault Logan) and Turkey and Algeria retains small design differences, both at the exterior and the interior. The interior differs mainly at the central part of the dashboard, whereas the exterior has a different front end design, incorporating the new large Renault badge, a different headlamp configuration and mirror-mounted indicators.\n\nIn Turkey, it is offered with a 1.2-litre 16-valve petrol engine, developing and , or a 1.5-litre diesel engine, developing and . In both cases, they are coupled to a 5-speed manual transmission. It is available in seven colours and two trim levels.\n\nIn Chile, it is equipped with a 1.6-litre petrol engine, capable of developing and . It comes in three trim levels: Expression, Dynamique and Privilege.\n\nIn the first quarter of 2017, it was facelifted, receiving design modifications to the front and more significantly to the rear design, but also to the interior, in Turkey (and nearby export markets) and, as well, in Chile. An important addition is the automatic manual transmission, called Easy-R.\n\nThe facelifted model is slightly different in terms of design and even equipment from the Renault Logan offered in the rest of Latin America.\n\nAlso in the first quarter of 2017, the model was launched in several Gulf states, such as Saudi Arabia, the United Arab Emirates, and Qatar.\n\n\n"}
{"id": "32473107", "url": "https://en.wikipedia.org/wiki?curid=32473107", "title": "Rooftop photovoltaic power station", "text": "Rooftop photovoltaic power station\n\nA rooftop photovoltaic power station, or rooftop PV system, is a photovoltaic system that has its electricity-generating solar panels mounted on the rooftop of a residential or commercial building or structure. The various components of such a system include photovoltaic modules, mounting systems, cables, solar inverters and other electrical accessories.\n\nRooftop mounted systems are small compared to ground-mounted photovoltaic power stations with capacities in the megawatt range. Rooftop PV systems on residential buildings typically feature a capacity of about 5 to 20 kilowatts (kW), while those mounted on commercial buildings often reach 100 kilowatts or more.\n\nThe urban environment provides a large amount of empty rooftop spaces and can inherently avoid the potential land use and environmental concerns. Estimating rooftop solar insolation is a multi-faceted process, as insolation values in rooftops are impacted by the following:\n\nThere are various methods for calculating potential solar PV roof systems including the use of Lidar and orthophotos. Sophisticated models can even determine shading losses over large areas for PV deployment at the municipal level.\n\nIn a grid connected rooftop photovoltaic power station, the generated electricity can sometimes be sold to the servicing electric utility for use elsewhere in the grid. This arrangement provides payback for the investment of the installer. Many consumers from across the world are switching to this mechanism owing to the revenue yielded. A public utility commission usually sets the rate that the utility pays for this electricity, which could be at the retail rate or the lower wholesale rate, greatly affecting solar power payback and installation demand.\n\nThe FIT as it is commonly known has led to an expansion in the solar PV industry worldwide. Thousands of jobs have been created through this form of subsidy. However it can produce a bubble effect which can burst when the FIT is removed. It has also increased the ability for localised production and embedded generation reducing transmission losses through power lines.\n\nA rooftop photovoltaic power station (either on-grid or off-grid) can be used in conjunction with other power components like diesel generators, wind turbines, batteries etc. These solar hybrid power systems may be capable of providing a continuous source of power.\n\nInstallers have the right to feed solar electricity into the public grid and hence receive a reasonable premium tariff per generated kWh reflecting the benefits of solar electricity to compensate for the current extra costs of PV electricity.\n\nAn electrical power system containing a 10% contribution from PV stations would require a 2.5% increase in load frequency control (LFC) capacity over a conventional system - in issue which may be countered by using synchronverters in the DC/AC-circuit of the PV system. The break-even cost for PV power generation was in 1996 found to be relatively high for contribution levels of less than 10%. Higher proportions of PV power generation gives lower break-even costs, but economic and LFC considerations imposed an upper limit of about 10% on PV contributions to the overall power systems.\n\nThere are many technical challenges to integrating large amounts of rooftop PV systems to the power grid. For example: \n\nIn 2018, the national average cost in the United States, after tax credits, for a 6kW residential system was $3.14, with a typical range of $2.71 to $3.57, according to one commericial website.\n\nIn the mid-2000s, solar companies used various financing plans for customers such as leases and power purchase agreements. Customers could pay for their solar panels over a span of years, and get help with payments from credits from net metering programs. As of May 2017, installation of a rooftop solar system costs an average of $20,000. In the past, it was more expensive.\n\nUtility Dive wrote, \"For most people, adding a solar system on top of other bills and priorities is a luxury\" and \"rooftop solar companies by and large cater to the wealthier portions of the American population.\"\n\nMost households that get solar arrays are \"upper middle-income\". The average household salary for solar customers is around $100,000.\n\nHowever, \"a surprising amount of low-income\" customers appeared in a study of income and solar system purchases. \"Based on the findings of the study, GTM researchers estimate that the four solar markets include more than 100,000 installations at low-income properties.\"\n\nA report released in June 2018 by the Consumer Energy Alliance (CEA) that analyzed U.S. solar incentives showed that a combination of federal, state and local incentives, along with the declining net cost of installing PV systems, has caused a greater usage of rooftop solar across the nation. According to \"Daily Energy Insider\", \"In 2016, residential solar PV capacity grew 20 percent over the prior year, the report said. The average installed cost of residential solar, meanwhile, dropped 21 percent to $2.84 per watt-dc in the first quarter of 2017 versus first quarter 2015.\" In fact, in eight states the group studied, the total government incentives for installing a rooftop solar PV system actually exceeded the cost of doing so.\n\nThe Jawaharlal Nehru National Solar Mission of the Indian government is planning to install utility scale grid-connected solar photovoltaic systems including rooftop photovoltaic systems with the combined capacity of up to 100 gigawatts by 2022.\n"}
{"id": "48849471", "url": "https://en.wikipedia.org/wiki?curid=48849471", "title": "Tafila Wind Farm", "text": "Tafila Wind Farm\n\nTafila Wind Farm is a 117 MW wind farm located in Tafilah Governorate, Jordan. The farm consists of 38 turbines and was inaugurated in December 2015 by King Abdullah II. The project, which costed $287 million, is the first wind farm in the Kingdom and the region. The venture aims to diversify energy resources in Jordan and boost the renewable energy contribution in the total energy mix.\n\n"}
{"id": "43107372", "url": "https://en.wikipedia.org/wiki?curid=43107372", "title": "Tethys (database)", "text": "Tethys (database)\n\nTethys is an online knowledge management system that provides the marine renewable energy (MRE) and offshore wind (OSW) energy communities with access to information and scientific literature on the environmental effects of devices. Named after the Greek titaness of the sea, the goal of the Tethys database is to promote environmental stewardship and the advancement of the wind and marine renewable energy communities. The website has been developed by the Pacific Northwest National Laboratory (PNNL) in support of the U.S. Department of Energy (DOE) Wind and Water Power Technologies Office. Tethys hosts information and activities associated with two international collaborations known as Annex IV and WREN, formed to examine the environmental effects of marine renewable energy projects and wind energy projects, respectively.\n\nAs industry, academia, and government seek to develop new renewable energy sources from moving water and wind, there exists an opportunity to gather potential environmental effects of these technologies. Tethys aims to evaluate and measure these effects to ensure that aquatic and avian animals, habitats, and ecosystem functions are not adversely affected, nor that important ocean and land uses are displaced. While these studies are presently scattered among different organizations, Tethys attempts to create a centralized hub where this information can be found. Each document is labeled with an environmental \"stressor\", \"receptor\", and \"interaction\" which categorize the type of potential harm, the affected area of the environment, and the means by which the potential impact may occur. The categories and the technology types covered are listed below:\n\nAnnex IV is a collaborative project among member nations of the IEA Ocean Energy Systems (OES) to examine environmental effects of ocean energy projects and research. There is currently a wide range of ocean energy technologies and devices in development around the world; the few data that exist on environmental effects of these technologies are dispersed amongst different countries and developers. While the US Department of Energy is the operating agent, currently (as of January 2018) 12 out of 25 national involved in the OES initiative are involved: Canada, China, Denmark, Ireland, Japan, Norway, Portugal, South Africa, Spain, Sweden, the United Kingdom, and the United States. There have been three phases of this initiative:\n\nWhile encouraging collaboration, products included two international workshops, a report, and a collection of metadata forms on project sites and research studies. The first workshop was held in Dublin Ireland in September 2010, drawing 58 international experts from various backgrounds to identify key interactions between device and environment, look into data collection details, and to scope out case studies for the report. The second workshop was also held in Dublin Ireland in October 2012, drawing 55 international experts from nine countries to review Tethys for content and functionality, to provide feedback on the report, and to guide future Annex IV activities. The final report focused on three case studies:\n\nThe metadata collection effort included project sites that were performing environmental baseline studies and/or monitoring studies and relevant research studies that were underway. Over 150 forms were collected, providing details on nearly every project that has been deployed to date and on the most current research being conducted. All of this metadata is hosted in Tethys along with associated reports and publications.\n\nDue to the success of the first phase, a second phase was suggested by all parties involved. The primary vision was to identify analysts for each country involved, who could act as representative point of contact for all the Annex IV activities. Activities included maintaining and collecting metadata forms, expanding Tethys to support a community of experts, five international workshops, and a report.\n\nThe first workshop was held in Seattle USA in June 2013, sponsored by the Northwest National Marine Renewable Energy Center, drawing 36 international experts to discuss instrumentation around devices. The second workshop was held in Stornoway UK in April 2014, drawing 45 international participants to discuss best practices for monitoring around devices. The third workshop was held in Wolfville Canada in November 2014, drawing 25 international participants to identify environmental monitoring, regulatory needs, and scientific capabilities to encourage the progression of the industry. The fourth workshop was held in Nantes France in September 2015, drawing 61 international participants to review the planned state of the science report. The fifth workshop was held in Edinburgh UK in February 2016, drawing 40 international participants to examine the collision risk for marine mammals around tidal turbines. These workshops were supplemented with a strong presence at most international conferences on marine renewable energy and the environment.\n\nA thriving international community was fostered through online events such as quarterly webinars and the occasional expert forum, where topical experts from around the world would engage in discussion around controversial subjects. Bi-weekly newsletters and the collection and dissemination of information have also contributed to the international community. The culmination of this phase of Annex IV came with the publication of the 2016 State of the Science report, a detailed description of the environmental effects of marine renewable energy development around the world, derived from years of international cooperation around these issues. The executive summary of the report is available in seven languages and summaries of each environmental issue is available in English.\n\nAnnex IV has been approved for a 4-year extension consisting of three interrelated tracks: (1) information gathering and analysis, (2) information dissemination, and (3) engaging the community to support research and monitoring needs. This phase will continue most ongoing activities, while expanding to engage the regulatory community and address socio-economic issues. The phase will again culminate with a large report and conference partnerships.\n\nWREN (Working Together to Resolve Environmental Effects of Wind Energy), also known as Task 34, was established by the IEA Wind Committee to address environmental issues associated with commercial development of land-based and offshore wind energy projects. While the National Renewable Energy Laboratory (NREL) is the operating agent, currently (as of January 2018) 11 out of 24 nations involved in IEA Wind are involved: Canada, France, Ireland, Netherlands, Norway, Portugal, Spain, Sweden, Switzerland, the United Kingdom, and the United States. There has been two phase of this initiative:\n\nThe goal of WREN is to facilitate international collaboration that advances global understanding of environmental effects of offshore and land-based wind energy development, though the formation of a community of practice around research, monitoring and management of the environmental effects of wind energy development. Two key products were developed during phase 1: 1) Tethys was expanded to include land-based wind and host WREN activities and 2) a white paper focused on adaptive management.\n\nThe second phase of WREN will include the expansion of Tethys, development of several white papers, continuation of the webinar series, and outreach and engagement efforts. All these activities are aimed at contributing to supporting the expansion of land-based and offshore wind energy deployment.\n\nAdditional functionality is regularly added to Tethys in response to peer reviews, surveys, and general comments from users. However, there are primary functions of Tethys that allow users to experience community, search through the data, and learn more about the new and exciting field of renewable energy.\n\nThe Knowledge Base is primarily displayed as a table view that utilizes the alphabetical column sorting, facet box selection, and keyword search to allow users to easily sift through the information. Over 3300 media entries relevant to the environmental effects of wind and marine renewable energy are available, consisting of journal articles, reports, websites, conference papers, presentations, workshop articles, theses, books, book sections, videos, datasets, magazine articles, project site information, and research study information. This is a growing database, where relevant materials that are newly published or discovered will be added.\n\nThe interactive Map Viewer shows the locations of geo-tagged project sites, research studies, and documents gathered from across the world. More than 2100 items appear on the map, a subset of the information available in the Knowledge Base. Users may interact with the map with zooming and panning functions, facet box selection, and a keyword search. Selecting one of the icons will reveal a specific page with more in-depth information. This is a growing database, where relevant materials that are newly published or discovered will be added.\n\nIn an effort to connect members of this growing community, Tethys is meant to act as a hub, providing resources and contacts for those looking for more information. One way is by providing links to similar databases that may have different approaches to viewing data, or that may provide a different focus on the data collected. Another page lists summaries of the regulatory frameworks in many of the major countries, providing links to agencies and laws rather than going into detail. There is also an extensive database of nearly 1200 organizations involved in marine and wind renewable energy and the environment, providing a list of publications affiliated with the organization and some basic information. Members of the Tethys community that have created a free account also have the ability to share their contact information and interests to with other community members in a searchable table.\n\nTethys also houses multimedia in the broadcast tab, meant to engage users in the Tethys community. Everything is freely available to the public and easily searchable.\n\nTethys began in 2011 hosted on a Semantic MediaWiki platform, but migrated to Drupal in early 2013. Drawing on many years of experience and systems development, developers have tailored the website to allow for semantic searches and the organization of data through tagging individual files, documents, and multimedia products. Content is regularly monitored and curated, though suggestions from the user community are always welcome.\n\nWhile Tethys is already internationally recognized as a leading resource for information on environmental impacts of offshore renewable energy, upcoming goals are to build a strong community that can address environmental concerns by pooling international experience and information. This will involve activities such as regular blogging, quarterly webinars, online expert forums, conference engagement, and social media.\n\n\n"}
{"id": "54612063", "url": "https://en.wikipedia.org/wiki?curid=54612063", "title": "The Genius of Birds", "text": "The Genius of Birds\n\nThe Genius of Birds is a 2016 book by nature writer Jennifer Ackerman.\n\n\"The Genius of Birds\" highlights new findings and discoveries in the field of bird intelligence. The book explores birds as thinkers (contrary to the cliché \"bird brain\") in the context of observed behavior in the wild and brings to it the scientific findings from lab and field research.\n\nNew research suggests that some birds, such as those in the family corvidae, can rival primates and even humans in forms of intelligence. Much like humans, birds have enormous brains relative to the rest of their bodies.\n\nAckerman highlights the complex social structures of avian society. They are capable of abstract thinking, problem solving, recognizing faces, gift giving, sharing, grieving, and meaningful communication with humans. Ackerman goes in depth to highlight scientific studies that uncover behavior such as tool usage, speaking in regional accents, navigation, and theory of mind.\n\nThe book is a New York Times Bestseller, and was named one of the 10 best nonfiction books of 2016 by The Wall Street Journal.\n"}
{"id": "106806", "url": "https://en.wikipedia.org/wiki?curid=106806", "title": "Yuma, Arizona", "text": "Yuma, Arizona\n\nYuma () is a city in and the county seat of Yuma County, Arizona, United States. It is located in the southwestern corner of the state, and the population of the city was 93,064 at the 2010 census, up from the 2000 census population of 77,515.\n\nYuma is the principal city of the Yuma, Arizona, Metropolitan Statistical Area, which consists of Yuma County. According to the United States Census Bureau, the 2014 estimated population of the Yuma MSA is 203,247. More than 85,000 retirees make Yuma their winter residence.\n\nYuma is located in the Sonoran Desert, Yuma Desert sub-region.\n\nThe area's first settlers for thousands of years were Native American cultures and historic tribes. Their descendants now occupy the Cocopah and Quechan reservations.\n\nIn 1540, Spanish colonial expeditions under Hernando de Alarcon and Melchior Diaz visited the area and immediately recognized the natural crossing of the Colorado River as an ideal spot for a city. The Colorado River narrows to slightly under 1,000 feet wide in one area. Military expeditions that crossed the Colorado River at the Yuma Crossing include Juan Bautista de Anza (1774), the Mormon Battalion (1848) and the California Column (1862).\n\nDuring and after the California Gold Rush to the late 1870s, the Yuma Crossing was known for its ferry crossings for the Southern Emigrant Trail. This was considered the gateway to California, as it was one of the few natural spots where travelers could cross the otherwise very wide Colorado River.\n\nFollowing the United States establishing Fort Yuma, two towns developed one mile downriver. The one on the California side was called Jaeger City, named after the owner of Jaeger's Ferry, which crossed the river there. It was for a time the larger of the two, with the Butterfield Overland Mail office and station, two blacksmiths, a hotel, two stores, and other dwellings.\n\nThe other was called Colorado City. Developed on the south side of the river in what is now Arizona by speculator Charles Poston, it was the site of the custom house. When started, it was just north of the border between Mexican-ruled Sonora, Mexico and California. After the Gadsden Purchase by the United States, the town bordered on the Territory of New Mexico. This area was designated as the Territory of Arizona in 1863. The Colorado City site at the time was duly registered in San Diego; both banks of the Colorado River just below its confluence with the Gila were recognized as being within the jurisdiction of California. The county of San Diego collected taxes from there for many years.\n\nFrom 1853 a smaller settlement, Arizona City, grew up on the high ground across from the fort and was organized under the name of its post office in 1858. It had adobe dwellings, two stores and two saloons. Colorado City and Jaeger City were almost completely destroyed by the Great Flood of 1862 and had to be rebuilt on higher ground. At that time Colorado City became part of Arizona City. It took the name Yuma in 1873.\n\nFrom 1854, Colorado City was the major steamboat stop for traffic up and down the Colorado River. After the 1862 flood, it became part of Arizona City. The steamboats transported passengers and equipment for the various mines and military outposts along the Colorado; Colorado City was the terminus of wagon traffic up the Gila River into New Mexico Territory. They offloaded the cargo from ships at the mouth of the Colorado River at Robinson's Landing and from 1864 at Port Isabel. From 1864, the Yuma Quartermaster Depot, today a state historic park, supplied all forts in present-day Arizona, as well as large parts of Colorado and New Mexico. After Arizona became a separate territory, Yuma became the county seat for Yuma County in 1871, replacing La Paz, the first seat.\n\nThe Southern Pacific Railroad bridged the river in 1877, and acquired George Alonzo Johnson's Colorado Steam Navigation Company, the only steamboat company on the river. Yuma became the new base of navigation on the river, ending the need for Port Isabel, which was abandoned in 1879. The warehouses and shipyard there were moved to Yuma.\n\nThe city of Yuma operates as a charter city under the Charter of the City of Yuma. The elected government of the city is the City Council which follows the mayor–council government system and whose members include the following:\n\nThe Mayor of the City of Yuma acts as the chief executive officer of the city, and is elected for a period of four years. The mayor is elected from the city at large. The mayor has the following powers and responsibilities: act as an ex officio chairman of the city council (ensuring that all ordinances thereof are enforced), call and preside over meetings, administer oaths and issue proclamations. The mayor is also recognized as the official head of the city by the courts and has the power to take command of the police and govern the city by proclamation during times of great danger.\n\nThe City of Yuma City Council is the governing body of the City of Yuma and is vested with all powers of legislation in municipal affairs. The council is composed of six council members elected from the city at large for four-year terms, as well as the Mayor of Yuma. A deputy mayor is also elected by the Council who shall act as Mayor during the temporary absence of the mayor. \nThe City Council appoints a city administrator who acts as the chief administrative officer of the city. The city administrator is directly responsible to the City Council for the administration of all city affairs placed in his charge by the City Charter, or by ordinances passed by the Council. Some of the administrator's duties include: see that all laws and provisions of the City Charter are faithfully executed, prepare and submit the annual budget and capital program to the City Council and keep the City Council fully advised as to the financial condition and future needs of the city.\n\nYuma is located near the borders of California to the west and Mexico to the south, and just west of the Gila River's confluence with the Colorado. The city is approximately 60 miles from the Gulf of California (Sea of Cortez), a branch of the Pacific.\n\nAccording to the United States Census Bureau, the city has a total area of , of which is land and (0.07%) is water.\n\nYuma is noted for its weather extremes. Of any populated place in the contiguous United States, Yuma is the driest, the sunniest, and the least humid, has the lowest frequency of precipitation, and has the highest number of days per year–175–with a daily maximum temperature of or higher.\n\nYuma features a hot desert climate (Köppen climate classification \"BWh\"), with extremely hot summers and warm winters. Atmospheric humidity is usually very low except during what are called \"Gulf surges\", when a maritime tropical air mass from the Gulf of California is drawn northward, usually in connection with the summer monsoon or the passage of a tropical storm to the south.\n\nThe sun is said to shine during about 90% of the daylight hours, making Yuma the sunniest place in the world. An annual mean sunshine duration of 4,015 h would have been recorded in the desert city.\n\nOn average, Yuma receives of rain annually. Even in the wettest year of 2005, only fell. The driest year at Yuma Airport has been 2007, with only recorded. On average, August and September are the wettest months of the year, while June is the driest.\n\nIn 1995, Yuma reached its all-time high of . The lowest recorded temperature was on the Yuma-Mesa area in January 2007. The temperature fell to for approximately two hours and harmed many crops grown in and around Yuma. The crop that suffered the most damage was the citrus on the Yuma mesa, most notably the lemon crop. It suffered a 75% to 95% loss of crop and trees, as stated by the Arizona Department of Agriculture in a February 2007 report. On average, the temperature lowers to the freezing mark in less than a quarter of years, and there are 118 days per year that reach or exceed , usually from April through October. During July and August the temperature fails to reach the century mark on at most, only a few days.\n\nIn 1997, the desert city sustained a full tropical storm after Hurricane Nora made landfall at the mouth of the Colorado River and quickly moved due north along it. This rare event cut power to 12,000 customers in Yuma, and dropped of rain at Marine Corps Air Station Yuma. The last time a hurricane had hit near Yuma was in mid-August 1977, when similar falls were recorded.\n\nAs of the census of 2010, there were 93,064 people. There were 38,626 housing units in Yuma city, 79.5% of which were occupied housing units. The racial makeup of the city was 68.8% White, 3.2% Black or African American, 1.8% Native American, 1.9% Asian, 0.2% Pacific Islander, and 4.5% from two or more races. 54.8% of the population were Hispanic or Latino of any race.\n\nAs of the census of 2000, there were 77,515 people, 26,649 households, and 19,613 families residing in the city. The population density was 726.8 people per square mile (280.6/km²). There were 34,475 housing units at an average density of 323.3 per square mile (124.8/km²). The racial makeup of the city was 68.3% White, 3.2% Black or African American, 1.5% Native American, 1.5% Asian, 0.2% Pacific Islander, 21.4% from other races, and 3.9% from two or more races. 45.7% of the population were Hispanic or Latino of any race.\n\nThere were 26,649 households out of which 38.8% had children under the age of 18 living with them, 56.6% were married couples living together, 13.1% had a female householder with no husband present, and 26.4% were non-families. 21.7% of all households were made up of individuals and 9.8% had someone living alone who was 65 years of age or older. The average household size was 2.79 and the average family size was 3.27.\n\nIn the city, the population was spread out with 29.6% under the age of 18, 11.9% from 18 to 24, 27.1% from 25 to 44, 17.5% from 45 to 64, and 13.9% who were 65 years of age or older. The median age was 31 years. For every 100 females, there were 99.1 males. For every 100 females age 18 and over, there were 97.2 males.\n\nAccording to the 2006 American Community Survey estimate, the median income for a household in the city was $39,885, and the median income for a family was $41,588. Males had a median income of $35,440 versus $27,035 for females. The per capita income for the city was $18,393. About 14.1% of families and 16.9% of the population were below the poverty line, including 23.4% of those under age 18 and 13.9% of those age 65 or over.\n\nHigh unemployment remains an issue in Yuma. Citing April 2014 data, the Bureau of Labor Statistics ranked Yuma as having the highest unemployment rate in the United States at 23.8 percent, above the 21.6 percent in El Centro, California. Yuma's agricultural workforce, which adjusts to the picking season, is cited by the Arizona Department of Commerce as the reason for the apparent high unemployment.\n\nThe Yuma Metropolitan Statistical Area has the highest unemployment rate in the United States as of 2018 at 20.9%. A large percentage of the work force is employed seasonally in agriculture, contributing to apparent unemployment.\n\nAccording to the City's 2016 \"Comprehensive Annual Financial Report\", the top employers in the Yuma Metropolitan Statistical Area in 2015 were:\n\nOther large employers include Bose, Dole Fresh Vegetables and Shaw Industries.\n\nYuma contains the historical Yuma Territorial Prison, the Yuma Quartermaster Depot State Historic Park (formerly known as the Yuma Crossing Historic Park), and a historic downtown area. Yuma is an \"Arizona Main Street City.\" Because of budget cutbacks at the state level, Arizona State Parks no longer operates the Territorial Prison and Quartermaster Depot. They are now operated by the Yuma Crossing National Heritage Area and the City of Yuma. The Yuma Visitors' Bureau oversees the Welcome Center at the Quartermaster Depot and is the official visitors' center for the Yuma Community.\n\nNear Yuma are the Kofa Mountain Range and wildlife refuge, Martinez and Mittry Lakes, and the Algodones Dunes.\n\nThe city is the location of the Marine Corps Air Station Yuma, which conducts an annual air show and many large-scale military exercises. There is also the Yuma Proving Ground, an Army base that tests new military equipment. Yuma Proving Ground is also home to the Special Operations Free Fall School, which provides training in free-fall parachute operations to Special Forces units in all branches of service, as well as those of other nations.\n\nThe Colorado River runs along the north and west side of town, serving as the border between Arizona and California. Yuma is an important station for trucking industry movement of goods between California, Arizona and Mexico.\n\nThe Rialto movie theatre once owned a Kilgen pipe organ, one of the most expensive pipe organs to have been made. Originally played as accompaniment to silent films, it has been moved to the Yuma Theatre.\n\nEvery February residents and visitors enjoy the annual rodeo, the Yuma Jaycees Silver Spur Rodeo. A parade opens the events. Cowboys and cowgirls from all over the country compete in the festivities.\n\nThe Yuma County Fair takes place annually in the spring at the fairgrounds in Yuma.\n\nA number of movies have been shot in the Yuma area, including \"The Sheik\" (1921), \"Beau Geste\" (1926), \"Beau Geste\" (1939), \"Beau Geste\" (1966), \"Gunga Din\" (1939), \"Flight of the Phoenix\" (1966), \"Return of the Jedi\" (1983) and \"Spaceballs\" (1987).\n\nYuma has a minor-league-caliber ballpark, Desert Sun Stadium, which was home to the Yuma Desert Rats of the North American League and site of home games of two (previously four) teams for the Arizona Winter League. The San Diego Padres used the field as a spring training facility from 1969 until 1993. and a Japanese baseball team, the Yakult Swallows, used the field for spring training from 1995 to 2015. Many local club sports exist in the area as well, including the Yuma Sidewinders Rugby Football Club. The rugby team participates in the Division III Arizona Men's Rugby League, and travels throughout Arizona, California and Nevada, as well as playing home games in Yuma.\n\nThe city is zoned to the Yuma Union High School District. Yuma has five public high schools: Yuma Union High School, Kofa High School, Cibola High School, Gila Ridge High School, Vista Alternative High School; and the private Yuma Catholic High School and Calvary Baptist School. Yuma also has four charter high schools: AZTEC High School, Carpe Diem Collegiate High School, Harvest Preparatory Academy, and YPIC Charter High School.\n\nYuma has two main elementary school districts, District One and Crane District, which include several schools as well as junior high schools. Yuma has four charter elementary school: AmeriSchools Academy North and South, Harvest Preparatory Academy, and Desert View Academy.\n\nAdditionally, Yuma has 6 six private elementary schools: Yuma Lutheran School, Yuma Adventist Christian School, Immaculate Conception School, St. Francis of Assisi School, Calvary Baptist School and Southwestern Christian School.\n\nArizona Western College is Yuma's community college, serving primarily as a choice for transfer students and those with shorter career goals.\n\nNorthern Arizona University has a branch campus in Yuma, as does the for-profit University of Phoenix.\n\nYuma is served by the Yuma County Library District which consists of a Main Library and several branches, including sites in Somerton, Wellton, Fortuna Foothills, and San Luis. A new main state-of-the-art library is now open.\n\n\n\n\n\n"}
