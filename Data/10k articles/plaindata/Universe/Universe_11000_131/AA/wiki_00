{"id": "12823159", "url": "https://en.wikipedia.org/wiki?curid=12823159", "title": "Ahu Tongariki", "text": "Ahu Tongariki\n\nAhu Tongariki is the largest ahu on Easter Island. Its moai were toppled during the island's civil wars and in the twentieth century the ahu was swept inland by a tsunami. It has since been restored and has fifteen moai including an 86 tonne moai that was the heaviest ever erected on the island. Ahu Tongariki is one kilometer from Rano Raraku and Poike in the Hotu-iti area of Rapa Nui National Park. All the moai here face sunset during Summer Solstice.\n\n\"Ahu Tongariki\" was the main centre and capital of the Hotu Iti, the eastern confederation of the Rapanui. \n\nIts moai were toppled during the island's civil wars. In 1960, a tsunami caused by an earthquake off the coast of Chile swept the Ahu Tongariki inland. \n\nAhu Tongariki was substantially restored in the 1990s by a multidisciplinary team headed by archaeologists Claudio Cristino (Director) and Patricia Vargas (Co-director executive team), in a five-year project carried out under an official agreement of the Chilean Government with Tadano Limited and the University of Chile.\n\nThis ahu is on the south coast of Rapa Nui, close to two previous volcanoes, Rano Raraku and Poike.\n\nPoike is one of the three main extinct volcanoes that form Rapa Nui, which last erupted between 230,000 and 705,000 years ago. Rano Raraku is a volcanic crater formed of consolidated volcanic ash or tuff, which compose the carved moai. Indeed, nearly half (hundreds) of the moai are still buried in the slopes of the Rano Raraku, the main moai quarry. The large, flat plain below Rano Raraku provided easy access to the tuff. \n\n\n"}
{"id": "411356", "url": "https://en.wikipedia.org/wiki?curid=411356", "title": "American Airlines Flight 1420", "text": "American Airlines Flight 1420\n\nAmerican Airlines Flight 1420 was a flight from Dallas-Fort Worth International Airport (DFW) to Little Rock National Airport in the United States. On June 1, 1999, the McDonnell Douglas MD-82 operating for Flight 1420 overran the runway upon landing in Little Rock and crashed. Eleven of the 145 people aboard, the captain and ten passengers, were killed in the crash.\n\nThe aircraft involved in the incident was a McDonnell Douglas MD-82 (registration \"\"), a derivative of the McDonnell Douglas DC-9 and part of the McDonnell Douglas MD-80 series of aircraft. It was delivered new to American Airlines in 1983, and had been operated continuously by the airline since, accumulating a total of 49,136 flight hours. The aircraft was equipped with Pratt & Whitney JT8D-217C turbofan jet engines.\n\nThe aircraft was equipped with X band weather radar, which is susceptible to attenuation during heavy precipitation, and did not have an attenuation alert to warn the flight crew of system impairment during heavy rainfall. The radar weather system had a forward-looking design that offered the flight crew only a limited field of view in front of the aircraft.\n\nFlight 1420 was commanded by Captain Richard Buschmann, age 48. Captain Buschmann was a very experienced chief pilot for American Airlines with 10,234 total flight hours, of which approximately half were accumulated flying the MD-80 series of aircraft. Buschmann graduated from the United States Air Force Academy in 1972, serving in the Air Force until 1979. Buschmann held the rank of lieutenant colonel with the US Air Force Reserve Command, and was hired by American Airlines in July 1979. Experienced at flying the Boeing 727 for American, he transitioned to flying the twin-engine McDonnell Douglas MD-80 series in 1991.\n\nThe flight's first officer was Michael Origel, age 35. The first officer had been with the airline for less than a year, and only had 182 hours of flight time with American Airlines as an MD-80 pilot. However, the first officer had trained as a pilot with the United States Navy and had prior commercial flight experience as a corporate pilot, with a total of 4,292 hours of flying experience at the time of the incident.\n\nFlight 1420 was staffed with four flight attendants, all of which were qualified on the MD-80 and had recently received refresher training on emergency procedures.\n\nFlight 1420 was scheduled to depart DFW at 20:28 Central Daylight Time and arrive in Little Rock at 21:41. However, the flight crew were advised before boarding Flight 1420 that their departure would be delayed, and that the National Weather Service had issued in-flight weather advisories indicating severe thunderstorms along Flight 1420's planned flight path. Adverse weather caused the plane that was intended for Flight 1420 to be delayed in arriving at DFW. Airline policy set a maximum pilot duty time of 14 hours, and Flight 1420 was the flight crew's last flight of the day. The first officer notified the airline's flight dispatcher that the flight crew would exceed their duty limit and be unable to depart if Flight 1420 did not depart by 23:16. The airline substituted another MD-80, tail number N215AA, which allowed Flight 1420 to depart DFW at 22:40.\n\nAt 23:04, air traffic controllers issued a weather advisory indicating severe thunderstorms in an area that included Little Rock airport. In the event that Flight 1420 was unable to land for any reason, Nashville International Airport was designated as an alternate airport, and the flight also had the option of turning back and landing at DFW. The flight crew witnessed lightning produced by the storm while on approach to Little Rock. However, the flight crew discussed the weather reports and decided to expedite their approach.\n\nAir traffic control at Little Rock originally told Flight 1420 to expect an approach to runway 22L. However, at 23:39, a controller advised Flight 1420 of a wind shear alert and a change in wind direction. As a result, Captain Buschmann requested a change to runway 4R so that the flight would have a headwind during landing, and Flight 1420 was cleared for a visual approach to runway 4R. Because they were already close to the airport, the controller had to direct Flight 1420 away from the airport in order to line them up for a landing on runway 4R. This resulted in Flight 1420 facing away from the airport for several minutes, and because the plane's weather radar had a narrow and forward-facing field of view, the flight crew could not see thunderstorms approaching the airport during their turn. As the aircraft approached runway 4R, a severe thunderstorm arrived over the airport, and at 23:44 the first officer notified the controller that they had lost sight of the runway. The controller then cleared Flight 1420 to land on runway 4R using an instrument landing system (ILS) approach.\n\nThe pilots rushed to land as soon as possible, leading to errors in judgment that included the crew's failure to complete the airline's pre-landing checklist before landing. This was a crucial event in the accident chain, as the crew overlooked multiple critical landing systems on the checklist. The flight crew failed to arm the automatic spoiler system, which automatically moves the spoiler control lever and deploys the spoilers upon landing. The pilots also failed to set the plane's automatic braking system. Autospoilers and autobrakes are essential to ensure the plane's ability to stop within the confines of a wet runway, especially one that is being subjected to strong and gusting winds. The flight crew also failed to set landing flaps, another item on the pre-flight checklist, but as the plane descended past , the first officer realized the flaps were not set and the flight crew set a 40-degree flap setting for landing.\n\nAt 23:49:32, the controller issued its last weather report before Flight 1420 landed, advising that winds at the airport were 330 degrees at 25 knots. The reported winds exceeded the MD-82's 20-knot crosswind limit for landing in reduced visibility on a wet runway. Despite the excessive crosswind and two wind shear reports, Captain Buschmann did not abandon the aircraft's approach into Little Rock, instead deciding to continue the approach to runway 4R.\n\nThe aircraft touched down on Runway 4R at 23:50:20. About 2 seconds after the wheels touched down, First Officer Origel stated: \"We're down. We're sliding!\" Because the pilots failed to arm the autospoiler, the spoilers did not deploy automatically on landing, and the flight crew did not deploy them manually. Spoilers disrupt the airflow over the wings and prevent them from generating lift. That causes the plane's weight to be borne by the landing gear. About 65 percent of Flight 1420's weight would have been supported by the plane's landing gear if the spoilers are deployed, but without the spoilers this number dropped to only 15 percent. With the light loading of the landing gear, the aircraft's brakes were ineffective at slowing down the plane, which continued down the runway at high speed. Directional control was lost when Captain Buschmann applied too much reverse thrust, which reduced the effectiveness of the plane's rudder and vertical stabilizer.\n\nThe aircraft continued past the end of the runway, traveling another 800 feet and striking a security fence and an ILS localizer array. The aircraft then collided with a structure built to support the landing lights for runway 22L, which extended out into the Arkansas River. Such structures are usually frangible, that is, designed to shear off on impact, but because the approach lights were located on the unstable river bank, they were firmly anchored. The collision with the nonfrangible structure crushed the airplane's nose and destroyed the left side of the plane's fuselage, from the cockpit back to the first two rows of coach seating. The impact broke the aircraft apart into multiple large sections, which came to a rest short of the river bank.\n\nCaptain Buschmann and 10 of the plane's 139 passengers died in the crash. This included 2 passengers that died in hospital in the weeks that followed. First Officer Origel, 3 of the 4 flight attendants, and 41 passengers sustained serious injuries.\n\nThe National Transportation Safety Board (NTSB) investigated the crash.\n\nThe NTSB conducted extensive testing in order to determine whether the automatic spoiler and brake systems had been armed by the pilots prior to landing.\n\nThe plane's cockpit voice recorder (CVR) was reviewed, and sounds that were consistent with the spoiler arming or automatically deploying were not recorded by the CVR. The NTSB conducted two test flights of American Airlines MD-80 aircraft, which confirmed that arming the spoiler created an audible \"click\" that could be clearly heard on CVR playback. The NTSB also conducted ground tests on similar aircraft, including another American Airlines MD-80 that overran the runway (but was not destroyed) after its autospoiler system failed to deploy during a landing in Palm Springs, California. The NTSB also found that the sounds made by manual deployment of the spoiler were distinguishable from the sounds made by automatic deployment of the spoiler system. No sounds consistent with arming or auto-deployment of the spoiler system were recorded on Flight 1420's CVR.\n\nAfter Flight 1420 and the Palm Springs incident, American Airlines revised its checklist so that pilots would confirm that the spoilers are armed for auto-deployment before landing, confirm spoiler deployment, and deploy spoilers manually if they failed to automatically deploy.\n\nThe NTSB investigation also focused on pilot behavior in inclement weather, to determine what impact the storms had on the pilots' decision-making process while approaching Little Rock Airport.\n\nExperts from the Massachusetts Institute of Technology (MIT) created a study recording behavior of pilots landing at Dallas/Fort Worth Airport, which aimed to see whether pilots were willing to land in thunderstorms. Within a total of 1,952 thunderstorm encounters, 1,310 pilots (67 percent) flew into thunderstorms during landing attempts. The study found that pilots exhibited more recklessness if they fell behind schedule, if they were attempting to land at night, and if aircraft in front of them successfully landed in bad weather. In a later interview, Greg Feith, the lead NTSB investigator, said he was surprised to learn that pilots exhibited this behavior. Feith added that the pilots may have exhibited \"get there-itis\" as the pilots knew that they were approaching their 14-hour duty limits.\n\nMultiple lawsuits were filed after the crash and on December 15, 1999, the Judicial Panel on Multidistrict Litigation consolidated the various federal lawsuits over the crash for consolidated and coordinated pretrial proceedings and assigned the case to the late United States District Court Senior Judge Henry Woods of the Eastern District of Arkansas. In the lawsuits the passengers sought compensatory and punitive damages from American Airlines.\n\nFrom the beginning Judge Woods separated the passenger cases into two groups: domestic and international passengers, because different laws governed the rights of the claimants in each category. For example, passengers traveling on international tickets were prohibited by an international treaty (the Warsaw Convention) from recovering punitive damages. Therefore, Judge Woods ruled only the domestic passengers would be permitted to pursue punitive damages claims.\n\nMeanwhile, the National Transportation Safety Board issued its determination on the cause of the crash:\n\nThe compensatory damages claims proceeded first. American Airlines \"admitted liability for the crash and individual trials were scheduled to assess the proper amount of compensatory damages. Thereafter American Airlines reached settlement agreements with a majority of the domestic Plaintiffs.\"\n\n\"Three compensatory damages trials involving domestic Plaintiffs were ultimately tried to a jury and awards of $5.7 million, $3.4 million and $4.2 million were made.\" These three Plaintiffs pursued but ultimately lost their claims for punitive damages. The District Court granted summary judgment in American Airlines' favor on punitive damages, finding under Arkansas law the evidence was insufficient to submit the issue to a jury to decide. This ruling was later upheld on appeal.\n\nIn the only liability trial arising out of the crash of Flight 1420, a federal jury in Little Rock awarded Captain Buschmann's family $2 million in wrongful death damages in a lawsuit they filed against the Little Rock National Airport. The jury decided Captain Buschmann's death occurred because the aircraft collided with illegal non-frangible approach light supports erected in what should have been the Runway Safety Area. It was concluded that Little Rock National Airport failed to comply with airport safety standards. Captain Buschmann's estate presented evidence the spoilers were deployed and malfunctioned (not the captain's fault), and that the aircraft was not in turbulence. The jury rejected the airport's argument that Captain Buschmann was at fault in causing his own death.\n\nIt has been stated the jury verdict completely absolved Captain Buschmann of all fault for the crash. However, 1) the National Transportation Safety Board has not changed its probable cause ruling and 2) American Airlines admitted liability for the crash and \"paid many millions of dollars in damages to the passengers and their families.\"\n\nAccording to a comment made about 10 years after the crash by David Rapoport, a lawyer who was a member of the PSC, \"after all these years [whether Captain Buschmann was \"absolved\" of all responsibility for the crash] is still a matter reasonable people who are fully informed may disagree on\", however, there should be consensus \"flight operations should not be conducted in the terminal area when thunderstorms are on the flight path; and non-frangible objects should not be placed where it is foreseeable an aircraft may go.\"\n\n\n\nA 2004 memorial ceremony was held adjacent to the airport. Jeana Varnell, one of the survivors, attended the ceremony and in a newspaper article, strongly objected to the memorializing of Captain Buschmann.\n\n\n"}
{"id": "2701330", "url": "https://en.wikipedia.org/wiki?curid=2701330", "title": "Anthelion", "text": "Anthelion\n\nAn anthelion (plural anthelia, from late Greek ανθηλιος, \"opposite the sun\") is a rare optical phenomenon of the halo family. It appears on the parhelic circle opposite to the sun as a faint white spot, not unlike a sundog, and may be crossed by an X-shaped pair of diffuse arcs.\nHow anthelia are formed is disputed. Walter Tape, among others, has argued they are not separate haloes, but simply where various haloes caused by horizontally oriented column-shaped ice crystals coincide on the parhelic circle to create a bright spot. If this theory is correct, anthelia should only appear together with these other haloes.\n\nHowever, anthelia occur unaccompanied by other plate crystal haloes, thus scientists have produced alternative explanations. The Dutch professor S.W. Visser proposed they form by two exterior light reflections in quadrangular prisms, while Robert Greenler has suggested two interior reflections in column-shaped crystals produces the phenomenon.\n\nWhile the anthelion area is usually sparse on haloes, in a complex display it features various rare optic phenomena: Flanking the anthelion on the parhelic circle are two 120° parhelia (and two Liljequist parhelia) caused by plate crystals. The Tricker and diffuse arcs are produced in singly oriented column crystals and form an Ankh-like shape passing through the anthelion. Wegener arcs occasionally cross the sky to converge in the anthelion.\n\n\n"}
{"id": "25101402", "url": "https://en.wikipedia.org/wiki?curid=25101402", "title": "Astrophysical X-ray source", "text": "Astrophysical X-ray source\n\nAstrophysical X-ray sources are astronomical objects with physical properties which result in the emission of X-rays.\n\nThere are a number of types of astrophysical objects which emit X-rays, from galaxy clusters, through black holes in active galactic nuclei (AGN) to galactic objects such as supernova remnants, stars, and binary stars containing a white dwarf (cataclysmic variable stars and super soft X-ray sources), neutron star or black hole (X-ray binaries). Some solar system bodies emit X-rays, the most notable being the Moon, although most of the X-ray brightness of the Moon arises from reflected solar X-rays. A combination of many unresolved X-ray sources is thought to produce the observed X-ray background. The X-ray continuum can arise from bremsstrahlung, either magnetic or ordinary Coulomb, black-body radiation, synchrotron radiation, inverse Compton scattering of lower-energy photons by relativistic electrons, knock-on collisions of fast protons with atomic electrons, and atomic recombination, with or without additional electron transitions.\n\nFurthermore, celestial entities in space are discussed as celestial X-ray sources. The origin of all observed astronomical X-ray sources is in, near to, or associated with a coronal cloud or gas at coronal cloud temperatures for however long or brief a period.\n\nClusters of galaxies are formed by the merger of smaller units of matter, such as galaxy groups or individual galaxies. The infalling material (which contains galaxies, gas and dark matter) gains kinetic energy as it falls into the cluster's gravitational potential well. The infalling gas collides with gas already in the cluster and is shock heated to between 10 and 10 K depending on the size of the cluster. This very hot gas emits X-rays by thermal bremsstrahlung emission, and line emission from metals (in astronomy, 'metals' often means all elements except hydrogen and helium). The galaxies and dark matter are collisionless and quickly become virialised, orbiting in the cluster potential well.\n\nAt a statistical significance of 8σ, it was found that the spatial offset of the center of the total mass from the center of the baryonic mass peaks cannot be explained with an alteration of the gravitational force law.\n\nA quasi-stellar radio source (quasar) is a very energetic and distant galaxy with an active galactic nucleus (AGN). QSO 0836+7107 is a Quasi-Stellar Object (QSO) that emits baffling amounts of radio energy. This radio emission is caused by electrons spiraling (thus accelerating) along magnetic fields producing cyclotron or synchrotron radiation. These electrons can also interact with visible light emitted by the disk around the AGN or the black hole at its center. These photons accelerate the electrons, which then emit X- and gamma-radiation via Compton and inverse Compton scattering.\n\nOn board the Compton Gamma Ray Observatory (CGRO) is the Burst and Transient Source Experiment (BATSE) which detects in the 20 keV to 8 MeV range. QSO 0836+7107 or 4C 71.07 was detected by BATSE as a source of soft gamma rays and hard X-rays. \"What BATSE has discovered is that it can be a soft gamma-ray source\", McCollough said. QSO 0836+7107 is the faintest and most distant object to be observed in soft gamma rays. It has already been observed in gamma rays by the Energetic Gamma Ray Experiment Telescope (EGRET) also aboard the Compton Gamma Ray Observatory.\n\nSeyfert galaxies are a class of galaxies with nuclei that produce spectral line emission from highly ionized gas. They are a subclass of active galactic nuclei (AGN), and are thought to contain supermassive black holes.\n\nThe following early-type galaxies (NGCs) have been observed to be X-ray bright due to hot gaseous coronae: 315, 1316, 1332, 1395, 2563, 4374, 4382, 4406, 4472, 4594, 4636, 4649, and 5128. The X-ray emission can be explained as thermal bremsstrahlung from hot gas (0.5-1.5 keV).\n\nUltraluminous X-ray sources (ULXs) are pointlike, nonnuclear X-ray sources with luminosities above the Eddington limit of 3 × 10 W for a black hole. Many ULXs show strong variability and may be black hole binaries. To fall into the class of intermediate-mass black holes (IMBHs), their luminosities, thermal disk emissions, variation timescales, and surrounding emission-line nebulae must suggest this. However, when the emission is beamed or exceeds the Eddington limit, the ULX may be a stellar-mass black hole. The nearby spiral galaxy NGC 1313 has two compact ULXs, X-1 and X-2. For X-1 the X-ray luminosity increases to a maximum of 3 × 10 W, exceeding the Eddington limit, and enters a steep power-law state at high luminosities more indicative of a stellar-mass black hole, whereas X-2 has the opposite behavior and appears to be in the hard X-ray state of an IMBH.\n\nBlack holes give off radiation because matter falling into them loses gravitational energy which may result in the emission of radiation before the matter falls into the event horizon. The infalling matter has angular momentum, which means that the material cannot fall in directly, but spins around the black hole. This material often forms an accretion disk. Similar luminous accretion disks can also form around white dwarfs and neutron stars, but in these the infalling gas releases additional energy as it slams against the high-density surface with high speed. In case of a neutron star, the infall speed can be a sizeable fraction of the speed of light.\nIn some neutron star or white dwarf systems, the magnetic field of the star is strong enough to prevent the formation of an accretion disc. The material in the disc gets very hot because of friction, and emits X-rays. The material in the disc slowly loses its angular momentum and falls into the compact star. In neutron stars and white dwarfs, additional X-rays are generated when the material hits their surfaces. X-ray emission from black holes is variable, varying in luminosity in very short timescales. The variation in luminosity can provide information about the size of the black hole.\n\nA Type Ia supernova is an explosion of a white dwarf in orbit around either another white dwarf or a red giant star. The dense white dwarf can accumulate gas donated from the companion. When the dwarf reaches the critical mass of , a thermonuclear explosion ensues. As each Type Ia shines with a known luminosity, Type Ia are called \"standard candles\" and are used by astronomers to measure distances in the universe.\n\nSN 2005ke is the first Type Ia supernova detected in X-ray wavelengths, and it is much brighter in the ultraviolet than expected.\n\nVela X-1 is a pulsing, eclipsing high-mass X-ray binary (HMXB) system, associated with the Uhuru source 4U 0900-40 and the supergiant star HD 77581. The X-ray emission of the neutron star is caused by the capture and accretion of matter from the stellar wind of the supergiant companion. Vela X-1 is the prototypical detached HMXB.\n\nAn intermediate-mass X-ray binary (IMXB) is a binary star system where one of the components is a neutron star or a black hole. The other component is an intermediate mass star.\n\nHercules X-1 is composed of a neutron star accreting matter from a normal star (HZ Her) probably due to Roche lobe overflow. X-1 is the prototype for the massive X-ray binaries although it falls on the borderline, , between high- and low-mass X-ray binaries.\n\nThe first extrasolar X-ray source was discovered on June 12, 1962. This source is called Scorpius X-1, the first X-ray source found in the constellation of Scorpius, located in the direction of the center of the Milky Way. Scorpius X-1 is some 9,000 ly from Earth and after the Sun is the strongest X-ray source in the sky at energies below 20 keV. Its X-ray output is 2.3 × 10 W, about 60,000 times the total luminosity of the Sun. Scorpius X-1 itself is a neutron star. This system is classified as a low-mass X-ray binary (LMXB); the neutron star is roughly 1.4 solar masses, while the donor star is only 0.42 solar masses.\n\nIn the late 1930s, the presence of a very hot, tenuous gas surrounding the Sun was inferred indirectly from optical coronal lines of highly ionized species. In the mid-1940s radio observations revealed a radio corona around the Sun. After detecting X-ray photons from the Sun in the course of a rocket flight, T. Burnight wrote, \"The sun is assumed to be the source of this radiation although radiation of wavelength shorter than 4 Å would not be expected from theoretical estimates of black body radiation from the solar corona.\" And, of course, people have seen the solar corona in scattered visible light during solar eclipses.\n\nWhile neutron stars and black holes are the quintessential point sources of X-rays, all main sequence stars are likely to have hot enough coronae to emit X-rays. A- or F-type stars have at most thin convection zones and thus produce little coronal activity.\n\nSimilar solar cycle-related variations are observed in the flux of solar X-ray and UV or EUV radiation. Rotation is one of the primary determinants of the magnetic dynamo, but this point could not be demonstrated by observing the Sun: the Sun's magnetic activity is in fact strongly modulated (due to the 11-year magnetic spot cycle), but this effect is not directly dependent on the rotation period.\n\nSolar flares usually follow the solar cycle. CORONAS-F was launched on July 31, 2001 to coincide with the 23rd solar cycle maximum.\nThe solar flare of October 29, 2003 apparently showed a significant degree of linear polarization (> 70% in channels E2 = 40-60 keV and E3 = 60-100 keV, but only about 50% in E1 = 20-40 keV) in hard X-rays, but other observations have generally only set upper limits.\nCoronal loops form the basic structure of the lower corona and transition region of the Sun. These highly structured and elegant loops are a direct consequence of the twisted solar magnetic flux within the solar body. The population of coronal loops can be directly linked with the solar cycle, it is for this reason coronal loops are often found with sunspots at their footpoints. Coronal loops populate both active and quiet regions of the solar surface. The Yohkoh Soft X-ray Telescope (SXT) observed X-rays in the 0.25-4.0 keV range, resolving solar features to 2.5 arc seconds with a temporal resolution of 0.5–2 seconds. SXT was sensitive to plasma in the 2-4 MK temperature range, making it an ideal observational platform to compare with data collected from TRACE coronal loops radiating in the EUV wavelengths.\n\nVariations of solar-flare emission in soft X-rays (10-130 nm) and EUV (26-34 nm) recorded on board CORONAS-F demonstrate for most flares observed by CORONAS-F in 2001–2003 UV radiation preceded X-ray emission by 1-10 min.\n\nWhen the core of a medium mass star contracts, it causes a release of energy that makes the envelope of the star expand. This continues until the star finally blows its outer layers off. The core of the star remains intact and becomes a white dwarf. The white dwarf is surrounded by an expanding shell of gas in an object known as a planetary nebula. Planetary nebulae seem to mark the transition of a medium mass star from red giant to white dwarf. X-ray images reveal clouds of multimillion degree gas that have been compressed and heated by the fast stellar wind. Eventually the central star collapses to form a white dwarf. For a billion or so years after a star collapses to form a white dwarf, it is \"white\" hot with surface temperatures of ~20,000 K.\n\nX-ray emission has been detected from PG 1658+441, a hot, isolated, magnetic white dwarf, first detected in an Einstein IPC observation and later identified in an Exosat channel multiplier array observation. \"The broad-band spectrum of this DA white dwarf can be explained as emission from a homogeneous, high-gravity, pure hydrogen atmosphere with a temperature near 28,000 K.\" These observations of PG 1658+441 support a correlation between temperature and helium abundance in white dwarf atmospheres.\n\nA super soft X-ray source (SSXS) radiates soft X-rays in the range of 0.09 to 2.5 keV. Super soft X-rays are believed to be produced by steady nuclear fusion on a white dwarf's surface of material pulled from a binary companion. This requires a flow of material sufficiently high to sustain the fusion.\n\nReal mass transfer variations may be occurring in V Sge similar to SSXS RX J0513.9-6951 as revealed by analysis of the activity of the SSXS V Sge where episodes of long low states occur in a cycle of ~400 days.\n\nRX J0648.0-4418 is an X-ray pulsator in the Crab nebula. HD 49798 is a subdwarf star that forms a binary system with RX J0648.0-4418. The subdwarf star is a bright object in the optical and UV bands. The orbital period of the system is accurately known. Recent XMM-Newton observations timed to coincide with the expected eclipse of the X-ray source allowed an accurate determination of the mass of the X-ray source (at least 1.2 solar masses), establishing the X-ray source as a rare, ultra-massive white dwarf.\n\nAccording to theory, an object that has a mass of less than about 8% of the mass of the Sun cannot sustain significant nuclear fusion in its core. This marks the dividing line between red dwarf stars and brown dwarfs. The dividing line between planets and brown dwarfs occurs with objects that have masses below about 1% of the mass of the Sun, or 10 times the mass of Jupiter. These objects cannot fuse deuterium.\nWith no strong central nuclear energy source, the interior of a brown dwarf is in a rapid boiling, or convective state. When combined with the rapid rotation that most brown dwarfs exhibit, convection sets up conditions for the development of a strong, tangled magnetic field near the surface. The flare observed by Chandra from LP 944-20 could have its origin in the turbulent magnetized hot material beneath the brown dwarf's surface. A sub-surface flare could conduct heat to the atmosphere, allowing electric currents to flow and produce an X-ray flare, like a stroke of lightning. The absence of X-rays from LP 944-20 during the non flaring period is also a significant result. It sets the lowest observational limit on steady X-ray power produced by a brown dwarf star, and shows that coronas cease to exist as the surface temperature of a brown dwarf cools below about 2500 °C and becomes electrically neutral.\nUsing NASA's Chandra X-ray Observatory, scientists have detected X-rays from a low mass brown dwarf in a multiple star system. This is the first time that a brown dwarf this close to its parent star(s) (Sun-like stars TWA 5A) has been resolved in X-rays. \"Our Chandra data show that the X-rays originate from the brown dwarf's coronal plasma which is some 3 million degrees Celsius\", said Yohko Tsuboi of Chuo University in Tokyo. \"This brown dwarf is as bright as the Sun today in X-ray light, while it is fifty times less massive than the Sun\", said Tsuboi. \"This observation, thus, raises the possibility that even massive planets might emit X-rays by themselves during their youth!\"\n\nElectric potentials of about 10 million volts, and currents of 10 million amps – a hundred times greater than the most powerful lightning bolts – are required to explain the auroras at Jupiter's poles, which are a thousand times more powerful than those on Earth.\n\nOn Earth, auroras are triggered by solar storms of energetic particles, which disturb Earth's magnetic field. As shown by the swept-back appearance in the illustration, gusts of particles from the Sun also distort Jupiter's magnetic field, and on occasion produce auroras.\n\nSaturn's X-ray spectrum is similar to that of X-rays from the Sun indicating that Saturn's X-radiation is due to the reflection of solar X-rays by Saturn's atmosphere. The optical image is much brighter, and shows the beautiful ring structures, which were not detected in X-rays.\n\nSome of the detected X-rays, originating from solar system bodies other than the Sun, are produced by fluorescence. Scattered solar X-rays provide an additional component.\n\nIn the Röntgensatellit (ROSAT) image of the Moon, pixel brightness corresponds to X-ray intensity. The bright lunar hemisphere shines in X-rays because it re-emits X-rays originating from the sun. The background sky has an X-ray glow in part due to the myriad of distant, powerful active galaxies, unresolved in the ROSAT picture. The dark side of the Moon's disk shadows this X-ray background radiation coming from the deep space. A few X-rays only seem to come from the shadowed lunar hemisphere. Instead, they originate in Earth's geocorona or extended atmosphere which surrounds the orbiting X-ray observatory. The measured lunar X-ray luminosity of ~1.2 × 10 W makes the Moon one of the weakest known non-terrestrial X-ray source.\n\nNASA's Swift Gamma-Ray Burst Mission satellite was monitoring Comet Lulin as it closed to 63 Gm of Earth. For the first time, astronomers can see simultaneous UV and X-ray images of a comet. \"The solar wind -- a fast-moving stream of particles from the sun -- interacts with the comet's broader cloud of atoms. This causes the solar wind to light up with X-rays, and that's what Swift's XRT sees\", said Stefan Immler, of the Goddard Space Flight Center. This interaction, called charge exchange, results in X-rays from most comets when they pass within about three times Earth's distance from the sun. Because Lulin is so active, its atomic cloud is especially dense. As a result, the X-ray-emitting region extends far sunward of the comet.\n\nThe celestial sphere has been divided into 88 constellations. The IAU constellations are areas of the sky. Each of these contains remarkable X-ray sources. Some of them are galaxies or black holes at the centers of galaxies. Some are pulsars. As with the astronomical X-ray sources, striving to understand the generation of X-rays by the apparent source helps to understand the Sun, the universe as a whole, and how these affect us on Earth.\n\nMultiple X-ray sources have been detected in the Andromeda Galaxy, using observations from the ESA's XMM-Newton orbiting observatory.\n\n3C 295 (Cl 1409+524) in Boötes is one of the most distant galaxy clusters observed by X-ray telescopes. The cluster is filled with a vast cloud of 50 MK gas that radiates strongly in X rays. Chandra observed that the central galaxy is a strong, complex source of X rays.\n\nHot X-ray emitting gas pervades the galaxy cluster MS 0735.6+7421 in Camelopardus. Two vast cavities – each 600,000 lyrs in diameter appear on opposite sides of a large galaxy at the center of the cluster. These cavities are filled with a two-sided, elongated, magnetized bubble of extremely high-energy electrons that emit radio waves.\n\nThe X-ray landmark NGC 4151, an intermediate spiral Seyfert galaxy has a massive black hole in its core.\n\nA Chandra X-ray image of Sirius A and B shows Sirius B to be more luminous than Sirius A. Whereas in the visual range, Sirius A is the more luminous.\n\nRegarding Cassiopea A SNR, it is believed that first light from the stellar explosion reached Earth approximately 300 years ago but there are no historical records of any sightings of the progenitor supernova, probably due to interstellar dust absorbing optical wavelength radiation before it reached Earth (although it is possible that it was recorded as a sixth magnitude star 3 Cassiopeiae by John Flamsteed on August 16, 1680). Possible explanations lean toward the idea that the source star was unusually massive and had previously ejected much of its outer layers. These outer layers would have cloaked the star and reabsorbed much of the light released as the inner star collapsed.\n\nCTA 1 is another SNR X-ray source in Cassiopeia. A pulsar in the CTA 1 supernova remnant (4U 0000+72) initially emitted radiation in the X-ray bands (1970–1977). Strangely, when it was observed at a later time (2008) X-ray radiation was not detected. Instead, the Fermi Gamma-ray Space Telescope detected the pulsar was emitting gamma ray radiation, the first of its kind.\n\nThree structures around Eta Carinae are thought to represent shock waves produced by matter rushing away from the superstar at supersonic speeds. The temperature of the shock-heated gas ranges from 60 MK in the central regions to 3 MK on the horseshoe-shaped outer structure. \"The Chandra image contains some puzzles for existing ideas of how a star can produce such hot and intense X-rays,\" says Prof. Kris Davidson of the University of Minnesota.\n\nAbell 400 is a galaxy cluster, containing a galaxy (NGC 1128) with two supermassive black holes 3C 75 spiraling towards merger.\n\nThe Chamaeleon complex is a large star forming region (SFR) that includes the Chamaeleon I, Chamaeleon II, and Chamaeleon III dark clouds. It occupies nearly all of the constellation and overlaps into Apus, Musca, and Carina. The mean density of X-ray sources is about one source per square degree.\n\nThe Chamaeleon I (Cha I) cloud is a coronal cloud and one of the nearest active star formation regions at ~160 pc. It is relatively isolated from other star-forming clouds, so it is unlikely that older pre-main sequence (PMS) stars have drifted into the field. The total stellar population is 200-300. The Cha I cloud is further divided into the North cloud or region and South cloud or main cloud.\n\nThe Chamaeleon II dark cloud contains some 40 X-ray sources. Observation in Chamaeleon II was carried out from September 10 to 17, 1993. Source RXJ 1301.9-7706, a new WTTS candidate of spectral type K1, is closest to 4U 1302–77.\n\n\"Chamaeleon III appears to be devoid of current star-formation activity.\" HD 104237 (spectral type A4e) observed by ASCA, located in the Chamaeleon III dark cloud, is the brightest Herbig Ae/Be star in the sky.\n\nThe galaxy cluster Abell 2142 emits X-rays and is in Corona Borealis. It is one of the most massive objects in the universe.\n\nFrom the Chandra X-ray analysis of the Antennae Galaxies rich deposits of neon, magnesium, and silicon were discovered. These elements are among those that form the building blocks for habitable planets. The clouds imaged contain magnesium and silicon at 16 and 24 times respectively, the abundance in the Sun.\n\nThe jet exhibited in X-rays coming from PKS 1127-145 is likely due to the collision of a beam of high-energy electrons with microwave photons.\n\nThe Draco nebula (a soft X-ray shadow) is outlined by contours and is blue-black in the image by ROSAT of a portion of the constellation Draco.\nAbell 2256 is a galaxy cluster of > 500 galaxies. The double structure of this ROSAT image shows the merging of two clusters.\n\nWithin the constellations Orion and Eridanus and stretching across them is a soft X-ray \"hot spot\" known as the Orion-Eridanus Superbubble, the Eridanus Soft X-ray Enhancement, or simply the Eridanus Bubble, a 25° area of interlocking arcs of Hα emitting filaments.\n\nA large cloud of hot gas extends throughout the Hydra A galaxy cluster.\n\nArp260 is an X-ray source in Leo Minor at RA Dec .\n\nIn the adjacent images are the constellation Orion. On the right side of the images is the visual image of the constellation. On the left is Orion as seen in X-rays only. Betelgeuse is easily seen above the three stars of Orion's belt on the right. The brightest object in the visual image is the full moon, which is also in the X-ray image. The X-ray colors represent the temperature of the X-ray emission from each star: hot stars are blue-white and cooler stars are yellow-red.\n\nStephan's Quintet are of interest because of their violent collisions. Four of the five galaxies in Stephan's Quintet form a physical association, and are involved in a cosmic dance that most likely will end with the galaxies merging. As NGC 7318B collides with gas in the group, a huge shock wave bigger than the Milky Way spreads throughout the medium between the galaxies, heating some of the gas to temperatures of millions of degrees where they emit X-rays detectable with the NASA Chandra X-ray Observatory. NGC 7319 has a type 2 Seyfert nucleus.\n\nThe Perseus galaxy cluster is one of the most massive objects in the universe, containing thousands of galaxies immersed in a vast cloud of multimillion degree gas.\n\nPictor A is a galaxy that may have a black hole at its center which has emitted magnetized gas at extremely high speed. The bright spot at the right in the image is the head of the jet. As it plows into the tenuous gas of intergalactic space, it emits X-rays. Pictor A is X-ray source designated H 0517-456 and 3U 0510-44.\n\nPuppis A is a supernova remnant (SNR) about 10 light-years in diameter. The supernova occurred approximately 3700 years ago.\n\nThe Galactic Center is at 1745–2900 which corresponds to Sagittarius A*, very near to radio source Sagittarius A (W24). In probably the first catalogue of galactic X-ray sources, two Sgr X-1s are suggested: (1) at 1744–2312 and (2) at 1755–2912, noting that (2) is an uncertain identification. Source (1) seems to correspond to S11.\n\nThe unusual shape of the Cartwheel Galaxy may be due to a collision with a smaller galaxy such as those in the lower left of the image. The most recent star burst (star formation due to compression waves) has lit up the Cartwheel rim, which has a diameter larger than the Milky Way. There is an exceptionally large number of black holes in the rim of the galaxy as can be seen in the inset.\n\nAs of August 27, 2007, discoveries concerning asymmetric iron line broadening and their implications for relativity have been a topic of much excitement. With respect to the asymmetric iron line broadening, Edward Cackett of the University of Michigan commented, \"We're seeing the gas whipping around just outside the neutron star's surface,\". \"And since the inner part of the disk obviously can't orbit any closer than the neutron star's surface, these measurements give us a maximum size of the neutron star's diameter. The neutron stars can be no larger than 18 to 20.5 miles across, results that agree with other types of measurements.\"\n\n\"We've seen these asymmetric lines from many black holes, but this is the first confirmation that neutron stars can produce them as well. It shows that the way neutron stars accrete matter is not very different from that of black holes, and it gives us a new tool to probe Einstein's theory\", says Tod Strohmayer of NASA's Goddard Space Flight Center.\n\n\"This is fundamental physics\", says Sudip Bhattacharyya also of NASA's Goddard Space Flight Center in Greenbelt, Maryland and the University of Maryland. \"There could be exotic kinds of particles or states of matter, such as quark matter, in the centers of neutron stars, but it's impossible to create them in the lab. The only way to find out is to understand neutron stars.\"\n\nUsing XMM-Newton, Bhattacharyya and Strohmayer observed Serpens X-1, which contains a neutron star and a stellar companion. Cackett and Jon Miller of the University of Michigan, along with Bhattacharyya and Strohmayer, used Suzaku's superb spectral capabilities to survey Serpens X-1. The Suzaku data confirmed the XMM-Newton result regarding the iron line in Serpens X-1.\n\nM82 X-1 is in the constellation Ursa Major at +. It was detected in January 2006 by the Rossi X-ray Timing Explorer.\nIn Ursa Major at RA 10 34 00.00 Dec +57° 40' 00.00\" is a field of view that is almost free of absorption by neutral hydrogen gas within the Milky Way. It is known as the Lockman Hole. Hundreds of X-ray sources from other galaxies, some of them supermassive black holes, can be seen through this window.\n\nA microquasar is a smaller cousin of a quasar that is a radio emitting X-ray binary, with an often resolvable pair of radio jets. SS 433 is one of the most exotic star systems observed. It is an eclipsing binary with the primary either a black hole or neutron star and the secondary is a late A-type star. SS 433 lies within SNR W50. The material in the jet traveling from the secondary to the primary does so at 26% of light speed. The spectrum of SS 433 is affected by Doppler shifts and by relativity: when the effects of the Doppler shift are subtracted, there is a residual redshift which corresponds to a velocity of about 12,000 kps. This does not represent an actual velocity of the system away from the Earth; rather, it is due to time dilation, which makes moving clocks appear to stationary observers to be ticking more slowly. In this case, the relativistically moving excited atoms in the jets appear to vibrate more slowly and their radiation thus appears red-shifted.\n\nLSI+61°303 is a periodic, radio-emitting binary system that is also the gamma-ray source, CG135+01. LSI+61°303 is a variable radio source characterized by periodic, non-thermal radio outbursts with a period of 26.5 d, attributed to the eccentric orbital motion of a compact object, probably a neutron star, around a rapidly rotating B0 Ve star, with a T ~26,000 K and luminosity of ~10 erg s. Photometric observations at optical and infrared wavelengths also show a 26.5 d modulation. Of the 20 or so members of the Be X-ray binary systems, as of 1996, only X Per and LSI+61°303 have X-ray outbursts of much higher luminosity and harder spectrum (kT ~ 10-20 keV) vs. (kT ≤ 1 keV); however, LSI+61°303 further distinguishes itself by its strong, outbursting radio emission. \"The radio properties of LSI+61°303 are similar to those of the \"standard\" high-mass X-ray binaries such as SS 433, Cyg X-3 and Cir X-1.\"\n\nThere are a growing number of recurrent X-ray transients, characterized by short outbursts with very fast rise times (tens of minutes) and typical durations of a few hours that are associated with OB supergiants and hence define a new class of massive X-ray binaries: Supergiant Fast X-ray Transients (SFXTs). XTE J1739–302 is one of these. Discovered in 1997, remaining active only one day, with an X-ray spectrum well fitted with a thermal bremsstrahlung (temperature of ∼20 keV), resembling the spectral properties of accreting pulsars, it was at first classified as a peculiar Be/X-ray transient with an unusually short outburst. A new burst was observed on April 8, 2008 with Swift.\n\nObservations made by Chandra indicate the presence of loops and rings in the hot X-ray emitting gas that surrounds Messier 87. These loops and rings are generated by variations in the rate at which material is ejected from the supermassive black hole in jets. The distribution of loops suggests that minor eruptions occur every six million years.\n\nOne of the rings, caused by a major eruption, is a shock wave 85,000 light-years in diameter around the black hole. Other remarkable features observed include narrow X-ray emitting filaments up to 100,000 light-years long, and a large cavity in the hot gas caused by a major eruption 70 million years ago.\n\nThe galaxy also contains a notable active galactic nucleus (AGN) that is a strong source of multiwavelength radiation, particularly radio waves.\n\nA magnetar is a type of neutron star with an extremely powerful magnetic field, the decay of which powers the emission of copious amounts of high-energy electromagnetic radiation, particularly X-rays and gamma rays. The theory regarding these objects was proposed by Robert Duncan and Christopher Thompson in 1992, but the first recorded burst of gamma rays thought to have been from a magnetar was on March 5, 1979. These magnetic fields are hundreds of thousands of times stronger than any man-made magnet, and quadrillions of times more powerful than the field surrounding Earth. As of 2003, they are the most magnetic objects ever detected in the universe.\n\nOn March 5, 1979, after dropping probes into the atmosphere of Venus, Venera 11 and Venera 12, while in heliocentric orbits, were hit at 10:51 am EST by a blast of gamma ray radiation. This contact raised the radiation readings on both the probes Konus experiments from a normal 100 counts per second to over 200,000 counts a second, in only a fraction of a millisecond. This giant flare was detected by numerous spacecraft and with these detections was localized by the interplanetary network to SGR 0526-66 inside the N-49 SNR of the Large Magellanic Cloud. And, Konus detected another source in March 1979: SGR 1900+14, located 20,000 light-years away in the constellation Aquila had a long period of low emissions, except the significant burst in 1979, and a couple after.\n\nWhat is the evolutionary relationship between pulsars and magnetars? Astronomers would like to know if magnetars represent a rare class of pulsars, or if some or all pulsars go through a magnetar phase during their life cycles. NASA's Rossi X-ray Timing Explorer (RXTE) has revealed that the youngest known pulsing neutron star has thrown a temper tantrum. The collapsed star occasionally unleashes powerful bursts of X-rays, which are forcing astronomers to rethink the life cycle of neutron stars.\n\n\"We are watching one type of neutron star literally change into another right before our very eyes. This is a long-sought missing link between different types of pulsars\", says Fotis Gavriil of NASA's Goddard Space Flight Center in Greenbelt, Maryland, and the University of Maryland, Baltimore.\nPSR J1846-0258 is in the constellation Aquila. It had been classed as a normal pulsar because of its fast spin (3.1 s) and pulsar-like spectrum. RXTE caught four magnetar-like X-ray bursts on May 31, 2006, and another on July 27, 2006. Although none of these events lasted longer than 0.14 second, they all packed the wallop of at least 75,000 Suns. \"Never before has a regular pulsar been observed to produce magnetar bursts\", says Gavriil.\n\n\"Young, fast-spinning pulsars were not thought to have enough magnetic energy to generate such powerful bursts\", says Marjorie Gonzalez, formerly of McGill University in Montreal, Canada, now based at the University of British Columbia in Vancouver. \"Here's a normal pulsar that's acting like a magnetar.\"\nThe observations from NASA's Chandra X-ray Observatory showed that the object had brightened in X-rays, confirming that the bursts were from the pulsar, and that its spectrum had changed to become more magnetar-like. The fact that PSR J1846's spin rate is decelerating also means that it has a strong magnetic field braking the rotation. The implied magnetic field is trillions of times stronger than Earth's field, but it's 10 to 100 times weaker than a typical magnetar. Victoria Kaspi of McGill University notes, \"PSR J1846's actual magnetic field could be much stronger than the measured amount, suggesting that many young neutron stars classified as pulsars might actually be magnetars in disguise, and that the true strength of their magnetic field only reveals itself over thousands of years as they ramp up in activity.\"\n\nDuring the solar cycle, as shown in the sequence of images of the Sun in X-rays, the Sun is almost X-ray dark, almost an X-ray variable. Betelgeuse, on the other hand, appears to be always X-ray dark. The X-ray flux from the entire stellar surface corresponds to a surface flux limit that ranges from 30-7000 ergs s cm at T=1 MK, to ~1 erg s cm at higher temperatures, five orders of magnitude below the quiet Sun X-ray surface flux.\n\nLike the red supergiant Betelgeuse, hardly any X-rays are emitted by red giants. The cause of the X-ray deficiency may involve\n\nProminent bright red giants include Aldebaran, Arcturus, and Gamma Crucis. There is an apparent X-ray \"dividing line\" in the H-R diagram among the giant stars as they cross from the main sequence to become red giants. Alpha Trianguli Australis (α TrA / α Trianguli Australis) appears to be a Hybrid star (parts of both sides) in the \"Dividing Line\" of evolutionary transition to red giant. α TrA can serve to test the several Dividing Line models.\n\nThere is also a rather abrupt onset of X-ray emission around spectral type A7-F0, with a large range of luminosities developing across spectral class F.\n\nIn the few genuine late A- or early F-type coronal emitters, their weak dynamo operation is generally not able to brake the rapidly spinning star considerably during their short lifetime so that these coronae are conspicuous by their severe deficit of X-ray emission compared to chromospheric and transition region fluxes; the latter can be followed up to mid-A type stars at quite high levels. Whether or not these atmospheres are indeed heated acoustically and drive an \"expanding\", weak and cool corona or whether they are heated magnetically, the X-ray deficit and the low coronal temperatures clearly attest to the inability of these stars to maintain substantial, hot coronae in any way comparable to cooler active stars, their appreciable chromospheres notwithstanding.\n\nThe Hot Ionized Medium (HIM), sometimes consisting of coronal gas, in the temperature range 10 – 10 K emits X-rays. Stellar winds from young clusters of stars (often with giant or supergiant HII regions surrounding them) and shock waves created by supernovae inject enormous amounts of energy into their surroundings, which leads to hypersonic turbulence. The resultant structures – of varying sizes – can be observed, such as stellar wind bubbles and superbubbles of hot gas, by X-ray satellite telescopes. The Sun is currently traveling through the Local Interstellar Cloud, a denser region in the low-density Local Bubble.\n\nIn addition to discrete sources which stand out against the sky, there is good evidence for a diffuse X-ray background. During more than a decade of observations of X-ray emission from the Sun, evidence of the existence of an isotropic X-ray background flux was obtained in 1956. This background flux is rather consistently observed over a wide range of energies. The early high-energy end of the spectrum for this diffuse X-ray background was obtained by instruments on board Ranger 3 and Ranger 5. The X-ray flux corresponds to a total energy density of about 5 x 10 eV/cm. The ROSAT soft X-ray diffuse background (SXRB) image shows the general increase in intensity from the Galactic plane to the poles. At the lowest energies, 0.1 – 0.3 keV, nearly all of the observed soft X-ray background (SXRB) is thermal emission from ~10 K plasma.\n\nBy comparing the soft X-ray background with the distribution of neutral hydrogen, it is generally agreed that within the Milky Way disk, super soft X-rays are absorbed by this neutral hydrogen.\n\nX-ray observations offer the possibility to detect (X-ray dark) planets as they eclipse part of the corona of their parent star while in transit. \"Such methods are particularly promising for low-mass stars as a Jupiter-like planet could eclipse a rather significant coronal area.\"\n\nThe first picture of the Earth in X-rays was taken in March 1996, with the orbiting Polar satellite. Energetically charged particles from the Sun cause aurora and energize electrons in the Earth's magnetosphere. These electrons move along the Earth's magnetic field and eventually strike the Earth's ionosphere, producing the X-ray emission.\n\n"}
{"id": "28648115", "url": "https://en.wikipedia.org/wiki?curid=28648115", "title": "Blaiken wind farm", "text": "Blaiken wind farm\n\nBetween 2011 and 2015, BlaikenVind AB, a joint venture of Skellefteå Kraft and Fortum are building one of Europe's largest wind farms in northern Sweden. When finished, the park will consist of 90 wind power plants and have a total capacity of 225 MW. Blaiken wind farm, has been chosen as a demonstration plant by the EU's NER300 programme, one of the world's largest funding programmes for climate-neutral energy. The project meets the challenge of developing good solutions for efficient wind power production in a cold and icy climate. \n"}
{"id": "3588722", "url": "https://en.wikipedia.org/wiki?curid=3588722", "title": "Cellular component", "text": "Cellular component\n\nCellular components are the complex biomolecules and structures of which cells, and thus living organisms, are composed. Cells are the structural and functional units of life. The smallest organisms are single cells, while the largest organisms are assemblages of trillions of cells. DNA is found in nearly all living cells; each cell carries chromosome(s) having a distinctive DNA sequence.\n\nExamples include macromolecules such as proteins and nucleic acids, biomolecular complexes such as a ribosome, and structures such as membranes, and organelles. While the majority of cellular components are located within the cell itself, some may exist in extracellular areas of an organism.\n\nCellular components may also be called biological matter or biological material. Most biological matter has the characteristics of soft matter, being governed by relatively small energies. All known life is made of biological matter. To be differentiated from other theoretical or fictional life forms, such life may be called \"carbon-based\", \"cellular\", \"organic\", \"biological\", or even simply \"living\" – as some definitions of life exclude hypothetical types of biochemistry.\n\n\n"}
{"id": "92514", "url": "https://en.wikipedia.org/wiki?curid=92514", "title": "Detergent", "text": "Detergent\n\nA detergent is a surfactant or a mixture of surfactants with cleaning properties in dilute solutions. These substances are usually alkylbenzenesulfonates, a family of compounds that are similar to soap but are more soluble in hard water, because the polar sulfonate (of detergents) is less likely than the polar carboxylate (of soap) to bind to calcium and other ions found in hard water.\n\nIn most household contexts, the term \"detergent\" by itself refers specifically to \"laundry detergent\" or \"dish detergent\", as opposed to \"hand soap\" or other types of cleaning agents. Detergents are commonly available as powders or concentrated solutions. Detergents, like soaps, work because they are amphiphilic: partly hydrophilic (polar) and partly hydrophobic (non-polar). Their dual nature facilitates the mixture of hydrophobic compounds (like oil and grease) with water. Because air is not hydrophilic, detergents are also foaming agents to varying degrees.\n\nDetergents are classified into three broad groupings, depending on the electrical charge of the surfactants.\n\nTypical anionic detergents are alkylbenzenesulfonates. The alkylbenzene portion of these anions is lipophilic and the sulfonate is hydrophilic. Two different varieties have been popularized, those with branched alkyl groups and those with linear alkyl groups. The former were largely phased out in economically advanced societies because they are poorly biodegradable. An estimated 6 billion kilograms of anionic detergents are produced annually for domestic markets.\n\nBile acids, such as deoxycholic acid (DOC), are anionic detergents produced by the liver to aid in digestion and absorption of fats and oils.\n\nCationic detergents that are similar to the anionic ones, with a hydrophilic component, but, instead of the anionic sulfonate group, the cationic surfactants have quaternary ammonium as the polar end. The ammonium sulfate center is positively charged.\n\nNon-ionic detergents are characterized by their uncharged, hydrophilic headgroups. Typical non-ionic detergents are based on polyoxyethylene or a glycoside. Common examples of the former include Tween, Triton, and the Brij series. These materials are also known as ethoxylates or PEGlyates and their metabolites, nonylphenol. Glycosides have a sugar as their uncharged hydrophilic headgroup. Examples include octyl thioglucoside and maltosides. HEGA and MEGA series detergents are similar, possessing a sugar alcohol as headgroup.\n\nZwitterionic detergents possess a net zero charge arising from the presence of equal numbers of +1 and −1 charged chemical groups. Examples include CHAPS.\n\nSee surfactants for more applications.\n\nIn World War I, there was a shortage of oils. Synthetic detergents were first made in Germany.\n\nOne of the largest applications of detergents is for household cleaning including dish washing and washing laundry. The formulations are complex, reflecting the diverse demands of the application and the highly competitive consumer market.\n\nBoth carburetors and fuel injector components of Otto engines benefit from detergents in the fuels to prevent fouling. Concentrations are about 300 ppm. Typical detergents are long-chain amines and amides such as polyisobuteneamine and polyisobuteneamide/succinimide.\n\nReagent grade detergents are employed for the isolation and purification of integral membrane proteins found in biological cells. Solubilization of cell membrane bilayers requires a detergent that can enter the inner membrane monolayer. Advancements in the purity and sophistication of detergents have facilitated structural and biophysical characterization of important membrane proteins such as ion channels also the disrupt membrane by binding Lipopolysaccharide, transporters, signaling receptors, and photosystem II.\n\n\n"}
{"id": "2677209", "url": "https://en.wikipedia.org/wiki?curid=2677209", "title": "Drilling and blasting", "text": "Drilling and blasting\n\nDrilling and blasting is the controlled use of explosives and other methods such as gas pressure blasting pyrotechnics, to break rock for excavation. It is practiced most often in mining, quarrying and civil engineering such as dam or road construction. The result of rock blasting is often known as a rock cut.\n\nDrilling and blasting currently utilizes many different varieties of explosives with different compositions and performance properties. Higher velocity explosives are used for relatively hard rock in order to shatter and break the rock, while low velocity explosives are used in soft rocks to generate more gas pressure and a greater heaving effect. For instance, an early 20th-century blasting manual compared the effects of black powder to that of a wedge, and dynamite to that of a hammer. The most commonly used explosives in mining today are ANFO based blends due to lower cost than dynamite.\n\nBefore the advent of tunnel boring machines, drilling and blasting was the only economical way of excavating long tunnels through hard rock, where digging is not possible. Even today, the method is still used in the construction of tunnels, such as in the construction of the Lötschberg Base Tunnel. The decision whether to construct a tunnel using a TBM or using a drill and blast method includes a number of factors. Tunnel length is a key issue that needs to be addressed because large TBMs for a rock tunnel have a high capital cost, but because they are usually quicker than a drill and blast tunnel the price per metre of tunnel is lower. This means that shorter tunnels tend to be less economical to construct with a TBM and are therefore usually constructed by drill and blast. Managing ground conditions can also have a significant effect on the choice with different methods suited to different hazards in the ground.\n\nThe use of explosives in mining goes back to the year 1627, when gunpowder was first used in place of mechanical tools in the Hungarian (now Slovak) town of Banská Štiavnica. The innovation spread quickly throughout Europe and the Americas.\n\nWhile drilling and blasting saw limited use in pre-industrial times using gunpowder (such as with the Blue Ridge Tunnel in the United States, built in the 1850s), it was not until more powerful (and safer) explosives, such as dynamite (patented 1867), as well as powered drills were developed, that its potential was fully realised.\n\nDrilling and blasting was successfully used to construct tunnels throughout the world, notably the Fréjus Rail Tunnel, the Gotthard Rail Tunnel, the Simplon Tunnel, the Jungfraubahn and even the longest road tunnel in the world, Lærdalstunnelen, are constructed using this method.\n\nIn 1990, 2.1 million tonnes (2.32 million short tons) of commercial explosives were consumed in the United States, representing an estimated expenditure of 3.5 to 4 billion 1993 dollars on blasting. Australia had the highest explosives consumption that year at 500 million tonnes (551 million short tons), with Scandinavian countries another leader in rock blasting (Persson et al. 1994:1).\n\nAs the name suggests, drilling and blasting works as follows:\n\n\nThe positions and depths of the holes (and the amount of explosive each hole receives) are determined by a carefully constructed pattern, which, together with the correct timing of the individual explosions, will guarantee that the tunnel will have an approximately circular cross-section.\n\nDuring operation, blasting mats may be used to contain the blast, suppress dust and noise, for fly rock prevention and sometimes to direct the blast.\n\nAs a tunnel or excavation progresses the roof and side walls need to be supported to stop the rock falling into the excavation. The philosophy and methods for rock support vary widely but typical rock support systems can include:\nTypically a rock support system would include a number of these support methods, each intended to undertake a specific role in the rock support such as the combination of rock bolting and shotcrete.\n\n\n"}
{"id": "8463", "url": "https://en.wikipedia.org/wiki?curid=8463", "title": "Dubnium", "text": "Dubnium\n\nDubnium is a synthetic chemical element with symbol Db and atomic number 105. Dubnium is highly radioactive: the most stable known isotope, dubnium-268, has a half-life of about 28 hours. This greatly limits the extent of research on dubnium.\n\nDubnium does not occur naturally on Earth and is produced artificially. The Soviet Joint Institute for Nuclear Research (JINR) claimed the first discovery of the element in 1968, followed by the American Lawrence Berkeley Laboratory in 1970. Both teams proposed their names for the new element and used them without formal approval. The long-standing dispute was resolved in 1993 by an official investigation of the discovery claims by the IUPAC/IUPAP Joint Working Party, resulting in credit for the discovery being officially shared between both teams. The element was formally named \"dubnium\" in 1997 after the town of Dubna, the site of the JINR.\n\nTheoretical research establishes dubnium as a member of group 5 in the 6d series of transition metals, placing it under vanadium, niobium, and tantalum. Dubnium should share most properties, such as its valence electron configuration and having a dominant +5 oxidation state, with the other group 5 elements, with a few anomalies due to relativistic effects. A limited investigation of dubnium chemistry has confirmed this. Solution chemistry experiments have revealed that dubnium often behaves more like niobium rather than tantalum, breaking periodic trends.\n\nUranium, element 92, is the heaviest element to occur in significant quantity in nature; heavier elements can only be produced practically by synthesis. The first synthesis of a new element—neptunium, element 93—was achieved in 1940 by a team of researchers in the United States. In the following years, American scientists synthesized the elements up to mendelevium, element 101, which was synthesized in 1955. From element 102, the priority of discoveries was contested between American and Soviet physicists. Their rivalry resulted in a race for new elements and credit for their discoveries, later named the Transfermium Wars.\n\nThe first report of the discovery of element 105 came from the Joint Institute for Nuclear Research (JINR) in Dubna, Moscow Oblast, Russian SFSR, Soviet Union, in April 1968. The scientists bombarded Am with a beam of Ne ions, and reported 9.4 MeV (with a half-life of 0.1–3 seconds) and 9.7 MeV (\"t\" > 0.05 s) alpha activities followed by alpha activities similar to those of either 103 or 103. Based on prior theoretical predictions, the two activity lines were assigned to 105 and 105, respectively.\n\nAfter observing the alpha decays of element 105, the researchers aimed to observe spontaneous fission (SF) of the element and study the resulting fission fragments. They published a paper in February 1970, reporting multiple examples of two such activities, with half-lives of 14 ms and . They assigned the former activity to Am and ascribed the latter activity to an isotope of element 105. They suggested that it was unlikely that this activity could come from a transfer reaction instead of element 105, because the yield ratio for this reaction was significantly lower than that of the Am-producing transfer reaction, in accordance with theoretical predictions. To establish that this activity was not from a (Ne,\"x\"n) reaction, the researchers bombarded a Am target with O ions; reactions producing 103 and 103 showed very little SF activity (matching the established data), and the reaction producing heavier 103 and 103 produced no SF activity at all, in line with theoretical data. The researchers concluded that the activities observed came from SF of element 105.\n\nIn April 1970, a team at Lawrence Berkeley Laboratory (LBL), in Berkeley, California, United States, claimed to have synthesized element 105 by bombarding californium-249 with nitrogen-15 ions, with an alpha activity of 9.1 MeV. To ensure this activity was not from a different reaction, the team attempted other reactions: bombarding Cf with N, Pb with N, and Hg with N. They stated no such activity was found in those reactions. The characteristics of the daughter nuclei matched those of 103, implying that the parent nuclei were of 105.\n\nThese results did not confirm the JINR findings regarding the 9.4 MeV or 9.7 MeV alpha decay of 105, leaving only 105 as a possibly produced isotope.\n\nJINR then attempted another experiment to create element 105, published in a report in May 1970. They claimed that they had synthesized more nuclei of element 105 and that the experiment confirmed their previous work. According to the paper, the isotope produced by JINR was probably 105, or possibly 105. This report included an initial chemical examination: the thermal gradient version of the gas-chromatography method was applied to demonstrate that the chloride of what had formed from the SF activity nearly matched that of niobium pentachloride, rather than hafnium tetrachloride. The team identified a 2.2-second SF activity in a volatile chloride portraying eka-tantalum properties, and inferred that the source of the SF activity must have been element 105.\n\nIn June 1970, JINR made improvements on their first experiment, using a purer target and reducing the intensity of transfer reactions by installing a collimator before the catcher. This time, they were able to find 9.1 MeV alpha activities with daughter isotopes identifiable as either 103 or 103, implying that the original isotope was either 105 or 105.\n\nJINR did not propose a name after their first report claiming synthesis of element 105, which would have been the usual practice. This led LBL to believe that JINR did not have enough experimental data to back their claim. After collecting more data, JINR proposed the name \"nielsbohrium\" (Ns) in honor of the Danish nuclear physicist Niels Bohr, a founder of the theories of atomic structure and quantum theory. When LBL first announced their synthesis of element 105, they proposed that the new element be named \"hahnium\" (Ha) after the German chemist Otto Hahn, the \"father of nuclear chemistry\", thus creating an element naming controversy.\n\nIn the early 1970s, both teams reported synthesis of the next element, element 106, but did not suggest names. JINR suggested establishing an international committee to clarify the discovery criteria. This proposal was accepted in 1974 and a neutral joint group formed. Neither team showed interest in resolving the conflict through a third party, so the leading scientists of LBL—Albert Ghiorso and Glenn Seaborg—traveled to Dubna in 1975 and met with the leading scientists of JINR—Georgy Flerov, Yuri Oganessian, and others— to try to resolve the conflict internally and render the neutral joint group unnecessary; after two hours of discussions, this failed. The joint neutral group never assembled to assess the claims and the conflict remained unsolved. In 1979, IUPAC suggested systematic element names to be used as placeholders until permanent names were established; under it, element 105 would be \"unnilpentium\", from the Latin roots \"un-\" and \"nil-\" and the Greek root \"pent-\" (meaning \"one\", \"zero\", and \"five\", respectively, the digits of the atomic number). Both teams ignored it as they did not wish to weaken their outstanding claims.\n\nIn 1981, the Gesellschaft für Schwerionenforschung (GSI; \"Society for Heavy Ion Research\") in Darmstadt, West Germany, claimed synthesis of element 107; their report came out five years after the first report from JINR but with greater precision, making a more solid claim on discovery. GSI acknowledged JINR's efforts by suggesting the name \"nielsbohrium\" for the new element. JINR did not suggest a new name for element 105, stating it was more important to determine its discoverers first.\n\nIn 1985, the International Union of Pure and Applied Chemistry (IUPAC) and the International Union of Pure and Applied Physics (IUPAP) formed a Joint Working Party (JWP) to assess discoveries and establish final names for the controversial elements. The party held meetings with delegates from the three competing institutes; in 1990, they established criteria on recognition of an element, and in 1991, they finished the work on assessing discoveries and disbanded. These results were published in 1993. According to the report, the first definitely successful experiment was the April 1970 LBL experiment, closely followed by the June 1970 JINR experiment, so credit for the discovery of the element should be shared between the two teams.\n\nLBL said that the input from JINR was overrated in the review. They claimed JINR was only able to unambiguously demonstrate the synthesis of element 105 a year after they did. JINR and GSI endorsed the report.\n\nIn 1994, IUPAC published a recommendation on naming the disputed elements. For element 105, they proposed \"joliotium\" (Jl) after the French physicist Frédéric Joliot-Curie, a contributor to the development of nuclear physics and chemistry; this name was originally proposed by the Soviet team for element 102, which by then had long been called nobelium. This recommendation was criticized by the American scientists for several reasons. Firstly, their suggestions were scrambled: the names \"rutherfordium\" and \"hahnium\", originally suggested by Berkeley for elements 104 and 105, were respectively reassigned to elements 106 and 108. Secondly, elements 104 and 105 were given names favored by JINR, despite earlier recognition of LBL as an equal co-discoverer for both of them. Thirdly and most importantly, IUPAC rejected the name \"seaborgium\" for element 106, having just approved a rule that an element could not be named after a living person, even though the 1993 report had given the LBL team the sole credit for its discovery.\n\nIn 1995, IUPAC abandoned the controversial rule and established a committee of national representatives aimed at finding a compromise. They suggested \"seaborgium\" for element 106 in exchange for the removal of all the other American proposals, except for the established name \"lawrencium\" for element 103. The equally entrenched name \"nobelium\" for element 102 was replaced by \"flerovium\" after Georgy Flerov, following the recognition by the 1993 report that that element had been first synthesized in Dubna. This was rejected by American scientists and the decision was retracted. The name \"flerovium\" was later used for element 114.\n\nIn 1996, IUPAC held another meeting, reconsidered all names in hand, and accepted another set of recommendations; it was approved and published in 1997. Element 105 was named \"dubnium\" (Db), after Dubna in Russia, the location of the JINR; the American suggestions were used for elements 102, 103, 104, and 106. The name \"dubnium\" had been used for element 104 in the previous IUPAC recommendation. The American scientists \"reluctantly\" approved this decision. IUPAC pointed out that the Berkeley laboratory had already been recognized several times, in the naming of berkelium, californium, and americium, and that the acceptance of the names \"rutherfordium\" and \"seaborgium\" for elements 104 and 106 should be offset by recognizing JINR's contributions to the discovery of elements 104, 105, and 106.\n\nDubnium, having an atomic number of 105, is a superheavy element; like all elements with such high atomic numbers, it is very unstable. The longest-lasting known isotope of dubnium, Db, has a half-life of around a day. No stable isotopes have been seen, and a 2012 calculation by JINR suggested that the half-lives of all dubnium isotopes would not significantly exceed a day. Dubnium can only be obtained by artificial production.\n\nThe short half-life of dubnium limits experimentation. This is exacerbated by the fact that the most stable isotopes are the hardest to synthesize. Elements with a lower atomic number have stable isotopes with a lower neutron-to-proton ratio than those with higher atomic number, meaning that the target and beam nuclei that could be employed to create the superheavy element have fewer neutrons than needed to form these most stable isotopes. (Different techniques based on rapid neutron capture and transfer reactions are being considered as of the 2010s, but those based on the collision of a large and small nucleus still dominate research in the area.)\n\nOnly a few atoms of Db can be produced in each experiment, and thus the measured lifetimes vary significantly during the process. During three experiments, 23 atoms were created in total, with a resulting half-life of . The second most stable isotope, Db, has been produced in even smaller quantities: three atoms in total, with lifetimes of 33.4 h, 1.3 h, and 1.6 h. These two are the heaviest isotopes of dubnium to date, and both were produced as a result of decay of the heavier nuclei Mc and Ts rather than directly, because the experiments that yielded them were originally designed in Dubna for Ca beams. For its mass, Ca has by far the greatest neutron excess of all practically stable nuclei, both quantitative and relative, which correspondingly helps synthesize superheavy nuclei with more neutrons, but this gain is compensated by the decreased likelihood of fusion for high atomic numbers.\n\nAccording to the periodic law, dubnium should belong to group 5, with vanadium, niobium, and tantalum. Several studies have investigated the properties of element 105 and found that they generally agreed with the predictions of periodic law. Significant deviations may nevertheless occur, due to relativistic effects, which dramatically change physical properties on both atomic and macroscopic scales. These properties have remained challenging to measure for several reasons: the difficulties of production of superheavy atoms, the low rates of production, which only allows for microscopic scales, requirements for a radiochemistry laboratory to test the atoms, short half-lives of those atoms, and the presence of many unwanted activities apart from those of synthesis of superheavy atoms. So far, studies have only been performed on single atoms.\n\nA direct relativistic effect is that as the atomic numbers of elements increase, the innermost electrons begin to revolve faster around the nucleus as a result of an increase of electromagnetic attraction between an electron and a nucleus. Similar effects have been found for the outermost s orbitals (and p ones, though in dubnium they are not occupied): for example, the 7s orbital contracts by 25% in size and is stabilized by 2.6 eV.\n\nA more indirect effect is that the contracted s and p orbitals shield the charge of the nucleus more effectively, leaving less for the outer d and f electrons, which therefore move in larger orbitals. Dubnium is greatly affected by this: unlike the previous group 5 members, its 7s electrons are slightly more difficult to extract than its 6d electrons.\nAnother effect is the spin–orbit interaction, particularly spin–orbit splitting, which splits the 6d subshell—the azimuthal quantum number ℓ of a d shell is 2—into two subshells, with four of the ten orbitals having their ℓ lowered to 3/2 and six raised to 5/2. All ten energy levels are raised; four of them are lower than the other six. (The three 6d electrons normally occupy the lowest energy levels, 6d.)\n\nA singly ionized atom of dubnium (Db) should lose a 6d electron compared to a neutral atom; the doubly (Db) or triply (Db) ionized atoms of dubnium should eliminate 7s electrons, unlike its lighter homologs. Despite the changes, dubnium is still expected to have five valence electrons; 7p energy levels have not been shown to influence dubnium and its properties. As the 6d orbitals of dubnium are more destabilized than the 5d ones of tantalum, and Db is expected to have two 6d, rather than 7s, electrons remaining, the resulting +3 oxidation state is expected to be unstable and even rarer than that of tantalum. The ionization potential of dubnium in its maximum +5 oxidation state should be slightly lower than that of tantalum and the ionic radius of dubnium should increase compared to tantalum; this has a significant effect on dubnium's chemistry.\n\nAtoms of dubnium in the solid state should arrange themselves in a body-centered cubic configuration, like the previous group 5 elements. The predicted density of dubnium is 29 g/cm.\n\nComputational chemistry is simplest in gas-phase chemistry, in which interactions between molecules may be ignored as negligible. Multiple authors have researched dubnium pentachloride; calculations show it to be consistent with the periodic laws by exhibiting the properties of a compound of a group 5 element. For example, the molecular orbital levels indicate that dubnium uses three 6d electron levels as expected. Compared to its tantalum analog, dubnium pentachloride is expected to show increased covalent character: a decrease in the effective charge on an atom and an increase in the overlap population (between orbitals of dubnium and chlorine).\n\nCalculations of solution chemistry indicate that the maximum oxidation state of dubnium, +5, will be more stable than those of niobium and tantalum and the +3 and +4 states will be less stable. The tendency towards hydrolysis of cations with the highest oxidation state should continue to decrease within group 5 but is still expected to be quite rapid. Complexation of dubnium is expected to follow group 5 trends in its richness. Calculations for hydroxo-chlorido- complexes have shown a reversal in the trends of complex formation and extraction of group 5 elements, with dubnium being more prone to do so than tantalum.\n\nExperimental results of the chemistry of dubnium date back to 1974 and 1976. JINR researchers used a thermochromatographic system and concluded that the volatility of dubnium bromide was less than that of niobium bromide and about the same as that of hafnium bromide. It is not certain that the detected fission products confirmed that the parent was indeed element 105. These results may imply that dubnium behaves more like hafnium than niobium.\n\nThe next studies on the chemistry of dubnium were conducted in 1988, in Berkeley. They examined whether the most stable oxidation state of dubnium in aqueous solution was +5. Dubnium was fumed twice and washed with concentrated nitric acid; sorption of dubnium on glass cover slips was then compared with that of the group 5 elements niobium and tantalum and the group 4 elements zirconium and hafnium produced under similar conditions. The group 5 elements are known to sorb on glass surfaces; the group 4 elements do not. Dubnium was confirmed as a group 5 member. Surprisingly, the behavior on extraction from mixed nitric and hydrofluoric acid solution into methyl isobutyl ketone differed between dubnium, tantalum, and niobium. Dubnium did not extract and its behavior resembled niobium more closely than tantalum, indicating that complexing behavior could not be predicted purely from simple extrapolations of trends within a group in the periodic table.\n\nThis prompted further exploration of the chemical behavior of complexes of dubnium. Various labs jointly conducted thousands of repetitive chromatographic experiments between 1988 and 1993. All group 5 elements and protactinium were extracted from concentrated hydrochloric acid; after mixing with lower concentrations of hydrogen chloride, small amounts of hydrogen fluoride were added to start selective re-extraction. Dubnium showed behavior different from that of tantalum but similar to that of niobium and its pseudohomolog protactinium at concentrations of hydrogen chloride below 12 moles per liter. This similarity to the two elements suggested that the formed complex was either or . After extraction experiments of dubnium from hydrogen bromide into diisobutyl carbinol (2,6-dimethylheptan-4-ol), a specific extractant for protactinium, with subsequent elutions with the hydrogen chloride/hydrogen fluoride mix as well as hydrogen chloride, dubnium was found to be less prone to extraction than either protactinium or niobium. This was explained as an increasing tendency to form non‐extractable complexes of multiple negative charges. Further experiments in 1992 confirmed the stability of the +5 state: Db(V) was shown to be extractable from cation‐exchange columns with α‐hydroxyisobutyrate, like the group 5 elements and protactinium; Db(III) and Db(IV) were not. In 1998 and 1999, new predictions suggested that dubnium would extract nearly as well as niobium and better than tantalum from halide solutions, which was later confirmed.\n\nThe first isothermal gas chromatography experiments were performed in 1992 with Db (half-life 35 seconds). The volatilities for niobium and tantalum were similar within error limits, but dubnium appeared to be significantly less volatile. It was postulated that traces of oxygen in the system might have led to formation of , which was predicted to be less volatile than . Later experiments in 1996 showed that group 5 chlorides were more volatile than the corresponding bromides, with the exception of tantalum, presumably due to formation of . Later volatility studies of chlorides of dubnium and niobium as a function of controlled partial pressures of oxygen showed that formation of oxychlorides and general volatility are dependent on concentrations of oxygen. The oxychlorides were shown to be less volatile than the chlorides.\n\nIn 2004–05, researchers from Dubna and Livermore identified a new dubnium isotope, Db, as a fivefold alpha decay product of the newly created element 115. This new isotope proved to be long-lived enough to allow further chemical experimentation, with a half-life of over a day. In the 2004 experiment, a thin layer with dubnium was removed from the surface of the target and dissolved in aqua regia with tracers and a lanthanum carrier, from which various +3, +4, and +5 species were precipitated on adding ammonium hydroxide. The precipitate was washed and dissolved in hydrochloric acid, where it converted to nitrate form and was then dried on a film and counted. Mostly containing a +5 species, which was immediately assigned to dubnium, it also had a +4 species; based on that result, the team decided that additional chemical separation was needed. In 2005, the experiment was repeated, with the final product being hydroxide rather than nitrate precipitate, which was processed further in both Livermore (based on reverse phase chromatography) and Dubna (based on anion exchange chromatography). The +5 species was effectively isolated; dubnium appeared three times in tantalum-only fractions and never in niobium-only fractions. It was noted that these experiments were insufficient to draw conclusions about the general chemical profile of dubnium.\n\nIn 2009, at the JAEA tandem accelerator in Japan, dubnium was processed in nitric and hydrofluoric acid solution, at concentrations where niobium forms and tantalum forms . Dubnium's behavior was close to that of niobium but not tantalum; it was thus deduced that dubnium formed . From the available information, it was concluded that dubnium often behaved like niobium, sometimes like protactinium, but rarely like tantalum.\n"}
{"id": "19436700", "url": "https://en.wikipedia.org/wiki?curid=19436700", "title": "ESolar", "text": "ESolar\n\neSolar is a privately held company that develops concentrating solar power (CSP) plant technology. The company was founded by the Pasadena-based business incubator Idealab in 2007 as a developer of CSP plant technology. The company aims to develop a low cost alternative to fossil fuels through a combination of small heliostats, modular architecture, and a high-precision sun-tracking system. In October 2017, an article in GreenTech Media suggested that eSolar ceased business in late 2016.\n\neSolar has designed heliostats that are smaller than the industry norm, allowing for pre-fabrication, mass-manufacturing, and easy installation, thereby reducing production and installation costs. eSolar announced a new heliostat design, referred to as SCS5, during the 2013 SolarPACES Conference in Las Vegas, Nevada. SCS5 offers a more simplified design and enhanced reliability to reduce total installed solar collector system cost by more than a third.\n\neSolar has developed a sun-tracking control system that is able to calibrate heliostats and monitor the performance of each heliostat within the field. Tests have shown an unprecedented pointing accuracy and high thermal concentration ratios. [citation]\n\nAn array of heliostats reflect solar radiation to a tower-mounted thermal receiver. In the direct-steam configuration, the concentrated solar energy boils water in the receiver to produce steam. The steam is piped to a steam turbine generator, which converts the energy to electricity. The steam out of the turbine is condensed and pressurized back into the receiver. In the molten salt configuration, the concentrated solar energy heats molten salt to store thermal energy for future use.\n\neSolar's field layout design is built around the concept of scalable modules. In the direct-steam configuration, each module comprises over 20,000 square meters of heliostats arranged in two subfields - north and south - which track the sun and concentrate solar energy to the tower mounted receiver. The field layout is a simple, regular design that eliminates precision surveying and ground penetration. In the molten salt configuration a hexagonal solar field consists of over 100,000 square meters of reflector area.\n\nA 46 MW eSolar power unit consists of sixteen heliostat fields and towers, a single steam turbine generator set, and a steam condenser, with a typical footprint of approximately 100 hectares (250 acres). These basic 46 MW units are designed to be scaled up to fit specific power requirements. In the molten salt configuration, 10 modules may be aggregated to build a 100MW, 50% capacity factor solar power plant. Plants of various electric output and capacity factor may be assembled through use or varying numbers of molten salt modules.\n\nIn the summer of 2009, eSolar unveiled the 5 MW Sierra SunTower plant, a commercial facility in Lancaster, California that demonstrates the company's technology. Sierra SunTower is interconnected to the Southern California Edison (SCE) grid and, as of spring 2010, is the only commercial CSP tower facility in North America.\n\nSierra SunTower includes two eSolar modules. 24,000 heliostats, divided between four sub-fields, track the sun and focus its energy onto two tower-mounted receivers. The focused heat converts feedwater piped to the receivers into superheated steam that drives a reconditioned 1947 GE turbine generator to produce electricity. The steam passes through a steam condenser, reverts to water through cooling, and the process repeats.\n\nDuring the 12 months of construction, Sierra SunTower created over 300 temporary jobs. In operation, the site employs 21 permanent employees.\n\nSierra Suntower has been certified by the California Energy Commission as a renewable energy facility. Power from the facility is sold under a Power Purchase Agreement (PPA) with SCE, providing renewable energy for up to 4,000 homes.\n\nThe 5 MW output from Sierra SunTower reduces emissions by 7,000 tons per year, an amount equivalent to planting of trees, removing 1,368 automobiles from the road, or saving 650,000 gallons of gasoline.\n\nThe eSolar Sierra SunTower generated 539 MWh (MegaWatt-hour) of electricity from August 1, 2010, to July 31, 2011. A total of 539 MWh of gross electrical energy has been generated at Sierra during the period Aug 1, 2010 and July 31, 2011. This is approximately 12.6% of the expected power generation of the initial estimate of 4270 MWh, a dismal result.\n\neSolar, with its partner, Babcock & Wilcox Power Generation Group, has been working on the design of a modular molten salt-based concentrating solar power plant since 2010. With the support from the U. S. Department of Energy, the companies try to come up with a design for plants with flexibility of the sizes from 50 to 200 MW by replicating the basic module without a redesign. Each module uses hexagonal heliostat field to reflect sunlight to the salt-in-tube external thermal receiver on top of the tower at the center. With many modules working together, the receivers heat the 285 °C cold salt input and return 565 °C hot salt to centrally located storage. The hot salt will then be used in the generator system to generate electricity. The new system incorporates the experience from the Sierra SunTower which uses a Babcock & Wilcox's water-based receiver, and the finding from the Solar Two project.\n\nNRG Energy, Inc. partnered with eSolar in February 2009 to develop solar power plants with a total generation capacity of up to 500 MW at sites within California and across the Southwestern United States. Additionally, NRG invested approximately $10 million for equity and associated development rights for three projects and a portfolio of PPAs to develop, build, own, and operate up to 11 eSolar modular solar generating units at these sites. The development assets will use eSolar's concentrating solar power (CSP) technology to sell renewable electricity under contracted PPAs with local utilities seeking competitively priced, zero-carbon solar power.\n\nIn January 2010, eSolar announced a partnership with Penglai Electric, a privately owned Chinese electrical power equipment manufacturer, to build 2,000 MW of solar thermal power plants in China by 2021.\n\nThe deal represents China's largest CSP project to date (Spring 2010) and has a total potential capital investment of more than $5 billion. Under the master licensing agreement, Penglai Electric will use eSolar's solar thermal technology. The first plant of 92 MW will break ground in 2010. The plants will be co-located with biomass electricity generation facilities, together eliminating 15 million tons of carbon dioxide emissions annually.\n\nIt appears that this partnership is no longer progressing.\n\nIn February 2010, Ferrostaal, a global power and industrial plant developer, partnered with eSolar to deploy turnkey solar power plants in countries including Spain, the United Arab Emirates, and South Africa. Under the partnership, eSolar will provide solar field and receiver technology, while Ferrostaal will supply the power block, act as general contractor, and manage financing activities.\n\nIn August 2011 General Electric(GE) made a strategic investment of up to $40 million in eSolar.\n\nIn December 2014, eSolar was awarded a commercial contract by Aalborg CSP to provide a solar collector system (SCS) solution for the Sundrop Farms Port Augusta expansion project. This expansion will increase the greenhouses by 20 hectares in support of a 10-year tomato supply contract with Coles.\n\nWhen eSolar's Sierra SunTower was unveiled in August 2009, California governor Arnold Schwarzenegger praised the company for, \"...proving that California’s energy and environmental leadership are advancing carbon-free, cost-effective energy that can be used around the world.\"\n\nIn December 2009, eSolar was honored by the World Economic Forum as a 2010 Technology Pioneer. The award recognizes eSolar’s technological innovation and global commitment to delivering a clean, low-cost energy alternative to fossil fuels. An interview with Bill Gross is featured on YouTube where he responds to four questions posed to each of the World Economic Forum's Technology Pioneer 2010 winners.\n\nIn December 2009, editors of \"Power Engineering\" magazine selected Sierra SunTower as the winner of the \"Best Renewable Project\", recognizing the facility as an exceptional power generation project toward meeting growing global demand.\n\nIn February 2010, Sierra SunTower won Renewable Energy World’s \"Renewable Project of the Year\" award. The award recognized eSolar's achievements in the clean energy industry by naming Sierra SunTower an exceptional breakthrough in the commercialization of solar thermal technology.\n\nIn February 2010, eSolar was named one of The Massachusetts Institute of Technology (MIT)'s \"Technology Review\"'s 2010 TR50. The TR50 is a list of the 50 most innovative companies in the world, those that have demonstrated superiority at creating technology that transforms how we live. The 2010 TR50 celebrates the development of emerging technologies and the progress made in those already established. eSolar is honored alongside such companies as Google, Apple, Twitter, and IBM.\n\nIn March 2010, the Wall Street Journal's first survey of venture-backed clean-technology companies featured eSolar as a Top 10 Cleantech Company with the capital, executive experience, and investor know-how to succeed in an increasingly crowded field.\n\neSolar has secured over $182 million in investment funds. Investors include Google.org, Oak Investment Partners, NRG Energy, ACME Group, Idealab, and Quercus Trust.\n\n\n\n"}
{"id": "40604748", "url": "https://en.wikipedia.org/wiki?curid=40604748", "title": "Energy Networks Association (United Kingdom)", "text": "Energy Networks Association (United Kingdom)\n\nThe Energy Networks Association (ENA) is the industry body funded by UK gas and electricity transmission and distribution licence holders.\n\nENA was formed in October 2003 from the dissolution of the Electricity Association into three separate industry bodies:\n\nFollowing the demerger of the Gas Distribution Network operations of British Gas Transco, they also joined ENA.\n\nENA member companies are:\n\n(list last updated April 2018).\n\nENA's role is to provide a strategic focus for the energy networks sector by communicating key messages. All its work is underpinned by technical expertise—more than half the association's staff are specialist engineers. ENA's online catalogue contains hundreds of reference documents relating to cables, contactor gear, electrical and mechanical composites, overhead transmissions and distribution lines, switchgear, engineering recommendations and other information. The association also records faults, defects and safety information on behalf of the networks industry.\n\n\n"}
{"id": "12877572", "url": "https://en.wikipedia.org/wiki?curid=12877572", "title": "Fan (machine)", "text": "Fan (machine)\n\nA fan is a powered machine used to create flow within a fluid, typically a gas such as air.\nA fan consists of a rotating arrangement of vanes or blades which act on the air. The rotating assembly of blades and hub is known as an impeller, a rotor, or a runner. Usually, it is contained within some form of housing or case. This may direct the airflow or increase safety by preventing objects from contacting the fan blades. Most fans are powered by electric motors, but other sources of power may be used, including hydraulic motors, handcranks, internal combustion engines, and solar power.\n\nMechanically, a fan can be any revolving vane or vanes used for producing currents of air. Fans produce air flows with high volume and low pressure (although higher than ambient pressure), as opposed to compressors which produce high pressures at a comparatively low volume. A fan blade will often rotate when exposed to an air fluid stream, and devices that take advantage of this, such as anemometers and wind turbines, often have designs similar to that of a fan.\n\nTypical applications include climate control and personal thermal comfort (e.g., an electric table or floor fan), vehicle engine cooling systems (e.g., in front of a radiator), machinery cooling systems (e.g., inside computers and audio power amplifiers), ventilation, fume extraction, winnowing (e.g., separating chaff of cereal grains), removing dust (e.g. sucking as in a vacuum cleaner), drying (usually in combination with a heat source) and to provide draft for a fire.\n\nWhile fans are often used to cool people, they do not actually cool air (if anything, electric fans warm it slightly due to the warming of their motors), but work by evaporative cooling of sweat and increased heat convection into the surrounding air due to the airflow from the fans. Thus, fans may become ineffective at cooling the body if the surrounding air is near body temperature and contains high humidity. \n\nThe punkah fan was used in India about 500 BCE. It was a handheld fan made from bamboo strips or other plant fibre, that could be rotated or fanned to move air. During British rule, the word came to be used by Anglo-Indians to mean a large swinging flat fan, fixed to the ceiling, and pulled by a servant, called the punkawallah.\nFor purposes of air conditioning, the Han Dynasty craftsman and engineer Ding Huan (fl. 180 CE) invented a manually operated rotary fan with seven wheels that measured 3 m (10 ft) in diameter; in the 8th century, during the Tang Dynasty (618–907), the Chinese applied hydraulic power to rotate the fan wheels for air conditioning, while the rotary fan became even more common during the Song Dynasty (960–1279).\n\nIn the 17th century, the experiments of scientists including Otto von Guericke, Robert Hooke and Robert Boyle, established the basic principles of vacuum and airflow. The English architect Sir Christopher Wren applied an early ventilation system in the Houses of Parliament that used bellows to circulate air. Wren's design would be the catalyst for much later improvement and innovation. The first rotary fan used in Europe was for mine ventilation during the 16th century, as illustrated by Georg Agricola (1494–1555).\n\nJohn Theophilus Desaguliers, a British engineer, demonstrated a successful use of a fan system to draw out stagnant air from coal mines in 1727 and soon afterwards he installed a similar apparatus in Parliament.\nGood ventilation was particularly important in coal mines to reduce casualties from asphyxiation. The civil engineer John Smeaton, and later John Buddle installed reciprocating air pumps in the mines in the North of England. However, this arrangement was not ideal as the machinery was liable to breaking down.\n\nWith the advent of practical steam power, fans could finally be used for ventilation. In 1837 William Fourness of England installed a steam-driven fan at Leeds. In 1849 a 6 m radius steam driven fan, designed by William Brunton, was made operational in the Gelly Gaer Colliery of South Wales. The model was exhibited at the Great Exhibition of 1851. Also in 1851 David Boswell Reid, a Scottish doctor, installed four steam powered fans in the ceiling of St George's Hospital in Liverpool, so that the pressure produced by the fans would force the incoming air upward and through vents in the ceiling. Improvements in the technology were made by James Nasmyth, Frenchman Theophile Guibal and J. R. Waddle.\n\nBetween 1882 and 1886 Schuyler Wheeler invented a fan powered by electricity. It was commercially marketed by the American firm Crocker & Curtis electric motor company. In 1882, Philip Diehl developed the world's first electric ceiling fan. During this intense period of innovation, fans powered by alcohol, oil, or kerosene were common around the turn of the 20th century.\nIn 1909, KDK of Japan pioneered the invention of mass-produced electric fans for home use. In the 1920s, industrial advances allowed steel fans to be mass-produced in different shapes, bringing fan prices down and allowing more homeowners to afford them. In the 1930s, the first art deco fan (the \"swan fan\") was designed. By the 1940s, Crompton Greaves of India became the world's largest manufacturer of electric ceiling fans mainly for sale in India, Asia and the Middle East. By the 1950s, table and stand fans were manufactured in colors that were bright and eye catching.\n\nWindow and central air conditioning in the 1960s caused many companies to discontinue production of fans. But in the mid 1970s, with an increasing awareness of the cost of electricity and the amount of energy used to heat and cool homes, turn-of-the-century styled ceiling fans became immensely popular again as both decorative and energy efficient units.\n\nIn 1998, Walter K. Boyd invented the HVLS ceiling fan. A lifelong inventor, Boyd was charged with developing a system to cool dairy cattle. Dairy cattle, when overheated, decrease milk production. Using the laws of physics and airflow, Boyd developed a slow moving fan that incorporated 10 aluminum blades and was 8-feet diameter. Unlike traditional ceiling fans that move quickly, this large fan moved slowly. Due to its diameter size, the fan moved a large column of air down and out 360 degrees and continuously mixed fresh air with the stale air inside the barn. They are used in many industrial and agricultural settings, because of their energy efficiency. It also cooled the inside of the barn without causing the dairy cattle undue stress or kicking up dust.\n\nAfter much testing, Boyd discovered HVLS fan technology to be energy efficient as it costs less to run one HVLS fan than it did to run 50 small high-speed fans. Due to the skyrocketing costs of energy, HVLS commercial ceiling fans are used today to supplement HVAC systems in industrial and commercial settings, including warehouses, manufacturing facilities and malls, as HVLS fans help lower heating and cooling costs.\n\nMechanical revolving blade fans are made in a wide range of designs. They are used on the floor, table, desk, or hung from the ceiling (ceiling fan). They can also be built into a window, wall, roof, chimney, etc. Most electronic systems such as computers include fans to cool the circuits inside, and in appliances such as hair dryers and portable space heaters and mounted/installed wall heaters. They are also used for moving air in air-conditioning systems, and in automotive engines, where they are driven by belts or by direct motor. Fans used for comfort create a wind chill by increasing the heat transfer coefficient, but do not lower temperatures directly. Fans used to cool electrical equipment or in engines or other machines do cool the equipment directly by forcing hot air into the cooler environment outside of the machine.\n\nThere are three main types of fans used for moving air, \"axial\", \"centrifugal\" (also called \"radial\") and \"cross flow\" (also called \"tangential\"). The American Society of Mechanical Engineers Performance Testing Code 11 (PTC) provides standard procedures for conducting and reporting tests on fans, including those of the centrifugal, axial, and mixed flows.\n\nAxial-flow fans have blades that force air to move parallel to the shaft about which the blades rotate. This type of fan is used in a wide variety of applications, ranging from small cooling fans for electronics to the giant fans used in wind tunnels. Axial flow fans are applied in air conditioning and industrial process applications. Standard axial flow fans have diameters from 300–400 mm or 1800 to 2000 mm and work under pressures up to 800 Pa. Special types of fans are used as low pressure compressor stages in aircraft engines.\nExamples of axial fans are:\n\nOften called a \"squirrel cage\" (because of its general similarity in appearance to exercise wheels for pet rodents) or \"scroll fan\", the centrifugal fan has a moving component (called an impeller) that consists of a central shaft about which a set of blades form a spiral, or ribs, are positioned. Centrifugal fans blow air at right angles to the intake of the fan, and spin the air outwards to the outlet (by deflection and centrifugal force). The impeller rotates, causing air to enter the fan near the shaft and move perpendicularly from the shaft to the opening in the scroll-shaped fan casing. A centrifugal fan produces more pressure for a given air volume, and is used where this is desirable such as in leaf blowers, blowdryers, air mattress inflators, inflatable structures, climate control, and various industrial purposes. They are typically quieter than comparable axial fans.\n\nThe \"cross-flow\" or \"tangential\" fan, sometimes known as a \"tubular\" fan, was patented in 1893 by Paul Mortier, and is used extensively in the HVAC industry. The fan is usually long in relation to the diameter, so the flow approximately remains two-dimensional away from the ends. The CFF uses an impeller with forward curved blades, placed in a housing consisting of a rear wall and vortex wall. Unlike radial machines, the main flow moves transversely across the impeller, passing the blading twice.\n\nThe flow within a cross-flow fan may be broken up into three distinct regions: a vortex region near the fan discharge, called an eccentric vortex, the through-flow region, and a paddling region directly opposite. Both the vortex and paddling regions are dissipative, and as a result, only a portion of the impeller imparts usable work on the flow. The cross-flow fan, or transverse fan, is thus a two-stage partial admission machine. The popularity of the crossflow fan in the HVAC industry comes from its compactness, shape, quiet operation, and ability to provide high pressure coefficient. Effectively a rectangular fan in terms of inlet and outlet geometry, the diameter readily scales to fit the available space, and the length is adjustable to meet flow rate requirements for the particular application.\n\nCommon household tower fans are also cross-flow fans. Much of the early work focused on developing the cross-flow fan for both high and low-flow-rate conditions, and resulted in numerous patents. Key contributions were made by Coester, Ilberg and Sadeh, Porter and Markland, and Eck. One interesting phenomenon particular to the cross-flow fan is that, as the blades rotate, the local air incidence angle changes. The result is that in certain positions the blades act as compressors (pressure increase), while at other azimuthal locations the blades act as turbines (pressure decrease).\n\nSince the flow both enters and exits the impeller radially, the crossflow fan is well suited for aircraft applications. Due to the 2D nature of the flow, the fan readily integrates into a wing for use in both thrust production and boundary-layer control. A configuration that utilizes a crossflow fan is located at the wing leading edge is the fanwing. This design creates lift by deflecting the wake downward due to the rotational direction of the fan, causing large Magnus force, similar to a spinning leading-edge cylinder. Another configuration utilizing a crossflow fan for thrust and flow control is the propulsive wing. In this design, the crossflow fan is placed near the trailing edge of a thick wing, and draws the air off the wing's suction (top) surface. By doing this, the propulsive wing is nearly stall-free, even at extremely high angles of attack, producing very high lift. The external links section provides links to these concepts.\n\nA cross flow fan, is a centrifugal fan in which the air flows through the fan, rather than through an inlet. The rotor of a cross flow fan is covered to create a pressure differential. When used in household fans, cross flow fans have smaller opening on one side and a larger opening on the other. Cross flow fans have openings of different sizes on the front and rear sides. The resultant pressure difference allows air to flow straight through the fan, even though the fan blades counter the flow of air on one side of the rotation. Cross flow fans give airflow along the entire width of the fan, however, they are noiser than ordinary centrifugal fans presumedly because the fan blades fight the flow of air on one side of the rotation unlike normal squirrel cage fans. Cross flow fans are often used in air conditioners, automobile ventilation systems, and for cooling in medium-sized equipment such as photocopiers.\n\nThe action of a fan or blower causes pressures slightly above atmospheric, which are called plenums.\n\nThis is an indirect blower system which collects pressurized airflow from a standard blower device that can be of any type mentioned in this article, and directs the collected airflow through a hollow tube or toroid, blowing a thin high-velocity laminar airflow from holes or a continuous slot across the surface of the tube or toroid. These fans have a 3-dimensional mixed-flow impeller in a lower compartment. Air is drawn in and compressed, before being expelled through an annulus, and accelerated over an airfoil lamp. Inducing and entraining ambient air through viscous shearing, the loop-shaped upper section multiplies the total airflow many times. The high velocity laminar airflow tends to drag ambient air along with it, due to viscous shear. Only around 7% of the total airflow actually passes through the fan itself.\n\nBellows are also used to move air, although not generally considered fans. A hand-operated bellows is essentially a bag with a nozzle and handles, which can be filled with air by one movement, and the air expelled by another. Typically it would comprise two rigid flat surfaces hinged at one end, where a nozzle is fitted, and with handles at the other.\n\nThe sides of the surfaces are joined by a flexible and air-proof material such as leather; the surfaces and joining material comprise a bag sealed everywhere but at the nozzle. (The joining material typically has a characteristic pleated construction that is so common that similar expanding fabric arrangements not used for moving air, such as on a folding camera, are called bellows.) Separating the handles expands the bag, which fills with air; squeezing them together expels the air. A simple valve (e.g., a flap) may be fitted so that air enters without having to come from the nozzle, which may be close to a fire.\n\nBellows produce a directed pressurized stream of air; the airflow volume is typically low with moderate pressure. They are an older technology, used mainly to produce a strong and directed airflow unlike non-electric bladed mechanical fans, before the introduction of electricity.\n\nThe Dyson Air Multiplier fans, and the Imperial C2000 series range hood fans, have no exposed fan blades or other visibly moving parts except their oscillating and tilting head. The airflow is generated using the Coandă effect; a small quantity of air from a high-pressure-bladed impeller fan, contained in the base rather than exposed, drives a large airmass via a low-pressure area created by the airfoil. The US Patent & Trademark Office initially ruled that Dyson's patent was not an improvement on the Toshiba patent on a nearly identical bladeless desktop fan granted in 1981.\nAir curtains and air doors also utilize this effect to help retain warm or cool air within an otherwise exposed area that lacks a cover or door. Air curtains are commonly used on open-face dairy, freezer, and vegetable displays to help retain chilled air within the cabinet using a laminar airflow circulated across the display opening. The airflow is typically generated by a mechanical fan of any type described in this article hidden in the base of the display cabinet.\n\nDifferences in air temperature will affect the density of air and can be used to induce air circulation through the mere act of heating or cooling an air mass. This effect is so subtle and works at such low air pressures that it does not appear to fit the definition of a fan technology. However, prior to the development of electricity, convective airflow was the primary method of inducing airflow in living spaces.\nOld fashioned oil and coal furnaces were not electric and operated simply on the principle of convection to move the warm air. Very large volume air ducts were sloped upwards away from the top of the furnace towards floor and wall registers above the furnace. Cool air was returned through similar large ducts leading to the bottom of the furnace.\nOlder houses from before electrification often had open duct grilles leading from the ceiling of a lower level to the floor of an upper level, to allow convective airflow to slowly rise up the building from one floor to the next.\nOuthouses commonly rely on a simple enclosed air channel in a corner of the structure to exhaust offensive odors. Exposed to sunlight, the channel is warmed and a slow convective air current is vented out the top of the building, while fresh air enters the pit through the seat hole.\n\nAn electrostatic fluid accelerator propels airflow by inducing motion in airborne charged particles. A high voltage electric field (commonly 25,000 to 50,000 volts) formed between exposed charged anode and cathode surfaces is capable of inducing airflow through a principle referred to as ionic wind. The airflow pressure is typically very low but the air volume can be large. However, a sufficiently high voltage potential can also cause the formation of ozone and nitrogen oxides, which are reactive and irritating to mucous membranes.\n\nFans generate noise from the rapid flow of air around blades and obstacles, and sometimes from the motor. Fan noise has been found to be roughly proportional to the fifth power of fan speed; halving speed reduces noise by about 15 dB.\n\nStandalone fans are usually powered by an electric motors, often attached directly to the motor's output, with no gears or belts. The motor is either hidden in the fan's center hub or extends behind it. For big industrial fans, three-phase asynchronous motors are commonly used, placed near the fan and driving it through a belt and pulleys. Smaller fans are often powered by shaded pole AC motors, or brushed or brushless DC motors. AC-powered fans usually use mains voltage, while DC-powered fans use low voltage, typically 24V, 12V, or 5 V. Cooling fans for computer equipment always use brushless DC motors, which generate much less electromagnetic interference than other types.\n\nIn machines with a rotating part, the fan is often connected to it rather than being powered separately. This is commonly seen in motor vehicles with internal combustion engines, large cooling systems, locomotives, and winnowing machines, where the fan is connected to the drive shaft or through a belt and pulleys. Another common configuration is a dual-shaft motor, where one end of the shaft drives a mechanism, while the other has a fan mounted on it to cool the motor itself. Window air conditioners commonly use a dual-shaft fan to operate separate blowers for the interior and exterior parts of the device.\n\nWhere electrical power or rotating parts are not readily available, fans may be driven by other methods. High-pressure gases such as steam can be used to drive a small turbine, and high-pressure liquids can be used to drive a pelton wheel, either which can provide the rotational drive for a fan. \n\nLarge, slow-moving energy sources such as a flowing river can also power a fan using a water wheel and a series of step-down gears or pulleys to increase the rotational speed to that which is required for efficient fan operation.\n\nElectric fans used for ventilation may be powered by solar panels instead of mains current. This is an attractive option because once the capital costs of the solar panel have been covered, the resulting electricity is free. In addition, electricity is always available when the sun is shining and the fan needs to run.\n\nA typical example uses a detached 10-watt, 12x12 inch (30x30 cm) solar panel and is supplied with appropriate brackets, cables, and connectors. It can be used to ventilate up to 1,250 square feet (100 m2) of area and can move air at up to 800 cubic feet per minute (400 L/s). Because of the wide availability of 12 V brushless DC electric motors and the convenience of wiring such a low voltage, such fans usually operate on 12 volts.\n\nThe detached solar panel is typically installed in the spot which gets most of the sun light and then connected to the fan mounted as far as 20 to 25 feet (6 to 7 m) away. Other permanently-mounted and small portable fans include an integrated (non-detachable) solar panel.\n\n"}
{"id": "1700015", "url": "https://en.wikipedia.org/wiki?curid=1700015", "title": "HVDC Three Gorges – Changzhou", "text": "HVDC Three Gorges – Changzhou\n\nThe HVDC Three Gorges – Changzhou is an long bipolar HVDC transmission line in China for the transmission of electric power from the Three Gorges power plant to the area of Changzhou.\n\nThe transmission line went into service in 2004. It runs from the static inverter station, Longquan, away from the Three Gorges power plant to the static inverter plant, Zhengping, near Changzhou. The HVDC Three Gorges-Changzhou is a bipolar 500 kV powerline with a maximum transmission power rating of 3,000 megawatts.\n\nA part of the line is the Yangtze River Crossing Wuhu over Yangtze River using tall pylons.\n\nThe electrode at Chujiahu is also used by HVDC Hubei - Shanghai for grounding.\n\n"}
{"id": "14307", "url": "https://en.wikipedia.org/wiki?curid=14307", "title": "Hall effect", "text": "Hall effect\n\nThe Hall effect is the production of a voltage difference (the Hall voltage) across an electrical conductor, transverse to an electric current in the conductor and to an applied magnetic field perpendicular to the current. It was discovered by Edwin Hall in 1879. For clarity, the original effect is sometimes called the ordinary Hall effect to distinguish it from other \"Hall effects\" which have different physical mechanisms.\n\nThe Hall coefficient is defined as the ratio of the induced electric field to the product of the current density and the applied magnetic field. It is a characteristic of the material from which the conductor is made, since its value depends on the type, number, and properties of the charge carriers that constitute the current.\n\nThe Hall effect was discovered in 1879 by Edwin Hall while he was working on his doctoral degree at Johns Hopkins University in Baltimore, Maryland. Eighteen years before the electron was discovered, his measurements of the tiny effect produced in the apparatus he used were an experimental tour de force, published under the name \"On a New Action of the Magnet on Electric Currents\".\n\nThe Hall effect is due to the nature of the current in a conductor. Current consists of the movement of many small charge carriers, typically electrons, holes, ions (see Electromigration) or all three. When a magnetic field is present, these charges experience a force, called the Lorentz force. When such a magnetic field is absent, the charges follow approximately straight, 'line of sight' paths between collisions with impurities, phonons, etc. However, when a magnetic field with a perpendicular component is applied, their paths between collisions are curved, thus moving charges accumulate on one face of the material. This leaves equal and opposite charges exposed on the other face, where there is a scarcity of mobile charges. The result is an asymmetric distribution of charge density across the Hall element, arising from a force that is perpendicular to both the 'line of sight' path and the applied magnetic field. The separation of charge establishes an electric field that opposes the migration of further charge, so a steady electric potential is established for as long as the charge is flowing.\n\nIn classical electromagnetism electrons move in the opposite direction of the current (by convention \"current\" describes a theoretical \"hole flow\"). In some semiconductors it \"appears\" \"holes\" are actually flowing because the direction of the voltage is opposite to the derivation below.\n\nFor a simple metal where there is only one type of charge carrier (electrons), the Hall voltage can be derived by using the Lorentz force and seeing that, in the steady-state condition, charges are not moving in the -axis direction. Thus, the magnetic force on each electron in the -axis direction is cancelled by a -axis electrical force due to the buildup of charges. The term is the drift velocity of the current which is assumed at this point to be holes by convention. The term is negative in the -axis direction by the right hand rule.\n\nIn steady state, , so , where is assigned in the direction of the -axis, (and not with the arrow of the induced electric field as in the image (pointing in the direction), which tells you where the field caused by the electrons is pointing).\n\nIn wires, electrons instead of holes are flowing, so and . Also . Substituting these changes gives\n\nThe conventional \"hole\" current is in the negative direction of the electron current and the negative of the electrical charge which gives where is charge carrier density, is the cross-sectional area, and is the charge of each electron. Solving for formula_3 and plugging into the above gives the Hall voltage:\n\nIf the charge build up had been positive (as it appears in some semiconductors), then the assigned in the image would have been negative (positive charge would have built up on the left side).\n\nThe Hall coefficient is defined as\n\nwhere is the current density of the carrier electrons, and is the induced electric field. In SI units, this becomes\n\n(The units of are usually expressed as m/C, or Ω·cm/G, or other variants.) As a result, the Hall effect is very useful as a means to measure either the carrier density or the magnetic field.\n\nOne very important feature of the Hall effect is that it differentiates between positive charges moving in one direction and negative charges moving in the opposite. The Hall effect offered the first real proof that electric currents in metals are carried by moving electrons, not by protons. The Hall effect also showed that in some substances (especially p-type semiconductors), it is more appropriate to think of the current as positive \"holes\" moving rather than negative electrons. A common source of confusion with the Hall effect is that holes moving to the left are really electrons moving to the right, so one expects the same sign of the Hall coefficient for both electrons and holes. This confusion, however, can only be resolved by modern quantum mechanical theory of transport in solids.\n\nThe sample inhomogeneity might result in spurious sign of the Hall effect, even in ideal van der Pauw configuration of electrodes. For example, positive Hall effect was observed in evidently n-type semiconductors. Another source of artifact, in uniform materials, occurs when the sample's aspect ratio is not long enough: the full Hall voltage only develops far away from the current-introducing contacts, since at the contacts the transverse voltage is shorted out to zero.\n\nWhen a current-carrying semiconductor is kept in a magnetic field, the charge carriers of the semiconductor experience a force in a direction perpendicular to both the magnetic field and the current. At equilibrium, a voltage appears at the semiconductor edges.\n\nThe simple formula for the Hall coefficient given above is usually a good explanation when conduction is dominated by a single charge carrier. However, in semiconductors the theory is more complex, because in these materials conduction can involve significant, simultaneous contributions from both electrons and holes, which may be present in different concentrations and have different mobilities. For moderate magnetic fields the Hall coefficient is\nor equivalently\nwith\nHere is the electron concentration, the hole concentration, the electron mobility, the hole mobility and the elementary charge.\n\nFor large applied fields the simpler expression analogous to that for a single carrier type holds.\n\nAlthough it is well known that magnetic fields play an important role in star formation, research models indicate that Hall diffusion critically influences the dynamics of gravitational collapse that forms protostars.\n\nFor a two-dimensional electron system which can be produced in a MOSFET, in the presence of large magnetic field strength and low temperature, one can observe the quantum Hall effect, in which the Hall conductance undergoes quantum Hall transitions to take on the quantized values.\n\nThe spin Hall effect consists in the spin accumulation on the lateral boundaries of a current-carrying sample. No magnetic field is needed. It was predicted by M. I. Dyakonov and V. I. Perel in 1971 and observed experimentally more than 30 years later, both in semiconductors and in metals, at cryogenic as well as at room temperatures.\n\nFor mercury telluride two dimensional quantum wells with strong spin-orbit coupling, in zero magnetic field, at low temperature, the quantum spin Hall effect has been recently observed.\n\nIn ferromagnetic materials (and paramagnetic materials in a magnetic field), the Hall resistivity includes an additional contribution, known as the anomalous Hall effect (or the extraordinary Hall effect), which depends directly on the magnetization of the material, and is often much larger than the ordinary Hall effect. (Note that this effect is \"not\" due to the contribution of the magnetization to the total magnetic field.) For example, in nickel, the anomalous Hall coefficient is about 100 times larger than the ordinary Hall coefficient near the Curie temperature, but the two are similar at very low temperatures. Although a well-recognized phenomenon, there is still debate about its origins in the various materials. The anomalous Hall effect can be either an \"extrinsic\" (disorder-related) effect due to spin-dependent scattering of the charge carriers, or an \"intrinsic\" effect which can be described in terms of the Berry phase effect in the crystal momentum space (-space).\n\nThe Hall effect in an ionized gas (plasma) is significantly different from the Hall effect in solids (where the Hall parameter is always much less than unity). In a plasma, the Hall parameter can take any value. The Hall parameter, , in a plasma is the ratio between the electron gyrofrequency, , and the electron-heavy particle collision frequency, :\n\nwhere\n\nThe Hall parameter value increases with the magnetic field strength.\n\nPhysically, the trajectories of electrons are curved by the Lorentz force. Nevertheless, when the Hall parameter is low, their motion between two encounters with heavy particles (neutral or ion) is almost linear. But if the Hall parameter is high, the electron movements are highly curved. The current density vector, , is no longer collinear with the electric field vector, . The two vectors and make the Hall angle, , which also gives the Hall parameter:\n\nHall probes are often used as magnetometers, i.e. to measure magnetic fields, or inspect materials (such as tubing or pipelines) using the principles of magnetic flux leakage.\n\nHall effect devices produce a very low signal level and thus require amplification. While suitable for laboratory instruments, the vacuum tube amplifiers available in the first half of the 20th century were too expensive, power consuming, and unreliable for everyday applications. It was only with the development of the low cost integrated circuit that the Hall effect sensor became suitable for mass application. Many devices now sold as Hall effect sensors in fact contain both the sensor as described above plus a high gain integrated circuit (IC) amplifier in a single package. Recent advances have further added into one package an analog-to-digital converter and I²C (Inter-integrated circuit communication protocol) IC for direct connection to a microcontroller's I/O port.\n\nHall effect devices (when appropriately packaged) are immune to dust, dirt, mud, and water. These characteristics make Hall effect devices better for position sensing than alternative means such as optical and electromechanical sensing.\n\nWhen electrons flow through a conductor, a magnetic field is produced. Thus, it is possible to create a non-contacting current sensor. The device has three terminals.\nA sensor voltage is applied across two terminals and the third provides a voltage proportional to the current being sensed. This has several advantages; no additional resistance (a \"shunt\", required for the most common current sensing method) need to be inserted in the primary circuit. Also, the voltage present on the line to be sensed is not transmitted to the sensor, which enhances the safety of measuring equipment.\n\nMagnetic flux from the surroundings (such as other wires) may diminish or enhance the field the Hall probe intends to detect, rendering the results inaccurate. Also, as Hall voltage is often on the order of millivolts, the output from this type of sensor cannot be used to directly drive actuators but instead must be amplified by a transistor-based circuit.\n\nWays to measure mechanical positions within an electromagnetic system, such as a brushless direct current motor, include (1) the Hall effect, (2) optical position encoder (e.g., absolute and incremental encoders) and (3) induced voltage by moving the amount of metal core inserted into a transformer. When Hall is compared to photo-sensitive methods, it is harder to get absolute position with Hall. Hall detection is also sensitive to stray magnetic fields.\n\nHall effect sensors are readily available from a number of different manufacturers, and may be used in various sensors such as rotating speed sensors (bicycle wheels, gear-teeth, automotive speedometers, electronic ignition systems), fluid flow sensors, current sensors, and pressure sensors. Common applications are often found where a robust and contactless switch or potentiometer is required. These include: electric airsoft guns, triggers of electropneumatic paintball guns, go-cart speed controls, smart phones, and some global positioning systems.\n\nHall sensors can detect stray magnetic fields easily, including that of Earth, so they work well as electronic compasses: but this also means that such stray fields can hinder accurate measurements of small magnetic fields. To solve this problem, Hall sensors are often integrated with magnetic shielding of some kind. For example, a Hall sensor integrated into a ferrite ring (as shown) can reduce the detection of stray fields by a factor of 100 or better (as the external magnetic fields cancel across the ring, giving no residual magnetic flux). This configuration also provides an improvement in signal-to-noise ratio and drift effects of over 20 times that of a bare Hall device.\n\nThe range of a given feedthrough sensor may be extended upward and downward by appropriate wiring. To extend the range to lower currents, multiple turns of the current-carrying wire may be made through the opening, each turn adding to the sensor output the same quantity; when the sensor is installed onto a printed circuit board, the turns can be carried out by a staple on the board. To extend the range to higher currents, a current divider may be used. The divider splits the current across two wires of differing widths and the thinner wire, carrying a smaller proportion of the total current, passes through the sensor.\n\nA variation on the ring sensor uses a split sensor which is clamped onto the line enabling the device to be used in temporary test equipment. If used in a permanent installation, a split sensor allows the electric current to be tested without dismantling the existing circuit.\n\nThe output is proportional to both the applied magnetic field and the applied sensor voltage. If the magnetic field is applied by a solenoid, the sensor output is proportional to the product of the current through the solenoid and the sensor voltage. As most applications requiring computation are now performed by small digital computers, the remaining useful application is in power sensing, which combines current sensing with voltage sensing in a single Hall effect device.\n\nBy sensing the current provided to a load and using the device's applied voltage as a sensor voltage it is possible to determine the power dissipated by a device.\n\nHall effect devices used in motion sensing and motion limit switches can offer enhanced reliability in extreme environments. As there are no moving parts involved within the sensor or magnet, typical life expectancy is improved compared to traditional electromechanical switches. Additionally, the sensor and magnet may be encapsulated in an appropriate protective material. This application is used in brushless DC motors.\n\nHall effect sensors, affixed to mechanical gauges that have magnetized indicator needles, can translate the physical position or orientation of the mechanical indicator needle into an electrical signal that can be used by electronic indicators, controls or communications devices.\n\nCommonly used in distributors for ignition timing (and in some types of crank and camshaft position sensors for injection pulse timing, speed sensing, etc.) the Hall effect sensor is used as a direct replacement for the mechanical breaker points used in earlier automotive applications. Its use as an ignition timing device in various distributor types is as follows. A stationary permanent magnet and semiconductor Hall effect chip are mounted next to each other separated by an air gap, forming the Hall effect sensor. A metal rotor consisting of windows and tabs is mounted to a shaft and arranged so that during shaft rotation, the windows and tabs pass through the air gap between the permanent magnet and semiconductor Hall chip. This effectively shields and exposes the Hall chip to the permanent magnet's field respective to whether a tab or window is passing through the Hall sensor. For ignition timing purposes, the metal rotor will have a number of equal-sized tabs and windows matching the number of engine cylinders. This produces a uniform square wave output since the on/off (shielding and exposure) time is equal. This signal is used by the engine computer or ECU to control ignition timing. Many automotive Hall effect sensors have a built-in internal NPN transistor with an open collector and grounded emitter, meaning that rather than a voltage being produced at the Hall sensor signal output wire, the transistor is turned on providing a circuit to ground through the signal output wire.\n\nThe sensing of wheel rotation is especially useful in anti-lock braking systems. The principles of such systems have been extended and refined to offer more than anti-skid functions, now providing extended vehicle handling enhancements.\n\nSome types of brushless DC electric motors use Hall effect sensors to detect the position of the rotor and feed that information to the motor controller. This allows for more precise motor control.\n\nApplications for Hall effect sensing have also expanded to industrial applications, which now use Hall effect joysticks to control hydraulic valves, replacing the traditional mechanical levers with contactless sensing. Such applications include mining trucks, backhoe loaders, cranes, diggers, scissor lifts, etc.\n\nA Hall-effect thruster (HET) is a relatively low power device that is used to propel some spacecraft, after it gets into orbit or farther out into space. In the HET, atoms are ionized and accelerated by an electric field. A radial magnetic field established by magnets on the thruster is used to trap electrons which then orbit and create an electric field due to the Hall effect. A large potential is established between the end of the thruster where neutral propellant is fed, and the part where electrons are produced; so, electrons trapped in the magnetic field cannot drop to the lower potential. They are thus extremely energetic, which means that they can ionize neutral atoms. Neutral propellant is pumped into the chamber and is ionized by the trapped electrons. Positive ions and electrons are then ejected from the thruster as a quasineutral plasma, creating thrust.\n\nThe Corbino effect is a phenomenon involving the Hall effect, but a disc-shaped metal sample is used in place of a rectangular one. Because of its shape the Corbino disc allows the observation of Hall effect–based magnetoresistance without the associated Hall voltage.\n\nA radial current through a circular disc, subjected to a magnetic field perpendicular to the plane of the disc, produces a \"circular\" current through the disc.\n\nThe absence of the free transverse boundaries renders the interpretation of the Corbino effect simpler than that of the Hall effect.\n\n\n"}
{"id": "39288041", "url": "https://en.wikipedia.org/wiki?curid=39288041", "title": "House energy rating", "text": "House energy rating\n\nThe house energy rating (HER) is a standard measure of comparison by which one can evaluate the energy efficiency of a new or an existing building. The comparison is generally done for energy requirements for heating and cooling of indoor space. The energy is the main criterion considered by any international building energy rating scheme but there are some other important factors such as production of greenhouse gases emission, indoor environment quality, cost efficiency and thermal comfort, which are considered by some schemes. Basically, the energy rating of a residential building provides detailed information on the energy consumption and the relative energy efficiency of the building. Hence, HERs inform consumers about the relative energy efficiency of homes and encourage them to use this information in making their house purchase decision.\nThere are many energy rating tools by which one can calculate the energy performance of a building. Basically all these tools involve a numerical description or prepare a computer-based model for the rating of a building against standard occupancy and activity templates. So, HERS uses a computer-simulation based methods for assessing the energy efficiency of buildings under standard conditions and its potential for improvement.\n\nHERS is a standardized scheme for evaluation of a home's energy efficiency and expected energy costs.Basically a HERS represents the guideline of a House energy rating. In all countries, HERS show variations in objectives, assessment methodologies & measurement criteria but after all this variation, the goal of all HERS is approximately same and these generate the output in same way.\n\nBasically HERS generates three types of outputs\n\n\nAs per the national energy policies, the HERS are known by different names in different countries. The implementation and promotion of HERS in a country depends upon the national energy policy. Beside all this, the aim of all HERS is almost same and it can be classified into three types.\n\nIncrease in population, Economic issues are the some factors which have escalated the energy demand across the world. In developed countries, the growth rate of energy consumption rate is 1.1%/year while in developing countries the energy consumption growth rate approximately 3 time to that of developed countries. Beside this high growth in energy consumption it also causes to increase the production of green house gases to the atmosphere. The increase in demand of energy, limited resource of convention energy sources, hike in conventional fuel prices, global warming are some important factors which Impetus us to adopt energy saving techniques and alternative sources of energy. Building sector consumes one third of world's resources. Building currently shares approximately 40% of energy in most of the countries and are considered among the largest end-use sector. As per International energy agency (IEA) world energy consumption and green house gases level is going to increase rapidly every year. IEA recognize the building sector as one of the most cost effective sector where energy consumption can be reduced . It is estimated that the energy consumption can be reduced to 1509 million tonnes of equivalent (Mtoe) and at the same time it will cause to reduce the green house gases production up to 12.6 gigatonnes(Gt) by 2050. The international energy outlook report reveals that the energy consumption is increasing in each year and the energy increment trends are shown in fig. 1.\n\nSo, we conclude that the building sector is one of the largest sector where energy consumption and green house gases emission can be reduced effectively by improving the energy efficiency of buildings and hence HERS can play a vital role in achieving all this.\n\nFive star is the first house-rating scheme of Australia, which was developed in 1980 by the GMI council of Australia. This scheme was basically based on the three basic elements, glass, mass and insulation of dwelling. Due to many limitation this system get failed in attaining popularity and in 1990s they develop Victorian scheme. This scheme attains some popularity but it was also not suitable for all climate of Australia.In 1993 a more flexible HERE, known by Graded five star rating system was developed. This rating scheme was much flexible and was suitable to all climatic conditions of Australia. Presently there are different HERS available in Australia which are used in different state of this country.Some of HERS used in this country are :\n\nIn Brazil, the first national program for energy efficiency in buildings (HERS), PROCEL EDIFICA was developed in 2003.The use of this rating scheme was extended to public and commercial sector in 2007 and from 2012 the operational rating is mandatory for both residential and commercial buildings. The rating system consist of a scale ranging from A to E basis, where A represent the most efficient and E represent the least efficient rating. The rating scheme consider three aspects of buildings;\nThe three groups are evaluate individually and the combined results of these indicates the level of efficiency of a building.\n\nHome energy ratings in Canada have been in existence since 1997. The two government energy rating programs in Canada are\nBoth of these programs use HOT2XP and HOT 2000 as their rating tools.\nBeside the aforesaid government rating programs in Canada, there are two standard bases are available for evaluating the building are;\nLEED is used in Canada as one of home rating scheme. This rating scheme is an adaption of the US green building council’s LEED and has been modified as per the Canadian climate, construction and regulation policies.\n\nIn China, the Ministry of Housing and Urban-Rural Development (MOHURD) develop a national building energy rating and labeling HERS in 2008. This HERS is mandatory for government buildings, big commercial complexes and those buildings applying for public retrofit funding or green label. this HERS consist a star rating scheme, ranging from 1 to 5 star. As per this HERS, more the star, more will be the energy efficiency of the building. The rating level of buildings is determined in three parameters;\n\nIn Denmark, the energy rating scheme are in existence since 1981. Denmark is the first country in Europe(EU) to begin issuing Energy Performance certificates (EPCs). The EPCs are mandatory in all types of buildings in Denmark. The rating system in Denmark includes three parts.\n\nIn France, \"Diagnostic de performance energetique\" (DPE) is used as HERS. This scheme was developed in November 2006 and in July 2007 its use becomes mandatory for all those buildings whose registration had been filed after 1 July 2007. This rating scheme consist of two types of measurements.\nBoth of the measurements comprises 7-label ratings, ranging from A (best) to G (worst) which are presented by the color coding. In ratings, the green color represents A and red color represents G label. In both cases of measurements, the buildings are evaluated in terms of necessary resource for heating, hot water production and air conditioning. The PDE of a building remains valid for 10 years.\n\nIn Ireland, the building energy ratings are in existence since 2007. In this country Building Energy Rating (BER) is used as EPC. The scheme was mandatory for new dwelling and in 2008, its use was extended to non-residential and public buildings, in 2009 the HERS cover the all types of buildings. BER is a calculation based HERS. Due to transparency in this HERS, there is a more awareness among the people and is accepted widely.\n\nIn Portugal, the EPCs scheme was launched in July 2007 and was implemented for new buildings. The use of this scheme was extended to existing buildings in January 2009. This rating scheme covers mainly the indoor air quality and energy performance of the buildings. This rating scheme is also a calculation based HERS. The compliance in the country is high and the EPC is issued only when 90% of building completion and transaction observed. There is a national database who covers all EPCs registration record and this is available for all countrymen.\n\nThe United Kingdom (UK) is one of the countries where HERS has been developed and implemented strongly from a very long time ago. In UK, Currently National Home Energy Rating (NHER) scheme is used widely. NHER scheme measure the thermal efficiency of the dwellings on a scale of 0-10 in terms of energy running cost. The dwelling rating is done through computer modeling which uses a computer program based on Building Research Establishment Domestic Energy Model (BREDEM). Basically NHER measure the energy efficiency of dwellings as a function of energy cost per square meter.The energy usage is calculated by considering the all aspects of buildings (location, design, construction, water heating, cooking, ventilation and appliances, lighting etc.) and for dwelling energy rating it use some standard assumptions, such as occupancy scenario, thermostat setting, occupant stay timings.\n\nIn the US, HERS are since 1980s. Among the various HERS energy rated homes of America is used widely. It is used in more than 18 states of US. This scheme uses a 100 points scale of efficiency and it is further divided into 10 categories of star rating which ranges from one star to five star plus. In this rating scheme a higher star rated house represents higher energy efficiency of the house. \nThe energy efficiency rating in this HERS represents the predicted energy consumption, represents the form of normalized annual energy consumption. This rating scheme consist a detailed measure of CFLs, water heater tanks, ceiling, floors and pipe insulation, efficient refrigerator and freezer, high efficient space and water heating equipments, air leakage and controls. \nThe other important rating schemes used in US are.:\n\n\n\n"}
{"id": "45326973", "url": "https://en.wikipedia.org/wiki?curid=45326973", "title": "ISO 7027", "text": "ISO 7027\n\nISO 7027:1999 is an ISO standard for water quality that enables the determination of turbidity. The ISO 7027 technique is used to determine the concentration of suspended particles in a sample of water by measuring the incident light scattered at right angles from the sample. The scattered light is captured by a photodiode, which produces an electronic signal that is converted to a turbidity.\n"}
{"id": "35110586", "url": "https://en.wikipedia.org/wiki?curid=35110586", "title": "Joule Centre", "text": "Joule Centre\n\nThe Joule Centre is a research centre based at the University of Manchester, England, that researches energy.\n\nThe Joule Centre for Energy Research was founded in October 2005 with £5 million from the Northwest Regional Development Agency and it cost £10 million.\n\nIt is based at the University of Manchester.\n\nIt conducts energy research.\n\n"}
{"id": "7980340", "url": "https://en.wikipedia.org/wiki?curid=7980340", "title": "Jumping jack (toy)", "text": "Jumping jack (toy)\n\nThe jumping jack is a toy whose origins date back thousands of years. The jointed jumping jack figure is a cross between a puppet and a paper doll. The figures are generally made from wood and their limbs are jointed and tied to a pull string. When the string is pulled and released, the arms and legs move up and down.\n\nAlthough the jumping jack is popularly thought of as a European toy, ivory dancer figures made to spin by pulling their strings, which were found at the archaeological site El Lisht and date back to ancient Egyptian times, are considered to be among the earliest forms of this family of mechanical toys.\n\nIn the mid-1700s, jumping jack figures known as “\"pantins\"” were popular among the French nobility.\n\nIn 1832 the \"Hampelmann\" was created by Carl Malss as a figure for the burlesque at Frankfurt am Main. Later the jumping jack toy became known as \"Hampelmann\" in German-speaking countries. They were manufactured in the Erzgebirge mountain range in Germany.\n\nIn 1926, in her first year as a student at the famous Bauhaus design school in Dessau, Germany, the textile designer Margaretha Reichardt undertook a preliminary course run by Josef Albers and László Moholy-Nagy. As part of the course she designed a modern version of the \"Hampelmann\", which was later produced commercially by Naef, a Swiss toy company. Her version is set in a wooden frame, but like traditional \"Hampelmänner\" he has articulated limbs that move when a string is pulled.\n\nOxford Reference cites the word \"quockerwodger\" as \"a wooden puppet which can be made to 'dance' by pulling its strings\".\n\n"}
{"id": "24958573", "url": "https://en.wikipedia.org/wiki?curid=24958573", "title": "Langley Park Wind Farm", "text": "Langley Park Wind Farm\n\nLangley Park Wind Farm is a wind farm near Langley Park, County Durham, England. It was developed by EDF Energy and is operated by Cumbria Wind Farms, the farm has a nameplate capacity of 8.2 MW, containing four REpower Systems' MM82 turbines each rated at 2.05MW.\n\nIn 2008 EDF accepted that it had failed to check the impact of the turbines on digital television reception, after residents of villages where the turbines lay on a line-of-site to the local TV transmitter complained of a loss of signal strength and severe Freeview Digital Services disruption, particularly when the turbine blades rotate.\n"}
{"id": "16933794", "url": "https://en.wikipedia.org/wiki?curid=16933794", "title": "Lindsay Hunt", "text": "Lindsay Hunt\n\nLindsay Hunt is a South African hunter turned conservationist who played an important role in a project to produce Cape Buffalo breeding stock free of bovine tuberculosis and foot-and-mouth disease. This project has resulted in the establishing of disease-free herds in all nine provinces of South Africa, away from the TB-ravaged areas of the Kruger National Park.\n\nDiscovered in 1990, buffalo bovine tuberculosis is an airborne bacterial disease. Infected buffalo may carry the disease for long periods, becoming emaciated and eventually succumbing to predation. Tuberculosis has had a devastating effect on wild buffalo herds, crossing the species barrier and widely contaminating predators, scavengers and herbivores, such as lion, leopard, cheetah, baboon, kudu, eland, bongo, oryx, sable antelope and waterbuck. Bovine tuberculosis, first reported in South Africa in 1880 in domestic cattle and in 1928 in wildlife in the Eastern Cape, probably arrived with European settlers and their livestock.\n\nThe South African National Parks Board felt the only practical solution to the epidemic, was to breed disease-free buffalo. Lindsay Hunt sourced his first buffalo breeding stock from the gene pool of the Kruger Park and developed systems that have been acclaimed in wildlife management circles.\n\n\nsee: Mutual of Omaha's Wild Kingdom special Buffalo Warrior.\n"}
{"id": "41622827", "url": "https://en.wikipedia.org/wiki?curid=41622827", "title": "List of European tornadoes in 2014", "text": "List of European tornadoes in 2014\n\nThis is a list of all tornadoes that were confirmed throughout Europe by the European Severe Storms Laboratory and local meteorological agencies during 2014. Unlike the United States, the original Fujita Scale and the TORRO scale are used to rank tornadoes across the continent.\n"}
{"id": "60871", "url": "https://en.wikipedia.org/wiki?curid=60871", "title": "Luminescence", "text": "Luminescence\n\nLuminescence is spontaneous emission of light by a substance not resulting from heat; it is thus a form of cold-body radiation. It can be caused by chemical reactions, electrical energy, subatomic motions or stress on a crystal. This distinguishes luminescence from incandescence, which is light emitted by a substance as a result of heating. Historically, radioactivity was thought of as a form of \"radio-luminescence\", although it is today considered to be separate since it involves more than electromagnetic radiation.\n\nThe dials, hands, scales, and signs of aviation and navigational instruments and markings are often coated with luminescent materials in a process known as \"luminising\".\n\nThe following are types of luminescence:\n\n\nLuminescence occurs in some minerals when they are exposed to low-powered sources of ultraviolet or infrared electromagnetic radiation (for example, portable UV lamps), at atmospheric pressure and atmospheric temperatures. This property of these minerals can be used during the process of mineral identification at rock outcrops in the field, or in the laboratory.\n\n"}
{"id": "34547637", "url": "https://en.wikipedia.org/wiki?curid=34547637", "title": "Lutsk compact overhead powerline", "text": "Lutsk compact overhead powerline\n\nLutsk compact overhead line in Volyn Oblast, Ukraine is a technically unique 10 kV power transmission line that connects a thermal power plant in Lutsk () to the Lutsk-Pivnichna substation ().\n\nThe 978 meter long line has 2 circuits. Both circuits use, what is unique at this voltage level, bundles of 6 wires as conductors. The bundle conductors of each circuit are combined by separation insulators in the span and at the pylons to a strand, which is mounted on suspension insulators at the pylons. This construction method gives a more narrow right of way, as a conventional power line with 6 conductors. The line has no ground wire attached to the main towers themselves; these wires are attached to adjacent lightning rod towers and parallel both sides of the route.\n\n\n"}
{"id": "7096055", "url": "https://en.wikipedia.org/wiki?curid=7096055", "title": "MANWEB", "text": "MANWEB\n\nMANWEB (Merseyside And North Wales Electricity Board) was the regional electricity supplier and distributor for Merseyside, North Wales and parts of Cheshire. It is now part of SP Energy Networks, itself a subsidiary of the Spanish energy company Iberdrola. In September 1994, MANWEB announced plans to slash five hundred jobs in the workforce.\n\nIt was originally created in 1947 as the nationalised Merseyside and North Wales Electricity Board in the Electricity Act 1947, but was privatised in 1990 to become Manweb plc.\n\nIt was purchased by Scottish Power in January 1996, and subsequently become SP Manweb plc. The Manweb name continued to be used alongside the Scottish Power logo on home and retail publications until the middle of 2007, when it was replaced by \"ScottishPower\". \n\nHowever, the Merseyside, Cheshire and North Wales electricity distributor continues as SP Manweb plc, which is managed along with the Scottish transmission operators SP Distribution plc and SP Transmission plc as SP Energy Networks. A fourth company, SP Power Systems Ltd maintains the distribution networks for each of these companies.\n\n"}
{"id": "39591991", "url": "https://en.wikipedia.org/wiki?curid=39591991", "title": "Maximum Power Point Tracking Using novel Bisection search Algorithm", "text": "Maximum Power Point Tracking Using novel Bisection search Algorithm\n\nIn the last decade, the research and development of photovoltaic (PV) system technology has been accelerated due to its free of pollution, silent operation, long life time and low maintenance cost and the improvement of solar cell technologies and the increasing demand for PV systems have led to a reduction of the price of PV module, the costs of PV systems are still too high. Therefore, it is an important to design the PV systems which can maximize energy production from Sun through solar modules. The output power of solar module is a function of solar radiance, temperature and operating point because of its nonlinear current-voltage (I-V) relationship. Therefore, the maximum power point tracker (MPPT) is widely used to maximize the power output of the solar module. As such, many MPPT techniques have been developed and implemented. The techniques used generally are Hill Climbing, Perturb and Observe (P&O), Incremental Conductance (INC), Factional Open-Circuit Voltage and Fractional Short-Circuit Current. The past results show that P&O and Incremental Conductance are in general the most efficient techniques. For these two techniques, the derivatives of voltage and power measured from the solar module are still required.\n\nBisection Search Method: Now a novel MPPT technique based on bisection search theorem (BST) is proposed without the necessity of derivative computation. Thus, the new technique is even simpler in computation, cheaper in implementation and faster in tracking. The experimental results from solar array simulator in the laboratory show that the proposed technique can track maximum power point very fast within a few steps. The Feasibility of the proposed technique is also verified under real solar modules at the presence of natural environmental conditions. Thus, it is expected to be widely used to replace conventional MPPT techniques in PV systems. This technique is very efficient for maximum power point tracking of the solar PV array. The schematic diagram is as given below in the figure.\n\nBisection Search Theorem: The bisection search theorem is one of the bracketing methods for finding roots of equations. Assume that function y= f(x) and an interval [a, b] which contains a root x* of f(x) that lies somewhere in the interval such that f(c)= 0. The BST systematically moves the end points of the interval closer and closer together in the pace of halving interval for each step until an interval of arbitrarily small width that brackets the zero is obtained. The decision step for this process is first to choose the midpoint c =(a+b) / 2 and then to analyze the three possibilities that might rise. If f (a) and f (b) have opposite signs, a zero lies in [a, c]. If f (a) and f (b) have opposite signs, a zero lies in [c, b]. If f (c)=0, then the zero is c. If either case (1) or (2) occurs, an interval half as wide as the original interval that contains the root.\n\nMaximum Power Point Tracking Using Bisection Search Method: In order to apply the BST into the MPPT technique in PV systems, the function of y= f(x) and the variable x should be chosen carefully. Fig. 2 shows a power-voltage (P -V) curve. From the (P -V) curve, it can be observed that the change in power with respect to voltage approaches zero at the maximum power point. Obviously, the powers at short circuit voltage (0 V) and open circuit voltage (Voc ) are zero, so maximum power should not happen in these two particular points even though the changes in power at these two points are also zero, which is caused by the small powers around these two points. Thus, tracking the maximum power point is essential to find the root in the function dP by regulating the voltage of solar module or solar array. As a result, the function y= f(x) can be regarded as the change in power dP, where the variable x is the voltage of solar module or solar array.\n\n[1] Bialasiewicz, J.T., “Renewable Energy Systems with Photovoltaic Power Generators: Operation and Modeling,” IEEE Trans. on Industrial Electronics, vol. 55, no. 7, pp. 2752–2758, July 2008.\n[2] D. Poponi, “Analysis of diffusion paths for photovoltaic technology based on experience curves,” Solar Energy, vol. 1, no. 74, pp. 331–349, 2003.\n[3] Tomas Markvart, Solar electricity, John Wiley & Sons Inc. 2000.\n"}
{"id": "39117935", "url": "https://en.wikipedia.org/wiki?curid=39117935", "title": "Mechanical properties of biomaterials", "text": "Mechanical properties of biomaterials\n\nMaterials that are used for biomedical or clinical applications are known as biomaterials. The following article deals with fifth generation biomaterials that are used for bone structure replacement. For any material to be classified for biomedical application three requirements must be met. The first requirement is that the material must be biocompatible; it means that the organism should not treat it as a foreign object. Secondly, the material should be biodegradable (for in-graft only); the material should harmlessly degrade or dissolve in the body of the organism to allow it to resume natural functioning. Thirdly, the material should be mechanically sound; for the replacement of load bearing structures, the material should possess equivalent or greater mechanical stability to ensure high reliability of the graft.\n\nThe biomaterial term is used for materials that can be used in biomedical and clinical applications. They are bioactive and biocompatible in nature. Currently, many types of metals and alloys (stainless steel, titanium, nickel, magnesium, Co–Cr alloys, Ti alloys), ceramics (zirconia, bioglass, alumina, hydroxyapatite) and polymers (acrylic, nylon, silicone, polyurethane, polycaprolactone, polyanhydrides) are used for load bearing application. This includes dental replacement and bone joining or replacement for medical and clinical application. Therefore their mechanical properties are very important. Mechanical properties of some biomaterials and bone are summarized in table 1. Among them hydroxyapatite is most widely studied bioactive and biocompatible material. However, it has lower young’s modulus and fracture toughness with brittle nature. Hence, it is required to produce a biomaterial with good mechanical properties.\n\nElastic modulus is simply defined as the ratio of stress to strain within the proportional limit. Physically, it represents the stiffness of a material within the elastic range when tensile or compressive load are applied. It is clinically important because it indicates the selected biomaterial has similar deformable properties with the material it is going to replace. These force-bearing materials require high elastic modulus with low deflection. As the elastic modulus of material increases fracture resistance decreases. It is desirable that the biomaterial elastic modulus is similar to bone. This is because if it is more than bone elastic modulus then load is born by material only; while the load is bear by bone only if it is less than bone material. The Elastic modulus of a material is generally calculated by bending test because deflection can be easily measured in this case as compared to very small elongation in compressive or tensile load. However, biomaterials (for bone replacement) are usually porous and the sizes of the samples are small. Therefore, nanoindentation test is used to determine the elastic modulus of these materials. This method has high precision and convenient for micro scale samples. Another method of elastic modulus measurement is non-destructive method. It is also clinically very good method because of its simplicity and repeatability since materials are not destroyed.\n\nHardness is one of the most important parameters for comparing properties of materials. It is used for finding the suitability of the clinical use of biomaterials. Biomaterial hardness is desirable as equal to bone hardness. If higher than the biomaterial, then it penetrates in the bone. As above said, biomaterials sample are very small therefore, micro and nano scale hardness test (Diamond Knoop and Vickers indenters) are used.\n\nStrength of materials is defined as the maximum stress that can be endured before fracture occurs. Strength of biomaterials (bioceramics) is an important mechanical property because they are brittle. In brittle materials like bioceramics, cracks easily propagate when the material is subject to tensile loading, unlike compressive loading. A number of methods are available for determining the tensile strength of materials, such as the bending flexural test, the biaxial flexural strength test and the weibull approach. In bioceramics, flaws influence the reliability and strength of the material during implantation and fabrication. There are a number of ways that flaws can be produced in bioceramics such as thermal sintering and heating. The importance is for bioceramics to have high reliability, rather than high strength.\n\nFracture toughness is required to alter the crack propagation in ceramics. It is helpful to evaluate the serviceability, performance and long term clinical success of biomaterials. It is reported that the high fracture toughness material improved clinical performance and reliability as compare to low fracture toughness. It can be measured by many methods e.g. indentation fracture, indentation strength, single edge notched beam, single edge pre cracked beam and double cantilever beam.\n\nFatigue is defined as failure of a material due to repeated/cyclic loading or unloading (tensile or compressive stresses). It is also an important parameter for biomaterial because cyclic load is applied during their serving life. In this cyclic loading condition, micro crack/flaws may be generated at the interface of the matrix and the filler. This micro crack can initiate permanent plastic deformation which results in large crack propagation or failure. During the cyclic load several factor also contribute to microcrack generation such as frictional sliding of the mating surface, progressive wear, residual stresses at grain boundaries, stress due to shear.\n\nTable 1: Summary of mechanical properties of cortical bone and biomaterial\n\n"}
{"id": "30083292", "url": "https://en.wikipedia.org/wiki?curid=30083292", "title": "Mingyang Wind Power", "text": "Mingyang Wind Power\n\nMing Yang Wind Power Group Limited (\"Ming Yang\", ) is the largest private wind turbine manufacturer in China and the fifth largest overall in the country. Since 1 October 2010, the company was listed on the New York Stock Exchange until 2016, June 22.\n\nThe company is focusing on designing, manufacturing, selling and servicing megawatt-class wind turbines. Ming Yang cooperates with aerodyn Energiesysteme, a leading wind turbine design firm based in Germany. Ming Yang's key customers include the five largest state-owned power producers in China, with an aggregate installed capacity accounting for more than 5.5% of China's newly installed capacity in 2010.\n\nThe company started wind turbine production in 2007, with a prototype of 1.5MW designed by aerodyn Energiesysteme.\n\nIn 2010 Ming Yang started SCD production (2.75MW and 3.0MW).\nThe SCD (Super Compact Drive) is an innovative two blade turbine by aerodyn (Husum WindEnergy AWARD 2009).\nIn 2013, the new offshore wind turbine SCD 6.5 was presented. A two bladed downwind offshore turbine with helicopter deck. The first was connected to the grid in 2015.\nLarger models are expected, and 12 MW is under development.\n\nIn 2011, Ming Yang ranks among top 4 of wind turbine supplier in China and Top 10 of wind turbine worldwide.\n\nMingyang won a bid for 87 MW (29 * 3 MW) two-bladed offshore wind turbines near Zhuhai in 2013.\n\n"}
{"id": "362728", "url": "https://en.wikipedia.org/wiki?curid=362728", "title": "Negative temperature", "text": "Negative temperature\n\nIn quantum thermodynamics, certain systems can achieve negative temperature; that is, their temperature can be expressed as a negative quantity on the Kelvin or Rankine scales.\n\nA system with a truly negative temperature on the Kelvin scale is \"hotter\" than any system with a positive temperature. If a negative-temperature system and a positive-temperature system come in contact, heat will flow from the negative- to the positive-temperature system. A standard example of such a system is population inversion in laser physics.\n\nTemperature is loosely interpreted as the average kinetic energy of the system's particles. The existence of negative temperature, let alone negative temperature representing \"hotter\" systems than positive temperature, would seem paradoxical in this interpretation. \nThe paradox is resolved by considering the more rigorous definition of thermodynamic temperature as the tradeoff between internal energy and entropy\ncontained in the system, with \"coldness\", the \"reciprocal\" of temperature, being the more fundamental quantity. \nSystems with a positive temperature will increase in entropy as one adds energy to the system, while \nsystems with a negative temperature will decrease in entropy as one adds energy to the system.\n\nClassical thermodynamic systems cannot achieve negative temperatures: adding heat always increases their entropy. \nThe possibility of a decrease in entropy as energy increases requires the system to \"saturate\" in entropy. This is only possible if the number of high energy states is limited. In classical Boltzmann statistics, the number of high energy states is unlimited (particle speeds can in principle be increased indefinitely).\nSystems bounded by a maximum amount of energy are generally forbidden in classical mechanics, and the phenomenon of negative temperature is strictly a \nquantum mechanical phenomenon. Some systems, however (see the examples below), have a maximum amount of energy that they can hold, and as they approach that maximum energy their entropy actually begins to decrease.\n\nThe definition of thermodynamic temperature as a function of the change in the system's entropy under reversible heat transfer \nformula_1:\nEntropy being a state function, the integral of over any cyclical process is zero.\nFor a system in which the entropy is purely a function of the system's energy , the temperature can be defined as:\n\nEquivalently, thermodynamic beta, or \"coldness\", is defined as \nwhere is the Boltzmann constant.\n\nNote that in classical thermodynamics, is defined in terms of temperature. This is reversed here, is the statistical entropy, a function of the possibly microstates of the system, and temperature conveys information on the distribution of energy levels among the possible microstates.\nFor systems with many degrees of freedom, the statistical and thermodynamic definitions of entropy are generally consistent with each other. \nHowever, for small systems and systems where the number of states decreases with energy, the definitions of statistical entropy and thermodynamic entropy are not necessarily consistent, and the temperatures derived from these entropies are different. \n\nSome theorists have proposed using an alternate definition of entropy originally proposed by Gibbs as a way to resolve these inconsistencies, although this new definition would create other inconsistencies.\n\nNegative temperatures can only exist in a system where there are a limited number of energy states (see below). As the temperature is increased on such a system, particles move into higher and higher energy states, and as the temperature increases, the number of particles in the lower energy states and in the higher energy states approaches equality. (This is a consequence of the definition of temperature in statistical mechanics for systems with limited states.) By injecting energy into these systems in the right fashion, it is possible to create a system in which there are more particles in the higher energy states than in the lower ones. The system can then be characterised as having a negative temperature.\n\nA substance with a negative temperature is not colder than absolute zero, but rather it is hotter than infinite temperature. As Kittel and Kroemer (p. 462) put it, \"The temperature scale from cold to hot runs:\n\nThe corresponding inverse temperature scale, for the quantity \"β\" = 1/\"kT\" (where \"k\" is Boltzmann's constant), runs continuously from low energy to high as +∞, … , 0, …, −∞. Because it avoids the abrupt jump from +∞ to −∞, β is considered more natural than \"T\". Although a system can have multiple negative temperature regions and thus have -∞ to +∞ discontinuities.\n\nIn many familiar physical systems, temperature is associated to the kinetic energy of atoms. Since there is no upper bound on the momentum of an atom, there is no upper bound to the number of energy states available when more energy is added, and therefore no way to get to a negative temperature. However, in statistical mechanics, temperature can correspond to other degrees of freedom than just kinetic energy (see below).\n\nThe distribution of energy among the various translational, vibrational, rotational, electronic, and nuclear modes of a system determines the macroscopic temperature. In a \"normal\" system, thermal energy is constantly being exchanged between the various modes.\n\nHowever, in some situations, it is possible to isolate one or more of the modes. In practice, the isolated modes still exchange energy with the other modes, but the time scale of this exchange is much slower than for the exchanges within the isolated mode. One example is the case of nuclear spins in a strong external magnetic field. In this case, energy flows fairly rapidly among the spin states of interacting atoms, but energy transfer between the nuclear spins and other modes is relatively slow. Since the energy flow is predominantly within the spin system, it makes sense to think of a spin temperature that is distinct from the temperature associated to other modes.\n\nA definition of temperature can be based on the relationship:\n\nThe relationship suggests that a \"positive temperature\" corresponds to the condition where entropy, \"S\", increases as thermal energy, \"q\", is added to the system. This is the \"normal\" condition in the macroscopic world, and is always the case for the translational, vibrational, rotational, and non-spin related electronic and nuclear modes. The reason for this is that there are an infinite number of these types of modes, and adding more heat to the system increases the number of modes that are energetically accessible, and thus increases the entropy.\n\nThe simplest example, albeit a rather nonphysical one, is to consider a system of \"N\" particles, each of which can take an energy of either \"+ε\" or \"-ε\" but are otherwise noninteracting. This can be understood as a limit of the Ising model in which the interaction term becomes negligible. The total energy of the system is\n\nwhere \"σ\" is the sign of the \"i\"th particle and \"j\" is the number of particles with positive energy minus the number of particles with negative energy. From elementary combinatorics, the total number of microstates with this amount of energy is a binomial coefficient:\n\nBy the fundamental assumption of statistical mechanics, the entropy of this microcanonical ensemble is\n\nWe can solve for thermodynamic beta (β = 1/kT) by considering it as a central difference without taking the continuum limit:\n\nhence the temperature\n\nThis entire proof assumes the microcanonical ensemble with energy fixed and temperature being the emergent property. In the canonical ensemble, the temperature is fixed and energy is the emergent property. This leads to (formula_14 refers to microstates):\nFollowing the previous example, we choose a state with two levels and two particles. This leads to microstates formula_18, formula_19, formula_20, and formula_21.\nThe resulting values for formula_28, formula_29, and formula_30 are all increasing with formula_31 and never need to enter a negative temperature regime.\n\nThe previous example is approximately realized by a system of nuclear spins in an external magnetic field. This allows the experiment to be run as a variation of nuclear magnetic resonance spectroscopy. In the case of electronic and nuclear spin systems, there are only a finite number of modes available, often just two, corresponding to spin up and spin down. In the absence of a magnetic field, these spin states are \"degenerate\", meaning that they correspond to the same energy. When an external magnetic field is applied, the energy levels are split, since those spin states that are aligned with the magnetic field will have a different energy from those that are anti-parallel to it.\n\nIn the absence of a magnetic field, such a two-spin system would have maximum entropy when half the atoms are in the spin-up state and half are in the spin-down state, and so one would expect to find the system with close to an equal distribution of spins. Upon application of a magnetic field, some of the atoms will tend to align so as to minimize the energy of the system, thus slightly more atoms should be in the lower-energy state (for the purposes of this example we will assume the spin-down state is the lower-energy state). It is possible to add energy to the spin system using radio frequency (RF) techniques. This causes atoms to \"flip\" from spin-down to spin-up.\n\nSince we started with over half the atoms in the spin-down state, this initially drives the system towards a 50/50 mixture, so the entropy is increasing, corresponding to a positive temperature. However, at some point, more than half of the spins are in the spin-up position. In this case, adding additional energy reduces the entropy, since it moves the system further from a 50/50 mixture. This reduction in entropy with the addition of energy corresponds to a negative temperature. In NMR spectroscopy, this corresponds to pulses with a pulse width of over 180° (for a given spin). While relaxation is fast in solids, it can take several seconds in solutions and even longer in gases and in ultracold systems; several hours were reported for silver and rhodium at picokelvin temperatures. It is still important to understand that the temperature is negative only with respect to nuclear spins. Other degrees of freedom, such as molecular vibrational, electronic and electron spin levels are at a positive temperature, so the object still has positive sensible heat. Relaxation actually happens by exchange of energy between the nuclear spin states and other states (e.g. through the nuclear Overhauser effect with other spins).\n\nThis phenomenon can also be observed in many lasing systems, wherein a large fraction of the system's atoms (for chemical and gas lasers) or electrons (in semiconductor lasers) are in excited states. This is referred to as a population inversion.\n\nThe Hamiltonian for a single mode of a luminescent radiation field at frequency \"ν\" is\nThe density operator in the grand canonical ensemble is\nFor the system to have a ground state, the trace to converge, and the density operator to be generally meaningful, \"βH\" must be positive semidefinite. So if \"hν\" < \"μ\", and \"H\" is negative semidefinite, then \"β\" must itself be negative, implying a negative temperature.\n\nThe two-dimensional systems can exist in negative temperature states.\n\nNegative temperatures have also been achieved in motional degrees of freedom. Using an optical lattice, upper bounds were placed on the kinetic energy, interaction energy and potential energy of cold atoms. This was done by tuning the interactions of the atoms from repulsive to attractive using a Feshbach resonance and changing the overall harmonic potential from trapping to anti-trapping, thus transforming the Bose-Hubbard Hamiltonian from formula_34. Performing this transformation adiabatically while keeping the atoms in the Mott insulator regime, it is possible to go from a low entropy positive temperature state to a low entropy negative temperature state. In the negative temperature state, the atoms macroscopically occupy the maximum momentum state of the lattice. The negative temperature ensembles equilibrated and showed long lifetimes in an anti-trapping harmonic potential.\n\n\n"}
{"id": "5825145", "url": "https://en.wikipedia.org/wiki?curid=5825145", "title": "Novotext", "text": "Novotext\n\nNovotext is a trade name for cotton textile-phenolic resin, essentially cotton-reinforced Bakelite. It was often used in car engines for gear wheels used to provide a direct drive to the camshaft as it is flexible and quiet-running. One of the first luxury cars to use this material for its camshaft drive gears was the Maybach Zeppelin of 1928. The material is known under various other names such as Turbax, Resitex, Celoron and Textolit. In bar form it is also known as Cartatextiel and Ferrozell and in sheet form as Harex, Tufnol and Micarta.\n\nTufnol is a composite material comprising phenolic resin and another material (paper, cotton fabric etc.). The two materials together complement each other's qualities. It is inherently water resistant and some grades are used as a lining in loaded bearings (e.g. stave bearings) where lubricating oil use is not feasible. Rather, it can be lubricated with water. It has very low friction characteristics; thus it finds its use in dusty, chemically sensitive environments. The ability to work without oil makes it a preferable choice for design engineers. Unfortunately, the preparation is not as environmentally friendly.\n\nProduction is achieved through the use of chopped strand mat (CSM) technique.\n\n"}
{"id": "9779742", "url": "https://en.wikipedia.org/wiki?curid=9779742", "title": "OE buoy", "text": "OE buoy\n\nAn OE Buoy or Ocean Energy Buoy is a wave power device that uses an Oscillating Water Column design. It was deployed in half-scale test mode in Spiddal near Galway in Ireland for over two years between 2007 and 2009. As of the 5th of March 2011 the model has been redeployed at the same site, primarily as a data collector for the EU funded Cores Project.\n\nIt was developed by the HMRC in Cork and is now owned and developed by the spun off Oceanenergy.\n\nThe OE Buoy is a version of a device known as the Backward Bent Duct Buoy (BBDB) which was invented in 1986 by wave energy pioneer and Japanese naval commander Yoshio Masuda.\n\n"}
{"id": "59249", "url": "https://en.wikipedia.org/wiki?curid=59249", "title": "Papaya", "text": "Papaya\n\nThe papaya (, ) (from Carib via Spanish), papaw, () or pawpaw () is the plant Carica papaya, one of the 22 accepted species in the genus \"Carica\" of the family Caricaceae. Its origin is in the tropics of the Americas, perhaps from southern Mexico and neighboring Central America.\n\nThe papaya is a small, sparsely branched tree, usually with a single stem growing from tall, with spirally arranged leaves confined to the top of the trunk. The lower trunk is conspicuously scarred where leaves and fruit were borne. The leaves are large, in diameter, deeply palmately lobed, with seven lobes. All parts of the plant contain latex in articulated laticifers. Papayas are dioecious. The flowers are 5-parted and highly dimorphic, the male flowers with the stamens fused to the petals. The female flowers have a superior ovary and five contorted petals loosely connected at the base. Male and female flowers are borne in the leaf axils, the males in multiflowered dichasia, the female flowers is few-flowered dichasia. The flowers are sweet-scented, open at night and are moth-pollinated. The fruit is a large berry about long and in diameter. It is ripe when it feels soft (as soft as a ripe avocado or a bit softer) and its skin has attained an amber to orange hue.\n\nNative to Mexico and northern South America, papaya has become naturalized throughout the Caribbean Islands, Florida, Texas, California, Hawaii, and other tropical and subtropical regions of the world.\n\nPapaya plants grow in three sexes: male, female, and hermaphrodite. The male produces only pollen, never fruit. The female produces small, inedible fruits unless pollinated. The hermaphrodite can self-pollinate since its flowers contain both male stamens and female ovaries. Almost all commercial papaya orchards contain only hermaphrodites.\n\nOriginally from southern Mexico (particularly Chiapas and Veracruz), Central America, and northern South America, the papaya is now cultivated in most tropical countries. In cultivation, it grows rapidly, fruiting within 3 years. It is, however, highly frost-sensitive, limiting its production to tropical climates. Temperatures below are greatly harmful if not fatal. In Florida, California, and Texas, growth is generally limited to southern parts of the states. It prefers sandy, well-drained soil, as standing water will kill the plant within 24 hours.\n\nFor cultivation, however, only female plants are used, since they give off a single flower each time, and close to the base of the plant, while the male gives off multiple flowers in long stems, which result in poorer quality fruit.\n\nTwo kinds of papayas are commonly grown. One has sweet, red or orange flesh, and the other has yellow flesh; in Australia, these are called \"red papaya\" and \"yellow papaw\", respectively. Either kind, picked green, is called a \"green papaya\".\n\nThe large-fruited, red-fleshed 'Maradol', 'Sunrise', and 'Caribbean Red' papayas often sold in U.S. markets are commonly grown in Mexico and Belize.\n\nIn 2011, Philippine researchers reported that by hybridizing papaya with \"Vasconcellea quercifolia\", they had developed conventionally bred, nongenetically engineered papaya resistant to PRV.\n\n\"Carica papaya\" was the first transgenic fruit tree to have its genome sequenced. In response to the papaya ringspot virus (PRV) outbreak in Hawaii, in 1998, genetically altered papaya were approved and brought to market (including 'SunUp' and 'Rainbow' varieties.) Varieties resistant to PRV have some DNA of this virus incorporated into the DNA of the plant. As of 2010, 80% of Hawaiian papaya plants were genetically modified. The modifications were made by University of Hawaii scientists who made the modified seeds available to farmers without charge.\n\nPapaya ringspot virus is a well-known virus within plants in Florida. The first signs of the virus are yellowing and vein-clearing of younger leaves, as well as mottling yellow leaves. Infected leaves may obtain blisters, roughen or narrow, with blades sticking upwards from the middle of the leaves. The petioles and stems may develop dark green greasy streaks and in time become shorter. The ringspots are circular, C-shaped markings that are darker green than the fruit itself. In the later stages of the virus, the markings may become gray and crusty. Viral infections impact growth and reduce the fruit's quality. One of the biggest effects that viral infections have on papaya is the taste. As of 2010, the only way to protect papaya from this virus is genetic modification.\n\nThe papaya mosaic virus destroys the plant until only a small tuft of leaves are left. The virus affects both the leaves of the plant and the fruit. Leaves show thin, irregular, dark-green lines around the borders and clear areas around the veins. The more severely affected leaves are irregular and linear in shape. The virus can infect the fruit at any stage of its maturity. Fruits as young as 2 weeks old have been spotted with dark-green ringspots about 1 inch in diameter. Rings on the fruit are most likely seen on either the stem end or the blossom end. In the early stages of the ringspots, the rings tend to be many closed circles, but as the disease develops, the rings will increase in diameter consisting of one large ring. The difference between the ringspot and the mosaic viruses is the ripe fruit in the ringspot has mottling of colors and mosaic does not.\n\nThe fungus anthracnose is known to specifically attack papaya, especially the mature fruits. The disease starts out small with very few signs, such as water-soaked spots on ripening fruits. The spots become sunken, turn brown or black, and may get bigger. In some of the older spots, the fungus may produce pink spores. The fruit ends up being soft and having an off flavor because the fungus grows into the fruit.\n\nThe fungus powdery mildew occurs as a superficial white presence on the surface of the leaf in which it is easily recognized. Tiny, light yellow spots begin on the lower surfaces of the leaf as the disease starts to make its way. The spots enlarge and white powdery growth appears on the leaves. The infection usually appears at the upper leaf surface as white fungal growth. Powdery mildew is not as severe as other diseases.\n\nThe fungus phytophthora blight causes damping-off, root rot, stem rot, stem girdling, and fruit rot. Damping-off happens in young plants by wilting and death. The spots on established plants start out as white, water-soaked lesions at the fruit and branch scars. These spots enlarge and eventually cause death. The most dangerous feature of the disease is the infection of the fruit which may be toxic to consumers. The roots can also be severely and rapidly infected, causing the plant to brown and wilt away, collapsing within days.\n\nThe papaya fruit fly lays its eggs inside of the fruit, possibly up to 100 or more eggs. The eggs usually hatch within 12 days when they begin to feed on seeds and interior parts of the fruit. When the larvae mature usually 16 days after being hatched, they eat their way out of the fruit, drop to the ground, and pupate in the soil to emerge within one to two weeks later as mature flies. The infected papaya will turn yellow and drop to the ground after infestation by the papaya fruit fly.\n\nThe two-spotted spider mite is a 0.5-mm-long brown or orange-red or a green, greenish yellow translucent oval pest. They all have needle-like piercing-sucking mouthparts and feed by piercing the plant tissue with their mouthparts, usually on the underside of the plant. The spider mites spin fine threads of webbing on the host plant, and when they remove the sap, the mesophyll tissue collapses and a small chlorotic spot forms at the feeding sites. The leaves of the papaya fruit turn yellow, gray, or bronze. If the spider mites are not controlled, they can cause the death of the fruit.\n\nThe papaya whitefly lays yellow, oval eggs that appear dusted on the undersides of the leaves. They eat papaya leaves, therefore damaging the fruit. There, the eggs developed into flies in three stages called instars. The first instar has well-developed legs and is the only mobile immature life stage. The crawlers insert their mouthparts in the lower surfaces of the leaf when they find it suitable and usually do not move again in this stage. The next instars are flattened, oval, and scale-like. In the final stage, the pupal whiteflies are more convex, with large, conspicuously red eyes.\n\nIn 2016, global production of papayas was 13.05 million tonnes, led by India with 44% of the world total (table). Global papaya production grew significantly over the early 21st century, mainly as a result of increased production in India and demand by the United States.\n\nRaw papaya pulp contains 88% water, 11% carbohydrates, and negligible fat and protein (table). In a 100 gram amount, papaya fruit provides 43 kilocalories and is a significant source of vitamin C (75% of the Daily Value, DV) and a moderate source of folate (10% DV), but otherwise has low content of nutrients (see table).\n\nThe ripe fruit of the papaya is usually eaten raw, without skin or seeds. The unripe green fruit can be eaten cooked, usually in curries, salads, and stews. Green papaya is used in Southeast Asian cooking, both raw and cooked. In Thai cuisine, papaya is used to make Thai salads such as \"som tam\" and Thai curries such as \"kaeng som\" when still not fully ripe. In Indonesian cuisine, the unripe green fruits and young leaves are boiled for use as part of \"lalab\" salad, while the flower buds are sautéed and stir-fried with chillies and green tomatoes as Minahasan papaya flower vegetable dish. Papayas have a relatively high amount of pectin, which can be used to make jellies. The smell of ripe, fresh papaya flesh can strike some people as unpleasant. In Brazil, the unripe fruits are often used to make sweets or preserves.\n\nThe black seeds of the papaya are edible and have a sharp, spicy taste. They are sometimes ground and used as a substitute for black pepper.\n\nIn some parts of Asia, the young leaves of the papaya are steamed and eaten like spinach.\n\nBoth green papaya fruit and the plant's latex are rich in papain, a protease used for tenderizing meat and other proteins, as practiced currently by indigenous Americans and people of the Caribbean region. It is now included as a component in some powdered meat tenderizers.\n\nPapaya skin, pulp and seeds contain a variety of phytochemicals, including carotenoids and polyphenols, as well as benzyl isothiocyanates and benzyl glucosinates, with skin and pulp levels that increase during ripening. Papaya seeds also contain the cyanogenic substance prunasin.\n\nIn some parts of the world, papaya leaves are made into tea as a treatment for malaria, but the mechanism is not understood and no treatment method based on these results has been scientifically proven.\n\nPapaya releases a latex fluid when not ripe, possibly causing irritation and an allergic reaction in some people.\n\n"}
{"id": "36976974", "url": "https://en.wikipedia.org/wiki?curid=36976974", "title": "Plasma railgun", "text": "Plasma railgun\n\nA plasma railgun is a linear accelerator which, like a projectile railgun, uses two long parallel electrodes to accelerate a \"sliding short\" armature. However, in a plasma railgun, the armature and ejected projectile consists of plasma, or hot, ionized, gas-like particles, instead of a solid slug of material. Scientific plasma railguns are typically operated in vacuum and not at air pressure. They are of value because they produce muzzle velocities of up to several hundreds of kilometers per second. Because of this, these devices have applications in magnetic confinement fusion (MCF), magneto-inertial fusion (MIF), High Energy Density Physics research (HEDP), laboratory astrophysics, and as a plasma propulsion engine for spacecraft.\n\nPlasma railguns appear in two principle topologies, linear and coaxial. Linear railguns consist of two flat plate electrodes separated by insulating spacers and accelerate sheet armatures. Coaxial railguns accelerate toroidal plasma armatures using a hollow outer conductor and a central, concentric, inner conductor.\n\nLinear plasma railguns place extreme demands on their insulators, as they must be an electrically insulating, plasma-facing vacuum component which can withstand both thermal and acoustic shocks. Additionally, a complex triple joint seal may exist at the breech of the bore, which can often pose an extreme engineering challenge. Coaxial accelerators require insulators only at the breech, but the plasma armature in that case is subject to the \"blow-by\" instability. This is an instability in which the magnetic pressure front can out-run or \"blow-by\" the plasma armature due to the radial dependence of acceleration current density, drastically reducing device efficiency. Coaxial accelerators use various techniques to mitigate this instability. In either design, a plasma armature is formed at the breech. As plasma railguns are an open area of research, the method of armature formation varies. However, techniques including exploding foils, gas cell burst disk injection, neutral gas injection via fast gas valve, and plasma capillary injection have been employed.\n\nAfter armature formation, the plasmoid is then accelerated down the length of the railgun by a current pulse driven through one electrode, through the armature, and out the other electrode, creating a large magnetic field behind the armature. Since the driver current through the armature is also moving through and normal to a self-generated magnetic field, the armature particles experience a Lorentz force, accelerating them down the length of the gun. Accelerator electrode geometry and materials are also open areas of research.\n\nPlasma rail guns are capable of producing controlled jets of given densities and velocities ranging from at least peak densities 1e13 to 1e16 particles/m^3 with velocities from 5 to 200 km/s dependent on device design configuration and operating parameters. Plasma rail guns are being evaluated for applications in magnetic confinement fusion for disruption mitigation and tokamak refueling.\n\nMagneto-inertial fusion seeks to implode a magnetized D-T fusion target using a spherically-symmetric, collapsing, conducting liner. Plasma railguns are being evaluated as a possible method of implosion linear formation for fusion.\n\nArrays of plasma railguns could be used to create pulsed implosions of ~1 Megabar peak pressure, allowing more access to chart this opening area of plasma physics\n\nHigh velocity jets of controllable density and temperature allow astrophysical phenomena such as solar wind, galactic jets, solar events and astrophysical plasma to be partially simulated in the laboratory and measured directly, in addition to astronomic and satellite observations.\n\n"}
{"id": "596381", "url": "https://en.wikipedia.org/wiki?curid=596381", "title": "Plasma weapon", "text": "Plasma weapon\n\nWhen discussing weapons in science fiction, a plasma weapon is a type of raygun that fires a stream, bolt(s), pulse or toroid of plasma (i.e. very hot, very energetic excited matter). The primary damage mechanism of these fictional weapons is usually thermal transfer; it typically causes serious burns, and often immediate death of living creatures, and melts or evaporates other materials. In certain fiction, plasma weapons may also have a significant kinetic energy component, that is to say the ionized material is projected with sufficient momentum to cause some secondary impact damage in addition to causing high thermal damage. In some fictions, like Star Wars, plasma is highly effective against mechanical targets such as droids. The ionized gas disrupts their systems.\n\nFictional plasma weapons are often visually analogous to real-life plasma torches that cut into materials for industrial use purposes; however, said torches currently only produce a plasma jet of several inches at most.\n\nPlasma weapons are often, especially in video games, depicted as very powerful, but short-ranged and/or less energy-efficient than other weapon types.\n\nCurrently, there are several real tools that are somewhat related to fictional plasma weapons:\n\n"}
{"id": "3528740", "url": "https://en.wikipedia.org/wiki?curid=3528740", "title": "Potassium aluminium borate", "text": "Potassium aluminium borate\n\nPotassium aluminium borate (KAlBO) is an ionic compound composed of potassium ions, aluminium ions, and borate ions. Its crystal form exhibits nonlinear optical properties. The ultraviolet beam at 266 nm can be obtained by fourth harmonic generation (FGH) of 1064 nm laser radiation through a nonlinear crystal KAlBO (KABO).\n\n"}
{"id": "8491096", "url": "https://en.wikipedia.org/wiki?curid=8491096", "title": "Poynting effect", "text": "Poynting effect\n\nThe Poynting effect may refer to two unrelated physical phenomena. Neither should be confused with the Poynting–Robertson effect. All of these effects are named after John Henry Poynting, an English physicist.\n\nIn solid mechanics, the Poynting effect is a large strain effect observed when an elastic cube is sheared between two plates and stress is developed in the direction normal to the sheared faces, or when a cylinder is subjected to torsion and the axial length changes. The Poynting phenomenon in torsion was noticed experimentally by J. H. Poynting.\n\nIn thermodynamics, the Poynting effect generally refers to the change in the vapor pressure of a liquid when a non-condensable gas is mixed with the vapor at saturated conditions. If one assumes that the vapor and the non-condensable gas behave as ideal gases and an ideal mixture, it can be shown that:\n\nwhere\n\nAs a common example, the ability to combine nitrous oxide and oxygen at high pressure while remaining in the gaseous form is due to the Poynting effect.\n\nEntonox is a 50:50 combination of the anesthetic gas nitrous oxide and oxygen. This combination is useful because it can provide a sufficient concentration of nitrous oxide to provide analgesia (pain relief) in sufficient oxygen so that the risk of hypoxemia is eliminated. This makes it safe to use by para-medical staff such as ambulance officers. However the ability to combine these two gases at the temperature and pressure in the cylinder while remaining in the gaseous form is unexpected based on the known properties of the two gases.\n\nThe Poynting effect involves the dissolution of gaseous O when bubbled through liquid NO, with vaporisation of the liquid to form a gaseous O/NO mixture.\n"}
{"id": "26599382", "url": "https://en.wikipedia.org/wiki?curid=26599382", "title": "Priestley space", "text": "Priestley space\n\nIn mathematics, a Priestley space is an ordered topological space with special properties. Priestley spaces are named after Hilary Priestley who introduced and investigated them. Priestley spaces play a fundamental role in the study of distributive lattices. In particular, there is a duality (\"Priestley duality\") between the category of Priestley spaces and the category of bounded distributive lattices.\n\nA \"Priestley space\" is an \"ordered topological space\" , i. e. a set equipped with a partial order and a topology , satisfying \nthe following two conditions:\n\n\n\nIt follows that for each Priestley space , the topological space is a Stone space; that is, it is a compact Hausdorff zero-dimensional space.\n\nSome further useful properties of Priestley spaces are listed below.\n\nLet be a Priestley space.\n\nA Priestley morphism from a Priestley space to another Priestley space is a map which is continuous and order-preserving.\n\nLet Pries denote the category of Priestley spaces and Priestley morphisms.\n\nPriestley spaces are closely related to spectral spaces. For a Priestley space , let denote the collection of all open up-sets of . Similarly, let denote the collection of all open down-sets of .\n\nTheorem:\nIf is a Priestley space, then both and are spectral spaces.\n\nConversely, given a spectral space , let denote the patch topology on ; that is, the topology generated by the subbasis consisting of compact open subsets of and their complements. Let also denote the specialization order of .\n\nTheorem:\nIf is a spectral space, then is a Priestley space.\n\nIn fact, this correspondence between Priestley spaces and spectral spaces is functorial and yields an isomorphism between Pries and the category Spec of spectral spaces and spectral maps.\n\nPriestley spaces are also closely related to bitopological spaces.\n\nTheorem:\nIf is a Priestley space, then is a pairwise Stone space. Conversely, if is a pairwise Stone space, then is a Priestley space, where is the join of and and is the specialization order of .\n\nThe correspondence between Priestley spaces and pairwise Stone spaces is functorial and yields an isomorphism between the category Pries of Priestley spaces and Priestley morphisms and the category PStone of pairwise Stone spaces and bi-continuous maps.\n\nThus, one has the following isomorphisms of categories:\n\nOne of the main consequences of the duality theory for distributive lattices is that each of these categories is dually equivalent to the category of bounded distributive lattices.\n\n\n"}
{"id": "18904541", "url": "https://en.wikipedia.org/wiki?curid=18904541", "title": "Red Leaf Resources", "text": "Red Leaf Resources\n\nRed Leaf Resources, Inc, is an oil-shale company based in Salt Lake City, Utah, United States. It is a developer of the shale oil extraction technology EcoShale In-Capsule Process. The company is affiliated with Questerre Energy.\n\nIn the Red Leaf Resources EcoShale In-Capsule Process a hot gas is generated by burning natural gas or pyrolysis gas. Generated hot gas is then circulated through oil shale rubble using sets of parallel pipes. The heat is transferred to the shale through the pipe walls rather than being injected directly into the rubble, thereby avoiding dilution of the product hydrocarbons with the heating gas. The oil shale rubble is enclosed by a low-cost earthen impoundment structure to prevent environmental contamination and to provide easier and more rapid reclamation after the extraction process is finished. Heat from the spent shale is recovered for enhancing the process's energy efficiency by passing cool gas through pipes and then using it for preheating adjacent capsules.\n\nRed Leaf Resources controls oil shale leases covering about on State of Utah School and Institutional Trust Lands in Seep Ridge, Utah. The acreage represents about of shale oil. Its 2009 pilot project produced more than of oil. In 2012, Red Leaf Resources formed a joint venture with Total S.A. to launch commercial scale production of , utilizing the EcoShale In-Capsule process on oil shale leaseholds in Utah's Uintah Basin. In December 2013, the ground water permit was issued for the oil shale mine and shale oil plant. Red Leaf plans to be producing of oil by the end of 2015.\n\nIn 2011, Red Leaf Resources signed a licence agreement with TomCo Energy allowing the latter to use the EcoShale technology for developing the Holliday block in Utah. TomCo filed an application for oil shale activities in January 2014.\n\nThe director, president and chief executive officer of the company is Adolph Lechtenberger. Todd Dana was the founder and former Chairman of Red Leaf Resources, Inc. Mr. Dana is the primary named Inventor of the Ecoshale Technology primary patents.\n\n"}
{"id": "1407099", "url": "https://en.wikipedia.org/wiki?curid=1407099", "title": "Reverse leakage current", "text": "Reverse leakage current\n\nReverse leakage current in a semiconductor device is the current from that semiconductor device when the device is reverse biased. \n\nWhen a semiconductor device is reverse biased it should not conduct any current, however, due to an increased barrier potential, the free electrons on the p side are dragged to the battery's positive terminal, while holes on the n side are dragged to the battery's negative terminal.\nThis produces a current of minority charge carriers and hence its magnitude is extremely small.\nFor constant temperatures, the reverse current is almost constant although the applied reverse voltage is increased up to a certain limit. Hence, it is also called reverse saturation current.\n\nThe term is particularly applicable to mostly semiconductor junctions, especially diodes and thyristors.\n\nReverse leakage current is also known as \"zero gate voltage-drain current\" with MOSFETs. The leakage current increased with temperature. As an example, the Fairchild Semiconductor FDV303N has a reverse leakage of up to 1 microamp at room temperature rising to 10 microamps with a junction temperature of 50 degrees Celsius.\nFor all basic purposes, leakage currents are very small, and, thus, are normally negligible.\n"}
{"id": "7171087", "url": "https://en.wikipedia.org/wiki?curid=7171087", "title": "San Antonio Reservoir", "text": "San Antonio Reservoir\n\nSan Antonio Reservoir is located in Alameda County, California, about three miles east-southeast of Sunol. It was built in 1964 by the City and County of San Francisco. Formed by the James H. Turner Dam across San Antonio Creek not far above where it flows into Alameda Creek, its purpose is to store water from the Hetch Hetchy Aqueduct and local wells and watersheds. It has a capacity of .\n\nThe reservoir is not open to the public.\n\n"}
{"id": "36068234", "url": "https://en.wikipedia.org/wiki?curid=36068234", "title": "Sebastian Möller", "text": "Sebastian Möller\n\nSebastian Möller (born 1968) is an expert for voice technology.\n\nSebastian Möller studied electrical engineering at the universities in Bochum (Germany), Orléans (France) and Bologna (Italy). From 1994 to 2005, he was a scientific researcher and later lecturer at the \"Institute of Communication Acoustics\" at Ruhr Universität Bochum specializing in voice transmission, voice technology and communication acoustics, as well as the quality of voice-based systems. Möller earned his habilitation at \"the Faculty of Electrical Engineering and Information Technology\" at Ruhr Universität Bochum in 2004 with a book discussing the quality of telephone-based speech dialog systems. He joined Telekom Innovation Laboratories (previously known as Deutsche Telekom Laboratories) in June 2005. In April 2007, he was appointed to a professorship at Technische Universität Berlin, and at Telekom Innovation Laboratories he is the head of the Quality and Usability Lab. In September 2008, Möller was a visiting fellow at the Marcs Institute (formerly Laboratories), University of Western Sydney in Australia, specializing in the evaluation of avatars. In November 2011, he was Visiting Professor at the Universidad de Granada (Spain), from February to April 2012 and from May to July 2014 Visiting Professor at the Ben Gurion University of the Negev in Be'er Sheva (Israel), in October 2013 Visiting Professor at NTNU in Trondheim (Norway), and since 2012, he is Adjunct Professor at the University of Canberra (Australia), where he also taught in February 2014. His book on \"Quality Engineering\" was published in 2010.\n\n\n"}
{"id": "29472173", "url": "https://en.wikipedia.org/wiki?curid=29472173", "title": "Sludge incineration", "text": "Sludge incineration\n\nSludge incineration (German: Klärschlammverbrennung, Chinese: 污泥焚烧发电) is a sewage sludge treatment process using incineration. It generates thermal energy from sewage sludge produced in sewage treatment plants. The process is in operation in Germany where Klärschlammverbrennung GmbH in Hamburg incinerates 1.3m tonnes of sludge annually. The process has also been trialed in China, where it has been qualified as an environmental investment project. However the energy balance of the process is not high, as sludge needs drying before incinerating. \n\nSewage sludge can be incinerated in mono-incineration or co-inceneration plants. In co-inceneration, sewage sludge is not the only fuel and it can be processed at coal fired power plants, cement plants and in some waste incineration facility.\n\nGermany currently has around 27 mono-incineration facilities, where only sewage sludge is processed.\n"}
{"id": "27118", "url": "https://en.wikipedia.org/wiki?curid=27118", "title": "Strontium", "text": "Strontium\n\nStrontium is the chemical element with symbol Sr and atomic number 38. An alkaline earth metal, strontium is a soft silver-white yellowish metallic element that is highly chemically reactive. The metal forms a dark oxide layer when it is exposed to air. Strontium has physical and chemical properties similar to those of its two vertical neighbors in the periodic table, calcium and barium. It occurs naturally mainly in the minerals celestine and strontianite, and is mostly mined from these. While natural strontium is stable, the synthetic Sr isotope is radioactive and is one of the most dangerous components of nuclear fallout, as strontium is absorbed by the body in a similar manner to calcium. Natural stable strontium, on the other hand, is not hazardous to health.\n\nBoth strontium and strontianite are named after Strontian, a village in Scotland near which the mineral was discovered in 1790 by Adair Crawford and William Cruickshank; it was identified as a new element the next year from its crimson-red flame test color. Strontium was first isolated as a metal in 1808 by Humphry Davy using the then-newly discovered process of electrolysis. During the 19th century, Strontium was mostly used in the production of sugar from sugar beet (see strontian process). At the peak of production of television cathode ray tubes, as much as 75 percent of strontium consumption in the United States was used for the faceplate glass. With the replacement of cathode ray tubes with other display methods, consumption of strontium has dramatically declined.\n\nStrontium is a divalent silvery metal with a pale yellow tint whose properties are mostly intermediate between and similar to those of its group neighbors calcium and barium. It is softer than calcium and harder than barium. Its melting (777 °C) and boiling (1655 °C) points are lower than those of calcium (842 °C and 1757 °C respectively); barium continues this downward trend in the melting point (727 °C), but not in the boiling point (2170 °C). The density of strontium (2.64 g/cm) is similarly intermediate between those of calcium (1.54 g/cm) and barium (3.594 g/cm). Three allotropes of metallic strontium exist, with transition points at 235 and 540 °C.\n\nThe standard electrode potential for the Sr/Sr couple is −2.89 V, approximately midway between those of the Ca/Ca (−2.84 V) and Ba/Ba (−2.92 V) couples, and close to those of the neighboring alkali metals. Strontium is intermediate between calcium and barium in its reactivity toward water, with which it reacts on contact to produce strontium hydroxide and hydrogen gas. Strontium metal burns in air to produce both strontium oxide and strontium nitride, but since it does not react with nitrogen below 380 °C, at room temperature, it forms only the oxide spontaneously. Besides the simple oxide SrO, the peroxide SrO can be made by direct oxidation of strontium metal under a high pressure of oxygen, and there is some evidence for a yellow superoxide Sr(O). Strontium hydroxide, Sr(OH), is a strong base, though it is not as strong as the hydroxides of barium or the alkali metals. All four dihalides of strontium are known.\n\nDue to the large size of the heavy s-block elements, including strontium, a vast range of coordination numbers is known, from 2, 3, or 4 all the way to 22 or 24 in SrCd and SrZn. The Ca ion is quite large, so that high coordination numbers are the rule. The large size of strontium and barium plays a significant part in stabilising strontium complexes with polydentate macrocyclic ligands such as crown ethers: for example, while 18-crown-6 forms relatively weak complexes with calcium and the alkali metals, its strontium and barium complexes are much stronger.\n\nOrganostrontium compounds contain one or more strontium–carbon bonds. They have been reported as intermediates in Barbier-type reactions. Although strontium is in the same group as magnesium, and organomagnesium compounds are very commonly used throughout chemistry, organostrontium compounds are not similarly widespread because they are more difficult to make and more reactive. Organostrontium compounds tend to be more similar to organoeuropium or organosamarium compounds due to the similar ionic radii of these elements (Sr 118 pm; Eu 117 pm; Sm 122 pm). Most of these compounds can only be prepared at low temperatures; bulky ligands tend to favor stability. For example, strontium dicyclopentadienyl, Sr(CH), must be made by directly reacting strontium metal with mercurocene or cyclopentadiene itself; replacing the CH ligand with the bulkier C(CH) ligand on the other hand increases the compound's solubility, volatility, and kinetic stability.\n\nBecause of its extreme reactivity with oxygen and water, strontium occurs naturally only in compounds with other elements, such as in the minerals strontianite and celestine. It is kept under a liquid hydrocarbon such as mineral oil or kerosene to prevent oxidation; freshly exposed strontium metal rapidly turns a yellowish color with the formation of the oxide. Finely powdered strontium metal is pyrophoric, meaning that it will ignite spontaneously in air at room temperature. Volatile strontium salts impart a bright red color to flames, and these salts are used in pyrotechnics and in the production of flares. Like calcium and barium, as well as the alkali metals and the divalent lanthanides europium and ytterbium, strontium metal dissolves directly in liquid ammonia to give a dark blue solution.\n\nNatural strontium is a mixture of four stable isotopes: Sr, Sr, Sr, and Sr. Their abundance increases with increasing mass number and the heaviest, Sr, makes up about 82.6% of all natural strontium, though the abundance varies due to the production of radiogenic Sr as the daughter of long-lived beta-decaying Rb. Of the unstable isotopes, the primary decay mode of the isotopes lighter than Sr is electron capture or positron emission to isotopes of rubidium, and that of the isotopes heavier than Sr is electron emission to isotopes of yttrium. Of special note are Sr and Sr. The former has a half-life of 50.6 days and is used to treat bone cancer due to strontium's chemical similarity and hence ability to replace calcium. While Sr (half-life 28.90 years) has been used similarly, it is also an isotope of concern in fallout from nuclear weapons and nuclear accidents due to its production as a fission product. Its presence in bones can cause bone cancer, cancer of nearby tissues, and leukemia. The 1986 Chernobyl nuclear accident contaminated about 30,000 km with greater than 10 kBq/m with Sr, which accounts for 5% of the core inventory of Sr.\n\nStrontium is named after the Scottish village of Strontian (Gaelic \"Sròn an t-Sìthein\"), where it was discovered in the ores of the lead mines. Originally named strontianite by Thomas Charles Hope the name was soon after shortened to strontium.\n\nIn 1790, Adair Crawford, a physician engaged in the preparation of barium, and his colleague William Cruickshank, recognised that the Strontian ores exhibited properties that differed from those in other \"heavy spars\" sources. This allowed Adair to conclude on page 355 \"... it is probable indeed, that the scotch mineral is a new species of earth which has not hitherto been sufficiently examined.\" The physician and mineral collector Friedrich Gabriel Sulzer analysed together with Johann Friedrich Blumenbach the mineral from Strontian and named it strontianite. He also came to the conclusion that it was distinct from the witherite and contained a new earth (neue Grunderde). In 1793 Thomas Charles Hope, a professor of chemistry at the University of Glasgow proposed the name \"strontites\". He confirmed the earlier work of Crawford and recounted: \"... Considering it a peculiar earth I thought it necessary to give it an name. I have called it Strontites, from the place it was found; a mode of derivation in my opinion, fully as proper as any quality it may possess, which is the present fashion.\" The element was eventually isolated by Sir Humphry Davy in 1808 by the electrolysis of a mixture containing strontium chloride and mercuric oxide, and announced by him in a lecture to the Royal Society on 30 June 1808. In keeping with the naming of the other alkaline earths, he changed the name to \"strontium\".\n\nThe first large-scale application of strontium was in the production of sugar from sugar beet. Although a crystallisation process using strontium hydroxide was patented by Augustin-Pierre Dubrunfaut in 1849 the large scale introduction came with the improvement of the process in the early 1870s. The German sugar industry used the process well into the 20th century. Before World War I the beet sugar industry used 100,000 to 150,000 tons of strontium hydroxide for this process per year. The strontium hydroxide was recycled in the process, but the demand to substitute losses during production was high enough to create a significant demand initiating mining of strontianite in the Münsterland. The mining of strontianite in Germany ended when mining of the celestine deposits in Gloucestershire started. These mines supplied most of the world strontium supply from 1884 to 1941. Although the celestine deposits in the Granada basin were known for some time the large scale mining did not start before the 1950s.\n\nDuring atmospheric nuclear weapons testing, it was observed that strontium-90 is one of the nuclear fission products with a relative high yield. The similarity to calcium and the chance that the strontium-90 might become enriched in bones made research on the metabolism of strontium an important topic.\n\nStrontium commonly occurs in nature, being the 15th most abundant element on Earth (its heavier congener barium being the 14th), estimated to average approximately 360 parts per million in the Earth's crust and is found chiefly as the sulfate mineral celestine (SrSO) and the carbonate strontianite (SrCO). Of the two, celestine occurs much more frequently in deposits of sufficient size for mining. Because strontium is used most often in the carbonate form, strontianite would be the more useful of the two common minerals, but few deposits have been discovered that are suitable for development.\n\nIn groundwater strontium behaves chemically much like calcium. At intermediate to acidic pH Sr is the dominant strontium species. In the presence of calcium ions, strontium commonly forms coprecipitates with calcium minerals such as calcite and anhydrite at an increased pH. At intermediate to acidic pH, dissolved strontium is bound to soil particles by cation exchange.\n\nThe mean strontium content of ocean water is 8 mg/l. At a concentration between 82 and 90 µmol/l of strontium, the concentration is considerably lower than the calcium concentration, which is normally between 9.6 and 11.6 mmol/l. It is nevertheless much higher than that of barium, 13 μg/l.\n\nThe three major producers of strontium as celestine as of 2015 are China (150,000 t), Spain (90,000 t), and Mexico (70,000 t); Argentina (10,000 t) and Morocco (2,500 t) are smaller producers. Although strontium deposits occur widely in the United States, they have not been mined since 1959.\n\nA large proportion of mined celestine (SrSO) is converted to the carbonate by two processes. Either the celestine is directly leached with sodium carbonate solution or the celestine is roasted with coal to form the sulfide. The second stage produces a dark-coloured material containing mostly strontium sulfide. This so-called \"black ash\" is dissolved in water and filtered. Strontium carbonate is precipitated from the strontium sulfide solution by introduction of carbon dioxide. The sulfate is reduced to the sulfide by the carbothermic reduction:\nAbout 300,000 tons are processed in this way annually.\n\nThe metal is produced commercially by reducing strontium oxide with aluminium. The strontium is distilled from the mixture. Strontium metal can also be prepared on a small scale by electrolysis of a solution of strontium chloride in molten potassium chloride:\n\nConsuming 75% of production, the primary use for strontium is in glass for colour television cathode ray tubes, where it prevents X-ray emission. This application for strontium is declining because CRTs are being replaced by other display methods. This decline has a significant influence on the mining and refining of strontium. All parts of the CRT must absorb X-rays. In the neck and the funnel of the tube, lead glass is used for this purpose, but this type of glass shows a browning effect due to the interaction of the X-rays with the glass. Therefore, the front panel is made from a different glass mixture with strontium and barium to absorb the X-rays. The average values for the glass mixture determined for a recycling study in 2005 is 8.5% strontium oxide and 10% barium oxide.\n\nBecause strontium is so similar to calcium, it is incorporated in the bone. All four stable isotopes are incorporated, in roughly the same proportions they are found in nature. However, the actual distribution of the isotopes tends to vary greatly from one geographical location to another. Thus, analyzing the bone of an individual can help determine the region it came from. This approach helps to identify the ancient migration patterns and the origin of commingled human remains in battlefield burial sites.\n\nSr/Sr ratios are commonly used to determine the likely provenance areas of sediment in natural systems, especially in marine and fluvial environments. Dasch (1969) showed that surface sediments of Atlantic displayed Sr/Sr ratios that could be regarded as bulk averages of the Sr/Sr ratios of geological terranes from adjacent landmasses. A good example of a fluvial-marine system to which Sr isotope provenance studies have been successfully employed is the River Nile-Mediterranean system. Due to the differing ages of the rocks that constitute the majority of the Blue and White Nile, catchment areas of the changing provenance of sediment reaching the River Nile delta and East Mediterranean Sea can be discerned through strontium isotopic studies. Such changes are climatically controlled in the Late Quaternary.\n\nMore recently, Sr/Sr ratios have also been used to determine the source of ancient archaeological materials such as timbers and corn in Chaco Canyon, New Mexico. Sr/Sr ratios in teeth may also be used to track animal migrations.\n\nStrontium aluminate is frequently used in glow in the dark toys, as it is chemically and biologically inert.\nStrontium carbonate and other strontium salts are added to fireworks to give a deep red colour. This same effect identifies strontium cations in the flame test. Fireworks consumes about 5% of the world's production. Strontium carbonate is used in the manufacturing of hard ferrite magnets.\n\nStrontium chloride is sometimes used in toothpastes for sensitive teeth. One popular brand includes 10% total strontium chloride hexahydrate by weight. Small amounts are used in the refining of zinc to remove small amounts of lead impurities. The metal itself has a limited use as a getter, to remove unwanted gases in vacuums by reacting with them, although barium may also be used for this purpose.\n\nSr is the active ingredient in Metastron, a radiopharmaceutical used for bone pain secondary to metastatic bone cancer. The strontium is processed like calcium by the body, preferentially incorporating it into bone at sites of increased osteogenesis. This localization focuses the radiation exposure on the cancerous lesion.\nSr has been used as a power source for radioisotope thermoelectric generators (RTGs). Sr produces approximately 0.93 watts of heat per gram (it is lower for the form of Sr used in RTGs, which is strontium fluoride). However, Sr has one third the lifetime and a lower density than Pu, another RTG fuel. The main advantage of Sr is that it is cheaper than Pu and is found in nuclear waste. The Soviet Union deployed nearly 1000 of these RTGs on its northern coast as a power source for lighthouses and meteorology stations.\n\nAcantharea, a relatively large group of marine radiolarian protozoa, produce intricate mineral skeletons composed of strontium sulfate. In biological systems, calcium is substituted in a small extent by strontium.\nIn the human body, most of the absorbed strontium is deposited in the bones. The ratio of strontium to calcium in human bones is between 1:1000 and 1:2000 roughly in the same range as in the blood serum.\n\nThe human body absorbs strontium as if it were its lighter congener calcium. Because the elements are chemically very similar, stable strontium isotopes do not pose a significant health threat. The average human has an intake of about two milligrams of strontium a day. In adults, strontium consumed tends to attach only to the surface of bones, but in children, strontium can replace calcium in the mineral of the growing bones and thus lead to bone growth problems.\n\nThe biological half-life of strontium in humans has variously been reported as from 14 to 600 days, 1000 days, 18 years, 30 years and, at an upper limit, 49 years. The wide-ranging published biological half-life figures are explained by strontium's complex metabolism within the body. However, by averaging all excretion paths, the overall biological half-life is estimated to be about 18 years. The elimination rate of strontium is strongly affected by age and sex, due to differences in bone metabolism.\n\nThe drug strontium ranelate aids bone growth, increases bone density, and lessen the incidence of vertebral, peripheral, and hip fractures. However, strontium ranelate also increases the risk of venous thromboembolism, pulmonary embolism and serious cardiovascular disorders, including myocardial infarction. Its use is therefore now restricted. Its beneficial effects are also questionable, since the increased bone density is partially caused by the increased density of strontium over the calcium which it replaces. Strontium also bioaccumulates in the body. Despite restrictions on strontium ranelate, strontium is still contained in some supplements. There is not much scientific evidence on risks of strontium chloride when taken by mouth, those with personal or family history of blood clotting disorders are advised to avoid strontium.\n\nStrontium has been shown to inhibit sensory irritation when applied topically to the skin. Topically applied, strontium has been shown to accelerate the recovery rate of the epidermal permeability barrier (skin barrier).\n\n"}
{"id": "45295325", "url": "https://en.wikipedia.org/wiki?curid=45295325", "title": "The Alliance for Solar Choice", "text": "The Alliance for Solar Choice\n\nThe Alliance for Solar Choice (TASC) leads rooftop solar power advocacy efforts across the United States.\n\nFounded by the largest rooftop solar energy companies in the United States of America, TASC represents the vast majority of the rooftop solar market. Its members include: Demeter Power Group, SunTime Energy, Geostellar, Inc., LGCY Power, Sunrun, and Solar Universe.\n\nTASC member companies are responsible for thousands of jobs and hundreds of thousands of rooftop solar installations on homes, schools, businesses, and government buildings across the country. According to recent Center for American Progress (CAP) studies, rooftop solar systems are now seeing overwhelming adoption in middle-class neighborhoods with median incomes ranging from $40,000 to $90,000.\n\nIn January 2013, the utility trade association Edison Electric Institute (EEI) issued a report titled “Disruptive Challenges: Financial Implications and Strategic Responses to a Changing Retail Electric Business”. The report describes the increasing popularity of consumer-driven rooftop solar, energy efficiency, and demand response as a “vicious cycle.” It details how utilities view rooftop solar as a “disruption” to their current business model, which guarantees utilities specific profit margins from large infrastructure projects funded by ratepayers. Peter Kind of Energy Infrastructure Advocates, the author of the report, makes recommendations on how electric utilities can defend against these ‘disruptive challenges.’ TASC and others in the solar industry have been working since 2013 to defend against utility attacks on core rooftop solar policies across the country.\n\nA primary target of utility attacks is net energy metering, or ‘NEM’. Net metering provides full retail credit to residents, businesses, schools, and other public agencies when their solar systems export surplus energy to the grid. The utility then sells this surplus energy to other customers nearby. Utility companies are attacking NEM to block the growth of rooftop solar, a strategy that has been highly publicized in the press.\n\nAnother utility strategy is to introduce legislation to monopolize the rooftop solar market. In 2014, utilities in South Carolina and Washington pushed legislation with this intention. TASC successfully lead efforts to defeat both attempts.\n\nUtilities also advocate for Value of Solar Tariffs (VOSTs) and Feed-in Tariffs (FITs). As revealed by national law firm Skadden, Arps, Slate, Meagher and Flom LLP, VOSTs create hidden taxes for consumers. In addition, VOSTs create annual market uncertainty that can hurt solar businesses, and they eliminate a customer’s right to actually use the power they generate. This right to use the power produced from solar panels on-site is philosophically important for many solar customers. With VOSTs and FITs, utilities control a homeowner’s solar energy. The homeowner has to sell all of their solar power to the utility as it’s produced. They buy all of the electricity they consume from the utility. The utility determines the price the homeowner receives for the solar power, and the homeowner – as indicated by the Skadden tax memo from – could be required to pay taxes on these payments. The payment price fluctuates according to utility calculations, so it is unpredictable from year to year.\n\nFollowing the recent, concerted attacks from utilities, in May 2014, Barclays PLC downgraded the entire electric utility sector due to a confluence of declining cost trends in distributed solar photovoltaic (PV) power generation and because residential-scale power storage is likely to disrupt the status quo. The Barclays report highlights that \"regulators are ultimately answerable to voters, and the latter are unlikely to tolerate a long halt in their ability to access a clearly beneficial product.\" \n\nThe founding members of TASC represent the vast majority of the nation’s rooftop solar market and include Demeter Power Group, Silevo, SolarCity, Solar Universe, Sunrun, and ZEP Solar.\n\nSunrun SVP of Public Policy Bryan Miller is the Chairman of The Alliance for Solar Choice.\n"}
{"id": "28599839", "url": "https://en.wikipedia.org/wiki?curid=28599839", "title": "The Economical Environmentalist", "text": "The Economical Environmentalist\n\nThe Economical Environmentalist is a book by Prashant Vaze about the effects of climate change and what practical measures can be taken to reduce the amount of energy one consumes. It covers how best to save energy and points out contradictions such as \"The one that particularly exasperates me is the “food miles” obsession, whereby we eschew tomatoes from Spain and roses flown in from Kenya, in favour of local products grown in a heated greenhouse with a far greater carbon footprint.\" In the book he compares the savings available for several real-world examples of people so one can see what changes are possible to make.\n"}
{"id": "30462", "url": "https://en.wikipedia.org/wiki?curid=30462", "title": "Triple point", "text": "Triple point\n\nIn thermodynamics, the triple point of a substance is the temperature and pressure at which the three phases (gas, liquid, and solid) of that substance coexist in thermodynamic equilibrium. For example, the triple point of mercury occurs at a temperature of −38.83440 °C and a pressure of 0.2 mPa.\n\nIn addition to the triple point for solid, liquid, and gas phases, a triple point may involve more than one solid phase, for substances with multiple polymorphs. Helium-4 is a special case that presents a triple point involving two different fluid phases (lambda point).\n\nThe triple point of water was used to define the kelvin, the base unit of thermodynamic temperature in the International System of Units (SI). The value of the triple point of water was fixed by definition, rather than measured, but that changed with the 2019 redefinition of SI base units. The triple points of several substances are used to define points in the ITS-90 international temperature scale, ranging from the triple point of hydrogen (13.8033 K) to the triple point of water (273.16 K, 0.01 °C, or 32.018 °F).\n\nThe term \"triple point\" was coined in 1873 by James Thomson, brother of Lord Kelvin.\n\nThe single combination of pressure and temperature at which liquid water, solid ice, and water vapor can coexist in a stable equilibrium occurs at exactly and a partial vapor pressure of . At that point, it is possible to change all of the substance to ice, water, or vapor by making arbitrarily small changes in pressure and temperature. Even if the total pressure of a system is well above the triple point of water, provided that the partial pressure of the water vapor is 611.657 pascals, then the system can still be brought to the triple point of water. Strictly speaking, the surfaces separating the different phases should also be perfectly flat, to negate the effects of surface tension.\n\nThe gas–liquid–solid triple point of water corresponds to the minimum pressure at which liquid water can exist. At pressures below the triple point (as in outer space), solid ice when heated at constant pressure is converted directly into water vapor in a process known as sublimation. Above the triple point, solid ice when heated at constant pressure first melts to form liquid water, and then evaporates or boils to form vapor at a higher temperature.\n\nFor most substances the gas–liquid–solid triple point is also the minimum temperature at which the liquid can exist. For water, however, this is not true because the melting point of ordinary ice decreases as a function of pressure, as shown by the dotted green line in the phase diagram. At temperatures just below the triple point, compression at constant temperature transforms water vapor first to solid and then to liquid (water ice has lower density than liquid water, so increasing pressure leads to a liquefaction).\n\nThe triple point pressure of water was used during the Mariner 9 mission to Mars as a reference point to define \"sea level\". More recent missions use laser altimetry and gravity measurements instead of pressure to define elevation on Mars.\n\nAt high pressures, water has a complex phase diagram with 15 known phases of ice and several triple points including ten whose coordinates are shown in the diagram. For example, the triple point at 251 K (−22 °C) and 210 MPa (2070 atm) corresponds to the conditions for the coexistence of ice Ih (ordinary ice), ice III and liquid water, all at equilibrium. There are also triple points for the coexistence of three solid phases, for example ice II, ice V and ice VI at 218 K (−55 °C) and 620 MPa (6120 atm).\n\nFor those high-pressure forms of ice which can exist in equilibrium with liquid, the diagram shows that melting points increase with pressure. At temperatures above (0 °C), increasing the pressure on water vapor results first in liquid water and then a high-pressure form of ice. In the range , ice I is formed first, followed by liquid water and then ice III or ice V, followed by other still denser high-pressure forms.\n\nTriple point cells are used in the calibration of thermometers. For exacting work, triple point cells are typically filled with a highly pure chemical substance such as hydrogen, argon, mercury, or water (depending on the desired temperature). The purity of these substances can be such that only one part in a million is a contaminant, called \"six nines\" because it is 99.9999% pure. When it is a water-based cell, a special isotopic composition called VSMOW is used because it is very pure and produces temperatures that are more comparable from lab to lab. Triple point cells are so effective at achieving highly precise, reproducible temperatures, an international calibration standard for thermometers called ITS–90 relies upon triple point cells of hydrogen, neon, oxygen, argon, mercury, and water for delineating six of its defined temperature points.\n\nThis table lists the gas–liquid–solid triple points of several substances. Unless otherwise noted, the data come from the U.S. National Bureau of Standards (now NIST, National Institute of Standards and Technology).\n\n"}
{"id": "7381144", "url": "https://en.wikipedia.org/wiki?curid=7381144", "title": "Turbine hall", "text": "Turbine hall\n\nThe turbine hall, generating hall or turbine building is a building or room in any steam cycle or hydroelectric power plant which houses a number of components vital to the generation of electricity from the steam that comes from the boiler, or from the water coming from the reservoir. The components in the turbine hall typically are the turbines and electric generators, and in the case of steam cycle plants, moisture separators and reheaters. A turbine hall is typically extremely loud, and in the case of steam cycle plants, hot.\n\nIn nuclear power plants, boiling water reactors present unique challenges since the steam going through the turbines may be radioactive. This means that the turbine hall has to be slightly contained and much unique maintenance must be performed. A typical plant will house one high-pressure turbine and two low-pressure turbines.\n\n\n"}
{"id": "23475962", "url": "https://en.wikipedia.org/wiki?curid=23475962", "title": "Verd antique", "text": "Verd antique\n\nVerd antique (obsolete French, from Italian, \"verde antico\", \"ancient green\"), also called verde antique or Ophite, is a serpentinite breccia popular since ancient times as a decorative facing stone. It is a dark, dull green, white-mottled (or white-veined) serpentine, mixed with calcite, dolomite, or magnesite, which takes a high polish. It is sometimes classed, erroneously, as a variety of marble (\"serpentine marble\", \"Connemara marble\", \"Moriah stone\", etc.). It has also been called and marketed as \"ophicalcite\" or \"ophite\".\n\nNon-brecciated varieties of a very similar serpentinite, sometimes also called \"verd antique\", have been quarried at Victorville, California; Cardiff, Maryland; and Rochester in Addison County, Vermont.\n\nVerd antique is used like marble especially in interior decoration and occasionally as outdoor trim, although the masses are frequently jointed and often only small slabs can be secured. It was known to the ancient Romans and was quarried especially at Casambala, near Larissa, Thessaly, in Greece. Verd antique was much used by the monumental builders of the Byzantine Empire and by the Ottomans after them. The term \"verd antique\" has been documented in English texts as early as 1745.\n\nVerd antique is the national gemstone of Ireland, where it is referred to as Connemara marble. \n"}
{"id": "2352404", "url": "https://en.wikipedia.org/wiki?curid=2352404", "title": "Wanda Jablonski", "text": "Wanda Jablonski\n\nWanda Jablonski (23 August 1920, in Czechoslovakia – 28 January 1992, in New York City) was an American journalist who covered the global petroleum industry.\n\nShe was the daughter of Polish petroleum geologist Eugene Jablonski, and was immersed in the oil industry throughout her childhood. She was at St George's School, Harpenden in England until July 1937; she gained her school certificate and got the form prize in July 1938 before leaving to study in America. She and her parents traveled widely, and although she became an American citizen, she developed great sympathy for other cultures – an attribute which as an adult enabled her to make deep contacts across the world oil industry, from the multinational oil companies to the leaders of oil-producing countries. She earned a B.A. from Cornell University in 1942 and an M.A. from the Columbia University Graduate School of Journalism the following year.\n\nJablonski began as the oil editor at the \"Journal of Commerce\", where she made her mark with a 1948 interview in Caracas with Juan Pablo Pérez Alfonso, then the Venezuelan oil minister, which cleverly synthesized the developing nations' viewpoint, in those days rarely heard in the Western hemisphere. She moved to \"Petroleum Week\" journal in 1954 and cemented her reputation, speaking on equal terms with oil ministers and company chairmen. A rare woman in a man's world, she was known throughout the oil industry simply as \"Wanda\".\n\nShe is credited with arranging the 1959 Cairo meeting where Abdullah Tariki, Juan Pablo Pérez Alfonso, and other oil ministers of Middle East signed the \"Gentleman’s Agreement,\" a precursor of Organization of the Petroleum Exporting Countries (OPEC), the international organization whose mission is to coordinate the policies of the oil-producing countries. In 1960, Jablonski reported to oil company executives that there was a marked hostility toward the West and a growing outcry against \"absentee landlordism\" in the Middle East. \"From offices in London, New York and Pittsburgh, top executives of oil companies were controlling destinies of Middle East oil-producing states.\" Ignoring her warning, in August 1960 the major oil companies unilaterally reduced the prices that were used to calculate how much revenue the producing countries received. As a direct result, in September 1960, representatives from oil-producing countries met and formed OPEC.\n\nShe then founded \"Petroleum Intelligence Weekly\" in 1961, the journal which came to be known as \"the bible of the oil industry\", and ran it until 1988.\n"}
{"id": "19781890", "url": "https://en.wikipedia.org/wiki?curid=19781890", "title": "Wiltshire Museum", "text": "Wiltshire Museum\n\nThe Wiltshire Museum, formerly known as Wiltshire Heritage Museum and Devizes Museum, is a museum, archive and library and art gallery in Devizes, Wiltshire, England. The museum was established and is still run by, the Wiltshire Archaeological and Natural History Society (WANHS), a registered charity founded in 1853. After the purchase of an old grammar school the museum was opened in 1873. Subsequently, it expanded into two Georgian houses on either side and still occupies this location today.\n\nThe museum maintains a collection covering the archaeology, art, history and natural history of Wiltshire. This collection covers periods of history from as far back as the Palaeolithic and also includes Neolithic, Bronze Age, Roman, Saxon, Mediaeval and more recent historical artefacts. Among the prehistoric collections are items from the Stonehenge and Avebury World Heritage Site. Several of the collections have been designated as being a significant part of England’s cultural heritage.\n\nOne of the most important collections at the museum is the finds from Bush Barrow, an early Bronze Age burial mound in Stonehenge World Heritage Site. The barrow was excavated by William Cunnington in 1808 and produced the richest and most important finds from a Bronze Age grave in the Stonehenge Landscape to date. The finds were acquired by the museum in 1883 and were displayed there until 1922 when they were indefinitely loaned to the British Museum. After a controversial restoration of the largest piece that may not reflect its original finish, the pieces were returned to Devizes in 1985. They are on display in the Gold from the Time of Stonehenge exhibition, opened in 2013.\n\nThe natural history collection includes remains of a plesiosaur called Bathyspondylus found at Swindon in 1774. \"Bathyspondylus swindoniensis\" was first described in 1982 from the Museum's specimens.\n\nIn 2010 the Museum ran a community bus service, the Henge Hopper, linking Avebury with Amesbury and Stonehenge.\n\n\n"}
{"id": "14190210", "url": "https://en.wikipedia.org/wiki?curid=14190210", "title": "Yoichi Kuroda", "text": "Yoichi Kuroda\n\nHe was founder of the activist organization \"Japan Tropical Forest Action Network\" (JATAN).\n\nDeforestation in Japan \n"}
