{"id": "39819074", "url": "https://en.wikipedia.org/wiki?curid=39819074", "title": "Acoustic droplet vaporization", "text": "Acoustic droplet vaporization\n\nAcoustic droplet vaporization (ADV) is the process by which superheated liquid droplets are phase-transitioned into gas bubbles by means of ultrasound. Perfluorocarbons and halocarbons are often used for the dispersed medium, which forms the core of the droplet. The surfactant, which forms a stabilizing shell around the dispersive medium, is usually composed of albumin or lipids.\n\nThere exist two main hypothesis that explain the mechanism by which ultrasound induces vaporization. One poses that the ultrasonic field interacts with the dispersed medium so as to cause vaporization in the bubble core. The other suggests that shockwaves from inertial cavitation, occurring near or within the droplet, cause the dispersed medium to vaporize.\n\n"}
{"id": "12019156", "url": "https://en.wikipedia.org/wiki?curid=12019156", "title": "Aeromechanics", "text": "Aeromechanics\n\nAeromechanics is the science about mechanics that deals with the motion of air and other gases, involving aerodynamics, thermophysics and aerostatics. It is the branch of mechanics that deals with the motion of gases (especially air) and their effects on bodies in the flow. The fluid flow and structure are interactive systems and their interaction is dynamic. The fluid force causes the structure to deform which changes its orientation to the flow and hence the resulting fluid force.\n\nAreas that comprise this are within the technology of aircraft and helicopters since these use propellers and rotors.\n\n"}
{"id": "42794518", "url": "https://en.wikipedia.org/wiki?curid=42794518", "title": "Alfred Ubbelohde", "text": "Alfred Ubbelohde\n\nAlfred René Jean Paul Ubbelohde FRS (14 December 1907 – 7 January 1988) was a Belgian-born English physical chemist.\n\nUbbelohde was born in Antwerp, Belgium, in 1907. He was educated at St Paul's School, 1920-1926 and at Christ Church, Oxford, 1926-1930. He was senior scholar of Christ Church from 1931-1933. He then became a senior researcher in the Department of Thermodynamics at Oxford, 1933-1935. This was followed by excursions as a Dewar Fellow of the Royal Institution, London, 1936-1940 as Principal Experimental Officer at the Ministry of Supply, 1940-1945 and as Professor of Chemistry at Queen's University, Belfast, 1945-1954. He was elected as a Fellow of the Royal Society in 1951.\n\nIn 1954, Ubbelohde became a professor of thermodynamics at Imperial College London, a position he held until 1975. In 1961 he was awarded CBE; from 1961 to 1975 he was the head of the Chemical Engineering Department at Imperial College; following this he was a senior research fellow at Imperial College from 1975-1988;\n\nUbbelohde's research interests included chemical thermodynamics, combustion, explosions and detonations, ionic melts, graphite and intercalation compounds. His group was the first to synthesise highly oriented pyrolytic graphite (HOPG). Over the course of his career, Ubbelohde wrote six books and some 400 publications. Ubbelohde was the chair of the Solvay Conferences on Chemistry (London), from 1959 to 1980. The Ubbelohde effect, which is the observation that dueteration increases hydrogen bond length, is named after him. He is also credited with coining the term proton conductor. He also is known for studying life from a thermodynamic perspective, and for his eccentric study of the thermodynamics of pigs. In 1960 he bought a pig farm where he raised over 100 pigs and studied them from a thermodynamic perspective. He authored numerous books on chemistry during his career.\n\nUbbelohde was elected a Fellow of the Royal Society (FRS) in 1951. He received the Liversidge Award of the Royal Society of Chemistry in 1959.\n"}
{"id": "18553892", "url": "https://en.wikipedia.org/wiki?curid=18553892", "title": "Bananas Unpeeled", "text": "Bananas Unpeeled\n\nBananas Unpeeled is a 26 minute film shot in Latin America and the Caribbean, investigating the social and environmental issues faced by plantation workers and small farmers in the Caribbean.\nIt examines fair trade policies and labeling as a positive alternative for workers on plantations where harsh working conditions and environmental damage are the norm.\n\nThe film was made by independent film-maker Nick Shaw and introduced by the political activist and comedian Mark Thomas.\n\nThis educational resource (the film and corresponding teacher action booklet) was quickly taken up by leading British NGO's (OXFAM, People & Planet). Used extensively by schools, universities, trade union organisations, etc., it helped with the introduction of the \"fair trade banana\" into Great Britain. The Co-Op UK supermarket chain was the first to introduce Fair trade Bananas into their stores. To date, several other supermarket chains have followed this example (e.g. Waitrose, Sainsbury's).\n\nThe story behind the making of the film is mentioned in the 2008 book \"Fighting the Banana Wars and other Fairtrade Battles\" by Harriet Lamb, C.B.E., director of the Fairtrade Foundation (UK).\n"}
{"id": "9021459", "url": "https://en.wikipedia.org/wiki?curid=9021459", "title": "Calamander", "text": "Calamander\n\nCalamander or coromandel is a valuable wood from Sri Lanka and Southeast Asia. It is a hazel-brown color, with black stripes (or the other way about), very heavy and hard. It is also known as Macassar ebony or variegated ebony and is closely related to genuine ebony, but is obtained from different species in the same genus; one of these is \"Diospyros quaesita\" Thwaites, from Sri Lanka. The name \"calamander\" comes from the local Sinhalese name, \"kalu-medhiriya\", which means dark chamber, referring to the characteristic ebony black wood. It is used in furniture, luthiery and for sculpture. \n\nCalamander has been logged to extinction over the last two to three hundred years and is no longer available for new work in any quantity. Furniture in calamander is so expensive and so well looked after that even recycling it is an unlikely source. A substitute, Macassar ebony, has similar characteristics and to the untrained eye is nearly the same but it lacks the depth of colour seen in genuine calamander. \n\n"}
{"id": "5626381", "url": "https://en.wikipedia.org/wiki?curid=5626381", "title": "Centre for Timber Engineering", "text": "Centre for Timber Engineering\n\nThe Centre for Timber Engineering (CTE) is part of Edinburgh Napier University, one of the UK's \"post 1992\" Universities with an emphasis on practical research and working with industry. CTE is a research and education unit within the Forest Products Research Institute. It is the only centre of its kind within the United Kingdom, providing research, consultancy, information, education and training related to timber in construction.\n\nIts activity ranges from fundamental research into timber's performance as an engineering material, right through to commercial testing and partnership working with industry on innovation and technology transfer projects.\n\nCTE was established in January 2002 following the result of a feasibility study carried out by The Scottish Forest Industries Cluster: a partnership between Forest Industries Development Council (now part of ConFor) and Scottish Enterprise.\n\n"}
{"id": "34964580", "url": "https://en.wikipedia.org/wiki?curid=34964580", "title": "Chamundeshwari Electricity Supply Corporation", "text": "Chamundeshwari Electricity Supply Corporation\n\nThe Chamundeshwari Electricity Supply Corporation Limited (CESC Mysore) is a company that provides electricity to five districts in the Indian State of Karnataka. It was carved out of Mangalore Electricity Supply Company Limited in 2005 and has its headquarters in Mysore. It is an undertaking of the government of Karnataka.\n\nThe five districts of Chamarajanagar, Hassan, Kodagu, Mandya, Mysore were distributed electricity by Karnataka Power Transmission Corporation (KPTCL) since its formation in 1999 to 2002. Mangalore Electricity Supply Company Limited (MESCOM) was one of four companies that was formed in 2002 from KPTCL, and distributed to the said districts before CESC was carved out of it in 2005 to meet the increasing demand in power. Kodagu was added to its jurisdiction in 2006.\n"}
{"id": "1415333", "url": "https://en.wikipedia.org/wiki?curid=1415333", "title": "Chayote", "text": "Chayote\n\nChayote (\"Sechium edule\") also known as mirliton squash is an edible plant belonging to the gourd family Cucurbitaceae. Chayote was one of the several foods introduced to the Old World during the Columbian Exchange. Also during this period, the plant spread from Mexico to other parts of the Americas, ultimately causing it to be integrated into the cuisine of many other Latin American nations.\n\nThe chayote fruit is mostly used cooked. When cooked, chayote is usually handled like summer squash; it is generally lightly cooked to retain the crispy consistency. Though rare and often regarded as especially unpalatable and tough in texture, raw chayote may be added to salads or salsas, most often marinated with lemon or lime juice. Whether raw or cooked, chayote is a good source of vitamin C.\n\nAlthough most people are familiar only with the fruit as being edible, the root, stem, seeds and leaves are edible as well. The tubers of the plant are eaten like potatoes and other root vegetables, while the shoots and leaves are often consumed in salads and stir fries, especially in Asia.\n\nThe common English name is from the Spanish word \"chayote\", a derivative of the Nahuatl word \"chayohtli\" ().\n\nLike other members of the gourd family, chayote has a sprawling habit, and it should only be planted if there is plenty of room in the garden. The roots are also highly susceptible to rot, especially in containers, and the plant in general is finicky to grow. However, in Australia and New Zealand, it is an easily grown yard or garden plant, set on a chicken wire support or strung against a fence.\n\nThe plant was first recorded by modern botanists in P. Browne's 1756 work, the \"Civil and Natural History of Jamaica\". Swartz included it in 1800 in its current genus \"Sechium\".\n\nIn the most common variety, the fruit is roughly pear-shaped, somewhat flattened and with coarse wrinkles, ranging from 10 to 20 cm in length. It looks like a green pear, and it has a thin, green skin fused with the green to white flesh, and a single, large, flattened pit. Some varieties have spiny fruits. The flesh has a fairly bland taste, and a texture is described as a cross between a potato and a cucumber. \n\nThe chayote vine can be grown on the ground, but as a climbing plant, it will grow onto anything, and can easily rise as high as 12 meters when support is provided. It has heart-shaped leaves, 10–25 cm wide and tendrils on the stem. The plant bears male flowers in clusters and solitary female flowers. The plant’s fruit is light green and elongated with deep ridges lengthwise.\n\nThe fruit does not need to be peeled to be cooked or fried in slices. It has a very mild flavor. It is commonly served with seasonings (e.g. salt, butter and pepper in Australia) or in a dish with other vegetables and/or flavorings. It can also be boiled, stuffed, mashed, baked, fried, or pickled in escabeche sauce. Both fruit and seed are rich in amino acids and vitamin C. Fresh green fruit are firm and without brown spots or signs of sprouting; smaller fruit are usually more tender. Chayote can be sliced lengthwise and eaten using salad dressing dip. The seed is edible and tasty to some when served cold when dipped in dressing.\n\nThe tuberous part of the root is starchy and eaten like a yam (can be fried). It can be used as pig or cattle fodder.\n\nThe leaves and fruit have diuretic, cardiovascular and anti-inflammatory properties, and a tea made from the leaves has been used in the treatment of arteriosclerosis and hypertension, and to dissolve kidney stones.\n\nIn Louisiana Creole and Cajun cuisine, the fruit, known as \"mirliton\" (pronounced ) also spelled \"mirletons\" or \"merletons\" (plural—the \"r\" is often silent, e.g. Cajun \"me-lay-taw\" or urban Creole \"miʁl-uh-tɔ̃ns\") is a popular seasonal dish for the holidays, especially around Thanksgiving, in a variety of recipes.\n\nChayote is an important part of traditional diets across Mesoamerica, and can be found in a variety of dishes.\n\nIn Brazil (locally called chuchu) and other Latin American countries, it is breaded and fried, or used cooked in salads, soups and soufflés.\n\nIn the Philippines, the plant is known as \"sayote\" and is grown mostly in mountainous part of the country such as Benguet and parts of Cordillera Administrative Region. Chayote is used in many kinds of dishes such as soup (often as a substitute for upo squash), stir-fried vegetables and chop suey. It was among the numerous vegetables, grains, and fruits introduced into the country via the Manila galleon trade.\n\nIn Indonesia, chayotes are called labu siam and widely planted for their shoots and fruit. It's generally used in Sundanese food as \"lalap\" and one of ingredients for Sundanese cuisine called \"sayur asem\".\n\nIn Tamil Nadu, South India, chayote is known as Maerakkai (மேரக்காய்)/chow-chow (சௌ சௌ) in Tamil and widely used in everyday cooking for recipes like \"sambar\", \"kootu\", \"poriyal\", \"thuvayal\", \"chutney\" and \"mor-kulambu\". Chow-Chow is the common name used in the markets.\n\nIn Burma/Myanmar, the chayote is known as \"Gurkha Thee or Gurkha fruit\" ဂေါ်ရခါးသီး and is very cheap and popular.\n\nIn China, the chayote is known as the \"Buddha's Hand Melon\" () or alternatively in Cantonese choko (cau1 kau4) 秋球 [lit. autumn ball], and is generally stir-fried. The common Australian and New Zealand word, choko, comes from the 19th century Cantonese market gardeners who introduced many vegetables into those countries.\n\nIn Taiwan, and southern mainland China, chayotes are widely planted for their shoots, known as \"lóng xü cài\" (Simplified Chinese: 龙须菜; Traditional Chinese 龍鬚菜), literally \"dragon-whisker vegetable\"). Along with the young leaves, the shoot is a commonly consumed vegetable in the region.\n\nIn Thai cuisine, the plant is known as \"sayongte\" () or \"fak maeo\" (, literally meaning \"Miao melon\"). It grows mainly in the mountains of northern Thailand. The young shoots and greens are often eaten stir-fried or in certain soups.\n\nIn Darjeeling, India and Nepal, the plant and fruit is called \"ishkus\" (इस्कुस in Nepali), probably derived from the word squash. Its shoots, fruit and roots are widely used for different varieties of curries.\n\nIn the Indian state of West Bengal, it is generally known as \"Squash\" (স্কোয়াশ). The whole vegetable is used to make curries, or it is sauteed. It is also cooked with fish, eggs or mutton. It is largely eaten during the summer and rainy season as it contains lots of water and is a good source of vitamin C. The young branches are also considered for making items as \"saag\" or can be added into preparing the Shukto. There are two varieties available; dark green and light green. The dark green variety is much more tender than the lighter one, which develops a fibrous texture around its seed if harvested or consumed lately.\n\nIn Karnataka, South India, Chayote is popularly referred to as \"seeme badanekaayi\" (ಸೀಮೆ ಬದನೇಕಾಯಿ) in Kannada or \"Bangalore brinjal (Bengaluru vankayya)\"; \"brinjal/eggplant/aubergine of the plateau\". It is used in vegetable stews like \"sambar\" and \"palya\".\n\nIn Réunion, the French overseas territory in the Indian Ocean near Mauritius, chou chou, as it is known, is served in many dishes especially in the highlands. A popular starter of Chou chou au Gratin (baked with a cheese sauce), as a side with a meal and even as a dessert.\n\nIn Mauritius, it is called sousou and is cultivated in the high plateau of the island. Mixed with beef, pork or chicken chou chou is widely used to make delicious steamed Chinese dumplings called niouk yen (boulette chou chou) or chow mai. These dumplings are very appreciated and can be found in restaurants and snacks all over the island. Stems and leaves are consumes in bouillon to accompany rice and other dishes. The chou chou is also consumed as pickle, salad, gratin, curry and sauté with beef, egg or chicken.\n\nIn Australia, where it is called \"choko\", a persistent urban legend is that McDonald's apple pies were made of chayotes, not apples. This eventually led McDonald's to emphasise the fact that real apples are used in their pies. This legend was based on an earlier belief that tinned pears were often disguised chayotes. A possible explanation for the rumor is that there are a number of recipes in Australia that advise chayotes can be used in part replacement of canned apples to make the fruit go farther in making apple pies. This likely arose because of the economies of \"mock\" food substitutes during the Depression Era, shortages of canned fruit in the years following World War II, and the fact apples do not grow in many tropical and subtropical parts of Australia, making them scarce. Chayotes, on the other hand, grow extensively in Australia, with many suburban backyards featuring chayote vines growing along their fence lines and outhouses.\n\nDue to its purported cell-regenerative properties, it is believed as a contemporary legend that this fruit caused the mummification of people from the Colombian town of San Bernardo who extensively consumed it. The very well preserved skin and flesh can be seen in the mummies today.\n\n\n\n"}
{"id": "837773", "url": "https://en.wikipedia.org/wiki?curid=837773", "title": "Daniel McFarlan Moore", "text": "Daniel McFarlan Moore\n\nDaniel McFarlan Moore (February 27, 1869 – June 15, 1936) was a U.S. electrical engineer and inventor. He developed a novel light source, the \"Moore lamp\", and a business that produced them in the early 1900s. The Moore lamp was the first commercially viable light-source based on gas discharges instead of incandescence; it was the predecessor to contemporary neon lighting and fluorescent lighting. In his later career Moore developed a miniature neon lamp that was extensively used in electronic displays, as well as vacuum tubes that were used in early television systems.\n\nHe was born in Northumberland, Pennsylvania on February 27, 1869. Moore was the son of the Reverend Alexander Davis and Maria Louisa Douglas Moore. He graduated from Lehigh University in 1889. Moore married Mary Alice Elliott, of New York City, on June 5, 1895. They had three children: Dorothy Mae Moore, (born 1900); Elliott McFarlan Moore (1902–1933); and Beatrice Jean Moore, (born 1912).\n\nHe began his career in 1890 working in the engineering department of the United Edison Manufacturing Company. At some point he started experimenting with producing light from glow discharges, which Heinrich Geissler had first developed in the 1850s. \"What’s wrong with my light?\" Thomas Edison is said to have asked when he learned that Moore had started to tinker with light-producing tubes of gas as a potential replacement for the incandescent bulb. Moore is reported to have replied undiplomatically, \"It’s too small, too hot and too red.\" Moore left in 1894 to form his own companies, the Moore Electric Company and the Moore Light Company.\n\nMoore had devised his glow discharge lighting system by 1896. The Moore Lamp was an extension of the well-known Geissler tube, which used glass tubes from which the air had been removed and a different gas inserted. The low-pressure gas glows when a current is passed through it. As described in 1915, \"In the Moore system of lighting the essential feature is the introduction of a special valve which automatically admits gas into the tube as the supply becomes exhausted.\" The Moore lamps utilized nitrogen or carbon dioxide as the luminous gas; Moore's innovation compensated for the gradual loss of gas in the lamp to the electrodes and the glass. Carbon dioxide gave a good quality white light. The first commercial installation was done in 1904 in a hardware store in Newark, New Jersey. The lamp yielded about 10 lumens per watt, which was about triple the output of incandescent lights based on carbon filaments. Arthur Bright has written, \"Despite the fact that the tube was expensive to install, complicated, and required very high voltages, its operating advantages were great enough for it to find restricted use in stores, offices, and similar general lighting uses as well as in photography and some advertising and decorative applications.\"\n\nThe modest success of the Moore tubes was among the drivers for developing better filaments for standard incandescent light bulbs. Tungsten filament bulbs were a sufficient improvement over carbon filaments that the Moore tubes \"gradually disappeared from the market, leaving only short carbon-dioxide tubes in use for color matching, in which they excelled because of their daylight color. The General Electric Company absorbed the two Moore companies and Moore's patents in 1912. Moore himself rejoined General Electric's laboratory force.\"\n\nMoore's inventions at General Electric included a miniature neon lamp that remained a fixture in electronic displays throughout the twentieth century, and was a forerunner of plasma displays. Both the lamp and his further inventions were also important to the early development of television. In particular, around 1917 Moore developed a \"negative glow\" neon lamp. These were miniature lamps with a very different design than the much larger neon tubes used for neon lighting; a Smithsonian Institution website notes, \"These small, low power devices use a physical principle called \"coronal discharge.\" Moore mounted two electrodes close together in a bulb and added neon or argon gas. The electrodes would glow brightly in red or blue, depending on the gas, and the lamps lasted for years. Since the electrodes could take almost any shape imaginable, a popular application has been fanciful decorative lamps. Glow lamps found practical use as indicators in instrument panels and in many home appliances until the acceptance of light-emitting diodes (LEDs) in the 1970s.\" In 1924 he invented the vacuum bulbs used in telephotography, and in 1925 improved it for use in television.\n\nMoore was awarded the John Scott Medal of The Franklin Institute in 1911.\n\nOn June 15, 1936, at the age of 67, Moore was shot to death on the lawn of his home in East Orange, New Jersey, by an unemployed inventor who became enraged after finding that an invention he filed for was already the subject of a patent granted to Moore.\n\n"}
{"id": "29162655", "url": "https://en.wikipedia.org/wiki?curid=29162655", "title": "Depauperate ecosystem", "text": "Depauperate ecosystem\n\nA depauperate ecosystem is one which is lacking in numbers or variety of species, often because it lacks enough stored chemical elements required for life. Thus, depauperate ecosystems often cannot support rapid growth of flora and fauna, high biomass density, and high biological diversity. An urchin barren is an example of a depauperate ecosystem.\n\nAn ecosystem is a biological community of interaction organisms and their actual physical environment. In Ecology, depauerate is an area that is so poor in species quantities and diversity. It lacks in numbers or a variety of species. Basically, a plant or animal is imperfectly developed. The reasons why there are depauperate areas are because the species do not have many competitors to fight with. Also, they have fewer resources causing the species not to survive without any protein or nutrients. Because these species lack the basic life necessities that they need, it’s hard for them to continue to carry on with life. ( The ecology of Adaptive Radiation). Therefore they aren’t reproducing the way that they are supposed to. In some cases, the species will actually start inbreeding. And because of that there are the same species everywhere. Therefore, competing with themselves, which can cause them to die. So, the area ends up falling short of the natural developmental size.\n\nAn example of a depauerate ecosystem is the growth of flora. This ecosystem cannot support the rapid growth of flora because it's in a severely diminished area. On top of that, the flora cannot grow because of the young geological age of the island. ( Primate Behavioral Ecology). Since the species aren’t alive anymore, they have turned into fossils. Apparently, you will find a lot of fossils in a depauperate area. You will find multiple fossils because there was a lack of stored chemical elements that were required for the life of the species.\n\n"}
{"id": "685002", "url": "https://en.wikipedia.org/wiki?curid=685002", "title": "Diesel–electric transmission", "text": "Diesel–electric transmission\n\nA diesel–electric transmission, or diesel–electric powertrain, is used by a number of vehicle and ship types for providing locomotion.\n\nA diesel–electric transmission system includes a diesel engine connected to an electrical generator, creating electricity that powers electric traction motors. No clutch is required. Before diesel engines came into widespread use, a similar system, using a petrol (gasoline) engine and called petrol–electric or gas–electric, was sometimes used. \n\nDiesel–electric transmission is used on railways by diesel electric locomotives and diesel electric multiple units, as electric motors are able to supply full torque at 0 RPM. Diesel–electric systems are also used in submarines and surface ships and some land vehicles.\n\nIn some high-efficiency applications, electrical energy may be stored in rechargeable batteries, in which case these vehicles can be considered as a class of hybrid electric vehicle.\n\nThe first diesel motorship was also the first diesel–electric ship, the Russian tanker \"Vandal\" from Branobel, which was launched in 1903. Steam turbine–electric propulsion has been in use since the 1920s (s), using diesel–electric powerplants in surface ships has increased lately. The Finnish coastal defence ships \"Ilmarinen\" and \"Väinämöinen\" laid down in 1928–1929, were among the first surface ships to use diesel–electric transmission. Later, the technology was used in diesel powered icebreakers.\n\nIn World War II the United States built diesel–electric surface warships. Due to machinery shortages destroyer escorts of the and es were diesel–electric, with half their designed horsepower (The and es were full-power steam turbine–electric). The s, on the other hand, were designed for diesel–electric propulsion because of its flexibility and resistance to damage.\n\nSome modern diesel–electric ships, including cruise ships and icebreakers, use electric motors in pods called azimuth thrusters underneath to allow for 360° rotation, making the ships far more maneuverable. An example of this is \"Harmony of the Seas\", the largest passenger ship as of 2016.\n\nGas turbines are also used for electrical power generation and some ships use a combination: \"Queen Mary 2\" has a set of diesel engines in the bottom of the ship plus two gas turbines mounted near the main funnel; all are used for generating electrical power, including those used to drive the propellers. This provides a relatively simple way to use the high-speed, low-torque output of a turbine to drive a low-speed propeller, without the need for excessive reduction gearing.\n\nEarly submarines used a direct mechanical connection between the engine and propeller, switching between diesel engines for surface running, and electric motors for submerged propulsion. This was effectively a \"parallel\" type of hybrid, since the motor and engine were coupled to the same shaft. On the surface, the motor (driven by the engine) was used as a generator to recharge the batteries and supply other electric loads. The engine would be disconnected for submerged operation, with batteries powering the electric motor and supplying all other power as well.\n\nTrue diesel–electric transmissions for submarines were first proposed by the United States Navy's Bureau of Engineering in 1928; instead of driving the propeller directly while running on the surface, the submarine's diesel would instead drive a generator that could either charge the submarine's batteries or drive the electric motor. This meant that motor speed was independent of the diesel engine's speed, and the diesel could run at an optimum and non-critical speed, while one or more of the diesel engines could be shut down for maintenance while the submarine continued to run using battery power. The concept was pioneered in 1929 in the S-class submarines , , and to test the concept. The first production submarines with this system were the \"Porpoise\"-class, and it was used on most subsequent US diesel submarines through the 1960s. The only other navy to adopt the system before 1945 was the British Royal Navy in the U-class submarines, although some submarines of the Imperial Japanese Navy used separate diesel generators for low-speed running.\n\nIn a diesel–electric transmission arrangement, as used on 1930s and later US Navy, German, Russian and other nations' diesel submarines, the propellers are driven directly or through reduction gears by an electric motor, while two or more diesel generators provide electric energy for charging the batteries and driving the electric motors. This mechanically isolates the noisy engine compartment from the outer pressure hull and reduces the acoustic signature of the submarine when surfaced. Some nuclear submarines also use a similar turbo-electric propulsion system, with propulsion turbo generators driven by reactor plant steam.\n\nDuring World War I, there was a strategic need for rail engines without plumes of smoke above them. Diesel technology was not yet sufficiently developed but a few precursor attempts were made, especially for petrol–electric transmissions by the French (Crochat-Collardeau, patent dated 1912 also used for tanks and trucks) and British (Dick, Kerr & Co. and British Westinghouse). About 300 of these locomotives, only 96 being standard gauge, were in use at various points in the conflict. Even before the war, the GE 57-ton gas-electric boxcab had been produced in the USA.\n\nIn the 1920s, diesel–electric technology first saw limited use in switchers (or \"shunters\"), locomotives used for moving trains around in railroad yards and assembling and disassembling them. An early company offering \"Oil-Electric\" locomotives was the American Locomotive Company (ALCO). The ALCO HH series of diesel–electric switcher entered series production in 1931. In the 1930s, the system was adapted for streamliners, the fastest trains of their day. Diesel–electric powerplants became popular because they greatly simplified the way motive power was transmitted to the wheels and because they were both more efficient and had greatly reduced maintenance requirements. Direct-drive transmissions can become very complex, considering that a typical locomotive has four or more axles. Additionally, a direct-drive diesel locomotive would require an impractical number of gears to keep the engine within its powerband; coupling the diesel to a generator eliminates this problem. An alternative is to use a torque converter or fluid coupling in a direct drive system to replace the gearbox. Hydraulic transmissions are claimed to be somewhat more efficient than diesel–electric technology.\n\nDiesel electric based buses have also been produced, including hybrid systems able to run on and store electrical power in batteries. The two main providers of hybrid systems for diesel–electric transit buses include Allison Transmission and BAE Systems. New Flyer Industries, Gillig Corporation, and North American Bus Industries are major customers for the Allison EP hybrid systems, while Orion Bus Industries is a major customer for the BAE HybriDrive system. Mercedes-Benz makes their own diesel–electric drive system, which is used in their Citaro.\n\nExamples include:\n\nIn the automobile industry, diesel engines in combination with electric transmissions and battery power are being developed for future vehicle drive systems. Partnership for a New Generation of Vehicles was a cooperative research program between the U.S. government and \"The Big Three\" automobile manufacturers (DaimlerChrysler, Ford Motor Company, and General Motors Corporation) that developed diesel hybrid cars.\n\n\n\nDiesel–electric propulsion has been tried on some military vehicles, such as tanks. The prototype TOG1 and TOG2 super heavy tanks of the Second World War used twin generators driven by V12 diesel engines. More recent prototypes include the SEP modular armoured vehicle and T95e. Future tanks may use diesel–electric drives to improve fuel efficiency while reducing the size, weight and noise of the power plant. Attempts with diesel–electric drives on wheeled military vehicles include the unsuccessful ACEC Cobra, MGV, and XM1219 Armed Robotic Vehicle.\n\n\n"}
{"id": "241034", "url": "https://en.wikipedia.org/wiki?curid=241034", "title": "Electron neutrino", "text": "Electron neutrino\n\nThe electron neutrino () is a subatomic lepton elementary particle which has no net electric charge. Together with the electron it forms the first generation of leptons, hence the name electron neutrino. It was first hypothesized by Wolfgang Pauli in 1930, to account for missing momentum and missing energy in beta decay, and was discovered in 1956 by a team led by Clyde Cowan and Frederick Reines (see Cowan–Reines neutrino experiment).\n\nIn the early 1900s, theories predicted that the electrons resulting from beta decay should have been emitted at a specific energy. However, in 1914, James Chadwick showed that electrons were instead emitted in a continuous spectrum.\n\nIn 1930, Wolfgang Pauli theorized that an undetected particle was carrying away the observed difference between the energy, momentum, and angular momentum of the initial and final particles.\n\nOn 4 December 1930, Pauli wrote a letter to the Physical Institute of the Federal Institute of Technology, Zürich, in which he proposed the electron neutron as a potential solution to solve the problem of the continuous beta decay spectrum. An excerpt of the letter reads:\nDear radioactive ladies and gentlemen,\n\nAs the bearer of these lines [...] will explain more exactly, considering the 'false' statistics of N-14 and Li-6 nuclei, as well as the continuous \"β\"-spectrum, I have hit upon a desperate remedy to save the \"exchange theorem\" of statistics and the energy theorem. Namely [there is] the possibility that there could exist in the nuclei electrically neutral particles that I wish to call neutrons, which have spin 1/2 and obey the exclusion principle, and additionally differ from light quanta in that they do not travel with the velocity of light: The mass of the neutron must be of the same order of magnitude as the electron mass and, in any case, not larger than 0.01 proton mass. The continuous \"β\"-spectrum would then become understandable by the assumption that in \"β\" decay a neutron is emitted together with the electron, in such a way that the sum of the energies of neutron and electron is constant.\n\nBut I don't feel secure enough to publish anything about this idea, so I first turn confidently to you, dear radioactives, with a question as to the situation concerning experimental proof of such a neutron, if it has something like about 10 times the penetrating capacity of a \"γ\" ray.\n\nI admit that my remedy may appear to have a small \"a priori\" probability because neutron, if they exist, would probably have long ago been seen. However, only those who wager can win, and the seriousness of the situation of the continuous \"β\"-spectrum can be made clear by the saying of my honored predecessor in office, Mr. Debye, [...] \"One does best not to think about that at all, like the new taxes.\" [...] So, dear radioactives, put it to test and set it right. [...]\n\nWith many greetings to you, also to Mr. Back, your devoted servant,\n\nA translated reprint of the full letter can be found in the September 1978 issue of \"Physics Today\".\n\nThe electron neutrino was discovered by Clyde Cowan and Frederick Reines in 1956.\n\nPauli originally named his proposed light particle a \"neutron\". When James Chadwick discovered a much more massive nuclear particle in 1932 and also named it a neutron, this left the two particles with the same name. Enrico Fermi, who developed the theory of beta decay, introduced the term \"neutrino\" in 1934 (it was jokingly coined by Edoardo Amaldi during a conversation with Fermi at the Institute of physics of via Panisperna in Rome, in order to distinguish this light neutral particle from Chadwick's neutron) to resolve the confusion. It was a pun on neutrone, the Italian equivalent of \"neutron\": the \"-one\" ending can be an augmentative in Italian, so \"neutrone\" could be read as the \"large neutral thing\"; \"-ino\" replaces the augmentative suffix with a diminutive one.\nUpon the prediction and discovery of a second neutrino, it became important to distinguish between different types of neutrinos. Pauli's neutrino is now identified as the \"electron neutrino\", while the second neutrino is identified as the \"muon neutrino\".\n\nThe electron neutrino has a corresponding antiparticle, the electron antineutrino (), which differs only in that some of its properties have equal magnitude but opposite sign. One open question of particle physics is whether or not neutrinos and anti-neutrinos are the same particle in which case it would be a Majorana fermion or whether they are different particles in which case they would be Dirac fermions. They are produced in beta decay and other types of weak interactions.\n\n\n"}
{"id": "1839752", "url": "https://en.wikipedia.org/wiki?curid=1839752", "title": "Electrostatic precipitator", "text": "Electrostatic precipitator\n\nAn electrostatic precipitator (ESP) is a filtration device that removes fine particles, like dust and smoke, from a flowing gas using the force of an induced electrostatic charge minimally impeding the flow of gases through the unit.\n\nIn contrast to wet scrubbers which apply energy directly to the flowing fluid medium, an ESP applies energy only to the particulate matter being collected and therefore is very efficient in its consumption of energy (in the form of electricity).\n\nThe first use of corona discharge to remove particles from an aerosol was by Hohlfeld in 1824. However, it was not commercialized until almost a century later.\n\nIn 1907 Frederick Gardner Cottrell, a professor of chemistry at the University of California, Berkeley, applied for a patent on a device for charging particles and then collecting them through electrostatic attraction—the first electrostatic precipitator. Cottrell first applied the device to the collection of sulphuric acid mist and lead oxide fumes emitted from various acid-making and smelting activities. Wine-producing vineyards in northern California were being adversely affected by the lead emissions.\n\nAt the time of Cottrell's invention, the theoretical basis for operation was not understood. The operational theory was developed later in Germany, with the work of Walter Deutsch and the formation of the Lurgi company.\n\nCottrell used proceeds from his invention to fund scientific research through the creation of a foundation called Research Corporation in 1912, to which he assigned the patents. The intent of the organization was to bring inventions made by educators (such as Cottrell) into the commercial world for the benefit of society at large. The operation of Research Corporation is funded by royalties paid by commercial firms after commercialization occurs. Research Corporation has provided vital funding to many scientific projects: Goddard's rocketry experiments, Lawrence's cyclotron, production methods for vitamins A and B, among many others.\n\nBy a decision of the US Supreme Court, the Corporation had to be split into several entities. The Research Corporation was separated from two commercial firms making the hardware: Research-Cottrell Inc. (operating east of the Mississippi River) and Western Precipitation (operating in the western states). The Research Corporation continues to be active to this day, and the two companies formed to commercialize the invention for industrial and utility applications are still in business as well.\n\nElectrophoresis is the term used for migration of gas-suspended charged particles in a direct-current electrostatic field. Traditional CRT television sets tend to accumulate dust on the screen because of this phenomenon (a CRT is a direct-current machine operating at about 15 kilovolts).\n\nThe most basic precipitator contains a row of thin vertical wires, and followed by a stack of large flat metal plates oriented vertically, with the plates typically spaced about 1 cm to 18 cm apart, depending on the application. The air stream flows horizontally through the spaces between the wires, and then passes through the stack of plates.\n\nA negative voltage of several thousand volts is applied between wire and plate. If the applied voltage is high enough, an electric corona discharge ionizes the air around the electrodes, which then ionizes the particles in the air stream.\n\nThe ionized particles, due to the electrostatic force, are diverted towards the grounded plates. Particles build up on the collection plates and are removed from the air stream.\n\nA two-stage design (separate charging section ahead of collecting section) has the benefit of minimizing ozone production, which would adversely affect health of personnel working in enclosed spaces. For shipboard engine rooms where gearboxes generate an oil mist, two-stage ESP's are used to clean the air, improving the operating environment and preventing buildup of flammable oil fog accumulations. Collected oil is returned to the gear lubricating system.\n\nPrecipitator performance is very sensitive to two particulate properties: 1) Electrical resistivity; and 2) Particle size distribution. These properties can be measured economically and accurately in the laboratory, using standard tests. Resistivity can be determined as a function of temperature in accordance with IEEE Standard 548. This test is conducted in an air environment containing a specified moisture concentration. The test is run as a function of ascending or descending temperature, or both. Data is acquired using an average ash layer[further explanation needed] electric field of 4 kV/cm. Since relatively low applied voltage is used and no sulfuric acid vapor is present in the test environment, the values obtained indicate the maximum ash resistivity.\n\nIn an ESP, where particle charging and discharging are key functions, resistivity is an important factor that significantly affects collection efficiency. While resistivity is an important phenomenon in the inter-electrode region where most particle charging takes place, it has a particularly important effect on the dust layer at the collection electrode where discharging occurs. Particles that exhibit high resistivity are difficult to charge. But once charged, they do not readily give up their acquired charge on arrival at the collection electrode. On the other hand, particles with low resistivity easily become charged and readily release their charge to the grounded collection plate. Both extremes in resistivity impede the efficient functioning of ESPs. ESPs work best under normal resistivity conditions.\n\nResistivity, which is a characteristic of particles in an electric field, is a measure of a particle's resistance to transferring charge (both accepting and giving up charges). Resistivity is a function of a particle's chemical composition as well as flue gas operating conditions such as temperature and moisture. Particles can have high, moderate (normal), or low resistivity.\n\nBulk resistivity is defined using a more general version of Ohm’s Law, as given in Equation () below:\n\nA better way of displaying this would be to solve for resistivity as a function of applied voltage and current, as given in Equation () below:\n\nResistivity is the electrical resistance of a dust sample 1.0 cm in cross-sectional area, 1.0 cm thick, and is recorded in units of ohm-cm. A method for measuring resistivity will be described in this article. The table below, gives value ranges for low, normal, and high resistivity.\n\nResistance affects electrical conditions in the dust layer by a potential electric field (voltage drop) being formed across the layer as negatively charged particles arrive at its surface and leak their electrical charges to the collection plate. At the metal surface of the electrically grounded collection plate, the voltage is zero, whereas at the outer surface of the dust layer, where new particles and ions are arriving, the electrostatic voltage caused by the gas ions can be quite high. The strength of this electric field depends on the resistance and thickness of the dust layer.\n\nIn high-resistance dust layers, the dust is not sufficiently conductive, so electrical charges have difficulty moving through the dust layer. Consequently, electrical charges accumulate on and beneath the dust layer surface, creating a strong electric field.\n\nVoltages can be greater than 10,000 volts. Dust particles with high resistance are held too strongly to the plate, making them difficult to remove and causing rapping problems.\n\nIn low resistance dust layers, the corona current is readily passed to the grounded collection electrode. Therefore, a relatively weak electric field, of several thousand volts, is maintained across the dust layer. Collected dust particles with low resistance do not adhere strongly enough to the collection plate. They are easily dislodged and become retained in the gas stream.\n\nThe electrical conductivity of a bulk layer of particles depends on both surface and volume factors. Volume conduction, or the motions of electrical charges through the interiors of particles, depends mainly on the composition and temperature of the particles. In the higher temperature regions, above , volume conduction controls the conduction mechanism. Volume conduction also involves ancillary factors, such as compression of the particle layer, particle size and shape, and surface properties.\n\nVolume conduction is represented in the figures as a straight-line at temperatures above . At temperatures below about , electrical charges begin to flow across surface moisture and chemical films adsorbed onto the particles. Surface conduction begins to lower the resistivity values and bend the curve downward at temperatures below .\n\nThese films usually differ both physically and chemically from the interiors of the particles owing to adsorption phenomena. Theoretical calculations indicate that moisture films only a few molecules thick are adequate to provide the desired surface conductivity. Surface conduction on particles is closely related to surface-leakage currents occurring on electrical insulators, which have been extensively studied. An interesting practical application of surface-leakage is the determination of dew point by measurement of the current between adjacent electrodes mounted on a glass surface. A sharp rise in current signals the formation of a moisture film on the glass. This method has been used effectively for determining the marked rise in dew point, which occurs when small amounts of sulfuric acid vapor are added to an atmosphere (commercial Dewpoint Meters are available on the market).\n\nThe following discussion of normal, high, and low resistance applies to ESPs operated in a dry state; resistance is not a problem in the operation of wet ESPs because of the moisture concentration in the ESP. The relationship between moisture content and resistance is explained later in this work.\n\nAs stated above, ESPs work best under normal resistivity conditions. Particles with normal resistivity do not rapidly lose their charge on arrival at the collection electrode. These particles slowly leak their charge to grounded plates and are retained on the collection plates by intermolecular adhesive and cohesive forces. This allows a particulate layer to be built up and then dislodged from the plates by rapping. Within the range of normal dust resistivity (between 10 and 2 x 10 ohm-cm), fly ash is collected more easily than dust having either low or high resistivity.\n\nIf the voltage drop across the dust layer becomes too high, several adverse effects can occur. First, the high voltage drop reduces the voltage difference between the discharge electrode and collection electrode, and thereby reduces the electrostatic field strength used to drive the gas ion-charged particles over to the collected dust layer. As the dust layer builds up, and the electrical charges accumulate on the surface of the dust layer, the voltage difference between the discharge and collection electrodes decreases. The migration velocities of small particles are especially affected by the reduced electric field strength.\n\nAnother problem that occurs with high resistivity dust layers is called back corona. This occurs when the potential drop across the dust layer is so great that corona discharges begin to appear in the gas that is trapped within the dust layer. The dust layer breaks down electrically, producing small holes or craters from which back corona discharges occur. Positive gas ions are generated within the dust layer and are accelerated toward the \"negatively charged\" discharge electrode. The positive ions reduce some of the negative charges on the dust layer and neutralize some of the negative ions on the \"charged particles\" heading toward the collection electrode. Disruptions of the normal corona process greatly reduce the ESP's collection efficiency, which in severe cases, may fall below 50% . When back corona is present, the dust particles build up on the electrodes forming a layer of insulation. Often this can not be repaired without bringing the unit offline.\n\nThe third, and generally most common problem with high resistivity dust is increased electrical sparking. When the sparking rate exceeds the \"set spark rate limit,\" the automatic controllers limit the operating voltage of the field. This causes reduced particle charging and reduced migration velocities toward the collection electrode. High resistivity can generally be reduced by doing the following:\n\n\nThin dust layers and high-resistivity dust especially favor the formation of back corona craters. Severe back corona has been observed with dust layers as thin as 0.1 mm, but a dust layer just over one particle thick can reduce the sparking voltage by 50%. The most marked effects of back corona on the current-voltage characteristics are:\n\n\nThe Figure below and to the left shows the variation in resistivity with changing gas temperature for six different industrial dusts along with three coal-fired fly ashes. The Figure on the right illustrates resistivity values measured for various chemical compounds that were prepared in the laboratory.\n\nResults for Fly Ash A (in the figure to the left) were acquired in the ascending temperature mode. These data are typical for a moderate to high combustibles content ash. Data for Fly Ash B are from the same sample, acquired during the descending temperature mode.\n\nThe differences between the ascending and descending temperature modes are due to the presence of unburned combustibles in the sample. Between the two test modes, the samples are equilibrated in dry air for 14 hours (overnight) at . This overnight annealing process typically removes between 60% and 90% of any unburned combustibles present in the samples. Exactly how carbon works as a charge carrier is not fully understood, but it is known to significantly reduce the resistivity of a dust.\n\nCarbon can act, at first, like a high resistivity dust in the precipitator. Higher voltages can be required in order for corona generation to begin. These higher voltages can be problematic for the TR-Set controls. The problem lies in onset of corona causing large amounts of current to surge through the (low resistivity) dust layer. The controls sense this surge as a spark. As precipitators are operated in spark-limiting mode, power is terminated and the corona generation cycle re-initiates. Thus, lower power (current) readings are noted with relatively high voltage readings.\n\nThe same thing is believed to occur in laboratory measurements. Parallel plate geometry is used in laboratory measurements without corona generation. A stainless steel cup holds the sample. Another stainless steel electrode weight sits on top of the sample (direct contact with the dust layer). As voltage is increased from small amounts (e.g. 20 V), no current is measured. Then, a threshold voltage level is reached. At this level, current surges through the sample... so much so that the voltage supply unit can trip off. After removal of the unburned combustibles during the above-mentioned annealing procedure, the descending temperature mode curve shows the typical inverted “V” shape one might expect.\n\nParticles that have low resistivity are difficult to collect because they are easily charged (very conductive) and rapidly lose their charge on arrival at the collection electrode. The particles take on the charge of the collection electrode, bounce off the plates, and become re-entrained in the gas stream. Thus, attractive and repulsive electrical forces that are normally at work at normal and higher resistivities are lacking, and the binding forces to the plate are considerably lessened. Examples of low-resistivity dusts are unburned carbon in fly ash and carbon black.\n\nIf these conductive particles are coarse, they can be removed upstream of the precipitator by using a device such as a cyclone mechanical collector.\n\nThe addition of liquid ammonia () into the gas stream as a conditioning agent has found wide use in recent years. It is theorized that ammonia reacts with contained in the flue gas to form an ammonium sulfate compound that increases the cohesivity of the dust. This additional cohesivity makes up for the loss of electrical attraction forces.\n\nThe table below summarizes the characteristics associated with low, normal and high resistivity dusts.\n\nThe moisture content of the flue gas stream also affects particle resistivity. Increasing the moisture content of the gas stream by spraying water or injecting steam into the duct work preceding the ESP lowers the resistivity. In both temperature adjustment and moisture conditioning, one must maintain gas conditions above the dew point to prevent corrosion problems in the ESP or downstream equipment. The figure to the right shows the effect of temperature and moisture on the resistivity of a cement dust. As the percentage of moisture in the gas stream increases from 6 to 20%, the resistivity of the dust dramatically decreases. Also, raising or lowering the temperature can decrease cement dust resistivity for all the moisture percentages represented.\n\nThe presence of in the gas stream has been shown to favor the electrostatic precipitation process when problems with high resistivity occur. Most of the sulfur content in the coal burned for combustion sources converts to . However, approximately 1% of the sulfur converts to . The amount of in the flue gas normally increases with increasing sulfur content of the coal. The resistivity of the particles decreases as the sulfur content of the coal increases.\n\nOther conditioning agents, such as sulfuric acid, ammonia, sodium chloride, and soda ash (sometimes as raw trona), have also been used to reduce particle resistivity. Therefore, the chemical composition of the flue gas stream is important with regard to the resistivity of the particles to be collected in the ESP. The table below lists various conditioning agents and their mechanisms of operation.\nIf injection of ammonium sulfate occurs at a temperature greater than about , dissociation into ammonia and sulfur trioxide results. Depending on the ash, may preferentially interact with fly ash as conditioning. The remainder recombines with ammonia to add to the space charge as well as increase cohesiveness of the ash.\n\nMore recently, it has been recognized that a major reason for loss of efficiency of the electrostatic precipitator is due to particle buildup on the charging wires in addition to the collection plates (Davidson and McKinney, 1998). This is easily remedied by making sure that the wires themselves are cleaned at the same time that the collecting plates are cleaned.\n\nSulfuric acid vapor () enhances the effects of water vapor on surface conduction. It is physically adsorbed within the layer of moisture on the particle surfaces. The effects of relatively small amounts of acid vapor can be seen in the figure below and to the right.\n\nThe inherent resistivity of the sample at is 5×10 ohm-cm. An equilibrium concentration of just 1.9 ppm sulfuric acid vapor lowers that value to about 7 x 10 ohm-cm.\n\nESPs continue to be excellent devices for control of many industrial particulate emissions, including smoke from electricity-generating utilities (coal and oil fired), salt cake collection from black liquor boilers in pulp mills, and catalyst collection from fluidized bed catalytic cracker units in oil refineries to name a few. These devices treat gas volumes from several hundred thousand ACFM to 2.5 million ACFM (1,180 m³/s) in the largest coal-fired boiler applications. For a coal-fired boiler the collection is usually performed downstream of the air preheater at about which provides optimal resistivity of the coal-ash particles. For some difficult applications with low-sulfur fuel hot-end units have been built operating above .\n\nThe original parallel plate–weighted wire design has evolved as more efficient (and robust) discharge electrode designs were developed, today focusing on rigid (pipe-frame) discharge electrodes to which many sharpened spikes are attached (barbed wire), maximizing corona production. Transformer-rectifier systems apply voltages of at relatively high current densities. Modern controls, such as an automatic voltage control, minimize electric sparking and prevent arcing (sparks are quenched within 1/2 cycle of the TR set), avoiding damage to the components. Automatic plate-rapping systems and hopper-evacuation systems remove the collected particulate matter while on line, theoretically allowing ESPs to stay in continuous operation for years at a time.\n\nElectrostatic precipitators can be used to sample biological airborne particles or aerosol for analysis. Sampling for bioaerosols requires precipitator designs optimised with a liquid counter electrode, which can be used to sample biological particles, e.g. viruses, directly into a small liquid volume to reduce unnecessary sample dilution. See Bioaerosols for more details.\n\nA wet electrostatic precipitator (WESP or wet ESP) operates with water vapor saturated air streams (100% relative humidity). WESPs are commonly used to remove liquid droplets such as sulfuric acid mist from industrial process gas streams. The WESP is also commonly used where the gases are high in moisture content, contain combustible particulate, or have particles that are sticky in nature.\n\nThe preferred and most modern type of WESP is a downflow tubular design. This design allows the collected moisture and particulate to form a moving slurry that helps to keep the collection surfaces clean. Plate style and upflow design WESPs are very unreliable and should not be used in applications where particulate is sticky in nature.\n\nPlate precipitators are commonly marketed to the public as air purifier devices or as a permanent replacement for furnace filters, but all have the undesirable attribute of being somewhat messy to clean. A negative side-effect of electrostatic precipitation devices is the potential production of toxic ozone and . However, electrostatic precipitators offer benefits over other air purifications technologies, such as HEPA filtration, which require expensive filters and can become \"production sinks\" for many harmful forms of bacteria.\n\nWith electrostatic precipitators, if the collection plates are allowed to accumulate large amounts of particulate matter, the particles can sometimes bond so tightly to the metal plates that vigorous washing and scrubbing may be required to completely clean the collection plates. The close spacing of the plates can make thorough cleaning difficult, and the stack of plates often cannot be easily disassembled for cleaning. One solution, suggested by several manufacturers, is to wash the collector plates in a dishwasher.\n\nSome consumer precipitation filters are sold with special soak-off cleaners, where the entire plate array is removed from the precipitator and soaked in a large container overnight, to help loosen the tightly bonded particulates.\n\nA study by the Canada Mortgage and Housing Corporation testing a variety of forced-air furnace filters found that ESP filters provided the best, and most cost-effective means of cleaning air using a forced-air system.\n\nThe first portable electrostatic air filter systems for homes was marketed in 1954 by Raytheon.\n"}
{"id": "261316", "url": "https://en.wikipedia.org/wiki?curid=261316", "title": "Energy crisis", "text": "Energy crisis\n\nAn energy crisis is any significant bottleneck in the supply of energy resources to an economy. In literature, it often refers to one of the energy sources used at a certain time and place, in particular those that supply national electricity grids or those used as fuel in vehicles.\n\nIndustrial development and population growth have led to a surge in the global demand for energy in recent years. In the 2000s, this new demand — together with Middle East tension, the falling value of the U.S. dollar, dwindling oil reserves, concerns over peak oil, and oil price speculation — triggered the 2000s energy crisis, which saw the price of oil reach an all-time high of $147.30 a barrel in 2008.\n\nMost energy crisis have been caused by localized shortages, wars and market manipulation. Some have argued that government actions like tax hikes, nationalisation of energy companies, and regulation of the energy sector, shift supply and demand of energy away from its economic equilibrium. However, the recent historical energy crisis listed below were not caused by such factors. Market failure is possible when monopoly manipulation of markets occurs. A crisis can develop due to industrial actions like union organized strikes and government embargoes. The cause may be over-consumption, aging infrastructure, choke point disruption or bottlenecks at oil refineries and port facilities that restrict fuel supply. An emergency may emerge during very cold winters due to increased consumption of energy.\n\nLarge fluctuations and manipulations in future derivatives can have a substantial impact on price. Large investment banks control 80% of oil derivatives as of May 2012, compared to 30% only a decade ago. This increase contributed to an improvement of global energy output from 117 687 TWh in 2000 to 143 851TWh in 2008. Limitations on free trade for derivatives could reverse this trend of growth in energy production. Kuwaiti Oil Minister Hani Hussein stated that \"Under the supply and demand theory, oil prices today are not justified,\" in an interview with Upstream.\n\nPipeline failures and other accidents may cause minor interruptions to energy supplies. A crisis could possibly emerge after infrastructure damage from severe weather. Attacks by terrorists or militia on important infrastructure are a possible problem for energy consumers, with a successful strike on a Middle East facility potentially causing global shortages. Political events, for example, when governments change due to regime change, monarchy collapse, military occupation, and coup may disrupt oil and gas production and create shortages. Fuel shortage can also be due to the excess and useless use of the fuels.\n\n\n\n“Peak oil” is the period when the maximum rate of global petroleum extraction is reached, after which the rate of production enters terminal decline. It relates to a long-term decline in the available supply of petroleum. This, combined with increasing demand, significantly increases the worldwide prices of petroleum derived products. Most significant is the availability and price of liquid fuel for transportation.\n\nThe US Department of Energy in the Hirsch report indicates that “The problems associated with world oil production peaking will not be temporary, and past 'energy crisis' experience will provide relatively little guidance.”\n\nTo avoid the serious social and economic implications a global decline in oil production could entail, the 2005 Hirsch report emphasized the need to find alternatives, at least ten to twenty years before the peak, and to phase out the use of petroleum over that time. Such mitigation could include energy conservation, fuel substitution, and the use of unconventional oil. Because mitigation can reduce the use of traditional petroleum sources, it can also affect the timing of peak oil and the shape of the Hubbert curve.\n\nEnergy policy may be reformed leading to greater energy intensity, for example in Iran with the 2007 Gas Rationing Plan in Iran, Canada and the National Energy Program and in the USA with the \"Energy Independence and Security Act of 2007\" also called the \"Clean Energy Act of 2007\". Another mitigation measure is the setup of a cache of secure fuel reserves like the United States Strategic Petroleum Reserve, in case of national emergency. Chinese energy policy includes specific targets within their 5-year plans.\n\nAndrew McKillop has been a proponent of a contract and converge model or capping scheme, to mitigate both emissions of greenhouse gases and a peak oil crisis. The imposition of a carbon tax would have mitigating effects on an oil crisis. The Oil Depletion Protocol has been developed by Richard Heinberg to implement a powerdown during a peak oil crisis. While many sustainable development and energy policy organisations have advocated reforms to energy development from the 1970s, some cater to a specific crisis in energy supply including Energy-Quest and the International Association for Energy Economics. The Oil Depletion Analysis Centre and the Association for the Study of Peak Oil and Gas examine the timing and likely effects of peak oil.\n\nEcologist William Rees believes that\nDue to a lack of political viability on the issue, government mandated fuel prices hikes are unlikely and the unresolved dilemma of fossil fuel dependence is becoming a wicked problem. A global soft energy path seems improbable, due to the rebound effect. Conclusions that the world is heading towards an unprecedented large and potentially devastating global energy crisis due to a decline in the availability of cheap oil lead to calls for a decreasing dependency on fossil fuel.\n\nOther ideas concentrate on design and development of improved, energy-efficient urban infrastructure in developing nations. Government funding for alternative energy is more likely to increase during an energy crisis, so too are incentives for oil exploration. For example, funding for research into inertial confinement fusion technology increased during the 1970s.\n\nKirk Sorensen and others have suggested that additional nuclear power plants, particularly liquid fluoride thorium reactors have the energy density to mitigate global warming and replace the energy from peak oil, peak coal and peak gas. The reactors produce electricity and heat so much of the transportation infrastructure should move over to electric vehicles. However, the high process heat of the molten salt reactors could be used to make liquid fuels from any carbon source.\n\nRather counterintuitively, the world economy has had to deal with the unforeseen consequences of the 2015-2016 oil glut also known as 2010s oil glut, a major energy crisis that took many experts by surprise. This oversupply crisis started with a considerable time-lag, more than six years after the beginning of the Great Recession: \"\"the price of oil\" [had] \"stabilized at a relatively high level (around $100 a barrel) unlike all previous recessionary cycles since 1980 (start of First Persian Gulf War). But nothing guarantee[d] such price levels in perpetuity\"\".\n\nThe macroeconomic implications of a supply shock-induced energy crisis are large, because energy is the resource used to exploit all other resources. When energy markets fail, an energy shortage develops. Electricity consumers may experience intentionally engineered rolling blackouts during periods of insufficient supply or unexpected power outages, regardless of the cause.\n\nIndustrialized nations are dependent on oil, and efforts to restrict the supply of oil would have an adverse effect on the economies of oil producers. For the consumer, the price of natural gas, gasoline (petrol) and diesel for cars and other vehicles rises. An early response from stakeholders is the call for reports, investigations and commissions into the price of fuels. There are also movements towards the development of more sustainable urban infrastructure.\n\nIn the market, new technology and energy efficiency measures become desirable for consumers seeking to decrease transport costs. January 30, 2008 Planet Ark. Examples include:\n\nOther responses include the development of unconventional oil sources such as synthetic fuel from places like the Athabasca Oil Sands, more renewable energy commercialization and use of alternative propulsion. There may be a Relocation trend towards local foods and possibly microgeneration, solar thermal collectors and other green energy sources.\n\nTourism trends and gas-guzzler ownership varies with fuel costs. Energy shortages can influence public opinion on subjects from nuclear power plants to electric blankets. Building construction techniques—improved insulation, reflective roofs, thermally efficient windows, etc.—change to reduce heating costs.\nAn electricity shortage is felt most acutely in heating, cooking, and water supply. Therefore, a sustained energy crisis may become a humanitarian crisis.\n\nIf an energy shortage is prolonged a crisis management phase is enforced by authorities. Energy audits may be conducted to monitor usage. Various curfews with the intention of increasing energy conservation may be initiated to reduce consumption. For example, to conserve power during the Central Asia energy crisis, authorities in Tajikistan ordered bars and cafes to operate by candlelight.\n\nIn the worst kind of energy crisis energy rationing and fuel rationing may be incurred. Panic buying may beset outlets as awareness of shortages spread. Facilities close down to save on heating oil; and factories cut production and lay off workers. The risk of stagflation increases.\n\nFictional scenarios have been explored in:\n\n\n\n\n"}
{"id": "152905", "url": "https://en.wikipedia.org/wiki?curid=152905", "title": "Ensete", "text": "Ensete\n\nEnsete is a genus of monocarpic flowering plants native to tropical regions of Africa and Asia. It is one of the two genera in the banana family, Musaceae, and includes the false banana or enset (\"E. ventricosum\"), an economically important food crop in Ethiopia.\n\nThe genus \"Ensete\" was first described by Paul Fedorowitsch Horaninow (1796-1865) in his \"Prodromus Monographiae Scitaminarum\" of 1862 in which he created a single species, \"Ensete edule\". However, the genus did not receive general recognition until 1947 when it was revived by E. E. Cheesman in the first of a series of papers in the \"Kew Bulletin\" on the classification of the bananas, with a total of 25 species.\n\nTaxonomically, the genus \"Ensete\" has shrunk since Cheesman revived the taxon. Cheesman acknowledged that field study might reveal synonymy and the most recent review of the genus by Simmonds (1960) listed just six. Recently the number has increased to seven as the Flora of China has, not entirely convincingly, reinstated \"Ensete wilsonii\". There is one species in Thailand, somewhat resembling \"E. superbum\", that has not been formally described, and possibly other Asian species.\n\nIt is possible to separate \"Ensete\" into its African and Asian species.\n\n\n\n\n"}
{"id": "33646775", "url": "https://en.wikipedia.org/wiki?curid=33646775", "title": "Foster Natural Gas/Oil Report", "text": "Foster Natural Gas/Oil Report\n\nThe Foster Natural Gas/Oil Report, formerly known as the Foster Natural Gas Report and Foster Associates Report, is a U.S.-based weekly newsletter published by Foster Associates, Inc. It was founded in Washington, D.C. on March 23, 1956 by J. Rhoades Foster and a group of economists. Its editor-in-chief is Edgar D. Boshart. \n\nThe report publishes news about issues and events relevant to the regulated natural gas and oil market in North America. Topics of interest include production, marketing, transportation, distribution and end use. It reviews activities at the Federal Energy Regulatory Commission (FERC) and the National Energy Board (NEB) of Canada and well as the public hearings and rulings of state regulatory agencies. The report also covers leaders in politics, industry and regulatory entities related to the natural gas industry and public utility companies.\n\n"}
{"id": "441654", "url": "https://en.wikipedia.org/wiki?curid=441654", "title": "Fuel protests in the United Kingdom", "text": "Fuel protests in the United Kingdom\n\nThe fuel protests in the United Kingdom were a series of campaigns held because of the cost of rising petrol and diesel fuel prices for road vehicle use. There have been three notable campaigns amongst many other protests in the 21st century. The first major protest in 2000 was primarily led by independent truck owner-operators. One group of truck owner-operators from the South East of England formed a protest group called \"TransAction\" that protested at oil refineries and fuel depots in Essex. Protests and blockades of oil facilities caused widespread disruption to the supply of petroleum products. The aim of the protests was to secure a reduction in the fuel duty rate on petrol and diesel, which the government refused to enact. After the protest ended, the government did announce a freeze on fuel duties, and promised changes would be made to the way that goods vehicles were taxed, which would include the taxing of foreign vehicles operating on British roads.\n\nSubsequent protests have not had as significant an impact but did result in panic buying in 2005 and again in 2007. The 2007 fuel protests were put together by several members of the old \"TransAction group\" from the South of England, reforming as \"TransAction 2007\". The protests of this period were not widely supported nor did they cause the same disruption as those in previous years. The 2007 protest which took place at the latter end of the year, culminated in 200–300 trucks descending on Central London with the police closing off the A40 fly-over for use as a truck park for the day. Following this, a rally was held at Marble Arch. Later in the day, a deputation went to 10 Downing Street to deliver a formal petition calling for the reduction of UK fuel duty. There were those who felt that the 2007 fuel protests were politically motivated, given that many truck owner operators and farmers would have supported the Conservative Party. The Conservative leader David Cameron gave them his support and promised a “fair fuel stabiliser”, a proposal to limit the price of petrol that was part of the Conservative manifesto for the 2010 UK general election and was announced to be implemented following the budget of March 2011. The “fair fuel stabiliser” which was meant to lower taxes levied on fuel as the price rose and raise tax as the price fell, in fact tax will still rise as the oil price rises but the tax will be capped at the level of inflation at that time and will be applied twice a year, but when the oil price is falling then the tax can be greater than inflation. Quote from \"Overview of Tax Legislation and Rates\" section 3.44: \"When oil prices are high, as now, fuel duty will increase by the retail prices index (RPI). However, if the oil price falls below a set trigger price on a sustained basis, the Government will increase fuel duty by RPI plus 1 penny per litre. The Government believes that a trigger price of $75 per barrel would be appropriate, and will set a final trigger price and mechanism after seeking the views of oil and gas companies and motoring groups\".\n\nIn the United Kingdom, tax on fuel for road use is made up of two elements—fuel duty and value added tax (VAT). Fuel duty is applied at a fixed amount per litre by fuel type, and VAT is then added as a percentage of the combined total of the cost of the fuel and the fuel duty. Historically, fuel duty was increased annually, broadly in line with inflation. In 1993 the fuel price escalator was introduced by the Conservatives, justified as being designed to encourage less motor vehicle use, and thus combat climate change. The idea was to annually increase fuel duty, initially at 3%, later rising to 5%, above the rate of inflation. The Blair administration then increased the rate at which the escalator exceeded inflation to 6%.\n\nBy 2000, tax accounted for 81.5% of the total cost of unleaded petrol, up from 72.8% in 1993. Fuel prices in the UK had risen from being amongst the cheapest in Europe to being the most expensive in the same time frame. The protesters said that higher transport costs in the UK were making it difficult for haulage industry to remain competitive. The worldwide price of oil had increased from $10 to $30 a barrel, the highest level in 10 years. Drivers in the UK were now paying an average of 80 pence a litre for unleaded and 80.8p for diesel. The government had already abandoned the fuel tax escalator in early 2000.\n\nIn 1999, lorry drivers had undertaken protests in London against rising fuel prices and announced their intentions for a nationwide campaign. The Conservative Party organised a day of protest on 29 July 2000 to draw attention to how fuel prices had increased under Labour, visiting town centres with petitions and distributing leaflets. The Boycott the Pumps campaign, also referred to as Dump the Pumps, was organised for 1 August 2000, with motorists being urged not to visit petrol stations on that day. Support for the day was reported to be patchy, with forecourts in the North-West being hit the hardest, some reporting a 50% drop in business.\n\nOn 8 September 2000, the Stanlow Refinery near Ellesmere Port in Cheshire was blockaded by Farmers for Action, led by David Handley. Over the next few days, pickets were reported at Milford Haven and an oil terminal at Avonmouth causing some petrol stations to run out of supplies. On 8 September 2000, fuel protesters blockaded several facilities for a limited period and disrupted fuel supplies to Yorkshire, North West England, and the Scottish Borders and demanding that the government reduce fuel taxes. Some of the protesters called for a reduction of between 15 and 26 pence per litre in duties.\n\nThe protests spread so that on 10 September 2000 they included facilities at the Manchester Fuels Terminal, Kingsbury Oil Terminal, the largest inland oil terminal, and at Cardiff Docks. Panic buying of petrol began to close some petrol stations as motorists queued for fuel which was beginning to be rationed and reports of garages increasing their prices substantially. Rolling roadblocks were also reported in North East England on the A1 and A55 roads. On 11 September 2000, the government obtained an Order in Council which was authorised by the Privy Council and the Queen to take emergency powers under the Energy Act 1976 to ensure delivery of fuel to essential services. By now six of the nine refineries and four oil distribution depots were subject to protests.\nBy Tuesday 12 September 2000, 3,000 petrol stations were reported to be closed due to a lack of fuel. There were also reports that there would be no fuel left within 48 hours. Tony Blair, the Prime Minister put the oil companies under pressure to resume deliveries. BP said that they would resume deliveries if police escorts were given to the tankers. Tony Blair had been in contact with the oil companies during the day and announced that supplies would be back to normal within 24 hours, with the oil companies having been ordered under the government's powers to commence deliveries to the emergency services. At the same time BBC News reported that the government's COBRA committee had drawn up plans to deal with the crisis, including using the military to assist in moving supplies and restricting the sale of fuel. South West Trains were reported to be reducing some of their services to preserve fuel supplies. Deliberately slow-moving convoys of lorries caused traffic jams on the M1 and M5 motorways.\n\nOn 13 September 2000 the government announced that 5% of normal fuel deliveries were made, however other reports indicated that only 3.8% amounting to compared with a normal daily sale of . In Scotland only very limited supplies were being delivered for emergency use only. Three-quarters of petrol stations were reported to be without fuel. Some NHS trusts cancelled non-essential operations due to staff difficulties in reaching work and ambulances were only able to answer emergency calls in most parts of the UK. The National Blood Service reported that it was coping and blood supplies to hospitals were not under threat but said that there \"were some significant problems in some parts of the country\". The government placed the National Health Service (NHS) on red alert. Supermarkets began rationing food due to difficulties in getting food deliveries through and there were reports of panic buying. Sainsbury's warned that they would run out of food within days having seen a 50% increase in their sales over the previous two days; Tesco and Safeway stated that they were rationing some items. The Royal Mail also reported they didn't have enough fuel supplies to maintain deliveries and that schools began to close. The government began deploying military tankers around the country and designated 2,000 petrol stations to receive supplies for essential services. Some deliveries commenced from the refineries and the police supplied escorts as required to ensure that tankers could move.\n\nOn 14 September 2000, the protests began to end. Several blockades of refineries were still in operation and the first deliveries were sent to designated distribution points under the emergency powers obtained by the government. Bus companies had warned that diesel stocks were running out and that services would need to be restricted to extend supplies. The protesters said that they were giving the government sixty days to act on the issue or they would protest further. A planned protest by truck drivers in London was contained by the Metropolitan Police and did not cause disruption. A later report following an analysis of the automated counting equipment on the road network the Department for Environment, Transport and the Regions showed that at the protest's peak, 14 September, car flows on UK motorways was 39% below normal levels and on major roads 25% below. However, for road haulage the numbers showed a smaller decline of 13% on both motorways and major roads.\n\nBy 16 September 2000, supplies were beginning to be restored, at first only to the government designated petrol stations, the number of which had risen to 3,300. The London Chamber of Commerce reported that the protests cost businesses £250 million a day. After the protests had ended the Institute of Directors estimated the cost to UK businesses at £1billion.\n\nThe conditions which catalysed and sustained the fuel protests of 2000 can be understood in terms of social movement theory, for example the existence of pre-existing social networks, capacity and resources.\n\nDuring the protests the oil companies were accused of collusion with the protesters by members of the government and its advisors. It was reported that the police had kept the roads clear yet tankers were being kept in the depots and not delivering petrol. The Transport and General Workers Union said that there had been incidents of intimidation against drivers of the fuel tankers. The possibility of court injunctions against the protesters was explored by TotalElfFina who received legal advice that it would be difficult to obtain and enforce one as there was not a named individual on which to serve the injunction. The company also stated that even if roads were clear, delivering fuel might change the mood of the protesters which had been \"amicable\" and that \"Getting fuel to the pumps would only solve the short-term problem and not deal with the original concerns of protesters\". The TGWU subsequently called for a public inquiry into reports of collusion between the demonstrators and the oil companies, saying that they had evidence of protestors being allowed access to the oil companies' sites without security checks and that drivers who had been willing to deliver fuel being told not to.\nThe government stated that they would not back down in the face of protests or introduce an emergency budget. The government argued that the rise in prices was due to increases in the world oil market prices and not the government's fuel duty. Whilst agreeing that the government could not make policy in response to the blockades, William Hague, Leader of the Opposition criticised the government for having increased taxes, whilst the Liberal Democrats argued that the government should have responded to the protest much earlier. The Amalgamated Engineering and Electrical Union called for a reduction in fuel duties during the action.\n\nA BBC opinion poll conducted by ICM of 514 people by telephone showed that the public support on 12 September 2000 for the protesters stood at 78% until the possibility of essential services being affected when it fell to 36%. An opinion poll for the Daily Mail of 502 people showed that over three quarters thought the government had handled the crisis badly. Two opinion polls shortly after the protests had ended showed the Conservative Party had overtaken or reached equal standing with the governing Labour Party. By November support for the renewal of protests and the revival of the Conservative's fortunes had both been reduced, with Labour retaking a poll lead.\n\nIn his pre-Budget report of 8 November 2000, the Chancellor, Gordon Brown, announced numerous changes which could ease the tax burden for motorists, and which included the taxing of foreign lorries using British roads. These changes included a cut in duty on ultra-low sulphur petrol, a freeze on fuel duty for other grades of fuel until at least April 2002 (effectively ending the fuel duty escalator), placing more vehicles into the lower vehicle excise duty (VED) band, an average cut of more than 50% on VED for lorries, and a \"Brit Disc\" vignette scheme requiring all lorries, including those from overseas, to pay tax to use British roads. The fuel duty freeze has been estimated to have cost the Treasury £2billion pounds annually in a 2004 report by the Economic and Social Research Council.\n\nA renewed protest that same month, involving a convoy from North East England to London, did not produce the same level of support or disruption as before. It ended with a protest in Hyde Park and the closure of the Westway by vehicles left parked on it. A similar protest from John O'Groats to Edinburgh resulted in around 80 vehicles congregating in the centre of the Scottish capital. There had been some panic buying of petrol due to this protest and there were temporary closures of some petrol stations.\n\nBrynle Williams who was one of the organisers of the protests later became a member of the Welsh Assembly for the Conservative party.\n\nIn August 2005, petrol increased in price to record highs of over 90 pence, with a small number of stations charging over £1 a litre. In September the average price had reached 94.6p a litre, with the rise being partially blamed on decreased world supply after Hurricane Katrina caused damage to some oil facilities in the United States of America.\n\nThe BBC reported on 7 September 2005 that the group responsible for the blockades in September 2000 was threatening to stage protests at oil refineries from 0600 BST on 14 September 2005 unless reductions in fuel duty were made. Newspapers reported that on 10 September 2005, the government had drawn up contingency plans to maintain the supply of fuel, including using 1000 army drivers to operate tankers, introducing fuel rationing and confiscating the driving licences of those who broke the law. Panic buying was reported on 13 September 2005 as drivers stocked up on fuel with drivers reported to be waiting an hour to fill their vehicles with petrol. At its height, around 3,000 petrol stations were emptied of fuel.\n\nHowever, on 14 September 2005, only a small number of protesters arrived at the refineries with no intention to start blockading the entrances. The UK Petroleum Industry Association said the day's protest had proved \"thankfully amazingly quiet\", with the largest event attended by People's Fuel Lobby leader Andrew Spence, attracting just 10 protesters at its peak. At the Stanlow Refinery, which was blockaded in 2000 only two protesters attended the demonstration. Further protests on 16 September 2005 occurred on the M4 motorway where lorries drove as slow as .\n\nIn responding to the protests, the government argued that lower than needed supplies by OPEC and the Katrina hurricane had a more significant impact on the price of fuel than the level of duty.\n\nTowards the end of 2007, fuel prices exceeded £1 per litre with a 2 pence rise in fuel tax in October, resulting in the highest diesel prices and the fourth highest for petrol in Europe. New protests were planned by two unconnected groups, one called Transaction 2007 and the Road Haulage Association (RHA). The Scottish branch of the RHA proposed a rolling roadblock by around 30 vehicles, whereas Transaction 2007 intended to protest outside oil refineries. Whilst the rolling road block attracted 45 vehicles driving at around on several motorways, the level of protest at oil refineries was lower than in 2000. One of the campaign aims of the RHA was the introduction of a fuel price regulator who would control duty during periods which was supported by Alex Salmond, First Minister of Scotland.\n\n"}
{"id": "27221296", "url": "https://en.wikipedia.org/wiki?curid=27221296", "title": "Hakavik Power Station", "text": "Hakavik Power Station\n\nHakavik Power Station is a 7 MW hydroelectric power plant at Øvre Eiker in Buskerud, Norway, located 25 metres above sea level.\n\nThe power station was inaugurated in 1922. It generates only railway traction current, single phase 55 kV at 16 2/3 hertz. The station is the starting point of a powerline to Sande and another to Sundhaugen Switching Station, where a 55 kV-line to Asker and another one to Neslandsvatn over Nordagutu and Skollenberg departs. The only other power station in Norway to do this is Kjofossen Power Station.\n\n\n"}
{"id": "85331", "url": "https://en.wikipedia.org/wiki?curid=85331", "title": "High pressure", "text": "High pressure\n\nIn science and engineering the study of high pressure examines its effects on materials and the design and construction of devices, such as a diamond anvil cell, which can create high pressure. By \"high pressure\" is usually meant pressures of thousands (kilobars) or millions (megabars) of times atmospheric pressure (about 1 bar or 100,000 Pa).\n\nPercy Williams Bridgman received a Nobel Prize in 1946 for advancing this area of physics by several magnitudes of pressure (400 MPa to 40,000 MPa). The list of founding fathers of this field includes also the names of Harry George Drickamer, Tracy Hall, Francis P. Bundy, Leonid F. Vereschagin, and Sergey M. Stishov.\n\nIt was by applying high pressure as well as high temperature to carbon that man-made diamonds were first produced as well as many other interesting discoveries. Almost any material when subjected to high pressure will compact itself into a denser form, for example, quartz, also called silica or silicon dioxide will first adopt a denser form known as coesite, then upon application of even higher pressure, form stishovite. These two forms of silica were first discovered by high-pressure experimenters, but then found in nature at the site of a meteor impact.\n\nChemical bonding is likely to change under high pressure, when the P*V term in the free energy becomes comparable to the energies of typical chemical bonds – i.e. at around 100 GPa. Among the most striking changes are metallization of oxygen at 96 GPa (rendering oxygen a superconductor), and transition of sodium from a nearly-free-electron metal to a transparent insulator at ~200 GPa. At ultimately high compression, however, all materials will metallize.\n\nHigh-pressure experimentation has led to the discovery of the types of minerals which are believed to exist in the deep mantle of the Earth, such as silicate perovskite, which is thought to make up half of the Earth's bulk, and post-perovskite, which occurs at the core-mantle boundary and explains many anomalies inferred for that region.\n\nPressure \"landmarks\": typical pressures reached by large-volume presses are up to 30–40 GPa, pressures that can be generated inside diamond anvil cells are ~1000 GPa, pressure in the center of the Earth is 364 GPa, and highest pressures ever achieved in shock waves are over 100,000 GPa.\n\n"}
{"id": "10889931", "url": "https://en.wikipedia.org/wiki?curid=10889931", "title": "IEEE 1613", "text": "IEEE 1613\n\nIEEE-1613 is the IEEE standard detailing environmental and testing requirements for communications networking devices in electric power substations. The standard is sponsored by the IEEE Power & Energy Society.\n\n"}
{"id": "10777584", "url": "https://en.wikipedia.org/wiki?curid=10777584", "title": "Keetch–Byram drought index", "text": "Keetch–Byram drought index\n\nThe Keetch–Byram drought index (KBDI), created by John Keetch and George Byram in 1968 for the United States Department of Agriculture's Forest Service, is a measure of drought conditions. It is commonly used for the purpose of predicting the likelihood and severity of wildfire. It is calculated based on rainfall, air temperature, and other meteorological factors. \n\nThe KBDI is an estimate of the soil moisture deficit, which is the amount of water necessary to bring the soil moisture to its full capacity. A high soil moisture deficit means there is little water available for evaporation or plant transpiration. This occurs in conditions of extended drought, and has significant effects on fire behaviour. \n\nIn the United States, it is expressed as a range from 0 to 800, referring to hundredths of an inch of deficit in water availability; in countries that use the metric system, it is expressed from 0 to 200, referring to millimetres.\n\n"}
{"id": "19244165", "url": "https://en.wikipedia.org/wiki?curid=19244165", "title": "Kværner process", "text": "Kværner process\n\nThe Kværner process or the Kværner carbon black and hydrogen process (CB&H) is a method of producing carbon black and hydrogen gas from hydrocarbons such as methane, natural gas and biogas. The process was developed in the 1980s by the Norwegian engineering firm Kværner, and was first commercially exploited in 1999.\n\nThe endothermic reaction separates hydrocarbons into carbon and hydrogen in a plasma burner at around 1600 °C.\n\nIn comparison to other reformation methods such as steam reforming and partial oxidation the natural gas is efficiently and completely transformed into pure carbon and hydrogen. Of the available energy of the feed, approximately 48% is contained in the hydrogen, 40% is contained in activated carbon and 10% in superheated steam.\n\nA variation of this process using plasma arc waste disposal was presented in 2009. Methane and natural gas is converted to hydrogen, heat and carbon using a plasma converter.\n\n"}
{"id": "54976053", "url": "https://en.wikipedia.org/wiki?curid=54976053", "title": "LArIAT", "text": "LArIAT\n\nLArIAT (Liquid Argon In A Testbeam) is a neutrino experiment located at the Fermi National Accelerator Laboratory (Fermilab), Chicago. It is capable of both accurately identifying and making precise 3D spatial and calorimetric measurements of particles. Currently, it is in the calibration phase of development. At present, the LArIAT collaboration has more than 65 members from 17 institutions worldwide, including Yale University, the University of Manchester, and KEK.\n\nLArIAT itself is a cylindrical cryostat with convex ends, capable of holding 550L of liquid argon. The cryostat is heavily insulated thermally, in part by a region of vacuum surrounding it. Inside the cryostat is the time projection chamber, in which the argon atoms are ionized by beam particles. An electric field is applied across the chamber perpendicular to the beam direction, causing the ionization electrons to move towards the side, where they are detected by two wire planes oriented at ±30° to the vertical. These wire planes are made up of 240 parallel wires with a separation of 4mm. A third wire plane of 225 wires, aligned along the vertical axis, is placed in front of the other two planes in order to prevent the electric field from interfering with the instrumentation.\n\nIonization of liquid argon emits characteristic 128 nm light (in the violet/ultraviolet range), which is detected by a scintillation read-out system. This system has a high efficiency, having been designed using systems developed for dark matter liquid argon detectors. This gives LArIAT an advantage over other existing liquid argon detectors which are only able to perform calorimetry using the wire grids.\n"}
{"id": "12673875", "url": "https://en.wikipedia.org/wiki?curid=12673875", "title": "LED Incapacitator", "text": "LED Incapacitator\n\nThe Light Emitting Diode (LED) Incapacitator is a weapon designed like a flashlight. It emits an extremely bright, rapid, and well-focused series of 'differently-colored random pulses.' Before the human eyes can focus in on one frequency, another frequency comes on, causing intracranial pressure, which results in headaches, nausea, vomiting, disorientation, irritability, and visual impairment to the target (opponent).\n\nThe non-lethal weapon is intended as a means of protection by law enforcement officials such as police and border patrols. The light emitted is capable of rendering opponents temporarily blind so that they can be subdued more easily.\n\nAccording to the United States Department of Homeland Security, the weapon works:\n\"By simultaneously overwhelming the subject both physiologically (temporarily blinding him) and psychophysically (disorienting him). A built-in range finder measures the distance to the nearest pair of eyeballs. Then, a “governor” sets the output and pulse train (a series of pulses and rests) to a level, frequency, and duration that are effective, but safe. The colors and pulses continuously change, leaving no time for the brain or eyes to adapt. After a few minutes, the effects wear off.\"\n\nThe manufacturer, Intelligent Optical Systems (IOS), was awarded an $800,000.00 contract by the United States Department of Homeland Security's Small Business Innovation Research Office. As of August, 2007, the first phase of the contract, which called for the development of a prototype, has been completed. Later phases involve the testing of the prototype by the Institute of Nonlethal Defense at Pennsylvania State University, and the development of a production version, planned to be more compact and easier to carry.\n\nBob Lieberman, president of IOS, said he envisions the device being used in \"confrontations at border crossings with suspected illegal aliens or drug runners\" among other possibilities. Projected users include \"air marshals, border patrol agents, other officers with the Transportation Security Administration and customs officers\" according to Gerald Kirwin of IOS. \"In that few seconds, the officer has a tactical advantage and will actually close in to subdue or control the potential adversary,\" Lieberman said. The device can be used from ranges of up to 30 feet, while other non-lethal devices require an officer to approach the target much more closely, according to Lieberman.\n\nGlenn Shwaery, a researcher in nonlethal technology at the University of New Hampshire, says \"If you disorient or distract somebody and cause them to look away, then they can't focus on their task, which could be aiming a weapon at someone, or looking at a screen with sensitive information, or dialing a phone.\" Shwaery says that an LED-based device could be safer than a laser-based device intended to achieve the same effects \"Getting an eye-safe wavelength with a laser has been very difficult.\" Shwaery says that the presence of a range-finder and circuitry to modify the intensity of the light depending on the range in the LED device increases its safety. \"The ideal goal for nonlethal technologies is that they be scalable.\" It was intended to be released to police, border patrol agents and National Guardsmen by 2010.\n\nSome test subjects have been completely unaffected by the light, and although the manufacturer makes claims about the product's effectiveness, the CEO of Intelligent Optics, the producer of the device, has commented \"I don’t think we've had anyone actually be sick.\" There is debate over how well the devices work and what their effects are.\n\nSome critics have questioned the desirability of developing and deploying such a device for border security use. Deborah Notkin, president of the American Immigration Lawyers Association said: \"It gives me pause, particularly in regards to Mexico. Mexico is a very important economic partner of ours. I would imagine that Mexico wouldn't be particularly happy with us using a device that would be more appropriate for criminals, not just for people trying to get across the border who are looking for better opportunities. \"\n\nIn regards to the statement above it must also be noted that people entering the United States illegally are criminals.\n\nPeter Herby, head of the legal division of the \"mines-arms unit\" at the International Committee of the Red Cross, suggests that such a device may raise many of the same issues as have arisen with laser-based blinding weapons, known as \"dazzlers\" because of the intense light with which they temporarily or permanently blind people. He also raises the issue of a possible black market in such weapons. \"Once they're in the hands of bad guys are the police going to have to wear protective gear to prevent them[selves] from being dazzled?\" Herby states that he is not familiar with this specific device.\n\nHowever Lieberman of IOS says: \"We're taking great care to make sure the intensities we're using fall within eye-safe limit. We're doing medically supervised tests.\"\n\n"}
{"id": "7290766", "url": "https://en.wikipedia.org/wiki?curid=7290766", "title": "List of national parks of Mexico", "text": "List of national parks of Mexico\n\nMexico recognizes 67 federally protected natural areas as national parks (), which are administered by the National Commission of Protected Natural Areas (CONANP), a branch of the federal Secretariat of the Environment and Natural Resources. Together, they cover a combined area of in 23 of the 31 Mexican states and the independent district of Mexico City, representing 0.73% of the territory of Mexico.\n\n\n\n"}
{"id": "14877855", "url": "https://en.wikipedia.org/wiki?curid=14877855", "title": "MIOS", "text": "MIOS\n\nMIOS or Meter Inter Operability Solution is a specification for interoperability of electric meters for remote meter reading applications in India proposed by IEEMA in 2008. and is developed and maintained by Metering India which is a group of Meter Manufacturers.\n\n"}
{"id": "25948250", "url": "https://en.wikipedia.org/wiki?curid=25948250", "title": "Mathematical visualization", "text": "Mathematical visualization\n\nMathematical visualization is an aspect of geometry which allows one to understand and explore mathematical phenomena via visualization. Classically this consisted of two-dimensional drawings or building three-dimensional models (particularly plaster models in the 19th and early 20th century), while today it most frequently consists of using computers to make static two or three dimensional drawings, animations, or interactive programs. Writing programs to visualize mathematics is an aspect of computational geometry.\n\nMathematical visualization is used throughout mathematics, particularly in the fields of geometry and analysis. Notable examples include plane curves, space curves, polyhedra, ordinary differential equations, partial differential equations (particularly numerical solutions, as in fluid dynamics or minimal surfaces such as soap films), conformal maps, fractals, and chaos.\n\n\nThe cover of the journal \"The Notices of the American Mathematical Society\" regularly features a mathematical visualization.\n\n\n"}
{"id": "36546958", "url": "https://en.wikipedia.org/wiki?curid=36546958", "title": "Misicuni Dam", "text": "Misicuni Dam\n\nThe Misicuni Multiplepurpose Project, better known as the Misicuni Dam, is a concrete-face rock-fill dam partly constructed on the Misicuni River about northwest of the city of Cochabamba, Bolivia. The dam will divert water from the Misicuni River to the Cochabamba Valley for several purposes to include providing water for irrigation and municipal water uses. In addition, the dam will have an associated 80 MW hydroelectric power station when complete. Construction on the dam began in June 2009 but was halted in November 2013 due to contract disputes. The company finished the construction and is starting the operations by September 2017 .\n\nThe project has three components:\n\nThe dam will be the highest and largest dam in Bolivia. Misicuni project director Ramiro Saniz said in 2009 that the water from the Misicuni river is not sufficient to fill the reservoir and that other sources are needed.\n\nThe public company in charge of developing the project is Proyecto Misicuni, an entity created by law in 1987. The contractor for the US$90 million dam component is the \"Consorcio Hidroelectrico Misicuni\" (CHM). The Misicuni consortium, led with 51 percent ownership by Grandi Lavori Fincosit S.p.A. of Italy, was the sole bidder for the project. Bidding was limited to Italian companies and CHM was the only company to submit a bid. The consortium also includes Bolivian, Colombian and Venezuelan firms.\n\nThe dam component was originally expected to be completed in 2014. However, in November 2013 the contract was canceled amid delays because CHM \"failed to pay for pension funds, health insurances and other labour benefits and to contract key technical personnel.\"\n\nThe tunnel provides 4.5-7.5 million cubic meters of water per year to Cochabamba since 2005, depending on whether the flow of the river is low or high and supplying about 10 percent of the city's drinking water. Once the dam will be completed, the amount of drinking water available will increase tenfold to 63 million cubic meters per year.\n\n1365 people live in the reservoir area that will be flooded and will be relocated. The Tunari National Park will also be affected.\n\nThe dam is partially financed by the Italian government through a 25 million Euro loan and the Development Bank of Latin America (CAF). Total funding from Italy for Phased II and III is USD 93 million. Bolivia will have to pay back the loan over in 20 years with a 0.10% interest rate.\n\nThe construction of the penstock, hydropower plant and power transmission line is funded by a US$101 million loan from the Inter-American Development Bank approved in 2009. These works were expected to be completed by 2015, but were only 30 percent completed as of April 2013.\n"}
{"id": "4721897", "url": "https://en.wikipedia.org/wiki?curid=4721897", "title": "OK-150 reactor", "text": "OK-150 reactor\n\nThe OK-150 reactor (1st generation) and its successor, the OK-900 reactor (2nd generation) are Soviet marine nuclear fission reactors used to power ships at sea. They are pressurized water reactors (PWRs) that use enriched uranium-235 fuel. They have been used in various Russian nuclear-powered icebreaker ships. The reactor was developed by OKBM.\n\nOK-150 specifications:\n\nDistilled water was used for heat transfer and as a moderator.\nThe core was 1.6 m high by 1 m diameter. It consisted of 219 fuel assemblies, totalling 7,704 fuel pins. There was a biological shield made of concrete mixed with metal shavings.\n\nOK-900A specifications:\n\nThree OK-150s were used to power the Soviet icebreaker \"Lenin\" at the time of its launch in 1957. Later, after damage caused by nuclear accidents in 1965 and 1967, these were removed and replaced with two OK-900s.\n\n"}
{"id": "520449", "url": "https://en.wikipedia.org/wiki?curid=520449", "title": "Pacific Plate", "text": "Pacific Plate\n\nThe Pacific Plate is an oceanic tectonic plate that lies beneath the Pacific Ocean. At , it is the largest tectonic plate.\n\nThe Pacific Plate contains an interior hot spot forming the Hawaiian Islands.\n\nHillis and Müller are reported to consider the Bird's Head Plate to be moving in unison with the Pacific Plate. Bird considers them to be unconnected.\n\nThe north-eastern side is a divergent boundary with the Explorer Plate, the Juan de Fuca Plate and the Gorda Plate forming respectively the Explorer Ridge, the Juan de Fuca Ridge and the Gorda Ridge. In the middle of the eastern side is a transform boundary with the North American Plate along the San Andreas Fault, and a boundary with the Cocos Plate. The south-eastern side is a divergent boundary with the Nazca Plate forming the East Pacific Rise. \n\nThe southern side is a divergent boundary with the Antarctic Plate forming the Pacific-Antarctic Ridge. \n\nThe western side, the plate is bounded by the Okhotsk Plate at the Kuril-Kamchatka Trench and the Japan Trench, forms a convergent boundary by subducting under the Philippine Sea Plate creating the Mariana Trench, has a transform boundary with the Caroline Plate, and has a collision boundary with the North Bismarck Plate. \n\nIn the south-west, the Pacific Plate has a complex but generally convergent boundary with the Indo-Australian Plate, subducting under it north of New Zealand forming the Tonga Trench and the Kermadec Trench. The Alpine Fault marks a transform boundary between the two plates, and further south the Indo-Australian Plate subducts under the Pacific Plate forming the Puysegur Trench. The southern part of Zealandia, which is to the east of this boundary, is the plate's largest block of continental crust. \n\nThe northern side is a convergent boundary subducting under the North American Plate forming the Aleutian Trench and the corresponding Aleutian Islands.\n\nThe Pacific Plate is almost entirely oceanic crust, but it contains some continental crust in New Zealand, Baja California, and coastal California.\n\nThe Pacific Plate has the distinction of showing one of the largest areal sections of the oldest members of seabed geology being entrenched into eastern Asian oceanic trenches. A geologic map of the Pacific Ocean seabed shows not only the geologic sequences, and associated Ring of Fire zones on the ocean's perimeters, but the various ages of the seafloor in a stairstep fashion, youngest to oldest, the oldest being consumed into the Asian oceanic trenches. The oldest member disappearing by way of the Plate Tectonics cycle is early-Cretaceous (145 to 137 million years ago).\n\nAll maps of the Earth's ocean floor geology show ages younger than 145 million years, only about 1/30 of the Earth's 4.55 billion year history. \n\n"}
{"id": "30611425", "url": "https://en.wikipedia.org/wiki?curid=30611425", "title": "Professional Petroleum Data Management Association", "text": "Professional Petroleum Data Management Association\n\nThe Professional Petroleum Data Management Association (PPDM Association) is a global, not-for-profit organization that works collaboratively within the petroleum industry to create and promote standards and best practices for data management. The Association’s vision is the global adoption of data management standards and best practices throughout the upstream (exploration and production) petroleum industry.\n\nThe focus of the Association is:\n\nHistorically, petroleum data has been managed and analyzed using many different proprietary systems. These systems are typically built based on local terminology and business needs, and use different practices for identifying, gathering, transferring and interpreting information. A typical operating company may use dozens (or even hundreds) of software applications. Each department is tuned to the needs of different segments of the company; production accounting, field operations, seismic exploration, reserves management and financial departments all store information about wells in their respective software applications.\n\nIn 1989, one Calgary-based oil company and several of its software suppliers recognized that a non-proprietary data model was preferable to separate and private models. The creation of a comprehensive data model is an expensive and lengthy process. A private data model precludes the purchase of third-party software and other data-dependent services, without expensive customization. Therefore, a group of technical experts began work on a “public” model, and other companies responded to the invitation to participate.\n\nAs the model developed, more companies accepted the concept of non-competitive collaboration and recognized that there is little competitive advantage to using an exclusive private data model. In 1991 the cooperative workgroup was incorporated in Alberta as a not-for-profit society, the Public Petroleum Data Model Association.\n\nThe initial data modeling efforts were on the requirements for well data in a relational database management system. Throughout the 1990s, the scope expanded to land rights, seismic surveys, well production and reserves, stratigraphy, records management, etc. Version 3.8 of the PPDM Model, released in 2008, covers 53 subject areas and has over 1700 tables.\n\nIn 2008, the name was changed to the Professional Petroleum Data Management Association. This acknowledges the expansion of the products and services beyond just the data model, and emphasizes the focus on developing the professional practice of data management for the petroleum industry.\n\nThe PPDM Model is suitable for a master data management system. The specifications DDL are provided for use in Oracle and SQL Server systems. In addition, the modular design allows selected portions to be used in business-specific databases and applications.\n\nAlthough it was originally called a public model, it was never freely distributed by the Association. “Public” always meant “non-proprietary” not “free.” The way to get the Model was (and still is) to become a member of the Association. However, many non-members around the world continue to use versions or adaptations of the PPDM model, usually because the model is embedded in a database or application purchased from a software provider. PPDM Lite, a simplified model based on Version 3.7, is available free to any registered user on the PPDM website.\n\nThe PPDM Model is a set of specifications for creating a relational database. It is NOT a set of data for the petroleum industry. By analogy, the model is only the blueprint for a huge pigeon loft (birdhouse); it does not have birds already in the pigeon-holes. However, the Model contains a vast amount of knowledge about the industry’s business practices (how the data are acquired and used.) The Association estimates that Version 3.8 represents over $100 million of invested funds and human resources since inception.\n\nThe PPDM Association is a member-based society. Membership is open to corporations and individuals from around the world. The membership fee structure is based on the member’s financial size (revenues.) Over half the annual revenue is from companies based in the USA; the balance is from Canada and the rest of the world. Associate membership is available to individuals.\n\nMost of the activities of the Association are financed by the annual membership fees and carried out by people who work for the member companies. Special projects are funded by sponsoring companies who provide additional funds and specialist personnel. The Association’s office in Calgary has a small staff for technical support, communication, and administration.\n\nThe members are represented by a board of directors. Each director is elected by the members to a term of two years (renewable.) They are mainly from large oil companies and software or data vendors.\n\nThe real power of the Association, and the quality of its products, is rooted in the cooperative approach to development, “The PPDM Way.” When several members recognize a data management need (e.g. expand the data model to a new subject area, or promote best practices for data governance), a draft charter is prepared. If the charter is approved by the PPDM board of directors, and sufficient resources are committed by the members, a workgroup is formed.\n\nCompanies supply the workgroup participants who identify the business requirements in detail, and develop the technical solutions. Workgroup meetings are typically spread out over a year or more because the participants continue to be active in their regular employment.\n\nSemantic misunderstanding causes data errors and mistakes in analysis of the data. In 2009, the PPDM Association developed a set of baseline definitions for a \"Well\" and its main components. These definitions are useful for comparison with the specific definitions used by key industry data providers, aggregators, and operating companies. The \"What Is A Well?\" definitions are a \"Rosetta stone\" framework for the petroleum industry.\n\n"}
{"id": "6421539", "url": "https://en.wikipedia.org/wiki?curid=6421539", "title": "S2C reactor", "text": "S2C reactor\n\nThe S2C reactor is a naval reactor used by the United States Navy to provide electricity generation and propulsion on warships. The S2C designation stands for:\n\n\nThis nuclear reactor is the shipboard equivalent of the S1C reactor, and was installed on the experimental USS \"Tullibee\" (SSN-597) submarine.\n"}
{"id": "52991019", "url": "https://en.wikipedia.org/wiki?curid=52991019", "title": "SPT-100", "text": "SPT-100\n\nSPT-100 is a Hall-effect ion thruster, part of the SPT-family of thrusters. SPT stands for \"Stationary Plasma Thruster\". It creates a stream of electrically charged xenon ions accelerated by an electric field and confined by a magnetic field.\n\nThruster is manufactured by Russian OKB Fakel, and was first launched onboard Gals-1 satellite in 1994. In 2003 Fakel debuted a second generation of the thruster, called SPT-100B, and in 2011 it presented further upgrades in SPT-100M prototypes. As of 2011 SPT-100 thrusters were used in 18 Russian and 14 foreign spacecrafts, including IPSTAR-II, Telstar-8 and Ekspress A and AM constellations.\n\n"}
{"id": "15172479", "url": "https://en.wikipedia.org/wiki?curid=15172479", "title": "Smith's Cloud", "text": "Smith's Cloud\n\nSmith's Cloud is a high-velocity cloud of hydrogen gas located in the constellation Aquila at Galactic coordinates \"l\" = 39°, \"b\" = −13°. The cloud was discovered in 1963 by Gail Bieger, \"née\" Smith, who was an astronomy student at Leiden University in the Netherlands.\n\nUsing the National Science Foundation's Robert C. Byrd Green Bank Telescope, radio astronomers have found that Smith's cloud has a mass of at least one million solar masses and measures long by wide in projection. The cloud is between and from Earth and has an angular diameter of 10 to 12 degrees, approximately as wide as the Orion constellation, or about 20 times the diameter of the full moon, although the cloud is not visible to the naked eye.\n\nThe cloud is apparently moving towards the disk of the Milky Way at 73 ± 26 kilometers per second. Smith's Cloud is expected to merge with the Milky Way in 27 million years at a point in the Perseus arm. Astronomers believe it will strike the Milky Way disk at a 45° angle, and its impact may produce a burst of star formation or a supershell of neutral hydrogen.\n\nProjecting the cloud's trajectory backwards through time, it is estimated that it had passed through the disk of the Milky Way some 70 million years ago. To have survived this previous encounter, astronomers have suggested that it is embedded inside a massive dark matter halo. The fact that it survived this previous encounter means that it is likely to be much more massive than previously thought, and may be a candidate for being a dark galaxy. In this scenario it would be a failed dwarf galaxy, with the ingredients to form a stellar galaxy, but few if any detectable stars. However, chemical abundance measurements from the Hubble Space Telescope argue against this hypothesis; these measurements show that the Smith Cloud has an average metallicity of one half of the solar value, indicating that its gas originates in the Galaxy, not from an extragalactic source. The cloud's orbit and metallicity are both consistent with an origin in the outer disk of the Milky Way. The mechanism by which this gas was released is not known.\n\n"}
{"id": "22858260", "url": "https://en.wikipedia.org/wiki?curid=22858260", "title": "Society of Canadian Ornithologists", "text": "Society of Canadian Ornithologists\n\nThe Society of Canadian Ornithologists, or Société des Ornithologistes du Canada, is an ornithological non-profit organization serving Canada’s ornithological community. It was founded in 1983, and is a member of the Ornithological Council.\n\nThe goals of the Society are to encourage and support research towards the understanding and conservation of Canadian birds, serve as a professional society for both amateur and professional Canadian ornithologists, represent Canadian ornithologists within professional ornithological societies, publish information about Canadian birds, and recognise excellence in research, conservation and mentorship in the Canadian ornithological community.\n\nThe Society produces the journal Avian Conservation and Ecology - Ecologie et Conservation des Oiseaux (ACE-ECO), which is published jointly with Bird Studies Canada, as well as a newsletter, \"Picoides\". It makes two annual awards, the Doris Huestis Speirs Award, which is given for outstanding lifetime contributions to Canadian ornithology, and the Jamie Smith Memorial Mentoring Award.\n\n\n"}
{"id": "38437309", "url": "https://en.wikipedia.org/wiki?curid=38437309", "title": "Solar dryer", "text": "Solar dryer\n\nSolar dryers are devices that use solar energy to dry substances, especially food. There are two general types of solar dryers: Direct and indirect.\n\nDirect solar dryers expose the substance to be dehydrated to direct sunlight. Historically, food and clothing was dried in the sun by using lines, or laying the items on rocks or on top of tents. In Mongolia cheese and meat are still traditionally dried using the top of the ger (tent) as a solar dryer. In these systems the solar drying is assisted by the movement of the air (wind) that removes the more saturated air away from the items being dried. More recently, complex drying racks and solar tents were constructed as solar dryers.\n\nOne modern type of solar dryer has a black absorbing surface which collects the light and converts it to heat; the substance to be dried is placed directly on this surface. These driers may have enclosures, glass covers and/or vents to in order to increase efficiency.\n\nIn indirect solar dryers, the black surface heats incoming air, rather than directly heating the substance to be dried. This heated air is then passed over the substance to be dried and exits upwards often through a chimney, taking moisture released from the substance with it. They can be very simple, just a tilted cold frame with black cloth to an insulated brick building with active ventilation and a back-up heating system. One of the advantages of the indirect system is that it is easier to protect the food, or other substance, from contamination whether wind-blown or by birds, insects, or animals. Also, direct sun can chemically alter some foods making them less appetizing.\n\n"}
{"id": "57183485", "url": "https://en.wikipedia.org/wiki?curid=57183485", "title": "Songwe Hydroelectric Power Station", "text": "Songwe Hydroelectric Power Station\n\nSongwe Hydroelectric Power Station, also Songwe Power Station, is a proposed hydropower plant, with planned capacity installation of when completed. Other related developments include the development of more dams for both power generation and irrigation purposes, and the creation of a Joint River Basin Authority.\n\nThe power station would be located on the Songwe River, straddling the common border between Tanzania and Malawi. Its location is south of the town of Itumba, approximately , south of Mbeya, the headquarters of Mbeya District.\n\nThis power station is the first to be developed by the \"Joint Songwe River Basin Commission\", co-owned by the Government of Tanzania and the Government of Malawi. The power station design calls for the creation of a reservoir, to be used for power generation and irrigation purposes in both countries. \n\nAs of May 2017, the feasibility studies and environmental and social impact assessment (ESIA) had been completed. The final design had been agreed upon. Commitment of funding from potential investors was being sought.\n\nThe African Development Bank (AfDB) funded the feasibility and design studies. The final design report was expected at the end of 2015. Other administrative and funding matters were expected to conclude in 2016. Construction was expected to start after that and conclude in 2022. Each country will be allocated from this project.\n\nThe total cost of the project is quoted at US$829 million, to be shared equally by the two countries. The AfDB has expressed willingness to fund this project.\n\n\n"}
{"id": "3366061", "url": "https://en.wikipedia.org/wiki?curid=3366061", "title": "Southwest Airlines Flight 1248", "text": "Southwest Airlines Flight 1248\n\nSouthwest Airlines Flight 1248 (WN1248, SWA1248) was a scheduled passenger flight from Baltimore, Maryland, to Chicago, Illinois, to Salt Lake City, Utah, and then to Las Vegas, Nevada. On December 8, 2005, the airplane slid off a runway at Chicago-Midway while landing in a snowstorm and crashed into automobile traffic, killing six-year-old Joshua Woods. It is the first accident involving the airline to result in the death of someone not on the plane itself.\n\nOn Thursday, December 8, 2005, Southwest Airlines Flight 1248 was scheduled to arrive at Chicago Midway International Airport from Baltimore-Washington International Thurgood Marshall Airport, and then continue on to Salt Lake City International Airport, then to Las Vegas McCarran International Airport. The flight circled over a small area in northwest Indiana several times before attempting to land in a snowstorm. The snowstorm had reduced visibility to less than one mile.\n\nAt around 7:15 PM CST, the pilot attempted a landing with nearly eight inches of snow on the ground in the area. Airport officials stated that the runway was cleared of snow prior to the time of landing. The latest reported weather had the wind from between east and east-southeast (100°) at .\n\nA south easterly wind would normally favor landing into the wind on runway 13 Center. The runway visual range was reported at , below the landing minimums for the Instrument Landing System approach to runway 13C. The only available runway with lower minimums was the opposite direction on 31C, which the crew selected, with the aircraft's groundspeed consequentially boosted by the tailwind. \nAlternately, the crew could have held in the air, waiting for the weather to improve, or they could have diverted to another airport, such as Chicago O'Hare International, whose substantially longer runways were 10 minutes' flying time away. Each of these options would have entailed considerable additional expense for Southwest, as well as missed connections and significant inconvenience for the flight's passengers. The National Transportation Safety Board identified the psychological pressure to complete their assigned task as one of the factors contributing to the crew's decision to land at Midway despite unfavorable conditions. Cockpit voice-recorder transcripts indicate the pilots had been concerned about the weather and, prior to landing, jokingly alluded to the movie \"Airplane!,\" saying, \"I picked a bad day to stop sniffin' glue.\"\n\nThe NTSB preliminary report determined that the aircraft touched down in the touchdown zone of the runway with of its length remaining; under the prevailing conditions of weather, wind, speed, and weight, the aircraft needed of runway to stop safely.\n\nA preliminary NTSB advisory says: \"The flying pilot (Captain) stated that he could not get the reverse thrust levers out of the stowed position. The first officer, after several seconds, noticed that the thrust reversers were not deployed, and activated the reversers without a problem. Flight data recorder information reveals that the thrust reversers were not deployed until 18 seconds after touchdown, at which point there was only about of usable runway remaining.\"\n\nThe 737 skidded during landing; subsequently, witnesses said the nosegear collapsed and the aircraft crashed into a barrier wall surrounding the airport, coming to rest on Central Avenue just south of the 55th Street intersection at the northwestern corner of the airport. The intersection was full of traffic, and the airplane hit at least three cars, killing a six-year-old boy named Joshua Woods, critically injuring five occupants of one car (two adults and three children), and seriously injuring four occupants of a second car. All were quickly taken to area hospitals. Three passengers from the aircraft were taken to hospitals with minor injuries. All tolled, twelve people were taken to hospitals after the incident. One other car hit was parked and unoccupied.\n\nThe aircraft, a Boeing 737-700 with tail number was delivered to Southwest in July 2004. As a \"Next Generation\" model, the 737 was equipped with the latest anti-skid and braking technology. The report noted that Southwest had only very recently begun actually using the autobrake systems, and that pilot training on proper use of auto brakes had been inadequate.\n\nThe National Transportation Safety Board was reported to be investigating, and Chicago Fire Department Commissioner Cortez Trotter said the aircraft would not be removed from the intersection until the NTSB gave clearance to do so following its on-site investigation. The nose of the aircraft was hoisted onto a flatbed tractor-trailer on Saturday, December 10, and the 737 was towed to a hangar for continued inspection.\n\nIt is now recommended practice for any new runway to have a clear area at least long at each end, called a 'runway safety area', to allow additional space for an aircraft that overruns the runway to decelerate and stop in relative safety. As Midway was constructed before these rules were put in place, it does not have this safety area. The accident renewed debates on the need for, and feasibility of, an engineered materials arrestor system, or EMAS, at Chicago Midway, given the lack of adequate overrun areas, and the surrounding residential neighborhoods. Additionally, actions taken by the city to acquire land for a buffer zone around the airport (in apparent recognition of the hazard) came to light after the crash. In 2007, installation began on modified, short-length arrestor beds. The first one was completed at the end of Runway 31C by summer 2007. EMAS beds have also been installed at the end of 04R, 13C and 22L.\n\nThe accident occurred 33 years to the day after United Airlines Flight 553, also a Boeing 737, crashed while approaching Midway Airport, killing 45.\n\nThe accident involving Flight 1248 was the first Southwest Airlines accident in the 35-year history of the company to result in a fatality. The previous major incident was in 2000 when Southwest Airlines Flight 1455 overran a runway at Burbank, California, injuring 43 and narrowly avoiding a catastrophe; the aircraft ended up outside a Chevron gas station. Although the Midway accident killed a person on the ground rather than a passenger or crew member, Southwest followed the tradition of retiring any flight number involved in a fatal crash; current flights from Baltimore to Chicago departing at or around 3:55 PM were designated Flight 1885 until that flight number was moved to a different flight (the nearest departure times on this route now have flight numbers 4279 and 269, operated with a larger Boeing 737-800. Southwest also petitioned the FAA in July 2006 to have the tail number of the aircraft changed to N286WN. After a lengthy repair, the aircraft emerged from Southwest's Midway hangar as N286WN in September 2006.\n\nAs a direct result of the accident, the U.S. Federal Aviation Administration created a Takeoff and Landing Performance Assessment Aviation Rule-making Committee. (i.e. TALPA ARC). In 2016, based on the recommendations of TALPA ARC, the FAA implemented new \"Runway Condition Code\" method for the communication of runway conditions from airport management to flight crew members.\n\n\n\n\n\n"}
{"id": "32077559", "url": "https://en.wikipedia.org/wiki?curid=32077559", "title": "Special Power Excursion Reactor Test Program", "text": "Special Power Excursion Reactor Test Program\n\nThe Special Power Excursion Reactor Test Program (SPERT) was a series of tests focusing on the safety of nuclear reactors. It was commissioned in 1954 by the U.S. Atomic Energy Commission, to be run by the Phillips Petroleum Company.\n\nThe SPERT-I reactor became operational in July 1955, with the first tests being performed in September.\n\nThe SPERT-II reactor became operational in March 1960.\n"}
{"id": "15303241", "url": "https://en.wikipedia.org/wiki?curid=15303241", "title": "The Megalithic Portal", "text": "The Megalithic Portal\n\nThe Megalithic Portal is a web resource dedicated to prehistoric archaeology and closely related subjects. The Megalithic Portal's mission is to document, publicise and protect ancient sites and help to ensure their preservation for future generations.\n\nFounded by chartered engineer Andy Burnham, the site database began in 1997 as Megalithic Mysteries, and the Prehistoric Web Index, from a database he originally compiled. The Megalithic Portal has existed in its current form since February 2001. The information is maintained by a team of voluntary editors and administrators and has become the centre of a keen user group. Contributors have been known to give up their jobs to travel, researching little-known ancient sites to add to the database. In 2002, Archaeology Magazine reviewed the Megalithic Portal, describing it as 'useful, fun, and accurate'. As of January 2010 the Megalithic Portal has been constituted as a non profit making membership society\n\nThe information contributed by thousands of visitors from all over the world covers types of prehistoric monument from chambered tombs and standing stones to hillforts and settlements, and much in between. There are many tens of thousands of listings, and over the years the site has extended beyond prehistoric megaliths, extending to, for example Pictish symbol stones in Scotland. While the site still calls itself the Megalithic Portal, it has also become the biggest online repository of data on related areas of interest such as holy wells and ancient crosses in the UK. Its listings are often referenced by noted web sites and in recent books on megaliths and Holy Wells.\n\nA recent project (April 2010) has involved contributors finding over 1000 ancient sites pictured on Google Street View in the UK and elsewhere \n\nSite features include:\n\n"}
{"id": "49370249", "url": "https://en.wikipedia.org/wiki?curid=49370249", "title": "Thioxanthate", "text": "Thioxanthate\n\nIn chemistry, a thioxanthate is an organosulfur compound with the formula RSCSX. When X is an alkali metal, the thioxanthate is a salt. When X is a transition metal, the thioxanthate is a ligand, and when X is an organic group, the compounds are called thioxanthate esters. They are usually yellow colored compounds that often dissolve in organic solvents. They are used as precursors to some catalysts, froth flotation agents, and additives for lubricants.\n\nThe alkali metal thioxanthates are produced by treating a thiol with a base in the presence of carbon disulfide, as illustrated by the preparation of sodium ethyl thioxanthate:.\nSodium ethyl thioxanthate is similar structurally to sodium ethyl xanthate.\n\nAlkylation of such thioxanthate anions gives thioxanthate esters, as illustrated by the preparation of ethyl methyl thioxanthate:\nThioxanthate esters are also called esters of trithiocarbonate.\n"}
{"id": "47105670", "url": "https://en.wikipedia.org/wiki?curid=47105670", "title": "Thomas Henry Moray", "text": "Thomas Henry Moray\n\nThomas Henry Moray (August 28, 1892 - May 18, 1974) was an inventor from Salt Lake City, Utah. He received a US patent 2,460,707 in February 1949, after a process of 17 years in discussions with the patent office. The\ntitle of the patent is \"Electrotherapeutic Device\", and although radiotherapy is mentioned, not details are given.\n\nMoray's invention was not commercially successful because of the high manufacturing cost compared to the amount of power produced and the small market for atomic powered batteries. Many unsubstantiated claims have been made in connection with attempts to sell books or ask for money. One claim was five kilowatts of electricity produced from a device costing sixty thousand dollars to build in 1926. If true, it represents a high cost for electricity compared to other sources, except in special situations like space research \n\nA counter culture has developed with claims about alternative energy, citing Moray as a leading example of lost opportunity and of free energy suppression. Since Moray patented his invention with detailed drawings and further described his ideas in books he wrote, the economics and technical operation can be understood with conventional science and engineering. A substantial reduction of manufacturing cost would be required to make wider use of power supplies based on Moray's invention. Vacuum tube circuits have been replaced by solid state electronics in most applications.\n"}
{"id": "7217234", "url": "https://en.wikipedia.org/wiki?curid=7217234", "title": "Tribelhorn", "text": "Tribelhorn\n\nThe Tribelhorn was a Swiss electric car manufactured from 1899 until 1919 in Zurich. Production began in earnest in 1902; three- and four-wheelers were offered. The company also produced trolleytrucks that were used in Gümmenen and Mühleberg Switzerland between 1918 and 1922 during the construction of a dam. It produced only light utility vans after 1919 when it was taken over by EFAG (Electrische Fahrzeuge AG).\nThe Company \"A. Tribelhorn & Cie. AG\" began in 1902 in Feldbach with the production of automobiles and trucks. In 1918 the name was changed to\" Electrische Fahrzeuge AG\" and moved to Altstetten. In 1920 the production was stopped.\n\nThere were vehicles manufactured exclusively with electric motor. The focus was on the production of commercial vehicles, while passenger cars were only produced in small quantities. 1902 created the first prototype.\n\nTwo vehicles of this brand are in the Verkehrshaus der Schweiz to be seen in Luzern.\n\nIt is also the name of a conference room at Tesla Motors, Inc.\n\n\nDavid Burgess Wise, \"The New Illustrated Encyclopedia of Automobiles\".\n"}
{"id": "49809925", "url": "https://en.wikipedia.org/wiki?curid=49809925", "title": "Uganda Atomic Energy Council", "text": "Uganda Atomic Energy Council\n\nThe Uganda Atomic Energy Council (UAEC) is a corporate body, established by the Atomic Energy Act of 2008, which was enacted by the Parliament of Uganda. \n\nThe headquarters and offices of UAEC are located in Amber House at 29-33 Kampala Road, in the central business district of Kampala, the capital city of Uganda. The coordinates of the agency's headquarters are:0°18'49.0\"N, 32°34'54.0\"E (Latitude:0.313611; Longitude:32.581667).\n\nUAEC is responsible for the regulation of the peaceful applications of ionising radiation, with the following specific objectives: (a) Protect the safety of individuals, society, and the environment from the dangers resulting from ionising radiation (b) Provide for the production and use of radiation sources and the management of radioactive waste (c) Provide for compliance with international safety requirements for the use of ionising radiation, radiation protection, and security of radioactive sources.\n\nSince 2012, Uganda has indicated its willingness, determination, and intention to develop nuclear power for peaceful means, using locally available uranium deposits. With an electrification rate of 20 percent as of June 2016, according to the Uganda Bureau of Statistics, the country will need more than what it can develop from hydroelectric sites, to satisfy the need for electricity nationwide. The country plans to generate 40,000 megawatts of electricity to meet its goals under the \"Vision 2040\" development plan. In October 2016, Uganda asked Russia for help in the development of nuclear power.\n\nThe five-member board included the following members as of July 2009:\n\n\n"}
{"id": "49185391", "url": "https://en.wikipedia.org/wiki?curid=49185391", "title": "UniEnergy Technologies", "text": "UniEnergy Technologies\n\nUniEnergy Technologies (UET) is a U.S. vanadium redox flow battery manufacturer in Mukilteo, Washington, which manufactures megawatt-scale energy storage systems for utility, commercial and industrial customers. The company was founded in 2012 by Dr. Gary Yang and Dr. Liyu Li to commercialize a new Vanadium electrolyte formulation the pair had developed while working at PNNL. The new formulation, a mixed-acid solution, was patented by PNNL and the patent was licensed to UET for commercialization. The mixed-acid vanadium electrolyte allows for a wider temperature range for operations, and double the energy density of the traditional vanadium electrolyte. \nThe company has designed a megawatt-scale flow battery using this new electrolyte for the purpose of allowing rapid deployment, manufacturing repeatability and lower costs. The company also employs an R&D team which works to make advances on the electrolyte chemistry and stack design.\n\nUET has a subsidiary in Germany, Vanadis Power which provides sales and services for Europe. The company has partnerships with Bolong New Materials, a vanadium electrolyte manufacturer, Rongke Power, the vanadium flow battery stack manufacturer. In December 2015 the company completed their B round funding series which included a major investment from Orix Corp.\n\nUniEnergy sells a 10kW, 34kWh fully integrated flow battery called the ReFlex. This product is sized to be a building block for commercial and utility scale deployments from kilowatts to multi-megawatt installations. \n\n"}
{"id": "43868206", "url": "https://en.wikipedia.org/wiki?curid=43868206", "title": "Volker Quaschning", "text": "Volker Quaschning\n\nVolker Quaschning (born 1969) is a German engineer and professor of renewable energy systems at the Hochschule für Technik und Wirtschaft Berlin, Germany.\n\nQuaschning studied electrical engineering at Karlsruhe Institute of Technology, then wrote his PhD on photovoltaics at the Technische Universität Berlin. After obtaining his habilitation on low-carbon power system scenarios for Germany, he worked for the German Aerospace Center in Almeria, Spain and lead research into concentrated solar power. In 2004, Quaschning was appointed professor of renewable energy systems at the Hochschule für Technik und Wirtschaft Berlin.\n\nQuaschning is the author of several books, including the scientific textbook \"Regenerative Energiesystems\" (Renewable energy systems), first published in 1998. In 2015, the ninth edition of this book was released. The book has been translated in English, Arabian, and Russian and a translation to the Kazakh language is in progress.\nAccording to Panos Konstantin, the book is \"highly recommendable\".\nIn 2016, an updated second edition in English was published.\n\n\n\n"}
{"id": "18953129", "url": "https://en.wikipedia.org/wiki?curid=18953129", "title": "Wood economy", "text": "Wood economy\n\nThe existence of a wood economy, or more broadly, a forest economy (since in many countries a bamboo economy predominates), is a prominent matter in many developing countries as well as in many other nations with temperate climate and especially in those with low temperatures. These are generally the countries with greater forested areas. The uses of wood in furniture, buildings, bridges, and as a source of energy are widely known. Additionally, wood from trees and bushes, can be employed in a wide variety, including those produced from wood pulp, as cellulose in paper, celluloid in early photographic film, cellophane, and rayon (a substitute for silk).\n\nAt the end of their normal usage, wood products can be burnt to obtain thermal energy, or can be used as a fertilizer. The potential environmental damage that a wood economy could occasion include (problems of reduction the biodiversity due to monoculture forestry—the intensive cultivation of very few types of trees); and CO emissions. However, forests can aid in reduction of atmospheric carbon dioxide and therefore decrease global warming.\n\nThe wood economy is historically the starting point of the civilizations worldwide, since eras preceding the Paleolithic and the Neolithic. It necessarily preceded ages of metals by many millenia, as the melting of metals was possible only through the discovery of techniques to light fire (usually obtained by the scraping of two very dry wooden rods) and the building of many simple machines and rudimentary tools, as canes, club handles, bows, arrows, lances. One of the most ancient handmade articles ever found is one smoothed pricked of wood (Clacton Spear) 250,000 years old (third interglacial period), that was buried under sediments in England, at Clacton-on-Sea.\n\nSuccessive civilizations such as the Egyptians and Sumerians built sophisticated objects of furniture. Many types of furniture in ivory and valuable woods have survived to our time practically intact, because secluded in inviolated secret tombs, they were protected from decay also by the dry environment of desert. Many buildings and parts of these (above all roofs) contained elements in wood (often of oak) forming structural supports and covering; means of transport such as boats, ships; and later (with the invention of the wheel) wagons and carriages, winches, flour mills powered by water, etc.\n\nThe main source of the lumber used in the world is forests, which can be classified as virgin, semivirgin and plantations. Much timber is removed for firewood by local populations in many countries, especially in the third world, but this amount can only be estimated, with wide margins of uncertainty.\n\nIn 1998, the worldwide production of \"roundwood\" (officially counted wood not used as firewood), was about , amounting to around 45% of the wood cultivated in the world. Cut logs and branches destined to become elements for building construction accounted for approximately 55% of the world's industrial wood production. 25% became wood pulp (including wood powder and truccioli) mainly destined for the production of paper and paperboard, and approximately 20% became panels in plywood and valuable wood for furnitures and objects of common use (FAO 1998). The World's largest producer and consumer of officially accounted wood is the United States, although the country that possesses the greatest area of forest is Russia.\n\nIn the 1970s, the countries with the largest forest area were: Soviet Union (approximately 8,800,000 km²), Brazil (5,150,000 km²), Canada (4,400,000 km²), United States (3,000,000 km²), Indonesia (1,200,000 km²) and Democratic Republic of Congo (1,000,000 km²). Other countries with important production and consumption of wood usually have a low density of population in relation to their territorial extension, here we can include countries as Argentina, Chile, Finland, Poland, Sweden, Ukraine.\n\nBy 2001 the rainforest areas of Brazil were reduced by a fifth (respect of 1970), to around 4,000,000 km²; the ground cleared was mainly destined for cattle pasture—Brazil is the world's largest exporter of beef with almost 200,000,000 head of cattle. The booming Brazilian ethanol economy based upon sugar cane cultivation, is likewise reducing forests area. Canadian forest was reduced by almost 30% to 3,101,340 km² over the same period.\n\nRegarding the problem of climate change, it is known that burning forests increase CO in atmosphere, while intact virgin forest or plantations act as sinks for CO, for these reasons wood economy fights greenhouse effect. The amount of CO absorbed depends on the type of trees, lands and the climate of the place where trees naturally grow or are planted. Moreover, by night plants do not photosynthesize, and produce CO, eliminated the successive day. Paradoxically in summer oxygen created by photosynthesis in forests near to cities and urban parks, interacts with urban air pollution (from cars, etc.) and is transformed by solar beams in ozone (molecule of three oxygen atoms), that while in high atmosphere constitutes a filter against ultraviolet beams, in the low atmosphere is a pollutant, able to provoke respiratory disturbances.\n\nIn a low-carbon economy, forestry operations will be focused on low-impact practices and regrowth. Forest managers will make sure that they do not disturb soil based carbon reserves too much. Specialized tree farms will be the main source of material for many products. Quick maturing tree varieties will be grown on short rotations in order to maximize output.\n\n\nBrazil has a long tradition in the harvesting of several types of trees with specific uses. Since the 1960s, imported species of pine tree and eucalyptus have been grown mostly for the plywood and paper pulp industries. Currently high-level research is being conducted, to apply the enzymes of sugar cane fermentation to cellulose in wood, in order to obtain methanol, but the cost is much higher when compared with ethanol derived from corn costs.\n\nThere is a close relation in the forestry economy between these countries; they have many tree genera in common, and Canada is the main producer of wood and wooden items destined to the US, the biggest consumer of wood and its byproducts in the world. The water systems of the Great Lakes, Erie Canal, Hudson River and Saint Lawrence Seaway to the east coast and the Mississippi River to the central plains and Louisiana allows transportation of logs at very low costs. On the west coast, the basin of the Columbia River has plenty of forests with excellent timber.\n\nThe agency Canada Wood Council calculates that in the year 2005 in Canada, the forest sector employed 930,000 workers (1 job in every 17), making around $108 billion of value in goods and services. For many years products derived from trees in Canadian forests had been the most important export items of the country. In 2011, exports around the world totaled some $64.3 billion – the single largest contributor to Canadian trade balance.\n\nCanada is the world leader in sustainable forest management practices. Only (28% of Canadian forests) are currently managed for timber production while an estimated are protected from harvesting by the current legislation.\n\n\n\nThe species that are ideal for the many uses in this type of economy are those employed by arboriculture, that are very well known for their features and the need for certain types of ground and climates.\n\n\nIn Sweden, Finland and to an extent Norway, much of the land area is forested, the pulp and paper industry is one of the most significant industrial sectors. Chemical pulping produces an excess of energy, since the organic matter in black liquor, mostly lignin and hemicellulose breakdown products, is burned in the recovery boiler. Thus, these countries have high proportions of renewable energy use (25% in Finland, for instance). Considerable effort is directed towards increasing the value and usage of forest products by companies and by government projects.\n\nThe burning of wood is currently the largest use of energy derived from a solid fuel biomass. Wood fuel may be available as firewood (e.g. logs, bolts, blocks), charcoal, chips, sheets, pellets and sawdust. Wood fuel can be used for cooking and heating through stoves and fireplaces, and occasionally for fueling steam engines and steam turbines that generate electricity. For many centuries many types of traditional ovens were used in order to benefit from the heat generated by wood combustion. Now, more efficient and clean solutions have been developed: advanced fireplaces (with heat exchangers), wood-fired ovens, wood-burning stoves and pellet stoves, that are able to filter and separate pollutants (centrifuging ashes with rotative filters), thus eliminating many emissions, also allowing to recover a higher quantity of heat that escaped with the chimney fumes.\n\nMean energy density of Wood, was calculated at around 6–17 Megajoule/Kilogram, depending on species and moisture content.\n\nCombustion of wood is, however, linked to the production of micro-environmental pollutants, as carbon dioxide (CO), carbon monoxide (CO) (an invisible gas able to provoke irreversible saturation of blood's hemoglobine), as well as nanoparticles.\n\nIn Italy poplar has been proposed as a tree cultivated to be transformed into biofuels, because of the excellent ratio of energy extracted from its wood because of poplar's fast growing and capture of atmospheric carbon dioxide to the small amount of energy needed to cultivate, cut and transport the trees. \"Populus x canadensis\" 'I-214', grows so fast that is able to reach in diameter and heights of in ten years.\n\nCharcoal is the dark grey residue consisting of impure carbon obtained by removing water and other volatile constituents from animal and vegetation substances. Charcoal is usually produced by slow pyrolysis, the heating of wood or other substances in the absence of oxygen. Charcoal can then be used as a fuel with a higher combustion temperature.\n\nWood gas generator (gasogen): is a bulky and heavy device (but technically simple) that transforms burning wood in a mix of molecular hydrogen (H), carbon monoxide (CO), carbon dioxide (CO), molecular nitrogen (N) and water vapor (HO). This gas mixture, known as \"wood gas\", \"poor gas\" or \"syngas\" is obtained after the combustion of dry wood in a reductive environment (low in oxygen) with a limited amount of atmospheric air, at temperatures of 900° Celsius, and can fuel an internal combustion engine.\nIn the time between World War I and World War II included, because of the lack of oil, in many countries, like Italy, France, Great Britain and Sweden, several gasoline-powered cars were modified, with the addition of a wood gas generator (a \"gasogen\"), a device powered by wood, coal, or burnable waste, able to produce (and purify) gas that immediately, in the same vehicle, could power a slightly modified ICE engine of a standard car (low-compression engine). Carburetor had to be changed with an air-gas mixer). There were several setbacks, as the great reduction of maximum speed and the need to drive using low gears and wisely dosing the amount of air. In modern cars, modified with a wood gas generator, gas emissions (CO, CO and NOx) are lower to those of the same vehicle running with gasoline (keeping the same catalytic converter).\n\nMethanol (the simplest alcohol) behaves as a liquid at 25 °C, is toxic and corrosive, and in organic chemistry basic books is often called \"the spirit of wood\", since it can be obtained from wood fermentation. Rarely, when unwise wine-makers mix small chunks of wood and leaves with grapes, methanol can be found as a pollutant of the blend of water, ethanol and other substances derived from grape's fermentation.\n\nThe best way to obtain methanol from wood is through syngas (CO, CO, H) produced by the anhydrous pyrolysis of wood, a method discovered by ancient Egyptians.\n\nMethanol can be used as an oxygen-rich additive for gasoline. However, it is usually much cheaper to produce methanol from methane or from syngas. Methanol is the most important base material for industrial chemistry, where it is often used to make more complex molecules through reactions of halogenation and chemical addition reaction.\n\nThe American M1 Abrams main battle tank is powered by a gas turbine of , that it is able to function also with a mix at 50% of wood powder and biodiesel, diesel fuel or kerosene. Its advantages over turbo-diesel engine, are the small size and light weight, the lack of a radiator (which gives an advantage against the effect of gun and cannon shots and missile strikes suffered in battle). A setback is the high fuel consumption, since the turbine engine has not the ability to work at a low revolutions per minute rate, much lower than ideal, and during the march this engine consumes twice as much fuel as a modern turbo-diesel engine with intercooler and direct injection.\n\nWood is relatively light in weight, because its specific weight is less than 500 kg/m³, this is an advantage, when compared against 2,000-2,500 kg/m³ for armed concrete or 7,800 kg/m³ for steel.\n\nWood is strong, because the efficiency of wood for structural purposes has qualities that are similar to steel.\n\nWood is used to build bridges (as the Magere bridge in Amsterdam), as well as water and air mills, and microhydro generators for electricity.\n\nHardwood is used as a material in wooden houses, and other structures with a broad range of dimensions. In traditional homes wood is preferred for ceilings, doors, floorings and windows. Wooden frames were traditionally used for home ceilings, but they risk collapse during fires.\n\nThe development of energy efficient houses including the \"passive house\" has revamped the importance of wood in construction, because wood provides acoustic and thermal insulation, with much better results than concrete.\n\nIn Japan, ancient buildings, of relatively high elevation, like pagodas, historically had shown to be able to resist earthquakes of high intensity, thanks to the traditional building techniques, employing elastic joints, and to the excellent ability of wooden frames to elastically deform and absorb severe accelerations and compressive shocks.\n\nIn 2006, Italian scientists from CNR patented a building system that they called \"SOFIE\", a seven-storey wooden building, 24 meters high, built by the \"Istituto per la valorizzazione del legno e delle specie arboree\" (Ivalsa) of San Michele all'Adige. In 2007 it was tested with the hardest Japanese antiseismic test for civil structures: the simulation of Kobe's earthquake (7.2 Richter scale), with the building placed over an enormous oscillating platform belonging to the NIED-Institute, located in Tsukuba science park, near the city of Miki in Japan. This Italian project, employed very thin and flexible panels in glued laminated timber, and according to CNR researchers could brought to the construction of much more safe houses in seismic areas.\n\nOne of the most enduring materials is the lumber from virginian southern live oak and white oak, specially live oak is 60% stronger than white oak and more resistant to moisture. As an example, the main component in the structure of battle ship USS Constitution, the world's oldest commissioned naval vessel afloat (launched in 1797) is white oak.\n\nOne of the most famous crisis of a wood-based economy is what happened in Classical Greece, where trees began to disappear specially in the areas of Attica, Boeotia and Peloponnesus where indiscriminate cutting of trees for several uses, associated to drought and wildfires led to a severe lack of timber in order to build lances, shields, ships, etc. and to a slow but progressive weakening in military and naval power of the peninsular kingdoms in Greece, that were overwhelmed by Epirus and by the Kingdom of Macedon, much more fertile lands because of their rainy winters. This process arrived to the apex with the conquest of Greece by Phillip II of Macedon.\n\nThe secret weapon of the Sarissaphoros soldiers (supported by peltast javelineers), commanded by Philipp II in the Battle of Chaeronea (338 BC) and in those that followed fought by Alexander the Great (which brought to the conquest of Lesser Asia, Babylon, Persia and Egypt), was the sarissa, a type of pike, longer and stronger (5–7 m.) than the other Greek lances, obtained from the heavy and strong cornel wood.\n\nRapa Nui, best known as \"Easter Island\", is a typical example of malthusianism, specifically how the exponential growth of a populace leads to the end of a renewable resource. At a certain point, the compelling societal need forces exploitation of the resource above and beyond the resource's natural rate of renewal.\n\nIt has been calculated that after the year 1000, around 10 million palmtrees were cut in Rapa Nui, resulting in the erosion of the fertile land, and eventually to a desertification around the 15th century. (This deforestation may have also been aggravated and/or caused by a rat infestation). This provoked a population reduction from 15,000 to 2,500 individuals. Without palmtree wood, no boats, or lances could be constructed. Without palm fibers, construction of ropes and fishing nets halted. This led to a decrease in the local fish harvest, which in turn led to a decrease in the quantity of dietary protein available to the island's inhabitants. At the end, the society became an easy prey for hunger and civil war. From 1600 to 1700, the people became superstitious in a fanatical way. In the last moments, there was a disintegration of society and total chaos. The destruction of the traditional symbols followed, leading to the eventual extinction of the Moais civilization and culture, even if there was not any external human enemy.\n\n\n\n\n\n"}
{"id": "382142", "url": "https://en.wikipedia.org/wiki?curid=382142", "title": "Ylem", "text": "Ylem\n\nYlem is a term that was used by George Gamow, his student Ralph Alpher, and their associates in the late 1940s for a hypothetical original substance or condensed state of matter, which became subatomic particles and elements as we understand them today. The term \"ylem\" was actually resuscitated (it appears in Webster's Second \"the first substance from which the elements were supposed to have been formed\") by Ralph Alpher.\n\nIn modern understanding, the \"ylem\" described as by Gamow was the primordial plasma, formed in baryogenesis, which underwent Big Bang nucleosynthesis and was opaque to radiation. Recombination of the charged plasma into neutral atoms made the Universe transparent at the age of 380,000 years, and the radiation released is still observable as cosmic microwave background radiation.\n\nIt comes from an obsolete Middle English philosophical word that Alpher said he found in Webster's dictionary. The word means something along the lines of \"primordial substance from which all matter is formed\" (that in ancient mythology of many different cultures was called the cosmic egg) and ultimately derives from the Greek \"ὕλη\" (\"hūlē, hȳlē\"), \"matter\", probably through an accusative singular form in Latin \"hylen, hylem\". In an oral history interview in 1968 Gamow talked about ylem as an old Hebrew word.\n\nThe ylem is what Gamow and colleagues presumed to exist immediately after the Big Bang. Within the ylem, there were assumed to be a large number of high-energy photons present. Alpher and Robert Herman made a scientific prediction in 1948 that we should still be able to observe these red-shifted photons today as an ambient cosmic microwave background radiation (CMBR) pervading all space with a temperature of about 5 kelvins (when the CMBR was actually first detected in 1965, its temperature was found to be 3 kelvins). It is now recognized that the CMBR originated at the transition from predominantly ionized hydrogen to non-ionized hydrogen at around 400,000 years after the Big Bang.\n\nAfter the term \"ylem\" was resuscitated by Alpher, it was used in the 1952 science fiction novel \"Jack of Eagles\" by James Blish. It was also used by John Brunner in his 1959 short story \"Round Trip\", reprinted in the collection \"Not Before Time\". Keith Laumer in the novel \"Dinosaur Beach\" introduces the ylem field 1969. The term is also used by British author Richard Calder in the 1990s to describe the quantum mechanical state of the \"quantum magic\" in the girls/robots in his \"Dead\" trilogy (\"Dead Girls\", \"Dead Boys\", \"Dead Things\"). John C. Wright used the term in his debut novel \"\" to describe a \"pseudo-matter\" that forms \"temporary virtual particles\". A German black metal band \"Dark Fortress\" also released an album titled \"Ylem\". A the trance classic by The Thrillseekers - Synaesthesia has a \"Ylem\" remix. The video game series Ultima uses \"Ylem\" as a Word of Power in its incantation and runic based spell casting system, its meaning being \"matter\".\n\nIn 1981, Trudy Myrrh Reagan formed an organization, \"Ylem: Artists Using Science and Technology\" (later written YLEM), in the San Francisco Bay Area.\n\nIt is also a usable word in the Official Scrabble Players Dictionary, Fifth edition.\n\n"}
