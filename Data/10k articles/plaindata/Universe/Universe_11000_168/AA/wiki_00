{"id": "29603461", "url": "https://en.wikipedia.org/wiki?curid=29603461", "title": "Aerolux Light Corporation", "text": "Aerolux Light Corporation\n\nAerolux Light Corporation was a manufacturer of artful gas-discharge light bulbs from the 1930s through the 1970s. Aerolux made these bulbs in a factory in New York City. US Patents dating back to the 1930s describe the design and construction of these bulbs.\n\nAerolux gas discharge light bulbs contained low pressure gas, either neon or argon, or a mixture of the two. Also within the bulb were metal sculptures coated with phosphors. These phosphors fluoresced when excited by glow discharge. Because glow discharge occurs readily at 110-120 volts AC, one could use these bulbs in standard household lamps in the United States.\n\nThe phosphors used in the bulbs were somewhat brittle, necessitating care in handling. Shaking or jarring the bulbs would cause flaking and migration of the phosphors to other parts of the metallic sculpture. Such handling would leave non-fluorescing portions of the sculpture and/or migration of phosphors to other surfaces within the bulb.\n\nAerolux bulbs consumed about 3-5 watts of power. The bulbs had high yield of light produced versus electricity consumed, generally in the range of 50-60 lumens/watt, compared to 12-18 lumens/watt for a tungsten filament incandescent bulb.\n\n"}
{"id": "2338314", "url": "https://en.wikipedia.org/wiki?curid=2338314", "title": "African Petroleum Producers Association", "text": "African Petroleum Producers Association\n\nAfrican Petroleum Producers Association, APPA (\"Association des pays africains producteurs de pétrole\" in French, \"Associação de Produtores de Petróleo Africanos\" in Portuguese and \"رابطة منتجي النفط الأفريقية\" in Arabic) is an organisation of African countries producing petroleum. It was founded in 1986. The headquarters of the organisation are in Brazzaville in the Congo. The Organization has since changed its name to African Petroleum Producers Organization. \n\nThe founding of APPA was spearheaded by Nigeria as an effort to mitigate the nation's dependency on Western technology and Western markets for oil export revenues. The objective of APPA was to promote cooperation in petrochemical research and technology.\n\nThe following African states are members of the African Petroleum Producers Association:\n\n"}
{"id": "45603629", "url": "https://en.wikipedia.org/wiki?curid=45603629", "title": "Anne Daw", "text": "Anne Daw\n\nAnne Daw is a South Australian campaigner for the protection of water resources and arable land from invasive mining activities.\n\nDaw grew up on her father's grazing property near Kingston in the state's South East region, and became the first community member to participate in the Government of South Australia's Roundtable for the \"Roadmap for Unconventional Gas Projects in South Australia\". She has also held a seat at the Round Table for Health and Energy Policy in Canberra. Daw has collaborated with the Environmental Defenders Office of South Australia, and formed the Limestone Coast Protection Alliance before going on to work independently \n\nDaw began studying the development of unconventional and coal seam gas resources in 2007 and has since become a media spokesperson on the topics of groundwater contamination and risks to agricultural and rural community water supplies associated with fracking.\n"}
{"id": "3306741", "url": "https://en.wikipedia.org/wiki?curid=3306741", "title": "Behenic acid", "text": "Behenic acid\n\nBehenic acid (also docosanoic acid) is a carboxylic acid, the saturated fatty acid with formula CHCOOH. In appearance, it consists of white to cream color crystals or powder with a melting point of 80 °C and boiling point of 306 °C.\n\nAt 9%, it is a major component of Ben oil (or behen oil), which is extracted from the seeds of the Ben-oil tree (\"Moringa oleifera\"). It is so named from the Persian month \"Bahman\", when the roots of this tree were harvested.\n\nBehenic acid is also present in some other oils and oil-bearing plants, including rapeseed (canola) and peanut oil and skins. It is estimated that one ton of peanut skins contains of behenic acid.\n\nAs a dietary oil, behenic acid is poorly absorbed. In spite of its low bioavailability compared with oleic acid, behenic acid is a cholesterol-raising saturated fatty acid in humans.\n\nCommercially, behenic acid is often used to give hair conditioners and moisturizers their smoothing properties. It is also used in lubricating oils, and as a solvent evaporation retarder in paint removers. Its amide is used as an anti-foaming agent in detergents, floor polishes and dripless candles. Reduction of behenic acid yields behenyl alcohol.\n\nPracaxi oil (from the seeds of \"Pentaclethra macroloba\") is a natural product with one of the highest concentrations of behenic acid, and is used in hair conditioners.\n\n"}
{"id": "7529592", "url": "https://en.wikipedia.org/wiki?curid=7529592", "title": "Beltweigher", "text": "Beltweigher\n\nA beltweigher or belt weigher is a piece of industrial control equipment used to gauge the mass or flow rate of material travelling over a troughed (cupped) conveyor belt of any length which is able to adequately contain the material being weighed. These are also known as belt scales, dynamic scales, conveyor scales, and in-motion weighers. Many such check weighers or feed weighers are an active part of the process flow control of the conveyor line.\n\nA belt weigher replaces a short section of the support mechanism of the belt, which might be one or more sets of idler rollers, or a short section of channel or plate. This weighed support is mounted on load cells, either pivoted, counterbalanced or not, or fully suspended. The mass measured by the load cells is integrated with the belt speed to compute the mass of material moving on the belt, after allowing for the mass of the belt itself. Belt weighers generally include the necessary electronics to perform this calculation, often in the form of a small industrialized microprocessor system.\n\nA belt weigher is normally mounted in a well supported straight section of belt, with no vertical or sideways curvature, and as close to level as is practicable. The weighed support must be aligned vertically and horizontally with the adjacent supports to avoid tensile forces in the belt skewing the measurement.\nDue the belt tension variation, frequent check calibration must be done. \n\nOutputs from belt weighers are typically:\n\nIn addition, some belt weigher controllers will offer features such as driving an output to stop the belt when a predefined mass of material has been measured, or a range of alarms to indicate nil flow, belt slippage and belt stoppage.\n\nUses include mineral and aggregate extraction, continuous mixing processes, control of variable rate feeders, port handling and ship loading processes.\n\n"}
{"id": "32306936", "url": "https://en.wikipedia.org/wiki?curid=32306936", "title": "Bilayer graphene", "text": "Bilayer graphene\n\nBilayer graphene is a material consisting of two layers of graphene. One of the first reports of bilayer graphene was in the seminal 2004 \"Science\" paper by Geim and colleagues, in which they described devices \"which contained just one, two, or three atomic layers\"\n\nBilayer graphene can exist in the AB, or Bernal-stacked form, where half of the atoms lie directly over the center of a hexagon in the lower graphene sheet, and half of the atoms lie over an atom, or, less commonly, in the AA form, in which the layers are exactly aligned. In Bernal stacked graphene, twin boundaries are common; transitioning from AB to BA stacking. Twisted layers, where one layer is rotated relative to the other, have also been observed.\n\nQuantum Monte Carlo methods have been used to calculate the binding energies of AA- and AB-stacked bilayer graphene, which are 11.5(9) and 17.7(9) meV per atom, respectively. This is consistent with the observation that the AB-stacked structure is more stable than the AA-stacked structure.\n\nBilayer graphene can be made by exfoliation from graphite or by chemical vapor deposition (CVD). In 2016, Rodney S. Ruoff and colleagues showed that large single-crystal bilayer graphene could be produced by oxygen-activated chemical vapour deposition. Later in the same year a Korean group reported the synthesis of wafer-scale single-crystal AB-stacked bilayer graphene \n\nLike monolayer graphene, bilayer graphene has a zero bandgap and thus behaves like a semimetal. In 2007, researchers predicted that a bandgap could be introduced if an electric displacement field were applied to the two layers: a so-called tunable band gap. An experimental demonstration of a tunable bandgap in bilayer graphene came in 2009. In 2015 researchers observed 1D ballistic electron conducting channels at bilayer graphene domain walls. Another group showed that the band gap of bilayer films on silicon carbide could be controlled by selectively adjusting the carrier concentration.\n\nIn 2014 researchers described the emergence of complex electronic states in bilayer graphene, notably the fractional quantum Hall effect and showed that this could be tuned by an electric field. In 2017 the observation of an even-denominator fractional quantum Hall state was reported in bilayer graphene.\n\nBilayer graphene showed the potential to realize a Bose–Einstein condensate of excitons. Electrons and holes are fermions, but when they form an exciton, they become bosons, allowing Bose-Einstein condensation to occur. Exciton condensates in bilayer systems have been shown theoretically to carry a large current.\n\nPablo Jarillo-Herrero of MIT and colleagues from Harvard and the National Institute for Materials Science, Tsukuba, Japan, have reported the discovery of superconductivity in bilayer graphene with a twist angle of 1.1° between the two layers. The discovery was announced in two papers published in Nature in March 2018. The bilayer graphene was prepared from exfoliated monolayer graphene, with the second layer being manually rotated to a set angle with respect to the first layer. A Tc value of 1.7 K was observed with such specimens. Jarillo-Herrero has suggested that it may be possible to “... imagine making a superconducting transistor out of graphene, which you can switch on and off, from superconducting to insulating. That opens many possibilities for quantum devices.”\n\nBilayer graphene can be used to construct field effect transistors or tunneling field effect transistors, exploiting the small energy gap. However, the energy gap is smaller than 250 meV and therefore requires the use of low operating voltage (< 250 mV), which is too small to obtain reasonable performance for a field effect transistor, but is very suited to the operation of tunnel field effect transistors, which according to theory from a paper in 2009 can operate with an operating voltage of only 100 mV.\n\nIn 2016 researchers proposed the use of bilayer graphene to increase the output voltage of tunnel transistors (TT). They operate at a lower operating voltage range (150 mV) than silicon transistors (500 mV). Bilayer graphene's energy band is unlike that of most semiconductors in that the electrons around the edges form a (high density) van Hove singularity. This supplies sufficient electrons to increase current flow across the energy barrier. Bilayer graphene transistors use \"electrical\" rather than \"chemical\" doping.\n\nIn 2017 an international group of researchers showed that bilayer graphene could act as a single-phase mixed conductor which exhibited Li diffusion faster than in graphite by an order of magnitude. In combination with the fast electronic conduction of graphene sheets, this system offers both ionic and electronic conductivity within the same single-phase solid material. This has important implications for energy storage devices such as lithium ion batteries.\n\nResearchers from the City University of New York have shown that sheets of bilayer graphene on silicon carbide temporarily become harder than diamond upon impact with the tip of an atomic force microscope. This was attributed to a graphite-diamond transition, and the behavior appeared to be unique to bilayer graphene. This could have applications in personal armor.\n\nHybridization processes change the intrinsic properties of graphene and/or induce poor interfaces. In 2014 a general route to obtain unstacked graphene via facile, templated, catalytic growth was announced. The resulting material has a specific surface area of 1628 m2 g-1, is electrically conductive and has a mesoporous structure.\n\nThe material is made with a mesoporous nanoflake template. Graphene layers are deposited onto the template. The carbon atoms accumulate in the mesopores, forming protuberances that act as spacers to prevent stacking. The protuberance density is approximately . Graphene is deposited on both sides of the flakes.\n\nDuring CVD synthesis the protuberances produce intrinsically unstacked double-layer graphene after the removal of the nanoflakes. The presence of such protuberances on the surface can weaken the π-π interactions between graphene layers and thus reduce stacking. The bilayer graphene shows a specific surface area of , a pore size ranging from 2 to 7 nm and a total pore volume of .\n\nUsing bilary graphene as cathode material for a lithium sulfur battery yielded reversible capacities of 1034 and 734 mA h/g at discharge rates of 5 and 10 C, respectively. After 1000 cycles reversible capacities of some 530 and 380 mA h/g were retained at 5 and 10 C, with coulombic efficiency constants at 96 and 98%, respectively.\n\nElectrical conductivity of 438 S/cm was obtained. Even after the infiltration of sulfur, electrical conductivity of 107 S cm/1 was retained. The graphene's unique porous structure allowed the effective storage of sulfur in the interlayer space, which gives rise to an efficient connection between the sulfur and graphene and prevents the diffusion of polysulfides into the electrolyte.\n\nHyperspectral global Raman imaging is an accurate and rapid technique to spatially characterize product quality. The vibrational modes of a system characterize it, providing information on stoichiometry, composition, morphology, stress and number of layers. Monitoring graphene's G and D peaks (around 1580 and 1360 cm) intensity gives direct information on the number of layers of the sample.\n"}
{"id": "2383993", "url": "https://en.wikipedia.org/wiki?curid=2383993", "title": "Campo de los Alisos National Park", "text": "Campo de los Alisos National Park\n\nThe Campo de los Alisos National Park (, literally \"Field of Alders\") is a federal protected area in Tucumán Province, Argentina. Established on 9 August 1995, it houses a representative sample of the southern Yungas montane jungle biodiversity in good state of conservation.\n\nLocated in the Chicligasta Department on the eastern slope of the \"Nevados del Aconquija\", the park has an area of . The Aconquija mountains are the southern extension of the Calchaquíes Valleys, the western first steps raising from the Gran Chaco plain into the Northwest region. Annual rainfall here oscillates between \n\nFlora and fauna vary considerably with the different heights, from a jungle at lower levels to snow-covered mountainous terrain at \n\n"}
{"id": "1068709", "url": "https://en.wikipedia.org/wiki?curid=1068709", "title": "Cuprate", "text": "Cuprate\n\nCuprate loosely refers to a material that can be viewed as containing anionic copper complexes. Examples include tetrachloridocuprate ([CuCl]), the superconductor YBaCuO, and the organocuprates ([Cu(CH)]). The term cuprates derives from the Latin word for copper, \"cuprum\". The term is mainly used in three contexts - oxide materials, anionic coordination complexes, and anionic organocopper compounds.\n\nOne of the simplest oxide-based cuprates is the copper(III) oxide KCuO. This species can be viewed as the K salt of the polyanion []. As such the material is classified as a cuprate. This dark blue diamagnetic solid is produced by heating potassium peroxide and copper(II) oxide in an atmosphere of oxygen:\n\nCopper forms many anionic \"cuprate\" coordination complexes with negatively charged ligands such as cyanide, hydroxide, and halides. Copper(I) derivatives tend to be colorless, copper(II) complexes are often turquoise-blue, and copper(III) and copper(IV) complexes are often orange-red.\nOne example of a copper(I)-based cuprate is tetracyanocuprate(I), [Cu(CN)].\n\nCopper(II) anions are most common, especially the chlorocuprates, such as trichlorocuprate(II) [CuCl], tetrachlorocuprate(II) [CuCl] and pentachlorocuprate(II) [CuCl]. The light blue solid sodium tetrahydroxycuprate is well known; it is prepared by heating cupric hydroxide with concentrated sodium hydroxide.\nDilithium tetrachlorocuprate (LiCuCl) is an effective catalyst for the couplings of alkyl halides in the Grignard reaction. It is prepared by mixing lithium chloride (LiCl) and copper(II) chloride (CuCl) in tetrahydrofuran.\n\nThere are also rare copper(III) and copper(IV) complexes such as the hexafluorocuprate(III) [CuF] and hexafluorocuprate(IV) [CuF], which are strong oxidizing agents.\n\nCuprates have a role in organic synthesis. Organic cuprates often have the formula [CuR] or [CuR], where R is an alkyl or aryl. These reagents find use as nucleophilic alkylating reagents. In stark contrast to the oxidic cuprates, the handling of these organocopper requires air-free techniques\n"}
{"id": "32107340", "url": "https://en.wikipedia.org/wiki?curid=32107340", "title": "Dennis M. Bushnell", "text": "Dennis M. Bushnell\n\nDennis M. Bushnell is a NASA scientist and lecturer. As chief scientist at NASA Langley Research Center, he is responsible for technical oversight and advanced program formulation. His work is focused mainly on new approaches to environmental issues, in particular to climate issues. Bushnell has received numerous awards for his work. Bushnell has promoted research at NASA into LENR (low energy nuclear reactions, or cold fusion).\n\nBushnell obtained his M.E. degree from the University of Connecticut in 1963 and his M.S. degree from the University of Virginia in 1967, both in the field of Mechanical Engineering.\n\n"}
{"id": "29262147", "url": "https://en.wikipedia.org/wiki?curid=29262147", "title": "Double fold", "text": "Double fold\n\nA double fold is a process of folding a paper sample first backwards and then forwards about the same line, i.e. one complete oscillation. The number of double folds that is required to make a test piece break is used to determine the material's folding endurance and fold number.\n\nThe total folding angle (about the folding line) differs depending on which folding tester is used, for instance the Köhler–Molin instrument folds about 156° on each side of the vertical line, resulting in a complete oscillation of about 2 × 312° for each double fold, while the MIT instrument employs a folding angle of about 135° on each side (that is, a complete oscillation of about 2 × 270° for each complete double fold).\n\nStrictly speaking, the first double fold is more than one complete oscillation since the flap needs to be folded from vertical to its starting position before the counting begins.\n\nThe process was analyzed and criticized by Nicholson Baker in his 2001 book \"Double Fold: Libraries and the Assault on Paper\".\n"}
{"id": "634764", "url": "https://en.wikipedia.org/wiki?curid=634764", "title": "Ecotype", "text": "Ecotype\n\nIn evolutionary ecology, an ecotype, sometimes called ecospecies, describes a genetically distinct geographic variety, population or race within a species, which is genotypically adapted to specific environmental conditions.\n\nTypically, though ecotypes exhibit phenotypic differences (such as in morphology or physiology) stemming from environmental heterogeneity, they are capable of interbreeding with other geographically adjacent ecotypes without loss of fertility or vigor.\nAn ecotype is a variant in which the phenotypic differences are too few or too subtle to warrant being classified as a subspecies. These can occur in the same geographic region where distinct habitats such as meadow, forest, swamp, and sand dunes provide ecological niches. Where similar ecological conditions occur in widely separated places it is possible for a similar ecotype to occur. This is different than a subspecies, which may exist across a number of different habitats. In animals, ecotypes can be regarded as micro-subspecies that owe their differing characteristics to the effects of a very local environment. Therefore, ecotypes have no taxonomic rank.\n\nEcotypes are closely related to morphs. In the context of evolutionary biology, genetic polymorphism is the occurrence in the equilibrium of two or more distinctly different phenotypes within a population of a species, in other words, the occurrence of more than one form or morph. The frequency of these discontinuous forms (even that of the rarest) is too high to be explained by mutation. In order to be classified as such, morphs must occupy the same habitat at the same time and belong to a panmictic population (whose all members can potentially interbreed). Polymorphism is actively and steadily maintained in populations of species by natural selection (most famously sexual dimorphism in humans) in contrast to \"transient polymorphisms\" where conditions in a habitat change in such a way that a \"form\" is being replaced completely by another.\n\nIn fact, Begon, Townsend, and Harper assert that\n\nThe notions \"form\" and \"ecotype\" may appear to correspond to a static phenomenon, however; this is not always the case. Evolution occurs continuously both in time and space, so that two ecotypes or forms may qualify as distinct species in only a few generations. Begon, Townsend, and Harper use an illuminating analogy on this:\n\nThus ecotypes and morphs can be thought of as precursory steps of potential speciation.\n\nExperiments indicate that sometimes ecotypes manifest only when separated by great spatial distances (of the order of 1,000 km). This is due to hybridization whereby different but adjacent varieties of the same species (or generally of the same taxonomic rank) interbreed, thus overcoming local selection. However other studies reveal that the opposite may happen, i.e., ecotypes revealing at very small scales (of the order of 10 m), within populations, and despite hybridization.\n\nIn ecotypes, it is common for continuous, gradual geographic variation to impose analogous phenotypic and genetic variation. This situation is called cline. A well-known example of a cline is the skin color gradation in indigenous human populations worldwide, which is related to latitude and amounts of sunlight. But often the distribution of ecotypes is bimodal or multimodal. This means that ecotypes may display two or more distinct and discontinuous phenotypes even within the same population. Such phenomenon may lead to speciation and can occur if conditions in a local environment change dramatically through space or time.\n\n\n"}
{"id": "40149655", "url": "https://en.wikipedia.org/wiki?curid=40149655", "title": "Energy in Kenya", "text": "Energy in Kenya\n\nEnergy in Kenya describes energy and electricity production, consumption, import and export in Kenya. Kenya's current effective installed (grid connected) electricity capacity is 2,351 MW, with peak demand of 1,802 MW, as at June 2018. At that time, demand was rising at a calculated rate of 3.6 percent annually, given that peak demand was 1,770 MW, at the beginning of 2018. Electricity supply is predominantly sourced from hydro and fossil fuel (thermal) sources.\n\nJust until recently the country lacked significant domestic reserves of fossil fuel. The country has over the years had to import substantial amounts of crude oil and natural gas. This might change with the discovery of oil reserves in Kenya, which relied on oil imports to meet about 42 percent of its energy needs in 2010. As of the end of June 2016, 55% of Kenyans were connected to the National grid, which is one of the highest connection rates in Sub-Saharan Africa. Per capita consumption in domestic households however, remains low.\n\nKenya is currently the largest producer of geothermal energy in Africa. It is one of two countries in Africa that produce geothermal energy, the other being Ethiopia. In 2010, geothermal accounted for almost 20 percent of Kenya's total electricity generation. The country has the potential to produce 10,000 megawatts of geothermal-powered electricity, according to Kenya's state-owned Geothermal Development Company. Total renewable energy capacity is at 60%, with most of that coming from Hydro-Power.\n\nIn Kenya, there are plans by the government of Kenya, to end the monopoly of the electricity distribution market; but until that happens, power distribution is only held by one company; Kenya Power and Lighting Company (Kenya Power).\n\nHowever, Kenya Electricity Generating Company (KenGen), is responsible for generating approximately 90% of installed capacity. Independent Power Producers (IPPs) are responsible for about 10% of installed capacity. The following IPPs are active in Kenya: (a) Westmont (b) AEP Energy Africa (Iberafrica) (c) OrPower4 Kenya Limited (a subsidiary of Ormat Technologies) (d) Tsavo Power Company (e) Aggreko\n(f) Africa Geothermal International\n\nThe biggest consumer of electricity in Kenya is Kenya Pipeline Company, followed by Bamburi Cement. As of July 2018, of the 6.5 million Kenya Power's customers, 5 percent or 348,459, were commercial customers (including businesses and factories). Of these, the largest 6,000, were responsible for 60 percent of the national power consumption, averaging in excess of 15,000 electricity units per month.\n\nKenya had one of the largest crude oil refineries in East Africa, the 90,000-barrels-per-day (bbl/d) Mombasa refinery. The refinery typically operated below capacity and processed Murban heavy crude from Abu Dhabi and other heavy Middle-Eastern crude grades. The refinery was shut down in February 2016.\n\nIn 2011, Kenya imported about 33,000 bbl/d of crude oil entirely from the United Arab Emirates, according to the Kenya National Bureau of Statistics (KNBS). Kenya imported 51,000 bbl/d of refined oil products in 2011, according to KNBS. Kenya has a product pipeline system that transports petroleum products from Mombasa to inland areas. \n\nIn 2017, Kenya consumed , of diesel fuel. The same year, the country used , of refined petrol.\n\nIn 2012 oil was discovered in Kenya. As of May 2016, proven reserves were estimated at 766 million barrels. This puts Kenya ahead of Uzbekistan in the global rankings. Tullow Oil, one of the companies prospecting for oil in the country, is of the opinion that the national reserves are in excess of 1 billion barrels.\n\nAfter the collapse of negotiations to build the Uganda-Kenya Crude Oil Pipeline, Kenya began to make plans to build the Kenya Crude Oil Pipeline on its own.\n\nFuelwood demand in the country is 3.5 million tonnes per year while its supply is 1.5 million tonnes per year. The massive deficit in fuelwood supply has led to high rates of deforestation in both exotic and indigenous vegetation resulting in adverse environmental effects such as desertification, land degradation, droughts and famine.\n\nSeven countries came together because they saw mutual benefit in having one power pool. The original countries were: (a) Burundi (b) Democratic Republic of the Congo (c) Egypt (d) Ethiopia (e) Kenya (f) Rwanda and (g) Sudan.\n\nLater, more countries joined the pool, including: (a) Tanzania (b) Libya (c) Djibouti and (d) Uganda.\n\nThe objective of the Eastern Africa Power Pool (EAPP) is to increase the volume and reduce the cost of electricity supply in Kenya; and to provide revenues to Ethiopia through the export of electricity from Ethiopia to Kenya. The process of connecting the Ethiopian grid to the Kenyan grid is underway.\n\nKenya also plans to be connected to the South African grid, through Tanzania, Zambia and Zimbabwe. That process is also underway as of July 2018.\n\nGeothermal power plants, which convert steam generated from hot rocks deep underground into electricity, have a prominent place in Kenya’s overarching development plans. These include the Vision 2030, the NCCAP, and the current ‘5000+ MW in 40 months initiative’. Geothermal power has the potential to provide reliable, cost-competitive, baseload power with a small carbon footprint, and reduces vulnerability to climate by diversifying power supply away from hydropower, which currently provides the majority of Kenya’s electricity. Kenya has set out ambitious targets for geothermal energy. It aims to expand its geothermal power production capacity to 5,000 MW by 2030, with a medium-term target of installing 1,887 MW by 2017. As of October 2014, Kenya has an installed geothermal capacity of approximately 340 MW. Although there is significant political will and ambition, reaching these ambitions is a major challenge.\nKenya Electricity Generating Company (KenGen) and Geothermal Development Company aim at raising the country's geothermal output from the current 593MW, to 1 GW by the year 2018 and 5 GW to the grid by 2030.\n\nIn 2017, the Kenya Nuclear Electrification Board (Kneb) estimated that a 1,000 MW nuclear plant could be operational by 2027 and cost Ksh500-600 billion ($5-$6 billion).\nIn September 2010 Former Energy and Petroleum Ministry PS Patrick Nyoike announced, that Kenya aims to build a 1,000 MW nuclear power plant between 2017–2022. \nFor Kenya to achieve middle-income status, Nyoike viewed nuclear energy as the best way to produce safe, clean, reliable and base load (constant supply) electricity. The projected cost using South Korean technology was US$3.5 billion.\n\nKenya emits .03 percent of the world carbon dioxide, which is about 12.62 (Million Metric Tons of CO₂).\n\n\n"}
{"id": "623774", "url": "https://en.wikipedia.org/wiki?curid=623774", "title": "Environmental impact of electricity generation", "text": "Environmental impact of electricity generation\n\nElectric power systems consist of generation plants of different energy sources, transmission networks, and distribution lines. Each of these components can have environmental impacts at multiple stages of their development and use including in their construction, during the generation of electricity, and in their decommissioning and disposal. We can split these impacts into operational impacts (fuel sourcing, global atmospheric and localized pollution) and construction impacts (manufacturing, installation, decommissioning, and disposal). This page looks exclusively at the operational environmental impact of electricity generation. The page is organized by energy source and includes impacts such as water usage, emissions, local pollution, and wildlife displacement.\n\nMore detailed information on electricity generation impacts for specific technologies and on other environmental impacts of electric power systems in general can be found under the .\n\nWater usage is one of the most obvious environmental impacts of electricity generation. All thermal cycles (coal, natural gas, nuclear, geothermal, and biomass) use water as a cooling fluid to drive the thermodynamic cycles that allow electricity to be extracted from heat energy. Other energy sources such as wind and solar use water for cleaning equipment, while hydroelectricity has water usage from evaporation from the reservoirs. The amount of water usage is often of great concern for electricity generating systems as populations increase and droughts become a concern. In addition, changes in water resources may impact the reliability of electricity generation. The power sector in the United States withdraws more water than any other sector and is heavily dependent on available water resources. According to the U.S. Geological Survey, in 2005, thermo-electric power generation water withdrawals accounted for 41 percent (201 Bgal/d) of all freshwater withdrawals. Nearly all of the water withdrawn for thermoelectric power was surface water used for once-through cooling at power plants. Withdrawals for irrigation and public supply in 2005 were 37% and 13% of all freshwater withdrawals respectively. Likely future trends in water consumption are covered here.\n\nDiscussions of water usage of electricity generation distinguish between water withdrawal and water consumption. According to the USGS, “withdrawal” is defined as the amount of water removed from the ground or diverted from a water source for use, while “consumption” refers to the amount of water that is evaporated, transpired, incorporated into products or crops, or otherwise removed from the immediate water environment. Both water withdrawal and consumption are important environmental impacts to evaluate.\n\nGeneral numbers for fresh water usage of different power sources are shown below.\n\nSteam-cycle plants (nuclear, coal, NG, solar thermal) require a great deal of water for cooling, to remove the heat at the steam condensers. The amount of water needed relative to plant output will be reduced with increasing boiler temperatures. Coal- and gas-fired boilers can produce high steam temperatures and so are more efficient, and require less cooling water relative to output. Nuclear boilers are limited in steam temperature by material constraints, and solar is limited by concentration of the energy source.\n\nThermal cycle plants near the ocean have the option of using seawater. Such a site will not have cooling towers and will be much less limited by environmental concerns of the discharge temperature since dumping heat will have very little effect on water temperatures. This will also not deplete the water available for other uses. Nuclear power in Japan for instance, uses no cooling towers at all because all plants are located on the coast. If dry cooling systems are used, significant water from the water table will not be used. Other, more novel, cooling solutions exist, such as sewage cooling at the Palo Verde Nuclear Generating Station.\n\nHydroelectricity's main cause of water usage is both evaporation and seepage into the water table.\n\nReference: Nuclear Energy Institute factsheet using EPRI data and other sources.\n\nSource(s): Adapted from US Department Of Energy, Energy Demand on Water Resources. Report to Congress on the Interdependence of Energy and Water, December 2006 (except where noted).*Cambridge Energy Research Associates (CERA) estimate. #Educated estimate.Water Requirements for Existing and Emerging Thermoelectric Plant Technologies. US Department Of Energy, National Energy Technology Laboratory, August 2008.Note(s): 3.6 GJ = gigajoule(s) == 1 MW·h = megawatt-hour(s), thus 1 L/GJ = 3.6 L/MW·h. B = Black coal (supercritical)-(new subcritical), Br = Brown coal (new subcritical), H = Hard coal, L = Lignite, cc = combined cycle, oc = open cycle, T = low-temperature/closed-circuit (geothermal doublet), T = high-temperature/open-circuit.\n\nMost electricity today is generated by burning fossil fuels and producing steam which is then used to drive a steam turbine that, in turn, drives an electrical generator.\n\nSuch systems allow electricity to be generated where it is needed, since fossil fuels can readily be transported. They also take advantage of a large infrastructure designed to support consumer automobiles. The world's supply of fossil fuels is large, but finite. Exhaustion of low-cost fossil fuels will have significant consequences for energy sources as well as for the manufacture of plastics and many other things. Various estimates have been calculated for exactly when it will be exhausted (see Peak oil). New sources of fossil fuels keep being discovered, although the rate of discovery is slowing while the difficulty of extraction simultaneously increases.\n\nMore serious are concerns about the emissions that result from fossil fuel burning. Fossil fuels constitute a significant repository of carbon buried deep underground. Burning them results in the conversion of this carbon to carbon dioxide, which is then released into the atmosphere. The estimated CO2 emission from the world's electrical power industry is 10 billion tonnes yearly. This results in an increase in the Earth's levels of atmospheric carbon dioxide, which enhances the greenhouse effect and contributes to global warming. The linkage between increased carbon dioxide and global warming is well accepted, though fossil-fuel producers vigorously contest these findings.\n\nDepending on the particular fossil fuel and the method of burning, other emissions may be produced as well. Ozone, sulfur dioxide, NO and other gases are often released, as well as particulate matter. Sulfur and nitrogen oxides contribute to smog and acid rain. In the past, plant owners addressed this problem by building very tall flue-gas stacks, so that the pollutants would be diluted in the atmosphere. While this helps reduce local contamination, it does not help at all with global issues.\n\nFossil fuels, particularly coal, also contain dilute radioactive material, and burning them in very large quantities releases this material into the environment, leading to low levels of local and global radioactive contamination, the levels of which are, ironically, higher than a nuclear power station as their radioactive contaminants are controlled and stored.\n\nCoal also contains traces of toxic heavy elements such as mercury, arsenic and others. Mercury vaporized in a power plant's boiler may stay suspended in the atmosphere and circulate around the world. While a substantial inventory of mercury exists in the environment, as other man-made emissions of mercury become better controlled, power plant emissions become a significant fraction of the remaining emissions. Power plant emissions of mercury in the United States are thought to be about 50 tons per year in 2003, and several hundred tons per year in China. Power plant designers can fit equipment to power stations to reduce emissions.\n\nAccording to Environment Canada:\n\"The electricity sector is unique among industrial sectors in its very large contribution to emissions associated with nearly all air issues. Electricity generation produces a large share of Canadian nitrogen oxides and sulphur dioxide emissions, which contribute to smog and acid rain and the formation of fine particulate matter. It is the largest uncontrolled industrial source of mercury emissions in Canada. Fossil fuel-fired electric power plants also emit carbon dioxide, which may contribute to climate change. In addition, the sector has significant impacts on water and habitat and species. In particular, hydro dams and transmission lines have significant effects on water and biodiversity.\"\nCoal mining practices in the United States have also included strip mining and removing mountain tops. Mill tailings are left out bare and have been leached into local rivers and resulted in most or all of the rivers in coal producing areas to run red year round with sulfuric acid that kills all life in the rivers.\n\nThe efficiency of some of these systems can be improved by cogeneration and geothermal (combined heat and power) methods. Process steam can be extracted from steam turbines. Waste heat produced by thermal generating stations can be used for space heating of nearby buildings. By combining electric power production and heating, less fuel is consumed, thereby reducing the environmental effects compared with separate heat and power systems.\n\nElectric cars burn no petroleum, thereby shifting any environmental impact from the car user to the electric utility. In South Africa an electric car, will be powered by coal generated electricity and harm the environment. In Norway an electric car will be powered by hydroelectricity and be harmless. Electric cars by themselves are neither beneficial or harmful, it depends how your region generates electricity.\n\nHomeowners can get 90% efficiency using natural gas to heat their home. Heat pumps are very efficient and burn no natural gas, shifting the environmental impacts from homeowners to electric utilities. Switching from natural gas to electricity in Alberta Canada burns natural gas and coal at about a 40% efficiency to supply the heat pump. In Quebec Canada where electric resistance heating is common, the heat pump will use 70% less hydroelectricity. Heat pumps may be beneficial for the environment or not, it depends how your region generates electricity.\n\nNuclear power plants do not burn fossil fuels and so do not directly emit carbon dioxide; because of the high energy yield of nuclear fuels, the carbon dioxide emitted during mining, enrichment, fabrication and transport of fuel is small when compared with the carbon dioxide emitted by fossil fuels of similar energy yield.\n\nA large nuclear power plant may reject waste heat to a natural body of water; this can result in undesirable increase of the water temperature with adverse effect on aquatic life.\n\nEmission of radioactivity from a nuclear plant is controlled by regulations. Abnormal operation may result in release of radioactive material on scales ranging from minor to severe, although these scenarios are very rare.\n\nMining of uranium ore can disrupt the environment around the mine. Disposal of spent fuel is controversial, with many proposed long-term storage schemes under intense review and criticism. Diversion of fresh or spent fuel to weapons production presents a risk of nuclear proliferation. Finally, the structure of the reactor itself becomes radioactive and will require decades of storage before it can be economically dismantled and in turn disposed of as waste.\n\nRenewable power technologies can have significant environmental benefits. Unlike coal and natural gas, they can generate electricity and fuels without releasing significant quantities of CO2 and other greenhouse gases that contribute to climate change, however the greenhouse gas savings from a number of biofuels have been found to be much less than originally anticipated, as discussed in the article Indirect land use change impacts of biofuels.\n\nBoth solar and wind have been criticized from an aesthetic point of view. However, methods and opportunities exist to deploy these renewable technologies efficiently and unobtrusively: fixed solar collectors can double as noise barriers along highways, and extensive roadway, parking lot, and roof-top area is currently available; amorphous photovoltaic cells can also be used to tint windows and produce energy. Advocates of renewable energy also argue that current infrastructure is less aesthetically pleasing than alternatives, but sited further from the view of most critics.\n\nThe major advantage of conventional hydroelectric dams with reservoirs is their ability to store potential power for later electrical production. The combination of a natural supply of energy and production on demand has made hydro power the largest source of renewable energy by far. Other advantages include longer life than fuel-fired generation, low operating costs, and the provision of facilities for water sports. Some dams also operate as pumped-storage plants balancing supply and demand in the generation system. Overall, hydroelectric power can be less expensive than electricity generated from fossil fuels or nuclear energy, and areas with abundant hydroelectric power attract industry.\n\nHowever, in addition to the advantages above, there are several disadvantages to dams that create large reservoirs. These may include: dislocation of people living where the reservoirs are planned, release of significant amounts of carbon dioxide at construction and flooding of the reservoir, disruption of aquatic ecosystems and bird life, adverse impacts on the river environment, potential risks of sabotage and terrorism, and in rare cases catastrophic failure of the dam wall.\n\nSome dams only generate power and serve no other purpose, but in many places large reservoirs are needed for flood control and/or irrigation, adding a hydroelectric portion is a common way to pay for a new reservoir. Flood control protects life/property and irrigation supports increased agriculture. Without power turbines, the downstream river environment would improve in several ways, however dam and reservoir concerns would remain unchanged.\n\nSmall hydro and run-of-the-river are two low impact alternatives to hydroelectric reservoirs, although they may produce intermittent power due to a lack of stored water.\n\nLand constrictions such as straits or inlets can create high velocities at specific sites, which can be captured with the use of turbines. These turbines can be horizontal, vertical, open, or ducted and are typically placed near the bottom of the water column.\n\nThe main environmental concern with tidal energy is associated with blade strike and entanglement of marine organisms as high speed water increases the risk of organisms being pushed near or through these devices. As with all offshore renewable energies, there is also a concern about how the creation of EMF and acoustic outputs may affect marine organisms. Because these devices are in the water, the acoustic output can be greater than those created with offshore wind energy. Depending on the frequency and amplitude of sound generated by the tidal energy devices, this acoustic output can have varying effects on marine mammals (particularly those who echolocate to communicate and navigate in the marine environment such as dolphins and whales). Tidal energy removal can also cause environmental concerns such as degrading farfield water quality and disrupting sediment processes. Depending on the size of the project, these effects can range from small traces of sediment build up near the tidal device to severely affecting nearshore ecosystems and processes.\n\nTidal barrages are dams built across the entrance to a bay or estuary that captures potential tidal energy with turbines similar to a conventional hydrokinetic dam. Energy is collected while the height difference on either side of the dam is greatest, at low or high tide. A minimum height fluctuation of 5 meters is required to justify the construction, so only 40 locations worldwide have been identified as feasible.\n\nInstalling a barrage may change the shoreline within the bay or estuary, affecting a large ecosystem that depends on tidal flats. Inhibiting the flow of water in and out of the bay, there may also be less flushing of the bay or estuary, causing additional turbidity (suspended solids) and less saltwater, which may result in the death of fish that act as a vital food source to birds and mammals. Migrating fish may also be unable to access breeding streams, and may attempt to pass through the turbines. The same acoustic concerns apply to tidal barrages. Decreasing shipping accessibility can become a socio-economic issue, though locks can be added to allow slow passage. However, the barrage may improve the local economy by increasing land access as a bridge. Calmer waters may also allow better recreation in the bay or estuary.\n\nElectrical power can be generated by burning anything which will combust. Some electrical power is generated by burning crops which are grown specifically for the purpose. Usually this is done by fermenting plant matter to produce ethanol, which is then burned. This may also be done by allowing organic matter to decay, producing biogas, which is then burned. Also, when burned, wood is a form of biomass fuel.\n\nBurning biomass produces many of the same emissions as burning fossil fuels. However, growing biomass captures carbon dioxide out of the air, so that the net contribution to global atmospheric carbon dioxide levels is small.\n\nThe process of growing biomass is subject to the same environmental concerns as any kind of agriculture. It uses a large amount of land, and fertilizers and pesticides may be necessary for cost-effective growth. Biomass that is produced as a by-product of agriculture shows some promise, but most such biomass is currently being used, for plowing back into the soil as fertilizer if nothing else.\n\nWind power harnesses mechanical energy from the constant flow of air over the surface of the earth. Wind power stations generally consist of wind farms, fields of wind turbines in locations with relatively high winds. A primary publicity issue regarding wind turbines are their older predecessors, such as the Altamont Pass Wind Farm in California. These older, smaller, wind turbines are rather noisy and densely located, making them very unattractive to the local population. The downwind side of the turbine does disrupt local low-level winds. Modern large wind turbines have mitigated these concerns, and have become a commercially important energy source. Many homeowners in areas with high winds and expensive electricity set up small wind turbines to reduce their electric bills.\n\nA modern wind farm, when installed on agricultural land, has one of the lowest environmental impacts of all energy sources:\n\nLandscape and heritage issues may be a significant issue for certain wind farms. However, when appropriate planning procedures are followed, the heritage and landscape risks should be minimal. Some people may still object to wind farms, perhaps on the grounds of aesthetics, but there is still the supportive opinions of the broader community and the need to address the threats posed by climate change.\n\nOffshore wind is similar to terrestrial wind technologies, as a large windmill-like turbine located in a fresh or saltwater environment. Wind causes the blades to rotate, which is then turned into electricity and connected to the grid with cables. The advantages of offshore wind are that winds are stronger and more consistent, allowing turbines of much larger size to be erected by vessels. The disadvantages are the difficulties of placing a structure in a dynamic ocean environment.\n\nThe turbines are often scaled-up versions of existing land technologies. However, the foundations are unique to offshore wind and are listed below:\n\nMonopile foundations are used in shallow depth applications (0–30 m) and consist of a pile being driven to varying depths into the seabed (10–40 m) depending on the soil conditions. The pile-driving construction process is an environmental concern as the noise produced is incredibly loud and propagates far in the water, even after mitigation strategies such as bubble shields, slow start, and acoustic cladding. The footprint is relatively small, but may still cause scouring or artificial reefs. Transmission lines also produce an electromagnetic field that may be harmful to some marine organisms.\n\nTripod fixed bottom foundations are used in transitional depth applications (20–80 m) and consist of three legs connecting to a central shaft that supports the turbine base. Each leg has a pile driven into the seabed, though less depth is necessary because of the wide foundation. The environmental effects are a combination of those for monopile and gravity foundations.\n\nGravity foundations are used in shallow depth applications (0–30 m) and consist of a large and heavy base constructed of steel or concrete to rest on the seabed. The footprint is relatively large and may cause scouring, artificial reefs, or physical destruction of habitat upon introduction. Transmission lines also produce an electromagnetic field that may be harmful to some marine organisms.\n\nGravity tripod foundations are used in transitional depth applications (10–40 m) and consist of two heavy concrete structures connected by three legs, one structure sitting on the seabed while the other is above the water. As of 2013, no offshore windfarms are currently using this foundation. The environmental concerns are identical to those of gravity foundations, though the scouring effect may be less significant depending on the design.\n\nFloating structure foundations are used in deep depth applications (40–900 m) and consist of a balanced floating structure moored to the seabed with fixed cables. The floating structure may be stabilized using buoyancy, the mooring lines, or a ballast. The mooring lines may cause minor scouring or a potential for collision. Transmission lines also produce an electromagnetic field that may be harmful to some marine organisms.\n\nGeothermal energy is the heat of the Earth, which can be tapped into to produce electricity in power plants. Warm water produced from geothermal sources can be used for industry, agriculture, bathing and cleansing. Where underground steam sources can be tapped, the steam is used to run a steam turbine. Geothermal steam sources have a finite life as underground water is depleted. Arrangements that circulate surface water through rock formations to produce hot water or steam are, on a human-relevant time scale, renewable.\n\nWhile a geothermal power plant does not burn any fuel, it will still have emissions due to substances other than steam which come up from the geothermal wells. These may include hydrogen sulfide, and carbon dioxide. Some geothermal steam sources entrain non-soluble minerals that must be removed from the steam before it is used for generation; this material must be properly disposed. Any (closed cycle) steam power plant requires cooling water for condensers; diversion of cooling water from natural sources, and its increased temperature when returned to streams or lakes, may have a significant impact on local ecosystems.\n\nRemoval of ground water and accelerated cooling of rock formations can cause earth tremors. Enhanced geothermal systems (EGS) fracture underground rock to produce more steam; such projects can cause earthquakes. Certain geothermal projects (such as one near Basel, Switzerland in 2006) have been suspended or canceled owing to objectionable seismicity induced by geothermal recovery. However, risks associated with \"hydrofracturing induced seismicity are low compared to that of natural earthquakes, and can be reduced by careful management and monitoring\" and \"should not be regarded as an impediment to further development of the Hot Rock geothermal energy resource\".\n\nCurrently solar photovoltaic power is used primarily in Germany and Spain where the governments offer financial incentives. In the U.S., Washington State also provides financial incentives. Photovoltaic power is also more common, as one might expect, in areas where sunlight is abundant.\n\nIt works by converting the sun's radiation into direct current (DC) power by use of photovoltaic cells. This power can then be converted into the more common AC power and fed to the power grid.\n\nSolar photovoltaic power offers a viable alternative to fossils fuels for its cleanliness and supply, although at a high production cost. Future technology improvements are expected to bring this cost down to a more competitive range.\n\nIts negative impact on the environment lies in the creation of the solar cells which are made primarily of silica (from sand) and the extraction of silicon from silica may require the use of fossil fuels, although newer manufacturing processes have eliminated CO production. Solar power carries an upfront cost to the environment via production, but offers clean energy throughout the lifespan of the solar cell.\n\nLarge scale electricity generation using photovoltaic power requires a large amount of land, due to the low power density of photovoltaic power. Land use can be reduced by installing on buildings and other built up areas, though this reduces efficiency.\n\nAlso known as solar thermal, this technology uses various types of mirrors to concentrate sunlight and produce heat. This heat is used to generate electricity in a standard Rankine cycle turbine. Like most thermoelectric power generation, this consumes water. This can be a problem, as solar powerplants are most commonly located in a desert environment due to the need for sunlight and large amounts of land. Many concentrated solar systems also use exotic fluids to absorb and collect heat while remaining at low pressure. These fluids could be dangerous if spilled.\n\nNegawatt power refers to investment to reduce electricity consumption rather than investing to increase supply capacity. In this way investing in Negawatts can be considered as an alternative to a new power station and the costs and environmental concerns can be compared.\n\nNegawatt investment alternatives to reduce consumption by improving efficiency include:\n\nNegawatt investment alternatives to reduce peak electrical load by time shifting demand include:\n\nNote that time shifting does not reduce total energy consumed or system efficiency; however, it can be used to avoid the need to build a new power station to cope with a peak load.\n\n\n"}
{"id": "25729952", "url": "https://en.wikipedia.org/wiki?curid=25729952", "title": "February 1969 nor'easter", "text": "February 1969 nor'easter\n\nThe February 1969 nor'easter was a severe winter storm that affected the Mid-Atlantic and New England regions of the United States between February 8 and February 10. The nor'easter developed on February 8, and as it moved towards the northeast, intensifying to become a powerful storm. The system dropped paralyzing snowfall, often exceeding . New York City bore the brunt of the storm, suffering extensive disruption. Thousands of travelers became stranded on roads and in airports. Overall, at least 94 people lost their lives to the storm. Following the event, the mayor of New York, John Lindsay, was criticized for failing to respond to the snowstorm adequately. Some areas of the city remained uncleared for over a week after the storm, and city schools were closed for several days.\n\nAn area of low pressure moved generally eastward from Oklahoma and produced heavy rains from Missouri to Ohio on February 8. By February 9, it had reached Kentucky. A new, secondary low pressure system formed over Georgia along the warm front associated with the primary low. As the secondary low matured along the U.S. East Coast, the initial center weakened rapidly, and heavy rainfall developed over the Carolinas in association with the new low. Mixed precipitation soon spread across the Mid-Atlantic States, and heavy snow began to fall from New Jersey northward by 1200 UTC on February 9.\n\nThe primary low dissipated, and the secondary low continued to intensify as it moved northeastward from the North Carolina coast to Long Island. Its forward motion slowed substantially, leading to increased precipitation totals over land. By 0000 UTC on February 10, the storm deepened to 970 millibars, having strengthened 32 millibars in an 18-hour period. At 1200 UTC, it was situated off Cape Cod, still an intense cyclone. On February 11, the storm moved out of the region.\n\nThe storm produced paralyzing snowfall from New Jersey through most of New England. Forecasts severely underestimated the duration of the storm, often predicting just a chance of snow. The highest totals—often exceeding —were reported in the Bangor, Maine area with Lewiston, Maine topping . Lesser accumulations up to —occurred in areas south to western Connecticut, Massachusetts, southern Vermont, northern Rhode Island, and eastern New Hampshire. Lighter snowfall extended as far south as central Virginia, and as far west as Indiana. The snow was accompanied by high winds, in some areas reaching . Heavy snow and gale warnings were declared across the region. Tides along the coast ran above normal during the storm.\n\nNew York City was struck particularly hard by the storm. Central Park reported of snow, and John F. Kennedy International Airport reported . It is estimated that 42 people perished, and several hundred more people were injured. The storm disrupted the city for days, and forced schools to close. Streets throughout Queens became impassable; mail service, buses, taxis, delivery vehicles, and trash collection were all disrupted. Thousands of motorists became trapped on the New York State Thruway. A snow emergency was issued in the city, and the Long Island Rail Road suspended all service at the time. The snowstorm left approximately 6,000 travelers stranded at Kennedy Airport. They slept on chairs and floors. Over 1,000 vehicles were stalled or abandoned on the Tappan Zee Bridge; most of these were removed within a day.\n\nOverall, at least 94 deaths were attributed to the storm. Throughout the region, the lack of delivery trucks also led to a shortage of food staples such as milk and bread.\n\nFollowing the storm, then-mayor John Lindsay was criticized for not dealing with the snow adequately. Portions of the city remained unplowed a week after the nor'easter, leading the mayor into \"political misfortune\". Lindsay's visit to Queens was poorly received, and his limousine had trouble driving through the streets of Rego Park. The mayor was booed by residents of Kew Gardens Hills. The storm became known as the \"Lindsay Snowstorm\", and created a political crisis; as a result, Lindsay lost the Republican primary for the next mayoral election. Lindsay was able to win the mayoral election by running on a third-party ticket, but he was politically weakened by the crisis.\n\nThe storm also had an economic impact. The New York Stock Exchange (NYSE) and American Stock Exchange (AMEX) closed as a result of the storm. It was the first time in history that the NYSE closed for a full day due to the weather, and the first time since 1918 that AMEX had done so. All commodity exchanges in New York City and the National Association of Securities Dealers also closed.\n\n\n\n"}
{"id": "1834678", "url": "https://en.wikipedia.org/wiki?curid=1834678", "title": "Fuel element failure", "text": "Fuel element failure\n\nA fuel element failure is a rupture in a nuclear reactor's fuel cladding that allows the nuclear fuel or fission products, either in the form of dissolved radioisotopes or hot particles, to enter the reactor coolant or storage water.\n\nThe \"de facto\" standard nuclear fuel is uranium dioxide or a mixed uranium/plutonium dioxide. This has a higher melting point than the actinide metals. Uranium dioxide resists corrosion in water and provides a stable matrix for many of the fission products; however, to prevent fission products (such as the noble gases) from leaving the uranium dioxide matrix and entering the coolant, the pellets of fuel are normally encased in tubes of a corrosion-resistant metal alloy (normally Zircaloy for water-cooled reactors).\n\nThose elements are then assembled into bundles to allow good handling and cooling. As the fuel fissions, the radioactive fission products are also contained by the cladding, and the entire fuel element can then be disposed of as nuclear waste when the reactor is refueled.\n\nIf, however, the cladding is damaged, those fission products (which are not immobile in the uranium dioxide matrix) can enter the reactor coolant or storage water and can be carried out of the core, into the rest of the primary cooling circuit, increasing contamination levels there.\n\nIn the EU, some work has been done in which fuel is overheated in a special research reactor named PHEBUS. During these experiments the emissions of radioactivity from the fuel are measured and afterwards the fuel is subjected to Post Irradiation Examination to discover more about what happened to it.\n"}
{"id": "45328806", "url": "https://en.wikipedia.org/wiki?curid=45328806", "title": "Galaxy X (galaxy)", "text": "Galaxy X (galaxy)\n\nGalaxy X is a postulated dark satellite dwarf galaxy of the Milky Way Galaxy. If it exists, it would be composed mostly of dark matter and interstellar gas with few stars. Its proposed location is some from the Sun, behind the disk of the Milky Way, and some in extent. Galactic coordinates would be (l=\n-27.4°,b=-1.08°).\n\nObservational evidence for this galaxy was presented in 2015, based on the claimed discovery of four, Cepheid variable stars by Sukanya Chakrabarti (RIT) and collaborators. Search for the stars was motivated by an earlier study that linked a warp in the HI (atomic hydrogen) disk of the Milky Way Galaxy to the tidal effects of a perturbing galaxy. The unseen perturber's mass was calculated to be about 1% of that of the Milky Way, which would make it the third heaviest satellite of the Milky Way, after the Magellanic Clouds (Large Magellanic Cloud and Small Magellanic Cloud, each some 10x larger than Galaxy X). In this hypothetical model, the putative satellite galaxy would have interacted with the Milky Way some 600 million years ago, coming as close as , and would now be moving away from the Milky Way.\n\nThe name \"Galaxy X\" was coined in 2011 in analogy to Planet X.\nIn November 2015, a group led by P. Pietrukowicz published a paper arguing against the existence of Galaxy X. These authors argued that the four stars were not actually Cepheid variable stars and that their distances might be very different than claimed in the discovery paper of Chakrabarti et al. On this basis, the authors stated that \"there is no evidence for a background dwarf galaxy\". However the galaxy is still regarded to exist by others, with the stars being examined to be actual Cepheids.\n\nList of claimed components of \"Galaxy X\"\n"}
{"id": "6681252", "url": "https://en.wikipedia.org/wiki?curid=6681252", "title": "Gallium phosphate", "text": "Gallium phosphate\n\nGallium phosphate (GaPO or gallium orthophosphate) is a colorless trigonal crystal with a hardness of 5.5 on the Mohs scale. GaPO is isotypic with quartz, possessing very similar properties, but the silicon atoms are alternately substituted with gallium and phosphorus, thereby doubling the piezoelectric effect. GaPO has many advantages over quartz for technical applications, like a higher electromechanical coupling coefficient in resonators, due to this doubling.\nContrary to quartz, GaPO is not found in nature. Therefore, a hydrothermal process must be used to synthesize the crystal.\n\nGaPO possesses, in contrast to quartz, no α-β phase transition, thus the low temperature structure (structure like α-quartz) of GaPO is stable up to 970°C, as are most of its other physical properties. Around 970°C another phase transition occurs which changes the low quartz structure into another structure similar with cristobalite.\n\nThe specific structure of GaPO shows the arrangement of tetrahedrons consisting of GaO and PO that are slightly tilted. Because of the helical arrangement of these tetrahedrons, two modifications of GaPO exist with different optical rotation (left and right).\n\nGaPO does not occur in nature; thus it must be grown synthetically. Presently, only one company in Austria produces these crystals commercially.\n\nPressure sensors based on quartz have to be cooled with water for applications at higher temperatures (above 300°C). Starting in 1994 it was possible to substitute these big sensors with miniaturized, non cooled ones, based on GaPO.\nFurther exceptional properties of GaPO for applications at high temperatures include its nearly temperature independent piezo effect and excellent electrical insulation up to 900°C. For bulk resonator applications, this crystal exhibits temperature compensated cuts of up to 500°C while having Q factors comparable with quartz. Due to these material properties, GaPO is very suitable for piezoelectric pressure sensors at high temperatures and for high temperature microbalance.\n\n"}
{"id": "7663519", "url": "https://en.wikipedia.org/wiki?curid=7663519", "title": "Generalized Maxwell model", "text": "Generalized Maxwell model\n\nThe Generalized Maxwell model also known as the Maxwell–Wiechert model (after James Clerk Maxwell and E Wiechert) is the most general form of the linear model for viscoelasticity. In this model several Maxwell elements are assembled in parallel. It takes into account that the relaxation does not occur at a single time, but in a set of times. Due to the presence of molecular segments of different lengths, with shorter ones contributing less than longer ones, there is a varying time distribution. The Wiechert model shows this by having as many spring–dashpot Maxwell elements as are necessary to accurately represent the distribution. The figure on the right shows the generalised Wiechert model.\n\nGiven formula_1 elements with moduli formula_2, viscosities formula_3, and relaxation times formula_4\n\nThe general form for the model for solids is given by :\n</math>\nformula_5\nformula_6\nformula_7\n\n</math>\nformula_8\nformula_9\n\nformula_10\nformula_9\nformula_12\nformula_5\nformula_6\nformula_15\nformula_16\nformula_9\n\nformula_18\nformula_9\nformula_20\n\nFollowing the above model with formula_21 elements yields the standard linear solid model:\n</math>\n\nGiven formula_1 elements with moduli formula_2, viscosities formula_3, and relaxation times formula_4\n\nThe general form for the model for liquids is given by:\n</math>\nformula_5\nformula_27\n\n</math>\nformula_8\nformula_9\n\nformula_10\nformula_9\nformula_12\nformula_5\nformula_34\nformula_35\nformula_9\n\nformula_37\nformula_9\nformula_39\n\nThe analogous model to the standard linear solid model is the three parameter fluid, also known as the Jeffrey model:\n</math>\n"}
{"id": "1306003", "url": "https://en.wikipedia.org/wiki?curid=1306003", "title": "Gluaiseacht", "text": "Gluaiseacht\n\nGluaiseacht for Global Justice is an Irish environmental, peace and social justice group. \"Gluaiseacht\" ( means \"movement\" in the Irish language. The group believes in non-violent resistance to the current form of capitalist globalisation. It was originally a network of Ecological and One World Societies at universities and colleges throughout Ireland. It is a member of the Irish Environmental Network.\n\nGluaiseacht has been involved in anti-nuclear protests at the Trident submarine base in Faslane, Scotland, and the Sellafield nuclear plant.\n\nGluaiseacht organised the 2002 Ecotopia gathering (an annual summer camp for activists in Europe) in association with EYFA\n\n\n"}
{"id": "2871250", "url": "https://en.wikipedia.org/wiki?curid=2871250", "title": "Grid connection", "text": "Grid connection\n\nIn electrical grids, a power system network integrates transmission grids, distribution grids, distributed generators and loads that have connection points called busses. A bus in home circuit breaker panels is much smaller than those used on the grid, where busbars can be 50 mm in diameter in electrical substations. Traditionally, these grid connections are unidirectional point to multipoint links. In distributed generation grids, these connections are bidirectional, and the reverse flow can raise safety and reliability concerns. Features in smart grids are designed to manage these conditions.\n\nA premises is generally said to have obtained \"grid connection\" when its service location becomes powered by a live connection to its service transformer.\n\nA power station is generally said to have achieved \"grid connection\" when it first supplies power outside of its own boundaries. However, a town is only said to have achieved \"grid connection\" when it is connected to several redundant sources, generally involving long-distance transmission.\n\nA balancing autorithy is the responsible entity that integrates resource plans ahead of time, maintains load-interchange-generation balance within a Balancing Authority Area, and supports Interconnection frequency in real time. \n\nThe transmission facility(ies) are interconnecting Balancing Authority Areas. Interconnection consists of one or more balancing area authorities that balance demand and generation within certain geographic areas of the interconnection. \n\n\n"}
{"id": "27486376", "url": "https://en.wikipedia.org/wiki?curid=27486376", "title": "Ionized-air glow", "text": "Ionized-air glow\n\nIonized-air glow is the fluorescent emission of characteristic blue–purple–violet light, of color called electric blue, by air subjected to an energy flux. \n\nWhen energy is deposited to air, the air molecules become excited. As air is composed primarily of nitrogen and oxygen, excited N and O molecules are produced. These can react with other molecules, forming mainly ozone and nitrogen(II) oxide. Water vapor, when present, may also play a role; its presence is characterized by the hydrogen emission lines. The reactive species present in the plasma can readily react with other chemicals present in the air or on nearby surfaces.\n\nThe excited nitrogen deexcites primarily by emission of a photon, with emission lines in ultraviolet, visible, and infrared band:\nThe blue light observed is produced primarily by this process. The spectrum is dominated by lines of single-ionized nitrogen, with presence of neutral nitrogen lines.\n\nThe excited state of oxygen is somewhat more stable than nitrogen. While deexcitation can occur by emission of photons, more probable mechanism at atmospheric pressure is a chemical reaction with other oxygen molecules, forming ozone:\nThis reaction is responsible for the production of ozone in the vicinity of strongly radioactive materials and electrical discharges.\n\nExcitation energy can be deposited in air by a number of different mechanisms:\n\nIn dry air, the color of produced light (e.g. by lightning) is dominated by the emission lines of nitrogen, yielding the spectrum with primarily blue emission lines. The lines of neutral nitrogen (NI), neutral oxygen (OI), singly ionized nitrogen (NII) and singly ionized oxygen (OII) are the most prominent features of a lightning emission spectrum.\n\nNeutral nitrogen radiates primarily at one line in red part of the spectrum. Ionized nitrogen radiates primarily as a set of lines in blue part of the spectrum. The strongest signals are the 443.3, 444.7, and 463.0 nm lines of singly ionized nitrogen.\n\nViolet hue can occur when the spectrum contains emission lines of atomic hydrogen. This may happen when the air contains high amount of water, e.g. with lightnings in low altitudes passing through rain thunderstorms. Water vapor and small water droplets ionize and dissociate easier than large droplets, therefore have higher impact on color.\n\nThe hydrogen emission lines at 656.3 nm (the strong H-alpha line) and at 486.1 nm (H-beta) are characteristic for lightnings.\n\nRydberg atoms, generated by low-frequency lightnings, emit at red to orange color and can give the lightning a yellowish to greenish tint.\n\nGenerally, the radiant species present in atmospheric plasma are N, N, O, NO (in dry air) and OH (in humid air). The temperature, electron density, and electron temperature of the plasma can be inferred from the distribution of rotational lines of these species. At higher temperatures, atomic emission lines of N and O, and (in presence of water) H, are present. Other molecular lines, e.g. CO and CN, mark presence of contaminants in the air.\n\nDespite the similarity of light color produced, the Čerenkov radiation is generated by a fundamentally different mechanism.\n\nČerenkov radiation is produced by charged particles which are traveling through a dielectric substance at a speed greater than the speed of light in that medium. The only types of charged particle radiation produced in the process of a criticality accident (fission reactions) are alpha particles, beta particles, positrons (which all come from the radioactive decay of unstable daughter products of the fission reaction) and energetic ions which are the daughter products themselves. Of these, only beta particles have sufficient penetrating power to travel more than a few centimeters in air. Since air is a very low density material, its index of refraction (around \"n\"=1.0002926) differs very little from that of a vacuum (\"n\"=1) and consequently the speed of light in air is only about 0.03% slower than its speed in a vacuum. Therefore, a beta particle emitted from decaying fission products would need to have a velocity greater than 99.97% \"c\" in order to produce Čerenkov radiation. Because the energy of beta particles produced during nuclear decay do not exceed energies of about 20 MeV (20.6 MeV for B is likely the most energetic with 17.9 MeV for Na being the next highest energy beta emitter) and the energy needed for a beta particle to attain 99.97% c is 21.1 MeV formula_1, the possibility of Čerenkov radiation produced in air via a fission criticality or a radioactive decay is virtually eliminated.\n\nČerenkov radiation can be however readily observed in more optically dense environments, \"e.g.\" in water or in transparent solids.\n\n"}
{"id": "11055459", "url": "https://en.wikipedia.org/wiki?curid=11055459", "title": "KAMINI", "text": "KAMINI\n\nKAMINI (Kalpakkam Mini reactor) is a research reactor at Indira Gandhi Center for Atomic Research in Kalpakkam, India. It achieved criticality on October 29, 1996. Designed and built jointly by the Bhabha Atomic Research Centre (BARC) and Indira Gandhi Centre for Atomic Research (IGCAR) it produces 30 KW of thermal energy at full power.\n\nKAMINI is cooled and moderated by light water, and fueled with uranium-233 metal produced by the thorium fuel cycle harnessed by the neighbouring FBTR reactor. \n\n, it is the world’s only thorium-based experimental reactor.\n\nKAMINI was the first and is currently the only reactor in the world designed specifically to use Uranium-233 fuel. Use of the large Thorium reserves to produce Nuclear fuel is a key strategy of India's nuclear energy program.\n\n"}
{"id": "55141765", "url": "https://en.wikipedia.org/wiki?curid=55141765", "title": "Kitepower", "text": "Kitepower\n\nKitepower is a registered trade mark of the Dutch company Enevate B.V. developing mobile airborne wind power systems.\nKitepower was founded in 2016 by Johannes Peschel and Roland Schmehl as a commercial spin-off from the Delft University of Technology’s airborne wind energy research group established by the former astronaut Wubbo Ockels. \nThe company is located in Delft, Netherlands, and currently comprises 18 employees (2018).\n\nBased on its first 20 kW (rated generator power) prototype, Kitepower is currently developing a scaled-up 100 kW system for the purpose of commercialization . Funding is provided by the European Commission's Horizon 2020 \"Fast Track to Innovation\" project REACH in which the company is collaborating with Delft University of Technology and industry partners Dromec, Maxon Motor and Genetrix.\n\nThe Kitepower system consists of three major components : a lightweight, high-performance kite, a load-bearing tether and a ground-based electric generator. Another important component is the so called kite control unit and together with the according control software for remotely steering the kite .\n\nFor energy production, the kite is operated in consecutive \"pumping cycles\" with alternating reel-out and reel-in phases: during reel-out the kite is flown in crosswind maneuvers (transverse to the incoming wind, commonly figure of eight patterns). This creates a large pulling force which is used to pull the tether from a ground-based drum that is connected to a generator. \nIn this phase electricity is generated. Once the maximum tether length is reached, the kite is reeled back, but this time depowered, such that it can be retracted with a low aerodynamic resistance. This phase consumes a small fraction of the previously generated power such that in total net energy is produced. The electricity is buffered by a rechargeable battery unit, or, in a kite park configuration, several systems can be operated with phase shifts such that the battery capacity can be reduced.\n\nAirborne wind energy promises to be a cost-competitive solution to existing renewable energy technologies.\nThe main advantages of the airborne wind energy technology are the reduced material usage compared to conventional wind turbines (no foundation, no tower) which allows reaching for higher altitudes and makes the systems more mobile in terms of location, and considerably cheaper in construction. \nChallenges are robustness and reliability of the flying wind energy system\nand the airspace requirements of the technology.\n\nFor the art project Windvogel of Dutch artist Daan Roosegaarde the Kitepower system was operated also during night, using a light-emitting tether \n\n\n\n"}
{"id": "35012847", "url": "https://en.wikipedia.org/wiki?curid=35012847", "title": "Koundi et le jeudi national", "text": "Koundi et le jeudi national\n\nKoundi et le jeudi national is a 2010 documentary film.\n\nKoundi is a large village with around 1,200 inhabitants, located in Cameroon's East Province. Aware of Koundi's richness in timber, the villagers decide to use it to alleviate poverty. They organise a union, the Organisation for Communal Interests, and create a cocoa plantation over several hectares to be able to depend on themselves. They also institute \"National Thursday\": Once a month, they all work on the development of the cocoa plantation. Village life is shown through the prism of self-management.\n\n"}
{"id": "20607372", "url": "https://en.wikipedia.org/wiki?curid=20607372", "title": "Light dark matter", "text": "Light dark matter\n\nIn astronomy and cosmology, light dark matter are dark matter weakly interacting massive particles (WIMPS) candidates with masses less than 1 GeV. These particles are heavier than warm dark matter and hot dark matter, but are lighter than the traditional forms of cold dark matter. The Lee-Weinberg bound limits the mass of the favored dark matter candidate, WIMPs, that interact via the weak interaction to formula_1 GeV. This bound arises as follows. The lower the mass of WIMPs is, the lower the annihilation cross section, which is of the order formula_2, where \"m\" is the WIMP mass and \"M\" the mass of the Z-boson. This means that low mass WIMPs, which would be abundantly produced in the early universe, freeze out (i.e. stop interacting) much earlier and thus at a higher temperature, than higher mass WIMPs. This leads to a higher relic WIMP density. If the mass is lower than formula_3 GeV the WIMP relic density would overclose the universe.\n\nSome of the few loopholes allowing one to avoid the Lee-Weinberg bound without introducing new forces below the electroweak scale have been ruled out by accelerator experiments (i.e. CERN, Tevatron), and in decays of B mesons.\n\nA viable way of building light dark matter models is thus by postulating new light bosons. This increases the annihilation cross section and reduces the coupling of dark matter particles to the Standard Model making them consistent with accelerator experiments.\n\nIn recent years, light dark matter has become popular due in part to the many benefits of the theory. Sub-GeV dark matter has been used to explain the positron excess in the galactic center observed by INTEGRAL, excess gamma rays from the galactic center and extragalactic sources. It has also been suggested that light dark matter may explain a small discrepancy in the measured value of the fine structure constant in different experiments.\n\n"}
{"id": "22830139", "url": "https://en.wikipedia.org/wiki?curid=22830139", "title": "Light non-aqueous phase liquid", "text": "Light non-aqueous phase liquid\n\nA light non-aqueous phase liquid (LNAPL) is a groundwater contaminant that is not soluble in water and has lower density than water, in contrast to a DNAPL which has higher density than water. Once a LNAPL infiltrates the ground, it will stop at the height of the water table because the LNAPL is less dense than water. Efforts to locate and remove LNAPLs is relatively less expensive and easier than for DNAPLs because LNAPLs float on top of the water in the underground water table.\n\nExamples of LNAPLs are benzene, toluene, xylene, and other hydrocarbons.\n\n\n"}
{"id": "57777800", "url": "https://en.wikipedia.org/wiki?curid=57777800", "title": "Ministry of Energy (Tanzania)", "text": "Ministry of Energy (Tanzania)\n\nThe Ministry of Energy is the government ministry of Tanzania which is responsible for facilitating the development of the energy sectors as separated with the Ministry of Mineral .\n"}
{"id": "4592194", "url": "https://en.wikipedia.org/wiki?curid=4592194", "title": "Molecular marker", "text": "Molecular marker\n\nA molecular marker is a molecule contained within a sample taken from an organism (biological markers) or other matter. It can be used to reveal certain characteristics about the respective source. DNA, for example, is a molecular marker containing information about genetic disorders, genealogy and the evolutionary history of life. Specific regions of the DNA (genetic markers) are used for diagnosing the autosomal recessive genetic disorder cystic fibrosis, taxonomic affinity (phylogenetics) and identity (DNA BarCoding). Further, life forms are known to shed unique chemicals, including DNA, into the environment as evidence of their presence in a particular location. Other biological markers, like proteins, are used in diagnostic tests for complex neurodegenerative disorders, such as Alzheimer's disease. Non-biological molecular markers are also used, for example, in environmental studies.\n\nIn genetics, a molecular marker (identified as genetic marker) is a fragment of DNA that is associated with a certain location within the genome. Molecular markers are used in molecular biology and biotechnology to identify a particular sequence of DNA in a pool of unknown DNA.\n\nThere are many types of genetic markers, each with particular limitations and strengths. Within genetic markers there are three different categories: \"First Generation Markers\", \"Second Generation Markers\", and \"New Generation Markers\". These types of markers may also identify dominance and co-dominance within the genome. Identifying dominance and co-dominance with a marker may help identify heterozygotes from homozygotes within the organism. Co-dominant markers are more beneficial because they identify more than one allele thus enabling someone to follow a particular trait through mapping techniques. These markers allow for the amplification of particular sequence within the genome for comparison and analysis.\n\nMolecular markers are effective because they identify an abundance of genetic linkage between identifiable locations within a chromosome and are able to be repeated for verification. They can identify small changes within the mapping population enabling distinction between a mapping species, allowing for segregation of traits and identity. They identify particular locations on a chromosome, allowing for physical maps to be created. Lastly they can identify how many alleles an organism has for a particular trait (bi allelic or poly allelic).\n\nGenomic markers as mentioned, have particular strengths and weakness, so, consideration and knowledge of the markers is necessary before use. For instance, a RAPD marker is dominant (identifying only one band of distinction) and it may be sensitive to reproducible results. This is typically due to the conditions in which it was produced. RAPD's are used also under the assumption that two samples share a same locus when a sample is produced. Different markers may also require different amounts of DNA. RAPD's may only need 0.02ug of DNA while an RFLP marker may require 10ug of DNA extracted from it to produce identifiable results. currently, SNP markers have turned out to be a potential tool in breeding programs in several crops.\n\nMolecular mapping aids in identifying the location of particular markers within the genome. There are two types of maps that may be created for analysis of genetic material. First, is a physical map, that helps identify the location of where you are on a chromosome as well as which chromosome you are on. Secondly there is a linkage map that identifies how particular genes are linked to other genes on a chromosome. This linkage map may identify distances from other genes using (cM) centiMorgans as a unit of measurement. Co-dominant markers can be used in mapping, to identify particular locations within a genome and can represent differences in phenotype. Linkage of markers can help identify particular polymorphisms within the genome. These polymorphisms indicate slight changes within the genome that may present nucleotide substitutions or rearrangement of sequence. When developing a map it is beneficial to identify several polymorphic distinctions between two species as well as identify similar sequence between two species.\n\nWhen using molecular markers to study the genetics of a particular crop, it must be remembered that markers have restrictions. It should first be assessed what the genetic variability is within the organism being studied. Analyze how identifiable particular genomic sequence, near or in candidate genes. Maps can be created to determine distances between genes and differentiation between species.\n\nGenetic markers can aid in the development of new novel traits that can be put into mass production. These novel traits can be identified using molecular markers and maps. Particular traits such as color, may be controlled by just a few genes. Qualitative traits (requires less that 2 genes) such as color, can be identified using MAS (marker assisted selection). Once a desired marker is found, it is able to be followed within different filial generations. An identifiable marker may help follow particular traits of interest when crossing between different genus or species, with the hopes of transferring particular traits to offspring.\n\nOne example of using molecular markers in identifying a particular trait within a plant is, Fusarium head blight in wheat. Fusarium head blight can be a devastating disease in cereal crops but certain varieties or offspring or varieties may be resistant to the disease. This resistance is inferred by a particular gene that can be followed using MAS (Marker Assisted Selection) and QTL (Quantitative Trait Loci). QTL's identify particular variants within phenotypes or traits and typically identify where the GOI (Gene of interest) is located. Once the cross has been made, sampling of offspring may be taken and evaluated to determine which offspring inherited the traits and which offspring did not. This type of selection is becoming more beneficial to breeders and farmers because it is reducing the amount of pesticides, fungicides and insecticides. Another way to insert a GOI is through mechanical or bacterial transmission. This is more difficult but may save time and money.\n\n\nIt has 5 applications in fisheries and aquaculture:\n\nBiochemical markers are generally the protein marker. These are based on the change in the sequence of amino acids in a protein molecule. The most important protein marker is alloenzyme. alloenzymes are variant forms of an enzyme that are coded by different alleles at the same locus and this alloenzymes differs from species to species. So for detecting the variation alloenzymes are used. These markers are type-i markers.\n\nAdvantages:\nDisadvantages:\nApplications:\n\n\n"}
{"id": "14844459", "url": "https://en.wikipedia.org/wiki?curid=14844459", "title": "Output transformerless", "text": "Output transformerless\n\nOutput transformerless (OTL) is a type of vacuum tube audio power amplifier, which omits an output transformer for the purpose of greater linearity and fidelity. Conventional vacuum tube amplifier designs rely upon an output transformer to couple the amplifier's output stage to the loudspeaker. Instead, OTLs use one of two primary methods for output stage coupling: direct coupling (DC) or capacitive coupling (AC).\n\nThere is some contention with respect to applying the broader term \"OTL\" to capacitively coupled designs and variants. The need to delineate these designs from their directly coupled counterparts has led to the informal adoption of several additional terms, including:\n\n\nBackground: The output coupling method of a vacuum tube amplifier generally serves two basic purposes:\n\n\nIn direct coupled OTL designs, both the necessary blocking of DC and matching of impedances are accomplished, respectively, through the topology of the amplifier's output section and the selection of vacuum tube types with sufficiently low impedance to allow effective power transfer to the loudspeaker. Typically, direct coupled OTL amplifiers will have a user-adjustable DC offset control, which allows the user to trim off any residual DC voltage residing at the amplifier's output terminals prior to operation. Servo-controlled variants also exist.\n\nLike the direct coupled designs, capacitively coupled designs rely on the selection of tube types with a sufficiently low impedance to effect the transfer of power to the loudspeaker. However, unlike direct coupled designs, capacitively coupled designs do not have inherent DC blocking by virtue of their topology. Instead, DC voltage in the output section is blocked by an \"output coupling capacitor\" - typically a large-value (3000-6000μF) electrolytic capacitor - which is interposed between the amplifier's output section and the loudspeaker.\n\nThere are several practical approaches to the design of an OTL amplifier's output section, each with their own respective strengths and weaknesses. While certain topologies lend themselves well to direct coupling, others are more suitable for capacitive coupling. The various designs in service may thus be grouped based upon their common output section topologies. Common topologies include:\n\n\nOTL power amplifiers for driving loudspeakers require multiple tubes in parallel to obtain the required drive current. An alternative is to use high impedance loudspeakers (now rare, but the Philips produced 400 and 800 ohm speakers, such as type number: AD4690/M800).\n\nOTL headphone amplifiers are more common, as typical headphones require the current that a single pair of tubes can provide.\n\nOTL designs are sometimes also used when driving long communication or interconnect cables, when a predictable and low output impedance is required.\n"}
{"id": "183120", "url": "https://en.wikipedia.org/wiki?curid=183120", "title": "Overpressure", "text": "Overpressure\n\nOverpressure (or blast overpressure) is the pressure caused by a shock wave over and above normal atmospheric pressure. The shock wave may be caused by sonic boom or by explosion, and the resulting overpressure receives particular attention when measuring the effects of nuclear weapons or thermobaric bombs.\n\nAccording to an article in the journal Toxicological Sciences,\n\n\"Blast overpressure (BOP), also known as high energy impulse noise, is a damaging outcome of explosive detonations and firing of weapons. Exposure to BOP shock waves alone results in injury predominantly to the hollow organ systems such as auditory, respiratory, and gastrointestinal systems.\"\n\nAn EOD suit worn by bomb disposal experts can protect against the deadly effects of BOP.\n\nOverpressure in an enclosed space is determined using \"Weibull's formula\":\n\nformula_1\n\nwhere:\n\n"}
{"id": "2392912", "url": "https://en.wikipedia.org/wiki?curid=2392912", "title": "Peek's law", "text": "Peek's law\n\nIn physics, Peek's law defines the electric potential gap necessary for triggering a corona discharge between two wires:\n\n\"e\" is the \"visual critical corona voltage\" or \"corona inception voltage\" (CIV), the voltage required to initiate a visible corona discharge between the wires.\n\n\"m\" is an irregularity factor to account for the condition of the wires. For smooth, polished wires, \"m\" = 1. For roughened, dirty or weathered wires, 0.98 to 0.93, and for cables, 0.87 to 0.83, namely the surface irregularities result in diminishing the corona threshold voltage. \n\n\"r\" is the radius of the wires in cm.\n\n\"S\" is the distance between the center of the wires.\n\n\"g\" is the \"visual critical\" electric field, and is given by:\n\nδ is the air density factor with respect to SATP (25°C and 76 cmHg):\n\n\"g\" is the \"disruptive electric field.\"\n\n\"c\" is an empirical dimensional constant.\n\n"}
{"id": "17600071", "url": "https://en.wikipedia.org/wiki?curid=17600071", "title": "Polydioctylfluorene", "text": "Polydioctylfluorene\n\nPolydioctylfluorene (PFO) is an organic compound, a polymer of 9,9-dioctylfluorene, with formula (CH(CH)). It is an electroluminescent conductive polymer that characteristically emits blue light. Like other polyfluorene polymers, it has been studied as a possible material for light-emitting diodes\n\nThe monomer has an aromatic fluorene core -CH- with two aliphatic \"n\"-octyl -CH tails attached to the central carbon. Polydioctylfluorene (PFO) can be found in liquid-crystalline, glassy, amorphous, semi-crystalline or β-chain formation. This variety is on account of the intermolecular forces that PFO can participate in. The secondary forces present in PFO are typically van der Waals, which are relatively weak. These weak forces makes it a solid that can also be used as a film on a substrate. The glassy films formed by PFO chains form solutions in good solvents, meaning it is at least partially soluble. These van der Waals also add complexity to the microstructure of PFO, which is why it has a wide range of solid formations. The solid formations though, typically form low density due to the low cooling rate of the polymer. The density of polydioctylfluorene is measured by using the process of ultraviolet photoelectron spectroscopy. Chain stiffness is also prominent in PFO, because of this it is predicted that the molecular weight is a factor of 2.7 lower than polystyrene, which can produce an approximation of 190 repeat units in a standard PFO chain. By changing the strain and temperature applied to the polymer’s structure results in an alteration of the PFO’s properties. Thermal treatment such as friction transfer can be applied to the structure, this is a way to alter the properties. The friction transfer aligns the structure to become crystalline or liquid crystalline. Polymer 196 is the most commonly studied type of polydioctylfluorene. In studies, polymer 196 has shown the most promising properties and the best crystallinity. Within the crystal structure of polymer 196 octyl side chains are inserted between the layer of the polymer to provide more space for efficiency in structuring the material.\n\nIn studies, the structure of polydioctylfluorene was observed by using grazing-incidence X-ray diffraction after applying friction to the structure. Experiments revealed PFO was present in crystalline films and liquid crystalline after cooling and use of friction. As a result of the friction exerted, twofold symmetry in PFO was broken. The friction transfer used to obtain a single crystal film is important in the process of fabricating polarized light emitting diodes.\n\nPolydioctylfluorene, can also be known as polymer 196 to polyfluorene. The molar mass of PFO ranges between 24,000-41,600 (g/mol) and because of this varying molar mass, many other properties vary as well. For example, the glass transition temperature can fall somewhere between 72-113 degrees Celsius. The absolute wavelength emitted by PFO can range between 386-389 nm in a solution of CHCl3, and falls around 389 in a solution of THF. The absolute film wavelength of PFO though falls between 380-394 nm. The melting point of a crystalline molecule of PFO is predicted to be about 150 degrees Celsius.\n\nThere have also been reports that some of the solid states of polydioctylfluorene are composted in sheet-like layers which are about 50-100 nm thick. As a result of these sheets, the glassy and semicrystalline states can be formed (excluding amorphous, liquid crystalline, and beta chain states). When cooled quickly, the chains tightly align, giving PFO a close packing factor, though because of the high complexity of the chains, this sometimes gets messy and creates the amorphous state. The parts of the molecule that add this complexity are the carbon rings (that are located in the backbone) making the molecule overall large in size.\n\nThe formation of beta-phase chains in PFO can be formed through dip-pen nanolithography, to represent wavelength changes in metamaterials. The dip-pen technique allows a scale of 500 nm > to be visible. The beta chains can be converted into the glassy films by adding extra stress to the main fluorine backbone unit, whether beta chains are formed is determined by peaks in wavelength absorption. Beta chains can also be confirmed to be present by using solvent to non-solvent mixtures. If the molecule were to be dipped into this mixture for ten seconds, the chains with no dissolution of films are able to produce these said beta chains.\n\nPolydioctylfluorene is a polymer light-emitting device known as PLED, which covalently bonds to the carbon hydrogen chains. PFO is a copolymer of basic polyfluorene, which enables it to release phosphorescent light. This basic fluorene backbone strengthens the molecule on account of the carbon rings.The cross-linking in polydioctylfluorene structure provides an efficient technique for hole-transport layers to emit light. Also, when a solvent-polymer compound is added the β-phase crystalline structure to be maintained. Efficiency in current can reach a maximum of about 17 cd/A and maximum luminance obtained can be approximately 14,000 cd/m(2). The hole-transport layers (HTLs) improve the polymer’s anode hole injection and greatly increase electron blocking. By having the capability to control the microstructure of phase domains gives an opportunity to optimize the optoelectronic properties of PFO based products. When needs for optoelectronic emittance are reached in polydioctylfluorene, the electroluminescence given off in dependent on the active layer in the conjugate polymer. Another way to affect the optoelectronic properties is by altering how dense the phase chain segments are ordered. Low densities can be achieved from tremendously slow crystallization while on the other hand directional crystalline solution can be achieved by use of thermal gradients.\n"}
{"id": "14639583", "url": "https://en.wikipedia.org/wiki?curid=14639583", "title": "Power purchase agreement", "text": "Power purchase agreement\n\nA power purchase agreement (PPA), or electricity power agreement, is a contract between two parties, one which generates electricity (the seller) and one which is looking to purchase electricity (the buyer). The PPA defines all of the commercial terms for the sale of electricity between the two parties, including when the project will begin commercial operation, schedule for delivery of electricity, penalties for under delivery, payment terms, and termination. A PPA is the principal agreement that defines the revenue and credit quality of a generating project and is thus a key instrument of project finance. There are many forms of PPA in use today and they vary according to the needs of buyer, seller, and financing counter parties.\n\nA power purchase agreement (PPA) is a legal contract between an electricity generator (provider) and a power purchaser (buyer, typically a utility or large power buyer/trader). Contractual terms may last anywhere between 5 and 20 years, during which time the power purchaser buys energy, and sometimes also capacity and/or ancillary services, from the electricity generator. Such agreements play a key role in the financing of independently owned (i.e. not owned by a utility) electricity generating assets. The seller under the PPA is typically an independent power producer, or \"IPP.\"\n\nIn the case of distributed generation (where the generator is located on a building site and energy is sold to the building occupant), commercial PPAs have evolved as a variant that enables businesses, schools, and governments to purchase electricity directly from the generator rather than from the utility. This approach facilitates the financing of distributed generation assets such as photovoltaic, micro-turbines, reciprocating engines, and fuel cells.\n\nUnder a PPA, the seller is the entity that owns the project. In most cases, the seller is organized as a special purpose entity whose main purpose is to facilitate non-recourse project financing.\n\nUnder a PPA, the buyer is typically a utility or a company that purchases the electricity to meet its customers' needs. In the case of distributed generation involving a commercial PPA variant, the buyer may be the occupant of the building—a business, school, or government for example. Electricity traders may also enter into PPA with the Seller.\n\nIn the United States, PPAs are typically subject to regulation by the Federal Energy Regulatory Commission (FERC). FERC determines which facilities applicable for PPAs under the Energy Policy Act of 2005. PPAs facilitate the financing of distributed generation assets such as photovoltaic, microturbines, reciprocating engines, and fuel cells.\n\nPPAs are typically subject to regulation at the state and federal level to varying degrees depending on the nature of the PPA and the extent to which the sale of electricity is regulated where the project is sited. In the U.S., FERC determines which facilities are considered to be exempt wholesale generators (EWG) or qualifying facilities and are applicable for PPAs under the Energy Policy Act of 2005.\n\nPower purchase agreements (PPAs) may be appropriate where:\n\nThe PPA is often regarded as the central document in the development of independent electricity generating assets (power plants). Because it defines the revenue terms for the project and credit quality, it is key to obtaining non-recourse project financing.\n\nOne of the key benefits of the PPA is that by clearly defining the output of the generating assets (such as a solar electric system) and the credit of its associated revenue streams, a PPA can be used by the PPA provider to raise non-recourse financing from a bank or other financing counterparty.\n\nThe PPA is considered contractually binding on the date that it is signed, also known as the effective date. Once the project has been built, the effective date ensures that the purchaser will buy the electricity that will be generated and that the supplier will not sell its output to anyone else except the purchaser.\n\nBefore the seller can sell electricity to the buyer, the project must be fully tested and commissioned to ensure reliability and comply with established commercial practices. The commercial operation date is defined as the date after which all testing and commissioning has been completed and is the initiation date to which the seller can start producing electricity for sale (i.e. when the project has been substantially completed). The commercial operation date also specifies the period of operation, including an end date that is contractually agreed upon.\n\nTypically, termination of a PPA ends on the agreed upon commercial operation period. A PPA may be terminated if abnormal events occur or circumstances result that fail to meet contractual guidelines. The seller has the right to curtail the delivery of energy if such abnormal circumstances arise, including natural disasters and uncontrolled events. The PPA may also allow the buyer to curtail energy in circumstances where the after-tax value of electricity changes.\nWhen energy is curtailed, it is usually because one of the parties involved was at fault, which results in paid damages to the other party. This may be excused in extraordinary circumstances such as natural disasters and the party responsible for repairing the project is liable for such damages. In situations where liability is not defined properly in the contract, the parties may negotiate force majeure to resolve these issues.\n\nMaintenance and operation of a generation project is the responsibility of the seller. This includes regular inspection and repair, if necessary, to ensure prudent practices. Liquidated damages will be applied if the seller fails to meet these circumstances. Typically, the seller is also responsible for installing and maintaining a meter to determine the quantity of output that will be sold. Under this circumstance, the seller must also provide real-time data at the request of the buyer, including atmospheric data relevant to the type of technology installed.\n\nThe PPA will distinguish where the sale of electricity takes place in relation to the location of the buyer and seller. If the electricity is delivered in a \"busbar\" sale, the delivery point is located on the high side of the transformer adjacent to the project. In this type of transaction, the buyer is responsible for transmission of the energy from the seller. Otherwise, the PPA will distinguish another delivery point that was contractually agreed on by both parties.\n\nElectricity rates are agreed upon as the basis for a PPA. Prices may be flat, escalate over time, or be negotiated in any other way as long as both parties agree to the negotiation. In a regulated environment, an Electricity Regulator will regulate the price. A PPA will often specify how much energy the supplier is expected to produce each year and any excess energy produced will have a negative impact on the sales rate of electricity that the buyer will be purchasing. This system is intended to provide an incentive for the seller to properly estimate the amount of energy that will be produced in a given period of time.\n\nThe PPA will also describe how invoices are prepared and the time period of response to those invoices. This also includes how to handle late payments and how to deal with invoices that became final after periods of inactivity regarding challenging the invoice. The buyer also has the authority to audit those records produced by the supplier in any circumstance. There is a defined timeline when PPA Provider has to send an invoice to the Generator or vice versa and if that timeline is not met then it has its own consequences, which varies from one PPA Provider to another.\n\nThe buyer will typically require the seller to guarantee that the project will meet certain performance standards. Performance guarantees let the buyer plan accordingly when developing new facilities or when trying to meet demand schedules, which also encourages the seller to maintain adequate records. In circumstances where the output from the supplier fails to meet the contractual energy demand by the buyer, the seller is responsible for retributing such costs. Other guarantees may be contractually agreed upon, including availability guarantees and power-curve guarantees. These two types of guarantees are more applicable in regions where the energy harnessed by the renewable technology is more volatile.\n\nA basic sample PPA between the Bonneville Power Administration and a wind power generating entity was developed as a reference for future PPAs. \nSolar PPAs are now being successfully utilized in the California Solar Initiative's Multifamily Affordable Solar Housing (MASH) program.\nThis aspect of the successful CSI program was just recently opened for applications.\n\nPPAs can be managed in the European market by service providers. The legal agreements between the statewide power sectors(seller) and the trader(buyer/who buys large quantity of power) will be treated as the PPA in power sector.\n\nData center owners Amazon, Google, and Microsoft have used PPAs to offset the emissions and power usage of cloud computing. Some manufacturers with heavy carbon emission footprints and energy usage such as Anheuser-Busch InBev have also shown interest in PPAs. In 2017, Anheuser-Busch InBev agreed to purchase using a PPA from the utility company Iberdrola in Mexico for 220 MW of new wind farm energy.\n\n\n"}
{"id": "32420169", "url": "https://en.wikipedia.org/wiki?curid=32420169", "title": "Powered aircraft", "text": "Powered aircraft\n\nA powered aircraft is an aircraft that uses onboard propulsion with mechanical power generated by an aircraft engine of some kind.\n\nAircraft propulsion nearly always uses either a type of propeller, or a form of jet propulsion. Other potential propulsion techniques such as ornithopters are very rarely used.\n\nA propeller or airscrew comprises a set of small, wing-like aerofoil \"blades\" set around a central hub which spins on an axis aligned in the direction of travel. The blades are set at a \"pitch\" angle to the airflow, which may be fixed or variable, such that spinning the propeller creates aerodynamic lift, or \"thrust\", in a forward direction.\n\nA \"tractor\" design mounts the propeller in front of the power source, while a \"pusher\" design mounts it behind. Although the pusher design allows cleaner airflow over the wing, tractor configuration is more common because it allows cleaner airflow to the propeller and provides a better weight distribution.\n\n\"Contra-rotating propellers\" have one propeller close behind another on the same axis, but rotating in the opposite direction.\n\nA variation on the propeller is to use many broad blades to create a fan. Such fans are usually surrounded by a ring-shaped fairing or duct, as \"ducted fans\".\n\nMany kinds of power plant have been used to drive propellers.\n\nThe earliest designs used man power to give dirigible balloons some degree of control, and go back to Jean-Pierre Blanchard in 1784. Attempts to achieve heavier-than-air man-powered flight did not succeed fully until Paul MacCready's Gossamer Condor in 1977.\nThe first powered flight of an aircraft was made in a steam-powered dirigible by Henri Giffard in 1852. Attempts to marry a practical lightweight steam engine to a practical fixed-wing airframe did not succeed until much later, by which time the internal combustion engine was already dominant.\n\nFrom the first controlled powered fixed-wing aircraft flight by the Wright brothers until World War II, propellers turned by the internal combustion piston engine were virtually the only type of propulsion system in use. The piston engine is still used in the majority of smaller aircraft produced, since it is efficient at the lower altitudes and slower speeds suited to propellers.\n\nTurbine engines need not be used as jets (see below), but may be geared to drive a propeller in the form of a turboprop. Modern helicopters also typically use turbine engines to power the rotor. Turbines provide more power for less weight than piston engines, and are better suited to small-to-medium size aircraft or larger, slow-flying types. Some turboprop designs mount the propeller directly on an engine turbine shaft, and are called propfans.\n\nOther less common power sources include:\n\nRotorcraft have spinning blades called a \"rotor\" which spins in the horizontal plane to provide lift. Forward thrust is usually obtained by angling the rotor disc slightly forward so that a proportion of its lift is directed backwards; these are called helicopters. Other rotorcraft are compound helicopters and autogyros which sometimes use other means of propulsion, such as propellers and jets.\n\nThe rotor of a helicopter may, like a propeller, be powered by a variety of methods such as an internal-combustion engine or jet turbine. Tip jets, fed by gases passing along hollow rotor blades from a centrally mounted engine, have been experimented with. Attempts have even been made to mount engines directly on the rotor tips.\n\nAirbreathing jet engines provide thrust by taking in air, compressing the air, injecting fuel into the hot compressed air mixture in a combustion chamber, the resulting accelerated exhaust ejects rearwards through a turbine which drives the compressor. The reaction against this acceleration provides the engine thrust.\n\nJet engines can provide much higher thrust than propellers, and are naturally efficient at higher altitudes, being able to operate above . They are also much more fuel-efficient at normal flight speeds than rockets. Consequently, nearly all high-speed and high-altitude aircraft use jet engines.\n\nThe early turbojet and modern turbofan use a spinning compressor and turbine to provide thrust. Many, mostly in military aviation, add an afterburner which injects extra fuel into the hot exhaust.\n\nUse of a turbine is not absolutely necessary: other designs include the crude pulse jet, high-speed ramjet and the still-experimental supersonic-combustion ramjet or scramjet. These mechanically simple designs require an existing airflow to work and cannot work when stationary, so they must be launched by a catapult or rocket booster, or dropped from a mother ship.\n\nThe bypass turbofan engines of the Lockheed SR-71 were a hybrid design – the aircraft took off and landed in jet turbine configuration, and for high-speed flight the afterburner was lit and the turbine bypassed, to create a ramjet.\n\nThe motorjet was a very early design which used a piston engine in place of the combustion chamber, similar to a turbocharged piston engine except that the thrust is derived from the turbine instead of the crankshaft. It was soon superseded by the turbojet and remained a curiosity.\n\nRocket propulsion offers very high thrust for light weight and has no height limit, but suffers from high fuel consumption and the need to carry oxidant as well as propellant.\n\nRocket-powered aircraft have been experimented with, and during the Second World War the Messerschmitt \"Komet\" fighter was developed and used operationally. \nSince then they have been restricted to specialised niches, such as the Bell X-1 which broke the sound barrier or the North American X-15 which was capable of flying at extremely high altitudes at the border with space as it was not dependent on atmospheric oxygen.\n\nRockets have more often been used as a supplement to the main powerplant, typically in the case of rocket-assisted take off to give more power for a heavily loaded aircraft or reduce the takeoff run. In a number of designs such as the prototype \"mixed-power\" Saunders-Roe SR.53 interceptor a rocket was used to provide high-speed climb and speed to reach the target while a smaller turbojet provided a slower and more economical return to base.\n\nThe ornithopter obtains thrust by flapping its wings. When the wing flaps, as opposed to gliding, it continues to develop lift as before, but the lift is rotated forward to provide a thrust component.\n\nWorking devices have been created for flight research and as prototypes, but the vertical oscillation of the fuselage, which tends to accompany the wing flapping, limits their usefulness. The only practical application is a flying model hawk used to freeze prey animals into stillness so that they can be captured.\n\nToys in the form of a flying model bird are also popular.\n\nA fixed-wing aircraft obtains lift from airflow over the wing resulting from motion due to forward thrust. A few other types, such as the rotary-winged autogyro, obtain lift through similar methods.\n\nSome types use a separate power system to create lift. These include the rotary-winged helicopter and craft that use lift jets (e.g. the flying bedstead).\n\nA hot air balloon requires a power source (normally a gas burner) for lift, but is not normally considered a \"powered aircraft\".\n\n\n"}
{"id": "52842187", "url": "https://en.wikipedia.org/wiki?curid=52842187", "title": "Proceedings of the Combustion Institute", "text": "Proceedings of the Combustion Institute\n\nThe Proceedings of the Combustion Institute are the proceedings of the biennial Combustion Symposium put on by The Combustion Institute. The publication contains the most significant contributions in fundamentals and applications fundamental research of combustion science combustion phenomena. Research papers and invited topical reviews are included on topics of reaction kinetics, soot, PAH and other large molecules, diagnostics, laminar flames, turbulent flames, heterogenous combustion, spray and droplet combustion, detonations, explosions & supersonic combustion, fire research, stationary combustion systems, internal combustion engine and gas turbine combustion, and new technology concepts. The editors-in-chief are Volker Sick (University of Michigan) and Alison Tomlin (University of Leeds).\n\nThe need for development of automotive engines, fuels, and aviation formed the basis for the organization which became The Combustion Institute. The first combustion symposium with published proceedings was in 1948. Since then,\n\nThe journal is abstracted and indexed in:\n\nAccording to the \"Journal Citation Reports\", the journal has a 2015 impact factor of 4.120.\n"}
{"id": "43371522", "url": "https://en.wikipedia.org/wiki?curid=43371522", "title": "Raghagan Dam", "text": "Raghagan Dam\n\nRaghagan Dam is a concrete gravity dam under construction 13 kilometers East of Khaar town, Bajaur Agency of FATA, Pakistan.\n\nConstruction of dam started in January, 2013 and is expected to complete by june 2018 with a projected cost of PKR 423.586 Millions.\nThe dam has a height 52 of feet and length 200 feet. The dam will irrigate area of around 3,500 acres land, with total water storage capacity of 1252 acre-feet.\n\n"}
{"id": "60088", "url": "https://en.wikipedia.org/wiki?curid=60088", "title": "Roentgenium", "text": "Roentgenium\n\nRoentgenium is a chemical element with symbol Rg and atomic number 111. It is an extremely radioactive synthetic element that can be created in a laboratory but is not found in nature. The most stable known isotope, roentgenium-282, has a half-life of 2.1 minutes, although the unconfirmed roentgenium-286 may have a longer half-life of about 10.7 minutes. Roentgenium was first created in 1994 by the GSI Helmholtz Centre for Heavy Ion Research near Darmstadt, Germany. It is named after the physicist Wilhelm Röntgen (also spelled Roentgen), who discovered X-rays.\n\nIn the periodic table, it is a d-block transactinide element. It is a member of the 7th period and is placed in the group 11 elements, although no chemical experiments have been carried out to confirm that it behaves as the heavier homologue to gold in group 11 as the ninth member of the 6d series of transition metals. Roentgenium is calculated to have similar properties to its lighter homologues, copper, silver, and gold, although it may show some differences from them.\n\nRoentgenium was first synthesized by an international team led by Sigurd Hofmann at the Gesellschaft für Schwerionenforschung (GSI) in Darmstadt, Germany, on December 8, 1994. The team bombarded a target of bismuth-209 with accelerated nuclei of nickel-64 and detected three nuclei of the isotope roentgenium-272:\n\nThis reaction had previously been conducted at the Joint Institute for Nuclear Research in Dubna (then in the Soviet Union) in 1986, but no atoms of Rg had then been observed. In 2001, the IUPAC/IUPAP Joint Working Party (JWP) concluded that there was insufficient evidence for the discovery at that time. The GSI team repeated their experiment in 2002 and detected three more atoms. In their 2003 report, the JWP decided that the GSI team should be acknowledged for the discovery of this element.\n\nUsing Mendeleev's nomenclature for unnamed and undiscovered elements, roentgenium should be known as \"eka-gold\". In 1979, IUPAC published recommendations according to which the element was to be called \"unununium\" (with the corresponding symbol of \"Uuu\"), a systematic element name as a placeholder, until the element was discovered (and the discovery then confirmed) and a permanent name was decided on. Although widely used in the chemical community on all levels, from chemistry classrooms to advanced textbooks, the recommendations were mostly ignored among scientists in the field, who either called it \"element 111\", with the symbol of \"E111\", \"(111)\" or even simply \"111\".\n\nThe name \"roentgenium\" (Rg) was suggested by the GSI team in 2004, to honor the German physicist Wilhelm Conrad Röntgen, the discoverer of X-rays. This name was accepted by IUPAC on November 1, 2004.\n\nRoentgenium has no stable or naturally occurring isotopes. Several radioactive isotopes have been synthesized in the laboratory, either by fusion of the nuclei of lighter elements or as intermediate decay products of heavier elements. Nine different isotopes of roentgenium have been reported with atomic masses 272, 274, 278–283, and 286 (283 and 286 unconfirmed), two of which, roentgenium-272 and roentgenium-274, have known but unconfirmed metastable states. All of these decay through alpha decay or spontaneous fission.\n\nAll roentgenium isotopes are extremely unstable and radioactive; in general, the heavier isotopes are more stable than the lighter. The most stable known roentgenium isotope, Rg, is also the heaviest known roentgenium isotope; it has a half-life of 2.1 minutes. (The unconfirmed Rg is even heavier and appears to have an even longer half-life of about 10.7 minutes, which would make it one of the longest-lived superheavy nuclides known; likewise, the unconfirmed Rg appears to have a long half-life of about 5.1 minutes.) The isotopes Rg and Rg have also been reported to have half-lives over a second. The remaining isotopes have half-lives in the millisecond range. The undiscovered isotope Rg has been predicted to be the most stable towards beta decay; however, no known roentgenium isotope has been observed to undergo beta decay. The unknown isotope Rg is also expected to have a long half-life of 1 second. Before their discovery, the isotopes Rg, Rg, and Rg were predicted to have long half-lives of 1 second, 1 minute, and 4 minutes respectively; however, they were discovered to have shorter half-lives of 4.2 milliseconds, 17 seconds, and 2.1 minutes respectively. Similarly, the measured half-life of the unconfirmed Rg of 5.1 minutes, while long, is less than the 10 minutes that had previously been predicted for it.\n\nRoentgenium is the ninth member of the 6d series of transition metals. Since copernicium (element 112) has been shown to be a group 12 metal, it is expected that all the elements from 104 to 111 would continue a fourth transition metal series. Calculations on its ionization potentials and atomic and ionic radii are similar to that of its lighter homologue gold, thus implying that roentgenium's basic properties will resemble those of the other group 11 elements, copper, silver, and gold; however, it is also predicted to show several differences from its lighter homologues.\n\nRoentgenium is predicted to be a noble metal. Based on the most stable oxidation states of the lighter group 11 elements, roentgenium is predicted to show stable +5 and +3 oxidation states, with a less stable +1 state. The +3 state is predicted to be the most stable. Roentgenium(III) is expected to be of comparable reactivity to gold(III), but should be more stable and form a larger variety of compounds. Gold also forms a somewhat stable −1 state due to relativistic effects, and it has been suggested roentgenium may do so as well: nevertheless, the electron affinity of roentgenium is expected to be around , significantly lower than gold's value of , so roentgenides may not be stable or even possible. The 6d orbitals are destabilized by relativistic effects and spin–orbit interactions near the end of the fourth transition metal series, thus making the high oxidation state roentgenium(V) more stable than its lighter homologue gold(V) (known only in one compound) as the 6d electrons participate in bonding to a greater extent. The spin-orbit interactions stabilize molecular roentgenium compounds with more bonding 6d electrons; for example, is expected to be more stable than , which is expected to be more stable than . Roentgenium(I) is expected to be difficult to obtain. Gold readily forms the cyanide complex , which is used in its extraction from ore through the process of gold cyanidation; roentgenium is expected to follow suit and form .\n\nThe probable chemistry of roentgenium has received more interest than that of the two previous elements, meitnerium and darmstadtium, as the valence s-subshells of the group 11 elements are expected to be relativistically contracted most strongly at roentgenium. Calculations on the molecular compound RgH show that relativistic effects double the strength of the roentgenium–hydrogen bond, even though spin–orbit interactions also weaken it by . The compounds AuX and RgX, where X = F, Cl, Br, O, Au, or Rg, were also studied. Rg is predicted to be the softest metal ion, even softer than Au, although there is disagreement on whether it would behave as an acid or a base. In aqueous solution, Rg would form the aqua ion [Rg(HO)], with an Rg–O bond distance of 207.1 pm. It is also expected to form Rg(I) complexes with ammonia, phosphine, and hydrogen sulfide.\n\nRoentgenium is expected to be a solid under normal conditions and to crystallize in the body-centered cubic structure, unlike its lighter congeners which crystallize in the face-centered cubic structure, due to its being expected to have different electron charge densities from them. It should be a very heavy metal with a density of around 28.7 g/cm; in comparison, the densest known element that has had its density measured, osmium, has a density of only 22.61 g/cm. This results from roentgenium's high atomic weight, the lanthanide and actinide contractions, and relativistic effects, although production of enough roentgenium to measure this quantity would be impractical, and the sample would quickly decay.\n\nThe stable group 11 elements, copper, silver, and gold, all have an outer electron configuration nd(n+1)s. For each of these elements, the first excited state of their atoms has a configuration nd(n+1)s. Due to spin-orbit coupling between the d electrons, this state is split into a pair of energy levels. For copper, the difference in energy between the ground state and lowest excited state causes the metal to appear reddish. For silver, the energy gap widens and it becomes silvery. However, as the atomic number increases, the excited levels are stabilized by relativistic effects and in gold the energy gap decreases again and it appears gold. For roentgenium, calculations indicate that the 6d7s level is stabilized to such an extent that it becomes the ground state and the 6d7s level becomes the first excited state. The resulting energy difference between the new ground state and the first excited state is similar to that of silver and roentgenium is expected to be silvery in appearance. The atomic radius of roentgenium is expected to be around 138 pm.\n\nUnambiguous determination of the chemical characteristics of roentgenium has yet to have been established due to the low yields of reactions that produce roentgenium isotopes. For chemical studies to be carried out on a transactinide, at least four atoms must be produced, the half-life of the isotope used must be at least 1 second, and the rate of production must be at least one atom per week. Even though the half-life of Rg, the most stable known roentgenium isotope, is 100 seconds, long enough to perform chemical studies, another obstacle is the need to increase the rate of production of roentgenium isotopes and allow experiments to carry on for weeks or months so that statistically significant results can be obtained. Separation and detection must be carried out continuously to separate out the roentgenium isotopes and allow automated systems to experiment on the gas-phase and solution chemistry of roentgenium, as the yields for heavier elements are predicted to be smaller than those for lighter elements. However, the experimental chemistry of roentgenium has not received as much attention as that of the heavier elements from copernicium to livermorium, despite early interest in theoretical predictions due to relativistic effects on the \"n\"s subshell in group 11 reaching a maximum at roentgenium. The isotopes Rg and Rg are promising for chemical experimentation and may be produced as the granddaughters of the moscovium isotopes Mc and Mc respectively; their parents are the nihonium isotopes Nh and Nh, which have already received preliminary chemical investigations.\n\n\n"}
{"id": "18593576", "url": "https://en.wikipedia.org/wiki?curid=18593576", "title": "Schöningen spears", "text": "Schöningen spears\n\nThe Schöningen spears are a set of eight wooden throwing spears from the Palaeolithic Age that were excavated between 1994 and 1998 in the open-cast lignite mine in Schöningen, Helmstedt district, Germany, together with an associated cache of approximately 16,000 animal bones. The excavations took place under the management of Hartmut Thieme of the Lower Saxony State Service for Cultural Heritage (NLD).\n\nOriginally assessed as being between 380,000 and 400,000 years old, they represent the oldest completely preserved hunting weapons of prehistoric Europe so far discovered. \nAs such they predate the age of Neanderthal Man (by convention taken to emerge after 300,000 years ago), and is associated with \"Homo heidelbergensis\".\nThe spears support the practice of hunting by archaic humans in Europe in the late Lower Paleolithic.\n\nThe age of the spears was estimated from their stratigraphic position, \"sandwiched between deposits of the Elsterian and Saalian glaciations, and situated within a well-studied sedimentary sequence.\". More recently, thermoluminescence dating of heated flints in a deposit beneath that which contained the spears suggested that the spears were between 337,000 and 300,000 years old.\n\nThe site of the finds (Schöningen 13/II sedimentary sequence 4) is one of 13 Palaeolithic places of discovery in the open-cast, lignite mine (working area, south) that was excavated in the course of the prospection of the quaternary surface layer from 1992 to 2009.\n\nThe excavation base that was excluded from coal mining represents a small segment of a former littoral zone. This zone has been visited over millennia, between the Elster- and Saale ice ages, by humans and animals alike. The pedestal displays five massive, layered sediment packages that were created by varying levels of the lake and silting-up processes.\n\nThanks to the quick, airtight covering of the archaeological layers by mud, the organic materials are exceptionally well preserved. In the sequence of the sedimentary layers, climate changes can be read with a high resolution - from a warm, dry phase to airy deciduous forests to tundra.\nThe spears themselves are from an approximately wide and long strip, parallel to the former lake shore in the sedimentary layer four, the late Holstein-interglacial. The archaeological layers beneath have only been partially excavated and have been an objective of a research excavation by the DFG (German Research Association) since 2010.\n\nTogether with the spears, some stone artefacts, chips as well as over 10,000 animal bones were found, amongst them 90% horse bones, followed by red deer and European bison. The horse bones come from \"Equus mosbachensis\" and are indicative of at least 20 individuals. They show numerous cut marks made by stone tools, but only a few bite marks made by animals. The site is interpreted by the excavator Harald Thieme as testimony of a hunting event as well as the following cutting up and preparation of the kill. According to his scenario, the thick reeds at the lake shore gave the hunters cover, from where the horses, trapped between the hunters and the lake, were culled with accurate spear throws. Because there are bones of young animals amongst the horse bones, he concludes that the hunt took place in autumn. Furthermore, he sees evidence of ritualistic activity, because the spears were left behind.\n\nThe spears, deformed by the load of the sediment pressure, are made from slim, straight spruce stems – except for spear IV which is made from pine wood. The spears vary in length from , with diameters ranging from .\n\nThey have been worked very thoroughly and are evidence of highly developed technological skills and of a workmanlike tradition. Like in today’s tournament javelins, the greatest diameter and therefore its centre of gravity is in the front third of the shaft. The tips are worked symmetrically from the base of the stems, and the end of the tips were worked beside the medullary ray, the weakest part of the stem, on purpose.\n\nIn their throwing qualities, the Schöningen Spears are equal to modern tournament javelins. During tests, athletes could throw replicas up to . The choice of the wood is likely to be climatically determined, because during the cooler climate near the end of the interglacial, conifers grew close to the site of the finds.\n\nMore unique wooden artefacts were found at the place of discovery of the wild horse hunting camp: a charred wooden staff (skewer) as well as a wooden tool, tapered at both ends, interpreted as a throwing stick. The stone tools at the place of discovery consist of different scraper-shaped and pointed forms. Evidence of blank production is missing; much retouched debris proves the reworking of the brought-along tools.\n\nAlso among the finds are the so-called \"grooved wooden tools\", excavated at the place of discovery No. 12. Made from the extremely hard wooden branch-bases of the European silver fir, and noticeably incised at one end, they may have been used as a mounting for stone blades. If this interpretation is correct, they are the oldest composite tools of mankind.\n\nThanks to the good preservation conditions at the place of discovery, there are many finds of small animals, among them small mammals, fish, molluscs and insects. Together with the carpological remains they make a detailed reconstruction of the climate and the environment of the passing of an interglacial period possible.\n\nThe spears and the place of discovery have revolutionized the picture of the cultural and social development of early humans. Previously, the widespread opinion was that \"Homo heidelbergensis\" (and Neanderthals, regarded as a descendant of \"H. heidelbergensis\") were simple beings without language that lived on plants and carrion. The spears and their correlated finds are evidence of complex technological skills and are the first obvious proof for an active (big game) hunt. It seems that \"H. heidelbergensis\" could hunt herd animals that can run faster than a human, and this suggests that they had sophisticated hunting strategies, a complex social structure and developed forms of communication (language ability). \"H. heidelbergensis\" therefore already had intellectual and cognitive skills like anticipatory planning, thinking, and acting, that up until then had only been attributed to modern humans.\n\nSince 2010, the excavations on top of the excavation base continued in the framework of a project by the Lower Saxony State Service for Cultural Heritage in Hannover and the Eberhard Karls University Tuebingen, Department of Early Prehistory and Quaternary Ecology of the Institute of Pre- and Protohistory and Mediaeval Archaeology, supported by the Deutsche Forschungsgemeinschaft (German Research Association). Numerous cooperation partners domestic and abroad are involved in the reprocessing and the evaluation of the excavations: Rijksuniversiteit Leiden (paleontology), Leuphana University Lueneburg (palynologie), Senckenberg Research Institute and Nature Museum in Frankfurt am Main, Leibniz University Hannover(geology), Institute for Quaternary Lumbers Langnau (wood anatomy), Romano-Germanic Central Museum Mainz and others.\n\nIn 2009, Lower Saxony allocated public funds from the increased funds for the economy package II for the construction of a research and development centre. The centre, close to the place of discovery, is devoted to the inter-disciplinary research of the Schöningen places of discovery, as well as to the Pleistocene archaeology, and presents the original finds in an experience-orientated, modern exhibition. The transparent research and laboratory area as well as an interactive visitor’s laboratory link the areas “research” and “museum”. A 24 hectare outdoor area presents typical plant communities of the interglacial, among them a pasture for wild horses. The place has been planned as an extracurricular place of learning. The building contractor was the town of Schöningen. Responsible for the conception and contextual planning was the Lower Saxony State Service of Cultural Heritage. The centre was opened at the beginning of 2013.\n\nArchaeologists at the University of Tübingen have questioned some of the initial interpretations of the site. Isotope analysis and wear patterns on the horses' teeth show a wide variety of habitat and diet amongst the animals, indicating that the faunal assemblage accumulated in many small events, rather than one large slaughter. Sediment analysis shows that the red colour previously thought to be a result of hearths and burning are actually iron compounds forming as the lake levels dropped in recent times. Lake algae, sponges, and small crustaceans found in the sediments show that the spears were never on dry land and that the deposit has always been submerged. These data suggest that instead of representing a big hunting event, the spears suggest less social complexity than originally suggested. They also suggest that the horses were hunted in shallow water rather than at the lake edge.\n\nWooden artifacts from the Palaeolithic age are very rarely delivered to posterity. Beside Schöningen, finds are the Clacton Spear from Clacton-on-Sea (England), Torralba (Spain), Ambrona (Spain) and Bad Cannstatt (Germany/Baden-Wuerttemberg), of which only the Clacton Spear has been preserved. The artificial character of calcified lumbers from the discovery site Bilzingsleben is debatable. A wooden stabbing lance from Lehringen, also from Lower Saxony, was found underneath the skeleton of a straight-tusked elephant and is aged approximately 125,000 years, so it is much younger. The elephant was possibly killed by it.\n\n"}
{"id": "522768", "url": "https://en.wikipedia.org/wiki?curid=522768", "title": "Secondarily aquatic tetrapods", "text": "Secondarily aquatic tetrapods\n\nSeveral groups of tetrapods have undergone secondary aquatic adaptation, an evolutionary transition from being purely terrestrial to living at least part of the time in water. These animals are called \"secondarily aquatic\" because although their ancestors lived on land for hundreds of millions of years, they all originally descended from aquatic animals (see Evolution of tetrapods). These ancestral tetrapods had never left the water, and were thus primarily aquatic, like modern fishes. Secondary aquatic adaptations tend to develop in early speciation as the animal ventures into water in order to find available food. As successive generations spend more time in the water, natural selection causes the acquisition of more adaptations. Animals of later generations may spend the majority of their life in the water, coming ashore for mating. Finally, fully adapted animals may take to mating and birthing in water.\n\n\"Archelon\" is a type of giant sea turtle dating from the Cretaceous Period, now long extinct. Its smaller cousins survive as the sea turtles of today.\n\n\"Mesosaurus\" (and other mesosaurids) were another group of anapsid reptiles to secondarily return to the sea, eschewing shells, and are also long extinct.\n\nLiving at the same time as, but not closely related to, dinosaurs, the mosasaurs resembled crocodiles but were more strongly adapted to marine life. They became extinct 66 million years ago, at the same time as the dinosaurs. \n\nModern diapsids which have made their own adaptions to allow them to spend significant time in the water include marine iguanas and marine crocodiles. Sea snakes are extensively adapted to the marine environment, giving birth to live offspring in the same way as the Euryapsida (see below) and are largely incapable of terrestrial activity. The arc of their adaptation is evident by observing the primitive Laticauda genus, which must return to land to lay eggs.\n\nThese marine reptiles had ancestors who moved back into the oceans. In the case of ichthyosaurs adapting as fully as the dolphins they superficially resemble, even giving birth to live offspring instead of laying eggs. Euryapsida is now no longer considered a valid taxonomic group (Motani, 2009).\n\nDuring the Paleocene Epoch (about 66 - 55 million years ago), a group of wolf-like artiodactyls related to \"Pakicetus\" began pursuing an amphibious lifestyle in rivers or shallow seas. They were the ancestors of modern whales, dolphins, and porpoises. The cetacea are extensively adapted to marine life and cannot survive on land at all. Their adaptation can be seen in many unique physiognomic characteristics such as the dorsal blowhole, baleen teeth, and the cranial 'melon' organ used for aquatic echolocation. The closest extant terrestrial relative to the whale is the hippopotamus, which spends much of its time in the water and whose name literally means \"horse of the river\".\n\nThe ancestors of the dugong and manatees first appeared in the fossil record about 45 to 50 million years ago in the ocean.\n\nThe fossil records show that phocids existed 12 to 15 million years ago, and odobenids about 14 million years ago. Their common ancestor must have existed even earlier than that.\n\nAlthough polar bears spend most of their time on the ice rather than in the water, polar bears show the beginnings of aquatic adaptation to swimming (high levels of body fat and nostrils that are able to close), diving, and thermoregulation. Distinctly polar bear fossils can be dated to about 100,000 years ago. The polar bear has thick fur and layers of fat on its body to protect it from the cold.\n\nProponents of the aquatic ape hypothesis believe that part of human evolution includes some aquatic adaptation, which has been said to explain human hairlessness, bipedalism, increased subcutaneous fat, descended larynx, vernix caseosa, a hooded nose and various other physiological and anatomical changes. The idea is not accepted by most scholars who study human evolution.\n"}
{"id": "52209545", "url": "https://en.wikipedia.org/wiki?curid=52209545", "title": "Stuart Williamson", "text": "Stuart Williamson\n\nStuart Williamson (born 1948, England) is a sculptor, teacher of sculpture and poet from North East England. He is a Fellow of the Royal British Society of Sculptors, a Member of the Society of Portrait Sculptors (UK), a Member of the National Sculpture Society (USA), and a Founding Member of the Portrait Sculptures Society of America. He is also a Member of the Salmagundi Club in Manhattan and was featured in their American Masters show in 2015.\n\nWilliamson was formerly senior sculptor for Madam Tussauds at their London studio and there and abroad conducted sittings for sculpture with many notable contemporary persons and celebrities, including Sophie, Countess of Wessex, Chinese leader Jiang Zemin, Sting, Little Richard, Eric Clapton, Bette Midler, Tony Bennett, Shirley MacLaine, Morgan Freeman, Michael Caine, Pavarotti, Australian singer Kylie Minogue, and Bollywood actor Amitabh Bachchan.\nThe artist has sculpted, or collaborated on the sculpting, of numerous historical figures both in wax and bronze. He sculpted three portraits of George Washington at three stages of Washington's life for Mount Vernon, Abraham Lincoln for both the Gettysburg Museum and Visitor Center and the President Lincoln's Cottage at the Soldiers' Home, and Simon Bolivar for a private collection. He led the team and established the sculptural style for the sculpting of Benjamin Franklin, George Washington and five other signers for the National Constitution Center in Philadelphia, sculpted Albert Einstein for the Griffith Observatory in Los Angeles, Thomas Jefferson for Monticello, Franklin Roosevelt and Eleanor Roosevelt for the Franklin D. Roosevelt Presidential Library and Museum in New York, Robert E. Lee for the Stratford Hall (plantation) Museum in Virginia, Harry S. Truman for the Harry S. Truman Presidential Library and Museum in Missouri, poet John Keats for Guy's Hospital in London and poet Walt Whitman for a private client in Michigan.\n\nWilliamson has also created many personal works, including, Pina Bausch (Choreographer) and Bruno Schulz, (Author of 'Street of Crocodiles', Teacher and Artist), has exhibited at the Royal Academy Summer Exhibition, annually at the Society of Portrait Sculptors show, the Mall Galleries and The American Masters Exhibition at the Salmagundi Club in Manhattan, and has exhibited many of his pastel drawings worldwide.\n\nIn 2001 Williamson was asked to train a group of Ecuadorian artists in portraiture, molding, wax casting, colouring, and hair insertion for the development of their Historical Museum at the Metropolitan Cultural Center in Quito, Ecuador.\n\nIn 2014 Williamson sculpted a twice life-size bust of Professor Kin Yon Tin, who was one of the founders of the Civil Engineering Department at Fudan University, China. The bust was unveiled in Wuhan, China 21 October 2016.\nStuart Williamson lives and works part-time in both New York City and Ecuador. He is at present devoted to writing poetry.\n\n"}
{"id": "419674", "url": "https://en.wikipedia.org/wiki?curid=419674", "title": "Surfboard wax", "text": "Surfboard wax\n\nSurfboard wax (also known as surfwax) is a formulation of natural and/or synthetic wax for application to the deck of a surfboard, bodyboard, or skimboard, to keep the surfer from slipping off the board when paddling out or riding a wave. It is also used to increase grip on the paddle of a surf kayak or dragon boat.\n\nSurfboard wax is generally composed of a mixture of paraffin, beeswax or other hard waxes; petroleum jelly can also be added to create a softer wax. Often exotic scents like coconut or bubblegum are added to give the wax an attractive scent. There are also natural alternatives available containing only organic substances like beeswax, vegetable oils (such as coconut or hemp oil), pine resin, tree pulp and natural essential oils. Many different commercial brands and varieties of surfboard wax optimized for different climates and water temperatures may be found at a surf shop.\n\nRemoving the wax can be time-consuming, or it can take just take a few minutes. A plastic scraper (typically found on the back of a wax comb) can be used to remove the wax in large chunks. Residual wax that has not been removed by scraping can be dealt with using a soft cloth, either on its own or in combination with (for example) coconut oil. Various commercial solvents are also available. Solvents, however, are usually avoided as they can damage the surface or paint job of the board.\n\nAnother method that many surfers prefer is to let their board lie in the sun with the deck side up. After about ten minutes, beach sand is spread onto the melted wax and the sand is then rubbed. This method enables surfers to take all the wax off at once. The board is then left smooth and ready to be stored away or to be re-waxed. A surfer can also use a wax comb to scrape or comb the wax off the board.\n\nThere are two methods that surfers use to maintain their wax. Usually a wax comb is used to maintain the grip of the wax. Usually, the comb is used to engrave a cross-cut pattern into the wax to create a tackier surface.\n\nMost surfboard wax comes labeled with a water temperature range for which it is ideal. Wax used in water colder than its rating will become hard and not provide the stickiness needed to stay on the board, while wax used in water warmer than its rating may melt.\n\nSome surfers layer different temperatures of wax to create the level of firmness and stickiness desired. \n\nThe normal procedure is to lay down a thin base coat of a high temperature wax, usually labeled for tropical water, to build up bumps and texture. This will not melt off. Then a layer of temperature-appropriate wax or sticky wax is applied on top of that. This ensures that, as one changes the wax for different temperatures, the cold water wax will not come into contact with the board directly. However some waxes are designed to work in all water conditions and have the ability to remain on the board at any temperature.\n\nSurfboard wax is applied by first putting a base coat onto the clean board. This harder base coat forms a bump pattern on the surfboard which the surfer will then stick to. The base coat is then topped with a top coat of wax of an appropriate temperature. \n\nThere are a few techniques that can be used to apply the wax to a surfboard, from rubbing small circles to making diagonal lines. There is little difference in the end result, the application a surfer uses simply depends on preference. When re-applying surfboard wax it is always crucial to remove all prior layers of wax in order to ensure the best results.\n\n\n"}
{"id": "56974691", "url": "https://en.wikipedia.org/wiki?curid=56974691", "title": "Ultra high temperature ceramic matrix composite", "text": "Ultra high temperature ceramic matrix composite\n\nUltra-high temperature ceramic matrix composites (UHTCMC) or Ultra-high Temperature Ceramic Composites (UHTCC) are a class of refractory ceramic matrix composites (CMCs), the motivation to develop UHTCMCs is to overcome the limits associated with the bulk UHTCs like ZrB, HfB, or their composites due to a catastrofic fracture easily under mechanical or thermo-mechanical loads because of cracks initiated by small defects or scratches. The European Commission funded a research project, C3HARME, under the NMP-19-2015 call of Framework Programmes for Research and Technological Development in 2016 (still ongoing) for the design, development, production and testing of a new class of ultra-refractory ceramic matrix composites reinforced with silicon carbide fibers and Carbon fibers suitable for applications in severe aerospace environments as possible near-zero ablation thermal protection system (TPS) materials (e.g heat shield) and for propulsion (e.g. rocket nozzle). The demand for reusable advanced materials with temperature capability over 2000°C has growing. Recently carbon fiber reinforced zirconium boride-based composites obtained by slurry infiltration (SI) and sintering has been investigated.\nThe European Commission funded a research project, C3HARME, under the NMP-19-2015 call of Framework Programmes for Research and Technological Development in 2016 (still ongoing) for the design, development, production and testing of a new class of ultra-refractory ceramic matrix composites reinforced with silicon carbide fibers and Carbon fiberssuitable for applications in severe aerospace environments.\n"}
{"id": "1908133", "url": "https://en.wikipedia.org/wiki?curid=1908133", "title": "Valdez Blockade", "text": "Valdez Blockade\n\nThe Exxon Valdez oil spill of 1989 had devastated the shore around Prince William Sound, diminishing the marine population. Consequently, the fishery industry in the area faced a sharp fall on their fish catch and revenue. Feeling little had been done to study the impact of the spill, a group of fishermen sailed off to begin a blockade of the Valdez Narrows on August 20, 1993. While tankers must pass through Valdez Narrows to enter the port of Valdez, seven tankers were held off in the three-day blockade.\n\nAs oil was continuing to pump through the Trans-Alaska Pipeline System, and tankers were keeping off shore, the storage tanks in Valdez would soon overflow. With the probability in interrupting the oil flow to prevent an overflow, and also facing a growing loss in profits, the government came in to settle the blockade. The blockade was called off after Interior Secretary Bruce Babbitt promised to release $5 million of the Exxon Valdez Oil Spill restoration funds for ecosystem-wide studies.\n\nComprehensive studies of the effects of the spill toward the ecosystem around Prince William Sound began in the following year.\n\n\n"}
{"id": "968879", "url": "https://en.wikipedia.org/wiki?curid=968879", "title": "Vortex tube", "text": "Vortex tube\n\nThe vortex tube, also known as the Ranque-Hilsch vortex tube, is a mechanical device that separates a compressed gas into hot and cold streams. The gas emerging from the \"hot\" end can reach temperatures of 200 °C (392 °F), and the gas emerging from the \"cold end\" can reach . It has no moving parts.\n\nPressurised gas is injected tangentially into a \"swirl chamber\" and accelerated to a high rate of rotation. Due to the conical nozzle at the end of the tube, only the outer shell of the compressed gas is allowed to escape at that end. The remainder of the gas is forced to return in an inner vortex of reduced diameter within the outer vortex.\n\nTo explain the temperature separation in a vortex tube, there are two main approaches:\n\nThis approach is based on first-principles physics alone and is not limited to vortex tubes only, but applies to moving gas in general. It shows that temperature separation in a moving gas is due only to enthalpy conservation in a moving frame of reference.\n\nThe thermal process in the vortex tube can be estimated in the following way: 1) The adiabatic expansion of the incoming gas, which cools the gas and turns its heat content into the kinetic energy of rotation. The total enthalpy, which is the sum of the enthalpy and the kinetic energy, is conserved. 2) The peripheric rotating gas flow moves towards the hot outlet. Here the heat recuperation effect takes place between the quickly rotating peripheric flow and the opposite slowly rotating axial flow. Here the heat transfers from axial flow to the peripheric one. 3) The kinetic energy of rotation turns into the heat by the means of the viscous dissipation. The temperature of the gas rises. As the total enthalpy has been increased during the heat recuperation process, this temperature is higher than the incoming gas. 4) Some of the hot gas leaves the hot outlet, carrying away the excess heat. 5) The rest of the gas turns towards the cold outlet. As it passes its way to the cold outlet, its heat energy is transferred to the peripheric flow. Although the temperature at the axis and at the periphery is about the same everywhere, the rotation is slower at the axis, so the total enthalpy is lower as well. 6) The low total enthalpy cooled gas from the axial flow leaves the cold outlet.\n\nThe main physical phenomenon of the vortex tube is the temperature separation between the cold vortex core and the warm vortex periphery. The \"vortex tube effect\" is fully explained with the work equation of Euler, also known as Euler's turbine equation, which can be written in its most general vectorial form as:\n\nwhere formula_2 is the total, or stagnation temperature of the rotating gas at radial position formula_3, the absolute gas velocity as observed from the stationary frame of reference is denoted with formula_4; the angular velocity of the system is formula_5 and formula_6 is the isobaric heat capacity of the gas. This equation was published in 2012; it explains the fundamental operating principle of vortex tubes. The search for this explanation began in 1933 when the vortex tube was discovered and continued for more than 80 years.\n\nThe above equation is valid for an adiabatic turbine passage; it clearly shows that while gas moving towards the center is getting colder the peripheral gas in the passage is \"getting faster\". Therefore, vortex cooling is due to angular propulsion. The more the gas cools by reaching the center, the more rotational energy it delivers to the vortex and thus the vortex rotates even faster. This explanation stems directly from the law of energy conservation. Compressed gas at room temperature is expanded in order to gain speed through a nozzle; it then climbs the centrifugal barrier of rotation during which energy is also lost. The lost energy is delivered to the vortex, which speeds its rotation. In a vortex tube, the cylindrical surrounding wall confines the flow at periphery and thus forces conversion of kinetic into internal energy, which produces hot air at the hot exit.\n\nTherefore, the vortex tube is a rotorless turboexpander. It consists of a rotorless radial inflow turbine (cold end, center) and a rotorless centrifugal compressor (hot end, periphery). The work output of the turbine is converted into heat by the compressor at the hot end.\n\nThis approach relies on observation and experimental data. It is specifically tailored to the geometrical shape of the vortex tube and the details of its flow and is designed to match the particular observables of the complex vortex tube flow, namely turbulence, acoustic phenomena, pressure fields, air velocities and many others. The earlier published models of the vortex tube are phenomenological. They are:\n\n\nMore on these models can be found in recent review articles on vortex tubes.\n\nThe phenomenological models were developed at an earlier time when the turbine equation of Euler was not thoroughly analyzed; in the engineering literature, this equation is studied mostly to show the work output of a turbine; while temperature analysis is not performed since turbine cooling has more limited application unlike power generation, which is the main application of turbines. Phenomenological studies of the vortex tube in the past have been useful in presenting empirical data. However, due to the complexity of the vortex flow this empirical approach was able to show only aspects of the effect but was unable to explain its operating principle. Dedicated to empirical details, for a long time the empirical studies made the vortex tube effect appear enigmatic and its explanation – a matter of debate.\n\nThe vortex tube was invented in 1931 by French physicist Georges J. Ranque. It was rediscovered by Paul Dirac in 1934 while he was searching for a device to perform isotope separation, see Helikon vortex separation process. German physicist improved the design and published a widely read paper in 1947 on the device, which he called a \"Wirbelrohr\" (literally, whirl pipe). \nIn 1954, Westley published a comprehensive survey entitled ‘‘A bibliography and survey of the vortex tube’’, which included over 100 references. In 1951 Curley and McGree , in 1956 Kalvinskas , in 1964 Dobratz , in 1972 Nash , and in 1979 Hellyar made important contribution to the RHVT literature by their extensive reviews on the vortex tube and its applications.\nFrom 1952 to 1963, C. Darby Fulton, Jr. obtained four U.S. patents relating to the development of the vortex tube. In 1961, Fulton began manufacturing the vortex tube under the company name Fulton Cryogenics. Dr. Fulton sold the company to Vortec, Inc. The vortex tube was used to separate gas mixtures, oxygen and nitrogen, carbon dioxide and helium, carbon dioxide and air in 1967 by Linderstrom-Lang.\n\nVortex tubes also seem to work with liquids to some extent, as demonstrated by Hsueh and Swenson in a laboratory experiment where free body rotation occurs from the core and a thick boundary layer at the wall. Air is separated causing a cooler air stream coming out the exhaust hoping to chill as a refrigerator. In 1988 R. T. Balmer applied liquid water as the working medium. It was found that when the inlet pressure is high, for instance 20-50 bar, the heat energy separation process exists in incompressible (liquids) vortex flow as well. Note that this separation is only due to heating; there is no longer cooling observed since cooling requires compressibility of the working fluid.\n\nVortex tubes have lower efficiency than traditional air conditioning equipment. They are commonly used for inexpensive spot cooling, when compressed air is available.\n\nCommercial vortex tubes are designed for industrial applications to produce a temperature drop of up to 71 °C (127 °F). With no moving parts, no electricity, and no Freon, a vortex tube can produce refrigeration up to using only filtered compressed air at 100 PSI (6.9 bar). A control valve in the hot air exhaust adjusts temperatures, flows and refrigeration over a wide range.\n\nVortex tubes are used for cooling of cutting tools (lathes and mills, both manually-operated and CNC machines) during machining. The vortex tube is well-matched to this application: machine shops generally already use compressed air, and a fast jet of cold air provides both cooling and removal of the \"chips\" produced by the tool. This completely eliminates or drastically reduces the need for liquid coolant, which is messy, expensive, and environmentally hazardous.\n\n\n\n"}
{"id": "37281193", "url": "https://en.wikipedia.org/wiki?curid=37281193", "title": "Wafering", "text": "Wafering\n\nWafering is the process by which a silicon crystal (boule) is made into wafers. This process is usually carried out by a multi wire saw which cuts multiple wafers from the same crystal at the same time. These wafers are then polished to the desired degree of flatness and thickness.\n\n\n"}
{"id": "38216008", "url": "https://en.wikipedia.org/wiki?curid=38216008", "title": "Western Canadian Select", "text": "Western Canadian Select\n\nWestern Canadian Select is one of North America's largest heavy crude oil streams. It is a heavy blended crude oil, composed mostly of bitumen blended with sweet synthetic and condensate diluents and 25 existing streams of both conventional and unconventional Alberta heavy crude oils at the large Husky Energy terminal in Hardisty, Alberta. Western Canadian Select—which is the benchmark for emerging heavy, high TAN (acidic) crudes— is one of many petroleum products from the Western Canadian Sedimentary Basin oil sands. WCS was launched in December 2004 as a new heavy oil stream by EnCana (now Cenovus), Canadian Natural Resources Limited, Petro-Canada (now Suncor) and Talisman Energy Inc. (now Repsol Oil & Gas Canada Inc.). Husky Energy has managed WCS terminal operations since 2004 and joined the WCS Founders in 2015.\n\nCrude prices are typically quoted at a particular location. Unless stated otherwise, the price of WCS is quoted at Hardisty and the price of West Texas Intermediate (WTI) is quoted at Cushing, Oklahoma. By December 14, 2015 with the price of WTI at $35 a barrel, WCS fell \"75 per cent to $21.82,\" the lowest in seven years and Mexico's Maya heavy crude was down \"73 per cent in 18 months to $27.74. By February 2016 WTI had dropped to US$29.85 and WCS was US$14.10 with a differential of $15.75. By June 2016 WTI was priced at US$46.09, Brent at MYMEX was US$47.39 and WCS was US$33.94 with a differential of US$12.15. By December 10, 2016 WTI had risen to US$51.46 and WCS was US$36.11 with a differential of $15.35. According to monthly data provided by the U.S. Energy Information Administration (EIA), in 2015 \"Canada remained the largest exporter of total petroleum to the United States exporting 3,789 thousand bpd in September and 3,401 thousand bpd in October.\" This has increased from 3,026 thousand bpd in September 2014. This represents 99% of Canada's oil exports and gives Americans no incentive to pay more for Canadian petroleum.\n\nBitumen comprises all of Canada's unconventional oil, and is either upgraded to synthetic light crude, processed into asphalt or blended with other crudes and refined into products such as diesel, gasoline and jet fuel oil.\n\n WCS was launched in December 2004 as a new heavy oil stream by Cenovus (then EnCana), Canadian Natural Resources Limited, Suncor (then Petro-Canada) and Talisman Energy Inc. \n\nSince it came onstream WCS has been blended at the Husky Hardisty terminal. According to Argus, in 2012 the WCS blend is produced by only the four companies mentioned above. \"[T]he prospects for adding new producers are complicated by the internal rules set in place to compensate each producer for its contributions to the blend.\"\n\nCompanies tied to WCS as benchmark such as MEG Energy Corp, whose output is bitumen, benefit with an annual cash flow increase of 40% with every $5 increase in the price of WCS. Crude from MEG's 210,000-barrel-a-day Christina Lake oil sands site is marketed as Access Western Blend, which competes with WCS. Others such as BlackPearl Resources Inc. and Northern Blizzard Resources Inc also benefit from the higher WCS price. \"In the seven weeks that heavy crude has staged its rebound, MEG shares are up 27 per cent, BlackPearl’s 37 per cent and Northern Blizzard’s 21 per cent.\"\n\nAccording to a report by real estate consultants Avison Young, by August 2015 in downtown Calgary, \"layoffs by major oil and gas companies\" were reflected in higher vacancy rates in the second quarter.\n\nBy 18 March 2015 the price of benchmark crude oils, WTI had dropped to $US43.34/bbl. from a high in June 2014 with WTI priced above US$107/bbl and Brent above US$115/bbl. WCS, a bitumen-derived crude, is a heavy crude that is similar to Californian heavy crudes, Mexico's Maya crude or Venezuelan heavy crude oils. On 15 March 2015 the differential between WTI and WCS was US$13.8. Western Canadian Select was among the cheapest crude oils in the world with a price of US$29.54/bbl on 15 March 2015, its lowest price since April 2009. By mid-April 2015 WCS had risen almost fifty percent to trade at $US44.94. By 2 June 2015 the differential between WTI and WCS was US$7.8, the lowest it had ever been. By 12 August 2015 the WCS price dropped to $23.31 and the WTI/WCS differential had risen to $19.75, the lowest price in nine years when BP temporarily shut down its Whiting, Indiana refinery for two weeks, the sixth largest refinery in the United States, to repair the largest crude distillation unit at its Whiting, Indiana refinery. At the same time Enbridge was forced to shut down Line 55 Spearhead pipeline and Line 59 Flanagan South pipeline in Missouri because of a crude oil leak. By December 2015 the price of WCS was US$23.46, the lowest price since December 2008 and The WTI-WCS differential was US$13.65. By June 2016 the price of WCS was US$33.94.\n\nIn mid-December 2015, when the price of both Brent and WTI was about $35 a barrel and WCS was $21.82, Mexico's comparable heavy sour crude, Maya was also down \"73 per cent in 18 months to $27.74. However, the Mexican government had somewhat protected its economy.\n\n\"The extremely viscous oil contained in oil sands deposits is commonly referred to as bitumen\" (CAS 8052-42-4) At the Husky Hardisty terminal, Western Canadian Select is blended from sweet synthetic and condensate diluents from 25 existing Canadian heavy conventional and unconventional bitumen crude oils.\n\nWestern Canadian Select is a heavy crude oil with an API gravity level of between 19 and 22 (API), 20.5° (Natural Gas and Petroleum Products 2009).\n\nWestern Canadian Select's characteristics are described as follows: Gravity, Density (kg/m3) 930.1, MCR (Wt%) 9.6, Sulphur (Wt%) 2.8-3.5%, TAN (Total Acid number) of (Mg KOH/g) 0.93.\n\nRefiners in North America consider a crude with a TAN value greater than 1.0 as \"high-TAN\". A refinery must be retrofitted in order to handle high TAN crudes. Thus, a high TAN crude is limited in terms of the refineries in North America that are able to process it. For this reason, the TAN value of WCS is consistently maintained under 1.0 through blending with light, sweet crudes and condensate. Certain other bitumen blends, such as Access Western Blend and Seal Heavy Blend, have higher TAN values and are considered high TAN.\n\nWCS has an API gravity of 19-22.\n\n\"Oil sands crude oil does not flow naturally in pipelines because it is too dense. A diluent is normally blended with the oil sands bitumen to allow it to flow in pipelines. For the purpose of meeting pipeline viscosity and density specifications, oil sands bitumen is blended with either synthetic crude oil (synbit) and/or condensate (Dilbit).\" WCS may be referred to as a syndilbit, since it may contain both synbit and dilbit.\n\nIn a study commissioned by the U.S. Department of State (DOS), regarding the Environmental Impact Statement (EIS) for the Keystone XL pipeline project, the DOS assumes \"that the average crude oil flowing through the pipeline would consist of about 50% Western Canadian Select (dilbit) and 50% Suncor Synthetic A (SCO).\"\n\nThe Canadian Society of Unconventional Resources (CSUR) identifies four types of oil: conventional oil, tight oil, oil shale and heavy oil like WCS.\n\nBy September 2014 Canada was exporting 3,026 thousand bpd to the United States. This increased to its peak of 3,789 thousand bpd in September, 2015 and 3,401 thousand bpd in October, 2015. This represents 99% of Canadian petroleum exports.\" Threshold volumes of WCS in 2010 were only approximately 250,000 barrels per day.\n\nA devastating wildfire that began on May 1, 2016, swept through Fort McMurray and resulted in the largest wildfire evacuation in Albertan history. As the fires progressed north of Fort McMurray \"oil sands production companies operating near Fort McMurray either shut down completely or operated at reduced rates.\" By 8 June 2016, the U. S. Department of Energy estimated that \"disruptions to oil production averaged about 0.8 million barrels per day (b/d) in May, with a daily peak of more than 1.1 million b/d. Although projects are slowly restarting as fires subside, it may take weeks for production to return to previous levels.\" The Fort McMurray fires did not significantly affect the price of WCS.\n\n\"According to EIA's February Short-Term Energy Outlook, production of petroleum and other liquids in Canada, which totaled 4.5 million barrels per day (b/d) in 2015, is expected to average 4.6 million b/d in 2016 and 4.8 million b/d in 2017. This increase is driven by growth in oil sands production of about 300,000 b/d by the end of 2017, which is partially offset by a decline in conventional oil production.\" The EIA claims that while oil sands projects may be operating at a loss, these projects are able to \"withstand volatility in crude oil prices.\" It would cost more to shut a project down - from $500 million to $1 billion than to operate at a loss.\n\nIn this table, which is based on the Scotiabank Equity Research and Scotiabank Economics report that was published 28 November 2014,\neconomist Mohr compares the cost of cumulative crude oil production in the fall of 2014. \n\nWCS is very expensive to produce. There are exceptions, such as Cenovus Energy's Christina Lake facility which produces some of the lowest-cost barrels in the industry.\n\nIn June 2012 Fairfield, Connecticut-based General Electric, with its focus on international markets, opened its Global Innovation Centre in downtown Calgary with \"130 privately employed scientists and engineers,\" the \"first of its kind in North America\" and the second in the world. GE's first Global Innovation centre is in Chengdu, China, which also opened in June 2012. GE's Innovation Centre is \"attempting to embed innovation directly into the architecture.\" James Cleland, general manager of the Heavy Oil Centre for Excellence, which makes up one-third of Global Innovation Centre, said, \"Some of the toughest challenges we have today are around environmental issues and cost escalations... The oil sands would be rebranded as eco-friendly oil or something like that; basically to have changed the game.\"\n\nGE's thermal evaporation technology developed in the 1980s for use in desalination plants and the power generation industry was repurposed in 1999 to improve on the water-intensive Steam Assisted Gravity Drainage (SAGD) method used to extract bitumen from the Athabasca Oil Sands. In 1999 and 2002 Petro-Canada’s MacKay River facility was the first to install 1999 and 2002 GE SAGD zero-liquid discharge (ZLD) systems using a combination of the new evaporative technology and crystallizer system in which all the water was recycled and only solids were discharged off site. This new evaporative technology began to replace older water treatment techniques employed by SAGD facilities, which involved the use of warm lime softening to remove silica and magnesium and weak acid cation ion exchange used to remove calcium.\n\nCleland describes how Suncor Energy is investigating the strategy of replication where engineers design an \"ideal\" small-capacity SAGD plant with a 400 to 600 bd capacity that can be replicated through \"successive phases of construction\" with cost-saving \"cookie cutter\", \"repeatable\" elements. Although the first replicable facility will not be online before 2019, Suncor hopes to eventually implement the technology on its leases at Meadow Creek, Lewis, MacKay River and Firebag.\n\nThe price of petroleum as quoted in news in North America, generally refers to the WTI Cushing Crude Oil Spot Price per barrel (159 liters) of either WTI/light crude as traded on the New York Mercantile Exchange (NYMEX) for delivery at Cushing, Oklahoma, or of Brent as traded on the Intercontinental Exchange (ICE, into which the International Petroleum Exchange has been incorporated) for delivery at Sullom Voe. West Texas Intermediate (WTI), also known as Texas Light Sweet, is a type of crude oil used as a benchmark in oil pricing and the underlying commodity of New York Mercantile Exchange's oil futures contracts. WTI is a light crude oil, lighter than Brent Crude oil. It contains about 0.24% sulfur, rating it a sweet crude, sweeter than Brent. Its properties and production site make it ideal for being refined in the United States, mostly in the Midwest and Gulf Coast (USGC) regions. WTI has an API gravity of around 39.6 (specific gravity approx. 0.827). Cushing, Oklahoma, a major oil supply hub connecting oil suppliers to the Gulf Coast, has become the most significant trading hub for crude oil in North America.\n\nThe National Bank of Canada's Tim Simard, argued that WCS is the benchmark for those buying shares in Canadian oil sands companies, such as Canadian Natural Resources Ltd., or Cenovus Energy Inc., Northern Blizzard Resources Inc., Pengrowth Energy Corp., or Twin Butte Energy Ltd or others where a \"big part of their exposure will be to heavy crude.”\n\nThe price of Western Canadian Select (WCS) crude oil (petroleum) per barrel suffers a differential against West Texas Intermediate (WTI) as traded on the New York Mercantile Exchange (NYMEX) as published by Bloomberg Media, which itself has a discount versus London-traded Brent oil. This is based on data on prices and differentials from Canadian Natural Resources Limited (TSX:CNQ)(NYSE:CNQ).\n\n\"West Texas Intermediate Crude oil (WTI) is a benchmark crude oil for the North American market, and Edmonton Par and Western Canadian Select (WCS) are benchmarks crude oils for the Canadian market. Both Edmonton Par and WTI are high-quality low sulphur crude oils with API gravity levels of around 40°. In contrast, WCS is a heavy crude oil with an API gravity level of 20.5°.\"\n\nWest Texas Intermediate WTI is a sweet, light crude oil, with an API gravity of around 39.6 and specific gravity of about 0.827, which is lighter than Brent crude. It contains about 0.24% sulfur thus is rated as a sweet crude oil (having less than 0.5% sulfur), sweeter than Brent which has 0.37% sulfur. WTI is refined mostly in the Midwest and Gulf Coast regions in the U.S., since it is high quality fuel and is produced within the country.\n\n\"WCS prices at a discount to WTI because it is a lower quality crude (3.51Wt. percent sulfur and 20.5 API gravity) and because of a transportation differential. The price of WCS is currently set at the U.S. Gulf Coast. It costs approximately $10/bbl for a barrel of crude to be transported from Alberta to the U.S. Gulf Coast, accounting for at least $10/bbl of the WTI-WCS discount. Pipeline constraints can also cause the transportation differential to rise significantly.\n\nBy March 2015, with the price of Ice Brent at US$60.55, and WTI at US$51.48, up US$1.10 from the previous day, WCS also rose US$1.20 to US$37.23 with a WTI-WCS price differential of US$14.25. By 2 June 2015 with Brent at US$64.88/bbl, WTI at US$60.19/bbl and WCS at US$52.39/bbl.\n\nAccording to the Financial Post, most Canadian investors continued to quote the price of WTI not WCS even though many Canadian oilsands producers sell at WCS prices, because WCS \"has always lacked the transparency and liquidity necessary to make it a household name with investors in the country.\" In 2014 Auspice created the Canadian Crude Excess Return Index to gauge WCS futures. Tim Simard, head of commodities at the National Bank of Canada, claims “WCS has \"some interesting different fundamental attributes than the conventional WTI barrel.\" WCS has \"better transparency and broader participation\" than Maya. However, he explained that in 2015 \"one of the only ways to take a position in oil is to use an ETF that is tied to WTI.\" Simard claims that when the global price of oil is lower, for example, \"the first barrels to be turned off in a low-price environment are heavy barrels\" making WCS \"closer to the floor\" than WTI.\n\nIn order to address the transparency and liquidity issues facing WCS, Auspice created the Canadian Crude Index (CCI), which serves as a benchmark for oil produced in Canada. The CCI allows investors to track the price, risk and volatility of the Canadian commodity. The CCI can be used to identify opportunities to speculate outright on the price of Canadian crude oil or in conjunction with West Texas Intermediate (WTI) to put on a spread trade which could represent the differential between the two. The CCI provides a fixed price reference for Canadian Crude Oil by targeting an exposure that represents a three-month rolling position in crude oil. To create a price representative of Canadian crude the index uses two futures contracts: A fixed price contract, which represents the price of crude oil at Cushing, Oklahoma, and a basis differential contract, which represents the difference in price between Cushing and Hardisty, Alberta. Both contracts are priced in U.S. dollars per barrel. Together, these create a fixed price for Canadian crude oil, and provide an accessible and transparent index to serve as a benchmark to build investable products upon, and could ultimately increase its demand to global markets.\n\nIn the spring of 2015, veteran journalist specializing in energy and finance, Jeffrey Jones, described how the price of WCS \"surged more than 70 per cent, outpacing West Texas intermediate (WTI), Brent\" and \"quietly\" became the \"hottest commodity in North American energy.\" In April 2015, Enbridge filled a \"new 570,000-barrel-a-day pipeline.\" A May 2015 TD Securities report provides some of the factors contributing the WCS price gains as \"normal seasonal strength driven by demand for the thick crude to make asphalt as road paving\", improvements to WCS access to various U.S. markets in spite of pipeline impediments, five-year high production levels and high heavy oil demand in U.S. refineries particularly in the US Midwest, a key market for WCS.\n\nBy 9 September 2015, the price of WCS was US$32.52 and the WTI-WCS differential was differential US$13.35.\n\nBy June 2015 the differential between WTI and WCS was US$7.8, the lowest it has ever been.\n\nIn a 2013 white paper for the Bank of Canada, authors Alquist and Guénette examined implications for high global oil prices for the North American market. They argued that North America was experiencing a crude oil inventory surplus. This surplus combined with the \"segmentation of the North American crude oil market from the global market\", contributed to \"the divergence between continental benchmark crudes such as WTI and Western Canada Select (WCS) and seaborne benchmark crudes such as Brent (Figure 3).11 I.\"\n\nAlberta's Minister of Finance argues that WCS \"should be trading on par with Mayan crude at about $94 a barrel.\" Maya crudes are close to WCS quality levels. However, Maya was trading at US$108.73/bbl in February 2013, while WCS was US$69/bbl. In his presentation to the U.S. Energy Information Administration (EIA) in 2013 John Foran demonstrated that Maya had traded at only a slight premium to WCS in 2010. Since then WCS price differentials widened \"with rising oil sands and tight oil production and insufficient pipeline capacity to access global markets.\" Mexico enjoys a location discount with its proximity to the heavy oil-capable refineries in the Gulf Coast. As well, Mexico began to strategically and successfully seek out joint venture refinery partnerships in the 1990s to create a market for its heavy crude oil in the U.S. Gulf. In 1993, (Petróleos Mexicanos, the state-owned Mexican oil company) and Shell Oil Company agreed on a joint US$1 billion refinery upgrading construction project which led to the construction of a new coker, hydrotreating unit, sulfur recovery unit and other facilities in Deer Park, Texas on the Houston Ship Channel in order to process large volumes of PEMEX heavy Maya crude while fulfilling the U.S. Clean Air Act requirements.\n\n(Prices except Maya for years 2007-February 2013)(Prices for Maya) (Prices for 24 April 2013).\n\nBy July 2013, Western Canadian Select (WCS) \"heavy oil prices climbed from US$75 to more than US$90 per barrel — the highest level since mid-2008, when WTI oil prices were at a record (US$147.90) — just prior to the 2008-09 'Great Recession'.\" WCS \"heavy oil prices were \"expected to remain at the US$90, which is closer to the world price for heavy crude and WCS 'true, inherent value'.\" The higher price of WCS oil off WTI was explained by \"new rail shipments alleviating some export pipeline constraints — and the return of WTI oil prices to international levels.\"\n\nBy January 2014 there was a proliferation of trains and pipelines carrying WCS along with an increased demand on the part of U.S. refineries. By early 2014 there were approximately 150,000 barrels a day of heavy oil being transported by rail.\n\nAccording to the Government of Alberta's June 2014 Energy Prices report the price of WCS rose 15% from $68.87 in April 2013 to $79.56 in April 2014 but experienced a low of $58 and a high of $91. During the same time period the price of the benchmark West Texas Intermediate (WTI) rose 10.9% averaging $102.07 a barrel in April 2014.\n\nHeavy discounts on Albertan crudes in 2012 were attributed to crudes being \"landlocked\" in the U.S. Midwest. Since that time, several major pipelines have been constructed to release that glut, including Seaway, the Southern leg of Keystone XL and Flanagan South.\n\nHowever, significant obstacles persist in approvals on pipelines to export crude from Alberta. In April 2013, Calgary-based Canada West Foundation warned that Alberta is \"running up against a [pipeline capacity] wall around 2016, when we will have barrels of oil we can’t move.\" For the time being, rail shipments of crude oil have filled the gap and narrowed the price differential between Albertan and North American crudes. However, additional pipelines exporting crude from Alberta will be required to support ongoing expansion in crude production.\n\nIn the United States, Democrats are concerned that Keystone XL would simply facilitate getting Alberta oil sands products to tidewater for export to China and other countries via the American Gulf Coast of Mexico.\n\nOn 1 August 2013, TransCanada CEO Russ Girling announced that the company was moving forward on the $12 billion 4,400-kilometre (2,700 mile) Energy East pipeline pipeline project with a proposed completion date in 2017 or 2018. In the long term, this would mean that WCS could be shipped to Atlantic tidewater via deep water ports such as Quebec City and Saint John. Potential heavy oil overseas destinations include India, where super refineries capable of processing vast quantities of oil sands oil are already under construction. In the meantime, Energy East pipeline would be used to send light sweet crude, such as Edmonton Par crude from Alberta to eastern Canadian refineries Montreal/Quebec City, for example. Eastern Canadian refineries, such as Imperial Oil Ltd. 88,000-barrel-a-day refinery in Dartmouth, N.S., currently import crude oil from North and West Africa and Latin America, according to Mark Routt, \"a senior energy consultant at KBC in Houston, who has a number of clients interested in the project.\" The proposed Energy East Pipeline will have the potential of carrying 1.1-million barrels of oil per day from Alberta and Saskatchewan to eastern Canada.\n\nPatricia Mohr, a Bank of Nova Scotia senior economist and commodities analyst, in her report on the economic advantages to Energy East, argued that, Western Canada Select, the heavy oil marker in Alberta, \"could have earned a much higher price in India than actually received\" in the first half of 2013 based on the price of Saudi Arabian heavy crude delivered to India\" if the pipeline had already been operational.In her report, Bohr predicted that initially Quebec refineries, owned by Suncor Energy Inc. and Valero, for example could access comprise light oil or upgraded synthetic crude from Alberta's oil sands via Energy East to displace \"imports priced off more expensive Brent crude.\" In the long term, supertankers using the proposed Irving/TransCanada deep-sea Saint John terminal could ship huge quantities of Alberta's blended bitumen, such as WCS to the super refineries in India. Mohr predicted in her report that the price of WCS would increase to US$90 per barrel in July, 2013 up from US$75.41 in June.\"\n\nCanada's largest refinery, capable of processing 300,000 barrels of oil per day, is owned and operated by Irving Oil, in the deep-water port of Saint John, New Brunswick, on the east coast. A proposed $300-million deep water marine terminal, to be constructed and operated jointly by TransCanada and Irving Oil Ltd., would be built near Irving Oil's import terminal with construction to begin in 2015.\n\nMaine-based Portland–Montreal Pipe Line Corporation, which consists of Portland Pipe Line Corporation (in the United States) and Montreal Pipe Line Limited (in Canada), is considering ways to carry Canadian oil sands crude to Atlantic tidewater at Portland's deep-water port. The proposal would mean that crude oil from the oil sands would be piped via the Great Lakes, Ontario, Quebec and New England to Portland, Maine. The pipelines are owned by ExxonMobil and Suncor.\n\nBy 2011, output from the Bakken Shale formation in North Dakota Crude was increasing faster than pipelines could be built. Oil producers and pipeline companies turned to railroads for transportation solutions. Bakken oil competes with WCS for access to transportation by pipeline and by rail. By the end of 2010, Bakken oil production rates had reached per day, thereby outstripping the pipeline capacity to ship oil out of the Bakken. By January 2011 Bloomberg News reported that Bakken crude oil producers were using railway cars to ship oil.\n\nIn 2013, there were new rail shipments of WCS. Since 2012, the amount of crude oil transported by rail in Canada had quadrupled and by 2014 it was expected to continue to surge.\n\nIn August 2013, then-U.S. Development Group's (now USD Partners) CEO, Dan Borgen, a Texas-based oil-by-rail pioneer, shifted his attention away from the U.S. shale oil plays towards the Canadian oil sands. Borgen \"helped introduce the energy markets to specialized terminals that can quickly load mile-long oil tank trains heading to the same destination - facilities that ... revolutionized the U.S. oil market.\" Since 2007, Goldman Sachs has played a leading role in financing USD's \"expansion of nearly a dozen specialized terminals that can quickly load and unload massive, mile-long trains carrying crude oil and ethanol across the United States.\" USD's pioneering projects included large-scale “storage in transit” (SIT) inspired by the European model for the petrochemicals industry. USD sold five of the specialized oil-by-rail US terminals to \"Plains All American Pipeline for $500 million in late 2012, leaving the company cash-rich and asset light.\" According to Leff, concerns have been raised about the link between Goldman Sachs and USD.\n\nBy January 2014 there was a proliferation of trains and pipelines carrying WCS along with an increased demand on the part of U.S. refineries. By early 2014 there were approximately 150,000 barrels a day of heavy oil being transported by rail.\n\nThe price of WCS rose in August 2014 as anticipated expansions in crude-by-rail capacity at Hardisty increased when USDG Gibson Energy's Hardisty Terminal, the new state-of-the-art crude-by-rail origination terminal and loading facility with pipeline connectivity, became operational in June 2014 with a capacity to load up to two 120-rail car unit trains per day (120,000 of heavy crude bbd). The Hardisty rail terminal can load up to two 120-railcar unit trains per day \"with 30 railcar loading positions on a fixed loading rack, a unit train staging area and loop tracks capable of holding five unit trains simultaneously.\" By 2015 there was \"a newly-constructed pipeline connected to Gibson Energy Inc.’s Hardisty storage terminal\" with \"over 5 million barrels of storage in Hardisty.\"\n\nIn 2014, CPR COO Keith Creel said CPR was in a growth position in 2014 thanks to the increased Alberta crude oil (WCS) transport that will account for one-third of CPR's new revenue gains through 2018 \"aided by improvements at oil-loading terminals and track in western Canada.\" By 2014 CP was shaped by CEO Hunter Harrison and American activist shareholder Bill Ackman. Americans own 73% of CP shares, while Canadians and Americans each own 50% of CN. In order to improve returns for their shareholders, railways cut back on their workforce and downsized the number of locomotives.\n\nCreel said in a 2014 interview that the transport of Alberta's heavy crude oil would account for about 60% of the CP's oil revenues, and light crude from the Bakken Shale region in Saskatchewan and the U.S. state of North Dakota would account for 40%. Prior to the implementation of tougher regulations in both Canada and the United States following the Lac-Mégantic rail disaster and other oil-related rail incidents which involved the highly volatile, sensitive light sweet Bakken crude, Bakken accounted for 60% of CP's oil shipments. Creel said that \"It [WCS is] safer, less volatile and more profitable to move and we’re uniquely positioned to connect to the West Coast as well as the East Coast.\n\nRail way officials claim that more Canadian oil-by-rail traffic is \"made up of tough-to-ignite undiluted heavy crude and raw bitumen.\"\n\nCPR's high capacity North Line, which runs from Edmonton to Winnipeg, is connected to \"all the key refining markets in North America.\" Chief Executive Hunter Harrison told the \"Wall Street Journal\" in 2014 that Canadian Pacific would improve tracks along its North Line as part of a plan to ship Alberta oil east.\n\nOn 21 September 2014 Suncor Energy Inc. loaded its first tanker of heavy crude, about 700,000 barrels of WCS, onto the tanker Minerva Gloria at the port of Sorel near Montreal, Quebec. The Minerva Gloria is an Aframax Crude Oil Tanker double hulled tanker with a Deadweight tonnage (DWT) 115,873 ton capacity. Its destination was Sarroch, on the Italian island of Sardinia. The Minerva Gloria measures × .\n\nThe Spanish oil company Repsol (REP.MC) obtained the licence from the U.S. Department of Commerce to export 600,000 barrels of WCS from the United States. The WCS was shipped via Freeport, Texas in the Gulf Coast (USGC) to the port of Bilbao on the Suezmax oil tanker, the Aleksey Kosygin. It is considered to be \"the first re-export of Canadian crude from the USGC to a non-US port.\" as the \"US government tightly controls any crude exports, including of non-US grades.\" The Brussels-based European Union's European Environment Agency (EEA) monitored the trade. WCS, with its API of 20.6 and sulphur content of 3.37%, has been controversial.\n\nIn December 2014, Repsol agreed to buy Talisman Energy (TLM.TO), Canada's fifth-largest independent oil producer, for US$8.3 billion which is estimated to be at about 50 per cent of Talisman's value in June 2014. By December 2014, the price of WCS had dropped to US$40.38 from $79.56 in April 2014. The global demand for oil decreased, production increased and the price of oil plunged starting in June and continuing to drop through December.\n\nInvestors placed bullish bets in the six weeks from January through February 8, 2013, on oil futures based on the U.S. central bank's bond-buying program that \"adds liquidity to the financial markets.\" The demand, and therefore the price, of commodities in general and oil in particular falls if and when such a program is scaled back. Even a rumor that a hedge fund is in trouble and was liquidating positions can cause the price of U.S. crude oil to fall. Signs of strong demand of crude oil from China and India with hopes of a tighter market can raise the price and even an oil rally. Investors also refer to the Energy Information Administration reports on U.S. inventories of commercial crude oil. The higher the inventory of crude oil, the lower the price. U.S. inventories of commercial crude oil hit their highest level on February 15, 2013 since July 2012.\"\n\nMost Western Canadian Select is piped to Illinois for refinement and then to Cushing, Oklahoma for sale. Western Canadian Select (WCS) futures contracts are available on the Chicago Mercantile Exchange (CME)while bilateral over-the-counter WCS swaps can be cleared on Chicago Mercantile Exchange (CME)'s ClearPort or by NGX.\n\nWCS is transported from Alberta to refineries with capacity to process heavy oil from the oil sands. The Petroleum Administration for Defense Districts (Padd II), in the US Midwest, have experience running the WCS blend. Most of WCS goes to refineries in the Midwestern United States where refineries \"are configured to process a large percentage of heavy, high-sulfur crude and to produce large quantities of transportation fuels, and low amounts of heavy fuel oil.\" While the US refiners \"invested in more complex refinery configurations with higher processing capability\" that use \"cheaper feedstocks\" like WCS and Maya, Canada did not. While Canadian refining capacity has increased through scale and efficiency, there are only 19 refineries in Canada compared to 148 in the United States.\n\nWCS crude oil with its \"very low API (American Petroleum Institute) gravity and high sulphur content and levels of residual metals\" requires specialized refining that few Canadian refineries have. It can only be processed in refiners modified with new metallurgy capable of running high-acid (TAN) crudes.\n\n\"The transportation costs associated with moving crude oil from the oil fields in Western Canada to the consuming regions in the east and the greater choice of crude qualities make it more economic for some refineries to use imported crude oil. Therefore, Canada’s oil economy is now a dual market. Refineries in Western Canada run domestically produced crude oil, refineries in Quebec and the eastern provinces run primarily imported crude oil, while refineries in Ontario run a mix of both imported and domestically produced crude oil. In more recent years, eastern refineries have begun running Canadian crude from east coast offshore production.\"\n\nUS refineries import large quantities of crude oil from Canada, Mexico, Columbia and Venezuela, and they began in the 1990s to build coker and sulfur capacity enhancements to accommodate the growth of these medium and heavy sour crude oils while meeting environment requirements and consumer demand for transportation fuels. \"While US refineries have made significant investments in complex refining hardware, which supports processing heavier, sourer crude into gasoline and distillates, similar investment outside the US has been pursued less aggressively. Medium and heavy crude oil make up 50% of US crude oil inputs and the US continues to expand its capacity to process heavy crude.\n\nLarge integrated oil companies that produce WCS in Canada have also started to invest in upgrading refineries in order to process WCS.\n\nThe BP Plc refinery in Whiting, Indiana is the sixth largest refinery in the US with a capacity of 413,500-barrel of crude oil per day (bpd). In 2012 BP began investing in a multi-billion modernization project at the Whiting refinery in order to distill WCS. This $4 billion refit was completed in 2014 and was one of the factors contributing to the increase in price of WCS. The centerpiece of the upgrade was Pipestill 12, the refinery's largest crude distillation unit, which came online in July 2013. Distillation units provide feedstock for all the other units of the refinery by distilling the crude as it enters the refinery. The Whiting refinery is situated close to the border between Indiana and Illinois. It is the major buyer of CWS and WTI from Cushing, Oklahoma, the delivery point of the US benchmark oil contract.\n\nOn 8 August 2015 there was a malfunction of piping inside Pipestill 12 causing heavy damage and the unit was offline until August 25. This was one of the major factos contributing to the drop in the price of oil with WCS at its lowest price in nine years.\n\nThe Toledo refinery in northwestern Ohio, in which BP has invested around $500 million on improvements since 2010, is a joint venture with Husky Energy, which operates the refinery, and processes approximately 160,000 barrels of crude oil per day. Since the early 2000s, the company has been focusing its refining business on processing crude from oil sands and shales.\n\nSince September 2013 WCS has been processed at Imperial Oil's Sarnia, Ontario, refinery and ExxonMobil Corporation's (XOM) has Joliet plant, Illinois and Baton Rouge, Louisiana.\nBy April 2013, Imperial Oil's Sarnia, Ontario refinery was the only plugged-in coking facility in eastern Canada that could process raw bitumen.\n\nIn July 2014 the Canadian Academy of Engineering identified the Sarnia-Lambton $10-billion oil sands bitumen upgrading project to produce refinery ready crudes, as a high priority national scale project.\n\nLloydminster heavy oil, a component in the Western Canadian Select (WCS) heavy oil blend, is processed at the CCRL Refinery Complex heavy oil upgrader which had a fire in the coker of the heavy oil upgrader section of the plant, on February 11, 2013. It was the third major incident in 16 months, at the Regina plant. The price of Western Canadian Select weakened against U.S. benchmark West Texas Intermediate (WTI) oil.\n\nThe Pine Bend Refinery, the largest oil refinery in Minnesota, located in the Twin Cities gets 80% of its incoming heavy crude from the Athabasca oil sands. The crude oil is piped from the northwest to the facility through the Lakehead and Minnesota pipelines which are also owned by Koch Industries. Most petroleum enters and exits the plant through a Koch-owned, 537-mile pipeline system that stretches across Minnesota and Wisconsin. The U. S. Energy Information Agency (EIA) ranked it at 14th in the country as of 2013 by production. By 2013 its nameplate capacity increased to per day.\n\nRepsol responded to the enforcement in January 2009 of the European Union's reduced sulphur content in automotive petrol and diesel from 50 to 10 parts per million, with heavy investment in upgrading their refineries. They upgrading three of their five refineries in Spain (Cartagena, A Coruña, Bilbao, Puertollano and Tarragona) with cokers that have the capacity to refine Western Canadian Select heavy oil. Many other European refineries closed as margins decreased. Repsol tested the first batches of WCS at its Spanish refineries in May 2014.\n\nIn 2012 Repsol completed its €3.15-billion upgrade and expansion of its Cartagena refinery in Murcia, Spain which included a new coking unit capable of refining heavy crude like WCS.\n\nRepsol's 2013 completed upgrades, which included a new coker unit and highly efficient cogeneration unit at their Petronor refinery at Muskiz near Bilbao, cost over 1 billion euros and represents \"the largest industrial investment in the history of the Basque Country.\" This new coker unit will produce \"higher-demand products such as propane, butane, gasoline and diesel\" and \" eliminate the production of fuel oil.\" The cogeneration unit will reduce CO2 emissions and help achieve Spain's Kyoto protocol targets. The refinery is self-sufficient in electricity and capable of distributing power to the grid.\n\nIn their 2013 article published in Oil & Gas Journal, Auers and Mayes suggest that the \"recent pricing disconnects have created opportunities for astute crude oil blenders and refiners to create their own substitutes for waterborne grades (like Alaska North Slope (ANS)) at highly discounted prices. A \"pseudo\" Alaskan North Slope substitute, for example, could be created with a blend of 55% Bakken and 45% Western Canadian Select at a cost potentially far less than the ANS market price.\" They argue that there are financial opportunities for refineries capable of blending, delivering, and refining \"stranded\" cheaper crude blends, like Western Canadian Select(WCS). In contrast to the light, sweet oil produced \"from emerging shale plays in North Dakota (Bakken) and Texas (Eagle Ford) as well as a resurgence of drilling in older, existing fields, such as the Permian basin\", the oil sands of Alberta is \"overwhelmingly heavy.\"\n\nThe CIBC reported that the oil industry continued to produce massive amounts of oil in spite of a stagnant crude oil market. Oil production from the Bakken formation alone was forecast in 2012 to grow by 600,000 barrels every year through 2016. By 2012, Canadian tight oil and oil sands production was also surging.\n\nBy the end of 2014, as the demand for global oil consumption continued to decline, the remarkably rapid oil output growth in ‘light, tight’ oil production in the North Dakota Bakken, the Permian and Eagle Ford Basins in Texas, while rejuvenating economic growth in \"U.S. refining, petrochemical and associated transportation industries, rail & pipelines\", [it also] \"destabilized international oil markets.\"\nSince 2000, the wider use of oil extraction technologies such as hydraulic fracturing and horizontal drilling, have caused a production boom in the Bakken formation which lies beneath the northwestern part of North Dakota. WSC and Bakken compete for pipelines and railway space. By the end of 2010, oil production rates had reached per day, thereby outstripping the pipeline capacity to ship oil out of the Bakken. This oil competes with WCS for access to transportation by pipeline and rail. Bakken production has also increased in Canada, although to a lesser degree than in the US, since the 2004 discovery of the Viewfield Oil Field in Saskatchewan. The same techniques of horizontal drilling and multi-stage massive hydraulic fracturing are used. In December 2012, 2,357 Bakken wells in Saskatchewan produced a record high of . The Bakken Formation also produces in Manitoba, but the yield is small, averaging less than in 2012.\n\n\"Just over 21% of North Dakota’s total 2013 gross domestic product (GDP) of $49.77 billion comes from natural resources and mining.\"\n\nRoyalty rates in Alberta are based on the price of WTI. That royalty rate is applied to a project's net revenue if the project has reached payout or Gross Revenue if the project has not yet reached payout. A project's revenue is a direct function of the price it is able to sell its crude for. Since WCS is a benchmark for oil sands crudes, revenues in the oil sands are discounted when the price of WCS is discounted. Those price discounts flow through to the royalty payments.\n\nThe Province of Alberta receives a portion of benefits from the development of energy resources in the form of royalties that fund in part programs like health, education and infrastructure.\n\nIn 2006-7, the oil sands royalty revenue was $2.411 billion. In 2007/08, it rose to $2.913 billion and it continued to rise in 2008/09 to $2.973 billion. Following the revised Alberta Royalty Regime, it fell in 2009/10 to $1.008 billion. In that year, Alberta's total resource revenue \"fell below $7 billion...when the world economy was in the grip of recession.\"\n\nIn February 2012, the Province of Alberta \"expected $13.4 billion in revenue from non-renewable resources in 2013-14. By January 2013, the province was anticipating only $7.4 billion. \"30 per cent of Alberta’s approximately $40-billion budget is funded through oil and gas revenues. Bitumen royalties represent about half of that total.\" In 2009/10, royalties from the oil sands amounted to $1.008 billion (Budget 2009 cited in Energy Alberta 2009.\n\nIn order to accelerate development of the oil sands, the federal and provincial governments more closely aligned taxation of the oil sands with other surface mining resulting in \"charging one per cent of a project’s gross revenues until the project’s investment costs are paid in full at which point rates increased to 25 per cent of net revenue. These policy changes and higher oil prices after 2003 had the desired effect of accelerating the development of the oil sands industry. \"A revised Alberta Royalty Regime was implemented on January 1, 2009. through which each oil sands project pays a gross revenue royalty rate of 1% (Oil and Gas Fiscal Regimes 2011:30). Oil and Gas Fiscal Regimes 2011 summarizes the petroleum fiscal regimes for the western provinces and territories. The Oil and Gas Fiscal Regimes described how royalty payments were calculated:\n\nWhen the price of oil per barrel is less than or equal to $55/bbl indexed against West Texas Intermediate (WTI) (Oil and Gas Fiscal Regimes 2011:30)(Indexed to the Canadian dollar price of West Texas Intermediate (WTI) (Oil and Gas Fiscal Regimes 2011:30) to a maximum of 9%). When the price of oil per barrel is less than or equal to $120/ bbl indexed against West Texas Intermediate (WTI) \"payout.\"\n\nPayout refers \"the first time when the developer has recovered all the allowed costs of the project, including a return allowance on those costs equal to the Government of Canada long-term bond rate [\"LTBR\"].\n\nIn order to encourage growth and prosperity and due to the extremely high cost of exploration, research and development, oil sands and mining operations pay no corporate, federal, provincial taxes or government royalties other than personal income taxes as companies often remain in a loss position for tax and royalty purposes for many years. Defining a loss position becomes increasingly complex when vertically-integrated multi-national energy companies are involved. Suncor claims their realized losses were legitimate and that Canada Revenue Agency (CRA) is unfairly claiming \"$1.2-billion\" in taxes which is jeopardizing their operations.\n\n\"Bitumen Valuation Methodology (BVM) is a method to determine for royalty purposes a value for bitumen produced in oil sands projects and either upgraded on-site or sold or transferred to affiliates. The BVM ensures that Alberta receives market value for its bitumen production, taken in cash or bitumen royalty-in-kind, through the royalty formula. Western Canadian Select (WCS), a grade or blend of Alberta bitumens, diluents (a product such as naphtha or condensate which is added to increase the ability of the oil to flow through a pipeline) and conventional heavy oils, developed by Alberta producers and stored and valued at Hardisty, AB was determined to be the best reference crude price in the development of a BVM.\"\n\nIn January 2013, the then Premier of Alberta, Alison Redford, used the term bitumen bubble to explain the impact of a dramatic and unanticipated drop in the amount of taxes and revenue from the oil sands linked to the deep discount price of Western Canadian Select against WTI and Maya crude oil, would result in deep cuts in the 2013 provincial budget. In 2012 oil prices rose and fell all year. Premier Redford described the \"bitumen bubble\" as the differential or \"spread between the different prices and the lower price for Alberta's Western Canadian Select (WCS).\" In 2013 alone, the \"bitumen bubble\" effect will result in about six billion dollars less in provincial revenue.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "277289", "url": "https://en.wikipedia.org/wiki?curid=277289", "title": "Wind power", "text": "Wind power\n\nWind power is the use of air flow through wind turbines to provide the mechanical power to turn electric generators. Wind power, as an alternative to burning fossil fuels, is plentiful, renewable, widely distributed, clean, produces no greenhouse gas emissions during operation, consumes no water, and uses little land. The net effects on the environment are far less problematic than those of nonrenewable power sources.\n\nWind farms consist of many individual wind turbines, which are connected to the electric power transmission network. Onshore wind is an inexpensive source of electric power, competitive with or in many places cheaper than coal or gas plants. Offshore wind is steadier and stronger than on land and offshore farms have less visual impact, but construction and maintenance costs are considerably higher. Small onshore wind farms can feed some energy into the grid or provide electric power to isolated off-grid locations.\n\nWind power gives variable power, which is very consistent from year to year but has significant variation over shorter time scales. \nIt is therefore used in conjunction with other electric power sources to give a reliable supply. \nAs the proportion of wind power in a region increases, a need to upgrade the grid and a lowered ability to supplant conventional production can occur. \nPower-management techniques such as having excess capacity, geographically distributed turbines, dispatchable sources, sufficient hydroelectric power, exporting and importing power to neighboring areas, energy storage, or reducing demand when wind production is low, can in many cases overcome these problems. \nWeather forecasting permits the electric-power network to be readied for the predictable variations in production that occur.\n\nIn 2017, global wind power capacity expanded 10% to 539 GW.. Yearly wind energy production grew 17% reaching 4.4% of worldwide electric power usage, and providing 11.6% of the electricity in the European Union.\nDenmark is the country with the highest penetration of wind power, with 43.4% of its consumed electricity from wind in 2017. \nAt least 83 other countries around the world are using wind power to supply their electric power grids.\n\nWind power has been used as long as humans have put sails into the wind. For more than two millennia wind-powered machines have ground grain and pumped water. Wind power was widely available and not confined to the banks of fast-flowing streams, or later, requiring sources of fuel. Wind-powered pumps drained the polders of the Netherlands, and in arid regions such as the American mid-west or the Australian outback, wind pumps provided water for livestock and steam engines.\n\nThe first windmill used for the production of electric power was built in Scotland in July 1887 by Prof James Blyth of Anderson's College, Glasgow (the precursor of Strathclyde University). Blyth's high, cloth-sailed wind turbine was installed in the garden of his holiday cottage at Marykirk in Kincardineshire and was used to charge accumulators developed by the Frenchman Camille Alphonse Faure, to power the lighting in the cottage, thus making it the first house in the world to have its electric power supplied by wind power. Blyth offered the surplus electric power to the people of Marykirk for lighting the main street, however, they turned down the offer as they thought electric power was \"the work of the devil.\" Although he later built a wind turbine to supply emergency power to the local Lunatic Asylum, Infirmary and Dispensary of Montrose the invention never really caught on as the technology was not considered to be economically viable.\n\nAcross the Atlantic, in Cleveland, Ohio a larger and heavily engineered machine was designed and constructed in the winter of 1887–1888 by Charles F. Brush, this was built by his engineering company at his home and operated from 1886 until 1900. The Brush wind turbine had a rotor in diameter and was mounted on an tower. Although large by today's standards, the machine was only rated at 12 kW. The connected dynamo was used either to charge a bank of batteries or to operate up to 100 incandescent light bulbs, three arc lamps, and various motors in Brush's laboratory.\n\nWith the development of electric power, wind power found new applications in lighting buildings remote from centrally-generated power. Throughout the 20th century parallel paths developed small wind stations suitable for farms or residences, and larger utility-scale wind generators that could be connected to electric power grids for remote use of power. Today wind powered generators operate in every size range between tiny stations for battery charging at isolated residences, up to near-gigawatt sized offshore wind farms that provide electric power to national electrical networks.\n\nWind energy is the kinetic energy of air in motion, also called wind.\nTotal wind energy flowing through an imaginary surface with area \"A\" during the time \"t\" is:\n\nwhere \"ρ\" is the density of air; \"v\" is the wind speed; \"Avt\" is the volume of air passing through \"A\" (which is considered perpendicular to the direction of the wind); \"Avtρ\" is therefore the mass \"m\" passing through \"A\". Note that ½ \"ρv\" is the kinetic energy of the moving air per unit volume.\n\nPower is energy per unit time, so the wind power incident on \"A\" (e.g. equal to the rotor area of a wind turbine) is:\n\nWind power in an open air stream is thus \"proportional\" to the \"third power\" of the wind speed; the available power increases eightfold when the wind speed doubles. Wind turbines for grid electric power therefore need to be especially efficient at greater wind speeds.\n\nWind is the movement of air across the surface of the Earth, affected by areas of high pressure and of low pressure.\nThe global wind kinetic energy averaged approximately 1.50 MJ/m over the period from 1979 to 2010, 1.31 MJ/m in the Northern Hemisphere with 1.70 MJ/m in the Southern Hemisphere. The atmosphere acts as a thermal engine, absorbing heat at higher temperatures, releasing heat at lower temperatures. The process is responsible for production of wind kinetic energy at a rate of 2.46 W/m sustaining thus the circulation of the atmosphere against frictional dissipation.\n\nThrough wind resource assessment it is possible to provide estimates of wind power potential globally, by country or region, or for a specific site. A global assessment of wind power potential is available via the Global Wind Atlas provided by the Technical University of Denmark in partnership with the World Bank. \nUnlike 'static' wind resource atlases which average estimates of wind speed and power density across multiple years, tools such as Renewables.ninja provide time-varying simulations of wind speed and power output from different wind turbine models at an hourly resolution.. More detailed, site specific assessments of wind resource potential can be obtained from specialist commercial providers, and many of the larger wind developers will maintain in-house modeling capabilities.\n\nThe total amount of economically extractable power available from the wind is considerably more than present human power use from all sources. \nAxel Kleidon of the Max Planck Institute in Germany, carried out a \"top down\" calculation on how much wind energy there is, starting with the incoming solar radiation that drives the winds by creating temperature differences in the atmosphere. He concluded that somewhere between 18 TW and 68 TW could be extracted.\n\nCristina Archer and Mark Z. Jacobson presented a \"bottom-up\" estimate, which unlike Kleidon's are based on actual measurements of wind speeds, and found that there is 1700 TW of wind power at an altitude of 100 metres over land and sea. Of this, \"between 72 and 170 TW could be extracted in a practical and cost-competitive manner\". They later estimated 80 TW. However research at Harvard University estimates 1 Watt/m on average and 2–10 MW/km capacity for large scale wind farms, suggesting that these estimates of total global wind resources are too high by a factor of about 4.\n\nThe strength of wind varies, and an average value for a given location does not alone indicate the amount of energy a wind turbine could produce there.\n\nTo assess prospective wind power sites a probability distribution function is often fit to the observed wind speed data. Different locations will have different wind speed distributions. The Weibull model closely mirrors the actual distribution of hourly/ten-minute wind speeds at many locations. The Weibull factor is often close to 2 and therefore a Rayleigh distribution can be used as a less accurate, but simpler model.\n\nA wind farm is a group of wind turbines in the same location used for production of electric power. A large wind farm may consist of several hundred individual wind turbines distributed over an extended area, but the land between the turbines may be used for agricultural or other purposes. For example, Gansu Wind Farm, the largest wind farm in the world, has several thousand turbines. A wind farm may also be located offshore.\n\nAlmost all large wind turbines have the same design — a horizontal axis wind turbine having an upwind rotor with three blades, attached to a nacelle on top of a tall tubular tower.\n\nIn a wind farm, individual turbines are interconnected with a medium voltage (often 34.5 kV), power collection system and communications network. In general, a distance of 7D (7 × Rotor Diameter of the Wind Turbine) is set between each turbine in a fully developed wind farm. At a substation, this medium-voltage electric current is increased in voltage with a transformer for connection to the high voltage electric power transmission system.\n\nInduction generators, which were often used for wind power projects in the 1980s and 1990s, require reactive power for excitation so substations used in wind-power collection systems include substantial capacitor banks for power factor correction. Different types of wind turbine generators behave differently during transmission grid disturbances, so extensive modelling of the dynamic electromechanical characteristics of a new wind farm is required by transmission system operators to ensure predictable stable behaviour during system faults. In particular, induction generators cannot support the system voltage during faults, unlike steam or hydro turbine-driven synchronous generators.\n\nToday these generators aren't used any more in modern turbines. Instead today most turbines use variable speed generators combined with partial- or full-scale power converter between the turbine generator and the collector system, which generally have more desirable properties for grid interconnection and have Low voltage ride through-capabilities. Modern concepts use either doubly fed machines with partial-scale converters or squirrel-cage induction generators or synchronous generators (both permanently and electrically excited) with full scale converters.\n\nTransmission systems operators will supply a wind farm developer with a grid code to specify the requirements for interconnection to the transmission grid. This will include power factor, constancy of frequency and dynamic behaviour of the wind farm turbines during a system fault.\n\nOffshore wind power refers to the construction of wind farms in large bodies of water to generate electric power. These installations can utilize the more frequent and powerful winds that are available in these locations and have less aesthetic impact on the landscape than land based projects. However, the construction and the maintenance costs are considerably higher.\n\nSiemens and Vestas are the leading turbine suppliers for offshore wind power. Ørsted, Vattenfall and E.ON are the leading offshore operators. As of October 2010, 3.16 GW of offshore wind power capacity was operational, mainly in Northern Europe. According to BTM Consult, more than 16 GW of additional capacity will be installed before the end of 2014 and the UK and Germany will become the two leading markets. Offshore wind power capacity is expected to reach a total of 75 GW worldwide by 2020, with significant contributions from China and the US. The UK's investments in offshore wind power have resulted in a rapid decrease of the usage of coal as an energy source between 2012 and 2017, as well as a drop in the usage of natural gas as an energy source in 2017.\n\nIn 2012, 1,662 turbines at 55 offshore wind farms in 10 European countries produced 18 TWh, enough to power almost five million households. As of September 2018 the Walney Extension in the United Kingdom is the largest offshore wind farm in the world at 659 MW.\n\nIn a wind farm, individual turbines are interconnected with a medium voltage (usually 34.5 kV) power collection system and communications network. At a substation, this medium-voltage electric current is increased in voltage with a transformer for connection to the high voltage electric power transmission system.\n\nA transmission line is required to bring the generated power to (often remote) markets. For an off-shore station this may require a submarine cable. Construction of a new high-voltage line may be too costly for the wind resource alone, but wind sites may take advantage of lines installed for conventionally fueled generation.\n\nOne of the biggest current challenges to wind power grid integration in the United States is the necessity of developing new transmission lines to carry power from wind farms, usually in remote lowly populated states in the middle of the country due to availability of wind, to high load locations, usually on the coasts where population density is higher. The current transmission lines in remote locations were not designed for the transport of large amounts of energy. As transmission lines become longer the losses associated with power transmission increase, as modes of losses at lower lengths are exacerbated and new modes of losses are no longer negligible as the length is increased, making it harder to transport large loads over large distances. However, resistance from state and local governments makes it difficult to construct new transmission lines. Multi state power transmission projects are discouraged by states with cheap electric power rates for fear that exporting their cheap power will lead to increased rates. A 2005 energy law gave the Energy Department authority to approve transmission projects states refused to act on, but after an attempt to use this authority, the Senate declared the department was being overly aggressive in doing so. Another problem is that wind companies find out after the fact that the transmission capacity of a new farm is below the generation capacity, largely because federal utility rules to encourage renewable energy installation allow feeder lines to meet only minimum standards. These are important issues that need to be solved, as when the transmission capacity does not meet the generation capacity, wind farms are forced to produce below their full potential or stop running all together, in a process known as curtailment. While this leads to potential renewable generation left untapped, it prevents possible grid overload or risk to reliable service.\n\nAs of 2015, there are over 200,000 wind turbines operating, with a total nameplate capacity of 432 GW worldwide. \nThe European Union passed 100 GW nameplate capacity in September 2012, while the United States surpassed 75 GW in 2015 and China's grid connected capacity passed 145 GW in 2015.\nIn 2015 wind power constituted 15.6% of all installed power generation capacity in the European Union and it generated around 11.4% of its power.\n\nWorld wind generation capacity more than quadrupled between 2000 and 2006, doubling about every 3 years. \nThe United States pioneered wind farms and led the world in installed capacity in the 1980s and into the 1990s. \nIn 1997 installed capacity in Germany surpassed the United States and led until once again overtaken by the United States in 2008. \nChina has been rapidly expanding its wind installations in the late 2000s and passed the United States in 2010 to become the world leader. \nAs of 2011, 83 countries around the world were using wind power on a commercial basis.\n\nThe actual amount of electric power that wind is able to generate is calculated by multiplying the nameplate capacity by the capacity factor, which varies according to equipment and location. \nEstimates of the capacity factors for wind installations are in the range of 35% to 44%.\n\nThe wind power industry set new records in 2014 – more than 50 GW of new capacity was installed. Another record breaking year occurred in 2015, with 22% annual market growth resulting in the 60 GW mark being passed. In 2015, close to half of all new wind power was added outside of the traditional markets in Europe and North America. This was largely from new construction in China and India. Global Wind Energy Council (GWEC) figures show that 2015 recorded an increase of installed capacity of more than 63 GW, taking the total installed wind energy capacity to 432.9 GW, up from 74 GW in 2006. In terms of economic value, the wind energy sector has become one of the important players in the energy markets, with the total investments reaching bn (bn), an increase of 4% over 2014.\n\nAlthough the wind power industry was affected by the global financial crisis in 2009 and 2010, GWEC predicts that the installed capacity of wind power will be 792.1 GW by the end of 2020 and 4,042 GW by end of 2050. The increased commissioning of wind power is being accompanied by record low prices for forthcoming renewable electric power. In some cases, wind onshore is already the cheapest electric power generation option and costs are continuing to decline. The contracted prices for wind onshore for the next few years are now as low as 30 USD/MWh.\n\nIn the EU in 2015, 44% of all new generating capacity was wind power; while in the same period net fossil fuel power capacity decreased.\n\nSince wind speed is not constant, a wind farm's annual energy production is never as much as the sum of the generator nameplate ratings multiplied by the total hours in a year. The ratio of actual productivity in a year to this theoretical maximum is called the capacity factor. Typical capacity factors are 15–50%; values at the upper end of the range are achieved in favourable sites and are due to wind turbine design improvements.\n\nOnline data is available for some locations, and the capacity factor can be calculated from the yearly output. For example, the German nationwide average wind power capacity factor over all of 2012 was just under 17.5% (45,867 GW·h/yr / (29.9 GW × 24 × 366) = 0.1746), and the capacity factor for Scottish wind farms averaged 24% between 2008 and 2010.\n\nUnlike fueled generating plants, the capacity factor is affected by several parameters, including the variability of the wind at the site and the size of the generator relative to the turbine's swept area. A small generator would be cheaper and achieve a higher capacity factor but would produce less electric power (and thus less profit) in high winds. Conversely, a large generator would cost more but generate little extra power and, depending on the type, may stall out at low wind speed. Thus an optimum capacity factor of around 40–50% would be aimed for.\n\nA 2008 study released by the U.S. Department of Energy noted that the capacity factor of new wind installations was increasing as the technology improves, and projected further improvements for future capacity factors. In 2010, the department estimated the capacity factor of new wind turbines in 2010 to be 45%. The annual average capacity factor for wind generation in the US has varied between 29.8% and 34% during the period 2010–2015.\n\nWind energy penetration is the fraction of energy produced by wind compared with the total generation. The wind power penetration in world electric power generation in 2015 was 3.5%.\n\nThere is no generally accepted maximum level of wind penetration. The limit for a particular grid will depend on the existing generating plants, pricing mechanisms, capacity for energy storage, demand management and other factors. An interconnected electric power grid will already include reserve generating and transmission capacity to allow for equipment failures. This reserve capacity can also serve to compensate for the varying power generation produced by wind stations. Studies have indicated that 20% of the total annual electrical energy consumption may be incorporated with minimal difficulty. These studies have been for locations with geographically dispersed wind farms, some degree of dispatchable energy or hydropower with storage capacity, demand management, and interconnected to a large grid area enabling the export of electric power when needed. Beyond the 20% level, there are few technical limits, but the economic implications become more significant. Electrical utilities continue to study the effects of large scale penetration of wind generation on system stability and economics.\n\nA wind energy penetration figure can be specified for different duration of time, but is often quoted annually. To obtain 100% from wind annually requires substantial long term storage or substantial interconnection to other systems which may already have substantial storage. On a monthly, weekly, daily, or hourly basis—or less—wind might supply as much as or more than 100% of current use, with the rest stored or exported. Seasonal industry might then take advantage of high wind and low usage times such as at night when wind output can exceed normal demand. Such industry might include production of silicon, aluminum, steel, or of natural gas, and hydrogen, and using future long term storage to facilitate 100% energy from variable renewable energy. Homes can also be programmed to accept extra electric power on demand, for example by remotely turning up water heater thermostats.\n\nIn Australia, the state of South Australia generates around half of the nation's wind power capacity. By the end of 2011 wind power in South Australia, championed by Premier (and Climate Change Minister) Mike Rann, reached 26% of the State's electric power generation, edging out coal for the first time. At this stage South Australia, with only 7.2% of Australia's population, had 54% of Australia's installed capacity.\n\nElectric power generated from wind power can be highly variable at several different timescales: hourly, daily, or seasonally. Annual variation also exists, but is not as significant. Because instantaneous electrical generation and consumption must remain in balance to maintain grid stability, this variability can present substantial challenges to incorporating large amounts of wind power into a grid system. Intermittency and the non-dispatchable nature of wind energy production can raise costs for regulation, incremental operating reserve, and (at high penetration levels) could require an increase in the already existing energy demand management, load shedding, storage solutions or system interconnection with HVDC cables.\n\nFluctuations in load and allowance for failure of large fossil-fuel generating units requires operating reserve capacity, which can be increased to compensate for variability of wind generation.\n\nWind power is variable, and during low wind periods it must be replaced by other power sources. Transmission networks presently cope with outages of other generation plants and daily changes in electrical demand, but the variability of intermittent power sources such as wind power, is more frequent than those of conventional power generation plants which, when scheduled to be operating, may be able to deliver their nameplate capacity around 95% of the time.\n\nPresently, grid systems with large wind penetration require a small increase in the frequency of usage of natural gas spinning reserve power plants to prevent a loss of electric power in the event that there is no wind. At low wind power penetration, this is less of an issue.\n\nGE has installed a prototype wind turbine with onboard battery similar to that of an electric car, equivalent of 1 minute of production. Despite the small capacity, it is enough to guarantee that power output complies with forecast for 15 minutes, as the battery is used to eliminate the difference rather than provide full output. In certain cases the increased predictability can be used to take wind power penetration from 20 to 30 or 40 per cent. The battery cost can be retrieved by selling burst power on demand and reducing backup needs from gas plants.\n\nIn the UK there were 124 separate occasions from 2008 to 2010 when the nation's wind output fell to less than 2% of installed capacity. A report on Denmark's wind power noted that their wind power network provided less than 1% of average demand on 54 days during the year 2002. Wind power advocates argue that these periods of low wind can be dealt with by simply restarting existing power stations that have been held in readiness, or interlinking with HVDC. Electrical grids with slow-responding thermal power plants and without ties to networks with hydroelectric generation may have to limit the use of wind power. According to a 2007 Stanford University study published in the \"Journal of Applied Meteorology and Climatology\", interconnecting ten or more wind farms can allow an average of 33% of the total energy produced (i.e. about 8% of total nameplate capacity) to be used as reliable, baseload electric power which can be relied on to handle peak loads, as long as minimum criteria are met for wind speed and turbine height.\n\nConversely, on particularly windy days, even with penetration levels of 16%, wind power generation can surpass all other electric power sources in a country. In Spain, in the early hours of 16 April 2012 wind power production reached the highest percentage of electric power production till then, at 60.46% of the total demand. In Denmark, which had power market penetration of 30% in 2013, over 90 hours, wind power generated 100% of the country's power, peaking at 122% of the country's demand at 2 am on 28 October.\n\nA 2006 International Energy Agency forum presented costs for managing intermittency as a function of wind-energy's share of total capacity for several countries, as shown in the table on the right. Three reports on the wind variability in the UK issued in 2009, generally agree that variability of wind needs to be taken into account by adding 20% to the operating reserve, but it does not make the grid unmanageable. The additional costs, which are modest, can be quantified.\n\nThe combination of diversifying variable renewables by type and location, forecasting their variation, and integrating them with dispatchable renewables, flexible fueled generators, and demand response can create a power system that has the potential to meet power supply needs reliably. Integrating ever-higher levels of renewables is being successfully demonstrated in the real world:\n\nSolar power tends to be complementary to wind. On daily to weekly timescales, high pressure areas tend to bring clear skies and low surface winds, whereas low pressure areas tend to be windier and cloudier. On seasonal timescales, solar energy peaks in summer, whereas in many areas wind energy is lower in summer and higher in winter. Thus the seasonal variation of wind and solar power tend to cancel each other somewhat. In 2007 the Institute for Solar Energy Supply Technology of the University of Kassel pilot-tested a combined power plant linking solar, wind, biogas and hydrostorage to provide load-following power around the clock and throughout the year, entirely from renewable sources.\n\nWind power forecasting methods are used, but predictability of any particular wind farm is low for short-term operation. For any particular generator there is an 80% chance that wind output will change less than 10% in an hour and a 40% chance that it will change 10% or more in 5 hours.\n\nHowever, studies by Graham Sinden (2009) suggest that, in practice, the variations in thousands of wind turbines, spread out over several different sites and wind regimes, are smoothed. As the distance between sites increases, the correlation between wind speeds measured at those sites, decreases.\n\nThus, while the output from a single turbine can vary greatly and rapidly as local wind speeds vary, as more turbines are connected over larger and larger areas the average power output becomes less variable and more predictable.\n\nWind power hardly ever suffers major technical failures, since failures of individual wind turbines have hardly any effect on overall power, so that the distributed wind power is reliable and predictable, whereas conventional generators, while far less variable, can suffer major unpredictable outages.\n\nTypically, conventional hydroelectricity complements wind power very well. When the wind is blowing strongly, nearby hydroelectric stations can temporarily hold back their water. When the wind drops they can, provided they have the generation capacity, rapidly increase production to compensate. This gives a very even overall power supply and virtually no loss of energy and uses no more water.\n\nAlternatively, where a suitable head of water is not available, pumped-storage hydroelectricity or other forms of grid energy storage such as compressed air energy storage and thermal energy storage can store energy developed by high-wind periods and release it when needed. The type of storage needed depends on the wind penetration level – low penetration requires daily storage, and high penetration requires both short and long term storage – as long as a month or more. Stored energy increases the economic value of wind energy since it can be shifted to displace higher cost generation during peak demand periods. The potential revenue from this arbitrage can offset the cost and losses of storage. For example, in the UK, the 1.7 GW Dinorwig pumped-storage plant evens out electrical demand peaks, and allows base-load suppliers to run their plants more efficiently. Although pumped-storage power systems are only about 75% efficient, and have high installation costs, their low running costs and ability to reduce the required electrical base-load can save both fuel and total electrical generation costs.\n\nIn particular geographic regions, peak wind speeds may not coincide with peak demand for electrical power. In the U.S. states of California and Texas, for example, hot days in summer may have low wind speed and high electrical demand due to the use of air conditioning. Some utilities subsidize the purchase of geothermal heat pumps by their customers, to reduce electric power demand during the summer months by making air conditioning up to 70% more efficient; widespread adoption of this technology would better match electric power demand to wind availability in areas with hot summers and low summer winds. A possible future option may be to interconnect widely dispersed geographic areas with an HVDC \"super grid\". In the U.S. it is estimated that to upgrade the transmission system to take in planned or potential renewables would cost at least USD 60 bn, while the society value of added windpower would be more than that cost.\n\nGermany has an installed capacity of wind and solar that can exceed daily demand, and has been exporting peak power to neighboring countries, with exports which amounted to some 14.7 billion kWh in 2012. A more practical solution is the installation of thirty days storage capacity able to supply 80% of demand, which will become necessary when most of Europe's energy is obtained from wind power and solar power. Just as the EU requires member countries to maintain 90 days strategic reserves of oil it can be expected that countries will provide electric power storage, instead of expecting to use their neighbors for net metering.\n\nThe capacity credit of wind is estimated by determining the capacity of conventional plants displaced by wind power, whilst maintaining the same degree of system security. According to the American Wind Energy Association, production of wind power in the United States in 2015 avoided consumption of 73 billion gallons of water and reduced emissions by 132 million metric tons, while providing USD 7.3 bn in public health savings.\n\nThe energy needed to build a wind farm divided into the total output over its life, Energy Return on Energy Invested, of wind power varies but averages about 20–25. Thus, the energy payback time is typically around a year.\n\nWind turbines reached grid parity (the point at which the cost of wind power matches traditional sources) in some areas of Europe in the mid-2000s, and in the US around the same time. Falling prices continue to drive the levelized cost down and it has been suggested that it has reached general grid parity in Europe in 2010, and will reach the same point in the US around 2016 due to an expected reduction in capital costs of about 12%.\n\nWind power is capital intensive, but has no fuel costs. The price of wind power is therefore much more stable than the volatile prices of fossil fuel sources. The marginal cost of wind energy once a station is constructed is usually less than 1-cent per kW·h.\n\nHowever, the estimated average cost per unit of electric power must incorporate the cost of construction of the turbine and transmission facilities, borrowed funds, return to investors (including cost of risk), estimated annual production, and other components, averaged over the projected useful life of the equipment, which may be in excess of twenty years. Energy cost estimates are highly dependent on these assumptions so published cost figures can differ substantially. In 2004, wind energy cost a fifth of what it did in the 1980s, and some expected that downward trend to continue as larger multi-megawatt turbines were mass-produced. In 2012 capital costs for wind turbines were substantially lower than 2008–2010 but still above 2002 levels. \nA 2011 report from the American Wind Energy Association stated, \"Wind's costs have dropped over the past two years, in the range of 5 to 6 cents per kilowatt-hour recently... about 2 cents cheaper than coal-fired electric power, and more projects were financed through debt arrangements than tax equity structures last year... winning more mainstream acceptance from Wall Street's banks... Equipment makers can also deliver products in the same year that they are ordered instead of waiting up to three years as was the case in previous cycles... 5,600 MW of new installed capacity is under construction in the United States, more than double the number at this point in 2010. Thirty-five percent of all new power generation built in the United States since 2005 has come from wind, more than new gas and coal plants combined, as power providers are increasingly enticed to wind as a convenient hedge against unpredictable commodity price moves.\"\n\nA British Wind Energy Association report gives an average generation cost of onshore wind power of around 3.2 pence (between US 5 and 6 cents) per kW·h (2005). Cost per unit of energy produced was estimated in 2006 to be 5 to 6 percent above the cost of new generating capacity in the US for coal and natural gas: wind cost was estimated at $55.80 per MW·h, coal at $53.10/MW·h and natural gas at $52.50. Similar comparative results with natural gas were obtained in a governmental study in the UK in 2011. In 2011 power from wind turbines could be already cheaper than fossil or nuclear plants; it is also expected that wind power will be the cheapest form of energy generation in the future. The presence of wind energy, even when subsidised, can reduce costs for consumers (€5 billion/yr in Germany) by reducing the marginal price, by minimising the use of expensive peaking power plants.\n\nA 2012 EU study shows base cost of onshore wind power similar to coal, when subsidies and externalities are disregarded. Wind power has some of the lowest external costs.\n\nIn February 2013 Bloomberg New Energy Finance (BNEF) reported that the cost of generating electric power from new wind farms is cheaper than new coal or new baseload gas plants. When including the current Australian federal government carbon pricing scheme their modeling gives costs (in Australian dollars) of $80/MWh for new wind farms, $143/MWh for new coal plants and $116/MWh for new baseload gas plants. The modeling also shows that \"even without a carbon price (the most efficient way to reduce economy-wide emissions) wind energy is 14% cheaper than new coal and 18% cheaper than new gas.\"<ref name=\"bnef.com/2013/02/07/renewable-cheaper\"></ref> \nPart of the higher costs for new coal plants is due to high financial lending costs because of \"the reputational damage of emissions-intensive investments\". The expense of gas fired plants is partly due to \"export market\" effects on local prices. Costs of production from coal fired plants built in \"the 1970s and 1980s\" are cheaper than renewable energy sources because of depreciation. In 2015 BNEF calculated LCOE prices per MWh energy in new powerplants (excluding carbon costs) :\n$85 for onshore wind ($175 for offshore), $66–75 for coal in the Americas ($82–105 in Europe), gas $80–100. A 2014 study showed unsubsidized LCOE costs between $37–81, depending on region. A 2014 US DOE report showed that in some cases power purchase agreement prices for wind power had dropped to record lows of $23.5/MWh.\n\nThe cost has reduced as wind turbine technology has improved. There are now longer and lighter wind turbine blades, improvements in turbine performance and increased power generation efficiency. Also, wind project capital and maintenance costs have continued to decline.\nFor example, the wind industry in the USA in early 2014 were able to produce more power at lower cost by using taller wind turbines with longer blades, capturing the faster winds at higher elevations. This has opened up new opportunities and in Indiana, Michigan, and Ohio, the price of power from wind turbines built 300 feet to 400 feet above the ground can now compete with conventional fossil fuels like coal. Prices have fallen to about 4 cents per kilowatt-hour in some cases and utilities have been increasing the amount of wind energy in their portfolio, saying it is their cheapest option.\n\nA number of initiatives are working to reduce costs of electric power from offshore wind. One example is the Carbon Trust Offshore Wind Accelerator, a joint industry project, involving nine offshore wind developers, which aims to reduce the cost of offshore wind by 10% by 2015. It has been suggested that innovation at scale could deliver 25% cost reduction in offshore wind by 2020. Henrik Stiesdal, former Chief Technical Officer at Siemens Wind Power, has stated that by 2025 energy from offshore wind will be one of the cheapest, scalable solutions in the UK, compared to other renewables and fossil fuel energy sources, if the true cost to society is factored into the cost of energy equation. He calculates the cost at that time to be 43 EUR/MWh for onshore, and 72 EUR/MWh for offshore wind.\n\nIn August 2017, the Department of Energy's National Renewable Energy Laboratory (NREL) published a new report on a 50% reduction in wind power cost by 2030. The NREL is expected to achieve advances in wind turbine design, materials and controls to unlock performance improvements and reduce costs. According to international surveyors, this study shows that cost cutting is projected to fluctuate between 24% and 30% by 2030. In more aggressive cases, experts estimate cost reduction Up to 40 percent if the research and development and technology programs result in additional efficiency.\n\nThe wind industry in the United States generates tens of thousands of jobs and billions of dollars of economic activity. Wind projects provide local taxes, or payments in lieu of taxes and strengthen the economy of rural communities by providing income to farmers with wind turbines on their land. \nWind energy in many jurisdictions receives financial or other support to encourage its development. Wind energy benefits from subsidies in many jurisdictions, either to increase its attractiveness, or to compensate for subsidies received by other forms of production which have significant negative externalities.\n\nIn the US, wind power receives a production tax credit (PTC) of 1.5¢/kWh in 1993 dollars for each kW·h produced, for the first ten years; at 2.2 cents per kW·h in 2012, the credit was renewed on 2 January 2012, to include construction begun in 2013. \nA 30% tax credit can be applied instead of receiving the PTC. \nAnother tax benefit is accelerated depreciation. Many American states also provide incentives, such as exemption from property tax, mandated purchases, and additional markets for \"green credits\". The Energy Improvement and Extension Act of 2008 contains extensions of credits for wind, including microturbines. Countries such as Canada and Germany also provide incentives for wind turbine construction, such as tax credits or minimum purchase prices for wind generation, with assured grid access (sometimes referred to as feed-in tariffs). These feed-in tariffs are typically set well above average electric power prices. \nIn December 2013 U.S. Senator Lamar Alexander and other Republican senators argued that the \"wind energy production tax credit should be allowed to expire at the end of 2013\" and it expired 1 January 2014 for new installations.\n\nSecondary market forces also provide incentives for businesses to use wind-generated power, even if there is a premium price for the electricity. For example, socially responsible manufacturers pay utility companies a premium that goes to subsidize and build new wind power infrastructure. Companies use wind-generated power, and in return they can claim that they are undertaking strong \"green\" efforts. In the US the organization Green-e monitors business compliance with these renewable energy credits.\nTurbine prices have fallen significantly in recent years due to tougher competitive conditions such as the increased use of energy auctions, and the elimination of subsidies in many markets. For example, Vestas, a wind turbine manufacturer, whose largest onshore turbine can pump out 4.2 megawatts of power, enough to provide electricity to roughly 5,000 homes, has seen prices for its turbines fall from €950,000 per megawatt in late 2016, to around €800,000 per megawatt in the third quarter of 2017.\n\nSmall-scale wind power is the name given to wind generation systems with the capacity to produce up to 50 kW of electrical power. Isolated communities, that may otherwise rely on diesel generators, may use wind turbines as an alternative. Individuals may purchase these systems to reduce or eliminate their dependence on grid electric power for economic reasons, or to reduce their carbon footprint. Wind turbines have been used for household electric power generation in conjunction with battery storage over many decades in remote areas.\n\nRecent examples of small-scale wind power projects in an urban setting can be found in New York City, where, since 2009, a number of building projects have capped their roofs with Gorlov-type helical wind turbines. Although the energy they generate is small compared to the buildings' overall consumption, they help to reinforce the building's 'green' credentials in ways that \"showing people your high-tech boiler\" cannot, with some of the projects also receiving the direct support of the New York State Energy Research and Development Authority.\n\nGrid-connected domestic wind turbines may use grid energy storage, thus replacing purchased electric power with locally produced power when available. The surplus power produced by domestic microgenerators can, in some jurisdictions, be fed into the network and sold to the utility company, producing a retail credit for the microgenerators' owners to offset their energy costs.\n\nOff-grid system users can either adapt to intermittent power or use batteries, photovoltaic or diesel systems to supplement the wind turbine. Equipment such as parking meters, traffic warning signs, street lighting, or wireless Internet gateways may be powered by a small wind turbine, possibly combined with a photovoltaic system, that charges a small battery replacing the need for a connection to the power grid.\n\nA Carbon Trust study into the potential of small-scale wind energy in the UK, published in 2010, found that small wind turbines could provide up to 1.5 terawatt hours (TW·h) per year of electric power (0.4% of total UK electric power consumption), saving 0.6 million tonnes of carbon dioxide (Mt CO) emission savings. This is based on the assumption that 10% of households would install turbines at costs competitive with grid electric power, around 12 pence (US 19 cents) a kW·h. A report prepared for the UK's government-sponsored Energy Saving Trust in 2006, found that home power generators of various kinds could provide 30 to 40% of the country's electric power needs by 2050.\n\nDistributed generation from renewable resources is increasing as a consequence of the increased awareness of climate change. The electronic interfaces required to connect renewable generation units with the utility system can include additional functions, such as the active filtering to enhance the power quality.\n\nThe environmental impact of wind power when compared to the environmental impacts of fossil fuels, is relatively minor. According to the IPCC, in assessments of the life-cycle global warming potential of energy sources, wind turbines have a median value of between 12 and 11 (geq/kWh) depending on whether off- or onshore turbines are being assessed. Compared with other low carbon power sources, wind turbines have some of the lowest global warming potential per unit of electrical energy generated.\n\nWhile a wind farm may cover a large area of land, many land uses such as agriculture are compatible with it, as only small areas of turbine foundations and infrastructure are made unavailable for use.\n\nThere are reports of bird and bat mortality at wind turbines as there are around other artificial structures. The scale of the ecological impact may or may not be significant, depending on specific circumstances. Prevention and mitigation of wildlife fatalities, and protection of peat bogs, affect the siting and operation of wind turbines.\n\nWind turbines generate some noise. At a residential distance of this may be around 45 dB, which is slightly louder than a refrigerator. At distance they become inaudible.\nThere are anecdotal reports of negative health effects from noise on people who live very close to wind turbines. \nPeer-reviewed research has generally not supported these claims.\n\nThe United States Air Force and Navy have expressed concern that siting large wind turbines near bases \"will negatively impact radar to the point that air traffic controllers will lose the location of aircraft.\"\n\nAesthetic aspects of wind turbines and resulting changes of the visual landscape are significant. Conflicts arise especially in scenic and heritage protected landscapes.\n\nNuclear power and fossil fuels are subsidized by many governments, and wind power and other forms of renewable energy are also often subsidized. For example, a 2009 study by the Environmental Law Institute assessed the size and structure of U.S. energy subsidies over the 2002–2008 period. The study estimated that subsidies to fossil-fuel based sources amounted to approximately $72 billion over this period and subsidies to renewable fuel sources totalled $29 billion. In the United States, the federal government has paid US$74 billion for energy subsidies to support R&D for nuclear power ($50 billion) and fossil fuels ($24 billion) from 1973 to 2003. During this same time frame, renewable energy technologies and energy efficiency received a total of US$26 billion. It has been suggested that a subsidy shift would help to level the playing field and support growing energy sectors, namely solar power, wind power, and biofuels. History shows that no energy sector was developed without subsidies.\n\nAccording to the International Energy Agency (IEA) (2011), energy subsidies artificially lower the price of energy paid by consumers, raise the price received by producers or lower the cost of production. \"Fossil fuels subsidies costs generally outweigh the benefits. Subsidies to renewables and low-carbon energy technologies can bring long-term economic and environmental benefits\". \nIn November 2011, an IEA report entitled \"Deploying Renewables 2011\" said \"subsidies in green energy technologies that were not yet competitive are justified in order to give an incentive to investing into technologies with clear environmental and energy security benefits\". The IEA's report disagreed with claims that renewable energy technologies are only viable through costly subsidies and not able to produce energy reliably to meet demand.\n\nHowever, IEA's views are not universally accepted. Between 2010 and 2016, subsidies for wind were between 1.3¢ and 5.7¢ per kWh. Subsidies for coal, natural gas and nuclear are all between 0.05¢ and 0.2¢ per kWh over all years. On a per-kWh basis, wind is subsidized 50 times as much as traditional sources.\n\nIn the United States, the wind power industry has recently increased its lobbying efforts considerably, spending about $5 million in 2009 after years of relative obscurity in Washington. By comparison, the U.S. nuclear industry alone spent over $650 million on its lobbying efforts and campaign contributions during a ten-year period ending in 2008.\n\nFollowing the 2011 Japanese nuclear accidents, Germany's federal government is working on a new plan for increasing energy efficiency and renewable energy commercialization, with a particular focus on offshore wind farms. Under the plan, large wind turbines will be erected far away from the coastlines, where the wind blows more consistently than it does on land, and where the enormous turbines won't bother the inhabitants. The plan aims to decrease Germany's dependence on energy derived from coal and nuclear power plants.\n\nSurveys of public attitudes across Europe and in many other countries show strong public support for wind power.\nAbout 80% of EU citizens support wind power.\nIn Germany, where wind power has gained very high social acceptance, hundreds of thousands of people have invested in citizens' wind farms across the country and thousands of small and medium-sized enterprises are running successful businesses in a new sector that in 2008 employed 90,000 people and generated 8% of Germany's electric power.\n\nBakker et al. (2012) discovered in their study that when residents did not want the turbines located by them their annoyance was significantly higher than those \"that benefited economically from wind turbines the proportion of people who were rather or very annoyed was significantly lower\".\n\nAlthough wind power is a popular form of energy generation, the construction of wind farms is not universally welcomed, often for aesthetic reasons.\n\nIn Spain, with some exceptions, there has been little opposition to the installation of inland wind parks. However, the projects to build offshore parks have been more controversial.\nIn particular, the proposal of building the biggest offshore wind power production facility in the world in southwestern Spain in the coast of Cádiz, on the spot of the 1805 Battle of Trafalgar has been met with strong opposition who fear for tourism and fisheries in the area, and because the area is a war grave.\n\nIn a survey conducted by Angus Reid Strategies in October 2007, 89 per cent of respondents said that using renewable energy sources like wind or solar power was positive for Canada, because these sources were better for the environment. Only 4 per cent considered using renewable sources as negative since they can be unreliable and expensive.\nAccording to a Saint Consulting survey in April 2007, wind power was the alternative energy source most likely to gain public support for future development in Canada, with only 16% opposed to this type of energy. By contrast, 3 out of 4 Canadians opposed nuclear power developments.\n\nA 2003 survey of residents living around Scotland's 10 existing wind farms found high levels of community acceptance and strong support for wind power, with much support from those who lived closest to the wind farms. The results of this survey support those of an earlier Scottish Executive survey 'Public attitudes to the Environment in Scotland 2002', which found that the Scottish public would prefer the majority of their electric power to come from renewables, and which rated wind power as the cleanest source of renewable energy. \nA survey conducted in 2005 showed that 74% of people in Scotland agree that wind farms are necessary to meet current and future energy needs. When people were asked the same question in a Scottish renewables study conducted in 2010, 78% agreed. The increase is significant as there were twice as many wind farms in 2010 as there were in 2005. The 2010 survey also showed that 52% disagreed with the statement that wind farms are \"ugly and a blot on the landscape\". 59% agreed that wind farms were necessary and that how they looked was unimportant. \nRegarding tourism, query responders consider power pylons, cell phone towers, quarries and plantations more negatively than wind farms. Scotland is planning to obtain 100% of electric power from renewable sources by 2020.\n\nIn other cases there is direct community ownership of wind farm projects. The hundreds of thousands of people who have become involved in Germany's small and medium-sized wind farms demonstrate such support there.\n\nA 2010 Harris Poll reflects the strong support for wind power in Germany, other European countries, and the United States.\n\nMany wind power companies work with local communities to reduce environmental and other concerns associated with particular wind farms. \nIn other cases there is direct community ownership of wind farm projects. Appropriate government consultation, planning and approval procedures also help to minimize environmental risks. \nSome may still object to wind farms but, according to The Australia Institute, their concerns should be weighed against the need to address the threats posed by climate change and the opinions of the broader community.\n\nIn America, wind projects are reported to boost local tax bases, helping to pay for schools, roads and hospitals. Wind projects also revitalize the economy of rural communities by providing steady income to farmers and other landowners.\n\nIn the UK, both the National Trust and the Campaign to Protect Rural England have expressed concerns about the effects on the rural landscape caused by inappropriately sited wind turbines and wind farms.\nSome wind farms have become tourist attractions. The Whitelee Wind Farm Visitor Centre has an exhibition room, a learning hub, a café with a viewing deck and also a shop. It is run by the Glasgow Science Centre.\n\nIn Denmark, a loss-of-value scheme gives people the right to claim compensation for loss of value of their property if it is caused by proximity to a wind turbine. The loss must be at least 1% of the property's value.\n\nDespite this general support for the concept of wind power in the public at large, local opposition often exists and has delayed or aborted a number of projects. \nFor example, there are concerns that some installations can negatively affect TV and radio reception and Doppler weather radar, as well as produce excessive sound and vibration levels leading to a decrease in property values. Potential broadcast-reception solutions include predictive interference modeling as a component of site selection. \nA study of 50,000 home sales near wind turbines found no statistical evidence that prices were affected.\n\nWhile aesthetic issues are subjective and some find wind farms pleasant and optimistic, or symbols of energy independence and local prosperity, protest groups are often formed to attempt to block new wind power sites for various reasons.\n\nThis type of opposition is often described as NIMBYism, but research carried out in 2009 found that there is little evidence to support the belief that residents only object to renewable power facilities such as wind turbines as a result of a \"Not in my Back Yard\" attitude.\n\nWind turbines are devices that convert the wind's kinetic energy into electrical power. The result of over a millennium of windmill development and modern engineering, today's wind turbines are manufactured in a wide range of horizontal axis and vertical axis types. The smallest turbines are used for applications such as battery charging for auxiliary power. Slightly larger turbines can be used for making small contributions to a domestic power supply while selling unused power back to the utility supplier via the electrical grid. Arrays of large turbines, known as wind farms, have become an increasingly important source of renewable energy and are used in many countries as part of a strategy to reduce their reliance on fossil fuels.\n\nWind turbine design is the process of defining the form and specifications of a wind turbine to extract energy from the wind.\nA wind turbine installation consists of the necessary systems needed to capture the wind's energy, point the turbine into the wind, convert mechanical rotation into electrical power, and other systems to start, stop, and control the turbine.\n\nIn 1919 the German physicist Albert Betz showed that for a hypothetical ideal wind-energy extraction machine, the fundamental laws of conservation of mass and energy allowed no more than 16/27 (59.3%) of the kinetic energy of the wind to be captured. This Betz limit can be approached in modern turbine designs, which may reach 70 to 80% of the theoretical Betz limit.\n\nThe aerodynamics of a wind turbine are not straightforward. The air flow at the blades is not the same as the airflow far away from the turbine. The very nature of the way in which energy is extracted from the air also causes air to be deflected by the turbine. In addition the aerodynamics of a wind turbine at the rotor surface exhibit phenomena that are rarely seen in other aerodynamic fields. The shape and dimensions of the blades of the wind turbine are determined by the aerodynamic performance required to efficiently extract energy from the wind, and by the strength required to resist the forces on the blade.\n\nIn addition to the aerodynamic design of the blades, the design of a complete wind power system must also address the design of the installation's rotor hub, nacelle, tower structure, generator, controls, and foundation.\nTurbine design makes extensive use of computer modelling and simulation tools. These are becoming increasingly sophisticated as highlighted by a recent state-of-the-art review by Hewitt et al. \nFurther design factors must also be considered when integrating wind turbines into electrical power grids.\n\n"}
{"id": "40875734", "url": "https://en.wikipedia.org/wiki?curid=40875734", "title": "Wolf Ridge Environmental Learning Center", "text": "Wolf Ridge Environmental Learning Center\n\nWolf Ridge Environmental Learning Center is an accredited learning center located in Finland, Minnesota. It is a learning center that focuses on k-12 environmental education.\n\nIn 1969, Jack Pichotta created a program for students in a high school in Cloquet, Minnesota. The program replaces a week of normal classes and instead teaches about environmental topics. These environmental topics are taught by over 100 environmental specialists. Pichotta joined a group of concerned educators to discuss environmental education in Minnesota. Pichotta and his team decided to use a closed down U.S. Forest Service camp as a school to teach about the environment. The camp was named Isabella Environmental Learning center after the camp's previous name, Isabella Job Corps Camp.\n\nIn 1974 Pichotta and his team decided that the program would need a new permanent location. Between 1975 and 1985 the program moved to Finland, Minnesota and eventually evolved into Wolf Ridge.\n\nWolf Ridge's main campus was built in the Sawtooth Mountains on a precipice overlooking Lake Superior. The campus contains about 2000 acres of mixed conifer-hardwood forest. Wolf Ridge has 2 small lakes- Raven Lake and Wolf Lake, as well as several streams. The Baptism River also flows through the property. Hills on campus include the tallest, Mystical Mountain, which stands at ~1500 feet and Marshal Mountain, which stands at 1405 feet. The main campus consists of a Dining Hall, an Energy Center, an Education Building, and a Science Center, as well as both the East Dorm and the West Dorm.\n\n"}
