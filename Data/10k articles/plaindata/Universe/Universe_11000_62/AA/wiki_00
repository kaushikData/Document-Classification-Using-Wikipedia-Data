{"id": "6701370", "url": "https://en.wikipedia.org/wiki?curid=6701370", "title": "A Nature Conservation Review", "text": "A Nature Conservation Review\n\nA Nature Conservation Review is a 2-volume work by Derek Ratcliffe, published by Cambridge University Press in 1977. It set out to identify the most important places for nature conservation in Great Britain. It is often known by the acronym NCR, and sites listed in it are termed \"NCR sites\".\n\nThe approach adopted by Ratcliffe was adapted and applied to the selection of sites important for geological conservation in the Geological Conservation Review. A Marine Nature Conservation Review has also been published.\n\nVolume 1 set out the rationale and methods used, and gave descriptions of the major habitat types.\n\nVolume 2 consisted entirely of a site inventory. Sites were grouped into six major habitat types:\n"}
{"id": "20568962", "url": "https://en.wikipedia.org/wiki?curid=20568962", "title": "A Place in the Land", "text": "A Place in the Land\n\nA Place in the Land is a 1998 American short documentary film directed by Charles Guggenheim. It was nominated for an Academy Award for Best Documentary Short.\n\nA Place in the Land considers the history of conservation stewardship in America as reflected in the property of Billings Farm, an operating dairy farm first established in 1871, and the Mount Tom, as well as through the work of George Perkins Marsh, Frederick Billings, and Laurance Rockefeller who were successive residents of the estate. The documentary is shown daily at the visitor center for the Billings Farm & Museum and the Marsh-Billings-Rockefeller National Historical Park. The National Park Service and the American Memory project of the Library of Congress served as advisers to the Woodstock Foundation in the production of the film.\n\n"}
{"id": "4003760", "url": "https://en.wikipedia.org/wiki?curid=4003760", "title": "Ammonium diuranate", "text": "Ammonium diuranate\n\nAmmonium diuranate or (ADU) ((NH)UO), is one of the radioactive intermediate chemical forms of uranium produced during yellowcake production. The name \"yellowcake\" originally given to this bright yellow substance, now applies to mixtures of uranium oxides which are actually hardly ever yellow. It also is an intermediate in mixed-oxide (MOX) fuel fabrication.\nIt is precipitated by adding aqueous ammonium hydroxide after uranium extraction by tertiary amines in kerosene. This precipitate is then thickened and centrifuged before being calcined to uranium oxide. Canadian practice favours the production of uranium oxide from ammonium diuranate, rather than from uranyl nitrate as is the case elsewhere.\n\nAmmonium diuranate was once used to produce colored glazes in ceramics. However when being fired this will decompose to uranium oxide, so the uranate was only used as a lower cost material than the fully purified uranium oxide.\n"}
{"id": "33028679", "url": "https://en.wikipedia.org/wiki?curid=33028679", "title": "Areva Wind", "text": "Areva Wind\n\nAreva Wind is a subsidiary of Areva, a French nuclear company. The company designs, assembles, installs and commissions 5-Megawatt wind turbines for offshore wind farms, a growing international industry. Each turbine can produce enough electricity to power 4,000 to 5,000 homes. Areva also designs and manufactures rotor blades through its subsidiary Areva Blades. Areva Wind is headquartered in Bremerhaven, Germany and Areva Blades is located in Stade, Germany.\n\nAreva Wind produced 6 of 12 turbines for the first German offshore test field alpha ventus and is producing turbines for commercial wind farms in the North Sea. Areva says it is Europe’s second largest offshore wind industry player, and it will have installed more than 120 turbines in the waters around Europe by 2013. The company is active in the UK and France and has stated that it plans to develop offshore wind in North America.\n\nAreva Wind began as Multibrid GmbH which, along with Areva Blades, was owned by the energy company Prokon Nord Energiesysteme. The Areva Group first invested in Multibrid in 2007, buying 51% of the company’s shares from Prokon Nord Energiesysteme for €150 million. It bought the remaining 49% of shares in 2010. In 2009, Areva acquired Areva Blades, formerly PN Rotor, and added the subsidiary to its Renewables business group.\n\nAreva Wind is one of four units in Areva’s Renewable Energies business group (AREVA Renewables). Jean Huby is Areva Wind’s Executive Vice President. Areva Wind is headquartered in the North of Germany. All of its M5000 wind turbines are developed and manufactured at Bremerhaven. The blades for the M5000 are manufactured in Stade (Areva Blades). Both plants are located near the port of Bremerhaven or Hamburg.\n\nAreva operates a 50/50-joint venture for offshore turbines with Gamesa Corporación Tecnológica called Adwen. For the year of 2015, Adwen had a market share of 18%.\n\nThe Areva M5000 (the M stands for “Multibrid”, which means multi-megawatts and a hybrid design) is designed for harsh offshore conditions. The design has been made by aerodyn Energiesysteme. The turbine design has got the iF Design Award 1999 [Bartsch / aerodyn]. It can power up to 5,000 households, assuming each household consumption of 1 kilowatt. Its hybrid drive system and lightweight blades were designed for compactness and reliability. Weighing 815 metric tons (including the 500-metric ton tower), the wind turbine operates at wind speeds of 3.5–25 m/s. The hub height is 90 m (LAT), and the diameter of the rotor is 116 m. \nThe turbine includes a unique anti corrosion system, an innovation which was given the European Inventor AWARD 2008 by the European Patent Office.\n\nWinergy and Adwen developed a gearbox for the AD 8-180 turbine (180 m rotor, 8 MW generator). The gearbox weighs 86 tonnes and has an input torque of 10 MNm. Adwen's new parent Siemens discontinued the AD8 in 2017.\n\nThe first prototype was installed in 2004. Three additional onshore turbines were installed in 2008 in order to develop expertise with tripods, large hub heights, erection, lifting equipment, logistics and serial manufacturing.\n\nIn 2009, Areva Wind completed the EPC supply and commissioning of 6 M5000 turbines for the first German offshore wind farm, alpha ventus. Alpha ventus is situated near the German island of Borkum in the North Sea. The farm was completed in September 2009 and incorporates 12 wind turbines with a total production of 60 MW. Six of those are Areva Wind M5000 models and the remaining 6 were made by REpower. The farm is owned and operated by the DOTI syndicate, which comprises E.ON Energy Projects GmbH, EWE AG, and Vattenfall Europe.\n\nAreva Wind was selected in 2010 to supply 40 Areva M5000 wind turbines for the 200 MW Borkum West II offshore project run by Trianel. This project is also located on the North Sea close to the alpha ventus test field.\n\nThe Global Tech North Sea Wind Park is a €1.3-billion project by Global Tech 1 North Sea Gmbh scheduled for construction between 2012 and 2013. It will consist of 80 M5000 turbines, all supplied by Areva Wind in an €800 million contract. The 400-MW farm will be situated 110 km northwest of Cuxhaven in the German Exclusive Economic Zone (AWZ). It is expected to help cut Germany’s carbon dioxide emissions by 1.2 million tons each year.\n\nIn May 2011, Areva signed a partnership agreement with GDF SUEZ and Vinci SA to respond jointly to the January 2011 call for tenders announced by French President Nicolas Sarkozy. The government is targeting 6,000 MW of offshore wind capacity by 2020 and intends to build 5 offshore wind farms along the French coast. The agreement applies exclusively to three wind farms at Dieppe-Le Tréport, Courseulles-sur-Mer and Fécamp. These wind farms are expected to serve the electricity requirements of several million people for an average duration of 30 years.\n\nIn June 2011, Iberdrola Renewables and Areva signed a memorandum of understanding to jointly develop offshore wind projects in France following the government’s announcement of a plan to reach a 6 GW target by 2020. The partners will compete for two of the country’s five offshore zones offered in a first phase of bidding.\n\nAreva's M5000 was the offshore turbine of the year 2012\n"}
{"id": "715418", "url": "https://en.wikipedia.org/wiki?curid=715418", "title": "Aubrey holes", "text": "Aubrey holes\n\nThe Aubrey holes are a ring of fifty-six (56) chalk pits at Stonehenge, named after the seventeenth-century antiquarian John Aubrey. They date to the earliest phases of Stonehenge in the late fourth and early third millennium BC. Despite decades of argument and analysis, their purpose is still unknown although an astronomical role has often been suggested.\n\nWhilst visiting the monument in 1666, Aubrey noticed five circular cavities in the ground and noted them in his records. These features were ignored or not seen by the later antiquarians to investigate the site, and it was not until the 1920s during the work carried out by Colonel William Hawley that Hawley's assistant Robert Newall identified a ring of pits he named in honour of Aubrey and his early survey.\n\nThe depressions seen by Aubrey himself are more likely to have been different features from those that now bear his name. Mike Pitts in a 1981 article in \"Nature\" pointed out that the holes had been backfilled thousands of years before Aubrey visited the site. The presence of later cremation burials and sarsen stone chips in the holes' upper fills supports this. That none of the other antiquarians who visited the site noticed any such holes implies that they were not permanent features either. Pitts argues that they were more likely to be the cavities left by features that had recently been removed. He has suggested that perhaps further megaliths stood at Stonehenge which occupied these other holes and are now lost.\n\nTwenty-five of the holes were excavated by Hawley in 1920 and seven more in 1924. In 1950 Stuart Piggott and Richard Atkinson dug two more Aubrey Holes which brought the total excavated to thirty-five, including one that Richard Colt Hoare may have encountered whilst digging beneath the fallen Slaughter Stone in the early nineteenth century. It was found that the pits were an average of 0.76m deep and 1.06m in diameter. Twenty-five of the pits contained later cremation burials inserted into their upper fills along with long bone pins which may have secured leather or cloth bags used to hold the remains. Their presence makes Stonehenge Britain's oldest cremation cemetery.\nThe pits appear to have been refilled with the freshly excavated chalk rubble soon after being dug as no weathering has been noted on the chalk sides of the pits. They may also have been dug out and refilled numerous times. The holes are in an accurate, 271.6m circumference circle, distributed around the edge of the area enclosed by Stonehenge's earth bank, with a standard deviation in their positioning of 0.4m. The circle they describe is around 5m inside the monument's bank. Twenty-one of the holes remain unexcavated and no reliable dating material has been recovered from the other thirty-five. The only available carbon date from the holes comes from charcoal in one of the later cremations. It gives the broad range of 2919-1519 cal BC. That sarsen stone chips have only been found in the upper fills of the excavated pits implies that the digging of the holes predates the megalithic phases of Stonehenge. From this stratigraphic evidence it is therefore likely that the holes were dug during the first phase of the monument, Stonehenge 1 (around 3100 BC) and were then reused for burials during Stonehenge 2 in successive centuries. By the time the standing stones of Stonehenge 3 were erected (around 2600 BC), the holes had fallen out of use.\n\nThe positions of the holes are today marked at the Stonehenge site by white discs laid in the ground surface. Archaeologists number them 1 to 56 counting clockwise from the later Slaughter Stone at the eastern side of the north east entrance. Hawley reburied the human cremations he found, placing them in the backfilled hole number 7. These remains were re-excavated in August 2008 as part of the Stonehenge Riverside Project. A plaque dating from the 1935 reburial was uncovered at the site. The project was detailed in an episode of the PBS TV series Nova around the same time.\n\nMany interpretations prefer an astronomical explanation for the purpose of the holes although this is by no means proved. It was thought that when the Aubrey holes were first dug, the only standing feature at Stonehenge was the Heelstone, which marked the point of the midsummer sunrise, viewed from the centre of the henge. The Heel Stone is now attributed to Stonehenge 3 and was therefore not contemporary with the holes.\n\nIt has been suggested that the Aubrey holes were originally intended to be postholes containing timbers or stones but this is uncertain. Analogous timber circles at sites such as Woodhenge have influenced this interpretation; the positioning of the Aubrey Hole circle relative to the earth bank and ditch at Stonehenge is reproduced at similar sites with rings of timber postholes. That the holes appear to have been filled soon after excavation and then possibly redug does not exclude the possibility that they held timber posts which were replaced intermittently. No evidence of postpipes has been recovered with the holes although modern archaeological excavation techniques have not been applied to the pits. If the putative timbers were purposefully removed when they fell out of use, then even this evidence would be difficult to spot however. The lack of full documentation from the early Stonehenge digs in the twentieth century and the disturbance caused by the insertion of the later cremations during Stonehenge 2 have also added to the uncertainty over the function of the holes.\n\nIn a survey of twentieth-century excavations at Stonehenge, English Heritage's \"Stonehenge in its landscape\", archaeologist Karen Walker collated and studied the surviving records from all the earlier work on the holes and concluded that \"Although the evidence is inconclusive, and will no doubt be the subject of continued debate, the authors are inclined to support the view that the Aubrey Holes held posts, which were removed, rather than burnt in situ or left to decay.\"\n\nIn August 2008, further excavation of Aubrey Hole 7 by Mike Parker-Pearson, Mike Pitts and Julian Richards led the archaeologists to suggest that the 56 holes held Welsh bluestones, since they are too shallow to be postholes. In fact the holes are identical in width, depth, and shape to the bluestones located elsewhere in Stonehenge. They are confident that the Aubrey holes belong to Stonehenge's initial phase of construction. Professor Parker-Pearson said: \"It's very exciting that we have evidence for stones right from its beginnings around 3000 BC. That's almost 500 years earlier than anyone had thought.\"\n\nThe astronomical readings of the holes are largely a product of the interpretation of them as being simple pits without any structural features. This approach has required finding an explanation which tends towards the theory that the holes were repeatedly dug, filled and redug and excludes possibilities relating to any possible timber posts standing in them. The theory that they may have been used to hold temporary markers for use in astronomical observations gained credence in the 1960s.\n\nAn early attempt to analyse the positions of the Aubrey holes was undertaken by Gerald Hawkins a professor of astronomy at Boston University in the 1960s using an IBM 7090 computer. In his book \"Stonehenge Decoded\", Hawkins argued that the various features at the Stonehenge monument were arranged in such a way to predict a variety of astronomical events. He believed that the key to the holes' purpose was the lunar eclipse, which occurs on average about once a year on a 346.62 day cycle. Lunar eclipses are not always visible as the moon may be below the horizon as it moves across the sky, but over 18 to 19 years (18.61 years to be precise) the date and position of a visible eclipse will return to its beginning point on the horizon again. As the motion of the moon's orbit also causes it to work its way across the sky on an 18.61 year cycle in what is known as the journey between major and minor standstill and back, the theory that this period was both measurable and useful to Neolithic peoples seemed attractive.\n\nLunar movements may have had calendrical significance to early peoples, especially farmers who would have benefited from the division of the year into periods which indicated the best times for planting. 18.61 is not a whole number and so it cannot be used to predict an eclipse without precision equipment, using only crude marker stones or timber posts in a circle instead. Hawkins' theory was that three 18.61 year cycles multiply out to 55.83, which is much closer to an integer and therefore easier to mark using 56 holes. Hawkins argued that the Aubrey Holes were used to keep track of this long time period and could accurately predict the recurrence of a lunar eclipse on the same azimuth, that which aligned with the Heel Stone, every 56 years. Going further, by placing marker stones at the ninth, eighteenth, twenty-eighth, thirty-seventh, forty-sixth and fifty sixth holes, Hawkins deduced that other intermediate lunar eclipses could also be predicted.\n\nMore recent examination, notably by Richard Atkinson, has proved Hawkins largely wrong, as it is now established that the different features at the monument that he tried to incorporate into many of his alignment theories were in use at different times and could not have worked alone, the lateness of the installation of the Heel Stone being the final nail in the coffin. Furthermore, the 56 year period is not in fact a reliable method of predicting eclipses and it is now accepted that they never repeat their date and position over three consecutive 18.61 year-long lunar cycles. Hawkins' theories also required strict observance of the phases of the moon which further complicated predictions using his model.\n\nIn 1966 Sir Fred Hoyle examined the arguments in favour of an astronomical purpose for the holes and concluded that the 28 day lunar cycle could still have been indicated by moving a marker stone representing the moon anticlockwise by two holes every day, ending up with 56 holes in total. By moving another marker anticlockwise two holes every 13 days, which represented the Sun, an annual journey around the circle could also be made. Two further marker stones would also have had to be moved round 3 holes per year to represent the points where the moon (or rather its marker) crossed the Sun marker's path. Hoyle proved that this could have been used to produce a much more reliable method of predicting eclipses as whenever the moon and sun markers are directly opposite each other and the other two stones occupy those same opposing points, an eclipse can be guaranteed. This also has the additional benefit of not needing any standing stones to be present at the site.\n\nAlthough less complex and romantic than Hawkins' 'stone age calculator' such a technique is certainly feasible if only in theory. Much more elaborate predictive practices have also been suggested for the holes although all these methods, including Hoyle's, require a high level of astronomical awareness and a grasp of some very abstract concepts including knowing where and when to first position the stones around the circle. It has also been pointed out by R. Colton and R. L. Martin that simpler methods exist, based on observing the position of each moonrise, which would have worked just as well and which would not require moving numerous markers amongst 56 holes. This diminishes the astronomical significance of the number of the Aubrey Holes and their circular arrangement and tends to suggest that any astronomical purpose for the site may have been no more than symbolic.\n\nOn astronomical symbolism several analysts from Gerald Hawkins to Anthony Johnson have noted that Plutarch reported that Typhon / Seth in Egyptian and Greek myth was identified as the shadow of the Earth which covers the Moon during lunar eclipses. Plutarch further records that the Pythagoreans symbolically associated Typhon with a polygon of 56 sides, hence the connection of 56 to lunar eclipses is explicit, at least for the Hellenistic era.\n\nAccording to the astrologers Bruno and Louise Huber the Aubrey holes were an astronomical abacus to mark the positions and calculate the movement of lunar nodes.\n\nMike Pitts has asserted that the Aubrey Holes did in fact originally hold timbers and compares the site at this stage to Woodhenge, The Sanctuary and other Neolithic timber circles. Such a view contradicts the archaeo-astronomical view of the holes as being a unique predictive device.\n\nIn fact, early Stonehenge may have been barely different from the other Neolithic timber circles of the British Isles which had varying numbers of postholes and orientations and could therefore not have been used for in island-wide eclipse predicting. The interpretation of such timber circles is unclear, although parallels were drawn with Native American totem poles by Stuart Piggott in a 1946 BBC radio lecture. A 50 cm high carved wooden figure found in the Thames Marshes in 1912 and carbon dated to 2460-1980 BCE has been used rather tenuously to support the theory of carved wooden posts serving a more terrestrial purpose.\n\nAnother possible explanation for the holes, suggested by Richard Atkinson, is that they were excavated in turn in some unknown ritual involving a procession around the inside of the monument. Others have pointed out the significance of the 28 day human menstrual cycle and argued that the holes may have been fertility indicators. Alexander Thom calculated that the circle of holes had been laid out in a circumference of 131 of his megalithic rods although this number has no known significance. A recent CAD study of the holes has shown that a 56 sided polygon can be generated by the simple use of square and circle geometry. Aubrey Burl also notes that the azimuth of the Heel Stone, beyond the Aubrey Circle, marks the midpoint in the swing of the Moon between major and minor standstill points, at 51.3 degrees.\n\nThe true purpose of the holes may never be known, although future excavation of the remaining twenty-four using modern archaeological techniques will certainly provide more information.\n\n"}
{"id": "20842907", "url": "https://en.wikipedia.org/wiki?curid=20842907", "title": "Beyond Oil", "text": "Beyond Oil\n\nBeyond Oil: The View from Hubbert's Peak is a 2006 book by Kenneth S. Deffeyes. Deffeyes is a geologist who warned of the coming oil crisis in a previous book called \"Hubbert's Peak\". \n\nIn this book, he explores the Earth's supply of potential replacement fuels. \"Beyond Oil\" evaluates a range of energy sources, from petroleum to hydrogen, and evaluates the advantages and disadvantages of each.\n\n"}
{"id": "3370", "url": "https://en.wikipedia.org/wiki?curid=3370", "title": "Boron nitride", "text": "Boron nitride\n\nBoron nitride is a heat and chemically resistant refractory compound of boron and nitrogen with the chemical formula BN. It exists in various crystalline forms that are isoelectronic to a similarly structured carbon lattice. The hexagonal form corresponding to graphite is the most stable and soft among BN polymorphs, and is therefore used as a lubricant and an additive to cosmetic products. The cubic (sphalerite structure) variety analogous to diamond is called c-BN; it is softer than diamond, but its thermal and chemical stability is superior. The rare wurtzite BN modification is similar to lonsdaleite and may even be harder than the cubic form.\n\nBecause of excellent thermal and chemical stability, boron nitride ceramics are traditionally used as parts of high-temperature equipment. Boron nitride has potential use in nanotechnology. Nanotubes of BN can be produced that have a structure similar to that of carbon nanotubes, i.e. graphene (or BN) sheets rolled on themselves, but the properties are very different.\n\nBoron nitride exists in multiple forms that differ in the arrangement of the boron and nitrogen atoms, giving rise to varying bulk properties of the material.\n\nThe amorphous form of boron nitride (a-BN) is non-crystalline, lacking any long-distance regularity in the arrangement of its atoms. It is analogous to amorphous carbon.\n\nAll other forms of boron nitride are crystalline.\n\nThe most stable crystalline form is the hexagonal one, also called h-BN, α-BN, g-BN, and \"graphitic boron nitride\". Hexagonal boron nitride (point group = D; space group = P6/mmc) has a layered structure similar to graphite. Within each layer, boron and nitrogen atoms are bound by strong covalent bonds, whereas the layers are held together by weak van der Waals forces. The interlayer \"registry\" of these sheets differs, however, from the pattern seen for graphite, because the atoms are eclipsed, with boron atoms lying over and above nitrogen atoms. This registry reflects the polarity of the B–N bonds. Still, h-BN and graphite are very close neighbors and even the BCN hybrids have been synthesized where carbon substitutes for some B and N atoms.\n\nCubic boron nitride has a crystal structure analogous to that of diamond. Consistent with diamond being less stable than graphite, the cubic form is less stable than the hexagonal form, but the conversion rate between the two is negligible at room temperature, as it is for diamond. The cubic form has the sphalerite crystal structure, the same as that of diamond, and is also called β-BN or c-BN.\n\nThe wurtzite form of boron nitride (w-BN; point group = C; space group = P6mc) has the same structure as lonsdaleite, a rare hexagonal polymorph of carbon. As in the cubic form, the boron and nitrogen atoms are grouped into tetrahedra, but in w-BN the angles between neighboring tetrahedra are different. As in the cubic form, the boron and nitrogen atoms are grouped into 6-membered rings; in the cubic form all rings are in the chair configuration, in w-BN the rings between 'layers' are in boat configuration. The Wurtzite form is thought to be very strong, and was estimated by a simulation as potentially having a strength 18% stronger than that of diamond, but because only small amounts of the mineral exist in nature, this has not yet been experimentally verified.\n\n\"Sources: amorphous BN, crystalline BN, graphite, diamond\".\n\nThe partly ionic structure of BN layers in h-BN reduces covalency and electrical conductivity, whereas the interlayer interaction increases resulting in higher hardness of h-BN relative to graphite. The reduced electron-delocalization in hexagonal-BN is also indicated by its absence of color and a large band gap. Very different bonding – strong covalent within the basal planes (planes where boron and nitrogen atoms are covalently bonded) and weak between them – causes high anisotropy of most properties of h-BN.\n\nFor example, the hardness, electrical and thermal conductivity are much higher within the planes than perpendicular to them. On the contrary, the properties of c-BN and w-BN are more homogeneous and isotropic.\n\nThose materials are extremely hard, with the hardness of bulk c-BN being slightly smaller and w-BN even higher than that of diamond. Polycrystalline c-BN with grain sizes on the order of 10 nm is also reported to have Vickers hardness comparable or higher than diamond. Because of much better stability to heat and transition metals, c-BN surpasses diamond in mechanical applications, such as machining steel. The thermal conductivity of BN is among the highest of all electric insulators (see table).\n\nBoron nitride can be doped p-type with beryllium and n-type with boron, sulfur, silicon or if co-doped with carbon and nitrogen. Both hexagonal and cubic BN are wide-gap semiconductors with a band-gap energy corresponding to the UV region. If voltage is applied to h-BN or c-BN, then it emits UV light in the range 215–250 nm and therefore can potentially be used as light-emitting diodes (LEDs) or lasers.\n\nLittle is known on melting behavior of boron nitride. It sublimates at 2973 °C at normal pressure releasing nitrogen gas and boron, but melts at elevated pressure.\n\nHexagonal and cubic (and probably w-BN) BN show remarkable chemical and thermal stabilities. For example, h-BN is stable to decomposition at temperatures up to 1000 °C in air, 1400 °C in vacuum, and 2800 °C in an inert atmosphere. The reactivity of h-BN and c-BN is relatively similar, and the data for c-BN are summarized in the table below. \n\nThermal stability of c-BN can be summarized as follows:\n\nBoron nitride is insoluble in the usual acids, but is soluble in alkaline molten salts and nitrides, such as LiOH, KOH, NaOH-NaCO, NaNO, LiN, MgN, SrN, BaN or LiBN, which are therefore used to etch BN.\n\nThe theoretical thermal conductivity of hexagonal Boron nitride nanoribbons (BNNRs) can approach 1700–2000 W/(m·K), which has the same order of magnitude as the experimental measured value for graphene, and can be comparable to the theoretical calculations for graphene nanoribbons. Moreover, the thermal transport in the BNNRs is anisotropic. The thermal conductivity of zigzag-edged BNNRs is about 20% larger than that of armchair-edged nanoribbons at room temperature.\n\nIn 2009, a naturally occurring boron nitride mineral in the cubic form (c-BN) was reported in Tibet, with a proposed name of \"qingsongite\". The substance was found in dispersed micron-sized inclusions in chromium-rich rocks. In 2013, the International Mineralogical Association affirmed the mineral and the name.\n\nBoron nitride is produced synthetically. Hexagonal boron nitride is obtained by the reacting boron trioxide (BO) or boric acid (HBO) with ammonia (NH) or urea (CO(NH)) in a nitrogen atmosphere:\n\nThe resulting disordered (amorphous) boron nitride contains 92–95% BN and 5–8% BO. The remaining BO can be evaporated in a second step at temperatures in order to achieve BN concentration >98%. Such annealing also crystallizes BN, the size of the crystallites increasing with the annealing temperature.\n\nh-BN parts can be fabricated inexpensively by hot-pressing with subsequent machining. The parts are made from boron nitride powders adding boron oxide for better compressibility. Thin films of boron nitride can be obtained by chemical vapor deposition from boron trichloride and nitrogen precursors. Combustion of boron powder in nitrogen plasma at 5500 °C yields ultrafine boron nitride used for lubricants and toners.\n\nBoron nitride reacts with iodine fluoride in trichlorofluoromethane at −30 °C to produce an extremely sensitive contact explosive, NI, in low yield.\nBoron nitride reacts with nitrides of alkali metals and lanthanides to form nitridoborate compounds. For example:\n\nSimilar to graphite, various molecules, such as NH or alkali metals, can be intercalated into hexagonal boron nitride, that is inserted between its layers. Both experiment and theory suggest the intercalation is much more difficult for BN than for graphite.\n\nSynthesis of c-BN uses same methods as that of diamond: Cubic boron nitride is produced by treating hexagonal boron nitride at high pressure and temperature, much as synthetic diamond is produced from graphite. Direct conversion of hexagonal boron nitride to the cubic form has been observed at pressures between 5 and 18 GPa and temperatures between 1730 and 3230 °C, that is similar parameters as for direct graphite-diamond conversion. The addition of a small amount of boron oxide can lower the required pressure to 4–7 GPa and temperature to 1500 °C. As in diamond synthesis, to further reduce the conversion pressures and temperatures, a catalyst is added, such as lithium, potassium, or magnesium, their nitrides, their fluoronitrides, water with ammonium compounds, or hydrazine. Other industrial synthesis methods, again borrowed from diamond growth, use crystal growth in a temperature gradient, or explosive shock wave. The shock wave method is used to produce material called heterodiamond, a superhard compound of boron, carbon, and nitrogen.\n\nLow-pressure deposition of thin films of cubic boron nitride is possible. As in diamond growth, the major problem is to suppress the growth of hexagonal phases (h-BN or graphite, respectively). Whereas in diamond growth this is achieved by adding hydrogen gas, boron trifluoride is used for c-BN. Ion beam deposition, plasma-enhanced chemical vapor deposition, pulsed laser deposition, reactive sputtering, and other physical vapor deposition methods are used as well.\n\nWurtzite BN can be obtained via static high-pressure or dynamic shock methods. The limits of its stability are not well defined. Both c-BN and w-BN are formed by compressing h-BN, but formation of w-BN occurs at much lower temperatures close to 1700 °C.\n\nWhereas the production and consumption figures for the raw materials used for BN synthesis, namely boric acid and boron trioxide, are well known (see boron), the corresponding numbers for the boron nitride are not listed in statistical reports. An estimate for the 1999 world production is 300 to 350 metric tons. The major producers and consumers of BN are located in the United States, Japan, China and Germany. In 2000, prices varied from about $75/kg to $120/kg for standard industrial-quality h-BN and were about up to $200–$400/kg for high purity BN grades.\n\nHexagonal BN (h-BN) is the most widely used polymorph. It is a good lubricant at both low and high temperatures (up to 900 °C, even in an oxidizing atmosphere). h-BN lubricant is particularly useful when the electrical conductivity or chemical reactivity of graphite (alternative lubricant) would be problematic. Another advantage of h-BN over graphite is that its lubricity does not require water or gas molecules trapped between the layers. Therefore, h-BN lubricants can be used even in vacuum, e.g. in space applications. The lubricating properties of fine-grained h-BN are used in cosmetics, paints, dental cements, and pencil leads.\n\nHexagonal BN was first used in cosmetics around 1940 in Japan. However, because of its high price, h-BN was soon abandoned for this application. Its use was revitalized in the late 1990s with the optimization h-BN production processes, and currently h-BN is used by nearly all leading producers of cosmetic products for foundations, make-up, eye shadows, blushers, kohl pencils, lipsticks and other skincare products.\n\nBecause of its excellent thermal and chemical stability, boron nitride ceramics are traditionally used as parts of high-temperature equipment. h-BN can be included in ceramics, alloys, resins, plastics, rubbers, and other materials, giving them self-lubricating properties. Such materials are suitable for construction of e.g. bearings and in steelmaking. Plastics filled with BN have less thermal expansion as well as higher thermal conductivity and electrical resistivity. Due to its excellent dielectric and thermal properties, BN is used in electronics e.g. as a substrate for semiconductors, microwave-transparent windows and as a structural material for seals. It can also be used as dielectric in resistive random access memories.\n\nHexagonal BN is used in xerographic process and laser printers as a charge leakage barrier layer of the photo drum. In the automotive industry, h-BN mixed with a binder (boron oxide) is used for sealing oxygen sensors, which provide feedback for adjusting fuel flow. The binder utilizes the unique temperature stability and insulating properties of h-BN.\n\nParts can be made by hot pressing from four commercial grades of h-BN. Grade HBN contains a boron oxide binder; it is usable up to 550–850 °C in oxidizing atmosphere and up to 1600 °C in vacuum, but due to the boron oxide content is sensitive to water. Grade HBR uses a calcium borate binder and is usable at 1600 °C. Grades HBC and HBT contain no binder and can be used up to 3000 °C.\n\nBoron nitride nanosheets (h-BN) can be deposited by catalytic decomposition of borazine at a temperature ~1100 °C in a chemical vapor deposition setup, over areas up to about 10 cm. Owing to their hexagonal atomic structure, small lattice mismatch with graphene (~2%), and high uniformity they are used as substrates for graphene-based devices. BN nanosheets are also excellent proton conductors. Their high proton transport rate, combined with the high electrical resistance, may lead to applications in fuel cells and water electrolysis.\n\nh-BN has been used since the mid-2000s as a bullet and bore lubricant in precision target rifle applications as an alternative to molybdenum disulfide coating, commonly referred to as \"moly\". It is claimed to increase effective barrel life, increase intervals between bore cleaning, and decrease the deviation in point of impact between clean bore first shots and subsequent shots.\n\nCubic boron nitride (CBN or c-BN) is widely used as an abrasive. Its usefulness arises from its insolubility in iron, nickel, and related alloys at high temperatures, whereas diamond is soluble in these metals to give carbides. Polycrystalline c-BN (PCBN) abrasives are therefore used for machining steel, whereas diamond abrasives are preferred for aluminum alloys, ceramics, and stone. When in contact with oxygen at high temperatures, BN forms a passivation layer of boron oxide. Boron nitride binds well with metals, due to formation of interlayers of metal borides or nitrides. Materials with cubic boron nitride crystals are often used in the tool bits of cutting tools. For grinding applications, softer binders, e.g. resin, porous ceramics, and soft metals, are used. Ceramic binders can be used as well. Commercial products are known under names \"Borazon\" (by Diamond Innovations), and \"Elbor\" or \"Cubonite\" (by Russian vendors). \n\nContrary to diamond, large c-BN pellets can be produced in a simple process (called sintering) of annealing c-BN powders in nitrogen flow at temperatures slightly below the BN decomposition temperature. This ability of c-BN and h-BN powders to fuse allows cheap production of large BN parts.\n\nSimilar to diamond, the combination in c-BN of highest thermal conductivity and electrical resistivity is ideal for heat spreaders. \n\nAs cubic boron nitride consists of light atoms and is very robust chemically and mechanically, it is one of the popular materials for X-ray membranes: low mass results in small X-ray absorption, and good mechanical properties allow usage of thin membranes, thus further reducing the absorption.\n\nLayers of amorphous boron nitride (a-BN) are used in some semiconductor devices, e.g. MOSFETs. They can be prepared by chemical decomposition of trichloroborazine with caesium, or by thermal chemical vapor deposition methods. Thermal CVD can be also used for deposition of h-BN layers, or at high temperatures, c-BN.\n\nHexagonal boron nitride can be exfoliated to mono or few atomic layer sheets. Due to its analogous structure to that of graphene but white appearance, atomically thin boron nitride is sometimes call “white graphene”.\n\nMechanical properties. Atomically thin boron nitride is one of the strongest electrically insulating materials. Monolayer boron nitride has an average Young’s modulus of 0.865TPa and fracture strength of 70.5GPa, and in contrast to graphene, whose strength decreases dramatically with increased thickness, few-layer boron nitride sheets have a strength similar to that of monolayer boron nitride.\n\nThermal conductivity. Atomically thin boron nitride has one of the highest thermal conductivity coefficients among semiconductors and electrical insulators, and its thermal conductivity increases with reduced thickness due to less intra-layer coupling.\n\nThermal stability. The air stability of graphene shows a clear thickness dependence: monolayer graphene is reactive to oxygen at 250°C, strongly doped at 300°C, and etched at 450°C; in contrast, bulk graphite is not oxidized until 800°C. Atomically thin boron nitride has much better oxidation resistance than graphene. Monolayer boron nitride is not oxidized till 700°C and can sustain up to 850°C in air; bilayer and trilayer boron nitride nanosheets have slightly higher oxidation starting temperatures. The excellent thermal stability, high impermeability to gas and liquid, and electrical insulation make atomically thin boron nitride potential coating materials for preventing surface oxidation and corrosion of metals and other two-dimensional (2D) materials, such as black phosphorus.\n\nBetter surface adsorption. Atomically thin boron nitride has been found to have better surface adsorption capabilities than bulk hexagonal boron nitride. According to theoretical and experimental studies, atomically thin boron nitride as an adsorbent experiences conformational changes upon surface adsorption of molecules, increasing adsorption energy and efficiency. The synergic effect of the atomic thickness, high flexibility, stronger surface adsorption capability, electrical insulation, impermeability, high thermal and chemical stability of BN nanosheets can increase the Raman sensitivity by up to two orders, and in the meantime attain long-term stability and extraordinary reusability not achievable by other materials.\n\nDielectric properties. Atomically thin hexagonal boron nitride is an excellent dielectric substrate for graphene, molybdenum disulphide (MoS), and many other 2D material-based electronic and photonic devices. As shown by electric force microscopy (EFM) studies, the electric field screening in atomically thin boron nitride shows a weak dependence on thickness, which is in line with the smooth decay of electric field inside few-layer boron nitride revealed by the first-principles calculations.\n\nRaman characteristics. Raman spectroscopy has been a useful tool to study a variety of 2D materials, and the Raman signature of high-quality atomically thin boron nitride was first reported by Gorbachev et al and Li et al. However, the two reported Raman results of monolayer boron nitride did not agree with each other. Cai et al, therefore, conducted systematic experimental and theoretical studies to reveal the intrinsic Raman spectrum of atomically thin boron nitride. It reveals that atomically thin boron nitride without interaction with a substrate has a G band frequency similar to that of bulk hexagonal boron nitride, but strain induced by the substrate can cause Raman shifts. Nevertheless, the Raman intensity of G band of atomically thin boron nitride can be used to estimate layer thickness and sample quality.\n\nBoron nitride nanomesh is a nanostructured two-dimensional material. It consists of a single BN layer, which forms by self-assembly a highly regular mesh after high-temperature exposure of a clean rhodium or ruthenium surface to borazine under ultra-high vacuum. The nanomesh looks like an assembly of hexagonal pores. The distance between two pore centers is 3.2 nm and the pore diameter is ~2 nm. Other terms for this material are boronitrene or white graphene.\n\nThe boron nitride nanomesh is not only stable to decomposition under vacuum, air and some liquids, but also up to temperatures of 800 °C. In addition, it shows the extraordinary ability to trap molecules and metallic clusters which have similar sizes to the nanomesh pores, forming a well-ordered array. These characteristics promise interesting applications of the nanomesh in areas like catalysis, surface functionalisation, spintronics, quantum computing and data storage media like hard drives.\n\n Boron nitride tubules were first made in 1989 by Shore and Dolan This work was patented in 1989 and published in 1989 thesis (Dolan) and then 1993 Science. The 1989 work was also the first preparation of amorphous BN by B-trichloroborazine and cesium metal. \n\nPreparation of Amorphous Boron Nitride and Its Conversion to a Turbostratic, Tubular Form\n\nEwan J. M. Hamilton1, \nShawn E. Dolan1, \nCharles M. Mann1, \nHendrik O. Colijn2, \nClare A. McDonald2, \nSheldon G. Shore1\n\n+ See all authors and affiliations \n\nScience 30 Apr 1993:\nVol. 260, Issue 5108, pp. 659-661\nDOI: 10.1126/science.260.5108.659 \nBoron nitride nanotubes were predicted in 1994 and experimentally discovered in 1995. They can be imagined as a rolled up sheet of h-boron nitride. Structurally, it is a close analog of the carbon nanotube, namely a long cylinder with diameter of several to hundred nanometers and length of many micrometers, except carbon atoms are alternately substituted by nitrogen and boron atoms. However, the properties of BN nanotubes are very different: whereas carbon nanotubes can be metallic or semiconducting depending on the rolling direction and radius, a BN nanotube is an electrical insulator with a bandgap of ~5.5 eV, basically independent of tube chirality and morphology. In addition, a layered BN structure is much more thermally and chemically stable than a graphitic carbon structure.\n\nBoron nitride aerogel is an aerogel made of highly porous BN. It typically consists of a mixture of deformed BN nanotubes and nanosheets. It can have a density as low as 0.6 mg/cm and a specific surface area as high as 1050 m/g, and therefore has potential applications as an absorbent, catalyst support and gas storage medium. BN aerogels are highly hydrophobic and can absorb up to 160 times their weight in oil. They are resistant to oxidation in air at temperatures up to 1200 °C, and hence can be reused after the absorbed oil is burned out by flame. BN aerogels can be prepared by template-assisted chemical vapor deposition using borazine as the feed gas. \n\nAddition of boron nitride to silicon nitride ceramics improves the thermal shock resistance of the resulting material. For the same purpose, BN is added also to silicon nitride-alumina and titanium nitride-alumina ceramics. Other materials being reinforced with BN include alumina and zirconia, borosilicate glasses, glass ceramics, enamels, and composite ceramics with titanium boride-boron nitride, titanium boride-aluminium nitride-boron nitride, and silicon carbide-boron nitride composition.\n\nBoron nitride (along with SiN, NbN, and BNC) is reported to show weak fibrogenic activity, and to cause pneumoconiosis when inhaled in particulate form. The maximum concentration recommended for nitrides of nonmetals is 10 mg/m for BN and 4 for AlN or ZrN.\n\n\n"}
{"id": "50700859", "url": "https://en.wikipedia.org/wiki?curid=50700859", "title": "Catching the Sun (film)", "text": "Catching the Sun (film)\n\nCatching the Sun is a 2015 documentary film on the growth of the solar power industry that premiered on Netflix in April, 2016. Directed by Shalini Kantayya, the film features portraits of diverse personalities and their roles in the transition to solar power. Unemployed workers in Richmond, California, businessmen in China, Tea Party activists, and would be White House adviser are all featured in the film. The film debunks a false dilemma that clean energy requires sacrificing economic prosperity.\n\nThe film begins with the 2012 Chevron refinery fire in Richmond, California and contrasts the possibilities for Richmond of an energy sector based on solar installation versus continued pollution and economic stagnation from an energy industry rooted on the oil refinery. The film gives a short history of solar technology in the United States, and the road not taken when, on his first day in office, Ronald Reagan removed the solar panels Jimmy Carter had placed on the White House, and abandoned policies to further solar development.\n\nThe film features unemployed workers in Northern California receiving Green-collar job training from Solar Richmond to work in the emergent field of solar installation. Van Jones features prominently in the documentary, which depicts his early work in Green for All, his family's relocation from Oakland to Washington, DC to take advantage of a White House job offer, and Jones's subsequent resignation in the face sustained attacks from conservative media.\n\nConservative Debbie Dooley from Atlanta, Georgia rejects an ideological divide between the left and right on clean energy. Through her organization, Conservatives for Energy Freedom, she advocates for clean energy by breaking down barriers that favor the incumbent energy industry and restrict a true free market.\n\nWhile ideological gridlock weighs on American energy policy, other countries move forward. A profile on Zhongwei Jiang, a Chinese entrepreneur based in Wuxi, China gives perspective on solar growth in Germany, India, and China. Jiang, who grew up without electricity until the age of seven, founded the solar company WesTech in 2003. Jiang conducts an international business in solar products and dreams of a \"Solar City\" in Texas where thousands of Americans would be employed in the solar sector.\n\nThe film is the documentary debut for Indian-born and Brooklyn-based, eco-activist and filmmaker Shalini Kantayya. Kantayya had a career breakthrough when she became a finalist in the 2007 reality TV show \"On the Lot\".\n\nThe film debuted at the 2015 LA Film Festival and had a limited theatrical release in the United States in Spring 2016. \"Catching the Sun\" made its public debut on Earth Day 2016 on Netflix.\n"}
{"id": "15901419", "url": "https://en.wikipedia.org/wiki?curid=15901419", "title": "Coal Authority", "text": "Coal Authority\n\nThe Coal Authority is a non-departmental public body of the United Kingdom government.\n\nIt was established under the Coal Industry Act 1994 to manage some functions, in which the British Coal Corporation (formerly the National Coal Board) had previously undertaken, including ownership of unworked coal. Its offices are in Mansfield, Nottinghamshire. It was funded by the Department for Business, Enterprise and Regulatory Reform, and now the Department for Business, Energy and Industrial Strategy (headed by Greg Clark) funds it.\n\nIt is responsible for licensing coal mining operations, and for providing information on coal reserves and past and future coal mining. It settles subsidence claims not falling on coal mining operators. It deals with the management and disposal of property, and with surface hazards, such as abandoned coal mine shafts. It operates a twenty four hour call out service for surface hazards.\n\n\n"}
{"id": "2428994", "url": "https://en.wikipedia.org/wiki?curid=2428994", "title": "Cockcroft–Walton generator", "text": "Cockcroft–Walton generator\n\nThe Cockcroft–Walton (CW) generator, or multiplier, is an electric circuit that generates a high DC voltage from a low-voltage AC or pulsing DC input. It was named after the British and Irish physicists John Douglas Cockcroft and Ernest Thomas Sinton Walton, who in 1932 used this circuit design to power their particle accelerator, performing the first artificial nuclear disintegration in history. They used this voltage multiplier cascade for most of their research, which in 1951 won them the Nobel Prize in Physics for \"Transmutation of atomic nuclei by artificially accelerated atomic particles\". The circuit was discovered in 1919, by Heinrich Greinacher, a Swiss physicist. For this reason, this doubler cascade is sometimes also referred to as the Greinacher multiplier. Cockcroft–Walton circuits are still used in particle accelerators. They also are used in everyday electronic devices that require high voltages, such as X-ray machines, television sets, microwave ovens and photocopiers.\n\nThe CW is a voltage multiplier that converts AC or pulsing DC electrical power from a low voltage level to a higher DC voltage level. It is made up of a voltage multiplier ladder network of capacitors and diodes to generate high voltages. Unlike transformers, this method eliminates the requirement for the heavy core and the bulk of insulation/potting required. Using only capacitors and diodes, these voltage multipliers can step up relatively low voltages to extremely high values, while at the same time being far lighter and cheaper than transformers. The biggest advantage of such circuits is that the voltage across each stage of the cascade is equal to only twice the peak input voltage in a half-wave rectifier. In a full-wave rectifier it is three times the input voltage. It has the advantage of requiring relatively low-cost components and being easy to insulate. One can also tap the output from any stage, like in a multitapped transformer.\n\nTo understand the circuit operation, see the diagram of the two-stage version at right. Assume the circuit is powered by an alternating voltage \"V\" with a peak value of \"V\", and initially the capacitors are uncharged. After the input voltage is turned on \nWith each change in input polarity, current flows up the \"stack\" of capacitors through the diodes, until they are all charged. All the capacitors are charged to a voltage of 2\"V\", except for \"C1\", which is charged to \"V\". The key to the voltage multiplication is that while the capacitors are charged in parallel, they are connected to the load in series. Since \"C2\" and \"C4\" are in series between the output and ground, the total output voltage (under no-load conditions) is \"V\" = 4\"V\". \n\nThis circuit can be extended to any number of stages. The no-load output voltage is twice the peak input voltage multiplied by the number of stages N or equivalently the peak-to-peak input voltage swing (\"V\") times the number of stages\nThe number of stages is equal to the number of capacitors in series between the output and ground.\n\nOne way to look at the circuit is that it functions as a charge \"pump\", pumping electric charge in one direction, up the stack of capacitors. The CW circuit, along with other similar capacitor circuits, is often called charge pump. For substantial loads, the charge on the capacitors is partially depleted, and the output voltage drops according to the output current divided by the capacitance.\n\nIn practice, the CW has a number of drawbacks. As the number of stages is increased, the voltages of the higher stages begin to \"sag\", primarily due to the electrical impedance of the capacitors in the lower stages. And, when supplying an output current, the voltage ripple rapidly increases as the number of stages is increased. For these reasons, CW multipliers with large number of stages are used only where relatively low output current is required. These effects can be partially compensated by increasing the capacitance in the lower stages, by increasing the frequency of the input power and by using an AC power source with a square or triangular waveform. By driving the CW from a high-frequency source, such as an inverter, or a combination of an inverter and HV transformer, the overall physical size and weight of the CW power supply can be substantially reduced.\n\nCW multipliers are typically used to develop higher voltages for relatively low-current applications, such as bias voltages ranging from tens or hundreds of volts to millions of volts for high-energy physics experiments or lightning safety testing. CW multipliers are also found, with a higher number of stages, in laser systems, high-voltage power supplies, X-ray systems, LCD backlighting, traveling-wave tube amplifiers, ion pumps, electrostatic systems, air ionisers, particle accelerators, copy machines, scientific instrumentation, oscilloscopes, television sets and cathode ray tubes, electroshock weapons, bug zappers and many other applications that use high-voltage DC.\n\nA similar circuit is the Marx generator, which has the same \"ladder\" structure, but consists of resistors, capacitors and spark gaps. The Marx generator produces short pulses, whereas the CW generator produces a constant DC. Unlike the Cockcroft-Walton multiplier (generator), the Marx generator need air for the spark gaps and can not be immersed in oil as an insulator.\n\n\n"}
{"id": "467830", "url": "https://en.wikipedia.org/wiki?curid=467830", "title": "Curveball", "text": "Curveball\n\nThe curveball is a type of pitch in baseball thrown with a characteristic grip and hand movement that imparts forward spin to the ball, causing it to dive in a downward path as it approaches the plate. Varieties of curveballs include the 12-6 curveball and the knuckle curve. Its close relatives are the slider and the slurve. The \"curve\" of the ball varies from pitcher to pitcher.\n\nOutside the context of baseball, variants of the expression \"to throw a curveball\" essentially translate to introducing a significant deviation to a preceding concept.\n\nThe curveball is gripped much like a cup or drinking glass is held. The pitcher places the middle finger on and parallel to one of the long seams, and the thumb just behind the seam on the opposite side of the ball such that if looking from the top down, the hand should form a \"C shape\" with the horseshoe pointing in towards the palm following the contour of the thumb. The index finger is placed alongside the middle finger, and the other two extraneous fingers are folded in towards the palm with the knuckle of the ring finger touching the leather. Occasionally some pitchers will flare out these two fingers straight and away from the ball to keep them clear of the throwing motion. The curveball and slider share nearly identical grips and throwing motion.\n\nThe delivery of a curveball is entirely different from that of most other pitches. The pitcher at the top of the throwing arc will snap the arm and wrist in a downward motion. The ball first leaves contact with the thumb and tumbles over the index finger thus imparting the forward or \"top-spin\" characteristic of a curveball. The result is the exact opposite pitch of the four-seam fastball's backspin, but with all four seams rotating in the direction of the flight path with forward-spin, with the axis of rotation perpendicular to the intended flight path much like a reel-type mower or a bowling ball.\n\nThe amount of break on the ball depends on how hard the pitcher can snap the throw off, or how much forward spin can be put on the ball. The harder the snap, the more the pitch will break. Curveballs primarily break downwards, but can also break toward the pitcher's off hand to varying degrees. Unlike the fastball, the height of the ball's flight path arc does not necessarily need to occur at the pitcher's release point, and often peaks shortly afterwards. Curveballs are thrown with considerably less velocity than fastballs, because of both the unnatural delivery of the ball and the general rule that pitches thrown with less velocity will break more. A typical curveball in the major collegiate level and above will average between 65 and 80 mph, with the average MLB curve at 77 mph.\n\nFrom a hitter's perspective, the curveball will start in one location (usually high or at the top of the strike zone) and then dive rapidly as it approaches the plate. The most effective curveballs will start breaking at the height of the arc of the ball flight, and continue to break more and more rapidly as they approach and cross through the strike zone. A curveball that a pitcher fails to put enough spin on will not break much and is colloquially called a \"hanging curve\". Hanging curves are usually disastrous for a pitcher because the low velocity, non-breaking pitch is left high in the zone where hitters can wait on it and drive it for power.\n\nThe curveball is a popular and effective pitch in professional baseball, but it is not particularly widespread in leagues with players younger than college-level players. This is with regard for the safety of the pitcher – not because of its difficulty – though the pitch is widely considered difficult to learn as it requires some degree of mastery and the ability to pinpoint the thrown ball's location. There is generally greater chance of throwing wild pitches when throwing the curveball.\n\nWhen thrown correctly, it could have a break from seven to as much as 20 inches in comparison to the same pitcher's fastball.\n\nDue to the unnatural motion required to throw it, the curveball is considered a more advanced pitch and poses inherent risk of injury to a pitcher’s elbow and shoulder. There has been a controversy, as reported in the \"New York Times\", March 12, 2012, about whether curveballs alone are responsible for injuries in young pitchers or whether it is the number of pitches thrown that are the predisposing factor. In theory, allowing time for the cartilage and tendons of the arm to fully develop would protect against injuries. While acquisition of proper form might be protective, Dr. James Andrews is quoted in the article as stating that in many children, insufficient neuromuscular control, lack of proper mechanics, and fatigue make maintenance of proper form unlikely.\n\nThe parts of the arm most commonly injured by the curveball are the ligaments in the elbow, the biceps, and the forearm muscles. Major elbow injury requires repair through elbow ligament reconstruction, or Tommy John surgery.\n\nCurveballs have a variety of trajectories and breaks among pitchers. This chiefly has to do with the arm slot and release point of a given pitcher, which is in turn governed by how comfortable the pitcher is throwing the overhand curveball.\n\nPitchers who can throw a curveball completely over handed with the arm slot more or less vertical will have a curveball that will break straight downwards. This is called a 12–6 curveball as the break of the pitch is on a straight path downwards like the hands of a clock at 12 and 6. The axis of rotation of a 12–6 curve is parallel with the level ground and perpendicular to its flight path.\n\nPitchers who throw their curveballs with the arm slot at an angle will throw a curveball that breaks down and toward the pitcher's off-hand. In the most extreme cases the curve will break very wide laterally. Because the slider and the curveball share nearly the same grip and have the same unique throwing motions, this curveball breaks much like a slider, and is colloquially termed a \"slurve.\" The axis of rotation on a slurve will still be more or less perpendicular to the flight path of the ball; the latter however, will not be parallel to the ground. With some pitchers, the difference between curveball and other pitches such as slider and slurve, may be difficult to detect or even describe. A less common term for this type of curveball is a 1-7 or 2-8 curve.\n\nGenerally the Magnus effect describes the laws of physics that make a curveball curve. A fastball travels through the air with backspin, which creates a higher pressure zone in the air ahead of and under the baseball. The baseball's raised seams augment the ball's ability to develop a boundary layer and therefore a greater differential of pressure between the upper and lower zones. The effect of gravity is partially counteracted as the ball rides on and into increased pressure. Thus the fastball falls less than a ball thrown without spin (neglecting knuckleball effects) during the 60 feet 6 inches it travels to home plate.\n\nOn the other hand, a curveball, thrown with topspin, creates a higher pressure zone on top of the ball, which deflects the ball downward in flight. Instead of counteracting gravity, the curveball adds additional downward force, thereby gives the ball an exaggerated drop in flight.\n\nThere was once a debate on whether a curveball actually curves or is an optical illusion. In 1949, Ralph B. Lightfoot, an aeronautical engineer at Sikorsky Aircraft, used wind tunnel tests to prove that a curveball does in fact actually curve. On whether a curveball is caused by an illusion, Baseball Hall of Fame pitcher Dizzy Dean has been quoted in a number of variations on this basic premise: \"Stand behind a tree 60 feet away, and I will whomp you with an optical illusion!\"\n\nHowever, optical illusion caused by the ball's spinning may play an important part in what makes curveballs difficult to hit. The curveball's trajectory is smooth, however the batter perceives a sudden, dramatic change in the ball's direction. When an object that is spinning and moving through space is viewed directly, the overall motion is interpreted correctly by the brain. However, as it enters the peripheral vision, the internal spinning motion distorts how the overall motion is perceived. A curveball's trajectory begins in the center of the batter's vision, but overlaps with peripheral vision as it approaches the plate, which may explain the suddenness of the break perceived by the batter. A peer-reviewed article on this hypothesis was published in 2010.\n\nPopular nicknames for the curveball include \"the bender\" and \"the hook\" (both describing the trajectory of the pitch), \"Uncle Charlie\", \"Lord Charles\", \"the yellow hammer\", and \"the yakker\". Because catchers frequently use two fingers to signal for a curve, the pitch is also referred to as \"the deuce\" or \"number two\".\n\nBaseball lore has it that the curveball was invented in the early 1870s by Candy Cummings (it is debatable). An early demonstration of the \"skewball\" or curveball occurred at the Capitoline Grounds in Brooklyn in August 1870 by Fred Goldsmith. In 1869, a reporter for the \"New York Clipper\" described Phonney Martin as an \"extremely hard pitcher to hit for the ball never comes in a straight line‚ but in a tantalizing curve.\" If the observation is true, this would pre-date Cummings and Goldsmith. In 1876, the first known collegiate baseball player to perfect the curveball was Clarence Emir Allen of Western Reserve College, now known as Case Western Reserve University. Both Allen, and teammate pitcher John P. Barden, became famous for employing the curve in the late 1870s. In the early 1880s, Clinton Scollard (1860–1932), a pitcher from Hamilton College in New York, became famous for his curve ball and later earned fame as a prolific American poet. In 1885, \"St. Nicholas\", a children's magazine, featured a story entitled, \"How Science Won the Game\". It told of how a boy pitcher mastered the curveball to defeat the opposing batters. In the early years of the sport, use of the curveball was thought to be dishonest and was outlawed, but officials could not do much to stop pitchers from using it.\n\nRecords of the Princeton University (then the College of New Jersey) game from September 26, 1863 in the New York Clipper of the Nassaus facing the Athletics refer to F. P. Henry, Princeton Class of 1866, \"slow pitching with a great twist to the ball achieved a victory over fast pitching.\" By 1866, many Princeton players were pitching and hitting \"curved balls.\"\n\nHarvard President Charles Eliot was among those opposed to the curve, claiming it was a dishonest practice that Harvard students should not want to partake in.\n\nIn the past, major league pitchers Bob Feller, Virgil Trucks, Herb Score, Camilo Pascual and Sandy Koufax were regarded as having outstanding curveballs. Other notable curveball pitchers since 1900 are/were Barry Zito, Kerry Wood, Adam Wainwright, Dwight Gooden, Nolan Ryan, David Wells, Darryl Kile, Clayton Kershaw, Orel Hershiser, Bert Blyleven, Yu Darvish, Justin Verlander, Steve Carlton, Mordecai Brown, Seth Lugo, and Lance McCullers Jr..\n\n"}
{"id": "1929116", "url": "https://en.wikipedia.org/wiki?curid=1929116", "title": "Derivative (chemistry)", "text": "Derivative (chemistry)\n\nIn chemistry, a derivative is a compound that is derived from a similar compound by a chemical reaction.\n\nIn the past, derivative also meant a compound that \"can be imagined to\" arise from another compound, if one atom or group of atoms is replaced with another atom or group of atoms, but modern chemical language now uses the term structural analog for this meaning, thus eliminating ambiguity. The term \"structural analogue\" is common in organic chemistry.\n\nIn biochemistry, the word is used for compounds that at least theoretically can be formed from the precursor compound.\n\nChemical derivatives may be used to facilitate analysis. For example, melting point (MP) analysis can assist in identification of many organic compounds. A crystalline derivative may be prepared, such as a semicarbazone or 2,4-dinitrophenylhydrazone (derived from aldehydes or ketones), as a simple way of verifying the identity of the original compound, assuming that a table of derivative MP values is available. Prior to the advent of spectroscopic analysis, such methods were widely used.\n\n"}
{"id": "53165275", "url": "https://en.wikipedia.org/wiki?curid=53165275", "title": "East Base", "text": "East Base\n\nEast Base on Stonington Island is the oldest American research station in Antarctica, having been commissioned by Franklin D. Roosevelt in 1939. The station was built as part of two US wintering expeditions – United States Antarctic Service Expedition (1939–1941) and Ronne Antarctic Research Expedition (1947–1948). The base covers from north to south and from east to west. The base was accorded the status of one of the Historic Sites and Monuments in Antarctica on 7 May 2004.\n\nThe Antarctic Service Expedition was the first government-funded expedition of Admiral Richard E. Byrd (his first two expeditions in 1928–1930 and 1933–1935 were privately funded). East Base was built using Army knockdown buildings and a crew of 23 led by Richard Black, after Admiral Byrd had to return to Washington on the USS \"Bear\". The war time pressures and pack-ice in the bay which prevented ship movement led to the evacuation of the base in 1941 by air.\n\nA private expedition led by Finn Ronne (second in command in the 1941 expedition) in 1947 ended with the participants' evacuation in 1948. The expedition crew included Jackie Ronne and Jennie Darlington, who became the first women to spend a winter in Antarctica. The base and all its equipment have since not been utilized, even though the British Antarctic Survey developed Base E in the vicinity of East Base. The British also occupied and modified the East Base during the construction of Base E. As of 2017, the base is frequented by tourists arriving on the continent.\n\n"}
{"id": "50363374", "url": "https://en.wikipedia.org/wiki?curid=50363374", "title": "Electrical Trades Union (United Kingdom)", "text": "Electrical Trades Union (United Kingdom)\n\nThe Electrical Trades Union (ETU) was a trade union representing electricians in the United Kingdom, much of its membership consisting of wiring fitters and telephone engineers.\n\nThe union was founded in 1889 with the merger of the Union of Electrical Operatives, a London-based union formed in 1868, and the Amalgamated Society of Telegraph and Telephone Construction Men, based in Manchester. Initially, the union had 570 members, most of whom were employees of the National Telephone Company. Its first part-time secretary, elected at the inaugural conference in 1890, was Dick Steadman.\n\nThe National Telephone Company's Brighton office was known for poor working conditions and, in 1891, an ETU branch was formed there, led by Alfred Ewer. After failed negotiations, the union began a strike, but this collapsed after five weeks. The remaining strikers were sacked, although the union helped them find them work elsewhere. In response, the union decided to appoint its first full-time general secretary; Arthur Walker was elected unopposed. He raised concerns that the union was unable to meet its commitments to out-of-work benefits to members. The became an immediate problem with a downturn in trade the following year; union membership peaked at 1,183 that year but then began to fall. The executive decided to institute a levy of three pennies per member per week in order to make up a shortfall, but this just led to more members leaving, and membership fell to only 402 in 1894. Walker was forced to resign after stealing union funds, and Steadman replaced him on a temporary basis.\n\nSteadman was unable to solve the union's problems, and membership reached an all-time low of 236 members at the end of 1895. Francis Sims was elected as a full-time general secretary, in a final attempt to turn its fortunes around. He undertook a tour of the UK, attempting to form new branches and strengthen existing ones. While this produced mixed results, membership began to recover, and a successful strike in Bolton in 1899, and an agreement signed with Sheffield Town Council in 1900 further improved his reputation. However, the following year, he too was found to have embezzled union funds, leading to his imprisonment for six months. On release, he tried to set up a breakaway union, the Electrical Wiremen's Union, but this failed to grow and was disbanded in 1903.\n\nAlfred Ewer was elected as Sims' replacement, and the union joined the Federation of Engineering and Shipbuilding Trades in 1906. Ewer's time in office was marked by conflict between the London-based executive and the provincial branches, culminating in a vote in 1907 to move the head office to Manchester. He disappeared in May 1907, and was later found to have emigrated to Australia without informing his wife or friends, having stolen £144 of union funds.\n\nJimmy Rowan, the union's national organiser, was elected as Ewer's replacement, and he ultimately served until 1941. He and oversaw rapid growth in the union; from 1,500 members in 1907, it grew to more than 80,000 on his retirement.\n\nIn 1918, the union balloted its members on joining the new Amalgamated Engineering Union, but this was not approved. The early 1920s proved a difficult time for the union, and Rowan negotiated a merger with the Transport and General Workers' Union, but this too was rejected by the membership. Walter Citrine was appointed as an assistant general secretary of the union in 1920, and transformed its finances, making the reputation which led to his later appointment as General Secretary of the Trades Union Congress.\n\nRowan was known for his anti-communism but, despite this, members of the Communist Party of Great Britain (CPGB) became prominent in the union under his leadership; by the 1950s, both General Secretary and General President were CPGB members.\n\nIn June 1961, the ETU was taken to court for \"conspiracy to defraud\" by the union leadership.\n\nAfter its leader Jock Byrne suffered a stroke, Frank Chapple became the union's leader in 1966. Unusually for a union leader at the time,Chapple espoused free-market thinking, and he aimed to rid his union of communists; his former union - the ETU had been run by communists. He was a \"reluctant loyalist\" to the Labour Party. The union went on to advocate nuclear power, privatisation of state-owned industries and membership of the European Union. \n\nIn July 1968, the ETU merged with the Plumbing Trades Union to form the Electrical, Electronic, Telecommunications and Plumbing Union.\n\n"}
{"id": "14163295", "url": "https://en.wikipedia.org/wiki?curid=14163295", "title": "Environmental impact of nuclear power", "text": "Environmental impact of nuclear power\n\nThe environmental impact of nuclear power results from the nuclear fuel cycle, operation, and the effects of nuclear accidents.\n\nThe greenhouse gas emissions from nuclear fission power are much smaller than those associated with coal, oil and gas, and the routine health risks are much smaller than those associated with coal. However, there is a \"catastrophic risk\" potential if containment fails, which in nuclear reactors can be brought about by overheated fuels melting and releasing large quantities of fission products into the environment. This potential risk could wipe out the benefits. The most long-lived radioactive wastes, including spent nuclear fuel, must be contained and isolated from the environment for a long period of time. On the other side, spent nuclear fuel could be reused, yielding even more energy, and reducing the amount of waste to be contained. The public has been made sensitive to these risks and there has been considerable public opposition to nuclear power.\n\nThe 1979 Three Mile Island accident and 1986 Chernobyl disaster, along with high construction costs, also compounded by delays resulting from a steady schedule of demonstrations, injunctions and political actions, caused by the anti-nuclear opposition, ended the rapid growth of global nuclear power capacity. A release of radioactive materials followed the 2011 Japanese tsunami which damaged the Fukushima I Nuclear Power Plant, resulting in hydrogen gas explosions and partial meltdowns classified as a Level 7 event. The large-scale release of radioactivity resulted in people being evacuated from a 20 km exclusion zone set up around the power plant, similar to the 30 km radius Chernobyl Exclusion Zone still in effect. But published works suggest that the radioactivity levels have lowered enough to now have only a limited impact on wildlife. In Japan, in July 2016, Fukushima Prefecture announced that the number of evacuees following the Great East Japan earthquake events, had fallen below 90,000, in part following the lifting of evacuation orders issued in some municipalities.\n\nNuclear power has at least three waste streams that may impact the environment:\n\nThe spent nuclear fuel from uranium-235 and plutonium-239 nuclear fission contains a wide variety of carcinogenic radionuclide isotopes such as strontium-90, iodine-131 and caesium-137, and includes some of the most long-lived transuranic elements such as americium-241 and isotopes of plutonium. The most long-lived radioactive wastes, including spent nuclear fuel, are usually managed to be contained and isolated from the environment for a long period of time. Spent nuclear fuel storage is mostly a problem in the United States, following a 1977 President Jimmy Carter prohibition to nuclear fuel recycling. France, Great Britain and Japan, are some of the countries which rejected the repository solution. Spent nuclear fuel is a valuable asset, not simply waste.\nDisposal of these wastes in engineered facilities, or repositories, located deep underground in suitable geologic formations is seen as the reference solution. The International Panel on Fissile Materials has said:\n\nIt is widely accepted that spent nuclear fuel and high-level reprocessing and plutonium wastes require well-designed storage for long periods of time, to minimize releases of the contained radioactivity into the environment. Safeguards are also required to ensure that neither plutonium nor highly enriched uranium is diverted to weapon use. There is general agreement that placing spent nuclear fuel in repositories hundreds of meters below the surface would be safer than indefinite storage of spent fuel on the surface.\nCommon elements of repositories include the radioactive waste, the containers enclosing the waste, other engineered barriers or seals around the containers, the tunnels housing the containers, and the geologic makeup of the surrounding area.\n\nThe ability of natural geologic barriers to isolate radioactive waste is demonstrated by the natural nuclear fission reactors at Oklo, Africa. During their long reaction period about 5.4 tonnes of fission products as well as 1.5 tonnes of plutonium together with other transuranic elements were generated in the uranium ore body. This plutonium and the other transuranics remained immobile until the present day, a span of almost 2 billion years. This is quite remarkable in view of the fact that ground water had ready access to the deposits and they were not in a chemically inert form, such as glass.\n\nDespite a long-standing agreement among many experts that geological disposal can be safe, technologically feasible and environmentally sound, a large part of the general public in many countries remains skeptical. One of the challenges facing the supporters of these efforts is to demonstrate confidently that a repository will contain wastes for so long that any releases that might take place in the future will pose no significant health or environmental risk.\n\nNuclear reprocessing does not eliminate the need for a repository, but reduces the volume, reduces the long term radiation hazard, and long term heat dissipation capacity needed. Reprocessing does not eliminate the political and community challenges to repository siting.\n\nThe countries that have made the most progress towards a repository for high-level radioactive waste have typically started with public consultations and made voluntary siting a necessary condition. This consensus seeking approach is believed to have a greater chance of success than top-down modes of decision making, but the process is necessarily slow, and there is \"inadequate experience around the world to know if it will succeed in all existing and aspiring nuclear nations\". Moreover, most communities do not want to host a nuclear waste repository as they are \"concerned about their community becoming a de facto site for waste for thousands of years, the health and environmental consequences of an accident, and lower property values\".\n\nIn a 2010 Presidential Memorandum, U.S. President Obama established the \"Blue Ribbon Commission on America’s Nuclear Future\". The Commission, composed of fifteen members, conducted an extensive two-year study of nuclear waste disposal. During their research the Commission visited Finland, France, Japan, Russia, Sweden, and the UK, and in 2012, the Commission submitted its final report. The Commission did not issue recommendations for a specific site but rather presented a comprehensive recommendation for disposal strategies. In their final report the Commission put forth seven recommendations for developing a comprehensive strategy to pursue. A major recommendation was that \"the United States should undertake an integrated nuclear waste management program that leads to the timely development of one or more permanent deep geological facilities for the safe disposal of spent fuel and high-level nuclear waste\".\n\nModerate amounts of low-level waste are through chemical and volume control system (CVCS). This includes gas, liquid, and solid waste produced through the process of purifying the water through evaporation. Liquid waste is reprocessed continuously, and gas waste is filtered, compressed, stored to allow decay, diluted, and then discharged. The rate at which this is allowed is regulated and studies must prove that such discharge does not violate dose limits to a member of the public (see radioactive effluent emissions).\n\nSolid waste can be disposed of simply by placing it where it will not be disturbed for a few years. There are three low-level waste disposal sites in the United States in South Carolina, Utah, and Washington. Solid waste from the CVCS is combined with solid radwaste that comes from handling materials before it is buried off-site.\n\nIn the United States environmental groups have said that uranium mining companies are attempting to avoid cleanup costs at disused uranium mine sites. Environmental remediation is required by many states after a mine becomes inactive. Environmental groups have filed legal objections to prevent mining companies from avoiding compulsory cleanups. Uranium mining companies have skirted the cleanup laws by reactivating their mine sites briefly from time-to-time. Letting the mines sites stay contaminated over decades increases the potential risk of radioactive contamination leeching into the ground according to one environmental group, the Information Network for Responsible Mining, which started legal proceedings about March 2013. Among the corporations holding mining companies with such rarely used mines is General Atomics.\n\nMost commercial nuclear power plants release gaseous and liquid radiological effluents into the environment as a byproduct of the Chemical Volume Control System, which are monitored in the US by the EPA and the NRC. Civilians living within of a nuclear power plant typically receive about 0.1 μSv per year. For comparison, the average person living at or above sea level receives at least 260 μSv from cosmic radiation.\n\nAll reactors in the United States are required by law to have a containment building. The walls of containment buildings are several feet thick and made of concrete and therefore can stop the release of any radiation emitted by the reactor into the environment. If a person is to worry about an energy source that releases large amounts of radiation into the environment, they should worry about coal-fired plants. \"The waste produced by coal plants is actually more radioactive than that generated by their nuclear counterparts. In fact, the fly ash emitted by a [coal] power plant—a by-product from burning coal for electricity—carries into the surrounding environment 100 times more radiation than a nuclear power plant producing the same amount of energy.\" Coal-fired plants are much more hazardous to people’s health than nuclear power plants as they release much more radioactive elements into the environment and subsequently expose people to greater levels of radiation than nuclear plants do. \"Estimated radiation doses ingested by people living near the coal plants were equal to or higher than doses for people living around the nuclear facilities. At one extreme, the scientists estimated fly ash radiation in individuals' bones at around 18 millirems (thousandths of a rem, a unit for measuring doses of ionizing radiation) a year. Doses for the two nuclear plants, by contrast, ranged from between three and six millirems for the same period. And when all food was grown in the area, radiation doses were 50 to 200 percent higher around the coal plants.\"\n\nThe total amount of radioactivity released through this method depends on the power plant, the regulatory requirements, and the plant's performance. Atmospheric dispersion models combined with pathway models are employed to accurately approximate the dose to a member of the public from the effluents emitted. Effluent monitoring is conducted continuously at the plant.\n\nA leak of radioactive water at Vermont Yankee in 2010, along with similar incidents at more than 20 other US nuclear plants in recent years, has kindled doubts about the reliability, durability, and maintenance of aging nuclear installations in the United States.\n\nTritium is a radioactive isotope of hydrogen that emits a low-energy beta particle and is usually measured in becquerels (i.e. atoms decaying per second) per liter (Bq/L). Tritium can be contained in water released from a nuclear plant. The primary concern for tritium release is the presence in drinking water, in addition to biological magnification leading to tritium in crops and animals consumed for food.\n\nTritium, the mass 3 isotope of hydrogen is deliberately created for thermonuclear weapons use, at government-owned reactors like Watts Bar, by irradiating lithium 6 with neutrons to fission i1. Light water reactors, the standard kind in the USA, generate small quantities of deuterium by neutron capture in the water. This consumes enough neutrons that the natural uranium needs enrichment to raise its fissile U-235 content from 0.72% to 3.6% for Pressurised Water Reactors. Canada's CANDU design uses \"heavy water\", deuterium oxide, and can use un-enriched uranium because deuterium captures so very few of the neutrons. So the rate of production of tritium from the small amount of deuterium in US reactors must be quite low.\n18 millilitres (ml) of water contain Avogadro's number of molecules, by definition, which is just over 6 times the 23rd power of 10, in other words 600 thousand million million million. That gives some idea of how small a unit the Becquerel is. A Litre, of course, is a thousand ml.\n\nLegal concentration limits have differed greatly from place to place (see table right). For example, in June 2009 the Ontario Drinking Water Advisory Council recommended lowering the limit from 7,000 Bq/L to 20 Bq/L. According to the NRC, tritium is the least dangerous radionuclide because it emits very weak radiation and leaves the body relatively quickly. The typical human body contains roughly 3,700 Bq of potassium-40. The amount released by any given nuclear plant also varies greatly; the total release for nuclear plants in the United States in 2003 was from nondetected up to 2,080 curies (77 TBq).\n\nUranium mining is the process of extraction of uranium ore from the ground. The worldwide production of uranium in 2009 amounted to 50,572 tonnes. Kazakhstan, Canada, and Australia are the top three producers and together account for 63% of world uranium production. A prominent use of uranium from mining is as fuel for nuclear power plants. The mining and milling of uranium present significant dangers to the environment.\n\n\"An average value for the thermal energy of coal is approximately 6150 kilowatt-hours(kWh)/ton. ... The thermal energy released in nuclear fission produces about 2 x 10E9 kWh/ton.\"\nIt follows that, for the same amount of energy, much less uranium needs to be mined than coal, cutting the environmental impacts of uranium mining on nuclear energy generation.\n\nIn 2010, 41% of the world's uranium production was produced by in-situ leaching, which uses solutions to dissolve the uranium while leaving the rock in place. The remainder was produced by conventional mining, in which the mined uranium ore is ground to a uniform particle size and then the uranium extracted by chemical leaching. The product is a powder of unenriched uranium, \"yellowcake,\" which is sold on the uranium market as UO. Uranium mining can use large amounts of water — for example, the Roxby Downs Olympic Dam mine in South Australia uses 35,000 m³ of water each day and plans to increase this to 150,000 m³ per day.\n\nThe Church Rock uranium mill spill occurred in New Mexico on July 16, 1979 when United Nuclear Corporation's Church Rock uranium mill tailings disposal pond breached its dam. Over 1,000 tons of solid radioactive mill waste and 93 millions of gallons of acidic, radioactive tailings solution flowed into the Puerco River, and contaminants traveled downstream to Navajo County, Arizona and onto the Navajo Nation. The accident released more radiation, although diluted by the 93 million gallons of mostly water and sulfuric acid, than the Three Mile Island accident that occurred four months earlier and was the largest release of radioactive material in U.S. history. Groundwater near the spill was contaminated and the Puerco rendered unusable by local residents, who were not immediately aware of the toxic danger.\n\nDespite efforts made in cleaning up cold war nuclear arms race uranium sites, significant problems stemming from the legacy of uranium development still exist today on the Navajo Nation and in the states of Utah, Colorado, New Mexico, and Arizona. Hundreds of abandoned mines, primarily used for the US arms race and not nuclear energy production, have not been cleaned up and present environmental and health risks in many communities. The Environmental Protection Agency estimates that there are 4000 mines with documented uranium production, and another 15,000 locations with uranium occurrences in 14 western states, most found in the Four Corners area and Wyoming. The \"Uranium Mill Tailings Radiation Control Act\" is a United States environmental law that amended the Atomic Energy Act of 1954 and gave the Environmental Protection Agency the authority to establish health and environmental standards for the stabilization, restoration, and disposal of uranium mill waste.\n\nNumerous studies have been done on possible effect of nuclear power in causing cancer. Such studies have looked for excess cancers in both plant workers and surrounding populations due to releases during normal operations of nuclear plants and other parts of the nuclear power industry, as well as excess cancers in workers and the public due to accidental releases. There is agreement that excess cancers in both plant workers and the surrounding public have been caused by accidental releases such as the Chernobyl accident. There is also agreement that some workers in other parts of the nuclear fuel cycle, most notably uranium mining – at least in past decades – have had elevated rates of cancer. However, numerous studies of possible cancers caused by nuclear power plants in normal operation have come to opposing conclusions, and the issue is a matter of scientific controversy and ongoing study.\n\nThere have been several epidemiological studies that say there is an increased risk of various diseases, especially cancers, among people who live near nuclear facilities. A widely cited 2007 meta-analysis by Baker \"et al.\" of 17 research papers was published in the \"European Journal of Cancer Care\". It offered evidence of elevated leukemia rates among children living near 136 nuclear facilities in the United Kingdom, Canada, France, United States, Germany, Japan, and Spain. However this study has been criticized on several grounds – such as combining heterogeneous data (different age groups, sites that were not nuclear power plants, different zone definitions), arbitrary selection of 17 out of 37 individual studies, exclusion of sites with zero observed cases or deaths, etc. Elevated leukemia rates among children were also found in a 2008 German study by Kaatsch \"et al.\" that examined residents living near 16 major nuclear power plants in Germany. This study has also been criticised on several grounds. These 2007 and 2008 results are not consistent with many other studies that have tended not to show such associations. The British Committee on Medical Aspects of Radiation in the Environment issued a study in 2011 of children under five living near 13 nuclear power plants in the UK during the period 1969–2004. The committee found that children living near power plants in Britain are no more likely to develop leukemia than those living elsewhere Similarly, a 1991 study for the National Cancer Institute found no excess cancer mortalities in 107 US counties close to nuclear power plants. However, in view of the ongoing controversy, the US Nuclear Regulatory Commission has requested the National Academy of Sciences to oversee a state-of-the-art study of cancer risk in populations near NRC-licensed facilities.\n\nA subculture of frequently undocumented nuclear workers do the dirty, difficult, and potentially dangerous work shunned by regular employees. The World Nuclear Association states that the transient workforce of \"nuclear gypsies\" – casual workers employed by subcontractors has been \"part of the nuclear scene for at least four decades.\" Existent labor laws protecting worker’s health rights are not properly enforced. A 15-country collaborative cohort study of cancer risks due to exposure to low-dose ionizing radiation, involving 407,391 nuclear industry workers showed significant increase in cancer mortality. The study evaluated 31 types of cancers, primary and secondary.\n\nNuclear power reactor accidents can result in a variety of radioisotopes being released into the environment. The health impact of each radioisotope depends on a variety of factors. Iodine-131 is potentially an important source of morbidity in accidental discharges because of its prevalence and because it settles on the ground. When iodine-131 is released, it can be inhaled or consumed after it enters the food chain, primarily through contaminated fruits, vegetables, milk, and groundwater. Iodine-131 in the body rapidly accumulates in the thyroid gland, becoming a source of beta radiation.\n\nThe 2011 Fukushima Daiichi nuclear disaster, the world's worst nuclear accident since 1986, displaced 50,000 households after radiation leaked into the air, soil and sea. Radiation checks led to bans of some shipments of vegetables and fish.\n\nProduction of nuclear power relies on the nuclear fuel cycle, which includes uranium mining and milling. Uranium workers are routinely exposed to low levels of radon decay products and gamma radiation. Risks of leukemia from acute and high doses of gamma radiation are well-known, but there is a debate about risks from lower doses. The risks of other hematological cancers in uranium workers have been examined in very few studies.\n\nIn terms of net radioactive release, the National Council on Radiation Protection and Measurements (NCRP) estimated the average radioactivity per short ton of coal is 17,100 millicuries/4,000,000 tons. With 154 coal plants in the United States, this amounts to emissions of 0.6319 TBq per year for a single plant.\n\nIn terms of dose to a human living nearby, it is sometimes cited that coal plants release 100 times the radioactivity of nuclear plants. This comes from NCRP Reports No. 92 and No. 95 which estimated the dose to the population from 1000 MWe coal and nuclear plants at 4.9 man-Sv/year and 0.048 man-Sv/year respectively (a typical Chest x-ray gives a dose of about 0.06 mSv for comparison). The Environmental Protection Agency estimates an added dose of 0.3 µSv per year for living within of a coal plant and 0.009 milli-rem for a nuclear plant for yearly radiation dose estimation. Nuclear power plants in normal operation emit less radioactivity than coal power plants.\n\nUnlike coal-fired or oil-fired generation, nuclear power generation does not directly produce any sulfur dioxide, nitrogen oxides, or mercury (pollution from fossil fuels is blamed for 24,000 early deaths each year in the U.S. alone). However, as with all energy sources, there is some pollution associated with support activities such as mining, manufacturing and transportation.\n\nA major European Union-funded research study known as ExternE, or Externalities of Energy, undertaken over the period of 1995 to 2005 found that the environmental and health costs of nuclear power, per unit of energy delivered, was €0.0019/kWh. This is lower than that of many renewable sources including the environmental impact caused by biomass use and the manufacture of photovoltaic solar panels, and was over thirty times lower than coals impact of €0.06/kWh, or 6 cents/kWh. However, the energy source of the lowest external costs associated with it was found to be wind power at €0.0009/kWh, which is an environmental and health impact just under half the price of Nuclear power.\n\nProponents argue that the problems of nuclear waste \"do not come anywhere close\" to approaching the problems of fossil fuel waste. A 2004 article from the BBC states: \"The World Health Organization (WHO) says 3 million people are killed worldwide by outdoor air pollution annually from vehicles and industrial emissions, and 1.6 million indoors through using solid fuel.\" In the U.S. alone, fossil fuel waste kills 20,000 people each year. A coal power plant releases 100 times as much radiation as a nuclear power plant of the same wattage. It is estimated that during 1982, US coal burning released 155 times as much radioactivity into the atmosphere as the Three Mile Island accident. The World Nuclear Association provides a comparison of deaths due to accidents among different forms of energy production. In their life-cycle comparison, deaths per TW-yr of electricity produced from 1970 to 1992 are quoted as 885 for hydropower, 342 for coal, 85 for natural gas, and 8 for nuclear. The figures include uranium mining, which can be a hazardous industry, with many accidents and fatalities.\n\nAs with all thermoelectric plants, nuclear power plants need cooling systems. The most common systems for thermal power plants, including nuclear, are:\n\n\nA 2011 study by the National Renewable Energy Laboratory determined that the median nuclear plant with cooling towers consumed 672 gallons of water per megawatt-hour, less than the median consumption of concentrating solar power (865 gal/MWhr for trough type, and 786 gal/MWhr for power tower type), slightly less than coal (687 gal/MWhr), but more than that for natural gas (198 gal/MWhr). Once-through cooling systems use more water, but less water is lost to evaporation. In the median US nuclear plant with once-through cooling, 44,350 gal/MWhr passes through the cooling system, but only 269 gal/MWhr (less than 1 percent) is consumed by evaporation.\n\nNuclear plants exchange 60 to 70% of their thermal energy by cycling with a body of water or by evaporating water through a cooling tower. This thermal efficiency is somewhat lower than that of coal-fired power plants, thus creating more waste heat.\n\nIt is possible to use waste heat in cogeneration applications such as district heating. The principles of cogeneration and district heating with nuclear power are the same as any other form of thermal power production. One use of nuclear heat generation was with the Ågesta Nuclear Power Plant in Sweden. In Switzerland, the Beznau Nuclear Power Plant provides heat to about 20,000 people. However, district heating with nuclear power plants is less common than with other modes of waste heat generation: because of either siting regulations and/or the NIMBY effect, nuclear stations are generally not built in densely populated areas. Waste heat is more commonly used in industrial applications.\n\nDuring Europe's 2003 and 2006 heat waves, French, Spanish and German utilities had to secure exemptions from regulations in order to discharge overheated water into the environment. Some nuclear reactors shut down.\n\nDuring the process of nuclear power generation, large volumes of water are used. The uranium fuel inside reactors undergoes induced nuclear fission which releases great amounts of energy that is used to heat water. The water turns into steam and rotates a turbine, creating electricity. Nuclear plants must collect around 600 gallons/MWh for this process, so the plants are built near bodies of water.\n\nA 2011 study by the National Renewable Energy Laboratory found that nuclear plants with cooling towers consumed 672 gal/MWhr. The water consumption intensity for nuclear was similar to that for coal electricity (687 gal/MWhr), lower than the consumption rates for concentrating solar power (865 gal/MWhr for CSP trough, 786 gal/MWhr for CSP tower), and higher than that of electricity generated by natural gas (198 gal/MWhr).\n\nWhen intaking water for cooling, nuclear plants, like all thermal power plants including coal, geothermal and biomass power plants, use special structures. Water is often drawn through screens to minimise to entry of debris. The problem is that many aquatic organisms are trapped and killed against the screens, through a process known as impingement. Aquatic organisms small enough to pass through the screens are subject to toxic stress in a process known as entrainment. Billions of marine organisms, such as fish, seals, shellfish, and turtles, essential to the food chain, are sucked into the cooling systems and destroyed.\n\nMany stages of the nuclear fuel chain — mining, milling, transport, fuel fabrication, enrichment, reactor construction, decommissioning and waste management — use fossil fuels, or involve changes to land use, and hence emit carbon dioxide and conventional pollutants. \nNuclear energy contributes a very small amount of emissions into the atmosphere which can cause many environmental problems such as global warming. Uranium is not burned in a nuclear power plant as coal is so there are no emissions from it. All of the waste that comes from the fission of uranium stays in the plant and is therefore able to be disposed of in a safe way in which the uranium is kept out of the environment. “About 73 percent of emissions-free electricity in the United States comes from nuclear plants.” Nuclear energy produces far less carbon dioxide than coal, 9 grams per kilowatt hour compared with 790–1017 grams per kilowatt hour for coal. Also, nuclear energy produces the same amount if not less greenhouse gasses than renewable resources.\nLike all energy sources, various life cycle analysis (LCA) studies have led to a range of estimates on the median value for nuclear power, with most comparisons of carbon dioxide emissions show nuclear power as comparable to renewable energy sources.\n\nTo better quantify and compare greenhouse gas emissions reported by researchers using many different assumptions and techniques, the US National Renewable Energy Laboratory is sponsoring meta-analysis studies using harmonization, in which reported life-cycle emissions are adjusted to consistent assumptions. The results commonly narrow the range of carbon emissions for a given energy source. The resulting 2012 study published in the \"Journal of Industrial Ecology\" analyzing CO2 life cycle assessment emissions from nuclear power determined that \"the collective LCA literature indicates that life cycle GHG emissions from nuclear power are only a fraction of traditional fossil sources and comparable to renewable technologies\". It also said that for the most common category of reactors, the light water reactor (LWR): \"Harmonization decreased the median estimate for all LWR technology categories so that the medians of BWRs, PWRs, and all LWRs are similar, at approximately 12 g CO2-eq/kWh\".\n\nWith this data in hand, therefore historically, nuclear power, primarily from ~1970 to 2013, is estimated to have prevented the atmospheric emission of 64 gigatonnes of CO2-equivalent.\n\nMany commentators have argued that an expansion of nuclear power would help combat climate change. Others have argued that it is one way to reduce emissions, but it comes with its own problems, such as risks related to severe nuclear accidents, war attacks on nuclear sites, nuclear terrorism and currently no generally accepted solution for the disposal of radioactive waste which needs to be heavily guarded for hundreds of thousands of years. These advocates also believe that there are better ways of dealing with climate change than investing in nuclear power, including the improved energy efficiency and greater reliance on decentralized and renewable energy sources.\n\nThere is also some uncertainty surrounding the future GHG emissions of nuclear power, which has to do with the potential for a declining uranium ore grade without a corresponding increase in the efficiency of enrichment methods. In a scenario analysis of future global nuclear development, as it could be effected by a decreasing global uranium market of average ore grade, the analysis determined that depending on conditions, median life cycle nuclear power GHG emissions could be between 9 and 110 g CO2-eq/kWh by 2050, with the latter figure regarded as an unrealistic \"worst-case scenario\" by the authors of the study.\n\nAlthough this future analyses deals with extrapolations for present Generation II reactor technology, the same paper also summarizes the literature on \"FBRs\"/Fast Breeder Reactors, of which two are in operation as of 2014 with the newest being the BN-800, for these reactors it states that the \"median life cycle GHG emissions ... [are] similar to or lower than [present] LWRs and purports to consume little or no uranium ore.\"\n\nThe worst accidents at nuclear power plants have resulted in severe environmental contamination. However, the extent of the actual damage is still being debated.\n\nIn March 2011 an earthquake and tsunami caused damage that led to explosions and partial meltdowns at the Fukushima I Nuclear Power Plant in Japan.\n\nRadiation levels at the stricken Fukushima I power plant have varied spiking up to 1,000 mSv/h (millisievert per hour), which is a level that can cause radiation sickness to occur at a later time following a one-hour exposure. Significant release in emissions of radioactive particles took place following hydrogen explosions at three reactors, as technicians tried to pump in seawater to keep the uranium fuel rods cool, and bled radioactive gas from the reactors in order to make room for the seawater.\n\nConcerns about the possibility of a large-scale release of radioactivity resulted in 20 km exclusion zone being set up around the power plant and people within the 20–30 km zone being advised to stay indoors. Later, the UK, France and some other countries told their nationals to consider leaving Tokyo, in response to fears of spreading nuclear contamination. \"New Scientist\" has reported that emissions of radioactive iodine and cesium from the crippled Fukushima I nuclear plant have approached levels evident after the Chernobyl disaster in 1986. On March 24, 2011, Japanese officials announced that \"radioactive iodine-131 exceeding safety limits for infants had been detected at 18 water-purification plants in Tokyo and five other prefectures\". Officials said also that the fallout from the Dai-ichi plant is \"hindering search efforts for victims from the March 11 earthquake and tsunami\".\n\nAccording to the Federation of Electric Power Companies of Japan, \"by April 27 approximately 55 percent of the fuel in reactor unit 1 had melted, along with 35 percent of the fuel in unit 2, and 30 percent of the fuel in unit 3; and overheated spent fuels in the storage pools of units 3 and 4 probably were also damaged\". As of April 2011, water is still being poured into the damaged reactors to cool melting fuel rods. The accident has surpassed the 1979 Three Mile Island accident in seriousness, and is comparable to the 1986 Chernobyl disaster. \"The Economist\" reports that the Fukushima disaster is \"a bit like three Three Mile Islands in a row, with added damage in the spent-fuel stores\", and that there will be ongoing impacts:\n\nYears of clean-up will drag into decades. A permanent exclusion zone could end up stretching beyond the plant’s perimeter. Seriously exposed workers may be at increased risk of cancers for the rest of their lives...\n\nJohn Price, a former member of the Safety Policy Unit at the UK's National Nuclear Corporation, has said that it \"might be 100 years before melting fuel rods can be safely removed from Japan's Fukushima nuclear plant\".\n\nIn the second half of August 2011, Japanese lawmakers announced that Prime Minister Naoto Kan would likely visit the Fukushima Prefecture to announce that the large contaminated area around the destroyed reactors would be declared uninhabitable, perhaps for decades. Some of the areas in the temporary radius evacuation zone around Fukushima were found to be heavily contaminated with radionuclides according to a new survey released by the Japanese Ministry of Science and Education. The town of Okuma was reported as being over 25 times above the safe limit of 20 millisieverts per year.\n\nInstead, 5 years later, the government expects to gradually lift the designation of some “difficult-to-return- zones”, a total area, from around 2021. Rain, wind and natural dissipation have removed radioactive contaminants, lowering levels, like at the central district of Okuma town, to 9 mSv/year, one-fifth the level of five years ago.\n\nAs of 2013 the 1986 Chernobyl disaster in the Ukraine was and remains the world's worst nuclear power plant disaster. Estimates of its death toll are controversial and range from 62 to 25,000, with the high projections including deaths that have yet to happen. Peer reviewed publications have generally supported a projected total figure in the low tens of thousands; for example an estimate of 16,000 excess cancer deaths are predicted to occur due to the Chernobyl accident out to the year 2065, whereas, in the same period, several hundred million cancer cases are expected from other causes (from International Agency for Research on Cancer published in the \"International Journal of Cancer\" in 2006). The IARC also released a press release stating \"To put it in perspective, tobacco smoking will cause several thousand times more cancers in the same population\", but also, referring to the numbers of different types of cancers, \"The exception is thyroid cancer, which, over ten years ago, was already shown to be increased in the most contaminated regions around the site of the accident\". The full version of the World Health Organization health effects report adopted by the United Nations, also published in 2006, included the prediction of, in total, no more of 4,000 deaths from cancer. A paper which the Union of concerned scientists took issue with the report, and they have, following the disputed linear no-threshold model (LNT) model of cancer susceptibility, instead estimated, for the broader population, that the legacy of Chernobyl would be a total of 25,000 excess cancer deaths worldwide. That places the total Chernobyl death toll below that of the worst dam failure accident in history, the Banqiao Dam disaster of 1975 in China.\n\nLarge amounts of radioactive contamination were spread across Europe due to the Chernobyl disaster, and cesium and strontium contaminated many agricultural products, livestock and soil. The accident necessitated the evacuation of the entire city of Pripyat and of 300,000 people from Kiev, rendering an area of land unusable to humans for an indeterminate period.\n\nAs radioactive materials decay, they release particles that can damage the body and lead to cancer, particularly cesium-137 and iodine-131. In the Chernobyl disaster, releases of cesium-137 contaminated land. Some communities, including the entire city of Pripyat, were abandoned permanently. One news source reported that thousands of people who drank milk contaminated with radioactive iodine developed thyroid cancer. The exclusion zone (approx. 30 km radius around Chernobyl) may have significantly elevated levels of radiation, which is now predominantly due to the decay of cesium-137, for around 10 half-lives of that isotope, which is approximately for 300 years.\n\nDue to the bioaccumulation of cesium-137, some mushrooms as well as wild animals which eat them, e.g. wild boars hunted in Germany and deer in Austria, may have levels which are not considered safe for human consumption. Mandatory radiation testing of sheep in parts of the UK that graze on lands with contaminated peat was lifted in 2012.\n\nIn 2007 The Ukrainian government declared much of the Chernobyl Exclusion Zone, almost , a zoological animal reserve. With many species of animals experiencing a population increase since human influence has largely left the region, including an increase in moose, bison and wolf numbers. However other species such as barn swallows and many invertebrates, e.g. spider numbers are below what is suspected. With much controversy amongst biologists over the question of, if in fact Chernobyl is now a wildlife reserve.\n\nThe SL-1, or Stationary Low-Power Reactor Number One, was a United States Army experimental nuclear power reactor which underwent a steam explosion and meltdown on January 3, 1961, killing its three operators; John Byrnes, Richard McKinley, and Richard Legg. The direct cause was the improper manual withdrawal of the central control rod, responsible for absorbing neutrons in the reactor core. This caused the reactor power to surge to about 20,000MW and in turn, an explosion occurred. The event is the only known fatal reactor accident in the United States and the first to occur in the world. The accident released about of iodine-131, which was not considered significant due to its location in a remote desert of Idaho. About of fission products were released into the atmosphere.\n\nRadiation exposure limits prior to the accident were 100 röntgens to save a life and 25 to save valuable property. During the response to the accident, 22 people received doses of 3 to 27 Röntgens full-body exposure. Removal of radioactive waste and disposal of the three bodies eventually exposed 790 people to harmful levels of radiation. The hands of the initial victims were buried separately from their bodies as a necessary measure in response to their radiation levels.\n\nNuclear power plants, uranium enrichment plants, fuel fabrication plants, and even potentially uranium mines are vulnerable to attacks which could lead to widespread radioactive contamination. The attack threat is of several general types: commando-like ground-based attacks on equipment which if disabled could lead to a reactor core meltdown or widespread dispersal of radioactivity; and external attacks such as an aircraft crash into a reactor complex, or cyber attacks. Terrorists could target nuclear power plants in an attempt to release radioactive contamination into the environment and community.\n\nNuclear reactors become preferred targets during military conflict and have been repeatedly attacked by military air strikes:\n\nThe United States 9/11 Commission has said that nuclear power plants were potential targets originally considered for the September 11, 2001 attacks. If terrorist groups could sufficiently damage safety systems to cause a core meltdown at a nuclear power plant, and/or sufficiently damage spent fuel pools, such an attack could lead to a widespread radioactive contamination. According to a 2004 report by the U.S. Congressional Budget Office, \"The human, environmental, and economic costs from a successful attack on a nuclear power plant that results in the release of substantial quantities of radioactive material to the environment could be great.\" An attack on a reactor’s spent fuel pool could also be serious, as these pools are less protected than the reactor core. The release of radioactivity could lead to thousands of near-term deaths and greater numbers of long-term fatalities.\n\nInsider sabotage occurs because insiders can observe and work around security measures. In a study of insider crimes, the authors repeatedly said that successful insider crimes depended on the perpetrators’ observation and knowledge of security vulnerabilities. Since the atomic age began, the U.S. Department of Energy’s nuclear laboratories have been known for widespread violations of security rules. A better understanding of the reality of the insider threat will help to overcome complacency and is critical to getting countries to take stronger preventative measures.\n\nResearchers have emphasized the need to make nuclear facilities extremely safe from sabotage and attacks that could release massive quantities of radioactivity into the environment and community. New reactor designs have features of passive safety, such as the flooding of the reactor core without active intervention by reactor operators. But these safety measures have generally been developed and studied with respect to accidents, not to the deliberate reactor attack by a terrorist group. However, the US Nuclear Regulatory Commission does now requires new reactor license applications to consider security during the design stage.\n\nFollowing the 2011 Fukushima I nuclear accidents there has been an increased focus on the risks associated with seismic activity and the potential for environmental radioactive release. Genpatsu-shinsai, meaning \"nuclear power plant earthquake disaster\" is a term which was coined by Japanese seismologist Professor Katsuhiko Ishibashi in 1997. It describes a domino effect scenario in which a major earthquake causes a severe accident at a nuclear power plant near a major population centre, resulting in an uncontrollable release of radiation in which the radiation levels make damage control and rescue impossible, and earthquake damage severely impedes the evacuation of the population. Ishibashi envisages that such an event would have a global impact seriously affecting future generations.\n\nThe 1999 Blayais Nuclear Power Plant flood was a flood that took place on the evening of December 27, 1999. It was caused when a combination of the tide and high winds from the extratropical storm Martin led to the sea walls of the Blayais Nuclear Power Plant in France being overwhelmed. The event resulted in the loss of the plant's off-site power supply and knocked out several safety-related systems, resulting in a event on the International Nuclear Event Scale. The incident illustrated the potential for flooding to damage multiple items of equipment throughout a plant, with the potential for radioactive release.\n\nAccording to Joshua M. Pearce of Michigan Technological University, on a global-scale a “sustainable nuclear power system” would entail: (i) dramatically improving efficient energy use and greenhouse gas emissions intensity by updating technology and functionality through the entire life cycle; (ii) improving nuclear security to reduce nuclear power risks and making sure that the nuclear industry can operate without large public nuclear accident insurance subsidies; (iii) eliminating all radioactive waste at the end of life and minimizing the environmental impact during the nuclear fuel cycle; and (iv) the nuclear industry must regain public trust or face obsolescence, as a diverse range of renewable energy technologies are quickly commercialized. Pearce also believes that the nuclear industry must address the issue of equity, both in the present and for later generations.\n\nNuclear decommissioning is the process by which a nuclear power plant site is dismantled so that it will no longer require measures for radiation protection. The presence of radioactive material necessitates processes that are occupationally dangerous, and hazardous to the natural environment, expensive, and time-intensive.\n\nMost nuclear plants currently operating in the US were originally designed for a life of about 30–40 years and are licensed to operate for 40 years by the US Nuclear Regulatory Commission. The average age of these reactors is 32 years. Therefore, many reactors are coming to the end of their licensing period. If their licenses are not renewed, the plants must go through a decontamination and decommissioning process. Many experts and engineers have noted there is no danger in these aged facilities, and current plans are to allow nuclear reactors to run for much longer lifespans.\n\nDecommissioning is an administrative and technical process. It includes clean-up of radioactivity and progressive demolition of the plant. Once a facility is fully decommissioned, no danger of a radiologic nature should persist. The costs of decommissioning are to be spread over the lifetime of a facility and saved in a decommissioning fund. After a facility has been completely decommissioned, it is released from regulatory control, and the licensee of the plant will no longer be responsible for its nuclear safety. With some plants the intent is to eventually return to \"greenfield\" status.\n"}
{"id": "39394223", "url": "https://en.wikipedia.org/wiki?curid=39394223", "title": "Estrima Birò", "text": "Estrima Birò\n\nBirò is a four-wheeled electric vehicle with two seats side by side. It is produced and distributed by Estrima, an Italian company located in Pordenone.\nIt belongs to the category of electric mopeds, in particular to light quadricycle mopeds.\nDepending on the country you are in it is legally comparable to a moped: its maximum speed is and can go up to with a small controller update. In Italy it can be driven from the age of 14 with a driver's license.\n\nEstrima is a startup company established in Pordenone in 2008 by its president, Matteo Maestri, with the intention of evolving the experience of the family corporation Brieda Srl into new emerging sectors.\nThe goal of Estrima is to reduce urban mobility problems such as traffic congestion and pollution by creating the smallest four-wheel electric vehicle on the market, the Birò.\nProduction of the vehicle started in June 2009, and in 2010 and 2011 Birò was the most registered electric vehicle in Italy among the categories of 2, 3 and 4 wheels.\nAt the end of 2012, Red Circle Investments, the company of the Rosso family, acquired 30% of the shares of Estrima.\n\nBirò is the smallest four-wheel electric vehicle on the market; it is wide and long. The vehicle has a reinforced dark-green roof and rear glass, a roomy compartment, a headrest and an armrest. The roof can be opened for ventilation. Available options are kit doors, radio pre-installation and an external trunk trolley.\n\nThe Birò has much larger windows than other microcars, resulting in better visibility for the driver and more appealing looks of the car.\n\nThe structure of the Birò is made of 3 mm thick tubular steel, protecting the driver and the passenger.\n\nThe very low center of gravity of Birò ensures stability and a good road grip.\n\nThe Birò is driven by two brushless 48 V electric motors, mounted directly in the rear wheels, producing a total of 4 kW of power. Thanks to the electric motors it has great pickup power that can be increased further by pulling the \"boost\" lever.\n\nFor the Birò two different kinds of batteries are available, a pure lead battery and a lithium battery. Depending on the type of batteries, the Birò has a range of about 40 or 70 km, varying according to temperature, driving style, travel type and weight.\n\n\nOn the maximum range, if a consumption indicator has been installed, the Ah consumed is the same as the kilometres travelled (25 Ah = 25 km).\n\nThe current used by the Birò for normal operations is restricted to a maximum of 80 A with a voltage ranging from 36 V to 53 V.\nA so-called boost function can be selected for uphill driving. When this function is used the current can temporarily be increased to a restricted maximum of 130 A.\n\n\nDuring normal operation the \"in drive\" voltage is 48 V (in 80 A mode) or 46 V (in 130 A mode).\n\nOn \"reserve\" range, the voltage is 45 V (80 A) and 43 V (130 A) so, at that point, one must find a way to quickly reach a point of recharge.\n\n"}
{"id": "3837308", "url": "https://en.wikipedia.org/wiki?curid=3837308", "title": "Extraterrestrial (TV program)", "text": "Extraterrestrial (TV program)\n\nExtraterrestrial (also Alien Worlds in the UK) is a British-American two-part television documentary miniseries, aired in 2005 in the UK by Channel 4, by the National Geographic Channel (as Extraterrestrial) in the US on Monday, May 30, 2005 and produced by Blue Wave Productions Ltd. The program focuses on the hypothetical and scientifically feasible evolution of alien life on extrasolar planets, providing model examples of two different fictional worlds, one in each of the series's two episodes.\n\nThe documentary is based on speculative collaboration of a group of American and British scientists, who were collectively commissioned by National Geographic. For the purposes of the documentary, the team of scientists divides two hypothetical examples of realistic worlds on which extraterrestrial life could evolve: A tidally locked planet orbiting a red dwarf star (dubbed \"Aurelia\") and a large moon (dubbed \"Blue Moon\") orbiting a gas giant in a binary star system. The scientific team of the series used a combination of accretion theory, climatology, and xenobiology to imagine the most likely locations for extraterrestrial life and most probable evolutionary path such life would take.\n\nThe \"Aurelia\" and \"Blue Moon\" concepts seen in the series were also featured in the touring exhibition The Science of Aliens.\n\nThe show's concept shares basic similarities with \"The Future is Wild\". Both series depict imaginary but scientifically-plausible ecosystems and the species that inhabit them, with commentary by scientists. The key difference is that in \"The Future is Wild\" the ecosystems represent the possible future evolution of life on planet Earth, while in \"Extraterrestrial\" they are designed from scratch based on possible conditions on extrasolar planets.\n\nAt the start of the documentary, the presenter and team of scientists draw attention to their reasons for speculating about life on extrasolar planets. Discoveries regarding extrasolar planets were first published in 1989 raising the prospect of whether life (as we know it or imagine it) could be supported on other planets. It is currently believed that for this to happen a planet must orbit in a relatively narrow band around its parent star, where temperatures are suitable for water to exist as a liquid. This region is called the habitable zone.\n\nThe most Earth-like exoplanets yet found, Gliese 667 Cc and Gliese 581g (disputed), have masses larger than Earth's and orbit red dwarf stars in the habitable zone.\n\nThe sensitivity of current detection methods makes it difficult for scientists to search for terrestrial planets smaller than this. To allow smaller bodies to be detected, NASA was studying a project called the Terrestrial Planet Finder (TPF), a two-telescope concept slated to begin launching around 2014. However, Congressional spending limits under House Resolution 20 passed on January 31, 2007 by the U.S. House of Representatives and February 14 by the U.S. Senate have all but canceled the program.\n\nPrior to the TPF's cancellation, astrophysicists had begun speculating about the best places to point the telescope in order to find Earth-like planets. Whereas life on Earth has formed around a stable yellow dwarf, solar twins are not as common in the galaxy as red dwarf stars (which have a mass of less than one-half that of the Sun and consequently emit less heat), or bigger, brighter blue giants. In addition, it is estimated that more than a quarter of all stars are at least binary systems, with as many as 10% of these systems containing more than two stars (trinary etc.)—unlike our own sun, which has no companion. Therefore, it may be prudent to consider how life might evolve in such environments. Such speculation may still be of use should a future planet-finding telescope be launched, and possibly for NASA's Kepler mission.\n\nThe first episode of the series focused on Aurelia, a hypothetical Earth-sized extrasolar planet orbiting a red dwarf star in our local area of the Milky Way.\n\nThe scientists on the project theorized that aiming the TPF at a red dwarf star might yield the best opportunities for seeing smaller planets. Due to the slow rate at which they burn hydrogen, red dwarfs have an enormous estimated lifespan, allowing plenty of time for life to evolve on surrounding planets. Also, red dwarfs are very common in the universe. Therefore, if they support habitable planets, it substantially increases the chances of finding life in the universe. However, being much dimmer than other stars, it will be harder to detect planetary systems around them. In addition, lower gravity would limit the potential size of a system. The discovery of Gliese 581g raises hopes of finding more red dwarf systems, including potentially habitable ones.\n\nHowever, the dwarf's smaller nature and fainter heat/light output would mean that such a planet would need to be particularly close to the star's surface. The cost of such an orbit would be that an Earth-sized body would become tidally locked. When this happens, the object presents the same face to its parent at all times as it orbits, just as the Moon does with the Earth (more technically, one sidereal day is exactly equal to one year for the orbiting body).\n\nTraditional scientific theories proposed that such a tidally locked planet might be incapable of holding on to an atmosphere. Having such a slow rotation would weaken the magnetic effect that protects the atmosphere from being blown away by solar wind (see Rare Earth hypothesis).\n\nNonetheless, the scientists employed by the programme decided to test the traditional assumptions for such a planet and start a model out for it from a protoplanetary disk through to its eventual death. Their estimations suggested such a planet could indeed hold on to its atmosphere, although with freakishly unusual results by Earth standards. Aurelia would be gravitationally locked. Due to this, Aurelia would not have seasons or a day/night cycle, as half would be in perpetual darkness, a permanent ice age. The other half would contain a giant, unending hurricane with permanent torrential rain at the point directly opposite the local star. Between these two zones, it would be suitable for life.\n\nThe giant hurricane might generate enormous waves in the ocean, which would migrate outwards. They would be wind-driven and would not reach the top of an ocean to the bottom, as a tsunami does. Nonetheless, waves as big and as devastating as those humans call freak waves might be regular. Simple bacterial and algal life would not be threatened.\n\nIn continued speculation, and assuming that there was land in this habitable zone, it would be likely to form large networks of river deltas and swampland, due to rain runoff from the nearby storm.\n\nAt the far end of assumptions about Aurelia were attempting to construct lifeforms based on Earthly evolutionary models and how ecosystems might develop. The scientists' assumptions included the idea that the long life of a red dwarf allows for evolution to fine-tune any ecosystem on the planet. The scientists involved in the project hypothesized that the vast majority, if not all, of extra-solar biology, will be carbon-based.\n\nThis assumption is often referred to by critics as carbon chauvinism, as it may be possible for life to form that is not based on carbon.\n\nFrom this carbon-based hypothesis, the scientific team assumed some form of staple photosynthesizing animal/plant combination would be the principal autotroph. They decided upon a plant-like creature called a Stinger Fan. It has five hearts and limited mobility. Its fan-like leaves trap the red dwarf star's energy to produce sugars. Its hearts pump them around its body.\n\nFeeding upon the Stinger Fans are six-legged semi-amphibious beaver-like creatures called Mudpods. They use their long, continually growing thumb claws to cut down a Stinger Fan and dam the river systems, creating artificial lagoons and swamps which provide safety from predators. Upon that animal, a large emu-like animal, the Gulphog, is the main predator. These 2-meter tall carnivores live socially in packs and display promising signs of intelligence. Finally, there is a second semi-amphibious creature called the Hysteria – a cross between a plague of tadpoles and piranha. These tiny, orange creatures can collect together (in a manner similar to slime molds) and form one huge super-organism, moving together up banks to paralyze and consume other animals. Sabian Slugs that live by the water can fall victim to the Hysteria, but it can take something as large as a Gulphog to satisfy them.\n\nThe planet's ecosystem suffers from a number of particular peculiarities, most notably evolutionary quirks to allow all living organisms to detect and avoid solar flares. Red dwarf stars are unstable and eject frequent solar flares. Such intense ultraviolet radiation is deadly to all carbon-based life forms as it breaks down the atomic bonds formed by organic compounds. The Gulphogs have adapted by having an ultraviolet light sensitive eye on top of their heads. Stinger Fans fold up to protect themselves. Mudpods have sensitive backs that can sense the ultraviolet rays. The Hysteria's protection is the water. However, the flare stage might only be when the red dwarfs are relatively young.\n\nThe second episode of the program focuses on a fictional moon called Blue Moon, which orbits an enormous gas giant that is itself orbiting a binary star system.\n\nThe Blue Moon is covered in life-giving water and an atmosphere so dense that enormous creatures hypothetically can take flight. The Blue Moon orbits a (a Jupiter-like planet that is cool enough to have visible rain clouds in its atmosphere) orbiting a close binary star system. The Blue Moon itself is roughly an earth mass but has an air pressure around three times that of Earth's at sea level.\n\nA distinguishing feature of Blue Moon is that it has no polar ice caps: the thick atmosphere keeps temperatures constant across the moon's surface. There is also a greenish haze over the moon from large carpets of floating moss and algae.\n\nThe denser atmosphere allows more massive creatures to remain airborne than on Earth. Skywhales, gargantuan whale-like animals which evolved away from the ocean into the air, fill the ecological niche this creates. Because of the increased muscle power from excess atmospheric oxygen, these creatures can have wingspans of ten meters and remain airborne their entire lives. They feed on the previously mentioned Air Moss. They evolved from seagoing animals into flying ones in one evolutionary leap.\n\nHigh levels of oxygen (30% of the atmosphere) push the atmosphere to the brink of spontaneous combustion during lightning storms. Carbon dioxide levels are thirty times higher than on Earth making the air clammy and warm.\n\nSkywhales are prey to the insect-like Stalkers, colony-living predators that have several different tasks. Scouts find skywhales and mark them with a special scent, then return to the nest to spread the word. Workers then swarm out in huge numbers, detecting the whale and working together to bring them down from the sky and kill them. Finally, there is a queen, who stays in the nest and constantly lays eggs that become new stalkers. This lifestyle is based on earth's hornets. The Stalkers are also prey, for the Pagoda branches are draped with the lethal webs of the plant-like ghost traps. Once a Stalker is caught in a ghost trap web, the carnivore uses its tentacles to lift its catch up into its mouth, to be digested by the acid in a primitive stomach.\n\nAs well as Skywhales, giant Kites also fly above the forest canopy. These parasol-like grazers can grow up to in diameter and still stay airborne. Their tethers help control their floating, while their jellyfish-like tentacles snatch Helibug larvæ from the water-filled sky pond. Helibugs have a trilaterally symmetrical body plan, with three eyes, three wings, three legs, three mouthparts and three tongues.\n\n70% of Blue Moon's land mass is coated with two main plant types, pagoda trees, and balloon plants. Pagoda trees interconnect with each other to allow them to grow tall. Their hollow leaves collect rainwater since the trees are too tall to draw it from the ground. Balloon plants release their seeds by filling them with hydrogen to float in the dense atmosphere, in a way similar to kelp on Earth.\n\nAt the foot of the pagoda forest there is a completely different ecosystem, governed by bioluminescent beings. A wide range of fungi, such as giant mushroom, helibugs and other creatures lurk in the shadows, waiting for any creature from the bottom to fall into their trap. The fungi species have an alert system that warns when the body of a creature falls dead, which soon ends up being digested by them.\n\nThe Blue Moon is threatened by mass wildfires that can wipe out entire pagoda forests. Balloon plants grow in the gaps resulting. The floating balloons released by the plants are full of explosive hydrogen, and when a fire hits, they explode like bombs, releasing seeds flying through the air. Skywhales and Kites will gain altitude until the fire ends. The ghost traps sway from branch to branch like monkeys using their tentacles. The Stalkers' escape strategy is unknown.\n\nChannel 4 provided a DVD release of the whole documentary (at a run length of 100 minutes) in January 2005.\n\n\n\n"}
{"id": "50065971", "url": "https://en.wikipedia.org/wiki?curid=50065971", "title": "Geothermal Development Company", "text": "Geothermal Development Company\n\nThe Geothermal Development Company (GDC), whose full name is Geothermal Development Company Limited, is a wholly owned parastatal of the government of Kenya. It is mandated to execute surface geothermal development, including prospecting, drilling, harnessing and selling steam to electricity generating companies for energy production and sale to the national grid.\n\nThe headquarters of GDC are located in the capital city of Nairobi, on Red Cross Road, in the neighborhood known as South C. The coordinates of GDC's headquarters are: 1°19'32.0\"S, 36°49'56.0\"E (Latitude:-1.325558; Longitude:36.832222).\n\nIn 2006, Kenya enacted the Energy Act No. 12 of 2006. The energy sector was unbundled into 5 sub-sectors: (a) generation (b) transmission (c) distribution (d) regulation and (e) policy. The country was heavily dependent on hydroelectric energy from the time of independence until the early 2000s. Due to unpredictable rainfall patterns, the levels of the country's rivers fell and Kenya underwent a marked reduction in electricity output in the 2003 to 2006 time frame.\n\nThe country has vast undeveloped geothermal potential along the Rift Valley, in excess of 10,000 MW. As at 2012, only 212.5MW of geothermal power was contributed to the national electric grid. As a response to these realities and to fulfill a pledge to install 5,000 MW of electricity by the year 2030, the Kenya Government, in 2008 incorporated Geothermal Development Company to carry out rapid geothermal exploration and drilling to allow independent power producers (IPPs) to build power stations and not only increase but diversify the national electricity grid.\n\nOutside development partners have offered help. GDC has plans to develop academic courses in geothermal energy at Kenyan universities, starting with courses at Dedan Kimathi University of Technology.\n\nIn November 2015, seven senior managers at GDC were terminated due to tendering irregularities. In August 2016, replacements for those terminated executives were advertised. In the 12 months ending 30 June 2015, the agency is reported to have made an aftertax profit of KES:1.6 billion (USD:16 million).\n\n\n"}
{"id": "34746739", "url": "https://en.wikipedia.org/wiki?curid=34746739", "title": "Green Plains Inc.", "text": "Green Plains Inc.\n\nGreen Plains Inc. is an American company based in Omaha, Nebraska that was founded in 2004. The company is the third largest ethanol fuel producer in North America (as of February 2012). It was reported in early 2012 that the company ships approximately one billion gallons of ethanol per year. The company employs approximately 640 people, and was founded by Barry Ellsworth. Green Plains Inc. is listed on the NASDAQ Stock Exchange as GPRE.\n\n\n\n"}
{"id": "6302572", "url": "https://en.wikipedia.org/wiki?curid=6302572", "title": "Hydrogen leak testing", "text": "Hydrogen leak testing\n\nHydrogen leak testing is the normal way in which a hydrogen pressure vessel or installation is checked for leaks or flaws. This usually involves charging hydrogen as a tracer gas into the device undergoing testing, with any leaking gas detected by hydrogen sensors. Various test mechanisms have been devised.\n\nIn the hydrostatic test, a vessel is filled with a nearly incompressible liquid – usually water or oil – and examined for leaks or permanent changes in shape. The test pressure is always considerably higher than the operating pressure to give a margin for safety, typically 150% of the operating pressure.\n\nIn the burst test, a vessel is filled with a gas and tested for leaks. The test pressure is always considerably more than the operating pressure to give a margin for safety, typically 200% or more of the operating pressure.\n\nThe helium leak test uses helium (the lightest inert gas) as a tracer gas and detects it in concentrations as small as one part in 10 million. The helium is selected primarily because it penetrates small leaks readily, is inert and will not react with the test piece while having a naturally low quantity in air making detection less complicated. It is possible to detect leaks as small as 5.10 Pa·m/s in vacuum mode and modern digital machines can detect 5.10 Pa·m/s in sniffing mode.\n\nUsually a vacuum inside the object is created with an external pump connected to the instrument. Alternatively helium can be injected inside the product while the product itself is enclosed in a vacuum chamber connected to the instrument. In this case, burst and leakage tests can be combined in one operation.\n\nDuring the hydrogen sensor test, the object is filled with a mixture of 5% hydrogen/ 95% nitrogen, (below 5.7% hydrogen) is non-flammable (ISO-10156). This is called typically a sniffing test. The handprobe connected to the microelectronic hydrogen sensors is used to check the object. An audiosignal increases in proximity of a leak. Detection of leaks go down to 5x10 cubic centimeters per second. Compared to the helium test, hydrogen is cheaper than helium, no need for a vacuum, the instrument could be cheaper but is not as sensitive as a helium leak detector so will not find smaller leaks.\n\nChemo-chromic hydrogen leak detectors are materials that can be proactively applied to a connection or fitting. In the event of a hydrogen leak, the chemo-chromic material changes color to alert an inspector that a leak is present. Chemo-chromic indicators can also be added to silicone tapes for hydrogen detection purposes.\n\n\n"}
{"id": "14882755", "url": "https://en.wikipedia.org/wiki?curid=14882755", "title": "IEC 61400-25", "text": "IEC 61400-25\n\nInternational standard IEC 61400-25 (Communications for monitoring and control of wind power plants, TC 88) provides uniform information exchange for monitoring and control of wind power plants. This addresses the issue of proprietary communication systems utilizing a wide variety of protocols, labels, semantics, etc., thus enabling one to exchange information with different wind power plants independently of a vendor. It is a subset of IEC 61400; a set of standards for designing wind turbines.\n\nThe IEC 61400-25 standard is a basis for simplifying the roles that the wind turbine and SCADA systems have to play. The crucial part of the wind power plant information, information exchange methods, and communication stacks are standardized. They build a basis to which procurement specifications and contracts could easily refer.\n\nThe standard has specified five mapping (IEC 61400-25-4) to communication protocol stacks in order to address the real wind power business needs for communication. The mappings specified in the part of IEC 61400-25 comprises a mapping to SOAP-based web services, OPC/XML-DA, IEC 61850-8-1 MMS, IEC 60870-5-104 and a mapping to DNP3.\n\nFor supporting the interoperability of the IEC 61400-25 standard, a user group was formed with a high degree of cooperation with user groups on IEC 61850 and DNP3. Users, manufacturers, service providers, and system integrators with interest in the specific issues on interoperatibility are recommend to address the IEC 61400-25 user group on http://www.USE61400-25.com/.\n\n\n"}
{"id": "39697749", "url": "https://en.wikipedia.org/wiki?curid=39697749", "title": "Kaiser effect (material science)", "text": "Kaiser effect (material science)\n\nThe Kaiser effect is a phenomenon observed in geology and material science that describes a pattern of acoustic emission or seismicity in a body of rock or other material subjected to repeated cycles of mechanical stress. In material that exhibits an initial seismic response under a certain load, the Kaiser effect describes the absence of acoustic emission or seismic events until that load is exceeded. The Kaiser effect results from discontinuities (fractures) created in material during previous steps that do not move, expand, or propagate until the former stress is exceeded. \n\nInduced seismicity associated with fluid pumping in boreholes and wells often exhibits the Kaiser effect, whereby seismicity may be observed shortly following an initial fluid injection, but further seismicity is limited if the fluid flow remains at a constant pressure. If the fluid pressure at the injection site is later increased, renewed seismicity may be observed due to the greater ease of fracturing caused by higher pore fluid pressure in the rock.\n\nThe Kaiser effect has also been observed in relation to recharge of magma chambers below active volcanic systems. \n\n"}
{"id": "21251691", "url": "https://en.wikipedia.org/wiki?curid=21251691", "title": "Keroselene", "text": "Keroselene\n\nKeroselene is a highly volatile derivative during the steam distillation of coal-tar. It was discovered in 1861 by Joshua Merrill. It has been used as a local anaesthetic.\n"}
{"id": "8375289", "url": "https://en.wikipedia.org/wiki?curid=8375289", "title": "Land Trust Alliance", "text": "Land Trust Alliance\n\nThe Land Trust Alliance, originally formed in 1982 as the Land Trust Exchange, is a national conservation organization representing more than 1,700 land trusts across the United States. The organization describes its objectives as working to increase the pace and quality of conservation by advocating favorable tax policies, training land trusts in best practices, and working to ensure the permanence of conservation in the face of continuing threats. The Land Trust Alliance is based in Washington, DC. As of December 2010, national, regional and local land trusts have conserved a total of 47 million acres (190,000 km) of land in America.\n\n"}
{"id": "22543360", "url": "https://en.wikipedia.org/wiki?curid=22543360", "title": "Low-carbon fuel standard", "text": "Low-carbon fuel standard\n\nA low-carbon fuel standard (LCFS) is a rule enacted to reduce carbon intensity in transportation fuels as compared to conventional petroleum fuels, such as gasoline and diesel. The most common low-carbon fuels are alternative fuels and cleaner fossil fuels, such as natural gas (CNG and LPG). The main purpose of a low-carbon fuel standard is to decrease carbon dioxide emissions associated with vehicles powered by various types of internal combustion engines while also considering the entire life cycle (\"well to wheels\"), in order to reduce the carbon footprint of transportation.\n\nThe first low-carbon fuel standard mandate in the world was enacted by California in 2007, with specific eligibility criteria defined by the California Air Resources Board (CARB) in April 2009 but taking effect in January 2011. Similar legislation was approved in British Columbia in April 2008, and by European Union which proposed its legislation in January 2007 and which was adopted in December 2008. The United Kingdom is implementing its Renewable Transport Fuel Obligation Program, which also applies the concept of low-carbon fuels.\n\nSeveral bills have been proposed in the United States for similar low-carbon fuel regulation at a national level but with less stringent standards than California. As of early 2010 none have been approved. The U.S. Environmental Protection Agency (EPA) issued its final rule regarding the expanded Renewable Fuel Standard (RFS2) for 2010 and beyond on February 3, 2010. This ruling, as mandated by the Energy Independence and Security Act of 2007 (EISA), included direct emissions and significant indirect emissions from land use changes.\n\nCalifornian Governor Arnold Schwarzenegger issued Executive Order S-1-07 on January 19, 2007 to enact a low-carbon fuel standard (LCFS). The LCFS requires oil refineries and distributors to ensure that the mix of fuel they sell in the Californian market meets the established declining targets for greenhouse gas (GHG) emissions measured in CO-equivalent grams per unit of fuel energy sold for transport purposes. The LCFS directive calls for a reduction of at least 10 percent in the carbon intensity of California's transportation fuels by 2020. These reductions include not only tailpipe emissions but also all other associated emissions from production, distribution and use of transport fuels within the state. Therefore, California LCFS considers the fuel's full life cycle, also known as the \"well to wheels\" or \"seed to wheels\" efficiency of transport fuels. The standard is also aimed to reduce the state’s dependence on petroleum, create a market for clean transportation technology, and stimulate the production and use of alternative, low-carbon fuels in California.\n\nThe LCFS is a mix of command and control regulation and emissions trading, as it will use market-based mechanisms that allow providers to choose how they will reduce emissions while responding to consumer demand. Some believe that oil companies could opt for several actions to comply. For example, they state that refiners and producers could improve the efficiency of the refineries and upstream production, or may purchase and blend more low-carbon ethanol into gasoline products, or purchase credits from electric utilities supplying low carbon electrons to electric passenger vehicles, or diversifying and selling low carbon hydrogen for use by vehicles as a product, or any new strategy as the standard is being designed. The California Global Warming Solutions Act of 2006 authorized the establishment of emissions trading in California, with rules to be adopted by 2010, and taking effect no later than January 2012.\n\nIn accordance to the California Global Warming Solutions Act of 2006 and the Governor's Directive, the California Air Resources Board is the agency responsible for developing the \"Low-Carbon Fuel Standard Program\", and it was directed to initiate the regulatory proceedings to establish and implement the LCFS.\" CARB identified the LCFS as an early action item with a regulation to be adopted and implemented by 2010. Also Executive Order S-1-07 ordered the California Environmental Protection Agency to coordinate activities between the University of California, the California Energy Commission and other state agencies to develop and propose a draft compliance schedule to meet the 2020 target.\n\nAs mandated by the Executive Order, a University of California team, led by Daniel Sperling of UC Davis and the late Alexander E. Farrell (UC Berkeley), developed two reports that established the technical feasibility of an LCFS, proposed the methodology to calculate the full life cycle GHG emissions from all fuels sold in the state, identified technical and policy issues, and provided a number of specific recommendations, thus providing an initial framework for the development of CARB's LCFS. This study was presented by Governor Schwarzenegger in May 2007 and they were the backbone of CARB's initial efforts to develop the LCFS, even though not all of the specific recommendations were incorporated in the final LCFS staff's proposed regulation.\n\nDuring 2008 and until the April 2009 LCFS ruling, CARB published in its website all technical reports prepared by its staff and collaborators regarding the definition and calculations related to the proposed LCFS regulation, conducted 16 public workshops, and also submitted its studies for external peer review. Before the April 23, 2009 ruling, the Board held a 45-day public hearing that received 229 comments, 21 of which were presented during the Board Hearing.\n\nAmong relevant and controversial comments submitted to CARB as public letters, on June 24, 2008, a group of 27 scientists and researchers from a number of universities and national laboratories, expressed their concerns arguing that there \"\"is not enough hard empirical data to base any sound policy regulation in regards to the indirect impacts of renewable biofuels production. The field is relative new, especially when compared to the vast knowledge base present in fossil fuel production, and the limited analyses are driven by assumptions that sometimes lack robust empirical validation\".\" With a similar opposing position, on October 23, 2008, a letter submitted to CARB by the New Fuels Alliance, representing more than two-dozen advanced biofuel companies, researchers and investors, questioned the Board intention to include indirect land use change (ILUC). In another public letter just before the ruling meeting, more than 170 scientists and economists sent a letter to CARB, urging it to account for GHG emissions from indirect land use change for biofuels and all other transportation fuels. They argued that \"...there are uncertainties inherent in estimating the magnitude of indirect land use emissions from biofuels, but assigning a value of zero is clearly not supported by the science.\"\n\nOn April 23, 2009, CARB approved the specific rules and carbon intensity reference values for the LCFS that will go into effect on January 1, 2011. The technical proposal was approved without modifications by a 9-1 vote, to set the 2020 maximum carbon intensity reference value for gasoline to 86 grams of carbon dioxide equivalent released per megajoule of energy produced. One standard was established for gasoline and the alternative fuels that can replace it, and a second similar standard is set for diesel fuel and its replacements. The regulation is based on an average declining standard of carbon intensity that is expected to achieve 16 million metric tons of greenhouse gas emission reductions by 2020. CARB expects the new generation of fuels to come from the development of technology that uses cellulosic ethanol from algae, wood, agricultural waste such as straw and switchgrass, and also natural gas from municipal solid waste. They also expect the standard to drive the availability of plug-in hybrid, battery electric and fuel-cell powered cars while promoting investment in infrastructure for electric charging stations and hydrogen fueling stations.\n\nThe ruling is controversial. Representatives of the US ethanol industry complained that this rule overstates the environmental effects of corn ethanol, and also criticized the inclusion of indirect effects of land-use changes as an unfair penalty to home-made corn ethanol because deforestation in the developing world is being tied to US ethanol production. The initial reference value set for 2011 for LCFS means that Mid-west corn ethanol will not meet the California standard unless current carbon intensity is reduced. Oil industry representatives complained that there is a cost associated to the new standard, as the LCFS will limit the use of corn ethanol blended in gasoline, thus leaving oil refiners with few available and viable options, such as sugarcane ethanol from Brazil, but this option means paying costly U.S. import tariffs. CARB officials and environmentalists reject such scenario because they think there will be plenty of time and economic incentive to developed inexpensive biofuels, hydrogen-based fuels, even ethanol from such cellulosic materials, or new ways to make ethanol out of corn with a smaller carbon footprint.\n\nBrazilian ethanol producers (UNICA), though they welcomed the ruling as they consider their sugarcane ethanol have passed a critical test and expect their biofuel to enter the California market in the future, UNICA also urged CARB to update the data and assumptions used, which according to them, is excessively penalizing their ethanol and is not reflecting the technology and agricultural practices currently in use in Brazil. UNICA disagreed with the assertion that indirect land-use changes can be accurately calculated with the current methodologies. Canadian officials also complained the standard could become an entry barrier to their Alberta oil sands, as producers will have to significantly reduce their emissions or purchase expensive credits from alternative energy producers in order for their non-conventional oil to be sold in California. They complained that the measure could be discriminating against Canadian oil sands crude as a high carbon intensity crude oil, while other heavy crude oils from other sources were not evaluated by CARB's studies.\n\nThe only Board member who voted against the ruling explained that he had \"hard time accepting the fact that we’re going to ignore the comments of 125 scientists\", referring to the letter submitted by a group of scientists questioning the indirect land use change penalty. \"They said the model was not good enough... to use at this time as a component part of such an historic new standard.\" CARB adopted only one main amendment to the staff proposal to bolster the standard review process, moving up the expected date of an expert working group to report on indirect land use change from January 2012 to January 2011. This change is expected to provide for a thoroughly review of the specific penalty for indirect land use change and correct it if possible. The CARB staff is also expected to report back to the Board on indirect impacts of other fuel pathways before the commencement of the standard in 2011.\n\nFuels were rated based on their carbon intensity, estimated in terms of the quantity of grams of carbon dioxide equivalent released for every megajoule of energy produced for their full life cycle, also referred to as the fuel pathway. Carbon intensity was estimated considering the direct carbon footprint for each fuel, and for biofuels the indirect land-use effects were also included. The resulting intensities for the main biofuels readily available are the following:\n\nThe LCFS standards established in CARB's rulemaking will be periodically reviewed. The first formal review will occur by January 1, 2011. Additional reviews are expected to be conducted approximately every three years thereafter, or as necessary. The 2011 review will consider the status of efforts to develop low carbon fuels, the compliance schedule, updated technical information, and provide recommendations on metrics to address the sustainable production of low carbon fuels.\n\nAccording to CARB's ruling, providers of transportation fuels must demonstrate that the mix of fuels they supply meet the LCFS intensity standards for each annual compliance period. They must report all fuels provided and track the fuels’ carbon intensity through a system of \"credits\" and \"deficits.\" Credits are generated from fuels with lower carbon intensity than the standard. Deficits result from the use of fuels with higher carbon intensity than the standard. A fuel provider meets its compliance obligation by ensuring that amount of credits it earns (or otherwise acquires from another party) is equal to, or greater than, the deficits it has incurred. Credits and deficits are generally determined based on the amount of fuel sold, the carbon intensity of the fuel, and the efficiency by which a vehicle converts the fuel into usable energy. Credits may be banked and traded within the LCFS market to meet obligations.\n\nTwo \"lookup tables\" (similar to the one above) and its carbon intensity values are part of the regulation, one for gasoline and another for diesel. The carbon intensity values can only be amended or expanded by regulatory amendments, and the Board delegated to the Executive Officer the responsibility to conduct the necessary rulemaking hearings and take final action on any amendments, other than amending indirect land-use change values included in the lookup tables.\n\nOn July 20, 2009, CARB published a Notice of Public Availability of modified text and availability of additional documents regarding the April 2009 rule making (Resolution 09-31), open for public comment until August 19. The supporting documents and information added to the rule making record include new pathways for Liquefied Natural Gas (LNG) from several sources, Compressed Natural Gas (CNG) from dairy digester biogas, biodiesel produced in California from used cooking oil, renewable diesel produced in California from tallow (U.S. sourced), and two additional new pathways for Brazilian sugarcane ethanol which reflect best practices already implemented in some regions of the country.\n\nThe two additional scenarios for sugarcane ethanol were requested by the Board in order to account for improved harvesting practices and the export of electricity from sugarcane ethanol plants in Brazil using energy from bagasse. These two scenarios are not to be considered average for all of Brazilian ethanol but specific cases when such practices are adopted in Brazil. Scenario 1 considers mechanized harvesting of cane which is gradually replacing the traditional practice of burning straw before harvesting cane, and the sale of electricity (co-generated) from power plants that are capable of exporting additional energy beyond that required for processing in the plant (co-product credit). Scenario 2 only considers the export of electricity (co-product) from power plants capable of producing the additional electricity for export. The assumptions or values for the baseline pathway published in February 2009 are the same, including the estimates of indirect land use change for all Brazilian sugarcane scenarios.\n\nOn December 2009 the Renewable Fuels Association (RFA) and Growth Energy, two U.S. ethanol lobbying groups, filed a lawsuit in the Federal District Court in Fresno, California, challenging the constitutionality of the California Low Carbon Fuel Standard (LCFS). The two organizations are arguing that the LCFS violates both the Supremacy Clause and the Commerce Clause of the US Constitution, and \"\"jeopardizes the nationwide market for ethanol\".\" In a press release both association announced that “\"If the United States is going to have a low carbon fuel standard, it must be based on sound science and it must be consistent with the U.S. Constitution\"...\" and that \"\"One state cannot dictate policy for all the others, yet that is precisely what California has aimed to do through a poorly conceived and, frankly, unconstitutional LCFS\".” Additional lawsuits against the California regulation were filed by refiners and truckers including Rocky Mountain Farmers Union; Redwood County Minnesota Corn and Soybean Growers; Penny Newman Grain, Inc.; Red Nederend; Fresno County Farm Bureau; Nisei Farmers League; California Dairy Campaign; National Petrochemical and Refiners Association; American Trucking Associations; Center for North American Energy Security; and the Consumer Energy Alliance. \nIn December 2011 a federal judge granted a preliminary injunction against the implementation of California's LCFS. In three separate rulings the judge rejected CARB's defense as he concluded that the state that the state acted unconstitutionally and the regulation \"“impermissibly treads into the province and powers of our federal government, reaches beyond its boundaries to regulate activity wholly outside of its borders.”\" CARB announced it intends to appeal the decision. The Ninth Circuit Court of Appeals issued a stay of the injunction on 23 April 2012 during the tendency of the litigation. In other words, the challenge to the constitutionality of the LCFS continues, but until it is resolved there is no bar on the CARB continuing to enforce the LCFS. (While the stay did not specifically authorize a return to the LCFS, CARB argued in its briefs before the Court that a stay would \"permit the LCFS to go back into effect as though the injunction had never been issued\". That is the approach currently taken by CARB and it continues to refine carbon intensity standards and applicability).\n\nIn 2011, a provision was added to the LCFS that allows refiners to receive credits for the deployment of innovative crude production technologies, such as carbon capture and sequestration or solar steam generation. Solar thermal enhanced oil recovery is a form of enhanced oil recovery (EOR), which is key to harvesting California’s heavy crude. Currently, California uses EOR to help produce about 60% of its crude output. By using solar power instead of natural gas to create steam for EOR, solar steam generation reduces the amount of emissions produced during oil extraction, thus lowering the overall carbon intensity of crude. California currently has two solar EOR projects in operation, one in McKittrick, operated by LINN Energy (formerly Berry Petroleum) using enclosed trough technology from GlassPoint Solar, and another in Coalinga operated by Chevron Corporation using BrightSource Energy power tower technology.\n\nCARB is currently considering an amendment to allow upstream operators to receive credits for deploying innovative crude production technologies.\n\nIn 2015, California's LCFS was re-adopted in order to address some of the issues in the original proposed standard. A number of the changes were made, including updated crude provisions, a new model used to be used to calculate carbon intensity, the establishment of a \"Credit Clearance\" process that would take effect at the end of the year should the LCFS credit market become too competitive, and other provisions.\n\nIn May 2016, the Seneca Solar Project became the first facility to start earning LCFS credits. Located in the North Midway Sunset oil field in Taft, Kern County, California, this facility met the threshold of 0.10gCO2/ MJ carbon intensity (CI) reduction.\n\nShortly after that, in August 2016, the SB 32 was passed, which changed the target for green house gas (GHG) reduction to 40% below the levels in 1990, to be achieved by 2030. It is anticipated that this will lead to the tightening of LCFS standards from 2020 all the way through 2030.\n\nIn November 2017, GlassPoint announced a partnership with Aera Energy to bring its enclosed trough technology to the South Belridge Oil Field, near Bakersfield, California. When done, the facility will be California’s largest solar EOR field. It is projected to produce approximately 12 million barrels of steam per year through a 850MW thermal solar steam generator. It will also cut carbon emissions from the facility by 376,000 metric tons per year.\n\nUsing California's LCFS as a model, several bills have been presented to establish a national low-carbon fuel standards at the federal level.\n\nSenators Barbara Boxer, Dianne Feinstein, and future President Barack Obama introduced in 2007 competing bills with varying versions of California's LCFS.\n\n\nIn March 2009, the Waxman-Markey Climate Bill was introduced in the U.S. House Committee on Energy and Commerce, and it has been praised by top Obama Administration officials. The bill requires a slightly higher targets for reductions in emissions of carbon dioxide, methane, and other greenhouse gases than those proposed by President Barack Obama. The bill proposed a 20-percent emissions reduction from 2005 levels by 2020 (Obama had proposed a 14 percent reduction by 2020). Both plans aim to reduce emissions by about 80 percent by 2050. The Climate Change Bill was approved by the U.S. House of Representatives on June 26, 2009. As approved, emissions would be cut 17 percent from 2005 levels by 2020, and 83 percent by 2050.\n\nThe Energy Independence and Security Act of 2007 (EISA) established new renewable fuel categories and eligibility requirements, setting mandatory life cycle greenhouse gas emissions thresholds for renewable fuel categories, as compared to those of average petroleum fuels used in 2005. EISA definition of life cycle GHG emissions explicitly mandated the U.S. Environmental Protection Agency (EPA) to include \"\"direct emissions and significant indirect emissions such as significant emissions from land use changes\".\"\n\nOn May 5, 2009 the U.S. Environmental Protection Agency (EPA) released its notice of proposed rulemaking for implementation of the 2007 modification of the Renewable Fuel Standard (RFS). The draft of the regulations was released for public comment during a 60-day period. EPA's proposed regulations also included the carbon footprint from indirect land-use changes, which, as CARB's ruling, caused controversy among ethanol producers. On the same day, President Barack Obama signed a Presidential Directive with the aim to advance biofuels research and improve their commercialization. The Directive established a Biofuels Interagency Working Group which has the mandate to come up with policy ideas for increasing investment in next-generation fuels, such as cellulosic ethanol, and for reducing the environmental footprint of growing biofuels crops, particularly corn-based ethanol.\n\nAn amendment was introduced in the House Appropriations Committee during the discussion of the fiscal 2010 Interior and Environment spending bill, aimed to prohibit EPA to consider indirect land-use changes in the RFS2 ruling for five years. This amendment was rejected on June 18, 2009 by a 30 to 29 vote. A similar amendment to the Waxman-Markey Climate Bill was introduced in the U.S. House Committee on Energy and Commerce. The Climate Bill was approved by the U.S. House of Representatives with a vote of 219 to 212, and included a mandate for EPA to exclude any estimation of international indirect land use changes due to biofuels for a five-year period for the purposes of the RFS2. During this period, more research is to be conducted to develop more reliable models and methodologies for estimating ILUC. By 2010 the bill is awaiting approval by the U.S. Senate.\n\nOn February 3, 2010, EPA issued its final rule regarding the expanded Renewable Fuel Standard (RFS2) for 2010 and beyond. The final rule revises the annual renewable fuel standards, and the required renewable fuel volume continues to increase reaching 36 billion gallons (136.3 billion liters) by 2022. For 2010, EISA set a total renewable fuel standard of 12.95 billion gallons (49.0 billion liters). This total volume, presented as a fraction of a refiner's or importer's gasoline and diesel volume, must be renewable fuel. The final 2010 standards set by EPA are shown in the table in the right side.\n\nAs mandated by law, and in order to establish the fuel category for each biofuel, EPA included in its modeling direct emissions and significant indirect emissions such as emissions from land use changes related to the full lifecycle. EPA's modeling of specific fuel pathways incorporated comments received through the third-party peer review process, and data and information from new studies and public comments. EPA's analysis determined that both ethanol produced from corn starch and biobutanol from corn starch comply with the 20% GHG emission reduction threshold required to classify as a renewable fuel. EISA grandfathered existing U.S. corn ethanol plants, and only requires the 20% reduction in life cycle GHG emissions for any renewable fuel produced at new facilities that commenced construction after December 19, 2007.\n\nEPA also determined that ethanol produced from sugarcane, both in Brazil and Caribbean Basin Initiative countries, complies with the applicable 50% GHG reduction threshold for the advanced fuel category. Both diesel produced from algal oils and biodiesel from soy oil and renewable diesel from waste oils, fats, and greases complies with the 50% GHG threshold for the biomass-based diesel category. Cellulosic ethanol and cellulosic diesel (based on currently modeled pathways) comply with the 60% GHG reduction threshold applicable to cellulosic biofuels.\n\nThe following table summarizes the mean GHG emissions estimated and the range of variations considering that the main source of uncertainty in the life cycle analysis is the emissions related to international land use change GHG emissions.\n\nUNICA, a Brazilian ethanol producers association, welcomed the ruling and commented that they hope the classification of Brazilian sugarcane ethanol as an advanced biofuel will contribute to influence those who seek to lift the trade barriers imposed against clean energy, both in the U.S. and the rest of the world. EPA's final ruling is expected to benefit Brazilian producers, as the blending mandate requires an increasing quota of advanced biofuels, which is not likely to be fulfill with cellulosic ethanol, and then it would force blenders to import more Brazilian sugarcane-based ethanol, despite the existing 54¢ per gallon tariff on ethanol imported directly from Brazil.\n\nIn the case of corn-based ethanol, EPA said that manufacturers would need to use “advanced efficient technologies” during production to meet RSF2 limits. The U.S. Renewable Fuels Association also welcomed the ruling, as ethanol producers \"\"require stable federal policy that provides them the market assurances they need to commercialize new technologies.\" However, they complained that \"EPA continues to rely on oft-challenged and unproven theories such as international indirect land use change to penalize U.S. biofuels to the advantage of imported ethanol and petroleum.\"\"\n\nEleven U.S. Northeast and Mid-Atlantic states have committed to analyzing a single low-carbon fuel standard for the entire region, driving commercialization and creating a larger market for fuels with low carbon intensity. The standard is aimed to reduce greenhouse gas emissions from fuels for vehicles and other uses, including fuel used for heating buildings, industrial processes, and electricity generation. Ten of these states are members of the Regional Greenhouse Gas Initiative (RGGI). California Air Resources Board (CARB) staff has been coordinating with representatives of these States. The states developing a regional LCFS are Connecticut, Delaware, Maine, Maryland, Massachusetts, New Hampshire, New Jersey, New York, Pennsylvania, Rhode Island, and Vermont.\n\nA Memorandum of Understanding concerning the development of the regional low carbon fuel standard program was signed by the Governors of each State on December 30, 2009, committing the states to an economic analysis of the program, consultation with stakeholders before ruling, and a draft model rule by early 2011.\n\nThe Legislative Assembly of British Columbia, Canada, approved in April 2008 the Renewable and Low Carbon Fuel Requirements Act, which mandates fuel suppliers in B.C. to sell gasoline and diesel containing 5% and 4% percent renewable fuels, respectively, by 2010, and allows the provincial government to set thresholds for the carbon intensity of fuels, taking into account their entire carbon footprint. The RLCFR Act also provides flexibility for regulated fuel suppliers to meet their obligations as they may receive notional transfers of renewable fuels and of attributable greenhouse gas emissions.\n\nThe EU has mainly acted to mitigate road transport greenhouse emissions mainly through its voluntary agreement on CO2 emissions from cars and subsequently through Regulation 443/2009 which sets mandatory CO2 emission limits for new cars. The EU promoted the use of biofuels through the directive on the promotion of the use of biofuels and other renewable fuels for transport (2003/30/EC), also known as Biofuel Directive, which calls for countries across the EU aiming at replacing 5,75% of all transport fossil fuels (petrol and diesel) with biofuels by 2010. None of these regulations, however, were based on carbon intensity of fuel. Fuel quality standards in the European Union are regulated by Directive 98/70/EC.\n\nOther European countries have their own mandates limiting consumption of conventional fossil fuels by substituting to cleaner fuels in order to reduce greenhouse gas emissions, such as the United Kingdom Renewable Transport Fuel Obligation Program (RTFO), requiring transport fuel suppliers to ensure that 5% of all road vehicle fuel comes from sustainable renewable sources by 2010.\n\nThe Renewable Transport Fuel Obligation is similar to California's LCFS in some aspects. Biofuel suppliers are required to report on the level of carbon savings and sustainability of the biofuels they supplied in order to receive Renewable Transport Fuel Certificates (RTFCs). Suppliers have to report on both the net GHG savings and the sustainability of the biofuels they supply according to the appropriate sustainability standards of the feedstocks from which they are produced and any potential indirect impacts of biofuel production, such as indirect land-use change or changes to food and other commodity prices that are beyond the control of individual suppliers. Suppliers that do not submit a report will not be eligible for RTFO certificates.\n\nCertificates can be claimed when renewable fuels are supplied and fuel duty is paid on them. At the end of the obligation period, these certificates may be redeemed to the RTFO Administrator to demonstrate compliance. Certificates can be traded, therefore, if obligated suppliers don't have enough certificates at the end of an obligation period they have to 'buy-out' the balance of their obligation by paying a buy-out price. The buy out price will be 15 pence per litre in the first two years.\n\nOn January 31, 2007, the European Commission (EC) proposed new standards for transport fuels to reduce full life cycle emissions by up to 10 percent between 2011 and 2020 This was three weeks after the California LCFS Directive was announced. The EU proposal aimed to encourage the development of low-carbon fuels and biofuels, considering reductions in greenhouse gas emissions caused by the production, transport and use of the suppliers fuels.\n\nOn December 2008 the European Parliament, among other measures to address climate change in the European Union, approved amendments to the fuel quality directive (98/70) as well as replacing the Biofuels Directive with a Directive on the promotion of Renewable Energy Sources as proposed by the European Commission. The revision of Directive 98/07/EC introduced a mechanism to monitor and reduce greenhouse gas emissions from the use of road transport fuels, requiring fuel suppliers to reduce GHG emissions by up to 10 percent by 2020 on a life cycle basis. Regarding land use changes, the EC was ordered to \"\"develop a concrete methodology to minimise greenhouse gas emissions caused by indirect land use changes\".\" The fuel directive includes provisions to promote sustainable biofuels which minimized the impacts of ILUC. The approved goal of 10 percent reduction in greenhouse gas emissions can be achieved in several ways, and not exclusively from low-carbon fuels:\nThe Commission is continuing development of the EU LCFS, in particular on the methodology for fossil fuel emissions, has recently consulted on various aspects of the implementation and responses have been published. Further work is underway to address Indirect Land Use Change emissions. Two modelling exercises and a model comparison exercise are being carried out to better understand the scale and nature of indirect land use change due to biofuels before the Commission makes proposals to address it.\n\nOn June 10, 2010, the EC adopted guidelines explaining how the Renewable Energy Directive (RED) should be implemented, as the Directive came into effect in December 2010. Three measures focus on the criteria for sustainability of biofuels and how to control that only sustainable biofuels are used in the EU. First, the Commission is encouraging E.U. nations, industry and NGOs to set up voluntary schemes to certify biofuel sustainability. Second, the EC laid down the rules to protect untouched nature, such as forests, wetlands and protected areas, and third, a set of rules to guarantee that biofuels deliver substantial reduction in well-to-wheel greenhouse gas emissions.\n\nThe EC decided to request governments, industry and NGO's to set up voluntary schemes to certify biofuel sustainability for all types of biofuels, including those imported into the EU. According to the EC, the overall majority of biofuels are produced in the EU, and for 2007, only 26% of biodiesel and 31% of bioethanol consumed in the EU was imported, mainly from Brazil and the United States. The Commission set standards that must be met for these schemes to gain EU recognition. One of the main criteria is that the certification scheme must be interdependently audited and fraud-resistant. Auditors must check the whole production chain, from the farmer and mill to the filling station (well-to-wheel life cycle). Auditors must check all the paper and inspect a sample of the farmers, mills and traders, and also whether the land where the feedstock for the ethanol is produced has been indeed farm land before and not a tropical forest or protected area. The certificates are to guarantee that all the biofuels sold under the label are sustainable and produced under the criteria set by the Renewable Energy Directive. Several private certification systems originally designed for sustainability more generally have adapted their standards to qualify for recognition under the Renewable Energy Directive, including the Roundtable on Sustainable Biomaterials and Bonsucro.\n\nEnvironmental groups complained the measures \"are too weak to halt a dramatic increase in deforestation.\" According to Greenpeace \"Indirect land use change impacts of biofuels (ILUC) production still are not properly addressed\" because if not properly regulated, \"ILUC impacts will continue causing major biodiversity loss and more greenhosuse gas emissions.\" On the other hand, industry representatives welcomed the introduction of a certification system and some dismissed the concerns regarding the lack of criteria about ILUC.UNICA, the Brazilian ethanol producers association welcome the rules more cautiously, as they consider \"\"that gaps in the rules needed to be filled in so the \"industry has a clear framework within which to operate\".\"\" Some other industry organizations also said that further clarification is needed in order to implement the Renewable Energy Directive.\n\nThe EC clarified that it would publish a report on the impacts of indirect land use by the end of 2010, as requested in the Renewable Energy Directive and on the basis of recently released reports that suggest that biofuels are saving greenhouse gas emissions.\n\nThe rules set by the Commission establish that biofuels should not be made from feedstocks from tropical forests or recently deforested areas, drained peatland, wetland or highly biodiverse areas. The corresponding Communication explains how this should be assessed and as an example, it makes it clear that the conversion of a forest to a palm oil plantation would not meet the sustainability requirements.\n\nThe Commission reiterated that Member States have to meet binding, national targets for renewable energy and that only those biofuels with high greenhouse gas emission savings count for the national targets. The corresponding Communication explains how to make the calculation, which not only includes carbon dioxide (CO), but also methane (CH) and nitrous oxide (NO), both stronger greenhouse gases than CO. Biofuels must deliver greenhouse gas savings of at least 35% compared to fossil fuels, rising to 50% in 2017 and to 60%, for biofuels from new plants, in 2018.\n\n"}
{"id": "47965690", "url": "https://en.wikipedia.org/wiki?curid=47965690", "title": "Magnetically assisted slip casting", "text": "Magnetically assisted slip casting\n\nMagnetically-assisted slip casting is a manufacturing technique that uses anisotropic stiff nanoparticle platelets in a ceramic, metal or polymer functional matrix to produce layered objects that can mimic natural objects such as nacre. Each layer of platelets is oriented in a different direction, giving the resulting object greater strength. The inventors claimed that the process is 10x faster than commercial 3D printing. The magnetisation and orientation of the ceramic platelets has been patented.\n\nThe technique involves pouring a suspension of magnetized ceramic micro-plates. Pores in the plaster mold absorb the liquid from the suspension, solidifying the material from the outside in. The particles are subjected to a strong magnetic field as they solidify that causes them to align in one direction. The field’s orientation is changed at regular intervals, moving the plates still in suspension, without disturbing already-solidified plates. By varying the composition of the suspension and the direction of the platelets, a continuous process can produce multiple layers with differing material properties in a single object. The resulting objects can closely imitate their natural models.\n\nResearchers produced an artificial tooth whose microstructure mimicked that of a real tooth. The outer layers, corresponding to enamel, were hard and structurally complex. The outer layers contained glass nanoparticles and aluminium oxide plates were aligned perpendicular to the surface. After the outer layers hardened, a second suspension was poured. It contained no glass, and the plates were aligned horizontally to the surface of the tooth. These deeper layers were tougher, resembling dentine. The tooth was then cooked at 1,600 degrees to compact and harden the material — a process known as sintering. The last step involved filling remaining pores with a synthetic monomer used in dentistry, which polymerizes after treatment. Hardness and durability approximated that of both the enamel and dentine of a tooth.\n\n"}
{"id": "29820707", "url": "https://en.wikipedia.org/wiki?curid=29820707", "title": "MigrantWatch", "text": "MigrantWatch\n\nMigrantWatch is a citizen science non-governmental organisation project in India for collection of information about bird migration. The organisation was conceived in July 2007 and is coordinated by the Science Programme of the National Centre for Biological Sciences, in association with Indian Birds journal.\n\nThe goal of the MigrantWatch programme is to collect information on the arrival, presence and departure of migrant birds that spend the winter in India and to assess any changes that occur in the timing of migration. The MigrantWatch program provides a website where registered members can upload observations of migratory bird species,and access all the sighting records and maps with data plotted. \n\nIn the first year, the program targeted nine species of migratory birds:\n\n\nSubsequently, the list was increased to 30 migratory species.\n"}
{"id": "34581099", "url": "https://en.wikipedia.org/wiki?curid=34581099", "title": "Montgrí, Medes Islands and Baix Ter Natural Park", "text": "Montgrí, Medes Islands and Baix Ter Natural Park\n\nMontgrí, Medes Islands and Baix Ter Natural Park () is a natural park located in the province of Girona, Catalonia, Spain. The park was established in 2010 and encompasses the Medes Islands, the Montgrí Massif and the mouth of the Ter river, covering a land area of and a marine protected area of . It stretches over the municipalities of L'Escala, Torroella de Montgrí, Pals, Bellcaire d'Empordà, Palau-sator, Ullà, Fontanilles and Gualta.\n\n"}
{"id": "1584230", "url": "https://en.wikipedia.org/wiki?curid=1584230", "title": "NASU Institute of Electrodynamics", "text": "NASU Institute of Electrodynamics\n\nNASU Institute of Electrodynamics (IED) () is a Ukraine leading science institution in field of electrical engineering, thermal power (heat energy), and research of electrodynamics located in Kiev, Ukraine as a part of the Ukrainian Academy of Sciences. It is well known for the prominent achievements in the field of computer science and electronics, made in early 1950s by Sergei Alekseyevich Lebedev.\n\nThe institute was established in 1947 on the basis of electrical engineering department of the NASU Energy Institute as the NASU Institute of Electrical Engineering. In 1963 it was renamed as the NASU Institute of Electrodynamics.\n\n\n\n"}
{"id": "3025025", "url": "https://en.wikipedia.org/wiki?curid=3025025", "title": "Palmitoleic acid", "text": "Palmitoleic acid\n\nPalmitoleic acid, or (9\"Z\")-hexadec-9-enoic acid, is an omega-7 monounsaturated fatty acid with the formula CH(CH)CH=CH(CH)COOH that is a common constituent of the glycerides of human adipose tissue. It is present in all tissues but, in general, found in higher concentrations in the liver. It is biosynthesized from palmitic acid by the action of the enzyme Stearoyl-CoA desaturase-1. \n\nPalmitoleic acid can be abbreviated as 16:1∆. Dietary sources of palmitoleic acid include breast milk, a variety of animal fats, vegetable oils, and marine oils. Macadamia oil (\"Macadamia integrifolia\") and sea buckthorn oil (\"Hippophae rhamnoides\") are botanical sources with high concentrations, containing 17% and 19-29% of palmitoleic acid, respectively. It also comprises 13.55% of the fats from the fruit of the durian species \"Durio graveolens\".\n"}
{"id": "41558978", "url": "https://en.wikipedia.org/wiki?curid=41558978", "title": "Patchy particles", "text": "Patchy particles\n\nPatchy particles are micron- or nanoscale colloidal particles that are anisotropically patterned, either by modification of the particle surface chemistry (\"enthalpic patches\"), through particle shape (\"entropic patches\"), or both. The particles have a repulsive core and highly interactive surfaces that allow for this assembly. The placement of these patches on the surface of a particle promotes bonding with patches on other particles. Patchy particles are used as a shorthand for modelling anisotropic colloids and proteins and for designing approaches to nanoparticle synthesis. Patchy particles range in valency from two (Janus particles) or higher.\n\nOne simulation done involves a Monte Carlo method, where the best “move” ensures equilibrium in the particle. One type of move is rototranslation. This is carried out by choosing a random particle, random angular and radial displacements, and a random axis of rotation. Rotational degrees of freedom need to be determined prior to the simulation. The particle is then rotated/moved according to these values. Also, the integration time step needs to be controlled because it will affect the resulting shape/size of the particle. \nAnother simulation done is the grand-canonical ensemble. In the grand-canonical ensemble, the system is in equilibrium with a thermal bath and reservoir of particles. Volume, temperature, and chemical potential are fixed. Because of these constants, a number of particles (n) changes. This is typically used to monitor phase behavior. With these additional moves, the particle is added at a random orientation and random position. \nOther simulations involve biased Monte Carlo moves. One type is aggregation volume-bias moves. It consists of 2 moves; the first tries to form bond between two previously unbonded particles, the second tries to break an existing bond by separation. Aggregation volume-bias moves reflects the following procedure: two particles are chosen, I and J, which are not neighboring particles, particle J is moved inside the bonding volume of particle I. This process is carried out uniformly. Another aggregation volume-bias move follows a method of randomly choosing a particle J that is bonded to I. Particle J is then moved outside the bonding volume of particle I, resulting in the two particles no longer being bonded. A third type of aggregation volume-bias move takes a particle I bonded to particle J and inserts it into a third particle.\n\nGrand canonical ensemble is improved by aggregation volume-bias moves. When aggregation volume-bias moves are applied, the rate of monomer formation and depletion in enhanced and the grand-canonical ensemble moves increase. \nA second biased Monte Carlo simulation is virtual move Monte Carlo. This is a cluster move algorithm. It was made to improve relaxation times in strongly interacting, low density systems and to better approximate diffusive dynamics in the system. This simulation is good for self-assembling and polymeric systems that can find natural moves that relax the system. \n\nSelf-Assembly is also a method to create patchy particles. This method allows formation of complex structures like chains, sheets, rings, icosahedra, square pyramids, tetrahedra, and twisted staircase structures. By coating the surface of particles with highly anistropic, highly directional, weakly interacting patches, the arrangement of the attractive patches can organize disordered particles into structures. The coating and the arrangement of the attractive patches is what contributes to the size, shape, and structure of the resulting particle. \n\nDeveloping entropic patches that will self-assemble into simple cubic, body-centered cubic (bcc), diamond, and dodecagonal quasicrystal structures. The local coordination shell partially dictates the structure that is assembled. Spheres are simulated with cubic, octahedral, and tetrahedral faceting. This allows for entropic patches to self-assemble.\nTetrahedral faceted spheres are targeted by beginning with simple spheres. In coordination with the faces of a tetrahedron, the sphere is sliced at four equal facets. Monte Carlo simulations were performed to determine different forms of α, the faceting amount. The particular faceting amount determines the lattice that assembles. Simple cubic lattices are achieved in a similar way by slicing cubic facets into spheres. This allows for the assembly of simple cubic lattices. A bcc crystal is achieved by faceting a sphere octahedrally.\nThe faceting amount, α, is used in the emergent valence self-assembly to determine what crystal structure will form. A perfect sphere is set as α=0. The shape that is faceted to the sphere is defined at α=1. By fluctuating the faceting amount between α=0 and α=1, the lattice can change. Changes include effects on self-assembly, packing structure, amount of coordination of the faceting patch to the sphere, shape of the faceting patch, type of crystal lattice formed, and the strength of the entropic patch.\n\n\n"}
{"id": "30578970", "url": "https://en.wikipedia.org/wiki?curid=30578970", "title": "Ped", "text": "Ped\n\nPeds are aggregates of soil particles formed as a result of pedogenic processes; this natural organization of particles forms discrete units separated by pores or voids. The term is generally used for macroscopic (visible; i.e. greater than 1 mm in size) structural units when observing soils in the field. Soil peds should be described when the soil is dry or slightly moist, as they can be difficult to distinguish when wet.\n\nThere are five major classes of macrostructure seen in soils: platy, prismatic, columnar, granular, and blocky. There are also structureless conditions. Some soils have simple structure, each unit being an entity without component smaller units. Others have compound structure, in which large units are composed of smaller units separated by persistent planes of weakness.\n\nIn platy structure, the units are flat and platelike. They are generally oriented horizontally. A special form, lenticular platy structure, is recognized for plates that are thickest in the middle and thin toward the edges. Platy structure is usually found in subsurface soils that have been subject to leaching or compaction by animals or machinery. The plates can be separated with a little effort by prying the horizontal layers with a pen knife. Platy structure tends to impede the downward movement of water and plant roots through the soil.\n\nThey are found most frequently in the C, E, Bs and K horizons as well as in sesquioxides (very old soils that are rich in iron and magnesium).\n\nIn the prismatic structure, the individual units are bounded by flat to rounded vertical faces. Units are distinctly longer vertically, and the faces are typically casts or molds of adjoining units. Vertices are angular or subrounded; the tops of the prisms are somewhat indistinct and normally flat. Prismatic structures are characteristic of the B horizons or subsoils. The vertical cracks result from freezing and thawing and wetting and drying as well as the downward movement of water and roots.\n\nIn the columnar structure, the units are similar to prisms and are bounded by flat or slightly rounded vertical faces. The tops of columns, in contrast to those of prisms, are very distinct and normally rounded. Columnar structure is common in the subsoil of sodium affected soils and soils rich in swelling clays such as the smectites and the kandite Halloysite. Columnar structure is very dense and it is very difficult for plant roots to penetrate these layers. Techniques such as deep plowing have help to restore some degree of fertility to these soils.\n\nIn blocky structure, the structural units are blocklike or polyhedral. They are bounded by flat or slightly rounded surfaces that are casts of the faces of surrounding peds. Typically, blocky structural units are nearly equidimensional but grade to prisms and to plates. The structure is described as angular blocky if the faces intersect at relatively sharp angles; as subangular blocky if the faces are a mixture of rounded and plane faces and the corners are mostly rounded. Blocky structures are common in subsoil but also occur in surface soils that have a high clay content. The strongest blocky structure is formed as a result of swelling and shrinking of the clay minerals which produce cracks. Sometimes the surface of dried-up sloughs and ponds shows characteristic cracking and peeling due to clays.\n\nIn the granular structure, the structural units are approximately spherical or polyhedral and are bounded by curved or very irregular faces that are not casts of adjoining peds. In other words, they look like cookie crumbs. Granular structure is common in the surface soils of rich grasslands and highly amended garden soils with high organic matter content. Soil mineral particles are both separated and bridged by organic matter breakdown products, and soil biota exudates, making the soil easy to work. Cultivation, earthworms, frost action and rodents mix the soil and decreases the size of the peds. This structure allows for good porosity and easy movement of air and water. This combination of ease in tillage, good moisture and air handling capabilities, and good structure for planting and germination, are definitive of the phrase \"good tilth\".\n\n"}
{"id": "41017951", "url": "https://en.wikipedia.org/wiki?curid=41017951", "title": "Peptoid nanosheet", "text": "Peptoid nanosheet\n\nIn nanobiotechnology, a peptoid nanosheet is a synthetic protein structure made from peptoids. Peptoid nanosheets have a thickness of about three nanometers and a length of up to 100 micrometers, meaning that they have a two-dimensional, flat shape that resembles paper on the nanoscale.\n\nThis makes them one of the thinnest known two-dimensional organic crystalline materials with an area to thickness ratio of greater than 10 nm. Peptoid nanosheets were discovered in the laboratory of Dr. Ron Zuckermann at the Lawrence Berkeley National Laboratory in 2010. Due to the ability to customize peptoids and therefore the properties of the peptoid nanosheet, it has possible applications in the areas of drug and small molecule delivery and biosensing.\n\nFor assembly, a purified amphiphilic polypeptoid of specific sequence is dissolved in aqueous solution. These form a monolayer (Langmuir–Blodgett film) on the air-water interface with their hydrophobic side chains oriented in air and hydrophilic side chains in the water. When this mono-layer is shrunk, it buckles into a bilayer with the hydrophobic groups forming the interior core of the peptoid nanosheet. This method has been standardized in the Zuckermann laboratory by repetitively tilting vials of peptoid solution at 85° before returning the vials to the upright position. This repetitive vial “rocking” motion lessens the interfacial area of the air-water interface inside the vial, compressing the peptoid mono-layer by a factor of four and causing the mono-layer to buckle into peptoid nanosheets. Using this method, nanosheets are produced in high yield, and 95% of the peptoid polymer starting material is efficiently converted into peptoid nanosheets after rocking the vials several hundred times.\n\nPeptoid nanosheets have a very high surface area, which can be readily functionalized to serve as a platform for sensing and templating. Also, their hydrophobic interiors can accommodate hydrophobic small molecule cargos, which have been demonstrated by the sequestration of Nile red when this dye was injected into an aqueous solution of the peptoid nanosheets. For these reasons, the hydrophobic interior of the 2D nanosheets could be an attractive platform for loading or embedding hydrophobic cargo, such as drug molecules, fluorophores, aromatic compounds, and metal nanoparticles.\n\n"}
{"id": "14287597", "url": "https://en.wikipedia.org/wiki?curid=14287597", "title": "Picratol", "text": "Picratol\n\nPicratol is a high explosive mixture, comprising 52% 'Explosive D' and 48% TNT. It has a detonation velocity of approximately 6,972 metres per second. Picratol has no civilian applications. It was exclusively intended for military use and was especially popular during the Second World War. The basic advantage of Picratol is its insensitivity to shock. As a result, it proved useful as the main explosive filling in armour-piercing shells and aerial bombs.\n\nPicratol is an obsolete explosive and is therefore unlikely to be encountered, except in the form of legacy munitions and unexploded ordnance.\n\n"}
{"id": "52150400", "url": "https://en.wikipedia.org/wiki?curid=52150400", "title": "Prout (unit)", "text": "Prout (unit)\n\nThe Prout is an obsolete unit of energy, whose value is:\n\nformula_1\n\nThis is equal to the binding energy of the deuteron.\n"}
{"id": "35173380", "url": "https://en.wikipedia.org/wiki?curid=35173380", "title": "Quasi-geostrophic equations", "text": "Quasi-geostrophic equations\n\nWhile geostrophic motion refers to the wind that would result from an exact balance between the Coriolis force and horizontal pressure-gradient forces, quasi-geostrophic (QG) motion refers to flows where the Coriolis force and pressure gradient forces are \"almost\" in balance, but with inertia also having an effect. \nAtmospheric and oceanographic flows take place over horizontal length scales which are very large compared to their vertical length scale, and so they can be described using the shallow water equations. The Rossby number is a dimensionless number which characterises the strength of inertia compared to the strength of the Coriolis force. The quasi-geostrophic equations are approximations to the shallow water equations in the limit of small Rossby number, so that inertial forces are an order of magnitude smaller than the Coriolis and pressure forces. If the Rossby number is equal to zero then we recover geostrophic flow.\n\nThe quasi-geostrophic equations were first formulated by Jule Charney.\n\nIn Cartesian coordinates, the components of the geostrophic wind are\nwhere formula_3 is the geopotential.\n\nThe geostrophic vorticity\n\ncan therefore be expressed in terms of the geopotential as\n\nEquation (2) can be used to find formula_6 from a known field formula_7. Alternatively, it can also be used to determine formula_8 from a known distribution of formula_9 by inverting the Laplacian operator.\n\nThe quasi-geostrophic vorticity equation can be obtained from the formula_10 and formula_11 components of the quasi-geostrophic momentum equation which can then be derived from the horizontal momentum equation\n\nThe material derivative in (3) is defined by\n\nThe horizontal velocity formula_15 can be separated into a geostrophic formula_16 and an ageostrophic formula_17 part\n\nTwo important assumptions of the quasi-geostrophic approximation are\n\nThe second assumption justifies letting the Coriolis parameter have a constant value formula_23 in the geostrophic approximation and approximating its variation in the Coriolis force term by formula_24. However, because the acceleration following the motion, which is given in (1) as the difference between the Coriolis force and the pressure gradient force, depends on the departure of the actual wind from the geostrophic wind, it is not permissible to simply replace the velocity by its geostrophic velocity in the Coriolis term. The acceleration in (3) can then be rewritten as\n\nThe approximate horizontal momentum equation thus has the form\n\nExpressing equation (7) in terms of its components,\n"}
{"id": "29201811", "url": "https://en.wikipedia.org/wiki?curid=29201811", "title": "Snipe (wood machining)", "text": "Snipe (wood machining)\n\nSnipe, in woodworking, is a noticeably deeper cut on the leading and/or trailing end of a board after having passed through a thickness planer or jointer. Its cause, in a jointer, is an out-feed table which is set too low relative to the cutter head, or in a thickness planer, an unnecessarily high setting of the Bed rollers of the in-feed table or a pressure bar which is set too high. The term has its origin in forestry where it is applied to a sloping surface or bevel cut on the fore end of a log to ease dragging. (OED)\n\n"}
{"id": "28191", "url": "https://en.wikipedia.org/wiki?curid=28191", "title": "Snow", "text": "Snow\n\nSnow refers to forms of ice crystals that precipitate from the atmosphere (usually from clouds) and undergo changes on the Earth's surface. It pertains to frozen crystalline water throughout its life cycle, starting when, under suitable conditions, the ice crystals form in the atmosphere, increase to millimeter size, precipitate and accumulate on surfaces, then metamorphose in place, and ultimately melt, slide or sublimate away. Snowstorms organize and develop by feeding on sources of atmospheric moisture and cold air. Snowflakes nucleate around particles in the atmosphere by attracting supercooled water droplets, which freeze in hexagonal-shaped crystals. Snowflakes take on a variety of shapes, basic among these are platelets, needles, columns and rime. As snow accumulates into a snowpack, it may blow into drifts. Over time, accumulated snow metamorphoses, by sintering, sublimation and freeze-thaw. Where the climate is cold enough for year-to-year accumulation, a glacier may form. Otherwise, snow typically melts seasonally, causing runoff into streams and rivers and recharging groundwater.\n\nMajor snow-prone areas include the polar regions, the upper half of the Northern Hemisphere and mountainous regions worldwide with sufficient moisture and cold temperatures. In the Southern Hemisphere, snow is confined primarily to mountainous areas, apart from Antarctica.\n\nSnow affects such human activities as transportation: creating the need for keeping roadways, wings, and windows clear; agriculture: providing water to crops and safeguarding livestock; sports such as skiing, snowboarding, and snowmachine travel; and warfare. Snow affects ecosystems, as well, by providing an insulating layer during winter under which plants and animals are able to survive the cold.\n\nSnow develops in clouds that themselves are part of a larger weather system. The physics of snow crystal development in clouds results from a complex set of variables that include moisture content and temperatures. The resulting shapes of the falling and fallen crystals can be classified into a number of basic shapes and combinations, thereof. Occasionally, some plate-like, dendritic and stellar-shaped snowflakes can form under clear sky with a very cold temperature inversion present.\n\nSnow clouds usually occur in the context of larger weather systems, the most important of which is the low pressure area, which typically incorporate warm and cold fronts as part of their circulation. Two additional and locally productive sources of snow are lake-effect (also sea-effect) storms and elevation effects, especially in mountains.\n\nMid-latitude cyclones are low pressure areas which are capable of producing anything from cloudiness and mild snow storms to heavy blizzards. During a hemisphere's fall, winter, and spring, the atmosphere over continents can be cold enough through the depth of the troposphere to cause snowfall. In the Northern Hemisphere, the northern side of the low pressure area produces the most snow. For the southern mid-latitudes, the side of a cyclone that produces the most snow is the southern side.\n\nA cold front, the leading edge of a cooler mass of air, can produce frontal snowsqualls—an intense frontal convective line (similar to a rainband), when temperature is near freezing at the surface. The strong convection that develops has enough moisture to produce whiteout conditions at places which line passes over as the wind causes intense blowing snow. This type of snowsquall generally lasts less than 30 minutes at any point along its path but the motion of the line can cover large distances. Frontal squalls may form a short distance ahead of the surface cold front or behind the cold front where there may be a deepening low pressure system or a series of trough lines which act similar to a traditional cold frontal passage. In situations where squalls develop post-frontally it is not unusual to have two or three linear squall bands pass in rapid succession only separated by 25 miles (40 kilometers) with each passing the same point in roughly 30 minutes apart. In cases where there is a large amount of vertical growth and mixing the squall may develop embedded cumulonimbus clouds resulting in lightning and thunder which is dubbed thundersnow.\n\nA warm front can produce snow for a period, as warm, moist air overrides below-freezing air and creates precipitation at the boundary. Often, snow transitions to rain in the warm sector behind the front.\n\nLake-effect snow is produced during cooler atmospheric conditions when a cold air mass moves across long expanses of warmer lake water, warming the lower layer of air which picks up water vapor from the lake, rises up through the colder air above, freezes and is deposited on the leeward (downwind) shores.\n\nThe same effect also occurs over bodies of salt water, when it is termed \"ocean-effect\" or \"bay-effect snow\". The effect is enhanced when the moving air mass is uplifted by the orographic influence of higher elevations on the downwind shores. This uplifting can produce narrow but very intense bands of precipitation, which deposit at a rate of many inches of snow each hour, often resulting in a large amount of total snowfall.\n\nThe areas affected by lake-effect snow are called snowbelts. These include areas east of the Great Lakes, the west coasts of northern Japan, the Kamchatka Peninsula in Russia, and areas near the Great Salt Lake, Black Sea, Caspian Sea, Baltic Sea, and parts of the northern Atlantic Ocean.\n\nOrographic or relief snowfall is caused when masses of air pushed by wind are forced up the side of elevated land formations, such as large mountains. The lifting of air up the side of a mountain or range results in adiabatic cooling, and ultimately condensation and precipitation. Moisture is removed by orographic lift, leaving drier, warmer air on the descending, leeward side. The resulting enhanced productivity of snow fall and the decrease in temperature with elevation means that snow depth and seasonal persistence of snowpack increases with elevation in snow-prone areas.\n\nA snowflake consists of roughly 10 water molecules, which are added to its core at different rates and in different patterns, depending on the changing temperature and humidity within the atmosphere that the snowflake falls through on its way to the ground. As a result, snowflakes vary among themselves, while following similar patterns.\n\nSnow crystals form when tiny supercooled cloud droplets (about 10 μm in diameter) freeze. These droplets are able to remain liquid at temperatures lower than , because to freeze, a few molecules in the droplet need to get together by chance to form an arrangement similar to that in an ice lattice. Then the droplet freezes around this \"nucleus\". In warmer clouds an aerosol particle or \"ice nucleus\" must be present in (or in contact with) the droplet to act as a nucleus. Ice nuclei are very rare compared to that cloud condensation nuclei on which liquid droplets form. Clays, desert dust and biological particles can be nuclei. Artificial nuclei include particles of silver iodide and dry ice, and these are used to stimulate precipitation in cloud seeding.\n\nOnce a droplet has frozen, it grows in the supersaturated environment—one where air is saturated with respect to ice when the temperature is below the freezing point. The droplet then grows by diffusion of water molecules in the air (vapor) onto the ice crystal surface where they are collected. Because water droplets are so much more numerous than the ice crystals due to their sheer abundance, the crystals are able to grow to hundreds of micrometers or millimeters in size at the expense of the water droplets by the Wegener–Bergeron–Findeisen process. The corresponding depletion of water vapor causes the ice crystals to grow at the droplets' expense. These large crystals are an efficient source of precipitation, since they fall through the atmosphere due to their mass, and may collide and stick together in clusters, or aggregates. These aggregates are snowflakes, and are usually the type of ice particle that falls to the ground. Although the ice is clear, scattering of light by the crystal facets and hollows/imperfections mean that the crystals often appear white in color due to diffuse reflection of the whole spectrum of light by the small ice particles.\n\nMicrography of thousands of snowflakes from 1885 onward, starting with Wilson Alwyn Bentley, revealed the wide diversity of snowflakes within a classifiable set of patterns. Closely matching snow crystals have been observed.\n\nNakaya developed a crystal morphology diagram, relating crystal shapes to the temperature and moisture conditions under which they formed, which is summarized in the following table.\n\nAs Nakaya discovered, shape is also a function of whether the prevalent moisture is above or below saturation. Forms below the saturation line trend more towards solid and compact. Crystals formed in supersaturated air trend more towards lacy, delicate and ornate. Many more complex growth patterns also form such as side-planes, bullet-rosettes and also planar types depending on the conditions and ice nuclei. If a crystal has started forming in a column growth regime, at around , and then falls into the warmer plate-like regime, then plate or dendritic crystals sprout at the end of the column, producing so called \"capped columns\".\n\nMagono and Lee devised a classification of freshly formed snow crystals that includes 80 distinct shapes. They documented each with micrographs.\n\nSnow accumulates from a series of snow events, punctuated by freezing and thawing, over areas that are cold enough to retain snow seasonally or perennially. Major snow-prone areas include the Arctic and Antarctic, the Northern Hemisphere, and alpine regions. The liquid equivalent of snowfall may be evaluated using a snow gauge or with a standard rain gauge, adjusted for winter by removal of a funnel and inner cylinder. Both types of gauges melt the accumulated snow and report the amount of water collected. At some automatic weather stations an ultrasonic snow depth sensor may be used to augment the precipitation gauge.\n\nSnow flurry, snow storm and blizzard describe snow events of progressively greater duration and intensity. A blizzard is a weather condition involving snow and has varying definitions in different parts of the world. In the United States, a blizzard occurs when two conditions are met for a period of three hours or more: A sustained wind or frequent gusts to , and sufficient snow in the air to reduce visibility to less than . In Canada and the United Kingdom, the criteria are similar. While heavy snowfall often occurs during blizzard conditions, falling snow is not a requirement, as blowing snow can create a ground blizzard.\n\nSnowstorm intensity may be categorized by visibility and depth of accumulation. Snowfall's intensity is determined by visibility, as follows:\n\nThe \"International Classification for Seasonal Snow on the Ground\" defines \"height of new snow\" as the depth of freshly fallen snow, in centimeters as measured with a ruler, that accumulated on a snowboard during an observation period of 24 hours, or other observation interval. After the measurement, the snow is cleared from the board and the board is placed flush with the snow surface to provide an accurate measurement at the end of the next interval. Melting, compacting, blowing and drifting contribute to the difficulty of measuring snowfall.\n\nGlaciers with their permanent snowpacks cover about 10% of the earth's surface, while seasonal snow covers about nine percent, mostly in the Northern Hemisphere, where seasonal snow covers about , according to a 1987 estimate. A 2007 estimate of snow cover over the Northern Hemisphere suggested that, on average, snow cover ranges from a minimum extent of each August to a maximum extent of each January or nearly half of the land surface in that hemisphere. A study of Northern Hemisphere snow cover extent for the period 1972–2006 suggests a reduction of over the 35-year period.\n\nThe following are world records regarding snowfall and snowflakes:\n\nAfter deposition, snow progresses on one of two paths that determine its fate, either \"ablation\" (mostly by melting) or transitioning from firn (multi-year snow) into \"glacier ice\". During this transition, snow \"is a highly porous, sintered material made up of a continuous ice structure and a continuously connected pore space, forming together the snow microstructure\". Almost always near its melting temperature, a snowpack is continually transforming these properties in a process, known as \"metamorphism\", wherein all three phases of water may coexist, including liquid water partially filling the pore space. Starting as a powdery deposition, snow becomes more granular when it begins to compact under its own weight, be blown by the wind, sinter particles together and commence the cycle of melting and refreezing. Water vapor plays a role as it deposits ice crystals, known as hoar frost, during cold, still conditions.\n\nOver the course of time, a snowpack may settle under its own weight until its density is approximately 30% of water. Increases in density above this initial compression occur primarily by melting and refreezing, caused by temperatures above freezing or by direct solar radiation. In colder climates, snow lies on the ground all winter. By late spring, snow densities typically reach a maximum of 50% of water. Snow that persists into summer evolves into névé, granular snow, which has been partially melted, refrozen and compacted. Névé has a minimum density of , which is roughly half of the density of liquid water.\n\nFirn is snow that has persisted for multiple years and has been recrystallized into a substance denser than névé, yet less dense and hard than glacial ice. Firn resembles caked sugar and is very resistant to shovelling. Its density generally ranges from to , and it can often be found underneath the snow that accumulates at the head of a glacier. The minimum altitude that firn accumulates on a glacier is called the \"firn limit\", \"firn line\" or \"snowline\".\n\nThere are four main mechanisms for movement of deposited snow: \"drifting\" of unsintered snow, \"avalanches\" of accumulated snow on steep slopes, \"snowmelt\" during thaw conditions, and the \"movement of glaciers\" after snow has persisted for multiple years and metamorphosed into glacier ice.\n\nWhen powdery, snow drifts with the wind from the location where it originally fell, forming deposits with a depth of several meters in isolated locations. After attaching to hillsides, blown snow can evolve into a snow slab, which is an avalanche hazard on steep slopes.\n\nAn avalanche (also called a snowslide or snowslip) is a rapid flow of snow down a sloping surface. Avalanches are typically triggered in a starting zone from a mechanical failure in the snowpack (slab avalanche) when the forces on the snow exceed its strength but sometimes only with gradually widening (loose snow avalanche). After initiation, avalanches usually accelerate rapidly and grow in mass and volume as they entrain more snow. If the avalanche moves fast enough some of the snow may mix with the air forming a powder snow avalanche, which is a type of gravity current. They occur in three major mechanisms:\n\nMany rivers originating in mountainous or high-latitude regions receive a significant portion of their flow from snowmelt. This often makes the river's flow highly seasonal resulting in periodic flooding during the spring months and at least in dry mountainous regions like the mountain West of the US or most of Iran and Afghanistan, very low flow for the rest of the year. In contrast, if much of the melt is from glaciated or nearly glaciated areas, the melt continues through the warm season, with peak flows occurring in mid to late summer.\n\nGlaciers form where the accumulation of snow and ice exceeds ablation. The area in which an alpine glacier forms is called a cirque (corrie or cwm), a typically armchair-shaped geological feature, which collects snow and where the snowpack compacts under the weight of successive layers of accumulating snow, forming névé. Further crushing of the individual snow crystals and reduction of entrapped air in the snow turns it into 'glacial ice'. This glacial ice will fill the cirque until it 'overflows' through a geological weakness or vacancy, such as the gap between two mountains. When the mass of snow and ice is sufficiently thick, it begins to move due to a combination of surface slope, gravity and pressure. On steeper slopes, this can occur with as little as 15 m (50 ft) of snow-ice.\n\nScientists study snow at a wide variety of scales that include the physics of chemical bonds and clouds; the distribution, accumulation, metamorphosis, and ablation of snowpacks; and the contribution of snowmelt to river hydraulics and ground hydrology. In doing so, they employ a variety of instruments to observe and measure the phenomena studied. Their findings contribute to knowledge applied by engineers, who adapt vehicles and structures to snow, by agronomists, who address the availability of snowmelt to agriculture, and those, who design equipment for sporting activities on snow. Scientists develop and others employ snow classification systems that describe its physical properties at scales ranging from the individual crystal to the aggregated snowpack. A sub-specialty is avalanches, which are of concern to engineers and outdoors sports people, alike.\n\nSnow science addresses how snow forms, its distribution, and processes affecting how snowpacks change over time. Scientists improve storm forecasting, study global snow cover and its effect on climate, glaciers, and water supplies around the world. The study includes physical properties of the material as it changes, bulk properties of in-place snow packs, and the aggregate properties of regions with snow cover. In doing so, they employ on-the-ground physical measurement techniques to establish ground truth and remote sensing techniques to develop understanding of snow-related processes over large areas.\n\nIn the field snow scientists often excavate a snow pit within which to make basic measurements and observations. Observations can describe features caused by wind, water percolation, or snow unloading from trees.Water percolation into a snowpack can create flow fingers and ponding or flow along capillary barriers, which can refreeze into horizontal and vertical solid ice formations within the snowpack. Among the measurements of the properties of snowpacks that the \"International Classification for Seasonal Snow on the Ground\" includes are: snow height, snow water equivalent, snow strength, and extent of snow cover. Each has a designation with code and detailed description. The classification extends the prior classifications of Nakaya and his successors to related types of precipitation and are quoted in the following table:\n\"All are formed in cloud, except for rime, which forms on objects exposed to supercooled moisture.\"\n\nIt also has a more extensive classification of deposited snow than those that pertain to airborne snow. The categories include both natural and man-made snow types, descriptions of snow crystals as they metamorphose and melt, the development of hoar frost in the snow pack and the formation of ice therein. Each such layer of a snowpack differs from the adjacent layers by one or more characteristics that describe its microstructure or density, which together define the snow type, and other physical properties. Thus, at any one time, the type and state of the snow forming a layer have to be defined because its physical and mechanical properties depend on them. Physical properties include microstructure, grain size and shape, snow density, liquid water content, and temperature.\n\nRemote sensing of snowpacks with satellites and other platforms typically includes multi-spectral collection of imagery. Multi-faceted interpretation of the data obtained allows inferences about what is observed. The science behind these remote observations has been verified with ground-truth studies of the actual conditions.\n\nSatellite observations record a decrease in snow-covered areas since the 1960s, when satellites when satellite observations began. In some areas, including China, snow cover has increased. In some regions such as China, a trend of increasing snow cover has been observed from 1978 to 2006. These changes are attributed to global climate change, which may lead to earlier melting and less coverage area. However, in some areas there may be an increase in snow depth because of higher temperatures for latitudes north of 40°. For the Northern Hemisphere as a whole the mean monthly snow-cover extent has been decreasing by 1.3% per decade.\n\nThe most frequently used methods to map and measure snow extent, snow depth and snow water equivalent employ multiple inputs on the visible–infrared spectrum to deduce the presence and properties of snow. The National Snow and Ice Data Center (NSIDC) uses the reflectance of visible and infrared radiation to calculate a normalized difference snow index, which is a ratio of radiation parameters that can distinguish between clouds and snow. Other researchers have developed decision trees, employing the available data to make more accurate assessments. One challenge to this assessment is where snow cover is patchy, for example during periods of accumulation or ablation and also in forested areas. Cloud cover inhibits optical sensing of surface reflectance, which has led to other methods for estimating ground conditions underneath clouds. For hydrological models, it is important to have continuous information about the snow cover. Passive microwave sensors are especially valuable for temporal and spatial continuity because they can map the surface beneath clouds and in darkness. When combined with reflective measurements, passive microwave sensing greatly extends the inferences possible about the snowpack.\n\nSnow science often leads to predictive models that include snow deposition, snow melt, and snow hydrology—elements of the Earth's water cycle—which help describe global climate change.\n\nGlobal climate change models (GCMs) incorporate snow as a factor in their calculations. Some important aspects of snow cover include its albedo (reflectivity of incident radiation, including light) and insulating qualities, which slow the rate of seasonal melting of sea ice. As of 2011, the melt phase of GCM snow models were thought to perform poorly in regions with complex factors that regulate snow melt, such as vegetation cover and terrain. These models typically derive snow water equivalent (SWE) in some manner from satellite observations of snow cover. The \"International Classification for Seasonal Snow on the Ground\" defines SWE as \"the depth of water that would result if the mass of snow melted completely\".\n\nGiven the importance of snowmelt to agriculture, hydrological runoff models that include snow in their predictions address the phases of accumulating snowpack, melting processes, and distribution of the meltwater through stream networks and into the groundwater. Key to describing the melting processes are solar heat flux, ambient temperature, wind, and precipitation. Initial snowmelt models used a degree-day approach that emphasized the temperature difference between the air and the snowpack to compute snow water equivalent, SWE. More recent models use an energy balance approach that take into account the following factors to compute \"Q\", the energy available for melt. This requires measurement of an array of snowpack and environmental factors to compute six heat flow mechanisms that contribute to \"Q\".\n\nSnow affects human activity in four major areas, transportation, agriculture, structures, and sports. Most transportation modes are impeded by snow on the travel surface. Agriculture often relies on snow as a source of seasonal moisture. Structures may fail under snow loads. Humans find a wide variety of recreational activities in snowy landscapes.\n\nSnow affects the rights of way of highways, airfields and railroads. They share a common tool for clearing snow, the snowplow. However, the application is different in each case—whereas roadways employ anti-icing chemicals to prevent bonding of ice, airfields may not; railroads rely on abrasives to enhance traction on tracks.\n\nIn the late 20th century, an estimated $2 billion was spent annually in North America on roadway winter maintenance, owing to snow and other winter weather events, according to a 1994 report by Kuemmel. The study surveyed the practices of jurisdictions within 44 US states and nine Canadian provinces. It assessed the policies, practices, and equipment used for winter maintenance. It found similar practices and progress to be prevalent in Europe.\n\nThe dominant effect of snow on vehicle contact with the road is diminished friction. This can be improved with the use of snow tires, which have a tread designed to compact snow in a manner that enhances traction. However, the key to maintaining a roadway that can accommodate traffic during and after a snow event is an effective anti-icing program that employs both chemicals and plowing. The FHWA \"Manual of Practice for an Effective Anti-icing Program\" emphasizes \"anti-icing\" procedures that prevent the bonding of snow and ice to the road. Key aspects of the practice include: understanding anti-icing in light of the level of service to be achieved on a given roadway, the climatic conditions to be encountered, and the different roles of deicing, anti-icing, and abrasive materials and applications, and employing anti-icing \"toolboxes\", one for operations, one for decision-making and another for personnel. The elements to the toolboxes are:\nThe manual offers matrices that address different types of snow and the rate of snowfall to tailor applications appropriately and efficiently.\n\nSnow fences, constructed upwind of roadways control snow drifting by causing windblown, drifting snow to accumulate in a desired place. They are also used on railways. Additionally, farmers and ranchers use snow fences to create drifts in basins for a ready supply of water in the spring.\n\nIn order to keep airports open during winter storms, runways and taxiways require snow removal. Unlike roadways, where chloride chemical treatment is common to prevent snow from bonding to the pavement surface, such chemicals are typically banned from airports because of their strong corrosive effect on aluminum aircraft. Consequently, mechanical brushes are often used to complement the action of snow plows. Given the width of runways on airfields that handle large aircraft, vehicles with large plow blades, an echelon of plow vehicles or rotary snowplows are used to clear snow on runways and taxiways. Terminal aprons may require or more to be cleared.\n\nProperly equipped aircraft are able to fly through snowstorms under Instrument flight rules. Prior to takeoff, during snowstorms they require deicing fluid to prevent accumulation and freezing of snow and other precipitation on wings and fuselages, which may compromise the safety of the aircraft and its occupants. In flight, aircraft rely on a variety of mechanisms to avoid rime and other types of icing in clouds, these include pulsing pneumatic boots, electro-thermal areas that generate heat, and fluid deicers that bleed onto the surface.\n\nRailroads have traditionally employed two types of snow plows for clearing track, the wedge plow, which casts snow to both sides, and the rotary snowplow, which is suited for addressing heavy snowfall and casting snow far to one side or the other. Prior to the invention of the rotary snowplow ca. 1865, it required multiple locomotives to drive a wedge plow through deep snow. Subsequent to clearing the track with such plows, a \"flanger\" is used to clear snow from between the rails that are below the reach of the other types of plow. Where icing may affect the steel-to-steel contact of locomotive wheels on track, abrasives (typically sand) have been used to provide traction on steeper uphills.\n\nRailroads employ snow sheds—structures that cover the track—to prevent the accumulation of heavy snow or avalanches to cover tracks in snowy mountainous areas, such as the Alps and the Rocky Mountains.\n\n\nSnow can be compacted to form a snow road and be part of a winter road route for vehicles to access isolated communities or construction projects during the winter. Snow can also be used to provide the supporting structure and surface for a runway, as with the Phoenix Airfield in Antarctica. The snow-compacted runway is designed to withstand approximately 60 wheeled flights of heavy-lift military aircraft a year.\n\nSnowfall can be beneficial to agriculture by serving as a thermal insulator, conserving the heat of the Earth and protecting crops from subfreezing weather. Some agricultural areas depend on an accumulation of snow during winter that will melt gradually in spring, providing water for crop growth, both directly and via runoff through streams and rivers, which supply irrigation canals. The following are examples of rivers that rely on meltwater from glaciers or seasonal snowpack as an important part of their flow on which irrigation depends: the Ganges, many of whose tributaries rise in the Himalayas and which provide much irrigation in northeast India, the Indus River, which rises in Tibet and provides irrigation water to Pakistan from rapidly retreating Tibetan glaciers, and the Colorado River, which receives much of its water from seasonal snowpack in the Rocky Mountains and provides irrigation water to some 4 million acres (1.6 million hectares).\n\nSnow is an important consideration for loads on structures. To address these, European countries employ \"Eurocode 1: Actions on structures - Part 1-3: General actions - Snow loads\". In North America, ASCE \"Minimum Design Loads for Buildings and Other Structures\" gives guidance on snow loads. Both standards employ methods that translate maximum expected ground snow loads onto design loads for roofs.\n\nSnow loads and icings are two principal issues for roofs. Snow loads are related to the climate in which a structure is sited. Icings are usually a result of the building or structure generating heat that melts the snow that is on it.\n\n\"Snow loads\" – The \"Minimum Design Loads for Buildings and Other Structures\" gives guidance on how to translate the following factors into roof snow loads:\nIt gives tables for ground snow loads by region and a methodology for computing ground snow loads that may vary with elevation from nearby, measured values. The \"Eurocode 1\" uses similar methodologies, starting with ground snow loads that are tabulated for portions of Europe.\n\n\"Icings\" – Roofs must also be designed to avoid ice dams, which result from meltwater running under the snow on the roof and freezing at the eave. Ice dams on roofs form when accumulated snow on a sloping roof melts and flows down the roof, under the insulating blanket of snow, until it reaches below freezing temperature air, typically at the eaves. When the meltwater reaches the freezing air, ice accumulates, forming a dam, and snow that melts later cannot drain properly through the dam. Ice dams may result in damaged building materials or in damage or injury when the ice dam falls off or from attempts to remove ice dams. The melting results from heat passing through the roof under the highly insulating layer of snow.\n\nIn areas with trees, utility distribution lines on poles are less susceptible to snow loads than they are subject to damage from trees falling on them, felled by heavy, wet snow. Elsewhere, snow can accrete on power lines as \"sleeves\" of rime ice. Engineers design for such loads, which are measured in kg/m (lb/ft) and power companies have forecasting systems that anticipate types of weather that may cause such accretions. Rime ice may be removed manually or by creating a sufficient short circuit in the affected segment of power lines to melt the accretions.\n\nSnow figures into many winter sports and forms of recreation, including skiing and sledding. Common examples include cross-country skiing, Alpine skiing, snowboarding, snowshoeing, and snowmobiling. The design of the equipment used, typically relies on the bearing strength of snow, as with skis or snowboards and contends with the coefficient of friction of snow to allow sliding, often enhance by ski waxes.\n\nSkiing is by far the largest form of winter recreation. As of 1994, of the estimated 65–75 million skiers worldwide, there were approximately 55 million who engaged in Alpine skiing, the rest engaged in cross-country skiing. Approximately 30 million skiers (of all kinds) were in Europe, 15 million in the US, and 14 million in Japan. As of 1996, there were reportedly 4,500 ski areas, operating 26,000 ski lifts and enjoying 390 million skier visits per year. The preponderant region for downhill skiing was Europe, followed by Japan and the US.\n\nIncreasingly, ski resorts are relying on snowmaking, the production of snow by forcing water and pressurized air through a snow gun on ski slopes. Snowmaking is mainly used to supplement natural snow at ski resorts. This allows them to improve the reliability of their snow cover and to extend their ski seasons from late autumn to early spring. The production of snow requires low temperatures. The threshold temperature for snowmaking increases as humidity decreases. Wet-bulb temperature is used as a metric since it takes air temperature and relative humidity into account. Snowmaking is a relatively expensive process in its energy consumption, thereby limiting its use.\n\nSki wax enhances the ability of a ski or other runner to slide over snow, which depends on both the properties of the snow and the ski to result in an optimum amount of lubrication from melting the snow by friction with the ski—too little and the ski interacts with solid snow crystals, too much and capillary attraction of meltwater retards the ski. Before a ski can slide, it must overcome the maximum value static friction. Kinetic (or dynamic) friction occurs when the ski is moving over the snow.\n\nSnow affects warfare conducted in winter, alpine environments or at high latitudes. The main factors are \"impaired visibility\" for acquiring targets during falling snow, \"enhanced visibility\" of targets against snowy backgrounds for targeting, and mobility for both mechanized and infantry troops. Snowfall can severely inhibit the logistics of supplying troops, as well. Snow can also provide cover and fortification against small-arms fire. Noted winter warfare campaigns where snow and other factors affected the operations include:\n\n\nBoth plant and animal life endemic to snow-bound areas develop ways to adapt. Among the adaptive mechanisms for plants are dormancy, seasonal dieback, survival of seeds; and for animals are hibernation, insulation, anti-freeze chemistry, storing food, drawing on reserves from within the body, and clustering for mutual heat.\n\nSnow interacts with vegetation in two principal ways, vegetation can influence the deposition and retention of snow and, conversely, the presence of snow can affect the distribution and growth of vegetation. Tree branches, especially of conifers intercept falling snow and prevent accumulation on the ground. Snow suspended in trees ablates more rapidly than that on the ground, owing to its greater exposure to sun and air movement. Trees and other plants can also promote snow retention on the ground, which would otherwise be blown elsewhere or melted by the sun. Snow affects vegetation in several ways, the presence of stored water can promote growth, yet the annual onset of growth is dependent on the departure of the snowpack for those plants that are buried beneath it. Furthermore, avalanches and erosion from snowmelt can scour terrain of vegetation.\n\nSnow supports a wide variety of animals both on the surface and beneath. Many invertebrates thrive in snow, including spiders, wasps, beetles, snow scorpionflys and springtails. Such arthropods are typically active at temperatures down to . Invertebrates fall into two groups, regarding surviving subfreezing temperatures: freezing resistant and those that avoid freezing because they are freeze-sensitive. The first group may be cold hardy owing to the ability to produce antifreeze agents in their body fluids that allows survival of long exposure to sub-freezing conditions. Some organisms fast during the winter, which expels freezing-sensitive contents from their digestive tracts. The ability to survive the absence of oxygen in ice is an additional survival mechanism.\n\nSmall vertebrates are active beneath the snow. Among vertebrates, alpine salamanders are active in snow at temperatures as low as ; they burrow to the surface in springtime and lay their eggs in melt ponds. Among mammals, those that remain active are typically smaller than . Omnivores are more likely to enter a torpor or be hibernators, whereas herbivores are more likely to maintain food caches beneath the snow. Voles store up to of food and pikas up to . Voles also huddle in communal nests to benefit from one another's warmth. On the surface, wolves, coyotes, foxes, lynx, and weasels rely on these subsurface dwellers for food and often dive into the snowpack to find them.\n\nExtraterrestrial \"snow\" includes water-based precipitation, but also precipitation of other compounds prevalent on other planets and moons in the Solar System. Examples are:\n\nLexicon\nNotable snow events\n\nRecreation\n\nRelated concepts\n\nScience and scientists\nSnow structures\n\n"}
{"id": "37713703", "url": "https://en.wikipedia.org/wiki?curid=37713703", "title": "Società Elettrica Sopracenerina", "text": "Società Elettrica Sopracenerina\n\nThe Società Elettrica Sopracenerina, or Electricity Company of Sopraceneri, is a Swiss electricity supply company. It operates throughout the Sopraceneri region of the canton of Ticino, and is headquartered in the city of Locarno. It is part of the Alpiq Group.\n"}
{"id": "235576", "url": "https://en.wikipedia.org/wiki?curid=235576", "title": "Suction", "text": "Suction\n\nSuction is the flow of a fluid into a partial vacuum, or region of low pressure. The pressure gradient between this region and the ambient pressure will propel matter toward the low pressure area. Dust is sucked into a vacuum cleaner when it is pushed in by the higher pressure air on the outside of the cleaner.\n\nThis is similar to what happens when humans breathe or drink through a straw. Both breathing and using a straw involve contracting the diaphragm and muscles around the rib cage. The increased volume in the chest cavity or thoracic cavity decreases the pressure inside, creating an imbalance with the ambient air pressure, or atmospheric pressure. This imbalance results in air pushing into the lungs or liquid pushing up through a straw and into the mouth.\n\nPumps typically have an inlet where the fluid (or air) enters the pump and an outlet where the fluid/air comes out. The inlet location is said to be at the suction side of the pump. The outlet location is said to be at the discharge side of the pump. Operation of the pump creates suction (a lower pressure) at the suction side so that fluid/air can enter the pump through the inlet. Pump operation also causes higher pressure at the discharge side by forcing the fluid/air out at the outlet. There may be pressure sensing devices at the pump's suction and/or discharge sides which control the operation of the pump. For example, if the suction pressure of a centrifugal pump is too high, a device may trigger the fluid pump to shut off to keep it from running dry; i. e. with no fluid entering.\n\nUnder normal conditions of atmospheric pressure suction can draw pure water up to a maximum height of approximately 10.3 m (33.9 feet). \n\nIn medicine, suction devices are used to clear airways of materials that would like to impede breathing or cause infections, to aid in surgery, and for other purposes.\n\n\n"}
{"id": "7206827", "url": "https://en.wikipedia.org/wiki?curid=7206827", "title": "Suspended solids", "text": "Suspended solids\n\nSuspended solids refers to small solid particles which remain in suspension in water as a colloid or due to the motion of the water. It is used as one indicator of water quality.\n\nIt is sometimes abbreviated SS, but is not to be confused with settleable solids, also abbreviated SS, which contribute to the blocking of sewer pipes.\n\nSuspended solids are important as pollutants and pathogens are carried on the surface of particles. The smaller the particle size, the greater the total surface area per unit mass of particle in grams, and so the higher the pollutant load that is likely to be carried.\n\nRemoval of suspended solids is generally achieved through the use of sedimentation and/or water filters (usually at a municipal level). By eliminating most of the suspended solids in a water supply, the significant water is usually rendered close to drinking quality. This is followed by disinfection to ensure that any free floating pathogens, or pathogens associated with the small remaining amount of suspended solids, are rendered ineffective.\n\nThe use of a very simple cloth filter, consisting of a folded cotton sari, drastically reduces the load of cholera carried in the water, and is suitable for use by the very poor; in this case, an appropriate technology method of disinfection might be added, such as solar water disinfection.\n\nA major exception to this generalization is arsenic contamination of groundwater, as arsenic is a very serious pollutant which is soluble, and thus not removed when suspended solids are removed. This makes it very difficult to remove, and finding an alternative water source is often the most realistic option.\n\n"}
{"id": "50921694", "url": "https://en.wikipedia.org/wiki?curid=50921694", "title": "Sustainable Dairying: Water Accord", "text": "Sustainable Dairying: Water Accord\n\nThe Sustainable Dairying: Water Accord is a voluntary agreement signed by leading participants of New Zealand's dairy industry in 2014. It serves as a successor to the 2003 Dairying and Clean Streams Accord.\n"}
{"id": "2392280", "url": "https://en.wikipedia.org/wiki?curid=2392280", "title": "Syriana", "text": "Syriana\n\nSyriana is a 2005 American geopolitical thriller film written and directed by Stephen Gaghan, and executive produced by George Clooney, who also stars in the film with an ensemble cast. Gaghan's screenplay is loosely adapted from Robert Baer's memoir \"See No Evil\". The film focuses on petroleum politics and the global influence of the oil industry, whose political, economic, legal, and social effects are experienced by a Central Intelligence Agency operative (George Clooney), an energy analyst (Matt Damon), a Washington, D.C. attorney (Jeffrey Wright), and a young unemployed Pakistani migrant worker (Mazhar Munir) in an Arab state in the Persian Gulf. The film also features an extensive supporting cast including Amanda Peet, Tim Blake Nelson, Mark Strong, Alexander Siddig, Amr Waked, and Academy Award winners Christopher Plummer, Chris Cooper and William Hurt.\n\nAs with Gaghan's screenplay for \"Traffic\", \"Syriana\" uses multiple, parallel storylines, jumping between locations in Iran, Texas, Washington, D.C., Switzerland, Spain and Lebanon.\n\nClooney won the Academy Award for Best Supporting Actor for his role as Bob Barnes, and Gaghan's script was nominated by the Academy for Best Original Screenplay. As of April 20, 2006, the film had grossed a total of $50.82 million at the U.S. box office and $42.9 million overseas, for a total of $93.73 million.\n\nU.S. energy giant Connex Oil is losing control of key oil fields in a Persian Gulf kingdom ruled by the al-Subaai family. The emirate's foreign minister, Prince Nasir, has granted natural gas drilling rights to a Chinese company, greatly upsetting the U.S. oil industry and the U.S. government. To compensate for its decreased production capacity, Connex initiates a shady merger with Killen, a smaller oil company that recently won the drilling rights to key petroleum fields in Kazakhstan. Connex-Killen ranks as the world's twenty-third largest economy, and antitrust regulators at the DOJ have concerns. A Washington, D.C.-based law firm headed by Dean Whiting is hired to smooth the way for the merger. Bennett Holiday is assigned to promote the impression of due diligence to the DOJ, deflecting any allegations of corruption.\n\nBryan Woodman is an American energy analyst based in Geneva. Woodman's supervisor directs him to attend a private party hosted by the emir at his estate in Spain, to offer his company's services. The emir's illness during the party prevents Woodman from speaking directly with him while, at the same time, the emir's younger son, Prince Meshal Al-Subaai, shows the estate's many rooms and areas to Chinese oil executives via remote-controlled cameras. No one notices that a crack in one of the swimming pool area's underwater lights has electrified the water. Just as Woodman and all the other guests are brought to the pool area, Woodman's son jumps into the pool and is electrocuted.\n\nIn reparation and out of sympathy for the loss of his son, Prince Nasir, the emir's older son, grants Woodman's company oil interests worth $75 million, and Woodman, though initially insulted by the offer, gradually becomes his economic advisor. Prince Nasir is dedicated to the idea of progressive reform and understands that oil dependency is not sustainable in the long term; Nasir wants to utilize his nation's oil profits to diversify the economy and introduce democratic reforms, in sharp contrast to his father's repressive government, which has been supported by American interests. His father, at the urging of the American government, names the younger Meshal as his successor, causing Nasir to attempt a coup.\n\nBob Barnes is a veteran CIA agent trying to stop illegal arms trafficking in the Middle East. While on assignment in Tehran to kill two arms dealers, Barnes notices that one of two anti-aircraft missiles intended to be used in a bombing was diverted to an Egyptian, while the other explodes and kills the two arms dealers. The dealers are later revealed to be Iranian Intelligence agents. Barnes makes his superiors nervous by writing memos about the missile theft and is subsequently reassigned to a desk job. However, unaccustomed to the political discretion required, he quickly embarrasses the wrong people by speaking his mind and is sent back to the field with the assignment of assassinating Prince Nasir, whom the CIA identifies as being the financier behind the Egyptian's acquisition of the missile. Prior to his reassignment, Barnes confides in his ex-CIA agent friend, Stan Goff, that he will return to Lebanon. Goff advises him to clear his presence with Hezbollah so they know he is not acting against them. Barnes travels to Lebanon, obtains safe passage from a Hezbollah leader, and hires a mercenary named Mussawi to help kidnap and kill Nasir. But Mussawi has become an Iranian agent and has Barnes abducted. Mussawi tortures Barnes and prepares to behead him, but the Hezbollah leader arrives and stops him.\n\nWhen the CIA learns that Mussawi plans to broadcast the agency's intention to kill Nasir, they set Barnes up as a scapegoat, portraying him as a rogue agent. Barnes's boss, Terry George, worries that Barnes might talk about the Nasir assassination plan and that killing Nasir with a drone would make it obvious as an American-backed assassination. He has Barnes's passports revoked, locks him out of his computer at work, and initiates an investigation of him. Barnes, however, learns from Goff that Whiting, working on behalf of a group of businessmen calling themselves The Committee to Liberate Iran, is responsible for Barnes's blackballing and the assassination, and threatens him and his family unless he halts the investigation and releases Barnes's passports.\n\nBarnes returns to the Middle East and approaches Prince Nasir's convoy to warn him of the assassination plan. As he arrives, a guided bomb from a circling Predator drone strikes the automobile of Nasir and his family, killing them instantly. Woodman, having earlier offered his seat in Nasir's car to members of his family, survives the missile strike and goes home to his wife and son.\n\nPakistani migrant workers Saleem Ahmed Khan and his son Wasim board a bus to go to work at a Connex refinery, only to discover that they have been laid off. Since the company had provided food and lodging, the workers face the threat of poverty and deportation due to their unemployed status. Wasim desperately searches for work but is refused because he doesn't speak Arabic. Wasim and his friend join a madrasa to learn Arabic in order to improve their employment prospects. While playing soccer, they meet a charismatic cleric who eventually leads them to execute a suicide attack on a Connex-Killen tanker using an explosive from the missing Tehran missile.\n\nBennett Holiday meets with Dean Whiting, who is convinced that Killen bribed someone to get the drilling rights in Kazakhstan. While investigating Connex-Killen's records, Holiday discovers a wire transfer of funds that leads back to a transaction between Texas oilman and Killen colleague Danny Dalton and Kazakh officials. Holiday tells Connex-Killen of his discovery, and they pretend not to have known about it. Holiday advises Dalton that he will likely be charged with corruption in order to serve as a \"body\" to get the DOJ off the back of the rest of Connex-Killen. U.S. Attorney Donald Farish III then strong-arms Holiday into giving the DOJ information about illegal activities he has discovered. Holiday gives up Dalton, but Farish says this is not enough. Holiday meets with the CEO of Killen Oil, Jimmy Pope, and informs him that the DOJ needs a second body in order to drop the investigation. Pope asks Holiday whether a person at Holiday's firm above him would be sufficient as the additional body. Holiday acknowledges that if the name were big enough, the DOJ would stop the investigation and allow the merger.\n\nHoliday is brought by his colleague and mentor Sydney Hewitt to meet with the Chairman & CEO of Connex Oil, Leland \"Lee\" Janus. Holiday reveals an under-the-table deal that Hewitt made while the Connex-Killen merger was being processed. Holiday has given Hewitt to the DOJ as the second body, thereby protecting the rest of Connex-Killen. Janus is able to accept the \"Oil Industry Man of the Year\" award with a load taken off his shoulders.\n\nWhile working on \"Traffic\", Stephen Gaghan began to see parallels between drug addiction and America's dependency on foreign oil. Another source of inspiration came from 9/11 and Gaghan's lack of knowledge about the Middle East. He said, \"When 9/11 happened, it suddenly was a war on terror, which I think of as a war on emotions. It all started to click for me.\" A few weeks after 9/11, Steven Soderbergh sent Gaghan a copy of ex-CIA officer Robert Baer's memoir, \"See No Evil\". The screenwriter read the book and wanted to turn it into a film because it added another layer to the story that Gaghan wanted to tell. Soderbergh bought the rights to \"See No Evil\" and negotiated the deal with Warner Bros.\n\nGaghan met Baer for lunch and then, for six weeks in 2002, the two men traveled from Washington to Geneva to the French Riviera to Lebanon, Syria and Dubai, meeting with lobbyists, arms dealers, oil traders, Arab officials and the spiritual leader of Hezbollah. Meeting Baer, Gaghan realized that the man had \"gone out there and done and seen things that he was not allowed to talk about, and wouldn't, but he was angry about and also trying to make amends for.\" Before any filming took place, Gaghan convinced Warner Bros. to give him an unlimited research budget and no deadline. He did his own legwork, meeting with oil traders in London and lawyers in Washington, D.C. Moments after arriving in Beirut in 2002, Gaghan was taken from the airport in a blindfold and hood where he met with Sheik Mohammed Hussein Fadlallah, who was interested in films. He decided to grant the writer an audience even though he had not requested one. In addition, Gaghan dined with men suspected of killing former Lebanese Prime Minister Rafiq Hariri and met with former Defense Policy Board chairman Richard Perle.\n\nGaghan has cited as influences on \"Syriana\", European films like Roberto Rossellini's \"Rome, Open City\", Costa Gavras' \"Z\", and Gillo Pontecorvo's \"The Battle of Algiers\".\n\nAnother influence, or resource—one that might also explain the movie's use of a documentary clip featuring John D. Rockefeller—is the fact that Gaghan's fashion-designer wife Minnie Mortimer is the great-granddaughter of onetime Standard Oil executive Henry Morgan Tilford.\n\nHarrison Ford turned down the role of Bob Barnes (the role played by George Clooney), regretting it later, stating, \"I didn't feel strongly enough about the truth of the material and I think I made a mistake.\" This is the second Stephen Gaghan-written role Ford has declined, having turned down the role of Robert Wakefield in \"Traffic\", a role that eventually went to Michael Douglas.\n\nGaghan shot in over 200 locations on four continents with 100 speaking parts. \"Syriana\" originally had five storylines, all of which were filmed. The fifth storyline, centering on Michelle Monaghan playing a Miss USA who becomes involved with a rich Arab oilman, was cut when the film became too complicated. Also, a role played by Greta Scacchi, as Bob Barnes' wife, was also cut before the final release. Parts of the film were shot in Dubai and other parts of the Middle East.\n\nThe film's title is suggested to derive from the hypothesized \"Pax Syriana,\" as an allusion to the necessary state of peace between Syria and the U.S. as it relates to the oil business. In a December 2005 interview, Baer told NPR that the title is a metaphor for foreign intervention in the Middle East, referring to post-World War II think tank strategic studies for the creation of an artificial state (such as Iraq, created from elements of the former Ottoman Empire) that ensured continued western access to crude oil. The movie's website states that \"‘Syriana’ is a real term used by Washington think-tanks to describe a hypothetical reshaping of the Middle East.\" Gaghan said he saw Syriana as \"a great word that could stand for man's perpetual hope of remaking any geographic region to suit his own needs.\"\nThe word \"Syriana\" derives from \"Syria\" + the Latin suffix \"-ana\"; it means, roughly, \"in the manner of Syria.\" Historically, Syria refers not to the state that since 1944 has borne the name, but to a more extensive land stretching from the eastern shores of the Mediterranean Sea to the middle Euphrates River and the western edge of the desert steppe, and from the Tauric system of mountains in the north to the edge of the Sinai desert in the south. This land was part of the Fertile Crescent, and has historically been a geopolitically crucial junction for trade routes from the east, from Asia Minor and the Aegean, and from Egypt, and has long been a focus of great power conflicts. The word \"Syria\" does not appear in the Hebrew original of the Scriptures, but appears in the Septuagint as the translation of Aram. Herodotus speaks of \"Syrians\" as identical with Assyrians, but the term's geographical significance was not well defined in pre-Greek and Greek times. As an ethnic term, \"Syrian\" came to refer in Antiquity to Semitic peoples living outside Mesopotamian and Arabian areas. Greco-Roman administrations were the first to apply the term to a definite district.\n\n\"Syriana\" was released on November 23, 2005 in limited release in only five theaters grossing $374,502 on its opening weekend. It went into wide release on December 9, 2005 in 1,752 theaters grossing $11.7 million on that weekend. It went on to make $50.8 million in North America and $43.1 million in the rest of the world for a worldwide total of $93.9 million. Censor authorities in some parts of the Middle East censored parts of the movie, because it depicted foreigners being ill-treated. Although abuse of foreign workers is rife, the censor authorities deemed such scenes as insulting.\n\n\"Syriana\" received generally positive reviews. On the review aggregator website Rotten Tomatoes, it has a rating of 72% based on 196 reviews, with an average score of 6.9/10. The film has a score of 76 out of 100 on Metacritic based on 40 critics indicating \"Generally favorable reviews\".\n\nAs a motion picture, the main criticism, even among reviewers who praised the film, was the confusion created by following numerous stories. Most critics stated that it was almost impossible to follow the plot, though some, notably Roger Ebert, praised precisely that quality of the film and offered an interesting hidden story possibility (a covert deal between the U.S. and China involving oil being shipped through Kazakhstan and passed off as coming from a different source). The audience confusion mimics the confusion of the characters, who are enmeshed in the events around them without a clear understanding of what precisely is going on. As with Gaghan's screenplay for \"Traffic\", \"Syriana\" uses multiple, parallel storylines, jumping from locations in Texas, Washington D.C., Switzerland, Spain, and the Middle East, leading film critic Ebert to describe the film as hyperlink cinema.\n\n\"Time\" magazine's Richard Corliss wrote, \"Gaghan relies on Clooney's agnostic heroism to lure viewers into his maze. When they get there, they will find not a conventionally satisfying movie but a kind of illustrated journalism: an engrossing, insider's tour of the world's hottest spots, grandest schemes and most dangerous men.\" In his review for the \"Los Angeles Times\", Kenneth Turan wrote, \"This is conspiracy-theory filmmaking of the most bravura kind, but if only a fraction of its suppositions are true, we—and the world—are in a world of trouble.\" \"USA Today\" gave the film three-and-a-half stars out of four and wrote, \"Gaghan assumes his audience is smart enough to follow his explosive tour of global petro-politics. The result is thought-provoking and unnerving, emotionally engaging and intellectually stimulating.\" \"Entertainment Weekly\" gave the film a \"B−\" rating and Lisa Schwarzbaum wrote, \"it's also the kind of movie that requires a viewer to work actively for comprehension, and to chalk up any lack of same to his or her own deficiency in the face of something so evidently smart.\"\n\nIn his review for \"The New York Observer\", Andrew Sarris wrote: \"If anything, \"Syriana\" tends to oversimplify a mind-bogglingly multifaceted problem that cannot so easily be resolved by a diatribe against the supposedly all-powerful 'Americans.'\" \"Rolling Stone\" magazine's Peter Travers gave the film his highest rating and praised George Clooney's performance: \"This is the best acting Clooney has ever done—he's hypnotic, haunting and quietly devastating.\" Philip French, in his review for \"The Observer\", praised the film as \"thoughtful, exciting and urgent\". In his review for \"The Guardian\", Peter Bradshaw wrote, \"But what complicates the plot is writer-director Stephen Gaghan's reluctance to criticise America too much. Instead of complexity, there is a blank, uncompelling tangle, which conceals a kind of complacent political correctness.\"\n\nEbert named it the second-best film of 2005, behind \"Crash\". Peter Travers of \"Rolling Stone\" named it as the third best film of 2005. \"Entertainment Weekly\" ranked \"Syriana\" as one of the 25 \"Powerful Political Thrillers\" in film history.\n\nGeorge Clooney won an Academy Award for Best Actor in a Supporting Role and a Golden Globe Award for Best Supporting Actor in a Motion Picture. He was also nominated for a BAFTA Award for Best Actor in a Supporting Role.\n\nThe film won the Grand Prix of the Belgian Syndicate of Cinema Critics.\n\nThe National Board of Review named \"Syriana\" one of the best films of the year and Stephen Gaghan's screenplay as the Best Adapted Screenplay.\n\n\n"}
{"id": "6801618", "url": "https://en.wikipedia.org/wiki?curid=6801618", "title": "The Planet (film)", "text": "The Planet (film)\n\nThe Planet is a Swedish documentary film on environmental issues, released in 2006. The film was made by Michael Stenberg, Johan Söderberg and Linus Torell for the big screen and was shot in English to reach an international audience. It includes interviews with 29 environmental scientists and experts including Stephen Peake, Herman Daly, Lester Brown, Gretchen Daily, Mathis Wackernagel, Norman Myers, Jill Jäger, George Monbiot, Robert Costanza, Will Steffen, and Jared Diamond.\n\nAt 8pm GMT on 21 March 2007, as part of the OXDOX:MK documentary film festival, it became the first ever simultaneous round the world screening of a film. After the screening, a panel of leading environmental experts answered questions from around the world from the Berrill Lecture Theatre at The Open University, England.\n\nThe TV adaptation consists of four episodes of 50–60 minutes:\n\n\n"}
{"id": "37422114", "url": "https://en.wikipedia.org/wiki?curid=37422114", "title": "Titanium biocompatibility", "text": "Titanium biocompatibility\n\nTitanium was first introduced into surgeries in the 1950s after having been used in dentistry for a decade prior. It is now the metal of choice for prosthetics, internal fixation, inner body devices, and instrumentation. Titanium is used from head to toe in biomedical implants. One can find titanium in neurosurgery, bone conduction hearing aids, false eye implants, spinal fusion cages, pacemakers, toe implants, and shoulder/elbow/hip/knee replacements along with many more. The main reason why titanium is often used in the body is due to titanium's biocompatibility and, with surface modifications, bioactive surface. The surface characteristics that affect biocompatibility are surface texture, steric hindrance, binding sites, and hydrophobicity (wetting). These characteristics are optimized to create an ideal cellular response. Some medical implants, as well as parts of surgical instruments are coated with titanium nitride (TiN).\n\nTitanium is considered the most biocompatible metal due to its resistance to corrosion from bodily fluids, bio-inertness, capacity for osseointegration, and high fatigue limit. Titanium's ability to withstand the harsh bodily environment is a result of the protective oxide film that forms naturally in the presence of oxygen. The oxide film is strongly adhered, insoluble, and chemically impermeable, preventing reactions between the metal and the surrounding environment. The mechanical properties of the material and the loading conditions in the host have, conventionally, influenced material selection for different clinical applications: predominantly Ti6Al4V in orthopaedics while commercially pure titanium in dentistry.\n\nIt has been suggested that titanium's capacity for osseointegration stems from the high dielectric constant of its surface oxide, which does not denature proteins (like tantalum, and cobalt alloys). Its ability to physically bond with bone gives titanium an advantage over other materials that require the use of an adhesive to remain attached. Titanium implants last longer and much higher forces are required to break the bonds that join them to the body compared to their alternatives.\n\nThe surface properties of a biomaterial play an important role in determining cellular response (cell adhesion and proliferation) to the material. Titanium's microstructure and high surface energy enable it to induce angiogenesis, which assists in the process of osseointegration.\n\nTitanium can have many different standard electrode potentials depending on its oxidation state. Solid titanium has a standard electrode potential of -1.63V. Materials with a greater standard electrode potential are more easily reduced, making them better oxidizing agents. As can be seen in the table below, solid titanium prefers to undergo oxidation, making it a better reducing agent.\nTitanium naturally passivates, forming an oxide film that becomes heterogeneous and polarized as a function of exposure time to bodily environments. This leads to the increased adsorption of hydroxyl groups, lipoproteins, and glycolipids over time. The adsorption of these compounds changes how the material interacts with the body and can improve biocompatibility. In titanium alloys such as Ti-Zr and Ti-Nb, zirconium and niobium ions that are liberated due to corrosion are not released into the patient's body, but rather added to the passivation layer. The alloying elements in the passive layer add a degree of biocompatibility and corrosion resistance depending on the original alloy composition of the bulk metal prior to corrosion.\n\nProtein surface concentration, (formula_1), is defined by the equation\n\nformula_2\n\nwhere Q is the surface charge density in C cm, M is the molar mass of the protein in g mol, n is the number of electrons transferred (in this case, one electron for each protonated amino group in the protein), and F is the Faraday constant in C mol.\n\nThe equation for collision frequency is as follows:\n\nformula_3\n\nwhere D = 8.83 × 10 cm s is the diffusion coefficient of the BSA molecule at 310 K, d = 7.2 nm is the “diameter” of the proteinwhich is equivalent to twice the Stokes radius, NA = 6.023 × 10 mol is Avogadro's number, and c* = 0.23 g L (3.3 μM) is the critical bulk supersaturation concentration.\n\nWetting occurs as a function of two parameters: surface roughness and surface fraction. By increasing wetting, implants can decrease the time required for osseointegration by allowing cells to more readily bind to the surface of an implant. Wetting of titanium can be modified by optimizing process parameters such as temperature, time, and pressure (shown in table below). Titanium with stable oxide layers predominantly consisting of TiO2 result in improved wetting of the implant in contact with physiological fluid.\n\nMechanical abrasion of the titanium oxide film leads to an increased rate of corrosion.\n\nTitanium and its alloys are not immune to corrosion when in the human body. Titanium alloys are susceptible to hydrogen absorption which can induce precipitation of hydrides and cause embrittlement, leading to material failure. \"Hydrogen embrittlement was observed as an in vivo mechanism of degradation under fretting-crevice corrosion conditions resulting in TiH formation, surface reaction and cracking inside Ti/Ti modular body tapers.\" Studying and testing titanium behavior in the body allow us to avoid malpractices that would cause a fatal breakdown in the implant, like the usage of dental products with high fluoride concentration or substances capable of lowering the pH of the media around the implant.\n\nThe cells at the implant interface are highly sensitive to foreign objects. When implants are installed into the body, the cells initiate an inflammatory response which could lead to encapsulation, impairing the functioning of the implanted device.\n\nThe ideal cell response to a bioactive surface is characterized by biomaterial stabilization and integration, as well as the reduction of potential bacterial infection sites on the surface. One example of biomaterial integration is a titanium implant with an engineered biointerface covered with biomimetic motifs. Surfaces with these biomimetic motifs have shown to enhance integrin binding and signaling and stem cell differentiation. Increasing the density of ligand clustering also increased integrin binding. A coating consisting of trimers and pentamers increased the bone-implant contact area by 75% when compared to the current clinical standard of uncoated titanium. This increase in area allows for increased cellular integration, and reduces rejection of implanted device. \nThe Langmuir isotherm:\n\nformula_4,\n\nwhere c is the concentration of the adsorbate formula_1 is the max amount of adsorbed protein, B is the affinity of the adsorbate molecules toward adsorption sites. The Langmuir isotherm can be linearized by rearranging the equation to,\n\nformula_6\n\nThis simulation is a good approximation of adsorption to a surface when compared to experimental values. The Langmuir isotherm for adsorption of elements onto the titanium surface can be determined by plotting the know parameters. An experiment of fibrinogen adsorption on a titanium surface \"confirmed the applicability of the Langmuir isotherm in the description of adsorption of fibrinogen onto Ti surface.\"\n"}
{"id": "23649871", "url": "https://en.wikipedia.org/wiki?curid=23649871", "title": "Topological insulator", "text": "Topological insulator\n\nIn the bulk of a non-interacting topological insulator, the electronic band structure resembles an ordinary band insulator, with the Fermi level falling between the conduction and valence bands. On the surface of a topological insulator there are special states that fall within the bulk energy gap and allow surface metallic conduction. Carriers in these surface states have their spin locked at a right-angle to their momentum (spin-momentum locking). At a given energy the only other available electronic states have different spin, so the \"U\"-turn scattering is strongly suppressed and conduction on the surface is highly metallic. Non-interacting topological insulators are characterized by an index (known as Z topological invariants) similar to the genus in topology.\n\nThe \"protected\" conducting states in the surface are required by time-reversal symmetry and the band structure of the material. The states cannot be removed by surface passivation if it does not break the time-reversal symmetry, which does not happen with potential and/or spin-orbit scattering, but happens in case of true magnetic impurities (e.g. spin-scattering).\n\nTime-reversal symmetry-protected edge states were predicted in 1987 to occur in quantum wells (very thin layers) of mercury telluride sandwiched between cadmium telluride and were observed in 2007. In 2007, they were predicted to occur in three-dimensional bulk solids of binary compounds involving bismuth. A 3D \"strong topological insulator\" exists which cannot be reduced to multiple copies of the quantum spin Hall state. \n\nThe first experimentally realized 3D topological insulator state (symmetry-protected surface states) was discovered in bismuth-antimony in 2008. Shortly thereafter symmetry-protected surface states were also observed in pure antimony, bismuth selenide, bismuth telluride and antimony telluride using ARPES. Many semiconductors within the large family of Heusler materials are now believed to exhibit topological surface states. In some of these materials the Fermi level actually falls in either the conduction or valence bands due to naturally occurring defects, and must be pushed into the bulk gap by doping or gating. The surface states of a 3D topological insulator is a new type of 2DEG (two-dimensional electron gas) where the electron's spin is locked to its linear momentum.<ref name=\"doi10.1038/nature08234\"></ref>\n\nFully bulk insulating or intrinsic 3D topological insulator states exist in Bi-based materials. \nIn 2014 it was shown that magnetic components, like the ones in spin-torque computer memory, can be manipulated by topological insulators.\n\nThe effect is related to the metal-insulator transitions (Bose–Hubbard model).\n\nSpin-momentum locking in the topological insulator allows symmetry-protected surface states to host Majorana particles if superconductivity is induced on the surface of 3D topological insulators via proximity effects. (Note that Majorana zero-mode can also appear without topological insulators.)\nThe non-trivialness of topological insulators is encoded in the existence of a gas of helical Dirac fermions. Helical Dirac fermions, which behave like massless relativistic particles, have been observed in 3D topological insulators. Note that the gapless surface states of topological insulator differ from those in the Quantum Hall effect: the gapless surface states of topological insulator are symmetry-protected (i.e. not topological), while the gapless surface states in Quantum Hall effect are topological (i.e. robust against any local perturbations that can break all the symmetries). The Z topological invariants cannot be measured using traditional transport methods, such as spin Hall conductance, and the transport is not quantized by the Z invariants. An experimental method to measure Z topological invariants was demonstrated which provide a measure of the Z topological order.\n(Note that the term Z topological order has also been used to describe the topological order with emergent Z gauge theory discovered in 1991.) More generally (in what is known as the \"Ten-fold way\") for each spatial dimensionality, each of the 10 Altland-Zirnbauer symmetry classes of random Hamiltonians labelled by the type of discrete symmetry (time-reversal symmetry, particle-hole symmetry, and chiral symmetry) has a corresponding group of topological invariants (either formula_1, formula_2 or trivial) as described by the periodic table of topological invariants.\n\nThe most promising applications of topological insulator are spintronic devices and dissipationless transistors for quantum computers based on the quantum spin Hall effect and quantum anomalous Hall effect. In addition, topological insulator materials have also found practical applications in advanced magnetoelectronic and optoelectronic devices.\n\n"}
{"id": "3492028", "url": "https://en.wikipedia.org/wiki?curid=3492028", "title": "Toyota Canada Inc.", "text": "Toyota Canada Inc.\n\nToyota Canada Inc. (TCI) is the exclusive distributor of Toyota, Lexus and Scion cars, SUV’s and trucks in Canada. Founded in 1964, Toyota has sold more than 4 million vehicles in Canada through a national network of 285 Toyota, Lexus and Scion dealerships. TCI’s head office is located in Toronto, Ontario, with regional offices in Vancouver, Calgary, Montreal and Halifax and parts distribution centres in Toronto and Vancouver.\nIn January 2013, TCI became a subsidiary of Toyota Motor Corporation (TMC) with 51% ownership share and Mitsui & Co. Ltd. as minority 49% shareholder. The current CEO and president of Toyota Canada is Larry Hutchinson, who replaced Seiji Ichii on January 1, 2016.\nIn October 1990, TCI expanded its operations to begin selling luxury vehicles to Canadians through the Lexus brand. Twenty years later, in October 2010, TCI further expanded its sales operations to begin selling Scion branded vehicles in Canada. As of July 2014, there are 247 Toyota, 38 Lexus, and 92 Scion franchises in Canada.\n\nIn 2014 half (50.0%) of all Toyota vehicles sold in Canada were built at Toyota Motor Manufacturing Canada, Inc. (TMMC) while 83.0% of all Toyota vehicles sold in Canada were produced at one of Toyota's 14 plants throughout North America. By comparison, in 2001 only 32.1% of Toyota vehicles sold in Canada were produced in Canada and only 45.1% were produced in North America. In 2017, Toyota Canada Inc. experienced its best year of sales to date with 224,547 vehicles sold, a 3.1% increase in overall annual sales growth. As of December 2013, Toyota, its group companies and dealerships have invested more than CDN $9 Billion in Canada and employ more than 24,000 Canadians from coast-to-coast.\nBeginning September 2014, Toyota celebrated its 50th Anniversary in Canada. The anniversary festivities began with the introduction of a special edition 2015 Toyota Corolla S and special edition 2015 RAV4 sporting unique Canadian interior and exterior features and paint colours. Only 2000 of each of these vehicles were produced for Canada.\n\nToyota operates two vehicle manufacturing facilities in Canada at Toyota Motor Manufacturing Canada, Inc. (TMMC) which build popular Toyota and Lexus vehicles for the North American market. The Toyota Corolla, Lexus RX 350 and Lexus RX 450h Hybrid are manufactured at TMMC’s North and South plant in Cambridge, Ontario. The Toyota RAV4 is manufactured at TMMC’s Woodstock, Ontario plant which was opened in 2007. The Toyota Matrix built at TMMC's Cambridge plant from 2002 ceased production in June 2014 while the Toyota RAV4 EV (electric vehicle for the North American market) built at TMMC's Woodstock plant from 2012 ceased production in August 2014.\n\nSince opening in 1988, TMMC has built more than 6 million vehicles for Canadian and U.S. consumers with the vast majority (approximately 4.6 million) being exported to the United States. In September 2003, TMMC's Cambridge facility was expanded and became the first Toyota plant outside Japan to manufacture a luxury vehicle Lexus RX). Production was further expanded in 2014 to also produce the (Lexus RX 450h. It is expected to continue to be the only Lexus manufacturer outside Japan until Fall 2015 when Lexus ES 350 production is expected to commence at Toyota Motor Manufacturing Kentucky, Inc. (TMMK). In 2014, TMMC remains the largest Toyota plant in North America by production volume (579,411 vehicles) with a 15% increase in production versus 2013.\n\nThe Toyota's Canadian operation has received various awards and recognitions. TMMC's plant has earned 14 J.D. Power & Associates Plant Quality awards including SIX (6) Gold awards and TWO (2) coveted Platinum Plant Quality Award in 2011 and 2014 – the first Toyota plant outside Japan that has ever won this award.\n\nToyota also operates several parts manufacturing operations in Canada including Canadian Auto Parts Toyota, Inc. (CAPTIN), a wholly owned subsidiary of TMC which manufactures aluminum alloy wheels for the global market. Established in Delta, British Columbia in 1983, this 24,645 m2 facility produced approximately 1.7 million aluminum alloy wheels and employed 310 people in 2013. In August 2011, CAPTIN and the University of British Columbia announced a partnership to refine the manufacturing process for water-cooled die casting to produce stronger, lighter and lower-cost aluminum wheels.\n\nToyota Canada’s Cold Weather Testing Centre was established in 1974 in Timmins, Ontario to test vehicles from across Toyota’s global lineup to ensure optimal performance in extreme cold weather conditions. With the addition of a cold chamber, Toyota can test vehicles year-round in harsh sub-zero conditions to ensure that vehicles meet highest customer expectations.\n\nIn 2011, Toyota introduced the Star Safety System™ as standard on every new vehicle. It features six advanced accident avoidance safety technologies:\n\nIn January 2011, Toyota launched the Collaborative Safety Research Center (CSRC) to serve as a catalyst for the advancement of auto safety in North America. Toyota shares its talent, technology and data with many research partners, including the University of Toronto, who focus on some of the most pressing issues of the day, from improving driver-and-vehicle command interfaces to advanced pre-crash notification systems. Most importantly, Toyota shares the results of its research so that the entire automotive industry can benefit.\n\nToyota also partners with other stakeholders engaged in road safety research, in the development of safe driving programs and new safety technologies, and in hands-on driver education. \nThe Traffic Injury Research Foundation (TIRF) is Canada's road safety research institute. Since 2002, Toyota Canada has partnered with TIRF to conduct studies and develop educational resources for young drivers and, more recently, turned the focus to helping all Canadians better understand the often complex and wide ranging safety technologies in today's automobiles.\n\nkartSTART is a hands-on driver's education program that uses go-karts to introduce young drivers to the demands and dynamics of life behind the wheel.\n\nTCI markets hybrid electric vehicles under the Toyota and Lexus brands throughout Canada, including the Prius, the world’s first mass-produced gas-electric hybrid vehicle. Since the launch of the Prius in Canada in 2000, Toyota and Lexus have added an additional 10 hybrids and 1 hybrid plug-in vehicle to its Canadian model line-up:\n\nSince the introduction of the Toyota Prius in 2000, more than 75% of all hybrid vehicles sold in Canada were made by Toyota. In September 2014, Toyota celebrated the sale of its 100,000th hybrid vehicle in Canada. Toyota estimates that Toyota and Lexus hybrids have saved Canadians almost 260 million litres of fuel - the equivalent of 103 Olympic-sized swimming pools.\n\nSince 2001, Toyota Canada Inc. has maintained ISO 14001 registration for an effective environmental management system (EMS). TCI’s head office was the first Toyota facility in North America to achieve this certification. \nTCI has established a Corporate Environmental Policy which outlines its commitment towards continually reducing the daily impact of all its activities, services and operations. TCI also contributes to the publication of an annual North American Environmental Report on progress, success, and future inspirations. A number of Toyota’s Canadian dealerships have also achieved LEED Gold Standard, one of the strictest levels of LEED certification.\n\nSince 2000, through the Toyota Evergreen Learning Grounds, Toyota Canada and its dealerships have contributed over $3.2 million in grants to more than 5,500 schools and impacted more than 1.15 Million students through projects that transform school grounds into green outdoor classrooms.\n"}
{"id": "13058788", "url": "https://en.wikipedia.org/wiki?curid=13058788", "title": "Waste Connections of Canada", "text": "Waste Connections of Canada\n\nWaste Connections of Canada (formerly Progressive Waste Solutions) is Canadian waste collection company that provides non-hazardous solid waste collection, recycling and disposal services to commercial, industrial, municipal and residential customers in 13 U.S. states and the District of Columbia and six Canadian provinces. It serves customers with vertically integrated collection and disposal assets. Waste Connections of Canada Ltd. is a subsidiary of US-based Waste Connections.\n\n\n\n"}
{"id": "21577311", "url": "https://en.wikipedia.org/wiki?curid=21577311", "title": "West Cork oil spill", "text": "West Cork oil spill\n\nThe West Cork oil spill was an oil spill off the southern coast of Ireland. The spill was first identified by the European Maritime Safety Agency's CleanSeaNet satellite monitoring system on 14 February 2009. An Irish Air Corps marine patrol aircraft confirmed the slick's presence near the which was undergoing refuelling around the same time. The British Coastguard and Irish Department of Transport agreed that around 300 tonnes of oil were spilled. The Russian Navy accepted responsibility for the incident but disputed the quantity, claiming around 20-30 tonnes had been spilt either whilst washing the decks or pumping out the bilges of the carrier, the Russian Navy made no notification to any authority at the time of the spill. The oil spill drifted eastwards and there were fears that the spill would wash up on the coast of south eastern Ireland or Wales but it broke up before this.\n\nOn 14 February 2009, the Irish Coast Guard received a European Maritime Safety Agency (EMSA) surveillance report indicating the presence of water pollution off the south coast of Ireland. EMSA's CleanSeaNet system made the initial detection and reported up to four separate slicks. The Coast Guard dispatched an Irish Air Corps CASA CN-235 maritime patrol aircraft to investigate which confirmed the presence of oil on the surface of the sea around a Russian Navy oil tanker and the \"Admiral Kuznetsov\" aircraft carrier. The spill was located in international waters 80 km (50 mi) south of Fastnet Rock, Ireland's most southwesterly point, and spread over an area measuring 6.4 km (4 mi) by 8 km (5 mi).\nOn 16 February, the Russian naval attaché in Ireland confirmed that the carrier had been carrying out replenishment of fuel at sea from a Russian supply tanker. The attaché confirmed an internal investigation was being carried out into the cause of the incident and said that Russian aerial surveillance considered that approximately 300 tonnes of oil was on the sea surface but could not tell how this happened or whether it was from their refuelling operations. The Russian Navy offered no explanation for the presence of the oil but began an internal investigation into the matter. Admiral Korolev, the commander of the Russian ships stated that the refuelling proceeded in a routine manner and that there had been no leaks. The Russian Navy stated that it was willing to share data in an attempt to identify the origin of the spill but stated that it was \"not catastrophic in nature and [did] not present a threat to the coastal environment\". The \"Admiral Kuznetsov\" is normally accompanied by at least one ocean-going tug in case of breakdown, on this occasion she was also accompanied by the \"Admiral Chabanenko\". The carrier was en route to its home port following exercises in the Mediterranean Sea where, on 6 January, it suffered a fire which resulted in the death of one sailor.\n\nThe British Coastguard initially estimated the quantity of oil spilled at around 1,000 tonnes, later revised down to 522 and then to 300 tonnes, a figure with which the Irish Department of Transport agreed with. If the original 1,000 tonnes estimate had proven accurate then this oil spill would have been the biggest to have affected Great Britain and Ireland since the ran aground near Milford Haven in 1996.\nLate on 17 February 2009, the spill was confirmed to have broken into three streams moving eastwards along Ireland's south coast, at a distance of around 48–64 km (30-40 mi) offshore. On 18 February changing wind patterns and unexpected mild weather had pushed the oil slick, which had not moved significantly since the evening before, further from shore. The Irish Coast Guard ran computer simulations of the spill and expected some oil to dissolve or evaporate. Depending on weather conditions, the spill could have washed up onto the Irish south-east coast in late February and may have hit Wales shortly thereafter. On 20 February, however, the spill was reported to be moving very slowly eastwards and the Irish Coast Guard said that it was possible that the slick may avoid the Irish coast completely, owing to favourable winds. By 23 February the spill was moving at just five nautical miles per day and was south of Cork harbour and continuing to disperse. By 25 February the slick was expected to move further away from the Irish coastline whilst continuing to disperse and break up and on 27 February the Irish Coast Guard confirmed it now expected the spill to disperse before making landfall unless there was a significant change to the expected weather. EMSA continued to monitor the spill until its complete dispersal on 8 March.\n\nThe spill threatened birds, dolphins, porpoises and seals native to Ireland's south coast. The Irish Farmers' Association (IFA) Aquaculture division called on the relevant local authorities and the Environmental Protection Agency to ensure that shellfish farmers in counties Cork, Wexford and Waterford were protected, as the area accounted for over 25% of Ireland's mussels and oyster production. The Bord Iascaigh Mhara (Irish Fisheries Board) tested wild and farmed shellfish on the southern coast as a precautionary measure. Both the Irish and Russian authorities stated that the spill had not affected fishing areas or coastal habitats.\n\nBoth , a British destroyer, and , an Irish Naval Service vessel responded to the scene. Samples of the oil were taken for analysis at the British Maritime and Coastguard Agency's (MCA) labs in Edinburgh, and Ireland requested samples of oils carried on board the Russian vessels via the Russian Embassy in Dublin. The Irish Coast Guard contracted a tug, the \"Celtic Isle\", equipped with oil dispersal equipment from a Cork-based company to assist in the clean-up operation. The tug assessed and attempted to deploy skimmers on 18 February, but the Irish Coast Guard director said that international experience showed that success could be very limited. “unless this fuel oil can be sprayed within the first day of a spill, it is very difficult to deal with, and collection at sea has a success rate of about 1%”. The \"Celtic Isle\" was stood down by 26 February after having little success in recovering the oil. The EMSA’s pollution response vessel for the Atlantic region, the \"Galway Fisher\", also made its way to Cork to take on board anti-pollution equipment and remain on stand-by. The MCA laboratory confirmed that the oil involved was a light crude oil on 21 or 22 February and further results announced on 26 February revealed the oil to be of Russian origin.\n\nIt was reported that both the British and Irish governments had drafted emergency plans for the cleaning of any affected coasts. Oil washed up in Ireland was due to be recovered mechanically by local authorities using bulldozers and skimmers with the assistance and supervision of the Coast Guard. By 23 February it was reported that none of the Wexford, Waterford or Cork local authorities had in place oil pollution response plans as required by law since 1999, but Wexford's plan was almost complete.\n\nThe Russian Navy vessels left the area around 18 February, having completed refuelling. On 20 February the Russian military stated that it believed that the spill might have been caused during cleaning of the deck of the \"Admiral Kuznetsov\". They dispatched a high-level diplomatic delegation to Ireland to discuss the impact of the spill which included experts in the field and Vice-Admiral Vyacheslav Popov, the deputy commander of the Russian Navy. They met with the Irish Minister of State for the Department of Transport, Noel Ahern, and Irish Coast Guard officials on 23 February. The discussions included an assessment of the cost of recovery of the oil and the determination of liability for the spill.\n\nOn 24 February, 10 days after the initial spill, the Russian delegation admitted, with extreme regret, responsibility for the incident. The high-level Russian military delegation told the Irish Coast Guard that the incident may have occurred when bilges were inadvertently pumped out 80 km southeast of Fastnet Rock. The Russian internal investigation stated that \"technical malfunction and human error\" were the causes of the spill. The Irish and Russian governments still disagreed over the size of the spill with Russian estimates placing it at 20 to 30 tons.\n\nThe Irish Coast Guard expressed disappointment that notification of the pollution incident had not been made earlier as this would have made spraying more efficient and reduced potential risk. However, Irish Coast Guard director Chris Reynolds said he accepted the delegation’s explanation. The total cost of the Irish government's monitoring operation was estimated at €250,000. Bilateral discussions were held with regards a Russian contribution towards these costs.\n"}
