{"id": "38544034", "url": "https://en.wikipedia.org/wiki?curid=38544034", "title": "1930 Curuçá River event", "text": "1930 Curuçá River event\n\nThe 1930 Curuçá River event was a meteoric air burst that occurred on 13 August 1930 over the area of Curuçá River in Brazil. It is based on the account of a single investigator who interviewed witnesses to the purported event, who then wrote a letter to the Vatican Observatory.\n\nThe event received little attention until 1995, when British astronomer Mark E. Bailey found in the Vatican Library archives a 1931 issue of \"L'Osservatore Romano\", which contained a dispatch from the Franciscan monk Fedele d'Alviano. D'Alviano had visited the region five days after the event and interviewed people from the region; they told him they were frightened of what had happened. According to Bailey, the Curuçá event was one of the most important impact events of the 20th century.\n\nInspired by Bailey's article and based on images from Landsat satellites, the Brazilian astrophysicist Ramiro de la Reza was able to identify a potential astrobleme—the remains of a meteorite impact crater—measuring 1 km in diameter, to the southeast of the village of Argemiro.\n\nIn the first week of June 1997, de la Reza led an expedition organized by Rede Globo and co-financed by ABC Television of Australia, to the region where the event occurred. A raised circular feature was found, but evidence is still lacking attesting that it came from a meteor impact. A record from the seismological observatory of San Calixto in La Paz indicated that the crater may have been created on the same date. However, the earthquake that was recorded was smaller than what would be expected from the formation of a crater that size (a seismic signal that would have been detected by stations all over the world) and the signal was more consistent with a local event near the seismograph.\n\nThe initial mass of the meteor has been estimated at between 1,000 and 25,000 tons. Estimates for the energy released have varied from 9 kilotons, 100 kilotons, and 5 megatons, though most estimates place the energy at below 1 megaton. Regardless, the event was significantly smaller than the 10–15 megaton Tunguska event.\n\n\n"}
{"id": "38413", "url": "https://en.wikipedia.org/wiki?curid=38413", "title": "Activation energy", "text": "Activation energy\n\nIn chemistry and physics, activation energy is the energy which must be provided to a chemical or nuclear system with potential reactants to result in: a chemical reaction, nuclear reaction, or various other physical phenomena. \n\nThe activation energy (\"E\") of a reaction is measured in joules (J) and or kilojoules per mole (kJ/mol) or kilocalories per mole (kcal/mol).\n\nActivation energy can be thought of as the magnitude of the potential barrier (sometimes called the energy barrier) separating minima of the potential energy surface pertaining to the initial and final thermodynamic state. For a chemical reaction, or division to proceed at a reasonable rate, the temperature of the system should be high enough such that there exists an appreciable number of molecules with translational energy equal to or greater than the activation energy.\n\nThe term Activation Energy was introduced in 1889 by the Swedish scientist Svante Arrhenius.\n\nThe Arrhenius equation gives the quantitative basis of the relationship between the activation energy and the rate at which a reaction proceeds. From the equation, the activation energy can be found through the relation\n\nwhere \"A\" is the pre-exponential factor for the reaction, \"R\" is the universal gas constant, \"T\" is the absolute temperature (usually in kelvins), and \"k\" is the reaction rate coefficient. Even without knowing \"A\", \"E\" can be evaluated from the variation in reaction rate coefficients as a function of temperature (within the validity of the Arrhenius equation).\n\nAt a more advanced level, the net Arrhenius activation energy term from the Arrhenius equation is best regarded as an experimentally determined parameter that indicates the sensitivity of the reaction rate to temperature. There are two objections to associating this activation energy with the threshold barrier for an elementary reaction. First, it is often unclear as to whether or not reaction does proceed in one step; threshold barriers that are averaged out over all elementary steps have little theoretical value. Second, even if the reaction being studied is elementary, a spectrum of individual collisions contributes to rate constants obtained from bulk ('bulb') experiments involving billions of molecules, with many different reactant collision geometries and angles, different translational and (possibly) vibrational energies—all of which may lead to different microscopic reaction rates.\n\nIn some cases, rates of reaction \"decrease\" with increasing temperature. When following an approximately exponential relationship so the rate constant can still be fit to an Arrhenius expression, this results in a negative value of \"E\". Elementary reactions exhibiting these negative activation energies are typically barrierless reactions, in which the reaction proceeding relies on the capture of the molecules in a potential well. Increasing the temperature leads to a reduced probability of the colliding molecules capturing one another (with more glancing collisions not leading to reaction as the higher momentum carries the colliding particles out of the potential well), expressed as a reaction cross section that decreases with increasing temperature. Such a situation no longer leads itself to direct interpretations as the height of a potential spot.\n\nA substance that modifies the transition state to lower the activation energy is termed a catalyst; a catalyst composed only of protein and (if applicable) small molecule cofactors is termed an enzyme. It is important to note that a catalyst increases the rate of reaction without being consumed by it. In addition, while the catalyst lowers the activation energy, it does not change the energies of the original reactants or products. Rather, the reactant energy and the product energy remain the same and only the \"activation energy\" is altered (lowered).\n\nIn the Arrhenius equation, the term activation energy (\"E\") is used to describe the energy required to reach the transition state. Likewise, the Eyring equation is a similar equation that also describes the rate of a reaction. Instead of also using \"E\", however, the Eyring equation uses the concept of Gibbs energy and the symbol *formula_2 to denote the energy of the transition state. This implies that the equation is similar but not identical to the Arrhenius one, because the Gibbs energy contains an entropic term in addition to the enthalpic one.\n\n"}
{"id": "12770079", "url": "https://en.wikipedia.org/wiki?curid=12770079", "title": "Anakena", "text": "Anakena\n\nAnakena is a white coral sand beach in Rapa Nui National Park on Rapa Nui (Easter Island), a Chilean island in the Pacific Ocean. Anakena has two ahus; Ahu-Ature has a single moai and Ahu Nao-Nao has seven, two of which have deteriorated. It also has a palm grove and a car park.\n\nAnakena is unusual for Easter Island in that it is one of only two small sandy beaches in an otherwise rocky coastline.\nAccording to island oral traditions, Anakena was the landing place of Hotu Matu'a, a Polynesian chief who led a two-canoe settlement party here and founded the first settlement on Rapa Nui.\n\nIt was later a ceremonial centre where islanders read from Rongorongo boards.\n\nAnakena featured in the Tangata manu or Birdman cult as in years when the new Birdman was from the western clans, he would end his celebrations at Anakena.\n\nModern archaeology has found traces of human settlement at Anakena as early as 1200 CE, though linguistic and other analysis indicates a range of dates for first settlement of Rapa Nui between 300 and 1200 CE.\n\nAnakena has been the site of several archaeological digs including those of Katherine Routledge in 1914 and both William Mulloy and Thor Heyerdahl in the 1950s, and both of its ahus have been restored.\n\nAnakena was used as one of the film locations for the 1994 Kevin Reynolds film \"Rapa Nui\".\n"}
{"id": "515256", "url": "https://en.wikipedia.org/wiki?curid=515256", "title": "Balsam", "text": "Balsam\n\nBalsam is the resinous exudate (or sap), which forms on certain kinds of trees and shrubs. Balsam (from Latin balsamum \"gum of the balsam tree,\" ultimately from Semitic, Aramaic \"busma\", Arabic \"basham\" and Hebrew \"basam\", \"spice\", \"perfume\") owes its name to the biblical Balm of Gilead.\nBalsam is a solution of plant-specific resins in plant-specific solvents (essential oils). Such resins can include resin acids, esters, or alcohols. The exudate is a mobile to highly viscous liquid and often contains crystallized resin particles. Over time and as a result of other influences the exudate loses its liquidizing components or gets chemically converted into a solid material (i.e. by autoxidation).\n\nSome authors require balsams to contain benzoic or cinnamic acid or their esters. Plant resins are sometimes classified according to other plant constituents in the mixture, for example as:\n\nUsually, animal secretions (musk, shellac, beeswax) are excluded from this definition.\n\n\nSome balsams, such as Balsam of Peru, may be associated with allergies. In particular, Euphorbia latex (\"wolf's milk\") is strongly irritant and is cytotoxic.\n\n"}
{"id": "6370024", "url": "https://en.wikipedia.org/wiki?curid=6370024", "title": "Belene Nuclear Power Plant", "text": "Belene Nuclear Power Plant\n\nThe Belene Nuclear Power Plant () is a planned nuclear power plant 3 km from Belene and 11 km from Svishtov in Pleven Province, northern Bulgaria, near the Danube River. It was intended to substitute four VVER-440 V230 reactors of the Kozloduy Nuclear Power Plant that were decommissioned as a prerequisite for Bulgaria to join the European Union.\n\nOn June 11, 2010, the Bulgarian government announced that it would freeze indefinitely the planned construction of the Belene nuclear power plant because it was uncertain when the investment would be returned. Five months later, on December 2, a non-binding memorandum of understanding was signed between NEK EAD, Rosatom, Altran and Fortum, setting up a 6.3 bln. euro price on the power station, after months of unsuccessful talks on the cost and redeemability of the project itself. Further disagreement and the persistent demands of the Bulgarian government to lower the cost under 5.0 billion euro led to the termination of the project in March 2012. However, in late 2012 the opposition initiated a referendum petition which was supposedly signed by over 600 thousand people and the first national referendum in the history of modern Bulgaria was held on January 27, 2013. A majority of the people had voted ′Yes' but despite that, the number of voters who attended the voting was too low for it to pass. The referendum passed the question further to the Parliament, which decided on 27 February 2013 to suspend it. Later on 30 May 2013 the newly elected Cabinet Prime Minister Oresharski announced there is a possible restart for it.\n\nThe discussions on constructing a second nuclear power plant started in the early 1970s. The Belene site was approved for the construction of a second Bulgarian NPP by a Council of Ministers decree on 20 March 1981. The site was handed to the Ministry of Economics on 31 December 1981 and the documentation for the construction site's preparation was prepared in late 1980 and early 1981 by Energoproekt Sofia. The site's preparation in accordance with the draft projects began in the early 1981.\n\nThe foundations of the future power plant were laid in 1987 after the design of Atomenergoproekt Kiev from the USSR and Energoproekt Sofia. The design suggested the construction of four VVER-1000/V 320 reactors. Between 1988 and 1990 40% of the construction work of reactor 1 was finished and 80% of the equipment was supplied. The project was abandoned in 1990 due to the restoration of capitalism in Bulgaria and only conservational work was done. Since then, measures have been continuously undertaken to preserve the supplied equipment, the construction site and the buildings; various investigations and assessments have been carried out with respect to the site suitability and the equipment status, all of which yielded positive conclusions. New investigations have been performed in relation to site safety and its compliance with international requirements. There has been particularly extensive research on the seismic safety of the chosen site. A number of missions were carried out by the International Atomic Energy Agency (IAEA) and other bodies of authority. All these came up with positive conclusions and confirmations that the Belene site is suitable for the construction of a nuclear power plant.\n\nIn 2002 the Government decided in-principle for a restart of the Belene Project. Fulfillment of all legislative requirements allowed the Government to approve the construction of a nuclear power plant on the Belene site with total rated capacity of 2000 megawatts. The Ministry of Energy began to renew the available equipment and examine the possible construction of the new nuclear plant. In February 2003 Minister Milko Kovachev sent letters to six leading companies in the sphere of nuclear energy asking them to provide up-to-date technical, economic and financial information regarding the project.\n\nA working group of experts was formed by an order of the Ministry of Energy and Energy Resources of 27 May 2003, which included experts from the Ministry of Energy and Energy Resources, the Ministry of Environment and Waters, the Ministry of Transport and Communication, the Ministry of Internal Affairs, the Ministry of Health, the Nuclear Regulatory Agency, the State Energy and Water Regulatory Commission, the State Agency for Civil Protection, the National Electric Company and BulAtom. A programme for the expert commission's work was approved on 4 July 2003.\n\nPursuant to the above-mentioned decision, on May 10, 2005, the National Electric Company launched a procedure for selection of a Contractor for the engineering, procurement, and commissioning of Belene Nuclear Power Plant, Units 1&2.\n\nIn late October 2006 the offer of the Russian Atomstroyexport, the French Framatome (Areva), and the German Siemens using third-generation VVER-1000/V-446B reactors was approved by the National Electric Company. The offer was selected due to the highest safety level guaranteed by several new independent active and passive safety systems, as well as the option for Atomstroyexport to buy back the old unit supplied in the 1980s. Another reasons was the 60-year operation term. According to the Atomstroyexport president, the first unit would be in operation by 2013 and the second a year later.\n\nOn 7 December 2007 the European Commission gave its favourable opinion to the NPP, saying that it met all requirements of articles 41 to 44 of the Euratom Treaty.\nA favourable opinion of the EC is one of the requirements for a Euratom loan.\n\nOn 18 January 2008, Atomstroyexport and Bulgaria's National Electric Company (NEC) signed the contract for the design, construction and installation of units 1 and 2 of the Belene NPP. On 3 September 2008, the construction of the Belene NPP officially started. According to the Minister of Energy Petar Dimitrov, the Belene plant would operate \"the most secure reactors existing in the world\"; he also asserted that \"the chance there would be a failure in those reactors is practically zero\". Prime Minister Stanishev and Minister Dimitrov also called the project \"a Renaissance for Bulgaria's nuclear energy\" and \"the largest industrial project in Bulgaria in the last eighteen years\". More than 10,000 construction workers would be employed in the project, with the first reactor expected to be operating by the end of 2013, the second by the end of 2014.\n\nAccording to the schedule, Unit 1 of the A92 design has to be erected for 6.5 years and Unit 2 for 7.5 years with consideration of the specific licensing terms as per the Bulgarian legislation. The longest time consuming activities are related to the design work and equipment delivery (58 months), as well as to the very construction and installation (51 months).\n\nThe negotiations stalled again after the GERB government decided to add an American or a European contractor to the project, as well as insisting for Atomstroyexport to lower the price to less than five billion euro. As no major European or American investor appeared, the talks continued to yield no results. This led to the official termination of the Belene project in March 2012. A thermal powerplant using gas from the South Stream pipeline will be built on the site, and the reactor for Unit 1 will be assembled as Unit 7 at the Kozloduy NPP.\n\nIn June 2016, the International Court of Arbitration awarded Atomstroyexport €620 million in compensation for equipment already manufactured for the plant, which will be delivered to Bulgaria following payment.\n\nAlthough the project was cancelled by parliament, the Bulgarian Socialist Party government of Plamen Oresharski (elected in May 29, 2013) has spoken publicly about restarting the project.\n\nIn 2016 discussions took place with Rosatom about the possibility of installing one of the reactors at Kozloduy Nuclear Power Plant and selling the second one to a third party. An alternative is a privately financed completion of Belene.\n\nOn 7 June 2018, the Bulgarian Parliament voted to abolish the moratorium on the construction of the power plant. The aim of the government is to complete the project through funding by a strategic investor, what interest has so far been declared by several companies. Minister of Energy should develop an investor selection procedure and propose options for structuring the project by 31 October 2018. According to the Minister, the plant can be completed within 7-8 years.\n\nThe AES 92 variant proposed for implementation at the Belene NPP site is a new generation VVER type reactor that has been licensed by regulatory authorities in Russia. It has been declared to meet all safety requirements as well as recommendations from the IAEA and INSAG and has been also confirmed by a special analysis of leading experts from EDF based on the recognized European Utility Requirements. This variant of the AES can make use of the two partially completed Belene NPP reactors started in the 1980s. The design directly uses a majority of the already built civil structures and facilities related to generic plant needs. Existing foundations and civil structures will be further evaluated with a view to incorporation into the new design.\n\nBelene NPP will be a Pressurized Water Reactor design with four first-stage coolant circulation loops per reactor. Reactor nominal thermal power is 3010 MWt and electrical net power is 1011 MWe. A typical refueling process takes 14 days with annual outage of between 20-28 and 40–50 days, depending on the scope of ongoing repair works. The AES 92 Reactor Facility has a 60-year lifetime design.\n\nBeing a Light Water Reactor of third generation type the AES 92 design has improved safety as well as technical and economic features. Main advantages of the AES 92 design over existing VVER nuclear power plants of the previous generation are:\n\nA specific feature of the third generation reactor presented in the AES 92 variant is the provision of a \"core catcher\" for severe accident cases. This insures against a containment integrity violation and release of highly radioactive substances into the environment.\n\nImproved safety in the AES 92 design accounts for projected improvements in radiation protection parameters during its operation. The calculated individual effective dose of personnel exposure for this design is 1 mSv/a, which is commensurate with the permissible annual effective dose limit for a person in the general population and in compliance with the Bulgarian normative base and European directives. As to the design dose rate per person of the greater population due to radioactive releases by the NPP: this value for the AES 92 variant is lower than 0.05 mSv/a, which represents less than 1/3 of the permissible dose rate as per article 10 of NRA’s regulations ensuring the safety of NPPs. This dose is pertinent to the effect of all sources of releases at the site. The normalized annual radioactive releases are also indicative for the improved environmental impact parameters (for 1000 MW). While target criteria stipulated for the unit design was < 6.7 GBq/a for liquid releases, the design value for AES 92 is lower by one order - 0.11 GBq/a. Similarly, with target criteria below 33.3 TBq/a for the liquid releases, the design value for AES 92 is 2.9 TBq/a.\n\nSpent Nuclear Fuel (SNF) management and the Radioactive Waste (RAW) handling is an important aspect of the environmental impact assessment for the new plant. The fuel envisaged to be used for AES 92 allows for up to 50 years storage of spent fuel assemblies in a storage pool and then an additional 10 years of “dry” storage keeping. The general plan provides for SNF transportation to Russia for reprocessing.\n\nPlasma incineration of solid wastes is envisaged for high-level waste generated during plant operation, with separate processing of liquid wastes depending on radioactivity. The anticipated total amount of conditioned wastes that will require disposal is less than 50 m per year for one reactor unit.\n\nThere is controversy over the Environmental Impact Assessment (EIA), which \"does not contain adequate information on the seismic conditions, nor does it address beyond design basis accidents or give details of the potential impacts of decommissioning\". Furthermore, following legal action by environmental groups, the authors of the original EIA confirmed, in court, that it was flawed and would require a new EIA once a designer and builder were appointed. The total cost of the project is now estimated by the operator to be around €7 billion (€4 billion for the power stations plus associated infrastructure development costs).\n\nEnvironmental organizations Greenpeace, Friends of the Earth (Europe), Urgewald, Bankwatch, World Information Service on Energy and the Bulgarian NGO BeleNE! oppose the plant's construction, and have expressed the following concerns:\n\n\nConcerns regarding the construction of the plant have mainly been felt in nearby Romania, with articles in the newspapers such as \"Cotidianul\", \"România Liberă\" and \"Ziarul\" even going as far as comparing the project with Chernobyl despite a new generation of VVER reactors is to be used, and not the cheaper graphite-moderated RBMK series like Chernobyl's.\n\nCritics say the project is economically flawed, open to corruption and mismanagement, and will cement Russian dominance of Bulgaria's energy sector.\n\n"}
{"id": "1133032", "url": "https://en.wikipedia.org/wiki?curid=1133032", "title": "Bobby Jindal", "text": "Bobby Jindal\n\nPiyush \"Bobby\" Jindal (born June 10, 1971) is an American politician who was the 55th Governor of Louisiana between 2008 and 2016, and previously served as a U.S. Congressman and as the vice chairman of the Republican Governors Association.\n\nIn 1996, Jindal was appointed secretary of the Louisiana Department of Health and Hospitals and in 1999, at age 28, he was appointed as the youngest president in the history of the University of Louisiana System. In 2001, President George W. Bush appointed Jindal as principal adviser to the U.S. Secretary of Health and Human Services.\n\nHe first ran for governor of Louisiana in 2003, but lost in the run-off election to Democratic candidate, Kathleen Blanco. In 2004, he was elected to the U.S. House of Representatives, becoming the second Indian American in Congress, and was re-elected in 2006. Jindal ran for governor again in the 2007 election and won, making him, at 36 years old, the second youngest governor of Louisiana after Huey P. Long, who was 35 when he was elected in 1928. Jindal was re-elected in 2011 in a landslide, winning more than 65% of the vote. He was the first Indian American governor, and the only one until South Carolina Governor Nikki Haley assumed office in 2011.\n\nOn June 24, 2015, Jindal announced his candidacy for the Republican nomination in the 2016 presidential election. He suspended his campaign in November 2015, subsequently announcing his support for Marco Rubio, who suspended his campaign on March 15, 2016.\n\nPiyush Jindal was born on June 10, 1971 in Baton Rouge, Louisiana. He is the first of two sons of Raj (née Gupta) and Amar Jindal, from Punjab, India. His father is a civil engineer and graduate of Guru Nanak Dev University and Punjab University. His mother is a graduate of Rajasthan University and worked in nuclear physics at the Post Graduate Institute of Medical Education and Research in Chandigarh. Before immigrating to the United States, both his parents were lecturers at an Indian engineering college.\n\nAt the time of their move to the U.S., Raj Jindal was to be a doctoral candidate in physics. They left Malerkotla, Punjab in January 1971, six months before their son was born. Jindal's paternal grandfather was a merchant from Khanpur, Samrala and his maternal grandfather was a Ferozepur banker.\n\nThe family settled near Louisiana State University. Jindal attended Baton Rouge Magnet High School, graduating in 1988. While in high school, he competed in tennis tournaments, started various enterprises such as a computer newsletter, retail candy business, and a mail-order software company. He spent free time working in the stands at LSU football games.\n\nJindal graduated from Brown University in 1992 at the age of 20, with honors in two majors, biology and public policy.\n\nJindal was one of only 50 students nationwide admitted to the Program in Liberal Medical Education (PLME), guaranteeing him a place at Brown Medical School. He has been credited with leading Brown University's College Republicans student group.\n\nJindal was named to the 1992 \"USA Today\" All-USA Academic Team. He applied to and was accepted by both Harvard Medical School and Yale Law School, but studied as a Rhodes Scholar where he received an MLitt in political science with an emphasis in health policy from the New College, Oxford in 1994. The subject of his thesis was \"A needs-based approach to health care\".\n\nAfter completing his studies at Oxford, Jindal turned down an offer to study for a D.Phil. in politics because his family couldn't afford to pay for his studies. Instead, Jindal joined the consulting firm McKinsey & Company. He then interned in the office of Rep. Jim McCrery of Louisiana, where McCrery assigned him to work on healthcare policy; Jindal spent two weeks studying Medicare to compile an extensive report on possible solutions to Medicare's financial problems, which he presented to McCrery.\n\nIn 1993, U.S. Representative Jim McCrery (whom Jindal had worked for as a summer intern) introduced him to Governor Mike Foster. In 1996, Foster appointed Jindal as Secretary of the Louisiana Department of Health and Hospitals, an agency that represented about 40 percent of the state budget and employed over 12,000 people. Foster called Jindal a genius who had a great deal of medical knowledge. Jindal was 24 at the time.\n\nDuring his tenure, Louisiana's Medicaid program went from bankruptcy with a $400 million deficit into three years of surpluses totaling $220 million.\n\nJindal was criticized during the 2007 campaign by the Louisiana AFL-CIO for closing some local clinics to reach that surplus. Under Jindal's term, Louisiana nationally rose to third place in child healthcare screenings, with child immunizations rising, and introduced new and expanded services for the elderly and the disabled.\n\nIn 1998, Jindal was appointed executive director of the National Bipartisan Commission on the Future of Medicare, a 17-member panel charged with devising plans to reform Medicare. In 1999, at the request of the Louisiana governor's office and the Louisiana State Legislature, Jindal examined how Louisiana might use its $4.4 billion share of the tobacco settlement.\n\nIn 1998, Jindal received the Samuel S. Beard Award for greatest public service by an individual 35 years old or under, an award given annually by Jefferson Awards.\n\nAt 28 years of age in 1999, Jindal was appointed to become the youngest-ever president of the University of Louisiana System, the nation's 16th largest system of higher education with over 80,000 students.\nIn March 2001, he was nominated by President George W. Bush to be Assistant Secretary of Health and Human Services for Planning and Evaluation. He was later unanimously confirmed by a vote of the United States Senate and began serving on July 9, 2001. In that position, he served as the principal policy adviser to the Secretary of Health and Human Services. He resigned from that post on February 21, 2003, to return to Louisiana and run for governor. He was assigned to help fight the nurse shortage by examining steps to improve nursing education.\n\nJindal came to national prominence during the 2003 election for governor of Louisiana. In what Louisianans call an \"open primary\" (but which is technically a nonpartisan blanket primary), Jindal finished first with 33 percent of the vote. He received endorsements from the largest paper in Louisiana, the \"Times-Picayune\"; the newly elected Democratic mayor of New Orleans, Ray Nagin; and the outgoing Republican governor, Mike Foster.\n\nIn the second balloting, Jindal faced the outgoing lieutenant governor, Kathleen Babineaux Blanco of Lafayette, a Democrat. Despite winning in Blanco's hometown, he lost many normally conservative parishes in north Louisiana, and Blanco prevailed with 52 percent of the popular vote.\n\nSome political analysts blamed Jindal's loss for his refusal to answer questions targeted at his religion and ethnic background brought up in several Democratic advertisements, which the Jindal campaign called \"negative attack ads.\" Despite losing the election in 2003, the run for governor made Jindal a well-known figure on the state's political scene and a rising star within the Republican Party.\n\nA few weeks after the 2003 gubernatorial runoff, Jindal decided to run for Louisiana's 1st congressional district. The incumbent, David Vitter, was running for the Senate seat being vacated by John Breaux. The Louisiana Republican Party endorsed him in the primary although Mike Rogers, also a Republican, was running for the same seat. The 1st District has been in Republican hands since a 1977 special election and is widely considered to be staunchly conservative. Jindal's campaign was able to raise over $1 million very early in the campaign, making it harder for other candidates to effectively raise funds to oppose him. He won the 2004 election with 78 percent of the vote.\n\nJindal was only the second Indian-American to be elected to the United States Congress, after Dalip Singh Saund was elected in November 1955.\n\nJindal won re-election to a second term with 88% of the vote.\n\nHe was the second Indian American elected to Congress. He has reportedly lived in Kenner, Metairie, and Baton Rouge.\n\nIn 2005, Jindal criticized Bush's budget for not calling for enough spending cuts. He warned of the growth of Medicaid saying \"Congress may act without them...there seems to be growing momentum that the status quo is not defensible.\" Jindal praised Bush's leadership on social security reform saying \"The administration has a lot more work to do to continue educating the American people about the very serious challenges facing Social Security.\"\n\nIn response to Hurricane Katrina, Jindal stated \"If we had been investing resources in restoring our coast, it wouldn't have prevented the storm, but the barrier islands would have absorbed some of the tidal surge.\"\n\n\nHe was made vice-chairman of the House Subcommittee on the Prevention of Nuclear and Biological Attacks. Jindal served as president of the incoming freshman class of congressmen, in 2004. He was elected to the position of House assistant majority whip, a senior leadership role. He served in this capacity from 2004 to 2006.\n\nOn January 22, 2007, Jindal announced his candidacy for governor. Polling data showed him with an early lead in the race, and he remained the favorite throughout the campaign. He defeated eleven opponents in the nonpartisan blanket primary held on October 20, including two prominent Democrats, State Senator Walter Boasso of Chalmette and Louisiana Public Service Commissioner Foster Campbell of Bossier City, and an independent, New Orleans businessman John Georges.\n\nJindal finished with 699,672 votes (54 percent). Boasso ran second with 226,364 votes (17 percent). Georges finished with 186,800 (14 percent), and Campbell, who is also a former state senator, ran fourth with 161,425 (12 percent). The remaining candidates collectively polled three percent of the vote. Jindal polled pluralities or majorities in 60 of the state's 64 parishes (equivalent to counties in other states). He lost narrowly to Georges in Orleans Parish, to Boasso in St. Bernard Parish (which Boasso represented in the Legislature), and in the two neighboring north Louisiana parishes of Red River and Bienville located south of Shreveport, both historically Democratic and supported Campbell. In the 2003 contest with Blanco, Jindal had lost most of the northern parishes. This marked the first time that a non-incumbent candidate for governor was elected without a runoff under the Louisiana election system.\n\nAs governor-elect, Jindal named a new ethics team, with Democratic Shreveport businesswoman Virginia Kilpatrick Shehee, the first woman to have served in the state senate, as the vice chairman of the panel. Jindal assumed the position of governor when he took the oath of office on January 14, 2008. At thirty-six, he became the youngest sitting governor in the United States. He is also Louisiana's first non-white governor since P. B. S. Pinchback served for thirty-five days during Reconstruction, and the first non-white governor to be elected (Pinchback succeeded to the position of lieutenant governor on the death of Oscar Dunn, then to governor upon the impeachment of Henry Clay Warmoth). Additionally, Jindal became the first Indian American to be elected governor of any state in the United States.\n\nIn 2008, Jindal was ranked one of the nation's most popular governors with an approval rating of 77%.\n\nOne of Jindal's first appointments was that of Mike Edmonson as superintendent of the Louisiana State Police. Edmonson had been for twenty preceding years the bodyguard and confidant of LSU Tigers football coaches. Edmonson was also the deputy secretary of the Department of Public Safety, an agency with more than 2,900 employees and a budget of nearly $500 million.\n\nIn 2014, Jindal was compelled to urge repeal of a state law that he had earlier signed which provided enhanced retirement benefits to Edmonson and, inadvertently, to one other state trooper. Jindal said that he was unaware that the legislation, called in the media the \"Edmonson Act,\" applied only to two persons. He urged the legislature to rewrite the law. Thereafter, Janice Clark, a state district court judge in Baton Rouge, declared that portion of the law enhancing the retirement benefits of Edmonson to be unconstitutional.\n\nAnother early appointee was that of former state representative Frank P. Simoneaux, a Baton Rouge attorney, as the chairman of the Louisiana Ethics Commission. Jimmy Faircloth, an attorney from Alexandria and Pineville, was the influential executive counsel from 2008 to 2009, when he stepped down to run unsuccessfully for the Louisiana Supreme Court. Faircloth was considered the legal architect of the special 2008 legislative session on ethics reform. He guided the Jindal administration through the aftermath of Hurricanes Gustav and Ike. After leaving the administration, he continued as a periodic legal advisor to Jindal.\nOn June 27, 2008, Louisiana's Secretary of State confirmed that a recall petition had been filed against Jindal in response to Jindal's refusal to veto a bill that would have more than doubled the current state legislative pay. During his gubernatorial campaign, Jindal had pledged to prevent legislative pay raises that would take effect during the current term.\n\nJindal responded by saying that he is opposed to the pay increase, but that he had pledged to let the legislature govern themselves.\n\nOn June 30, 2008, Jindal reversed his earlier position by vetoing the pay raise legislation, stating that he made a mistake by staying out of the pay raise issue. In response, the petitioners dropped their recall effort.\n\nStandard and Poor's raised Louisiana's bond rating and credit outlook from stable to positive in 2009. In announcing this change, the organization gave credit to the state's strong management and \"commitment to streamlining its government functions.\" Jindal met with President Barack Obama in October 2009 where the governor pushed for increased federal dollars to cover rising Medicaid costs, speeding the construction of hurricane-protection barriers, and financing the proposed Louisiana State University teaching hospital. During a town hall meeting, Obama praised Jindal as a \"hard working man who is doing a good job\" for the State, and expressed support for the governor's overhaul of the State's educational system in the area of increased charter schools.\n\nLouisiana state government watchdog C.B. Forgotston, former counsel to the House Appropriations Committee who supported Jindal's election in 2007, has expressed disappointment with the governor in regard to the legislative pay raise and other fiscal issues. Forgotston said he would grade Jindal an A+ in public relations and a D in fiscal performance in office.\n\nJindal negotiated an agreement whereby Foster Farms, a private chicken processor, would receive $50 million in taxpayer funds to purchase a chicken processing plant owned by bankrupt Pilgrim's Pride.\n\nSome claimed there is a conflict of interest in that Pilgrim's Pride founder Lonnie \"Bo\" Pilgrim contributed $2500 to Jindal's campaign in 2007. Other contributors to Jindal's campaign who benefited from economic development spending include Albemarle and Edison Chouest Offshore. Jindal however released a statement saying that this legislation saved over 1,000 jobs, serves as a stimulus to Louisiana's economy, and had wide bipartisan support.\nJindal oversaw one of the largest evacuations in U.S. history (nearly two million people) in late August 2008 prior to the Louisiana landfall of Hurricane Gustav. He issued mandatory evacuation orders for the state's coastal areas and activated 3,000 National Guardsman to aid in the exodus. He also ordered the state to purchase generators to provide needed power to hospitals and nursing homes without power. Government officials vacated hospitals and nursing homes and put the poor, the ill, and the elderly on buses and trains out of town. The evacuation was credited as one reason that Gustav resulted in only 16 deaths in the U.S. The state's successful response to Hurricane Gustav was in stark contrast to the failed hurricane response system for Hurricane Katrina in 2005. Jindal received bipartisan praise for his leadership during Gustav. Jindal had been scheduled to address the Republican National Convention, but cancelled his plans in order to focus on Louisiana's needs during the storm.\n\nJindal announced his intention to seek reelection in 2011. With high approval ratings and excessive amounts of campaign funds, Democrats struggled to land a recruit of any substance. Running against four Democrats, a Libertarian and four independents in the jungle primary, Jindal received 66% of the vote in the blanket primary, thereby winning election in the first round.\n\nIn August 2011, the American Legislative Exchange Council (ALEC) awarded Jindal the Thomas Jefferson Freedom Award for \"outstanding public service\".\n\nOn October 25, 2011, in preparing for his second term, Jindal tapped Republican state representative Chuck Kleckley of Lake Charles and State Senator John Alario of Westwego as his choices for Speaker of the Louisiana House of Representatives and Louisiana Senate President, respectively. Lawmakers routinely approved the governor's choices for the two leadership positions. Alario is a long-term Democrat who switched parties prior to the 2011 elections. Jindal in January 2012 elevated John C. White, the short-term superintendent at the Recovery School District in New Orleans, to the position of state superintendent of education.\n\nIn August 2012, Jindal declared a statewide state of emergency due to the threat of subsidence and subsurface instability that threatens the lives and property of the citizens of the state.\n\nBy the end of Jindal's second term, he saw a marked drop in his state popularity and problems such as a budget deficit and cuts to public expenditure. He could not stand for a third term because the governor of Louisiana is subject to term limits.\n\nIn January 2013, Jindal released a plan that would eliminate the Louisiana state income tax, which he felt would expand business investment in the state, and then raise sales taxes in order to keep the plan revenue-neutral. Self-styled taxpayer watchdog and former legislative aide C.B. Forgotston correctly predicted that Jindal's plan would fail to clear the legislature because of the higher sales taxes, the lack of needed support from Democrats, and the likelihood that the plan would not increase overall state revenues.\n\nOn April 8, 2013, the first day of the legislative session, Jindal dropped the plan after acknowledging some negative response to the plan from legislators and the public, but said he would still like the legislature to formulate its own plan that could end the state income tax.\n\nJindal announced, in September 2014, a six-point energy platform that would, among other things, open up energy production on federal land and eliminated proposed carbon restrictions.\n\nOn February 8, 2008, conservative radio host Rush Limbaugh mentioned on his syndicated show that Jindal could be a possible choice for the Republican vice presidential nomination in 2008. He said that Jindal might be perceived as an asset to John McCain's campaign because he has wide support in the conservative and moderate wings of the Republican Party and his immigrant past offsets McCain's white heritage. If McCain had won the presidency, he would have been the oldest president ever inaugurated to a first term. Heightening the speculation, McCain invited Jindal, Gov. Charlie Crist of Florida, Gov. Tim Pawlenty of Minnesota and McCain's former rivals Mitt Romney and Mike Huckabee to meet at McCain's home in Arizona on May 23, 2008, according to a Republican familiar with the decision; Romney, Huckabee, and Pawlenty, all of whom were already well acquainted with McCain, declined because of prior commitments. The meeting may have served a different purpose, such as consideration of Jindal for the opportunity to speak at the 2008 Republican National Convention, in a similar fashion to Barack Obama at the 2004 Democratic National Convention, cementing a place for him in the party and opening the gate for a future run for the presidency. Speculation was fueled by simultaneous July 21, 2008, reports that McCain was making a sudden visit to Louisiana to confer again with Jindal and that McCain was readying to name his running mate within a week. However, on July 23, 2008, Jindal said that he would not be the Republican vice presidential nominee in 2008. Jindal added that he \"never talked to the senator [McCain] about the vice presidency or his thoughts on selecting the vice president.\" Ultimately, on August 29, 2008, McCain chose then-Gov. Sarah Palin of Alaska as his running mate. While Jindal was given a prime-time speech slot at the party convention, he was not offered the keynote speech. During the presidential campaign, Jindal expressed admiration for both Senators McCain and Obama, and maintained that both have made positive contributions to the nation.\n\nOn February 24, 2009, Jindal delivered the official Republican response to President Obama's address to a joint session of Congress. Jindal called the president's economic stimulus plan \"irresponsible\" and argued against government intervention. He used Hurricane Katrina to warn against government solutions to the economic crisis. \"Today in Washington, some are promising that government will rescue us from the economic storms raging all around us,\" Jindal said. \"Those of us who lived through Hurricane Katrina, we have our doubts.\" He praised the late sheriff Harry Lee for standing up to the government during Katrina. The speech met with biting reviews from some members of both the Democratic and the Republican parties. Referring to Jindal as \"devoid of substantive ideas for governing the country\", political commentator Rachel Maddow summarized Jindal's Katrina remark as follows: \"[Jindal states that] since government failed during Hurricane Katrina, we should understand, not that government should not be allowed to fail again, but that government...never works. That government can't work, and therefore we should stop seeking a functioning government.\" David Johnson, a Republican political strategist criticized Jindal's mention of Hurricane Katrina, stating \"The one thing Republicans want to forget is Katrina.\" While Jindal's speech was poorly received by several Democratic and Republican critics, others argued that the speech should be judged on substance rather than delivery style.\n\nJindal's story of meeting Lee in the immediate aftermath of Hurricane Katrina was questioned following the speech, as Jindal was not in New Orleans at the time.\n\nOn February 27, 2009, a spokesman for Jindal clarified the timing of the meeting, stating that the story took place days after the storm. The opportunity to give the response to President Obama's speech was compared by some commentators to winning \"second prize in a beauty contest\", a reference to the board game Monopoly.\n\nJindal had been mentioned as a potential candidate for the 2012 presidential election. On December 10, 2008, Jindal indicated that he would likely not run for president in 2012, saying he will focus on his re-election in 2011 and that this would make transitioning to a national campaign difficult, though he did not rule out a possible 2012 presidential bid.\nSpeculation increased when Republicans chose Jindal to deliver the response to President Obama's first address to a joint session of Congress.\n\nThe Jindal for President Draft Council Inc. PAC was formed in 2009 to raise funds for a future presidential run. Jindal has stated that he has no involvement with the PAC.\n\nIn April 2010, while speaking at the Southern Republican Leadership Conference, Jindal ruled out running for the Republican nomination for President in 2012.\n\nIn 2012, Jindal traveled across the country in support of the Mitt Romney-Paul Ryan ticket. Because Louisiana and other Deep South states voted heavily for the GOP, Jindal could hence devote his campaign time elsewhere. In August 2012, \"Politico\" reported that \"Bobby Jindal would be considered [for] and would likely take\" appointment as United States Secretary of Health and Human Services in a potential Romney cabinet.\n\nAfter the defeat of Romney-Ryan, Jindal called for his party to return to \"the basics... If we want people to like us, we have to like them first,\" he said on the interview program \"Fox News Sunday\". As the incoming president of the Republican Governors Association, which had thirty members in 2013, Jindal questioned Romney for having criticized President Obama as having provided \"extraordinary financial gifts from the government\". In reply to Romney, Jindal said, \"You don't start to like people by insulting them and saying their votes were bought.\" Jindal said that his party must convince a majority of voters that it supports the middle class and the principle of upward mobility. He also criticized what he termed \"stupid\" remarks regarding rape and conception made in 2012 by defeated Republican U.S. Senate nominees Todd Akin in Missouri and Richard Mourdock in Indiana.\n\nIn November 2012, after the election, Jindal was featured in a \"Time\" magazine article titled \"2016: Let's Get The Party Started\", where he was listed as a possible Republican candidate for the presidency in 2016. The article cited his fiscal and social conservative policies and his Indian American background, which would bring diversity to the GOP.\n\nIn 2013, with polls showing Jindal's approval ratings in Louisiana falling significantly, some analysts wrote off Jindal as a serious national contender, though others pointed to Romney as an example of someone who still won the Presidential nomination despite poor approval ratings from his home state.\n\nIn October 2013, Jindal told \"Fox News Sunday\" that he was still mulling a 2016 presidential run.\n\nOn May 18, 2015, Jindal formed a presidential exploratory committee to determine whether he would run as a candidate in the 2016 presidential election, and he announced his candidacy on June 24.\n\nAs of early September, Jindal was polling at 1 percent among the Republican primary electorate. On November 17, 2015, Jindal appeared on \"Special Report with Bret Baier\" on the Fox News Channel, announcing that he was ending his run for president, saying \"I've come to the realization that this is not my time.\"\n\nDuring his campaign, Jindal called Donald Trump a \"narcissist\" and an \"egomaniacal madman\", but afterward said that he would support Trump because \"electing Donald Trump would be the second-worst thing we could do this November, better only than electing Hillary Clinton to serve as the third term for the Obama administration's radical policies.\"\n\nJindal has a 100% pro-life voting record according to the National Right to Life Committee. He opposes abortion in general, but does not condemn medical procedures aimed at saving the life of the mother that indirectly result in the loss of the unborn child, such as salpingectomy for an ectopic pregnancy.\n\nIn 2003, Jindal stated that he did not object to the use of emergency contraception in the case of rape if the victim requests it. While in the House of Representatives, he supported two bills to prohibit transporting minors across state lines to obtain an abortion; the bills aimed to prevent doctors and others from helping a minor avoid parental notification laws in their home state by procuring an abortion in another state. He opposed and voted against expanding public funding of embryonic stem cell research.\n\nJindal opposed the legalization of same-sex marriage. In Congress, he voted for the Federal Marriage Amendment to restrict marriage to a union between one man and one woman. He also voted against the Local Law Enforcement Hate Crimes Prevention Act of 2007. In December 2008, Jindal announced the formation of the Louisiana Commission on Marriage and Family,\n\nFollowing the 2013 Supreme Court's rulings on DOMA and Proposition 8, he said: \"I believe every child deserves a mom and a dad. This opinion leaves the matter of marriage to the states where people can decide. In Louisiana, we will opt for traditional marriage. How about we let the people decide for themselves, via their representatives and via referendum?\"\n\nIn April 2015, Jindal announced that he would sign into law the Louisiana Marriage and Conscience Act proposed by newly elected Republican state representative Mike Johnson. In a guest editorial in \"The New York Times\", Jindal said that he has been contacted by several corporations who oppose the bill: \"They are free to voice their opinions, but they will not deter me.\" Johnson's bill proposed to bar the state from revoking licenses or refusing to engage in contract with individuals or businesses because they oppose marriage between two persons of the same sex. Johnson's bill was meant to guarantee the tax status of groups that support only traditional marriage. In May 2015, the legislature killed the measure. Four Republican members, Pete Huval of Breaux Bridge, Gregory A. Miller of Norco, Clay Schexnayder of Gonzales, and Nancy Landry of Lafayette, joined Democrats in killing the bill. Jindal responded by issuing Executive Order BJ-2015-8, (the \"Marriage and Conscience Order\"), which attempted to achieve the goals of the failed legislation. Johnson said he intended to re-introduce the measure in 2016.\n\nHe vetoed state legislation to increase pay for state legislators. However, the Louisiana governor's office was ranked last for transparency in the United States both prior, and subsequent, to Jindal's election, as reported by the WDSU I-Team. At least two legislators, state representatives Walker Hines and Neil Abramson, argued that this may be attributed to legislation that removed the governor's records from the public domain; they argued that the legislation was surreptitiously inserted as a last-minute amendment into an education bill by Jindal's office on the last day of the 2008 session, providing no time to properly review it before it passed the legislature and was signed into law by Jindal.\n\nIn 2014, Jindal signed into law a bill sponsored by Democratic state representative Jeff Arnold of New Orleans to permit Francis C. Heitmeier, a Democratic former member of both houses of the Louisiana Legislature and an unsuccessful 2006 candidate for Louisiana Secretary of State, to lobby legislators even though Heitmeier's brother, David Heitmeier, was, at the time, the sitting senator for District 7, which includes the Algiers neighbourhood of New Orleans. The special exemption permitted an immediate family member of an elected official who was a lobbyist for the executive branch of state government for the year prior to 9 January 2012, to be able to lobby the legislature. David Heitmeier abstained from voting on the measure which was written with the intent of benefiting Francis Heitmeier.\n\nJindal stated his support of the Second Amendment's right to bear arms. He opposed efforts to restrict gun rights and received an endorsement from the National Rifle Association. Jindal earned an A rating from Gun Owners of America while he was in Congress.\n\nAs a Congressman, he sponsored the Disaster Recovery Personal Protection Act of 2006 with Senator Vitter.\n\nIn July 2015, during an interview with CBS, Jindal stated that he supported stricter background checks, and that every state should begin to enact tougher background checks on gun buyers. He hunts.\n\nAs a private citizen, Jindal voted in 2002 for the Louisiana constitutional amendment known as the Stelly Plan which lowered some sales taxes in exchange for higher income taxes. After taking office, Jindal cut taxes a total of six times, including the largest income tax cut in Louisiana's history – a cut of $1.1 billion over five years, along with accelerating the elimination of the tax on business investments. In January 2013, Jindal said he wanted to eliminate all Louisiana corporate and personal income taxes, without giving details for his proposal.\n\nAs U.S. Representative from Louisiana, Jindal received grades of B in 2005, B- in 2006, and C in 2007 from the National Taxpayers Union, a conservative taxpayers advocacy organization. As Governor of Louisiana, Jindal received grades of A in 2010, B in 2012, and B in 2014 from the Cato Institute, a libertarian think tank, in their biennial Fiscal Policy Report Card on America's Governors.\n\nIn 2008, Jindal came out in favour of the Common Core State Standards Initiative, which Louisiana adopted in 2010.\n\nIn 2014, Jindal wrote that \"It has become fashionable in the news media to believe there is a right-wing conspiracy against Common Core.\"\n\nIn 2015, Jindal said that investments in technology would render Common Core obsolete.\n\nJindal proposed budgets that impose cuts on higher education funding in Louisiana, leading to protests from students and education advocates. Jindal proposed several controversial education reforms, including vouchers for low income students in public schools to attend private institutions using Minimum Foundation Program funds.\n\nThe legislation also included controversial changes in teacher evaluations, tenure and pensions. Hundreds of teachers, administrators and public education supporters protested against the legislation at the capital of Louisiana, some of whom cancelled classes to attend demonstrations. Many participants circulated petitions to recall Jindal and Republican House Speaker Chuck Kleckley. In April 2012, a Louisiana Public Broadcasting program examined possible conflicts between aspects of the Jindal education reform plan and the federal desegregation orders still in place in many parts of Louisiana.\n\nJindal said he believed that every child learns differently. For him, some will accomplish great things in a public school while some will learn better in an online program, and still others will make waves in charter schools, or in parochial schools and dual-enrollment programmes. However, in all these choices, the parents must be trusted to make the best decisions for their children.\n\nJindal signed a law that permitted teachers at public schools to supplement standard evolutionary curricula with analysis and critiques that may include intelligent design. The law forbids \"the promotion of any religious doctrine and will not discriminate against religion or non-religion\". Louisiana ACLU Director Marjorie Esman said that if the act was utilized as written, it would be on firm constitutional footing, stating that the Act is \"susceptible to a constitutional challenge.\" Despite calls for a veto from John Derbyshire and some genetics professors at Brown University, Jindal signed the Louisiana Academic Freedom Act which passed with a vote of 94–3 in the State House and 35–0 in the State Senate in 2008.\n\nThe Society for Integrative and Comparative Biology rejected New Orleans as a site for their 2010 meeting and the American Society for Biochemistry and Molecular Biology will not conduct future meetings in Louisiana.\n\nJindal opposed the Fairness Doctrine believing it to be a violation of the Constitution's guarantee of free speech and vowed protection of property rights. Jindal voted to extend the Patriot Act, voted in favour of the Military Commissions Act of 2006, supported a constitutional amendment banning flag burning, and voted for the Real ID Act of 2005.\n\nIn the 2009 legislative session, Jindal expressed support for a bill by state representative James H. \"Jim\" Morris of Oil City, which would permit motorcyclists to choose whether or not to wear a helmet. Morris' bill easily passed the House but was blocked in the\nSenate Health Committee.\n\nHe criticized illegal immigration as a drain on the economy, as well as being unfair to those who entered the country by legal means. He voted to build a fence along the Mexican border and opposed granting amnesty for illegal immigrants.\n\nJindal refused to accept federal funds to expand Medicaid after the passage of the Affordable Care Act, costing his state $1.65 billion in federal health-care assistance for the poor. He supported increased health insurance portability; laws promoting coverage of pre-existing medical conditions; a cap on malpractice lawsuits; an easing of restrictions on importation of prescription medications; the implementation of a streamlined electronic medical records system; an emphasis on preventative care rather than emergency room care; and tax benefits aimed at making health insurance more affordable for the uninsured and targeted to promote universal access. During Jindal tenure, over 11,000 uninsured children were added to the State's Children's Health Insurance Program. He also opposed a federal government-run, single-payer system, but supported state efforts to reduce the uninsured population. He has also supported expanding services for autistic children, and promoted a national childhood cancer database. In collaboration with Health Secretary Alan Levine, Jindal drafted the Louisiana Health First Initiative. This plan focused on expanding health insurance coverage for the state's indigent population, increasing Medicaid choice, reducing fraud, authorizing funding of a new charity hospital, and increasing transparency in Medicaid by making performance measures available over the internet. Jindal supported co-payments in Medicaid. Due to a congressional reduction in federal Medicaid financing rates, the Jindal administration chose to levy the largest slice of cuts on the network of LSU charity hospitals and clinics, requiring some facilities to close.\n\nJindal issued an executive order increasing office recycling programs, reducing solid waste and promoting paperless practices, offering tax credit for hybrid fuel vehicles, increasing average fuel economy goals by 2010, as well as increasing energy efficiency goals and standards for the state. He has stated his opposition to and voted for the criminalization of oil cartels such as OPEC. As a representative in the House, he supported a $300-million bill to fund Louisiana coastal restoration. In addition, he was the chief sponsor of successful legislation to expand the Jean Lafitte National Historical Park by over . Jindal signed bill SB 469 that would limit actions aimed at oil and gas companies operating along the coast. Jindal has pledged state support for the development of economically friendly cars in northeastern Louisiana in conjunction with alternative energy advocate T. Boone Pickens. In September 2014, Jindal stated that global warming was more about increasing government regulation, and released an energy plan that was critical of the Obama Administration's policies.\n\nIn 2007, Jindal led the Louisiana House delegation and ranked 14th among House members in requested earmark funding at nearly $97 million (however in over 99% of these requests, Jindal was a co-sponsor and not the primary initiator of the earmark legislation). $5 million of Jindal's earmark requests were for state defense and indigent healthcare related expenditures, another $50 million was for increasing the safety of Louisiana's waterways and levees after breaches following Hurricane Katrina, and the remainder was targeted towards coastal restoration and alternative energy research. As governor, in 2008, Jindal used his line item veto to strike $16 million in earmarks from the state budget but declined to veto $30 million in legislator-added spending. Jindal vetoed over 250 earmarks in the 2008 state budget, twice the total number of such vetoes by previous governors in the preceding 12 years.\n\nJindal has been an opponent of the American Recovery and Reinvestment Act of 2009. Citing concerns that the augmentation of unemployment insurance may obligate the state to raise taxes on businesses, Jindal had indicated his intention to forgo federal stimulus plan funds ($98 million) aimed at increasing unemployment insurance for Louisiana. Louisiana has since been obligated to raise taxes on businesses because the unemployment trust fund had dropped below the prescribed threshold. Louisiana was set to receive about $3.8 billion overall. Jindal intends to accept at least $2.4 billion from the stimulus package. He called parts of the plan \"irresponsible\", saying that \"the way to lead is not to raise taxes and put more money and power in hands of Washington politicians.\"\n\nIn 2015, Jindal travelled to the UK to speak out against so-called \"no-go zones\" that are in London and other western cities. British Prime Minister David Cameron had earlier stated that there were not any no-go zones in the UK. Jindal later confirmed his meaning \"I knew that by speaking the truth we were going to make people upset.\" When later asked by CNN to provide specific examples, Jindal declined. He later added that some Muslim immigrants are trying to \"colonize\" cities in Europe and \"overtake the culture\", and that it could happen next in the U.S.\n\nJindal was raised in a Hindu household. He is of Indian descent and is a U.S. citizen by birthright. He converted to Christianity while in Baton Rouge Magnet High School. During his first year at Brown University, he was baptized into the Roman Catholic Church as an adult under the RCIA. His family attends weekly Mass at Saint Aloysius Parish in Baton Rouge.\nBefore Jindal was born, his father Amar Jindal was assistant professor of engineering at Punjab University in Chandigarh. After settling into Louisiana, both elder Jindals transitioned into new fields. Jindal's father went on to work with a Louisiana railroad company, and his mother transitioned into IT. Jindal's mother, Raj Jindal, serves now as information technology director for the Louisiana Workforce Commission (formerly the Louisiana Department of Labor) and served as Assistant Secretary to former State Labor Secretary Garey Forster during the administration of Gov. Mike Foster Jindal has a younger brother, Nikesh, who is a registered Republican and supported his brother's campaign for governor. Nikesh went to Dartmouth College, where he graduated with honors, and then Yale Law School. Nikesh is now a lawyer in Washington, D.C.\n\nJindal's nickname dates to his childhood identification with Bobby Brady, a character from the 1970s sitcom \"The Brady Bunch\". He has been known by his nickname ever since, although his legal name remains Piyush Jindal.\n\nIn 1997, Jindal married Supriya Jolly, who was born in New Delhi, India and moved to Baton Rouge with her parents when she was four years old. The two attended the same high school, but Supriya's family moved from Baton Rouge to New Orleans after her freshman year. They did not begin dating until much later when Jindal invited her to a Mardi Gras party after another friend had canceled. Supriya Jindal earned a bachelor's degree in chemical engineering and an M.B.A. degree from Tulane University. She is working on a PhD in marketing at Louisiana State University. She created The Supriya Jindal Foundation for Louisiana's Children, a non-profit organization aimed at improving math and science education in grade schools.\n\nThey have three children: Selia Elizabeth, Shaan Robert, and Slade Ryan. Shaan was born with a congenital heart defect and had surgery as an infant. The Jindals have been outspoken advocates for children with congenital defects, particularly those without insurance. In 2006, he and his wife delivered their third child at home, with him receiving medical coaching by phone to deliver their boy.\n\nJindal enjoys hunting in Louisiana.\n\nA list of Jindal's published writings up to 2001 can be found in the hearing report for his 2001 U.S. Senate confirmation. They include newspaper columns, law review articles, and first authorships in several scientific and policy articles that have appeared in the prominent \"Journal of the American Medical Association\", \"Journal of the Louisiana State Medical Association\", and \"Hospital Outlook\".\n\nJindal's pre-2001 writings include several articles in the \"New Oxford Review\", one of which later made news during his 2003 gubernatorial race. In that 1994 article titled \"Physical Dimensions of Spiritual Warfare\", Jindal described the events leading up to an apparent exorcism of a friend and how he felt unable to help her at the time. However, Jindal questioned whether what he saw was actually an example of \"spiritual warfare\".\nIn November 2010, Jindal published the book \"Leadership and Crisis\", a semi-autobiography significantly influenced by his experiences with the most recent Gulf Oil Spill caused by the Deepwater Horizon explosion.\n\nGovernor of Louisiana, 2003\nU.S. Representative, 1st Congressional District, 2004\nU.S. Representative, 1st Congressional District, 2006\nGovernor of Louisiana, 2007\nGovernor of Louisiana, 2011\n\n\n(1) Jindal's Inauguration as Louisiana's 55th Governor from January 14, 2008 \n\n(2) Second inauguration from January 9, 2012 \n\n(3) Final State of the State Address from April 13, 2015 \n\n"}
{"id": "421940", "url": "https://en.wikipedia.org/wiki?curid=421940", "title": "Bragg's law", "text": "Bragg's law\n\nIn physics, Bragg's law, or Wulff–Bragg's condition, a special case of Laue diffraction, gives the angles for coherent and incoherent scattering from a crystal lattice. When X-rays are incident on an atom, they make the electronic cloud move, as does any electromagnetic wave. The movement of these charges re-radiates waves with the same frequency, blurred slightly due to a variety of effects; this phenomenon is known as Rayleigh scattering (or elastic scattering). The scattered waves can themselves be scattered but this secondary scattering is assumed to be negligible.\n\nA similar process occurs upon scattering neutron waves from the nuclei or by a coherent spin interaction with an unpaired electron. These re-emitted wave fields interfere with each other either constructively or destructively (overlapping waves either add up together to produce stronger peaks or are subtracted from each other to some degree), producing a diffraction pattern on a detector or film. The resulting wave interference pattern is the basis of diffraction analysis. This analysis is called \"Bragg diffraction\".\n\nBragg diffraction (also referred to as the Bragg formulation of X-ray diffraction) was first proposed by Lawrence Bragg and his father William Henry Bragg in 1913 in response to their discovery that crystalline solids produced surprising patterns of reflected X-rays (in contrast to that of, say, a liquid). They found that these crystals, at certain specific wavelengths and incident angles, produced intense peaks of reflected radiation. The concept of Bragg diffraction applies equally to neutron diffraction and electron diffraction processes. Both neutron and X-ray wavelengths are comparable with inter-atomic distances (~ 150 pm) and thus are an excellent probe for this length scale.\n\nLawrence Bragg explained this result by modeling the crystal as a set of discrete parallel planes separated by a constant parameter \"d\". It was proposed that the incident X-ray radiation would produce a Bragg peak if their reflections off the various planes interfered constructively. The interference is constructive when the phase shift is a multiple of 2; this condition can be expressed by Bragg's law (see \"Bragg condition\" section below) and was first presented by Lawrence Bragg on 11 November 1912 to the Cambridge Philosophical Society. Although simple, Bragg's law confirmed the existence of real particles at the atomic scale, as well as providing a powerful new tool for studying crystals in the form of X-ray and neutron diffraction. Lawrence Bragg and his father, William Henry Bragg, were awarded the Nobel Prize in physics in 1915 for their work in determining crystal structures beginning with NaCl, ZnS, and diamond. They are the only father-son team to jointly win. Lawrence Bragg was 25 years old, making him the youngest physics Nobel laureate.\n\nBragg diffraction occurs when radiation, with a wavelength comparable to atomic spacings, is scattered in a specular fashion by the atoms of a crystalline system, and undergoes constructive interference. For a crystalline solid, the waves are scattered from lattice planes separated by the interplanar distance \"d\". When the scattered waves interfere constructively, they remain in phase since the difference between the path lengths of the two waves is equal to an integer multiple of the wavelength. The path difference between two waves undergoing interference is given by 2\"d\"sin\"θ\", where \"θ\" is the scattering angle (see figure on the right). The effect of the constructive or destructive interference intensifies because of the cumulative effect of reflection in successive crystallographic planes of the crystalline lattice (as described by Miller notation). This leads to Bragg's law, which describes the condition on \"θ\" for the constructive interference to be at its strongest:\n\nwhere \"n\" is a positive integer and \"λ\" is the wavelength of the incident wave. Note that moving particles, including electrons, protons and neutrons, have an associated wavelength called \"de Broglie wavelength\". A diffraction pattern is obtained by measuring the intensity of scattered waves as a function of scattering angle. Very strong intensities known as Bragg peaks are obtained in the diffraction pattern at the points where the scattering angles satisfy Bragg condition. As mentioned in the introduction, this condition is a special case of the more general Laue equations, and the Laue equations can be shown to reduce to the Bragg condition under additional assumptions.\n\nThe phenomena of Bragg diffraction by a crystal lattice shares similar characteristics with that of thin film interference, which has an identical condition in the limit where the refractive indices of the surrounding medium (e.g. air) and the interfering medium (e.g. oil) are equal.\n\nSuppose that a single monochromatic wave (of any type) is incident on aligned planes of lattice points, with separation formula_2, at angle formula_3. Points A and C are on one plane, and B is on the plane below. Points ABCC' form a quadrilateral.\nThere will be a path difference between the ray that gets reflected along AC' and the ray that gets transmitted along AB, then reflected along BC. This path difference is\n\nThe two separate waves will arrive at a point with the same phase, and hence undergo constructive interference, if and only if this path difference is equal to any integer value of the wavelength, i.e.\n\nwhere the same definition of formula_6 and formula_7 apply as above.\n\nTherefore,\n\nfrom which it follows that\n\nPutting everything together,\n\nwhich simplifies to formula_11 which is Bragg's law.\n\nIf only two planes of atoms were diffracting, as shown in the pictures, then the transition from constructive to destructive interference would be gradual as a function of angle, with gentle maxima at the Bragg angles. However, since many atomic planes are interfering in real materials, very sharp peaks surrounded by mostly destructive interference result.\n\nA rigorous derivation from the more general Laue equations is available (see page: Laue equations).\n\nA colloidal crystal is a highly ordered array of particles that forms over a long range (from a few millimeters to one centimeter in length); colloidal crystals have appearance and properties roughly analogous to their atomic or molecular counterparts. It has been known for many years that, due to repulsive Coulombic interactions, electrically charged macromolecules in an aqueous environment can exhibit long-range crystal-like correlations, with interparticle separation distances often being considerably greater than the individual particle diameter. Periodic arrays of spherical particles give rise to interstitial voids (the spaces between the particles), which act as a natural diffraction grating for visible light waves, when the interstitial spacing is of the same order of magnitude as the incident lightwave. In these cases in nature, brilliant iridescence (or play of colours) is attributed to the diffraction and constructive interference of visible lightwaves according to Bragg’s law, in a matter analogous to the scattering of X-rays in crystalline solid. The effects occur at visible wavelengths because the separation parameter \"d\" is much larger than for true crystals.\n\nVolume Bragg gratings (VBG) or volume holographic gratings (VHG) consist of a volume where there is a periodic change in the refractive index. Depending on the orientation of the modulation of the refractive index, VBG can be used either to transmit or reflect a small bandwidth of wavelengths. Bragg's law (adapted for volume hologram) dictates which wavelength will be diffracted:\n\nwhere \"n\" is a positive integer, \"λ\" the diffracted wavelength, \"Λ\" the step of the grating, \"θ\" the angle between the incident beam and the normal (N) of the entrance surface and \"φ\" the angle between the normal and the grating vector (K). Radiation that does not match Bragg's law will pass through the VBG undiffracted. The output wavelength can be tuned over a few hundred nanometers by changing the incident angle (\"θ\"). VBG are being used to produce widely tunable laser source or perform global hyperspectral imagery (see Photon etc.).\n\nBragg's law, as stated above, can be used to obtain the lattice spacing of a particular cubic system through the following relation:\n\nwhere formula_14 is the lattice spacing of the cubic crystal, and \"h\", \"k\", and \"ℓ\" are the Miller indices of the Bragg plane. Combining this relation with Bragg's law:\n\nThese selection rules can be used for any crystal with the given crystal structure. KCl has a face-centered cubic Bravais lattice. However, the K and the Cl ion have the same number of electrons and are quite close in size, so that the diffraction pattern becomes essentially the same as for a simple cubic structure with half the lattice parameter. Selection rules for other structures can be referenced elsewhere, or derived. Lattice spacing for the other crystal systems can be found here.\n\n\n\n"}
{"id": "28044140", "url": "https://en.wikipedia.org/wiki?curid=28044140", "title": "Central Electricity Regulatory Commission", "text": "Central Electricity Regulatory Commission\n\nCentral Electricity Regulatory Commission (CERC), a key regulator of power sector in India, is a statutory body functioning with quasi-judicial status under sec – 76 of the Electricity Act 2003. CERC was initially constituted on 24 July 1998 under the Ministry of Power’s Electricity Regulatory Commissions Act, 1998 for rationalization of electricity tariffs, transparent policies regarding subsidies, promotion of efficient and environmentally benign policies, and for matters connected Electricity Tariff regulation. CERC was instituted primarily to regulate the tariff of Power Generating companies owned or controlled by the government of India, and any other generating company which has a composite scheme for power generation and interstate transmission of energy, including tariffs of generating companies.\n\nOn 2 July 1998, recognizing the needs for reforms in the electricity sector nationwide, the Central government of India moved forward to enact the Electricity Regulatory Commission Act of 1998, which mandated the creation of the Central Electricity Regulation Commission with the charge of setting the tariff of centrally owned or controlled generation companies. Ministry of Power, India, has published the Electricity Regulatory Commissions Act, 1998. Apart from CERC, the act also introduced a provision for the states to create the State Electricity Regulation Commission (SERC) along with the power to set the tariffs without having to enact separate state laws.\n\nMr.S.L.Rao was the first Chairman of CERC (1998–2001).\n\nDuring March 2004, Indian Institute of Management – Ahmedabad (IIM-A) called for the merger of the Central Electricity Authority (CEA) and Central Electricity Regulatory Commission (CERC) on the grounds that technical and economic regulatory functions need to be carried out in close coordination. Even though the Electricity Act (EA) 2003 envisages separate identity for CERC and CEA, and there is a necessity for separation in the short run, the two regulators should be merged eventually, as there are substantial synergies between them. But Ministry of Power rejected IIM-A’s recommendations in this regard and observed that the tariff fixation is in the exclusive domain of electricity regulatory commissions (ERCs), and no other entity or government has any role in this regard.\n\nOn 1 September 2009, CERC has entered into an Memorandum of Understanding (MoU) with world-renowned USA’s Federal Energy Regulatory Commission for Development and regulatory oversight of Power market, Grid Reliability, Energy Efficiency, Transmission and Distribution services in India.\n\n\n\n\n\nA system of single-part tariffs was in vogue in India for pricing of thermal power, prior to 1992. The single-part tariff for a station was calculated to cover both the fixed cost as well as the variable (energy) cost at a certain (normative) generation level.\n\nDemerit:\n\nFinding that the single-part tariff, particularly for Central generating stations, was conducive neither to economic generation of power as per merit-order, nor to satisfactory operation of the regional grids, the government of India adopted in 1992 a two-part tariff formula for NTPC stations based on the recommendations of the KP Rao Committee.\n\nRecognizing that there would be no motivation on the part of NTPC (Central generating stations) to maintain a high level of efficiency and availability if it was paid the full fixed cost irrespective of level of generation and variable cost for the quantum of energy actually generated, the K.P. Rao Committee had recommended a scheme of incentive/disincentive, as a variant of a simple two-part tariff. The scheme provided for linking of incentive and disincentive with \"Plant Load Factor (PLF)\" plus \"deemed generation\", which in effect is \"Plant availability\".\n\nThe serious problems of regional grid operation however continued even after 1992. This was because the K.P. Rao Committee had been able to tackle only one end; the Central generation side. Overdrawals by some State Electricity Board's during peak-load hours and under-drawals during off-peak hours continued unabated, causing serious frequency excursions and peretual operational/commercial disputes.\n\nIn the year 1994, M/s ECC of USA were commissioned under a grant from Asian Development Bank to undertake a comprehensive study of the Indian power system and recommend a suitable tariff structure.ECC submitted their report in February, 1994, recommending Availability Tariff for generating stations, which was accepted in principle by GOI in November, 1994. A National Task Force (NTF) was constituted by the Ministry of Power in February, 1995 to oversee the implementation of ECC’s recommendations. Based on NTF deliberations between 1995 and 1998, Ministry of Power had crystallized the formulation for the so-called Availability-based tariff (ABT).\n\nWith the spirit of the Electricity Regulatory Commissions Act 1998 and consequent upon transfer of relevant powers vested under section 43 A (2) of the Electricity (Supply) Act 1948 to the CERC with effect from 15 May 1999, GOI forwarded the above draft ABT notification to CERC vide OM dated 31.5.1999 for finalization after due deliberation. The draft notification was then issued through a public notice and comments/objections were invited. The Commission in July 1999 held detailed hearings on the above. The ABT order dated 4 January 2000 of the Commission departs significantly from the draft notification as also from the prevailing tariff design\n\nTariff for supply of electricity shall comprise two parts: \n\nThe annual fixed cost (AFC) of a generating station or a transmission system shall consist of the following components\n\nThe Energy charge shall cover the primary fuel cost and limestone consumption cost\n(where applicable), and shall be payable by every beneficiary for the total energy scheduled to\nbe supplied to such beneficiary with fuel and limestone price adjustment\n\nAppellate Tribunal for Electricity has been established by Central Government for those who are not satisfied with the Central Electricity Regulatory Commission order or with a state. The Tribunal has the authority to overrule or amend that order, just like the Income-Tax tribunal or the Central Administrative Tribunal. The tribunal has to be approached within 45 days of the aggrieved person from getting the order.\n\nSince 1 April 1999 CEA has entrusted CERC with the task of regulating power tariffs of central government power utilities, inter-state generating companies, inter-state transmission tariffs. Section −76 of Electricity Act, 2003 stipulates that CERC shall consist of a Chairperson and three other Members. And one of the CERC members (Ex-Officio) has to be Chairman of CEA.<br>In Indian Power Sector, CEA takes care of: \nWhereas CERC take care of third aspect of power sector regulation -<br> \nNational electricity policy is normally formulated in consultation with and taking into account the views of the Central Electricity Regulatory Commission (CERC), Central Electricity Authority (CEA), and state governments.\n\nCERC and State Electricity Regulatory Commission (SERC) are the two electricity regulators – one operating at the central level and the other at various state levels. CERC’s primary function was to regulate the tariffs of central generating stations as well as for all interstate generation, transmission and supply of power. Whereas SERC’s primary function was to determine bulk and retail tariffs to be charged to customers, regulate the operations of intrastate transmission, including those of the State Load Despatch Center (SLDC).\nDuring Parliamentary Standing Committee on Energy in the year 2001, SERC being established in states, for formulating standards relating to quality, continuity and reliability of service for the electricity industry have failed in their efforts. There was a proposal of having benches of the Central Electricity Regulatory Commission (CERC) in five to six locations instead of having a SERC in each state, but the Committee that has rejected the proposal stating it was not possible unless states were willing to accept such a proposal.\n\nMoP entrusts CERC for providing escalation rate for coal and gas, \"inflation rate\" based on \"WPI\" and \"CPI\", \"discount rate\", and \"dollar-rupee exchange variation rate\" for the purpose tariff determination.\n\nCentral Electricity Regulatory Commission (CERC) has issued the Power Market Regulations, 2010 which will govern transactions related to ‘'Energy trading'’ by companies like Indian Energy Exchange (IEX), Power Exchange India (PXI), National Power Exchange (NPX) in various contracts related to electricity.\nThe regulations have been issued by the CERC in exercise of its powers under section 66 of the Electricity Act, 2003, which is aimed at taking measures conducive to development of the electricity industry, promoting competition therein, protecting interest of consumers and enhancing supply of electricity.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "15328761", "url": "https://en.wikipedia.org/wiki?curid=15328761", "title": "Companhia Energética de São Paulo", "text": "Companhia Energética de São Paulo\n\nThe Companhia Energética de São Paulo (CESP) is the largest producer of electricity in the state of São Paulo, with total installed power of 7,455 MW, and the third largest in Brazil. It owns and operates six hydroelectric plants integrated into the National Interconnected System.\n\n"}
{"id": "3644820", "url": "https://en.wikipedia.org/wiki?curid=3644820", "title": "Construction paper", "text": "Construction paper\n\nConstruction paper (also called sugar paper) is a tough, coarse, coloured paper. The texture is slightly rough, and the surface is unfinished. Due to the source material, mainly wood pulp, small particles are visible on the paper's surface. It is used for projects or crafts.\nThe origin of the term \"sugar paper\" lies in its use for making bags to contain sugar. It is related to the \"blue paper\" used by confectionery bakers from the 17th century England onwards; for example, in the baking of Regency ratafia cakes (or macaroons).\n\nThe animated television series \"Blue's Clues\" and \"South Park\" (initially) were made using construction paper and stop motion.\n\nThe term \"construction paper\" was associated with the material in the early 20th century, although the general process for creating the paper began in the late 19th century when industrialized paper production and synthetic dye technology were combined. Around that time, construction paper was primarily advertised for classroom settings as an effective canvas for supporting multiple drawing media. The process for creating the paper involved a machine oriented process that exposed the paper to dyes while it was still pulp, resulting in a thorough distribution and brilliance of colour. The primary dyes involved in producing construction paper were abundant until Germany, the main producer of aniline for dyes at the time, became involved in World War I and ceased its exports. The shortage marked a period in which construction paper was created using substitute colouring methods.\n\nOne of the defining features of construction paper is the radiance of its colours. Before the methodology behind construction paper's colouring was introduced, most paper was coloured by pigments and vegetable oil, which had weaker staining capabilities. Synthetic dyes were later developed, which provided a wider range of colours, stronger dyeing strength, and had lower costs. However, the colours given by synthetic dyes tend to fade over short periods of time, an effect often seen in construction paper, noted by greying colours and brittleness.\n\n"}
{"id": "25494567", "url": "https://en.wikipedia.org/wiki?curid=25494567", "title": "December 2009 North American blizzard", "text": "December 2009 North American blizzard\n\nThe December 2009 North American blizzard was a powerful nor'easter that formed over the Gulf of Mexico in December 2009, and became a major snowstorm that affected the East Coast of the United States and Canadian Atlantic provinces. The snowstorm brought record-breaking December snowfall totals to Washington, D.C., Baltimore, and Philadelphia.\n\nThe blizzard disrupted several regions, and in some areas the snowfall rate prevented snow plows from maintaining the roads. The blizzard caused flights and trains to be canceled, and left areas without power. Kentucky, Maryland, Virginia, West Virginia, and New Castle and Kent counties in Delaware declared a state of emergency. Seven deaths were reported to have been caused by the storm.\n\nOn December 16, 2009, meteorologists identified a storm forming in the Gulf of Mexico. It produced record rainfall in regions of Texas and had the potential to strengthen as it moved through Georgia and Florida and further north. Weather models accurately predicted that this storm would meet with cold air while retaining its heavy precipitation. By the afternoon of December 19, the large, low pressure region had moved off the East coast, intensifying and bringing heavy snow to the major Mid-Atlantic cities. Blizzard warnings were issued in Washington, D.C., Baltimore, and Long Island. As the storm moved northward along the East coast, at one point it measured across 14 states. The storm produced whiteout conditions and dumped about of snow in major cities along the Eastern seaboard.\n\nThe storm produced record 24-hour snowfall in Washington, D.C. and Roanoke, Virginia, where nearly of snow accumulated. Interior areas of West Virginia saw of snow. The storm broke the record for the amount of snow in a single December event at Ronald Reagan Washington National Airport, where of snow accumulated. The National Weather Service in Brookhaven, New York reported of snow, the town's largest snowfall since 1949.\n\nIn Philadelphia, snowfall reached more than per hour, resulting in significant disruption of Interstate 95. By Sunday, December 20, of snowfall had accumulated in Philadelphia, surpassing the city's second-largest record snowfall of February 11–12, 1983 – which itself was surpassed less than two months later by the February 5–6, 2010 North American blizzard. The storm also broke a 100-year-old record for the largest single December storm, previously on December 25–26, 1909. The storm was reported by meteorologists to share attributes of the 1983 storm.\n\nIn more mountainous areas, snowfall was even heavier. By midnight Saturday morning, snowfall in Boone, North Carolina had reached , Asheville, North Carolina accumulated up to , while Greensboro, North Carolina received . Portions of eastern Kentucky received as much as . In the Roanoke Valley, over had fallen by midnight. By Sunday morning, Norwich, Connecticut received of snow, while over of snow fell in Boston, Massachusetts.\n\nWinter storm warnings for New York and a blizzard warning for Long Island expired at 11 a.m. Sunday, warnings for the Boston and Providence metro areas and much of southeastern New England expired at noon. A blizzard warning for Cape Cod expired at 1 p.m. The storm reached southwestern portions of Nova Scotia, delivering up to of snow in Digby, Yarmouth, Shelburne and Queens counties. Portions of Newfoundland and Labrador received freezing rain as well.\n\nSome regions affected by the storm experienced winds up to with gusts of . Ronald Reagan Washington National Airport, Washington Dulles International Airport, and Baltimore/Washington International Thurgood Marshall Airport saw whiteout conditions, causing flights to be delayed or canceled. Of 740 scheduled departures at Washington National and 530 at Dulles, only 14 and 12, respectively, were able to take off. President Barack Obama, arriving at Andrews Air Force Base after the UN Climate Conference, was forced to return to the White House by motorcade instead of helicopter. More than 800 flights were canceled at New York City's three major airports.\n\nTrains were also canceled or delayed. Service on Washington, D.C.'s Metrorail was suspended to all outdoor stations at 1 pm on December 19 and remained suspended until late in the day December 20; underground service remained operational. One Amtrak train, carrying 255 passengers, halted for six hours while a frozen track switch was fixed.\n\nOn roads, snow plows were unable to keep up with fast snow accumulations. Road accidents and stuck vehicles further hindered snow removal. On some major highways, traffic slowed to five miles per hour. In West Virginia, on Interstate 77 between Ghent and Beckley, thousands of motorists were stranded for up to 18 hours due to impassable roads. The interstate was closed for 15 hours. Governor Joe Manchin has launched a full investigation into why the roads were in such poor condition. Greyhound Lines canceled 294 routes through Maryland, Virginia, Washington, D.C., and West Virginia on December 19 and suspended service in and out of New York late Saturday.\n\nThousands of power outages were reported in Virginia, Kentucky, and North Carolina, reportedly caused by snow weighing down on power lines. By midnight Saturday morning, when the storm had just begun to strike the area, reported power outages had already exceeded 40,000. In Kentucky, 107,000 power outages were reported. A snow emergency was declared in Washington, D.C., where Mayor Adrian Fenty asked residents to avoid venturing onto the roads. States of emergency were declared in Kentucky, Maryland, Virginia, West Virginia and New Castle and Kent counties in Delaware.\n\nThe storm was held responsible for seven deaths in North Carolina, Ohio, Pennsylvania, Virginia, and West Virginia, including one death resulting from a head-on collision between a snowmobile driver and a horse-drawn carriage.\n\nThe nor'easter, which arrived on the last weekend before Christmas, hurt sales at retail stores in affected regions, but boosted online sales. Super Saturday, the last Saturday before Christmas, typically nets $15 billion in retail sales. It is estimated that 30% of this revenue comes from the northeastern United States. Stores that managed to open saw reduced traffic. The storm resulted in an estimated loss of $2 billion in retail sales.\n\nDue to widespread accumulation of heavy snow, the storm was ranked as a high-end Category 2 (\"significant\") winter storm, on the Northeast Snowfall Impact Scale. The ranking is based on the amount of snowfall, the area, and the population affected.\n\nNew Jersey's major towns and cities were particularly hard hit with snow and some rain coming onshore. During the day, rain inundated the coastline as temperatures were not cold enough for snow. Around 5 P.M, heavy snow pushed the rain offshore, making for blizzard conditions along the coast. In New Jersey, anywhere from 2 to 20 inches of snow fell. During the height of the storm, power was knocked out in places from Paterson, New Jersey to Cape May, New Jersey.\n\nNew York City was put under a blizzard warning during the height of the storm. Winds of up to 60 mph knocked out power in the NYC Metro. Nearby towns in New Jersey and on Long Island experienced the brunt of the storm along with New York City.\n\n\n"}
{"id": "1821411", "url": "https://en.wikipedia.org/wiki?curid=1821411", "title": "Diamond-like carbon", "text": "Diamond-like carbon\n\nDiamond-like carbon (DLC) is a class of amorphous carbon material that displays some of the typical properties of diamond. DLC is usually applied as coatings to other materials that could benefit from some of those properties.\n\nDLC exists in seven different forms. All seven contain significant amounts of sp hybridized carbon atoms. The reason that there are different types is that even diamond can be found in two crystalline polytypes. The more common one has its carbon atoms arranged in a cubic lattice, while the less common one, lonsdaleite, has a hexagonal lattice. By mixing these polytypes in various ways at the nanoscale level of structure, DLC coatings can be made that at the same time are amorphous, flexible, and yet purely sp bonded \"diamond\". The hardest, strongest, and slickest is such a mixture, known as tetrahedral amorphous carbon (ta-C). For example a coating of only 2 μm thickness of \"ta-C\" increases the resistance of common (e.g., type 304) stainless steel against abrasive wear, changing its lifetime in such service from one week to 85 years. Such \"ta-C\" can be considered to be the \"pure\" form of DLC, since it consists only of sp bonded carbon atoms. Fillers such as hydrogen, graphitic sp carbon, and metals are used in the other 6 forms to reduce production expenses or to impart other desirable properties.\n\nThe various forms of DLC can be applied to almost any material that is compatible with a vacuum environment. In 2006, the market for outsourced DLC coatings was estimated as about 30,000,000 € in the European Union. In October 2011, \"Science Daily\" reported that researchers at Stanford University have created a super-hard amorphous diamond under conditions of ultrahigh pressure, which lacks the crystalline structure of diamond but has the light weight characteristic of carbon.\n\nNaturally occurring diamond is almost always found in the crystalline form with a purely cubic orientation of sp bonded carbon atoms. Sometimes there are lattice defects or inclusions of atoms of other elements that give color to the stone, but the lattice arrangement of the carbons remains cubic and bonding is purely sp. The internal energy of the cubic polytype is slightly lower than that of the hexagonal form and growth rates from molten material in both natural and bulk synthetic diamond production methods are slow enough that the lattice structure has time to grow in the lowest energy (cubic) form that is possible for sp bonding of carbon atoms. In contrast, DLC is typically produced by processes in which high energy precursive carbons (\"e.g.\" in plasmas, in filtered cathodic arc deposition, in sputter deposition and in ion beam deposition) are rapidly cooled or quenched on relatively cold surfaces. In those cases cubic and hexagonal lattices can be randomly intermixed, layer by atomic layer, because there is no time available for one of the crystalline geometries to grow at the expense of the other before the atoms are \"frozen\" in place in the material. Amorphous DLC coatings can result in materials that have no long-range crystalline order. Without long range order there are no brittle fracture planes, so such coatings are flexible and conformal to the underlying shape being coated, while still being as hard as diamond. In fact this property has been exploited to study atom-by-atom wear at the nanoscale in DLC.\n\nThere are several methods producing DLC, which rely on the lower density of sp than sp carbon. So the application of pressure, impact, catalysis, or some combination of these at the atomic scale can force sp bonded carbon atoms closer together into sp bonds. This must be done vigorously enough that the atoms cannot simply spring back apart into separations characteristic of sp bonds. Usually techniques either combine such a compression with a push of the new cluster of sp bonded carbon deeper into the coating so that there is no room for expansion back to separations needed for sp bonding; or the new cluster is buried by the arrival of new carbon destined for the next cycle of impacts. It is reasonable to envisage the process as a \"hail\" of projectiles that produce localized, faster, nanoscale versions of the classic combinations of heat and pressure that produce natural and synthetic diamond. Because they occur independently at many places across the surface of a growing film or coating, they tend to produce an analog of a cobblestone street with the cobbles being nodules or clusters of sp bonded carbon. Depending upon the particular \"recipe\" being used, there are cycles of deposition of carbon and impact or continuous proportions of new carbon arriving and projectiles conveying the impacts needed to force the formation of the sp bonds. As a result, \"ta-C\" may have the structure of a cobblestone street, or the nodules may \"melt together\" to make something more like a sponge or the cobbles may be so small as to be nearly invisible to imaging. A classic \"medium\" morphology for a \"ta-C\" film is shown in the figure.\n\nAs implied by the name, diamond-like carbon (DLC), the value of such coatings accrues from their ability to provide some of the properties of diamond to surfaces of almost any material. The primary desirable qualities are hardness, wear resistance, and slickness (DLC film friction coefficient against polished steel ranges from 0.05 to 0.20 ). DLC properties highly depends on plasma treatment deposition parameters, like effect of bias voltage, DLC coating thickness, interlayer thickness, etc. Moreover, the heat treatment also change the coating properties such as hardness, toughness and wear rate.\n\nHowever, which properties are added to a surface and to what degree depends upon which of the 7 forms are applied, and further upon the amounts and types of diluents added to reduce the cost of production. In 2006 the Association of German Engineers, VDI, the largest engineering association in Western Europe issued an authoritative report VDI2840 in order to clarify the existing multiplicity of confusing terms and trade names. It provides a unique classification and nomenclature for diamond-like-carbon (DLC) and diamond films. It succeeded in reporting all information necessary to identify and to compare different DLC films which are offered on the market. Quoting from that document:These [sp] bonds can occur not only with crystals - in other words, in solids with long-range order - but also in amorphous solids where the atoms are in a random arrangement. In this case there will be bonding only between a few individual atoms and not in a long-range order extending over a large number of atoms. The bond types have a considerable influence on the material properties of amorphous carbon films. If the sp type is predominant the film will be softer, if the sp type is predominant the film will be harder. A secondary determinant of quality was found to be the fractional content of hydrogen. Some of the production methods involve hydrogen or methane as a catalyst and a considerable percentage of hydrogen can remain in the finished DLC material. When it is recalled that the soft plastic, polyethylene is made from carbon that is bonded purely by the diamond-like sp bonds, but also includes chemically bonded hydrogen, it is not surprising to learn that fractions of hydrogen remaining in DLC films degrade them almost as much as do residues of sp bonded carbon. The VDI2840 report confirmed the utility of locating a particular DLC material onto a 2-dimensional map on which the X-axis described the fraction of hydrogen in the material and the Y-axis described the fraction of sp bonded carbon atoms. The highest quality of diamond-like properties was affirmed to be correlated with the proximity of the map point plotting the (X,Y) coordinates of a particular material to the upper left corner at (0,1), namely 0% hydrogen and 100% sp bonding. That \"pure\" DLC material is \"ta-C\" and others are approximations that are degraded by diluents such as hydrogen, sp bonded carbon, and metals. Valuable properties of materials that are \"ta-C\", or nearly \"ta-C\" follow.\n\nWithin the \"cobblestones\", nodules, clusters, or \"sponges\" (the volumes in which local bonding is sp) bond angles may be distorted from those found in either pure cubic or hexagonal lattices because of intermixing of the two. The result is internal (compressive) stress that can appear to add to the hardness measured for a sample of DLC. Hardness is often measured by nanoindentation methods in which a finely pointed stylus of natural diamond is forced into the surface of a specimen. If the sample is so thin that there is only a single layer of nodules, then the stylus may enter the DLC layer between the hard cobblestones and push them apart without sensing the hardness of the sp bonded volumes. Measurements would be low. Conversely, if the probing stylus enters a film thick enough to have several layers of nodules so it cannot be spread laterally, or if it enters on top of a cobblestone in a single layer, then it will measure not only the real hardness of the diamond bonding, but an apparent hardness even greater because the internal compressive stress in those nodules would provide further resistance to penetration of the material by the stylus. Nanoindentation measurements have reported hardness as great as 50% more than values for natural crystalline diamond. Since the stylus is blunted in such cases or even broken, actual numbers for hardness that exceed that of natural diamond are meaningless. They only show that the hard parts of an optimal \"ta-C\" material will break natural diamond rather than the inverse. Nevertheless, from a practical viewpoint it does not matter how the resistance of a DLC material is developed, it can be harder than natural diamond in usage. One method of testing the coating hardness is by means of the Persoz pendulum.\n\nThe same internal stress that benefits the hardness of DLC materials makes it difficult to bond such coatings to the substrates to be protected. The internal stresses try to \"pop\" the DLC coatings off of the underlying samples. This challenging downside of extreme hardness is answered in several ways, depending upon the particular \"art\" of the production process. The most simple is to exploit the natural chemical bonding that happens in cases in which incident carbon ions supply the material to be impacted into sp bonded carbon atoms and the impacting energies that are compressing carbon volumes condensed earlier. In this case the first carbon ions will impact the surface of the item to be coated. If that item is made of a carbide-forming substance such as Ti or Fe in steel a layer of carbide will be formed that is later bonded to the DLC grown on top of it. Other methods of bonding include such strategies as depositing intermediate layers that have atomic spacings that grade from those of the substrate to those characteristic of sp bonded carbon. In 2006 there were as many successful recipes for bonding DLC coatings as there were sources of DLC.\n\nDLC coatings are often used to prevent wear due to their excellent tribological properties. DLC is very resistant to abrasive and adhesive wear making it suitable for use in applications that experience extreme contact pressure, both in rolling and sliding contact. DLC is often used to prevent wear on razor blades and metal cutting tools, including lathe inserts and milling cutters. DLC is used in bearings, cams, cam followers, and shafts in the automobile industry. The coatings reduce wear during the 'break-in' period, where drive train components may be starved for lubrication.\n\nDLCs may also be used in chameleon coatings that are designed to prevent wear during launch, orbit, and re-entry of land-launched space vehicles. DLC provides lubricity at ambient atmosphere and at vacuum unlike graphite, which requires moisture to be lubricious. Isolated carbon particles embedded diamond-like carbon coatings are the recent development in this area. The wear rate of amorphous DLC can be reduced up to 60% by embedding isolated carbon nanoparticles embedded simultaneous to DLC deposition. The isolated particles were in-situ created through rapid plasma quenching with Helium pulses. \n\nDespite the favorable tribological properties of DLC it must be used with caution on ferrous metals. If it is used at higher temperatures, the substrate or counter face may carburize, which could lead to loss of function due to a change in hardness. The final, end use temperature of a coated component should be kept below the temperature at which a PVC DLC coating is applied.\n\nIf a DLC material is close enough to \"ta-C\" on plots of bonding ratios and hydrogen content it can be an insulator with a high value of resistivity. Perhaps more interesting is that if prepared in the \"medium\" cobblestone version such as shown in the above figure, electricity is passed through it by a mechanism of hopping conductivity. In this type of conduction of electricity the electrons move by quantum mechanical tunneling between pockets of conductive material isolated in an insulator. The result is that such a process makes the material something like a semiconductor. Further research on electrical properties is needed to explicate such conductivity in \"ta-C\" in order to determine its practical value. However, a different electrical property of emissivity has been shown to occur at unique levels for \"ta-C\". Such high values allow for electrons to be emitted from \"ta-C\" coated electrodes into vacuum or into other solids with application of modest levels of applied voltage. This has supported important advances in medical technology.\n\nApplications of DLC typically utilize the ability of the material to reduce abrasive wear. Tooling components, such as endmills, drill bits, dies and molds often use DLC in this manner. DLC is also used in the engines of modern supersport motorcycles, Formula 1 racecars, NASCAR vehicles, and as a coating on hard-disk platters and hard-disk read heads to protect against head crashes. Virtually all of the multi-bladed razors used for wet shaving have the edges coated with hydrogen-free DLC to reduce friction, preventing abrasion of sensitive skin. It is also being used as a coating by some weapon manufacturers/custom gunsmiths. Some forms have been certified in the EU for food service and find extensive uses in the high-speed actions involved in processing novelty foods such as potato chips and in guiding material flows in packaging foodstuffs with plastic wraps. DLC coats the cutting edges of tools for the high-speed, dry shaping of difficult exposed surfaces of wood and aluminium, for example on automobile dashboards.\n\nThe wear, friction, and electrical properties of DLC make it an appealing material for medical applications. Fortunately, DLC has proved to have excellent bio-compatibility as well. This has enabled many medical procedures, such as Percutaneous coronary intervention employing brachytherapy to benefit from the unique electrical properties of DLC. At low voltages and low temperatures electrodes coated with DLC can emit enough electrons to be arranged into disposable, micro-X-ray tubes as small as the radioactive seeds that are introduced into arteries or tumors in conventional brachytherapy. The same dose of prescribed radiation can be applied \"from the inside, out\" with the additional possibility to switch on and off the radiation in the prescribed pattern for the X-rays being used. DLC has proved to be an excellent coating to prolong the life of and reduce complications with replacement hip joints and artificial knees. It also has been successfully applied to coronary artery stents, reducing the incidence of thrombosis. The implantable human heart pump can be considered the ultimate biomedical application where DLC coating is used on blood contacting surfaces of the key components of the device.\n\nThe Space Black stainless steel Apple Watch is coated with diamond-like carbon.\n\nThe increase in lifetime of articles coated with DLC that wear out because of abrasion can be described by the formula \"f = (g)\", where \"g\" is a number that characterizes the type of DLC, the type of abrasion, the substrate material and μ is the thickness of the DLC coating in μm. For \"low-impact\" abrasion (pistons in cylinders, impellers in pumps for sandy liquids, etc.), \"g\" for pure \"ta-C\" on 304 stainless steel is 66. This means that one-μm thickness (that is ~5% of the thickness of a human hair-end) would increase service lifetime for the article it coated from a week to over a year and two-μm thickness would increase it from a week to 85 years. These are measured values; though in the case of the 2 μm coating the lifetime was extrapolated from the last time the sample was evaluated until the testing apparatus itself wore out.\n\nThere are environmental arguments that a sustainable economy ought to encourage articles not engineered to lower performance or to fail prematurely. This in turn will reduce the need to support greater production of units and their frequent replacement, which might provide an economic disincentive to manufacturers of such devices.\n\nCurrently there are about 100 outsource vendors of DLC coatings that are loaded with amounts of graphite and hydrogen and so give much lower g-numbers than 66 on the same substrates.\n\n\n"}
{"id": "17110072", "url": "https://en.wikipedia.org/wiki?curid=17110072", "title": "Double-pair mating", "text": "Double-pair mating\n\nDouble-pair mating (DPM) is a mating (crossing) design used in plant breeding. Each individual is mated with two others.\n\nIn Fig. 1 a connected variant of DPM is shown. DPM is an efficient mating design in balanced breeding programmes, where equal contribution from each breeding population member is desired. With DPM the number of new families created is equal to the number of individuals mated. DPM allows to efficiently utilise positive assortative mating for more efficient use of the breeding population members for deployment to seed orchards. In comparison with single pair mating, DPM has the advantages that the genes from the individual are transmitted to next generation even if one of the crosses fails; that safer estimates of breeding values of the parents get possible (useful for seed orchards, where tested trees are preferred); and that genes from different ancestors have a better chance to combine.\n"}
{"id": "5909388", "url": "https://en.wikipedia.org/wiki?curid=5909388", "title": "Ecotropism", "text": "Ecotropism\n\nEcotropism or ecotropic (from \"eco\" – hearth and \"tropic\" – to turn towards) refers to the philosophy that for human culture to be healthy, it must exist as in an ecological niche and thereby relate appropriately with all the fields of forces of nature, organic and inorganic. The following form of the term has been used since 1990 in the publication of \"Toward an Ecotropic Poetry\" Published by The Open Theatre, Austin Tx 1990, written by John John Campion and John Herndon.\n\nEcotropism can also indicate that a pathogen, like a virus or a bacterium, has a narrow host range and can infect one or a small group of species or cell culture lines.\n\n"}
{"id": "4183809", "url": "https://en.wikipedia.org/wiki?curid=4183809", "title": "Eimac", "text": "Eimac\n\nEimac is a trade mark of Eimac Products, part of the Microwave Power Products Division of Communications & Power Industries. It produces power vacuum tubes for radio frequency applications such as broadcast and radar transmitters.\n\nThe San Francisco Bay Area was an early center of ham radio with about 10% of the operators in the United States. William Eitel, Jack McCullough, and Charles Litton, Sr., who together pioneered vacuum tube manufacturing in the Bay Area, were hobbyists with training in technology gained locally who participated in development of shortwave radio by the ham radio hobby. High frequency, and especially, Very high frequency, VHF, transmission in the 10 meter band, required higher quality power tubes than were manufactured by the consortium of RCA, Western Electric, General Electric, Westinghouse which controlled vacuum tube manufacture. Litton pioneered the glass lathe which made mass production of reliable high quality power tubes possible.\n\nWhile employed by the small San Francisco, California manufacturing firm of Heintz & Kaufman which manufactured custom radio equipment Bill Eitel (amateur radio call sign W6UF) and Jack McCullough (W6CHE) convinced company president Ralph Heintz (W6XBB) to allow them to develop a transmitting tube that could operate at lower voltages than those then available to the amateur radio market, such as the RCA 204A or the 852. Their effort was a success and resulted in production of the HK-354. Shortly after in 1934, Eitel and McCullough left H&K to form Eitel McCullough Corp. in San Bruno California. \n\nThe first product produced under the trade mark \"Eimac\" was the 150T power triode. The new company thrived during World War II by selling tubes to the U.S. military for use in radar equipment.\n\nContracts to provide transmission tubes for radar and other radio equipment during World War II required adaption of mass production, research to improve the reliability of tubes, and development of standardized manufacturing techniques which could be performed by unskilled workers. The workforce expanded from a few hundred to several thousand. During the war Eimac produced out hundreds of thousands radar tubes.\n\nA union organizing drive in 1939-40 by the strong Bay area labor movement was fought off by adoption of a strategy of welfare capitalism which included pensions and other generous benefits, profit sharing, and such extras as a medical clinic and a cafeteria. An atmosphere of cooperation and collaboration was established,\n\nAs wartime orders ceased and a large supply of military surplus transmission tubes flooded the market the firm laid off 90% of its workers and closed its plant in Salt Lake City. Reallocation of the FM band by the FCC in 1945, however, provided an opportunity for the firm to market a superior power tetrode tube which it had developed.\n\nBeginning in 1947, Eimac operated FM radio station KSBR from their plant in San Bruno, California, one of only two FM stations in the United States to test the new Rangertone tape recorders (adapted from the German Magnetophon recorders). In need of more space, the company moved to San Carlos in 1959. Eimac's San Carlos plant was dedicated on April 16, 1959. By that time, the company had the following subsidiaries: National Electronics, Inc., Geneva, Illinois, and Eitel-McCullough, S.A., Geneva, Switzerland. During the Cold war era, Eimac supplied U.S. military with klystron power tubes and electron power tubes used in the defense communications network, navigation, detection, ranging and fire-control radars.\n\nIn the beginning of May 1959, the company announced its newly-produced giant klystron tube powered the Massachusetts Institute of Technology’s radar which recently established contact with planet Venus. The super-power klystron, developed under Rome Air Development Center sponsorship. Eimac klystrons also has been chosen for NATO's tropospheric scatter communications network.\n\nIn 1965, Eimac merged with Varian Associates and became known as the Eimac Division. In August 1995, Varian Associates sold the Electron Device Business to Leonard Green & Partners, a private equity fund, and members of management. Together, they formed Communications & Power Industries.\n\nIn January 2004, affiliates of The Cypress Group, a private equity fund, acquired CPI.\n\nIn February 2011, an affiliate of Veritas Capital, a private equity investment firm acquired CPI.\n\nIn 2006 CPI relocated the Eimac facility from 301 Industrial Way to their operation in Palo Alto.\n\nhttp://cityofsancarlos.org/pamf/default.asp\n\n"}
{"id": "14460653", "url": "https://en.wikipedia.org/wiki?curid=14460653", "title": "Emma Must", "text": "Emma Must\n\nEmma Must (born 1966) is an English environmental activist, teacher, and poetwho previously worked as a librarian.\n\nShe was awarded the Goldman Environmental Prize in 1995 for her efforts on land protection, particularly her influence on British road building policies through her road protest against the M3 motorway extension at Twyford Down.\n\nMust went on to work with Alarm UK!, (an umbrella group for the nationwide road building protest), Transport 2000 (later renamed Campaign for Better Transport) and World Development Movement.\n\n\n"}
{"id": "22691628", "url": "https://en.wikipedia.org/wiki?curid=22691628", "title": "European Underground Rare Event Calorimeter Array", "text": "European Underground Rare Event Calorimeter Array\n\nThe European Underground Rare Event Calorimeter Array (EURECA) is a planned dark matter search experiment using cryogenic detectors and an absorber mass of up to 1 tonne. The project will be built in the Modane Underground Laboratory and will bring together researchers working on the CRESST and EDELWEISS experiments.\n\nEURECA featured prominently in the ASPERA road map of Astroparticle Physics experiments in Europe.\n\nDark matter is one of the significant unsolved problems in modern science. There is considerable evidence from astronomy and cosmology that a significant fraction of the mass of the Universe, and of galaxies is made up of non-luminous material. The nature of dark matter is currently unknown. However a popular hypothesis is that it consists of Weakly Interacting Massive Particles (WIMPs), particles with a large mass, but which only interact with ordinary matter through the weak nuclear force, so the majority that pass through the Earth do not hit a single atom. The aim of dark matter search experiments such as EURECA is to test this hypothesis by searching for WIMP dark matter interactions. WIMPs are predicted to exist by supersymmetry theory, which predicts a wide range of scattering cross-sections down to 10pb, corresponding to an interaction rate of ~1 event per year in a 1 tonne detector. Existing experiments such as CRESST and EDELWEISS have already ruled out higher interaction rates, but EURECA will search down to this lower limit.\n\nCryogenic dark matter experiments use particle detectors operating at millikelvin temperatures to search for the elastic scattering of WIMPs of an atomic nuclei. A particle interaction inside an absorber crystal will create a large number of phonons, these thermalise inside a thermometer on the crystal surface, which records the rise in temperature. Such cryogenic detectors are used as they combine a high sensitivity with a low energy threshold and excellent resolution.\n\nDark matter experiments are located in deep underground laboratories, and use extensive shielding to reduce the background radiation levels from cosmic rays. Early experiments were limited by the remaining background due to radioactive impurities close to the detectors. Therefore the second phase of CRESST and EDELWEISS used new detectors capable of distinguishing electron recoil events from nuclear recoils. Electron recoils are produced by alpha, beta and gamma particles which account for the vast majority of background events. WIMPs (and also neutrons) produce nuclear recoils. This is done by measuring an additional signal, which is much higher for electron recoils than nuclear recoils. CRESST detectors measure the scintillation light produced in a CaWO or ZnWO absorber crystal. EDELWEISS detectors measure the ionization produced in a semiconducting germanium crystal.\n\nEURECA will take this cryogenic detector technology pioneered by CRESST and EDELWEISS further by building a 1 tonne absorber mass made up from a large number of cryogenic detector modules. The experiment plans to use a range of detector materials. This provides a way to show if a positive signal is due to dark matter, as the event rate is expected to scale with the atomic mass of the target nuclei. Whereas the event rate from neutrons will be higher for lighter nuclei.\n\nThe EURECA collaboration includes the member institutions of CRESST, EDELWEISS, and ROSEBUD dark matter experiments, and some new members. These are: \n\n\nThe collaboration spokesman is Gilles Gerbier. The experiment will be built in the Modane Underground Laboratory, in the Fréjus road tunnel between France and Italy, the deepest underground laboratory in Europe.\n\nEURECA researchers are currently involved in data taking and analysis for CRESST and EDELWEISS. In addition, there are various R&D activities under way associated with scaling up the detector technology to a 1-tonne scale. These include:\n\n\n\n"}
{"id": "1269792", "url": "https://en.wikipedia.org/wiki?curid=1269792", "title": "Eyachtal Span", "text": "Eyachtal Span\n\nThe Eyachtal Span is the crossing of the Eyach valley with a 110 kV line in Neuenbürg and Höfen an der Enz in the Black Forest in Germany. The Eyachtal Span was built in 1992. With a span of 1444 metres, it is the greatest span of a power line in Germany. The pylons on which the Eyach Span is fixed are 70 metres high and located on Heuberg and Eiberg.\n"}
{"id": "13976612", "url": "https://en.wikipedia.org/wiki?curid=13976612", "title": "Harris functional", "text": "Harris functional\n\nIn density functional theory (DFT), the Harris energy functional is a non-self-consistent approximation to the Kohn-Sham density functional theory. It gives the energy of a combined system as a function of the electronic densities of the isolated parts. The energy of the Harris functional varies much less than the energy of the Kohn-Sham functional as the density moves away from the converged density.\n\nKohn-Sham equations are the one-electron equations that must be solved in a self-consistent fashion in order to find the ground state density of a system of interacting electrons:\n\nThe density, formula_2 is given by that of the Slater determinant formed by the spin-orbitals of the occupied states:\n\nformula_3\n\nwhere the coefficients formula_4 are the occupation numbers given by the Fermi-Dirac distribution at the temperature of the system with the restriction formula_5,\n\nwhere formula_6 is the total number of electrons. In the equation above, formula_7 is the Hartree potential and formula_8 is the exchange-correlation potential, which are expressed in terms of the electronic density. Formally, one must solve these equations self-consistently, for which the usual strategy is to pick an initial guess for the density, formula_9, substitute in the Kohn-Sham equation, extract a new density formula_10 and iterate the process until convergence is obtained. When the final self-consistent density formula_11 is reached, the energy of the system is expressed as:\n\nAssuming that we have an approximate electron density formula_13, which is different from the exact electron density formula_14. We construct exchange-correlation potential formula_15 and the Hartree potential formula_16 based on the approximate electron density formula_17. Kohn-Sham equations are then solved with the XC and Hartree potentials and eigenvalues are then obtained; that is, we perform one single iteration of the self-consistency calculation. The sum of eigenvalues is often called the band structure energy:\n\nformula_18\n\nwhere formula_19 loops over all occupied Kohn-Sham orbitals. Harris energy functional is defined as\n\nIt was discovered by Harris that the difference between the Harris energy formula_21 and the exact total energy is to the second order of the error of the approximate electron density, i.e., formula_22. Therefore, for many systems the accuracy of Harris energy functional may be sufficient. The Harris functional was originally developed for such calculations rather than self-consistent convergence, although it can be applied in a self-consistent manner in which the density is changed. Many density-functional tight-binding methods, such as DFTB+, Fireball, and Hotbit, are built based on the Harris energy functional. In these methods, one often does not perform self-consistent Kohn-Sham DFT calculations and the total energy is estimated using the Harris energy functional, although a version of the Harris functional where one does perform self-consistency calculations has been used. These codes are often much faster than conventional Kohn-Sham DFT codes that solve Kohn-Sham DFT in a self-consistent manner.\n\nWhile the Kohn-Sham DFT energy is a variational functional (never lower than the ground state energy), the Harris DFT energy was originally believed to be anti-variational (never higher than the ground state energy). This was, however, conclusively demonstrated to be incorrect.\n"}
{"id": "29704395", "url": "https://en.wikipedia.org/wiki?curid=29704395", "title": "Hugo Jabini", "text": "Hugo Jabini\n\nHugo Jabini is a Saramakan from the Laduani village by the Suriname River. He was awarded the Goldman Environmental Prize in 2009, jointly with Wanze Eduards, for their efforts of protecting their traditional land and tropical rainforests against industrial logging, supported by the Inter-American Commission on Human Rights and the Inter-American Court. \n"}
{"id": "23859688", "url": "https://en.wikipedia.org/wiki?curid=23859688", "title": "Iceland Deep Drilling Project", "text": "Iceland Deep Drilling Project\n\nThe Iceland Deep Drilling Project (IDDP) is a geothermal project established in 2000 by a consortium of the National Energy Authority of Iceland (Orkustofnun)(OS) and four of Iceland's leading energy companies: Hitaveita Sudurnesja (HS), Landsvirkjun, Orkuveita Reykjavíkur and Mannvit Engineering. The consortium is referred to as \"Deep Vision\".\n\nThe aim is to improve the economics of geothermal energy production. Its strategy is to look at the usefulness of supercritical hydrothermal fluids as an economic energy source. This necessitates drilling to depths of greater than in order to tap the temperatures of more than . The drilling is at a rifted plate margin on the mid-oceanic ridge. Producing steam from a well in a reservoir hotter than — at a proposed rate of around should be sufficient to generate around 45 MW. If this is correct, then the project could be a major step towards developing high-temperature geothermal resources.\n\n\"Deep Vision\" recognized at its inception that much research would be needed regarding the poorly understood supercritical environment and as such sought to promote inclusion of the wider scientific community.\n\nFunding has come from the members of the consortium, the International Continental Scientific Drilling Program and the US National Science Foundation.\n\nThis project has also been used for purposes such as university research. Researchers from UC Davis, UC Riverside, Stanford University, and the University of Oregon have taken the opportunity to collaborate with each other and the IDDP. They have aimed their investigation to gain information about extracting energy from hot rocks on land. To do this, they have been gathering important information from the borehole they sunk where seawater circulates through deep, hot rock. This should give important new clues about black smokers, hydrothermal vents that spew minerals and superheated water deep below the ocean. These support unique microorganism communities living within them.\n\nThe 49th Volume of the journal \"Geothermics\", released in January 2014, is entirely dedicated to the first well of the IDDP.\n\nThe borehole of this well was unintentionally drilled into a magma reservoir in 2009. The hole was initially planned to drill down to hot rock below , but drilling was ceased when the drill struck magma at only deep. This same occurrence has only been recorded once, in a Hawaiian geothermal well in 2007, but in that instance, it resulted in the sealing and abandonment of the hole.\n\nIn IDDP-1 the decision was made to continue the experimental well, and upon inserting cold water into the well, which was over . The resultant well was the first operational Magma-EGS, and was at the time the most powerful geothermal well ever drilled. While not producing electricity on the grid, it was calculated that the output of the well would have been sufficient to produce 36 MW of electricity. The well was eventually shut down after a valve failure occurred while attempting to connect the output to a central generator.\n\nFive years before IDDP-1 was made, a borehole was drilled at Reykjanesvirkjun. It was named RN-15 or REY H015 (Reykjanes-15) and is just one of many geothermal boreholes drilled in the Reykjanes peninsula since 1956. It reached a maximum depth of 2.5 km (1.55 mi).\n\nIt was always known that RN-15 could be deepened, after a good result of the drilling. About 10 years later, IDDP decided to continue drilling under the project name IDDP-2. The plan is to reach a maximum depth of 5 km (3.11 mi) before the end of 2016, making it by far the deepest borehole in Iceland. Scientists are hoping to reach a temperature of , which would be the hottest blast of any hole in the world, breaking the former record of the IDDP-1 Krafla borehole.\n\n\n\n"}
{"id": "624209", "url": "https://en.wikipedia.org/wiki?curid=624209", "title": "Induction heating", "text": "Induction heating\n\nInduction heating is the process of heating an electrically conducting object (usually a metal) by electromagnetic induction, through heat generated in the object by eddy currents. An induction heater consists of an electromagnet, and an electronic oscillator that passes a high-frequency alternating current (AC) through the electromagnet. The rapidly alternating magnetic field penetrates the object, generating electric currents inside the conductor called eddy currents. The eddy currents flowing through the resistance of the material heat it by Joule heating. In ferromagnetic (and ferrimagnetic) materials like iron, heat may also be generated by magnetic hysteresis losses. The frequency of current used depends on the object size, material type, coupling (between the work coil and the object to be heated) and the penetration depth.\n\nAn important feature of the induction heating process is that the heat is generated inside the object itself, instead of by an external heat source via heat conduction. Thus objects can be heated very rapidly. In addition there need not be any external contact, which can be important where contamination is an issue. Induction heating is used in many industrial processes, such as heat treatment in metallurgy, Czochralski crystal growth and zone refining used in the semiconductor industry, and to melt refractory metals which require very high temperatures. It is also used in induction cooktops for heating containers of food; this is called induction cooking.\n\nInduction heating allows the targeted heating of an applicable item for applications including surface hardening, melting, brazing and soldering and heating to fit. Iron and its alloys respond best to induction heating, due to their ferromagnetic nature. Eddy currents can, however, be generated in any conductor, and magnetic hysteresis can occur in any magnetic material. Induction heating has been used to heat liquid conductors (such as molten metals) and also gaseous conductors (such as a gas plasma - see Induction plasma technology). Induction heating is often used to heat graphite crucibles (containing other materials) and is used extensively in the semiconductor industry for the heating of silicon and other semiconductors. Utility frequency (50/60 Hz) induction heating is used for many lower cost industrial applications as inverters are not required.\n\nAn induction furnace uses induction to heat metal to its melting point. Once molten, the high-frequency magnetic field can also be used to stir the hot metal, which is useful in ensuring that alloying additions are fully mixed into the melt. Most induction furnaces consist of a tube of water-cooled copper rings surrounding a container of refractory material. Induction furnaces are used in most modern foundries as a cleaner method of melting metals than a reverberatory furnace or a cupola. Sizes range from a kilogram of capacity to a hundred tonnes capacity. Induction furnaces often emit a high-pitched whine or hum when they are running, depending on their operating frequency. Metals melted include iron and steel, copper, aluminium, and precious metals. Because it is a clean and non-contact process it can be used in a vacuum or inert atmosphere. Vacuum furnaces make use of induction heating for the production of specialty steels and other alloys that would oxidize if heated in the presence of air.\n\nA similar, smaller-scale process is used for induction welding. Plastics may also be welded by induction, if they are either doped with ferromagnetic ceramics (where magnetic hysteresis of the particles provides the heat required) or by metallic particles.\n\nSeams of tubes can be welded this way. Currents induced in a tube run along the open seam and heat the edges resulting in a temperature high enough for welding. At this point, the seam edges are forced together and the seam is welded. The RF current can also be conveyed to the tube by brushes, but the result is still the same – the current flows along the open seam, heating it.\n\nIn induction cooking, an induction coil in the cook-top heats the iron base of cookware by magnetic induction. Copper-bottomed pans, aluminum pans and other non-ferrous pans are generally unsuitable. The heat induced in the base is transferred to the food by conduction. The benefits of induction cookers include efficiency, safety (the induction cook-top is not heated itself) and speed. Both permanently installed and portable induction cookers are available.\n\nInduction brazing is often used in higher production runs. It produces uniform results and is very repeatable. There are many types of industrial equipment where Induction brazing is used. For instance, Induction is used for brazing carbide to shaft.\n\nInduction heating is used in \"cap sealing\" of containers in the food and pharmaceutical industries. A layer of aluminum foil is placed over the bottle or jar opening and heating by induction to fuse it to the container. This provides a tamper-resistant seal, since altering the contents requires breaking the foil.\n\nInduction heating is often used to heat an item causing it to expand prior to fitting or assembly. Bearings are routinely heated in this way using utility frequency (50/60 Hz) and a laminated steel transformer type core passing through the centre of the bearing.\n\nInduction heating is often used in the heat treatment of metal items.\nThe most common applications are induction hardening of steel parts, induction soldering/brazing as a means of joining metal components and induction annealing to selectively soften an area of a steel part.\n\nInduction heating can produce high power densities which allow short interaction times to reach the required temperature. This gives tight control of the heating pattern with the pattern following the applied magnetic field quite closely and allows reduced thermal distortion and damage.\n\nThis ability can be used in hardening to produce parts with varying properties. The most common hardening process is to produce a localised surface hardening of an area that needs wear-resistance, while retaining the toughness of the original structure as needed elsewhere. The depth of induction hardened patterns can be controlled through choice of induction-frequency, power-density and interaction time.\n\nLimits to the flexibility of the process arise from the need to produce dedicated inductors for many applications. This is quite expensive and requires the marshalling of high current densities in small copper inductors, which can require specialized engineering and 'copper-fitting'.\n\nInduction heating is used in plastic injection molding machines. Induction heating improves energy efficiency for injection and extrusion processes. Heat is directly generated in the barrel of the machine, reducing warm-up time and energy consumption. The induction coil can be placed outside thermal insulation, so it operates at low temperature and has a long life. The frequency used ranges from 30 kHz down to 5 kHz, decreasing for thicker barrels. The reduction in cost of inverter equipment has made induction heating increasingly popular. Induction heating can also be applied to molds, offering more even mold temperature and improved product quality.\n\nThe basic setup is an AC power supply that provides electricity with low voltage but very high current and high frequency. The workpiece to heat is placed inside an air coil driven by the power supply, usually in combination with a resonant tank capacitor to increase the reactive power. The alternating magnetic field induces eddy currents in the workpiece.\n\nThe frequency of the inductive current determines the depth that the induced eddy currents penetrate into the workpiece. In the simplest case of a solid round bar, the induced current decreases exponentially from the surface. An \"effective\" depth of the current-carrying layers can be derived as formula_1, where formula_2 is the depth in centimeters, formula_3 is the resistivity of the workpiece in ohm-centimeters, formula_4 is the dimensionless relative magnetic permeability of the workpiece, and formula_5 is the frequency of the ac field in Hz. The equivalent resistance of the workpiece and thus the efficiency is a function of the workpiece diameter formula_6 over the reference depth formula_2, increasing rapidly up to about formula_8. Since the workpiece diameter is fixed by the application, the value of formula_9 is determined by the reference depth. Decreasing the reference depth requires increasing the frequency. Since the cost of induction power supplies increase with frequency, supplies are often optimized to achieve a critical frequency at which formula_8. If operated below critical frequency, heating efficiency is reduced because eddy currents from either side of the workpiece impinge upon one another and cancel out. Increasing the frequency beyond the critical frequency creates minimal further improvement in heating efficiency, although it is used in applications that seek to heat treat only the surface of the workpiece.\n\nRelative depth varies with temperature because the resistivities and permeability vary with temperature. For steel, the relative permeability drops to 1 above the Curie temperature. Thus the reference depth can vary with temperature by a factor of 2-3 for nonmagnetic conductors, and by as much as 20 for magnetic steels.\n\nMagnetic materials improve the induction heat process because of hysteresis. Materials with high permeability (100–500) are easier to heat with induction heating. Hysteresis heating occurs below the Curie temperature where materials retain their magnetic properties. High permeability below the Curie temperature in the workpiece is useful. Temperature difference, mass, and specific heat influence the workpiece heating.\n\nThe energy transfer of induction heating is affected by the distance between the coil and the workpiece. Energy losses occur through heat conduction from workpiece to fixture, natural convection, and thermal radiation.\n\nThe induction coil is usually made of copper tubing and fluid cooled. Diameter, shape, and number of turns influence the efficiency and field pattern.\n\nThe furnace consists of a circular hearth which contains the charge to be melted in the form of a ring. The metal ring is large in diameter and is magnetically interlinked with an electrical winding energized by an AC source.\nIt is essentially a transformer where charge to be heated forms a single turn short circuit secondary and is magnetically coupled to the primary by an iron core.\n\n"}
{"id": "33020039", "url": "https://en.wikipedia.org/wiki?curid=33020039", "title": "Jack’s Magazine", "text": "Jack’s Magazine\n\nJack's Magazine (also known as the Saltwater River Gunpowder Magazine) is located on the Maribyrnong River at Maribyrnong, Victoria. The complex opened in 1878, to provide safe storage for bonded gunpowder and explosives imported into the colony of Victoria. The twin bluestone vaulted buildings are concealed behind high earth mound blast walls, and a tall bluestone wall, with a canal connecting it to the river. It was designed by government architect, William Wardell, Inspector General, Public Works Department and built by contractor George Cornwell.\n\nIt is registered by the National Trust, and listed on the Victorian Heritage Register. The associated Footscray Ammunition Factory was mostly demolished and redeveloped for the Delfin Lend Lease Corporation's Edgewater estate. Jack's Magazine was proposed to be redeveloped for a commercial use, but following several calls for expressions of interests it was decided that public ownership was the only practical management for the historic site. Jack's Magazine is now managed by Working Heritage – a committee appointed by the Victorian Government to manage heritage properties on Crown Land.\n"}
{"id": "469604", "url": "https://en.wikipedia.org/wiki?curid=469604", "title": "Jerrycan", "text": "Jerrycan\n\nA jerrycan (also written as jerry can or jerrican) is a robust liquid container made from pressed steel. It was designed in Germany in the 1930s for military use to hold of fuel. The development of the jerrycan was a significant improvement on earlier designs, which required tools and funnels to use. Today similar designs are used for fuel and water containers, some of which are also produced in plastic. The designs usually emulate the original steel design and are still known as jerrycans.\n\nUses for the cans have expanded beyond the original intended use of carrying fuel. Today, a can's use is denoted by its colouring, and occasionally, imprinted labelling on the container itself. This is to prevent contamination of the can's contents by mixing different fuels or mixing fuel with water.\n\nThe US version of the jerrycan is covered by military specification MIL-C-1283 and has been produced since the early 1940s by a number of US manufacturers, according to a current manufacturer, Blitz. The National Stock Number is 7240-00-222-3088. It is considered obsolete by a new A-A-59592A specification, having been replaced with plastic versions.\n\nThe history of the jerrycan is notable because the German design was reverse engineered and subsequently copied, with minor modifications, by the Allies during the Second World War. The name of the jerrycan refers to its German origins, Jerry being wartime slang for Germans.\n\nThe \"Wehrmacht-Einheitskanister\", as it was known in Germany, was first developed in 1937 by the Müller engineering firm in Schwelm to a design by their chief engineer Vinzenz Grünvogel. A similar design was used in 1936 during the Spanish Civil War, where they had a company logo for Ambi-Budd Presswerk GmbH. Among others, the Wehrmacht had specified that a soldier should be able to carry either two full containers or four empty ones, which is the reason the triple handles were fitted. To achieve the required filling and draining speed, it was fitted with a large spout and flip top closure. A hole in the closure retainer made it possible to fit a securing pin or wire with a lead seal. The rectangular shape made it stackable. The indentations ensured a full can would not be severely damaged when falling from a vehicle, while a dip coat of paint on the inside protected it from corrosion.\n\nBy 1939 the German military had thousands of such cans stockpiled in anticipation of war. Motorised troops were issued the cans with lengths of rubber hose in order to siphon fuel from any available source, as a way to aid their rapid advance through Poland at the start of the Second World War.\n\nIn 1939, American engineer Paul Pleiss had built a vehicle to journey to India with his German colleague. After building the car, they realised they did not have any storage for emergency water. The German engineer had access to the stockpile of jerrycans at Berlin Tempelhof Airport and managed to take three of them. They drove across 11 national borders without incident until Field Marshal Göring sent a plane to take the engineer home. The German engineer also gave Pleiss complete specifications for the manufacture of the can. Pleiss continued on to Calcutta, put his car in storage, and flew back to Philadelphia, where he told American military officials about the can. He could raise no interest. Without a sample, he realised he could not get anywhere. He eventually shipped the car to New York by a roundabout method, and sent a can to Washington. The War Department decided instead to use World War I cans with two screw closures, which required both a spanner and funnel for pouring.\n\nThe one jerrycan in American possession was sent to Camp Holabird, Maryland, where it was redesigned. The new design retained the handles, size and shape. The US can could be stacked interchangeably with German or British cans. The weld was replaced with rolled seams which were prone to leakage. For fuel cans, the lining was removed and a wrench and funnel were required. A similar water can was also adopted, with a flip-top lid and enamel lining. \nThe US designed jerrycan was widely used by US Army and Marine Corps units. In all overseas theaters, fuel and other petroleum products represented about 50% of all supply needs, measured by weight. In the European Theatre of Operations alone, over 19 million were required to support US forces by May 1945. \n\nThe jerrycan played an important role in ensuring fuel supply to Allied operations. A single standard US 2.5 ton truck could carry of fuel loaded in jerrycans. \nUS logisticians requested over 1.3 million per month to replace losses; these cans were provided by US and British manufacturers, but supply could not keep up with demand. Loss of jerrycans in units was severe, with 3.5 million reported 'lost' in October 1944, for example. At one point in August 1944, lack of cans (caused by losses) actually limited the supply of fuel that could be brought forward to combat units, even though the fuel was available in rear areas.\n\nThe US design was slightly lighter than the German can ( vs. for the German version). These fuel containers were subsequently used in all theatres of war around the world. Such was the importance of the cans in the war effort that President Roosevelt noted \"Without these cans it would have been impossible for our armies to cut their way across France at a lightning pace which exceeded the German Blitzkrieg of 1940.\"\n\nAt the beginning of the Second World War the British Army was equipped with two simple fuel containers: the container made of pressed steel, and the container made from tin plate. The 2-gallon containers were relatively strong, but were expensive to produce. Manufactured primarily in Egypt, the 4-gallon containers were plentiful and inexpensive, but they had a tendency to leak after minor damage. Early 4-gallon containers were packed in pairs in wooden cases. When stacked, the timber framing protected the tins and prevented the upper layers of tins from crushing the lower. As the war progressed, the wooden case was replaced with either thin plywood or cardboard cases, neither of which provided much protection. 4-gallon containers carrying fuel were hazardous to the cargo ships carrying them. The leaking fuel would accumulate in cargo holds. At least one such ship exploded.\n\nThough adequate for transport along European roads, the 4-gallon containers proved extremely unsatisfactory during the North African Campaign. The crimped or soldered seams easily split during transport, especially off road over the rock strewn deserts of North Africa. In addition, the containers were easily punctured by even minor trauma. Because of these problems the troops referred to the 4-gallon containers as \"flimsies\". Transport of fuel over rough terrain often resulted in as much as 25% of the fuel being lost through seam failures or punctures. Fuel leaks gave vehicles a propensity to catch fire. The containers were routinely discarded after a single use, and severely hampered the operation of the British Eighth Army. A more successful and popular use for the 4-gallon container was to convert it into a cooking stove, referred to as a 'Benghazi burner'.\n\nWhen the British Army first saw the German fuel cans during the Norwegian Campaign in 1940, they immediately saw the advantages of the superior design. The three handles allowed easy handling by one or two people, or movement bucket brigade-style. The handle design also allows for two empty cans to be carried in each hand, utilizing the outer handle.\n\nThe sides of the can were marked with cross-like indentations that strengthened the can while allowing the contents to expand, as did an air pocket under the handles when the can was filled correctly. This air pocket allowed the container to float if dropped in water. Rather than a screw cap, the containers used a cam lever release mechanism with a short spout secured with a snap closure and an air-pipe to the air pocket which enabled smooth pouring (which was omitted in some copies). The interior was also lined with an impervious plastic, first developed for steel beer barrels that would allow the can to be used for either water or gasoline. The can was welded and had a gasket for a leak-proof mouth. \n\nThe British used cans captured from the \"Jerries\" (Germans) – hence \"jerrycans\" – in preference to their own containers as much as possible. Later in 1940, Pleiss was in London and British officers asked him about the design and manufacture of the jerrycan. Pleiss ordered the second of his three jerrycans flown to London. After the second capture of Benghazi at the end of 1941, large numbers of Axis jerrycans were captured, sufficient to equip some units such as the Long Range Desert Group.\n\nThe strength of the \"Wehrmachtskanister\" was determined in the Soviet Union. Its design was later copied and the Soviet Army accepted it as the standard container for liquids. This container is still being produced and used in modern Russia. In civilian use this container is used primarily for automotive fuel and lubricants.\n\nThe German/British design jerrycan is still a standard fuel and other liquids container in the armies of the NATO countries.\n\nFinnish designer Eero Rislakki designed a plastic jerrycan in 1970 with a small screwable stopper on the top side behind the handle to allow air flowing in to ensure smooth fuel outflow. It is lighter than the original design yet almost as sturdy. It was quickly adopted by the Finnish armed forces, and is commercially available.\n\nThe Jerrican is defined by the Code of Federal Regulation, 49 CFR 171.8 as \"a metal or plastic packaging of rectangular or polygonal cross-section\".\n\n\nThese new regulations do not apply to OSHA-approved metal safety containers, but rather to the common red plastic, portable gas cans. The regulations apply only to newly manufactured gasoline cans, and there is no requirement on the part of users to discard their existing cans or to upgrade, although the EPA provides informational resources for implementing community Gas Can Exchange Programs.\n\nFurthermore, in the state of California, the following colours are mandated:\n\nPer ASTM F852, the particular shades should be \"medium yellow\" and \"medium blue\".\n\nThe transportation of dangerous goods (which includes liquid fuels) within Europe is governed by the UN \"European Agreement concerning the International Carriage of Dangerous Goods by Road\" (ADR). The term \"jerrican\" is defined within Chapter 1.2 of the 2011 ADR as \"a metal or packaging of rectangular or polygonal cross-section with one or more orifices\", a definition which includes the traditional jerrycan but which also covers a wide range of other packagings.\n\nThe ADR sets performance standards for packaging and specifies what standard of packaging is required for each type of dangerous good, including gasoline/petrol and diesel fuels. The traditional jerrycan is available in UN-marked approved versions which satisfy the requirements of the ADR.\n\n"}
{"id": "14235687", "url": "https://en.wikipedia.org/wiki?curid=14235687", "title": "Leonid Zamyatin", "text": "Leonid Zamyatin\n\nLeonid Mitrofanovich Zamyatin () (born 9 March 1922) is a former Soviet ambassador and diplomat. He graduated from the Moscow Aviation Institute, and worked as a diplomat from 1946. He became an adviser to the Soviet delegation at the United Nations, and a permanent representative of the Soviet Union on the IAEA Board of Governors. From 1962 to 1970, he served in the Ministry of Foreign Affairs of the Soviet Union, becoming head of the press department. From 1970 to 1978, he was director general of TASS, the official news agency of the Soviet Union. He was Chairman of the International Information Department of the Central Committee of the Communist Party of the Soviet Union from 1978 to 1986. In 1986, he was appointed the Soviet ambassador to the United Kingdom. He was forced to resign his ambassadorship after his refusal to condemn the 1991 August Coup against Mikhail Gorbachev.\n"}
{"id": "10849414", "url": "https://en.wikipedia.org/wiki?curid=10849414", "title": "Lever rule", "text": "Lever rule\n\nThe lever rule is a tool used to determine the mole fraction (\"x\") or the mass fraction (\"w\") of each phase of a binary equilibrium phase diagram. It can be used to determine the fraction of liquid and solid phases for a given binary composition and temperature that is between the liquidus and solidus line.\n\nIn an alloy or a mixture with two phases, α and β, which themselves contain two elements, A and B, the lever rule states that the mass fraction of the α phase is\n\nwhere\nall at some fixed temperature or pressure.\n\nSuppose an alloy at an equilibrium temperature \"T\" consists of formula_4 mass fraction of element B. Suppose also that at temperature \"T\" the alloy consists of two phases, α and β, for which the α consists of formula_2, and β consists of formula_3. Let the mass of the α phase in the alloy be formula_8 so that the mass of the β phase is formula_9, where formula_10 is the total mass of the alloy.\n\nBy definition, then, the mass of element B in the α phase is formula_11, while the mass of element B in the β phase is formula_12. Together these two quantities sum to the total mass of element B in the alloy, which is given by formula_13. Therefore,\n\nBy rearranging, one finds that\n\nThis final fraction is the mass fraction of the α phase in the alloy.\n\nBefore any calculations can be made, a \"tie line\" is drawn on the phase diagram to determine the mass fraction of each element; on the phase diagram to the right it is line segment LS. This tie line is drawn horizontally at the composition's temperature from one phase to another (here the liquid to the solid). The mass fraction of element B at the liquidus is given by \"w\" (represented as \"w\" in this diagram) and the mass fraction of element B at the solidus is given by \"w\" (represented as \"w\" in this diagram). The mass fraction of solid and liquid can then be calculated using the following lever rule equations:\n\nwhere \"w\" is the mass fraction of element B for the given composition (represented as \"w\" in this diagram).\n\nThe numerator of each equation is the original composition we are interested in +/- the opposite \"lever arm\". That is if you want the mass fraction of solid then take the difference between the liquid composition and the original composition. And then the denominator is the overall length of the arm so the difference between the solid and liquid compositions. If you're having difficulty realising why this is so, try visualising the composition when \"w\" approaches \"w\". Then the liquid concentration will start increasing.\n\nThere is now more than one two-phase region. The tie line drawn is from the solid alpha to the liquid and by dropping a vertical line down at these points the mass fraction of each phase is directly read off the graph, that is the mass fraction in the x axis element. The same equations can be used to find the mass fraction of alloy in each of the phases, i.e. w is the mass fraction of the whole sample in the liquid phase.\n"}
{"id": "759298", "url": "https://en.wikipedia.org/wiki?curid=759298", "title": "Linear density", "text": "Linear density\n\nLinear density is the measure of a quantity of any characteristic value per unit of length. Linear mass density (titer in textile engineering, the amount of mass per unit length) and linear charge density (the amount of electric charge per unit length) are two common examples used in science and engineering.\n\nThe term linear density is most often used when describing the characteristics of one-dimensional objects, although linear density can also be used to describe the density of a three-dimensional quantity along one particular dimension. Just as density is most often used to mean mass density, the term linear density likewise often refers to linear mass density. However, this is only one example of a linear density, as any quantity can be measured in terms of its value along one dimension.\n\nConsider a long, thin rod of mass formula_1 and length formula_2. To calculate the average linear mass density, formula_3, of this one dimensional object, we can simply divide the total mass, formula_1, by the total length, formula_2:\nIf we describe the rod as having a varying mass (one that varies as a function of position along the length of the rod, formula_7), we can write:\nEach infinitesimal unit of mass, formula_9, is equal to the product of its linear mass density, formula_10, and the infinitesimal unit of length, formula_11:\nThe linear mass density can then be understood as the derivative of the mass function with respect to the one dimension of the rod (the position along its length formula_13)\n\nThe SI unit of linear mass density is the kilogram per meter (kg/m).\n\nLinear density of fibers and yarns can be measured by many methods. The simplest one is to measure a length of material and weigh it. However, this requires a large sample and masks the variability of linear density along the thread, and is difficult to apply if the fibers are crimped or otherwise cannot lay flat relaxed. If the density of the material is known, the fibers are measured individually and have a simple shape, a more accurate method is direct imaging of the fiber with SEM to measure the diameter and calculation of the linear density. Finally, linear density is directly measured with a vibroscope. The sample is tensioned between two hard points, mechanical vibration is induced and the fundamental frequency is measured.\n\nConsider a long, thin wire of charge formula_14 and length formula_2. To calculate the average linear charge density, formula_16, of this one dimensional object, we can simply divide the total charge, formula_14, by the total length, formula_2:\nIf we describe the wire as having a varying charge (one that varies as a function of position along the length of the rod, formula_7), we can write:\nEach infinitesimal unit of charge, formula_22, is equal to the product of its linear charge density, formula_23, and the infinitesimal unit of length, formula_11:\nThe linear charge density can then be understood as the derivative of the charge function with respect to the one dimension of the wire (the position along its length, formula_7)\n\nNotice that these steps were the exact same ones we took before to find :formula_28\n\nThe SI unit of linear charge density is the coulomb per meter (C/m).\n\nIn drawing or printing, the term linear density also refers to how densely or heavily a line is drawn.\n\nCommon units include:\n\n"}
{"id": "28345268", "url": "https://en.wikipedia.org/wiki?curid=28345268", "title": "Listing number", "text": "Listing number\n\nIn mathematics, a Listing number of a topological space is one of several topological invariants introduced by the 19th-century mathematician Johann Benedict Listing and later given this name by Charles Sanders Peirce. Unlike the later invariants given by Bernhard Riemann, the Listing numbers do not form a complete set of invariants: two different two-dimensional manifolds may have the same Listing numbers as each other.\n\nThere are four Listing numbers associated with a space. The smallest Listing number counts the number of connected components of a space, and is thus equivalent to the zeroth Betti number.\n"}
{"id": "33179421", "url": "https://en.wikipedia.org/wiki?curid=33179421", "title": "Low molecular-mass organic gelators", "text": "Low molecular-mass organic gelators\n\nLow molecular-mass organic gelators (LMOGs) are a relatively new and dynamic soft materials capable of numerous possible applications; LMOGs are the monomeric sub-unit which form self-assembled fibrillar networks (SAFINs) that entrap solvent between the strands. SAFINs arise from the formation of strong non-covalent interactions between LMOG monomeric sub-units. As SAFINs are forming, the long fibers become intertwined and trap solvent molecules. Once solvent molecules are entrapped within the network, they are immobilized by surface tension effects. The stability of a gel is dependent on the equilibrium between the assembled network and the dissolved gelators. One characteristic of an LMOG, that demonstrates its stability, is its ability to contain an organic solvent at the boiling point of that solvent due to extensive solvent-fibrillar interactions. \nGels self-assemble through non-covalent interactions such as π-stacking, hydrogen-bonding, or Van der Waals interactions to form volume-filling 3D networks. Self-assembly is key to gel formation and dependent upon reversible bond formation.\nThe propensity of a low molecular weight molecule to form LMOGs is classified by its Minimum Gelation Concentration (MGC). The MGC is the lowest possible gelator concentration needed to form a stable gel. A lower MGC is desired to minimize the amount of gelator material needed to form gels. Super gelators have a MGC of less than 1 wt%.\n\nLMOGs were first reported in the 1930s, but advances in the field were more often than not discoveries of chance; as there existed little theoretical understanding of gel formation. During this time LMOGs found applications in thickening lubricants, printing inks, and napalm. Interest in the field dwindled for several decades until the mid-1990s when Hanabusa, Shinkai, and Hamilton designed numerous LMOGs which form thermoreversible intermolecular amide-carbonyl hydrogen bonds. The LMOGs developed by Hanabusa \"et. al\" were suitable for forming hard gels, including gels with chloroform, which had been resistant to gelation prior to their discovery. These new LMOGs were rationally designed and represented the first time that scientists had been able to discover new LMOGs based on supramolecular principles. From these earliest studies and screening numerous compounds, it was determined that for thermoreversible gels based on the amide-carbonyl hydrogen bond, amino acid structure, enantiopurity, hydrophilic-lypophilic ratio, and increasing peptide substitution greatly affected the gelling ability of various new compounds.\n\nThe aforementioned principles that developed in this field's infancy have proved successful in allowing researchers to tune LMOGs for different functions. Today, LMOGs have been extensively studied for their unique properties. This newfound functional diversity has led to a wide range of possible applications for LMOGs in agriculture, drug delivery, pollutant/heavy metal remediation, luminescent devices, and chemical sensing.\n\nThe majority of LMOGs can be triggered to form by manipulating the systems' properties, such as the pH, solvent, exposure to light, or by introducing oxidizing or reducing reagents. Researchers have proposed a set of guidelines for successful gel formation\n\nTraditionally, gel phase transitions are strictly temperature dependent. However, it has recently been shown that non-liquid crystalline gelators, composed of (R)-18-(n-alkylamino)octadecan-7-ols (HSN-n), undergo first order gel-to-gel phase transitions; leading to different morphologies of the gel in carbon tetrachloride (CCl). The uniqueness of this discovery stems from the idea that it is the solvent molecules entering and exiting the structure which leads to the different structural morphologies. All other previously known gel phase transitions have occurred as the result of temperature changes and only one previous case documents this type of solvent dependent morphological change. However, even in the case of N-isopropylacrylamide hydrogels that underwent conformational changes (folding and unfolding of their polymer chains); it occurred only via a temperature dependent process which resulted in water molecules, near the structure, entering or exiting the structure.\nThe stability of a formed gelation matrix is dependent on the equilibrium between the assembled network and the dissolved gelator assemblies. \nLMOGs are functionally diverse and can be composed of both polar and non-polar regions (amphiphiles).\n\nScanning Electron Microscopy is a useful means for researchers to determine the structural properties of a low molecular-mass weight gel. These gels exhibit a wide range of structures; from fibrous strands (of various lengths) to ribbons and tubes. The structure of these gels is a key factor in their ability to gel solvents or water. Their tertiary structure determines the critical gelation concentration of the gel.\n\nGenerally, rheology is used to study the flow of matter within a substance. In order for a substance to be considered a gel it must possess solid-like traits when characterized by rheological measurements. Rheological characterization, tests materials by applying stress to measure the material's resistance to deformation. From rheological measurements, a gel can be classified as either a \"strong\" or \"weak\" gel. This classification emphasizes the strength of the interactions between gelator molecules in a particular gel. A \"weak\" gel is often not considered a true gel because it does not conform to a purely solid-like material's rheological traits. Instead, \"weak\" gels are generally better classified as viscoelastic liquids.\n\nAs a result of this distinction, these classes of gels demonstrate different elasticity as calculated by the elastic modulus, a mathematical model for predicting the elasticity of different materials under different stressors. The shear modulus (G) of a \"strong\" gel exhibits a smaller dissipation of energy than \"weak\" gels, and the \"strong\" gel's G-values plateau for longer periods of time. Furthermore, rheological properties of different gels can occasionally be used to compare naturally occurring biopolymer gels with synthetic LMOGs.\n\nResearchers have not been able to reliably predict novel LMOGs. A key aspect in predicting new gelator materials is understanding the interaction between the gel molecules and the solvent. The most common solvents for LMOGs are organic in nature and result in organogels. Much rarer are hydrogels, or gels that form with water as the solvent. Several attempts have been made to quantify the gel and solvent interaction using a variety of parameters:\n\nPheromone release devices Multiple reservoir-type controlled release devices (CRDs) have been developed to achieve the controlled release of highly volatile pheromones into an agricultural setting; whereby they could act as pesticides throughout the growing season. There are several draw-backs associated with current CRDs because they involve multi-step preparation protocols, exhibit low pheromone-holding capacities, are not biodegradable, and exhibit leaking of the pheromones when compressed or broken. To address these functional issues a sugar alcohol-based amphiphilic super-gelator, mannitol dioctanoate (M8), has been developed that efficiently gelled the pheromones, 2-heptanone and lauryl acetate. The miticide, 2-heptanone controls the parasitic mite, varroa (\"Varroa destructor\"), that are responsible for honey bee (\"Apis mellifera L\") colony destruction. The researchers further developed the application of this supergelator by developing a reservoir-type CRD that consisted of the 2-heptanone gel in a vapor-barrier-film sealed pouch which was then activated by boring a small hole through the vapor barrier. The CRD had a high loading capacity of 92% wt/wt allowing for the construction of small devices with a high biocompatibility and because, M8 is composed of mannitol and fatty acids it is also biodegradable.\n\nResearchers have been exploring LMOGs belonging to a class of molecules called cyclohexane trisamides due to their ability to form hydrogels. By attaching functional groups to the gelator molecule, the researchers can adjust the gelation properties. The gels transition to the liquid state upon changes in temperature or pH Taking it one step further, the researchers attached an amino acid and a model drug to the gel molecule and added an enzyme to the gel matrix. When the temperature or pH was changed, the gel molecules entered the liquid phase where the amino acid and drug molecule could be cleaved from the gel molecule by the enzyme. Researchers believe these LMOGs may some day be used as a fast, two-step release drug delivery system.\n\nIn 2010 researchers developed phase-selective gelators toward the containment and treatment of oil spills. They developed a class of LMOGs that were capable of gelling diesel, gasoline, pump, mineral, and silocone oils. These LMOGs were composed of dialkanoate derivatives of the sugar alcohols, mannitol and sorbitol. These sugar alcohol derivatives were ideal as they are biodegradable, inexpensive, and non-toxic. Once the oil was taken up by the gel fibers; it could then be separated from the gel by utilizing vacuum distillation and furthermore the gelator could be recycled.\n\nSome gels can be used in luminescent devices such as OLED's and/or fluorescent sensors. One example of an OLED type LMOG is mono-substituted ethynyl-pyrene. This gelator forms a stable gel with DMF, toluene, or cyclohexane while maintaining its luminescence. Another important characteristic of these gels is that they maintain high charge carrier mobility. This means that the gel can pass sufficient current in an electronic luminescent device.\n\nFurthermore, luminescent gels can also be utilized as sensors. These sensors operate by forming a stable luminescent gel in the presence of different analytes. One example of a luminescent gel for sensing fluoride anions is presented by Prasad and Rajamalli. This example utilizes poly(aryl ether) dendrons attached to a core aryl ether with [anthracene] attached. Upon forming a stable yellow gel (under normal gelation conditions), if fluoride is introduced in the presence of the gel, the gel undergoes a gel to sol transition and becomes bright red. Being able to visually detect a color change in the presence of a dilute analyte is a promising field application of LMOG materials.\n\nMolecular gels can be sensitized toward an external stimuli aka light, heat, or chemicals. Also, LMOG's can be sensitized by the incorporation of a receptor\nunit or a spectroscopically active unit into the gelator molecule. A variety of quinoxalinones were recently developed that act as a mercury sensor by forming a gel when these ligands complex to mercury. \nA nonplanar dihydropyridine derivative was induced to gel upon oxidizing the molecule with nitric oxide and then dissolving the oxidized ligand in DMSO/water and then heating and cooling the mixture. This gel has the useful application as it can therefore act as a nitric oxide sensor.\n\nAerogel\n\nNanogel (insulation)mo\n\nGel Permeation Chromatography\n\nRheology\n\n"}
{"id": "18986312", "url": "https://en.wikipedia.org/wiki?curid=18986312", "title": "Marorka", "text": "Marorka\n\nMarorka is a company which specializes in marine energy management. Marorka’s head office is in Reykjavik, Iceland along with its servers and data storage infrastructure are supplied with electricity generated using 100% renewable energy resources – geothermal and hydroelectric. Marorka has international offices in Hamburg, GE, Singapore, SG, Athens, GR and Dubai, AE. Marorka's mission is to deliver products and services to vessel owners and operators to save fuel, increase profitability and reduce harmful emissions. The company was founded in June 2002 and resulted from the PhD thesis of Jón Ágúst Thorsteinsson, Entrepreneur and founder.\n\nMarorka has developed reliable, automated, on-board and online energy management systems for the international shipping industry. Marorka’s products and services enable vessel operators to optimize fuel consumption by maximizing the energy efficiency of their vessels through the implementation of real-time monitoring and decision support, which are essential components of any operational optimization process. Marorka’s commitment towards energy efficiency goes beyond the technological dimension – ships’ officers play a significant role in increasing energy efficiency and reducing emissions.\nMarorka's field of work:\n\nMarorka’s systems have been installed on board vessels of various types and sizes; Fishing vessels, Cargo vessels, Bulk carriers, LNG carriers, Cruise ships and Research vessels.\n\nIn 2008, Marorka received the Nordic Council Environment Prize. 37 companies were nominated for the award.\n\nMarorka has implemented a quality management system built on the standard, certified by Det Norske Veritas.\n\n\n\n"}
{"id": "38885156", "url": "https://en.wikipedia.org/wiki?curid=38885156", "title": "Millwork (building material)", "text": "Millwork (building material)\n\nMillwork building materials are historically any woodmill-produced building construction interior-finish, exterior-finish, or decorative components. Stock profiled and patterned millwork building components fabricated by milling at a planing mill can usually be installed with minimal alteration. Today, millwork also encompasses items that are made using alternatives to wood, including synthetics, plastics, and wood-adhesive composites.\n\nMillwork building materials include the ready-made carpentry elements usually installed in any building. Many of the specific features of the space are created using different types of architectural millwork: doors, window casings, and cabinets to name just a few. The materials used in millwork items today are most often graded-lumber, code compliant fasteners, various glasses, and other decorative coatings and finishes. Most millwork building materials can be installed with little or no modification as part of the construction process.\n\nHistorically, the term millwork applied to building elements made specifically from wood. During the \"Golden Age\" of millworking (1880–1910), virtually everything in the house was made from wood. During this time, the millwork produced in the United States became standardized nationwide.\n\nToday, the increase in the use of synthetic materials has led many professionals to consider any item that is composed of a combination of wood and synthetic elements to also be properly defined as millwork. This includes products that make use of pressed-wood chips in the design, such as melamine coated shelving.\n\nThere are two types of manufacturers of millwork goods. In one, referred to as \"stock millwork\", commodity fabricators mass-produce trims and building components—with the end product being low cost, interchangeable items for commercial or home builders. In another, the product is custom produced for individuals or individual building projects—usually a costlier option which is referred to as \"architectural millwork.\"\n\nMillwork building materials are used for both decoration and to increase the utility of buildings.\n"}
{"id": "38068423", "url": "https://en.wikipedia.org/wiki?curid=38068423", "title": "Nuclear Electric Insurance Limited", "text": "Nuclear Electric Insurance Limited\n\nNuclear Electric Insurance Limited (NEIL) is a mutual insurance company which insures all nuclear power plants in the United States as well as some facilities internationally. The company is based in Wilmington, Delaware, and is registered in Bermuda.\n\nIt was founded in 1980 in response to the 1979 Three Mile Island accident. In 1997 NEIL merged with Nuclear Mutual Limited, of Bermuda.\n\n"}
{"id": "55608661", "url": "https://en.wikipedia.org/wiki?curid=55608661", "title": "Nuclear reactor heat removal", "text": "Nuclear reactor heat removal\n\nThe removal of heat from nuclear reactors is an essential step in the generation of energy from nuclear reactions. In nuclear engineering there are a number of empirical or semi-empirical relations used for quantifying the process of removing heat from a nuclear reactor core so that the reactor operates in the projected temperature interval that depends on the materials used in the construction of the reactor. The effectiveness of removal of heat from the reactor core depends on many factors, including the cooling agents used and the type of reactor. Common coolers for nuclear reactors include: heavy water, the first alkaline metals (such as sodium and lithium), lead or lead-based alloys, and <chem>NaK</chem>.\n\nThe thermal energy produced in nuclear fuel comes mainly from the kinetic energy of fission fragments. Therefore, the heat generated per volume unit is proportional to the fraction of nuclear fissionable fuel burned in the unit of time:\n\nformula_1\n\nwhere formula_2 represents the number of atoms in a cubic meter of fuel, a is the amount of energy released in the fuel in each fission reaction (~181 MeV),  formula_3 is the neutronic flux, and formula_4 is the effective section of the fission.\n\nThe total heat produced in the nuclear reactor is:\n\nformula_5\n\nwhere formula_6 is the mean neutronic flux and V is the fuel volume (normally measured in formula_7).\n\nRecovery of this amount of heat is achieved by using cooling fluids whose temperature at the entrance to the reactor channel formula_8  will increase with the distance traveled in the channel. The thermal balance of the channel is expressed by the relationship:\n\nformula_9\n\nwhere formula_10 is the flow rate of the cooling agent, formula_11 is the specific heat at constant pressure, formula_12 is the increase in the temperature of the fluid after passing a distance formula_13 in the channel, formula_14 is the heat generated per unit volume of the fuel, formula_15 is the fuel cell radius and formula_16 is the number of channel bars.\n\nUnder these conditions, the temperature of the cooling agent at distance z travelled into the cooling channel inside nuclear reactor is obtained by integrating the previous equation:\n\nformula_17\n\nThe difference between the temperature of the outer surface of the tube-channel formula_18 and the temperature of the fluid is obtained from the relationship:\n\nformula_19\n\nwhere  formula_20 is the local heat flow on the casing - cooler contact surface unit and formula_21 is the heat transfer agent casing-cooling agent.\n\nThe heat discharge from the PWR and PHWR reactors is made by pressurized water under forced convection. The general expression for determining the transfer coefficient is given by the Dittus - Boelter equation:\n\nformula_22\n\nwhere formula_23 is Nusselt's number ( formula_24, formula_21 is the heat transfer coefficient, formula_26 is the equivalent diameter, formula_27 is the thermal conductivity of the fluid); formula_28 is a constant (formula_28=0.023);  formula_30 is the number of Reynolds ( formula_31) formula_32 is the average velocity of the fluid in the section considered, formula_33 is the density of the fluid and formula_34 is its dynamic viscosity); formula_35 is the number of Prandtl (formula_36).\n\nIf the flow of the fluid is made under conditions of a great difference between its temperature and the contact surface, the transfer coefficient is determined from the relationship:\n\nformula_37\n\nwhere formula_38 is the dynamic viscosity of the coolant at the temperature of the adhering fluid film at the surface of the casing. The relation presented above is valid in the case of a long channel with formula_39, where formula_40 is the length of the channel.\n\nThe transfer coefficient for cooling the pipes by natural convection is obtained from:\n\nformula_41\n\nwhere formula_42 is the Grashof number given by the expression:\n\nformula_43\n\nWe use the notation  formula_44 for the volume expansion coefficient of the fluid, formula_45 is the gravitational acceleration and formula_46 is the difference between the average wall temperatures of the casing and the cooling agent.\n\nIn boiling water cooled reactors (BWR) and partly in pressure water cooled reactors (PWR and PHWR) the heat transfer is made with a vapor phase in the cooling medium, which is why this type of heat transfer is called heat transfer in a biphasic system. This allows obtaining much higher transfer coefficients than the one-phase heat transfer described in the Dittus-Boelter equation.\nIncreasing the flow of heat, reducing the agent flow and lowering the pressure can lead to increased temperature of the cooled surface. If the temperature of the fluid in the channel section that we consider is lower than the boiling temperature under local pressure conditions, the vaporization is limited to the immediate vicinity of the surface and in this case the boiling is called submerged boiling. There is no proportionality between the heat flow and the difference between the surface temperature and the coolant temperature that allows the definition of a heat transfer coefficient similar to the one-phase case. In this situation we can use the equation of Jens and Lottes, which establishes a connection between the difference formula_47 between the surface temperature and the boiling temperature of the cooling agent under local pressure conditions formula_48 below the thermal flux formula_20:      \n\nformula_50  \n\nwhere formula_51 and formula_52\n\nIf the temperature of the fluid in the channel section considered is slightly higher than the boiling temperature under local pressure conditions, the heat transfer is by boiling with nucleation, forming vapor bubbles trained by the cooling agent (that becomes biphasic throughout its entire volume). However, the vapor content is relatively small and the continuous phase remains the liquid phase. The vapor content of the PHW-CANDU reactor is about 0.03-0.04 kg steam / kg of agent, thus increasing the amount of heat transported by the unit mass of agent by over 10%. If the cooled surface temperature far exceeds the boiling temperature of the cooling agent in the channel section, the vapor content of the agent increases considerably, the continuous phase becoming the vapor phase and the liquid phase becoming only a suspension between vapors. The cooled surface remains covered with a liquid film which still provides a very high heat transfer coefficient, formula_53 at BWR compared to formula_54 at PWR. The film of liquid is continuously fed with drops from the agent suspension.\n\nA further increase in surface temperature leads to a temporary interruption of continuity of the liquid film adhering to the cooled surface. Watering of the surface continues, however, by the drops of liquid in the suspension that are present in the cooling agent as long as the heat flow remains below a value that depends on local conditions (value that is called critical flux). Over this flux there is a thermal transfer crisis characterized by a sudden decrease in the transfer coefficient due to the presence of only one-phase transfer. The heat transfer coefficient in the pre-crisis period can be determined from the relationship:\n\nformula_55\n\nwhere formula_56\n\nIn these formulas the following notations were made: formula_57 is pressure losses for the two phases (water and vapors), formula_58 ( formula_20 - the thermal flux, formula_60- the enthalpy of the biphasic liquid-gaseous mixture). The heat transfer coefficient during the crisis is related to the critical heat flow formula_61 through a linear relationship, of the equation type formula_19 that was presented before:\n\nformula_63\n\nWhere formula_64 is the temperature of the surface in thermal transfer crisis, and formula_65 is the temperature of the vapor at saturation.\n\nThe critical flow is obtained by using the Kutateladze's formula:\n\nformula_66\n\nwhere formula_67 (J/kg)is the latent heat of vaporization, formula_68 and formula_69 are density of the liquid and saturation vapor, formula_70 is the superficial tension in N / m and formula_45 is the gravitational acceleration. The heat transfer to the gas-cooled reactors is carried out by forced convection. For a gaseous thermal agent, the heat transfer coefficient can be deduced from a relation of the type Dittus-Boelter, but taking into account, for the intervening sizes, the values ​​corresponding to the average temperature of the fluid film denoted by the index formula_72:\n\nformula_73\n\nwhich differs in the use of water by a slightly lower value of the coefficient a.\n\nForced flow relationships established for fluids are also not valid for liquid metals. The coefficient of heat transfer for circular pipelines with constant heat flux, where the heat evacuation is achieved by the turbulent flow of the molten metals, can be estimated with a relation of the type:\n\nformula_74\n\nwhere formula_75 is the number of Peclet (formula_76).\n\nFor exemplification of the above formulas the hydrodynamic parameters of some types of reactors can be found in the following table:\nG1 and EL-4 are reactors that were built in France, while VVER-440 is a reactor that has been constructed in the Soviet Union.\n"}
{"id": "13440591", "url": "https://en.wikipedia.org/wiki?curid=13440591", "title": "Onium", "text": "Onium\n\nAn onium (plural: onia) is a bound state of a particle and its antiparticle. They are usually named by adding the suffix \"-onium\" to the name of the constituting particle except for muonium which, despite its name, is not a bound muon–antimuon onium, but an electron–antimuon bound state, and whose name was assigned by IUPAC. A muon–antimuon onium would be named true muonium or muononium.\n\nPositronium is an onium which consists of an electron and a positron bound together as a long-lived metastable state. Positronium has been studied since the 1950s to understand bound states in quantum field theory. A recent development called non-relativistic quantum electrodynamics (NRQED) used this system as a proving ground.\n\nPionium, a bound state of two oppositely-charged pions, is interesting for exploring the strong interaction. This should also be true of protonium. The true analogs of positronium in the theory of strong interactions are the quarkonium states: they are mesons made of a heavy quark and antiquark (namely, charmonium and bottomonium). Exploration of these states through non-relativistic quantum chromodynamics (NRQCD) and lattice QCD are increasingly important tests of quantum chromodynamics.\n\nUnderstanding bound states of hadrons such as pionium and protonium is also important in order to clarify notions related to exotic hadrons such as mesonic molecules and pentaquark states.\n\n"}
{"id": "20218705", "url": "https://en.wikipedia.org/wiki?curid=20218705", "title": "Operation Chrome Dome", "text": "Operation Chrome Dome\n\nOperation Chrome Dome was a United States Air Force Cold-War era mission from 1960 to 1968 in which B-52 Stratofortress strategic bomber aircraft armed with thermonuclear weapons remained on continuous airborne alert, flying routes to points on the Soviet Union border.\n\nDuring the Cold War, General Thomas S. Power initiated a program whereby B-52s performed airborne alert duty under code names such as \"Head Start\", \"Chrome Dome\", \"Hard Head\", \"Round Robin\", and \"Operation Giant Lance\". Bombers loitered near points outside the Soviet Union to provide rapid first strike or retaliation capability in case of nuclear war.\n\nThe missions in 1964 involved a B-52D that left Sheppard Air Force Base, Texas and flew across the United States to New England and headed out to the Atlantic Ocean. The aircraft refueled over the Atlantic heading north to and around Newfoundland. The bomber changed course and flew northwesterly over Baffin Bay towards Thule Air Base, Greenland. At this point it flew west across Queen Elizabeth Islands of Canada. Continuing to Alaska, it refueled over the Pacific Ocean again heading south-east and returned to Sheppard AFB.\n\nBy 1966, three separate missions were being flown - one East over the Atlantic and the Mediterranean, another north to Baffin Bay, and a third over Alaska.\n\nThe following military units were involved:\n\nThe program was involved in the following nuclear-weapons accidents:\n\n"}
{"id": "54423", "url": "https://en.wikipedia.org/wiki?curid=54423", "title": "Phase transition", "text": "Phase transition\n\nThe term phase transition (or phase change) is most commonly used to describe transitions between solid, liquid, and gaseous states of matter, as well as plasma in rare cases. A phase of a thermodynamic system and the states of matter have uniform physical properties. During a phase transition of a given medium, certain properties of the medium change, often discontinuously, as a result of the change of some external condition, such as temperature, pressure, or others. For example, a liquid may become gas upon heating to the boiling point, resulting in an abrupt change in volume. The measurement of the external conditions at which the transformation occurs is termed the phase transition. Phase transitions commonly occur in nature and are used today in many technologies.\n\nExamples of phase transitions include:\n\n\nPhase transitions occur when the thermodynamic free energy of a system is non-analytic for some choice of thermodynamic variables (cf. phases). This condition generally stems from the interactions of a large number of particles in a system, and does not appear in systems that are too small. It is important to note that phase transitions can occur and are defined for non-thermodynamic systems, where temperature is not a parameter. Examples include: quantum phase transitions, dynamic phase transitions, and topological (structural) phase transitions. In these types of systems other parameters take the place of temperature. For instance, connection probability replaces temperature for percolating networks.\n\nAt the phase transition point (for instance, boiling point) the two phases of a substance, liquid and vapor, have identical free energies and therefore are equally likely to exist. Below the boiling point, the liquid is the more stable state of the two, whereas above the gaseous form is preferred.\n\nIt is sometimes possible to change the state of a system diabatically (as opposed to adiabatically) in such a way that it can be brought past a phase transition point without undergoing a phase transition. The resulting state is metastable, i.e., less stable than the phase to which the transition would have occurred, but not unstable either. This occurs in superheating, supercooling, and supersaturation, for example.\n\nPaul Ehrenfest classified phase transitions based on the behavior of the thermodynamic free energy as a function of other thermodynamic variables. Under this scheme, phase transitions were labeled by the lowest derivative of the free energy that is discontinuous at the transition. \"First-order phase transitions\" exhibit a discontinuity in the first derivative of the free energy with respect to some thermodynamic variable. The various solid/liquid/gas transitions are classified as first-order transitions because they involve a discontinuous change in density, which is the (inverse of the) first derivative of the free energy with respect to pressure. \"Second-order phase transitions\" are continuous in the first derivative (the order parameter, which is the first derivative of the free energy with respect to the external field, is continuous across the transition) but exhibit discontinuity in a second derivative of the free energy. These include the ferromagnetic phase transition in materials such as iron, where the magnetization, which is the first derivative of the free energy with respect to the applied magnetic field strength, increases continuously from zero as the temperature is lowered below the Curie temperature. The magnetic susceptibility, the second derivative of the free energy with the field, changes discontinuously. Under the Ehrenfest classification scheme, there could in principle be third, fourth, and higher-order phase transitions.\n\nThough useful, Ehrenfest's classification has been found to be an incomplete method of classifying phase transitions, for it does not take into account the case where a derivative of free energy diverges (which is only possible in the thermodynamic limit). For instance, in the ferromagnetic transition, the heat capacity diverges to infinity. The same phenomenon is also seen in superconducting phase transition.\n\nIn the modern classification scheme, phase transitions are divided into two broad categories, named similarly to the Ehrenfest classes:\n\nFirst-order phase transitions are those that involve a latent heat. During such a transition, a system either absorbs or releases a fixed (and typically large) amount of energy per volume. During this process, the temperature of the system will stay constant as heat is added: the system is in a \"mixed-phase regime\" in which some parts of the system have completed the transition and others have not. Familiar examples are the melting of ice or the boiling of water (the water does not instantly turn into vapor, but forms a turbulent mixture of liquid water and vapor bubbles). Imry and Wortis showed that quenched disorder can broaden a first-order transition. That is, the transformation is completed over a finite range of temperatures, but phenomena like supercooling and superheating survive and hysteresis is observed on thermal cycling.\n\nSecond-order phase transitions are also called \"continuous phase transitions\". They are characterized by a divergent susceptibility, an infinite correlation length, and a power-law decay of correlations near criticality. Examples of second-order phase transitions are the ferromagnetic transition, superconducting transition (for a Type-I superconductor the phase transition is second-order at zero external field and for a Type-II superconductor the phase transition is second-order for both normal-state—mixed-state and mixed-state—superconducting-state transitions) and the superfluid transition. In contrast to viscosity, thermal expansion and heat capacity of amorphous materials show a relatively sudden change at the glass transition temperature which enables accurate detection using differential scanning calorimetry measurements. Lev Landau gave a phenomenological theory of second-order phase transitions.\n\nApart from isolated, simple phase transitions, there exist transition lines as well as multicritical points, when varying external parameters like the magnetic field or composition.\n\nSeveral transitions are known as \"infinite-order phase transitions\".\nThey are continuous but break no symmetries. The most famous example is the Kosterlitz–Thouless transition in the two-dimensional XY model. Many quantum phase transitions, e.g., in two-dimensional electron gases, belong to this class.\n\nThe liquid–glass transition is observed in many polymers and other liquids that can be supercooled far below the melting point of the crystalline phase. This is atypical in several respects. It is not a transition between thermodynamic ground states: it is widely believed that the true ground state is always crystalline. Glass is a \"quenched disorder\" state, and its entropy, density, and so on, depend on the thermal history. Therefore, the glass transition is primarily a dynamic phenomenon: on cooling a liquid, internal degrees of freedom successively fall out of equilibrium. Some theoretical methods predict an underlying phase transition in the hypothetical limit of infinitely long relaxation times. No direct experimental evidence supports the existence of these transitions.\n\nA disorder-broadened first-order transition occurs over a finite range of temperatures where the fraction of the low-temperature equilibrium phase grows from zero to one (100%) as the temperature is lowered. This continuous variation of the coexisting fractions with temperature raised interesting possibilities. On cooling, some liquids vitrify into a glass rather than transform to the equilibrium crystal phase. This happens if the cooling rate is faster than a critical cooling rate, and is attributed to the molecular motions becoming so slow that the molecules cannot rearrange into the crystal positions. This slowing down happens below a glass-formation temperature Tg, which may depend on the applied pressure. If the first-order freezing transition occurs over a range of temperatures, and Tg falls within this range, then there is an interesting possibility that the transition is arrested when it is partial and incomplete. Extending these ideas to first-order magnetic transitions being arrested at low temperatures, resulted in the observation of incomplete magnetic transitions, with two magnetic phases coexisting, down to the lowest temperature. First reported in the case of a ferromagnetic to anti-ferromagnetic transition, such persistent phase coexistence has now been reported across a variety of first-order magnetic transitions. These include colossal-magnetoresistance manganite materials, magnetocaloric materials, magnetic shape memory materials, and other materials.\nThe interesting feature of these observations of Tg falling within the temperature range over which the transition occurs is that the first-order magnetic transition is influenced by magnetic field, just like the structural transition is influenced by pressure. The relative ease with which magnetic fields can be controlled, in contrast to pressure, raises the possibility that one can study the interplay between Tg and Tc in an exhaustive way. Phase coexistence across first-order magnetic transitions will then enable the resolution of outstanding issues in understanding glasses.\n\nIn any system containing liquid and gaseous phases, there exists a special combination of pressure and temperature, known as the critical point, at which the transition between liquid and gas becomes a second-order transition. Near the critical point, the fluid is sufficiently hot and compressed that the distinction between the liquid and gaseous phases is almost non-existent. This is associated with the phenomenon of critical opalescence, a milky appearance of the liquid due to density fluctuations at all possible wavelengths (including those of visible light).\n\nPhase transitions often involve a symmetry breaking process. For instance, the cooling of a fluid into a crystalline solid breaks continuous translation symmetry: each point in the fluid has the same properties, but each point in a crystal does not have the same properties (unless the points are chosen from the lattice points of the crystal lattice). Typically, the high-temperature phase contains more symmetries than the low-temperature phase due to spontaneous symmetry breaking, with the exception of certain accidental symmetries (e.g. the formation of heavy virtual particles, which only occurs at low temperatures).\n\nAn order parameter is a measure of the degree of order across the boundaries in a phase transition system; it normally ranges between zero in one phase (usually above the critical point) and nonzero in the other. At the critical point, the order parameter susceptibility will usually diverge.\n\nAn example of an order parameter is the net magnetization in a ferromagnetic system undergoing a phase transition. For liquid/gas transitions, the order parameter is the difference of the densities.\n\nFrom a theoretical perspective, order parameters arise from symmetry breaking. When this happens, one needs to introduce one or more extra variables to describe the state of the system. For example, in the ferromagnetic phase, one must provide the net magnetization, whose direction was spontaneously chosen when the system cooled below the Curie point. However, note that order parameters can also be defined for non-symmetry-breaking transitions. Some phase transitions, such as superconducting and ferromagnetic, can have order parameters for more than one degree of freedom. In such phases, the order parameter may take the form of a complex number, a vector, or even a tensor, the magnitude of which goes to zero at the phase transition.\n\nThere also exist dual descriptions of phase transitions in terms of disorder parameters. These indicate the presence of line-like excitations such as vortex- or defect lines.\n\nSymmetry-breaking phase transitions play an important role in cosmology. It has been speculated by Lee Smolin and Jeremy Bernstein that, in the hot early universe, the vacuum (i.e. the various quantum fields that fill space) possessed a large number of symmetries. As the universe expanded and cooled, the vacuum underwent a series of symmetry-breaking phase transitions. For example, the electroweak transition broke the SU(2)×U(1) symmetry of the electroweak field into the U(1) symmetry of the present-day electromagnetic field. This transition is important to understanding the asymmetry between the amount of matter and antimatter in the present-day universe (see electroweak baryogenesis.)\n\nProgressive phase transitions in an expanding universe are implicated in the development of order in the universe, as is illustrated by the work of Eric Chaisson and David Layzer. See also Relational order theories.\n\nContinuous phase transitions are easier to study than first-order transitions due to the absence of latent heat, and they have been discovered to have many interesting properties. The phenomena associated with continuous phase transitions are called critical phenomena, due to their association with critical points.\n\nIt turns out that continuous phase transitions can be characterized by parameters known as critical exponents. The most important one is perhaps the exponent describing the divergence of the thermal correlation length by approaching the transition. For instance, let us examine the behavior of the heat capacity near such a transition. We vary the temperature of the system while keeping all the other thermodynamic variables fixed, and find that the transition occurs at some critical temperature \"T\" . When is near \"T\" , the heat capacity typically has a power law behavior,\n\nThe heat capacity of amorphous materials has such a behaviour near the glass transition temperature where the universal critical exponent α = 0.59 A similar behavior, but with the exponent instead of , applies for the correlation length.\n\nThe exponent is positive. This is different with . Its actual value depends on the type of phase transition we are considering.\n\nIt is widely believed that the critical exponents are the same above and below the critical temperature. It has now been shown that this is not necessarily true: When a continuous symmetry is explicitly broken down to a discrete symmetry by irrelevant (in the renormalization group sense) anisotropies, then some exponents (such as formula_2, the exponent of the susceptibility) are not identical.\n\nFor −1 < α < 0, the heat capacity has a \"kink\" at the transition temperature. This is the behavior of liquid helium at the lambda transition from a normal state to the superfluid state, for which experiments have found = -0.013±0.003.\nAt least one experiment was performed in the zero-gravity conditions of an orbiting satellite to minimize pressure differences in the sample. This experimental value of α agrees with theoretical predictions based on variational perturbation theory.\n\nFor 0 < < 1, the heat capacity diverges at the transition temperature (though, since < 1, the enthalpy stays finite). An example of such behavior is the 3D ferromagnetic phase transition. In the three-dimensional Ising model for uniaxial magnets, detailed theoretical studies have yielded the exponent ∼ +0.110.\n\nSome model systems do not obey a power-law behavior. For example, mean field theory predicts a finite discontinuity of the heat capacity at the transition temperature, and the two-dimensional Ising model has a logarithmic divergence. However, these systems are limiting cases and an exception to the rule. Real phase transitions exhibit power-law behavior.\n\nSeveral other critical exponents, , and , are defined, examining the power law behavior of a measurable physical quantity near the phase transition. Exponents are related by scaling relations, such as\nIt can be shown that there are only two independent exponents, e.g. and .\n\nIt is a remarkable fact that phase transitions arising in different systems often possess the same set of critical exponents. This phenomenon is known as \"universality\". For example, the critical exponents at the liquid–gas critical point have been found to be independent of the chemical composition of the fluid.\n\nMore impressively, but understandably from above, they are an exact match for the critical exponents of the ferromagnetic phase transition in uniaxial magnets. Such systems are said to be in the same universality class. Universality is a prediction of the renormalization group theory of phase transitions, which states that the thermodynamic properties of a system near a phase transition depend only on a small number of features, such as dimensionality and symmetry, and are insensitive to the underlying microscopic properties of the system. Again, the divergence of the correlation length is the essential point.\n\nThere are also other critical phenomena; e.g., besides \"static functions\" there is also \"critical dynamics\". As a consequence, at a phase transition one may observe critical slowing down or \"speeding up\". The large \"static universality classes\" of a continuous phase transition split into smaller \"dynamic universality\" classes. In addition to the critical exponents, there are also universal relations for certain static or dynamic functions of the magnetic fields and temperature differences from the critical value.\n\nAnother phenomenon which shows phase transitions and critical exponents is percolation. The simplest example is perhaps percolation in a two dimensional square lattice. Sites are randomly occupied with probability p. For small values of p the occupied sites form only small clusters. At a certain threshold p a giant cluster is formed and we have a second-order phase transition. The behavior of P near p is, P~(p-p), where β is a critical exponent.\n\nPhase transitions play many important roles in biological systems. Examples include the lipid bilayer formation, the coil-globule transition in the process of protein folding and DNA melting, liquid crystal-like transitions in the process of DNA condensation, and cooperative ligand binding to DNA and proteins with the character of phase transition.\n\nIn \"biological membranes\", gel to liquid crystalline phase transitions play a critical role in physiological functioning of biomembranes. In gel phase, due to low fluidity of membrane lipid fatty-acyl chains, membrane proteins have restricted movement and thus are restrained in exercise of their physiological role. Plants depend critically on photosynthesis by chloroplast thylakoid membranes which are exposed cold environmental temperatures. Thylakoid membranes retain innate fluidity even at relatively low temperatures because of high degree of fatty-acyl disorder allowed by their high content of linolenic acid, 18-carbon chain with 3-double bonds. Gel-to-liquid crystalline phase transition temperature of biological membranes can be determined by many techniques including calorimetry, flouorescence, spin label electron paramagnetic resonance and NMR by recording measurements of the concerned parameter by at series of sample temperatures. A simple method for its determination from 13-C NMR line intensities has also been proposed.\n\nIt has been proposed that some biological systems might lie near critical points. Examples include neural networks in the salamander retina, bird flocks\ngene expression networks in Drosophila, and protein folding. However, it is not clear whether or not alternative reasons could explain some of the phenomena supporting arguments for criticality. It has also been suggested that biological organisms share two key properties of phase transitions: the change of macroscopic behavior and the coherence of a system at a critical point.\n\nIn groups of organisms in stress (when approaching critical transitions), correlations tend to increase, while at the same time, fluctuations also increase. This effect is supported by many experiments and observations of groups of people, mice, trees, and grassy plants.\n\n\n\n"}
{"id": "17094147", "url": "https://en.wikipedia.org/wiki?curid=17094147", "title": "Phosphoroscope", "text": "Phosphoroscope\n\nA phosphoroscope is piece of experimental equipment devised in 1857 by physicist A. E. Becquerel to measure how long it takes a phosphorescent material to stop glowing after it has been excited.\n\nIt consists of two rotating disks with holes in them. The holes are arranged on each disk at equal angular intervals and a constant distance from the centre, but the holes in one disk do not align with the holes in the other. A sample of phosphorescent material is placed in between the two disks. Light coming in through a hole in one of the discs excites the phosphorescent material which then emits light for a short amount of time. The disks are then rotated and by changing their speed, the length of time the material glows can be determined.\n\n"}
{"id": "216191", "url": "https://en.wikipedia.org/wiki?curid=216191", "title": "Post-consumer waste", "text": "Post-consumer waste\n\nPost-consumer waste is a waste type produced by the end consumer of a material stream; that is, where the waste-producing use did not involve the production of another product.\n\nThe terms of pre-consumer and post-consumer recycled materials are defined in the ISO standard number 14021 (1999). These definitions are the most widely recognized and verified definitions as used by manufacturers and procurement officers worldwide. \n\nQuite commonly, it is simply the waste that individuals routinely discard, either in a waste receptacle or a dump, or by littering, incinerating, pouring down the drain, or washing into the gutter.\n\nPost-consumer waste is distinguished from pre-consumer waste, which is the reintroduction of manufacturing scrap (such as trimmings from paper production, defective aluminum cans, etc.) back into the manufacturing process. Pre-consumer waste is commonly used in manufacturing industries, and is often not considered recycling in the traditional sense.\n\nPost-consumer waste consists of:\n\nIn many countries, such as the United States, there is no reasonable expectation of privacy in post-consumer waste once it leaves the consumer's home. Anyone can search it, including the police, and any incriminating evidence recovered can be used at trial. This doctrine was established in The \"California v. Greenwood\" case, in which the U.S. Supreme Court held that there is no common law expectation of privacy for discarded materials. This has since led people to argue the legality of taking post-consumer waste for salvage value.\n\n"}
{"id": "35558321", "url": "https://en.wikipedia.org/wiki?curid=35558321", "title": "Reckahn Solar Park", "text": "Reckahn Solar Park\n\nReckahn Solar Park is a photovoltaic power station in Reckahn, Southwest of Berlin, Germany. It has a capacity of 37.7 megawatt (MW) and was constructed in three phases. \"Reckahn I\" was 22.661 MW covering and was built by Beck Energy GmbH (Belectric) using 292,000 First Solar thin-film CdTe-panels, and was expected to produce about 22 gigawatt-hours per year. \"Reckahn II\" added 13.3 MW, using 172,000 modules on a site. \"Reckahn III\", completed in 2011, added 1.8 MW, bringing the total to 37.7 MW. The FIT is 21.1 Euro cents per kilowatt-hour.\n\n"}
{"id": "7321828", "url": "https://en.wikipedia.org/wiki?curid=7321828", "title": "Ryan &amp; Company Lumber Yard", "text": "Ryan &amp; Company Lumber Yard\n\nThe Ryan & Company Lumber Yard (also known as Ryan Bros., Inc.) is a historic site in Apopka, Florida. It is located at 215 East Fifth Street. On February 25, 1993, it was added to the U.S. National Register of Historic Places.\n\n"}
{"id": "9673217", "url": "https://en.wikipedia.org/wiki?curid=9673217", "title": "Sakurai's Object", "text": "Sakurai's Object\n\nSakurai's Object (V4334 Sagittarii) is a star in the constellation of Sagittarius. It is thought to have previously been a white dwarf that, as a result of a very late thermal pulse, swelled and became a red giant. It is located at the center of a planetary nebula and is believed to currently be in thermal instability and within its final shell helium flash phase.\n\nAt the time of its discovery, astronomers believed Sakurai's Object to be a slow nova. Later spectroscopic analysis suggested that the star was not a nova, but had instead undergone a very late thermal pulse similar to that of V605 Aquilae, causing it to vastly expand. V605 Aquilae, which was discovered in 1919, is the only other star known to have been observed during the high luminosity phase of a very late thermal pulse, and models predict that Sakurai's Object, over the next few decades, will follow a similar life cycle.\n\nSakurai's Object and other similar stars are expected to end up as helium-rich white dwarfs after retracing their evolution track from the \"born-again\" giant phase back to the white dwarf cooling track. There are few other suspected \"born-again\" objects, one example being FG Sagittae. Having erupted in 1995, it is expected that Sakurai's Object's final helium flash will be the first well-observed one.\n\nAn International Astronomical Union Circular sent on February 23, 1996 announced the discovery of a \"possible 'slow' nova\" of magnitude 11.4 by Yukio Sakurai, an amateur astronomer. Japanese astronomer Syuichi Nakano reported the discovery, drawing attention to the fact that the object had not been visible in images from 1993 nor in Harvard–Smithsonian Center for Astrophysics records for the years 1930-1951, despite it appearing to slowly brighten over the previous years. Nakano wrote that \"While the outburst [suggests] a slow or symbiotic nova, the lack of obvious emission lines one year after brightening is very unusual.\"\n\nFollowing the initial announcement, Hilmar Duerbeck published a study investigating the \"possible final helium flash\" seen by Sakurai. In it, they noted that the location of Sakurai's Object corresponded to a faint object detected in 1976 of magnitude 21, and discussed other observations in the years 1994–1996, by which time the magnitude had increased to around 11–15. By investigating the measured fluxes, angular diameter, and mass of the nebula, a distance of 5.5 kpc and luminosity of was determined. The researchers noted that this was in agreement with their appearance and model predictions and that the outburst luminosity was in the area of 3100 solar luminosities; lower than predicted by a factor of 3.\n\nThe first infrared observations were published in 1998, in which both near and far infrared spectroscopy data was presented. The collected data showed Sakurai's Object's steep brightening in 1996, followed by a sharp decline in 1999 as expected. It was later found that the star's steep decline in light was due to the circumstellar dust located around the star, which was present at a temperature of ~680 K. Further infrared data recorded by the United Kingdom Infrared Telescope was published in 2000, in which findings of the changing absorption lines were discussed.\n\nObservations from the United Kingdom Infrared Telescope (UKIRT) in 1999 revealed that the star is in a \"RCB-like\" phase with the release of dust and huge loss of mass.\n\nSince 2005, it has been observed in the ejected particles of Sakurai's Object that photoionization of carbon is taking place.\n\nSakurai's Object is a highly evolved post-asymptotic giant branch star which has, following a brief period on the white dwarf cooling track, undergone a helium shell flash (also known as a very late thermal pulse). The star is thought to have a mass of around . Observations of Sakurai's Object show increasing reddening and pulsing activity, suggesting that the star is exhibiting thermal instability during its final helium-shell flash.\n\nPrior to its reignition V4334 Sgr is thought to have been cooling towards a white dwarf with a temperature around 100,000 K and a luminosity around . The luminosity rapidly increased about a hundred-fold and then the temperature decreased to around 10,000 K. The star developed the appearance of an F class supergiant (F2 Ia). The apparent temperature continued to cool to below 6,000 K and the star was gradually obscured at optical wavelengths by the formation of carbon dust, similar to an R CrB star. Since then the temperature has increased to around 20,000 K.\n\nThe properties of Sakurai's Object are quite similar to that of V605 Aquilae. V605, discovered in 1919, is the only other known star observed during the high luminosity phase of a very late thermal pulse, and Sakurai's Object is modeled to increase in temperature in the next few decades to match the current state of V605.\n\nDuring the second half of 1998 an optically thick dust shell obscured Sakurai's Object, causing a rapid decrease in visibility of the star, until in 1999 it disappeared from optical wavelength observations altogether. Infrared observations showed that the dust cloud around the star is primarily carbon in an amorphous form. In 2009 it was discovered that the dust shell is strongly asymmetrical, as a disc with a major axis oriented at an angle of 134°, and inclination of around 75°. The disc is thought to be growing more opaque due to the fast spectral evolution of the source towards lower temperatures.\n\nSakurai's Object is surrounded by a planetary nebula created following the star's red giant phase around 8300 years ago. It has been determined that the nebula has a diameter of 44 arcseconds and expansion velocity of roughly 32 km/s.\n\nResearch in 1996 revealed that Sakurai's Object possessed the characteristics of a R Coronae Borealis variable star with the anomaly of Carbon-13 (C) deficit. Also, the metallicity of Sakurai's object in 1996 was similar to that of V605 Aquilae in 1921. However, it is expected that Sakurai's object will grow in its metallicity to match that of V605 Aquilae.\n\nA significant amount of new star formation and star destruction data is expected to be recorded from continued observation of Sakurai's Object, as well as be used as reference data in the future research of similar stars. The reason that stars such as Sakurai's Object and V605 Aquilae exist, as well as experience a shorter lifespan compared to most stars, is largely unknown. Sakurai's Object and V605 Aquilae have been observed experiencing born-again behavior for only 10 years, while FG Sagittae has undergone such behavior for 120 years. It is hypothesized that this is due to Sakurai's Object and V605 Aquilae evolving to the asymptotic giant branch of stars for the first time, while FG Sagittae is undergoing the process a second time.\n\n\n"}
{"id": "1163678", "url": "https://en.wikipedia.org/wiki?curid=1163678", "title": "Sesbania", "text": "Sesbania\n\nSesbania is a genus of flowering plants in the pea family, Fabaceae, and the only genus found in tribe Sesbanieae. Riverhemp is a common name for plants in this genus. Notable species include the rattlebox (\"Sesbania punicea\"), spiny sesbania (\"Sesbania bispinosa\"), and \"Sesbania sesban\", which is used in cooking. Plants of this genus, some of which are aquatic, can be used in alley cropping to increase the soil's nitrogen content. The species of rhizobia responsible for nitrogen fixation in \"Sesbania rostrata\" is \"Azorhizobium caulinodans\".\n\nSome 60 species are currently accepted, with about 39 still unresolved. The largest number of species are found in Africa, and the remainder in Australia, Hawaii, and Asia.\n\nFossil seed pods from upper Oligocene resembling \"Sesbania\" have been found in the Hungarian locality of Eger Wind-brickyard. The fossil species grew in a swampy and riparian environment.\n\n\n\n\n\n"}
{"id": "25282247", "url": "https://en.wikipedia.org/wiki?curid=25282247", "title": "Solar flower tower", "text": "Solar flower tower\n\nAORA's Solar Flower Tower is a hybrid power generator that utilizes solar and alternative fuels, including diesel fuel, natural gas, liquefied natural gas, biogas, and other biofuels, to provide a constant green power source targeted for community-sized production. A module, dubbed the Solar Flower Tower because it looks like a golden yellow tulip, creates about 100 kW of electricity. The basis of the design is to use solar heated compressed air to spin a micro turbine. What makes the micro turbine unique is the efficiency of smaller power blocks, which allows small-scale construction, meaning simpler operation and less land needed.\n\nFormerly known as EDIG Solar, AORA is an Israeli based company that develops solar-hybrid power generators. The EDIG group of companies has contracted engineering project with organizations such as Ministry of Defense (Israel), El Al Israel Airlines, and the National Health Service Provider. EDIG presently has a relationship with the Weizmann Institute of Science, providing engineering services. It was at the Weizmann Institute of Science where the solar thermal technology was developed. After the technology was licensed to EDIG, continued advancement was developed until they decided to turned their new solar division into a sub-company, EDIG Solar, now known as AROA.\n\nIn half an acre of land (about 40% of a football field), a solar tower module is surrounded by thirty heliostats reflecting the sun ray into a special solar receiver inside the module. The receiver heats the turbine's compressed air to about 1,000°C, and the heated air is sent into the turbine's expander to create electricity.\n\n\n"}
{"id": "2396049", "url": "https://en.wikipedia.org/wiki?curid=2396049", "title": "Solarroller", "text": "Solarroller\n\nSolarroller is a BEAM dragster photovore robot run by solar panel that utlilizes sunlight. In competitions between solarrollers, each one must run one meter in the shortest time possible. Components include pager motors,\ncapacitors, resistors, transistors, and solar panels.\n\nThere are several different kinds of configurations of solarrollers, with bigger or smaller wheels, one or two motors. Configurations differences include: \n\nThis robot type always moves forwards. The motor drives one or more wheels. A \"Solar Engine\" circuit is used to feed the robot.\nSolarroller's speed is directly related to the amount of light robot registers on its optical sensor. Most are driven by an electronic \"relaxation oscillator\", in which a charge is accumulated in a capacitor while at rest and then suddenly released in the drive mechanism. \n\n\n\n"}
{"id": "23582129", "url": "https://en.wikipedia.org/wiki?curid=23582129", "title": "Toyota i-REAL", "text": "Toyota i-REAL\n\nThe i-REAL is a 'Personal Mobility Concept' made by automotive giant Toyota that was planned to be put on sale sometime around 2010. It is a development of previous Toyota Personal Mobility vehicles including the i-unit and i-Swing. As with said previous vehicles, the i-REAL is a 3-wheeled electrically powered one-passenger vehicle, running on lithium-ion batteries.\n\nIn Low-Speed Mode, the vehicle is upright, and moves around at 'walking pace' at similar eyesight height to pedestrians, without taking up a large amount of space. In High-Speed Mode, the Toyota extends in length by leaning back and extending the single rear wheel to improve aerodynamics and stability, thus being able to achieve a speed of 18.6 mph, or 30 km/h. It leans into corners, like other tall, one-man vehicles such as the Segway, to prevent it from tipping over.\n\nThere are two joysticks, one for each hand. Either joystick controls the i-Real, so left- and right-handed people will be equally at home. You push the joystick forwards to go forwards, left to go left, right to go right and pull back to stop. Perimeter-monitoring sensors detect when a collision with a person or object is imminent and alerts the driver by emitting a noise and vibrating. At the same time, it alerts people around it of its movements through use of light and sound.\n\nThe i-REAL was driven on the BBC's motoring programme Top Gear in 2008 by Richard Hammond. (Series 12)\n\n"}
{"id": "43697903", "url": "https://en.wikipedia.org/wiki?curid=43697903", "title": "Waelz process", "text": "Waelz process\n\nThe Waelz process is a method of recovering zinc and other relatively low boiling point metals from metallurgical waste (typically EAF flue dust) and other recycled materials using a rotary kiln (waelz kiln).\n\nThe zinc enriched product is referred to as waelz oxide, and the reduced zinc by product as waelz slag.\n\nThe concept of using a rotary kiln for the recovery of Zinc by volatization dates to at least 1888. A process was patented by Edward Dedolph in 1910. Subsequently, the Dedpolph patent was taken up and developed by Metallgesellschaft (Frankfurt) with Chemische Fabrik Griesheim-Elektron but without leading to a production scale ready process. In 1923 the Krupp Grusonwerk independently developed a process (1923), named the \"Waelz process\" (from the German \"Waelzen\", a reference to the motion of the materials in the kiln); the two German firms later collaborated and improved the process marketing under the name \"Waelz-Gemeinschaft\" (German for Waelz association).\n\nThe process consists of treating zinc containing material, in which zinc can be in the form zinc oxide, zinc silicate, zinc ferrite, zinc sulphide together with a carbon containing reductant/fuel, within a rotary kiln at 1000 °C to 1500 °C. The kiln feed material comprising zinc 'waste', fluxes and reductant (coke) is typically pelletized before addition to the kiln. The chemical process involves the reduction of zinc compounds to elemental zinc (boiling point 907 °C) which volatalises, which oxidises in the vapour phase to zinc oxide. The zinc oxide is collected from the kiln outlet exhaust by filters/electrostatic precipitators/settling chambers etc.\n\nKiln size is typically long / internal diameter, with a rotation speed of around 1 rpm. The recovered dust (\"Waelz oxide\") is enriched in zinc oxide and is a feed product for zinc smelters, the zinc reduced by-product is known as \"Waelz slag\". Sub-optimal features of the process are high energy consumption, and lack of iron recovery (and iron rich slag). The process also captures other low boiling metals in the \"waelz oxide\" including lead, cadmium and silver. Halogen compounds are also present in the product oxide.\n\nIncreased use of galvanised steel has resulted in increased levels of zinc in steel scrap which in turn leads to higher levels of zinc in electric arc furnace flue dusts - as of 2000 the waelz process is considered to be a \"best available technology\" for flue dust zinc recovery, and the process is used at industrial scale worldwide.\n\nAs of 2014 the Waelz process is the preferred or most widely used process for zinc recovery of zinc from electric arc furnace dust (90%).\n\nAlternative production and experimental scale zinc recovery processes include the rotary hearth treatment of pelletised zinc containing dust (Kimitsu works, Nippon Steel); the SDHL (Saage, Dittrich, Hasche, Langbein) process, an efficiency modification of the Waelz process; the \"DK process\" a modified blast furnace process producing pig iron and zinc (oxide) dust from blast furnace dusts, sludges and other wastes; and the PRIMUS process (multi-stage zinc volatilisation furnace).\n\n"}
{"id": "23209317", "url": "https://en.wikipedia.org/wiki?curid=23209317", "title": "Warm core ring", "text": "Warm core ring\n\nA warm core ring is a type of mesoscale eddy which breaks off from an ocean current, e.g. the Gulf Stream or the Kuroshio Current. The ring is an independent circulatory system of warm water which can persist for several months. The rest of this article will use the Gulf Stream by way of an example but these mesoscale eddies also form in most powerful ocean currents, such as the Kuroshio or Agulhas currents.\n\nSuch rings can be detected using infrared satellites or sea height anomalies and are easily identifiable against the surrounding colder waters. These systems will drift west from their origin at the Gulf Stream until they break apart on the coastal shelf or are reabsorbed by the Gulf Stream.\n\nThis type of system is theorized to have helped develop several hurricanes, most notably Hurricane Katrina, into significantly stronger storms due to the abundance of warmer ocean water reaching down to a significant depth. In addition, these eddies can damage offshore drilling equipment due to their currents.\n\nWarm core rings are also known for affecting wildlife, bringing warm-water creatures to unusual areas.\n\nAs the Gulf Stream flows and moves over time, it can develop large loops which can eventually be pinched off from the current, forming an independent eddy of warmer Sargasso Sea water circulating clockwise similar to the direction of the Gulf Stream. While many warm core rings are created in the Gulf of Mexico, they can develop anywhere along the eastern coast of the United States north of the Gulf Stream. Rings are anywhere from 100–200 km across and can include warmer waters as deep as 1500 meters. These rings develop every 6–11 months on average. However, rings in the Gulf of Mexico can last much longer and are formed at much more irregular intervals than along the eastern U.S. coast. In an active year, up to 15 warm core rings can form due to the Gulf Stream.\n\nRings will drift to the west-southwest at 3–5 km/day for several months up to a year. The rings always rotate clockwise due to the direction of the Gulf Stream and can reach rotational velocities of up to 1 m/s. Usually warm core rings cannot move onto the continental shelf because they reach deeper than the seafloor on the shelf by over 1000 meters, though they can near the shelf.\n\nWarm core rings are often reabsorbed by the Gulf Stream, but they can break apart on their own as well if they move onto the continental shelf.\n\nWarm core rings are easily observed in the Gulf of Mexico or elsewhere through the use of infrared imagery by weather satellites. Since the ocean water temperature of the ring is significantly higher than the surrounding waters, these rings show up easily in infrared images. This, coupled with models of ring movement, allow well-developed tracking of the rings. Because warm core rings include warm water to a significant depth, infrared satellites can differentiate the temperature, unlike cold core rings, which cannot be easily detected.\n\nWarm core rings are also detected by sea surface height anomalies. Since warm water takes up more space as it expands than cold water, the large amount of warm water causes an upwelling in sea height which can be detected by buoys.\n\nWarm core rings have been linked to the intensification of several hurricanes passing over their location. Because high sea surface temperature as well as warmer water at greater depth is the primary intensifier of a hurricane, warm core rings account for tremendous strengthening of these storms.\n\nNotably, Hurricane Opal passed over a ring and had sudden increases of wind speed from 110 miles per hour to 135 miles per hour shortly before landfall, a trend also seen in Hurricane Allen and Hurricane Camille. There is evidence that Hurricane Katrina and Hurricane Rita, both notable storms which reached Category 5 intensity, as well as Hurricane Ivan, were also strengthened by warm core rings.\n\nWarm core rings typically include far less biological specimens than the surrounding ocean. When the rings approach continental shelves, coastal currents are affected, which can cause organisms to drift onto the shelf that ordinarily would not be there. In fact, there are human accounts of sea turtles and tropical fish which normally live in much warmer waters coming near the coastal shelf due to the deep, warm waters of a warm core ring.\n\nDue to currents around warm core rings of up to nearly 5 miles per hour, this phenomenon can damage offshore oil platforms and increase the risk of accidents.\n\nMany fish species’ life cycle involves two distinct habitats. The adults live in warmer temperate waters south of Cape Hatteras, NC while the juveniles are found in estuaries of cooler waters north of Cape Hatteras. Warm Core Rings play an important role in the transport of larvae between the two habitats. Species like the bluefish (Pomatomus saltatrix) and pearly razorfish (Xyrichtys novacula) spawn near the western edge of the Gulf Stream just south of Cape Hatteras. Because of the convergence of the Gulf Stream from the south and cooler coastal water current from the north, most water around Cape Hatteras flows into the Gulf Stream. The larvae released near this convergence is swept into the Gulf Stream and flows north. Since the larvae are planktonic, they don’t swim into the center of the Gulf Stream but stay near the western edge. This is beneficial for when warm core rings form. Warm core rings are formed when the crest of a meander breaks off from the Gulf Stream. Any larvae in the crest of the meanders are then entrapped in the warm core ring. Once the warm core ring breaks way, it takes a southwesternly path towards the coast. The interaction between warm core rings and the continental shelf creates a weakening of the ring and enables the larvae to escape and continue their journey to nearby estuaries. The warm core rings formed along the northeastern states can last between 4 and 5 months. During this time the larvae grow so that by the time they reach the estuaries, they are able to swim away from the warm core ring into the estuaries.\n\n"}
{"id": "4148957", "url": "https://en.wikipedia.org/wiki?curid=4148957", "title": "Weapons-grade nuclear material", "text": "Weapons-grade nuclear material\n\nWeapons-grade nuclear material is any fissionable nuclear material that is pure enough to make a nuclear weapon or has properties that make it particularly suitable for nuclear weapons use. Plutonium and uranium in grades normally used in nuclear weapons are the most common examples. (These nuclear materials have other categorizations based on their purity.)\n\nOnly fissile isotopes of certain elements have the potential for use in nuclear weapons. For such use, the concentration of fissile isotopes uranium-235 and plutonium-239 in the element used must be sufficiently high. Uranium from natural sources is enriched by isotope separation, and plutonium is produced in a suitable nuclear reactor.\n\nExperiments have been conducted with uranium-233. Neptunium-237 and some isotopes of americium might be usable, but it is not clear that this has ever been implemented.\n\nTen countries have produced weapons-grade nuclear material:\n\nNatural uranium is made weapons-grade through isotopic enrichment. Initially only about 0.7% of it is fissile U-235, with the rest being almost entirely uranium-238 (U-238). They are separated by their differing masses. Highly enriched uranium is considered weapons-grade when it has been enriched to about 90% U-235.\n\nU-233 is produced from thorium-232 by neutron capture. The U-233 produced thus does not require enrichment and can be relatively easily chemically separated from residual Th-232. It is therefore regulated as a special nuclear material only by the total amount present. U-233 may be intentionally down-blended with U-238 to remove proliferation concerns.\n\nWhile U-233 would thus seem ideal for weaponization, a significant obstacle to that goal is the co-production of trace amounts of uranium-232 due to side-reactions. U-232 hazards, a result of its highly radioactive decay products such as thallium-208, are significant even at 5 parts per million. Implosion nuclear weapons require U-232 levels below 50 PPM (above which the U-233 is considered \"low grade\"; cf. \"Standard weapon grade plutonium requires a Pu-240 content of no more than 6.5%.\" which is 65,000 PPM, and the analogous Pu-238 was produced in levels of 0.5% (5000 PPM) or less). Gun-type fission weapons would require low U-232 levels and low levels of light impurities on the order of 1 PPM.\n\nPu-239 is produced artificially in nuclear reactors when a neutron is absorbed by U-238, forming U-239, which then decays in a rapid two-step process into Pu-239. It can then be separated from the uranium in a nuclear reprocessing plant.\n\nWeapons-grade plutonium is defined as being predominantly Pu-239, typically about 93% Pu-239. Pu-240 is produced when Pu-239 absorbs an additional neutron and fails to fission. Pu-240 and Pu-239 are not separated by reprocessing. Pu-240 has a high rate of spontaneous fission, which can cause a nuclear weapon to pre-detonate. This makes plutonium unsuitable for use in gun-type nuclear weapons. To reduce the concentration of Pu-240 in the plutonium produced, weapons program plutonium production reactors (e.g. B Reactor) irradiate the uranium for a far shorter time than is normal for a nuclear power reactor. More precisely, weapons-grade plutonium is obtained from uranium irradiated to a low burnup.\n\nThis represents a fundamental difference between these two types of reactor. In a nuclear power station, high burnup is desirable. Power stations such as the obsolete British Magnox and French UNGG reactors, which were designed to produce either electricity or weapons material, were operated at low power levels with frequent fuel changes using online refuelling to produce weapons-grade plutonium. Such operation is not possible with the light water reactors most commonly used to produce electric power. In these the reactor must be shut down and the pressure vessel disassembled to gain access to the irradiated fuel.\n\nPlutonium recovered from LWR spent fuel, while not weapons grade, can be used to produce nuclear weapons at all levels of sophistication, though in simple designs it may produce only a fizzle yield. Weapons made with reactor-grade plutonium would require special cooling to keep them in storage and ready for use. A 1962 test at the U.S. Nevada National Security Site (then known as the Nevada Proving Grounds) used non-weapons-grade plutonium produced in a Magnox reactor in the United Kingdom. The plutonium used was provided to the United States under the 1958 US-UK Mutual Defence Agreement. Its isotopic composition has not been disclosed, other than the description \"reactor grade\" and it has not been disclosed which definition was used in describing the material this way. The plutonium was apparently sourced from the military Magnox reactors at Calder Hall or Chapelcross. The content of Pu-239 in material used for the 1962 test was not disclosed, but has been inferred to have been at least 85%, much higher than typical spent fuel from currently operating reactors.\n\nOccasionally, low-burnup spent fuel has been produced by a commercial LWR when an incident such as a fuel cladding failure has required early refuelling. If the period of irradiation has been sufficiently short, this spent fuel could be reprocessed to produce weapons grade plutonium.\n\n"}
