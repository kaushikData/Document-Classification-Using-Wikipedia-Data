{"id": "4661656", "url": "https://en.wikipedia.org/wiki?curid=4661656", "title": "Alkali salt", "text": "Alkali salt\n\nAlkali salts or basic salts are salts that are the product of the neutralization of a strong base and a weak acid.\n\nRather than being neutral (as some other salts), alkali salts are bases as their name suggests. What makes these compounds basic is that the conjugate base from the weak acid hydrolyzes to form a basic solution. In sodium carbonate, for example, the carbonate from the carbonic acid hydrolyzes to form a basic solution. The chloride from the hydrochloric acid in sodium chloride does not hydrolyze, though, so sodium chloride is not basic.\n\nThe difference between a basic salt and an alkali is that an alkali is the soluble hydroxide compound of an alkali metal or an alkaline earth metal. A basic salt is any salt that hydrolyzes to form a basic solution. \n\nAnother definition of a basic salt would be a salt that contains amounts of both hydroxide and other anions. White lead is an example. It is basic lead carbonate, or lead carbonate hydroxide. \n\nThese materials are known for their high levels of dissolution in polar solvents.\n\nThese salts are insoluble and are obtained through precipitation reactions.\n\n\n'Alkaline salts' are often the major component of alkaline dishwasher detergent powders.\n\nThese salts may include:\n\nExamples of other strongly alkaline salts, include:\n\n"}
{"id": "50640598", "url": "https://en.wikipedia.org/wiki?curid=50640598", "title": "Atlas Cedar Biosphere Reserve", "text": "Atlas Cedar Biosphere Reserve\n\nThe Atlas Cedar Biosphere Reserve (established 2016) is a UNESCO Biosphere Reserve located in \nthe central Atlas Mountains of Morocco. The biosphere reserve is home to 75% of the world’s majestic Atlas cedar (\"Cedrus atlantica\") tree population. This part of the Atlas Mountains is rich in ecosystems and its peaks, reaching up to , provide the region with critically important water resources. Fruit plantations, modern agriculture and tourist activities, which have replaced semi-nomadic pastoral traditions, are taking their toll on scarce water resources. The rich local Berber culture is particularly strong in this area.\n"}
{"id": "44944644", "url": "https://en.wikipedia.org/wiki?curid=44944644", "title": "Bandu Dhotre", "text": "Bandu Dhotre\n\nBandu Dhotre (born 1979) is an Indian wildlife activist and president of environmental organisation Eco-Pro. For his work in rescuing wildlife, \"India Today\" has identified him as a \"local hero\".\nHe has received Nation Youth Award in year 2013-14 by the Ministry of Youth Affairs and Sports for his enormous contributions in the field of wildlife conservation and community service.\n"}
{"id": "11994459", "url": "https://en.wikipedia.org/wiki?curid=11994459", "title": "Beriev A-60", "text": "Beriev A-60\n\nThe Beriev A-60 is a Soviet/Russian airborne laser laboratory aircraft based on the Ilyushin Il-76MD transport.\n\nIn the 1970s a special aviation complex was established by the Soviets at Taganrog machine-building factory to develop airborne laser technology for the Soviet military.\n\nIn 1977 Beriev OKB started the design of a flying laboratory designated '1А'. The purpose was to solve the complex scientific and engineering problems regarding the creation of an airborne laser and also to facilitate research on the distribution of beams in the top layers of an atmosphere. Work on this topic occurred with wide cooperation between the enterprises and the scientific organizations of the USSR, but the basic partner OKB was TSKB Almaz headed by B.V.Bunkin.\n\nThe Il-76MD was selected as the base aircraft for the flying laboratory. To accommodate the laser many changes were made to the basic IL-76 design, which drastically changed the appearance of the plane.\n\n\nThe problem of accommodating the laser gun was therefore solved and it did not spoil the aerodynamics of the base aircraft. The laser system was 1 MW, created by one of the branches of the Institute of Atomic Energy, Kurchatov. This carbon dioxide laser was developed for installation on the IL-76.\n\nThe '1A' flying laboratory first flew on 19 August 1981 under E.A. Lakhmostov.\n\nOn 29 August 1991, the crew led by test pilot V.P. Demyanovski flew the second flying laboratory which received the name '1А2' СССР-86879. A new variant of a laser system was installed as a result of tests on '1А'.\n\nApparently, after being mothballed for more than 15 years, the project was recently (May 2009) reactivated, according to the eyewitness accounts about an A-60 spotted flying in the Rostov on Don and Taganrog regions. It is now parked at Taganrog airport. 47°11'53.92\"N 38°51'46.05\"E\n\nRussia has developed a military airborne laser mounted in a A-60, designated 1LK222 Sokol Eshelon. The second A-60 laboratory can be seen at this reference.\n\n\nRelated development:\n\nComparable aircraft:\n\n"}
{"id": "1663955", "url": "https://en.wikipedia.org/wiki?curid=1663955", "title": "Blackwater, New Mexico", "text": "Blackwater, New Mexico\n\nBlackwater, New Mexico is the designation of a HVDC back-to-back facility for the power exchange between the asynchronous power grids of Texas and New Mexico. It was built by Brown Boveri in 1985 and can transfer a power up to 200 megawatts. The used voltage is 57 kV.\n"}
{"id": "5169565", "url": "https://en.wikipedia.org/wiki?curid=5169565", "title": "Blue Canyon Wind Farm", "text": "Blue Canyon Wind Farm\n\nBlue Canyon Wind Farm is the largest wind farm in Oklahoma, United States. The project, located in the Slick Hills north of Lawton, consists of four phases with a total output of 423.45 MW.\n, Blue Canyon remains Oklahoma's largest wind farm; however, several organizations including Oklahoma Gas & Electric plan to greatly increase Oklahoma's wind power capacity, and future projects may be larger.\n\nBlue Canyon I consists of 45 Vestas NM72 1.65 MW wind turbines, with a collective nameplate capacity of 74.25 MW. It began commercial operations in December 2003, and is owned by EDP Renewables North America.\n\nBlue Canyon II consists of 84 Vestas V80 1.8 MW wind turbines, with a collective nameplate capacity of an additional 151.2 MW. It is owned and operated by Horizon Wind Energy, a subsidiary of Energias de Portugal, a world leading Portuguese utility, it began commercial operations in December 2005.\n\nBlue Canyon V consists of 66 GE sle 1.5 MW turbines, with a collective nameplate capacity of an additional 99 MW. It began commercial operations in December 2009.\n\nBlue Canyon VI consists of 55 Vestas V90 1.8 MW turbines, with a collective nameplate capacity of an additional 99 MW.\n\n\n"}
{"id": "1647676", "url": "https://en.wikipedia.org/wiki?curid=1647676", "title": "Bond energy", "text": "Bond energy\n\nIn chemistry, bond energy (\"E\") or bond enthalpy (\"H\") is the measure of bond strength in a chemical bond. IUPAC defines bond energy as the average value of the gas-phase bond dissociation energies (usually at a temperature of 298 K) for all bonds of the same type within the same chemical species. For example, the carbon–hydrogen bond energy in methane \"H\"(C–H) is the enthalpy change involved with breaking up one molecule of methane into a carbon atom and four hydrogen radicals, divided by 4. Tabulated bond energies are generally values of bond energies averaged over a number of selected typical chemical species containing that type of bond. Bond energy (\"E\") or bond enthalpy (\"H\") should not be confused with bond-dissociation energy. Bond energy is the average of all the bond-dissociation energies in a molecule, and will show a different value for a given bond than the bond-dissociation energy would. This is because the energy required to break a single bond in a specific molecule differs for each bond in that molecule. For example, methane has four C–H bonds and the bond-dissociation energies are 435 kJ/mol for \"D\"(CH–H), 444 kJ/mol for \"D\"(CH–H), 444 kJ/mol for \"D\"(CH–H) and 339 kJ/mol for \"D\"(C–H). Their average, and hence the bond energy, is 414 kJ/mol, even though not a single bond required specifically 414 kJ/mol to be broken.\n\nBond strength (energy) can be directly related to the bond length and bond distance. Therefore, we can use the metallic radius, ionic radius, or covalent radius of each atom in a molecule to determine the bond strength. For example, the \"covalent\" radius of boron is estimated at 83.0 pm, but the bond length of B–B in BCl is 175 pm, a significantly larger value. This would indicate that the bond between the two boron atoms is a rather \"weak\" single bond. In another example, the metallic radius of rhenium is 137.5 pm, with a Re–Re bond length of 224 pm in the compound ReCl. From this data, we can conclude that the bond is a very strong bond or a quadruple bond. This method of determination is most useful for covalently bonded compounds.\n\nThere are several contributing factors but usually the most important is the difference in the electronegativity of the two atoms bonding together.\n\n\n\n"}
{"id": "6529735", "url": "https://en.wikipedia.org/wiki?curid=6529735", "title": "Bubble (physics)", "text": "Bubble (physics)\n\nA bubble is a globule of one substance in another, usually gas in a liquid. \nDue to the Marangoni effect, bubbles may remain intact when they reach the surface of the immersive substance.\n\nBubbles are seen in many places in everyday life, for example:\n\nBubbles form, and coalesce, into globular shapes, because those shapes are at a lower energy state. For the physics and chemistry behind it, see nucleation.\n\nBubbles are visible because they have a different refractive index (RI) than the surrounding substance. For example, the RI of air is approximately 1.0003 and the RI of water is approximately 1.333. Snell's Law describes how electromagnetic waves change direction at the interface between two mediums with different IR; thus bubbles can be identified from the accompanying refraction and internal reflection even though both the immersed and immersing mediums are transparent.\n\nThe above explanation only holds for bubbles of one medium submerged in another medium (e.g. bubbles of gas in a soft drink); the volume of a membrane bubble (e.g. soap bubble) will not distort light very much, and one can only see a membrane bubble due to thin-film diffraction and reflection.\n\nNucleation can be intentionally induced, for example to create a bubblegram in a solid.\n\nIn medical ultrasound imaging, small encapsulated bubbles called contrast agent are used to enhance the contrast.\n\nIn thermal inkjet printing, vapor bubbles are used as actuators. They are occasionally used in other microfluidics applications as actuators.\n\nThe violent collapse of bubbles (cavitation) near solid surfaces and the resulting impinging jet constitute the mechanism used in ultrasonic cleaning. The same effect, but on a larger scale, is used in focused energy weapons such as the bazooka and the torpedo. Pistol shrimp also use a collapsing cavitation bubble as a weapon. The same effect is used to treat kidney stones in a lithotripter. Marine mammals such as dolphins and whales use bubbles for entertainment or as hunting tools. Aerators cause dissolution of gas in the liquid by injecting bubbles.\n\nChemical and metallurgic engineers rely on bubbles for operations such as distillation, absorption, flotation and spray drying. The complex processes involved often require consideration for mass and heat transfer, and are modelled using fluid dynamics.\n\nThe star-nosed mole and the American water shrew can smell underwater by rapidly breathing through their nostrils and creating a bubble. \n\nWhen bubbles are disturbed, they pulsate (that is, they oscillate in size) at their natural frequency. Large bubbles (negligible surface tension and thermal conductivity) undergo adiabatic pulsations, which means that no heat is transferred either from the liquid to the gas or vice versa. The natural frequency of such bubbles is determined by the equation:\n\nwhere:\n\nSmaller bubbles undergo isothermal pulsations. The corresponding equation for small bubbles of surface tension σ (and negligible liquid viscosity) is\n\nExcited bubbles trapped underwater are the major source of liquid sounds, such as inside our knuckles during knuckle cracking, and when a rain droplet impacts a surface of water. \n\nInjury by bubble formation and growth in body tissues is the mechanism of decompression sickness, which occurs when supersaturated dissolved inert gases leave solution as bubbles during decompression. The damage can be due to mechanical deformation of tissues due to bubble growth in situ, or by blocking blood vessels where the bubble has lodged.\n\nArterial gas embolism can occur when a gas bubble is introduced to the circulatory system and it lodges in a blood vessel which is too small for it to pass through under the available pressure difference. This can occur as a result of decompression after hyperbaric exposure, a lung overexpansion injury, during intravenous fluid administration, or during surgery.\n\n"}
{"id": "36133183", "url": "https://en.wikipedia.org/wiki?curid=36133183", "title": "Carbon pricing in Australia", "text": "Carbon pricing in Australia\n\nA carbon pricing scheme in Australia, commonly dubbed by its critics as a \"carbon tax\", was introduced by the Gillard Labor Government in 2011 as the \"Clean Energy Act 2011\" which came into effect on 1 July 2012. As a result of being in place for such a short time, and because the then Opposition leader Tony Abbott indicated he intended to repeal \"the carbon tax\", regulated organisations responded in a rather tepid and informal manner, with very few investments in emissions reductions being made. The scheme was repealed on 17 July 2014, backdated to 1 July 2014. In its place the Abbott Government set up the Emission Reduction Fund in December 2014.\n\nThe carbon price was part of a broad energy reform package called the Clean Energy Futures Plan, which aimed to reduce greenhouse gas emissions in Australia by 5% below 2000 levels by 2020 and 80% below 2000 levels by 2050. The plan set out to achieve these targets by encouraging Australia's largest emitters to increase energy efficiency and invest in sustainable energy. The scheme was administered by the Clean Energy Regulator. Compensation to industry and households was funded by the revenue derived from the charge. The scheme required entities which emit over 25,000 tonnes of carbon dioxide equivalent greenhouse gases per year, and which were not in the transport or agriculture sectors, to obtain emissions permits, called carbon units. Carbon units were either purchased from the government or issued free as part of industry assistance measures. As part of the scheme, personal income tax was reduced for those earning less than $80,000 per year and the tax-free threshold was increased from $6,000 to $18,200. Initially the price of a permit for one tonne of carbon was fixed at $23 for the 2012–13 financial year, with unlimited permits being available from the government. The fixed price rose to $24.15 for 2013–14. The government had announced that the scheme was part of a transition to an emissions trading scheme in 2014–15, where the available permits will be limited in line with a pollution cap. The scheme primarily applied to electricity generators and industrial sectors. It did not apply to road transport and agriculture. The Department of Climate Change and Energy Efficiency stated that in June 2013 only 260 entities were subject to the scheme, of which approximately 185 were liable to pay for carbon units. Domestic aviation did not face the carbon price scheme per se, but was subject to an additional fuel excise levy of approximately 6 cents per litre.\n\nIn February 2012, the \"Sydney Morning Herald\" reported that Clean Energy Future carbon price scheme had not deterred new investment in the coal industry, as spending on exploration had increased by 62% in 2010-2011, more than any other mineral commodity. The government agency Geoscience Australia reported that investment in coal prospecting reached $520 million in 2010-2011. Falls in carbon emissions were observed following implementation of this policy. It was noted that emissions from sectors subject to the pricing mechanism were\n1.0% lower and nine months after the introduction of the pricing scheme, Australia's carbon dioxide emissions from electricity generation had fallen to a 10-year low, with coal generation down 11% from 2008 to 2009. However, attribution of these trends to carbon pricing have been disputed, with Frontier Economics claiming trends are largely explained by factors unrelated to the carbon tax. Electricity demand had been falling and in 2012 was at the lowest level seen since 2006 in the National Electricity Market.\n\nIn October 2006 the Stern Review on the effect of climate change on the world's economy was released for the British government. This report recommended a range of measures including ecotaxes to address the market failure represented by climate change with the least amount of economic and social disruption. In response to this report and subsequent pressure from the Kim Beazley led Labor opposition, in December 2006 the Howard Government established the Prime Ministerial Task Group on Emissions Trading, chaired by Peter Shergold, to advise on the implementation of an emissions trading scheme (ETS) in Australia. In opposition, Kevin Rudd called for a cut to greenhouse gas emissions by 60% before 2050. Both the incumbent Howard Government and the Rudd Labor opposition promised to implement an emissions trading scheme (ETS) before the 2007 federal election. Following the release of the final Shergold report, the Howard government committed to introduce an ETS in June 2007.\n\nGoing into the 2007 federal election, the Labor opposition party presented itself as a \"pro-climate\" alternative to the Government, with Kevin Rudd, who had by then deposed Beazley as leader, famously describing climate change as \"the great moral challenge of our generation\". Labor differentiated itself from the government by promising an ETS with an earlier start date of 2010 rather than the 2012 timeframe advocated by Howard. It also promised ratification of the Kyoto Protocol, investment in clean coal and renewable energy, and slightly more aggressive targets for renewable energy.\n\nLabor won the election on 24 November 2007, and on 3 December 2007 the Rudd Government signed the ratification of the Kyoto Protocol at the 2007 United Nations Climate Change Conference. By ratifying the Kyoto Protocol, Australia committed to keeping emissions to no more than 108% of its 1990 emissions level by 2012. Australia's ratification came into effect on 11 March 2008.\n\nThe Rudd government began negotiating the passage of an ETS through the Parliament. The Opposition led by Brendan Nelson called for the vote on the government's ETS be delayed until after the United Nations climate change summit in Copenhagen in December 2009. Prime Minister Rudd said in response that it would be \"an act of absolute political cowardice, an absolute failure of leadership not to act on climate change until other nations had done so\" and the government pursued the early introduction of the Scheme.\n\nOn 16 July 2008, the Rudd Government released a green paper for its Carbon Pollution Reduction Scheme (CPRS) (also known as Australia's ETS), outlining the intended design of the scheme. The CPRS was criticised by those who were both for and against action to mitigate climate change. Environmental lobby groups protested that the emissions reductions targets were too low, and that the level of assistance to polluters was too high. Industry and business lobby groups however argued for more permits and assistance to offset the economic impacts of the scheme on many enterprises, particularly given the context of the global financial crisis. Malcolm Turnbull became the new Liberal Opposition Leader on 18 September 2008. On 30 September 2008, the Garnaut Climate Change Review, commissioned in April 2007 by Rudd when he was leader of the Opposition, released its final report. Garnaut recommended a price between $20 and $30 per tonne of carbon dioxide (CO) equivalent with a rise of 4% each year. A more detailed white paper on the CPRS was released on 15 December 2008.\n\nUnable to secure the support of the Australian Greens for their preferred model, the government entered negotiations with Turnbull, and in the lead up to the Copenhagen Conference, presented an amended CPRS scheme, with the support of Turnbull. The Turnbull-led Opposition supported the CPRS scheme in principle, although at times over 2009 they indicated disagreement with various details including the timing of implementation of the scheme, timing of the vote on the relevant legislation and on the level of assistance to be provided to polluting industries. The Opposition was able to negotiate greater compensation for polluters affected by the scheme in November 2009.\n\nShortly before the Senate was due to vote on the carbon bills, on 1 December 2009 Tony Abbott replaced Turnbull as leader of the Liberal Party. Abbot immediately called a secret ballot on support for the ETS among coalition MPs, which was overwhelmingly rejected. The Coalition then withdrew their support for the carbon pricing policy and joined the Greens and Independents in voting against the relevant legislation in the Parliament of Australia on 2 December 2009. As the Rudd government required the support of either the Coalition or the Greens to secure passage of the bill, it was defeated in the Senate. Abbott described Labor's ETS plan as a 'Great big tax on everything'. The Copenhagen Conference was unsuccessful in advancing international agreement.\n\nAbbott announced a new Coalition policy on carbon emission reduction in February 2010, which committed the Coalition to a 5% reduction in emissions by 2020. Abbott proposed the creation of an 'emissions reduction fund' to provide 'direct' incentives to industry and farmers to reduce carbon emissions. In April 2010, Rudd deferred attempts to advance the scheme to at least 2013, opting not to present the legislation to the Senate a second time, creating a trigger for a double dissolution election. In June 2010, Julia Gillard replaced Rudd as leader of the Labor Party and became Prime Minister. Factional leader and key Gillard supporter Bill Shorten said that the sudden announcement of change of policy on the ETS was a factor that had contributed to a collapse in support for Rudd's leadership.\n\nShortly afterwards Gillard called a federal election for 21 August 2010. During the election campaign Gillard stated that she supported a price on carbon emissions and that she would prosecute the case for action for as long as she needed to win community support. However, she also indicated that she would not introduce carbon pricing until there was a sufficient consensus on the issue, and she specifically ruled out the introduction of a \"carbon tax\".\n\nThe result of the election left Australia with its first hung parliament in 70 years. To form a majority in the House of Representatives both of the major parties needed to acquire the support of cross-benchers, including the Greens. After two weeks of negotiations Gillard had enough support to gain a majority including the support of the Greens and their single MP in the House, Adam Bandt. Gillard, therefore, remained Prime Minister and Abbott remained in Opposition. One of the conditions for Greens support was that the formation of a cross-party parliamentary committee to determine policy on climate change. Gillard honoured that agreement and on 27 September 2010 the Multi-Party Climate Change Committee (MPCCC) was formed, its terms of reference including that it was to report to Cabinet on ways to introduce a carbon price. The MPCCC agreed on the introduction of a fixed carbon price commencing 1 July 2012, transitioning to a flexible-price cap-and-trade ETS on 1 July 2015. Initially the price of permits is fixed and the quantity unlimited i.e. there is no cap; the scheme thus functions similarly, and is popularly referred to as a tax.\n\nIn February 2011, the government proposed the Clean Energy Bill, which the opposition claimed to be a broken election promise. The Liberal Party vowed to overturn the bill if it was elected.\n\nThe Gillard Government had asked the Productivity Commission to report on the steps taken by eight major economies to address climate change. In June 2011, the report found that more than 1,000 climate policies were already enacted across the globe. It also supported a market-based carbon price as being the most cost-effective way to reduce emissions. The report's findings were one of the major reasons that support for the carbon tax was provided by independent Tony Windsor. Windsor made it clear that he would not support the clean energy legislation if it included a carbon tax on transport fuels. He did not want to penalise people who lived in rural areas, where there was no public transport as an alternative to private vehicles.\n\nThe Clean Energy Plan was released on 10 July 2011. The Clean Energy Bill 2011 passed the House of Representatives in October 2011 and the Senate in November 2011 and was thus brought into law.\n\nOn 1 July 2012 the Australian Federal government introduced a carbon price scheme. To offset the impact of the tax on some sectors of society, the government reduced income tax (by increasing the tax-free threshold) and increased pensions and welfare payments slightly to cover expected price increases, as well as introducing compensation for some affected industries. On 17 July 2014, a report by the Australian National University estimated that the Australian scheme had cut carbon emissions by as much as 17 million tonnes, the biggest annual reduction in greenhouse gas emissions in 24 years of records in 2013 as the carbon tax helped drive a large drop in pollution from the electricity sector.\n\nOn 17 July 2014, the Abbott Government passed repeal legislation through the Senate to abolish the carbon pricing scheme. In its place the government set up the Emission Reduction Fund, paid by taxpayers from consolidated revenue, which according to RepuTex, a markets consultancy, estimated the government’s main climate policy may only meet a third of the emissions reduction challenge if Australia is to cut 2000 levels by 5% by 2020.\n\nThe carbon price came into effect on 1 July 2012 and applied to direct emissions from a facility (scope-1 emissions), and not to indirect emissions (scope-2 emissions). The scheme only applied to facilities which emit more than 25,000 tonnes CO-e per year, and did not apply to agriculture or to transport fuels. The carbon price was set at AUD$23 per tonne of emitted CO-e on selected fossil fuels consumed by major industrial emitters and government bodies such as councils.\n\nAgricultural emissions were exempt due to difficulty in tracking emissions and the related complexity of administering such a scheme. Households and business use of light vehicles did not incur a carbon price. However, changes to the fuel tax regime were proposed to effectively impose a carbon tax on business liquid and gaseous fuel emissions. There were plans for heavy on-road vehicles to pay from 1 July 2014.\n\nIn effect, the scope of the scheme meant that only a small number of large electricity generators and larger industrial plants were subject to the carbon price scheme. The tax was payable by surrendering carbon units, which had been either purchased (at $20 per tonne in 2012–13) or acquired free under an industry assistance program. The pricing mechanism was expected to cover 60% of Australia's carbon emissions. 75% of each company's annual obligation were to be paid by 15 June each year with the remaining 25% by the following 1 February.\n\nA list of companies which have paid carbon tax, and the amount which each had paid was published by the Clean Energy Regulator (CER). This was called the Liable Entities Public Information Database or LEPID. The LEPID for 2012–13 was updated on 12 July 2013 and the companies which were the fifteen largest payers of carbon tax in 2012–13 are shown in the summary below (related companies are grouped together where identifiable).\n\nThe Climate Change Authority, a statutory agency, was created to advise the government on the setting of carbon pollution caps, to conduct periodic reviews of the carbon pricing process, and to report on progress towards meeting national targets. These pollution caps were to form the basis for the cap-and-trade structure to commence in 2015.\n\nThe Government ran several major ‘Industry Assistance’ programs to reduce the impact of carbon tax for the 185 affected companies. These have the effect of significantly reducing the actual carbon tax raised.\n\nThe ‘Jobs and Competitiveness Program’ was for the non-electricity sector and was targeted at the ‘emissions-intensive trade-exposed’ activities – that is, companies which emitted a lot of CO2 and were exposed to imports or who trade internationally. There was a list of 48 trade-exposed activities, including business such as steel making, alumina refining, cement making and similar activities.\n\nDepending on whether a company was ‘highly’ or ‘moderately’ emissions intensive, it received 94.5% or 66% of ‘average industry carbon costs’ supplied as free carbon units.\n\nOverall in 2012–13 under the ‘Jobs and Competitiveness Program’, there were 104 million free carbon units issued to 123 applicants, valued at approximately $2.4 billion. The fifteen largest recipients of free carbon units in 2012–13, with related companies grouped together where identifiable, were:\n\nTo put this into context, the LEPID list indicated that the total amount of carbon units to be surrendered would be 283 million units for 2012–13. 37% of these were awarded for free under the Jobs and Competitiveness Program.\n\nUnder the ‘Coal Fired Generation Assistance’ for coal-based electricity generating companies the Government gave out 42 million of free carbon units each year, valued at almost $5 billion. These were only issued to the generators with the highest amount of CO2 emission intensity, above 1.0 tonne of CO2 per MWh of energy. These were primarily the brown coal-fired generators in Victoria's Latrobe Valley\n\nThe free units were shared according to their size and the amount of CO2 produced compared to a more efficient black coal-fired power station. The list of companies which received the free units was published by the Clean Energy Regulator. Nine power stations qualified – the big four brown coal plants in Victoria, and five other much smaller plants. The four big brown coal plants in Victoria received the majority share of free carbon units, around 37 million of the 42 million free carbon units in September each year.\n\nWith an average emissions intensity of 1.3, that effectively meant there was no carbon tax on the first 20 TWh (or approximately 50%) they collectively produced each year.\n\nThe Steel Transformation Plan was a $500 million package for Australia's two steelmakers. In 2012, payments of $160 million were made, $200 M to BlueScope and $70 M to OneSteel.\n\nBecause the Australian carbon tax does not apply to all fossil fuels usage, it only had an effect on some of the emitters of greenhouse gases. Among those emitters to which it applied, emissions were significantly lower after introduction of the tax. According to the Investor Group on Climate Change, emissions from companies subject to the tax went down 7% with the introduction of the tax, and the tax was \"the major contributor\" to this reduction.\n\nAustralia’s total greenhouse gas emissions increased by 0.3% in the first six months of the Carbon Tax to December 2012 to 276.5 Mt CO2 equiv, while Australia's gross domestic product grew at a rate of 2.5% per annum.\n\nGreenhouse emissions from stationary energy (excluding electricity) and transport grew by 4% in the first six months of the carbon tax to December 2012.\n\nHowever, there is a five-year trend for emissions from the electricity generation sector in Australia to decline. Electricity emissions peaked at 38% of the national total in September quarter 2008, coinciding with the start of the Global Financial Crisis. In December 2012, electricity emissions were just 33% of national emissions. The decline is due partly to an almost 6% reduction in electricity demand in the National Electricity market since 2008. This fall in electricity demand followed:· \nOther factors contributing to the five-year fall in greenhouse emissions from the electricity sector are:· \nThe Australian Government said in July 2013 that the carbon tax was a factor in reducing the emissions intensity in the National Electricity Market from 0.92 t of CO2 per MWh to 0.87 in the 11 months following its introduction.\n\nSince the carbon tax was introduced, wholesale electricity prices in the National Electricity market have increased significantly. The Energy Users Association of Australia in its June 2013 paper said that electricity generators have been able to pass through more than 100% of the cost of the carbon tax\". “If the outcomes observed in the spot market persist then it can be unequivocally concluded that both fossil fuel generators and renewable generators will have gained as a result of emission pricing, at users’ expense. Surely this is not what was intended.\"\"\n\nFrontier Economics said the reduction in emissions from the electricity sector in the first year of the carbon tax was \"<nowiki>'largely explained by factors unrelated to the carbon tax'.</nowiki>\" \n\nThe Energy Users Association of Australia (EUAA) said in June 2013 \"we suggest that it cannot be said that pricing emissions has reduced emissions in stationary energy to any meaningful extent\" \n\nAGL – In relation to its purchase of the Loy Yang brown coal fired power station in 2012, one of the single largest emitters of CO2 in Australia states “\"On the supply side of the business, the most significant strategic development was the decision to buy the Loy Yang A power station. ... The Board also recognised that coal fired generation would be required for decades to come if the demand from Australian households and businesses for electricity was to continue to be satisfied” \"\n\nAdelaide Brighton (Australia’s second largest cement producer) “….\"AdelaideBrighton expects it will significantly mitigate the impact of the carbon tax over the next five years by:· \"\nBlueScope (Australia’s largest steelmaker) \"\"When funds from the Steel Transformation Plan are taken into account, the Company does not expect to face a net carbon liability over the period\"”.\n\nDavid Kassulke, the manager of AJ Bush & Sons, expressed grave concerns over the carbon tax during the lead up to its implementation. However, he now says the carbon tax has had a positive impact on the business. The company expects to cut carbon emissions from 85,000 to 30,000 tonnes per year with the construction of a new biogas plant in 2013.\n\n\"The end result of the introduction of the new biogas technology will not only be a saving of millions of dollars in energy and carbon costs, but also an opportunity for the company to be positioned at the cutting edge of renewable energy technology in the rendering industry, Mr Kassulke said.\"\n\n“The (biogas technology) investment is a good way to modernise and will dramatically reduce our emissions.\"\n\n“It will mean that we will reduce our emissions to the point where we will no longer be a big polluter any more.\"\n\n“What the imposition of the carbon tax has done is make industry take stock of what it is currently doing and has forced it to look at doing things in a better way.\"\n\n“It means companies are now looking at ways to use less energy which equates to less cost and a subsequent reduction in the tax that is being levied.“That has been the intention of the tax and clearly from that perspective it is working and working well.”\n\nThe introduction of a carbon price in Australia was controversial. The day before the 2010 federal election, Prime Minister, Julia Gillard sent out a message regarding carbon pricing, stating \"I don't rule out the possibility of legislating a Carbon Pollution Reduction Scheme, a market-based mechanism.\" However the article also articulated her position on that term of government. \"While any carbon price would not be triggered until after the 2013 election... She would legislate the carbon price next term if sufficient consensus existed\", and the federal opposition accused the government of breaking an election promise to not introduce a carbon tax. Julia Gillard responded to these accusations by saying that circumstances changed following the 2010 election. Then opposition leader Tony Abbott criticised the carbon pricing policy on economic grounds referring to it as \"toxic\" and likening it to an octopus embracing the whole of the economy. He pledged to repeal the tax after the 18 clean energy bills passed through the House of Representatives and stated that the next election would be a referendum on the \"carbon tax\".\n\nThe opposition (and since the 2013 election the Abbott government) proposed an alternative \"direct-action\" carbon emissions reduction scheme. Modelling produced by the Department of the Treasury indicated that this scheme would cost twice as much as the Clean Energy Futures Plan. Abbott was unable to find an Australian economist who supported his policy, although he did cite international economists who are supportive. Tony Abbott's \"Direct Action Plan\" has been criticised because there is no disincentive to continue polluting at the same rate, meaning that emissions will increase rather than decrease by 2020. In addition, \"under Direct Action it is the public, not polluters who pay.\"\n\nThe Australian Renewable Energy Agency (ARENA) was established as part of the Clean Energy Fund, and commenced operations on 1 July 2012. It consolidated existing renewable energy technology innovation programs. It had funds to provide financial assistance to research, develop, demonstrate, deploy and commercialise renewable energy in Australia and related technologies. The government-established but independent Clean Energy Finance Corporation (CEFC) commenced investment operations from 1 July 2013, with a focus on investments in renewable energy, low-emissions and energy efficiency technology and the manufacturing companies that produce materials used in such technologies.\n\nThe majority of big emitters in Australia supported a price on carbon as at July 2012. However business groups and some big emitters, especially in the mining sector, were opposed to the pricing scheme.\n\nResearch by Preston Teeter and Jorgen Sandberg at the University of Queensland revealed that liable organisations responded with very few investments in emissions reduction activities, largely due to the great deal of policy uncertainty surrounding the scheme. \nOne criticism of the carbon pricing scheme has been that Australia should not proceed with its introduction ahead of other countries. However, according to the Department of Climate Change and Energy Efficiency, Australia will be one of around 50 jurisdictions implementing similar schemes worldwide. The starting price of $23 per tonne has also been a point of contention.\n\nEmissions figures from the 2010–11 financial year suggest the electricity generation sector may be due to pay around $3.9 billion. Loans have been made available so that electricity generators can purchase carbon permits. Macquarie Generation, a Government of New South Wales owned electricity generator, wrote down the value of its assets by about $1 billion as a result of the carbon tax. Power generators in the La Trobe Valley also face substantial write-downs.\n\nModelling undertaken by the Virgin Australia airline calculated that the average increase per flight would be $3. They responded by implementing a surcharge of between $1.00 and $5.00 to a one-way flight starting in July 2012. Qantas is raising its ticket prices by between $1.50 and $5.50.\n\nIn a survey conducted by the Economic Society of Australia, 60% of economists thought the carbon pricing proposal was sound economic policy, while 25% disagreed. A number of public protests both in support of and against the carbon price (or tax) have been held in the run up to its introduction. These include the No Carbon Tax Climate Sceptics rallies and Say Yes demonstrations.\n\nThe carbon pricing scheme was intended to improve energy efficiency, convert electricity generation from coal to alternatives and shift economic activity towards a low carbon economy. Its impact on business was forecast to be 0.1 – 0.2% lower than the business as usual scenario. The scheme aimed to prevent 160 million tonnes of carbon dioxide from entering the atmosphere by 2020, as well as generating $24 billion over three years.\n\nIn May 2012, the Australian Competition and Consumer Commission (ACCC) reported it was investigating about 100 cases where customers had possibly been misled into paying excessive price rises falsely claimed to be as a result of the carbon tax. By the middle of June, the commission was investigating about 200 cases. The consumer watchdog also set up a phone hotline and online form for complaints regarding excess pricing claimed to be due to the carbon tax. The ACCC had forecast that home construction costs would be at the lower end of the 0.7% to 1.8% range predicted by building companies. The Housing Industry Association estimated an average new house would experience a price increase of between 0.8% and 1.7% due to the carbon price. Housing construction was expected to be significantly impacted by the carbon tax because new homes require cement, bricks, aluminium, and glass, which are all typically energy-intensive materials. A forecast by the Centre for International Economics predicted the housing construction industry could decline by 12.6% as a result of the carbon price.\n\nThe coal industry was expected to be impacted due to the emissions produced as coal is mined, however a similar expense is not expected to be incurred by Australia's coal exporting competitors. The Institute of Public Affairs claimed that the Australian coal industry would lose jobs to overseas competitors and mines will be closed. Despite the announcement of the scheme, spending on mineral exploration in the March quarter was the highest ever at $1.086 billion. The impact on the LNG industry in Australia was expected to be minor to moderate. No major projects were expected to be cancelled as a result of the introduction of the carbon pricing scheme. Dairy farmers will be impacted because of higher power costs for milk processing.\n\nHousehold bills were expected to rise by an average of around $5 per week. Energy retailer Synergy said the carbon price would result in a 7.1% rise to power bills.\n\nBecause carbon pricing would indirectly flow through to consumers, the Australian government implemented household assistance measures.\n\nThe measures included changes to income tax: the tax-free threshold increased from $6,000 to $18,200 on 1 July 2012, and was scheduled to rise to $19,400 from 1 July 2015. The changes meant those earning less than $20,000 received a tax cut with those earning up to $5,000 receiving the greatest tax reduction. The changes were described as the biggest overhaul of taxation since the Goods and Services Tax was introduced in 2000.\n\nOther steps included direct payments into bank accounts beginning in May 2012. The payments, called the Clean Energy Advance, were targeted at low- and middle-income households.\n\nSome industries received direct compensation. As part of the Energy Security Fund, $1 billion was promised to highly emissions-intensive coal-fired generators. Most of that funding was intended for coal-fired power generators in Victoria. Research by the Grattan Institute suggested that no black coal mining or liquefied natural gas projects would be scrapped as a result of carbon pricing, regardless of industry compensation; it further claimed that, if coupled with compensation, the carbon pricing regime would in fact leave the steel industry better off.\n\nUnder the Carbon Farming Initiative, farmers and graziers would have been able to plant trees to earn carbon credits, which could have been on-sold to companies liable to pay a carbon price. The Clean Technology Investment Program was touted as helping the manufacturing sector to support investments in \"energy-efficient capital equipment and low emission technologies, processes and products\". Companies in the food sector would also have been able to apply for grants to improve their energy efficiency.\n\nSix months after the introduction of carbon pricing the Department of Climate Change and Renewable Energy reported a 9% decrease in emissions from electricity generators.\n\nNine months after the introduction of the pricing scheme, Australia's emissions of carbon dioxide due to electricity generation fell to a 10-year low, with coal generation down 6% from 2008 to 2009.\n\nHeading into the 2013 Australian federal election, the Liberal Party platform included the removal of the 'Carbon Tax', claiming that the election was in effect a referendum on carbon pricing in Australia. The incoming Liberal Government placed removing the carbon pricing scheme at the head of its legislative program.\n\nThe carbon tax repeal legislation received Royal Assent on 17 July 2014 and the bills which were part of the package became law, with effect from 1 July 2014.\n\n\n"}
{"id": "9591772", "url": "https://en.wikipedia.org/wiki?curid=9591772", "title": "Corgee", "text": "Corgee\n\nA corgee is an obsolete unit of mass equal to 212 moodahs, or rush mat bundles of rice. The unit was used in the Canara (now Kanara) region of Karnataka in India.\n\n"}
{"id": "1582810", "url": "https://en.wikipedia.org/wiki?curid=1582810", "title": "Crown molding", "text": "Crown molding\n\nCrown molding encapsulates a large family of moldings which are designed to gracefully flare out to a finished top edge. Crown molding is generally used for capping walls, pilasters, and cabinets, and is used extensively in the creation of interior and exterior cornice assemblies and door and window hoods. .\n\nIn recent times, crown moldings have generally made their appearance as mostly decorated plaster or wooden trim where walls meet ceilings.\n\nCrown molding is typically applied along the seams where ceiling meets wall. Usually it is not placed flush against the wall nor against the ceiling. Instead, when viewed from the molding's end (or as a cross-section), it, the ceiling, and the wall form a \"hollow\" triangle. This adds a difficulty to the installation process, namely the need for complex cuts to form corners where two walls meet. \n\nThere are two common ways to fashion inside corners. One is to use a compound miter saw to cut the ends of the corner pieces along two axes simultaneously. The other, called coping, is a two step process, first to cut a simple miter and then to use a coping saw to undercut the miters.\n\nMany different companies now manufacture crown molding in materials such as plastic and foam. These typically are offered with corner blocks, and are popular with DIY home improvement enthusiasts.\n\nThe use of a coped joint for interior corners saves the trouble of having to determine and cut the exact inside degree measurement, since most corners are not exactly 90/45 degrees. Outside corners must be mitered, and care must be taken in measuring and cutting, since not all outside corners measure true. If the angle is not exactly 45/22.5 degrees, a corner measuring device or piece of scrap crown molding may be used to obtain the right measurement before the final cut is made.\n\nFitting crown molding requires a cut at the correct combination of miter angle and bevel angle. The calculation of these angles is affected by two variables: (1) the spring angle (or crown angle, typically sold in 45 degree and 38 degree formats), and (2) the wall angle.\n\nPre-calculated crown molding tables or software can be used to facilitate the determination of the correct angles. Given the spring angle and the wall angle, the formulas used to calculate the miter angle and the bevel angle are:\n\n\n"}
{"id": "27384706", "url": "https://en.wikipedia.org/wiki?curid=27384706", "title": "Denailer", "text": "Denailer\n\nA denailer is a tool for removing nails from lumber to facilitate its reuse. Two types of \"denailer\" are available:\n\n\n"}
{"id": "20344155", "url": "https://en.wikipedia.org/wiki?curid=20344155", "title": "Electrical grid", "text": "Electrical grid\n\nAn electrical grid is an interconnected network for delivering electricity from producers to consumers. It consists of\n\n\nPower stations may be located near a fuel source, at a dam site (to take advantage of renewable energy sources), and are often located away from heavily populated areas. The electric power which is generated is stepped up to a higher voltage at which it connects to the electric power transmission net.\n\nThe bulk power transmission network will move the power long distances, sometimes across international boundaries, until it reaches its wholesale customer (usually the company that owns the local electric power distribution network).\n\nOn arrival at a substation, the power will be stepped down from a transmission level voltage to a distribution level voltage. As it exits the substation, it enters the distribution wiring. Finally, upon arrival at the service location, the power is stepped down again from the distribution voltage to the required service voltage(s).\n\nElectrical grids vary in size from covering a single building through \"national grids\" which cover whole countries, to \"transnational grids\" which can cross continents.\n\nAlthough electrical grids are wide spread, 1.4 billion people are not connected to an electricity grid.\n\nElectrical grids can be prone to malicious intrusion or attack; thus, there is a need for electric grid security. Also as electric grids modernize and introduce computers, cyber threats also start to become a security risk.\n\nEarly electric energy was produced near the device or service requiring that energy. In the 1880s, electricity competed with steam, hydraulics, and especially coal gas. Coal gas was first produced on customer's premises but later evolved into gasification plants that enjoyed economies of scale. In the industrialized world, cities had networks of piped gas, used for lighting. But gas lamps produced poor light, wasted heat, made rooms hot and smoky, and gave off hydrogen and carbon monoxide. In the 1880s electric lighting soon became advantageous compared to gas lighting.\n\nElectric utility companies took advantage of economies of scale and moved to centralized power generation, distribution, and system management. With long distance power transmission it became possible to interconnect stations to balance load and improve load factors.\n\nIn the United Kingdom, Charles Merz, of the Merz & McLellan consulting partnership, built the Neptune Bank Power Station near Newcastle upon Tyne in 1901, and by 1912 had developed into the largest integrated power system in Europe. Merz was appointed head of a Parliamentary Committee and his findings led to the Williamson Report of 1918, which in turn created the Electricity Supply Bill of 1919. The bill was the first step towards an integrated electricity system. The Electricity (Supply) Act of 1926 led to the setting up of the National Grid. The Central Electricity Board standardized the nation's electricity supply and established the first synchronized AC grid, running at 132 kilovolts and 50 Hertz. This started operating as a national system, the National Grid, in 1938.\n\nIn the United States in the 1920s, utilities formed joint-operations to share peak load coverage and backup power. In 1934, with the passage of the Public Utility Holding Company Act (USA), electric utilities were recognized as public goods of importance and were given outlined restrictions and regulatory oversight of their operations. The Energy Policy Act of 1992 required transmission line owners to allow electric generation companies open access to their network and led to a restructuring of how the electric industry operated in an effort to create competition in power generation. No longer were electric utilities built as vertical monopolies, where generation, transmission and distribution were handled by a single company. Now, the three stages could be split among various companies, in an effort to provide fair accessibility to high voltage transmission. The Energy Policy Act of 2005 allowed incentives and loan guarantees for alternative energy production and advance innovative technologies that avoided greenhouse emissions.\n\nIn France, electrification began in the 1900s, with 700 communes in 1919, and 36,528 in 1938. At the same time, the nearby networks began to interconnect: Paris in 1907 at 12 kV, the Pyrénées in 1923 at 150 kV, and finally almost all of the country interconnected in 1938 at 220 kV. By 1946, the grid is the world's most dense. That year that state nationalised the industry, by uniting the private companies as Électricité de France. The frequency was standardised at 50 Hz, and the 225 kV network replaces 110 and 120. From 1956, service voltage is standardised at 220 / 380 V, replacing the previous 127/220 V. During the 1970s, the 400 kV network, the new European standard, was implemented.\n\nGrids are designed to supply voltages at largely constant amplitudes. This has to be achieved with varying demand, variable reactive loads, and even nonlinear loads, with electricity provided by generators and distribution and transmission equipment that are not perfectly reliable. Often grids use tap changers on transformers near to the consumers to adjust the voltage and keep it within specification.\n\nTransmission networks are complex with redundant pathways. For example, see the map of the United States' (right) high-voltage transmission network.\n\nThe structure, or \"topology\" of a grid can vary depending on the constraints of budget, requirements for system reliability, and the load and generation characteristics. The physical layout is often forced by what land is available and its geology. Distribution networks are divided into two types, radial or network.\n\nThe simplest topology for a distribution or transmission grid is a \"radial\" structure. This is a \"tree\" shape where power from a large supply radiates out into progressively lower voltage lines until the destination homes and businesses are reached. However, single failures can take out entire branches of the tree.\n\nMost transmission grids offer the reliability that more complex \"mesh networks\" provide. The expense of mesh topologies restrict their application to transmission and medium voltage distribution grids. Redundancy allows line failures to occur and power is simply rerouted while workmen repair the damaged and deactivated line.\n\nOther topologies used are \"looped\" systems found in Europe and \"tied ring\" networks.\nIn cities and towns of North America, the grid tends to follow the classic \"radially fed\" design. A substation receives its power from the transmission network, the power is stepped down with a transformer and sent to a bus from which feeders fan out in all directions across the countryside. These feeders carry three-phase power, and tend to follow the major streets near the substation. As the distance from the substation grows, the fanout continues as smaller laterals spread out to cover areas missed by the feeders. This tree-like structure grows outward from the substation, but for reliability reasons, usually contains at least one unused backup connection to a nearby substation. This connection can be enabled in case of an emergency, so that a portion of a substation's service territory can be alternatively fed by another substation.\n\nA synchronous grid or an \"interconnection\" is a group of distribution areas all operating with three phase alternating current (AC) frequencies synchronized (so that peaks occur at virtually the same time). This allows transmission of AC power throughout the area, connecting a large number of electricity generators and consumers and potentially enabling more efficient electricity markets and redundant generation. Interconnection maps are shown of North America (right) and Europe (below left).\n\nA large failure in one part of the grid - unless quickly compensated for - can cause current to re-route itself to flow from the remaining generators to consumers over transmission lines of insufficient capacity, causing further failures. One downside to a widely connected grid is thus the possibility of cascading failure and widespread power outage. A central authority is usually designated to facilitate communication and develop protocols to maintain a stable grid. For example, the North American Electric Reliability Corporation gained binding powers in the United States in 2006, and has advisory powers in the applicable parts of Canada and Mexico. The U.S. government has also designated National Interest Electric Transmission Corridors, where it believes transmission bottlenecks have developed.\n\nSome areas, for example rural communities in Alaska, do not operate on a large grid, relying instead on local diesel generators.\n\nAn entire synchronous grid runs at the same frequency. Where interconnection to a neighboring grid, operating at a different frequency, is required, a frequency converter is required. High voltage direct current links can connect two grids that operate at different frequencies or that are not maintaining synchronism.\n\nIn a synchronous grid all the generators must run at the same frequency, and must stay very nearly in phase with each other and the grid. For rotating generators, a local governor regulates the driving torque, maintaining constant speed as loading changes. Droop speed control ensures that multiple parallel generators share load changes in proportion to their rating. Generation and consumption must be balanced across the entire grid, because energy is consumed as it is produced. Energy is stored in the immediate short term by the rotational kinetic energy of the generators.\n\nSmall deviations from the nominal system frequency are very important in regulating individual generators and assessing the equilibrium of the grid as a whole. When the grid is heavily loaded, the frequency slows, and governors adjust their generators so that more power is output (droop speed control). When the grid is lightly loaded the grid frequency runs above the nominal frequency, and this is taken as an indication by Automatic Generation Control systems across the network that generators should reduce their output.\n\nIn addition, there's often central control, which can change the parameters of the AGC systems over timescales of a minute or longer to further adjust the regional network flows and the operating frequency of the grid. For timekeeping purposes, over the course of a day the nominal frequency will be allowed to vary so as to balance out momentary deviations and to prevent line-operated clocks from gaining or losing significant time.\n\nHigh-voltage direct current lines or variable-frequency transformers can be used to connect two alternating current interconnection networks which are not necessarily synchronized with each other. This provides the benefit of interconnection without the need to synchronize an even wider area. For example, compare the wide area synchronous grid map of Europe (above left) with the map of HVDC lines (below right).\n\nElectric utilities between regions are many times interconnected for improved economy and reliability. Electrical interconnectors allow for economies of scale, allowing energy to be purchased from large, efficient sources. Utilities can draw power from generator reserves from a different region to ensure continuing, reliable power and diversify their loads. Interconnection also allows regions to have access to cheap bulk energy by receiving power from different sources. For example, one region may be producing cheap hydro power during high water seasons, but in low water seasons, another area may be producing cheaper power through wind, allowing both regions to access cheaper energy sources from one another during different times of the year. Neighboring utilities also help others to maintain the overall system frequency and also help manage tie transfers between utility regions.\n\nA \"wide area synchronous grid\" (also called an \"interconnection\" in North America) is an electrical grid at a regional scale or greater that operates at a synchronized frequency and is electrically tied together during normal system conditions. These are also known as synchronous zones, the largest of which is the synchronous grid of Continental Europe (ENTSO-E) with 667 gigawatts (GW) of generation, and the widest region served being that of the IPS/UPS system serving countries of the former Soviet Union. Synchronous grids with ample capacity facilitate electricity market trading across wide areas. In the ENTSO-E in 2008, over 350,000 megawatt hours were sold per day on the European Energy Exchange (EEX).\n\nAll of the interconnects in North America are synchronized at a nominal 60 Hz, while those of Europe run at 50 Hz. Interconnections can be tied to each other via high-voltage direct current power transmission lines (DC ties), or with variable-frequency transformers (VFTs), which permit a controlled flow of energy while also functionally isolating the independent AC frequencies of each side.\n\nThe benefits of synchronous zones include pooling of generation, resulting in lower generation costs; pooling of load, resulting in significant equalizing effects; common provisioning of reserves, resulting in cheaper primary and secondary reserve power costs; opening of the market, resulting in possibility of long term contracts and short term power exchanges; and mutual assistance in the event of disturbances.\n\nOne disadvantage of a wide-area synchronous grid is that problems in one part can have repercussions across the whole grid. For example, in 2018 Kosovo used more power than it generated due to a row with Serbia, leading to the phase in the whole Synchronous grid of Continental Europe lagging behind what it should have been. The frequency dropped to 49.996 Hz. This caused certain kinds of clocks to become six minutes slow.\n\nA microgrid is a localized group of electricity sources and loads that normally operates connected to and synchronous with the traditional wide area synchronous grid (macrogrid), but can also disconnect to \"island mode\" — and function autonomously as physical and/or economic conditions dictate.\n\nIn this way, a microgrid can effectively integrate various sources of distributed generation (DG), especially Renewable Energy Sources (RES), and can supply emergency power, changing between island and connected modes.\n\nControl and protection are challenges to microgrids.\n\nVarious planned and proposed systems to dramatically increase transmission capacity are known as super, or mega grids. The promised benefits include enabling the renewable energy industry to sell electricity to distant markets, the ability to increase usage of intermittent energy sources by balancing them across vast geological regions, and the removal of congestion that prevents electricity markets from flourishing. Local opposition to siting new lines and the significant cost of these projects are major obstacles to super grids. One study for a European super grid estimates that as much as 750 GW of extra transmission capacity would be required- capacity that would be accommodated in increments of 5 GW HVDC lines. A recent proposal by Transcanada priced a 1,600-km, 3-GW HVDC line at $3 billion USD and would require a corridor wide. In India, a recent 6 GW, 1,850-km proposal was priced at $790 million and would require a wide right of way. With 750 GW of new HVDC transmission capacity required for a European super grid, the land and money needed for new transmission lines would be considerable.\n\nDemand response is a grid management technique where retail or wholesale customers are requested either electronically or manually to reduce their load. Currently, transmission grid operators use demand response to request load reduction from major energy users such as industrial plants.\n\nDespite the novel institutional arrangements and network designs of the electrical grid, its power delivery infrastructures suffer aging across the developed world. Contributing factors to the current state of the electric grid and its consequences include:\n\nThirty-seven states plus the District of Columbia took some action to modernize electric grids in the first quarter of 2017, according to the North Carolina Clean Energy Technology Center. The states did so to make electricity systems \"more resilient and interactive\". The most common actions that states took were \"advanced metering infrastructure deployment\" (19 states did this), smart grid deployment and \"time-varying rates for residential customers\".\n\nLegislatively, in the first quarter of the year 82 relevant bills were introduced in different parts of the United States. At the close of the quarter, most of the bills remained pending. For example, legislators in Hawaii introduced a bill that would create an energy storage tax credit. In California, the state Senate had a bill that would \"create a new energy storage rebate program\".\n\nIn August 2018, Advanced Energy Economy (AEE) and Citizens for Responsible Energy Solutions Forum (CRES Forum) published a policy paper that gave five recommendations on ways to modernize the U.S. electric power grid. These recommendations are to streamline the federal permit process for advanced energy projects; encourage grid planners to consider alternatives to investment in transmission; allow energy storage and energy efficiency to compete with additional energy generation; allow large customers to choose their own sources of electricity; and allow utilities and consumers to benefit from cloud computing software.\n\nWith everything interconnected, and open competition occurring in a free market economy, it starts to make sense to allow and even encourage distributed generation (DG). Smaller generators, usually not owned by the utility, can be brought on-line to help supply the need for power. The smaller generation facility might be a home-owner with excess power from their solar panel or wind turbine. It might be a small office with a diesel generator. These resources can be brought on-line either at the utility's behest, or by owner of the generation in an effort to sell electricity. Many small generators are allowed to sell electricity back to the grid for the same price they would pay to buy it.\n\nAs the 21st century progresses, the electric utility industry seeks to take advantage of novel approaches to meet growing energy demand. Utilities are under pressure to evolve their classic topologies to accommodate distributed generation. As generation becomes more common from rooftop solar and wind generators, the differences between distribution and transmission grids will continue to blur. In July 2017 the CEO of Mercedes-Benz said that the energy industry needs to work better with companies from other industries to form a \"total ecosystem\", to integrate central and distributed energy resources (DER) to give customers what they want. The electrical grid was originally constructed so that electricity would flow from power providers to consumers. However, with the introduction of DER, power needs to flow both ways on the electric grid, because customers may have power sources such as solar panels.\n\nThe smart grid would be an enhancement of the 20th century electrical grid, using two-way communications and distributed so-called intelligent devices. Two-way flows of electricity and information could improve the delivery network. Research is mainly focused on three systems of a smart grid – the infrastructure system, the management system, and the protection system.\n\nThe infrastructure system is the energy, information, and communication infrastructure underlying of the smart grid that supports:\n\nA smart grid would allow the power industry to observe and control parts of the system at higher resolution in time and space. One of the purposes of the smart grid is real time information exchange to make operation as efficient as possible. It would allow management of the grid on all time scales from high-frequency switching devices on a microsecond scale, to wind and solar output variations on a minute scale, to the future effects of the carbon emissions generated by power production on a decade scale.\n\nThe management system is the subsystem in smart grid that provides advanced management and control services. Most of the existing works aim to improve energy efficiency, demand profile, utility, cost, and emission, based on the infrastructure by using optimization, machine learning, and game theory. Within the advanced infrastructure framework of smart grid, more and more new management services and applications are expected to emerge and eventually revolutionize consumers' daily lives.\n\nThe protection system of a smart grid provides grid reliability analysis, failure protection, and security and privacy protection services. While the additional communication infrastructure of a smart grid provides additional protective and security mechanisms, it also presents a risk of external attack and internal failures. In a report on cyber security of smart grid technology first produced in 2010, and later updated in 2014, the US National Institute of Standards and Technology pointed out that the ability to collect more data about energy use from customer smart meters also raises major privacy concerns, since the information stored at the meter, which is potentially vulnerable to data breaches, can be mined for personal details about customers.\n\nIn the U.S., the Energy Policy Act of 2005 and Title XIII of the Energy Independence and Security Act of 2007 are providing funding to encourage smart grid development. The objective is to enable utilities to better predict their needs, and in some cases involve consumers in a time-of-use tariff. Funds have also been allocated to develop more robust energy control technologies.\n\nAs there is some resistance in the electric utility sector to the concepts of distributed generation with various renewable energy sources and microscale cogen units, several authors have warned that mass-scale grid defection is possible because consumers can produce electricity using off grid systems primarily made up of solar photovoltaic technology.\n\nThe Rocky Mountain Institute has proposed that there may be widescale grid defection. This is backed up by studies in the Midwest. However, the paper points out that grid defection may be less likely in countries like Germany which have greater power demands in winter.\n\nDue to the enormous capital outlays, utilities were a vertically integrated business throughout the 20th century owning the power generation, the (transmission) lines while also managing the bills (commercialization). Nowadays technological progress has enabled individuals and groups to take on the functions that used to be the sole domain of the utility. Adding to the shift is the impact of aging infrastructure on reliability, security and performance factors.\n\n\n"}
{"id": "9129100", "url": "https://en.wikipedia.org/wiki?curid=9129100", "title": "Elin Lerum Boasson", "text": "Elin Lerum Boasson\n\nElin Lerum Boasson (born 28 May 1978) is a Norwegian environmentalist and was chairman of Natur og Ungdom in 2001 and 2002. She became active in the organisation in 1991, and joined the board in 1997 before she was elected deputy chairman in 1999. She has a Master of Political Science degree from the University of Oslo and has since 2005 worked at Nansen Institute as a researcher (as of 2009).\n"}
{"id": "23620187", "url": "https://en.wikipedia.org/wiki?curid=23620187", "title": "Energy in Eritrea", "text": "Energy in Eritrea\n\nEnergy in Eritrea is an industry lacking in natural resources, though it has plenty of potential.\n\nOil and gas exploration in the Red Sea off Eritrea began in the 1930s. Following independence, the country began awarding production contracts in 1995. However, as of January 2003, Eritrea had no proven reserves of crude oil or natural gas. It also has no known reserves of coal. As a result, the country, as of 2001, has had no output of oil, natural gas or coal. Petroleum imports and consumption were estimated each at in 2002. In 1997, due to high costs, Eritrea and Ethiopia agreed to shut down their joint operations at the petroleum refinery at Assab and import refined petroleum products. The refinery had a capacity of . In 2000 an estimated of oil were shipped through the Bab el-Mandeb, a narrow waterway between Eritrea, Yemen, and Djibouti that connects the Gulf of Aden with the Red Sea.\n\nAs of August 2003, about 80% of the population was without electricity, which was available only in the larger cities and towns, although the government was constructing additional electrical distribution lines. In 2002, net electricity generation was 243 GWh, of which 100% came from fossil fuels. In the same year, consumption of electricity totaled 226 GWh. As of August 2003, Eritrea had about 60 MW of diesel-fired generating capacity.\n"}
{"id": "2190938", "url": "https://en.wikipedia.org/wiki?curid=2190938", "title": "Flora Guerrero", "text": "Flora Guerrero\n\nFlora Guererro Goff is a painter, environmentalist, and founder of Guardianes de los Arboles (Guardians of the trees) in Cuernavaca, Mexico. The daughter of the painter Jesus Guerrero Galvan, Guerrero is a supporter of environmental social activism in Mexico including the protection of the Forest of Water, Mexico City's primary water source as well as threatened urban areas within the city of Cuernavaca. As an artist her religious paintings are on permanent display at various churches and she was invited to mount a one-woman exhibition, inaugurated October 16, 2009, at the Mision del Sol.\n"}
{"id": "40883387", "url": "https://en.wikipedia.org/wiki?curid=40883387", "title": "French cleat", "text": "French cleat\n\nA French cleat is way of securing a cabinet, mirror, artwork or other object to a wall. It is a molding with a 30–45 degree slope used to hang cabinets or other objects.\n\nFrench cleats can be used in pairs, or with a cleat mounted to the wall and a matching edge cut into the object to be hung.\n\nThe wall side of a French cleat can be mounted securely without having to hold the full weight of the cabinet while securing it. The cleat will not be seen later, so it does not matter where it is drilled; this means that it can be screwed into wall studs relatively independent of the lateral position of the cabinet.\nThe cleat can be the full length of the cabinet, so it allows supporting the cabinet at least at every stud behind it.\nIf the wall cleat is left slightly shorter than the cabinet, the cabinet can be shifted left and right slightly after it is hung, for perfect positioning.\n\nOnce the cleat is secured to the wall, the cabinet can be simply lifted onto it. Because no fine maneuvering is required, even a relatively heavy cabinet can be hung easily this way.\n\nThe bottom of the cabinet can be secured to the wall to be sure that it will not get pushed off the cleat while in use.\n"}
{"id": "18616290", "url": "https://en.wikipedia.org/wiki?curid=18616290", "title": "Gamma ray", "text": "Gamma ray\n\nA gamma ray or gamma radiation (symbol γ or formula_1), is a penetrating electromagnetic radiation arising from the radioactive decay of atomic nuclei. It consists of the shortest wavelength electromagnetic waves and so imparts the highest photon energy. Paul Villard, a French chemist and physicist, discovered gamma radiation in 1900 while studying radiation emitted by radium. In 1903, Ernest Rutherford named this radiation \"gamma rays\" based on their relatively strong penetration of matter; he had previously discovered two less penetrating types of decay radiation, which he named alpha rays and beta rays in ascending order of penetrating power.\n\nGamma rays from radioactive decay are in the energy range from a few keV to ~8 MeV, corresponding to the typical energy levels in nuclei with reasonably long lifetimes. The energy spectrum of gamma rays can be used to identify the decaying radionuclides using gamma spectroscopy. Very-high-energy gamma rays in the 100–1000 TeV range have been observed from sources such as the Cygnus X-3 microquasar.\n\nNatural sources of gamma rays originating on Earth are mostly as a result of radioactive decay and secondary radiation from atmospheric interactions with cosmic ray particles. However there are other rare natural sources, such as terrestrial gamma-ray flashes, that produce gamma rays from electron action upon the nucleus. Notable artificial sources of gamma rays include fission, such as occurs in nuclear reactors, as well as high energy physics experiments, such as neutral pion decay and nuclear fusion.\n\nGamma rays and X-rays are both electromagnetic radiation and they overlap in the electromagnetic spectrum; so that over a range of energies they cannot be differentiated by detection only. One way to distinguish them is by their origin, and in the case of X-rays, the origin is outside the nucleus due to electron interaction. Terminology varies between scientific disciplines. In astrophysics gamma rays are conventionally defined as having photon energies above 100 keV and are the subject of gamma ray astronomy, while radiation below 100 keV is classified as X-rays and is the subject of X-ray astronomy. This convention stems from the early man-made X-rays, which had energies only up to 100 KeV, whereas many gamma rays could go to higher energies. A large fraction of astronomical gamma rays are screened by Earth's atmosphere.\n\nGamma rays are ionizing radiation and are thus biologically hazardous. Due to their high penetration power, they can damage bone marrow and internal organs. Unlike alpha and beta rays, they pass easily through the body and thus pose a formidable radiation protection challenge.\n\nThe first gamma ray source to be discovered was the radioactive decay process called \"gamma decay\". In this type of decay, an excited nucleus emits a gamma ray almost immediately upon formation. Paul Villard, a French chemist and physicist, discovered gamma radiation in 1900, while studying radiation emitted from radium. Villard knew that his described radiation was more powerful than previously described types of rays from radium, which included beta rays, first noted as \"radioactivity\" by Henri Becquerel in 1896, and alpha rays, discovered as a less penetrating form of radiation by Rutherford, in 1899. However, Villard did not consider naming them as a different fundamental type. Later, in 1903, Villard's radiation was recognized as being of a type fundamentally different from previously named rays by Ernest Rutherford, who named Villard's rays \"gamma rays\" by analogy with the beta and alpha rays that Rutherford had differentiated in 1899. The \"rays\" emitted by radioactive elements were named in order of their power to penetrate various materials, using the first three letters of the Greek alphabet: alpha rays as the least penetrating, followed by beta rays, followed by gamma rays as the most penetrating. Rutherford also noted that gamma rays were not deflected (or at least, not deflected) by a magnetic field, another property making them unlike alpha and beta rays.\n\nGamma rays were first thought to be particles with mass, like alpha and beta rays. Rutherford initially believed that they might be extremely fast beta particles, but their failure to be deflected by a magnetic field indicated that they had no charge. In 1914, gamma rays were observed to be reflected from crystal surfaces, proving that they were electromagnetic radiation. Rutherford and his co-worker Edward Andrade measured the wavelengths of gamma rays from radium, and found that they were similar to X-rays, but with shorter wavelengths and (thus) higher frequency. This was eventually recognized as giving them more energy per photon, as soon as the latter term became generally accepted. A gamma decay was then understood to usually emit a gamma photon.\n\nNatural sources of gamma rays on Earth include gamma decay from naturally occurring radioisotopes such as potassium-40, and also as a secondary radiation from various atmospheric interactions with cosmic ray particles. Some rare terrestrial natural sources that produce gamma rays that are not of a nuclear origin, are lightning strikes and terrestrial gamma-ray flashes, which produce high energy emissions from natural high-energy voltages. Gamma rays are produced by a number of astronomical processes in which very high-energy electrons are produced. Such electrons produce secondary gamma rays by the mechanisms of bremsstrahlung, inverse Compton scattering and synchrotron radiation. A large fraction of such astronomical gamma rays are screened by Earth's atmosphere. Notable artificial sources of gamma rays include fission, such as occurs in nuclear reactors, as well as high energy physics experiments, such as neutral pion decay and nuclear fusion.\n\nA sample of gamma ray-emitting material that is used for irradiating or imaging is known as a gamma source. It is also called a radioactive source, isotope source, or radiation source, though these more general terms also apply to alpha- and beta-emitting devices. Gamma sources are usually sealed to prevent radioactive contamination, and transported in heavy shielding.\n\nGamma rays are produced during gamma decay, which normally occurs after other forms of decay occur, such as alpha or beta decay. An excited nucleus can decay by the emission of an or particle. The daughter nucleus that results is usually left in an excited state. It can then decay to a lower energy state by emitting a gamma ray photon, in a process called gamma decay.\n\nThe emission of a gamma ray from an excited nucleus typically requires only 10 seconds. Gamma decay may also follow nuclear reactions such as neutron capture, nuclear fission, or nuclear fusion. Gamma decay is also a mode of relaxation of many excited states of atomic nuclei following other types of radioactive decay, such as beta decay, so long as these states possess the necessary component of nuclear spin. When high-energy gamma rays, electrons, or protons bombard materials, the excited atoms emit characteristic \"secondary\" gamma rays, which are products of the creation of excited nuclear states in the bombarded atoms. Such transitions, a form of nuclear gamma fluorescence, form a topic in nuclear physics called gamma spectroscopy. Formation of fluorescent gamma rays are a rapid subtype of radioactive gamma decay.\n\nIn certain cases, the excited nuclear state that follows the emission of a beta particle or other type of excitation, may be more stable than average, and is termed a metastable excited state, if its decay takes (at least) 100 to 1000 times longer than the average 10 seconds. Such relatively long-lived excited nuclei are termed nuclear isomers, and their decays are termed isomeric transitions. Such nuclei have half-lifes that are more easily measurable, and rare nuclear isomers are able to stay in their excited state for minutes, hours, days, or occasionally far longer, before emitting a gamma ray. The process of isomeric transition is therefore similar to any gamma emission, but differs in that it involves the intermediate metastable excited state(s) of the nuclei. Metastable states are often characterized by high nuclear spin, requiring a change in spin of several units or more with gamma decay, instead of a single unit transition that occurs in only 10 seconds. The rate of gamma decay is also slowed when the energy of excitation of the nucleus is small.\n\nAn emitted gamma ray from any type of excited state may transfer its energy directly to any electrons, but most probably to one of the K shell electrons of the atom, causing it to be ejected from that atom, in a process generally termed the photoelectric effect (external gamma rays and ultraviolet rays may also cause this effect). The photoelectric effect should not be confused with the internal conversion process, in which a gamma ray photon is not produced as an intermediate particle (rather, a \"virtual gamma ray\" may be thought to mediate the process).\n\nOne example of gamma ray production due to radionuclide decay is the decay scheme for Cobalt 60, as illustrated in the accompanying diagram. First, decays to excited by beta decay emission of an electron of 0.31 MeV. Then the excited decays to the ground state (see nuclear shell model) by emitting gamma rays in succession of 1.17 MeV followed by 1.33 MeV. This path is followed 99.88% of the time:\n\nAnother example is the alpha decay of to form ; which is followed by gamma emission. In some cases, the gamma emission spectrum of the daughter nucleus is quite simple, (e.g. /) while in other cases, such as with (/ and /), the gamma emission spectrum is complex, revealing that a series of nuclear energy levels exist.\n\nGamma rays are produced in many processes of particle physics. Typically, gamma rays are the products of neutral systems which decay through electromagnetic interactions (rather than a weak or strong interaction). For example, in an electron–positron annihilation, the usual products are two gamma ray photons. If the annihilating electron and positron are at rest, each of the resulting gamma rays has an energy of ~ 511 keV and frequency of ~ . Similarly, a neutral pion most often decays into two photons. Many other hadrons and massive bosons also decay electromagnetically. High energy physics experiments, such as the Large Hadron Collider, accordingly employ substantial radiation shielding. Because subatomic particles mostly have far shorter wavelengths than atomic nuclei, particle physics gamma rays are generally several orders of magnitude more energetic than nuclear decay gamma rays. Since gamma rays are at the top of the electromagnetic spectrum in terms of energy, all extremely high-energy photons are gamma rays; for example, a photon having the Planck energy would be a gamma ray.\n\nA few gamma rays in astronomy are known to arise from gamma decay (see discussion of SN1987A), but most do not.\n\nPhotons from astrophysical sources that carry energy in the gamma radiation range are often explicitly called gamma-radiation. In addition to nuclear emissions, they are often produced by sub-atomic particle and particle-photon interactions. Those include electron-positron annihilation, neutral pion decay, bremsstrahlung, inverse Compton scattering, and synchrotron radiation.\nIn October 2017, scientists from various European universities proposed a means for sources of GeV photons using lasers as exciters through a controlled interplay between the cascade and anomalous radiative trapping.\n\nThunderstorms can produce a brief pulse of gamma radiation called a terrestrial gamma-ray flash. These gamma rays are thought to be produced by high intensity static electric fields accelerating electrons, which then produce gamma rays by bremsstrahlung as they collide with and are slowed by atoms in the atmosphere. Gamma rays up to 100 MeV can be emitted by terrestrial thunderstorms, and were discovered by space-borne observatories. This raises the possibility of health risks to passengers and crew on aircraft flying in or near thunderclouds.\n\nExtraterrestrial, high energy gamma rays include the gamma ray background produced when cosmic rays (either high speed electrons or protons) collide with ordinary matter, producing pair-production gamma rays at 511 keV. Alternatively, bremsstrahlung are produced at energies of tens of MeV or more when cosmic ray electrons interact with nuclei of sufficiently high atomic number (see gamma ray image of the Moon at the beginning of this article, for illustration).\nThe gamma ray sky (see illustration at right) is dominated by the more common and longer-term production of gamma rays that emanate from pulsars within the Milky Way. Sources from the rest of the sky are mostly quasars. Pulsars are thought to be neutron stars with magnetic fields that produce focused beams of radiation, and are far less energetic, more common, and much nearer sources (typically seen only in our own galaxy) than are quasars or the rarer gamma-ray burst sources of gamma rays. Pulsars have relatively long-lived magnetic fields that produce focused beams of relativistic speed charged particles, which emit gamma rays (bremsstrahlung) when those strike gas or dust in their nearby medium, and are decelerated. This is a similar mechanism to the production of high-energy photons in megavoltage radiation therapy machines (see bremsstrahlung). Inverse Compton scattering, in which charged particles (usually electrons) impart energy to low-energy photons boosting them to higher energy photons. Such impacts of photons on relativistic charged particle beams is another possible mechanism of gamma ray production. Neutron stars with a very high magnetic field (magnetars), thought to produce astronomical soft gamma repeaters, are another relatively long-lived star-powered source of gamma radiation.\n\nMore powerful gamma rays from very distant quasars and closer active galaxies are thought to have a gamma ray production source similar to a particle accelerator. High energy electrons produced by the quasar, and subjected to inverse Compton scattering, synchrotron radiation, or bremsstrahlung, are the likely source of the gamma rays from those objects. It is thought that a supermassive black hole at the center of such galaxies provides the power source that intermittently destroys stars and focuses the resulting charged particles into beams that emerge from their rotational poles. When those beams interact with gas, dust, and lower energy photons they produce X-rays and gamma rays. These sources are known to fluctuate with durations of a few weeks, suggesting their relatively small size (less than a few light-weeks across). Such sources of gamma and X-rays are the most commonly visible high intensity sources outside our galaxy. They shine not in bursts (see illustration), but relatively continuously when viewed with gamma ray telescopes. The power of a typical quasar is about 10 watts, a small fraction of which is gamma radiation. Much of the rest is emitted as electromagnetic waves of all frequencies, including radio waves.\nThe most intense sources of gamma rays, are also the most intense sources of any type of electromagnetic radiation presently known. They are the \"long duration burst\" sources of gamma rays in astronomy (\"long\" in this context, meaning a few tens of seconds), and they are rare compared with the sources discussed above. By contrast, \"short\" gamma-ray bursts of two seconds or less, which are not associated with supernovae, are thought to produce gamma rays during the collision of pairs of neutron stars, or a neutron star and a black hole.\n\nThe so-called \"long-duration\" gamma-ray bursts produce a total energy output of about 10 joules (as much energy as our Sun will produce in its entire life-time) but in a period of only 20 to 40 seconds. Gamma rays are approximately 50% of the total energy output. The leading hypotheses for the mechanism of production of these highest-known intensity beams of radiation, are inverse Compton scattering and synchrotron radiation from high-energy charged particles. These processes occur as relativistic charged particles leave the region of the event horizon of a newly formed black hole created during supernova explosion. The beam of particles moving at relativistic speeds are focused for a few tens of seconds by the magnetic field of the exploding hypernova. The fusion explosion of the hypernova drives the energetics of the process. If the narrowly directed beam happens to be pointed toward the Earth, it shines at gamma ray frequencies with such intensity, that it can be detected even at distances of up to 10 billion light years, which is close to the edge of the visible universe.\n\nDue to their penetrating nature, gamma rays require large amounts of shielding mass to reduce them to levels which are not harmful to living cells, in contrast to alpha particles, which can be stopped by paper or skin, and beta particles, which can be shielded by thin aluminium. Gamma rays are best absorbed by materials with high atomic numbers and high density, which contribute to the total stopping power. Because of this, a lead (high Z) shield is 20–30% better as a gamma shield than an equal mass of another low-Z shielding material, such as aluminium, concrete, water, or soil; lead's major advantage is not in lower weight, but rather its compactness due to its higher density. Protective clothing, goggles and respirators can protect from internal contact with or ingestion of alpha or beta emitting particles, but provide no protection from gamma radiation from external sources.\n\nThe higher the energy of the gamma rays, the thicker the shielding made from the same shielding material is required. Materials for shielding gamma rays are typically measured by the thickness required to reduce the intensity of the gamma rays by one half (the half value layer or HVL). For example, gamma rays that require (0.4″) of lead to reduce their intensity by 50% will also have their intensity reduced in half by of granite rock, 6 cm (2½″) of concrete, or 9 cm (3½″) of packed soil. However, the mass of this much concrete or soil is only 20–30% greater than that of lead with the same absorption capability. Depleted uranium is used for shielding in portable gamma ray sources, but here the savings in weight over lead are larger, In a nuclear power plant, shielding can be provided by steel and concrete in the pressure and particle containment vessel, while water provides a radiation shielding of fuel rods during storage or transport into the reactor core. The loss of water or removal of a \"hot\" fuel assembly into the air would result in much higher radiation levels than when kept under water.\n\nWhen a gamma ray passes through matter, the probability for absorption is proportional to the thickness of the layer, the density of the material, and the absorption cross section of the material. The total absorption shows an exponential decrease of intensity with distance from the incident surface:\nwhere x is the thickness of the material from the incident surface, μ= \"n\"σ is the absorption coefficient, measured in cm, \"n\" the number of atoms per cm of the material (atomic density) and σ the absorption cross section in cm.\n\nAs it passes through matter, gamma radiation ionizes via three processes: the photoelectric effect, Compton scattering, and pair production.\n\nThe secondary electrons (and/or positrons) produced in any of these three processes frequently have enough energy to produce much ionization themselves.\n\nAdditionally, gamma rays, particularly high energy ones, can interact with atomic nuclei resulting in ejection of particles in photodisintegration, or in some cases, even nuclear fission (photofission).\n\nHigh-energy (from 80 GeV to ~10 TeV) gamma rays arriving from far-distant quasars are used to estimate the extragalactic background light in the universe: The highest-energy rays interact more readily with the background light photons and thus the density of the background light may be estimated by analyzing the incoming gamma ray spectra.\n\nGamma spectroscopy is the study of the energetic transitions in atomic nuclei, which are generally associated with the absorption or emission of gamma rays. As in optical spectroscopy (see Franck–Condon effect) the absorption of gamma rays by a nucleus is especially likely (i.e., peaks in a \"resonance\") when the energy of the gamma ray is the same as that of an energy transition in the nucleus. In the case of gamma rays, such a resonance is seen in the technique of Mössbauer spectroscopy. In the Mössbauer effect the narrow resonance absorption for nuclear gamma absorption can be successfully attained by physically immobilizing atomic nuclei in a crystal. The immobilization of nuclei at both ends of a gamma resonance interaction is required so that no gamma energy is lost to the kinetic energy of recoiling nuclei at either the emitting or absorbing end of a gamma transition. Such loss of energy causes gamma ray resonance absorption to fail. However, when emitted gamma rays carry essentially all of the energy of the atomic nuclear de-excitation that produces them, this energy is also sufficient to excite the same energy state in a second immobilized nucleus of the same type.\n\nGamma rays provide information about some of the most energetic phenomena in the universe; however, they are largely absorbed by the Earth's atmosphere. Instruments aboard high-altitude balloons and satellites missions, such as the Fermi Gamma-ray Space Telescope, provide our only view of the universe in gamma rays.\n\nGamma-induced molecular changes can also be used to alter the properties of semi-precious stones, and is often used to change white topaz into blue topaz.\n\nNon-contact industrial sensors commonly use sources of gamma radiation in refining, mining, chemicals, food, soaps and detergents, and pulp and paper industries, for the measurement of levels, density, and thicknesses. Typically, these use Co-60 or Cs-137 isotopes as the radiation source.\n\nIn the US, gamma ray detectors are beginning to be used as part of the Container Security Initiative (CSI). These machines are advertised to be able to scan 30 containers per hour.\n\nGamma radiation is often used to kill living organisms, in a process called irradiation. Applications of this include the sterilization of medical equipment (as an alternative to autoclaves or chemical means), the removal of decay-causing bacteria from many foods and the prevention of the sprouting of fruit and vegetables to maintain freshness and flavor.\n\nDespite their cancer-causing properties, gamma rays are also used to treat some types of cancer, since the rays also kill cancer cells. In the procedure called gamma-knife surgery, multiple concentrated beams of gamma rays are directed to the growth in order to kill the cancerous cells. The beams are aimed from different angles to concentrate the radiation on the growth while minimizing damage to surrounding tissues.\n\nGamma rays are also used for diagnostic purposes in nuclear medicine in imaging techniques. A number of different gamma-emitting radioisotopes are used. For example, in a PET scan a radiolabeled sugar called fludeoxyglucose emits positrons that are annihilated by electrons, producing pairs of gamma rays that highlight cancer as the cancer often has a higher metabolic rate than the surrounding tissues. The most common gamma emitter used in medical applications is the nuclear isomer technetium-99m which emits gamma rays in the same energy range as diagnostic X-rays. When this radionuclide tracer is administered to a patient, a gamma camera can be used to form an image of the radioisotope's distribution by detecting the gamma radiation emitted (see also SPECT). Depending on which molecule has been labeled with the tracer, such techniques can be employed to diagnose a wide range of conditions (for example, the spread of cancer to the bones via bone scan).\n\nGamma rays cause damage at a cellular level and are penetrating, causing diffuse damage throughout the body. However, they are less ionising than alpha or beta particles, which are less penetrating.\n\nLow levels of gamma rays cause a stochastic health risk, which for radiation dose assessment is defined as the \"probability\" of cancer induction and genetic damage. High doses produce deterministic effects, which is the \"severity\" of acute tissue damage that is certain to happen. These effects are compared to the physical quantity absorbed dose measured by the unit gray (Gy).\n\nWhen gamma radiation breaks DNA molecules, a cell may be able to repair the damaged genetic material, within limits. However, a study of Rothkamm and Lobrich has shown that this repair process works well after high-dose exposure but is much slower than in the case of a low-dose exposure.\n\nThe natural outdoor exposure in Great Britain ranges from 0.1 to 0.5 µSv/h with significant increase around known nuclear and contaminated sites. Natural exposure to gamma rays is about 1 to 2 mSv per year, and the average total amount of radiation received in one year per inhabitant in the USA is 3.6 mSv. There is a small increase in the dose, due to naturally occurring gamma radiation, around small particles of high atomic number materials in the human body caused by the photoelectric effect.\n\nBy comparison, the radiation dose from chest radiography (about 0.06 mSv) is a fraction of the annual naturally occurring background radiation dose. A chest CT delivers 5 to 8 mSv. A whole-body PET/CT scan can deliver 14 to 32 mSv depending on the protocol. The dose from fluoroscopy of the stomach is much higher, approximately 50 mSv (14 times the annual background).\n\nAn acute full-body equivalent single exposure dose of 1 Sv (1000 mSv) causes slight blood changes, but 2.0–3.5 Sv (2.0–3.5 Gy) causes very severe syndrome of nausea, hair loss, and hemorrhaging, and will cause death in a sizable number of cases—-about 10% to 35% without medical treatment. A dose of 5 Sv (5 Gy) is considered approximately the LD (lethal dose for 50% of exposed population) for an acute exposure to radiation even with standard medical treatment. A dose higher than 5 Sv (5 Gy) brings an increasing chance of death above 50%. Above 7.5–10 Sv (7.5–10 Gy) to the entire body, even extraordinary treatment, such as bone-marrow transplants, will not prevent the death of the individual exposed (see \"Radiation poisoning\"). (Doses much larger than this may, however, be delivered to selected parts of the body in the course of radiation therapy.)\n\nFor low-dose exposure, for example among nuclear workers, who receive an average yearly radiation dose of 19 mSv, the risk of dying from cancer (excluding leukemia) increases by 2 percent. For a dose of 100 mSv, the risk increase is 10 percent. By comparison, risk of dying from cancer was increased by 32 percent for the survivors of the atomic bombing of Hiroshima and Nagasaki.\n\nThe following table shows radiation quantities in SI and non-SI units:\nThe measure of the ionizing effect of gamma and X-rays in dry air is called the exposure, for which a legacy unit, the röntgen was used from 1928. This has been replaced by kerma, now mainly used for instrument calibration purposes but not for received dose effect. The effect of gamma and other ionizing radiation on living tissue is more closely related to the amount of energy deposited in tissue rather than the ionisation of air, and replacement radiometric units and quantities for radiation protection have been defined and developed from 1953 onwards. These are:\n\nThe conventional distinction between X-rays and gamma rays has changed over time. Originally, the electromagnetic radiation emitted by X-ray tubes almost invariably had a longer wavelength than the radiation (gamma rays) emitted by radioactive nuclei. Older literature distinguished between X- and gamma radiation on the basis of wavelength, with radiation shorter than some arbitrary wavelength, such as 10 m, defined as gamma rays. Since the energy of photons is proportional to their frequency and inversely proportional to wavelength, this past distinction between X-rays and gamma rays can also be thought of in terms of its energy, with gamma rays considered to be higher energy electromagnetic radiation than are X-rays.\n\nHowever, since current artificial sources are now able to duplicate any electromagnetic radiation that originates in the nucleus, as well as far higher energies, the wavelengths characteristic of radioactive gamma ray sources vs. other types now completely overlap. Thus, gamma rays are now usually distinguished by their origin: X-rays are emitted by definition by electrons outside the nucleus, while gamma rays are emitted by the nucleus. Exceptions to this convention occur in astronomy, where gamma decay is seen in the afterglow of certain supernovas, but radiation from high energy processes known to involve other radiation sources than radioactive decay is still classed as gamma radiation.\nFor example, modern high-energy X-rays produced by linear accelerators for megavoltage treatment in cancer often have higher energy (4 to 25 MeV) than do most classical gamma rays produced by nuclear gamma decay. One of the most common gamma ray emitting isotopes used in diagnostic nuclear medicine, technetium-99m, produces gamma radiation of the same energy (140 keV) as that produced by diagnostic X-ray machines, but of significantly lower energy than therapeutic photons from linear particle accelerators. In the medical community today, the convention that radiation produced by nuclear decay is the only type referred to as \"gamma\" radiation is still respected.\n\nDue to this broad overlap in energy ranges, in physics the two types of electromagnetic radiation are now often defined by their origin: X-rays are emitted by electrons (either in orbitals outside of the nucleus, or while being accelerated to produce bremsstrahlung-type radiation), while gamma rays are emitted by the nucleus or by means of other particle decays or annihilation events. There is no lower limit to the energy of photons produced by nuclear reactions, and thus ultraviolet or lower energy photons produced by these processes would also be defined as \"gamma rays\". The only naming-convention that is still universally respected is the rule that electromagnetic radiation that is known to be of atomic nuclear origin is \"always\" referred to as \"gamma rays\", and never as X-rays. However, in physics and astronomy, the converse convention (that all gamma rays are considered to be of nuclear origin) is frequently violated.\n\nIn astronomy, higher energy gamma and X-rays are defined by energy, since the processes that produce them may be uncertain and photon energy, not origin, determines the required astronomical detectors needed. High-energy photons occur in nature that are known to be produced by processes other than nuclear decay but are still referred to as gamma radiation. An example is \"gamma rays\" from lightning discharges at 10 to 20 MeV, and known to be produced by the bremsstrahlung mechanism.\n\nAnother example is gamma-ray bursts, now known to be produced from processes too powerful to involve simple collections of atoms undergoing radioactive decay. This is part and parcel of the general realization that many gamma rays produced in astronomical processes result not from radioactive decay or particle annihilation, but rather in non-radioactive processes similar to X-rays. Although the gamma rays of astronomy often come from non-radioactive events, a few gamma rays in astronomy are specifically known to originate from gamma decay of nuclei (as demonstrated by their spectra and emission half life). A classic example is that of supernova SN 1987A, which emits an \"afterglow\" of gamma-ray photons from the decay of newly made radioactive nickel-56 and cobalt-56. Most gamma rays in astronomy, however, arise by other mechanisms.\n\n\n"}
{"id": "27944144", "url": "https://en.wikipedia.org/wiki?curid=27944144", "title": "Global storm activity of 2006", "text": "Global storm activity of 2006\n\nGlobal storm activity of 2006 profiles the major worldwide storms, including blizzards, ice storms, and other winter events, from January 1, 2006 to December 31, 2006. Winter storms are events in which the dominant varieties of precipitation are forms that only occur at cold temperatures, such as snow or sleet, or a rainstorm where ground temperatures are cold enough to allow ice to form (i.e. freezing rain). It may be marked by strong wind, thunder and lightning (a thunderstorm), heavy precipitation, such as ice (ice storm), or wind transporting some substance through the atmosphere (as in a dust storm, snowstorm, hailstorm, etc.). Other major non winter events such as large dust storms, Hurricanes, cyclones, tornados, gales, flooding and rainstorms are also caused by such phenomena to a lesser or greater existent.\n\nVery rarely, well-defined winter storms may form during the summer, though it would usually have to be an abnormally cold summer, such as the Summer of 1816 in the Northeastern United States. In many locations in the Northern Hemisphere, the most powerful winter storms usually occur in March and, in regions where temperatures are cold enough, April.\n\nSevere Tropical Cyclone Clare was a moderate strength cyclone which hit Western Australia in January 2006. The storm formed as an area of low pressure in the Arafura Sea, on 4 January 2006, and moved westward. It ultimately peaked at Category 3 intensity on the Australian tropical cyclone scale. It moved ashore on the coast of Pilbara and proceeded inland, dissipating on 10 January. Clare produced winds of at Karratha and triggered widespread torrential rainfall that led to flooding. Following its usage, the name \"Clare\" was retired by the Bureau of Meteorology, and will never be used again for a tropical cyclone in the area affected by it. Ahead of the storm's landfall, local and state officials issued a \"red alert\" for several locations along the storm's prdicted path. 2,000 people were evacuated in the Karratha region. In areas between Broome and Port Hedland, people were urged to tidy up debris and organise disaster supplies to prepare for the storm.\n, several ports were closed and some oil rigs were shut down at the time to. There were heavy floods in the affected region and parts of East Timor to.\n\nOn 24 January, a broad area of low pressure developed near the coast of Queensland after a monsoonal trough passed through the region. Northeasterly winds flowing into the system quickly increased convection, resulting in heavy rainfall over coastal regions of Queensland. The slow movement of the developing low continued through 26 January before turning northeast in response to a mid-level ridge to the north. On 28 January, the JTWC began monitoring the system as Tropical Storm 10P and shortly after, the Bureau of Meteorology classified the storm as a Category 1 cyclone and gave it the name Jim. Torrential rainfall affected portions of coastal Queensland between 26 and 27 January. In a 24-hour span, of rain fell in Home Hill, leading to minor flooding. On 28 January, the cyclone brushed Flinders Reef, New Caledonia, Willis Island and Lihou Reef, bringing winds up to to all three areas.\n\nThe TCWC Brisbane issued a gale warning for a Tropical Low near the northern tip of Cape York Peninsula on February 22. The low moved in an easterly direction. It quickly strengthened and became Tropical Cyclone Kate on the same day. Kate moved eastwards and weakened into a tropical low on February 24. Coastal Queensland was badly hit. In the Shire of Noosa, six surfers sustained serious injuries after wading into turbulent waters. Waves up to tossed the six surfers, leaving them with injuries ranging from broken noses and fractured ankles to head wounds from surfboards.\nHeavy sandstorms sweep Mauritania's Sahel on July 12 and 17.\n\n50 houses were damaged with 7 houses completely losing roofs and two people received minor injuries in the suburb of Leschenault in Australind, Western Australia which is located south of Perth. Western Australian Bureau of Meteorology measured the tornado to be a F2 on the Fujita scale with the damage area measuring around 100m by 2000m.\n\nBetween 13 and 29 August, major storm-induced flood hit the Cambodia. On the 13th the Battambang, Pursat and Kampong Thom were the first to be hit. The heavy rainfall started at evening time of 13 August in Kampong Speu Province and ended on the 14th. Kampot was flooded by heavy rain on the 16th along with five affected districts, 92 communes, 482 villages until the 17th. The Cambodian Red Cross Society gave help to the storm's victims. The storm burnt itself out over Thailand and Laos on the 29th. The Stung Sen River and Mekong river burst their banks.\n\nThe major storm-induced flood hit the Cambodian provinces of Kandal, Koh Kong, Kampot, Kampong Speu, Kampong Thom, Battambang, Pursat, Rattanakiri and the municipality of Phnom Penh particularly badly, as were the Thai provinces Amphoe Chiang Saen and Chiang Rai Province. Cambodian officials reported five deaths (two in Kampong Speu and 3 in Kampot). It was said that 252 homes had been flooded, 12 homes had been washed away and about 6,000 families had been evacuated from low-lying and coastal regions that were prone to flooding of this type. The Laotian town of Chiang Saen, Sekong Province and Attapeu Province and Vientiane Prefecture were briefly flooded in places the 29th and 30th.\n\nAn isolated, strong tornado was reported in Remagen in Germany on the evening of August 21. Significant damage was reported in the area as it hit a campground. One person was killed and several others were injured as a result. It was the fourth tornado fatality in Europe in 2006.\n\nWhile not a major event, the first widespread winter weather event took place in the higher elevations of the Northwestern United States and as far south as Utah, and especially across the higher elevations of western Canada. The snow did not affect any of the major cities in the area, but did affect travel. The snow also had a positive impact in that it significantly reduced the number of wildfires in the area.\n\nSuch heavy snowfall is not unusual in September, especially in the northern Rocky Mountains.\n\nAnother storm moved into the Rocky Mountain region, dropping of snow throughout the mountains of Utah, Wyoming, and Colorado. The Black Hills near Deadwood, South Dakota also saw up to a foot of snow. Gothic, Colorado and Alta, Utah both reported .\n\nOn the 8th and 9th 32 died as an unusually heavy rain storm hits Thailand. 43 provinces are flooded, with Chiang Mai Province being the worst off. 1,000 were injured and approximately another 138,000 had been made ill by the polluted water supplies left after the storm had destroyed most of the water channes, seuwerage systems and water pipe lines. The Thai government estimate that of rice fields and farmland have been destroyed.\n\nA low pressure system moving through the Great Lakes region, accompanied by a record-breaking cold snap, combined to produce significant early-season snowfall across the region. Several areas on the Lower Peninsula of Michigan recorded their earliest-ever measurable snowfall, including 0.2\" at Detroit on October 12, beating the old record from October 13, 1909, and of snow fell over western portions of the Upper Peninsula. A foot of snow also fell across portions of southwestern Ontario in the Niagara region with significant amounts also recorded in northwestern Ontario north and west of Thunder Bay.\n\nRecord-breaking snowfall of also occurred in the highly localized lake effect snowband areas around Buffalo, New York, with Buffalo setting two consecutive daily October snowfall records, recording a total of . The resulting heavy, wet snow downed tree limbs and power lines, leaving 350,000 people without electricity in western New York. It also closed a large section of Interstate 90 from Rochester to Dunkirk and killed three people. Governor George Pataki declared a state of emergency in the hard-hit counties. The bands were very localized; very little snow fell in most other areas.\n\nThe first Plains blizzard of the season occurred over the Front Range of Colorado. Blizzard warnings were issued, with of snow combining with winds as strong as in some areas. Snow accumulations in the mountains reached up to . Dozens of school districts were closed and highways were blocked throughout the region. Most flights out of Denver International Airport were either canceled or significantly delayed.\n\nSignificant amounts of snow were also reported across northeastern Ontario and western and central Quebec from October 26 to October 30. Accumulations exceeded locally 20 centimetres (8 inches).\n\nThe Puget Sound area received a Pineapple Express that dumped several inches of rain over the area in a period of four days caused massive flooding, two deaths, and extensive damage to Mount Rainier National Park. The rain contributed significantly towards making November 2006 the wettest on record for Seattle.\n\nThe first major winter storm of the season in the Upper Midwest dumped heavy snow across parts of Minnesota, Wisconsin and the Upper Peninsula of Michigan. The highest amounts were in western Wisconsin, east of the Twin Cities, where up to 16 inches (41 cm) of snow fell. Schools and roads were closed as a result. Portions of Northeastern Ontario, including Greater Sudbury, also received over 15 centimetres on the night of the 10th into the 11th, with moderate snow falling across central Quebec later that day.\n\nA nor'easter impacted parts of South Carolina and Georgia in areas that typically don't receive snow, especially in November. The storm produced thunder snow for a time at Charleston, South Carolina, the only time thunder snow has been reported. Generally 1–2 inches was observed in interior areas from Jenkins County, Georgia to Colleton County, South Carolina. Not only was this a winter weather oddity, it was record setting. Charleston and Savannah, Georgia both observed their earliest snowfall on record. The powerful storm also brought heavy rains, severe beach erosion, and damaging winds to South Carolina and Georgia. This storm also brought snow flurries as far south as central Florida, near Orlando, the earliest that snow had ever been recorded that far south.\n\nA widespread and severe storm complex tracked across the entire northern and central parts of North America in the last week of November. It produced a variety of severe weather, including heavy snow, rain, freezing rain, sleet, high winds, extreme cold, a serial derecho and several tornadoes.\n\nThe most severe impacts were in the Midwest where several fatalities were reported and extensive power outages occurred.\n\nA severe, but localized, lake effect snow event took place in parts of the Great Lakes region. The hardest hit community was London, Ontario, where over 50 cm (20 inches) of snow fell. The heavy snow virtually shut down the community, with many roads and highways closed and even shutting down the transit system for the first time since 1978. Other areas on the leeward side of the Great Lakes saw lesser snowfall amounts.\n\nWhile a severe rain and wind event took place in the Pacific Northwest causing significant damage and power outages, the highland areas saw blizzard conditions, along with hurricane-force winds. Some areas received over 16 inches (40 cm) of snow along with winds in excess of 80 mph (130 km/h). The blizzard also stalled rescue efforts on Mount Hood.\n\nAnother major winter storm slammed into the High Plains and central Rocky Mountains on December 19 and continued through December 21. The storm produced heavy snow across a large area covering six states centered around Denver, Colorado. Areas in the foothills received up to 27 inches (68 cm) of snow, which closed many highways, including several Interstates. The area was crippled as a result, with schools and most businesses closed and the local transit system shut down. The heavy snow also closed Denver International Airport as the Christmas rush began.\n\nSome areas expected up to 3 feet (90 cm) of snow. In addition, up to 7 inches (18 cm) fell as far south as New Mexico. The Four Corners region saw up to 18 inches (45 cm) of snow in the mountains, with up to 6 inches (15 cm) in the valleys.\n\nGovernor Bill Owens declared a state of emergency, which allowed state funds to be used to activate the Colorado National Guard. Four people were killed by the storm.\n\nA rare winter storm blanketed parts of the Middle East including southern Jordan which the area was paralysed due to heavy snow. Numerous roads leading to the area's main cities were shut down. The country's civil and defense teams had to rescue more than 1,400 who were trapped across various areas of the country. Air Force helicopters also assisted in the rescue efforts. No fatalities were reported.\n\nAnother massive blizzard hit the Front Range of Colorado and adjacent Plains areas. Approximately 1–2 feet of snow fell along the Front Range, cancelling many flights and closing some roads, while up to fell in the surrounding foothills and mountains. At least a foot of snow, combined in some areas with up to of freezing rain, fell from the Texas Panhandle north along the High Plains into South Dakota. Ice fell all the way north into Ontario, and from December 31 into January 1, ice fell in northern New England before the storm weakened and exited the coast. The area around Albuquerque, New Mexico saw 1–3 feet of snow, including a record one day snowfall of on December 29. One area in the mountains of New Mexico saw an incredible 58 inches (4 feet, 10 inches). The storm overall brought 16.5 inches to Albuquerque, helping the city achieve its second-highest monthly snowfall total on record. Western Kansas saw up to of snow, and a huge sweep of the central Plains for stranded travelers was undertaken in the days after the storm. 12 people were killed in the storm; 10 in traffic accidents across Colorado, Texas, and Minnesota, 1 from a tornado in Texas, where severe thunderstorms occurred, and 1 from carbon monoxide poisoning from a generator in western Kansas.\n\n"}
{"id": "12924018", "url": "https://en.wikipedia.org/wiki?curid=12924018", "title": "Gubkin Russian State University of Oil and Gas", "text": "Gubkin Russian State University of Oil and Gas\n\nThe Gubkin Russian State University of Oil and Gas () is a university in Moscow. The university was founded on 17 April 1930 and is named after the geologist Ivan Gubkin. The university is affectionally known as Kerosinka (), meaning \"kerosene stove\".\n\nThe institute was part of the Moscow Geological Exploration Institute (MGRI) but later became a separate entity.\n\nDuring the Soviet period, the university, along with the Moscow State University of Railway Engineering, was known for admitting students of Jewish origin while other universities unofficially barred Jewish students.\n\nAffiliates of the Gubkin institute exist in Orenburg, Ashgabat, Turkmenistan and Tashkent, Uzbekistan.\n\nThe work of geologists and geophysicists is complex and multifaceted. Only detailed study of the structure of the Earth's deposits with the use of modern geophysical instruments and computer technologies can confidently identify oil and gas deposits at the depth of several kilometers. The Faculty trains geologists and geophysicists in a variety of areas and specializations of geoscience to meet the industry’s needs.\n\nTraining is provided by a team of highly qualified and renowned researchers. Among them are Russian State Prize Laureates, Honored Scientists of the Russian Federation, Honored Geologists and Geophysicists.\nThe Faculty offers Bachelor, Master and Ph.D. programs.\n\n\nAleksandr V. Lobusev is author of more than 90 scientific papers. He is also the holder of the patent: \"Method of developing oil and gas fields”. He is member of many renowned public organizations: the Society of Petroleum Engineers (SPE); Chairman of the Committee on Science of Gubkin University Academic Council, Public Council of the Ministry of Natural Resources and Ecology of the Russian Federation, full member of the Academy of Natural Sciences of Russia.\n\nHydrocarbons are produced in Siberian blistering cold and in violent African heat, deep under the sea and in the desert. It is the reservoir engineer who plays the major role in this process of development and exploitation of oil and gas deposits. The Faculty of Reservoir Engineering prepares professionals in drilling, well completion and simulation, development and exploitation of on- and offshore oil, gas and gas condensate fields, research in physics and hydraulics. Students of the Faculty receive in-depth knowledge in geology, economics, engineering mechanics, oil field chemicals and computer systems.\n\nThe Faculty offers a variety of Bachelor, Master and Ph.D. programs.\n\n\nProfessor Bondarenko teaches the course on \"Physics of oil and gas reservoir.\" He has published more than 40 scientific works. Professor Bovanenko is the Laureate of the Government of the Russian Federation in the field of Science and Technology, Honorary Person of Higher Professional Education of the Russian Federation.\n\nPipeline transport is the most important part of the fuel and energy complex. Often oil and gas fields are located in remote areas. Therefore, the effectiveness of the oil and gas industry is largely dependent on the reliable and safe operation of pipeline systems. The Faculty provides training in a wide range of subjects and programs related to pipeline engineering and operation.\n\nThe Faculty includes 75 professors giving lectures to the students on the fundamentals and cutting-edge developments in pipeline engineering.\n\nThe Faculty offers Bachelor, Master and Ph.D. programs.\n\n\nProfessor Korolenok is author of over 110 scientific papers and textbooks for students, including 3 monographs and 12 scientific and technical reviews. He is Member of the Scientific and Technical Council of OAO \"Gazprom\". The research results of professor Korolenok were used to develop a number of industry practices and regulations, which were introduced into the practice of gas transportation companies such as \"Gazprom\", “Transneft\", Rosneftegazstroy and others.\n\nThe question “What do the graduates of the Department of Mechanical Engineering do?” can be answered briefly and very clearly: they create machinery and installations for the oil and gas industry, and “teach” these how to work. They design and operate equipment, certify petroleum products and technologies, ensure the security of workers life, manage risks related to industrial production, insure industrial objects and people - our graduates can do all this and many other things.\nThe Faculty offers Bachelor, Master and Ph.D. programs.\n\n\n\nProfessor Prygaev is author of more than 60 scientific papers, co-author of international translator and guide \"Electrodes for manual arc welding\" (2000). In 2012 the International Society for Engineering Education awarded professor Prygaev the title of “International Lecturer in Engineering”. In 2010 professor was awarded the title of the “Honored Person of Higher Education of the Russian Federation”.\n\nThe areas of training offered by the Faculty are extremely diverse. For example, students at the Department of Oil Processing Technologies learn about the technology and processes for the production of petroleum products, explore indicators of quality for oil products and raw materials, develop innovative solutions to improve the energy efficiency of processing plants. The Department of Gas Chemistry looks into the methods for obtaining substances and materials from hydrocarbon gases applying physical and chemical processes. The Department of Chemistry and Technology of Lubricants students master production technologies of motor oils, lubricants and fluids.\n\nThe Department of Technology of Chemicals for the Oil and Gas Industry does research in the field of development of drilling fluids, technologies for enhanced oil recovery and oil production intensification. The Department of Organic and Petroleum Chemistry trains specialists in the fields of hydrocarbon chemistry, heteroatomic and high-molecular compounds of petroleum, as well as their thermal and thermo-catalytic transformations. The Department of Physical and Colloid Chemistry students explore chemical phenomena using theoretical and experimental methods of physics. And finally the Department of Industrial Ecology students can master the environmental protection, waste utilization and other green technologies.\n\nThe Faculty offers Bachelor, Master and Ph.D. programs.\n\n\nProfessor Tonkonogov is corresponding member of the Academy of Natural Sciences (2007), author of over 100 scientific works and inventions. Professor Tonkonogov gives lectures on the theory of chemical and technological processes of basic organic and petrochemical synthesis; theoretical foundations of chemical engineering of fuel energy and carbon materials; technology of production.\n\nThe Faculty of Automation and Computer Engineering prepares specialists in the field of mathematical and computer modeling, design and effective use of computer technology, information-measuring and electrical systems and facilities, tools, automation and control systems. All educational programs of the Faculty are closely related with the tasks of oil and gas industry. The Faculty students receive in-depth training in physics and mathematics, studying information technology, mathematical methods of modeling and analysis of complex systems, acquire knowledge on the technology of designing and programming of modern information-measuring, computing and control systems.\n\nThe Faculty offers Bachelor, Master and Ph.D. programs.\n\n\nProfessor Khrabrov is engaged in scientific research in the field of information-measuring systems for measurement and control of multiphase flow production of oil, gas and gas condensate wells. He conducted research in the Urengoy and Orenburg gas condensate fields. He is author of 22 scientific and educational works and holds 2 patents.\n\nGraduates of the Faculty are able to successfully address challenges of the oil and gas industry. They work in governmental bodies, oil and gas companies, research and development organizations as economists, managers, financiers, marketers related to the oil and gas industry.\n\nThe Faculty of Economics and Management maintains close ties with universities in China, Germany, France, UK, USA, Holland, Norway, Sweden and other countries as we as with the international oil and gas majors such as Gazprom, Rosneft, BP, Statoil and others.\n\nAll educational programs in the Faculty are closely related with the tasks of oil and gas industry.\n\nThe Faculty offers Bachelor, Master and Ph.D. programs.\n\n\nIn line with the globalization process the petroleum business is becoming more and more international. To manage the business in international oil and gas companies the enrolment of professionals with specific knowledge of doing petroleum business internationally is required.\n\nTherefore, becoming a top manager of oil and gas companies requires comprehensive education. The Faculty of International Oil and Gas Business offers Bachelor, Master and Ph.D. programs in the areas of Petroleum Economics and Management, Energy Trading, Energy Logistics, World Economy, Geopolitics and Strategic Resource Management.\n\n\nProfessor Telegina is an expert in the Global Energy Security and World Energy Economics. In 1997- 1999 Dr. Telegina was Deputy Minister of Fuel and Energy of the Russian Federation and was responsible for international cooperation and investment in the energy sector of Russia. She worked as Deputy Chairman of the International Conference of the European Energy Charter. For many years Dr. Telegina has been member of the Council of Russian Oil Exporters Union.\n\nThe study at the Gubkin School of Law gives students general and specific knowledge in the field of jurisprudence. Along with the general legal studies much attention is paid to the specificity of legal activities of enterprises and organizations in the oil and gas industry. The industry professionals from major oil and gas companies such as Gazprom, BP, Shell, TOTAL, and Governmental bodies (Supreme Court, Ministry of Natural Resources and Environment and others) as well as the experts of law and consulting firms (DLA Piper, Noland Consulting, Bureau of labor law) give the lectures in the School.\n\nThe Faculty offers Bachelor, Master and Ph.D. programs.\n\n\nIn addition to training students the School provides the Russian Language training course for international students in the framework of one year Preparatory course.\n\nThe School’s main task is to develop in students’ patriotism, civic responsibility and team spirit, as well as their capability to maintain and contribute to the University and the national oil and gas sector traditions. The School includes the Museum of History of Gubkin Russian State University of Oil and Gas.\n\n\nInternational School of Business offers business education aimed at improving management skills and development of leadership qualities, according to the global standards of training managers for oil, gas and energy business.\nThe knowledge and skills obtained in the School effectively address the challenges faced by senior managers of the international petroleum companies.\n\nThe School offers the Master (MBA) and Doctoral (DBA) Degrees in Business Administration.\n\nProfessor Telegina is an expert in the Global Energy Security and World Energy Economics. In 1997- 1999 Dr. Telegina was Deputy Minister of Fuel and Energy of the Russian Federation and was responsible for international cooperation and investment in the energy sector of Russia. She worked as Deputy Chairman of the International Conference of the European Energy Charter. For many years Dr. Telegina has been member of the Council of Russian Oil Exporters Union.\n\nGubkin offers a wide range of Bachelor (4 years of study) programs related to various fields in the petroleum industry such as geology and petroleum engineering, mining law, petroleum economics and management, law and others.\nAll Bachelor Programs are Russian taught.\n\nTo learn the Russian Language in Gubkin, please refer to the one-year PREPARATORY COURSE Program.\n\nGubkin offers more than 50 Master Programs in all possible areas of Petroleum Industry.\n\nThe duration of Master Programs is 2 years (4 semesters).\n\nThese include regular Gubkin programs which are all Russian taught as well as Joint Master Programs which are English taught.\n\nGubkin offers the opportunity to receive the degree of Candidate of Science (Ph.D.) in 17 scientific areas related to the petroleum industry, such as development, exploration, production, transportation, economics and others.\nIn this scientific framework the University offers 44 programs/majors to do the Ph.D. research in.\n\nGubkin has close ties with petroleum industry around the world. Cooperation with companies includes internship, scholarships and grants for students and young researchers, different forms of sponsorship, joint R&D programs, and guest-lectures of professionals from the industry on challenges and achievements in technology developments and other topics related to the industry.\n\nThe University has cooperation agreements with international oil and gas companies, such as\n\n\n\n\n\nAnd many others.\n\nGubkin University has cooperation agreements with many leading Institutes and Universities around the world.\n\nThe main fields of cooperation lie in the educational and research dimensions.\n\nCooperation in the field of education made it possible to offer Gubkin students the following options:\n\nThe list of Gubkin University partners includes more than 120 universities around the world.\n\nAmong them are:\n\n\n\n\nGubkin University takes part in the International Well Control Forum.\n\nGubkin is a member of the International Association of Drilling Contractors.\n\nGubkin University hosts national UNIDO Industrial Centre of ecological management and clean technologies for petroleum industry.\n\nGubkin representatives take an active part in the work of international professional associations such as:\n\n\n\n\n\n\nThe history of Gubkin University started in 1930 when the Moscow Academy of Mines was reorganized into six Institutes. Thus the Petroleum Engineering Department of the Academy became the base for the new Moscow Petroleum Institute. The Institute was named in honor of Academician Ivan Gubkin, “who contributed greatly to the foundation of higher education aimed at (of training engineers, needed urgently for the socialist industry”. At this time the young Soviet state was in want of fuel and energy for industrial development. The petroleum industry faced new challenges such as the development of new fields, application of advanced methods of operation, introduction of new production technologies, further development of oil refining technologies and construction of new plants, electrification of the oil industry, as well as the planning, organization and improving the safety of petroleum production.\n\nNone of these problems could be solved without significant increase in the number of petroleum engineers and Gubkin Institute became the main base for preparing such specialists.\n\nAt the beginning there were only three faculties at the University, namely: Geology and Oil Exploration, Petroleum Engineering and Refining.\n\nThe first Rector of the University and the founders of its educational and research schools were Ivan M. Gubkin himself and many famous founders of the Russian (USSR) oil and gas science such as: A.P. Krylov, Yu.A. Kosygin, L.S. Leibenzon, S.S. Nametkin, A.V. Topchiev, I.I. Chernyaev. M.I. Varentsov, N.L. Buslenko, M.A. Kapelyushnikov, M.F. Mirchink, N.S. Nametkin, L.V. Pustovalov, K.A. Chepikov, S.F. Fedorov, G.F. Mirchink, Ya.M. Paushkin; V.I. Isagulyants, L.Sh. Davitashvili; and many others.\n\nNowadays there are more than ten teaching faculties at Gubkin preparing specialists in almost every field of the petroleum industry: from petroleum geology and engineering to energy economics and trading.\nSince the University’s establishment more than 85,000 students have been graduated from Gubkin.\n\nThe University is justifiably proud of its alumni working in all leading oil and gas Russian and international companies.\n\n\n\n"}
{"id": "20579367", "url": "https://en.wikipedia.org/wiki?curid=20579367", "title": "Haidach gas storage", "text": "Haidach gas storage\n\nHaidach gas storage is an underground natural gas storage in the town of Haidach near Salzburg, Austria. It is the second largest gas storage facility in Central Europe.\n\nIn 1997, Rohöl-Aufsuchungs Aktiengesellschaft (RAG) discovered the Haidach gas reservoir holding a total gas in place of 4.3 billion cubic meters (bcm). After depletion of the reservoir, it was planned to convert into a gas storage. The contract for use of the Haidach reservoir as a storage for natural gas was signed between RAG, Wingas and Gazprom Export on 13 May 2005. Construction works started in 2005. The gas storage started officially operating on 24 May 2007.\n\nHaidach underground gas storage uses depleted Haidach porous sandstone gas reservoir at a depth of . The operating capacity of the gas storage in the first phase is 1.2 bcm. In April 2011, after completing the second phase, the operating capacity would be doubled. The gas storage is connected to the Austrian and German gas grids at Burghausen/Überackern through the long Austria-Bavaria gas pipeline (ABG).\n\nThe construction of the first stage cost €250 millions.\n\nHaidach gas storage is a joint project of RAG, Wingas and Gazprom Export. It is operated by Austrian energy company EVN.\n"}
{"id": "43647930", "url": "https://en.wikipedia.org/wiki?curid=43647930", "title": "Holweck pump", "text": "Holweck pump\n\nA Holweck pump is a type of vacuum pump that utilises the drag of air molecules against a rotating surface. The modern turbomolecular pump is a more advanced version based on similar operation, and a Holweck pump is often used as the backing pump for it. The Holweck pump can produce a vacuum as low as .\n\nThe device was invented in the early 1920s by Fernand Holweck as part of his apparatus for his work in studying soft X-rays. \n\nIt was manufactured by French scientific instrument maker, Charles Beaudouin.\n\n"}
{"id": "21223842", "url": "https://en.wikipedia.org/wiki?curid=21223842", "title": "ISO 15971", "text": "ISO 15971\n\nISO 15971 is an ISO standard for calorific value measurement of natural gas and its substitutes. The methods it covered does not involve gas composition determination and the related calculations. \n\nThe standard includes the operation of instruments that are used in the measurements and the guidelines in selecting and assessing, as well as the installation and the operation of such instruments.\n\n\n"}
{"id": "33055281", "url": "https://en.wikipedia.org/wiki?curid=33055281", "title": "Inland Petroleum Distribution System", "text": "Inland Petroleum Distribution System\n\nInland Petroleum Distribution System (IPDS) a rapid deployment, general support, bulk fuel storage and pipeline system designed to move bulk fuel forward in a theater of operations. \nThe system has a design throughput of per day based on per minute at 20 hours per operational day. The IPDS system has three primary subsystems: tactical petroleum terminal, pipeline segments, and pump stations. The IPDS was designed by and for the U.S. Army and Marine Corps for use with the U.S. Navy offshore Petroleum Distribution System (OPDS). OPDS tankers are the SS MOUNT WASHINGTON, SS American Osprey, SS Petersburg, and the SS CHESAPEAKE.\n"}
{"id": "23183048", "url": "https://en.wikipedia.org/wiki?curid=23183048", "title": "International Energy Forum", "text": "International Energy Forum\n\nThe International Energy Forum (IEF) is an inter-governmental, non-profit international organisation which aims to foster greater mutual understanding and awareness of common energy interests among its members. The 72 Member Countries of the Forum are signatories to the IEF Charter, which outlines the framework of the global energy dialogue through this inter-governmental arrangement. \n\nThe IEF is the neutral facilitator of informal, open, informed and continuing global energy dialogue. Recognising their interdependence in the field of energy, the member countries of the IEF co-operate under the neutral framework of the Forum to foster greater mutual understanding and awareness of common energy interests in order to ensure global energy security.\n\nThe IEF is unique in that participants not only include IEA and OPEC countries, but also key international actors such as Brazil, China, India, Mexico, Russia, and South Africa. The IEF member countries account for more than 90 percent of global oil and gas supply and demand. \n\nThe Forum's biennial Ministerial Meetings are the world's largest gathering of Energy Ministers. The magnitude and diversity of this engagement is a testament to the position of the IEF as a neutral facilitator and honest broker of solutions in the common interest.\n\nThrough the Forum and its associated events, IEF Ministers, their officials, energy industry executives, and other experts engage in a dialogue of increasing importance to global energy security.\n\nThe IEF is promoted by a permanent Secretariat based in the Diplomatic Quarter of Riyadh, Saudi Arabia. \n\nThe International Energy Forum also coordinates the Joint Organisations Data Initiative (JODI) which is a concrete outcome of the global energy dialogue.\n\nThe International Energy Forum aims to provide a platform for member-states to have access to open discussion and dialogue between countries that make up the global energy market. The Forum aims to gather all aspects of the energy market; producer, consumer and transit states. The goal of the forum is to create better understanding of the market on all sides, and to increase mutual awareness and understanding of existing member states.\n\nThe fundamental aims of the Forum are: \n\n\nThe concept of a systematic producer-consumer dialogue emerged in the 1970s as a part of the general reorganisation in the global political and economic order with energy markets transforming the structure within individual countries as well as power balances and relations between countries. In the wake of the first Gulf War in the early 1990s, consumers and producers recognised their joint interest in the stability of the oil market, creating greater awareness, and understanding sensitivities toward each others interests. \n\nThe Gulf War revealed the importance of a concerted and coordinated global response to an adverse supply shock. On October 1, 1990 at the United Nations General Assembly, Venezuelan President, Carlos Andrés Pérez called for an urgent meeting of producers and consumers under the auspices of the United Nations to help the world face the growing uncertainties and politics of the oil market. With the support of French President François Mitterrand, and Norwegian Prime Minister, Gro Brundtland, political support was gained for the initiation of a Ministerial Seminar of producers and consumers. \n\nEstablished in Paris in July 1991, the International Energy Forum was created in order to stabilise the global energy market after the 1970s energy crisis, and the 1980s oil glut. One of the main priorities of the Forum was to bring together member-states and private corporations in order to increase awareness of national and international interests, and the workings of the market in order to avoid the instabilities of the previous two decades. \n\nHeadquartered in the Diplomatic Quarter of Riyadh, Saudi Arabia, The International Energy Forum is a Secretariat organization. The forum is governed by an executive board which is composed of 31 representatives of ministries of the respective member states. The body is lead by Secretary General, Dr. Sun Xiansheng of China. Xiansheng was appointed on August 1st, 2016.\n\nThe International Energy Forum Secretariat is to ensure that the Forum is promoting a neutral platform for the exchange of information and views regarding conflicts and the future of the energy industry. Another goal of the executive board is to include both public and private entities in the global energy market in order to bring forth multiple viewpoints to the Forum.\n\nAdditional duties that are performed by the Executive Board include organizing all of the Forum's activities. These include all meetings and summits that are put on by the Forum, and also coordinates the Forum's Programme of Work. \n\nIEF Energy Ministers recognized that the exchange and free dissemination of energy market data helps to mitigate uncertainty by improving market transparency and facilitating well-informed decision-making that instils investor conﬁdence, supports market stability and strengthens energy security. The Joint Organisations Data Initiative, coordinated by the IEF since 2005, relies on the combined efforts of the eight JODI partner organisations (APEC, EUROSTAT, GECF, IEA, OLADE, OPEC, and UNSD), and more than 100 national administrations, and industry stakeholders to gather, verify and transmit the ofﬁcial data that populates JODI’s two public databases JODI-Oil and JODI-Gas with key monthly supply and demand indicators.\n\n"}
{"id": "40177372", "url": "https://en.wikipedia.org/wiki?curid=40177372", "title": "Karol Pollak", "text": "Karol Pollak\n\nKarol Franciszek Pollak (November 15, 1859 – December 17, 1928) was a Polish electrotechnician, inventor and businessman.\n\nHe was born in Sanok, Poland. His father was Karol Pollak (1818–80) who was a printer, bookseller and publisher, well known in Sanok. Karol (not to be mistaken with his father) worked in his youth as an electrician and showed great technical skills in it. In 1883 he was employed in the laboratory of British company \"The Patent Utilisation Co\". He designed and recorded his first patents in that period. In 1885 he attended electrotechnics studies at the Royal Polytechnic University in Charlottenburg.\n\nIn Berlin, Pollak ran electrotechnical factory \"G. Wehr Telegraphen-Bau-Anstalt\". Later he returned to Britain to commercialize his patents, which were released under anglicised version of his name, \"Charles Pollak\". In 1886, he became the director of a Paris company of electric tramways of his design. In the meantime he worked on the design of Electrochemical cell. He was very successful in this topic and it made him famous. Later he founded battery factories in Frankfurt, Germany and Liesing, Austria. Many battery-manufacturing companies have licensed his designs.\n\nIn 1899 he founded his own laboratory and proceeded with further research. He obtained 98 patents on his inventions.\n\nIn 1922 he returned to Poland, where a year later he founded a factory in Biała, which exists to the present day. The company started under the name of \"Polskie Towarzystwo Akumulatorowe\" and was co-founded by professor and president of Poland, Ignacy Mościcki. However, Pollak was the first president of this company.\n\nPollak is sometimes referred as the \"Edison of Poland\". In 1925 he received the title doctor honoris causa of Warsaw University of Technology.\n\nHis numerous inventions also cover other areas, among them: electric motors, color printing device, and a type of microphone. His main activity was related to chemical sources of energy - galvanic cells and batteries. He obtained a patent for manufacturing lead-acid batteries.\nHe also has designed commutator and electrolytic rectifiers. In 1895 he was the first to suggest the use of full bridge diode rectification circuit, later known by Leo Graetz. In 1896, Pollak invented the electrolytic capacitor\n\n"}
{"id": "33857502", "url": "https://en.wikipedia.org/wiki?curid=33857502", "title": "Kennicutt–Schmidt law", "text": "Kennicutt–Schmidt law\n\nIn astronomy, the Kennicutt–Schmidt law (or simply Schmidt law) is an empirical relation between the gas density and star formation rate (SFR) in a given region. The relation was first examined by Maarten Schmidt in a 1959 paper where he proposed that the SFR surface density scales as some positive power formula_1 of the local gas surface density. i.e.\nIn general, the SFR surface density formula_3 is in units of solar masses per year per square parsec formula_4 and the gas surface density in grams per square parsec formula_5. Using an analysis of gaseous helium and young stars in the solar neighborhood, the local density of white dwarfs and their luminosity function, and the local helium density, Schmidt suggested a value of formula_6 (and very likely between 1 and 3). All of the data used were gathered from the Milky Way, and specifically the solar-neighborhood.\n\nIn 1989, Robert Kennicutt found that the Hformula_7 intensities of every galaxy in a sample of 7 could be fit with the Schmidt law. More recently, he examined the connection between gas density and SFR for nearly 100 nearby galaxies to estimate a value of formula_8.\n"}
{"id": "52841581", "url": "https://en.wikipedia.org/wiki?curid=52841581", "title": "Kiltera Ogham Stones", "text": "Kiltera Ogham Stones\n\nKiltera Ogham Stones are two ogham stones forming a National Monument located in County Waterford, Ireland.\n\nKiltera Ogham Stones is located in a field on the east bank of the Munster Blackwater, 1.8 km (1.1 mi) west of Aglish.\n\nKiltera Ogham Stones were carved in the 6th/7th centuries AD.\n\nThe larger stone (CIIC 266) dates from c. AD 500–700 and is slate, with quartz veins and measures 132 × 51 × 13 cm. The inscription reads ᚛ᚉᚑᚂᚂᚐᚁᚑᚈ ᚋᚒᚉᚖ ᚂᚒᚌᚐ ᚋᚐᚊᚔ ᚂᚑᚁᚐᚉᚉᚑᚅᚐ᚜ (COLLABOT MUCOI LUGA MAQI LOBACCONA), \"Of Cóelub of the tribe of Lug, son of Lubchú.\"\n\nThe smaller stone (CIIC 267) reads ᚛ᚋᚓᚇᚒᚄᚔ ᚋᚒᚉᚖ ᚂᚒᚌᚐ᚜ (MEDUSI MUCOI LUGA), \"Medusi of the tribe of Lug.\" It measures 130 × 41 × 25 cm.\n\nA third stone (CIIC 268), carved c. AD 540–600, later removed to the National Museum, read ᚛ᚉᚐᚈᚈᚒᚃᚔᚏ᚜ (CATTUVIR).\n"}
{"id": "738149", "url": "https://en.wikipedia.org/wiki?curid=738149", "title": "Kármán line", "text": "Kármán line\n\nThe Kármán line, or Karman line, lies at an altitude of above Earth's sea level and commonly represents the boundary between Earth's atmosphere and outer space. This definition is accepted by the Fédération aéronautique internationale (FAI), which is an international standard-setting and record-keeping body for aeronautics and astronautics.\n\nThe line is named after Theodore von Kármán (1881–1963), a Hungarian American engineer and physicist, who was active primarily in aeronautics and astronautics. He was the first person to calculate that the atmosphere around this altitude becomes too thin to support aeronautical flight, since a vehicle at this altitude would have to travel faster than orbital velocity to derive sufficient aerodynamic lift to support itself. The line is approximately at the turbopause, above which atmospheric gasses are not well-mixed. The mesopause atmospheric temperature minimum has been measured to vary from 85 to 100 km, which places the line at or near the bottom of the thermosphere. \n\nIn the final chapter of his autobiography Kármán addresses the issue of the edge of outer space:\n\nAn atmosphere does not abruptly end at any given height, but becomes progressively thinner with altitude. Also, depending on how the various layers that make up the space around the Earth are defined (and depending on whether these layers are considered part of the actual atmosphere), the definition of the edge of space could vary considerably: If one were to consider the thermosphere and exosphere part of the atmosphere and not of space, one might have to extend the boundary to space to at least above sea level. The Kármán line thus is an arbitrary definition based on the following considerations:\n\nAn aircraft only stays aloft if it constantly travels forward relative to the air (airspeed is not dependent on speed relative to ground), so that the wings can generate lift. The thinner the air, the faster the plane must go to generate enough lift to stay up.\n\nThe amount of lift required at any given point can be calculated by the lift equation:\n\nwhere\n\nLift (\"L\") generated is directly proportional to the air density (\"ρ\"). All other factors remaining unchanged, true airspeed (\"v\") must increase to compensate for less air density (\"ρ\") at higher altitudes.\n\nAn orbiting spacecraft only stays in the sky if the centrifugal component of its movement around the Earth is enough to balance the downward pull of gravity. If it goes slower, the pull of gravity gradually makes its altitude decrease. The required speed is called \"orbital velocity,\" and it varies with the height of the orbit. For the International Space Station, or a space shuttle in low Earth orbit, the orbital velocity is about 27,000 km per hour (17,000 miles per hour).\n\nFor an airplane flying higher and higher, the increasingly thin air provides less and less lift, requiring increasingly higher speed to create enough lift to hold the airplane up. It eventually reaches an altitude where it must fly so fast to generate lift that it reaches orbital velocity. The Kármán line is the altitude where the speed necessary to aerodynamically support the airplane's full weight equals orbital velocity (assuming wing loading of a typical airplane). In practice, supporting full weight wouldn't be necessary to maintain altitude because the curvature of the Earth adds centrifugal lift as the airplane reaches orbital speed. However, the Karman line definition ignores this effect because orbital velocity is implicitly sufficient to maintain any altitude regardless of atmospheric density. The Karman line is therefore the highest altitude at which orbital speed provides sufficient aerodynamic lift to fly in a straight line that doesn't follow the curvature of the Earth's surface.\n\nAbove 100 kilometers the air density is about 1/2,200,000 the density on the surface. At the Karman line, the air density ρ is such that\n\nwhere\n\nAlthough the calculated altitude was not exactly 100 km, Kármán proposed that 100 km be the designated boundary to space, because the round number is more memorable, and the calculated altitude varies minutely as certain parameters are varied. An international committee recommended the 100 km line to the FAI, and upon adoption, it became widely accepted as the boundary to space for many purposes. However, there is still no international legal definition of the demarcation between a country's air space and outer space.\n\nAnother hurdle to strictly defining the boundary to space is the dynamic nature of Earth's atmosphere. For example, at an altitude of , the atmosphere's density can vary by a factor of five, depending on the time of day, time of year, AP magnetic index, and recent solar flux.\n\nThe FAI uses the Kármán line to define the boundary between aeronautics and astronautics:\nThe expression \"edge of space\", is often used (by, for instance, the FAI in some of their publications) to refer to a region below the conventional 100 km boundary to space, which is often meant to include substantially lower regions as well. Thus, certain balloon or airplane flights might be described as \"reaching the edge of space\". In such statements, \"reaching the edge of space\" merely refers to going higher than average aeronautical vehicles commonly would.\n\nIn 1963 Andrew G. Haley discussed the Kármán line in his book \"Space Law and Government\". In a chapter on the limits of national sovereignty, he made a survey of major writers’ views. He indicated the inherent imprecision of the Line:\n\nThe U.S. Air Force definition of an astronaut is a person who has flown higher than above mean sea level, approximately the line between the mesosphere and the thermosphere. NASA formerly used the FAI's figure, though this was changed in 2005, to eliminate any inconsistency between military personnel and civilians flying in the same vehicle, when three veteran NASA X-15 pilots (John B. McKay, William H. Dana and Joseph Albert Walker) were retroactively (two posthumously) awarded their astronaut wings, as they had flown between and in the 1960s, but at the time had not been recognized as astronauts. The latter altitude exceeds the modern international definition of the boundary of space.\n\nTwo recent papers (J.C. McDowell, Harvard-Smithsonian Center for Astrophysics, and T. Gangale, JSD, University of Nebraska-Lincoln)\nadvocate that the demarcation of space should be at , citing as evidence von Kármán's original notes and calculations (which concluded the boundary should be 270,000 ft), plus functional, cultural, physical, technological, mathematical, and historical factors.\n\nAnother definition proposed in international law discussions defines the lower boundary of space as the lowest perigee attainable by an orbiting space vehicle, but does not specify an altitude. Due to atmospheric drag, the lowest altitude at which an object in a circular orbit can complete at least one full revolution without propulsion is approximately , whereas an object can maintain an elliptical orbit with perigee as low as about without propulsion. Above altitudes of approximately the sky is completely black.\n\n\n"}
{"id": "1803019", "url": "https://en.wikipedia.org/wiki?curid=1803019", "title": "Memphis Summer Storm of 2003", "text": "Memphis Summer Storm of 2003\n\nThe Memphis Summer Storm of 2003 was a severe derecho event that affected parts of the Southern United States, particularly southwestern Tennessee and northern Mississippi, including the Memphis metropolitan area. It left 7 people dead and enormous damage across the region.\n\nOn July 22, 2003, a progressive derecho with straight-line winds in excess of 100 miles per hour (160 km/h) struck Crittenden, DeSoto, Fayette, and Shelby Counties, including the city of Memphis. The storm passed through the area between 6 and 7 am.\n\nOver 300,000 homes, 70% of Shelby County, were left without power in the wake of the storm. Two individuals were left dead as a direct result of the storm, with several more deaths due to fires caused by unattended candles or generator accidents.\n\nThis storm was very similar to the derecho that went through Kansas City, Missouri in June 1982, as well as one that hit St. Louis, Missouri on July 19, 2006.\n\nAccording to the respected Ms. Vicky Armstrong at Southwest Tennessee Community College, there were many individuals cutting in line at gas stations because of fears over increased gasoline prices or gasoline shortages.\n\nThe storm became commonly known in the area as \"Hurricane Elvis\" as its winds reached the level of a Category 2 hurricane. As the storm crossed the Mississippi River into Downtown Memphis, a barge recorded an unofficial wind reading of 108 mph (174 km/h). Coincidentally, the National Hurricane Center's rotation of tropical cyclone names had identified that season's \"D\" storm, \"Danny,\" only two days earlier, thus the region's next real hurricane, Hurricane Erika (2003), would also start with an \"E.\" \nThe National Weather Service refers to the storm as the \"Mid South Derecho of 2003\".\n\nIt took two weeks to restore power to many of those who had lost it in the storm. Temperatures reached past 90°F (32°C) leaving many residents unprotected from dangerously hot conditions. Many Memphians were distressed that though they had survived hurricane-like conditions and weeks without power, little national news coverage was given to the event. Shelby County Mayor A.C. Wharton said in a CNBC interview that he was \"Feeling a bit lonely, because it seems that from a national perspective it never happened. We suffered a 'dry-land hurricane'... yet we see on the national news over in Galveston, they got more coverage on a hurricane that never did occur.\" In contrast, less than a month later, national news coverage was extensive when New York City residents were left without power for a few hours.\n\nIn 2009, the United States Department of Homeland Security claimed that the city of Memphis owed the Federal Emergency Management Agency (FEMA) $2 million. It claimed that Memphis misspent the money that was originally given to help clean up the city following Hurricane Elvis.\n\n\n"}
{"id": "52803334", "url": "https://en.wikipedia.org/wiki?curid=52803334", "title": "New physical principles weapons", "text": "New physical principles weapons\n\nNew physical principles weapons are a wide range of weapons or systems created using emerging technologies, like wave, psychophysical, and genetic weapons.\n\nThis definition is similar to \"new types of weapons of mass destruction and new systems of such weapons\" used in documentation from United Nations General Assembly sessions since 1975 and \"non-lethal weapons\" used by the North Atlantic Treaty Organization (NATO).\n\nThe term is currently used primarily in Russia. The \"Encyclopedia of the Russian Ministry of Defense\" identifies the following types of new physical principles weapons that have been developed to varying degrees by the 21st century: \n\nNew types of \"weapons of mass destruction and new systems of such weapons\" were defined by the United Nations General Assembly in 1975. In 1976, the US State Department stated that these weapons are based on \"qualitatively new principles of action\", which can be new due to the nature of the impact, target to be attacked, method of action, or how they are used. Examples given were infrasound weapons designed to damage internal organs and affect human behavior; genetic weapons, the use of which would affect the mechanism of heredity; ray weapons capable of affecting blood and intracellular plasma; robotic military equipment; unmanned controlled aircraft; and weapon systems, like aerospace weapon systems, where nuclear weapons are transported by spaceships and thereby more dangerous.\n\nIn the never-adopted draft treaty of 1975, the proposed language in the United Nations Disarmament Conference classification for \"new weapons of mass description\" to be banned included:\n\nThe Convention on the Prohibition of Military or Any Other Hostile Use of Environmental Modification Techniques of 1977 does not identify specific weapons or technology.\n\nNATO definition of non-lethal weapons include new responsive technologies—like lasers, kinetic and acoustic devices. Examples of non-lethal weapons are counter-personnel and radio-frequency vehicle stopping technologies. The United States Department of Defense's current and future non-lethal weapons programs include active denial, counter-personnel capability that creates a heating sensation, quickly repelling potential adversaries with minimal risk of injury in such missions as force protection, perimeter defense, crowd control, patrols/convoys, defensive and offensive operations.\n\nNew weapons of mass destruction that have harmful effects on people and the ecological balance of the planet are, in essence, criminal and contrary to the Convention on the Prohibition of Military or Any Other Hostile Use of Environmental Modification Techniques of 1977. Rather than identifying specific weapons, the Convention on the Prohibition of Military or Any Other Hostile Use of Environmental Modification Techniques of 1977 prohibits use of \"military or any other hostile use of environmental modification techniques having widespread, long-lasting or severe effects as the means of destruction, damage or injury to any other State Party\". This includes deliberate actions taken to change the atmosphere, hydrosphere, outer space, or in other ways affect the dynamics, structure or composition of the earth.\n\nThe Biological and Toxin Weapons Convention of 1972. The Seventh Review Conference was held in Geneva in December 2011, which resulted in the Final Declaration document that affirmed that \"under all circumstances the use of bacteriological (biological) and toxin weapons is effectively prohibited by the Convention\". The Protocol on Blinding Laser Weapons, Protocol IV of the 1980 Convention on Certain Conventional Weapons, was issued by the United Nations on October 13, 1995 and came into force on July 30, 1998.\n\nRussian military doctrine refers to new physical principles weapons, while describing the main features of these weapons as comparable in effect to nuclear weapons but more acceptable in political terms. It was reported in October 2016 that Russia had tested a new electronic weapon, based on new physical principles, that uses directed-energy to neutralize on-board aircraft equipment, unmanned combat aerial vehicles, and precision weapons. In 2015, it announced a blinding weapon, Rook, that creates a disabling light interference, which can be used against night-vision equipment. It uses ultraviolet, infrared, and visible regions of the spectrum and can change the width and direction of the beam to target specific objects. In November 2016, Putin states that the new physical principles weapons comply with all of the country's international obligations.\n\n"}
{"id": "77474", "url": "https://en.wikipedia.org/wiki?curid=77474", "title": "Nihonium", "text": "Nihonium\n\nNihonium is a synthetic chemical element with the symbol Nh and atomic number 113. It is extremely radioactive; its most stable known isotope, nihonium-286, has a half-life of about 10 seconds. In the periodic table, nihonium is a transactinide element in the p-block. It is a member of period 7 and group 13 (boron group).\n\nNihonium was first reported to have been created in 2003 by a Russian–American collaboration at the Joint Institute for Nuclear Research (JINR) in Dubna, Russia, and in 2004 by a team of Japanese scientists at Riken in Wakō, Japan. The confirmation of their claims in the ensuing years involved independent teams of scientists working in the United States, Germany, Sweden, and China, as well as the original claimants in Russia and Japan. In 2015, the IUPAC/IUPAP Joint Working Party recognised the element and assigned the priority of the discovery and naming rights for the element to Riken, as it judged that they had demonstrated that they had observed element 113 before the JINR team did so. The Riken team suggested the name \"nihonium\" in 2016, which was approved in the same year. The name comes from the common Japanese name for .\n\nVery little is known about nihonium, as it has only been made in very small amounts that decay away within seconds. The anomalously long lives of some superheavy nuclides, including some nihonium isotopes, are explained by the \"island of stability\" theory. Experiments support the theory, with the half-lives of the confirmed nihonium isotopes increasing from milliseconds to seconds as neutrons are added and the island is approached. Nihonium has been calculated to have similar properties to its homologues boron, aluminium, gallium, indium, and thallium. All but boron are post-transition metals, and nihonium is expected to be a post-transition metal as well. It should also show several major differences from them; for example, nihonium should be more stable in the +1 oxidation state than the +3 state, like thallium, but in the +1 state nihonium should behave more like silver and astatine than thallium. Preliminary experiments in 2017 showed that elemental nihonium is not very volatile; its chemistry remains largely unexplored.\n\nThe syntheses of elements 107 to 112 were conducted at the GSI Helmholtz Centre for Heavy Ion Research in Darmstadt, Germany, from 1981 to 1996. These elements were made by cold fusion reactions, in which targets made of thallium, lead, and bismuth, which are around the stable configuration of 82 protons, are bombarded with heavy ions of period 4 elements. This creates fused nuclei with low excitation energies due to the stability of the targets' nuclei, significantly increasing the yield of superheavy elements. Cold fusion was pioneered by Yuri Oganessian and his team in 1974 at the Joint Institute for Nuclear Research (JINR) in Dubna, Soviet Union. Yields from cold fusion reactions were found to decrease significantly with increasing atomic number; the resulting nuclei were severely neutron-deficient and short-lived. The GSI team attempted to synthesise element 113 via cold fusion in 1998 and 2003, bombarding bismuth-209 with zinc-70, but were unsuccessful both times.\n\nFaced with this problem, Oganessian and his team at the JINR turned their renewed attention to the older hot fusion technique, in which heavy actinide targets were bombarded with lighter ions. Calcium-48 was suggested as an ideal projectile, because it is very neutron-rich for a light element (combined with the already neutron-rich actinides) and would minimise the neutron deficiencies of the nuclides produced. Being doubly magic, it would confer benefits in stability to the fused nuclei. In collaboration with the team at the Lawrence Livermore National Laboratory (LLNL) in Livermore, California, United States, they made an attempt on element 114 (which was predicted to be a magic number, closing a proton shell, and more stable than element 113).\n\nIn 1998, the JINR–LLNL collaboration started their attempt on element 114, bombarding a target of plutonium-244 with ions of calcium-48:\n\nA single atom was observed which was thought to be the isotope 114: the results were published in January 1999. Despite numerous attempts to repeat this reaction, an isotope with these decay properties has never again been found, and the exact identity of this activity is unknown. A 2016 paper considered that the most likely explanation of the 1998 result is that two neutrons were emitted by the produced compound nucleus, leading to 114 and electron capture to 113, while more neutrons were emitted in all other produced chains. This would have been the first report of a decay chain from an isotope of element 113, but it was not recognised at the time, and the assignment is still uncertain. A similar long-lived activity observed by the JINR team in March 1999 in the Pu + Ca reaction may be due to the electron-capture daughter of 114, 113; this assignment is also tentative.\n\nThe now-confirmed discovery of element 114 was made in June 1999 when the JINR team repeated the first Pu + Ca reaction from 1998; following this, the JINR team used the same hot fusion technique to synthesise elements 116 and 118 in 2000 and 2002 respectively via the Cm + Ca and Cf + Ca reactions. They then turned their attention to the missing odd-numbered elements, as the odd protons and possibly neutrons would hinder decay by spontaneous fission and result in longer decay chains.\n\nThe first report of element 113 was in August 2003, when it was identified as an alpha decay product of element 115. Element 115 had been produced by bombarding a target of americium-243 with calcium-48 projectiles. The JINR–LLNL collaboration published its results in February 2004:\n\nFour further alpha decays were observed, ending with the spontaneous fission of isotopes of element 105, dubnium.\n\nWhile the JINR–LLNL collaboration had been studying fusion reactions with Ca, a team of Japanese scientists at the Riken Nishina Center for Accelerator-Based Science in Wakō, Japan, led by Kōsuke Morita had been studying cold fusion reactions. Morita had previously studied the synthesis of superheavy elements at the JINR before starting his own team at Riken. In 2001, his team confirmed the GSI's discoveries of elements 108, 110, 111, and 112. They then made a new attempt on element 113, using the same Bi + Zn reaction that the GSI had attempted unsuccessfully in 1998. Despite the much lower yield expected than for the JINR's hot fusion technique with calcium-48, the Riken team chose to use cold fusion as the synthesised isotopes would alpha decay to known daughter nuclides and make the discovery much more certain, and would not require the use of radioactive targets. In particular, the isotope 113 expected to be produced in this reaction would decay to the known Bh, which had been synthesised in 2000 by a team at the Lawrence Berkeley National Laboratory (LBNL) in Berkeley.\n\nThe bombardment of Bi with Zn at Riken began in September 2003. The team detected a single atom of 113 in July 2004 and published their results that September:\n\nThe Riken team observed four alpha decays from 113, creating a decay chain passing through Rg, Mt, and Bh before terminating with the spontaneous fission of Db. The decay data they observed for the alpha decay of Bh matched the 2000 data, lending support for their claim. Spontaneous fission of its daughter Db had not been previously known; the American team had observed only alpha decay from this nuclide.\n\nWhen the discovery of a new element is claimed, the Joint Working Party (JWP) of the International Union of Pure and Applied Chemistry (IUPAC) and the International Union of Pure and Applied Physics (IUPAP) assembles to examine the claims according to their criteria for the discovery of a new element, and decides scientific priority and naming rights for the elements. According to the JWP criteria, a discovery must demonstrate that the element has an atomic number different from all previously observed values. It should also preferably be repeated by other laboratories, although this requirement has been waived where the data is of very high quality. Such a demonstration must establish properties, either physical or chemical, of the new element and establish that they are those of a previously unknown element. The main techniques used to demonstrate atomic number are cross-reactions (creating claimed nuclides as parents or daughters of other nuclides produced by a different reaction) and anchoring decay chains to known daughter nuclides. For the JWP, priority in confirmation takes precedence over the date of the original claim. Both teams set out to confirm their results by these methods.\nIn June 2004 and again in December 2005, the JINR–LLNL collaboration strengthened their claim for the discovery of element 113 by conducting chemical experiments on Db, the final decay product of 115. This was valuable as none of the nuclides in this decay chain were previously known, so that their claim was not supported by any previous experimental data, and chemical experimentation would strengthen the case for their claim, since the chemistry of dubnium is known. Db was successfully identified by extracting the final decay products, measuring spontaneous fission (SF) activities and using chemical identification techniques to confirm that they behave like a group 5 element (dubnium is known to be in group 5). Both the half-life and decay mode were confirmed for the proposed Db which lends support to the assignment of the parent and daughter nuclei to elements 115 and 113 respectively. Further experiments at the JINR in 2005 confirmed the observed decay data.\n\nIn November and December 2004, the Riken team studied the Tl + Zn reaction, aiming the zinc beam onto a thallium rather than a bismuth target, in an effort to directly produce Rg in a cross-bombardment as it is the immediate daughter of 113. The reaction was unsuccessful, as the thallium target was physically weak compared to the more commonly used lead and bismuth targets, and it deteriorated significantly and became non-uniform in thickness. The reasons for this weakness are unknown, given that thallium has a higher melting point than bismuth. The Riken team then repeated the original Bi + Zn reaction and produced a second atom of 113 in April 2005, with a decay chain that again terminated with the spontaneous fission of Db. The decay data were slightly different from those of the first chain: this could have been because an alpha particle escaped from the detector without depositing its full energy, or because some of the intermediate decay products were formed in metastable isomeric states.\n\nIn 2006, a team at the Heavy Ion Research Facility in Lanzhou, China, investigated the Am + Mg reaction, producing four atoms of Bh. All four chains started with an alpha decay to Db; three chains ended there with spontaneous fission, as in the 113 chains observed at Riken, while the remaining one continued via another alpha decay to Lr, as in the Bh chains observed at LBNL.\n\nIn June 2006, the JINR–LLNL collaboration claimed to have synthesised a new isotope of element 113 directly by bombarding a neptunium-237 target with accelerated calcium-48 nuclei:\n\nTwo atoms of 113 were detected. The aim of this experiment had been to synthesise the isotopes 113 and 113 that would fill in the gap between isotopes produced via hot fusion (113 and 113) and cold fusion (113). After five alpha decays, these nuclides would reach known isotopes of lawrencium, assuming that the decay chains were not terminated prematurely by spontaneous fission. The first decay chain ended in fission after four alpha decays, presumably originating from Db or its electron-capture daughter Rf. Spontaneous fission was not observed in the second chain even after four alpha decays. A fifth alpha decay in each chain could have been missed, since Db can theoretically undergo alpha decay, in which case the first decay chain would have ended at the known Lr or No and the second might have continued to the known long-lived Md, which has a half-life of 51.5 days, longer than the duration of the experiment: this would explain the lack of a spontaneous fission event in this chain. In the absence of direct detection of the long-lived alpha decays, these interpretations remain unconfirmed, and there is still no known link between any superheavy nuclides produced by hot fusion and the well-known main body of the chart of nuclides.\n\nThe JWP published its report on elements 113–116 and 118 in 2011. It recognised the JINR–LLNL collaboration as having discovered elements 114 and 116, but did not accept either team's claim to element 113 and did not accept the JINR–LLNL claims to elements 115 and 118. The JINR–LLNL claim to elements 115 and 113 had been founded on chemical identification of their daughter dubnium, but the JWP objected that current theory could not distinguish between group 4 and group 5 elements by their chemical properties with enough confidence to allow this assignment. The decay properties of all the nuclei in the decay chain of element 115 had not been previously characterised before the JINR experiments, a situation which the JWP generally considers \"troublesome, but not necessarily exclusive\", and with the small number of atoms produced with neither known daughters nor cross-reactions the JWP considered that their criteria had not been fulfilled. The JWP did not accept the Riken team's claim either due to inconsistencies in the decay data, the small number of atoms of element 113 produced, and the lack of unambiguous anchors to known isotopes.\n\nIn early 2009, the Riken team synthesised the decay product Bh directly in the Cm + Na reaction to establish its link with 113 as a cross-bombardment. They also established the branched decay of Db, which sometimes underwent spontaneous fission and sometimes underwent the previously known alpha decay to Lr.\n\nIn late 2009, the JINR–LLNL collaboration studied the Bk + Ca reaction in an effort to produce element 117, which would decay to elements 115 and 113 and bolster their claims in a cross-reaction. They were now joined by scientists from Oak Ridge National Laboratory (ORNL) and Vanderbilt University, both in Tennessee, United States, who helped procure the rare and highly radioactive berkelium target necessary to complete the JINR's calcium-48 campaign to synthesise the heaviest elements on the periodic table. Two isotopes of element 117 were synthesised, decaying to element 115 and then element 113:\n\nThe new isotopes 113 and 113 produced did not overlap with the previously claimed 113, 113, and 113, so this reaction could not be used as a cross-bombardment to confirm the 2003 or 2006 claims.\n\nIn March 2010, the Riken team again attempted to synthesise Rg directly through the Tl + Zn reaction with upgraded equipment; they failed again and abandoned this cross-bombardment route.\n\nAfter 450 more days of irradiation of bismuth with zinc projectiles, Riken produced and identified another 113 atom in August 2012. In this case, a series of six alpha decays was observed, leading to an isotope of mendelevium:\n\nThis decay chain differed from the previous observations at Riken mainly in the decay mode of Db, which was previously observed to undergo spontaneous fission, but in this case instead alpha decayed; the alpha decay of Db to Lr is well-known. The team calculated the probability of accidental coincidence to be 10, or totally negligible. The resulting Md atom then underwent electron capture to Fm, which underwent the seventh alpha decay in the chain to the long-lived Cf, which has a half-life of around thirteen years.\n\nThe Bk + Ca experiment was repeated at the JINR in 2012 and 2013 with consistent results, and again at the GSI in 2014. In August 2013, a team of researchers at Lund University in Lund, Sweden, and at the GSI announced that they had repeated the 2003 Am + Ca experiment, confirming the findings of the JINR–LLNL collaboration. The same year, the 2003 experiment had been repeated at the JINR, now also creating the isotope 115 that could serve as a cross-bombardment for confirming their discovery of the element 117 isotope 117, as well as its daughter 113 as part of its decay chain. Further confirmation was published by the team at the LBNL in 2015.\n\nIn December 2015, the conclusions of a new JWP report were published by IUPAC in a press release, in which element 113 was awarded to Riken; elements 115, 117, and 118 were awarded to the collaborations involving the JINR. A joint 2016 announcement by IUPAC and IUPAP had been scheduled to coincide with the publication of the JWP reports, but IUPAC alone decided on an early release because the news of Riken being awarded credit for element 113 had been leaked to Japanese newspapers. For the first time in history, a team of Asian physicists would name a new element. The JINR considered the awarding of element 113 to Riken unexpected, citing their own 2003 production of elements 115 and 113, and pointing to the precedents of elements 103, 104, and 105 where IUPAC had awarded joint credit to the JINR and LBNL. They stated that they respected IUPAC's decision, but reserved determination of their position for the official publication of the JWP reports.\n\nThe full JWP reports were published in January 2016. The JWP recognised the discovery of element 113, assigning priority to Riken. They noted that while the individual decay energies of each nuclide in the decay chain of 113 were inconsistent, their sum was now confirmed to be consistent, strongly suggesting that the initial and final states in 113 and its daughter Db were the same for all three events. The decay of Db to Lr and Md was previously known, firmly anchoring the decay chain of 113 to known regions of the chart of nuclides. The JWP considered that the JINR–LLNL collaborations of 2004 and 2007, producing element 113 as the daughter of element 115, did not meet the discovery criteria as they had not convincingly determined the atomic numbers of their nuclides through cross-bombardments, which were considered necessary since their decay chains were not anchored to previously known nuclides. They also considered that the previous JWP's concerns over their chemical identification of the dubnium daughter had not been adequately addressed. The JWP recognised the JINR–LLNL–ORNL–Vanderbilt collaboration of 2010 as having discovered elements 117 and 115, and accepted that element 113 had been produced as their daughter, but did not give this work shared credit.\n\nAfter the publication of the JWP reports, Sergey Dimitriev, the lab director of the Flerov lab at the JINR where the discoveries were made, remarked that he was happy with IUPAC's decision, mentioning the time Riken spent on their experiment and their good relations with Morita, who had learnt the basics of synthesising superheavy elements at the JINR.\n\nThe sum argument advanced by the JWP in the approval of the discovery of element 113 was later criticised in a May 2016 study from Lund University and the GSI, as it is only valid if no gamma decay or internal conversion takes place along the decay chain, which is not likely for odd nuclei, and the uncertainty of the alpha decay energies measured in the 113 decay chain was not small enough to rule out this possibility. If this is the case, similarity in lifetimes of intermediate daughters becomes a meaningless argument, as different isomers of the same nuclide can have different half-lives: for example, the ground state of Ta has a half-life of hours, but an excited state Ta has never been observed to decay. This study found reason to doubt and criticise the IUPAC approval of the discoveries of elements 115 and 117, but the data from Riken for element 113 was found to be congruent, and the data from the JINR team for elements 115 and 113 to probably be so, thus endorsing the IUPAC approval of the discovery of element 113. Two members of the JINR team published a journal article rebutting these criticisms against the congruence of their data on elements 113, 115, and 117 in June 2017.\n\nUsing Mendeleev's nomenclature for unnamed and undiscovered elements, nihonium should be known as \"eka-thallium\". In 1979, IUPAC published recommendations according to which the element was to be called \"ununtrium\" (with the corresponding symbol of \"Uut\"), a systematic element name as a placeholder, until the discovery of the element is confirmed and a name is decided on. The recommendations were widely used in the chemical community on all levels, from chemistry classrooms to advanced textbooks, but were mostly ignored among scientists in the field, who called it \"element 113\", with the symbol of \"E113\", \"(113)\", or even simply \"113\".\n\nBefore the JWP recognition of their priority, the Japanese team had unofficially suggested various names: \"japonium\", after their home country; \"nishinanium\", after Japanese physicist Yoshio Nishina, the \"founding father of modern physics research in Japan\"; and \"rikenium\", after the institute. After the recognition, the Riken team gathered in February 2016 to decide on a name. Morita expressed his desire for the name to honour the fact that element 113 had been discovered in Japan. \"Japonium\" was considered, making the connection to Japan easy to identify for non-Japanese, but it was rejected as Jap is considered an ethnic slur. The name \"nihonium\" was chosen after an hour of deliberation: it comes from , one of the two Japanese pronunciations for the name of Japan. The discoverers also intended to reference the support of their research by the Japanese people (Riken being almost entirely government-funded), recover lost pride and trust in science among those who were affected by the Fukushima Daiichi nuclear disaster, and honour Japanese chemist Masataka Ogawa's 1908 discovery of rhenium, which he named \"nipponium\" with symbol Np after the other Japanese pronunciation of Japan's name. As Ogawa's claim had not been accepted, the name \"nipponium\" could not be reused for a new element, and its symbol Np had since been used for neptunium. In March 2016, Morita proposed the name \"nihonium\" to IUPAC, with the symbol Nh.\nThe former president of IUPAP, Cecilia Jarlskog, complained at the Nobel Symposium on Superheavy Elements in Bäckaskog Castle, Sweden, in June 2016 about the lack of openness involved in the process of approving new elements, and stated that she believed that the JWP's work was flawed and should be redone by a new JWP. A survey of physicists determined that many felt that the Lund–GSI 2016 criticisms of the JWP report were well-founded, but that the conclusions would hold up if the work was redone, and the new president, Bruce McKellar, ruled that the proposed names should be released in a joint IUPAP–IUPAC press release. Thus, IUPAC and IUPAP publicised the proposal of \"nihonium\" that June, and set a five-month term to collect comments, after which the name would be formally established at a conference. The name was officially approved in November 2016. The naming ceremony for the new element was held in Tokyo, Japan, in March 2017, with Naruhito, Crown Prince of Japan, in attendance.\n\nNihonium has no stable or naturally occurring isotopes. Several radioactive isotopes have been synthesised in the laboratory, either by fusing two atoms or by observing the decay of heavier elements. Eight different isotopes of nihonium have been reported with atomic masses 278, 282–287, and 290 (Nh and Nh are unconfirmed); they all decay through alpha decay to isotopes of roentgenium; there have been indications that nihonium-284 can also decay by electron capture to copernicium-284.\n\nThe stability of nuclei quickly decreases with the increase in atomic number after curium, element 96, whose half-life is over ten thousand times longer than that of any subsequent element. All isotopes with an atomic number above 101 undergo radioactive decay with half-lives of less than 30 hours: this is because of the ever-increasing Coulomb repulsion of protons, so that the strong nuclear force cannot hold the nucleus together against spontaneous fission for long. Calculations suggest that in the absence of other stabilising factors, elements with more than 103 protons should not exist. Researchers in the 1960s suggested that the closed nuclear shells around 114 protons and 184 neutrons should counteract this instability, and create an \"island of stability\" containing nuclides with half-lives reaching thousands or millions of years. The existence of the island is still unproven, but the existence of the superheavy elements (including nihonium) confirms that the stabilising effect is real, and in general the known superheavy nuclides become longer-lived as they approach the predicted location of the island.\n\nAll nihonium isotopes are unstable and radioactive; the heavier nihonium isotopes are more stable than the lighter ones, as they are closer to the centre of the island. The most stable known nihonium isotope, Nh, is also the heaviest; it has a half-life of 8 seconds. The isotope Nh, as well as the unconfirmed Nh and Nh, have also been reported to have half-lives of over a second. The isotopes Nh and Nh have half-lives of 1 and 0.1 seconds respectively. The remaining two isotopes have half-lives between 0.1 and 100 milliseconds: Nh has a half-life of 70 milliseconds, and Nh, the lightest known nihonium isotope, is also the shortest-lived, with a half-life of 1.4 milliseconds. This rapid increase in the half-lives near the closed neutron shell at \"N\" = 184 is seen in roentgenium, copernicium, and nihonium (elements 111 through 113), where each extra neutron so far multiplies the half-life by a factor of 5 to 20.\n\nNihonium is the first member of the 7p series of elements and the heaviest group 13 element on the periodic table, below boron, aluminium, gallium, indium, and thallium. All the group 13 elements except boron are metals, and nihonium is expected to follow suit. Nihonium is predicted to show many differences from its lighter homologues. The major reason for this is the spin–orbit (SO) interaction, which is especially strong for the superheavy elements, because their electrons move much faster than in lighter atoms, at velocities close to the speed of light. In relation to nihonium atoms, it lowers the 7s and the 7p electron energy levels (stabilising those electrons), but two of the 7p electron energy levels are stabilised more than the other four. The stabilisation of the 7s electrons is called the inert pair effect, and the separation of the 7p subshell into the more and less stabilised parts is called subshell splitting. Computational chemists see the split as a change of the second, azimuthal quantum number \"l\", from 1 to 1/2 and 3/2 for the more and less stabilised parts of the 7p subshell, respectively. For theoretical purposes, the valence electron configuration may be represented to reflect the 7p subshell split as 7s 7p. The first ionisation energy of nihonium is expected to be 7.306 eV, the highest among the metals of group 13. Similar subshell splitting should exist for the 6d electron levels, with four being 6d and six being 6d. Both these levels are raised to be close in energy to the 7s ones, high enough to possibly be chemically active. This would allow for the possibility of exotic nihonium compounds without lighter group 13 analogues.\n\nPeriodic trends would predict nihonium to have an atomic radius larger than that of thallium due to it being one period further down the periodic table, but calculations suggest nihonium has an atomic radius of about 170 pm, the same as that of thallium, due to the relativistic stabilisation and contraction of its 7s and 7p orbitals. Thus nihonium is expected to be much denser than thallium, with a predicted density of about 16 to 18 g/cm compared to thallium's 11.85 g/cm, since nihonium atoms are heavier than thallium atoms but have the same volume. Bulk nihonium is expected to have a hexagonal close-packed crystal structure, like thallium. The melting and boiling points of nihonium have been predicted to be 430 °C and 1100 °C respectively, exceeding the values for gallium, indium, and thallium, following periodic trends.\n\nThe chemistry of nihonium is expected to be very different from that of thallium. This difference stems from the spin–orbit splitting of the 7p shell, which results in nihonium being between two relatively inert closed-shell elements (copernicium and flerovium), an unprecedented situation in the periodic table. Nihonium is expected to be less reactive than thallium, because of the greater stabilisation and resultant chemical inactivity of the 7s subshell in nihonium compared to the 6s subshell in thallium. The standard electrode potential for the Nh/Nh couple is predicted to be 0.6 V, so nihonium should be a rather noble metal, as unreactive as rhodium and ruthenium.\n\nThe metallic group 13 elements are typically found in two oxidation states: +1 and +3. The former results from the involvement of only the single p electron in bonding, and the latter results in the involvement of all three valence electrons, two in the s-subshell and one in the p-subshell. Going down the group, bond energies decrease and the +3 state becomes less stable, as the energy released in forming two additional bonds and attaining the +3 state is not always enough to outweigh the energy needed to involve the s-electrons. Hence, for aluminium and gallium +3 is the most stable state, but +1 gains importance for indium and by thallium it becomes more stable than the +3 state. Nihonium is expected to continue this trend and have +1 as its most stable oxidation state.\n\nThe simplest possible nihonium compound is the monohydride, NhH. The bonding is provided by the 7p electron of nihonium and the 1s electron of hydrogen. The SO interaction causes the binding energy of nihonium monohydride to be reduced by about 1 eV and the nihonium–hydrogen bond length to decrease as the bonding 7p orbital is relativistically contracted. This is unique among the 7p element monohydrides; all the others have relativistic expansion of the bond length instead of contraction. Another effect of the SO interaction is that the Nh–H bond is expected to have significant pi bonding character (side-on orbital overlap), unlike the almost pure sigma bonding (head-on orbital overlap) in thallium monohydride (TlH). The analogous monofluoride (NhF) should also exist. Nihonium(I) is predicted to be more similar to silver(I) than thallium(I): the Nh ion is expected to more willingly bind anions, so that NhCl should be quite soluble in excess hydrochloric acid or ammonia; TlCl is not. In contrast to Tl, which forms the strongly basic hydroxide (TlOH) in solution, the Nh cation should instead hydrolyse all the way to the amphoteric oxide NhO, which would be soluble in aqueous ammonia and weakly soluble in water.\n\nThe adsorption behaviour of nihonium on gold surfaces in thermochromatographical experiments is expected to be closer to that of astatine than that of thallium. The destabilisation of the 7p subshell effectively leads to a valence shell closing at the 7s 7p configuration rather than the expected 7s 7p configuration with its stable octet. As such, nihonium, like astatine, can be considered to be one p-electron short of a closed valence shell. Hence, even though nihonium is in group 13, it has several properties similar to the group 17 elements. (Tennessine in group 17 has some group-13-like properties, as it has three valence electrons outside the 7s 7p closed shell.) Nihonium is expected to be able to gain an electron to attain this closed-shell configuration, forming the −1 oxidation state like the halogens (fluorine, chlorine, bromine, iodine, and astatine). This state should be more stable than it is for thallium as the SO splitting of the 7p subshell is greater than that for the 6p subshell. Nihonium should be the most electronegative of the metallic group 13 elements, even more electronegative than tennessine, the period 7 congener of the halogens: in the compound NhTs, the negative charge is expected to be on the nihonium atom rather than the tennessine atom. The −1 oxidation should be more stable for nihonium than for tennessine. The electron affinity of nihonium is calculated to be around 0.68 eV, higher than thallium's at 0.4 eV; tennessine's is expected to be 1.8 eV, the lowest in its group. It is theoretically predicted that nihonium should have an enthalpy of sublimation around 150 kJ/mol and an enthalpy of adsorption on a gold surface around −159 kJ/mol.\n\nSignificant 6d involvement is expected in the Nh–Au bond, although it is expected to be more unstable than the Tl–Au bond and entirely due to magnetic interactions. This raises the possibility of some transition metal character for nihonium. On the basis of the small energy gap between the 6d and 7s electrons, the higher oxidation states +3 and +5 have been suggested for nihonium. Some simple compounds with nihonium in the +3 oxidation state would be the trihydride (NhH), trifluoride (NhF), and trichloride (NhCl). These molecules are predicted to be T-shaped and not trigonal planar as their boron analogues are: this is due to the influence of the 6d electrons on the bonding. The heavier nihonium tribromide (NhBr) and triiodide (NhI) are trigonal planar due to the increased steric repulsion between the peripheral atoms; accordingly, they do not show significant 6d involvement in their bonding, though the large 7s–7p energy gap means that they show reduced sp hybridisation compared to their boron analogues.\n\nThe bonding in the lighter NhX molecules can be considered as that of a linear species (similar to HgF or ) with an additional Nh–X bond involving the 7p orbital of nihonium perpendicular to the other two ligands. These compounds are all expected to be highly unstable towards the loss of an X molecule and reduction to nihonium(I):\nNihonium thus continues the trend down group 13 of reduced stability of the +3 oxidation state, as all five of these compounds have lower reaction energies than the unknown thallium(III) iodide. The +3 state is stabilised for thallium in anionic complexes such as , and the presence of a possible vacant coordination site on the lighter T-shaped nihonium trihalides is expected to allow a similar stabilisation of and perhaps .\n\nThe +5 oxidation state is unknown for all lighter group 13 elements: calculations predict that nihonium pentahydride (NhH) and pentafluoride (NhF) should have a square pyramidal molecular geometry, but also that both would be highly thermodynamically unstable to loss of an X molecule and reduction to nihonium(III). Despite its instability, the possible existence of nihonium pentafluoride is entirely due to relativistic effects allowing the 6d electrons to participate in the bonding. Again, some stabilisation is expected for anionic complexes, such as . The structures of the nihonium trifluoride and pentafluoride molecules are the same as those for chlorine trifluoride and pentafluoride.\n\nThe chemical characteristics of nihonium have yet to be determined unambiguously. The isotopes Nh, Nh, and Nh have half-lives long enough for chemical investigation. From 2010 to 2012, some preliminary chemical experiments were performed at the JINR to determine the volatility of nihonium. The isotope Nh was investigated, made as the daughter of Mc produced in the Am+Ca reaction. The nihonium atoms were synthesised in a recoil chamber and then carried along polytetrafluoroethylene (PTFE) capillaries at 70 °C by a carrier gas to the gold-covered detectors. About ten to twenty atoms of Nh were produced, but none of these atoms were registered by the detectors, suggesting either that nihonium was similar in volatility to the noble gases (and thus diffused away too quickly to be detected) or, more plausibly, that pure nihonium was not very volatile and thus could not efficiently pass through the PTFE capillaries. Formation of the hydroxide NhOH should ease the transport, as nihonium hydroxide is expected to be more volatile than elemental nihonium, and this reaction could be facilitated by adding more water vapour into the carrier gas. It seems likely that this formation is not kinetically favoured, so the longer-lived isotopes Nh and Nh were considered more desirable for future experiments.\n\nA 2017 experiment at the JINR, producing Nh and Nh via the Am+Ca reaction as the daughters of Mc and Mc, avoided this problem by removing the quartz surface, using only PTFE. No nihonium atoms were observed after chemical separation, implying an unexpectedly large retention of nihonium atoms on PTFE surfaces. This experimental result for the interaction limit of nihonium atoms with a PTFE surface disagrees significantly with previous theory, which expected a lower value of 14.00 kJ/mol. This suggests that the nihonium species involved in the previous experiment was likely not elemental nihonium but rather nihonium hydroxide, and that high-temperature techniques such as vacuum chromatography would be necessary to further probe the behaviour of elemental nihonium. Bromine saturated with boron tribromide has been suggested as a carrier gas for experiments on nihonium chemistry; this oxidises nihonium's lighter congener thallium to thallium(III), providing an avenue to investigate the oxidation states of nihonium, similar to earlier experiments done on the bromides of group 5 elements, including the superheavy dubnium.\n\n"}
{"id": "776619", "url": "https://en.wikipedia.org/wiki?curid=776619", "title": "Ocean gyre", "text": "Ocean gyre\n\nIn oceanography, a gyre () is any large system of circulating ocean currents, particularly those involved with large wind movements. Gyres are caused by the Coriolis effect; planetary vorticity along with horizontal and vertical friction, determine the circulation patterns from the \"wind stress curl\" (torque).\n\nThe term \"gyre\" can be used to refer to any type of vortex in the air or the sea, even one that is man-made, but it is most commonly used in oceanography to refer to the major ocean systems.\n\nThe following are the five most notable gyres:\n\n\nTropical gyres are less unified and tend to be mostly east-west with minor north-south extent.\n\nThe center of a subtropical gyre is a high pressure zone. Circulation around the high pressure is clockwise in the northern hemisphere and counterclockwise in the southern hemisphere, due to the Coriolis effect. The high pressure in the center is due to the westerly winds on the northern side of the gyre and easterly trade winds on the southern side. These cause frictional surface currents towards the latitude at the center of the gyre.\n\nThis build-up of water in the center creates flow towards the equator in the upper of the ocean, through rather complex dynamics. This flow is returned towards the pole in an intensified western boundary current. The boundary current of the North Atlantic Gyre is the Gulf Stream, of the North Pacific Gyre the Kuroshio Current, of the South Atlantic Gyre the Brazil Current, of the South Pacific Gyre the East Australian Current, and of the Indian Ocean Gyre the Agulhas Current.\n\nSubpolar gyres form at high latitudes (around 60°). Circulation of surface wind and ocean water is counterclockwise in the Northern Hemisphere, around a low-pressure area, such as the persistent Aleutian Low and the Icelandic Low. Surface currents generally move outward from the center of the system. This drives the Ekman transport, which creates an upwelling of nutrient-rich water from the lower depths.\n\nSubpolar circulation in the southern hemisphere is dominated by the Antarctic Circumpolar Current, due to the lack of large landmasses breaking up the Southern Ocean. There are minor gyres in the Weddell Sea and the Ross Sea, the Weddell Gyre and Ross Gyre, which circulate in a clockwise direction.\n\nRecently, stronger winds, especially the subtropical trade winds in the Pacific ocean have provided a mechanism for vertical heat distribution. The effects are changes in the ocean currents, increasing the subtropical overturning, which are also related to the El Niño and La Niña phenomena. Depending on natural variability, during La Niña years around 30% more heat from the upper ocean layer is transported into the deeper ocean. Several studies in recent years, found a multidecadal increase in OHC of the deep and upper ocean regions and attribute the heat uptake to anthropogenic warming.\n\nOcean gyres are known to collect pollutants. The Great Pacific Garbage Patch in the Northern Pacific is an island of marine debris, believed to have been collected by the North Pacific Gyre.\n\n\n"}
{"id": "4923796", "url": "https://en.wikipedia.org/wiki?curid=4923796", "title": "Plasma channel", "text": "Plasma channel\n\nA plasma channel is a conductive channel of plasma. A plasma channel can be formed in these ways:\nA plasma channel has a low electrical resistance and, once formed, will permit continuous current flow if the energy source that heats the plasma can be maintained. Unlike a normal electrical conductor, the resistance (and voltage drop) across an unconfined plasma channel decreases with increasing current flow, a property called negative resistance. As a result, an electric spark that initially required a very high voltage to initiate avalanche breakdown within the insulating gas will rapidly evolve into a hot, low-voltage electric arc if the electrical power source can continue to deliver sufficient power to the arc. Plasma channels tend to self constrict (see plasma pinch) due to magnetic forces stemming from the current flowing through the plasma.\n\nOn Earth plasma channels are most frequently encountered in lightning storms.\n\n"}
{"id": "1464827", "url": "https://en.wikipedia.org/wiki?curid=1464827", "title": "Plateau (mathematics)", "text": "Plateau (mathematics)\n\nA plateau of a function is a part of its domain where the function has constant value.\n\nMore formally, let \"U\", \"V\" be topological spaces. A plateau for a function \"f\": \"U\" → \"V\" is a path-connected set of points \"P\" of \"U\" such that for some \"y\" we have\nfor all \"p\" in \"P\".\n\nPlateaus can be observed in mathematical models as well as natural systems. In nature, plateaus can be observed in physical, chemical and biological systems. An example of an observed plateau in the natural world is in the tabulation of biodiversity of life through time. \n"}
{"id": "1611987", "url": "https://en.wikipedia.org/wiki?curid=1611987", "title": "Plug-in hybrid", "text": "Plug-in hybrid\n\nA plug-in hybrid electric vehicle (PHEV) is a hybrid electric vehicle whose battery can be recharged by plugging it into an external source of electric power, as well by its on-board engine and generator. Most PHEVs are passenger cars, but there are also PHEV versions of commercial vehicles and vans, utility trucks, buses, trains, motorcycles, scooters, and military vehicles.\n\nSimilarly to all-electric vehicles, plug-in hybrids displace emissions from the car tailpipe to the generators powering the electricity grid. These generators may be renewable, or may have lower emission than an internal combustion engine. Charging the battery from the grid can cost less than using the on-board engine, helping to reduce operating cost. \n\nMass-produced plug-in hybrids were available to the public in China and the United States in 2010. By the end of 2016, there were over 30 models of series-production highway legal plug-in hybrids for retail sales. Plug-in hybrid cars are available mainly in the United States, Canada, Western Europe, Japan, and China. Top-selling models were the Chevrolet Volt family, the Mitsubishi Outlander P-HEV, and the Toyota Prius PHV.\n\n, the global stock of plug-in hybrid cars totaled almost 800,000 units, out of over two million light-duty plug-in electric cars on the world roads at the end of 2016. , the United States ranked as the world's largest plug-in hybrid car market with a stock of 193,770 units, followed by China with 86,580 vehicles, the Netherlands with 78,160, Japan with 55,470 units, and the UK with 28,250.\n\nA plug-in hybrid's all-electric range is designated by PHEV-\"[miles]\" or PHEV\"[kilometers]\"km in which the number represents the distance the vehicle can travel on battery power alone. For example, a PHEV-20 can travel twenty miles (32 km) without using its combustion engine, so it may also be designated as a PHEV32km.<br>\nFor these cars to be battery operated, they go through charging processes that use different currents. These currents are known as Alternating Current (AC) used for on board chargers and Direct Current (DC) used for external charging.<br>\n\nOther popular terms sometimes used for plug-in hybrids are \"grid-connected hybrids\", \"Gas-Optional Hybrid Electric Vehicle\" (GO-HEV) or simply \"gas-optional hybrids\". GM calls its Chevrolet Volt series plug-in hybrid an \"Extended-Range Electric Vehicle\".\n\nThe Lohner-Porsche Mixte Hybrid, produced as early as 1899, was the first hybrid electric car. Early hybrids could be charged from an external source before operation. However, the term \"plug-in hybrid\" has come to mean a hybrid vehicle that can be charged from a standard electrical wall socket. The term \"plug-in hybrid electric vehicle\" was coined by UC Davis Professor Andrew Frank, who has been called the \"father of the modern plug-in hybrid\". The July 1969 issue of \"Popular Science\" featured an article on the General Motors XP-883 plug-in hybrid. The concept commuter vehicle housed six lead–acid batteries in the trunk area and a transverse-mounted DC electric motor turning a front-wheel drive. The car could be plugged into a standard North American 120 volt AC outlet for recharging.\n\nIn 2003, Renault began selling the Elect'road, a plug-in series hybrid version of their popular Kangoo, in Europe. In addition to its engine, it could be plugged into a standard outlet and recharged to 95% range in about 4 hours. After selling about 500 vehicles, primarily in France, Norway and the UK, at a price of about €25,000, the Elect'road was redesigned in 2007.\n\nWith the availability of hybrid vehicles and the rising gas prices in the United States starting around 2004, interest in plug-in hybrids increased. Some plug-in hybrids were conversions of existing hybrids; for example, the 2004 CalCars converision of a Prius to add lead acid batteries and a range of up to using only electric power.\n\nIn 2006, both Toyota and General Motors announced plans for plug-in hybrids. GM's Saturn Vue project was cancelled, but the Toyota plug-in was certified for road use in Japan in 2007. \n\nOn September 5, 2007 Quantum Technologies and Fisker Coachbuild, LLC announced the launch of a joint venture in Fisker Automotive. Fisker intended to build a US$80,000 luxury PHEV-50, the Fisker Karma, initially scheduled for late 2009.\n\nIn 2007, Aptera Motors announced their Typ-1 two-seater. However, the company folded in December 2011.\n\nIn 2007, Chinese car manufacturer BYD Auto, owned by China's largest mobile phone battery maker, announced it would be introducing a production PHEV-60 sedan in China in the second half of 2008. BYD exhibited it January 2008 at the North American International Auto Show in Detroit. Based on BYD's midsize F6 sedan, it uses lithium iron phosphate (LiFeP0)-based batteries instead of lithium-ion, and can be recharged to 70% of capacity in just 10 minutes.\n\nIn 2007 Ford delivered the first Ford Escape Plug-in Hybrid of a fleet of 20 demonstration PHEVs to Southern California Edison. As part of this demonstration program Ford also developed the first flexible-fuel plug-in hybrid SUV, which was delivered in June 2008. This demonstration fleet of plug-ins has been in field testing with utility company fleets in the U.S. and Canada, and during the first two years since the program began, the fleet has logged more than 75,000 miles. In August 2009 Ford delivered the first Escape Plug-in equipped with intelligent vehicle-to-grid (V2G) communications and control system technology, and Ford plans to equip all 21 plug-in hybrid Escapes with the vehicle-to-grid communications technology. Sales of the Escape PHEV are scheduled for 2012.\n\nOn January 14, 2008, Toyota announced they would start sales of lithium-ion battery PHEVs by 2010, but later in the year Toyota indicated they would be offered to commercial fleets in 2009.\n\nOn March 27, the California Air Resources Board (CARB) modified their regulations, requiring automobile manufacturers to produce 58,000 plug-in hybrids during 2012 through 2014. This requirement is an asked-for alternative to an earlier mandate to produce 25,000 pure zero-emissions vehicles, reducing that requirement to 5,000. On June 26, Volkswagen announced that they would be introducing production plug-ins based on the Golf compact. Volkswagen uses the term 'TwinDrive' to denote a PHEV. In September, Mazda was reported to be planning PHEVs. On September 23, Chrysler announced that they had prototyped a plug-in Jeep Wrangler and a Chrysler Town and Country mini-van, both PHEV-40s with series powertrains, and an all-electric Dodge sports car, and said that one of the three vehicles would go into production.\n\nOn October 3, the U.S. enacted the Energy Improvement and Extension Act of 2008. The legislation provided tax credits for the purchase of plug-in electric vehicles of battery capacity over 4 kilowatt-hours. The federal tax credits were extended and modified by the American Clean Energy and Security Act of 2009, but now the battery capacity must be over 5 kWh and the credit phases out after the automaker has sold at least 200,000 vehicles in the U.S.\n\nOn December 15, 2008 BYD Auto began selling its F3DM in China, becoming the first production plug-in hybrid sold in the world, though initially was available only for corporate and government customers. Sales to the general public began in Shenzhen in March 2010, but because the F3DM nearly doubles the price of cars that run on conventional fuel, BYD expects subsidies from the local government to make the plug-in affordable to personal buyers. Toyota tested 600 pre-production Prius Plug-ins in Europe and North America in 2009 and 2010.\n\nVolvo Cars built two demonstration versions of Volvo V70 Plug-in Hybrids in 2009 but did not proceed with production. The V60 plug-in hybrid was released in 2011 and was available for sale.\n\nIn October 2010 Lotus Engineering unveiled the Lotus CityCar, a plug-in series hybrid concept car designed for flex-fuel operation on ethanol, or methanol as well as regular gasoline. The lithium battery pack provides an all-electric range of , and the 1.2-liter flex-fuel engine kicks in to allow to extend the range to more than .\n\nGM officially launched the Chevrolet Volt in the U.S. on November 30, 2010, and retail deliveries began in December 2010. Its sibling the Opel/Vauxhall Ampera was launched in Europe between late 2011 and early 2012. The first deliveries of the Fisker Karma took place in July 2011, and deliveries to retail customers began in November 2011. The Toyota Prius Plug-in Hybrid was released in Japan in January 2012, followed by the United States in February 2012. Deliveries of the Prius PHV in Europe began in late June 2012. The Ford C-Max Energi was released in the U.S. in October 2012, the Volvo V60 Plug-in Hybrid in Sweden by late 2012.\n\nThe Honda Accord Plug-in Hybrid was released in selected U.S. markets in January 2013, and the Mitsubishi Outlander P-HEV in Japan in January 2013, becoming the first SUV plug-in hybrid in the market. Deliveries of the Ford Fusion Energi began in February 2013. BYD Auto stopped production of its BYD F3DM due to low sales, and its successor, the BYD Qin, began sales in Costa Rica in November 2013, with sales in other countries in Latin America scheduled to begin in 2014. Qin deliveries began in China in mid December 2013. [\n\nDeliveries to retail customers of the limited edition McLaren P1 supercar began in the UK in October 2013, and the Porsche Panamera S E-Hybrid began deliveries in the U.S. in November 2013. The first retail deliveries of the Cadillac ELR took place in the U.S. in December 2013. The BMW i8 and the limited edition Volkswagen XL1 were released to retail customers in Germany in June 2014. The Porsche 918 Spyder was also released in Europe and the U.S. in 2014. The first units of the Audi A3 Sportback e-tron and Volkswagen Golf GTE were registered in Germany in August 2014.\n\nIn December 2014 BMW announced the group is planning to offer plug-in hybrid versions of all its core-brand models using eDrive technology developed for its BMW i brand plug-in vehicles (BMW i3 and BMW i8). The goal of the company is to use plug-in technology to continue offering high performance vehicles while reducing emissions below 100g/km. At the time of the announcement the carmaker was already testing a BMW 3 Series plug-in hybrid prototype. The first model available for retail sales will be the 2016 BMW X5 eDrive, with the production version unveiled at the 2015 Shanghai Motor Show. The second generation Chevrolet Volt was unveiled at the January 2015 North American International Auto Show, and retail deliveries began in the U.S. and Canada in October 2015.\n\nIn March 2015 Audi said they planned on making a plug-in hybrid version of every model series, and that they expect plug-in hybrids, together with natural gas vehicles and battery-electric drive systems, to have a key contribution in achieving the company's targets. The Audi Q7 e-tron will follow the A3 e-tron already in the market. Also in March 2015, Mercedes-Benz announced that the company's main emphasis regarding alternative drives in the next years will be on plug-in hybrids. The carmaker plans to introduce 10 new plug-in hybrid models by 2017, and its next release was the Mercedes-Benz C 350 e, Mercedes’ second plug-in hybrid after the S 500 Plug-In Hybrid. Other plug-in hybrid released in 2015 are the BYD Tang, Volkswagen Passat GTE, Volvo XC90 T8, and the Hyundai Sonata PHEV.\n\nGlobal combined Volt/Ampera family sales passed the 100,000 unit milestone in October 2015. By the end of 2015, over 517,000 highway legal plug-in hybrid electric cars have been sold worldwide since December 2008 out of total global sales of more than 1.25 million light-duty plug-in electric cars.\n\nIn February 2016, BMW announced the introduction of the \"iPerformance\" model designation, which will be given to all BMW plug-in hybrid vehicles from July 2016. The aim is to provide a visible indicator of the transfer of technology from BMW i to the BMW core brand. The new designation will be used first on the plug-in hybrid variants of the new BMW 7 Series, the BMW 740e iPerformance, and the 3 Series, the BMW 330e iPerformance.\n\nHyundai Motor Company made the official debut of its three model Hyundai Ioniq line-up at the 2016 Geneva Motor Show. The Ioniq family of electric drive vehicles includes the Ioniq Plug-in, which is expected to achieve a fuel economy of in all-electric mode. The Ioniq Plug-in is scheduled to be released in the U.S. in the fourth quarter of 2017.\n\nThe second generation Prius plug-in hybrid, called Prius Prime in the U.S. and Prius PHV in Japan, was unveiled at the 2016 New York International Auto Show. Retail deliveries of the Prius Prime began in the U.S. in November 2016, and is scheduled to be released Japan by the end of 2016. The Prime has an EPA-rated all-electric range of , over twice the range of the first generation model, and an EPA rated fuel economy of in all-electric mode (EV mode), the highest MPGe rating in EV mode of any vehicle rated by EPA. Unlike its predecessor, the Prime runs entirely on electricity in EV mode. Global sales of the Mitsubishi Outlander P-HEV passed the 100,000 unit milestone in March 2016. BYD Qin sales in China reached the 50,000 unit milestone in April 2016, becoming the fourth plug-in hybrid to pass that mark.\n\nIn June 2016, Nissan announced it will introduce a compact range extender car in Japan before March 2017. The series plug-in hybrid will use a new hybrid system, dubbed e-Power, which debuted with the Nissan Gripz concept crossover showcased at the 2015 Frankfurt Auto Show.\n\nIn December 2017, Honda began retail deliveries of the Honda Clarity Plug-In Hybrid in the United States and Canada, with an EPA rated electric-only range of .\n\nPHEVs are based on the same three basic powertrain architectures of conventional hybrids; a \"series hybrid\" is propelled by electric motors only, a \"parallel hybrid\" is propelled both by its engine and by electric motors operating concurrently, and a \"series-parallel hybrid\" operates in either mode. While a plain hybrid vehicle charges its battery from its engine only, a plug-in hybrid can obtain a significant amount of the energy required to recharge its battery from external sources.\n\nThe battery charger can be on-board or external to the vehicle. The process for an on-board charger is best explained as AC power being converted into DC power, resulting in the battery being charged. On-board chargers are limited in capacity by their weight and size, and by the limited capacity of general-purpose AC outlets. Dedicated off-board chargers can be as large and powerful as the user can afford, but require returning to the charger; high-speed chargers may be shared by multiple vehicles.\n\nUsing the electric motor's inverter allows the motor windings to act as the transformer coils, and the existing high-power inverter as the AC-to-DC charger. As these components are already required on the car, and are designed to handle any practical power capability, they can be used to create a very powerful form of on-board charger with no significant additional weight or size. AC Propulsion uses this charging method, referred to as \"reductive charging\".\n\nA plug-in hybrid operates in charge-depleting and charge-sustaining modes. Combinations of these two modes are termed blended mode or mixed-mode. These vehicles can be designed to drive for an extended range in all-electric mode, either at low speeds only or at all speeds. These modes manage the vehicle's battery discharge strategy, and their use has a direct effect on the size and type of battery required:\n\nCharge-depleting mode allows a fully charged PHEV to operate exclusively (or depending on the vehicle, almost exclusively, except during hard acceleration) on electric power until its battery state of charge is depleted to a predetermined level, at which time the vehicle's internal combustion engine or fuel cell will be engaged. This period is the vehicle's all-electric range. This is the only mode that a battery electric vehicle can operate in, hence their limited range.\n\nMixed mode describes a trip using a combination of multiple modes. For example, a car may begin a trip in low speed charge-depleting mode, then enter onto a freeway and operate in blended mode. The driver might exit the freeway and drive without the internal combustion engine until all-electric range is exhausted. The vehicle can revert to a charge sustaining-mode until the final destination is reached. This contrasts with a charge-depleting trip which would be driven within the limits of a PHEV's all-electric range.\n\nThe optimum battery size varies depending on whether the aim is to reduce fuel consumption, running costs, or emissions, but a recent study concluded that \"The best choice of PHEV battery capacity depends critically on the distance that the vehicle will be driven between charges. Our results suggest that for urban driving conditions and frequent charges every 10 miles or less, a low-capacity PHEV sized with an AER (all electric range) of about 7 miles would be a robust choice for minimizing gasoline consumption, cost, and greenhouse gas emissions. For less frequent charging, every 20–100 miles, PHEVs release fewer GHGs, but HEVs are more cost effective. \"\n\nPHEVs typically require deeper battery charging and discharging cycles than conventional hybrids. Because the number of full cycles influences battery life, this may be less than in traditional HEVs which do not deplete their batteries as fully. However, some authors argue that PHEVs will soon become standard in the automobile industry. Design issues and trade-offs against battery life, capacity, heat dissipation, weight, costs, and safety need to be solved. Advanced battery technology is under development, promising greater energy densities by both mass and volume, and battery life expectancy is expected to increase.\n\nThe cathodes of some early 2007 lithium-ion batteries are made from lithium-cobalt metal oxide. This material is expensive, and cells made with it can release oxygen if overcharged. If the cobalt is replaced with iron phosphates, the cells will not burn or release oxygen under any charge. At early 2007 gasoline and electricity prices, the a break-even point is reached after six to ten years of operation. The payback period may be longer for plug-in hybrids, because of their larger, more expensive batteries.\n\nNickel–metal hydride and lithium-ion batteries can be recycled; Toyota, for example, has a recycling program in place under which dealers are paid a US$200 credit for each battery returned. However, plug-in hybrids typically use larger battery packs than comparable conventional hybrids, and thus require more resources. Pacific Gas and Electric Company (PG&E) has suggested that utilities could purchase used batteries for backup and load leveling purposes. They state that while these used batteries may be no longer usable in vehicles, their residual capacity still has significant value. More recently, General Motors (GM) has said it has been \"approached by utilities interested in using recycled Volt batteries as a power storage system, a secondary market that could bring down the cost of the Volt and other plug-in vehicles for consumers\".\n\nUltracapacitors (or \"supercapacitors\") are used in some plug-in hybrids, such as AFS Trinity's concept prototype, to store rapidly available energy with their high power density, in order to keep batteries within safe resistive heating limits and extend battery life. The CSIRO's UltraBattery combines a supercapacitor and a lead acid battery in a single unit, creating a hybrid car battery that lasts longer, costs less and is more powerful than current technologies used in plug-in hybrid electric vehicles (PHEVs).\n\nThere are several companies that are converting fossil fuel non-hybrid vehicles to plug-in hybrids:\n\nAftermarket conversion of an existing production hybrid to a plug-in hybrid ) typically involves increasing the capacity of the vehicle's battery pack and adding an on-board AC-to-DC charger. Ideally, the vehicle's powertrain software would be reprogrammed to make full use of the battery pack's additional energy storage capacity and power output.\n\nMany early plug-in hybrid electric vehicle conversions have been based on the Toyota Prius. Some of the systems have involved replacement of the vehicle's original NiMH battery pack and its electronic control unit. Others add an additional battery back onto the original battery pack. \n\nIn recent years, demand for all- electric vehicles, especially in the United States market, has been driven by government incentives through subsidies, lobbyists, and taxes. In particular, American sales of the Nissan Leaf have depended on generous incentives and special treatment in the state of Georgia, the top selling Leaf market. According to international market research, 60% of respondents believe a battery driving range of less than is unacceptable even though only 2% drive more than that distance per day. Among popular current all-electric vehicles, only the Tesla (with the most expensive version of the Model S offering a range in the U.S. Environmental Protection Agency 5-cycle test) significantly exceeds this threshold. The Nissan Leaf has an EPA rated range of for the 2013 model year.\n\nPlug-in hybrids provide the extended range and potential for refueling of conventional hybrids while enabling drivers to use battery electric power for at least a significant part of their typical daily driving. The average trip to or from work in the United States in 2009 was , while the average distance commuted to work in England and Wales in 2011 was slightly lower at . Since building a PHEV with a longer all-electric range adds weight and cost, and reduces cargo and/or passenger space, there is not a specific all-electric range that is optimal. The accompanying graph shows the observed all-electric range, in miles, for four popular U.S. market plug-in hybrids, as tested by Popular Mechanics magazine.\n\nA key design parameter of the Chevrolet Volt was a target of for the all-electric range, selected to keep the battery size small and lower costs, and mainly because research showed that 78% of daily commuters in the U.S. travel or less. This target range would allow most travel to be accomplished electrically driven and the assumption was made that charging will take place at home overnight. This requirement translated using a lithium-ion battery pack with an energy storage capacity of 16 considering that the battery would be used until the state of charge (SOC) of the battery reached 30%.\n\nIn October 2014 General Motors reported, based on data collected through its OnStar telematics system since Volt deliveries began, and with over 1 billion miles (1.6 billion km) traveled, that Volt owners drive about 62.5% of their trips in all-electric mode. In May 2016, Ford reported, based on data collected from more than 610 million miles (976 million km) logged by its electrified vehicles through its telematics system, that drivers of these vehicles run an average of annually on their vehicles, with about half of those miles operating in all-electric mode. A break down of these figures show an average daily commute of for Ford Energi plug-in hybrid drivers. Ford notes that with the enhanced electric range of the 2017 model year model, the average Fusion Energi commuter could go the entire day using no gasoline, if the car is fully charged both, before leaving for work and before leaving for home. According to Ford data, currently most customers are likely charging their vehicles only at home.\n\nThe 2015 edition of the EPA's annual report \"Light-Duty Automotive Technology, Carbon Dioxide Emissions, and Fuel Economy Trends\" estimates the following utility factors for 2015 model year plug-in hybrids to represent the percentage of miles that will be driven using electricity by an average driver, whether in electric only or blended modes: 83% for the BMW i3 REx, 66% for the Chevrolet Volt, 45% for the Ford Energi models, 43% for the McLaren P1, 37% for the BMW i8, and 29% for the Toyota Prius PHV. A 2014 analysis conducted by the Idaho National Laboratory using a sample of 21,600 all-electric cars and plug-in hybrids, found that Volt owners traveled on average 9,112 miles in all-electric mode (e-miles) per year, while Leaf owners traveled 9,697 e-miles per year, despite the Volt's shorter all-electric range, about half of the Leaf's.\n\nBetween January and August 2014, a period during which US sales of conventional hybrids slowed, US sales of plug-in hybrids grew from 28,241 to 40,748 compared to the same period in 2013. US sales of all-electric vehicles also grew during the same period: from 29,917 vehicles in the January to August 2013 period to 40,349 in January to August 2014.\n\nPlug-in hybrids have the potential to be even more efficient than conventional hybrids because a more limited use of the PHEV's internal combustion engine may allow the engine to be used at closer to its maximum efficiency. While a Prius is likely to convert fuel to motive energy on average at about 30% efficiency (well below the engine's 38% peak efficiency), the engine of a PHEV-70 would be likely to operate far more often near its peak efficiency because the batteries can serve the modest power needs at times when the combustion engine would be forced to run well below its peak efficiency. The actual efficiency achieved depends on losses from electricity generation, inversion, battery charging/discharging, the motor controller and motor itself, the way a vehicle is used (its duty cycle), and the opportunities to recharge by connecting to the electrical grid.\n\nEach kilowatt hour of battery capacity in use will displace up to of petroleum fuels per year (gasoline or diesel fuels). Also, electricity is multi-sourced and, as a result, it gives the greatest degree of energy resilience.\n\nThe actual fuel economy for PHEVs depends on their powertrain operating modes, their all-electric range, and the amount of driving between charges. If no gasoline is used the miles per gallon gasoline equivalent (MPG-e) depends only on the efficiency of the electric system. The first mass production PHEV available in the U.S. market, the 2011 Chevrolet Volt, with an EPA rated all-electric range of , and an additional gasoline-only extended range of has an EPA combined city/highway fuel economy of 93 MPG-e in all-electric mode, and in gasoline-only mode, for an overall combined gas-electric fuel economy rating of equivalent (MPG-e). The EPA also included in the Volt's fuel economy label a table showing fuel economy and electricity consumed for five different scenarios: 30, 45, 60 and driven between a full charge, and a never charge scenario. According to this table the fuel economy goes up to equivalent (MPG-e) with driven between full charges.\n\nFor the more comprehensive fuel economy and environment label that will be mandatory in the U.S. beginning in model year 2013, the National Highway Traffic Safety Administration (NHTSA) and Environmental Protection Agency (EPA) issued two separate fuel economy labels for plug-in hybrids because of their design complexity, as PHEVS can operate in two or three operating modes: all-electric, blended, and gasoline-only. One label is for series hybrid or extended range electric vehicle (like the Chevy Volt), with all-electric and gasoline-only modes; and a second label for blended mode or series-parallel hybrid, that includes a combination of both gasoline and plug-in electric operation; and gasoline only, like a conventional hybrid vehicle.\n\nThe Society of Automotive Engineers (SAE) developed their recommended practice in 1999 for testing and reporting the fuel economy of hybrid vehicles and included language to address PHEVs. An SAE committee is currently working to review procedures for testing and reporting the fuel economy of PHEVs. The Toronto Atmospheric Fund tested ten retrofitted plug-in hybrid vehicles that achieved an average of 5.8 litres per 100 kilometre or 40.6 miles per gallon over six months in 2008, which was considered below the technology's potential.\n\nIn real world testing using normal drivers, some Prius PHEV conversions may not achieve much better fuel economy than HEVs. For example, a plug-in Prius fleet, each with a all-electric range, averaged only in a test in Seattle, and similar results with the same kind of conversion battery models at Google's RechargeIT initiative. Moreover, the additional battery pack costs –.\n\nA study published in 2014 by researchers from Lamar University, Iowa State University and Oak Ridge National Laboratory compared the operating costs of plug-in hybrid electric vehicles (PHEVs) of various electric ranges (10, 20, 30, and 40 miles) with conventional gasoline vehicles and hybrid-electric vehicles (HEVs) for different payback periods, considering different charging infrastructure deployment levels and gasoline prices. The study concluded that:\n\nDisadvantages of plug-in hybrids include the additional cost, weight, and size of a larger battery pack. According to a 2010 study by the National Research Council, the cost of a lithium-ion battery pack is about /kW·h of usable energy, and considering that a PHEV-10 requires about 2.0 kW·h and a PHEV-40 about 8 kW·h, the estimated manufacturer cost of the battery pack for a PHEV-10 is around and it goes up to for a PHEV-40. According to the same study, even though costs are expected to decline by 35% by 2020, market penetration is expected to be slow and therefore PHEVs are not expected to significantly impact oil consumption or carbon emissions before 2030, unless a fundamental breakthrough in battery technologies occurs.\n\nAccording to the 2010 NRC study, although a mile driven on electricity is cheaper than one driven on gasoline, lifetime fuel savings are not enough to offset plug-ins' high upfront costs, and it will take decades before the break even point is achieved. Furthermore, hundreds of billions of dollars in government subsidies and incentives are likely to be required to achieve a rapid plug-in market penetration in the U.S.\n\nA 2013 study by the American Council for an Energy-Efficient Economy reported that battery costs came down from per kilowatt hour in 2007 to per kilowatt hour in 2012. The U.S. Department of Energy has set cost targets for its sponsored battery research of per kilowatt hour in 2015 and per kilowatt hour by 2022. Cost reductions through advances in battery technology and higher production volumes will allow plug-in electric vehicles to be more competitive with conventional internal combustion engine vehicles.\n\nA study published in 2011 by the Belfer Center, Harvard University, found that the gasoline costs savings of plug-in electric cars over the vehicles’ lifetimes do not offset their higher purchase prices. This finding was estimated comparing their lifetime net present value at 2010 purchase and operating costs for the U.S. market, and assuming no government subidies. According to the study estimates, a PHEV-40 is more expensive than a conventional internal combustion engine, while a battery electric vehicle (BEV) is more expensive. The study also examined how this balance will change over the next 10 to 20 years, assuming that battery costs will decrease while gasoline prices increase. Under the future scenarios considered, the study found that BEVs will be significantly less expensive than conventional cars ( to cheaper), while PHEVs, will be more expensive than BEVs in almost all comparison scenarios, and only less expensive than conventional cars in a scenario with very low battery costs and high gasoline prices. BEVs are simpler to build and do not use liquid fuel, while PHEVs have more complicated powertrains and still have gasoline-powered engines.\n\nIncreased pollution is expected to occur in some areas with the adoption of PHEVs, but most areas will experience a decrease. A study by the ACEEE predicts that widespread PHEV use in heavily coal-dependent areas would result in an increase in local net sulfur dioxide and mercury emissions, given emissions levels from most coal plants currently supplying power to the grid. Although clean coal technologies could create power plants which supply grid power from coal without emitting significant amounts of such pollutants, the higher cost of the application of these technologies may increase the price of coal-generated electricity. The net effect on pollution is dependent on the fuel source of the electrical grid (fossil or renewable, for example) and the pollution profile of the power plants themselves. Identifying, regulating and upgrading single point pollution source such as a power plant—or replacing a plant altogether—may also be more practical. From a human health perspective, shifting pollution away from large urban areas may be considered a significant advantage.\n\nAccording to a 2009 study by The National Academy of Science, \"Electric vehicles and grid-dependent (plug-in) hybrid vehicles showed somewhat higher nonclimate damages than many other technologies.\" Efficiency of plug-in hybrids is also impacted by the overall efficiency of electric power transmission. Transmission and distribution losses in the USA were estimated at 7.2% in 1995 and 6.5% in 2007. By life cycle analysis of air pollution emissions, natural gas vehicles are currently the lowest emitter.\n\nThe additional electrical consumption to recharge the plug-in vehicles could push many households in areas that do not have off-peak tariffs into the higher priced tier and negate financial benefits. Customers under such tariffs could see significant savings by being careful about when the vehicle was charged, for example, by using a timer to restrict charging to off-peak hours. Thus, an accurate comparison of the benefit requires each household to evaluate its current electrical usage tier and tariffs weighed against the cost of gasoline and the actual observed operational cost of electric mode vehicle operation.\n\nThe effect of PHEVs on greenhouse emissions is complex. Plug-in hybrid vehicles operating on all-electric mode do not emit harmful tailpipe pollutants from the onboard source of power. The clean air benefit is usually local because depending on the source of the electricity used to recharge the batteries, air pollutant emissions are shifted to the location of the generation plants. In the same way, PHEVs do not emit greenhouse gases from the onboard source of power, but from the point of view of a well-to-wheel assessment, the extent of the benefit also depends on the fuel and technology used for electricity generation. From the perspective of a full life cycle analysis, the electricity used to recharge the batteries must be generated from zero-emission sources such as renewable (e.g. wind power, solar energy or hydroelectricity) or nuclear power for PEVs to have almost none or zero well-to-wheel emissions. On the other hand, when PEVs are recharged from coal-fired plants, they usually produce slightly more greenhouse gas emissions than internal combustion engine vehicles. In the case of plug-in hybrid electric vehicle when operating in hybrid mode with assistance of the internal combustion engine, tailpipe and greenhouse emissions are lower in comparison to conventional cars because of their higher fuel economy.\n\nIn 2009 researchers at Argonne National Laboratory adapted their GREET model to conduct a full well-to-wheels (WTW) analysis of energy use and greenhouse gas (GHG) emissions of plug-in hybrid electric vehicles for several scenarios, considering different on-board fuels and different sources of electricity generation for recharging the vehicle batteries. Three US regions were selected for the analysis, California, New York, and Illinois, as these regions include major metropolitan areas with significant variations in their energy generation mixes. The full cycle analysis results were also reported for the US generation mix and renewable electricity to examine cases of average and clean mixes, respectively This 2009 study showed a wide spread of petroleum use and GHG emissions among the different fuel production technologies and grid generation mixes. The following table summarizes the main results:\n\nThe Argonne study found that PHEVs offered reductions in petroleum energy use as compared with regular hybrid electric vehicles. More petroleum energy savings and also more GHG emissions reductions were realized as the all-electric range increased, except when electricity used to recharged was dominated by coal or oil-fired power generation. As expected, electricity from renewable sources realized the largest reductions in petroleum energy use and GHG emissions for all PHEVs as the all-electric range increased. The study also concluded that plug-in vehicles that employ biomass-based fuels (biomass-E85 and -hydrogen) may not realize GHG emissions benefits over regular hybrids if power generation is dominated by fossil sources.\nA 2008 study by researchers at Oak Ridge National Laboratory analyzed oil use and greenhouse gas (GHG) emissions of plug-in hybrids relative to hybrid electric vehicles under several scenarios for years 2020 and 2030. The study considered the mix of power sources for 13 U.S. regions that would be used during recharging of vehicles, generally a combination of coal, natural gas and nuclear energy, and to a lesser extend renewable energy. A 2010 study conducted at Argonne National Laboratory reached similar findings, concluding that PHEVs will reduce oil consumption but could produce very different greenhouse gas emissions for each region depending on the energy mix used to generate the electricity to recharge the plug-in hybrids. \n\nIn October 2014, the U.S. Environmental Protection Agency published the 2014 edition of its annual report \"Light-Duty Automotive Technology, Carbon Dioxide Emissions, and Fuel Economy Trends\". For the first time, the report presents an analysis of the impact of alternative fuel vehicles, with emphasis in plug-in electric vehicles because as their market share is approaching 1%, PEVs began to have a measurable impact on the U.S. overall new vehicle fuel economy and emissions.\n\nEPA's report included the analysis of 12 all-electric passengers cars and 10 plug-in hybrids available in the market as model year 2014. For purposes of an accurate estimation of emissions, the analysis took into consideration the differences in operation between those PHEVs like the Chevrolet Volt that can operate in all-electric mode without using gasoline, and those that operate in a blended mode like the Toyota Prius PHV, which uses both energy stored in the battery and energy from the gasoline tank to propel the vehicle, but that can deliver substantial all-electric driving in blended mode. In addition, since the all-electric range of plug-in hybrids depends on the size of the battery pack, the analysis introduced a utility factor as a projection, on average, of the percentage of miles that will be driven using electricity (in electric only and blended modes) by an average driver. The following table shows the overall EV/hybrid fuel economy expressed in terms of miles per gallon gasoline equivalent (mpg-e) and the utility factor for the ten MY2014 plug-in hybrids available in the U.S. market. The study used the utility factor (since in pure EV mode there are no tailpipe emissions) and the EPA best estimate of the tailpipe emissions produced by these vehicles in real world city and highway operation based on the EPA 5-cycle label methodology, using a weighted 55% city/45% highway driving. The results are shown in the following table.\n\nIn addition, the EPA accounted for the upstream emissions associated with the production and distribution of electricity required to charge the PHEVs. Since electricity production in the United States varies significantly from region to region, the EPA considered three scenarios/ranges with the low end of the range corresponding to the California powerplant emissions factor, the middle of the range represented by the national average powerplant emissions factor, and the upper end of the range corresponding to the powerplant emissions factor for the Rockies. The EPA estimates that the electricity GHG emission factors for various regions of the country vary from 346 g CO2/kW-hr in California to 986 g CO2/kW-hr in the Rockies, with a national average of 648 g CO2/kW-hr. The following table shows the tailpipe emissions and the combined tailpipe and upstream emissions for each of the 10 MY 2014 PHEVs available in the U.S. market.\n\nMost emission analysis use average emissions rates across regions instead of marginal generation at different times of the day. The former approach does not take into account the generation mix within interconnected electricity markets and shifting load profiles throughout the day. An analysis by three economist affiliated with the National Bureau of Economic Research (NBER), published in November 2014, developed a methodology to estimate marginal emissions of electricity demand that vary by location and time of day across the United States. The study used emissions and consumption data for 2007 through 2009, and used the specifications for the Chevrolet Volt (all-electric range of ). The analysis found that marginal emission rates are more than three times as large in the Upper Midwest compared to the Western U.S., and within regions, rates for some hours of the day are more than twice those for others. Applying the results of the marginal analysis to plug-in electric vehicles, the NBER researchers found that the emissions of charging PEVs vary by region and hours of the day. In some regions, such as the Western U.S. and Texas, emissions per mile from driving PEVs are less than those from driving a hybrid car. However, in other regions, such as the Upper Midwest, charging during the recommended hours of midnight to 4 a.m. implies that PEVs generate more emissions per mile than the average car currently on the road. The results show a fundamental tension between electricity load management and environmental goals as the hours when electricity is the least expensive to produce tend to be the hours with the greatest emissions. This occurs because coal-fired units, which have higher emission rates, are most commonly used to meet base-level and off-peak electricity demand; while natural gas units, which have relatively low emissions rates, are often brought online to meet peak demand. This pattern of fuel shifting explains why emission rates tend to be higher at night and lower during periods of peak demand in the morning and evening.\n\nSince 2008, plug-in hybrids have been commercially available from both specialty manufacturers and from main-stream producers of internal combutsion engine vehicles. \n\n, the global stock of highway-capable plug-in hybrid electric cars totaled 517,100 units, out of total cumulative global sales of 1.257 million light-duty plug-in electric vehicles (41.1%). The global ratio between all-electrics (BEVs) and plug-in hybrids (PHEVs) has consistently been 60:40 between 2014 and the first half of 2016, mainly due to the large all-electric market in China. In the U.S. and Europe, the ratio is approaching a 50:50 split.\n\nGlobal sales of plug-in hybrids grew from over 300 units in 2010 to almost 9,000 in 2011, jumped to over 60,000 in 2012, and reached almost 222,000 in 2015. , the United States was the world's largest plug-in hybrid car market with a stock of 193,770 units, followed by China with 86,580 vehicles, the Netherlands with 78,160, Japan with 55,470 units, and the UK with 28,250. About 279,000 light-duty plug-in hybrids were sold in 2016, raising the global stock to almost 800,000 highway legal plug-in hybrid electric cars, out of over 2 million light-duty plug-in electric cars on the world roads at the end of 2016.\n\nThe Netherlands, Sweden, the UK, and the United States have the largest shares of plug-in hybrid sales as percentage of total plug-in electric passenger vehicle sales. The Netherlands has the world's largest share of plug-in hybrids among its plug-in electric passenger car stock, with 86,162 plug-in hybrids registered at the end of October 2016, out of 99,945 plug-in electric cars and vans, representing 86.2% of the country's stock of light-duty plug-in electric vehicles. Sweden ranks next with 16,978 plug-in hybrid cars sold between 2011 and August 2016, representing 71.7% of total plug-in electric passenger car sales registrations. Plug-in hybrid registrations in the UK between up to August 2016 totaled 45,130 units representing 61.6% of total plug-in car registrations since 2011. In the United States, plug-in hybrids represent 47.2% of the 506,450 plug-in electric cars sold between 2008 and August 2016.\n\nIn November 2013 the Netherlands became the first country where a plug-in hybrid topped the monthly ranking of new car sales. During November sales were led by the Mitsubishi Outlander P-HEV with 2,736 units, capturing a market share of 6.8% of new passenger cars sold that month. Again in December 2013 the Outlander P-HEV ranked as the top selling new car in the country with 4,976 units, representing a 12.6% market share of new car sales. These record sales allowed the Netherlands to become the second country, after Norway, where plug-in electric cars have topped the monthly ranking of new car sales. , the Netherlands was the country with highest plug-in hybrid market concentration, with 1.45 vehicles registered per 1,000 people. Most of the initial growth of the Dutch plug-in hybrid stock took place in 2013, with 20,164 units sold that year representing a rate of growth of 365% from 2012. Another surge in plug-in hybrid sales took place in 2015, particularly during the last two months, with 41,226 plug-in hybrids registered in 2015.\n\nThe dominance of plug-in hybrids in the Netherlands is reflected by the fact that, since their inception in 2011 up until October 2016, five out of the top six registered plug-in electric models are plug-in hybrids. , among all plug-in passenger car registered in the Netherlands, the Mitsubishi Outlander P-HEV leads registrations (24,825), followed by the Volvo V60 Plug-in Hybrid (15,015), the Volkswagen Golf GTE (9,710), the Tesla Model S all-electric car (5,681), the Audi A3 Sportback e-tron (5,227), and the Mercedes-Benz C 350 e (5,092).\n\nThe following table presents the top ranking countries according to its plug-in hybrid segment market share of total new car sales in 2013:\n\nThe following table presents the plug-in hybrid models in the top selling countries for each model through December 2015:\n\nSeveral countries have established grants and tax credits for the purchase of new plug-in electric vehicles (PEVs) including plug-in hybrid electric vehicles, and usually the economic incentive depends on battery size. The U.S. offers a federal income tax credit up to , and several states have additional incentives. The UK offers a Plug-in Car Grant up to a maximum of (). As of April 2011, 15 of the 27 European Union member states provide tax incentives for electrically chargeable vehicles, which includes all Western European countries plus the Czech Republic and Romania. Also 17 countries levy carbon dioxide related taxes on passenger cars as a disincentive. The incentives consist of tax reductions and exemptions, as well as of bonus payments for buyers of all-electric and plug-in hybrid vehicles, hybrid vehicles, and some alternative fuel vehicles.\n\nIncentives for the development of PHEVs are included in the Energy Independence and Security Act of 2007. The Energy Improvement and Extension Act of 2008, signed into law on October 3, 2008, grants a tax credits for the purchase of PHEVs. President Barack Obama's New Energy for America calls for deployment of 1 million plug-in hybrid vehicles by 2015, and on March 19, 2009, he announced programs directing $2.4 billion to electric vehicle development.\n\nThe American Recovery and Reinvestment Act of 2009 modifies the tax credits, including a new one for plug-in electric drive conversion kits and for 2 or 3 wheel vehicles. The ultimate total included in the Act that is going to PHEVs is over $6 billion.\n\nIn March 2009, as part of the American Recovery and Reinvestment Act, the US Department of Energy announced the release of two competitive solicitations for up to $2 billion in federal funding for competitively awarded cost-shared agreements for manufacturing of advanced batteries and related drive components as well as up to $400 million for transportation electrification demonstration and deployment projects. This announcement will also help meet the President Barack Obama's goal of putting one million plug-in hybrid vehicles on the road by 2015.\n\nPublic deployments also include:\n\nGM's roadmap for plug-in ready communities includes: consumer incentives to make this early technology more affordable; public and workplace charging infrastructure; consumer-friendly electricity rates and renewable electricity options; government and corporate vehicle purchases; supportive permitting and codes for vehicle charging; and other incentives such as high-occupancy-vehicle (HOV) lanes access\n\nElectrification of transport (electromobility) is a priority in the European Union Research Programme. It also figures prominently in the European Economic Recovery Plan presented November 2008, in the frame of the Green Car Initiative. DG TREN will support a large European \"electromobility\" project on electric vehicles and related infrastructure with a total budget of around €50 million as part of the Green Car Initiative.\n\nOrganizations that support plug-in hybrids include the World Wide Fund for Nature,\", National Wildlife Federation, and CalCars.\n\nOther supportive organizations are Plug In America, the Alliance for Climate Protection, Friends of the Earth, the Rainforest Action Network, Rocky Mountain Institute (Project Get Ready), the San Francisco Bay Area Council, the Apollo Alliance, the Set America Free Coalition, the Silicon Valley Leadership Group, and the Plug-in Hybrid Electric School Bus Project,\n\nFPL and Duke Energy has said that by 2020 all new purchases of fleet vehicles will be plug-in hybrid or all-electric.\n\n\n"}
{"id": "1437156", "url": "https://en.wikipedia.org/wiki?curid=1437156", "title": "Rhombic drive", "text": "Rhombic drive\n\nThe rhombic drive is a specific method of transferring mechanical energy, or work, used when a single cylinder is used for two separately oscillating pistons.\n\nIt was originally developed around 1900 for the twin-cylinder Lanchester car engine where it allowed perfect balancing of the inertial forces on both pistons.  A current example of its use is on beta type-Stirling engines; the drive's complexity and tight tolerances, causing a high cost of manufacture, is a hurdle for the widespread usage of this drive.\n\nIn its simplest form, the drive utilizes a jointed rhomboid to convert linear work from a reciprocating piston to rotational work.   The connecting rod of the piston is rigid as opposed to a common reciprocating engine which directly connects the piston to the crankshaft with a flexible joint in the piston. Instead, the rod connects to one corner of a rhombus.   When force is applied to the piston, it pushes down; at the same time, the outer corners of the rhomboid push out. They push on two cranks/flywheels which cause them to rotate, each in opposite directions. As the wheels rotate the rhombus progresses its change of shape from being flattened in the direction of the piston axis at top dead centre to being flattened in the perpendicular direction to the piston axis at bottom dead centre.\n\nIn the pictured example the left crank/flywheel turns clockwise and the right crank/flywheel anticlockwise. They turn at the same angular velocity and this can be reinforced by intermeshing them as gear wheels.\n\n"}
{"id": "18015996", "url": "https://en.wikipedia.org/wiki?curid=18015996", "title": "Robert Peter Gale", "text": "Robert Peter Gale\n\nRobert Peter Gale (born October 11, 1945) is an American physician and medical researcher. He is known for research in leukemia and other bone marrow disorders (such as aplastic anemia).\n\nGale received his A.B. degree with honors in biology and chemistry from Hobart College in 1966 and his M.D. degree from the State University of New York at Buffalo in 1970 (with Evan Caukins, Robin Bannerman and John Edwards). His postgraduate medical training (internal medicine, hematology and oncology) was at the University of California, Los Angeles (UCLA) from 1970–1973. In 1976 he received a Ph.D. in microbiology and immunology from the University of California at Los Angeles (UCLA) following doctoral work focusing on cancer immunology (with John Fahey). His postdoctoral studies at UCLA were funded by the U.S. National Institutes of Health (NIH) and the Leukemia Society of America, where he was the Bogart Fellow and Scholar.\n\nFrom 1973–1993, Gale was on the faculty of the UCLA School of Medicine in the Department of Medicine, Division of Hematology & Oncology where he focused on the molecular biology, immunology and treatment of leukemia. He also developed the bone marrow transplant program supported by the NIH. At UCLA, he was active in the Department of Psychology, where he and his colleagues studied interactions between stress, immunity and cancer.\n\nFrom 1980–1997, Gale was Chairman of the Scientific Advisory Committee of the Center for International Blood and Marrow Transplant Research (CIBMTR), an organization of more than 400 transplant centers in over 60 countries worldwide working together to analyze and advance knowledge about blood cell and bone marrow transplants. In 1989–2003 Gale chaired the Scientific Advisory Board of the Center for Advanced Studies in Leukemia, a charity funding innovation leukemia research.\n\nFrom 1986–1993, Gale was President of the Armand Hammer Center for Advanced Studies in Nuclear Energy and Health, a foundation supporting research on medical aspects of nuclear issues. From 1985–1990 Gale was the Wald Scholar in Biomedical Communications at UCLA.\n\nFrom 1993–1999, Gale was Senior Physician and Corporate Director of Bone Marrow and Blood Cell Transplantation at Salick Health Care (SHC), Inc. in Los Angeles (now Aptium Oncology), a subsidiary of AstraZeneca. Gale was also responsible for developing cancer treatment guidelines (in collaboration with colleagues at RAND and Value Health Sciences) and for studying medical aspects of managed cancer care.\n\nFrom 2000–2004 he was Senior Vice-President for Medical Affairs at Antigenics Inc., in New York where he was responsible for design, implementation and analysis of clinical trials of cancer vaccines. He was also Senior Medical Consultant to Oxford Health Plans in areas of advanced medical technologies. From 2004 to 2007, Gale was Senior Vice-President of Research for ZIOPHARM Oncology in Boston, MA and New York, NY which he helped co-found. His focus was on developing and testing new cancer therapies. In 2007 Gale joined Celgene Corporation (Summit, NJ) where he is Executive Director of Clinical Research, Hematology and Oncology. His activities include development and execution of clinical trials in blood and bone marrow cancers, transplantation and immune disorders. Since 2005 Gale has been a Visiting Professor of Haematoloy in the Department of Medicine, Division of Experimental Medicine, Section of Haematology, Imperial College, London assigned to Hammersmith Hospital. He is an editor, co-editor and/or reviewer of many scientific journals in hematology, oncology, immunology, transplantation and internal medicine. Prof. Gale is regarded as a world expert on the medical response to nuclear and radiation accidents and has participated in rescue efforts at Chernobyl, Goiania, Tokaimura, Fukushima and others. In 2012, after extensive analysis of the Japanese data, he said that \"the increased risk of cancer incidence [from the Fukushima incident] would be only 0.002 percent for a member of the Japanese public\".\n\nGale has contributed to basic science and clinical research in bone marrow transplantation where he made contributions to understanding the immune-mediated anti-leukemia effects of transplants (graft-versus-leukemia [GvL]. He has also advanced understanding other complex immune effects of transplants in humans, like graft-versus-host disease and post-transplant immune deficiency. He has worked on alternate sources of hematopoietic stem cells including fetal liver transplants.\n\nGale has published over 1000 scientific articles and more than 20 books, mostly on leukemia (biology and treatment), transplantation (biology, immunology and treatment), cancer immunology and radiation (biological effects and accident response). He has written on medical topics, nuclear energy and weapons and politics of US-Soviet relations in articles for \"The New York Times\", \"Los Angeles Times\", \"Washington Post\", \"USA Today\" and \"Wall Street Journal\". In addition to his academic publications, Gale has written popular books on Chernobyl and US nuclear energy policy. He has written parts of screenplays for and/or appeared in several movies including \"\" (with Jon Voight), \"Fat Man and Little Boy\" (with Paul Newman) and \"City of Joy\" (with Patrick Swazye). His latest book \"Radiation: What it is, What you need to know\" with Eric Lax, was published in February 2013.\n\nAwards for his scientific achievements include the Presidential Award, New York Academy of Sciences, Scientist of Distinction Award, Weizmann Institute of Science, Distinguished Alumni Award from Hobart College and Intra-Science Research Foundation Award. He holds honorary degrees including D.Sc. from Albany Medical College, D.Sc from The State University of New York Buffalo, L.H.D. from Hobart College and D.P.S from MacMurray College. In 2018 he was accepted as a fellow in the Royal College of Physicians (RCP) https://www.rcplondon.ac.uk. He received an Emmy award for his work on a \"60 Minutes\" special report about Chernobyl.\n\nIn 1986, he was asked by the government of the Soviet Union to coordinate medical relief efforts for victims of the Chernobyl disaster. In 1987, he was asked by the government of Brazil to coordinate medical relief efforts for the Goiania accident. In 1988, he was part of the U.S. medical emergency team sent in the aftermath of the earthquake in Armenia. In 1999 he was asked by the government of Japan to help treat victims of the Tokaimura nuclear accident. In 2011 Gale was called to Japan to deal with medical consequences of the Fukushima nuclear power station accident. He met with members of the Prime Minister's office on several occasions and has addressed the Diet on 3 occasions. Gale has also been a neutral war observer for the governments of Croatia and Armenia and a medical consultant to the government of Tartarstan. Gale has received several awards for his humanitarian activities including the Olender Peace Prize, City of Los Angeles Humanitarian Award and Myasthenia Gravis Foundation Humanitarian Award.\n\nGale lives in Los Angeles, New York City and Big Sky, MT with his wife Laura.\n\n"}
{"id": "2530543", "url": "https://en.wikipedia.org/wiki?curid=2530543", "title": "Rocking horse", "text": "Rocking horse\n\nA rocking horse is a child's toy, usually shaped like a horse and mounted on rockers similar to a rocking chair. There are two sorts, the one where the horse part sits rigidly attached to a pair of curved rockers that are in contact with the ground, and a second sort, where the horse hangs on a rigid frame by iron straps the horse moves only relative to the frame, which does not move.\n\nPredecessors of the rocking horse may be seen in the rocking cradle, the tilting seats used during the Middle Ages for jousting practice as well as the wheeled hobby horse. The toy in its current form did not appear before the 17th century, though some conflicting sources note medieval manuscripts including references to carved rocking horses, presumably of the toy kind.\n\nFrom the 19th century onward, rocking horses became more commonly considered as child's toy. Mostly built by hobby woodcrafters, and ranging from relatively crude to finely ornamented and the toys of future kings, it was not until the late 19th century that the production became industrialised.\n\nIn 2006, the Guinness Book of World Records certified Katlinel and Les Hartness of California as having the largest hand-carved wooden rocking horse on record. This rocking horse was built in 2000 and is 7 feet 8 inches tall and weighs 1,200 pounds. It can be seen at renaissance faires, faerie festivals, and at private parties and events where up to 3 adults or 4-5 children can ride it together at one time. According to Les and Katlinel, the youngest rider has been six weeks old and the oldest, 94 years.\n\n\n"}
{"id": "7884988", "url": "https://en.wikipedia.org/wiki?curid=7884988", "title": "Salt tectonics", "text": "Salt tectonics\n\nSalt tectonics is concerned with the geometries and processes associated with the presence of significant thicknesses of evaporites containing rock salt within a stratigraphic sequence of rocks. This is due both to the low density of salt, which does not increase with burial, and its low strength.\n\nSalt structures (excluding undeformed layers of salt) have been found in more than 120 sedimentary basins across the world.\n\nStructures may form during continued sedimentary loading, without any external tectonic influence, due to gravitational instability. Pure halite has a density of 2160 kg/m. When initially deposited, sediments generally have a lower density of 2000 kg/m³, but with loading and compaction their density increases to 2500 kg/m³, which is greater than that of salt. Once the overlying layers have become denser, the weak salt layer will tend to deform into a characteristic series of ridges and depressions, due to a form of Rayleigh–Taylor instability. Further sedimentation will be concentrated in the depressions and the salt will continue to move away from them into the ridges. At a late stage, diapirs tend to initiate at the junctions between ridges, their growth fed by movement of salt along the ridge system, continuing until the salt supply is exhausted. During the later stages of this process the top of the diapir remains at or near the surface, with further burial being matched by diapir rise, and is sometimes referred to as \"downbuilding\". The Schacht Asse II and Gorleben salt domes in Germany are an example of a purely passive salt structure.\n\nSuch structures do not always form when a salt layer is buried beneath a sedimentary overburden. This can be due to a relatively high strength overburden or to the presence of sedimentary layers interbedded within the salt unit that increase both its density and strength.\n\nActive tectonics will increase the likelihood of salt structures developing. In the case of extensional tectonics, faulting will both reduce the strength of the overburden and thin it. In an area affected by thrust tectonics, buckling of the overburden layer will allow the salt to rise into the cores of anticlines, as seen in salt domes in the Zagros Mountains.\n\nIf the pressure within the salt body becomes sufficiently high it may be able to push through its overburden, this is known as \"forceful\" diapirism. Many salt diapirs may contain elements of both active and passive salt movement. An active salt structure may pierce its overburden and from then on continue to develop as a purely passive salt diapir.\n\nIn those cases where salt layers do not have the conditions necessary to develop passive salt structures, the salt may still move into relatively low pressure areas around developing folds and faults. Such structures are described as \"reactive\".\n\nWhen one or more salt layers are present during extensional tectonics, a characteristic set of structures are formed. Extensional faults propagate up from the middle part of the crust until they encounter the salt layer. The weakness of the salt prevents the fault from propagating through. However, continuing displacement on the fault offsets the base of the salt and causes bending of the overburden layer. Eventually the stresses caused by this bending will be sufficient to fault the overburden. The types of structures developed depend on the initial salt thickness. In the case of a very thick salt layer there is no direct spatial relationship between the faulting beneath the salt and that in the overburden, such a system is said to be \"unlinked\". For intermediate salt thicknesses, the overburden faults are spatially related to the deeper faults, but offset from them, normally into the footwall; these are known as \"soft-linked\" systems. When the salt layer becomes thin enough, the fault that develops in the overburden is closely aligned with that beneath the salt, and forms a continuous fault surface after only a relatively small displacement, forming a \"hard-linked\" fault.\n\nIn areas of thrust tectonics salt layers act as preferred detachment planes. In the Zagros fold and thrust belt, variations in the thickness and therefore effectiveness of the late Neoproterozoic to Early Cambrian Hormuz salt are thought to have had a fundamental control on the overall topography.\n\nWhen a salt layer becomes too thin to be an effective detachment layer, due to salt movement, dissolution or removal by faulting, the overburden and the underlying sub-salt basement become effectively \"welded\" together. This may cause the development of new faults in the cover sequence and is an important consideration when modeling the migration of hydrocarbons.\nSalt welds may also develop in the vertical direction by putting the sides of a former diapir in contact.\n\nSalt that pierces to the surface, either on land or beneath the sea, tends to spread laterally away and such salt is said to be \"allochthonous\". Salt glaciers are formed on land where this happens in an arid environment, such as in the Zagros Mountains. Offshore tongues of salt are generated that may join together with others from neighbouring piercements to form canopies.\n\nA significant proportion of the world’s hydrocarbon reserves are found in structures related to salt tectonics, including many in the Middle East, the South Atlantic passive margins (Brazil, Gabon and Angola) and the Gulf of Mexico.\n\n"}
{"id": "19590441", "url": "https://en.wikipedia.org/wiki?curid=19590441", "title": "Secretary of State for Energy and Climate Change", "text": "Secretary of State for Energy and Climate Change\n\nHer Majesty's Principal Secretary of State for Energy and Climate Change was a British government cabinet position from 2008 to 2016. The Department for Energy and Climate Change was created on 3 October 2008 when former Prime Minister Gordon Brown reshuffled his Cabinet.\n\nThe Energy and Climate Change Secretary revived the earlier post of the Secretary of State for Energy as head of the Department of Energy, existing from 1974 to 1992. After which, the Department of Energy was merged into the Department of Trade and Industry under the Conservative government of Sir John Major in 1992.\n\nSixteen years later, and immediately prior to the creation of the new department, energy policy was the responsibility of the Department for Business, Enterprise and Regulatory Reform (itself now a defunct government department, superseded by the Department for Business, Innovation and Skills).\n\nIn 2010, Lord Marland was made Parliamentary Under-Secretary of State.\n\nOn 3 February 2012, Chris Huhne resigned from the post after it was announced that he would be prosecuted for perverting the course of justice, in relation to accusations that he passed on speeding penalties to his ex-wife to avoid losing his own licence. The post was taken over by Ed Davey on the same day.\n\nThe post was formed into the new Department for Business, Energy and Industrial Strategy in July 2016.\n\nColour key\n\n"}
{"id": "148911", "url": "https://en.wikipedia.org/wiki?curid=148911", "title": "Separated sets", "text": "Separated sets\n\nIn topology and related branches of mathematics, separated sets are pairs of subsets of a given topological space that are related to each other in a certain way: roughly speaking, neither overlapping nor touching.\nThe notion of when two sets are separated or not is important both to the notion of connected spaces (and their connected components) as well as to the separation axioms for topological spaces.\n\nSeparated sets should not be confused with separated spaces (defined below), which are somewhat related but different.\nSeparable spaces are again a completely different topological concept.\n\nThere are various ways in which two subsets of a topological space \"X\" can be considered to be separated.\n\n\nThe \"separation axioms\" are various conditions that are sometimes imposed upon topological spaces which can be described in terms of the various types of separated sets.\nAs an example, we will define the T axiom, which is the condition imposed on separated spaces.\nSpecifically, a topological space is \"separated\" if, given any two distinct points \"x\" and \"y\", the singleton sets {\"x\"} and {\"y\"} are separated by neighbourhoods.\n\nSeparated spaces are also called \"Hausdorff spaces\" or \"T spaces\".\nFurther discussion of separated spaces may be found in the article Hausdorff space.\nGeneral discussion of the various separation axioms is in the article Separation axiom.\n\nGiven a topological space \"X\", it is sometimes useful to consider whether it is possible for a subset \"A\" to be separated from its complement.\nThis is certainly true if \"A\" is either the empty set or the entire space \"X\", but there may be other possibilities.\nA topological space \"X\" is \"connected\" if these are the only two possibilities.\nConversely, if a nonempty subset \"A\" is separated from its own complement, and if the only subset of \"A\" to share this property is the empty set, then \"A\" is an \"open-connected component\" of \"X\".\n\nFor more on connected spaces, see Connected space.\n\nGiven a topological space \"X\", two points \"x\" and \"y\" are \"topologically distinguishable\" if there exists an open set that one point belongs to but the other point does not.\nIf \"x\" and \"y\" are topologically distinguishable, then the singleton sets {\"x\"} and {\"y\"} must be disjoint.\nOn the other hand, if the singletons {\"x\"} and {\"y\"} are separated, then the points \"x\" and \"y\" must be topologically distinguishable.\nThus for singletons, topological distinguishability is a condition in between disjointness and separatedness.\n\nFor more about topologically distinguishable points, see Topological distinguishability.\n\n"}
{"id": "12696599", "url": "https://en.wikipedia.org/wiki?curid=12696599", "title": "Seto Windhill", "text": "Seto Windhill\n\nThe is a collection of wind turbines located on the peaks of mountains along the Sadamisaki Peninsula, in the town of Ikata, Ehime Prefecture, Japan. The windfarm borders the Seto Wind Hill Park.\n\nThe installation consists of 11 Mitsubishi Heavy Industries MWT-1000s with a nameplate capacity of 1000 kW. They were erected starting September 2002, and began full operation in October 2003.\n\n\n"}
{"id": "28739420", "url": "https://en.wikipedia.org/wiki?curid=28739420", "title": "Sky footage", "text": "Sky footage\n\nSky footage (skf) is the total exposure of any area being considered for a solar panel installation. This includes roof installation or ground installation where the area is dedicated to the capturing of solar energy.\n\nThe term is also applied to the vertical volume that a structure occupies. The sky footage comes into consideration when building power lines, water towers, and radio antennas.\n"}
{"id": "53042084", "url": "https://en.wikipedia.org/wiki?curid=53042084", "title": "Surfactant leaching", "text": "Surfactant leaching\n\nSurfactant leaching of acrylic (latex) paints, also known as streak staining, streaking, weeping, exudation, etc., occurs when the freshly painted surface becomes wet and water-soluble components of the paint (dispersants, surfactants, thickeners, glycols, etc.) leach out of the paint in sticky brown streaks. This may happen, e.g., due to rain or dew for exterior surfaces, or water vapor condensation on interior ones. On the external surfaces the streaks will normally weather off in several weeks, and removal of them before that time is impractical, especially because it may damage the paint before it is completely cured. The streaking phenomenon may also be observed for some silicone sealants.\n\nThe leaching effect should be taken into an account by manufacturers when formulating latex paints. A common approach is replacing water-soluble ingredients with volatile organic compounds (VOCs), which are not environmentally safe.\n"}
{"id": "4496333", "url": "https://en.wikipedia.org/wiki?curid=4496333", "title": "Sydney to Hobart Yacht Race waterspout", "text": "Sydney to Hobart Yacht Race waterspout\n\nThe south coast tornado was a tornadic waterspout spawned by a supercell thunderstorm off the south coast of New South Wales on 26 December 2001, during the Sydney to Hobart Yacht Race. The tornado passed very close to the yacht \"Nicorette II\", which was severely damaged but able to complete the race with a spare mainsail. \"Nicorette\" recorded wind speeds of close to (making the tornado at least F2 on the Fujita scale) and was struck by hail the size of golf balls. According to the boat's meteorologist, the tornado began with a diameter of around , but grew in size until it was across. The waterspout proceeded to strike several other boats with weaker winds. The tornado should not be confused with the severe storm that wrought havoc on the race in 1998.\n\n\n"}
{"id": "50899206", "url": "https://en.wikipedia.org/wiki?curid=50899206", "title": "Tornadoes of 1963", "text": "Tornadoes of 1963\n\nThis page documents the tornadoes and tornado outbreaks of 1963, primarily in the United States. Most tornadoes form in the U.S., although some events may take place internationally. Tornado statistics for older years like this often appear significantly lower than modern years due to fewer reports or confirmed tornadoes.\n"}
{"id": "41834", "url": "https://en.wikipedia.org/wiki?curid=41834", "title": "Uninterruptible power supply", "text": "Uninterruptible power supply\n\nAn uninterruptible power supply or uninterruptible power source (UPS) is an electrical apparatus that provides emergency power to a load when the input power source or mains power fails. A UPS differs from an auxiliary or emergency power system or standby generator in that it will provide near-instantaneous protection from input power interruptions, by supplying energy stored in batteries, supercapacitors, or flywheels. The on-battery run-time of most uninterruptible power sources is relatively short (only a few minutes) but sufficient to start a standby power source or properly shut down the protected equipment. It is a type of continual power system.\n\nA UPS is typically used to protect hardware such as computers, data centers, telecommunication equipment or other electrical equipment where an unexpected power disruption could cause injuries, fatalities, serious business disruption or data loss. UPS units range in size from units designed to protect a single computer without a video monitor (around 200 volt-ampere rating) to large units powering entire data centers or buildings. The world's largest UPS, the 46-megawatt Battery Electric Storage System (BESS), in Fairbanks, Alaska, powers the entire city and nearby rural communities during outages.\n\nThe primary role of any UPS is to provide short-term power when the input power source fails. However, most UPS units are also capable in varying degrees of correcting common utility power problems:\n\nSome manufacturers of UPS units categorize their products in accordance with the number of power-related problems they address.\n\nThe three general categories of modern UPS systems are \"on-line\", \"line-interactive\" and \"standby\". An on-line UPS uses a \"double conversion\" method of accepting AC input, rectifying to DC for passing through the rechargeable battery (or battery strings), then inverting back to 120 V/230 V AC for powering the protected equipment. A line-interactive UPS maintains the inverter in line and redirects the battery's DC current path from the normal charging mode to supplying current when power is lost. In a standby (\"off-line\") system the load is powered directly by the input power and the backup power circuitry is only invoked when the utility power fails. Most UPS below 1 kVA are of the line-interactive or standby variety which are usually less expensive.\n\nFor large power units, Dynamic Uninterruptible Power Supplies (DUPS) are sometimes used. A synchronous motor/alternator is connected on the mains via a choke. Energy is stored in a flywheel. When the mains power fails, an eddy-current regulation maintains the power on the load as long as the flywheel's energy is not exhausted. DUPS are sometimes combined or integrated with a diesel generator that is turned on after a brief delay, forming a diesel rotary uninterruptible power supply (DRUPS).\n\nA fuel cell UPS has been developed in recent years using hydrogen and a fuel cell as a power source, potentially providing long run times in a small space. \n\nThe offline/standby UPS (SPS) offers only the most basic features, providing surge protection and battery backup. The protected equipment is normally connected directly to incoming utility power. When the incoming voltage falls below or rises above a predetermined level the SPS turns on its internal DC-AC inverter circuitry, which is powered from an internal storage battery. The UPS then mechanically switches the connected equipment on to its DC-AC inverter output. The switchover time can be as long as 25 milliseconds depending on the amount of time it takes the standby UPS to detect the lost utility voltage. The UPS will be designed to power certain equipment, such as a personal computer, without any objectionable dip or brownout to that device.\n\nThe line-interactive UPS is similar in operation to a standby UPS, but with the addition of a multi-tap variable-voltage autotransformer. This is a special type of transformer that can add or subtract powered coils of wire, thereby increasing or decreasing the magnetic field and the output voltage of the transformer. This may also be performed by a \"buck–boost transformer\" which is distinct from an autotransformer, since the former may be wired to provide galvanic isolation.\n\nThis type of UPS is able to tolerate continuous undervoltage brownouts and overvoltage surges without consuming the limited reserve battery power. It instead compensates by automatically selecting different power taps on the autotransformer. Depending on the design, changing the autotransformer tap can cause a very brief output power disruption, which may cause UPSs equipped with a power-loss alarm to \"chirp\" for a moment.\n\nThis has become popular even in the cheapest UPSs because it takes advantage of components already included. The main 50/60 Hz transformer used to convert between line voltage and battery voltage needs to provide two slightly different turns ratios: One to convert the battery output voltage (typically a multiple of 12 V) to line voltage, and a second one to convert the line voltage to a slightly higher battery charging voltage (such as a multiple of 14 V). The difference between the two voltages is because charging a battery requires a delta voltage (up to 13–14 V for charging a 12 V battery). Furthermore, it is easier to do the switching on the line-voltage side of the transformer because of the lower currents on that side.\n\nTo gain the \"buck/boost\" feature, all that is required is two separate switches so that the AC input can be connected to one of the two primary taps, while the load is connected to the other, thus using the main transformer's primary windings as an autotransformer. The battery can still be charged while \"bucking\" an overvoltage, but while \"boosting\" an undervoltage, the transformer output is too low to charge the batteries.\n\nAutotransformers can be engineered to cover a wide range of varying input voltages, but this requires more taps and increases complexity, and expense of the UPS. It is common for the autotransformer to cover a range only from about 90 V to 140 V for 120 V power, and then switch to battery if the voltage goes much higher or lower than that range.\n\nIn low-voltage conditions the UPS will use more current than normal so it may need a higher current circuit than a normal device. For example, to power a 1000-W device at 120 V, the UPS will draw 8.33 A. If a brownout occurs and the voltage drops to 100 V, the UPS will draw 10 A to compensate. This also works in reverse, so that in an overvoltage condition, the UPS will need less current.\n\nIn an online UPS, the batteries are always connected to the inverter, so that no power transfer switches are necessary. When power loss occurs, the rectifier simply drops out of the circuit and the batteries keep the power steady and unchanged. When power is restored, the rectifier resumes carrying most of the load and begins charging the batteries, though the charging current may be limited to prevent the high-power rectifier from overheating the batteries and boiling off the electrolyte. The main advantage of an on-line UPS is its ability to provide an \"electrical firewall\" between the incoming utility power and sensitive electronic equipment.\n\nThe online UPS is ideal for environments where electrical isolation is necessary or for equipment that is very sensitive to power fluctuations. Although it was at one time reserved for very large installations of 10 kW or more, advances in technology have now permitted it to be available as a common consumer device, supplying 500 W or less. The initial cost of the online UPS may be higher, but its total cost of ownership is generally lower due to longer battery life. The online UPS may be necessary when the power environment is \"noisy\", when utility power sags, outages and other anomalies are frequent, when protection of sensitive IT equipment loads is required, or when operation from an extended-run backup generator is necessary.\n\nThe basic technology of the online UPS is the same as in a standby or line-interactive UPS. However it typically costs much more, due to it having a much greater current AC-to-DC battery-charger/rectifier, and with the rectifier and inverter designed to run continuously with improved cooling systems. It is called a \"double-conversion\" UPS due to the rectifier directly driving the inverter, even when powered from normal AC current.\n\nOnline UPS typically have Static transfer switch (STS) for increasing reliability .\n\nThese hybrid Rotary UPS designs do not have official designations, although one name used by UTL is \"double conversion on demand\". This style of UPS is targeted towards high-efficiency applications while still maintaining the features and protection level offered by double conversion.\n\nA hybrid (double conversion on demand) UPS operates as an off-line/standby UPS when power conditions are within a certain preset window. This allows the UPS to achieve very high efficiency ratings. When the power conditions fluctuate outside of the predefined windows, the UPS switches to online/double-conversion operation. In double-conversion mode the UPS can adjust for voltage variations without having to use battery power, can filter out line noise and control frequency. \n\nFerroresonant units operate in the same way as a standby UPS unit; however, they are online with the exception that a ferroresonant transformer, is used to filter the output. This transformer is designed to hold energy long enough to cover the time between switching from line power to battery power and effectively eliminates the transfer time. Many ferroresonant UPSs are 82–88% efficient (AC/DC-AC) and offer excellent isolation.\n\nThe transformer has three windings, one for ordinary mains power, the second for rectified battery power, and the third for output AC power to the load.\n\nThis once was the dominant type of UPS and is limited to around the range. These units are still mainly used in some industrial settings (oil and gas, petrochemical, chemical, utility, and heavy industry markets) due to the robust nature of the UPS. Many ferroresonant UPSs utilizing controlled ferro technology may not interact with power-factor-correcting equipment.\n\nA UPS designed for powering DC equipment is very similar to an online UPS, except that it does not need an output inverter. Also, if the UPS's battery voltage is matched with the voltage the device needs, the device's power supply will not be needed either. Since one or more power conversion steps are eliminated, this increases efficiency and run time.\n\nMany systems used in telecommunications use an extra-low voltage \"common battery\" 48 V DC power, because it has less restrictive safety regulations, such as being installed in conduit and junction boxes. DC has typically been the dominant power source for telecommunications, and AC has typically been the dominant source for computers and servers.\n\nThere has been much experimentation with 48 V DC power for computer servers, in the hope of reducing the likelihood of failure and the cost of equipment. However, to supply the same amount of power, the current would be higher than an equivalent 115 V or 230 V circuit; greater current requires larger conductors, or more energy lost as heat.\n\nA laptop computer is a classic example of a PC with a DC UPS built in.\n\nHigh voltage DC (380 V) is finding use in some data center applications, and allows for small power conductors, but is subject to the more complex electrical code rules for safe containment of high voltages.\n\nA rotary UPS uses the inertia of a high-mass spinning flywheel (flywheel energy storage) to provide short-term \"ride-through\" in the event of power loss. The flywheel also acts as a buffer against power spikes and sags, since such short-term power events are not able to appreciably affect the rotational speed of the high-mass flywheel. It is also one of the oldest designs, predating vacuum tubes and integrated circuits.\n\nIt can be considered to be \"on line\" since it spins continuously under normal conditions. However, unlike a battery-based UPS, flywheel-based UPS systems typically provide 10 to 20 seconds of protection before the flywheel has slowed and power output stops. It is traditionally used in conjunction with standby generators, providing backup power only for the brief period of time the engine needs to start running and stabilize its output.\n\nThe rotary UPS is generally reserved for applications needing more than 10,000 W of protection, to justify the expense and benefit from the advantages rotary UPS systems bring. A larger flywheel or multiple flywheels operating in parallel will increase the reserve running time or capacity.\n\nBecause the flywheels are a mechanical power source, it is not necessary to use an electric motor or generator as an intermediary between it and a diesel engine designed to provide emergency power. By using a transmission gearbox, the rotational inertia of the flywheel can be used to directly start up a diesel engine, and once running, the diesel engine can be used to directly spin the flywheel. Multiple flywheels can likewise be connected in parallel through mechanical countershafts, without the need for separate motors and generators for each flywheel.\n\nThey are normally designed to provide very high current output compared to a purely electronic UPS, and are better able to provide inrush current for inductive loads such as motor startup or compressor loads, as well as medical MRI and cath lab equipment. It is also able to tolerate short-circuit conditions up to 17 times larger than an electronic UPS, permitting one device to blow a fuse and fail while other devices still continue to be powered from the rotary UPS.\n\nIts life cycle is usually far greater than a purely electronic UPS, up to 30 years or more. But they do require periodic downtime for mechanical maintenance, such as ball bearing replacement. In larger systems redundancy of the system ensures the availability of processes during this maintenance. Battery-based designs do not require downtime if the batteries can be hot-swapped, which is usually the case for larger units. Newer rotary units use technologies such as magnetic bearings and air-evacuated enclosures to increase standby efficiency and reduce maintenance to very low levels.\n\nTypically, the high-mass flywheel is used in conjunction with a motor-generator system. These units can be configured as:\n\nIn case No. 3 the motor generator can be synchronous/synchronous or induction/synchronous. The motor side of the unit in case Nos. 2 and 3 can be driven directly by an AC power source (typically when in inverter bypass), a 6-step double-conversion motor drive, or a 6-pulse inverter. Case No. 1 uses an integrated flywheel as a short-term energy source instead of batteries to allow time for external, electrically coupled gensets to start and be brought online. Case Nos. 2 and 3 can use batteries or a free-standing electrically coupled flywheel as the short-term energy source.\n\nUPS systems come in several different forms and sizes. However, the two most common forms are tower and rack-mount.\n\nTower models stand upright on the ground or on a desk/shelf, and are typically used in network workstations or desktop computer applications.\n\nRack-mount models can be mounted in standard 19\" rack enclosures and can require anywhere from 1U to 12U (rack space). They are typically used in server and networking applications.\n\nIn large business environments where reliability is of great importance, a single huge UPS can also be a single point of failure that can disrupt many other systems. To provide greater reliability, multiple smaller UPS modules and batteries can be integrated together to provide redundant power protection equivalent to one very large UPS. \"N+1\" means that if the load can be supplied by N modules, the installation will contain N+1 modules. In this way, failure of one module will not impact system operation.\n\nMany computer servers offer the option of redundant power supplies, so that in the event of one power supply failing, one or more other power supplies are able to power the load. This is a critical point – each power supply must be able to power the entire server by itself.\n\nRedundancy is further enhanced by plugging each power supply into a different circuit (i.e. to a different circuit breaker).\n\nRedundant protection can be extended further yet by connecting each power supply to its own UPS. This provides double protection from both a power supply failure and a UPS failure, so that continued operation is assured. This configuration is also referred to as 1+1 or 2N redundancy. If the budget does not allow for two identical UPS units then it is common practice to plug one power supply into mains power and the other into the UPS.\n\nWhen a UPS system is placed outdoors, it should have some specific features that guarantee that it can tolerate weather without any effects on performance. Factors such as temperature, humidity, rain, and snow among others should be considered by the manufacturer when designing an outdoor UPS system. Operating temperature ranges for outdoor UPS systems could be around −40 °C to +55 °C.\n\nOutdoor UPS systems can either be pole, ground (pedestal), or host mounted. Outdoor environment could mean extreme cold, in which case the outdoor UPS system should include a battery heater mat, or extreme heat, in which case the outdoor UPS system should include a fan system or an air conditioning system.\nA solar inverter, or PV inverter, or solar converter, converts the variable direct current (DC) output of a photovoltaic (PV) solar panel into a utility frequency alternating current (AC) that can be fed into a commercial electrical grid or used by a local, off-grid electrical network. It is a critical BOS–component in a photovoltaic system, allowing the use of ordinary AC-powered equipment. Solar inverters have special functions adapted for use with photovoltaic arrays, including maximum power point tracking and anti-islanding protection.\n\nA problem in the combination of a double-conversion UPS and a generator is the voltage distortion created by the UPS. The input of a double-conversion UPS is essentially a big rectifier. The current drawn by the UPS is non-sinusoidal. This can cause the voltage from the AC mains or a generator to also become non-sinusoidal. The voltage distortion then can cause problems in all electrical equipment connected to that power source, including the UPS itself. It will also cause more power to be lost in the wiring supplying power to the UPS due to the spikes in current flow. This level of \"noise\" is measured as a percentage of \"total harmonic distortion of the current\" (THD). Classic UPS rectifiers have a THD level of around 25%–30%. To reduce voltage distortion, this requires heavier mains wiring or generators more than twice as large as the UPS.\n\nThere are several solutions to reduce the THD in a double-conversion UPS:\n\nClassic solutions such as passive filters reduce THD to 5%–10% at full load. They are reliable, but big and only work at full load, and present their own problems when used in tandem with generators.\n\nAn alternative solution is an active filter. Through the use of such a device, THD can drop to 5% over the full power range. The newest technology in double-conversion UPS units is a rectifier that does not use classic rectifier components (thyristors and diodes) but uses high-frequency components instead. A double-conversion UPS with an insulated-gate bipolar transistor rectifier and inductor can have a THD as small as 2%. This completely eliminates the need to oversize the generator (and transformers), without additional filters, investment cost, losses, or space.\n\nPower management (PM) requires\n\nThe basic computer-to-UPS control methods are intended for one-to-one signaling from a single source to a single target. For example, a single UPS may connect to a single computer to provide status information about the UPS, and allow the computer to control the UPS. Similarly, the USB protocol is also intended to connect a single computer to multiple peripheral devices.\n\nIn some situations it is useful for a single large UPS to be able to communicate with several protected devices. For traditional serial or USB control, a \"signal replication\" device may be used, which for example allows one UPS to connect to five computers using serial or USB connections. However, the splitting is typically only one direction from UPS to the devices to provide status information. Return control signals may only be permitted from one of the protected systems to the UPS.\n\nAs Ethernet has increased in common use since the 1990s, control signals are now commonly sent between a single UPS and multiple computers using standard Ethernet data communication methods such as TCP/IP. The status and control information is typically encrypted so that for example an outside hacker can not gain control of the UPS and command it to shut down.\n\nDistribution of UPS status and control data requires that all intermediary devices such as Ethernet switches or serial multiplexers be powered by one or more UPS systems, in order for the UPS alerts to reach the target systems during a power outage. To avoid the dependency on Ethernet infrastructure, the UPSs can be connected directly to main control server by using GSM/GPRS channel also. The SMS or GPRS data packets sent from UPSs trigger software to shut down the PCs to reduce the load.\n\nThere are three main types of UPS batteries: Valve Regulated Lead Acid (VRLA), Flooded Cell or VLA batteries,Lithium Ion batteries.\nThe run-time for a battery-operated UPS depends on the type and size of batteries and rate of discharge, and the efficiency of the inverter. The total capacity of a lead–acid battery is a function of the rate at which it is discharged, which is described as Peukert's law.\n\nManufacturers supply run-time rating in minutes for packaged UPS systems. Larger systems (such as for data centers) require detailed calculation of the load, inverter efficiency, and battery characteristics to ensure the required endurance is attained.\n\nWhen a lead–acid battery is charged or discharged, this initially affects only the reacting chemicals, which are at the interface between the electrodes and the electrolyte. With time, the charge stored in the chemicals at the interface, often called \"interface charge\", spreads by diffusion of these chemicals throughout the volume of the active material.\n\nIf a battery has been completely discharged (e.g. the car lights were left on overnight) and next is given a fast charge for only a few minutes, then during the short charging time it develops only a charge near the interface. The battery voltage may rise to be close to the charger voltage so that the charging current decreases significantly. After a few hours this interface charge will spread to the volume of the electrode and electrolyte, leading to an interface charge so low that it may be insufficient to start a car.\n\nDue to the interface charge, brief UPS \"self-test\" functions lasting only a few seconds may not accurately reflect the true runtime capacity of a UPS, and instead an extended \"recalibration\" or \"rundown\" test that deeply discharges the battery is needed.\n\nThe deep discharge testing is itself damaging to batteries due to the chemicals in the discharged battery starting to crystallize into highly stable molecular shapes that will not re-dissolve when the battery is recharged, permanently reducing charge capacity. In lead acid batteries this is known as sulfation but also affects other types such as nickel cadmium batteries and lithium batteries. Therefore, it is commonly recommended that rundown tests be performed infrequently, such as every six months to a year.\n\nMulti-kilowatt commercial UPS systems with large and easily accessible battery banks are capable of isolating and testing individual cells within a \"battery string\", which consists of either combined-cell battery units (such as 12-V lead acid batteries) or individual chemical cells wired in series. Isolating a single cell and installing a jumper in place of it allows the one battery to be discharge-tested, while the rest of the battery string remains charged and available to provide protection.\n\nIt is also possible to measure the electrical characteristics of individual cells in a battery string, using intermediate sensor wires that are installed at every cell-to-cell junction, and monitored both individually and collectively. Battery strings may also be wired as series-parallel, for example two sets of 20 cells. In such a situation it is also necessary to monitor current flow between parallel strings, as current may circulate between the strings to balance out the effects of weak cells, dead cells with high resistance, or shorted cells. For example, stronger strings can discharge through weaker strings until voltage imbalances are equalized, and this must be factored into the individual inter-cell measurements within each string.\n\nBattery strings wired in series-parallel can develop unusual failure modes due to interactions between the multiple parallel strings. Defective batteries in one string can adversely affect the operation and lifespan of good or new batteries in other strings. These issues also apply to other situations where series-parallel strings are used, not just in UPS systems but also in electric vehicle applications.\n\nConsider a series-parallel battery arrangement with all good cells, and one becomes shorted or dead:\n\nThe only way to prevent these subtle series-parallel string interactions is by not using parallel strings at all and using separate charge controllers and inverters for individual series strings.\n\nEven just a single string of batteries wired in series can have adverse interactions if new batteries are mixed with old batteries. Older batteries tend to have reduced storage capacity, and so will both discharge faster than new batteries and also charge to their maximum capacity more rapidly than new batteries.\n\nAs a mixed string of new and old batteries is depleted, the string voltage will drop, and when the old batteries are exhausted the new batteries still have charge available. The newer cells may continue to discharge through the rest of the string, but due to the low voltage this energy flow may not be useful, and may be wasted in the old cells as resistance heating.\n\nFor cells that are supposed to operate within a specific discharge window, new cells with more capacity may cause the old cells in the series string to continue to discharge beyond the safe bottom limit of the discharge window, damaging the old cells.\n\nWhen recharged, the old cells recharge more rapidly, leading to a rapid rise of voltage to near the fully charged state, but before the new cells with more capacity have fully recharged. The charge controller detects the high voltage of a nearly fully charged string and reduces current flow. The new cells with more capacity now charge very slowly, so slowly that the chemicals may begin to crystallize before reaching the fully charged state, reducing new cell capacity over several charge/discharge cycles until their capacity more closely matches the old cells in the series string.\n\nFor such reasons, some industrial UPS management systems recommend periodic replacement of entire battery arrays potentially using hundreds of expensive batteries, due to these damaging interactions between new batteries and old batteries, within and across series and parallel strings.\n\n\n"}
{"id": "7191759", "url": "https://en.wikipedia.org/wiki?curid=7191759", "title": "World Coal Association", "text": "World Coal Association\n\nThe World Coal Association (WCA) is an international non-profit, non-governmental association based in London, United Kingdom. It was created to represent the global coal industry. The association was formerly called the World Coal Institute (WCI) but changed its name in November 2010. The WCA undertakes lobbying, organises workshops, and provides coal information to decision makers in international energy and environmental policy and research discussions, as well as supplying information to the general public and educational organisations on the benefits and issues surrounding the use of coal. It also promotes clean coal technologies.\n\nIt has participated in a number of United Nations and International Energy Agency (IEA) workshops, boards, and forums, including the UN Commission on Sustainable Development, the UN Framework Convention on Climate Change, the IEA Working Party on Fossil Fuels, and the IEA Coal Industry Advisory Board. It is also part of the Carbon Sequestration Leadership Forum.\n\nIt is co-author for a report on the future of coal in ASEAN nations. \n\n\n"}
{"id": "344413", "url": "https://en.wikipedia.org/wiki?curid=344413", "title": "World Solar Challenge", "text": "World Solar Challenge\n\nThe World Solar Challenge or the Bridgestone World Solar Challenge since 2013 due to the sponsorship of Bridgestone Corporation is a biennial solar-powered car race which covers through the Australian Outback, from Darwin, Northern Territory to Adelaide, South Australia.\n\nThe race attracts teams from around the world, most of which are fielded by universities or corporations although some are fielded by high schools. The race has a 30-year history spanning thirteen races, with the inaugural event taking place in 1987.\n\nThe 30th anniversary event was held October 8–15, 2017.\n\nThe objective of this competition is to promote research on solar-powered cars. Teams from universities and enterprises participate. In 2015, 43 teams from 23 countries competed in the race.\n\nEfficient balancing of power resources and power consumption is the key to success during the race. At any moment in time the optimal driving speed depends on the weather forecast and the remaining capacity of the batteries. The team members in the escort cars will continuously remotely retrieve data from the solar car about its condition and use these data as input for prior developed computer programs to work out the best driving strategy.\n\nIt is equally important to charge the batteries as much as possible in periods of daylight when the car is not racing. To capture as much solar energy as possible, the solar panels are generally directed such that these are perpendicular to the incident sun rays. Sometimes the whole solar array is tilted for this purpose.\n\n\n\nThe idea for the competition originates from Danish-born adventurer Hans Tholstrup. He was the first to circumnavigate the Australian continent in a open boat. At a later stage in his life he became involved in various competitions with fuel saving cars and trucks. Already in the 1980s, he became aware of the necessity to explore sustainable energy as a replacement for the limited available fossil fuel. Sponsored by BP, he designed the world's first solar car, called The Quiet Achiever, and traversed the between Sydney, New South Wales and Perth, Western Australia in 20 days. That was the precursor of the World Solar Challenge.\n\nAfter the 4th race, he sold the rights to the state of South Australia and leadership of the race was assumed by Chris Selwood.\n\nThe race was held every three years until 1999 when it was switched to every two years.\n\nThe first edition of the World Solar Challenge was run in 1987 when the winning entry, GM's Sunraycer won with an average speed of . Ford Australia's \"Sunchaser\" came in second. The \"Solar Resource\", which came in 7th overall, was first in the Private Entry category.\n\nThe 1990 World Solar Challenge was won by the \"Spirit of Biel\", built by Biel School of Engineering and Architecture in Switzerland followed by Honda in second place. Video coverage here.\n\nThe 1993 World Solar Challenge was won by the Honda \"Dream\", and Biel School of Engineering and Architecture took second. Video coverage here.\n\nIn the 1996 World Solar Challenge, the Honda \"Dream\" and Biel School of Engineering and Architecture once again placed first and second overall, respectively.\n\nThe 1999 World Solar Challenge was finally won by a \"home\" team, the Australian Aurora team's \"Aurora 101\" took the prize while Queen's University was the runner-up in the closely contested WSC so far. The SunRayce class of American teams was won by Massachusetts Institute of Technology.\n\nThe 2001 World Solar Challenge was won by Nuna of the Delft University of Technology from the Netherlands, participating for the first time. Aurora took second place.\n\nIn the 2003 World Solar Challenge \"Nuna 2\", the successor to the winner of 2001 won again, with an average speed of , while Aurora took second place again.\n\nIn the 2005 World Solar Challenge the top finishers were the same for the third consecutive race as Nuon's \"Nuna 3\" won with a record average speed of , and Aurora was the runner-up.\n\nThe 2007 World Solar Challenge saw the Dutch Nuon Solar team scored their fourth successive victory with \"Nuna 4\" in the Challenge Class, averaging under the new, more restrictive rules, while the Belgian Punch Powertrain Solar Team's \"Umicar Infinity\" placed second.\n\nThe Adventure Class was added this year, run under the old rules, and won by Japanese Ashiya team's \"Tiga\".\n\nThe Japanese Ashiya team's \"Tiga\" won the Adventure Class, run under the old rules, with an average speed of .\n\nThe 2009 World Solar Challenge was won by the \"Tokai Challenger\", built by the Tokai University Solar Car Team in Japan with an average speed of . The longtime reigning champion Nuon Solar Team's \"Nuna 5\" finished in second place.\n\nThe Sunswift IV built by students at the University of New South Wales, Australia was the winner of the Silicon-based Solar Cell Class, while Japan's Osaka Sangyo University's \"OSU Model S\" won the Adventure class.\n\nIn the 2011 World Solar Challenge Tokai University took their second title with an updated \"Tokai Challenger\" averaging , and finishing just an hour before \"Nuna 6\" of the Delft University of Technology. The race was marred by delays caused by wildfires.\n\nThe 2013 World Solar Challenge featured the introduction of the Cruiser Class, which comprised more 'practical' solar cars with 2–4 occupants. The inaugural winner was Solar Team Eindhoven's \"Stella\" from Eindhoven University of Technology in the Netherlands with an average speed of , while the second place team was the \"SunCruiser\" from Hochschule Bochum in Germany, who inspired the creation of the Cruiser Class by racing more practical solar cars in previous WSC races. The Australian team, the University of New South Wales solar racing team Sunswift was the fastest competitor to complete the race, but was awarded third place overall after points were awarded for 'practicality' and for carrying passengers.\n\nIn the Challenger Class, the Dutch team from Delft University of Technology took back the title with \"Nuna 7\" and an average speed of , while defending champions Tokai University finished second after an exciting close race, which saw a 10–30 minute race distance, though they drained the battery in final stint due to bad weather and finished some 3 hours later; an opposite situation of the previous challenge in 2011.\n\nThe Adventure Class was won by Aurora's \"Aurora Evolution\".\n\nThe 2015 World Solar Challenge was held on October 18–25 with the same classes as the 2013 race.\n\nIn the Cruiser Class, the winner was once again Solar Team Eindhoven's \"Stella Lux\" from Eindhoven University of Technology in the Netherlands with an average speed of , while the second place team was Kogakuin University from Japan who was the first to cross the finish line, but did not receive as many points for passenger-kilometers and practicality. Bochum took 3rd place this year with the latest in their series of cruiser cars.\n\nIn the Challenger Class, the team from Delft University of Technology retained the title with \"Nuna 8\" and an average speed of , while their Dutch counterparts, the University of Twente, who led most of the race, finished just 8 minutes behind them in second place, making 2015 the closest finish in WSC history. Tokai University passed the University of Michigan on the last day of the race to take home the bronze.\n\nThe Adventure Class was won by the Houston High School solar car team from Houston, Mississippi, United States.\n\nThe 2017 World Solar Challenge was held on October 8–15, featuring the same classes as 2015. The Dutch NUON team won again in the Challenger class, which concluded on 2017-10-12, and in the Cruiser Class, the winner was once again Solar Team Eindhoven, from the Netherlands as well.\n\n\n\n\n"}
