{"id": "15885473", "url": "https://en.wikipedia.org/wiki?curid=15885473", "title": "1884 Howard, South Dakota tornado", "text": "1884 Howard, South Dakota tornado\n\nThe 1884 Howard, South Dakota tornado was a large tornado that occurred on August 28, 1884, near Howard, South Dakota, which was then part of the Dakota Territory. \n\nIt demolished one farm, located south of Bridgewater, South Dakota. Four people were killed there and two injured. It was one of an outbreak of four very strong tornadoes in the area that day. Contemporary records and survivors' recollections indicate that the storms were F3 or F4 on the Fujita scale, but cannot currently be verified.\n\nThis was one of the first tornadoes of which there is a photograph. The photographer was F. N. Robinson, who observed the tornado from a street in the town of Howard, about east of the storm track.\n\nThere was also another photographer from Huron, South Dakota who took multiple photographs of the same tornado an hour earlier but they were unfortunately destroyed during the engraving process.\n\n"}
{"id": "13472059", "url": "https://en.wikipedia.org/wiki?curid=13472059", "title": "1972 Great Daylight Fireball", "text": "1972 Great Daylight Fireball\n\nThe Great Daylight Fireball (or US19720810) was an Earth-grazing fireball that passed within of Earth's surface at 20:29 UTC on August 10, 1972. It entered Earth's atmosphere at a speed of in daylight over Utah, United States (14:30 local time) and passed northwards leaving the atmosphere over Alberta, Canada. It was seen by many people and recorded on film and by space-borne sensors. An eyewitness to the event, located in Missoula, Montana, saw the object pass directly overhead and heard a double sonic boom. The smoke trail lingered in the atmosphere for several minutes.\n\nThe atmospheric pass modified the object's mass and orbit around the Sun, but it is probably still in an Earth-crossing orbit and passed close to Earth again in August 1997. \n\nAnalysis of its appearance and trajectory showed the object was about 3–14 m (10–45 ft) in diameter, depending on whether it was a comet made of ices, or a stony and therefore denser asteroid. Other sources identified it as an Apollo asteroid in an Earth-crossing orbit that would make a subsequent close approach to Earth in August 1997. In 1994, Czech astronomer Zdeněk Ceplecha reanalysed the data and suggested the passage would have reduced the asteroid's mass to about a third or half of its original mass (reducing its diameter to .)\n\nThe object was tracked by military surveillance systems and sufficient data obtained to determine its orbit both before and after its 100-second passage through Earth's atmosphere. Its velocity was reduced by about and the encounter significantly changed its orbital inclination from 15 degrees to 7 degrees.\n\nThe US19720810 meteoroid is described in the preface of the first chapter of Arthur C. Clarke's \"The Hammer of God\".\n\nThe clip featuring the fireball is shown in the 1994 made-for-TV film \"Without Warning\", in which it is described as a 500-metre asteroid narrowly missing the Earth by just thousands of feet.\n\n\n"}
{"id": "42070652", "url": "https://en.wikipedia.org/wiki?curid=42070652", "title": "Abnormal grain growth", "text": "Abnormal grain growth\n\nAbnormal or discontinuous grain growth, also referred to as exaggerated or secondary recrystallisation grain growth, is a grain growth phenomenon through which certain energetically favorable grains (crystallites) grow rapidly in a matrix of finer grains resulting in a bimodal grain size distribution.\n\nIn ceramic materials this phenomenon can result in the formation of elongated prismatic, acicular (needle-like) grains in a densified matrix with implications for improved fracture toughness through the impedance of crack propagation.\n\nAbnormal grain growth (AGG) is encountered in metallic or ceramic systems exhibiting one or more of several characteristics.\n\nAlthough many gaps remain in our fundamental understanding of AGG phenomena, in all cases abnormal grain growth occurs as a result of very high local rates of interface migration and is enhanced by the localised formation of liquid at grain boundaries.\n\nAbnormal grain growth is often recorded as an undesirable phenomenon occurring during the sintering of ceramic materials as rapidly growing grains may lower the hardness of the bulk material through Hall Petch type effects. However, the controlled introduction of dopants to bring about controlled AGG may be used to impart fibre-toughening in ceramic materials. In piezoelectric ceramics the occurrence of AGG may bring about the degradation of piezoelectric effect and thus in these systems AGG is avoided.\n\n\n"}
{"id": "1740813", "url": "https://en.wikipedia.org/wiki?curid=1740813", "title": "Acid salt", "text": "Acid salt\n\nAcid salts are a class of salts that produce an acidic solution after being dissolved in a solvent. Its formation as a substance has a greater electrical conductivity than that of the pure solvent. An acidic solution formed by acid salt is made during partial neutralization of diprotic or polyprotic acids. A \"half-neutralization\" occurs due to the remaining of replaceable hydrogen atoms from the partial dissociation of weak acids that have not been reacted with hydroxide ions (OH) to create water molecules. Acid salt is an ionic compound consisted of an anion, contributed from a weak parent acid, and a cation, contributed from a strong parent base.\n\nAcid-base property of the resulting solution from a neutralization reaction depends on the remaining salt products. A salt containing reactive cations undergo hydrolysis by which they react with water molecules, causing deprotonation of the conjugate acids.\n<br>\nFor example, the acid salt ammonium chloride is the main species formed upon the \"half neutralization\" of ammonia in hydrochloric acid solution:\n\nSome acid salts such as sodium bicarbonate, NaHCO are used in baking. They are found in baking soda, bread soda or cooking soda and are typically divided into low-temperature (or single-acting) and high-temperature (or double-acting) acid salts. Common low-temperature acid salts react at room temperature to produce a leavening effect. They include cream of tartar, calcium phosphate, and citrates. High-temperature acid salts produce a leavening effect during baking and are usually aluminium salts such as calcium aluminium phosphate. Some acid salts may also be found in non-dairy coffee creamers. And also disodium phosphate, NaHPO is used in foods and monosodium phosphate, NaHPO is used in animal feed, toothpaste and evaporated milk.\n\nAn acid with higher \"K\" value dominates the chemical reaction. It serves as a better contributor of proton (H). A comparison between the \"K\" and \"K\" indicates the acid-base property of the resulting solution by which:\nOther possible factors that could vary pH level of a solution are the relevant equilibrium constants and the additional amounts of any base or acid.\n\n"}
{"id": "40400006", "url": "https://en.wikipedia.org/wiki?curid=40400006", "title": "Air bearing", "text": "Air bearing\n\nAir bearings (also known as aerostatic or aerodynamic bearings) are bearings that use a thin film of pressurized gas to provide a low friction load-bearing interface between surfaces. The two surfaces do not touch, thus avoiding the traditional bearing-related problems of friction, wear, particulates, and lubricant handling, and offer distinct advantages in precision positioning, such as lacking backlash and static friction, as well as in high-speed applications.\n\nA differentiation is made between aerodynamic bearings, which establish the air cushion through the relative motion between static and moving parts, and aerostatic bearings, in which the pressure is being externally inserted.\n\nGas bearings are being mainly used in precision machinery tools (measuring and processing machines) and high-speed machines (spindle, small-scale turbomachinery, precision gyroscopes).\n\nGas-lubricated bearings are classified in two groups, depending on the source of pressurization of the gas film providing the load-carrying capacity:\n\nHybrid bearings combining the two families also exist. In such cases, a bearing is typically fed with externally-compressed gas at low speed and then relies partially or entirely on the self-pressurizing effect at higher speeds.\n\nAmong these two technological categories, gas bearings are classified depending on the kind of linkage they realize:\n\nThe main air bearing types fall under the following categories:\n\nPressurized gas acts as a lubricant in the gap between bearing moving parts. The gas cushion carries the load without any contact between the moving parts. Normally, the compressed gas is supplied by a compressor. A key goal of supplying the gas pressure in the gap is that the stiffness and damping of the gas cushion reaches the highest possible level. In addition, gas consumption and uniformity of gas supply into the gap are crucial for the behaviors of aerostatic bearings.\n\nSupplying gas to the interface between moving elements of an aerostatic bearing can be achieved in a few different methods:\n\nThere is no single best approach to feeding the film. All methods have their advantages and disadvantages specific to each application.\n\nDead volumes refer in particular to chambers and canals existing in conventional aerostatic bearings in order to distribute the gas and increase the compressed pressure within the gap. The cavity inside porous (sintered) gas bearings are also attributed to dead volume.\n\nWith conventional single nozzle aerostatic bearings, the compressed air flows through a few relatively large nozzles (diameter 0.1 – 0.5 mm) into the bearing gap. The gas consumption thus allows only some flexibility such that the bearing’s features (force, moments, bearing surface, bearing gap height, damping) can be adjusted only insufficiently. However, in order to allow a uniform gas pressure even with only some nozzles, aerostatic bearing manufacturers take constructive techniques. In doing so, these bearings cause dead volumes (non-compressible and thus weak air volume). In effect, this dead volume is very harmful for the gas bearing’s dynamic and causes self-excited vibrations.\n\nThe pre-pressured chamber consists of a chamber around the centralized nozzle. Usually, this chamber’s ratio is between 3% and 20% of the bearing’s surface. Even with a chamber depth of 1/100 mm, the dead volume is very high. In the worst cases, these air bearings consist of a concave bearing surface instead of a chamber. Disadvantages of these air bearings include a very poor tilt stiffness.\n\nTypically, conventional aerostatic bearings are implemented with chambers and canals. This design assumes that with a limited amount of nozzles, the dead volume should decrease while distributing the gas within the gap uniformly. Most constructive ideas refer to special canal structures. Since the late 1980s, aerostatic bearings with micro canal structures without chambers are manufactured. However, this technique also has to manage problems with dead volume. With an increasing gap height, the micro canal’s load and stiffness decreases. As in the case of high-speed linear drives or high-frequency spindles, this may cause serious disadvantages.\n\nLaser-drilled micro nozzle aerostatic bearings make use of computerized manufacturing and design techniques to optimize performance and efficiency. This technology allows manufacturers more flexibility in manufacturing. In turn this allows a larger design envelope in which to optimize their designs for a given application. In many cases engineers can create air bearings that approach the theoretical limit of performance.\nRather than a few large nozzles, aerostatic bearings with lots of micro nozzles avoid dynamically disadvantageous dead volumes. Dead volumes refer to all cavities in which gas cannot be compressed during decrease of the gap. These appear as weak gas pressure stimulates vibration. Examples of the benefits are: linear drives with accelerations of more than 1,000 m/s² (100 g), or impact drives with even more than 100,000 m/s² (10,000 g) due to high damping in combination with dynamic stiffness; sub-nanometer movements due to lowest noise-induced errors; and seal-free transmission of gas or vacuum for rotary and linear drives via the gap due to guided air supply.\n\nMicro-nozzle aerostatic bearings achieve an effective, nearly perfect pressure distribution within the gap with a large number of micro nozzles. Their typical diameter is between 0.02 mm and 0.06 mm. The narrowest cross-section of these nozzles lies exactly at the bearing’s surface. Thereby the technology avoids a dead volume on the supporting air bearing’s surface and within the area of the air supplying nozzles.\n\nThe micro nozzles are automatically drilled with a laser beam that provides top-quality and repeatability. The physical behaviors of the air bearings prove to have a low variation for large as well as for small production volumes. In contrast to conventional bearings, with this technique the air bearings require no manual or costly manufacturing.\n\nThe advantages of the micro-nozzle air bearing technology include:\n\nSome of these advantages, such as the high flexibility, the excellent static and dynamic properties in combination, and a low noise excitation, prove to be unique among all other aerostatic bearings.\n\nStandard air bearings are offered with various mountings to link them in a system:\n\n\n\nGas-lubricated bearings are usually modeled using the Reynolds equation to describe the evolution of pressure in the thin film domain. Unlike liquid-lubricated bearings, the gas lubricant has to be considered as compressible, leading to a non-linear differential equation to be solved.\nNumerical methods such as Finite difference method or Finite element method are common for the discretization and the resolution of the equation, accounting for the boundary conditions associated to each bearing geometry (linear-motion, journal and thrust bearings). In most cases, the gas film can be considered as isothermal and respecting the ideal gas law, leading to a simplification of the Reynolds equation.\n\nEven for movements which cause damage due to disruptive wear with roller bearings, lifetimes of the drive systems are unlimited.\nIn order to provide confidence and for the first investigations, an initial conversion from a conventional oil-guided turbo charger into air-guided was done. For a real future version, the use of results obtained from high-temperature solutions, mass products (proved production costs) and high-frequency spindles (know-how of dynamic background) will be very helpful.\n\nIn terms of the measurement of wafers and flat panels, it is very important to place the sensor chip precisely and without any contact along the surface. Therefore, the chip is integrated directly into the bearing’s surface. The maximum distance tolerance to the surface which refers to the gap variation of the air bearing, is smaller than 0.5 µm. When placing the air bearing with the sensor chip, they must not touch the wafer surface being measured. As for the up-and-down movement, a pneumatic piston is used which is, for repeatability reasons, also air-guided. The preload of the air bearing and thus the gap height are also adjusted with this piston.\nFor the electrical testing of wafers the chuck can be lifted stick-slip-free up to 3 mm. The needed contact force for the probe is adjustable and independent from stroke. The lift drive is based on a voice coil motor; the guidance is air-guided. An air-guided pneumatic piston between the chuck and the drive limits the contact force.\n\nThe filigree structure enables through light measurements for the 300 nm chip production with the utmost precision of less than 1 nm. In particular, the air bearings are designed for lowest air consumption with the highest stiffness.\nThe High-accelerated Doppler drive supports and guides a carbon fiber mirror (surface 500 mm x 250 mm) with an acceleration of up to 300 m/s² and a flexible movement profile with high precision. The solution consists of an air-guided drive: The beam (length 900 mm), which is fixed at the mirror, is manufactured of carbon fibre and carries the magnets of the linear motors. Cables/tubes (motor, air bearing, measurement system) do not move in order to avoid breakages due to high load cycles. The air-bearings are absolutely insensitive against geometric fluctuation as a result of a temperature change.\nBeside the performance, the reliability is extremely important for a production machine. The air-guided solution is designed to be statically determined. The iron-core linear motor and piston bearings achieve the preload for the air bearings. Thereby, the drive is easy to assemble and insensitive against geometric variations, for instance through temperature influences or the disposition of the machines.\n\nFat- and oil-free drives for respirators, stick-slip-free movements of scanners or a high rotary speed of large rotors have all been achieved with air bearings.\nHigh rotary speed (> 5.5 Hz / 330 rpm), low operation costs, no noise, large inner rotor diameter (> 1 m), small weight of rotor and frame, tilt possibility of the rotor as well as a high reliability. Besides a direct drive, a belt drive is also possible.\n\nPrimarily, stick-slip-free movements and/or smallest forces are required. The air bearing technology is predestinated for fat/oil-free high-dynamic movements with short strokes.\nWith air-guided units, optical components can be arranged to have the same diameter on a rotary table. The air bearing with vacuum preload and a constant bearing gap height floats contact-less on top of the rotary table.\nThe linear slider, which is air-guided and statically determined, guarantees a high-precision positioning of the optical component before grinding. The self-aligning process is done without friction or force. When clamped the component retains its position for further manufacturing in the sub-micrometer-range.\n\nWhen transporting solar panels for satellites in a launching rocket, these must be folded. After reaching orbit, they unfold via a spring mechanism, weightlessly and without friction. This process requires prior testing on Earth due to reliability reasons. During the testing design, the solar panels are hung on magnetic preloaded air-bearings that compensate for gravity. In doing so, the unfolding movement process is carried out with a minimum friction impact which means that the solar panels are tested at close to reality. Moreover, the design offers absolutely maintenance-free handling with equal sequential movements.\n\nThe air-bearing components (diameter 34 mm) with integrated magnets are so small such that they are able to glide contact-free along conventional rolled sheet plates smoothly and with a bearing gap height of about 25 µm. The holding force of an air bearing for one solar panel averages 600 N. This force is achieved by an equal distribution of the load on 16 single air bearing elements. The unfolding process of the solar panels has been developed for an area of 21 m x 2.5 m.\n\nThe permanent magnetic preloaded air-bearing guidance system may be used for many types of hanging transportation movements as well as for many other applications, such as for instance for the stick-slip-free positioning of components during assembly.\n\n"}
{"id": "18501354", "url": "https://en.wikipedia.org/wiki?curid=18501354", "title": "Aqaba Thermal Power Plant", "text": "Aqaba Thermal Power Plant\n\nThe Aqaba Thermal Power Station is the largest power station in Jordan. It has a total generation capacity of 656 MW, which consists of five steam turbines units (5 x 130 MW), and two hydraulic turbines (2 x 3 MW). The power station is fueled by natural gas and by fuel oil. It is operated by the Central Electricity Generating Company of Jordan.\n\nThe Aqaba Thermal Power Station was established in 1986 as an oil-fueled power station. After construction of the Arab Gas Pipeline, the power station was switched to use natural gas.\n"}
{"id": "42004809", "url": "https://en.wikipedia.org/wiki?curid=42004809", "title": "Australian Renewable Energy Agency", "text": "Australian Renewable Energy Agency\n\nThe Australian Renewable Energy Agency (ARENA) is an independent agency of the Australian federal government, established in 2012 to manage Australia's renewable energy programs, with the objective of increasing supply and competitiveness of Australian renewable energy sources.\n\nARENA was established in 2012 as an independent statutory authority to manage the government's renewable energy programs. Legal establishment came with the passing of the Australian Renewable Energy Agency Act 2011 (ARENA Act). The legislation passed parliament in November 2011 with the support of the Australian Greens and the Liberal and National coalition opposition as well as the governing Labor Party. ARENA commenced operations on 1 July 2012. The agency resulted from negotiations within the Australian parliament under the Gillard Government, with the intention of providing more secure funding for renewable energy programs in the context of political changes.\n\nWhile ARENA was created as part of the Clean Energy Future package together with the Clean Energy Finance Corporation, these are separate institutions. ARENA has consolidated various earlier renewable programs and research and development projects from the Australian Centre for Renewable Energy, the Australian Solar Institute and the former Department of Resources, Energy and Tourism.\n\nThe agency's responsibilities cover funding of renewable energy research and development, demonstration and commercialisation, including large scale deployment, as well as the sharing of knowledge and information about these technologies.\n\nARENA was established with a total funding allocation of $3.2 billion out to 2020. In the 2013 budget the Labor government deferred $370 million of the agency's funding, extending the timeline to 2022. The subsequent Abbott Government proposed to cut $435 million from ARENA's budget, followed by an additional $40 million, but has affirmed its support for the agency.\n\nARENA carries out its mission through programs such as these:\n\nApproximately $1 billion of ARENA's budget has been committed to various projects, including: \n\n\n"}
{"id": "3907710", "url": "https://en.wikipedia.org/wiki?curid=3907710", "title": "Automatic quartz", "text": "Automatic quartz\n\nAutomatic quartz is a collective term describing watch movements that combine a self-winding rotor mechanism (as used in automatic mechanical watches) to generate electricity with a piezoelectric quartz crystal as its timing element. Such movements aim to provide the advantages of quartz without the inconvenience and environmental impact of batteries. Several manufacturers employ this technique.\n\nA rotating pendulum inside the case is attached to a relatively large gear which meshes with a very small pinion. As the wearer moves, the pendulum turns and spins the pinion at a very high speed - up to 100,000 rpm. This is coupled to a miniature electrical generator which charges a storage device which is a capacitor(s) or a rechargeable battery. A typical full charge will last between two weeks and six months.\n\nJapanese company Seiko pioneered the technique which it unveiled at the Baselworld 1986 trade show under the trial name AGM. \nThe first such watch was released in Germany in January 1988 and April of the same year in Japan (under the name Auto-Quartz). The watches had an average monthly rate of ±15 sec and provided 75 hours of continuous operation when fully powered. Early \"automatic quartz\" movements were called AGS (Automatic Generating System).\n\nIn 1991 the company introduced the Kinetic brand name. \n\nToday Seiko offers a wide range of watches with various Kinetic movements. The top of the line is the caliber 9T82, included in Sportura (international brand) and PROSPEX (only marketed in Japan) Collection. It is sold in limited volume at a price range of about US$3000 which makes it one of the most expensive automatic quartz watches. \nKinetic technology has also been used in some of Seiko's Pulsar and Lorus watches. As of 2007, Seiko has sold more than eight million automatic quartz watches.\n\nThe different calibres of Kinetic watches currently are relatively large and heavy, weighing in at 1/3 of a pound (150 grams) or more on many models. Therefore, most Seiko Kinetic watches are only available in a men's size.\n\nMovement calibers：\n\n(*) In use as of at Aug-2008\n\nSwiss company ETA SA, part of the Swatch group, made seven different automatic quartz movements, calling them Autoquartz. They were part of the premium Flatline series of movements and were sold to a variety of watch vendors, primarily European and American. High grade movements designed to last as long as their premium mechanical movements, they had between 15 and 53 jewels. Unlike most quartz watches, Autoquartz could be calibrated to increase their accuracy. Several vendors had their Autoquartz watches COSC certified. In 2006 to increase production of its highly demanded mechanical movements, Swatch discontinued supplying the Autoquartz line to customers (service and parts are still available). Then in 2009, possibly due to available production capacity or stocked parts, Tissot reintroduced the Autoquartz in its PRC200 dive watch. The Autoquartz movement used by Tissot is gold plated and carries the designation ETA 205.914.\n\nMovement calibers:\n\nManufacturers who employ or employed ETA movements: Tissot, Rado in their Accustar line of watches, Longines, Swatch, Omega (Omega Seamaster Omega-matic), Dugena (K-Tech), Wenger (GST Field Terragraph Autoquartz), Hermès (Nomade), Roberge (Altaïr), Mido (Multifort), Bovet (Autoquartz calibre 11BQ01), Fortis (Spacematic Eco), Belair (Autoquartz), Franck Muller (Transamerica), HTO (Grand Voyager) and Cyma.\n\nCitizen, one of the world's largest watch manufacturers, also built an autoquartz-powered watch: the Eco-Drive Duo (released in December 1998). Novel to this watch was the use of both mechanical power as well as a solar cell. This model was an attempt to enter higher-priced markets (at a cost of around $1000 USD), but the technology failed to attract consumer interest and Citizen has since stopped making use of the unique movement. No other autoquartz powered watch from Citizen is known; all other Eco-Drive models only use solar power or thermal power.\n\nVentura is a small Swiss watch manufacturer claiming to be \"the World's only manufacturer of automatic digital watches\". Their VEN_99 movement was the only watch to ever combine autoquartz and digital readout of time (LCD) in one package. Offered were three models: the Sparc rx, fx and px. In late 2006, the company started selling their movement with an incorporated alarm, another exclusive feature. All hardware is proprietary to Ventura.\n\nIn 2007 the company went into bankruptcy. Support was available from an independent entity. In 2011 the company re-emerged from bankruptcy and continued to sell its models, introducing the \"2nd gen Micro-Generating-System\" and marketing the watch (Sparc MGS) integrating it as the world's first and only digital-readout multi-function automatic quartz module. Unlike with other manufacturers the watch movement (VEN_10) and power source (MGS) are separate units, only linked by a single wire.\n\nIn spite of the relatively complex mechanical parts used, Seiko has positioned their kinetic watches to be medium-priced. Exceptions are kinetic with other complications such as chronograph movement 9T82, 7L22 and direct drive movements. ETA sold Autoquartz to a variety of Swiss manufacturers with pricing below $100 (Swatch) to multiple thousands (Omega, Baume et Mercier, et al.). Ventura prices its automatic quartz watches at around 2000-4000 Euro.\n\n\n"}
{"id": "17349402", "url": "https://en.wikipedia.org/wiki?curid=17349402", "title": "Betafite", "text": "Betafite\n\nBetafite is a mineral group in the pyrochlore supergroup, with the chemical formula (Ca,U)(Ti,Nb,Ta)O(OH). Betafite typically occurs as a primary mineral in granite pegmatites, rarely in carbonatites. Defined by the B-site atom Ti, Atencio et al.(2010) combined and considered the ideas portrayed in (Hatert and Burke)(2008) and a modernization of (Hogarth)(1977) system for nomenclature of pyrochlore and betafite in order to further rationalize the naming process of this grouping of minerals. Therefore, Atencio et al. (2010), states that only two of the mineral species that were formerly recognized under the previous nomenclature system of betafite in Hogarth (1977) are now recognized. They are \"oxyuranobetafite\" and \"oxycalciobetafite\". Now the term betafite is a synonym or varietal group name under the pyrochlore super group (Christy and Atencio 2013). \nThe pyrochlore supergroup minerals conform to the general formula, ABXY, where the \"m\", \"w\", and \"n\" variables represent the parameters that indicate incomplete occupancy of the A, Y, and X sites (Atencio et al. 2010). They crystallize isometric system with a space group of Fdm or its subgroups where Betafite has a hexoctahedral class (Hogarth 1977). Site A is generally an 8-coordinated cation with a ~1.0 Å radius, B site is generally a 6-coordinated cation, which contains the elements Ti, Nb, and Ta usually for betafite, (Atencio et al. 2010, site X is generally O but can subjugate to OH and F, and site Y is typically an anion but can also be a vacancy, H2O, or a very large monovalent cation, like Cs, K, and Rb.\n\nThe oxycalciobetafite in Camara et al. (2004) occurs in the pyroclastic formation belonging to the main effusive stage of the Vico activity where it is contained within foid-bearing syenite, which also holds optical observances of K-feldspar, and minor amphibole, plagioclase, magnetite, sodalite and rare biotite. While with SEM-EDS, titanite, apatite, and baddeleyite were also observed. While the geologic occurrence of oxycalciobetafite located on the moon has been rather difficult to ascertain. Oxyuranobetafite is also located on the moon and is described in Mokhov et al. (2008).\n\nBetafite is an important ore of thorium, uranium, and niobium. Though, there is no real applicable use as of yet for the accepted betafite species, the pyrochlore super-group that contains the former betafites of Hogarth (1977) are mentioned in Lumpkin and Ewing (1996), Turner (1928) and many others, all with the recurring theme of either extracting uranium, thorium, and niobium from the pyrochlore super-group ores. The depletion of urananite rich ore bodies has led to the search and mild application of refractory uranium minerals as a source of uranium to keep up with the increasing demands.\n\n"}
{"id": "19781957", "url": "https://en.wikipedia.org/wiki?curid=19781957", "title": "BirdLife South Africa", "text": "BirdLife South Africa\n\nBirdLife South Africa, formerly the South African Ornithological Society (SAOS), is the South African national partner organisation of BirdLife International.\n\nIt has a membership of 5 000, many of whom belong to more than 32 affiliated bird clubs. BirdLife South Africa's vision is to promote the enjoyment, understanding, study and conservation of wild birds and their habitats. It publishes an ornithological journal, \"Ostrich\", covering the birds of Africa and its islands, as well as the magazine, African BirdLife.\n\nOne of the major projects with which it is involved is the Second Southern African Bird Atlas Project (SABAP2). It is one of three partners which lead this project: the other two are the Animal Demography Unit at the University of Cape Town and the South African National Biodiversity Institute (SANBI).\n\nBirdLife South Africa has three Honorary patrons, Mrs Gaynor Rupert, Dr Precious Moloi-Motsepe and Mr Mark Shuttleworth.\n\n\n\n"}
{"id": "12597201", "url": "https://en.wikipedia.org/wiki?curid=12597201", "title": "Brunei LNG", "text": "Brunei LNG\n\nThe Brunei LNG (BLNG) is the LNG plant in Lumut, Brunei. Established in 1969 and was opened in 1973, it was the first LNG plant in the Western Pacific. The operating company—Brunei LNG Sdn Bhd—is owned by the Government of Brunei (50%), Shell Overseas Trading Limited and Mitsubishi Corporation (both 25%). Brunei LNG operates five LNG trains and produces 6.71 million tonnes every year of liquified natural gas. It has approximately 500 personnel.\n\nThe facility uses Air Products' AP-C3MR process and has three LNG storage tanks capable of holding .\n\nBrunei LNG operates seven LNG carriers through the joint venture company, Brunei Shell Tankers. The first four carriers were delivered between October 1972 and October 1975, with a maximum storage capacity of . These older ships were built in France. The three newer vessels were built in Korea (Amali/Arkat) and Japan (Abadi) by Daewoo and Mitsubishi Heavy Industries|Mitsubishi Nagasaki respectively. The first of these three ships was delivered in June 2002, with the most recent ship, Amadi, being delivered in July 2011. These ships hold between of LNG collectively. \n\n"}
{"id": "207561", "url": "https://en.wikipedia.org/wiki?curid=207561", "title": "CPU power dissipation", "text": "CPU power dissipation\n\nCentral processing unit power dissipation or CPU power dissipation is the process in which central processing units (CPUs) consume electrical energy, and dissipate this energy in the form of heat due to the resistance in the electronic circuits.\n\nDesigning CPUs that perform tasks efficiently without overheating is a major consideration of nearly all CPU manufacturers to date. Some CPU implementations use very little power; for example, the CPUs in mobile phones often use just a few watts of electricity, while some microcontrollers used in embedded systems may consume only a few milliwatts or even as little as a few microwatts. In comparison, CPUs in general-purpose personal computers, such as desktops and laptops, dissipate significantly more power because of their higher complexity and speed. These microelectronic CPUs may consume power in the order of a few watts to hundreds of watts. Historically, early CPUs implemented with vacuum tubes consumed power on the order of many kilowatts.\n\nCPUs for desktop computers typically use a significant portion of the power consumed by the computer. Other major uses include fast video cards, which contain graphics processing units, and power supplies. In laptops, the LCD's backlight also uses a significant portion of overall power. While energy-saving features have been instituted in personal computers for when they are idle, the overall consumption of today's high-performance CPUs is considerable. This is in strong contrast with the much lower energy consumption of CPUs designed for low-power devices. One such CPU, the Intel XScale, can run at 600 MHz consuming under 1 W of power, whereas Intel x86 PC processors in the same performance bracket consume a few times more energy.\n\nThere are some engineering reasons for this pattern. \n\nProcessor manufacturers usually release two power consumption numbers for a CPU:\n\nFor example, the Pentium 4 2.8 GHz has 68.4 W typical thermal power and 85 W maximum thermal power. When the CPU is idle, it will draw far less than the typical thermal power. Datasheets normally contain the thermal design power (TDP), which is the maximum amount of heat generated by the CPU, which the cooling system in a computer is required to dissipate. Both Intel and Advanced Micro Devices (AMD) have defined TDP as the maximum heat generation for thermally significant periods, while running worst-case non-synthetic workloads; thus, TDP is not reflecting the actual maximum power of the processor. This ensures the computer will be able to handle essentially all applications without exceeding its thermal envelope, or requiring a cooling system for the maximum theoretical power (which would cost more but in favor of extra headroom for processing power).\n\nIn many applications, the CPU and other components are idle much of the time, so idle power contributes significantly to overall system power usage. When the CPU uses power management features to reduce energy use, other components, such as the motherboard and chipset, take up a larger proportion of the computer's energy. In applications where the computer is often heavily loaded, such as scientific computing, performance per watt (how much computing the CPU does per unit of energy) becomes more significant.\n\nThere are several factors contributing to the CPU power consumption; they include dynamic power consumption, short-circuit power consumption, and power loss due to transistor leakage currents:\n\nformula_1\n\nThe dynamic power consumption originates from the activity of logic gates inside a CPU. When the logic gates toggle, energy is flowing as the capacitors inside them are charged and discharged. The dynamic power consumed by a CPU is approximately proportional to the CPU frequency, and to the square of the CPU voltage:\nwhere is capacitance, is frequency, and is voltage.\n\nWhen logic gates toggle, some transistors inside may change states. As this takes a finite amount of time, it may happen that for a very brief amount of time some transistors are conducting simultaneously. A direct path between the source and ground then results in some short-circuit power loss. The magnitude of this power is dependent on the logic gate, and is rather complex to model on a macro level.\n\nPower consumption due to leakage power emanates at a micro-level in transistors. Small amounts of currents are always flowing between the differently doped parts of the transistor. The magnitude of these currents depend on the state of the transistor, its dimensions, physical properties and sometimes temperature. The total amount of leakage currents tends to inflate for increasing temperature and decreasing transistor sizes.\n\nBoth dynamic and short-circuit power consumption are dependent on the clock frequency, while the leakage current is dependent on the CPU supply voltage. It has been shown that the energy consumption of a program shows convex energy behavior, meaning that there exists an optimal CPU frequency at which energy consumption is minimal.\n\nPower consumption can be reduced in several ways, including the following:\n\nHistorically, processor manufacturers consistently delivered increases in clock rates and instruction-level parallelism, so that single-threaded code executed faster on newer processors with no modification. More recently, in order to manage CPU power dissipation, processor makers favor multi-core chip designs, thus software needs to be written in a multi-threaded or multi-process manner to take full advantage of such hardware. Many multi-threaded development paradigms introduce overhead, and will not see a linear increase in speed when compared to the number of processors. This is particularly true while accessing shared or dependent resources, due to lock contention. This effect becomes more noticeable as the number of processors increases.\n\nRecently, IBM has been exploring ways to distribute computing power more efficiently by mimicking the distributional properties of the human brain.\n\n\n"}
{"id": "54135206", "url": "https://en.wikipedia.org/wiki?curid=54135206", "title": "Charter Stones", "text": "Charter Stones\n\nCharter Stones date back to ancient times when such stones were granted to individuals or communities in lieu of written charters to signify the granting of land by the crown, feudal overlords or other individuals. They were used to record ownership of land before written documents came into general use.\n\nThe stones were sometimes engraved or were instead distinctive in terms of colour, composition or shape. The use of these stones may relate to the common practice of using boundary stones to establish precise limits to areas of land ownership but differ in that the proof of the land ownership was invested in them. Being stone they had a permanence that gave them an advantage over charters written on vellum, etc. An essential element was that the stone in question had once belonged to the donor and was next held by the grantee. The size, weight and individuality of charter stones helped to ensure that they were not easily stolen or moved any significant distance.\n\nIn Old Dailly a tradition has it that when worship was discontinued at the old kirk in 1695 the people of Dailly insisted that the 'Blue stone' or charter stone should be removed from the old kirk to the new parish church. The people of Old Dailly refused to part with their ancient stone and in his historical notes located in Sir Walter Scott’s “Lord of The Isles”, he records this conflict in Dailly parish and notes that it was settled ‘when man, woman and child form both communities marched out and by one desperate engagement put an end to the conflict’. The inhabitants of Old Dailly triumphed and the 'Blue Stone' still resides in the old churchyard. Old Dailly is sometimes recorded as the 'Blue Stone Burgh'. Smith in 1895 mentions charter stones plural and suggests that the tradition of trials of strength, in common with the 'Leper's Charter Stone' at Prestwick, was linked with proof that the person granted the land involved was mature enough to hold it.\n\nAt the Bruce's Well, Kingcase in Prestwick, records describe the \"Lepers' Charter Stone\" as being the shape of a sheep's kidney, formed of basalt, blue in colour and as smooth as glass. It weighed around 15 stones or 95 Kilograms and its weight was such that it could only be lifted with arms extended and cupped in a cavity in the stone. Lifting it was considered proof of the passage to manhood. Sources state that some English dragoons encamped one night at Bruce's Well where the charter stone was kept and somehow managed to break it. The pieces were collected and carefully kept by the freemen of Prestwick and later incorporated into the consolidated walls of the old St Ninian's Chapel (see video). Another source states that \"\"... a low character of the name of Allison, who rented a salt-pan in the neighbourhood, about the year 1800 bribed a drunk man, who broke the stone in pieces.\" \nThe \"Clach na Cudainn\" (Gaelic for 'stone of the tub') or \"Charter Stone of Inverness\" was kept in the market place, set in a frame and bound with iron. Also known as the Clachnacuddin Stone it is said that as long as the stone is preserved then Inverness will flourish. The upper surface was supposedly worn flat by the action of women resting their tubs or pails on it whilst they rested. \n\nNear Blair Atholl at Pitagowan in Perthshire the \"Clach na h-Iobairt\" (the stone of offering) or 'Bridge of Tilt' standing stone is said to be a charter stone that records a land grant, possibly to the Church of Kilmaveonaig. \nThe Stone of Scone was regarded as being the charter stone of the Kingdom of Scotland. \n\nIn Wales charter stones are recorded from Llanllyr in Merioneth (circa 8th century) and Merthyr Mawr (11th century) and Ogmore (11th century) in Glamorgan. The example at Merthyr Mawr carries the inscription \"in grefium in propium\" which translates literally as \"ownership was registered\"\" and commemorates St Glywys. The Ogmore charter stone also honours St Glywys and records a Bishop Fili who was the grantee of an \"ager\", a field. \n\nIn England the ancient London Stone has been put forward as a charter stone due to its proximity to lands once held by Canterbury Cathedral. The Kirkby Stephen charter stone in Cumbria is till used on St Luke’s Fair day in October for the reading of the market's charter. Also in Cumbria is the \"Ca'an Stone\" in the main street of Kendal, once part of the market cross, but possibly older and once used as a place where proclamations were read.\n\nA possible example has been recorded at Stoke near Hartland in North Devon. \n\nIn Ireland's County Armagh an eighth century example of a charter stone is recorded at Kilnasaggart.\n\nIn the Isle of Man it is recorded that it was common for charter stones to be given as a sign of transfer of ownership, the stone itself once having been the property of the donor.\n\nRecords show that before the Norman conquest other items that once belonged to the donor of property were given in lieu of a written charter, such as swords, helmets and especially horns.\n\nTradition has it that charter stones are sometimes possessed with special powers such as bringing good luck to those who touch them and in some cases they can supposedly cure certain illness, etc. The Old Dailly stone is said to have held the right of being a sanctuary stone. \n\nThe larger of the two Old Dailly charter stones weighs between and the smaller between and like the old Lepers' charter stone at Prestwick is smooth and their shape make them difficult to grip them easily and over the years they became a weight lifting challenge. In international stone lifting circles the name \"The Big Blue\" was the name given to the largest Old Dailly stone as a ‘lifting’ or ‘testing’ stone however the local council have bound both with metal hoops and they cannot at present be lifted.\n\n"}
{"id": "5398124", "url": "https://en.wikipedia.org/wiki?curid=5398124", "title": "Chashma Nuclear Power Plant", "text": "Chashma Nuclear Power Plant\n\nThe Chashma Nuclear Power Plant (CHASNUPP) or Chashma Nuclear Power Complex, near Chashma Colony and Kundian town, Mianwali District, Punjab, Pakistan, is a commercial nuclear power plant consisting of four operating units (CHASHNUPP-I, CHASHNUPP-II, CHASHNUPP-III and CHASHNUPP-IV) and one planned unit (CHASHNUPP-V). Chashma Nuclear Power Plant reactors and other facilities are being built and operated by the Pakistan Atomic Energy Commission (PAEC) with Chinese support under the approval and guidelines of International Atomic Energy Agency.\n\nThe IAEA as well as the United States Department of Energy recognised the urgency of Pakistan's energy needs, which is expected to grow seven to eight times by 2030.\n\nIn November 2006, The International Atomic Energy Agency approved an agreement with the Pakistan Atomic Energy Commission for new nuclear power plants to be built in the country with Chinese assistance. The 35-member Board of Governors of the IAEA unanimously approved the safeguards agreement for any future Nuclear Power Plants that Pakistan will be constructing.\n\nThe 325 MW unit 1 (CHASNUPP-I) is a pressurised water reactor that began commercial operation in May 2000. It is an CNP-300 nuclear reactor design like the other 3 units in the Chashma Nuclear Power Plant, and the first Chinese export of a nuclear power plant.\n\nThe 325 MW unit 2, (CHASNUPP-II) is like CHASNUPP-I. Unit 2 was officially inaugurated on 10 May 2011 by former Prime Minister Yousaf Raza Gillani.\n\nOn 28 April 2009 a general engineering and design contract for CHASNUPP-3 and CHASNUPP-4 was signed with \"Shanghai Nuclear Engineering Research and Design Institute\" (SNERDI). The units will both have generation capacity of 340 MW and a design life of 40 years.On 6 March 2013 the dome of the third reactor was lifted into place.\n\nThe 340 MW Unit 3 (CHASNUPP-3) was officially inaugurated on 28 December 2016 by Prime Minister Nawaz Sharif.\n\nThe reactor type is CNP-300.\n\nCHASHNUPP-4 has been connected to national grid on 29 June, 2017 . Formal inauguration is to be held on 08 September, 2017 by Prime Minister of Pakistan, Shahid Khaqan Abbasi.\n\nThe reactor type is also CNP-300.\n\nOn March 2013, Pakistan and China agreed to build a fifth unit (CHASNUPP-5). It will be an ACP-1000. China National Nuclear Corporation and the Pakistan Atomic Energy Commission had signed a cooperation agreement for the construction of a 1,100 MW ACP1000 (Hualong One) nuclear reactor at the Chashma nuclear power plant in Punjab province in Pakistan.\n\nThe complex has its own training establishment, known as CHASCENT (CHASNUPP Center of Nuclear Training). CHASCENT houses a Full Scope Training Simulator (FSTS) for CHASNUPP-1 and a similar facility for CHASNUPP-2 is under construction. The simulator is used for providing training to the nuclear power plant operators. Apart from training nuclear plant operators the centre offers various engineering programs at diploma and degree levels. The centre is currently in a phase of expansion to cater to an ever-increasing demand for quality technicians and engineers within PAEC.It is largest Nuclear hub of Pakistan.\n\n\n"}
{"id": "12212311", "url": "https://en.wikipedia.org/wiki?curid=12212311", "title": "Cheakamus River derailment", "text": "Cheakamus River derailment\n\nThe Cheakamus River derailment occurred on August 5, 2005, when nine cars from a Canadian National Railway freight train derailed into the Cheakamus River in British Columbia. The cars contained approximately 40,000 litres of caustic soda (sodium hydroxide), which entered the river, killing more than 500,000 fish from 10 different species, including chinook salmon, coho salmon, pink salmon, and rainbow trout, both freshwater and ocean-dwelling. \n\nOn November 5, 2005, federal transport minister Jean Lapierre ordered CN to limit the number of cars of its conventional trains travelling in the area of the derailment between Squamish and Clinton to 80 cars, as a result of the derailment; the train involved had 144 cars.\n\nThe derailment cost CN at least $7 million.\n"}
{"id": "417846", "url": "https://en.wikipedia.org/wiki?curid=417846", "title": "Chemical energy", "text": "Chemical energy\n\nChemical energy is the potential of a chemical substance to undergo a transformation through a chemical reaction to transform other chemical substances. Examples include batteries, food, gasoline, and etc.. Breaking or making of chemical bonds involves energy, which may be either absorbed or evolved from a chemical system.\n\nEnergy that can be released (or absorbed) because of a reaction between a set of chemical substances is equal to the difference between the energy content of the products and the reactants, if the initial and final temperatures are the same. This change in energy can be estimated from the bond energies of the various chemical bonds in the reactants and products. It can also be calculated from formula_1, the internal energy of formation of the reactant molecules, and formula_2 the internal energy of formation of the product molecules. The internal energy change of a chemical process is equal to the heat exchanged if it is measured under conditions of constant volume and equal initial and final temperature, as in a closed container such as a bomb calorimeter. However, under conditions of constant pressure, as in reactions in vessels open to the atmosphere, the measured heat change is not always equal to the internal energy change, because pressure-volume work also releases or absorbs energy. (The heat change at constant pressure is called the enthalpy change; in this case the enthalpy of reaction, if initial and final temperatures are equal).\n\nAnother useful term is the heat of combustion, which is the energy mostly of the weak double bonds of molecular oxygen released due to a combustion reaction and often applied in the study of fuels. Food is similar to hydrocarbon and carbohydrate fuels, and when it is oxidized to carbon dioxide and water, the energy released is analogous to the heat of combustion (though not assessed in the same way as a hydrocarbon fuel — see food energy).\n\nChemical potential energy is a form of potential energy related to the structural arrangement of atoms or molecules. This arrangement may be the result of chemical bonds within a molecule or otherwise. Chemical energy of a chemical substance can be transformed to other forms of energy by a chemical reaction. As an example, when a fuel is burned the chemical energy of molecular oxygen is converted to heat, and the same is the case with digestion of food metabolized in a biological organism. Green plants transform solar energy to chemical energy (mostly of oxygen) through the process known as photosynthesis, and electrical energy can be converted to chemical energy and vice versa through electrochemical reactions.\n\nThe similar term chemical potential is used to indicate the potential of a substance to undergo a change of configuration, be it in the form of a chemical reaction, spatial transport, particle exchange with a reservoir, etc. It is \"not\" a form of potential energy itself, but is more closely related to free energy. The confusion in terminology arises from the fact that in other areas of physics not dominated by entropy, all potential energy is available to do useful work and drives the system to spontaneously undergo changes of configuration, and thus there is no distinction between \"free\" and \"non-free\" potential energy (hence the one word \"potential\"). However, in systems of large entropy such as chemical systems, the total amount of energy present (and conserved by the first law of thermodynamics) of which this Chemical Potential Energy is a part, is separated from the amount of that energy—Thermodynamic Free Energy (which Chemical potential is derived from)—which (appears to) drive the system forward spontaneously as its entropy increases (in accordance with the \"second\" law).\n"}
{"id": "11640735", "url": "https://en.wikipedia.org/wiki?curid=11640735", "title": "Corn Belt derecho", "text": "Corn Belt derecho\n\nThe Corn Belt derecho was a progressive derecho which affected a large area of the central United States on June 29, 1998. In the morning, thunderstorms, including a supercell, developed over South Dakota and tracked into central Iowa. As the thunderstorms reached central Iowa, a strong rear-inflow jet developed which caused the thunderstorm to take on a different characteristic, becoming a derecho. It traveled more than 600 miles in about ten hours, causing more than $125 million worth of widespread damage destruction, especially to crops, and was responsible for power outages to nearly a half a million people.\n\nAt 1200 UTC (7:00 am. CDT), a stationnary front extended from South Dakota to Southern Michigan, bringing warm and humid to its South. Temperature was around at this early hour and the dew point was at . At 850 mb, the southwesterly flow was maintaining this situation while at higher levels the flow was turnig to the northwest, bringing drier and colder air. Daytime heating would increase the instability of the airmass and the CAPE was expected to reach a strong 3689 J/kg. At the same time, there was relatively weak synoptic-scale forcing, the surface flow being a barometric col.\n\nAlong the front in South Dakota, an unorganized area of thunderstorms formed by 9:00 a.m. CDT. They rapidly organized and spread along the front, moving east-southeast into northeast Nebraska. By midday, the storms reached northwestern and north central Iowa, supercells among them, while forming an west-east band and assuming a bow echo shape.\n\nIn the early afternoon, a second area of thunderstorms formed west of Des Moines and merged with the original bow echo line which accelerating east-southeast into Illinois by 4:00 p.m. The line evolved into a classic large scale bow echo, showing a \"book end vortex\" on its northern end, becoming a progressive derecho. Damage, especially to crops and trees, became continuous from the Iowa border into Indiana as most of the damage was produced by strong straight-line winds on the leading edge of the gust front. Some embedded supercells, showing smaller-scale vortices on radars, produced narrower corridors of more intense damage, with measured wind gusts up to at least .\n\nThe derecho crossed central and southern Indiana during the early to mid evening while its highest wind gusts decreased somewhat compared with those observed earlier in the day. The system became a roughly west-east arc and turned more southward as it moved into Kentucky by late evening, dissipating gradually\n\nBy the end of the morning, the thunderstorms produced hail up to the size of hen's eggs and locally damaging wind in Nebraska. By mid-day, supercells along the bow echo in Iowa began to produce very strong winds, up to tennis ball-sized hail, and several mostly short-lived tornadoes. On Doppler weather radar, a large fast-moving mesocyclone associated of the track of a supercell was nearly in contact with the ground as it moved from southwest Boone County east-southeast across the northern and eastern parts of the Des Moines metro area.\n\nOver the Davenport, Iowa NWS Weather Office area of responsibility, numerous reports of wind gusts ranging from 80 to 100 mph were received. The highest measured wind gust of mph was reported in Washington, Iowa near coordinates . This is the highest unofficial recorded wind gust in the history of the state of Iowa. At the same moment, the area of green in the radar display to the right shows the velocities toward the weather radar. The lighter shade, beneath the orange arrow, represented Doppler-estimated mean wind speeds in excess of all along the gust front, and the yellow circle are mesocyclone detections.\n\nIn Illinois, railroad cars where topples, steel power transmission towers were bent, and many buildings were seriously damages during the afternoon. By the evening into Indiana, hundreds of trees were uprooted in the Bedford and Indianapolis areas, two semi-trailer trucks were blown off Interstate 65 near Columbus.. By late evening, damage into Kentucky was minimal, mostly limited to toppled trees and several blown off roofs.\n\nAlong with the long-lived derecho, 28 tornadoes were reported, one an F2 which injured 85 people in central Iowa. Over eight states, the derecho and associated tornadoes killed one person and injured 174.\n\n\nCopy from the above referenced website.\n"}
{"id": "34622934", "url": "https://en.wikipedia.org/wiki?curid=34622934", "title": "Cry, Onion!", "text": "Cry, Onion!\n\nCry, Onion! (, lit. \"Onion Colt\", also known as \"The Smell of Onion\") is a 1975 Spaghetti Western comedy film directed by Enzo G. Castellari. It is openly comedic and parodic.\n\n\n\"Cry, Onion!\" was released in Italy on 25 August 1975.\n"}
{"id": "25481409", "url": "https://en.wikipedia.org/wiki?curid=25481409", "title": "D'Arcy Concession", "text": "D'Arcy Concession\n\nThe D'Arcy Concession was a petroleum oil concession that was signed in 1901 between William Knox D'Arcy and Mozzafar al-Din, Shah of Persia. The oil concession gave D'Arcy the exclusive rights to prospect for oil in Persia (now Iran). During this exploration for oil, D'Arcy and his team encountered financial troubles and struggled to find sellable amounts of oil. They were about to give up but eventually struck large commercial quantities of oil in 1908. After these large commercial quantities of oil were found, the Anglo-Persian Oil Company took over the concession in 1909.\n\nWilliam Knox D'Arcy was born in Devon, England in 1849. When he emigrated to Australia, he took a chance by organizing a syndicate to reopen and get an Australian gold mine back into operation. It turned out that this gold mine still had a lot of gold yet to be found. From this, D'Arcy became a very wealthy man and he returned to England looking for a new investment and to take another chance. This investment and chance would eventually be to prospect for oil in Persia and the venture later became known as the D'Arcy Concession.\n\nDuring the 1890s, research and reports were being published that Persia had great oil potential. Some of D'Arcy's advisers made D'Arcy aware of these reports and promised him wealth if he invested in this venture. D'Arcy agreed and sent out representatives to Tehran to win a concession that would give him the exclusive rights to prospect for oil in Persia. On April 16, 1901, negotiations commenced between D'Arcy's representatives and Shah Mozzafar al-Din over the potential oil concession.\n\nDuring this time period Great Britain and Russia had a great rivalry over the influence each wanted inside Persia. Both imperial powers believed that Persia and the Middle East were important to their imperial economic and military interests. Russia wanted to expand its influence into Persia and Britain believed that this would be a direct threat towards its precious Indian possessions. These two countries fought for influence in Persia through numerous concessions and loans throughout the 19th century. Many British government officials believed that this was where the D'Arcy concession could help. A British oil concession would help tip the balance of power in Persia in Britain's favour. As a result, the British government and its officials in Persia gave full political support to D'Arcy and his potential oil concession. Once the Russians found out about the negotiations between D'Arcy and the Shah, the Russian prime minister tried to block the impending negotiations. The Russian prime minister was able to slow the pace of the agreement until D'Arcy's representative in Tehran offered the Shah an extra £5,000 to close the agreement.\n\nThe extra £5,000 worked and influenced the Shah to sign the concession. At Tehran's Sahebqaraniyyeh Palace on May 28, 1901, Shah Mozzafar al-Din signed the 18 point concession. This 18 point concession would give D'Arcy the exclusive rights to prospect, explore, exploit, transport and sell natural gas, petroleum, asphalt and mineral waxes in Persia. This concession also granted D'Arcy these rights for a 60-year period and it covered an area of 1,242,000 square kilometers. This covered three quarters of the country and D'Arcy purposely excluded the five most northerly provinces from the concession because of their proximity to Russia. In return, the Shah received £20,000 cash, another £20,000 worth of shares, and 16 percent of annual net profits, from the operating companies of the concession.\n\nBetween the Government of His Imperial Majesty the Shah of Persia, of the one part, and William Knox D'Arcy, of independent means, residing in London at No. 42, Grosvenor Square (hereinafter called \"the Concessionaire\") of the other part;\n\nThe following has by these presents been agreed on and arranged:\n\nArticle 1. The Government of His Imperial Majesty the Shah grants to the concessionnaire by these presents a special and exclusive privelege to search for, obtain, exploit, develop, render suitable for trade, carry away and sell natural gas petroleum, asphalt and ozokerite throughout the whole extent of the Persian Empire for a term of sixty years as from the date of these presents.\n\nArticle 2. This privelege shall comprise the exclusive right of laying the pipelines necessary from the deposits where there may be found one or several of the said products up to the Persian Gulf, as also the necessary distributing branches. It shall also comprise the right of constructing and maintaining all and any wells, reservoirs, stations, pump services, accumulation services and distribution services, factories and other works and arrangements that may be deemed necessary.\n\nArticle 3. The Imperial Persian Government grants gratuitously to the concessionnaire all uncultivated lands belonging to the State which the concessionnaire's engineers may deem necessary for the construction of the whole or any part of the above-mentioned works. As for cultivated lands belonging to the State, the concessionnaire must purchase them at the fair and current price of the province.\n\nThe Government also grants to the concessionnaire the right of acquiring all and any other lands or buildings necessary for the said purpose, with the consent of the proprietors, on such conditions as may be arranged between him and them without their being allowed to make demands of a nature to surcharge the prices ordinarily current for lands situate in their respective localities.\n\nHoly places with all their dependencies within a radius of 200 Persian archines are formally excluded.\n\nArticle 4. As three petroleum mines situate at Schouster, Kassre-Chirine, in the Province of Kermanschah, and Daleki, near Bouchir, are at present let to private persons and produce an annual revenue of two thousand tomans for the benefit of the Government, it has been agreed that the three aforesaid mines shall be comprised in the Deed of Concession in conformity with Article 1, on condition that, over and above the 16 per cent mentioned in Article 10, the concessionnaire shall pay every year the fixed sum of 2,000 (two thousand) tomans to the Imperial Government.\n\nArticle 5. The course of the pipe-lines shall be fixed by the concessionnaire and his engineers.\n\nArticle 6. Notwithstanding what is above set forth, the privelege granted by these presents shall not extend to the provinces of Azerbadjan, Ghilan, Mazendaran, Asdrabad, and Khorassan, but on the express condition that the Persian Imperial Government shall not grant to any other person the right of constructing a pipe-line to the southern rivers or to the South coast of Persia.\n\nArticle 7. All lands granted by these presents to the concessionnaire or that may be acquired by him in the manner provided for in Articles 3 and 4 of these presents, as also all products exported, shall be free of all imposts and taxes during the term of the present concession. All material and apparatuses necessary for the exploration, working and development of the deposits, and for the construction and development of the pipe- lines, shall enter Persia free of all taxes and Custom-House duties.\n\nArticle 8. The concessionnaire shall immediately send out to Persia and at his own cost one or several experts with a view to their exploring the region in which there exist, as he believes, the said products, and in the event of the report of the expert being in the opinion of the concessionnaire of a satisfactory nature, the latter shall immediately send to Persia and at his own cost all the technical staff necessary, with the working plant and machinery required for boring and sinking wells and ascertaining the value of the property.\n\nArticle 9. The Imperial Persian Government authorises the concessionnaire to found one or several companies for the working of the concession.\n\nThe names, \"statutes\" and capital of the said companies shall be fixed by the concessionnaire, and the directors shall be chosen by him on the express condition that, on the formation of each company, the concessionnaire shall give official notice of such information to the Imperial Government, through the medium of the Imperial Commissioner, and shall forward the \"statutes\", with information as to the places at which such company is to operate. Such company or companies shall enjoy all the rights and priveleges granted to the concessionnaire, but they must assume all his engagements and responsibilities.\n\nArticle 10. It shall be stipulated in the contract between the concessionnaire, of the one part, and the company, of the other part, that the latte is, within the term of one month as from the date of the formation of the first exploitation company, to pay the Imperial Persian Government the sum of 20,000 sterling in cash, and an additional sum of 20,000 sterling in paid-up shares of the first company founded by virtue of the foregoing article. It shall also pay the said Government annually a sum equal to 16 per cent of the annual net profits of any company or companies that may be formed in accordance with the said article.\n\nArticle 11. The said Government shall be free to appoint an Imperial Commissioner, who shall be consulted by the concessionnaire and the directors of the companies to be formed. He shall supply all and any useful information at his disposal, and he shall inform them of the best course to be adopted in the interest of the undertaking. He shall establish, by agreement with the concessionnaire, such supervision as he may deem expedient to safeguard the interests of the Imperial Government.\n\nThe aforesaid powers of the Imperial Commissioner shall be set forth in the \"statutes\" of the companies created.\n\nThe concessionnaire shall pay the Commissioner thus appointed an annual sum of 1,000 sterling for his services as from the date of the formation of the first company.\n\nArticle 12. The workmen employed in the service of the company shall be subject to His Imperial Majesty the Shah, except the technical staff, such as the managers, engineers, borers and foremen.\n\nArticle 13. At any place in which it may be proved that the inhabitants of the country now obtain petroleum for their own use, the company must supply them gratuitously with the quantity of petroleum that they themselves got previously. Such quantity shall be fixed according to their own declarations, subject to the supervision of the local authority.\n\nArticle 14. The Imperial Government binds itself to take all and any necessary measures to secure the safety and the carrying out of the object of this concession of the plant and of the apparatuses, of which mention is made, for the purposes of the undertaking of the company, and to protect the representatives, agents and servents of the company. The Imperial Government having thus fulfilled its engagements, the concessionnaire and the companies created by him shall not have power, under any pretext whatever, to claim damages from the Persian Government.\n\nArticle 15. On the expiration of the term of the present concession, all materials, buildings and apparatuses then used by the company for the exploitation of its industry shall become the property of the said Government, and the company shall have no right to any indemnity in this connection.\n\nArticle 16. If within the term of two years as from the present date the concessionnaire shall not have established the first said companies authorised by Article 9 of the present agreement, the present concession shall become null and void.\n\nArticle 17. In the event of there arising between the parties to the present concession any dispute of difference in respect of its interpretation or the rights or responsibilities of one or the other of the parties therefrom resulting, such dispute or difference shall be submitted to two arbitrators at Tehran, one of whom shall be named by each of the parties, and to an umpire who shall be appointed by the arbitrators before the proceed to arbitrate. The decision of the arbitrators or, in the event of the latter disagreeing, that of the umpire shall be final.\n\nArticle 18. This Act of Concession, made in duplicate, is written in the French language and translated into Persian with the same meaning.\n\nBut, in the event of there being any dispute in relation to such meaning, the French text shall alone prevail.\n\nOnce D'Arcy was granted the oil concession, his first order of business was to assemble a team to carry out the exploration for oil. The assembled team would carry out the daily operations in Persia for D'Arcy, as he would never set foot on Persian soil. D'Arcy hired Alfred T. Marriot to secure the concession and Dr. M. Y. Young to be the company's medical officer in Persia. George Reynolds was hired for the exploration because of his previous drilling experience. The first area chosen to explore for oil was Chiah Surkh, which is located near what is today's Iran-Iraq border.\nThe task for D'Arcy and his team would prove to be very difficult. This first site at Chiah Surkh had hostile terrain, warring tribes that often refused to recognize the Shah's authority and any concessions he granted, very few roads for transportation, and was nearly three hundred miles away from the Persian Gulf. The local population near the site also had a hostile culture towards Western ideas, technology and presence. Local religion also played a factor as the dominating Shia sect in this region also resisted political authority and had a hostile attitude towards the outside world, including Christians and Sunni Moslems. Not only was the region and terrain hostile, but shipping each piece of equipment to the drilling sites was also extremely difficult. The equipment often had to be carried by man and mule through mountainous terrain.\n\nThe actual drilling did not begin until the end of 1902. The working conditions were rough, as temperatures reached as high as 120 degrees Fahrenheit, equipment often broke down, there were shortages of food and water, and there was an abundance of insects that often bothered the workers. In 1903 D'Arcy began to worry as very little oil had been found and he began to run out of money. Expenses continued to mount as he already spent £160,000 and needed at least another £120,000. It became evident that D'Arcy would either need more funding through loans or he would need to be bought out.\n\nAfter being advised by Thomas Boverton Redwood, who was very knowledgeable on international oil developments, D'Arcy tried applying for a loan from the British Admiralty. At this time, before oil was considered a critical resource, the loan was ultimately denied. Shortly after D'Arcy's loan was denied, one of the wells at Chiah Surkh, in early 1904, became a producer. The output of oil was small, as the well only produced about 200 gallons per day. With or without this well being turned into a producer, D'Arcy still needed money to continue his venture. D'Arcy became busy looking around for investors and he even began to search in France for foreign investors, without much success. To make matters worse, shortly after they struck oil, the well at Chiah Surkh began to run out and turn into a trickle.\n\nBack in London, the British Admiralty began to reconsider and feared that if they did not intervene, then D'Arcy would sell out his concession to other foreign investors or lose the concession altogether. They feared that the French or the Russians would take over the concession and then have a major influence in Persia. The British Admiralty decided to intervene and try to play matchmaker and find an investor to help out D'Arcy and keep the concession under British control. One way they did this was using a British spy named Sidney Reilly, who allegedly disguised himself as a priest, and convinced D'Arcy to sell the majority of his concession to a \"good 'Christian' enterprise. They were successful and with the help of Redwood they were able to strike a deal with the firm called Burmah Oil. D'Arcy and Burmah Oil made the deal in London in 1905 that made an agreement establishing the Concession Syndicate, renamed as the Anglo Persian Oil Co.\n\nWith new capital being provided by the agreement with Burmah Oil, the exploration was able to continue. The focus for drilling now shifted towards southwestern Persia. The drilling site was closed at Chiah Surkh and the equipment was shipped to the southwest of Persia. The new drilling site was established at Masjid-i-Suleiman. Once again Reynolds encountered problems in this region with hostile tribes and the local population. Reynolds often had to pay them a high fee and guarantee them a share of profits in order to protect the concession. Disease also slowed down production and even Reynolds became very ill from contaminated drinking water.\nIn 1907, without any major findings of oil, D'Arcy once again became anxious. He decided to sell off the majority of his shares to Burmah Oil for £203,067 cash and £900,000 in shares. This meant Burmah acquired the majority of D'Arcy's interests in the exploitation company.\n\nAfter mounting expenditure, continuous funding from Burmah Oil and very few results, Burmah also began to grow impatient. It was now 1908 and no commercial quantities of oil had been found and it began to seem as if the beginning of the end was near. Burmah Oil sent a letter to Reynolds telling him to slow down production and pack up the equipment because the project was nearly over. Just as the letter was making its way to Reynolds, at 4:00 am on May 26, 1908, people at the site woke up to shouting, as a fifty-foot gusher of petroleum shot up the drilling rig. At last, commercial quantities of oil had been struck at the Masjid-i-Suleiman site.\n\nIn 1909, shortly after the commercial quantities of oil were found, Burmah Oil thought that a new corporate structure needed to be created in order to work the concession. This led to the creation of the Anglo-Persian Oil Company and Burmah Oil made it a publicly traded company, by selling shares off to the public in 1909. At this time Burmah Oil maintained control and the majority of the ordinary shares. William Knox D'Arcy was compensated for his exploration costs and became a director of the new company.\n"}
{"id": "33922824", "url": "https://en.wikipedia.org/wiki?curid=33922824", "title": "Dependent source", "text": "Dependent source\n\nIn the theory of electrical networks, a dependent source is a voltage source or a current source whose value depends on a voltage or current elsewhere in the network.\n\nDependent sources are useful, for example, in modelling the behavior of amplifiers. A bipolar junction transistor can be modelled as a dependent current source whose magnitude depends on the magnitude of the current fed into its controlling base terminal. An operational amplifier can be described as a voltage source dependent on the differential input voltage between its input terminals. Practical circuit elements have properties such as finite power capacity, voltage, current, or frequency limits that mean an ideal source is only an approximate model. Accurate modelling of practical devices requires using several idealized elements in combination.\n\nDependent sources can be classified as follows:\n\n\nDependent sources are not necessarily linear. For example, MOSFET switches can be modeled as a voltage-controlled current source when \nformula_5 and formula_6.\n\nHowever, the relationship between the current flowing through it and formula_7 is approximately:\n\nIn this case, the current is not linear to formula_7, but rather approximately proportional to the square of formula_10.\nAs for the case of linear dependent sources, the proportionality constant between dependent and independent variables is dimensionless if they are both currents (or both voltages). A voltage controlled by a current has a proportionality factor expressed in units of resistance (ohms), and this constant is sometimes called \"transresistance\". A current controlled by a voltage has the units of conductance (siemens), and is called \"transconductance\". Transconductance is a commonly used specification for measuring the performance of field effect transistors and vacuum tubes.\n"}
{"id": "29025619", "url": "https://en.wikipedia.org/wiki?curid=29025619", "title": "Disulfur monoxide", "text": "Disulfur monoxide\n\nDisulfur monoxide or sulfur suboxide is an inorganic compound with formula SO. It is one of the lower sulfur oxides. It is a colourless gas and condenses to give a pale coloured solid that is unstable at room temperature. It is a bent molecule with an S−S−O angle of 117.88°, S−S bond length of 188.4pm, and S−O bond length of 146.5pm.\n\nDisulfur monoxide was discovered by Peter W. Schenk in 1933. However, only when Myers and Meschi studied it, did the actual composition and shape of the molecule become known.\n\nIt can be formed by many methods, including combustion of sulfur vapour in a deficiency of oxygen. It arises by oxidizing sulfur with copper oxide:\n\nOther routes include the reaction of thionyl chloride with silver sulfide:\n\nIt also arises via thermal decomposition of sulfur dioxide in a glow discharge.\n\nDisulfur monoxide forms a yellow solution in carbon tetrachloride. The solid can be obtained at liquid nitrogen temperatures, often appearing dark colored owing to impurities. On decomposition at room temperature it forms SO via the formation of polysulfur oxides.\n\nDisulfur monoxide was first produced by P. W. Schenk in 1933 with a glow discharge though sulfur vapour and sulfur dioxide. He discovered that the gas could survive for hours at single digit pressures of mercury in clean glass, but decomposed near 30mm Hg Schenk assigned the formula as SO and called it sulfur monoxide. In 1940 K Kondrat'eva and V Kondrat'ev proposed the formula as SO, disulfur dioxide. In 1956, D. J. Meschi and R. J. Myers established the formula as SO.\n\"Desulfovibrio desulfuricans\" is claimed to produce SO.\nSO can be found coming from volcanoes on Io. It can form from 1 to 6% when hot 100 bar S and SO gas erupts from volcanoes. It is believed that Pele on Io is surrounded by solid SO.\n\nCondensed solid SO displays absorption bands at 420 and 530 nm. These are likely to be due to S and S.\n\nThe microwave spectrum of SO has the following rotational parameters: A=41915.44, B=5059.07, and C=4507.19 MHz.\n\nIn the ultraviolet SO has absorption band systems in the ranges 2500 to 3400 Å, and 1900 to 2400 Å. There are bands at 3235 and 3278 Å. The band in the 3150 to 3400 Å range is due to CA'-XA'(π←π) transition.\n\nThe bond angle S−S−O is 109°. The harmonic frequency for S−S stretching is 415.2 cm.\n\nA self decomposition of SO can form trisulfur (S) and SO. Also \"5,6-di-tert-butyl-2,3,7-trithiabicyclo[2.2.1]hept-5-ene 2-endo-7-endo-dioxide\" when heated can form SO. It reacts with diazoalkanes to form dithiirane 1-oxides.\n\nDisulfur monoxide is a ligand bound to transition metals. These are formed by oxidation peroxide oxidation of a disulfur ligands. Excessive oxygen can yield a dioxygendisulfur ligand, which can be reduced in turn with triphenylphosphine. Examples are: \"[Ir(dppe)SO]\", \"OsCl(NO)(PPh)SO\", \"NbCl(η-CH)SO\", \"Mn(CO)(η-CMe)SO\", \"Re(CO)(η-CMe)SO\", \"Re(CO)(η-CH)SO\".\n\nThe molybdenum compound Mo(CO)(SCNEt) reacts with elemental sulfur and air to form a compound Mo(SO)(SCNEt). Another way to form these complexes is to combine sulfonyliminooxo-λ-sulfurane (OSNSO.R) complexes with hydrogen sulfide. Complexes formed in this way are: IrCl(CO)(PPh)SO; Mn(CO)(η-CH)SO. With hydrosulfide and a base followed by oxygen, OsCl(NO)(PPh)SO can be made.\n\nCyclic disulfur monoxide has been made from SO by irradiating the solid in an inert gas matix with 308 nm ultraviolet light.\n"}
{"id": "3070392", "url": "https://en.wikipedia.org/wiki?curid=3070392", "title": "Dodo Club", "text": "Dodo Club\n\nThe Dodo Club is the children's wing of the Durrell Wildlife Conservation Trust based in Jersey, Channel Islands.\n\nThe main focus of the club is environmental awareness and citizen science projects among younger members of the Trust. The Dodo Club is so named because the logo of the Durrell Wildlife Conservation Trust is a dodo, chosen by the founder Gerald Durrell as a reminder of man's wanton environmental destruction.\n"}
{"id": "563540", "url": "https://en.wikipedia.org/wiki?curid=563540", "title": "Emil Lenz", "text": "Emil Lenz\n\nHeinrich Friedrich Emil Lenz (; ; also Emil Khristianovich Lenz, ; 12 February 1804 – 10 February 1865), usually cited as Emil Lenz, was a Russian physicist of Baltic German ethnicity. He is most noted for formulating Lenz's law in electrodynamics in 1834.\n\nLenz was born in Dorpat (nowadays Tartu, Estonia), at that time in the Governorate of Livonia in the Russian Empire. After completing his secondary education in 1820, Lenz studied chemistry and physics at the University of Dorpat. He traveled with the navigator Otto von Kotzebue on his third expedition around the world from 1823 to 1826. On the voyage Lenz studied climatic conditions and the physical properties of seawater. The results have been published in \"Memoirs of the St. Petersburg Academy of Sciences\" (1831). \n\nAfter the voyage, Lenz began working at the University of St. Petersburg, Russia, where he later served as the Dean of Mathematics and Physics from 1840 to 1863 and was Rector from 1863 until his death in 1865. Lenz also taught at the Petrischule in 1830 and 1831, and at the Mikhailovskaya Artillery Academy.\n\nLenz had begun studying electromagnetism in 1831. Besides the law named in his honor, Lenz also independently discovered Joule's law in 1842; to honor his efforts on the problem, it is also given the name the \"Joule–Lenz law,\" named also for James Prescott Joule.\n\nLenz eagerly participated in development of the electroplating technology, invented by his friend and colleague Moritz von Jacobi. In 1839, Lenz produced several medallions using electrotyping. Along with the electrotyped relief produced by Jacobi the same year, these were the first instances of galvanoplastic sculpture.\nLenz died in Rome, after suffering from a stroke.\n\nA small lunar crater on the far side of the moon is named after him.\n\n\n\n \n"}
{"id": "18020168", "url": "https://en.wikipedia.org/wiki?curid=18020168", "title": "Engkanto", "text": "Engkanto\n\nEngkanto are mythical environmental spirits that are said to have the ability to appear in human form. They are often associated with the spirits of ancestors in the Philippines. They are also characterized as spirit sorts like sirens, dark beings, elves, and more. Belief in their existence has likely existed for centuries, and continues to this day.\n\nEngkanto have many similarities to humans in that they age, appear to have male and female sexes, can suffer from illness and indeed even die. They are an object of mythology for many Filipinos. Often told by adults as stories and shown on media. They have different appearances. Some appear to be beautiful having blue eyes, fair complexion and golden hair. They may however have unusual features such as high-bridged noses, fair skin, blond hair and lack of philtrum. They have a wide range of appearances but one common fact of a different feeling or vibe than humans. Other variants exhibit sexual dimorphism such as Bagobo spirits which are separated into the female \"tahamaling\" and the male \"mahomanay\". The female spirit is alleged to have red complexion while the male have a fair complexion. Their dwellings will normally appear as natural features, for example large rocks or trees, or shadows in human form; although to humans they have befriended they can appear as magnificent palaces. These creatures prefer large trees and nature such as the \"balete\" in which they also place their belongings. An engkanto may choose to stay by a human's side as told by stories where characters are usually in either a sense of trance or a deep loss of energy. Engkanto may be good or bad.\n\nEngkanto are most commonly known for either extreme malignant effects, or an overwhelming influence of luck. Those the Engkanto do not favor had become depressed, suffered from madness, or even disappeared for days or months, possibly as a result of the human possession. They are also said to be capable of causing fevers and skin diseases such as boils. These spirits also sometimes lead travelers astray in the forest, even kidnap them. This, however, is said to be avoidable by bringing an \"\"Anting-anting\" or \"Agimat\"\" a piece of magical charm or amulet that wards away evil spirits and prevents them from harming the wielder. However, if they do favor someone they are generous and capable of bringing power and riches to that person. Shaman often try to commune with Engkanto on holy days to obtain better healing powers from them, as well as learning how to better deal with evil spirits.\n\nFrancisco Demetrio made a study of 87 folk stories from Visayas and Mindanao relating to Engkanto. He contended the Engkanto were based on early European friars.\n"}
{"id": "12591223", "url": "https://en.wikipedia.org/wiki?curid=12591223", "title": "Filter press", "text": "Filter press\n\nAn industrial filter press is a tool used in separation processes, specifically to separate solids and liquids. The process uses the principle of pressure drive, as provided by a slurry pump. Among other uses, filter presses are utilized in marble factories in order to separate water from mud in order to reuse the water during the marble cutting process.\n\nGenerally, the slurry that will be separated is injected into the center of the press and each chamber of the press is filled. Optimal filling time will ensure the last chamber of the press is loaded before the mud in the first chamber begins to cake. As the chambers fill, pressure inside the system will increase due to the formation of thick sludge. Then, the liquid is strained through filter cloths by force using pressurized air, but the use of water could be more cost-efficient in certain cases, such as if water was re-used from a previous process.\n\nThe first form of filter press was invented in the United Kingdom in 1853, used in obtaining seed oil through the use of pressure cells. However, there were many disadvantages associated with them, such as high labour requirement and discontinuous process. Major developments in filter press technology started in the middle of 20th century. In Japan in 1958, Kenichiro Kurita and Seiichi Suwa succeeded in developing the world's first automatic horizontal-type filter press to improve the cake removal efficiency and moisture absorption. Nine years later, Kurita Company began developing flexible diaphragms to decrease moisture in filter cakes. The device enables optimisation of the automatic filtration cycle, cake compression, cake discharge and filter-cloth washing leading to the increment in opportunities for various industrial applications. A detailed historical review, dating back to when the Shang Dynasty used presses to extract tea from camellia the leaves and oil from the hips in 1600 BC, was compiled by K. McGrew.\n\nThere are three main basic types of filter presses: plate and frame filter presses, recessed plate and frame filter presses and automatic filter presses.\n\nA plate and frame filter press is the most fundamental design, and many now refer it as a \"membrane filter plate\". This type of filter press consists of many plates and frames assembled alternately with the supports of a pair of rails. The presence of a centrifuge pump ensures the remaining suspended solids do not settle in the system, and its main function is to deliver the suspension into each of the separating chambers in the plate and frame filter. For each of the individual separating chambers, there is one hollow filter frame separated from two filter plates by filter cloths. The introduced slurry flows through a port in each individual frame, and the filter cakes are accumulated in each hollow frame. As the filter cake becomes thicker, the filter resistance increases as well. So when the separating chamber is full, the filtration process is stopped as the optimum pressure difference is reached. The filtrate that passes through filter cloth is collected through collection pipes and stored in the filter tank. Filter cake (suspended solid) accumulation occurs at the hollow plate frame, then being separated at the filter plates by pulling the plate and frame filter press apart. The cakes then fall off from those plates and are discharged to the final collection point.\n\nCake discharge can be done in many ways. For example: Shaking the plates while they are being opened or shaking the cloths. A scraper can also be used, by moving from one chamber to another and scraping the cake off the cloth. At the end of each run, the cloths are cleaned using wash liquid and are ready to start the next cycle.\n\nAn automatic filter press has the same concept as the manual filter and frame filter, except that the whole process is fully automated. It consists of larger plate and frame filter presses with mechanical \"plate shifters\". The function of the plate shifter is to move the plates and allow rapid discharge of the filter cakes accumulated in between the plates. It also contains a diaphragm compressor in the filter plates which aids in optimizing the operating condition by further drying the filter cakes.\n\nA recessed plate filter press is made up of polypropylene squares at about 2 to 4 feet across with a concave depression and a hole in the center of each. Two plates join together to form a chamber to pressurize the slurry and squeeze the filtrate out through the filter cloth lining in the chamber. It is capable of holding 12 to 80 plates adjacent to each other, depending on the required capacity. When the filter press is closed, a series of chambers is formed. The differences with the plate and frame filter are that the plates are joined together in such a way that the cake forms in the recess on each plate, meaning that the cake thickness is restricted to 32mm unless extra frames are used as spacers. However, there are disadvantages to this method, such as longer cloth changing time, inability to accommodate filter papers, and the possibility of forming uneven cake.\n\nFilter presses are used in a huge variety of different applications, from dewatering of mineral mining slurries to blood plasma purification. At the same time, filter press technology is widely established for ultrafine coal dewatering as well as filtrate recovery in coal preparation plants. According to G.Prat, the \"filter press is proven to be the most effective and reliable technique to meet today's requirement\". One of the examples is Pilot scale plate filter press, which is specialized in dewatering coal slurries. In 2013 the Society for Mining, Metallurgy and Exploration published an article highlighting this specific application. It was mentioned that the use of the filter press is very beneficial to plant operations, since it offers dewatering ultraclean coal as product, as well as improving quality of water removed to be available for equipment cleaning.\n\nOther industrial uses for automatic membrane filter presses include municipal waste sludge dewatering, ready mix concrete water recovery, metal concentrate recovery, and large-scale fly ash pond dewatering.\n\nMany specialized applications are associated with different types of filter press that are currently used in various industries. Plate filter press is extensively used in sugaring operations such as the production of maple syrup in Canada, since it offers very high efficiency and reliability. According to M.Isselhardt, \"appearance can affect the value of maple syrup and customer's perception of quality\". This makes the raw syrup filtration process extremely crucial in achieving desired product with high quality and appealing form, which again suggested how highly appreciated filter press methods are in industry.\n\nTable 1 Classification of filter press.\nTypical range of particle size and feed concentration are 1-100 μm and 1-30% by weight. The slurry feed concentration generally has large amount of ultrafine particles. The percentage of solids concentration in the slurry feed is normally more than 10% by weight.\n\nLimitation in pressure resistance of the filter cake is up to a maximum of 800 kPa. Flow rate of filtrate is controlled by feed pump. In filter press methodology, positive pressure filtration is used instead of vacuum filtration with high-energy consumption.\n\nPlate and frame filter press produce up to 99% of solids recovery, and the moisture left in the cake commonly ranges from 15% to 20%. During cake washing, 90% of the filtrate can be removed by the wash liquid.\n\nHere are some typical filter press calculation used for handling operation applied in waste water treatment:\n\nS=\nWhere,\n\nformula_1\n\nWhere:\n(S × P) gives the filter run time.\n\nformula_2\n\nWhere:\n\n\nThose are the most important factors that affect the rate of filtration. When filtrate pass through the filter plate, deposition of solids are formed and increases the cake thickness, which also increase Rc while Rf is assumed to be constant. The flow resistance from cake and filter medium can be studied by calculating the flow rate of filtration through them.\n\nIf the flow rate is constant, the relationship between pressure and time can be obtained. The filtration must be operated by increasing pressure difference to cope with the increase in flow resistance resulting from pore clogging. The filtration rate is mainly affected by viscosity of the filtrate as well as resistance of the filter plate and cake.\n\nHigh filtration rate can be obtained from producing thin cake. However, a conventional filter press is a batch system and the process must be stopped to discharge the filter cake and reassemble the press, which is time consuming. Practically, maximum filtration rate is obtained when the filtration time is greater than the time taken to discharge the cake and reassemble the press to allow for cloth's resistance.\nProperties of the filter cake affect the filtration rate, and it is desirable for the particle's size to be as large as possible to prevent pore blockage by using a coagulant. From experimental work, flow rate of liquid through the filter medium is proportional to the pressure difference. As the cake layer forms, pressure applies to the system increases and the flow rate of filtrate decreases. If the solid is desired, the purity of the solid can be increased by cake washing and air drying.\nSample of filter cake can be taken from different locations and weighed to determine the moisture content by using overall material balance.\n\nThe selecting of filter press type depends on the value of liquid phase or the solid phase. If extracting liquid phase is desired, then filter press is among the most appropriate methods to be used.\n\nNowadays, filter plates are made from polymers or steel coated with polymer. They give good drainage surface for filter cloths. The plate sizes are ranged from 10 by 10 cm to 2.4 by 2.4 m and 0.3 to 20 cm for the frame thickness.\n\nTypical cloth areas can range from 1 m or less on laboratory scale to 1000 m in a production environment, even though plates can provide filter areas up to 2000 m. Normally, plate and frame filter press can form up to 50 mm of cake thickness, however, it can be push up to 200 mm for extreme cases. Recessed plate press can form up to 32 mm of cake thickness.\n\nIn the early days of press use in the municipal waste biosolids treatment industry, issues with cake sticking to the cloth was problematic and many treatment plants adopted less effective centrifuge or belt filter press technologies. Since then, there have been great enhancements in fabric quality and manufacturing technology that have made this issue obsolete. Unlike the USA, automatic membrane filter technology is the most common method to dewater municipal waste biosolids in Asia. Moisture is typically 10-15% lower and less polymer is required—which saves on trucking and overall disposal cost.\n\nThe operating pressure is commonly up to 7 bars for metal. The improvement of the technology makes it possible to remove large amount of moisture at 16 bar of pressure and operate at 30 bars. However, the pressure is 4-5 bars for wood or plastic frames. If the concentration of solids in the feed tank increase until the solid particles are attached to each other. It is possible to install moving blades in the filter press to reduce resistance to flow of liquid through the slurry.\nFor the process prior to cake discharge, air blowing is used for cakes that have permeability of 10 to 10 m.\n\nPre-treatment of the slurries before filtration is required if the solid suspension has settled down. Coagulation as pre-treatment can improve the performance of filter press because it increases the porosity of the filter cake leading to faster filtration. Varying the temperature, concentration and pH can control the size of the flocs. Moreover, if the filter cake is impermeable and difficult for the flow of filtrate, filter aid chemical can be added to the pre-treatment process to increase the porosity of the cake, reduce the cake resistance and obtain thicker cake. However, filter aids need to be able to remove from the filter cake either by physical or chemical treatment. A common filter aid is Kieselguhr, which give 0.85 voidage.\n\nIn terms of cake handling, batch filter press requires large discharge tray size in order to contain large amount of cake and the system is more expensive compared to continuous filter press with the same output.\n\nThere are two possible methods of washing that are being employed, the \"simple washing\" and the \"thorough washing\". For simple washing, the wash liquor flows through the same channel as the slurry with high velocity, causing erosion of the cakes near the point of entry. Thus the channels formed are constantly enlarged and therefore uneven cleaning is normally obtained. A better technique is by thorough washing in which the wash liquor is introduced through a different channel behind the filter cloth called washing plates. It flows through the whole thickness of the cakes in opposite direction first and then with the same direction as the filtrate. The wash liquor is normally discharged through the same channel as the filtrate. After washing, the cakes can be easily removed by supplying compressed air to remove the excess liquid.\n\nNowadays filter presses are widely used in many industries, they would also produce different types of wastes. Harmful wastes such as toxic chemical from dye industries, as well as pathogen from waste stream might accumulate in the waste cakes; hence the requirement for treating those wastes would be different. Therefore, before discharge waste stream into the environment, application of post-treatment would be an important disinfection stage. It is to prevent health risks to the local population and the workers that are dealing with the waste (filter cakes) as well as preventing negative impacts to our ecosystem. Since filter press would produce large amount of waste, if it was to be disposed by land reclamation, it is recommended to dispose to the areas that are drastically altered like mining areas where development and fixation of vegetation are not possible. Another method is by incineration, which would destroy the organic pollutants and decrease the mass of the waste. It is usually done in a closed device by using a controlled flame.\n\nMany debates have been discussed about whether or not filter presses are sufficient to compete with modern equipment currently as well as in the future, since filter presses were among one of the oldest machine-driven dewatering devices. Efficiency improvements are possible in many applications where modern filter presses have the best characteristics for the job, however, despite the fact that many mechanical improvements have been made, filter presses still remain to operate on the same concept as when first invented. A lack of progress in efficiency improvement as well as a lack of research on conquering associated issues surrounding filter presses have suggested a possibility of performance inadequacy. At the same time, many other types of filter could do the same or better job as press filters. In certain cases, it is crucial to compare characteristics and performances.\n\nFilter presses offer a wide range of application, one of its main propositions is the ability to provide a large filter area in a relatively small footmark. Surface area available is one of the most important dimensions in any filtering process, since it maximises filter flow rate and capacity. A standard size filter press offers a filter area of 216 m, whereas a standard belt filter only offers approximately 15 m.\n\nFilter presses are commonly used to dewater high-solids slurries in metal processing plants, one of the press filter technology that could deliver the job is the Rotary Pressure Filter method, which provides continuous production in a single unit, where filtration is directed via pressure. However, in cases where solids concentration in high-solids slurries is too high (50%+), it is better to handle these slurries using vacuum filtration, such as a continuous Indexing Vacuum Belt Filter, since high concentration of solids in slurries will increase pressure and if pressure is too high, the equipment might be damaged and/or less efficient operation.\n\nIn the future, market demands for modern filtration industry are going to become finer and higher degree in separation, and particularly on the purpose of material recycling, energy saving, and green technology. In order to meet increasing demands for higher degree of dewatering from difficult-to-filter material, super-high pressure filters are required. Therefore, the trend in increasing the pressure for the automatic filter press will keep on developing in the future.\n\nThe conventional filter press mechanisms usually use mechanical compression and air to \nde-liquoring; however, the efficiency of producing low-moisture cake is limited. An alternative method has been introduced by using steam instead of air for cake dewatering. Steam dewatering technique can be a competitive method since it offers product of low-moisture cake.\n"}
{"id": "1656118", "url": "https://en.wikipedia.org/wiki?curid=1656118", "title": "Foe (unit)", "text": "Foe (unit)\n\nA foe is a unit of energy equal to 10 joules or 10 ergs, used to express the large amount of energy released by a supernova. The word is an acronym derived from a part of the pronunciation \"ten to the power of fifty-one ergs\". \n\nIt was coined by Gerald E. Brown of Stony Brook University in his work with Hans Bethe, because \"it came up often enough in our work\". A bethe (B) is equivalent to a \"foe\". The \"bethe\" is named after Hans Bethe. It was coined by Steven Weinberg.\n\nThis unit of measure is convenient because a supernova typically releases about one foe of observable energy in a very short period (which can be measured in seconds). In comparison, if the Sun had its current luminosity throughout its entire lifetime, it would release 3.827 W × 3.1536 s/yr × 10 yr ≈ 1.2 foe. One solar mass has a rest mass energy of 1787 foe.\n\n"}
{"id": "2429160", "url": "https://en.wikipedia.org/wiki?curid=2429160", "title": "Framework Convention on the Protection and Sustainable Development of the Carpathians", "text": "Framework Convention on the Protection and Sustainable Development of the Carpathians\n\nThe Framework Convention on the Protection and Sustainable Development of the Carpathians \"(Carpathian Convention)\" is a framework type convention pursuing a comprehensive policy and cooperating in the protection and sustainable development of the Carpathians. Designed to be an innovative instrument to ensure protection and foster sustainable development of this outstanding region and living environment, the Convention is willing to improve the quality of life, to strengthen local economies and communities. \n\nIt aims as well at providing conservation and restoration of unique, rare and typical natural complexes and objects of recreational and other importance situated in the heart of Europe, preventing them from negative anthropogenic influences through the promotion of joint policies for sustainable development among the seven countries of the region (Czech Republic, Hungary, Poland, Romania, Serbia and Montenegro, Slovakia and Ukraine).\n\nIn 2001, United Nations Environment Programme / Regional Office for Europe UNEP/ROE was requested by the Government of Ukraine to service a regional cooperation process aiming at the protection and sustainable development of the Carpathian Mountains, a major transboundary mountain range shared by the seven countries. In response to this request, UNEP/ROE promoted an Alpine-Carpathian Partnership. \n\nIn 2002, during the UN International Year of the Mountains, the Alpine-Carpathian partnership has been initiated and launched by the Ministry of the Environment and Territory of Italy, at the time President of the Alpine Convention. Since then, UNEP/ROE serviced five negotiation meetings of the Carpathian countries. \n\nAt the Fifth Ministerial Conference \"Environment for Europe\" (Kiev, May 2003), the Carpathian countries adopted the Framework Convention on the Protection and Sustainable Development of the Carpathians consequently signed by all seven countries.\n"}
{"id": "10132968", "url": "https://en.wikipedia.org/wiki?curid=10132968", "title": "Fuel mass fraction", "text": "Fuel mass fraction\n\nIn combustion physics, fuel mass fraction is the ratio of fuel mass flow to the total mass flow of a fuel mixture. If an air flow is fuel free, the fuel mass fraction is zero; in pure fuel without trapped gases, the ratio is unity. As fuel is burned in a combustion process, the fuel mass fraction is reduced. The definition reads as\n\nwhere \n"}
{"id": "1145750", "url": "https://en.wikipedia.org/wiki?curid=1145750", "title": "Geophilic", "text": "Geophilic\n\nGeophilic means soil loving or preferring the soil. This term is usually used when referring to certain types of fungi or molds that live in the soil. Many of these organisms are usually recovered from the soil but occasionally infect humans and animals. They cause a marked inflammatory reaction, which limits the spread of the infection and may lead to a spontaneous cure but may also leave scars.\n\nCan also refer to someone who loves the earth, sustainability, or “green” initiatives. An individual with these tendencies may be referred to as a \"geophile.\"\n"}
{"id": "28581923", "url": "https://en.wikipedia.org/wiki?curid=28581923", "title": "Giurgiu Power Station", "text": "Giurgiu Power Station\n\nThe Giurgiu Power Station is a large thermal power plant located in Giurgiu, having 3 generation groups of 50 MW each resulting a total electricity generation capacity of 150 MW.\n\n\n"}
{"id": "3089250", "url": "https://en.wikipedia.org/wiki?curid=3089250", "title": "Haircloth", "text": "Haircloth\n\nHaircloth is a stiff, unsupple fabric typically made from horsehair and/or from the wooly hair of a camel. Although \"horsehair\" generally refers to the hair of a horse's mane or tail, haircloth itself is sometimes called horsehair. Horse or camel hair woven into haircloth may be fashioned into clothing or upholstery.\n\nIn tailoring applications, haircloth is woven using cotton warp and horsehair weft. In traditional suit construction, haircloth is used to stiffen the front panels in mens' suit jackets, and Savile Row tailors still make bespoke suits this way. However, in modern suits, haircloth is often replaced with synthetic fabrics.\n\nIn the history of brewing, for drying the malt, haircloth was spread over the kiln floor to keep grain from dropping down into the furnace. Perforated metal or tile (gratings, meshes) were also used, but had a drawback of scorching the grain. \n"}
{"id": "40921638", "url": "https://en.wikipedia.org/wiki?curid=40921638", "title": "Heptacosylic acid", "text": "Heptacosylic acid\n\nHeptacosylic acid, or heptacosanoic acid or carboceric acid, is a 27-carbon long-chain saturated fatty acid with the chemical formula CH(CH)COOH.\n\n"}
{"id": "18995926", "url": "https://en.wikipedia.org/wiki?curid=18995926", "title": "Lifting gas", "text": "Lifting gas\n\nBecause of Archimedes' principle, a lifting gas is required for aerostats to create buoyancy. Its density is lower than that of air (about 1.29 kg/m, 1.29 g/L). Only certain lighter than air gases are suitable as lifting gases.\n\nHeated atmospheric air is frequently used in recreational ballooning. According to the Ideal gas law, an amount of gas (and also a mixture of gases such as air) expands as it is heated. As a result, a certain volume of gas has a lower weight as the temperature is higher. The average temperature of air in a hot air balloon is about .\n\nHydrogen, being the lightest existing gas (7% the density of air), seems to be the most appropriate gas for lifting. But hydrogen has several disadvantages:\n\nHelium is the second lightest gas. For that reason, it is an attractive gas for lifting as well. Small size of helium molecules increases its lifting value.\n\nA major advantage is that this gas is noncombustible. But the use of helium has some disadvantages, too:\n\nThe gaseous state of water is lighter than air, incombustible and much cheaper than helium. The concept of using steam for lifting is therefore already 200 years old. The biggest challenge has always been to make a material that can resist it. In 2003, a university team in Berlin, Germany, has successfully made a 150 °C steam lifted balloon. However, such a design is generally impractical due to high boiling point and condensation.\n\nAmmonia is sometimes used to fill weather balloons. Due to its high boiling point (compared to helium and hydrogen), ammonia could potentially be refrigerated and liquefied aboard an airship to reduce lift and add ballast (and returned to a gas to add lift and reduce ballast). Ammonia gas is relatively heavy, poisonous, and an irritant.\n\nMethane, the main component of natural gas, is sometimes used as a lift gas when hydrogen and helium are not available. It has the advantage of not leaking through balloon walls as rapidly as the smaller molecules of hydrogen and helium. However, methane is highly flammable and like hydrogen is not appropriate for use in passenger-carrying airships. It is also relatively dense and a potent greenhouse gas.\n\nHydrogen fluoride is lighter than air and could theoretically be used as a lifting gas. However, it is extremely corrosive, highly toxic, expensive, is heavier than other lifting gases, and has a high boiling point of 19.5 °C. Its use would therefore be impractical. \n\nIn the past, coal gas, a mixture of hydrogen, carbon monoxide and other gases, was also used in balloons. It was widely available and cheap; the down side was a higher density (reducing lift) and the high toxicity of the carbon monoxide.\n\nAcetylene is 10% lighter than air and could be used as a lifting gas. Its extreme flammability and low lifting power make it an unattractive choice.\n\nHydrogen cyanide, which is 7% lighter than air, is technically capable of being used as a lifting gas at temperatures above its boiling point of 25.6 °C. Its extreme toxicity, low buoyancy, and high boiling point have precluded such a use.\n\nNeon is lighter than air and could lift a balloon. Like helium, it is incombustible. However, it is rare on Earth and expensive, and is among the heavier lifting gases.\n\nPure nitrogen has the advantage that it is inert and abundantly available, because it is the major component of air. However, because nitrogen is only 3% lighter than air, it is not an obvious choice for a lifting gas.\n\nTheoretically, an aerostatic vehicle could be made to use a vacuum or partial vacuum. As early as 1670, over a century before the first manned hot-air balloon flight, the Italian monk Francesco Lana de Terzi envisioned a ship with four vacuum spheres.\n\nIn a theoretically perfect situation with weightless spheres, a 'vacuum balloon' would have 7% more net lifting force than a hydrogen-filled balloon, and 16% more net lifting force than a helium-filled one. However, because the walls of the balloon must be able to remain rigid without imploding, the balloon is impractical to construct with all known materials. Despite that, sometimes there is discussion on the topic.\n\nAnother medium that in theory could be used is a plasma: Ions repelling each other could give a pressure intermediate between vacuum and hydrogen and hence that counteracts the atmospheric pressure. \nThe energy and the containment requirements are extremely impractical, so that it may only be interesting for science fiction.\n\nIt is also possible to combine some of the above solutions. A well-known example is the Rozière balloon which combines a core of helium with an outer shell of hot air.\n\nHydrogen and helium are the most commonly used lift gases. Although helium is twice as heavy as (diatomic) hydrogen, they are both significantly lighter than air, making this difference negligible.\n\nThe lifting power in air of hydrogen and helium can be calculated using the theory of buoyancy as follows:\n\nThus helium is almost twice as dense as hydrogen. However, buoyancy depends upon the \"difference\" of the densities (ρ) − (ρ) rather than upon their ratios. Thus the difference in buoyancies is about 8%, as seen from the buoyancy equation:\nWhere F = Buoyant force (in Newton); g = gravitational acceleration = 9.8066 m/s² = 9.8066 N/kg; V = volume (in m³).\nTherefore, the amount of mass that can be lifted by hydrogen in air at sea level, equal to the density difference between hydrogen and air, is:\nand the buoyant force for one m of hydrogen in air at sea level is:\nTherefore, the amount of mass that can be lifted by helium in air at sea level is:\nand the buoyant force for one m of helium in air at sea level is:\n\nThus hydrogen's additional buoyancy compared to helium is:\n\nThis calculation is at sea level at 0 °C. For higher altitudes, or higher temperatures, the amount of lift will decrease proportionally to the air density, but the ratio of the lifting capability of hydrogen to that of helium will remain the same. This calculation does not include the mass of the envelope need to hold the lifting gas.\n\nAt higher altitudes, the air pressure is lower and therefore the pressure inside the balloon is also lower. This means that while the mass of lifting gas and mass of displaced air for a given lift are the same as at lower altitude, the volume of the balloon is much greater at higher altitudes.\n\nA balloon that is designed to lift to extreme heights (stratosphere), must be able to expand enormously in order to displace the required amount of air. That is why such balloons seem almost empty at launch, as can be seen in the photo.\n\nA different approach for high altitude ballooning, especially used for long duration flights is the superpressure balloon. A superpressure balloon maintains a higher pressure inside the balloon than the external (ambient) pressure.\n\nBecause of the enormous density difference between water and gases (water is about 1,000 times more dense than most gases), the lifting power of underwater gases is very strong. The type of gas used is largely inconsequential because the relative differences between gases is negligible in relation to the density of water. However, some gases can liquefy under high pressure, leading to an abrupt loss of buoyancy.\n\nA submerged balloon that rises will expand or even explode because of the strong pressure reduction, unless gas is able to escape continuously during the ascent or the balloon is strong enough to withstand the change in pressure.\n\nA balloon can only have buoyancy if there is a medium that has a higher average density than the balloon itself.\n\n"}
{"id": "1908365", "url": "https://en.wikipedia.org/wiki?curid=1908365", "title": "Neon lighting", "text": "Neon lighting\n\nNeon lighting consists of brightly glowing, electrified glass tubes or bulbs that contain rarefied neon or other gases. Neon lights are a type of cold cathode gas-discharge light. A neon tube is a sealed glass tube with a metal electrode at each end, filled with one of a number of gases at low pressure. A high potential of several thousand volts applied to the electrodes ionizes the gas in the tube, causing it to emit colored light. The color of the light depends on the gas in the tube. Neon lights were named for neon, a noble gas which gives off a popular orange light, but other gases and chemicals are used to produce other colors, such as hydrogen (red), helium (yellow), carbon dioxide (white), and mercury (blue). Neon tubes can be fabricated in curving artistic shapes, to form letters or pictures. They are mainly used to make dramatic, multicolored glowing signage for advertising, called neon signs, which were popular from the 1920s to the 1950s. \nThe term can also refer to the miniature neon glow lamp, developed in 1917, about seven years after neon tube lighting. While neon tube lights are typically meters long, the neon lamps can be less than one centimeter in length and glow much more dimly than the tube lights. They are still in use as small indicator lights. Through the 1970s, neon glow lamps were widely used for numerical displays in electronics, for small decorative lamps, and as signal processing devices in circuity. While these lamps are now antiques, the technology of the neon glow lamp developed into contemporary plasma displays and televisions.\n\nNeon was discovered in 1898 by the British scientists William Ramsay and Morris W. Travers. After obtaining pure neon from the atmosphere, they explored its properties using an \"electrical gas-discharge\" tube that was similar to the tubes used for neon signs today. Georges Claude, a French engineer and inventor, presented neon tube lighting in essentially its modern form at the Paris Motor Show from December 3–18, 1910. Claude, sometimes called \"the Edison of France\", had a near monopoly on the new technology, which became very popular for signage and displays in the period 1920-1940. Neon lighting was an important cultural phenomenon in the United States in that era; by 1940, the downtowns of nearly every city in the US were bright with neon signage, and Times Square in New York City was known worldwide for its neon extravagances. There were 2000 shops nationwide designing and fabricating neon signs. The popularity, intricacy, and scale of neon signage for advertising declined in the U.S. following the Second World War (1939–1945), but development continued vigorously in Japan, Iran, and some other countries. In recent decades architects and artists, in addition to sign designers, have again adopted neon tube lighting as a component in their works.\n\nNeon lighting is closely related to fluorescent lighting, which developed about 25 years after neon tube lighting. In fluorescent lights, the light emitted by rarefied gases within a tube is used exclusively to excite fluorescent materials that coat the tube, which then shine with their own colors that become the tube's visible, usually white, glow. Fluorescent coatings and glasses are also an option for neon tube lighting, but are usually selected to obtain bright colors.\n\nNeon is a noble gas chemical element and an inert gas that is a minor component of the Earth's atmosphere. It was discovered in 1898 by the British scientists William Ramsay and Morris W. Travers. When Ramsay and Travers had succeeded in obtaining pure neon from the atmosphere, they explored its properties using an \"electrical gas-discharge\" tube that was similar to the tubes used today for neon signs. Travers later wrote, \"the blaze of crimson light from the tube told its own story and was a sight to dwell upon and never forget.\" The procedure of examining the colors of the light emitted from gas-discharge (or \"Geissler\" tubes) was well-known at the time, since the colors of light (the \"spectral lines\") emitted by a gas discharge tube are, essentially, fingerprints that identify the gases inside.\n\nImmediately following neon's discovery, neon tubes were used as scientific instruments and novelties. However, the scarcity of purified neon gas precluded its prompt application for electrical gas-discharge lighting along the lines of Moore tubes, which used more common nitrogen or carbon dioxide as the working gas, and enjoyed some commercial success in the US in the early 1900s. After 1902, Georges Claude's company in France, Air Liquide, began producing industrial quantities of neon as a byproduct of the air liquefaction business. From December 3–18, 1910, Claude demonstrated two large ( long), bright red neon tubes at the Paris Motor Show.\nThese neon tubes were essentially in their contemporary form. The range of outer diameters for the glass tubing used in neon lighting is 9 to 25 mm; with standard electrical equipment, the tubes can be as long as . The pressure of the gas inside is in the range 3-20 Torr (0.4-3 kPa), which corresponds to a partial vacuum in the tubing. Claude had also solved two technical problems that substantially shortened the working life of neon and some other gas discharge tubes, and effectively gave birth to a neon lighting industry. In 1915 a US patent was issued to Claude covering the design of the electrodes for gas-discharge lighting; this patent became the basis for the monopoly held in the US by his company, Claude Neon Lights, for neon signs through the early 1930s.\n\nClaude's patents envisioned the use of gases such as argon and mercury vapor to create different colors beyond those produced by neon. In the 1920s, fluorescent glasses and coatings were developed to further expand the range of colors and effects for tubes with argon gas or argon-neon mixtures; generally, the fluorescent coatings are used with an argon/mercury-vapor mixture, which emits ultraviolet light that activates the fluorescent coatings. By the 1930s, the colors from combinations of neon tube lights had become satisfactory for some general interior lighting applications, and achieved some success in Europe, but not in the US. Since the 1950s, the development of phosphors for color televisions has created nearly 100 new colors for neon tube lighting.\n\nAround 1917, Daniel McFarlan Moore, then working at the General Electric Company, developed the miniature neon lamp. The glow lamp has a very different design than the much larger neon tubes used for signage; the difference was sufficient that a separate US patent was issued for the lamp in 1919. A Smithsonian Institution website notes, \"These small, low power devices use a physical principle called \"coronal discharge.\" Moore mounted two electrodes close together in a bulb and added neon or argon gas. The electrodes would glow brightly in red or blue, depending on the gas, and the lamps lasted for years. Since the electrodes could take almost any shape imaginable, a popular application has been fanciful decorative lamps. Glow lamps found practical use as electronic components, and as indicators in instrument panels and in many home appliances until the acceptance of Light-Emitting Diodes (LEDs) starting in the 1970s.\"\n\nAlthough some neon lamps themselves are now antiques, and their use in electronics has declined markedly, the technology has continued to develop in artistic and entertainment contexts. Neon lighting technology has been reshaped from long tubes into thin flat panels used for plasma displays and plasma television sets.\n\nWhen Georges Claude demonstrated an impressive, practical form of neon tube lighting in 1910, he apparently envisioned that it would be used as a form of lighting, which had been the application of the earlier Moore tubes that were based on nitrogen and carbon dioxide discharges. Claude's 1910 demonstration of neon lighting at the \"Grand Palais\" (Grand Palace) in Paris lit a peristyle of this large exhibition space. Claude's associate, Jacques Fonseque, realized the possibilities for a business based on signage and advertising. By 1913 a large sign for the vermouth Cinzano illuminated the night sky in Paris, and by 1919 the entrance to the Paris Opera was adorned with neon tube lighting.\n\nNeon signage was received with particular enthusiasm in the United States. In 1923, Earle C. Anthony purchased two neon signs from Claude for his Packard car dealership in Los Angeles, California; these literally stopped traffic. Claude's US patents had secured him a monopoly on neon signage, and following Anthony's success with neon signs, many companies arranged franchises with Claude to manufacture neon signs. In many cases companies were given exclusive licenses for the production of neon signs in a given geographical area; by 1931, the value of the neon sign business was $16.9 million, of which a significant percentage was paid to Claude Neon Lights, Inc. by the franchising arrangements. Claude's principal patent expired in 1932, which led to a great expansion in the production of neon signage. The industry's sales in 1939 were about $22.0 million; the expansion in volume from 1931 to 1939 was much larger than the ratio of sales in the two years suggests.\n\nRudi Stern has written, \"The 1930s were years of great creativity for neon, a period when many design and animation techniques were developed. ... Men like O. J. Gude and, in particular, Douglas Leigh took neon advertising further than Georges Claude and his associates had ever envisioned. Leigh, who conceived and created the archetypal Times Square spectacular, experimented with displays that incorporated smells, fog, and sounds as part of their total effect. ... Much of the visual excitement of Times Square in the thirties was a result of Leigh's genius as a kinetic and luminal artist.\" Major cities throughout the United States and in several other countries also had elaborate displays of neon signs. Events such as the Chicago Century of Progress Exposition (1933–34), the Paris World's Fair (1937) and New York World's Fair (1939) were remarkable for their extensive use of neon tubes as architectural features. Stern has argued that the creation of \"glorious\" neon displays for movie theaters led to an association of the two, \"One's joy in going to the movies became inseparably associated with neon.\"\n\nThe Second World War (1939–1945) arrested new sign installations around most of the world. Following the war, the industry resumed. Marcus Thielen writes of this era, \"...after World War II, government programs were established to help re-educate soldiers. The Egani Institute (New York City) was one of few schools in the country that taught neon-trade secrets. The American streamlined design from the 1950s would be unimaginable without the use of neon.\" The development of Las Vegas, Nevada as a resort city is inextricably linked with neon signage; Tom Wolfe wrote in 1965, \"Las Vegas is the only city in the world whose skyline is made neither of buildings, like New York, nor of trees, like Wilbraham, Massachusetts, but signs. One can look at Las Vegas from a mile away on route 91 and see no buildings, no trees, only signs. But such signs! They tower. They revolve, they oscillate, they soar in shapes before which the existing vocabulary of art history is helpless.\"\n\nOverall, however, neon displays became less fashionable, and some cities discouraged their construction with ordinances. Nelson Algren titled his 1947 collection of short stories \"The Neon Wilderness\" (as a synonym of \"urban jungle\" for Chicago). Margalit Fox has written, \"... after World War II, as neon signs were replaced increasingly by fluorescent-lighted plastic, the art of bending colored tubes into sinuous, gas-filled forms began to wane.\" A dark age persisted at least through the 1970s, when artists adopted neon with enthusiasm; in 1979 Rudi Stern published his manifesto, \"Let There Be Neon\". Marcus Thielen wrote in 2005, on the 90th anniversary of the US patent issued to Georges Claude, \"The demand for the use of neon and cold cathode in architectural applications is growing, and the introduction of new techniques like fiberoptics and LED — into the sign market have strengthened, rather than replaced, neon technology. The evolution of the 'waste' product neon tube remains incomplete 90 years after the patent was filed.\"\n\nIn neon glow lamps, the luminous region of the gas is a thin, \"negative glow\" region immediately adjacent to a negatively charged electrode (or \"cathode\"); the positively charged electrode (\"anode\") is quite close to the cathode. These features distinguish glow lamps from the much longer and brighter \"positive column\" luminous regions in neon tube lighting. The energy dissipation in the lamps when they are glowing is very low (about 0.1 W), hence the distinguishing term cold-cathode lighting.\n\nSome of the applications of neon lamps include:\nThe small size of the negative glow region of a neon lamp, and the flexible electronic properties that were exploited in electronic circuits, led to the adoption of this technology for the earliest plasma panel displays. The first monochrome dot matrix plasma panel displays were developed in 1964 at the University of Illinois for the PLATO educational computing system. They had the characteristic color of the neon lamp; their inventors, Donald L. Bitzer, H. Gene Slottow, and Robert H. Wilson, had achieved a working computer display that remembered its own state, and did not require constant refreshing from the central computer system. The relationship between these early monochrome displays and contemporary, color plasma displays and televisions was described by Larry F. Weber in 2006, \"All plasma TVs on the market today have the same features that were demonstrated in the first plasma display which was a device with only a single cell. These features include alternating sustain voltage, dielectric layer, wall charge, and a neon-based gas mixture.\" As in colored neon lamps, plasma displays use a gas mixture that emits ultraviolet light. Each pixel has a phosphor that emits one of the display's base colors.\n\nThe mid to late 1980s was a period of resurgence in neon production. Sign companies developed a new type of signage called channel lettering, in which individual letters were fashioned from sheet metal.\n\nWhile the market for neon lighting in outdoor advertising signage has declined since the mid twentieth century, in recent decades neon lighting has been used consciously in art, both in individual objects and integrated into architecture. Frank Popper traces the use of neon lighting as the principal element in artworks to Gyula Košice's late 1940s work in Argentina. Among the later artists whom Popper notes in a brief history of neon lighting in art are Stephen Antonakos, the conceptual artists Joseph Kosuth and Bruce Nauman, Martial Raysse, Chryssa, Piotr Kowalski, Maurizio Nannucci and François Morellet in addition to Lucio Fontana, Dan Flavin or Mario Merz.\n\nSeveral museums in the United States are now devoted to neon lighting and art, including the Museum of Neon Art (founded by neon artist Lili Lakich, Los Angeles, 1981), the Neon Museum (Las Vegas, founded 1996), the American Sign Museum (Cincinnati, founded 1999). These museums restore and display historical signage that was originally designed as advertising, in addition to presenting exhibits of neon art. Several books of photographs have also been published to draw attention to neon lighting as art. In 1994, Christian Schiess has published an anthology of photographs and interviews devoted to fifteen \"light artists\".\n\n\n\n"}
{"id": "40711593", "url": "https://en.wikipedia.org/wiki?curid=40711593", "title": "Nirmalapura Wind Farm", "text": "Nirmalapura Wind Farm\n\nThe Nirmalapura Wind Farm is a wind farm consisting of seven wind turbines, located on the west coast of Nirmalapura, Puttalam, Sri Lanka. The plant is owned by , and was commissioned in September 2011.\n\nTransportation of the wind turbines were carried out by Agility Logistics, while construction of the turbine foundations were completed by the International Construction Consortium at a cost of . The Ceylon Electricity Board pays the wind farm company a flat rate of over a 20-year period. The wind power company is a joint venture between Akbar Brothers, Debug Group, Hayleys, and Hirdaramani Group.\n\n"}
{"id": "38969643", "url": "https://en.wikipedia.org/wiki?curid=38969643", "title": "Nowy Tomyśl Wind Turbines", "text": "Nowy Tomyśl Wind Turbines\n\nThe Nowy Tomyśl Wind Turbines are the tallest wind turbines in the world. They are situated in Paproć, a suburb of Nowy Tomyśl in Poland and were erected in 2012. Each of these two wind turbines has a generating capacity of 2500 kW. Both rotors have a diameter of 100 metres and are mounted on 160 metre tall free-standing lattice towers, which are the tallest free-standing lattice towers in Poland.\n\nThe basement of each of these turbines weighs 1500 tons, the tower 350 tons and the gondola 100 tons.\n\n"}
{"id": "41832257", "url": "https://en.wikipedia.org/wiki?curid=41832257", "title": "Nuclear labor issues", "text": "Nuclear labor issues\n\nNuclear labor issues exist within the international nuclear power industry and the nuclear weapons production sector worldwide, impacting upon the lives and health of laborers, itinerant workers and their families.\n\nA subculture of frequently undocumented workers do the dirty, difficult, and potentially dangerous work shunned by regular employees. They are called in the vernacular Nuclear Nomads, Bio-Robots, Lumnizers, Glow Boys, Radium Girls, the Fukushima 50, Liquidators, Atomic Gypsies, Gamma Sponges, Nuclear Gypsies, Genpatsu Gypsies, Nuclear Samurai and Jumpers. When they exceed their allowable radiation exposure limit at a specific facility, they often migrate to a different nuclear facility. The industry implicitly accepts this conduct as it can not operate without these practices. The World Nuclear Association states that the transient workforce of \"nuclear gypsies\" - casual workers employed by subcontractors has been \"part of the nuclear scene for at least four decades.\"\n\nExistent labor laws protecting worker's health rights are not always properly enforced. Records are required to be kept, but frequently they are not. Some personnel were not properly trained resulting in their own exposure to toxic amounts of radiation. At several facilities there are ongoing failures to perform required radiological screenings or to implement corrective actions.\n\nMany questions regarding these nuclear worker conditions go unanswered, and with the exception of a few whistleblowers, the vast majority of laborers - unseen, underpaid, overworked and exploited, have few incentives to share their stories. The median annual wage for hazardous radioactive materials removal workers, according to the U.S. Bureau of Labor Statistics is $37,590 in the U.S - $18 per hour. A 15-country collaborative cohort study of cancer risks due to exposure to low-dose ionizing radiation, involving 407,391 nuclear industry workers showed significant increase in cancer mortality. The study evaluated 31 types of cancers, primary and secondary.\n\nIn 1942 thirty indigenous Dené men were recruited to mine uranium, locally known as \"the money rock\" for three dollars per day at the Port Radium mine. By 1998, 14 of these workers had died of lung, colon and kidney cancers, according to the North West Territory's Cancer Registry. The Dené were not told of the hazards of mining uranium, and breathed radioactive dust, slept on the ore, and ate fish from the tailings ponds. Ottawa was the world's largest supplier of uranium at that time, and the United States the biggest buyer, according to declassified U.S. documents. In subsequent decades, thousands of Native miners were not warned of the risks.\n\nNamibia's Rössing Uranium Mine is the longest-operating open-pit uranium mine, and one of the largest in the world. The company is owned and operated by Rio Tinto, one of the world's largest mining groups, and Rössing Uranium Limited. The uranium mill tailings dam has been leaking for a number of years, and on January 17, 2014, a catastrophic structural failure of a leach tank caused a major spill. The France-based laboratory, Commission de Recherche et d'Information Independentantes sur la Radioactivite (CRIIAD) reported elevated levels of radioactive materials in the area surrounding the mine.\n\nThere have been numerous reports published on labor and human rights conditions at the mine. Workers were not informed of the dangers of working with radioactive materials and the health effects thereof. The Director of Labor Resource and Research Institute (LaRRI), Hilma Shindondola-Mote, mine employees asserted that Rössing did not provide them with explanation of health problems from exposure to uranium.\n\nAt the open cut Kayelekera uranium mine near Karonga, Malawi (Africa), a mine employee, Khwima Phiri, was killed on July 20, 2013. He was struck in the chest and killed while inflating a wheel. There have been allegations of radiation-induced diseases among the mine workers and nearby residents. The Malawi government has been unable to verify these, stating that the absence of monitoring equipment. On June 19, 2011 a truck at the mine caught fire, killing the driver. On September 23, 2010, workers were ordered to work despite the fact that the mine could not provide them with dust masks to protect them against radioactive materials.\n\nThe American and British demand for large quantities of uranium to use in nuclear weapons initiated New Zealand's uranium survey during WWII. In 1944 in Wellington, geologists and physicists assembled two exploration teams to survey South Island, particularly the granite deposits and black beach sand areas. In 1945, Fiordland, Milford Sound, Nancy Sound and other locations were surveyed, resulting in the December 7, 1945 NZ Atomic Energy Act granting full ownership of any discovered radioactive elements - however not to the indigenous peoples whose ancestral lands contained these materials. In 1955, another rich uranium deposit was discovered by prospectors Frederick Cassin and Charles Jacobsen. In the following years prospectors traveled through rainforests and other terrain with Geiger counters, jackhammers and drills. These workers were exposed to unsafe levels of radiation through exposure to and inhalation of dust. In Australia, uranium mining was no less unrestrained than in New Zealand. At the Nabarlek, Rum Jungle, Hunter's Hill, Rockhole and Moline mines, gamma radiation exceeded safe levels by 50% causing chronic health problems for miners and workers.\n\nBetween 1949 and 1989, over 4,000 uranium mines in the Four Corners region produced more than 225,000,000 tons of uranium ore. This activity affected a large number of Native American nations, including the Laguna, Navajo, Zuni, Southern Ute, Ute Mountain, Hopi, Acoma and other Pueblo cultures. Many of these peoples worked in the mines, mills and processing plants in New Mexico, Arizona, Utah and Colorado. These workers were not only poorly paid, they were seldom informed of dangers nor were they given appropriate protective gear. The government, mine owners, scientific, and health communities were all well aware of the hazards of working with radioactive materials at this time. Due to the Cold War demand for increasingly destructive and powerful nuclear weapons, these laborers were both exposed to and brought home large amounts of radiation in the form of dust on their clothing and skin. Epidemiological studies of the families of these workers have shown increased incidents of radiation-induced cancers, miscarriages, cleft palates and other birth defects. The extent of these genetic effects on indigenous populations and the extent of DNA damage remains to be resolved. Uranium mining on the Navajo reservation continues to be a disputed issue as former Navajo mine workers and their families continue to suffer from health problems.\n\n\n\nFollowing a large earthquake and tsunami on March 11, 2011 three nuclear reactors melted-down at the TEPCO Fukushima Daiichi power station in Japan. Despite TEPCO's ongoing efforts to stabilize, decommission, decontaminate and contain the radioactive materials, many workers have been exposed to significant doses of radiation. Both skilled and unskilled laborers work on the extensive clean-up crew, many of those involved in the most dangerous work are on short contracts. These \"nuclear gypsies\" or \"jumpers\" are often recruited from day labor sites across Japan.\n\nContract labor in the nuclear industry is not new. Years prior to the Fukushima accident, the \"Los Angeles Times\" reported in 1999 that nearly 90% of Japanese nuclear power plant workers were subcontracted to perform the most hazardous jobs. Included in the report is the incident at the Tokaimura JCO Co. nuclear plant, 80 miles north of Tokyo, where 150 workers were exposed to radiation, including one fatality, based on the Japan Nuclear Safety Commission report. In 1999, the \"Los Angeles Times\" reported that nearly 90% of Japanese nuclear power plant workers were subcontracted to perform the most hazardous jobs. In 2010, the year before the Fukushima accident, eight-eight percent of Japan's nuclear workforce of 83,000 workers were contracted, not full-time workers. The Tokyo-based Citizens' Nuclear Information Center reported that temporary workers absorbed 16 times higher levels of radiation than regular TEPCO employees. Other sources cite higher dose levels and alleged worker abuse. The first responders to the accident, the \"Fukushima 50\" have refused to be photographed, as TEPCO and the Japanese government has not released their names and faces, they remain unknowable and forgotten. Keeping the cleanup crew fully staffed, 24 hours per day, on 12-hour shifts, rotating every three days is a physical and logistical challenge to an emergency that will sustain for decades during which an ongoing stream of workers is required.<ref name=\"Time, by Krista Mahr, March 31, 2011 http://globalspin.blogs.time.com/2011/03/31/whats-in-store-for-japan's-embattled-nuclear-workers/\"></ref> In a lecture given May 3, 2011 to the All Freeter's Union in Tokyo by the photographer Kenji Higuchi, \"The Truth of the Fukushima 50\", he cites TEPCO's lack of responsible oversight. He is of the opinion that the Fukushima 50 are victims of unsafe working conditions, not heroes, as they are depicted in the media. The few workers who have come forward, such as Shingo Kanno, describe themselves as \"nuclear samurai\", helping to save Japan from the spread of radiation while doing menial labor at the Fukushima nuclear plant. Upon arriving onsite, some workers were told by their managers that the level of radiation was so high their annual exposure limit could be reached within an hour. The extent of the disaster has initiated searches for clean-up workers from other countries, including the U.S. Many clean-up workers at Fukushima have found that they are not eligible for free cancer screenings from TEPCO or the Japanese government. As of November, 2012 only 3.7% have been granted screenings, although many have been exposed to high levels of radiation, and all work in highly contaminated zones.\n\nJapan's second largest construction company, Obayashi Corporation, was found to, perhaps illegally, assign homeless men from the Sendai train station to work as decontamination laborers at the crippled reactors. Several arrests were made of members of Japanese criminal syndicates, Yamaguchi-gumi, Inagawa-kai, and Sumiyoshi-sai, for arranging black-market labor recruitment operations for Obayashi. The day-labor gray markets in Tokyo and Osaka were also found to recruit homeless men, paying them $6 per hour after deductions for food and lodging. Other workers were paid as little as $10 per month after deductions. Some workers report they were simply left unpaid. Among the temporary clean-up workers who have come forward, such as Tetsuya Hayashi, was told he would at Fukushima monitoring worker exposures for two weeks during the summer of 2012. Upon arriving at the disaster site, he was deployed to an area with extremely high radiation levels, rather than the monitoring station. Although Hayashi was provided with protective gear, he thinks the agency engaged in \"bait and switch\" approaches to recruitment. Later he accepted a second contract job from another agency at TEPCO's Fukushima Daiichi plant, working on spent fuel rod tanks. He reported that the new contracting agency only paid him 2/3rds of his wages. In over 80 interviews of workers conducted by Reuter's journalists, a frequent complaint was the lack of proper training. They also cited alliances between the contractors, subcontractors and Yazuka organized crime group. While TEPCO does not make worker wages public, the interviewees stated their average earnings were between $6 and $12 per hour. Another worker to speak out, Ryo Goshima, claims his employment broker skimmed half his pay from his wages. The oversight is poorly managed by TEPCO and the Japanese government; as of mid-2013 several hundred small companies had been granted decontamination work. According to the Carnegie Endowment for International Peace global think tank report, complete remediation of the site is likely to take three or four decades.\n\nBetween January 2015 and March 2015 there was a ten-fold increase of workers at the Fukushima Daiichi plant who received exposures in excess of 5 mSv, according to a TEPCO report. TEPCO's records show that 57 workers were exposed to 5 to 20 mSv in January, 2015; 327 workers exposed to that rate spectrum in February 2015: and in March 2015, 585 workers were exposed to the 5-20 mSv range. On January 19, 2015, a worker died at the Fukushima Daiichi NPS after falling into an empty water tank. The following day, January 20, at the Fukushima Daini plant, a worker's head was trapped between a 7000 kg piece of moving machinery and the scaffolding, killing him. At another TEPCO plant, Kashiwazaki Kariwa NPS, a worker was seriously injured on January 19, 2015. In response, work at the three nuclear power plants was suspended by TEPCO to analyze the accidents, and develop a safety plan. On October 20, 2015, the \"New York Times\" reported that Japan will begin to pay Fukushima accident disaster laborers recompense for cancers developed from participating in the clean up of the triple meltdowns and fuel pool clean-ups.\n\nThe Dōnen accident () occurred on March 11, 1997. A small explosion occurred at a nuclear reprocessing plant, exposing 37 workers to radiation. On September 30, 1999, a more serious accident occurred resulting in two deaths, at the JCO (formerly Japan Nuclear Fuel Conversion Company) facility in Tokai. Ibaraki Prefecture. While preparing enriched uranium fuel for use in the Jōyō experimental breeder reactor, a criticality occurred causing a criticality lasting 20 hours during which the nuclear fission chain reaction emitted intense gamma and neutron radiation. At least 667 workers, nearby residents and emergency response team members were exposed to excess radiation. Two technicians, Hisachi Ouchi and Masato Shinohara died from the accident. Radiation levels at the plant were 15,000 time higher than normal.\n\nFrance is an international leader in the nuclear power industry throughout the world. A study by the National Institute for Health and Medical Research (INSERM) in France concluded that the largest and least visible population of chronic exposure to ionizing radiation are the nuclear industry's \"thousands and even hundreds of thousands of workers who perform daily maintenance and upkeep operations and tasks in nuclear plants, nuclear testing facilities, research centers, reprocessing plants, and nuclear waste management centers.\" France's 50-year long nuclear industry has not historically kept records of worker's internal and external exposure to radiation. The effects of risk to workers and the impact of subcontracting the most dangerous tasks within the industry is intensified by nuclear secrecy. On May 22, 1986, a nuclear fuel reprocessing plant at La Hague in Normandy, sustained a mechanical malfunction. Five workers were exposed to unsafe levels of radiation and hospitalized. On April 12, 1987, the Tricastin Nuclear Power Plant fast breeder reactor coolant leaked contaminating seven workers. In July 2008, approximately 100 workers were exposed to a radiation leak.\n\nThe Chernobyl nuclear reactor meltdown occurred on April 26, 1986, in the Ukraine, during a test of the Unit 4 reactor systems. The explosion and fire caused by human error released massive amounts of radioactive material into the environment, irradiation a large area of Europe, in particular Belarus, Ukraine and the Russian Federation. The cleanup of the radioactive meltdown debris involved 600,000 laborers (NRC statistics), known as \"jumpers\" or liquidators\". These cleanup workers received hundreds of times of the average annual radiation dose allowed in the United States. Statistics on the numbers of deaths, illnesses and genetically produced mutagenic diseases in the following generations remains in debate depending on the source of information. The statistics vary from 4,000 deaths to 93,000 deaths. According to the 2011 report of the German Affiliate of International Physicians for the Prevention of Nuclear War (IPPNW), \"Health Effects of Chernobyl: 25 years after the reactor catastrophe\" based on Yablokov's 2010 report, there were 830,000 clean-up workers; 350,000 evacuees from the 30 km highly contaminated zone; 8,300,000 people who were affected within the heavily irradiated area in Belarus, Ukraine and Russia; and six hundred million (600,000,000) European people who had exposures to radiation from the accident (Fairlie, 2007). It is estimated that 700,000 \"liquidators\" - clean up workers - received 100 millisieverts of radiation, and others received higher doses.\n\nThe purpose of the Mayak Production Association facility was to produce plutonium for nuclear weapons. In its earlier years of operation, exposures to radiation were significantly higher than at other similar facilities. Mayak was one of the largest nuclear facilities in the Russian Federation, and was formerly known as Chelyabinsk-40 and later as Chelyabinsk-65. It was the site of the Kyshtym disaster (1957) when a storage tank explosion released 50-100 tons of high-level radioactive waste, contaminating a 290 square mile area in the eastern Ural mountains, causing radiation sickness and death. The event was rated 6 \"serious accident\" on the 7-level INES nuclear incident/accident scale. The incident received little attention, as it was kept secret for 30 years. Many laborers who worked at the plant during the 1950s and 1960s died from exposures to radiation. The accident was first reported in 1976by Zhores Medvedev in the journal New Scientist, it was in 1992 that the Russian government officially acknowledged the accident.\n\nThe Sellafield nuclear reprocessing plant, located on the coast of the Irish Sea, is built on the former site of the Windscale nuclear reactor and Calder Hall. The British government began developing the site in 1947 as the Windscale Piles plutonium production plant, its graphite reactor core was cooled by air, rather than water as the US reactors at the Hanford site. By 1952 the facility was separating plutonium from spent uranium fuel. In 1957 the Windscale fire destroyed the core of Pile #1, exposing workers to 150 times the \"safe dose limit\" of radioactivity and releasing approximately 750 terabecquerels of radioactive material into the environment. The incident is rated a \"5\" on the International Nuclear Event Scale (INES) of nuclear accidents and incidents. A 1990 study of childhood leukemia and other cancers in the offspring of Sellafield, Dounreay and Seascale nuclear workers show elevated levels of occurrence. There have been 21 significant accidents and incidents of radioactive material releases between 1950 and 2000. Tissue samples and organs were removed from 65 deceased former Sellafield workers, as announced by Trade Secretary, Alistair Darling in 2007, and confirmed by Peter Lutchwyche of the British Nuclear Group. On January 28, 1998, a damaged plutonium-contaminated filter in building B209, causing thirteen workers to be evacuated, necessitating two workers to undergo tests for internal as well as external contamination. Photographic documentation of equipment contaminated with plutonium, poor signage and substandard barriers were cited. \"Glow Boys\", a 1999 film by Mark Ariel Waller, interprets this event and others in relation to energy, economy and power and labor. In January 2014, Sellafield issued an order for thousands of workers to not report to work due to elevated levels of radioactivity onsite.\n\nIn a report based on reviews of raw data on nuclear worker health drafted by the Department of Energy (DOE) and the White House National Economic Council (NEC), the U.S. government found that workers at 14 nuclear weapons plants were exposed to unsafe levels of radiation and other toxins, resulting in a wider range of cancers. The Applied-Industrial Chemical and Energy Workers Union states that workers had higher rates of leukemia, lung cancer, bladder cancer and other diseases. The DOE and NEC panel found that nearly 600,000 nuclear weapons workers developed other cancers as well: Hodgkin's lymphoma, prostate cancer, kidney cancer, and salivary gland cancer. The Oak Ridge K-25 facility, Tennessee, Savannah River Site, the Hanford Site, Rocky Flats Plant, Fernald Feed Materials Production Center, Lawrence Livermore National Laboratory and Los Alamos National Laboratory are among the 14 sites studied. Statistics from the Department of Labor, Office of Workers Compensation Program (OWCP) Division of Energy Employees Occupational Illness Compensation are found posted weekly. The U.S. Federal Register Executive Order 13179, of December 11, 2000 states that thousands of Americans who built the U.S. nuclear defense: \n\npaid a high price for their service, developing disabling or fatal illnesses as a result of exposure to beryllium, ionizing radiation, and other hazards unique to nuclear weapons production and testing. Too often, these workers were neither adequately protected from, nor informed of, the occupational hazards to which they were exposed.\n\nThe document goes on to state that existing worker's compensation programs have failed due to long latency periods of radiation-caused disease as well as inadequate record keeping of data.\n\nThe exposure of military workers and contractors to radioactive materials that exceed safe doses is well documented. After the bombings of Hiroshima and Nagasaki, military workers were sent to these areas to examine and clean up the rubble. Many of these U.S. veterans developed bone marrow and blood abnormalities, multiple myeloma, leukemia, Hodgkin's disease, myelofibrosis and cancers. During the nuclear weapons testing in the Marshall Islands approximately 300,000 GI's were exposed to radiation, the U.S. Department of Defense estimates 210,000 servicemen, however the National Association of Atomic Veterans cite between 250,000 and 400,000. The 2008-9 National Cancer Institute/U.S. Department of Health reports that exposure to radiation from nuclear weapons testing is a worldwide issue of significant concern. \n\nHundreds of thousands of military personnel and civilians in the United States received significant radiation doses as a result of their participation in nuclear weapons testing and supporting occupations and industries, including nuclear fuel and weapons production, and uranium mining, milling, and ore transport. Hundreds of thousands more were irradiated at levels sufficient to cause cancer and other diseases. These populations include the families of military and civilian workers, and people – known as \"downwinders\" – living or working in communities surrounding or downstream from testing and related activities, and in relatively distant areas to which nuclear fallout or other radioactive material spread. Federal responses to the plight of affected individuals have been unsatisfactory.\n\nFor decades, radioactive isotopes of plutonium, uranium, radium, thorium and technetium were released from the Fernald Feed Materials Production Center in Ohio, entering into the air, land and water, including deep ground water of the Great Miami aquifer. Workers and area residents show higher rates of systemic lupus erythematosus, certain cancers, and low blood cell counts. A study by the National Institute for Occupational Safety and Health (NIOSH) determined that salaried workers had lower mortality rates than per-hour workers, despite both cohorts increased malignancies of blood, bone, spleen, lymph and thyroid cancers. While the plant was under construction in 1952, labor disputes broke out between carpenters and other laborers, in what was reported as \"rioting\" and \"mob action\". In 1954, a chemical explosion causes the death of two workers. In 1959, a strike ensues at the factory regarding the quota system. Machinists, steel workers and sheet metal workers strike. In 1974, employees voice their concerns over health hazards. In 1984, National Lead of Ohio, the manager of the site, admits that radioactive dust was released, and groundwater contaminated. In 1990, Fernald employees and/or their survivors filed a class action suit over health hazards.\n\nThe Hanford Nuclear Reservation (HNR), also known as the Hanford Site, located in Washington State in the western United States adjacent to the Columbia River, is a nuclear materials production complex that is in the process of being decommissioned. HNR was founded in 1943 as part of the Manhattan Project for large-scale production of plutonium for use in nuclear weapons, including the first nuclear bomb tested at the Trinity site in New Mexico, and the Fat Man nuclear bomb used at Nagasaki, Japan, during WWII. Hanford is considered the most contaminated nuclear waste site America. Much of the clean-up has focused on water and land contamination from leaking tanks, as well as airborne radioactive dusts.\n\nIn 1976, a chemical reaction caused a glove box to explode at the Plutonium Finishing Plant, contaminating Harold McCluskey (aged 64). The site of the accident, (242-Z )was closed-off due to high levels of radioactivity, decontamination did not begin until 2014, thirty eight years after the accident. The \"McCluskey Room\" was used to separate americium from plutonium during the Cold War. McCluskey received the highest dosage of americium of any human being, 500 times the occupational standard, and was so radioactive, his body had to be removed by remote control and placed in a steel and concrete isolation tank where glass and metal were removed from his skin and tissues. He survived the accident. After five months of treatment, involving scrubbings and shots of zinc DTPA, he was permitted to return home, as his radiation count had fallen from 500 above standard to 200 times above safe occupational level.\n\nIdaho National Laboratory near Arco, Idaho was founded in 1949 as a nuclear reactor testing laboratory. Some consider it to be the site of the first fatal accident in the nuclear military/industrial sector when the SL-1 boiling water reactor melted down, killing two reactor operators, a third operator died shortly thereafter. When a control rod in the reactor was removed manually causing a power surge and ensuing criticality, a steam explosion occurred in the reactor vessel. The event caused the reactor lid to be blown nine feet into the air. The three operators were heavily irradiated and their remains were buried in lead coffins.\n\nThere have been other accidents involving radioactive uranium and plutonium in later decades, including an incident in 2011 when seventeen workers were exposed to low-level radiation from plutonium.\n\nThe occupational health studies of the Los Alamos National Laboratory and surrounding communities show elevated levels of certain disease rates among workers. A plutonium core for a nuclear weapon, nicknamed the \"Demon Core\" was involved in two accidents at LANL in 1945 and 1946, leading to the acute radiation poisoning and later the deaths of scientists Harry Daghlian and Louis Slotin. The first criticality incident occurred on August 21, 1945 when physicist Harry Daghlian accidentally dropped the core, causing a burst of neutron radiation that contaminated him and a security guard, Private Robert J. Hemmerly. The second incident caused the death of physicist, Louis Slotkin, and contaminated seven other employees.\n\nThe secret atomic city of Oak Ridge, Tennessee was part of the Manhattan Complex. Workers there were exposed to radioactive materials at plants X-10, K-25 and Y-12, and qualify for compensation from the 2011 Energy Employee Occupational Illness Compensation Act (RECA) for illnesses resulting from their work at the Oak Ridge Reservation. Workers there were exposed to highly enriched uranium and plutonium due to inadequate storage and security at the Oak Ridge plant.\n\nThe Pantex Plant is a nuclear weapons assembly and disassembly plant located in the Texas Panhandle region. It also provides technology for manufacturing, evaluating and testing nuclear explosives. It is listed by the United States Environmental Protection Agency as a Superfund Site. A 2014 report in the Global Security Newswire, reports that the contractor overseeing the Pantex nuclear weapons facility was cited for numerous safety hazard incidents. The U.S. Department of Energy cited B&W Pantex (Bechtel and Babcock & Wilcox) for six safety incidents. The DOE Office of Health, Safety and Security's chief of enforcement and oversight, John Boulden, states these \"events are significant in that they involved improper management, handling or labeling of highly hazardous materials, including explosives, which have the potential to cause serious injury or death.\" B&W Pantex did not receive any fines for this breach of worker's safety. As of 2015, the U.S. government plans to spend $1 trillion over the next thirty years to modernize its nuclear stockpile. Plans to cut spending include cutting health and retirement benefits for workers in the nuclear weapons industry. The Government Accountability office confirms the National Nuclear Safety Administration officer's statement: \"reducing labor costs represents a large share of cost savings to be achieved.\" Worker's benefits via the Consolidated Nuclear Security contract at Pantex, as well as at Oak Ridge, Tennessee's Y-12 National Security Complex, will be cut as per Department of Energy regulation Order 350.1.\n\nBetween 1957-1964, Rocketdyne located at the Santa Susana Field Laboratory, 30 miles north of Los Angeles, California operated ten experimental nuclear reactors. Numerous accidents occurred including a core meltdown. Experimental reactors of that era were not required to have the same type of containment structures that shield modern nuclear reactors. During the Cold War time in which the accidents that occurred at Rockedyne, these events were not publicly reported by the Department of Energy. \nIn 1979, Rocketdyne released to the public that these events occurred. In 1999 the site was remediated, although thousands of pounds of contaminated sodium coolant cannot be accounted for. Local residents, including former workers filed a class-action suit in 2005, and were awarded $30 million. Many of the workers and local residents were already deceased at the time of the settlement.\n\nThe employees at Rocky Flats Plant near Denver Colorado made plutonium warhead triggers (known as pits) for the United States nuclear weapons arsenal. The area surrounding the plant is contaminated with radioactive plutonium. According to Marco Kaltofen, and engineer and president of the Boston Chemical Data Corporation, \"The material is still there, it's still on the surface.\" According to the EPA and the Colorado health department, former plant workers, as well as current construction workers might have greater exposure through inhaling radioactive dust than the average construction worker. The 1982 documentary film, \"Dark Circle\", discloses worker safety issues at the Rocky Flats Plant, and lack of workplace regulations. Hazards at Rocky Flats included perforated (damaged) gloves for handling radioactive materials, and incidents when workers directly inhaled irradiated air.\n\nOn October 3, 1975, plutonium-laced sludge breached the office wall of health inspector, Byron Vaigneur at the South Carolina-based Savannah River nuclear Weapons Site. He later developed breast cancer and chronic beryllium disease. According to a 2015 report by the Tribune News Service, Vaigneur is one of 107,394 Americans who have developed cancer and other environmental diseases from working in the nuclear weapons industry over the past 70 years. Nuclear stockpile related disease has cost American taxpayers $12 billion in medical expense payouts to workers.\n\nIncidents of worker exposure to radioactive materials in the commercial nuclear energy industry is well documented. A recent report by PBS investigative reporter and a year-long investigation by McClatchy News showed that there are more than 33,000 male and female nuclear workers who have died from nuclear work related illnesses, and more than 100,000 people in the U.S. diagnosed with cancer and other radiologically induced diseases.\n\nThousands of contracted nuclear power plant \"jumpers\", \"nuclear janitors\" or \"Glow Boys\" employed by Atlantic Nuclear Services, Inc. (ANC) and other agencies are recruited to quickly resolve breakdowns, plug leaks, and clean up spills before reaching the allowed dose of radiation exposure. Officially known as nozzle dam technicians, enter containment structures to work on the steam generators. They work swiftly as within five minutes a jumper can be exposed to 1 rem of radiation (equivalent to 50 chest X-rays). A 1982 report states that the NRC limits contract worker exposures to 5 rems per year, however a 1984 report states that the NRC allows jumpers to be exposed to 5 to 12 rems per year. In addition to the danger of external contamination, jumpers can be exposed to internal contamination from breathing or ingesting airborne radioactive particles. The archive of event notification reports from the Nuclear Regulatory Commission, dated from 1999 - 2014, is located at http://www.nrc.gov/reading-rm/doc-collections/event-status/event/ Event reports from the International Atomic Energy Agency is located at: http://www-news.iaea.org/EventList.aspx\n\nNuclear divers are laborers that work fully submerged in radiated water at nuclear reactors. There are three types of diver tasks: radioactive dives, non-radioactive dives, both of which occur inside reactors, and \"mud-work\" that involves cleaning out cooling-water intake systems in lakes, rivers and oceans. In 1986, two divers were killed while cleaning intake pipes at the Crystal River Plant in Florida. In 2006, diver Michael Pickart performed a dive inside an Arkansas nuclear reactor, and was exposed to 450 millirems of radiation.\n\nRadium workers in the early 20th century, known as Radium Girls or Luminizers, incurred exposure doses that caused skeletal diseases including bone cancer. Radium was used as an alleged medical \"cure\" for a variety of ailments, as well as to create luminous clock and instrument dials. Radium-dial painters, mostly young women at production facilities in New Jersey, Pennsylvania, Illinois and other sites, succumbed to occupational injury and disease. Between the years of 1915 and 1959, there were 1,747 females and 161 males employed as \"measured dial\" Luminizers, and 1,910 unmeasured female workers, and 315 unmeasured male workers. The most common health issue was \"radium jaw\" (bone necrosis), anemia, epidermoid carcinomas, and sarcomas. The National Academy of Sciences Biological Effects of Ionizing Radiation, BEIR VII Phase 2 report, shows that women and children are more susceptible to increased cancer mortality than men. (Page 311 of the report shows this data in a graph.)\n\nThe 1991 Final Report of the Nuclear Shipyard Worker Study (NSWS) analyzed the effects of radiation exposure in the U.S. to three cohort groups: 27,872 high-dose nuclear workers, 10348 low-dose nuclear workers, and a control group of 32,510 shipyard workers not exposed to radiation. Dose reconstruction for occupational radiation exposure used by the U.S. Department of Labor assumes that the probability of cancer is \"at least as likely as not\" rendering it complex for workers to claim compensation via The Act.\n\nThe most famous of U.S. case of on an incident involving a nuclear worker is that of Karen Silkwood, an employee of the Kerr-McGee Cimarron Fuel Fabrication Site in Crescent, Oklahoma. Silkwood was a technician, whose job was to make plutonium fuel pellets for assembly into nuclear reactor fuel rods. She was also a labor union activist negotiating for higher health and safety standards. In 1974, the Oil, Chemical and Atomic Workers Union stated that the Kerr-McGee plant had not only manufactured defective fuel rods, but that it had falsified records, and put employees' safety at risk. During the time that she was involved in these labor disputes, on November 5, 1974, she found that she had been contaminated with plutonium over 400 times the legal limit. On November 7, it was found that her internal lung contamination was dangerously high during breath tests, and urine samples. On November 13, 1974, Silkwood was driving to a union meeting with documents regarding her case. She died on the way to the meeting from a severe hit-and-run automobile crash that damaged both the rear end and front end of her vehicle. There is much speculation that her car was forced off the road by another vehicle. Her body was examined by Los Alamos Laboratory Tissue Analysis Program as requested by the Atomic Energy Commission and the State Medical Examiner. It was found that there were significant amounts of plutonium in her lungs, and even higher amounts in her gastrointestinal organs. In 2014, her Lawyer, Gerry Spence gave a two part interview, on the implications of her case in relation to compensation for radiation injury, and on proving strict liability and physical injury in nuclear facilities.\n\nThe Three Mile Island accident in Pennsylvania occurred on March 28, 1979 was rated a 5 on the 7-point International Nuclear Event Scale resulting in the meltdown of radioactive fuel in the Unit 2 reactor.\n\nOn January 4, 1986, a tank containing uranium hexafluoride (UF6) ruptured, releasing 14.5 tons of gaseous UF6 into the environment and causing the death of James Harrison, a 25-year old African American/Cherokee worker, and the hospitalization of 37 workers at the plant. Approximately 100 downwinders were affected by the leak, and treated for inhalation of the toxic gas. The tank was overloaded with 2000 pounds beyond its capacity.\n\nLocated in Western upstate New York, the West Valley nuclear site operated as a commercial nuclear material reprocessing site from 1966 to 1972. In those years the plant processes high and low-level waste, and had a high incident rate of workers exposed to radiation; Science journal reported \"almost without precedent in a major nuclear facility.\" In 1980 the U.S. Congress approved an Act (P.L. 96-368) that required the U.S. Department of Energy (DOE) and other agencies to clean up contaminated water and land resources, at the cost of $5.2 billion. In 2006, New York State filed a lawsuit against the DOE to commit to a long-term clean up and stewardship plan, assigning Federal accountability, and reimbursement of costs to New York state.\n\nThe Waste Isolation Pilot Plant, was designed as a pilot, test study site for deep geologic storage of radioactive waste. It is managed by the U.S. Department of Energy (DOE) and currently serves as the nation's only deep geological repository for transuranic (TRU) nuclear waste generated by the military and defense industry. It is located in Southern New Mexico near the border of Texas and Mexico. It has been disposing of waste 2,150 feet underground in the ancient Permian Sea salt formation since 1999, accepting waste from 22 national atomic legacy sites. Designed to last tens thousand years, the WIPP site had its first leak of airborne radioactive materials on February 1, 2014. 140 employees working underground at the time were sheltered indoors. 13 of these tested positive for internal radioactive contamination. Internal exposure to radioactive isotopes is more serious than external exposure, as these particles lodge in the body for decades, irradiating the surrounding tissues, thus increasing the risk of future cancers and other health effects. A second leak at the plant occurred shortly after the first, releasing plutonium and other radiotoxins, causing concern for communities living near the repository. Since opening in 1999, the WIPP \"pilot site\" has received over 11,000 shipments of TRU waste (transuranic waste). During the February 14, 2014 leak, 22 workers were exposed to radioactive materials. Don Hancock, Director of the Nuclear Waste Safety Program for the SouthWest Research and information Center describes the theory of how nitrate salts in the \"kitty litter\" absorbent interacted with plutonium causing the breach of one or more 55-gallon drums stored at WIPP through a chemical reaction that caused an inflagration. Fundamental questions remain regarding the Department of Energy's clean up standards for WIPP, as there is not a \"clean-up\" standard or regulation for the underground site, by either the DOE oversight or the company contracted to oversee the site, Nuclear Waste Partnership. Over the past 15 years, 91,000 cubic meters of radioactive waste, and more than 171,000 containers of radioactive waste have been placed at WIPP - more than any other site in the country.\n\n"}
{"id": "29970497", "url": "https://en.wikipedia.org/wiki?curid=29970497", "title": "Optical lift", "text": "Optical lift\n\nOptical lift is an optical analogue of aerodynamic lift, in which a cambered refractive object with differently shaped top and bottom surfaces experiences a stable transverse lift force when placed in a uniform stream of light.\n\n The ability of light to apply pressure to objects is known as radiation pressure, which was first postulated in 1619 and proven in 1900. This is the principle behind the solar sail, which uses light radiation pressure to move through space. A 2010 study by physicist Grover Swartzlander and colleagues of the Rochester Institute of Technology in Rochester, New York shows light is also capable of creating the more complex force of \"lift\", which is the force generated by airfoils that make an airplane rise upwards as it travels forward. This study was published on December 2010 in \"Nature Photonics\" journal. Swartzlander predicted, observed and experimentally verified at a micrometer-scale that when applying a beam of laser light to a semi-cylindrical refractive rod, it automatically torques into a stable angle of attack, and then exhibits uniform motion.\n\nThe experiment began as computer models that suggested when light is incident on a tiny object shaped like a wing, a stable lift force is applied to the particle. Then the researchers decided to do physical experiments in the laboratory, and they created tiny, transparent, micrometer-sized rods that were flat on one side and rounded on the other, rather like airplane wings. They immersed the lightfoils in water and bombarded them with 130 mW infrared laser light from underneath the chamber. Radiation pressure pushes the particles along the direction of propagation, this is called the scatter force, but the excitement came when the particles were forced to the side in a direction perpendicular to the direction of propagating light. The transverse force on the particles is the lift force. The researchers discovered not only that the rods experienced stable lift, but that, depending on refractive index, the rod could have up to two stable angles of attack it rotated to when exposed to the laser light. Symmetrical spheres tested did not exhibit this same lift effect.\n\nIn optical lift, created by a \"lightfoil\", the lift is created within the transparent object as light shines through it and is refracted by its inner surfaces. In the lightfoil rods a greater proportion of light leaves in a direction perpendicular to the beam and this side therefore experiences a larger radiation pressure and hence, lift.\n\n The 2010 discovery of stable optical lift is considered by some physicists to be \"most surprising\". Unlike optical tweezers, an intensity gradient is not required to achieve a transverse force. Many rods may therefore be lifted simultaneously in a single quasi-uniform beam of light. Swartzlander and his team propose using optical lift to power micromachines, transport microscopic particles in a liquid, or to help on self-alignment and steering of solar sails, a form of spacecraft propulsion for interstellar space travel. Solar sails are generally designed to harness light to \"push\" a spacecraft, whereas Swartzlander designed their lightfoil to lift in a perpendicular direction; this is where the idea of being able to steer a future solar sail spacecraft may be applied.\n\nSwartzlander said the next step would be to test lightfoils in air and experiment with a variety of materials with different refractive properties, and with incoherent light.\n\n\n"}
{"id": "8417256", "url": "https://en.wikipedia.org/wiki?curid=8417256", "title": "Paraconical pendulum", "text": "Paraconical pendulum\n\nMaurice Allais, a French researcher, invented the paraconical pendulum around 1950, and during the 1950s he conducted six marathon series of long-term observations, during each of which his team manually operated and manually monitored his pendulum non-stop over about a month. The objective was to investigate possible changes over time of the characteristics of the motion, hypothesized to yield information about asymmetries of inertial space (sometimes described as \"aether flow\").\n\nThe defining feature of the \"paraconical\" or \"ball-borne\" pendulum is that the pendulum is carried upon a ball which rests upon a flat, thus permitting it to rotate about its vertical axis as well as swinging in two perpendicular directions. Therefore a paraconical pendulum has three degrees of freedom. When the rotation of the Earth is taken into account, the formal equations in classical mechanics of the behavior of this dynamical system are rather complex. The rolling friction of the ball upon the flat is extremely low, so that a paraconical pendulum has a high Q factor, principally determined by air resistance. Typically a paraconical pendulum is built as a solid body with a stiff rod, rather than with a flexible wire or cord, and its length is usually about one meter. If an accurately spherical ball and an accurately planar flat are used, a paraconical pendulum is a highly sensitive instrument: as first noted by Allais, and now confirmed by modern researchers, its motion exhibits a 24.8 hour cyclic pattern, presumably a manifestation of lunar influence. \n\n\n"}
{"id": "578784", "url": "https://en.wikipedia.org/wiki?curid=578784", "title": "Photobiology", "text": "Photobiology\n\nPhotobiology is the scientific study of the interactions of light (technically, non-ionizing radiation) and living organisms. The field includes the study of photophysics, photochemistry, photosynthesis, photomorphogenesis, visual processing, circadian rhythms, photomovement, bioluminescence, and ultraviolet radiation effects.\n\nThe division between ionizing radiation and non-ionizing radiation is typically considered to be a photon energy greater than 10 eV, which approximately corresponds to both the first ionization energy of oxygen, and the ionization energy of hydrogen at about 14 eV.\n\n\n"}
{"id": "12825821", "url": "https://en.wikipedia.org/wiki?curid=12825821", "title": "Power system simulation", "text": "Power system simulation\n\nElectrical power system simulation involves power system modeling and network simulation in order to analyze electrical power systems using design/offline or real-time data. Power system simulation software's are a class of computer simulation programs that focus on the operation of electrical power systems. These types of computer programs are used in a wide range of planning and operational situations for:\nApplications of power system simulation include:\n\nThese programs typically make use of mathematical optimization techniques such linear programming, quadratic programming, and mixed integer programming.\n\nKey elements of power systems that are modeled include:\n\nThere are many power simulation software packages in commercial and non-commercial forms that range from utility-scale software to study tools.\n\nThe load-flow calculation is the most common network analysis tool for examining the undisturbed and disturbed network within the scope of operational and strategic planning. \n\nUsing network topology, transmission line parameters, transformer parameters, generator location and limits, and load location and compensation, the load-flow calculation can provide voltage magnitudes and angles for all nodes and loading of network components, such as cables and transformers. With this information, compliance to operating limitations such as those stipulated by voltage ranges and maximum loads, can be examined. This is, for example, important for determining the transmission capacity of underground cables, where the influence of cable bundling on the load capability of each cable has to be taken also into account.\n\nDue to the ability to determine losses and reactive-power allocation, load-flow calculation also supports the planning engineer in the investigation of the most economical operation mode of the network.\n\nWhen changing over from single and/or multi-phase infeed low-voltage meshed networks to isolated networks, load-flow calculation is essential for operational and economical reasons. Load-flow calculation is also the basis of all further network studies, such as motor start-up or investigation of scheduled or unscheduled outages of equipment within the outage simulation.\n\nEspecially when investigating motor start-up, the load-flow calculation results give helpful hints, for example, of whether the motor can be started in spite of the voltage drop caused by the start-up current.\n\nShort circuit analysis analyzes the power flow after a fault occurs in a power network. The faults may be three-phase short circuit, one-phase grounded, two-phase short circuit, two-phase grounded, one-phase break, two-phase break or complex faults. Results of such an analysis may help determine the following: \n\nThe goal of transient stability simulation of power systems is to analyse the stability of a power system from sub-second to several tens of seconds. Stability in this aspect is the ability of the system to quickly return to a stable operating condition after being exposed to a disturbance such as for example a tree falling over an overhead line resulting in the automatic disconnection of that line by its protection systems. In engineering terms, a power system is deemed stable if the substation voltage levels and the rotational speeds of motors and generators return to their normal values in a quick and continuous manner.\nModels typically use the following inputs:\nThe acceptable amount of time it takes grid voltages return to their intended levels is dependent on the magnitude of voltage disturbance, and the most common standard is specified by the CBEMA curve in Figure. 1. This curve informs both electronic equipment design and grid stability data reporting.\n\nThe problem of unit commitment involves finding the least-cost dispatch of available generation resources to meet the electrical load.\n\nGenerating resources can include a wide range of types:\n\nThe key decision variables that are decided by the computer program are:\n\nThe latter decisions are binary {0,1}, which means that the mathematical problem is not continuous.\n\nIn addition, generating plants are subject to a number of complex technical constraints, including:\n\nThese constraints have many different variants; all this gives rise to a large class of mathematical optimization problems.\n\nElectricity flows through an AC network according to Kirchhoff's Laws. Transmission lines are subject to thermal limits (simple megawatt limits on flow), as well as voltage and electrical stability constraints.\n\nThe simulator must calculate the flows in the AC network that result from any given combination of unit commitment and generator megawatt dispatch, and ensure that AC line flows are within both the thermal limits and the voltage and stability constraints. This may include contingencies such as the loss of any one transmission or generation element - a so-called security-constrained optimal power flow (SCOPF), and if the unit commitment is optimized inside this framework we have a security-constrained unit commitment (SCUC).\n\nIn optimal power flow (OPF) the generalised scalar objective to be minimised is given by:\n\nwhere \"u\" is a set of the control variables, \"x\" is a set of independent variables, and the subscript 0 indicates that the variable refers to the pre-contingency power system.\"\n\nThe SCOPF is bound by equality and inequality constraint limits. The equality constraint limits are given by the pre and post contingency power flow equations, where \"k\" refers to the \"k\"th contingency case:\n\nThe equipment and operating limits are given by the following inequalities:\n\nThe objective function in OPF can take on different forms relating to active or reactive power quantities that we wish to either minimise or maximise. For example we may wish to minimise transmission losses or minimise real power generation costs on a power network.\n\nOther power flow solution methods like stochastic optimization incorporate the uncertainty found in modeling power systems by using the probability distributions of certain variables whose exact values are not known. When uncertainties in the constraints are present, such as for dynamic line ratings, chance constrained optimization can be used where the probability of violating a constraint is limited to a certain value. Another technique to model variability is the Monte Carlo method, in which different combinations of inputs and resulting outputs are considered based on the probability of their occurrence in the real world. This method can be applied to simulations for system security and unit commitment risk, and it is increasingly being used to model probabilistic load flow with renewable and/or distributed generation.\n\nThe cost of producing a megawatt of electrical energy is a function of:\n\nIn addition to this, generating plant incur fixed costs including:\n\nAssuming perfect competition, the market-based price of electricity would be based purely on the cost of producing the \"next\" megawatt of power, the so-called \"short-run marginal cost\" (SRMC). This price however might not be sufficient to cover the fixed costs of generation, and thus power market prices rarely show purely SRMC pricing. In most established power markets, generators are \"free\" to offer their generation capacity at prices of their choosing. Competition and use of financial contracts keeps these prices close to SRMC, but inevitably offers price above SRMC do occur (for example during the California energy crisis of 2001).\n\nIn the context of power system simulation, a number of techniques have been applied to simulate imperfect competition in electrical power markets:\n\nVarious heuristics have also been applied to this problem. The aim is to provide \"realistic\" forecasts of power market prices, given the forecast supply-demand situation.\n\nPower system long-term optimization focuses on optimizing the multi-year expansion and retirement plan for generation, transmission, and distribution facilities. The optimization problem will typically consider the long term investment cash flow and a simplified version of OPF / UC (Unit commitment), to make sure the power system operates in a secure and economic way. This area can be categorized as:\n\n\nA well-defined power systems study requirement is critical to the success of any project as it will reduce the challenge of selecting the qualified service provider and the right analysis software. The system study specification describes the project scope, analysis types, and the required deliverable. The study specification must be written to match the specific project and industry requirements and will vary based on the type of analysis.\n\nGeneral Electric's MAPS (Multi-Area Production Simulation) is a production simulation model used by various Regional Transmission Organizations and Independent System Operators in the United States to plan for the economic impact of proposed electric transmission and generation facilities in FERC-regulated electric wholesale markets. Portions of the model may also be used for the commitment and dispatch phase (updated on 5 minute intervals) in operation of wholesale electric markets for RTO and ISO regions. ABB's PROMOD is a similar software package. These ISO and RTO regions also utilize a GE software package called MARS (Multi-Area Reliability Simulation) to ensure the power system meets reliability criteria (a loss-of-load-expectation (LOLE) of no greater than 0.1 days per year). Further, a GE software package called PSLF (Positive Sequence Load Flow) and a Siemens software package called PSSE (Power System Simulation for Engineering) analyzes load flow on the power system for short-circuits and stability during preliminary planning studies by RTOs and ISOs.\n"}
{"id": "10484411", "url": "https://en.wikipedia.org/wiki?curid=10484411", "title": "Richard Tol", "text": "Richard Tol\n\nRichard S. J. Tol (born 2 December 1969, Hoorn, the Netherlands) is a professor of economics at the University of Sussex. He is also professor of the economics of climate change at the Vrije Universiteit Amsterdam. He is a member of the Academia Europaea.\n\nTol obtained an M.Sc. in econometrics & operations research in 1992 and a Ph.D. in economics in 1997 at the VU University Amsterdam. His doctoral thesis was titled, \"A decision-analytic treatise of the enhanced greenhouse effect\". In 1998, he contributed with some nineteen other academics to a joint project of the United Nations Environment Programme at his home university.\n\nHe regularly participates in studies of the Energy Modeling Forum, is an editor of \"Energy Economics\", associate editor of \"Environmental and Resource Economics\", and a member of the editorial board of \"Environmental Science and Policy\", and \"Integrated Assessment\". IDEAS/RePEc ranks him among the top 250 economists in the world.\n\nTol specialises in energy economics and environmental economics, with a particular interest in climate change, such as the economics of global warming. Previously, Tol was a Research Professor at the Economic and Social Research Institute. Before that, Tol was the Michael Otto Professor of Sustainability and Global Change and director of the Center for Marine and Atmospheric Sciences and board member of the Center for Marine and Climate Research at the University of Hamburg. Tol was a board member of the International Max Planck Research Schools on Earth System Modeling and Maritime Affairs and the European Forum on Integrated Environmental Assessment. From 1998–2008 he was an adjunct professor at Carnegie Mellon University's Department of Engineering and Public Policy, and from 2010–2011 an adjunct professor at Trinity College, Dublin's Department of Economics.\n\nAccording to Tol \"the impact of climate change is relatively small\". He was also among the US Senate Republican Party's \"list of scientists disputing man-made global warming claims\", which stated that Tol \"dismissed the idea that mankind must act now to prevent catastrophic global warming\".\n\nTol characterises his position as arguing that the economic costs of climate policy should be kept in proportion to its benefits.\n\nHe argues against the 2 °C 'guardrail' target for limiting temperature rises. Tol does not advocate another target, but has suggested that a carbon tax of $20/tC would be a policy in line with estimates of the cost of carbon. He acknowledges that this level of taxation is too low to significantly discourage fossil fuel use but argues it would help to stimulate the development of fuel-saving technology and improve the competitiveness of renewable energy sources. He states that compliance may affect the coal and oil industries and the people they employ.\n\nIn an interview with Der Spiegel in 2005, he argued that temperature rises between 2–4 °C would also have advantages. North of a line drawn from Paris to Munich, people would benefit, e.g., from reduced energy bills. However, south of it, people would be overall \"losers\" of climate change.\n\nIn 2007, Tol predicted a reduction in annual economic growth by 0.4% in the Republic of Ireland if greenhouse gases were reduced by 3% per year.\n\nIn 2009, Tol published an influential paper that combined data from several earlier studies, concluding that at least some amount of global warming could lead to economic gains. In 2014, he published an update, correcting missing minus signs that had turned economic costs into benefits and adding data overlooked before; the mistakes he attributed to \"gremlins\". According to Tol, the old and new results were not significantly different. The degree to which the corrected, more pessimistic results alter the original conclusions and their policy implications was hotly debated. In 2015 it was reported that a second round of corrections to the paper was necessary.\n\nTol was a coordinating lead author for the IPCC Fifth Assessment Report Working Group II: Impacts, Adaptation and Vulnerability. Tol said in March 2014 that he had withdrawn from the writing team for the Summary for Policy Makers of the report in September 2013, citing disagreement with the profile of the report which he considered too alarmist and putting too little emphasis on opportunities to adapt to climate changes. On May 20, 2014, Tol wrote, in an article for FoxNews.com, that the IPCC is alarmist because it favors initial scientific papers published on an issue, rather than the follow-up papers, which, he says, tend to \"pooh-pooh the initial drama.\"\n\nBjørn Lomborg chose Tol to participate in his \"Copenhagen Consensus\" project in 2008. In 2008, Tol collaborated with Gary Yohe, Richard G. Richels and Geoffrey Blanford to prepare the \"Challenge Paper\" on global warming which examined three approaches devised by Lomborg for tackling the issue. The 3 results were then compared with 27 similar investigations, 3 each relating to 9 other 'challenges' in the areas of health and environment. Of the 30 policy alternatives that resulted, Lomborg's ranking procedure rated the 2 dealing with controlling emissions of greenhouse gases 29th and 30th in terms of cost effectiveness.\n\nA \"perspective paper\" by Anil Markandya of the University of Bath on the Yohe/Tol study stated that \"a short time period analysis is misleading\" when all the costs are incurred during the period examined but benefits continue to accrue after its conclusion. He pointed out that the study \"stops short of the most that can be supported on a cost benefit basis\" and stated that \"it does not seem reasonable\" to rely solely on Tol's own FUND model when alternatives \"reported in the peer-reviewed literature are also credible\".\n\nGary Yohe later accused Lomborg of \"deliberate distortion of our conclusions\", adding that \"as one of the authors of the Copenhagen Consensus Project's principal climate paper, I can say with certainty that Lomborg is misrepresenting our findings thanks to a highly selective memory\". In a subsequent joint statement settling their differences, Lomborg and Yohe agreed that the \"failure\" of Lomborg's emissions reduction plan \"could be traced to faulty design\".\n\nLomborg awarded Tol a position on his Copenhagen Consensus panel again in 2009. According to Tol, \"Lomborg successfully punches holes in climate hysteria\" and \"plays a useful role in the debate on climate policy\".\n\nAccording to Tol, \"it is not clear whether climate change would lead to conflict\". Citing a lack of suitable methods for evaluating hypothetical conflicts numerically, he examines what he calls plausible scenarios, such as drought and migration in the Horn of Africa or an upsurge in terrorism.\n\nRegarding terrorism, he says \"it may well be that a Maldivian terrorist will try and blow up the headquarters of ExxonAramco\". Regarding the Horn of Africa scenario, he acknowledges it might cause substantial human suffering but assesses the probability of this actually happening as unlikely. He concludes that \"poor and exhausted people are unlikely to take up arms, and if they do, they are probably not very effective\".\n"}
{"id": "1970679", "url": "https://en.wikipedia.org/wiki?curid=1970679", "title": "Saucisson (pyrotechnics)", "text": "Saucisson (pyrotechnics)\n\nIn early military engineering, a saucisson (French for a large, dry-filled sausage) was a primitive type of fuse, consisting of a long tube or hose of cloth or leather, typically about an inch and half in diameter (37 mm), damp-proofed with pitch and filled with black powder. It was normally laid in a protective wooden trough, and ignited by use of a torch or slow match. Saucissons were used to fire fougasses, petards, mines and camouflets.\n\nVery long fascines were also called saucissons.\n\nLater, in early 20th century mining jargon, a saucisson referred to the flexible casings used for explosives in mine operations.\n"}
{"id": "49330245", "url": "https://en.wikipedia.org/wiki?curid=49330245", "title": "Sawantwadi toys", "text": "Sawantwadi toys\n\nSawantwadi toys refers to hand made works of art made of wood in Sawantwadi a town in Sindhudurg district of Maharashtra, an Indian state. Sawantwadi is considered to be India's largest market for wooden models of fruits and wooden toys. Most of these toys are made in the village of Kolgaon in Sawantwadi taluka, these toys are made from the wood of the Indian Coral tree (Erythrina variegata). Craftsmen who make these toys belong to the Chittari community who came to Sawantwadi from Karwar and Goa. It is reported that the makers of these toys face the challenge of competition from mass produced cheap Chinese products. A news story dated May, 2013, bemoans the lack of state support to this traditional art form.\n\nA 2010 news story reports of attempts to obtain Geographical indication registration for the toys.\n"}
{"id": "746497", "url": "https://en.wikipedia.org/wiki?curid=746497", "title": "Slip (aerodynamics)", "text": "Slip (aerodynamics)\n\nA slip is an aerodynamic state where an aircraft is moving \"somewhat\" sideways as well as forward relative to the oncoming airflow or relative wind. In other words, for a conventional aircraft, the nose will be pointing in the opposite direction to the bank of the wing(s). The aircraft is not in coordinated flight and therefore is flying inefficiently.\n\nFlying in a slip is aerodynamically inefficient, since the lift-to-drag ratio is reduced. More drag is at play consuming energy but not producing lift. Inexperienced or inattentive pilots will often enter slips unintentionally during turns by failing to coordinate the aircraft with the rudder. Airplanes can readily enter into a slip climbing out from take-off on a windy day. If left unchecked, climb performance will suffer. This is especially dangerous if there are nearby obstructions under the climb path and the aircraft is underpowered or heavily loaded.\n\nA slip can also be a \"piloting maneuver\" where the pilot deliberately enters one type of slip or another. Slips are particularly useful in performing a short field landing over an obstacle (such as trees, or power lines), or to avoid an obstacle (such as a single tree on the extended centerline of the runway), and may be practiced as part of emergency landing procedures. These methods are also commonly employed when flying into farmstead or rough country airstrips where the landing strip is short. Pilots need to touch down with ample runway remaining to slow down and stop.\n\nThere are common situations where a pilot may deliberately enter a slip by using opposite rudder and aileron inputs, most commonly in a landing approach at low power.\n\nWithout flaps or spoilers it is difficult to increase the steepness of the glide without adding significant speed. This excess speed can cause the aircraft to fly in ground effect for an extended period, perhaps running out of runway. In a forward slip much more drag is created, allowing the pilot to dissipate altitude without increasing airspeed, increasing the angle of descent (glide slope). Forward slips are especially useful when operating pre-1950s training aircraft, aerobatic aircraft such as the Pitts Special or any aircraft with inoperative flaps or spoilers.\n\nOften, if an airplane in a slip is made to stall, it displays very little of the yawing tendency that causes a skidding stall to develop into a spin. A stalling airplane in a slip may do little more than tend to roll into a wings-level attitude. In fact, in some airplanes stall characteristics may even be improved.\n\nAerodynamically these are identical once established, but they are entered for different reasons and will create different ground tracks and headings relative to those prior to entry. Forward-slip is used to steepen an approach (reduce height) without gaining much airspeed, benefiting from the increased drag. The sideslip moves the aircraft sideways (often, only in relation to the wind) where executing a turn would be inadvisable, drag is considered a byproduct. Most pilots like to enter sideslip just before flaring or touching down during a crosswind landing.\n\nThe forward slip changes the heading of the aircraft away from the down wing, while retaining the original \"track\" (flight path over the ground) of the aircraft.\n\nTo execute a forward slip, the pilot banks into the wind and applies opposing rudder (e.g., right aileron + left rudder) in order to keep moving towards the target. If you were the target you would see the plane's nose off to one side, a wing off to the other side and tilted down toward you. The pilot must make sure that the plane's nose is low enough to keep airspeed up. However, airframe speed limits such as V and V must be observed.\n\nA forward-slip is useful when a pilot has set up for a landing approach with excessive height or must descend steeply beyond a tree line to touchdown near the runway threshold. Assuming that the plane is properly lined up for the runway, the forward slip will allow the aircraft \"track\" to be maintained while steepening the descent without adding excessive airspeed. Since the heading is not aligned with the runway, forward-slip must be removed before touchdown to avoid excessive side loading on the landing gear, and if a cross wind is present an appropriate sideslip may be necessary at touchdown as described below.\n\nIn the United States, student pilots are required to know how to do forward slips before embarking on their first solo flight. The logic is that in the event of an engine failure, the pilot will have to land on the first attempt and will not have a chance to go around if the aircraft is too high or too fast .\n\nThe sideslip also uses aileron and opposite rudder. In this case it is entered by lowering a wing and applying exactly enough opposite rudder so the airplane does not turn (maintaining the same \"heading\"), while maintaining safe airspeed with pitch or power. Compared to Forward-slip, less rudder is used: just enough to stop the change in the heading.\n\nIn the sideslip condition, the airplane's longitudinal axis remains parallel to the original flightpath, but the airplane no longer flies straight along its original track. Now, the horizontal component of lift forces the airplane to move sideways toward the low wing. This is the still-air, headwind or tailwind scenario. In case of crosswind, if the wing is lowered into the wind, the wind may push the airplane downwind and airplane may fly the original track. This is the sideslip approach technique used by many pilots in crosswind conditions [sideslip without slipping]. Other method is the crab-technique in which nose is pointed into the wind and the resulting drift keeps the airplane on the desired track. Wings are maintained level all the time.\n\nA sideslip may be used exclusively to remain lined up with a runway centerline while on approach in a crosswind or be employed in the final moments of a crosswind landing. To commence sideslipping, the pilot rolls the airplane toward the wind to maintain runway centerline position while maintaining heading on the centerline with the rudder. Sideslip causes one main landing gear to touch down first, followed by the second main gear. This allows the wheels to be constantly aligned with the track, thus avoiding any side load at touchdown.\n\nThe sideslip method for crosswind landings is not suitable for long-winged and low-sitting aircraft such as gliders, where instead a crab angle (heading into the wind) is maintained until a moment before touchdown.\n\nAircraft manufacturer Airbus recommends sideslip approach only in low crosswind conditions.\n\nThe sideslip angle, also called angle of sideslip (AOS, AoS, formula_1, Greek letter beta), is a term used in fluid dynamics and aerodynamics and aviation. It relates to the rotation of the aircraft centerline from the relative wind. In flight dynamics it is given the shorthand notation formula_1 (beta) and is usually assigned to be \"positive\" when the relative wind is coming from the right of the nose of the airplane. The sideslip angle formula_1 is essentially the directional angle of attack of the airplane. It is the primary parameter in directional stability considerations.\n\nIn vehicle dynamics, side slip angle is defined as the angle made by the velocity vector to longitudinal axis of the vehicle at the center of gravity in an instantaneous frame. As the lateral acceleration increases during cornering, the side slip angle decreases. Thus at very high speed turns and small turning radius, there is a high lateral acceleration and formula_1 could be a negative value.\n\nThere are other, specialized circumstances where slips can be useful in aviation. For example, during aerial photography, a slip can lower one side of the aircraft to allow ground photos to be taken through a side window. Pilots will also use a slip to land in icing conditions if the front windshield has been entirely iced over—by landing slightly sideways, the pilot is able to see the runway through the aircraft's side window. Slips also play a role in aerobatics and aerial combat.\n\n\nWhen an aircraft is put into a forward slip with no other changes to the throttle or elevator, the pilot will notice an increased rate of descent (or reduced rate of \"ascent\"). This is usually mostly due to increased drag on the fuselage. The airflow over the fuselage is at a sideways angle, increasing the relative frontal area, which increases drag.\n\n\n"}
{"id": "30045", "url": "https://en.wikipedia.org/wiki?curid=30045", "title": "Terbium", "text": "Terbium\n\nTerbium is a chemical element with symbol Tb and atomic number 65. It is a silvery-white, rare earth metal that is malleable, ductile, and soft enough to be cut with a knife. The ninth member of the lanthanide series, terbium is a fairly electropositive metal that reacts with water, evolving hydrogen gas. Terbium is never found in nature as a free element, but it is contained in many minerals, including cerite, gadolinite, monazite, xenotime, and euxenite.\n\nSwedish chemist Carl Gustaf Mosander discovered terbium as a chemical element in 1843. He detected it as an impurity in yttrium oxide, YO. Yttrium and terbium are named after the village of Ytterby in Sweden. Terbium was not isolated in pure form until the advent of ion exchange techniques.\n\nTerbium is used to dope calcium fluoride, calcium tungstate and strontium molybdate, materials that are used in solid-state devices, and as a crystal stabilizer of fuel cells which operate at elevated temperatures. As a component of Terfenol-D (an alloy that expands and contracts when exposed to magnetic fields more than any other alloy), terbium is of use in actuators, in naval sonar systems and in sensors.\n\nMost of the world's terbium supply is used in green phosphors. Terbium oxide is in fluorescent lamps and television and monitor cathode ray tubes (CRTs). Terbium green phosphors are combined with divalent europium blue phosphors and trivalent europium red phosphors to provide trichromatic lighting technology, a high-efficiency white light used for standard illumination in indoor lighting.\n\nTerbium is a silvery-white rare earth metal that is malleable, ductile and soft enough to be cut with a knife. It is relatively stable in air compared to the earlier, more reactive lanthanides in the first half of the lanthanide series. Terbium exists in two crystal allotropes with a transformation temperature of 1289 °C between them. The 65 electrons of a terbium atom are arranged in the electron configuration [Xe]4f6s; normally, only three electrons can be removed before the nuclear charge becomes too great to allow further ionization, but in the case of terbium, the stability of the half-filled [Xe]4f configuration allows further ionization of a fourth electron in the presence of very strong oxidizing agents such as fluorine gas.\n\nThe terbium(III) cation is brilliantly fluorescent, in a bright lemon-yellow color that is the result of a strong green emission line in combination with other lines in the orange and red. The yttrofluorite variety of the mineral fluorite owes its creamy-yellow fluorescence in part to terbium. Terbium easily oxidizes, and is therefore used in its elemental form specifically for research. Single terbium atoms have been isolated by implanting them into fullerene molecules.\n\nTerbium has a simple ferromagnetic ordering at temperatures below 219 K. Above 219 K, it turns into a helical antiferromagnetic state in which all of the atomic moments in a particular basal plane layer are parallel, and oriented at a fixed angle to the moments of adjacent layers. This unusual antiferromagnetism transforms into a disordered paramagnetic state at 230 K.\n\nThe most common oxidation state of terbium is +3, as in . The +4 state is known in TbO and TbF.\nTerbium burns readily to form a mixed terbium(III,IV) oxide:\n\nIn solution, terbium forms only trivalent ions. Terbium is quite electropositive and reacts slowly with cold water and quite quickly with hot water to form terbium hydroxide:\n\nTerbium metal reacts with all the halogens, forming white trihalides:\n\nTerbium dissolves readily in dilute sulfuric acid to form solutions containing the pale pink terbium(III) ions, which exist as [Tb(OH)] complexes:\n\nTerbium combines with nitrogen, carbon, sulfur, phosphorus, boron, selenium, silicon and arsenic at elevated temperatures, forming various binary compounds such as TbH, TbH, TbB, TbS, TbSe, TbTe and TbN. In those compounds, Tb mostly exhibits the oxidation states +3 and sometimes +2. Terbium(II) halogenides are obtained by annealing Tb(III) halogenides in presence of metallic Tb in tantalum containers. Terbium also forms sesquichloride TbCl, which can be further reduced to TbCl by annealing at 800 °C. This terbium(I) chloride forms platelets with layered graphite-like structure.\n\nOther compounds include \nTerbium(IV) fluoride is a strong fluorinating agent, emitting relatively pure atomic fluorine when heated rather than the mixture of fluoride vapors emitted from CoF or CeF.\n\nNaturally occurring terbium is composed of its only stable isotope, terbium-159; the element is thus called mononuclidic and monoisotopic. Thirty-six radioisotopes have been characterized, with the heaviest being terbium-171 (with atomic mass of 170.95330(86) u) and lightest being terbium-135 (exact mass unknown). The most stable synthetic radioisotopes of terbium are terbium-158, with a half-life of 180 years, and terbium-157, with a half-life of 71 years. All of the remaining radioactive isotopes have half-lives that are much less than a quarter of a year, and the majority of these have half-lives that are less than half a minute. The primary decay mode before the most abundant stable isotope, Tb, is electron capture, which results in production of gadolinium isotopes, and the primary mode after is beta minus decay, resulting in dysprosium isotopes.\n\nThe element also has 27 nuclear isomers, with masses of 141–154, 156, and 158 (not every mass number corresponds to only one isomer). The most stable of them are terbium-156m, with half-life of 24.4 hours and terbium-156m2, with half-life of 22.7 hours; this is longer than half-lives of most ground states of radioactive terbium isotopes, except only those with mass numbers 155–161.\n\nSwedish chemist Carl Gustaf Mosander discovered terbium in 1843. He detected it as an impurity in yttrium oxide, YO. Yttrium is named after the village of Ytterby in Sweden. Terbium was not isolated in pure form until the advent of ion exchange techniques.\n\nMosander first separated yttria into three fractions, all named for the ore: yttria, erbia, and terbia. \"Terbia\" was originally the fraction that contained the pink color, due to the element now known as erbium. \"Erbia\" (containing what we now call terbium) originally was the fraction that was essentially colorless in solution. The insoluble oxide of this element was noted to be tinged brown.\n\nLater workers had difficulty in observing the minor colorless \"erbia\", but the soluble pink fraction was impossible to miss. Arguments went back and forth as to whether erbia even existed. In the confusion, the original names got reversed, and the exchange of names stuck, so that the pink fraction referred eventually to the solution containing erbium (which in solution, is pink). It is now thought that workers using double sodium or potassium sulfates to remove ceria from yttria inadvertently lost the terbium into the ceria-containing precipitate. What is now known as terbium was only about 1% of the original yttria, but that was sufficient to impart a yellowish color to the yttrium oxide. Thus, terbium was a minor component in the original fraction containing it, where it was dominated by its immediate neighbors, gadolinium and dysprosium.\n\nThereafter, whenever other rare earths were teased apart from this mixture, whichever fraction gave the brown oxide retained the terbium name, until at last, the brown oxide of terbium was obtained in pure form. The 19th century investigators did not have the benefit of the UV fluorescence technology to observe the brilliant yellow or green Tb(III) fluorescence that would have made terbium easier to identify in solid mixtures or solutions.\n\nTerbium is contained along with other rare earth elements in many minerals, including monazite ((Ce,La,Th,Nd,Y)PO with up to 0.03% terbium), xenotime (YPO) and euxenite ((Y,Ca,Er,La,Ce,U,Th)(Nb,Ta,Ti)O with 1% or more terbium). The crust abundance of terbium is estimated as 1.2 mg/kg. No terbium-dominant mineral has yet been found.\n\nCurrently, the richest commercial sources of terbium are the ion-adsorption clays of southern China; the concentrates with about two-thirds yttrium oxide by weight have about 1% terbia. Small amounts of terbium occur in bastnäsite and monazite; when these are processed by solvent extraction to recover the valuable heavy lanthanides as samarium-europium-gadolinium concentrate, terbium is recovered therein. Due to the large volumes of bastnäsite processed relative to the ion-adsorption clays, a significant proportion of the world's terbium supply comes from bastnäsite.\n\nCrushed terbium-containing minerals are treated with hot concentrated sulfuric acid to produce water-soluble sulfates of rare earths. The acidic filtrates are partially neutralized with caustic soda to pH 3–4. Thorium precipitates out of solution as hydroxide and is removed. After that the solution is treated with ammonium oxalate to convert rare earths into their insoluble oxalates. The oxalates are decomposed to oxides by heating. The oxides are dissolved in nitric acid that excludes one of the main components, cerium, whose oxide is insoluble in HNO. Terbium is separated as a double salt with ammonium nitrate by crystallization.\n\nThe most efficient separation routine for terbium salt from the rare-earth salt solution is ion exchange. In this process, rare-earth ions are sorbed onto suitable ion-exchange resin by exchange with hydrogen, ammonium or cupric ions present in the resin. The rare earth ions are then selectively washed out by suitable complexing agent. As with other rare earths, terbium metal is produced by reducing the anhydrous chloride or fluoride with calcium metal. Calcium and tantalum impurities can be removed by vacuum remelting, distillation, amalgam formation or zone melting.\n\nTerbium is used as a dopant in calcium fluoride, calcium tungstate, and strontium molybdate, materials that are used in solid-state devices, and as a crystal stabilizer of fuel cells which operate at elevated temperatures, together with ZrO.\n\nTerbium is also used in alloys and in the production of electronic devices. As a component of Terfenol-D, terbium is used in actuators, in naval sonar systems, sensors, in the SoundBug device (its first commercial application), and other magnetomechanical devices. Terfenol-D is a terbium alloy that expands or contracts in the presence of a magnetic field. It has the highest magnetostriction of any alloy.\n\nTerbium oxide is used in green phosphors in fluorescent lamps and color TV tubes. Sodium terbium borate is used in solid state devices. The brilliant fluorescence allows terbium to be used as a probe in biochemistry, where it somewhat resembles calcium in its behavior. Terbium \"green\" phosphors (which fluoresce a brilliant lemon-yellow) are combined with divalent europium blue phosphors and trivalent europium red phosphors to provide the trichromatic lighting technology which is by far the largest consumer of the world's terbium supply. Trichromatic lighting provides much higher light output for a given amount of electrical energy than does incandescent lighting.\n\nTerbium is also used to detect endospores, as it acts as an assay of dipicolinic acid based on photoluminescence.\n\nAs with the other lanthanides, terbium compounds are of low to moderate toxicity, although their toxicity has not been investigated in detail. Terbium has no known biological role.\n\n"}
{"id": "25914626", "url": "https://en.wikipedia.org/wiki?curid=25914626", "title": "Tina Birbili", "text": "Tina Birbili\n\nKonstantina Birbili () (born 1969), commonly known as Tina Birbili, was the Minister for the Environment, Energy and Climate Change of Greece until June 17, 2011. Birbili was the first holder of this office, which was created to succeed the former Ministry for the Environment, Physical Planning and Public Works by Greek prime minister George Papandreou in October 2009.\n\nBirbili had no prior government experience on her appointment, but she had worked as an advisor to prime minister Papandreou during his tenure at the foreign ministry. She was regarded as a strong advocate for environmental issues within the Panhellenic Socialist Movement party.\n\nA physicist by training, Birbili attended the University of Athens and London's Imperial College, obtaining a PhD in 1995.\n\n"}
{"id": "215051", "url": "https://en.wikipedia.org/wiki?curid=215051", "title": "Tropospheric ozone", "text": "Tropospheric ozone\n\nOzone (O) is a trace gas of the troposphere, with an average concentration of 20-30 parts per billion by volume (ppbv), with close to 100 ppbv in polluted areas. . Ozone is also an important constituent of the stratosphere, where the ozone layer exists. The troposphere is the lowest layer of the Earth's atmosphere. It extends from the ground up to a variable height of approximately 14 kilometers above sea level. Ozone is least concentrated in the ground layer (or planetary boundary layer) of the troposphere. It's concentration increases as height above sea level increases, with a maximum concentration at the tropopause. About 90% of total ozone in the atmosphere is in the stratosphere, and 10% is in the troposphere. Although tropospheric ozone is less concentrated than stratospheric ozone, it is of concern because of its health effects. Ozone in the troposphere is considered a greenhouse gas, and may contribute to global warming. \n\nPhoto-chemical and chemical reactions involving ozone drive many of the chemical processes that occur in the troposphere by day and by night. At abnormally high concentrations brought about by human activities (the largest source being emissions from combustion of fossil fuels), it is a pollutant, and a constituent of smog. \n\nPhotolysis of ozone occurs at wavelengths below approximately 310-320 nano-meters . This reaction initiates the chain of chemical reactions that remove carbon monoxide, methane, and other hydrocarbons from the atmosphere via oxidation. Therefore, the concentration of tropospheric ozone affects how long these compounds remain in the air. If the oxidation of carbon monoxide or methane occur in the presence of nitrogen monoxide (NO), this chain of reactions has a net product of ozone added to the system. \n\nOzone in the atmosphere can be measured by remote sensing technology, or by \"in-situ\" monitoring technology. Because ozone absorbs light in the UV spectrum, the most common way to measure ozone is to measure how much of this light spectrum is absorbed in the atmosphere. Because the stratosphere has higher ozone concentration than the troposphere, it is important for remote sensing instruments to be able to determine altitude along with the concentration measurements. The TOMS-EP instrument aboard a satellite from NASA is an example of an ozone layer measuring satellite , and TES is an example of an ozone measuring satellite that is specifically for the troposphere. Lidar is a common ground based remote sensing technique to measure ozone. TOLnet is the network of ozone observing lidars across the United States. \n\nOzonesondes are a form of in situ, or local measurements. An ozonesonde is an ozone measuring instrument attached to a meteorological balloon, so that the instrument can directly measure ozone concentration at the varying altitudes along the balloon's upward path. The information collected from the instrument attached to the balloon is transmitted back using radiosonde technology. NOAA has worked to create a global network of tropospheric ozone measurements using ozonesondes. \n\nOzone is also measured in air quality environmental monitoring networks. In these networks, in-situ ozone monitors based on ozone's UV-absorption properties are used to measure ppb-levels in ambient air.\n\nThe majority of tropospheric ozone formation occurs when nitrogen oxides (NOx), carbon monoxide (CO) and volatile organic compounds (VOCs), react in the atmosphere in the presence of sunlight, specifically the UV spectrum. NOx, CO, and VOCs are considered ozone precursors. Motor vehicle exhaust, industrial emissions, and chemical solvents are the major anthropogenic sources of these ozone precursors. Although the ozone precursors often originate in urban areas, winds can carry NOx hundreds of kilometers, causing ozone formation to occur in less populated regions as well.\n\nThe chemical reactions that produce tropospheric ozone are a series of interrelated cycles (known as the HOx and NOx cycles); They start with the oxidation of carbon monoxide (CO) or VOCs (such as butane). To begin the process, CO and VOCs are oxidized by the hydroxyl radical (OH) to form carbon dioxide (CO), and water (HO) in the CO oxidation case. These oxidizing reactions then produce the peroxy radical (HO) that will react with NO to produce NO. NO is subsequently photolyzed during by daytime, thus resulting in NO and a single oxygen atom. This single oxygen atom reacts with molecular oxygen O to produce ozone. \n\nAn outline of the chain reaction that occurs in oxidation of CO, producing O: \n\nThe reaction begins with the oxidation of CO by the hydroxyl radical (OH). The radical adduct (•HOCO) is unstable and reacts rapidly with oxygen to give a peroxy radical, HO:\n\nPeroxy-radicals then go on to react with NO to produce NO, which is photolysed by UV-A radiation to give a ground-state atomic oxygen, which then reacts with molecular oxygen to form ozone. \n\nThe amount of ozone produced through these reactions in ambient air can be estimated using a modified Leighton relationship. The limit on these interrelated cycles producing ozone is the reaction of •OH with NO to form nitric acid at high NOx levels. If nitrogen monoxide (NO) is instead present at very low levels in the atmosphere (less than 10 approximately ppt), the of peroxy radicals (HO• ) formed from the oxidation will instead react with themselves to form peroxides, and not produce ozone.  \n\nHealth effects depend on ozone precursors, which is a group of pollutants, primarily generated during the combustion of fossil fuels. Reaction with daylight ultraviolet (UV) rays and these precursors create ground-level ozone pollution (Tropospheric Ozone).\nOzone is known to have the following health effects at concentrations common in urban air:\n\nA statistical study of 95 large urban communities in the United States found significant association between ozone levels and premature death. The study estimated that a one-third reduction in urban ozone concentrations would save roughly 4000 lives per year (Bell et al., 2004). Tropospheric Ozone causes approximately 22,000 premature deaths per year in 25 countries in the European Union. (WHO, 2008)\n\nThe United States Environmental Protection Agency has developed an Air Quality index to help explain air pollution levels to the general public. 8-hour average ozone mole fractions of 76 to 95 nmol/mol are described as \"Unhealthy for Sensitive Groups\", 96 nmol/mol to 115 nmol/mol as \"unhealthy\" and 116 nmol/mol to 404 nmol/mol as \"very unhealthy\" . The EPA has designated over 300 counties of the United States, clustered around the most heavily populated areas (especially in California and the Northeast), as failing to comply with the National Ambient Air Quality Standards.\n\nMelting of sea ice releases molecular chlorine, which reacts with UV radiation to produce chlorine radicals. Because chlorine radicals are highly reactive, they can expedite the degradation of methane and tropospheric ozone and the oxidation of mercury to more toxic forms. Ozone production rises during heat waves, because plants absorb less ozone. It is estimated that curtailed ozone absorption by plants is responsible for the loss of 460 lives in the UK in the hot summer of 2006. A similar investigation to assess the joint effects of ozone and heat during the European heat waves in 2003, concluded that these appear to be additive.\n\n\n\n"}
