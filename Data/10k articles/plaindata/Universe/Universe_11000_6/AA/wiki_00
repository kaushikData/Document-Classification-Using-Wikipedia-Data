{"id": "31224455", "url": "https://en.wikipedia.org/wiki?curid=31224455", "title": "Applications of the Stirling engine", "text": "Applications of the Stirling engine\n\nApplications of the Stirling engine range from mechanical propulsion to heating and cooling to electrical generation systems. A Stirling engine is a heat engine operating by cyclic compression and expansion of air or other gas, the \"working fluid\", at different temperature levels such that there is a net conversion of heat to mechanical work. The Stirling cycle heat engine can also be driven in reverse, using a mechanical energy input to drive heat transfer in a reversed direction (i.e. a heat pump, or refrigerator).\n\nThere are several design configurations for Stirling engines that can be built, many of which require rotary or sliding seals, which can introduce difficult tradeoffs between frictional losses and refrigerant leakage. A free-piston variant of the Stirling engine can be built, which can be completely hermetically sealed, reducing friction losses and completely eliminating refrigerant leakage. For example, a Free Piston Stirling Cooler (FPSC) can convert an electrical energy input into a practical heat pump effect, used for high-efficiency portable refrigerators and freezers. Conversely, a free-piston electrical generator could be built, converting a heat flow into mechanical energy, and then into electricity. In both cases, energy is usually converted from/to electrical energy using magnetic fields in a way that avoids compromising the hermetic seal.\n\nIt is often claimed that the Stirling engine has too low a power/weight ratio, too high a cost, and too long a starting time for automotive applications. They also have complex and expensive heat exchangers. A Stirling cooler must reject twice as much heat as an Otto engine or Diesel engine radiator. The heater must be made of stainless steel, exotic alloy or ceramic to support high heating temperatures needed for high power density, and to contain hydrogen gas that is often used in automotive Stirlings to maximize power. The main difficulties involved in using the Stirling engine in an automotive application are startup time, acceleration response, shutdown time, and weight, not all of which have ready-made solutions.\n\nHowever, a modified Stirling engine has been introduced that uses concepts taken from a patented internal-combustion engine with a sidewall combustion chamber (US patent 7,387,093) that promises to overcome the deficient power-density and specific-power problems, as well as the slow acceleration-response problem inherent in all Stirling engines. It could be possible to use these in co-generation systems that use waste heat from a conventional piston or gas turbine engine's exhaust and use this either to power the ancillaries (e.g.: the alternator) or even as a turbo-compound system that adds power and torque to the crankshaft.\n\nAutomobiles exclusively powered by Stirling engines were developed in test projects by NASA, as well as earlier projects by the Ford Motor Company using engines provided by Philips, and by American Motors Corporation (AMC) with several cars equipped with units from Sweden's United Stirling built under a license from Philips. The NASA vehicle test projects were designed by contractors and designated MOD I and MOD II.\n\nNASA's Stirling MOD 1 powered engineering vehicles were built in partnership with the United States Department of Energy (DOE) and NASA, under contract by AMC's AM General to develop and demonstrate practical alternatives for standard engines. The United Stirling AB's P-40 powered AMC Spirit was tested extensively for over and achieved average fuel efficiency up to . A 1980 4-door liftback VAM Lerma was also converted to United Stirling P-40 power to demonstrate the Stirling engine to the public and to promote the U.S. government's alternative engine program.\n\nTests conducted with the 1979 AMC Spirit, as well as a 1977 Opel and a 1980 AMC Concord revealed the Stirling engines \"could be developed into an automotive power train for passenger vehicles and that it could produce favorable results.\" However, progress was achieved with equal-power spark-ignition engines since 1977, and the Corporate Average Fuel Economy (CAFE) requirements that were to be achieved by automobiles sold in the U.S. were being increased. Moreover, the Stirling engine design continued to exhibit a shortfall in fuel efficiency There were also two major drawbacks for consumers using the Stirling engines: first was the time needed to warm up – because most drivers do not like to wait to start driving; and second was the difficulty in changing the engine's speed – thus limiting driving flexibility on the road and traffic. The process of auto manufacturers converting their existing facilities and tooling for the mass production of a completely new design and type of powerplant was also questioned.\n\nThe MOD II project in 1986 produced one of the most efficient automotive engines ever made. The engine reached a peak thermal efficiency of 38.5%, compared to a modern spark-ignition gasoline engine, which has a peak efficiency of 20-25%. The Mod II project replaced the normal spark-ignition engine in a 1985 4-door Chevrolet Celebrity notchback. In the 1986 MOD II Design Report (Appendix A) the results showed that highway gas mileage was increased from and achieved an urban range of with no change in vehicle gross weight. Startup time in the NASA vehicle was a maximum of 30 seconds,while Ford's research vehicle used an internal electric heater to quickly start the engine, giving a start time of only a few seconds. The high torque output of the Stirling engine at low speed eliminated the need for a torque converter in the transmission resulting in decreased weight and transmission drivetrain losses negating somewhat the weight disadvantage of the Stirling in auto use. This resulted in increased efficiencies being mentioned in the test results.\n\nThe experiments indicated that the Stirling engine could improve vehicle operational efficiency by ideally detaching the Stirling from direct power demands, eliminating a direct mechanical linkage as used in most current vehicles. Its prime function used in an extended-range series electric hybrid vehicle would be as a generator providing electricity to drive the electric vehicle traction motors and charging a buffer battery set. In a petro-hydraulic hybrid the Stirling would perform a similar function as in a petro-electric series-hybrid turning a pump charging a hydraulic buffer tank. Although successful in the MOD 1 and MOD 2 phases of the experiments, cutbacks in funding further research and lack of interest by automakers ended possible commercialization of the Automotive Stirling Engine Program.\n\nStirling engines as part of a hybrid electric drive system may be able to bypass the design challenges or disadvantages of a non-hybrid Stirling automobile.\n\nIn November 2007, a prototype hybrid car using solid biofuel and a Stirling engine was announced by the Precer project in Sweden.\n\nThe New Hampshire Union Leader reports that Dean Kamen has developed a series plug-in hybrid car using a Ford Think. DEKA, Kamen's technology company in the Manchester Millyard, has demonstrated an electric car, the DEKA Revolt, that can reach approximately on a single charge of its lithium battery.\n\nRobert McConaghy created the first flying stirling engine powered aircraft in August 1986. The Beta type engine weighed 360 grams, and produced only 20 Watts of power. The engine was attached to the front of a modified Super Malibu radio control glider with a gross takeoff weight of 1 kg. The best published test flight lasted 6 minutes and exhibited \"barely enough power to make the occasional gentle turn and maintain altitude\".\n\nThe Stirling engine could be well suited for underwater power systems where electrical work or mechanical power is required on an intermittent or continuous level. General Motors have undertaken a considerable amount of work on advanced Stirling cycle engines which include thermal storage for underwater applications. United Stirling, in Malmö, Sweden, are developing an experimental four–cylinder engine using hydrogen peroxide as an oxidant in underwater power systems. The SAGA (Submarine Assistance Great Autonomy) submarine became operational in the 1990s and is driven by two Stirling engines supplied with diesel fuel and liquid oxygen. This system also has potential for surface-ship propulsion, as the engine's size is less of a concern, and placing the radiator section in seawater rather than open air (as a land-based engine would be) allows for it to be smaller.\n\nSwedish shipbuilder Kockums has built 8 successful Stirling powered submarines since the late 1980s. They carry compressed oxygen to allow fuel combustion submerged, providing heat for the Stirling engine. They are currently used on submarines of the \"Gotland\" and \"Södermanland\" classes. They are the first submarines in the world to feature Stirling air-independent propulsion (AIP), which extends their underwater endurance from a few days to several weeks.\n\nThis capability has previously only been available with nuclear-powered submarines.\n\nThe Kockums engine also powers the Japanese \"Sōryū\"-class submarine.\n\nStirling engines can power pumps to move fluids like water, air and gasses. For instance the ST-5 from Stirling Technology Inc. power output of that can run a 3 kW generator or a centrifugal water pump.\n\nIn a combined heat and power (CHP) system, mechanical or electrical power is generated in the usual way, however, the waste heat given off by the engine is used to supply a secondary heating application. This can be virtually anything that uses low temperature heat. It is often a pre-existing energy use, such as commercial space heating, residential water heating, or an industrial process.\n\nThermal power stations on the electric grid use fuel to produce electricity. However, there are large quantities of waste heat produced which often go unused. In other situations, high-grade fuel is burned at high temperature for a low temperature application. According to the second law of thermodynamics, a heat engine can generate power from this temperature difference. In a CHP system, the high temperature primary heat enters the Stirling engine heater, then some of the energy is converted to mechanical power in the engine, and the rest passes through to the cooler, where it exits at a low temperature. The \"waste\" heat actually comes from engine's main \"cooler\", and possibly from other sources such as the exhaust of the burner, if there is one.\n\nThe power produced by the engine can be used to run an industrial or agricultural process, which in turn creates biomass waste refuse that can be used as free fuel for the engine, thus reducing waste removal costs. The overall process can be efficient and cost effective.\n\nInspirit Energy, a UK-based company have a gas fired CHP unit called the Inspirit Charger which is on sale in 2016. The floor standing unit generates 3 kW of electrical and 15 kW of thermal energy.\n\nWhisperGen, a New Zealand firm with offices in Christchurch, has developed an \"AC Micro Combined Heat and Power\" Stirling cycle engine. These microCHP units are gas-fired central heating boilers which sell unused power back into the electricity grid. WhisperGen announced in 2004 that they were producing 80,000 units for the residential market in the United Kingdom. A 20 unit trial in Germany was conducted in 2006.\n\nPlaced at the focus of a parabolic mirror, a Stirling engine can convert solar energy to electricity with an efficiency better than non-concentrated photovoltaic cells, and comparable to concentrated photovoltaics. On August 11, 2005, Southern California Edison announced an agreement with Stirling Energy Systems (SES) to purchase electricity created using over 30,000 Solar Powered Stirling Engines over a twenty-year period sufficient to generate 850 MW of electricity. These systems, on an 8,000 acre (19 km) solar farm will use mirrors to direct and concentrate sunlight onto the engines which will in turn drive generators. \"In January, 2010, four months after breaking ground, Stirling Energy partner company Tessara Solar completed the 1.5 MW Maricopa Solar power plant in Peoria, Arizona, just outside Phoenix. The power plant is composed of 60 SES SunCatchers.\" The SunCatcher is described as \"a large, tracking, concentrating solar power (CSP) dish collector that generates 25 kilowatts (kW) of electricity in full sun. Each of the 38-foot-diameter collectors contains over 300 curved mirrors (heliostats) that focus sunlight onto a power conversion unit, which contains the Stirling engine. The dish uses dual-axis tracking to follow the sun precisely as it moves across the sky.\" There have been disputes over the project due to concerns of environmental impact on animals living on the site. The Maricopa Solar Plant has been closed.\n\nThere is a potential for nuclear-powered Stirling engines in electric power generation plants. Replacing the steam turbines of nuclear power plants with Stirling engines might simplify the plant, yield greater efficiency, and reduce the radioactive byproducts. A number of breeder reactor designs use liquid sodium as coolant. If the heat is to be employed in a steam plant, a water/sodium heat exchanger is required, which raises some concern as sodium reacts violently with water. A Stirling engine eliminates the need for water anywhere in the cycle. This would have advantages for nuclear installations in dry regions.\n\nUnited States government labs have developed a modern Stirling engine design known as the Stirling Radioisotope Generator for use in space exploration. It is designed to generate electricity for deep space probes on missions lasting decades. The engine uses a single displacer to reduce moving parts and uses high energy acoustics to transfer energy. The heat source is a dry solid nuclear fuel slug, and the heat sink is radiation into free space itself.\n\nIf supplied with mechanical power, a Stirling engine can function in reverse as a heat pump for heating or cooling. In the late 1930s, the Philips Corporation of the Netherlands successfully utilized the Stirling cycle in cryogenic applications. Experiments have been performed using wind power driving a Stirling cycle heat pump for domestic heating and air conditioning.\n\nAny Stirling engine will also work \"in reverse\" as a heat pump: when mechanical energy is applied to the shaft, a temperature difference appears between the reservoirs. The essential mechanical components of a Stirling cryocooler are identical to a Stirling engine. In both the engine and the heat pump, heat flows from the expansion space to the compression space; however, input work is required in order for heat to flow \"uphill\" against a thermal gradient, specifically when the compression space is hotter than the expansion space. The external side of the expansion-space heat exchanger may be placed inside a thermally insulated compartment such as a vacuum flask. Heat is in effect pumped out of this compartment, through the working gas of the cryocooler and into the compression space. The compression space will be above ambient temperature, and so heat will flow out into the environment.\n\nOne of their modern uses is in cryogenics and, to a lesser extent, refrigeration. At typical refrigeration temperatures, Stirling coolers are generally not economically competitive with the less expensive mainstream Rankine cooling systems, because they are less energy-efficient. However, below about −40...−30 °C, Rankine cooling is not effective because there are no suitable refrigerants with boiling points this low. Stirling cryocoolers are able to \"lift\" heat down to −200 °C (73 K), which is sufficient to liquefy air (specifically the primary constituent gases oxygen, nitrogen and argon). They can go as low as 40–60 K for single-stage machines, depending on the particular design. Two-stage Stirling cryocoolers can reach temperatures of 20 K, sufficient to liquify hydrogen and neon. Cryocoolers for this purpose are more or less competitive with other cryocooler technologies. The coefficient of performance at cryogenic temperatures is typically 0.04–0.05 (corresponding to a 4–5% efficiency). Empirically, the devices show a linear trend, typically with the , where \"T\" is the cryogenic temperature. At these temperatures, solid materials have lower specific heat values, so the regenerator must be made from unexpected materials, such as cotton.\n\nThe first Stirling-cycle cryocooler was developed at Philips in the 1950s and commercialized in such places as liquid air production plants. The Philips Cryogenics business evolved until it was split off in 1990 to form the Stirling Cryogenics BV, The Netherlands. This company is still active in the development and manufacturing of Stirling cryocoolers and cryogenic cooling systems.\n\nA wide variety of smaller Stirling cryocoolers are commercially available for tasks such as the cooling of electronic sensors and sometimes microprocessors. For this application, Stirling cryocoolers are the highest-performance technology available, due to their ability to lift heat efficiently at very low temperatures. They are silent, vibration-free, can be scaled down to small sizes, and have very high reliability and low maintenance. As of 2009, cryocoolers were considered to be the only widely deployed commercially successful Stirling devices.\n\nA Stirling heat pump is very similar to a Stirling cryocooler, the main difference being that it usually operates at room temperature. At present, its principal application is to pump heat from the outside of a building to the inside, thus heating it at lowered energy costs.\n\nAs with any other Stirling device, heat flow is from the expansion space to the compression space. However, in contrast to the Stirling \"engine\", the expansion space is at a \"lower\" temperature than the compression space, so instead of producing work, an \"input\" of mechanical work is required by the system (in order to satisfy the Second Law of Thermodynamics). The mechanical energy input can be supplied by an electrical motor, or an internal combustion engine, for example. When the mechanical work for the heat pump is provided by a second Stirling engine, then the overall system is called a \"heat-driven heatpump\".\n\nThe expansion side of the heat pump is thermally coupled to the heat source, which is often the external environment. The compression side of the Stirling device is placed in the environment to be heated, for example a building, and heat is \"pumped\" into it. Typically there will be thermal insulation between the two sides so there will be a temperature rise inside the insulated space.\n\nHeat pumps are by far the most energy-efficient types of heating systems, since they \"harvest\" additional heat from the environment, rather than turning all their input energy directly into heat. In accordance with the Second Law of Thermodynamics, heat pumps always require the \"additional input\" of some external energy to \"pump\" the collected heat \"uphill\" against a temperature differential.\n\nCompared to conventional heat pumps, Stirling heat pumps often have a higher coefficient of performance . To date, Stirling systems have seen limited commercial use; however, use is expected to increase along with market demand for energy conservation, and adoption will likely be accelerated by technological refinements.\n\nThe Free Piston Stirling Cooler (FPSC) is a completely sealed heat transfer system that has only two moving parts (a piston and a displacer), and which can use helium as the working fluid. The piston is typically driven by an oscillating magnetic field that is the source of the power needed to drive the refrigeration cycle. The magnetic drive allows the piston to be driven without requiring any seals, gaskets, O-rings, or other compromises to the hermetically sealed system. Claimed advantages for the system include improved efficiency and cooling capacity, lighter weight, smaller size and better controllability.\n\nThe FPSC was invented in 1964 by William Beale (1928-2016), a professor of Mechanical Engineering at Ohio University in Athens, Ohio. He founded Sunpower Inc., which researches and develops FPSC systems for military, aerospace, industrial, and commercial applications. A FPSC cooler made by Sunpower was used by NASA to cool instrumentation in satellites.\n\nOther suppliers of FPSC technology include the Twinbird Company of Japan and Global Cooling NV of the Netherlands, which (like Sunpower) has a research center in Athens, Ohio.\n\nFor several years starting around 2004, the Coleman Company sold a version of the Twinbird \"SC-C925 Portable Freezer Cooler 25L\" under its own brand name, but it has since discontinued offering the product. The portable cooler can be operated more than a day, maintaining sub-freezing temperatures while powered by an automotive battery. This cooler is still being manufactured, with Global Cooling now coordinating distribution to North America and Europe. Other variants offered by Twinbird include a portable deep freezer (to −80 °C), collapsible coolers, and a model for transporting blood and vaccine.\n\nA low temperature difference (LTD, or Low Delta T) Stirling engine will run on any low temperature differential, for example the difference between the palm of a hand and room temperature, or room temperature and an ice cube. A record of only 0.5 °C temperature differential was achieved in 1990. Usually they are designed in a gamma configuration for simplicity, and without a regenerator, although some have slits in the displacer typically made of foam for partial regeneration. They are typically unpressurized, running at pressure close to 1 atmosphere. The power produced is less than 1 W, and they are intended for demonstration purposes only. They are sold as toys and educational models.\n\nHowever, larger (typically 1 m square) low temperature engines have been built for pumping water using direct sunlight with minimal or no magnification.\n\nLos Alamos National Laboratory has developed an \"Acoustic Stirling Heat Engine\" with no moving parts. It converts heat into intense acoustic power which (quoted from given source) \"can be used directly in acoustic refrigerators or pulse-tube refrigerators to provide heat-driven refrigeration with no moving parts, or ... to generate electricity via a linear alternator or other electro-acoustic power transducer\".\n\nWhisperGen, (bankruptcy 2012) a New Zealand-based company has developed Stirling engines that can be powered by natural gas or diesel. An agreement has been signed with Mondragon Corporación Cooperativa, a Spanish firm, to produce WhisperGen's microCHP (Combined Heat and Power) and make them available for the domestic market in Europe. Some time ago E.ON UK announced a similar initiative for the UK. Domestic Stirling engines would supply the client with hot water, space heating and a surplus electric power that could be fed back into the electric grid.\n\nBased on the companies' published performance specifications, the off-grid diesel fueled unit produces combined heat (5.5 kW heat) and electric (800W electric) output, from a unit being fed 0.75 liters of automotive grade diesel fuel per hour. Whispergen units are claimed to operate as a combined co-generation unit reaching as high as ~80% operating efficiency.\n\nHowever the preliminary results of an Energy Saving Trust review of the performance of the WhisperGen microCHP units suggested that their advantages were marginal at best in most homes. However another author shows that Stirling engine microgeneration is the most cost effective of various microgeneration technologies in terms of reducing CO.\n\nMSI (Taiwan) developed a miniature Stirling engine cooling system for personal computer chips that uses the waste heat from the chip to drive a fan.\n\nIn all thermal power plants there has to be an exhaust of waste heat. However, there's no reason that the waste heat cannot be diverted to run stirling engines to pump seawater through reverse osmosis assemblies except that any additional use of the heat raises the effective heat sink temperature for the thermal power plant resulting in some loss of energy conversion efficiency. In a typical nuclear power plant, two-thirds of the thermal energy produced by the reactor is waste heat. In a stirling assembly the waste heat has potential to be used as an additional source of electricity.\n"}
{"id": "27709071", "url": "https://en.wikipedia.org/wiki?curid=27709071", "title": "Australite", "text": "Australite\n\nAustralites are tektites found in Australia. They are mostly dark or black, and have shapes including discs and bowls that are not seen in other tektites. NASA used the shape of \"flanged button\" australites in designing re-entry modules for the Apollo program in the 1960s.\n\nIndigenous Australians termed australites \"ooga\" (\"staring eyes\"), and they were used as sacred objects or as cutting tools. Europeans found out about australites in 1857, when explorer Thomas Mitchell gave naturalist Charles Darwin a mysteriously shaped piece of natural black glass. Darwin thought that australites were of volcanic origin due to their similarity to obsidian, volcanic glass. Later, australites were called \"blackfellows' buttons\" and \"obsidian bombs\".\n\nOne of the first scientists to seriously study australites was Charles Fenner, who saw his first australite in 1907. He believed that australites were glass meteorites.\n\nEarly theories about the source of australites included volcanoes, the bushfires that are common in Australia, or fusion of sand by lightning (fulgurites). Some scientists believed them to be meteorites, possibly lunar meteorites ejected from the Moon in impacts (now disproved due to the different composition of lunar rocks).\n\nAlthough different theories about the origin of australites are still circulating, most scientists believe that australites formed during a large asteroid or comet impact on the Earth. The impact ejected myriads of small rocks right out of the atmosphere. The australites acquired their streamlined, aerodynamic forms when they re-entered the Earth's atmosphere while molten and travelling at high velocities.\n\nMost australites are found in Southern Australia, below 25 degrees latitude. Based on similar ages and compositions, they represent the southern edge of the vast Australasian tektite strewnfield that stretches from southern China to Australia. The Australasian strewnfield has an age of 0.8 million years and is the result of an impact in Indochina.\n\nThe primary forms of australites are sphere, oval, boat, dumbbell and teardrop. Australites are smaller than other tektites and different in shape. Their initial velocity was higher than other tektites: enough to propel them just out of the Earth's atmosphere, so they then re-entered the atmosphere and underwent a rare secondary melting.\n\nOne of the most famous australite shapes is the \"flanged button\". Flanged buttons are rare, but the most unusual and rarest australites are discs, bowls, plates and other small forms (mini tektites). They are very thin. George Baker called them \"flying flanges\", the result of distortion of \"initially small primary forms by aerodynamic frictional heating\". He wrote: Well preserved shapes are found near Port Campbell in western Victoria.\n\nChapman and Larson experimented on the ablation of australites. At first they seemed to prove their extraterrestrial origin, but later in repeated studies they claimed that the australites could not have come from outside the Earth-Moon system. They were able to reproduce the shapes, flanges and the peculiar network of the rings on the front face in great detail.\n\n"}
{"id": "42705189", "url": "https://en.wikipedia.org/wiki?curid=42705189", "title": "BTTC Centre", "text": "BTTC Centre\n\nBTTC Centre is a Class A 12-story green building located at Ortigas Avenue corner Roosevelt Avenue, Greenhills, San Juan, Metro Manila, Philippines. It is the first green building in Greenhills, San Juan to receive a Gold pre-certification for Core & Shell under LEED. Developed by Hantex Corporation, it is an office, commercial and retail type of property with a floor plate of 1,384 square meters per floor. BTTC Centre is also among the 58 projects currently registered for LEED certification, together with the Zuellig Building in Makati and Megaworld 8 Campus Building and Wells Fargo Headquarters, which are both in Bonifacio Global City. It is also an IT-Center PEZA Certified Building.\n\nBTTC Centre is managed by real estate services firm CBRE Philippines. Completed in December 2012, this building has maintenance systems and recycles water with its built-in sewerage treatment plant, installed with double-glazed glass to provide abundant entry of light and also cut approximately around 70% of the heat from outside,keeping the building cool at all times. \nThe BTTC Centre was designed by ADGo Architecture and Design, Inc. headed by its principal architect, Architect Daniel Go. \n\n\n"}
{"id": "49191413", "url": "https://en.wikipedia.org/wiki?curid=49191413", "title": "Barium stannate", "text": "Barium stannate\n\nBarium stannate is an oxide of barium and tin with the chemical formula BaSnO. It is a wide band gap semiconductor with a perovskite crystal structure.\n"}
{"id": "1067640", "url": "https://en.wikipedia.org/wiki?curid=1067640", "title": "Barrier pipe", "text": "Barrier pipe\n\nA barrier pipe is a type of plastic water pipe used in domestic heating systems. The pipe is manufactured with a barrier that prevents oxygen from penetrating the material and entering the water system, reducing the risk of corrosion.\nThe 'barrier' is usually a resin material bonded between the outer and inner layer of the pipe itself. The pipe been either a cross linked polyethylene or polybutylene.\n"}
{"id": "2211120", "url": "https://en.wikipedia.org/wiki?curid=2211120", "title": "Beryllium oxide", "text": "Beryllium oxide\n\nBeryllium oxide (BeO), also known as beryllia, is an inorganic compound with the formula BeO. This colourless solid is a notable electrical insulator with a higher thermal conductivity than any other non-metal except diamond, and exceeds that of most metals. As an amorphous solid, beryllium oxide is white. Its high melting point leads to its use as a refractory material. It occurs in nature as the mineral bromellite. Historically and in materials science, beryllium oxide was called glucina or glucinium oxide. Formation of BeO from beryllium and oxygen releases the highest energy per mass of reactants for any chemical reaction, close to 24 MJ/kg.\n\nBeryllium oxide can be prepared by calcining (roasting) beryllium carbonate, dehydrating beryllium hydroxide, or igniting metallic beryllium:\nIgniting beryllium in air gives a mixture of BeO and the nitride BeN. Unlike the oxides formed by the other group 2 elements (alkaline earth metals), beryllium oxide is amphoteric rather than basic.\n\nBeryllium oxide formed at high temperatures (>800 °C) is inert, but dissolves easily in hot aqueous ammonium bifluoride (NHHF) or a solution of hot concentrated sulfuric acid (HSO) and ammonium sulfate ((NH)SO).\n\nBeO crystallizes in the hexagonal wurtzite structure, featuring tetrahedral Be and O centres, like lonsdaleite and w-BN (with both of which it is isoelectronic). In contrast, the oxides of the larger group-2 metals, i.e., MgO, CaO, SrO, BaO, crystallize in the cubic rock salt motif with octahedral geometry about the dications and dianions. At high temperature the structure transforms to a tetragonal form.\n\nIn the vapour phase, beryllium oxide is present as discrete diatomic molecules. In the language of valence bond theory, these molecules can be described as adopting sp orbital hybridisation on both atoms, featuring one σ (between one sp orbital on each atom) and one π bond (between aligned p orbitals on each atom oriented perpendicular to the molecular axis). Molecular orbital theory provides a slightly different picture with no net sigma bonding (because the 2s orbitals of the two atoms combine to form a filled sigma bonding orbital and a filled sigma* anti-bonding orbital) and two pi bonds formed between both pairs of p orbitals oriented perpendicular to the molecular axis. The sigma orbital formed by the p orbitals aligned along the molecular axis is unfilled. The corresponding ground state is ...(2sσ)(2sσ*)(2pπ) (as in the isoelectronic C molecule), where both bonds can be considered as dative bonds from oxygen towards beryllium.\n\nHigh-quality crystals may be grown hydrothermally, or otherwise by the Verneuil method. For the most part, beryllium oxide is produced as a white amorphous powder, sintered into larger shapes. Impurities, like carbon, can give a variety of colours to the otherwise colourless host crystals.\n\nSintered beryllium oxide is a very stable ceramic. Beryllium oxide is used in rocket engines and as a transparent protective over-coating on aluminised telescope mirrors.\n\nBeryllium oxide is used in many high-performance semiconductor parts for applications such as radio equipment because it has good thermal conductivity while also being a good electrical insulator. It is used as a filler in some thermal interface materials such as thermal grease. Some power semiconductor devices have used beryllium oxide ceramic between the silicon chip and the metal mounting base of the package to achieve a lower value of thermal resistance than a similar construction of aluminium oxide. It is also used as a structural ceramic for high-performance microwave devices, vacuum tubes, magnetrons, and gas lasers. BeO has been proposed as a neutron moderator for naval marine high-temperature gas-cooled reactors (MGCR), as well as NASA's Kilopower nuclear reactor for space applications.\n\nBeO is carcinogenic in powdered form and may cause chronic beryllium disease. Once fired into solid form, it is safe to handle if not subjected to machining that generates dust. Beryllium oxide ceramic is not a hazardous waste under federal law in the USA.\n\n"}
{"id": "3332449", "url": "https://en.wikipedia.org/wiki?curid=3332449", "title": "Black Gate (capacitor)", "text": "Black Gate (capacitor)\n\nBlack Gate is the name of a brand of audio grade electrolytic capacitor made in Japan. They have acquired a reputation for very high quality for use in the signal path, and power supplies, of audio circuitry.\n\nThe quality of capacitors may vary considerably depending on their construction and the type of insulating materials used. They are also known to deteriorate (or \"drift\") over time, just like any electrolytic capacitor.\n\nThe Black Gate capacitors are considered by some (Audio Note among others) to be the best electrolytic capacitor ever made although they are now discontinued. They do however, continue to be held in high regard by many high-end Hi-Fi manufacturers; \nDue to dwindling stock and demand, prices have skyrocketed lately. Many of their capacitors on the market are now fakes or just not available.\n\nBlack Gate capacitors base their manufacture on something called 'The Transcendent Electron Transfer' theory. The manufacturer attributes the capacitor's sonic quality to the use of fine graphite particles used in the separator between its anode and cathode. While the manufacturer may claim that graphite(graphite is a conductor not insulator) is used as the dielectric in its capacitors, the truth is the carbon impregnated paper inside the Black Gate capacitors was the cathode, the aluminum foil was the anode and the alumina (aluminum oxide) layer that formed on the surface of the aluminum anode was the dielectric (separator) between the two plates. Since aluminum oxide has the characteristics well suited for its dielectric duty to insulate the cathode from the anode whilst permitting a relatively high flux environment as the oxide layer formed could be very thin allowing the plates to be very close to each other, thus increasing capacitance.\n\nMany audiophiles believe it can take many hours before the maximum sonic benefits are heard in audio circuits that use Black Gates. This long settling-in procedure is often a controversial issue when auditioning such equipment, as the frequency response is said to tend to shift around greatly during this period, making the equipment sound different from one audition to another. Once completely 'burnt-in' however, the benefits are said to be heard clearly. This settling period or burn in period was most likely attributed to the aluminum layer completing its reaction to form a complete and stable oxide layer on its surface once current and voltage are applied to the capacitor in a circuit.\n\nThe Black Gate production has stopped in 2006, said to be caused by problems between Jelmax Co., Ltd. and Rubycon Corp., after 18 years of availability. The capacitors were manufactured by Rubycon under license for Jelmax Co., Ltd. which should close its doors in August 2007, once all their stocks get sold out.\n\nOnce the capacitors will become unavailable, or the current stocks become too old to be used due to deterioration, alternatives will have to be found. The Black Gate was very affordable when comparing it to some other similar-performing capacitors, and was easily available.\n\nHigh-end audio\n\nDIY audio\n"}
{"id": "4466718", "url": "https://en.wikipedia.org/wiki?curid=4466718", "title": "Brighton tornado", "text": "Brighton tornado\n\nThe Brighton tornado is the strongest storm recorded in Melbourne to date.\n\nOn the afternoon of 2 February 1918, with prevailing north-westerly winds and hot sultry weather (typical conditions for Melbourne thunderstorms). After a severe storm formed and moved off Port Phillip, two tornadoes struck Brighton beach simultaneously at approximately 5:45 pm and proceeded inland, converging near the junction of Halifax and Church Streets. Five minutes later, a third tornado struck. The tornadoes then tracked east over open fields.\n\nDamage retrospectively rated F3 on the Fujita scale was observed in places. Two people were killed a man and a boy, while the drowning of a woman at St Kilda beach is believed to be related to the same storm cell. Over 6 were injured in the Brighton area.\n\nThe tornado completely destroyed the Hawthorn Road Methodist church, which was later rebuilt. Numerous homes were demolished. The tornado badly damaged the Brighton Baths, tore the roof off Royal Terminus Hotel and destroyed the verandah of Grimley's Hotel. Extensive damage was incurred to infrastructure on the Sandringham railway line. Several community and sporting facilities were destroyed including the cricket club grandstand and a bandstand. It also damaged the burial monument of Adam Lindsay Gordon in the Brighton general cemetery.\n\n"}
{"id": "33758326", "url": "https://en.wikipedia.org/wiki?curid=33758326", "title": "Coal in Europe", "text": "Coal in Europe\n\nCoal production in Europe is falling, and imports exceed production. There is, however, growing controversy in Europe over the use of coal, as many denounce it for reasons such as health risks and links to global warming.\n\nInternational Energy Agency reports data for EU28 countries since 1990. According to IEA, EU28 countries use of coal as fuel has went from 5,289 TWh in 1990 to 3057 TWh in 2015, a reduction of 42%. During the same period, coal use in the world increased by 73%. EU28 use of coal:\n\n\nCoal includes anthracite, bituminous coal, lignite, and peat. Coal from fields differ in ash and moisture content, energy value, volatile elements, sulphur content, and other properties. Anthracite and bituminous coal are relatively high value compared to lignite and peat, which have lower energy and higher moisture contents. Coal is often used in the iron and steel industry, or to produce energy.\n\nRussia (365 Mt), Germany (176 Mt) and Poland (131 Mt) are the largest producers of coal in Europe as of 2016. Largest net importer was Germany with 53 Mt, and the largest net exporter was Russia with 147 Mt. Largest electricity production from coal in 2016 were in Germany (284 TWh), Russia (159 TWh) and Poland (133 TWh).\n\nIn 2009 the new electricity capacity in the European Union was 25 GW: 10.2 GW from wind power, 6.6 GW from natural gas, 4.2 GW from photovoltage solar power, and 2.4 GW from coal power.\nThe total amount of public support for electricity production from coal is around €10 billion in 2012. Since 1970 this has accumulated to between €100 and 200 billion. When disregarding subsidies and externalities, coal has the lowest average cost in EU. Coal has the highest external cost.\n\nCoal, as the largest artificial contributor to carbon dioxide emissions, has been attacked for its detrimental effects on health. Coal has been linked to acid rain, smog pollution, respiratory diseases, mining accidents, reduced agricultural yields and climate change. Proponents of coal downplay these claims and instead advocate the low cost of using coal for energy. Many European countries, such as Italy, have turned to coal as natural gas and oil prices rose.\n\nCoal technology has also advanced over the years, and emissions of soot and gases released in the burning of coal have been greatly reduced. New coal pollution mitigation technology, which often refers to carbon capture and storage, is a new and still-developing technology that seeks to capture carbon dioxide from power plants, and prevent it from entering the atmosphere by storing it. Proponents of this approach argue that it can effectively eliminate coal's contributions to climate change, while opponents doubt whether it can be done on a large scale.\n\nThe Dutch Research Institute CE Delft estimates that the worldwide \"external costs\", or hidden costs, of coal in 2007 were €360 billion, excluding the costs of accidents, mining damages, and any loss of cultural heritage or human rights violations that occur as a result of coal production. According to IEA the coal based emissions in 1971–2008 were 303,262 Mt worldwide, 58,210 Mt (19.2%) in OECD Europe, and 5,086 Mt (1.7%) in non-OECD Europe. Europe here excludes European Russia and all the ex-Soviet states. The estimated external costs of coal carbon emissions in 2007 were €69 billion in OECD Europe and €6 billion in non-OECD Europe.\n\nThe coal mining industry also has occupational hazards. In the Komi Republic, Russia, at the centre of the mining industry, occupational diseases are five time more prevalent than in the rest of the Russian Federation. Accidents are also known to happen in coal mines, caused by the liberation of methane from mining.\n\nAnnual coal carbon emissions (2005–2008 average) were highest per capita in Europe in Czech Republic 7.4, Kazakhstan 6.9, Poland 5.5, Finland 4.8, Serbia 4.5 and Germany 4.1.\n\n\n"}
{"id": "27327048", "url": "https://en.wikipedia.org/wiki?curid=27327048", "title": "Comisión Nacional de Energía (Spain)", "text": "Comisión Nacional de Energía (Spain)\n\nThe Comisión Nacional de la Energía (in short CNE and in English National Energy Commission) of Spain is the regulator of energy systems, created by Law 34/1998 of October 7, the hydrocarbons sector, and developed by Royal Decree 1339/1999 of 31 July, which adopted its regulation.\n\nIts objectives are to ensure effective competition in energy systems and the objectivity and transparency of its operations for the benefit of all individuals who operate these systems and consumers. For this purpose, energy systems means the market power and Markets hydrocarbon s both liquid and gaseous (natural gas, oil...).\n\nThe CNE was closed in 2013 along with the Comisión del Mercado de las Telecomunicaciones (CMT), the Comisión Nacional de la Competencia (CNC) and the Comisión Nacional del Sector Postal (CNSP). These bodies were unified in the Comisión Nacional de los Mercados y de la Competencia (CNMC) in October 2013.\n\nThe National Energy Commission is governed by a Board of Directors, composed of the President, Vice-President, six Directors and a Secretary, with voice but no vote. The President, Vice President and Directors are appointed by the Government of Spain, by Royal Decree on the proposal of Minister of Industry, for a term of six years, which could not be removed and may not be nominated by more than 2 terms (max 12).\n\nThe Council's current composition is:\n\nDirectors:\nMaria del Carmen Fernandez Rozado\n\nThe president of the CNE, Maria Teresa Costa Campi, was secretary of Industry and Energy of the Generalitat of Catalonia, as well as deputy for Barcelona in the last parliament by the PSC-PSOE.\n\nPast presidents of the CNE were Pedro Meroño (PP) and Miguel Angel Fernandez Ordonez (PSOE), and former Chairman was Vicente Lopez Ibor Mayor.\n\n\n"}
{"id": "3127858", "url": "https://en.wikipedia.org/wiki?curid=3127858", "title": "Critical ionization velocity", "text": "Critical ionization velocity\n\nCritical ionization velocity (CIV), or critical velocity (CV), is the relative velocity between a neutral gas and plasma (an ionized gas), at which the neutral gas will start to ionize. If more energy is supplied, the velocity of the atoms or molecules will not exceed the critical ionization velocity until the gas becomes almost fully ionized.\n\nThe phenomenon was predicted by Swedish engineer and plasma scientist, Hannes Alfvén, in connection with his model on the origin of the Solar System (1942). At the time, no known mechanism was available to explain the phenomenon, but the theory was subsequently demonstrated in the laboratory. Subsequent research by Brenning and Axnäs (1988) have suggested that a \"lower hybrid\" plasma instability is involved in transferring energy from the larger ions to electrons so that they have sufficient energy to ionize. Application of the theory to astronomy through a number of experiments have produced mixed results.\n\nThe Royal Institute of Technology in Stockholm carried out the first laboratory tests, and found that (a) the relative velocity between a plasma and neutral gas could be increased to the critical velocity, but then additional energy put into the system went into ionizing the neutral gas, rather than into\nincreasing the relative velocity, (b) the critical velocity is roughly independent of the pressure and magnetic field.\n\nIn 1973, Lars Danielsson published a review of critical ionization velocity, and concluded that the existence of the phenomenon \"is proved by sufficient experimental evidence\". In 1976, Alfvén reported that \"The first observation of the critical velocity effect under cosmic conditions was reported by Manka et al. (1972) from the Moon. When an abandoned lunar [391] excursion module was made to impact on the dark side of the Moon not very far from the terminator, a gas cloud was produced which when it had expanded so that it was hit by the solar wind gave rise to superthermal electrons.\"\n\nIn the laboratory, critical ionization velocity has been recognised for some time, and is seen in the penumbra produced by a dense plasma focus device (or plasma gun). Its existence in cosmic plasmas has not been confirmed.\n\nIn 1986, Gerhard Haerendel, suggested that critical velocity ionization may stabilize the plasma flow in a cometary coma. In 1992, E. Golbraikh and M. Filippov argued that critical ionization velocity could play a role in coronal mass ejections and solar flares, and in 1992, Anthony Peratt and Gerrit Verschuur suggested that interstellar neutral hydrogen emissions bore the signature of critical velocity ionization.\n\nA 2001 review of the phenomenon by Shu T. Lai reports that \".. laboratory experiments, and computer simulations have all shown CIV as feasible and reasonably understood, although all CIV experiments in space have yielded negative results with perhaps three exceptions\".\n\nAlso in 2001, C. Konz, et al., \".. discuss the critical velocity effect as a possible explanation for the observed Hα emission [..] in the Galactic halo near the edges of cold gas clouds of the Magellanic Stream\"\n\nThe CIV phenomenon has been also demonstrated in different Low Temperature Plasma (LTP) laboratory experiments where a plasma and a neutral gas are in relative motion across a magnetic field such as magnetrons. Simulations involving plasma accelerated in a gas similar to the cross-field LTP experiments shows rotating instabilities moving at a velocity close to the critical ionization velocity\n\nMathematically, the critical ionization velocity of a neutral cloud, that is, when the cloud begins to become ionized, is when the relative kinetic energy is equal to the ionization energy, that is:\n\nwhere \"eV\" is the ionization potential of the atoms or molecules in the gas cloud, \"m\" is the mass, \"v\" is the velocity. The phenomenon is also called the \"Critical velocity ionization\", and also \"Critical velocity effect\".\n\nAlfvén considered a neutral gas cloud entering the Solar System, and noted that a neutral atom will fall towards the Sun under the influence of gravity, and its kinetic energy will increase. If their motion is random, collisions will cause the gas temperature to rise, so that at a certain distance from the Sun, the gas will ionize. Alfvén writes that the ionization potential of the gas, V, occurs when:\n\nthat is, at a distance of:\n\n(where r is the ion distance from the Sun of mass \"M\", \"m\"' is the atom weight, \"V\" is in volts, \"k\" is the gravitational constant). Then when the gas becomes ionized, electromagnetic forces come into effect, of which the most important is the magnetic force which is usually greater than the gravitational force which gives rise to a magnetic repulsion from the Sun. In other words, a neutral gas falling from infinity toward the Sun is stopped at a distance r where it will accumulate, and perhaps condense into planets.\n\nAlfvén found that by taking a gas cloud with an average ionisation voltage of 12 V, and average atomic weight of 7, then the distance r is found to coincide with the orbit of Jupiter.\n\nThe critical ionization velocity of hydrogen 50.9 x 10 cm/s (50.9 km/s), and helium is 34.3 x 10 cm/s (34.3 km/s).\n\nAlfvén discusses his thoughts behind critical velocity, in his NASA publications Evolution of the Solar System. After criticising the \"Inadequacy of the Homogeneous Disc Theory\", he writes:\n\n\".. it is more attractive to turn to the alternative that the secondary bodies derive from matter falling in from \"infinity\" (a distance large compared to. the satellite orbit). This matter (after being stopped and given sufficient angular momentum) accumulates at specific distances from the central body. Such a process may take place when atoms or molecules in free fall reach a kinetic energy equal to their ionization energy. At this stage, the gas can become ionized by the process discussed in sec. 21.4; the ionized gas can then be stopped by the magnetic field of the central body and receive angular momentum by transfer from the central body as described in sec. 16.3.\".\n\n"}
{"id": "6060943", "url": "https://en.wikipedia.org/wiki?curid=6060943", "title": "Crude oil assay", "text": "Crude oil assay\n\nA crude oil assay is the chemical evaluation of crude oil feedstocks by petroleum testing laboratories. Each crude oil type has unique molecular and chemical characteristics. No two crude oil types are identical and there are crucial differences in crude oil quality. The results of crude oil assay testing provide extensive detailed hydrocarbon analysis data for refiners, oil traders and producers. Assay data help refineries determine if a crude oil feedstock is compatible for a particular petroleum refinery or if the crude oil could cause yield, quality, production, environmental and other problems.\n\nThe assay can be an inspection assay or comprehensive assay. Testing can include crude oil characterization of whole crude oils and the various boiling range fractions produced from physical or simulated distillation by various procedures. Information obtained from the petroleum assay is used for detailed refinery engineering and client marketing purposes. Feedstock assay data are an important tool in the refining process. \n\n"}
{"id": "53476992", "url": "https://en.wikipedia.org/wiki?curid=53476992", "title": "Curie–von Schweidler law", "text": "Curie–von Schweidler law\n\nThe Curie–von Schweidler law refers to the response of dielectric material to the step input of a direct current (DC) voltage first observed by Jacques Curie and Egon Ritter von Schweidler.\n\nAccording to this law, the current decays according to a power law:\n\nwhere formula_2 is the current at a given charging time, formula_3, and formula_4 is the decay constant such that formula_5. Given that the dielectric has a finite conductance, the equation for current measured through a dielectric under a DC electrical field is:\n\nwhere formula_7 is a constant of proportionality, formula_8 is the decay constant (i.e., formula_9), and formula_10 is the intrinsic conductance of the dielectric. This stands in contrast to the Debye formulation, which states that the current is proportional an exponential function with a time constant, formula_11, according to:\n\nThe Curie–von Schweidler behavior has been observed in many instances such as those shown by Andrzej Ka Johnscher and Jameson \"et al\". It has been interpreted as a many-body problem by Jonscher, but can also be formulated as an infinite number of resistor-capacitor circuits. This comes from the fact that the power law can be expressed as:\n\nwhere formula_14 is the Gamma function. Effectively, this relationship shows the power law expression to be composed of an infinite \"weighted sum\" of Debye responses.\n"}
{"id": "20906118", "url": "https://en.wikipedia.org/wiki?curid=20906118", "title": "Delaware Mountain Wind Farm", "text": "Delaware Mountain Wind Farm\n\nDelaware Mountain Wind Farm is a wind farm located in the Delaware Mountains of Culberson County, Texas. The farm consists of thirty-eight EWC Zond Z-48 750 kilowatt wind turbines that produce up to 28.5 megawatts of electricity. All electricity produced by the project is purchased by the Lower Colorado River Authority. The project was completed in 1999 by Orion Energy LLC and National Wind Power. NextEra Energy Resources now owns and operates the wind farm. As a result of damage to the wind farm and transmission infrastructure by an ice storm in November 2013, the Delaware Mountain Wind Farm and the adjacent Wind Power Partners 94 wind farm have been off-line. NextEra has informed the ERCOT, Electric Reliability Council of Texas, that it is dismantling the farms and \"... they will be decommissioned and retired on 7 August 2014.\"\n"}
{"id": "55042878", "url": "https://en.wikipedia.org/wiki?curid=55042878", "title": "Dunloe Ogham Stones", "text": "Dunloe Ogham Stones\n\nDunloe Ogham Stones (CIIC 197–203, 241) is a collection of ogham stones forming a National Monument located in County Kerry, Ireland.\n\nDunloe Ogham Stones are located 1 km south of Beaufort, to the south of the River Laune.\n\nThe stones were carved in the 5th and 6th centuries AD and served as burial markers. Seven were discovered in 1838 forming the ceiling of a souterrain near Dunloe Castle and were moved to their current site by 1945.\nAnother stone comes from the old church of Kilbonane.\n\nThe Kilbonane stone is in the centre (CIIC 241) and the others are arranged around it.\n\n"}
{"id": "23547123", "url": "https://en.wikipedia.org/wiki?curid=23547123", "title": "Dynamic vapor sorption", "text": "Dynamic vapor sorption\n\nDynamic vapor sorption (DVS) is a gravimetric technique that measures how quickly and how much of a solvent is absorbed by a sample: such as a dry powder absorbing water. It does this by varying the vapor concentration surrounding the sample and measuring the change in mass which this produces. Water vapor is most commonly used, but it is also possible to use a wide range of organic solvents.\n\nDr Daryl Williams, founder of Surface Measurement Systems Ltd, invented Dynamic Vapor Sorption in 1991 and the first instrument was delivered to Pfizer UK in 1992. DVS was originally developed to replace the time and labor-intensive desiccators and saturated salt solutions to measure water vapor sorption isotherms.\n\nThe main application of DVS is to measure water sorption isotherms. In general, a vapor sorption isotherm shows the equilibrium amount of vapor sorbed as a function of steady state relative vapor pressure at a constant temperature. For water sorption isotherms, water relative vapor pressure is more commonly expressed as relative humidity. In a DVS experiment this is accomplished by exposing a sample to a series of step changes in relative humidity and monitoring the mass change as a function of time. The sample mass must be allowed to reach gravimetric equilibrium at each step change in humidity before progressing to the next humidity level. Then, the equilibrium mass values at each relative humidity step are used to generate the isotherm. Isotherms are typically divided into two components: \"sorption\" for increasing humidity steps and \"desorption\" for decreasing humidity steps. Sorption can be further divided into \"adsorption\" (sorbate located on the surface) and \"absorption\" (sorbate penetrates the bulk).\n\nFigure 1 shows a typical water sorption result from a DVS experiment for a microcrystalline cellulose sample. The kinetic data (Figure 1a) shows the change in mass and humidity as a function of time. From the kinetic results, the rate of water uptake and water diffusion coefficients can be determined. The equilibrium mass values at the end of each humidity step were used to calculate the sorption and desorption isotherms (Figure 1b). The difference in water vapor uptake between the sorption and desorption isotherms is called the hysteresis. The shape and location of the isotherm hysteresis can elucidate information about the sorption mechanism and sample porosity. Although an isotherm experiment is the most common use of a DVS instrument, humidity (or other vapor) ramping experiments can be performed to investigate vapor-induced phase changes. These changes include: glassy to rubbery transitions, amorphous to crystalline conversions, and sample deliquescence.\n\nDVS measurement has applications over a wide range of industries. Both equilibrium vapor sorption isotherms and vapor sorption kinetic results can yield vital information for materials ranging from pharmaceuticals to fuel cells. Although water sorption experiments are most common, the use of organic vapor in DVS experiments can reveal additional sample properties. The below sections highlight how DVS experiments are utilized in several industries.\n\nThe moisture sorption properties of pharmaceutical materials such as excipients, drug formulations and packaging films are recognized as critical factors in determining their storage, stability, processing and application performance. Further, vapor sorption experiments can be used to study hydrate and solvate formation. Gravimetric vapor sorption experiments are one of the most sensitive methods for determining amorphous contents, which may have a detrimental impact on the stability, manufacturability and dissolution characteristics of the formulated drug product.\n\nThe moisture sorption properties of food products are recognized as critical factors in determining their storage, stability, processing and application performance. DVS is also used to measure moisture and flavor diffusion properties for packaging and barrier applications. Further, moisture sorption plays critical roles in the storage and performance of agricultural products like pesticides, herbicides, fertilizers, and seeds.\n\nDVS experiments have been widely used in the study of personal care materials. For instance, the moisturization of hair samples with different chemical (i.e. conditioning, coloring, and bleaching) and mechanical (i.e. perming, combing, and blow-drying) treatments. The hydration behavior of skin samples has also been studied by DVS. Other moisture sorption applications related to the personal care industry include the dehydration of contact lenses and superabsorbent polymers.\n\nIn particular to building materials, moisture sorption has significant implications for cements, woods, insulation materials, and fibers. Moisture damage is a significant factor limiting a building’s lifespan. As well, moisture infusion through a building’s outer structure can have a significant effect on indoor air quality and air-conditioning load.\n\nA critical parameter affecting the performance of proton exchange membranes is the water content. Water is typically supplied to the fuel cell by humidifying the gas feed stream. The level of hydration within the proton exchange membrane is vital to its performance: if the hydration level is too low, the polymers exhibit greatly reduced ionic conductivity; if hydration level is too high, excess water can flood the pores in the gas diffusion layer and impede mass transport within the electrode structure. For these reasons, DVS has been used to study the water sorption and transport properties of proton exchange membranes.\n\n"}
{"id": "28279388", "url": "https://en.wikipedia.org/wiki?curid=28279388", "title": "Eastham Oil Terminal", "text": "Eastham Oil Terminal\n\nEastham Oil Terminal is situated close to the small town of Eastham on the Wirral Peninsula, beside the Manchester Ship Canal. It was built during the 1950s close to the Queen Elizabeth II Dock and stores oil refined at Stanlow Refinery, to which it is connected by pipeline. The site is currently operated by Nynas.\n\n"}
{"id": "52234502", "url": "https://en.wikipedia.org/wiki?curid=52234502", "title": "Electra (company)", "text": "Electra (company)\n\nElectra is the main electricity and water company in Cape Verde. It was founded as a public company on April 17, 1982 (under decree-law no. 37/1982) by the merger of Electricidade e Água do Mindelo (EAM - Mindelo Electricity and Water), Central Eléctrica da Praia (CEP - Praia Central Electricity) and Electricidade e Água do Sal (EAS - Sal Electricity and Water). In 1998 it was converted into a public limited company. In December 1999 the majority of the stocks was sold to Portuguese companies: 30.6% to Electricidade de Portugal and 20.4% to Águas de Portugal. Since 2013, the company consists of three entities: \n\nElectra serves all islands of Cape Verde except Boa Vista, where electricity and water are produced and distributed by the public-private company \"Águas e Energia de Boavista\".\n\n\n"}
{"id": "54666767", "url": "https://en.wikipedia.org/wiki?curid=54666767", "title": "Electricité Du Liban", "text": "Electricité Du Liban\n\nElectricité du Liban (Electricity of Lebanon – EDL) is the main Lebanese electricity producer. It was founded by Decree No. 16878 dated July 10, 1964, and mandated the responsibility of the generation, transmission, and distribution of electrical energy in Lebanon.\n"}
{"id": "17655525", "url": "https://en.wikipedia.org/wiki?curid=17655525", "title": "Energy efficient mortgage", "text": "Energy efficient mortgage\n\nAn energy efficient mortgage (EEM) (or \"green mortgage\") is a loan product that allows borrowers to reduce their utility bill costs by allowing them to finance the cost of incorporating energy-efficient features into a new housing purchase or the refinancing of existing housing. \n\nBorrowers who qualify for an EEM need to complete a home inspection by an energy rater working off qualification standards created by the U.S. Department of Energy. The results of this energy audit can then be used when applying for an EEM.\n\nFirst introduced in 1980, EEMs are currently sponsored by all mortgage programs insured by the U.S. federal government. To date, the popularity of the product has been somewhat limited: \"The New York Times\" estimates less than 1% of all U.S. home loans are EEMs.\n\n"}
{"id": "9478", "url": "https://en.wikipedia.org/wiki?curid=9478", "title": "Erbium", "text": "Erbium\n\nErbium is a chemical element with symbol Er and atomic number 68. A silvery-white solid metal when artificially isolated, natural erbium is always found in chemical combination with other elements. It is a lanthanide, a rare earth element, originally found in the gadolinite mine in Ytterby in Sweden, from which it got its name.\n\nErbium's principal uses involve its pink-colored Er ions, which have optical fluorescent properties particularly useful in certain laser applications. Erbium-doped glasses or crystals can be used as optical amplification media, where Er ions are optically pumped at around 980 or and then radiate light at in stimulated emission. This process results in an unusually mechanically simple laser optical amplifier for signals transmitted by fiber optics. The wavelength is especially important for optical communications because standard single mode optical fibers have minimal loss at this particular wavelength.\n\nIn addition to optical fiber amplifier-lasers, a large variety of medical applications (i.e. dermatology, dentistry) rely on the erbium ion's emission (see ) when lit at another wavelength, which is highly absorbed in water in tissues, making its effect very superficial. Such shallow tissue deposition of laser energy is helpful in laser surgery, and for the efficient production of steam which produces enamel ablation by common types of dental laser.\n\nA trivalent element, pure erbium metal is malleable (or easily shaped), soft yet stable in air, and does not oxidize as quickly as some other rare-earth metals. Its salts are rose-colored, and the element has characteristic sharp absorption spectra bands in visible light, ultraviolet, and near infrared. Otherwise it looks much like the other rare earths. Its sesquioxide is called erbia. Erbium's properties are to a degree dictated by the kind and amount of impurities present. Erbium does not play any known biological role, but is thought to be able to stimulate metabolism.\n\nErbium is ferromagnetic below 19 K, antiferromagnetic between 19 and 80 K and paramagnetic above 80 K.\n\nErbium can form propeller-shaped atomic clusters ErN, where the distance between the erbium atoms is 0.35 nm. Those clusters can be isolated by encapsulating them into fullerene molecules, as confirmed by transmission electron microscopy.\n\nErbium metal tarnishes slowly in air and burns readily to form erbium(III) oxide:\n\nErbium is quite electropositive and reacts slowly with cold water and quite quickly with hot water to form erbium hydroxide:\n\nErbium metal reacts with all the halogens:\n\nErbium dissolves readily in dilute sulfuric acid to form solutions containing hydrated Er(III) ions, which exist as rose red [Er(OH)] hydration complexes:\n\nNaturally occurring erbium is composed of 6 stable isotopes, , , , , , and with being the most abundant (33.503% natural abundance). 29 radioisotopes have been characterized, with the most stable being with a half-life of , with a half-life of , with a half-life of , with a half-life of , and with a half-life of . All of the remaining radioactive isotopes have half-lives that are less than , and the majority of these have half-lives that are less than 4 minutes. This element also has 13 meta states, with the most stable being with a half-life of .\n\nThe isotopes of erbium range in atomic weight from () to (). The primary decay mode before the most abundant stable isotope, , is electron capture, and the primary mode after is beta decay. The primary decay products before are element 67 (holmium) isotopes, and the primary products after are element 69 (thulium) isotopes.\n\nErbium (for Ytterby, a village in Sweden) was discovered by Carl Gustaf Mosander in 1843. Mosander was working with a sample of what was thought to be the single metal oxide yttria, derived from the mineral gadolinite. He discovered that the sample contained at least two metal oxides in addition to pure yttria, which he named \"erbia\" and \"terbia\" after the village of Ytterby where the gadolinite had been found. Mosander was not certain of the purity of the oxides and later tests confirmed his uncertainty. Not only did the \"yttria\" contain yttrium, erbium, and terbium; in the ensuing years, chemists, geologists and spectroscopists discovered five additional elements: ytterbium, scandium, thulium, holmium, and gadolinium. Erbia and terbia, however, were confused at this time. A spectroscopist mistakenly switched the names of the two elements during spectroscopy. After 1860, terbia was renamed erbia and after 1877 what had been known as erbia was renamed terbia. Fairly pure ErO was independently isolated in 1905 by Georges Urbain and Charles James. Reasonably pure erbium metal was not produced until 1934 when Wilhelm Klemm and Heinrich Bommer reduced the anhydrous chloride with potassium vapor. It was only in the 1990s that the price for Chinese-derived erbium oxide became low enough for erbium to be considered for use as a colorant in art glass.\n\nThe concentration of erbium in the Earth crust is about 2.8 mg/kg and in the sea water 0.9 ng/L. This concentration is enough to make erbium about 45th in elemental abundance in the Earth's crust.\n\nLike other rare earths, this element is never found as a free element in nature but is found bound in monazite sand ores. It has historically been very difficult and expensive to separate rare earths from each other in their ores but ion-exchange chromatography methods developed in the late 20th century have greatly brought down the cost of production of all rare-earth metals and their chemical compounds.\n\nThe principal commercial sources of erbium are from the minerals xenotime and euxenite, and most recently, the ion adsorption clays of southern China; in consequence, China has now become the principal global supplier of this element. In the high-yttrium versions of these ore concentrates, yttrium is about two-thirds of the total by weight, and erbia is about 4–5%. When the concentrate is dissolved in acid, the erbia liberates enough erbium ion to impart a distinct and characteristic pink color to the solution. This color behavior is similar to what Mosander and the other early workers in the lanthanides would have seen in their extracts from the gadolinite minerals of Ytterby.\n\nCrushed minerals are attacked by hydrochloric or sulfuric acid that transforms insoluble rare-earth oxides into soluble chlorides or sulfates. The acidic filtrates are partially neutralized with caustic soda (sodium hydroxide) to pH 3–4. Thorium precipitates out of solution as hydroxide and is removed. After that the solution is treated with ammonium oxalate to convert rare earths into their insoluble oxalates. The oxalates are converted to oxides by annealing. The oxides are dissolved in nitric acid that excludes one of the main components, cerium, whose oxide is insoluble in HNO. The solution is treated with magnesium nitrate to produce a crystallized mixture of double salts of rare-earth metals. The salts are separated by ion exchange. In this process, rare-earth ions are sorbed onto suitable ion-exchange resin by exchange with hydrogen, ammonium or cupric ions present in the resin. The rare earth ions are then selectively washed out by suitable complexing agent. Erbium metal is obtained from its oxide or salts by heating with calcium at under argon atmosphere.\n\nErbium's everyday uses are varied. It is commonly used as a photographic filter, and because of its resilience it is useful as a metallurgical additive. Other uses:\n\nErbium does not have a biological role, but erbium salts can stimulate metabolism. Humans consume 1 milligram of erbium a year on average. The highest concentration of erbium in humans is in the bones, but there is also erbium in the human kidneys and liver.\n\nErbium is slightly toxic if ingested, but erbium compounds are not toxic. Metallic erbium in dust form presents a fire and explosion hazard.\n\n\n"}
{"id": "8980050", "url": "https://en.wikipedia.org/wiki?curid=8980050", "title": "Fouling community", "text": "Fouling community\n\nFouling communities are communities of organisms found on artificial surfaces like the sides of docks, marinas, harbors, and boats. Settlement panels made from a variety of substances have been used to monitor settlement patterns and to examine several community processes (e.g., succession, recruitment, predation, competition, and invasion resistance). These communities are characterized by the presence of a variety of sessile organisms including ascidians, bryozoans, mussels, tube building polychaetes, sea anemones, sponges, barnacles, and more. Common predators on and around fouling communities include small crabs, starfish, fish, limpets, chitons, other gastropods, and a variety of worms.\n\nFouling communities follow a distinct succession pattern in a natural environment.\n\nFouling communities are a part of a healthy aquatic system.\n\nHelps test the ecological effectiveness of artificial coral reefs.\n\nFouling communities can have a negative economic impact on humans, such as by damaging the bottom of boats.\n\nIt can, when attached to the bottoms of boats, bring invasive species to locations where there use to be none. \n\nFouling communities were highlighted particularly in the literature of marine ecology as a potential example of alternate stable states through the work of John Sutherland in the 1970s at Duke University, although this was later called into question by Connell and Sousa.\n\n\n"}
{"id": "18983908", "url": "https://en.wikipedia.org/wiki?curid=18983908", "title": "Friends of Coal", "text": "Friends of Coal\n\nFriends of Coal is an advocacy group that functions in several states and works closely with state coal trade organizations.\n\nIt was founded in West Virginia by the coal industry as a countermeasure to grassroots environmental justice movements during the summer of 2002 over debates about legislation concerning weight limits on West Virginia highways for trucks carrying coal. It subsequently broadened its efforts to improve the image of the coal industry and to link the coal industry to the economic and social self-identity of people who live near coal mines; as part of the latter effort, it sponsors local events like car shows and sports events.\n\n"}
{"id": "27190", "url": "https://en.wikipedia.org/wiki?curid=27190", "title": "Geography of Saint Helena", "text": "Geography of Saint Helena\n\nSaint Helena is an island in the South Atlantic Ocean, about midway between South America and Africa. St Helena has a land area of 122 square kilometres and is part of the territory of Saint Helena, Ascension and Tristan da Cunha which includes Ascension Island and the island group of Tristan da Cunha.\n\nThe climate of Saint Helena island can be described as tropical, marine and mild, tempered by the Benguela Current and trade winds which blow almost continuously. Similarly, the climate of Tristan da Cunha is marine, mild and also tempered by trade winds, although the climate is temperate in nature. Ascension Island is warmer and wetter than St Helena.\n\nSaint Helena has a rugged, volcanic terrain, with small scattered plateaus and plains, with the largest area of level ground on the island being Prosperous Bay Plain in the eastern arid area. The other islands of the group have a volcanic origin. The highest point on the island is Diana's Peak at 818 metres (2,684 ft), though Queen Mary's Peak on Tristan da Cunha is the highest in the British territory at 2,062 m.\n\nA natural hazard on Tristan da Cunha is active volcanism, though this is not the case on St Helena itself.\n\nSt. Helena exists because of the St. Helena hotspot which began to produce basaltic lava about 145 million years ago when it was near the constructive plate margin of the Mid-Atlantic Ridge. The movement of the African Plate away from the hotspot has left the chain of the St. Helena seamounts, which may connect with the Cameroon Volcanic Line. St. Helena, the most south westerly point on the chain, is close to the plate margin, but the last volcanic eruptions occurred about 7 million years ago.\n\nThe first investigations of St. Helena geology were made when Charles Darwin visited the island on the voyage of H.M.S. Beagle in July 1836. He \"used the observations on St Helena to formulate an intermediate hypothesis (published in 1844 in his book \"Geological Observations on the volcanic islands and parts of South America visited during the Voyage of H.M.S. \"Beagle\") - volcanoes rise by slow, gradual and episodic events\".\n\nThe Barn is a capping of younger lavas upon weaker rocks. The Barn features cliffs on the side that faces the sea. It overlooks the pyroclasts and weak flows of Turk's Cap Valley to the south.\n\nTaking their names from the story of Lot in the Book of Genesis, \"Lot\" and \"Lot's Wife\" are two solitary pillars of rock topping two valleys near Sandy Bay. The pillars are phonolitic intrusions, which are more resistant to erosion than surrounding volcanic features which have, in time, eroded away.\n\nSaint Helena possesses fish as a main natural resource. Land use in the island group is divided between arable land (with 12.9% of the area given to this) and other uses, which occupy the remaining 87.1%.\n\nIn terms of maritime claims, St Helena has an exclusive fishing zone of 200 nautical miles, and a territorial sea of twelve nautical miles.\n\nJust off the coast of St Helena island itself are numerous small islands. Starting from the north and running clockwise, these are: Shore Island, George Island, Rough Rock Island, Flat Rock, The Buoys, Sandy Bay Island, Black Horse Island, White Bird Island, Frightus Island, Jar Rock, Castle Point Rock, Robert Rock, Salt Rock, Speery Island, Flat Rock, The Needle, Lower and Upper Black Rock, Bird Island, Black Rock, Thompson's Valley Island, Peaked Island, Egg Island, Lady's Chair, Lighter Rock, Long Ledge, and Red Rock.\n\nSaint Helena harbours at least 40 species of plants unknown anywhere else in the world, and Ascension is a breeding ground for sea turtles and sooty terns.\n\nThe island of St Helena is divided into 8 districts, which are used for electoral and administrative purposes.\n\n"}
{"id": "2572362", "url": "https://en.wikipedia.org/wiki?curid=2572362", "title": "Georges Claude", "text": "Georges Claude\n\nGeorges Claude (24 September 187023 May 1960) was a French engineer and inventor. He is noted for his early work on the industrial liquefaction of air, for the invention and commercialization of neon lighting, and for a large experiment on generating energy by pumping cold seawater up from the depths. He has been considered by some to be \"the Edison of France\". Claude was an active collaborator with the German occupiers of France during the Second World War, for which he was imprisoned in 1945 and stripped of his honors.\n\nGeorges Claude studied at the École supérieure de physique et de chimie industrielles de la ville de Paris (ESPCI). He then held several positions. He was an electrical inspector in a cable factory and the laboratory manager in an electric works. He founded and edited a magazine, \"L'Étincelle Électrique\" (\"The Electric Spark\"); his important friendship with Jacques-Arsène d'Arsonval apparently dates from this time. About 1896, Claude learned of the explosion risk for bottled acetylene, which was used at the time for lighting. Acetylene is explosive when stored under pressure. Claude showed that acetylene dissolved well in acetone, equivalent to storing it under 25 atmospheres of pressure, reduced the risk in handling the gas.\n\nIn 1902 Claude devised what is now known as the Claude system for liquifying air. The system enabled the production of industrial quantities of liquid nitrogen, oxygen, and argon; Claude's approach competed successfully with the earlier system of Carl von Linde (1895). Claude and businessman Paul Delorme founded \"L'Air Liquide, S.A.\" (Air Liquide), which is presently a large multinational corporation headquartered in Paris, France.\n\nInspired by Geissler tubes and by Daniel McFarlan Moore's invention of a nitrogen-based light (the \"Moore tube\"), Claude developed neon tube lighting to exploit the neon that was produced as a byproduct of his air liquefaction business. These were all \"glow discharge\" tubes that generate light when an electric current is passed through the rarefied gas within the tube. Claude's first public demonstration of a large neon light was at the Paris Motor Show (\"Salon de l'Automobile et du Cycle\"), 3–18 December 1910. Claude's first patent filing for his technologies in France was on 7 March 1910. Claude himself wrote in 1913 that, in addition to a source of neon gas, there were two principal inventions that made neon lighting practicable. First were his methods for purifying the neon (or other inert gases such as argon). Claude developed techniques for purifying the inert gases within a completely sealed glass tube, which distinguished neon tube lighting from the Moore tubes; the latter had a device for replenishing the nitrogen or carbon dioxide gases within the tube. The second invention was ultimately crucial for the development of the Claude lighting business; it was a design for minimizing the degradation (by \"sputtering\") of the electrodes that transfer electric current from the external power supply to the glowing gases within the sign.\n\nThe terms \"neon light\" and \"neon sign\" are now often applied to electrical lighting incorporating sealed glass tubes filled with argon, mercury vapor, or other gases instead of neon. In 1915 a U.S. patent was issued to Claude covering the design of the electrodes for neon lights; this patent became the strongest basis for the monopoly held in the U.S. by his company, Claude Neon Lights, through the early 1930s.\n\nIn 1923, Georges Claude and his French company Claude Neon, introduced neon gas signs to the United States, by selling two to Earle C. Anthony, the owner of Packard car dealerships in San Francisco and Los Angeles. Photos from 1923 of the one in San Francisco show the neon Packard sign. Neon lighting quickly became a popular fixture in outdoor advertising. Visible even in daylight, people would stop and stare at the first neon signs for hours, dubbed \"liquid fire.\"\n\nClaude's mentor and friend was Jacques-Arsène d'Arsonval, the inventor of the \"Ocean Thermal Energy Conversion\" (OTEC) concept. Claude was also the first person to build prototype plants of that technology. Claude built his plant in Cuba in 1930. The system produced 22 kilowatts of electricity with a low-pressure turbine.\n\nIn 1935, Claude constructed another plant, this time aboard a 10,000-ton cargo vessel moored off the coast of Brazil. Weather and waves destroyed both plants before they could become net power generators. (Net power is the amount of power generated after subtracting power needed to run the system.)\n\nEven as a young engineer, Claude was unsympathetic to democratic rule. In 1933 he joined the \"Action française\", which favored restoration of a monarchy in France. He was a close friend of the monarchist leader Charles Maurras. Following the 1940 defeat of France by Germany at the beginning of the Second World War, the subsequent German occupation of northern France and establishment of the Vichy regime in the south, Claude publicly supported French collaboration with Germany. Among his other activities, he published several tracts supporting collaboration. He was a member of a Distinguished Committee of the \"Groupe Collaboration\", which had been founded in September, 1940. He was nominated by the Vichy regime as a member of the \"\" in 1941.\n\nFollowing the Allied liberation of France in 1944, Claude was taken into custody on 2 December 1944 because of his collaboration with the Axis Powers. He was removed from the French Academy of Sciences. In 1945 he was tried and convicted of propaganda work favoring collaboration, but was cleared of another charge that he helped design the V-1 flying bomb. He was condemned to life imprisonment, and was imprisoned. In 1950 he was released from prison, with acknowledgment of his research on ocean thermal energy conversion.\n\nClaude wrote several semi-popular descriptions of his research, in addition to his wartime tracts and a memoir.\n\n"}
{"id": "1418996", "url": "https://en.wikipedia.org/wiki?curid=1418996", "title": "Grog (clay)", "text": "Grog (clay)\n\nGrog, also known as firesand and chamotte, is a ceramic raw material. It has high percentage of silica and alumina. It can be produced by firing selected fire clays to high temperature before grinding and screening to specific particle sizes. It can also be produced from pitchers. The particle size distribution is generally coarser in size than the other raw materials used to prepare clay bodies. It tends to be porous and have low density. \n\nIt is normally available as a powder or chippings, and is an important ingredient in Coade stone.\n\nGrog is composed of: 40% minimum alumina, 30% minimum silica, 4% maximum Iron(III) oxide, 2% maximum of calcium oxide and magnesium oxide combined.\n\nIts melting point is approximately . Its boiling point is over . Its water absorption is 7% maximum. Its thermal expansion coefficient is 5.2 mm/m and thermal conductivity is 0.8 W/(m·K) at 100 °C and 1.0 W/(m·K) at 1000 °C. It is also not easily wetted by steel.\n\nGrog is used in pottery and sculpture to add a gritty, rustic texture called \"tooth\"; it also reduces shrinkage and aids even drying. This prevents defects such as cracking, crow feet patterning, and lamination. The coarse particles open the green clay body to allow gases to escape. It also adds structural strength to hand-built and thrown pottery during shaping although it can diminish fired strength.\n\nThe finer the grog particles are, the closer the clay bond, and the denser and stronger the resulting fired product. \"The strength in the dry state increases with grog down as fine as that passing the 100-mesh sieve, but decreases with material passing the 200-mesh sieve.\" \n\nIn archaeological terminology, 'grog' is crushed fired pottery of any type that is added as a temper to unfired clay. Several pottery types from the European Bronze Age are typologised on the basis of their grog inclusions.\n\n\n"}
{"id": "2306605", "url": "https://en.wikipedia.org/wiki?curid=2306605", "title": "Index of solar energy articles", "text": "Index of solar energy articles\n\nThis is a list of solar energy topics.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "2091630", "url": "https://en.wikipedia.org/wiki?curid=2091630", "title": "Iron(II) sulfide", "text": "Iron(II) sulfide\n\nIron(II) sulfide or ferrous sulfide (Br.E. sulphide) is one of a family chemical compounds and minerals with the approximate formula . Iron sulfides are often iron-deficient non-stoichiometric. All are black, water-insoluble solids.\n\nFeS can be obtained by the heating of iron and sulfur:\n\nFeS adopts the nickel arsenide structure, featuring octahedral Fe centers and trigonal prismatic sulfide sites.\n\nIron sulfide reacts with hydrochloric acid, releasing hydrogen sulfide\n\nIn moist air, iron sulfides oxidize to hydrated ferrous sulfate.\n\nIron sulfides occur widely in nature in the form of iron–sulfur proteins.\n\nAs organic matter decays under low-oxygen (or hypoxic) conditions such as in swamps or dead zones of lakes and oceans, sulfate-reducing bacteria reduce various sulfates present in the water, producing hydrogen sulfide. Some of the hydrogen sulfide will react with metal ions in the water or solid to produce iron or metal sulfides, which are not water-soluble. These metal sulfides, such as iron(II) sulfide, are often black or brown, leading to the color of sludge.\n\nPyrrhotite is a waste product of the \"Desulfovibrio\" bacteria, a sulfate reducing bacteria.\n\nWhen eggs are cooked for a long time, the yolk's surface may turn green. This color change is due to iron(II) sulfide, which forms as iron from the yolk reacts with hydrogen sulfide released from the egg white by the heat. This reaction occurs more rapidly in older eggs as the whites are more alkaline.\n\nThe presence of ferrous sulfide as a visible black precipitate in the growth medium peptone iron agar can be used to distinguish between microorganisms that produce the cysteine metabolizing enzyme cysteine desulfhydrase and those that do not. Peptone iron agar contains the amino acid cysteine and a chemical indicator, ferric citrate. The degradation of cysteine releases hydrogen sulfide gas that reacts with the ferric citrate to produce ferrous sulfide.\n\n"}
{"id": "19752987", "url": "https://en.wikipedia.org/wiki?curid=19752987", "title": "Kaimai hydro power scheme", "text": "Kaimai hydro power scheme\n\nThe Kaimai hydro power scheme is a hydroelectric power scheme on the Wairoa River in the Bay of Plenty in New Zealand. The scheme is operated by TrustPower. It has four operational power stations, and formerly the McLaren Falls Station which was decommissioned in 1989 following the commissioning of the Ruahihi Station.\n\n\n"}
{"id": "28794596", "url": "https://en.wikipedia.org/wiki?curid=28794596", "title": "Lead titanate", "text": "Lead titanate\n\nLead(II) titanate is an inorganic compound with the chemical formula PbTiO. It is the lead salt of titanic acid. Lead(II) titanate is a yellow powder that is insoluble in water.\n\nAt high temperatures, lead titanate adopts a cubic perovskite structure. At 760 K, the material undergoes a second order phase transition to a tetragonal perovskite structure which exhibits ferroelectricity. Lead titanate is one of the end members of the lead zirconate titanate (Pb<nowiki>[</nowiki><nowiki>]</nowiki> 0≤\"x\"≤1, PZT) system, which is technologically one of the most important ferroelectric and piezoelectric ceramics; PbTiO has a high ratio of k33 to kp with a high kt.\n\nLead titanate occurs in nature as mineral macedonite.\n\nLead titanate is toxic, like other lead compounds. It irritates skin, mucous membranes and eyes. It may also cause harm to unborn babies and might have effects on fertility.\n\nThe solubility of hydrothermally-synthesized perovskite-phase PbTiO in water was experimentally determined at 25 and 80 °C to depend on pH and vary from 4.9x10 mol/kg at pH≈3, to 1.9x10 mol/kg at pH≈7.7, to \"undetectable\" (<3.2x10 mol/kg) in the range 10<pH<11. At still higher pH values, the solubility increased again. The solubility was apparently incongruent and was quantified as the analytical concentration of Pb.\n"}
{"id": "35937890", "url": "https://en.wikipedia.org/wiki?curid=35937890", "title": "Locally compact field", "text": "Locally compact field\n\nIn algebra, a locally compact field is a topological field whose topology forms a locally compact space (in particular, it is a Hausdorff space). Examples are discrete fields and local fields such as the field of complex numbers and the \"p\"-adic fields. Since one can always give discrete topology to a field, any field can be turned into a locally compact field.\n\n"}
{"id": "1340684", "url": "https://en.wikipedia.org/wiki?curid=1340684", "title": "Ludwig von Hagemeister", "text": "Ludwig von Hagemeister\n\nLudwig August von Hagemeister (; Leontij Andrianovic Gagemejster; 1780 Drostenhof, Governorate of Livonia, Russian Empire – December 24, 1833) was a Baltic German who held the rank of Captain of the 1st rank in the Imperial Russian Navy. He was a maritime explorer of Alaska and the Pacific Ocean, and served briefly in 1817-1818 as the second Chief Manager of the Russian-American Company.\n\nHagemeister was born to an ethnic Baltic German family in the Russian Empire. He began his service in the Russian naval forces in 1795 as a volunteer midshipman. Along with a group of fellow junior servicemen, he joined the British Royal Navy in 1802 on board the \"HMS Argus\".\n\nIn 1806 he was directed to take the \"Neva\" with provisions needed to Russian America; this was the first of three circumnavigations in his career to New Archangel. Departing on 20 October 1806 from Kronstadt, the \"Neva\" began to cross the Atlantic Ocean to the south, to go around the continent of Africa and sail East. After passing the Cape of Good Hope, Hagemeister and his subordinates were the first Russian crew to visit the Australian mainland, reaching the port of Port Jackson on 4 June.\n\nReaching New Archangel (now Sitka, Alaska) on 13 September, Hagemeister was stationed on Kodiak Island for most of 1808. The Russian forces were at war with the Tlingit. He was ordered by Chief Manager of the Russian American Company (RAC), Aleksandr Baranov, to secure needed provisions from the Kingdom of Hawaii on 30 November 1808 and perhaps find a suitable location for a Russian colony there. Hagemeister explored portions of the North Pacific and met with King Kamehameha I in early 1809. Amicable conversations were commenced with the monarch and Hagemeister purchased a cargo of salt. Delivering the salt to New Archangel, he later departed for Saint Petersburg in 1810 on the \"Neva\".\n\nHagemeister reached Petropavlovsk-Kamchatsky on 28 May and began the overland trek through Siberia. His circumnavigation was recognized by Russian authorities at St. Petersburg, as he was promoted to Captain lieutenant and awarded the Order of St. Vladimir 4th class. He served as chairman of the Admiralty of Irkutsk from 1812 to 1815, in which capacity Hagemeister was responsible for building the first ships to cross Lake Baikal.\n\nVasily Golovnin encouraged him to plan another journey to New Archangel, and worked for his appointment to an RAC ship. Besides delivering more needed provisions, Hagemeister was ordered to investigate Baranov's financial management of the company and deputed to replace him as Chief Manager if he felt it necessary. Baranov, then 70, had requested relief from his position for years in order to return to Russia. Commanding the \"Kutuzov\", Hagemeister started his second circumnavigation in 1816. Hagemeister arrived at New Archangel during 1817, with his second-in-command, Semyon Yanovsky.\n\nBefore examining Baranov's financial records, Hagemeister traveled to northern California to formalize relations with the Kashia Pomo people. He was the principal negotiator with leaders of the local Pomo band, and obtained the privilege to establish Fort Ross. The parties signed a treaty on 22 September 1817, with tribal officials reported to have said that \"They are very pleased to see Russians occupy this land, for they now live in safety from other Indians who used to attack them from time to time. This security began only from the time of Russian settlement.\" The agreement was made by RAC officials as a part of a scheme to take over Alta California from the Spanish Empire.\n\nReturning to New Archangel, Hagemeister and other naval officers spent the winter examining the financial records of the RAC. Sources disagree on what was found: Khlebnikov says that the records showed no evidence of malfeasance. Borneman says that they found a discrepancy in the amount of supplies listed in accounting records. On 11 January 1818 Baranov was, \"in a most arrogant way\", removed by Hagemeister from office.\n\nHagemeister took over, changing the methods of payment to the \"promyslenniki\" workers, by giving currency for salaries and abandoning the share system in favor of provisions. He worked to improve relations with the Hawaiian Kingdom, as they had been damaged from the Schäffer affair. Hagemeister outlawed trade by non-RAC Europeans, a policy later adopted by the Russian government. This policy was contained in the Ukase of 1821, a decree from the Imperial Government that extended Russian land claims to northern California and banned all non-Russian European merchants from the area.\n\nHagemeister appointed his direct second-in-command, Semyon Ivanovich Yanovsky, as the next Chief Manager. In late October 1818, Hagemeister and Baranov departed from New Archangel; the latter was pleased to be at last returning to Russia. But Baranov died in 1819, while they were still at sea, and was buried at sea. In 1820 Yanovsky was replaced by another naval officer, chosen directly by the Board of the RAC.\n\nIn 1828–1829, Hagemeister made his final circumnavigation on the ship \"Krotky\". During this journey, he surveyed the Menshikov Atoll (Kwajalein) in the Marshall Islands, plotting it on the map and specifying the location of some other islands. His diaries are composed in several European languages, including German, Russian, French, Portuguese, English and Spanish.\n\n\n"}
{"id": "578365", "url": "https://en.wikipedia.org/wiki?curid=578365", "title": "MIRACL", "text": "MIRACL\n\nMIRACL, or Mid-Infrared Advanced Chemical Laser, is a directed energy weapon developed by the US Navy. It is a deuterium fluoride laser, a type of chemical laser.\n\nThe MIRACL laser first became operational in 1980. It can produce over a megawatt of output for up to 70 seconds, making it the most powerful continuous wave (CW) laser in the US. Its original goal was to be able to track and destroy anti-ship cruise missiles, but in later years it was used to test phenomenologies associated with national anti-ballistic and anti-satellite laser weapons. Originally tested at a contractor facility in California, as of the later 1990s and early 2000s, it was located at a facility () in the White Sands Missile Range in New Mexico.\n\nThe beam size in the resonator is about 21 cm (8.3 in) high and 3 cm (1.2 in) wide. The beam is then reshaped to a 14 x 14 cm (5.5 in x 5.5 in) square.\n\nAmid much controversy in October 1997, MIRACL was tested against MSTI-3, a US Air Force satellite at the end of its original mission in orbit at a distance of 432 km (268 mi). MIRACL failed during the test and was damaged and the Pentagon claimed mixed results for other portions of the test. A second, lower-powered chemical laser was able to temporarily blind the MSTI-3 sensors during the test.\n\n"}
{"id": "1082916", "url": "https://en.wikipedia.org/wiki?curid=1082916", "title": "Magnetic helicity", "text": "Magnetic helicity\n\nThe helicity of a smooth vector field defined on a domain in 3D space is the standard measure of the extent to which the field lines wrap and coil around one another. As to magnetic helicity, this \"vector field\" is magnetic field. It is a generalization of the topological concept of linking number to the differential quantities required to describe the magnetic field. As with many quantities in electromagnetism, magnetic helicity (which describes magnetic field lines) is closely related to fluid mechanical helicity (which describes fluid flow lines).\n\nIf magnetic field lines follow the strands of a twisted rope, this configuration would have nonzero magnetic helicity; left-handed ropes would have negative values and right-handed ropes would have positive values.\n\nFormally,\n\nwhere \n\nMagnetic helicity has units of Wb (webers squared) in SI units and Mx (maxwells squared) in Gaussian Units.\n\nIt is a conserved quantity in ideal magnetohydrodynamics (zero resistivity), and still remains conserved in a good approximation even with a small but finite resistivity, in which case magnetic reconnection dissipates energy. The concept is useful in solar dynamics and in dynamo theory.\nMagnetic helicity is a gauge-dependent quantity, because formula_4 can be redefined by adding a gradient to it (gauge transformation). However, for perfectly conducting boundaries or periodic systems without a net magnetic flux, the magnetic helicity is gauge invariant. A gauge-invariant \"relative helicity\" has been defined for volumes with non-zero magnetic flux on their boundary surfaces. If the magnetic field is turbulent and weakly inhomogeneous a magnetic helicity \"density\" and its associated flux can be defined in terms of the density of field line linkages.\n\n\n"}
{"id": "13818545", "url": "https://en.wikipedia.org/wiki?curid=13818545", "title": "Maneuvering speed", "text": "Maneuvering speed\n\nIn aviation, the maneuvering speed of an aircraft is an airspeed limitation selected by the designer of the aircraft. At speeds close to, and faster than, the maneuvering speed, full deflection of any flight control surface should not be attempted because of the risk of damage to the aircraft structure.\n\nThe maneuvering speed of an aircraft is shown on a cockpit placard and in the aircraft's flight manual but is not commonly shown on the aircraft's airspeed indicator.\n\nIn the context of air combat maneuvering (ACM), the maneuvering speed is also known as corner speed or cornering speed.\n\nIt has been widely misunderstood that flight below maneuvering speed will provide total protection from structural failure. In response to the destruction of American Airlines Flight 587, a CFR Final Rule was issued clarifying that \"flying at or below the design maneuvering speed does not allow a pilot to make multiple large control inputs in one airplane axis or single full control inputs in more than one airplane axis at a time\". Such actions \"may result in structural failures at any speed, including below the maneuvering speed.\"\n\nV is the design maneuvering speed and is a calibrated airspeed. Maneuvering speed cannot be slower than formula_1 and need not be greater than V.\n\nIf formula_2 is chosen by the manufacturer to be exacly formula_1 the aircraft will stall in a nose-up pitching maneuver before the structure is subjected to its limiting aerodynamic load. However, if formula_2 is selected to be greater than formula_1, the structure will be subjected to loads which exceed the limiting load unless the pilot checks the maneuver.\n\nThe maneuvering speed or maximum operating maneuvering speed depicted on a cockpit placard is calculated for the maximum weight of the aircraft. Some Pilot's Operating Handbooks also present safe speeds for weights less than the maximum.\n\nThe formula used to calculate a safe speed for a lower weight is formula_6, where V is maneuvering speed (at maximum weight), W is actual weight, W is maximum weight.\n\nSome aircraft have a maximum operating maneuvering speed V. Note that this is a different concept than design maneuvering speed. The concept of maximum operating maneuvering speed was introduced to the US type-certification standards for light aircraft in 1993. The maximum operating maneuvering speed is selected by the aircraft designer and cannot be more than formula_1, where V is the stalling speed of the aircraft, and \"n\" is the maximal allowed positive load factor.\n\n\n"}
{"id": "47466", "url": "https://en.wikipedia.org/wiki?curid=47466", "title": "Mesopause", "text": "Mesopause\n\nThe mesopause is the temperature minimum at the boundary between the mesosphere and the thermosphere atmospheric regions. Due to the lack of solar heating and very strong radiative cooling from carbon dioxide, the mesosphere is the coldest region on Earth with temperatures as low as -100 °C (-148 °F or 173 K). The altitude of the mesopause for many years was assumed to be at around 85 km (53 mi.), but observations to higher altitudes and modeling studies in the last 10 years have shown that in fact the mesopause consists of two minima - one at about 85 km and a stronger minimum at about 100 km (62 mi).\n\nAnother feature is that the summer mesopause is cooler than the winter (sometimes referred to as the \"mesopause anomaly\"). It is due to a summer-to-winter circulation giving rise to upwelling at the summer pole and downwelling at the winter pole. Air rising will expand and cool resulting in a cold summer mesopause and conversely downwelling air results in compression and associated increase in temperature at the winter mesopause. In the mesosphere the summer-to-winter circulation is due to gravity wave dissipation, which deposits momentum against the mean east-west flow, resulting in a small north-south circulation.\n\nIn recent years the mesopause has also been the focus of studies on global climate change associated with increases in CO. Unlike the troposphere, where greenhouse gases result in the atmosphere heating up, increased CO in the mesosphere acts to cool the atmosphere due to increased radiative emission. This results in a measurable effect - the mesopause should become cooler with increased CO. Observations do show a decrease of temperature of the mesopause, though the magnitude of this decrease varies and is subject to further study. Modeling studies of this phenomenon have also been carried out.\n\n"}
{"id": "55048075", "url": "https://en.wikipedia.org/wiki?curid=55048075", "title": "Nature Ecology and Evolution", "text": "Nature Ecology and Evolution\n\nNature Ecology and Evolution is a monthly peer-reviewed scientific journal published by Nature Publishing Group covering all aspects of research on Ecology. It was established in 2017. Its first and current editor-in-chief is Patrick Goymer.\n"}
{"id": "11272657", "url": "https://en.wikipedia.org/wiki?curid=11272657", "title": "Nuclear program of Saudi Arabia", "text": "Nuclear program of Saudi Arabia\n\nSaudi Arabia is not known to have a nuclear weapons program. From an official and public standpoint, Saudi Arabia has been an opponent of nuclear weapons in the Middle East, having signed the Treaty on the Non-Proliferation of Nuclear Weapons, and is a member of the coalition of countries demanding a Nuclear-weapon-free zone in the Middle East. Studies of nuclear proliferation have not identified Saudi Arabia as a country of concern.\n\nHowever, over the years there have been media reports of Saudi Arabia's intent to purchase a nuclear weapon from an outside source. In 2003, a leaked strategy paper laid out three possible options for the Saudi government: to acquire a nuclear deterrent, to ally with and become protected by an existing nuclear nation, or to try to reach agreement on having a nuclear-free Middle East. UN officials and weapon specialists have suggested this review was prompted by a distancing of relations with the US, concerns over Iran's nuclear program, and the lack of international pressure on Israel to give up its nuclear weapons.\n\nIn May 2008, the United States and Saudi Arabia signed a memorandum of understanding, as part of the United States' vintage Atoms for Peace program, to boost Saudi efforts for a civilian nuclear program. This program did not involve support for development of nuclear weapons.\n\nHistorically, Pakistan and Saudi Arabia have had a cordial relationship. Pakistani political scientists and historians have noted that Saudi interest in nuclear technology began in the 1970s after Prime Minister Zulfikar Ali Bhutto convened a meeting of Pakistan's leading theoretical physicists (who went on to join the King Fahd University of Petroleum and Minerals) with the Saudi royal government during a visit by the Saudi royal family to Pakistan in 1974 as part of the 2nd OIC conference at Lahore. At this meeting, Bhutto noted the advances made in the Israeli and the Indian nuclear programmes, which he took as attempts to intimidate the Muslim world.\n\nIt is widely believed that Saudi Arabia has been a major financier of Pakistan's own integrated atomic bomb project since 1974, a programme founded by former prime minister Zulfikar Ali Bhutto. In the 1980s, Chief Martial Law Administrator and President General Zia-ul-Haq paid a state visit to Saudi Arabia where he unofficially told the King that: \"Our achievements are yours\". This cooperation was allegedly furthered by socialist prime minister Benazir Bhutto in 1995. In 1998, the conservative Prime minister Nawaz Sharif informed Saudi Arabia confidentially before ordering the nuclear tests (see \"Chagai-I\" and \"Chagai-II\") in the Weapon-testing labs-III (WTL) located in the Chagai remote site in Balochistan Province of Pakistan. In June 1998, the Prime Minister paid a farewell visit to King Fahd and publicly thanked the Saudi government for supporting the country after the tests. Shortly thereafter, Saudi Minister of Defense Prince Sultan went with Prime Minister Sharif on a tour of a classified institute, the Kahuta Research Laboratories (KRL), where leading scientist Abdul Qadeer Khan briefed the Prince and Prime Minister Sharif on nuclear physics and sensitive issues involving nuclear explosive devices.\n\nSince 1998, Western diplomats and intelligence agencies have long believed that an agreement exists in which Pakistan would sell Saudi Arabia nuclear warheads and its own nuclear technology should security in the Persian Gulf deteriorate. Both countries have sharply denied the existence of such an agreement. In 2003, globalsecurity.org reported that Pakistan and Saudi Arabia had entered a secret agreement on nuclear cooperation providing Saudi Arabia with nuclear weapons technology in return for access to cheap oil for Pakistan.\n\nIn March 2006, the German magazine \"Cicero\" reported that Saudi Arabia had, since 2003, received assistance from Pakistan to acquire nuclear missiles and warheads. Satellite photos allegedly reveal an underground city with nuclear silos containing Ghauri rockets in Al-Sulaiyil, south of the capital Riyadh. Pakistan has denied aiding Saudi Arabia in its nuclear ambitions.\n\nIsraeli-Saudi's nuclear collaboration\n\nWestern security agencies believe that Israel is selling nuclear information to Saudi Arabia, \"Arabi21\" has reported. According to Israeli writer Ami Dor-on, such information will give the government in Riyadh nuclear weapons capabilities. Writing on the \"News One\" website, Dor-on said that the joint intention is to make sure that Iran will not be the only country in the region that possesses such weapons which threaten the security and safety of the Kingdom.\n\nAccording to Dor-on, the Tel Aviv is aware that Saudis would eventually make the move for developing nuclear weapons and want to make sure they would not go to other regional players such as Pakistan to obtain the know-how.\n\nIn January 2012, Chinese Premier Wen Jiabao signed a mutual cooperation deal on nuclear energy with King Abdullah, during Premier Jiabao's visit to the Middle East. The details of such cooperation were not fully provided by the government-controlled Saudi Press Agency, but according to Hashim Yamani, president of the King Abdullah City for Atomic and Renewable Energy, the kingdom has planned 16 commercial nuclear power reactors by 2030.\n\nIn 1994, Mohammed al Khilewi, second-in-command of the Saudi mission to the United Nations, applied for asylum in the United States. He provided a packet of 10,000 documents that allegedly described long-time Saudi support of the Iraqi nuclear weapons program. According to these documents, during Saddam Hussein's regime in Iraq the Saudis supported the Iraqi nuclear program with $5 billion, on the condition that workable nuclear technology and possibly even nuclear weapons would be transferred to Saudi Arabia. Khilewi obtained asylum in the US, with the consent of Saudi Arabia. Khilewi's allegations have not been confirmed by any other source. US officials have stated that they have no evidence of Saudi assistance to Iraqi nuclear development. Saudi officials denied the allegations.\n\nSenior Clinton administration officials responsible for Mideast affairs at the time Khilewi sought asylum, including Robert Pelletreau of the State Department and Bruce Riedel of the National Security Council, said they found nothing in Khilewi's debriefings to back up the Media reports about a Saudi nuclear program. \"There was nothing there,\" Pelletreau said. (Vartan 2005)\n\nThe Arab States of the Persian Gulf plan to start their own joint civilian nuclear program. An agreement in the final days of the Bush administration provided for cooperation between the United Arab Emirates and the United States of America in which the United States would sell the UAE nuclear reactors and nuclear fuel. The UAE would, in return, renounce their right to enrich uranium for their civilian nuclear program. At the time of signing, this agreement was touted as a way to reduce risks of nuclear proliferation in the Persian Gulf. However, Mustafa Alani of the Dubai-based Gulf Research Center stated that, should the Nuclear Non-Proliferation Treaty collapse, nuclear reactors such as those slated to be sold to the UAE under this agreement could provide the UAE with a path toward a nuclear weapon, raising the specter of further nuclear proliferation. In March 2007, foreign ministers of the six-member Gulf Cooperation Council met in Saudi Arabia to discuss progress in plans agreed in December 2006, for a joint civilian nuclear program.\n\nIn 2011, Prince Turki al-Faisal, who has served as the Saudi intelligence chief and as ambassador to the United States has\nsuggested that the kingdom might consider producing nuclear weapons if it found itself between the atomic arsenals of Iran and Israel. In 2012, it was confirmed that Saudi Arabia would launch its own nuclear weapons program immediately if Iran successfully developed nuclear weapons. In such an eventuality, Saudi Arabia would start work on a new ballistic missile platform, purchase nuclear warheads from overseas and aim to source uranium to develop weapons-grade material.\n\nOfficials in the U.S. alliance believe Saudi Arabia and Pakistan have an understanding in which Islamabad would supply the kingdom with warheads if security in the Persian Gulf was threatened. A U.S. official told \"The Times\" that Riyadh could have the nuclear warheads in a matter of days of approaching Islamabad. Pakistan's ambassador to Saudi Arabia, Mohammed Naeem Khan, was quoted as saying that \"Pakistan considers the security of Saudi Arabia not just as a diplomatic or an internal matter but as a personal matter.\" Naeem also said that the Saudi leadership considered Pakistan and Saudi Arabia to be one country. Any threat to Saudi Arabia is also a threat to Pakistan. Other vendors were also likely to enter into a bidding war if Riyadh indicated that it was seeking nuclear warheads. Both Saudi Arabia and Pakistan have denied the existence of any such agreement. Western intelligence sources have told \"The Guardian\" that the Saudi monarchy has paid for up to 60% of the Pakistan's atomic bomb projects and in return has the option to buy five to six nuclear warheads off the shelf.\n\nIn November 2013, a variety of sources told BBC Newsnight that Saudi Arabia had invested in Pakistani nuclear weapons projects and believes it could obtain nuclear bombs at will. Earlier in the year, a senior NATO decision maker told Mark Urban, a senior diplomatic and defense editor, that he had seen intelligence reporting that nuclear weapons made in Pakistan on behalf of Saudi Arabia are now sitting ready for delivery. In October 2013, Amos Yadlin, a former head of Israeli military intelligence, told a conference in Sweden that if Iran got the bomb, \"the Saudis will not wait one month. They already paid for the bomb, they will go to Pakistan and bring what they need to bring.\" Since 2009, when King Abdullah of Saudi Arabia warned visiting US special envoy to the Middle East Dennis Ross that if Iran crossed the threshold, \"we will get nuclear weapons\", the kingdom has sent the Americans numerous signals of its intentions. Gary Samore, who until March 2013 was President Barack Obama's counter-proliferation adviser, told BBC Newsnight: \"I do think that the Saudis believe that they have some understanding with Pakistan that, in extremis, they would have claim to acquire nuclear weapons from Pakistan.\"\n\nAccording to the US based think-tank, the Center for Strategic and International Studies, the BBC report on possible nuclear sharing between Pakistan and Saudi Arabia is partially incorrect. There is no indication of the validity or credibility of the BBC’s sources, nor does the article expand on what essentially constitutes an unverified lead. It noted that if Pakistan were to transfer nuclear warheads onto Saudi soil, it is highly unlikely that either nation would face any international repercussions if both nations were to follow strict nuclear sharing guidelines similar to that of NATO. A research paper produced by the British House of Commons Defence Select Committee states that as long as current NATO nuclear sharing arrangements remain in place, NATO states would have few valid grounds for complaint if such a transfer were to occur.\n\nIn May 2015, in response to The Sunday Times of London report that the Saudis had \"taken the 'strategic decision' to acquire 'off-the-shelf' atomic weapons from Pakistan,\" amid growing fears of a nuclear-armed Iran, a Saudi defense official dismissed it as speculation.\n\nIn 1987, Saudi Arabia purchased Chinese-made CSS-2 intermediate-range ballistic missiles designed and used by the Chinese as a nuclear-armed missile, but reportedly sold to Saudi Arabia with conventional high-explosive warheads. However their low circular error probable accuracy (1–1.5 km) makes them unsuitable for effective military use against military targets when carrying a conventional warhead. The CSS-2 has a range of 4,850 km with a payload of either 2,150 or 2,500 kg. These missiles were delivered with between 50 and 35 transporter erector launcher trucks. These missiles were the first weapons of the Royal Saudi Strategic Missile Force, a separate branch of Saudi Arabia's armed forces. In 2013 the existence of the Royal Saudi Strategic Missile Force was officially announced.\n\n\"Newsweek\" quoted an anonymous source in 2014 that Saudi Arabia had acquired CSS-5 intermediate-range ballistic missiles from China in 2007 with \"Washington's quiet approval on the condition that CIA technical experts could verify they were not designed to carry nuclear warheads\". The Center for Strategic and International Studies lists the CSS-5 as being capable of carrying either 250-kiloton or 500-kiloton nuclear or various types of conventional high-explosive warheads. The CSS-5, while it has a comparatively shorter range (2,800 km) and half the payload (1 ton) of the CSS-2, is solid-fueled, thus can be set up and placed on alert status more easily than the liquid-fueled CSS-2, and its accuracy is much greater (circular error probable of 30 meters).\n\n\n"}
{"id": "29943863", "url": "https://en.wikipedia.org/wiki?curid=29943863", "title": "Peacekeeper Rail Garrison", "text": "Peacekeeper Rail Garrison\n\nThe Peacekeeper Rail Garrison is a mobile missile system that was developed by the United States Air Force during the 1980s as part of a plan to place fifty MGM-118A Peacekeeper intercontinental ballistic missiles on the rail network of the United States. The railcars were intended, in case of increased threat of nuclear war, to be deployed onto the nation's rail network to avoid being destroyed by a first strike counterforce attack by the Soviet Union. However the plan was cancelled as part of defense cutbacks following the end of the Cold War, and the Peacekeeper missiles were installed in silo launchers as LGM-118s instead.\n\nTrain-based ICBMs do offer some advantages over missiles in fixed silos, namely that the enemy can never be sure where they are, or more accurately, where all of them are at any given moment. But as a 2014 RAND study pointed out, rail and truck launchers have their drawbacks. Maintaining a missile on a train is more difficult than in a silo, while rail lines and roads can be blocked by snow, which tends to restrict railroad ICBMs to warmer climates. In addition, because there are only a limited number of rail lines and highways in an area, enemy surveillance can focus on a few areas. And, once located, mobile missiles are more vulnerable than ICBMs in hardened silos.\n\nOn December 19, 1986, the White House announced that U.S. President Ronald Reagan had given approval to a plan for the development of a railroad-based system for basing part of the planned LGM-118 Peacekeeper – originally referred to as \"M-X\" – intercontinental ballistic missile (ICBM) force. Intended to increase the survivability of the force in the event of a counterforce nuclear attack by the Soviet Union, the 50 train-based missile launchers, fitted two to each of twenty-five trains, would supplement a force of 50 silo-based missiles that would replace existing Minuteman missiles.\n\nEach train was planned to consist of two locomotives (appeared to be EMD GP40-2 in unclassified diagrams ), two cars for housing security forces (using a modified box car), two launchers each holding a single missile (using a modified box car), a launch control car (using a modified Westinghouse box car), a fuel car, and a maintenance car (using a modified box car). Each launching car would carry one missile in a tube that, upon the receipt of an authenticated firing command, would elevate to fire the missile from the bed of the car. The launch cars were long, and when loaded with a missile weighed over or . A crew of 42 people—including the train commander, four launch control officers, four railroad engineers, one medic, six maintenance personnel, and 26 security police—could live in the launch control and security cars for up to one month.\n\nFollowing testing in 1989 at Hudson, CO two ex-CSX locomotives, an EMD GP40-2 and an EMD GP38-2, were sent to Precision National in Mount Vernon, IL for modification to GP40-2DE (Dynamic braking, Extended range) with bulletproof glass in the cab windows. The trains were expected to be in service within about 2 years. Strategic Air Command wanted the first trains stationed at F. E. Warren Air Force Base near the Union Pacific mainline at Cheyenne, WY. The expected in-service date was December 1992. Rail Garrisons would resemble small freight yards with four spurs leading to slant-sided steel and earthen hardened shelters to house the trains.\n\nTwo hi-cube boxcars were constructed by St Louis Refrigerator Car Company. They were modified at Westinghouse, receiving bogie trucks, distributing their of weight over 8 axles. Low-slung beams were added to the underside to level the car against the track during launch. roof panels were designed to fall off when the missile was erected. The Air Force was seeking US$2.16 billion in fiscal year 1991 to purchase the first seven MX trains.\n\nThe deployment plan called for the trains to be permanently based in shelters that would be constructed on Strategic Air Command bases throughout the United States, with the missile crews on continuous alert. Ten bases were in the running; Fairchild Air Force Base (Spokane, WA), Malmstrom Air Force Base (Great Falls, MT), Minot Air Force Base (Minot, ND), Grand Forks Air Force Base (Grand Forks, ND), Dyess Air Force Base (Abilene, TX), Whiteman Air Force Base, Knob Noster, MO), Blytheville Air Force Base (Blytheville, AR), Little Rock Air Force Base (Little Rock, AR), Barksdale Air Force Base (Shreveport, LA), and Wurtsmith Air Force Base (Oscoda, MI). Upon the receipt of a signal indicating an increase in alert level, the trains would be \"flushed\", dispersing onto the American railroad network, thus making it difficult for an enemy to determine where the missiles were at any given time to target them.\n\nMajor contractors for the rail garrison system were Boeing Aerospace Corporation, Westinghouse Marine Division and Rockwell International Autonetics. The proposed main garrison for the weapons deployment system was to be F.E. Warren Air Force Base in Wyoming, with each selected garrison hosting up to four trains.\n\nAfter several years of development, the prototype Rail Garrison Car was delivered to the U.S. Air Force on October 4, 1990. After undergoing initial evaluation at Vandenberg Air Force Base, the car was then sent to the Transportation Test Center in Pueblo, Colorado for further testing on the Association of American Railroads' test track.\n\nIn 1991, with the end of the Cold War, the Peacekeeper rail garrison system was cancelled. As a result, all operational Peacekeeper missiles produced were installed in former Minuteman silos. Following termination, the prototype rail garrison car was delivered to the National Museum of the United States Air Force at Wright-Patterson Air Force Base, Ohio, in 1994 for public display.\n\nNotes\n\nCitations\n\nBibliography\n"}
{"id": "271860", "url": "https://en.wikipedia.org/wiki?curid=271860", "title": "Petroleum jelly", "text": "Petroleum jelly\n\nPetroleum jelly, petrolatum, white petrolatum, soft paraffin/paraffin wax or multi-hydrocarbon, CAS number 8009-03-8, is a semi-solid mixture of hydrocarbons (with carbon numbers mainly higher than 25), originally promoted as a topical ointment for its healing properties.\n\nAfter petroleum jelly became a medicine chest staple, consumers began to use it for many ailments, as well as cosmetic purposes, including toenail fungus, genital rashes (non-STD), nosebleeds, diaper rash, and chest colds. Its folkloric medicinal value as a \"cure-all\" has since been limited by better scientific understanding of appropriate and inappropriate uses. It is recognized by the U.S. Food and Drug Administration (FDA) as an approved over-the-counter (OTC) skin protectant and remains widely used in cosmetic skin care.\n\nThe raw material for petroleum jelly was discovered in 1859 in Titusville, Pennsylvania, United States, on some of the country's first oil rigs. Workers disliked the paraffin-like material forming on rigs because it caused them to malfunction, but they used it on cuts and burns because they believed that it hastened healing.\n\nRobert Chesebrough, a young chemist whose previous work of distilling fuel from the oil of sperm whales had been rendered obsolete by petroleum, went to Titusville to see what new materials had commercial potential. Chesebrough took the unrefined black \"rod wax\", as the drillers called it, back to his laboratory to refine it and explore potential uses.\nChesebrough discovered that by distilling the lighter, thinner oil products from the rod wax, he could create a light-colored gel. Chesebrough patented the process of making petroleum jelly by in 1872. The process involved vacuum distillation of the crude material followed by filtration of the still residue through bone char.\n\nChesebrough traveled around New York demonstrating the product to encourage sales by burning his skin with acid or an open flame, then spreading the ointment on his injuries and showing his past injuries healed, he claimed, by his miracle product.\n\nHe opened his first factory in 1870 in Brooklyn using the name Vaseline.\n\nPetroleum jelly is a mixture of hydrocarbons, having a melting point usually close to human body temperature, approximately . It is flammable only when heated to liquid; then the fumes will light, not the liquid itself, so a wick material like leaves, bark, or small twigs is needed to ignite petroleum jelly. It is colorless or has a pale yellow color (when not highly distilled), translucent, and devoid of taste and smell when pure. It does not oxidize on exposure to the air and is not readily acted on by chemical reagents. It is insoluble in water. It is soluble in dichloromethane, chloroform, benzene, diethyl ether, carbon disulfide and oil of turpentine.\n\nDepending on the specific application of petroleum jelly, it may be USP, B.P., or Ph. Eur. grade. This pertains to the processing and handling of the petroleum jelly so it is suitable for medicinal and personal-care applications.\n\nBecause they feel similar when applied to human skin, there is a common misconception that petroleum jelly and glycerol (glycerine) are physically similar. Petroleum jelly is a non-polar hydrophobic (water-repelling) hydrocarbon and insoluble in water. Glycerol is an alcohol that is strongly hydrophilic (water-attracting): by continuously absorbing moisture from the air (hygroscopic), it produces the feeling of wetness on the skin. This feeling of wetness is similar to the feeling of greasiness produced by petroleum jelly.\n\nMost uses of petroleum jelly exploit its lubricating and coating properties.\n\nVaseline brand First Aid Petroleum Jelly, or carbolated petroleum jelly containing phenol to give the jelly additional antibacterial effect, has been discontinued. During World War II, a variety of petroleum jelly called \"red veterinary petrolatum,\" or Red Vet Pet for short, was often included in life raft survival kits. Acting as a sunscreen, it provides protection against ultraviolet rays.\n\nThe American Academy of Dermatology recommends keeping skin injuries moist with petroleum jelly to reduce scarring. A verified medicinal use is to protect and prevent moisture loss of the skin of a patient in the initial post-operative period following laser skin resurfacing.\n\nThere is one case report published in 1994 indicating petroleum jelly should not be applied to the inside of the nose due to the risk of lipid pneumonia, but this was only ever reported in one patient. However, petroleum jelly is used extensively by otolaryngologists—ear, nose, and throat surgeons—for nasal moisture and epistaxis treatment, and to combat nasal crusting. Large studies have assessed petroleum jelly applied to the nose for short durations to have no significant side effects.\n\nHistorically, it was also consumed for internal use and even promoted as \"Vaseline confection\".\n\nMost petroleum jelly today is used as an ingredient in skin lotions and cosmetics, providing various types of skin care and protection by minimizing friction or reducing moisture loss, or by functioning as a grooming aid.\n\nBy reducing moisture loss, petroleum jelly can prevent chapped hands and lips, and soften nail cuticles.\n\nThis property is exploited to provide heat insulation: petroleum jelly can be used to keep swimmers warm in water when training or during channel crossings or long ocean swims. It can prevent chilling of the face due to evaporation of skin moisture during cold weather outdoor sports.\n\nIn the first part of the twentieth century, petroleum jelly, either pure or as an ingredient, was also popular as a hair pomade. When used in a 50/50 mixture with pure beeswax, it makes an effective moustache wax.\n\nPetroleum jelly can be used to reduce the friction between skin and clothing during various sport activities, for example to prevent chafing of the seat region of cyclists, the nipples of long distance runners wearing loose T-shirts, and is commonly used in the crotch area of wrestlers and footballers.\n\nPetroleum jelly is commonly used as a personal lubricant because it does not dry out like water-based lubricants, and has a distinctive \"feel\", different from that of K-Y and related methylcellulose products. However, it is not recommended for use with condoms during sexual activity because it swells latex and thus increases the chance of rupture. It is also not recommended for vaginal intercourse because it may increase the risk of yeast infection and bacterial vaginosis in women.\n\nPetroleum jelly can be used to coat corrosion-prone items such as metallic trinkets, non-stainless steel blades, and gun barrels prior to storage as it serves as an excellent and inexpensive water repellent. It is used as an environmentally friendly underwater antifouling coating for motor boats and sailing yachts. It was recommended in the Porsche owner’s manual as a preservative for light alloy (alleny) anodized Fuchs wheels to protect them against corrosion from road salts and brake dust. “Every three months (after regular cleaning) the wheels should be coated with petroleum jelly.”\n\nIt can be used to finish and protect wood, much like a mineral oil finish. It is used to condition and protect smooth leather products like bicycle saddles, boots, motorcycle clothing, and used to put a shine on patent leather shoes (when applied in a thin coat and then gently buffed off).\n\nPetroleum jelly can be used to lubricate zippers and slide rules. It was also recommended by Porsche in maintenance training documentation for lubrication (after cleaning) of \"Weatherstrips on Doors, Hood, Tailgate, Sun Roof\". The publication states \"…before applying a new coat of lubricant…\" \"Only acid-free lubricants may be used, for example: glycerine, Vaseline, tire mounting paste, etc. These lubricants should be rubbed in, and excessive lubricant wiped off with a soft cloth.\" It is used in bullet lubricant compounds.\n\nPetroleum jelly is a useful material when incorporated into candle wax formulas. The petroleum jelly softens the overall blend, allows the candle to incorporate additional fragrance oil, and facilitates adhesion to the sidewall of the glass. Petroleum jelly is used to moisten nondrying modelling clay such as plasticine, as part of a mix of hydrocarbons including those with greater (paraffin wax) and lesser (mineral oil) molecular weights. It is used as a tack reducer additive to printing inks to reduce paper lint \"picking\" from uncalendered paper stocks. It can be used as a release agent for plaster molds and castings. It is used in the leather industry as a waterproofing cream.\n\nPetroleum jelly is mixed with a high proportion of strong inorganic chlorates due to it acting as a plasticizer and a fuel source. An example of this is Cheddite C which consists of a ratio of 9:1, KClO3 to petroleum jelly. This mixture is unable to detonate without the use of a blasting cap.\n\nIt was used as a stabiliser in the manufacture of the propellant Cordite. \n\nPetroleum jelly can be used to coat the inner walls of terrariums to prevent animals crawling out and escaping.\n\nA stripe of petroleum jelly can be used to prevent the spread of a liquid. For example, it can be applied close to the hairline when using a home hair dye kit to prevent the hair dye from irritating or staining the skin. It is also used to prevent diaper rash.\n\nPetroleum jelly is used to gently clean a variety of surfaces, ranging from makeup removal from faces to tar stain removal from leather.\n\nPetroleum jelly is used to moisturize the paws of dogs, and to inhibit fungal growth on aquatic turtles’ shells. It is a common ingredient in hairball remedies for domestic cats.\n\nPetroleum jelly is very sticky and hard to remove from non-biological surfaces with the usual and customary cleaning agents typically found in the home. It may be dissolved with paint thinner or other petroleum solvents such as acetone, which dissolves many plastics.\n\nPetroleum jelly is slightly soluble in alcohol. To avoid damage to plastics and minimize ventilation issues, isopropyl (rubbing) alcohol can be used to remove petroleum jelly from most surfaces. Isopropyl alcohol is inert to most household surfaces, including almost every plastic, and removes petroleum jelly efficiently. While alcohol causes fewer ventilation problems than petroleum solvents, ventilation is still recommended, especially if large surface areas are involved.\n\nPetroleum jelly is also soluble in lower-molecular-weight oils. Using an oil to dissolve the petroleum jelly first can render it more soluble to solvents and soaps that would not dissolve pure petroleum jelly. Vegetable oils such as canola and olive oil are commonly used to aid in the removal of petroleum jelly from hair and skin.\n\nIn 2015 German consumer watchdog Stiftung Warentest analyzed cosmetics containing mineral oils. After developing a new detection method they found high concentrations of Mineral Oil Aromatic Hydrocarbons (MOHA) and even polyaromatics in products containing mineral oils with Vaseline products containing the most MOHA of all tested cosmetics (up to 9%). The European Food Safety Authority sees MOHA and polyaromatics as possibly carcinogenic. Based on the results, Stiftung Warentest warns not to use Vaseline for lip care or any product that is based on mineral oils. \n\n"}
{"id": "50252496", "url": "https://en.wikipedia.org/wiki?curid=50252496", "title": "Polar metal", "text": "Polar metal\n\nA polar metal, metallic ferroeletric, or ferroelectric metal is a metal that contains an electric dipole moment. Its components have an ordered electric dipole. Such metals should be unexpected, because the charge should conduct by way of the free electrons in the metal and neutralize the polarized charge. However they do exist. One substance family that can produce a polar metal is the nickelate perovskites. One example interpreted to show polar metallic behavior is lanthanum nickelate, LaNiO. A thin film of LaNiO grown on the (111) crystal face of lanthanum aluminate, (LaAlO) was interpreted to be both conductor and a polar material at room temperature. The resistivity of this system, however, shows an upturn with decreasing temperature, hence does not strictly adhere to the definition of a metal. Also, when grown 3 or 4 unit cells thick (1-2 nm) on the (100) crystal face of LaAlO, the LaNiO can be a polar insulator or polar metal depending on the atomic termination of the surface. Lithium osmate, LiOsO also undergoes a ferrorelectric transition when it is cooled below 140K. The point group changes from \"Rc\" to \"R3c\" losing its centrosymmetry. At room temperature and below lithium osmate is an electric conductor, in single crystal, polycrystalline or powder forms, and the ferroelectric form only appears below 140K. Above 140K the material behaves like a normal metal.\n\nP. W. Anderson and E. I. Blount predicted that a ferroelectric metal could exist in 1965. They were inspired to make this prediction based on superconducting transitions, and the ferroelectric transition in barium titanate. The prediction was that atoms do not move far and only a slight crystal non-symmetrical deformation occurs, say from cubic to tetragonal. This transition they called martensitic. They suggested looking at sodium tungsten bronze and InTl alloy. They realised that the free electrons in the metal would neutralise the effect of the polarization at a global level, but that the conduction electrons do not strongly affect transverse optical phonons, or the local electric field inherent in ferroelectricity.\n"}
{"id": "39131099", "url": "https://en.wikipedia.org/wiki?curid=39131099", "title": "Pressure cooker bomb", "text": "Pressure cooker bomb\n\nA pressure cooker bomb is an improvised explosive device (IED) created by inserting explosive material into a pressure cooker and attaching a blasting cap into the cover of the cooker.\n\nPressure cooker bombs have been used in a number of attacks in the 21st century. Among them have been the 2006 Mumbai train bombings, 2010 Stockholm bombings (failed to explode), the 2010 Times Square car bombing attempt (failed to explode), the 2013 Boston Marathon bombing, and the 2017 Manchester Arena bombing. \n\nPressure cooker bombs are relatively easy to construct. With the exception of the explosive charge itself, most of the materials required can be easily obtained. The bomb can be triggered using a simple electronic device such as a digital watch, garage door opener, cell phone, pager, kitchen timer, or alarm clock. The power of the explosion depends on the size of the pressure cooker and the amount and type of explosives used.\n\nSimilar to a pipe bomb, the containment provided by the pressure cooker means that the energy from the explosion is confined until the pressure cooker itself explodes. This in turn creates a relatively large explosion using low explosives and generating potentially lethal shrapnel.\n\nFrench police prevented a terrorist attack in Strasbourg, France, on New Year's Eve 2000. Ten Islamic militants were convicted for the plot.\n\nFrom 2002–04, pressure cooker bombs were widely used in terror and IED attacks in Afghanistan, India, and Pakistan.\n\nIn 2003, a terrorist from Chechnya named Abudullah, carrying a pressure cooker bomb detonated explosives and killed six people before being arrested near Kabul International Airport in Afghanistan. The Taliban claimed responsibility. In 2004, the Department of Homeland Security issued a warning to US agencies about pressure cookers being converted to IEDs.\n\nIn July 2006, in Mumbai, India, 209 people were killed and 714 injured by pressure cooker bombs in the 2006 Mumbai train bombings. According to Mumbai Police, the bombings were carried out by Lashkar-e-Taiba and Students Islamic Movement of India (SIMI).\n\nStep-by-step instructions for making pressure cooker bombs were published in an article titled \"Make a Bomb in the Kitchen of Your Mom\" in the Al-Qaeda-linked \"Inspire\" magazine in the summer of 2010, by \"The AQ chef\". The article describes the technique as a simple way to make a highly effective bomb. Analysts believe the work was the brainchild of Anwar al-Awlaki, and edited by him and by Samir Khan. \"Inspire\"'s goal is to encourage \"lone wolf\" Jihadis to attack what they view as the enemies of \"Jihad\", including the United States and its allies.\nSeveral Islamic radical terrorist attempts in the 2010s involved pressure cooker bombs. The unsuccessful Times Square car bombing attempt in May 2010, in New York City, included a pressure cooker bomb which failed to detonate. The bomb-maker, Faisal Shahzad, was sentenced to life in prison. In the December 2010 Stockholm bombings, a suicide bomber with extreme views on Islam set up a pressure cooker bomb, which failed to detonate. In July 2011, Naser Jason Abdo, a U.S. Army private at Fort Hood, Texas, who took pressure cooker bomb-making tips from the Al-Qaeda magazine article, was arrested for planning to blow up a restaurant frequented by U.S. soldiers. Two pressure cookers and bomb-making materials were found in his hotel room. He was sentenced to life in prison.\n\nIn Pakistan, in March 2010, six employees of World Vision International were killed by a remotely detonated pressure cooker bomb. In October 2012, French police found a makeshift pressure cooker with bomb-making materials near Paris as part of an investigation into an attack on a kosher grocery store.\n\nTwo pressure cooker bombs were used in the Boston Marathon bombings in April 2013. The pressure cookers were filled with nails, ball bearings, and black powder. Initially, it was believed the devices were triggered by kitchen-type egg timers, however, subsequent evidence indicated a remote device was used to trigger the bombs. One of the bombers, Dzhokhar Tsarnaev, told investigators that he learned the technique from an article in \"Inspire\" magazine.\n\nOn Canada Day 2013, pressure cooker bombs failed to explode at the Parliament Building in Victoria, British Columbia.\n\nOn May 19, 2016, passengers on a bus in Wroclaw, Poland, alerted the driver to a suspicious package. The driver removed the package from the bus. Shortly later it exploded with no fatalities but did injure one woman slightly. Authorities believed it was a three liter pressure cooker packed with nails and nitrate explosive.\n\nOn September 17, 2016, an explosion occurred in Lower Manhattan, New York, wounding 29 civilians. The origin of the explosion was found to be a pressure cooker bomb. At least one other bomb was found unexploded. A suspect for that explosion and others in New Jersey, Ahmad Khan Rahami, was captured two days later.\n\nBoth the 2010 Stockholm bombings and the foiled 2016 Sweden terrorism plot involved pressure-cooker bombs.\n\n\n"}
{"id": "682937", "url": "https://en.wikipedia.org/wiki?curid=682937", "title": "Quantum phase transition", "text": "Quantum phase transition\n\nIn physics, a quantum phase transition (QPT) is a phase transition between different quantum phases (phases of matter at zero temperature). Contrary to classical phase transitions, quantum phase transitions can only be accessed by varying a physical parameter—such as magnetic field or pressure—at absolute zero temperature. The transition describes an abrupt change in the ground state of a many-body system due to its quantum fluctuations. Such a quantum phase transition can be a second-order phase transition.\n\nTo understand quantum phase transitions, it is useful to contrast them to classical phase transitions (CPT) (also called thermal phase transitions). A CPT describes a cusp in the thermodynamic properties of a system. It signals a reorganization of the particles; A typical example is the freezing transition of water describing the transition between liquid and solid. The classical phase transitions are driven by a competition between the energy of a system and the entropy of its thermal fluctuations. A classical system does not have entropy at zero temperature and therefore no phase transition can occur. Their order is determined by the first discontinuous derivative of a thermodynamic potential. \nA phase transition from water to ice, for example, involves latent heat (a discontinuity of the internal energy formula_1) and is of first order. A phase transition from a ferromagnet to a paramagnet is continuous and is of second order. (See phase transition for Ehrenfest's classification of phase transitions by the derivative of free energy which is discontinuous at the transition). \nThese continuous transitions from an ordered to a disordered phase are described by an order parameter, which is zero in the \ndisordered and nonzero in the ordered phase. For the aforementioned ferromagnetic transition, the order parameter would represent the total magnetization of the system.\n\nAlthough the thermodynamic average of the order parameter is zero in the disordered state, its fluctuations can be nonzero and become \nlong-ranged in the vicinity of the critical point, where their typical length scale \"ξ\" (correlation length) and typical fluctuation decay time scale \"τ\" (correlation time) diverge:\n\nwhere\n\nis defined as the relative deviation from the critical temperature \"T\". We call \"ν\" the (correlation length) \"critical exponent\" and \"z\" the \"dynamical critical exponent\". Critical behavior of nonzero temperature phase transitions is fully described by classical thermodynamics; quantum mechanics does not play any role even if the actual phases require a quantum mechanical description (e.g. superconductivity).\n\nTalking about \"quantum\" phase transitions means talking about transitions at \"T\" = 0: by tuning a non-temperature parameter like pressure, chemical composition or magnetic field, one could suppress e.g. some transition temperature like the Curie or Néel temperature to 0 K.\n\nAs a system in equilibrium at zero temperature is always in its lowest-energy state, a QPT cannot be explained by thermal fluctuations. Instead, quantum fluctuations, arising from Heisenberg's uncertainty principle, drive the loss of order characteristic of a QPT. The QPT occurs at the quantum critical point (QCP), where quantum fluctuations driving the transition diverge and \nbecome scale invariant in space and time. \n\nAlthough absolute zero is not physically realizable, characteristics of the transition can be detected in the system's low-temperature behavior near the critical point. At nonzero temperatures, classical \nfluctuations with an energy scale of \"kT\" compete with the quantum fluctuations of energy scale \"ħω.\" Here \"ω\" is the characteristic frequency of the quantum oscillation and is inversely proportional to the correlation time. Quantum fluctuations dominate the system's behavior in the region where \"ħω\" > \"kT\", known as the quantum critical region. This quantum critical behavior manifests itself in unconventional and unexpected physical behavior like novel non Fermi liquid phases. From a theoretical point of view, a phase diagram like the one shown on the right is expected: the QPT separates an ordered from a disordered phase (often, the low temperature disordered phase is referred to as 'quantum' disordered).\n\nAt high enough temperatures, the system is disordered and purely classical. Around the classical phase transition, the system is governed by classical thermal fluctuations (light blue area). This region becomes narrower with decreasing energies and converges towards the quantum critical point (QCP). Experimentally, the 'quantum critical' phase, which is still governed by quantum fluctuations, is the most interesting one.\n\n\n"}
{"id": "4526198", "url": "https://en.wikipedia.org/wiki?curid=4526198", "title": "Solar panels on spacecraft", "text": "Solar panels on spacecraft\n\nSpacecraft operating in the inner Solar System usually rely on the use of photovoltaic solar panels to derive electricity from sunlight. In the outer solar system, where the sunlight is too weak to produce sufficient power, radioisotope thermoelectric generators (RTGs) are used as a power source.\n\nThe first spacecraft to use solar panels was the Vanguard 1 satellite, launched by the US in 1958. This was largely because of the influence of Dr. Hans Ziegler, who can be regarded as the father of spacecraft solar power.\n\nSolar panels on spacecraft supply power for two main uses:\n\n\nFor both uses, a key figure of merit of the solar panels is the specific power (watts generated divided by solar array mass), which indicates on a relative basis how much power one array will generate for a given launch mass relative to another. Another key metric is stowed packing efficiency (deployed watts produced divided by stowed volume), which indicates how easily the array will fit into a launch vehicle. Yet another key metric is cost (dollars per watt).\n\nTo increase the specific power, typical solar panels on spacecraft use close-packed solar cell rectangles that cover nearly 100% of the sun-visible area of the solar panels,\nrather than the solar wafer circles which, even though close-packed, cover about 90% of the sun-visible area of typical solar panels on earth. However, some solar panels on spacecraft have solar cells that cover only 30% of the sun-visible area.\n\nSolar panels need to have a lot of surface area that can be pointed towards the Sun as the spacecraft moves. More exposed surface area means more electricity can be converted from light energy from the Sun. Since spacecraft have to be small, this limits the amount of power that can be produced.\n\nAll electrical circuits generate waste heat; in addition, solar arrays act as optical and thermal as well as electrical collectors. Heat must be radiated from their surfaces. High-power spacecraft may have solar arrays that compete with the active payload itself for thermal dissipation. The innermost panel of arrays may be \"blank\" to reduce the overlap of views to space. Such spacecraft include the higher-power communications satellites (e.g., later-generation TDRS) and Venus Express, not high-powered but closer to the Sun.\n\nSpacecraft are built so that the solar panels can be pivoted as the spacecraft moves. Thus, they can always stay in the direct path of the light rays no matter how the spacecraft is pointed. Spacecraft are usually designed with solar panels that can always be pointed at the Sun, even as the rest of the body of the spacecraft moves around, much as a tank turret can be aimed independently of where the tank is going. A tracking mechanism is often incorporated into the solar arrays to keep the array pointed towards the sun.\n\nSometimes, satellite operators purposefully orient the solar panels to \"off point,\" or out of direct alignment from the Sun. This happens if the batteries are completely charged and the amount of electricity needed is lower than the amount of electricity made; off-pointing is also sometimes used on the International Space Station for orbital drag reduction.\n\nSpace contains varying levels of ionizing radiation, that which includes flares and other solar events. Some satellites orbit within the protective zone of the magnetosphere, while others do not.\n\nGallium arsenide-based solar cells are typically favored over crystalline silicon in industry because they have a higher efficiency and degrade more slowly than silicon in the radiation present in space. The most efficient solar cells currently in production are multi-junction photovoltaic cells. These use a combination of several layers of gallium arsenide, indium gallium phosphide, and germanium to capture more energy from the solar spectrum. Leading edge multi-junction cells are capable of exceeding 38.8% under non-concentrated AM1.5G illumination and 46% using concentrated AM1.5G illumination.\n\nTo date, solar power, other than for propulsion, has been practical for spacecraft operating no farther from the Sun than the orbit of Jupiter. For example, \"Juno\", \"Magellan\", \"Mars Global Surveyor\", and \"Mars Observer\" used solar power as does the Earth-orbiting, Hubble Space Telescope. The \"Rosetta\" space probe, launched 2 March 2004, used its of solar panels as far as the orbit of Jupiter (5.25 AU); previously the furthest use was the \"Stardust\" spacecraft at 2 AU. Solar power for propulsion was also used on the European lunar mission SMART-1 with a Hall effect thruster.\n\nThe \"Juno\" mission, launched in 2011, is the first mission to Jupiter (arrived at Jupiter on July 4, 2016) to use solar panels instead of the traditional RTGs that are used by previous outer solar system missions, making it the furthest spacecraft to use solar panels to date. It has of panels.\n\nAnother spacecraft of interest is \"Dawn\" which went into orbit around 4 Vesta in 2011. It used ion thrusters to get to Ceres.\n\nThe potential for solar powered spacecraft beyond Jupiter has been studied.\n\nThe International Space Station also uses solar arrays to power everything on the station. The 262,400 solar cells cover around of space. There are four sets of solar arrays that power the station and the fourth set of arrays were installed in March 2009. 84 to 120 kilowatts of electricity can be generated from these solar arrays.\n\nFor future missions, it is desirable to reduce solar array mass, and to increase the power generated per unit area. This will reduce overall spacecraft mass, and may make the operation of solar-powered spacecraft feasible at larger distances from the sun. Solar array mass could be reduced with thin-film photovoltaic cells, flexible blanket substrates, and composite support structures. Solar array efficiency could be improved by using new photovoltaic cell materials and solar concentrators that intensify the incident sunlight. Photovoltaic concentrator solar arrays for primary spacecraft power are devices which intensify the sunlight on the photovoltaics. This design uses a flat lens, called a Fresnel lens, which takes a large area of sunlight and concentrates it onto a smaller spot. The same principle is used to start fires with a magnifying glass on a sunny day.\n\nSolar concentrators put one of these lenses over every solar cell. This focuses light from the large concentrator area down to the smaller cell area. This allows the quantity of expensive solar cells to be reduced by the amount of concentration. Concentrators work best when there is a single source of light and the concentrator can be pointed right at it. This is ideal in space, where the Sun is a single light source. Solar cells are the most expensive part of solar arrays, and arrays are often a very expensive part of the spacecraft. This technology may allow costs to be cut significantly due to the utilization of less material.\n\n"}
{"id": "4298184", "url": "https://en.wikipedia.org/wiki?curid=4298184", "title": "Solid film lubricant", "text": "Solid film lubricant\n\nSolid film lubricants are paint-like coatings of very fine particles of lubricating pigment blended with a binder and other additives. The lubricant is applied to a substrate by spray, dip or brush methods and, once cured, creates a solid film which repels water, reduces friction and increases the wear life of the substrate to which it has been applied. Certain film lubricants also offer additional properties such as corrosion inhibition. Solid film lubricants are used in the automotive, transportation and aerospace industries.\n"}
{"id": "7490280", "url": "https://en.wikipedia.org/wiki?curid=7490280", "title": "Spent caustic", "text": "Spent caustic\n\nSpent caustic is a waste industrial caustic solution that has become exhausted and is no longer useful (or spent). Spent caustics are made of sodium hydroxide or potassium hydroxide, water, and contaminants. The contaminants have consumed the majority of the sodium (or potassium) hydroxide and thus the caustic liquor is spent, for example, in one common application HS (\"gas\") is scrubbed by the NaOH (\"aqueous\") to form NaHS (\"aq\") and HO (\"l\"), thus consuming the caustic.\n\n\nSpent caustics are malodorous wastewaters that are difficult to treat in conventional wastewater processes. Typically the material is disposed of by high dilution with biotreatment, deep well injection, incineration, wet air oxidation, Humid Peroxide Oxidation or other speciality processes. Most ethylene spent caustics are disposed of through wet air oxidation.\n\n"}
{"id": "247737", "url": "https://en.wikipedia.org/wiki?curid=247737", "title": "Transonic", "text": "Transonic\n\nIn aeronautics, transonic (or transsonic) flight is flying at or near the speed of sound 343 meters per second (1,235 km/h; 1,125 ft/s; 767 mph; 667 kn), at sea level under average conditions), relative to the air through which the vehicle is traveling. A typical convention used is to define transonic flight as speeds in the range of Mach 0.72 to 1.0 ( at sea level).\n\nThis condition depends not only on the travel speed of the craft, but also on the temperature of the airflow in the vehicle's local environment. It is formally defined as the range of speeds between the critical Mach number, when some parts of the airflow over an air vehicle or airfoil are supersonic, and a higher speed, typically near Mach 1.2, when most of the airflow is supersonic. Between these speeds some of the airflow is supersonic, but a significant fraction is not.\n\nMost modern jet powered aircraft are engineered to operate at transonic air speeds. Transonic airspeeds see a rapid increase in drag from about Mach 0.8, and it is the fuel costs of the drag that typically limits the airspeed. Attempts to reduce wave drag can be seen on all high-speed aircraft. Most notable is the use of swept wings, but another common form is a wasp-waist fuselage as a side effect of the Whitcomb area rule.\n\nSevere instability can occur at transonic speeds. Shock waves move through the air at the speed of sound. When an object such as an aircraft also moves at the speed of sound, these shock waves build up in front of it to form a single, very large shock wave. During transonic flight, the plane must pass through this large shock wave, as well as contend with the instability caused by air moving faster than sound over parts of the wing and slower in other parts.\n\nTransonic speeds can also occur at the tips of rotor blades of helicopters and aircraft. This puts severe, unequal stresses on the rotor blade and may lead to accidents if it occurs. It is one of the limiting factors of the size of rotors and the forward speeds of helicopters (as this speed is added to the forward-sweeping [leading] side of the rotor, possibly causing localized transonics).\n\nAt transonic speeds supersonic expansion fans form intense low-pressure, low-temperature areas at various points around an aircraft. If the temperature drops below the dew point a visible cloud will form. These clouds remain with the aircraft as it travels. It is not necessary for the aircraft as a whole to reach supersonic speeds for these clouds to form. Typically, the tail of the aircraft will reach supersonic flight while the bow of the aircraft is still in subsonic flight. A bubble of supersonic expansion fans terminating by a wake shockwave surround the tail. As the aircraft continues to accelerate, the supersonic expansion fans will intensify and the wake shockwave will grow in size until infinity is reached, at which point the bow shockwave forms. This is Mach 1 and the Prandtl–Glauert singularity.\n\nIn astrophysics, wherever there is evidence of shocks (standing, propagating or oscillating), the flow close by must be transonic, as only supersonic flows form shocks. All black hole accretions are transonic. Many such flows also have shocks very close to the black holes.\n\nThe outflows or jets from young stellar objects or disks around black holes can also be transonic since they start subsonically and at a far distance they are invariably supersonic. Supernovae explosions are accompanied by supersonic flows and shock waves. Bow shocks formed in solar winds are a direct result of transonic winds from a star. It has been long thought that a bow shock was present around the heliosphere of our solar system. This was recently found not to be the case according to IBEX data.\n\n"}
{"id": "9519906", "url": "https://en.wikipedia.org/wiki?curid=9519906", "title": "Valve RF amplifier", "text": "Valve RF amplifier\n\nA valve RF amplifier (UK and Aus.) or tube amplifier (U.S.), is a device for electrically amplifying the power of an electrical radio frequency .\n\nLow to medium power valve amplifiers for frequencies below the microwaves were largely replaced by solid state amplifiers during the 1960s and 1970s, initially for receivers and low power stages of transmitters, transmitter output stages switching to transistors somewhat later. Specially constructed valves are still in use for very high power transmitters, although rarely in new designs.\n\nValves are high voltage/low current devices in comparison with transistors. Tetrode and pentode valves have very flat anode current vs. anode voltage indicating high anode output impedances. Triodes show a stronger relationship between anode voltage and anode current.\n\nThe high working voltage makes them well suited for radio transmitters and valves remain in use today for very high power short wave radio transmitters, where solid state techniques would require many devices in parallel, and very high DC supply currents. High power solid state transmitters also require complex combining and tuning networks, whereas a valve-based transmitter would use a single relatively simple tuned network. Thus while solid state high power short wave transmitters are technically possible, economic considerations still favor valves above 3 MHz and 10,000 watts. Amateurs also use valve amplifiers in the 500-1500 watt range mainly for economic reasons.\n\nValve audio amplifiers typically amplify the entire audio range between 20 Hz and 20 kHz or higher. They use an iron core transformer to provide a suitable high impedance load to the valve(s) while driving a speaker, which is typically 8 Ohms. Audio amplifiers normally use a single valve in class A, or a pair in class B or class AB.\n\nAn RF power amplifier is tuned to a single frequency as low as 18 kHz and as high as the UHF range of frequencies, for the purpose of radio transmission or industrial heating. They use a narrow tuned circuit to provide the valve with a suitably high load impedance and feed a load that is typically 50 or 75 Ohms. RF amplifiers normally operate Class C or Class AB.\n\nAlthough the frequency ranges for audio amplifiers and RF amplifiers overlap, the class of operation, method of output coupling and percent operational bandwidth will differ. Power valves are capable of high frequency response, up to at least 30 MHz. Indeed, many of the Directly Heated Single Ended Triode (DH-SET) audio amplifiers use radio transmitting valves originally designed to operate as RF amplifiers in the high frequency range.\n\n\n\nThe most efficient valve-based RF amplifiers operate Class C. If used with no tuned circuit in the output, this would distort the input signal, producing harmonics. However, class C amplifiers normally use a high Q output network which removes the harmonics, leaving an undistorted sine wave identical to the input waveform. Class C is suitable only for amplifying signals with a constant amplitude, such as FM, FSK, and some CW (Morse code) signals. Where the amplitude of the input signal to the amplifier varies as with single sideband modulation, amplitude modulation, video and complex digital signals, the amplifier must operate class A or AB, to preserve the envelope of the driving signal in an undistorted form. Such amplifiers are referred to as linear amplifiers.\n\nIt is also common to modify the gain of an amplifier operating class C so as to produce amplitude modulation. If done in a linear manner, this modulated amplifier is capable of low distortion. The output signal can be viewed as a product of the input RF signal and the modulating signal.\n\nThe development of FM broadcasting improved fidelity by using a greater bandwidth which was available in the VHF range, and where atmospheric noise was absent. FM also has an inherent ability to reject noise, which is mostly amplitude modulated. Valve technology suffers high-frequency limitations due to cathode-anode transit time. However, tetrodes are successfully used into the VHF range and triodes into the low GHz range. Modern FM broadcast transmitters use both valve and solid state devices, with valves tending to be more used at the highest power levels. FM transmitters operate class C with very low distortion.\n\nToday’s “digital” radio that carries coded data over various phase modulations (such as GMSK, QPSK etc.) and also the increasing demand for spectrum have forced a dramatic change in the way radio is used, e.g. the cellular radio concept. Today’s cellular radio and digital broadcast standards are extremely demanding in terms of the spectral envelope and out of band emissions that are acceptable (in the case of GSM for example, −70 dB or better just a few hundred kilohertz from center frequency). Digital transmitters must therefore operate in the linear modes, with much attention given to achieving low distortion.\n\nValve stages were used to amplify the received radio frequency signals, the intermediate frequencies, the video signal and the audio signals at the various points in the receiver. Historically (pre WWII) \"transmitting tubes\" were among the most powerful tubes available, were usually direct heated by thoriated filaments that glowed like light bulbs. Some tubes were built to be very rugged, capable of being driven so hard that the anode would itself glow cherry red, the anodes being machined from solid material (rather than fabricated from thin sheet) to be able to withstand this without distorting when heated. Notable tubes of this type are the 845 and 211. Later beam power tubes such as the 807 and (direct heated) 813 were also used in large numbers in (especially military) radio transmitters.\n\nToday, radio transmitters are overwhelmingly solid state, even at microwave frequencies (cellular radio base stations). Depending on the application, a fair number of radio frequency amplifiers continue to have valve construction, due to their simplicity, where as, it takes several output transistors with complex splitting and combining circuits to equal the same amount of output power of a single valve.\n\nValve amplifier circuits are significantly different from broadband solid state circuits. Solid state devices have a very low output impedance which allows matching via a broadband transformer covering a large range of frequencies, for example 1.8 to 30 MHz. With either class C or AB operation, these must include low pass filters to remove harmonics. While the proper low pass filter must be switch selected for the frequency range of interest, the result is considered to be a \"no tune\" design. Valve amplifiers have a tuned network that serves as both the low pass harmonic filter and impedance matching to the output load. In either case, both solid state and valve devices need such filtering networks before the RF signal is output to the load.\n\nUnlike audio amplifiers, in which the analog output signal is of the same form and frequency as the input signal, RF circuits may modulate low frequency information (audio, video, or data) onto a carrier (at a much higher frequency), and the circuitry comprises several distinct stages. For example, a radio transmitter may contain:\n\n\nThe most common anode circuit is a tuned LC circuit where the anodes are connected at a voltage node. This circuit is often known as the anode tank circuit.\n\nAn example of this used at VHF/UHF include the 4CX250B, an example of a twin tetrode is the QQV06/40A.\n\nNeutralization is a term used in TGTP (tuned grid tuned plate) amplifiers for the methods and circuits used for stabilization against unwanted oscillations at the operating frequency caused by the inadvertent introduction of some of the output signal back into the input circuits. This mainly occurs via the grid to plate capacity, but can also come via other paths, making circuit layout important. To cancel the unwanted feedback signal, a portion of the output signal is deliberately introduced into the input circuit with the same amplitude but opposite phase.\n\nWhen using a tuned circuit in the input, the network must match the driving source to the input impedance of the grid. This impedance will be determined by the grid current in Class C or AB2 operation. In AB1 operation, the grid circuit should be designed to avoid excessive step up voltage, which although it might provide more stage gain, as in audio designs, it will increase instability and make neutralization more critical.\n\nIn common with all three basic designs shown here, the anode of the valve is connected to a resonant LC circuit which has another inductive link which allows the RF signal to be passed to the output.\nThe circuit shown has been largely replaced by a Pi network which allows simpler adjustment and adds low pass filtering.\n\nThe anode current is controlled by the electrical potential (voltage) of the first grid. A DC bias is applied to the valve to ensure that the part of the transfer equation which is most suitable to the required application is used. The input signal is able to perturb (change) the potential of the grid, this in turn will change the anode current (also known as the plate current).\n\nIn the RF designs shown on this page, a tuned circuit is between the anode and the high voltage supply. This tuned circuit is brought to resonance presenting an inductive load that is well matched to the valve and thus results in an efficient power transfer.\n\nAs the current flowing through the anode connection is controlled by the grid, then the current flowing through the load is also controlled by the grid.\n\nOne of the disadvantages of a tuned grid compared to other RF designs is that neutralization is required.\n\nA passive grid circuit used at VHF/UHF frequencies might use the 4CX250B tetrode. An example of a twin tetrode would be the QQV06/40A. The tetrode has a screen grid which is between the anode and the first grid, which being grounded for RF, acts as a shield to reducing the effective capacitance between the first grid and the anode. The combination of the effects of the screen grid and the grid damping resistor often allow the use of this design without neutralization. The screen found in tetrodes and pentodes, greatly increases the valve's gain by reducing the effect of anode voltage on anode current.\n\nThe input signal is applied to the valve's first grid via a capacitor. The value of the grid resistor determines the gain of the amplifier stage. The higher the resistor the greater the gain, the lower the damping effect and the greater the risk of instability. With this type of stage good layout is less vital.\n\n\n\nThis design normally uses a triode so valves such as the 4CX250B are not suitable for this circuit, unless the screen and control grids are joined, effectively converting the tetrode into a triode. This circuit design has been used at 1296 MHz using disk seal triode valves such as the 2C39A.\n\nThe grid is grounded and the drive is applied to the cathode through a capacitor. The heater supply must be isolated from the cathode as unlike the other designs the cathode is not connected to RF ground. Some valves, such as the 811A, are designed for \"zero bias\" operation and the cathode can be at ground potential for DC. Valves that require a negative grid bias can be used by putting a positive DC voltage on the cathode. This can be achieved by putting a zener diode between the cathode and ground or using a separate bias supply.\n\n\n\nThe valve interelectrode capacitance which exists between the input and output of the amplifier and other stray coupling may allow enough energy to feed back into input so as to cause self oscillation in an amplifier stage. For the higher gain designs this effect must be counteracted. Various methods exist for introducing an out of phase signal from the output back to the input so that the effect is cancelled. Even when the feed back is not sufficient to cause oscillation it can produce other effects, such as difficult tuning. Therefore, neutralization can be helpful, even for an amplifier that does not oscillate. Many grounded grid amplifiers use no neutralization, but at 30 MHz adding it can smooth out the tuning.\n\nAn important part of the neutralization of a tetrode or pentode is the design of the screen grid circuit. To provide the greatest shielding effect, the screen must be well grounded at the frequency of operation. Many valves will have a \"self neutralizing\" frequency somewhere in the VHF range. This results from a series resonance consisting of the screen capacity and the inductance of the screen lead thus providing a very low impedance path to ground.\n\nTransit time effects are important at these frequencies, so feedback is not normally usable and for performance critical applications alternative linearisation techniques have to be used such as degeneration and feedforward.\n\nNoise figure is not usually an issue for power amplifier valves, however, in receivers using valves it can be important. While such uses are obsolete, this information is included for historical interest.\n\nLike any amplifying device, valves add noise to the signal to be amplified. Even with a hypothetical perfect amplifier, however, noise is unavoidably present due to thermal fluctuations in the signal source (usually assumed to be at room temperature, \"T\" = 295 K). Such fluctuations cause an electrical noise power of formula_1, where \"k\" is the Boltzmann constant and \"B\" the bandwidth. Correspondingly, the voltage noise of a resistance \"R\" into an open circuit is formula_2 and the current noise into a short circuit is formula_3.\n\nThe noise figure is defined as the ratio of the noise power at the output of the amplifier relative to the noise power that would be present at the output if the amplifier were noiseless (due to amplification of thermal noise of the signal source). An equivalent definition is: noise figure is the factor by which insertion of the amplifier degrades the signal to noise ratio. It is often expressed in decibels (dB). An amplifier with a 0 dB noise figure would be perfect.\n\nThe noise properties of tubes at audio frequencies can be modeled well by a perfect noiseless tube having a source of voltage noise in series with the grid. For the EF86 tube, for example, this voltage noise is specified (see e.g., the Valvo, Telefunken or Philips data sheets) as 2 microvolts integrated over a frequency range of approximately 25 Hz to 10 kHz. (This refers to the integrated noise, see below for the frequency dependence of the noise spectral density.) This equals the voltage noise of a 25 kΩ resistor. Thus, if the signal source has an impedance of 25 kΩ or more, the noise of the tube is actually smaller than the noise of the source. For a source of 25 kΩ, the noise generated by tube and source are the same, so the total noise power at the output of the amplifier is twice the noise power at the output of the perfect amplifier. The noise figure is then two, or 3 dB. For higher impedances, such as 250 kΩ, the EF86's voltage noise is formula_4 lower than the sources's own noise. It therefore adds 1/10 of the noise power caused by the source, and the noise figure is 0.4 dB. For a low-impedance source of 250 Ω, on the other hand, the noise voltage contribution of the tube is 10 times larger than the signal source, so that the noise power is one hundred times larger than that caused by the source. The noise figure in this case is 20 dB.\n\nTo obtain low noise figure the impedance of the source can be increased by a transformer. This is eventually limited by the input capacity of the tube, which sets a limit on how high the signal impedance can be made if a certain bandwidth is desired.\n\nThe noise voltage density of a given tube is a function of frequency. At frequencies above 10 kHz or so, it is basically constant (\"white noise\"). White noise is often expressed by an equivalent noise resistance, which is defined as the resistance which produces the same voltage noise as present at the tube input. For triodes, it is approximately (2-4)/\"g\", where \"g\" is the transconductivity. For pentodes, it is higher, about (5-7)/\"g\". Tubes with high \"g\" thus tend to have lower noise at high frequencies. For example, it is 300 Ω for one half of the ECC88, 250 Ω for an E188CC (both have \"g\" = 12.5 mA/V) and as low as 65 Ω for a tride-connected D3a (\"g\" = 40 mA/V).\n\nIn the audio frequency range (below 1–100 kHz), \"1/\"f\"\" noise becomes dominant, which rises like 1/\"f\". (This is the reason for the relatively high noise resistamnce of the EF86 in the above example.) Thus, tubes with low noise at high frequency do not necessarily have low noise in the audio frequency range. For special low noise audio tubes, the frequency at which 1/\"f\" noise takes over is reduced as far as possible, maybe to something like a kilohertz. It can be reduced by choosing very pure materials for the cathode nickel, and running the tube at an optimized (generally low) anode current.\n\nAt radio frequencies, things are more complicated: (i) The input impedance of a tube has a real component that goes down like 1/\"f\"² (due to cathode lead inductance and transit time effects). This means the input impedance can no longer be increased arbitrarily in order to reduce the noise figure. (ii) This input resistance has its own thermal noise, just like any resistor. (The \"temperature\" of this resistor for noise purposes is more close to the cathode temperature than to room temperature). Thus, the noise figure of tube amplifiers increases with frequency. At 200 MHz, a noise figure of 2.5 (or 4 dB) can be reached with the ECC2000 tube in an optimized \"cascode\"-circuit with an optimized source impedance. At 800 MHz, tubes like EC8010 have noise figures of about 10 dB or more. Planar triodes are better, but very early, transistors have reached noise figures substantially lower than tubes at UHF. Thus, the tuners of television sets were among the first parts of consumer electronics were transistors were used.\n\nSemiconductor amplifiers have overwhelmingly displaced valve amplifiers for low and medium power applications at all frequencies.\n\nValves continue to be used in some high-power, high-frequency amplifiers used for short wave broadcasting, VHF and UHF TV and (VHF) FM radio, also in existing \"radar, countermeasures equipment, or communications equipment\" (p. 56, Symons .. a reference now a decade old) using specially designed valves, such as the klystron, gyrotron, traveling-wave tube, and crossed-field amplifier; however, new designs for such products are now invariably semiconductor-based.\n\n"}
{"id": "15645192", "url": "https://en.wikipedia.org/wiki?curid=15645192", "title": "Yorktown Refinery", "text": "Yorktown Refinery\n\nYorktown Refinery was an oil refinery located alongside of the York River built in 1956. It is now used by Plains All American Pipeline LP as a rail and water oil terminal.\n\nThe refinery used to be operated by Giant Industries and earlier operated by BP/Amoco. \nGiant Industries was acquired by Western Refining in 2007. The refinery could run high TAN crude oil (crude oil with a high content of naphthenic acids). The refining operations were shut down in the fall of 2010 and the refinery was later demolished.\n\nVirginia's only oil refinery becoming storage facility \n\n"}
