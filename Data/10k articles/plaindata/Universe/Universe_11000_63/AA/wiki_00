{"id": "22321775", "url": "https://en.wikipedia.org/wiki?curid=22321775", "title": "1999 Southern Brazil blackout", "text": "1999 Southern Brazil blackout\n\nThe 1999 Southern Brazil blackout was a widespread power outage (the largest ever at the time) that occurred in Brazil on March 11, 1999. \n\nThe blackout involved São Paulo, Rio de Janeiro, Minas Gerais, Goiás, Mato Grosso, Mato Grosso do Sul and Rio Grande do Sul, affecting an estimated 75 to 97 million people. A chain reaction was started when a lightning strike occurred at 22h 16m at an electricity substation in Bauru, São Paulo State causing most of the 440kV circuits at the substation to trip. Brazil was undergoing a severe investment crisis during 1999, which limited spending on maintenance and expansion of the power grid. With few routes for the power to flow from the generating stations via the 440kV system (a very important system to São Paulo state, carrying electricity generated by the Paraná river) a lot of generators automatically shut down because they did not have any load. The world's biggest power plant at the time, Itaipu, tried to support the load that was no longer being supplied by the 440kV power plants, but the 750kV AC lines and the 600kV DC lines that connected the plant to the rest of the system could not take the load and tripped too.\nSouth of São Paulo the consumers experienced an overfrequency, caused because they had more generation than load, mostly because Itaipu was now connected only to this sub-system, but that problem was automatically solved by all generators in the area, that reduced their loads.\nThe rest of the system experienced a much bigger problem, an underfrequency, since the system had a lot of load and not enough generation capacity. Some generators tripped because of the overfrequency, which aggravated the problem, and after an automatic rejection of 35% of the sub-system load the underfrequency did not go away. This caused the system to break in many pieces, São Paulo and Rio de Janeiro states were split with a few areas remaining online. Most of the Minas Gerais system remained online, and powered Brazil's capital, Brasília, as well as the state of Goias and some of Espirito Santo.\n\nIn Rio, the military police placed 1,200 men in the streets to avoid looting. In São Paulo, traffic authorities announced they closed the city's tunnels to prevent assaults. More than 60,000 people were on Rio's subway when lights went out.\nAt midnight, power began returning to some areas.\n\n"}
{"id": "31622840", "url": "https://en.wikipedia.org/wiki?curid=31622840", "title": "2009 energy efficiency and renewable energy research investment", "text": "2009 energy efficiency and renewable energy research investment\n\nThe 2009 energy efficiency and renewable energy research investment was a part of the American Recovery and Reinvestment Act of 2009 and it increased federal funds in renewable energy. The package included $50 billion in spending and $20 billion in tax provisions. The research within the investment also increased; as a result $8.8 billion went into renewable energy research. The investment also included a boost for \"green buildings\". Federal buildings received $4.5 billion in renovations; public housing was also granted $4 billion in renovations. An additional $250 million was made out to energy-efficient affordable housing, in part by installing insulation. Environment America, an organization of state-based, environmental advocacy organizations, reviewed the final bill and stated there was $32.80 billion in funding for clean energy projects, $26.86 billion for energy efficiency initiatives and $18.95 billion for green transportation, totaling $78.61 billion just for \"green\" projects.\n\nDuring his 2008 presidential campaign President Barack Obama stated to help and invest federal money into the green energy industry in the United States. President Obama stated that the plan would put 460,000 Americans to work on renewable energy projects and double the amount of renewable energy produced over the next three years. The plan provided funds to 2 million homes by improving things such as insulation problems. The government pledged to use federal funds to improve the efficiency of 75 percent of federal buildings.\n\nPresident Obama made a plan that put 460,000 Americans to work on energy projects and doubled the amount of alternative energy produced over the next three years. The number of green energy jobs increased. In the short term, the plan would provide funds to improve 2 million homes by improving things such as insulation and leaky windows. The government also hired additional workers to improve the efficiency of 75 percent of federal buildings. $500 million was distributed as grants for training workers in their respected jobs markets. That sum includes $50 million for communities battered by job losses and restructuring the auto industry after the financial crisis of 2008. The energy, education and labor departments also started a partnership to help link the unemployed with jobs, training and education opportunities together.\n\nThe United States government’s need to have the capital market back up their initiatives was studied by University of Massachusetts Amherst economist Robert Pollin and was written in Nation magazine in February 2009. Bank, as he research stated, would be required to devote a percentage of loan portfolios to green investments in the next decade, he wrote, while larger tax credits could be provided to homes and businesses for installation of solar energy and other renewable energy. Funds from a cap-and-trade emissions program or a carbon tax could also be recycled back.\n\nWind farm developers had extended tax credits for every kilowatt hour of electricity they produce. Until now, the credit expired every year and investors were hesitant to put money into projects that could lose their favored tax status in a matter of months. The wind industry called for many of the swaps to the tax credits that the stimulus bill includes. That credit was extended for up to three years. The United States wind industry in 2008 installed about 42% of all the new electric generating space added that year and created 35,000 jobs, in construction and manufacturing. The renewable energy measures in the stimulus bill proposed to aid and sustain green energy as well as encourage additional clean energy job creation.\n\nSolar energy developers were also included in the bill. The bill backed proposed statements in the stimulus package that would change the current system of tax credits for investing in the alternative energy project.\n\nSince the recession has begun to effect the economy, many banks didn't have enough revenue and tax bills and weren't very interested in partnering to get tax credit. So the money to invest in alternative energy decreased significantly. In the stimulus package it shifted federal money directly to developers, in addition to keeping the original tax breaks for the renewable energy programs in America.\n\nGreen building developers have to now get permits and rights-of-way to build lines across private property. The stimulus package doesn't address one big obstacle for green energy building, the cheap cost of burning coal. To make electricity renewable energy advocates are trying to persuade Congress to put a carbon price on carbon-heavy fossil fuels that contribute to global warming.\n"}
{"id": "28686169", "url": "https://en.wikipedia.org/wiki?curid=28686169", "title": "Abertura Photovoltaic Power Station", "text": "Abertura Photovoltaic Power Station\n\nAbertura Photovoltaic Power Station () is a photovoltaic power station in the municipality of Abertura, Cáceres in Spain. It has a total capacity of 23.1 MWp. The solar park was built by Iberinco. Double axes solar trackers were provided by Mecasolar and Inspira. The financing consortium was led by West LB, Bank of Scotland and Dexia. A technical advisor was Sylcom Solar.\n\n"}
{"id": "28974704", "url": "https://en.wikipedia.org/wiki?curid=28974704", "title": "Adams–Williamson equation", "text": "Adams–Williamson equation\n\nThe Adams–Williamson equation, named after L. H. Adams and E. D. Williamson, is an equation used to determine density as a function of radius, more commonly used to determine the relation between the velocities of seismic waves and the density of the Earth's interior. Given the average density of rocks at the Earth's surface and profiles of the P-wave and S-wave speeds as function of depth, it can predict how density increases with depth. It assumes that the compression is adiabatic and that the Earth is spherically symmetric, homogeneous, and in hydrostatic equilibrium. It can also be applied to spherical shells with that property. It is an important part of models of the Earth's interior such as the Preliminary reference Earth model (PREM).\n\nWilliamson and Adams first developed the theory in 1923. They concluded that \"It is therefore impossible to explain the high density of the Earth on the basis of compression alone. The dense interior cannot consist of ordinary rocks compressed to a small volume; we must therefore fall back on the only reasonable alternative, namely, the presence of a heavier material, presumably some metal, which, to judge from its abundance in the Earth's crust, in meteorites and in the Sun, is probably iron.\"\n\nThe two types of seismic body waves are compressional waves (P-waves) and shear waves (S-waves). Both have speeds that are determined by the elastic properties of the medium they travel through, in particular the bulk modulus \"K\", the shear modulus \"μ\", and the density \"ρ\". In terms of these parameters, the P-wave speed \"v\" and the S-wave speed \"v\" are\n\nThese two speeds can be combined in a seismic parameter\n\nThe definition of the bulk modulus,\n\nis equivalent to\nSuppose a region at a distance \"r\" from the Earth's center can be considered a fluid in hydrostatic equilibrium, it is acted on by gravitational attraction from the part of the Earth that is below it and pressure from the part above it. Also suppose that the compression is adiabatic (so thermal expansion does not contribute to density variations). The pressure \"P\"(\"r\") varies with \"r\" as\n\nwhere \"g\"(\"r\") is the gravitational acceleration at radius \"r\".\n\nIf Equations , and are combined, we get the Adams–Williamson equation:\n\nThis equation can be integrated to obtain\n\nwhere \"r\" is the radius at the Earth's surface and \"ρ\" is the density at the surface. Given \"ρ\" and profiles of the P- and S-wave speeds, the radial dependence of the density can be determined by numerical integration.\n"}
{"id": "66535", "url": "https://en.wikipedia.org/wiki?curid=66535", "title": "Alternation of generations", "text": "Alternation of generations\n\nAlternation of generations (also known as metagenesis) is the type of life cycle that occurs in those plants and algae in the Archaeplastida and the Heterokontophyta that have distinct sexual haploid and asexual diploid stages. In these groups, a multicellular gametophyte, which is haploid with \"n\" chromosomes, alternates with a multicellular sporophyte, which is diploid with 2\"n\" chromosomes, made up of \"n\" pairs. A mature sporophyte produces spores by meiosis, a process which reduces the number of chromosomes to half, from 2\"n\" to \"n\". \n\nThe haploid spores germinate and grow into a haploid gametophyte. At maturity, the gametophyte produces gametes by mitosis, which does not alter the number of chromosomes. Two gametes (originating from different organisms of the same species or from the same organism) fuse to produce a zygote, which develops into a diploid sporophyte. This cycle, from gametophyte to gametophyte (or equally from sporophyte to sporophyte), is the way in which all land plants and many algae undergo sexual reproduction.\n\nThe relationship between the sporophyte and gametophyte varies among different groups of plants. In those algae which have alternation of generations, the sporophyte and gametophyte are separate independent organisms, which may or may not have a similar appearance. In liverworts, mosses and hornworts, the sporophyte is less well developed than the gametophyte and is largely dependent on it. Although moss and hornwort sporophytes can photosynthesise, they require additional photosynthate from the gametophyte to sustain growth and spore development and depend on it for supply of water, mineral nutrients and nitrogen. By contrast, in all modern vascular plants the gametophyte is less well developed than the sporophyte, although their Devonian ancestors had gametophytes and sporophytes of approximately equivalent complexity. In ferns the gametophyte is a small flattened autotrophic prothallus on which the young sporophyte is briefly dependent for its nutrition. In flowering plants, the reduction of the gametophyte is much more extreme; it consists of just a few cells which grow entirely inside the sporophyte.\n\nAnimals develop differently. They directly produce haploid gametes. No haploid spores capable of dividing are produced, so they do not have a haploid gametophyte alternating with a diploid sporophyte. (Some insects have a sex-determining system whereby haploid males are produced from unfertilized eggs; however the females are diploid.)\n\nLife cycles of plants and algae with alternating haploid and diploid multicellular stages are referred to as diplohaplontic (the equivalent terms haplodiplontic, diplobiontic or dibiontic are also in use). Life cycles, such as those of animals, in which there is only a diploid multicellular stage are referred to as diplontic. Life cycles in which there is only a haploid multicellular stage are referred to as haplontic.\n\nAlternation of generations is defined as the alternation of multicellular diploid and haploid forms in the organism's life cycle, regardless of whether or not these forms are free-living. In some species, such as the alga \"Ulva lactuca\", the diploid and haploid forms are indeed both free-living independent organisms, essentially identical in appearance and therefore said to be isomorphic. The free-swimming, haploid gametes form a diploid zygote which germinates into a multicellular diploid sporophyte. The sporophyte produces free-swimming haploid spores by meiosis that germinate into haploid gametophytes.\n\nHowever, in some other groups, either the sporophyte or the gametophyte is very much reduced and is incapable of free living. For example, in all bryophytes the gametophyte generation is dominant and the sporophyte is dependent on it. By contrast, in all modern vascular land plants the gametophytes are strongly reduced, although the fossil evidence indicates that they were derived from isomorphic ancestors. In seed plants, the female gametophyte develops totally within the sporophyte which protects and nurtures it and the embryo sporophyte that it produces. The pollen grains, which are the male gametophytes, are reduced to only a few cells (just three cells in many cases). Here the notion of two generations is less obvious; as Bateman & Dimichele say \"[s]porophyte and gametophyte effectively function as a single organism\". The alternative term 'alternation of phases' may then be more appropriate.\n\nDebates about alternation of generations in the early twentieth century can be confusing because various ways of classifying \"generations\" co-exist (sexual vs. asexual, gametophyte vs. sporophyte, haploid vs. diploid, etc.).\n\nInitially, Chamisso and Steenstrup described the succession of differently organized generations (sexual and asexual) in animals as \"alternation of generations\", while studying the development of tunicates, cnidarians and trematode animals. This phenomenon is also known as heterogamy. Presently, the term \"alternation of generations\" is almost exclusively associated with the life cycles of plants, specifically with the alternation of haploid gametophytes and diploid sporophytes.\n\nWilhelm Hofmeister demonstrated the morphological alternation of generations in plants, between a spore-bearing generation (sporophyte) and a gamete-bearing generation (gametophyte). By that time, a debate emerged focusing on the origin of the asexual generation of land plants (i.e., the sporophyte) and is conventionally characterized as a conflict between theories of antithetic (Čelakovský, 1874) and homologous (Pringsheim, 1876) alternation of generations. Čelakovský coined the words sporophyte and gametophyte.\n\nEduard Strasburger (1874) discovered the alternation between diploid and haploid nuclear phases, also called cytological alternation of nuclear phases. Although most often coinciding, morphological alternation and nuclear phases alternation are sometimes independent of one another, e.g., in many red algae, the same nuclear phase may correspond to two diverse morphological generations. In some ferns which lost sexual reproduction, there is no change in nuclear phase, but the alternation of generations is maintained.\n\nThe diagram above shows the fundamental elements of the alternation of generations in plants. The many variations found in different groups of plants are described by use of these concepts later in the article. Starting from the right of the diagram, the processes involved are as follows:\n\nThe 'alternation of generations' in the life cycle is thus between a diploid (2\"n\") generation of sporophytes and a haploid (\"n\") generation of gametophytes.\nThe situation is quite different from that in animals, where the fundamental process is that a diploid (2\"n\") individual directly produces haploid (\"n\") gametes by meiosis. Spores (i.e. haploid cells which are able to undergo mitosis) are not produced, so neither is an asexual multi-cellular generation that alternates with a sexual multi-cellular generation. (Some insects have haploid males that develop from unfertilized eggs, but the females are all diploid.)\n\nThe diagram shown above is a good representation of the life cycle of some multi-cellular algae (e.g. the genus \"Cladophora\") which have sporophytes and gametophytes of almost identical appearance and which do not have different kinds of spores or gametes.\n\nHowever, there are many possible variations on the fundamental elements of a life cycle which has alternation of generations. Each variation may occur separately or in combination, resulting in a bewildering variety of life cycles. The terms used by botanists in describing these life cycles can be equally bewildering. As Bateman and Dimichele say \"[...] the alternation of generations has become a terminological morass; often, one term represents several concepts or one concept is represented by several terms.\"\n\nPossible variations are:\n\nThere are some correlations between these variations, but they are just that, correlations, and not absolute. For example, in flowering plants, microspores ultimately produce microgametes (sperm) and megaspores ultimately produce megagametes (eggs). However, in ferns and their allies there are groups with undifferentiated spores but differentiated gametophytes. For example, the fern \"Ceratopteris thalictrioides\" has spores of only one kind, which vary continuously in size. Smaller spores tend to germinate into gametophytes which produce only sperm-producing antheridia.\n\nThe diagram shows the alternation of generations in a species which is heteromorphic, sporophytic, oogametic, dioicous, heterosporic and dioecious. A seed plant example might be a willow tree (most species of the genus \"Salix\" are dioecious). Starting in the centre of the diagram, the processes involved are:\n\nThe term \"plants\" is taken here to mean the Archaeplastida, i.e. the glaucophytes, red and green algae and land plants.\n\nAlternation of generations occurs in almost all multicellular red and green algae, both freshwater forms (such as \"Cladophora\") and seaweeds (such as \"Ulva\"). In most, the generations are homomorphic (isomorphic) and free-living. Some species of red algae have a complex triphasic alternation of generations, in which there is a gametophyte phase and two distinct sporophyte phases. For further information, see Red algae: Reproduction.\n\nLand plants all have heteromorphic (anisomorphic) alternation of generations, in which the sporophyte and gametophyte are distinctly different. All bryophytes, i.e. liverworts, mosses and hornworts, have the gametophyte generation as the most conspicuous. As an illustration, consider a monoicous moss. Antheridia and archegonia develop on the mature plant (the gametophyte). In the presence of water, the biflagellate sperm from the antheridia swim to the archegonia and fertilisation occurs, leading to the production of a diploid sporophyte. The sporophyte grows up from the archegonium. Its body comprises a long stalk topped by a capsule within which spore-producing cells undergo meiosis to form haploid spores. Most mosses rely on the wind to disperse these spores, although \"Splachnum sphaericum\" is entomophilous, recruiting insects to disperse its spores. For further information, see Liverwort: Life cycle, Moss: Life cycle, Hornwort: Life cycle.\n\nIn ferns and their allies, including clubmosses and horsetails, the conspicuous plant observed in the field is the diploid sporophyte. The haploid spores develop in sori on the underside of the fronds and are dispersed by the wind (or in some cases, by floating on water). If conditions are right, a spore will germinate and grow into a rather inconspicuous plant body called a prothallus. The haploid prothallus does not resemble the sporophyte, and as such ferns and their allies have a heteromorphic alternation of generations. The prothallus is short-lived, but carries out sexual reproduction, producing the diploid zygote that then grows out of the prothallus as the sporophyte. For further information, see Fern: Life cycle.\n\nIn the spermatophytes, the seed plants, the sporophyte is the dominant multicellular phase; the gametophytes are strongly reduced in size and very different in morphology. The entire gametophyte generation, with the sole exception of pollen grains (microgametophytes), is contained within the sporophyte. The life cycle of a dioecious flowering plant (angiosperm), the willow, has been outlined in some detail in an earlier section (A complex life cycle). The life cycle of a gymnosperm is similar. However, flowering plants have in addition a phenomenon called 'double fertilization'. Two sperm nuclei from a pollen grain (the microgametophyte), rather than a single sperm, enter the archegonium of the megagametophyte; one fuses with the egg nucleus to form the zygote, the other fuses with two other nuclei of the gametophyte to form 'endosperm', which nourishes the developing embryo. For further information, see Double fertilization.\n\nIt has been proposed that the basis for the emergence of the diploid phase of the life cycle (sporophyte) as the dominant phase (e.g. as in vascular plants) is that diploidy allows masking of the expression of deleterious mutations through genetic complementation. Thus if one of the parental genomes in the diploid cells contained mutations leading to defects in one or more gene products, these deficiencies could be compensated for by the other parental genome (which nevertheless may have its own defects in other genes). As the diploid phase was becoming predominant, the masking effect likely allowed genome size, and hence information content, to increase without the constraint of having to improve accuracy of DNA replication. The opportunity to increase information content at low cost was advantageous because it permitted new adaptations to be encoded. This view has been challenged, with evidence showing that selection is no more effective in the haploid than in the diploid phases of the lifecycle of mosses and angiosperms.\n\nSome organisms currently classified in the clade Rhizaria and thus not plants in the sense used here, exhibit alternation of generations. Foraminifera undergo a heteromorphic alternation of generations between haploid gamont and diploid agamont forms. The single-celled haploid organism is typically much larger than the diploid organism.\n\nFungal mycelia are typically haploid. When mycelia of different mating types meet, they produce two multinucleate ball-shaped cells, which join via a \"mating bridge\". Nuclei move from one mycelium into the other, forming a heterokaryon (meaning \"different nuclei\"). This process is called plasmogamy. Actual fusion to form diploid nuclei is called karyogamy, and may not occur until sporangia are formed. Karogamy produces a diploid zygote, which is a short-lived sporophyte that soon undergoes meiosis to form haploid spores. When the spores germinate, they develop into new mycelia.\n\nThe life cycle of slime moulds is very similar to that of fungi. Haploid spores germinate to form swarm cells or myxamoebae. These fuse in a process referred to as \"plasmogamy\" and \"karyogamy\" to form a diploid zygote. The zygote develops into a plasmodium, and the mature plasmodium produces, depending on the species, one to many fruiting bodies containing haploid spores.\n\nAlternation between a multicellular diploid and a multicellular haploid generation is never encountered in animals. In some animals, there is an alternation between parthenogenic and sexually reproductive phases (heterogamy). Both phases are diploid. This has sometimes been called \"alternation of generations\", but is quite different. In some other animals, such as hymenopterans, males are haploid and females diploid, but this is always the case rather than there being an alternation between distinct generations.\n\n\n"}
{"id": "11441532", "url": "https://en.wikipedia.org/wiki?curid=11441532", "title": "American Sign Museum", "text": "American Sign Museum\n\nThe American Sign Museum in Cincinnati, Ohio, preserves, archives, and displays a collection of signs. The museum also displays the equipment utilized in the design and manufacture of signs. Tod Swormstedt began working on the museum in 1999. It opened to the public in 2005.\n\nSwormstedt's family owns the signage industry trade journal \"Signs of the Times\", which has been published since 1906. Swormstedt's grandfather, H.C. Menefee, was the first editor of the publication, and purchased it for himself in 1911. Swormstedt had been working at the journal for over twenty years before becoming inspired to start a sign museum in 1999. His family provided $1 million for the project, and figures from the signage industry gave donations of their own. The museum was founded as a nonprofit corporation. Swormstedt considered building the museum in Los Angeles, St. Louis, Memphis, and other sites, but eventually settled on Cincinnati, the base of operations for \"Signs of the Times\".\n\nOver 200 signs and other objects are on display at the museum, and over 3,800 items are cataloged. The collection ranges from the late nineteenth century to the 1970s. Highlights of the collection include samples of gold leaf lettering on glass, a Sputnik-like plastic orb from an Anaheim shopping center, a rotating neon windmill from a Denver donut shop, Las Vegas showcards, and a fiberglass Frisch's Big Boy statue with a slingshot in his pocket. (The slingshot was omitted from later models of the Big Boy statue.) One can also find signs from businesses such as Big Bear Stores, Howard Johnson's, and Earl Scheib. Over the museum's entrance, visitors are greeted by a fiberglass genie from a Los Angeles carpet cleaning company.\n\nIn 2008, the museum acquired a single-arch 1963 McDonald's sign from Huntsville, Alabama. The sign features McDonald's Speedee character, who was phased out in favor of Ronald McDonald in the 1960s. In 2009, the museum added a neon sign from Johnny’s Big Red Grill, once a popular restaurant among Cornell University students.\n\nMany signs owned by the museum were too large to fit the original exhibit space. To better accommodate the collection, the museum began purchasing a property in Camp Washington, Cincinnati, in 2007. The new location is part of the Oesterlein Machine Company-Fashion Frocks, Inc. Complex, a National Register of Historic Places building. The museum opened in its new home in June 2012, and the building displays about 500 signs and artifacts, many of which are on a faux streetscape in a town called \"Signville\".\n\nNeonworks of Cincinnati moved its business into the museum's new location and features a live exhibit showing visitors how they restore neon signs.\n\n\n"}
{"id": "4267366", "url": "https://en.wikipedia.org/wiki?curid=4267366", "title": "Arcing horns", "text": "Arcing horns\n\nArcing horns (sometimes arc-horns) are projecting conductors used to protect insulators or switch hardware on high voltage electric power transmission systems from damage during flashover. Overvoltages on transmission lines, due to atmospheric electricity, lightning strikes, or electrical faults, can cause arcs across insulators (flashovers) that can damage them. Alternately, atmospheric conditions or transients that occur during switching can cause an arc to form in the breaking path of a switch during its operation. Arcing horns provide a path for flashover to occur that bypasses the surface of the protected device. Horns are normally paired on either side of an insulator, one connected to the high voltage part and the other to ground, or at the breaking point of a switch contact. They are frequently to be seen on insulator strings on overhead lines, or protecting transformer bushings.\n\nThe horns can take various forms, such as simple cylindrical rods, circular guard rings, or contoured curves, sometimes known as 'stirrups'.\n\nHigh voltage equipment, particularly that which is installed outside, such as overhead power lines, is commonly subject to transient overvoltages, which may be caused by phenomena such as lightning strikes, faults on other equipment, or switching surges during circuit re-energisation. Overvoltage events such as these are unpredictable, and in general cannot be completely prevented. Line terminations, at which a transmission line connects to a busbar or transformer bushing, are at greatest risk to overvoltage due to the change in characteristic impedance at this point.\n\nAn electrical insulator serves to provide physical separation of conducting parts, and under normal operating conditions is continuously subject to a high electric field which occupies the air surrounding the equipment. Overvoltage events may cause the electric field to exceed the dielectric strength of air and result in the formation of an arc between the conducting parts and over the surface of the insulator. This is called flashover. Contamination of the surface of the insulator reduces the breakdown strength and increases the tendency to flash over. On an electrical transmission system, protective relays are expected to detect the formation of the arc and automatically open circuit breakers to discharge the circuit and extinguish the arc. Under a worst case, this process may take as long as several seconds, during which time the insulator surface would be in close contact with the highly energetic plasma of the arc. This is very damaging to an insulator, and may shatter brittle glass or ceramic disks, resulting in its complete failure.\n\nArcing horns form a spark gap across the insulator with a lower breakdown voltage than the air path along the insulator surface, so an overvoltage will cause the air to break down and the arc to form between the arcing horns, diverting it away from the surface of the insulator. An arc between the horns is more tolerable for the equipment, providing more time for the fault to be detected and the arc to be safely cleared by remote circuit breakers. The geometry of some designs encourages the arc to migrate away from the insulator, driven by rising currents as it heats the surrounding air. As it does so, the path length increases, cooling the arc, reducing the electric field and causing the arc to extinguish itself when it can no longer span the gap. Other designs can utilise the magnetic field produced by the high current to drive the arc away from the insulator. This type of arrangement can be known as a magnetic blowout.\n\nDesign criteria and maintenance regimes may treat arcing horns as sacrificial equipment, cheaper and more easily replaced than the insulator, failure of which can result in complete destruction of the equipment it insulates. Failure of insulator strings on overhead lines could result in the parting of the line, with significant safety and cost implications.\n\nArcing horns thus play a role in the process of correlating system protection with protective device characteristics, known as insulation coordination. The horns should provide, amongst other characteristics, near-infinite impedance during normal operating conditions to minimise conductive current losses, low impedance during the flashover, and physical resilience to the high temperature of the arc.\n\nAs operating voltages increase, greater consideration must be given to such design principles. At medium voltages, one of the two horns may be omitted as the horn-to-horn gap can otherwise be small enough to be bridged by an alighting bird. Alternatively, duplex gaps consisting of two sections on opposite sides of the insulator can be fitted. Low voltage distribution systems, in which the risk of arcing is much lower, may not use arcing horns at all.\n\nThe presence of the arcing horns necessarily disturbs the normal electric field distribution across the insulator due to their small but significant capacitance. More importantly, a flashover across arcing horns produces an earth fault resulting in a circuit outage until the fault is cleared by circuit breaker operation. For this reason, non-linear resistors known as varistors can replace arcing horns at critical locations.\n\nArcing horns are sometimes installed on air-insulated switchgear and transformers to protect the switch arm from arc damage. When a high voltage switch breaks a circuit, an arc can establish itself between the switch contacts before the current can be interrupted. The horns are designed to endure the arc rather than the contact surfaces of the switch itself.\n\nArcing horns are not to be confused with corona rings (or the similar grading rings) which are ring-shaped assemblies surrounding connectors, or other irregular hardware pieces on high potential equipment. Corona rings and grading rings are intended to equalize and redistribute accumulated potential away from components that might be subject to local accumulation and destructive discharges, although sometimes either device may be installed in close proximity to an arcing horn assembly.\n"}
{"id": "759360", "url": "https://en.wikipedia.org/wiki?curid=759360", "title": "Area density", "text": "Area density\n\nThe areal density (also known as area density, surface density, superficial density, or density thickness) of a two-dimensional object is calculated as the mass per unit area. The SI derived unit is: kilogram per square metre (kg·m). In the paper and fabric industries, it is called grammage and is expressed in grams per square meter (gsm); for paper in particular, it may be expressed as pounds per ream of standard sizes (\"basis ream\").\n\nIt can be calculated as:\n\nor\n\nwhere,\n\nA special type of area density is called column (mass) density (also columnar mass density), denoted \"ρ\" or \"σ\". It is the mass of substance per unit area integrated along a path; It is obtained integrating volumetric density formula_3 over a column:\n\nIn general the integration path can be slant or oblique incidence (as in, for example, line of sight propagation in atmospheric physics). A common special case is a vertical path, from the bottom to the top of the medium:\n\nwhere formula_6 denotes the vertical coordinate (e.g., height or depth).\n\nColumnar density formula_7 is closely related to the vertically averaged volumetric density formula_8 as\n\nwhere formula_10; notice that formula_8, formula_7, and formula_13 have units of, for example, grams per cubic metre, grams per square metre, and metres, respectively.\n\nColumn number density refers instead to a number density type of quantity: the number or count of a substance—rather than the mass—per unit area integrated along a path:\n\nIt is a quantity commonly retrieved by remote sensing instruments, for instance the Total Ozone Mapping Spectrometer (TOMS) which retrieves ozone columns around the globe. Columns are also returned by the differential optical absorption spectroscopy (DOAS) method and are a common retrieval product from nadir-looking microwave radiometers.\n\nA closely related concept is that of ice or liquid water path, which specifies the volume per unit area or depth instead of mass per unit area, thus the two are related:\n\nAnother closely related concept is optical depth.\n\nIn Astronomy the column density is generally used to indicate the number of atoms or molecules per square cm (cm) along the line of sight in a particular direction, as derived from observations of e.g. the 21-cm hydrogen line or from observations of a certain molecular species. Also the interstellar extinction can be related to the column density of H or H.\n\nThe concept of area density can be useful when analysing accretion disks. In the case of a disk seen face-on, area density for a given area of the disk is defined as column density: that is, either as the mass of substance per unit area integrated along the vertical path that goes through the disk (line-of-sight), from the bottom to the top of the medium:\n\nwhere formula_6 denotes the vertical coordinate (e.g., height or depth), or as the number or count of a substance—rather than the mass—per unit area integrated along a path (column number density):\n\nAreal density is used to quantify and compare different types media used in data storage devices such as hard disk drives, optical disc drives and tape drives. The current unit of measure is typically gigabits per square inch.\n\nThe area density is often used to describe the thickness of paper; e.g., 80 g/m is very common.\n\nFabric \"weight\" is often specified as mass per unit area, grams per square meter (gsm) or ounces per square yard. It is also sometimes specified in ounces per yard in a standard width for the particular cloth. One gram per square meter equals 0.0295 ounces per square yard; one ounce per square yard equals 33.9 grams per square meter.\n\nIt is also an important quantity for the absorption of radiation. \n\nWhen studying bodies falling through air, area density is important because resistance depends on area, and gravitational force is dependent on mass.\n\nBone density is often expressed in grams per square centimeter (g·cm) as measured by x-ray absorptiometry, as a proxy for the actual density.\n\nThe body mass index is expressed in units of kilograms per square meter, though the area figure is nominal, being simply the square of the height.\n\nThe total electron content in the ionosphere is a quantity of type columnar number density.\n\nSnow water equivalent is a quantity of type columnar mass density.\n\n"}
{"id": "33409078", "url": "https://en.wikipedia.org/wiki?curid=33409078", "title": "Automatic Generation Control", "text": "Automatic Generation Control\n\nIn an electric power system, automatic generation control (AGC) is a system for adjusting the power output of multiple generators at different power plants, in response to changes in the load. Since a power grid requires that generation and load closely balance moment by moment, frequent adjustments to the output of generators are necessary. The balance can be judged by measuring the system frequency; if it is increasing, more power is being generated than used, which causes all the machines in the system to accelerate. If the system frequency is decreasing, more load is on the system than the instantaneous generation can provide, which causes all generators to slow down.\n\nBefore the use of automatic generation control, one generating unit in a system would be designated as the regulating unit and would be manually adjusted to control the balance between generation and load to maintain system frequency at the desired value. The remaining units would be controlled with speed droop to share the load in proportion to their ratings. With automatic systems, many units in a system can participate in regulation, reducing wear on a single unit's controls and improving overall system efficiency, stability, and economy.\n\nWhere the grid has tie interconnections to adjacent control areas, automatic generation control helps maintain the power interchanges over the tie lines at the scheduled levels. With computer-based control systems and multiple inputs, an automatic generation control system can take into account such matters as the most economical units to adjust, the coordination of thermal, hydroelectric, and other generation types, and even constraints related to the stability of the system and capacity of interconnections to other power grids.\n\nTurbine generators in a power system have stored kinetic energy due to their large rotating masses. All the kinetic energy stored in a power system in such rotating masses is a part of the grid inertia. When system load increases, grid inertia is initially used to supply the load. This, however, leads to a decrease in the stored kinetic energy of the turbine generators. Since the mechanical power of these turbines correlates with the delivered electrical power, the turbine generators have a decrease in angular velocity, which is directly proportional to a decrease in frequency in synchronous generators. \n\nThe purpose of the turbine-governor control is to maintain the desired system frequency by adjusting the mechanical power output of the turbine. These controllers have become automated and at steady state, the frequency-power relation for turbine-governor control is,\n\nformula_1\n\nwhere,\n\nformula_2 is the change in turbine mechanical power output\n\nformula_3 is the change in a reference power setting\n\nformula_4 is the regulation constant which quantifies the sensitivity of the generator to a change in frequency\n\nformula_5 is the change in frequency.\n\nFor steam turbines, steam turbine governing adjusts the mechanical output of the turbine by increasing or decreasing the amount of steam entering the turbine via a throttle valve.\n\nLoad-frequency control is employed to allow an area to first meet its own load demands, then to assist in returning the steady-state frequency of the system, Δf, to zero. Load-frequency control operates with a response time of a few seconds to keep system frequency stable.\n\nThe goal of economic dispatch is to minimize total operating costs in an area by determining how the real power output of each generating unit will meet a given load. Generating units have different costs to produce a unit of electrical energy, and incur different costs for the losses in transmitting energy to the load. An economic dispatch algorithm will run every few minutes to select the combination of generating unit power setpoints that minimizes overall cost, subject to the constraints of transmission limitation or security of the system against failures. Further constraints may be imposed by the water supply of hydroelectric generation, or by the availability of sun and wind power.\n\n"}
{"id": "26157113", "url": "https://en.wikipedia.org/wiki?curid=26157113", "title": "Beyond-armour effect", "text": "Beyond-armour effect\n\nBeyond-armour effect is a term coined by Försvarets Fabriksverk (FFV), a semi-governmental Swedish defense firm, while developing the AT4. From the 1980s this phrase was used in its brochures, press releases, weapon instruction manuals and other documentation to denote the \"post-penetration effect\" of the AT4's HEAT anti-armour warhead against the interior and occupants of armoured vehicles.\n\nThe phrase now has become more or less standard in modern military terminology.\n\nDuring World War II, man-portable antitank weapons using shaped charge warheads, more commonly known today as HEAT projectiles, came into widespread use with almost all armies. These warheads have the advantage of not being affected by the projectile's velocity. They penetrate armour by the detonation of an explosive charge fitted in a liner in the shape of a cone or, more precisely, an ogive. The liner is often made of a soft metal, such as copper. Detonation of this shaped charge turns it into a highly focused stream moving forward at extreme speeds. Like medium and high velocity solid shot armour piercing projectiles, these warheads also cause spalling on the interior of the vehicle's armour plate.)\n\nA problem with shaped charge warheads is that if the ogive shaped liner is deep, the warhead will have more penetration but will form a smaller hole; smaller holes are associated with less damage inside the armoured vehicle than larger ones. Research on shaped charge warheads has shown a hole that is the size of a large coin on the outside of a tank turret will have the diameter of a pencil lead on the turret's inner face. If, on the other hand, the ogive is shallow, it will have less penetration, but cause a larger hole on the inside which will result in a massive spalling.\n\nIn 1954, during the siege of Dien Bien Phu, France had dismantled and flown in a number of M-24 Chaffee light tanks. Their thickest armor was only 25.4 mm. The Viet Minh's main infantry antitank weapon was the old World War Two U.S. 2.36-inch bazooka, captured from Nationalist Chinese forces and supplied by Communist China. During the siege the French launched counterattacks using the M-24 in support of their infantry. One Chaffee took seven hits from 2.36-inch bazookas and still continued to fight, demonstrating that portable rocket launchers were hardly a flawless tank-killer.\n\nOther examples became evident during the Vietnam War. The North Vietnamese and their allied forces in South Vietnam were equipped with two types of light anti-armour weapons. One was the 1950s era B-40, which was a Chinese manufactured version of the Russian RPG-2, and the other was the newer RPG-7. The RPG-2 had a shallow cone and the RPG-7 a deep cone. The B-40 had a maximum penetration against armour of approximately 150 mm while the RPG-7 penetration was more than double that of the B-40. But to the surprise of the North Vietnamese, the B-40, while not effective against heavy tanks, was far more effective than the RPG-7 in killing or wounding the occupants of light armoured vehicles, and in igniting the ammunition storage or the vehicle's fuel, including the more modern M113 APC which U.S. and South Vietnamese forces at that time operated in large numbers. In addition, the B-40 proved more effective than the RPG-7 against non-armoured targets, like the bunkers at fire base camps. There are many stories from U.S. Vietnam veterans of the enemy attacking or ambushing very effectively with the B-40. The U.S. Army also discovered that in combat, with rare exceptions, more than one hit by the M72 LAW rocket was required to disable or kill the North Vietnamese PT-76 light tank.\n\nThe combat record of light anti-armour weapons was studied by the engineers at FFV working on a replacement for the Pansarskott m/68 in the late 1970s. The result was the AT4 with a HEAT warhead that had, as the early AT4 brochures stated, a \"special beyond-armour effect\". The designers of the AT4's warhead intended that one hit would cause massive damage to both the occupants and the interior of the targeted vehicle. FFV has said very little about the design of the AT4's warhead, being willing only to describe its effects. Defense journalists and military experts have speculated on the warhead's design. A cutaway photo of the AT4's projectile gave clues. Instead of the standard cone shape liner used by other HEAT warheads, the AT4 has a unique trumpet-shaped liner. Trumpet-shaped liners are believed to be more effective in resisting the counter-blast of reactive armour tiles, that explode and disrupt the HEAT warhead's particle stream. There has been speculation that the liner in the AT4's HEAT warhead is constructed of a special aluminum alloy. Others have stated it is primarily a copper liner with a secondary liner of aluminum bonded to the back of the copper liner. Another clue to the effectiveness of the AT4 is the presence of a \"focus ring\" to concentrate the blast. All public statements to 1984 on the subject are speculative. Tests conducted by the U.S. Army on the AT4 confirmed that the claims made by FFV regarding the AT4's devastating post-penetration effect were substantially correct.\n\nAs described by FFV in their first AT4 brochure in 1983, the \"beyond-armour effect\" had five distinctive characteristics upon penetration of an armoured vehicle:\n\n\nFFV claims that besides the effect of the massive spalling and fragmentation on the occupants of the armoured vehicle, which is standard for all HEAT warheads to a degree, the massive overpressure and intense heat will ignite ammunition and more importantly the armoured vehicle's diesel fuel. The intense light effect will blind any occupants for at least seven minutes, adding to the obscuring effect of the dense smoke created by the AT4's HEAT warhead.\n\n"}
{"id": "4602758", "url": "https://en.wikipedia.org/wiki?curid=4602758", "title": "Bismuth germanate", "text": "Bismuth germanate\n\nBismuth germanium oxide or bismuth germanate is an inorganic chemical compound of bismuth, germanium and oxygen. Most commonly the term refers to the compound with chemical formula (BGO), with the cubic evlitine crystal structure, used as a scintillator. (The term may also refer to a different compound with formula BiGeO, an electro-optical material with sillenite structure, and .)\n\nWhen irradiated by x-rays or gamma rays, bismuth germanate emits photons of wavelengths between 375-650 nm, with peak at 480 nm. It produces about 8500 photons per megaelectronvolt of the high energy radiation absorbed. It has good radiation hardness (parameters remaining stable up to 5.10 Gy), high scintillation efficiency, good energy resolution between 5-20 MeV, is mechanically strong, and is not hygroscopic. It has very high density, 7.13 g/cm³ and a high Z value. Its melting point is 1050 °C. It is the most common oxide-based scintillator.\n\nBismuth germanium oxide is used in detectors in particle physics, aerospace physics, nuclear medicine, geology exploration, and other industries. Bismuth germanate arrays are used for gamma pulse spectroscopy. BGO crystals are also used in positron emission tomography detectors.\n\nCommercially available crystals are grown by the Czochralski process and usually supplied in the form of cuboids or cylinders. Large crystals can be obtained.\n\nBismuth germanate has high electro-optic coefficients (3.3 pm/V for BiGeO), making it useful in nonlinear optics for building Pockels cells, and can also be used for photorefractive devices for ultraviolet range.\n\nThe BiGeO crystals are piezoelectric, show strong electro-optical and acousto-optical effects, and find limited use in the field of crystal oscillators and SAW devices. Single crystal rods and fibers can be grown by floating zone process from a rod of mixture of bismuth oxide and germanium oxide. The crystals are transparent and brown colored.\n\nThe crystals of BGO and similar compounds BSO (BiSiO, bismuth silicon oxide, sillenite) and BTO (BiTiO), are photorefractive and photoconductive. BGO and BSO crystals are efficient photoconductors with low dark conductivity. They can be used in electro-optical applications, like optical PROM, PRIZ spatial light modulators, realtime hologram recording, correlators, and systems for adaptive correction of ultrashort laser pulses, and in fiber optic sensors for electric and magnetic fields. Waveguide structures allow uniform illumination over wide spectral range. Thin film sillenite structures, which can be deposited e.g. by sputtering, have wide range of potential applications. BSO crystals are used in optically addressed spatial light modulators and in liquid crystal light valves. The optical activity of BTO is much smaller than of BGO and BSO. Unlike somewhat similar performing perovskites, sillenites aren't ferroelectric.\n\nThe materials can find use in phased-array optics.\n\nWhen sputtering, the target has to be kept below 450 °C as otherwise the bismuth vapor pressure would get the composition out of stoichiometry, but above 400 °C to form the piezoelectric γ phase.\n\n\n"}
{"id": "1262460", "url": "https://en.wikipedia.org/wiki?curid=1262460", "title": "Bond-dissociation energy", "text": "Bond-dissociation energy\n\nThe bond-dissociation energy (BDE, D\", or DH°\") is one measure of the strength of a chemical bond A–B. It can be defined as the standard enthalpy change when A–B is cleaved by homolysis to give fragments A and B, which are usually radical species. The enthalpy change is temperature dependent, and the bond-dissociation energy is often defined to be the enthalpy change of the homolysis at 0 K (absolute zero), although the enthalpy change at 298 K (standard conditions) is also a frequently encountered parameter. As a typical example, the bond-dissociation energy for one of the C–H bonds in ethane (CH) is defined as the standard enthalpy change of the process\n\nTo convert a molar BDE to the energy needed to dissociate the bond \"per molecule\", the conversion factor 23.060 kcal/mol (96.485 kJ/mol) for each eV can be used.\n\nA variety of experimental techniques, including spectrometric determination of energy levels, generation of radicals by pyrolysis or photolysis, measurements of chemical kinetics and equilibrium, and various calorimetric and electrochemical methods have been used to measure bond dissociation energy values. Nevertheless, bond dissociation energy measurements are challenging and are subject to considerable error. The majority of currently known values are accurate to within ± 1 or 2 kcal/mol. Moreover, values measured in the past, especially before the 1970s, can be especially unreliable and have been subject to revisions on the order of 10 kcal/mol (e.g., benzene C-H bonds, from 103 kcal/mol in 1965 to the modern accepted value of 112.9(5) kcal/mol). Even in modern times (between 1990 and 2004), the O-H bond of phenol has been reported to be anywhere from 85.8 to 91.0 kcal/mol. On the other hand, the bond dissociation energy of H at 298 K has been measured to high precision and accuracy: \"DH\"° (H–H) = 104.1539(1) kcal/mol.\n\nThe term \"bond-dissociation energy\" is similar to the related notion of \"bond-dissociation enthalpy\" (or \"bond enthalpy\"), which is sometimes used interchangeably. However, some authors make the distinction that the bond-dissociation energy (\"D\") refers to the enthalpy change at 0 K, while the term bond-dissociation enthalpy is used for the enthalpy change at 298 K (unambiguously denoted \"DH\"°). The former parameter tends to be favored in theoretical and computational work, while the latter is more convenient for thermochemical studies. For typical chemical systems, the numerical difference between the quantities is small and the distinction can often be ignored. For a hydrocarbon RH where R is significantly larger than H, for instance, the relationship \"D\"(R–H) ≈ \"DH\"°(R–H) – 1.5 kcal/mol is a good approximation. Some textbooks ignore the temperature dependence, while others have defined the bond-dissociation energy to be the reaction enthalpy of homolysis at 298 K.\n\nThe bond dissociation energy is related to but slightly different from the depth of the associated potential energy well of the bond, \"D\", known as the \"electronic energy\". This is due to the existence of a zero-point energy ε for the vibrational ground state, which reduces the amount of energy needed to reach the dissociation limit. Thus, \"D\" is slightly less than \"D\", and the relationship \"D\" = \"D\" – ε holds.\n\nThe bond dissociation energy is an enthalpy change of a particular chemical process, namely homolytic bond cleavage, and \"bond strength\" as measured by the BDE should not be regarded as an intrinsic property of a particular bond type but rather as an energy change that depends on chemical context. For instance, Blanksby and Ellison cites the example of ketene (HC=CO), which has a C=C bond dissociation energy of 79 kcal/mol, while ethylene (HC=CH) has a bond dissociation energy of 174 kcal/mol. This vast difference is accounted for by the thermodynamic stability of carbon monoxide (CO), formed upon C=C bond cleavage of ketene. The difference in availability of spin states upon fragmentation further complicates the use of BDE as a measure of bond strength for head-to-head comparisons, and force constants have been suggested as an alternative. \n\nHistorically, the vast majority of tabulated bond energy values are bond enthalpies. More recently, however, the \"free energy\" analogue of bond-dissociation \"enthalpy\", known as the \"bond-dissociation free energy\" (BDFE), has become more prevalent in the chemical literature. The BDFE of a bond A–B can be defined in the same way as the BDE as the standard free energy change (Δ\"G\"°) accompanying homolytic dissociation of AB into A and B. However, it is often thought of and computed stepwise as the sum of the free energy changes of heterolytic bond dissociation (A–B → A + :B), followed by one-electron reduction of A (A + \"e\" → A•) and one-electron oxidation of B (:B → •B + \"e\"). In contrast to the BDE, which is usually defined and measured in the gas phase, the BDFE is often determined in the solution phase with respect to a solvent like DMSO, since the free energy changes for the aforementioned thermochemical steps can be determined from parameters like acid dissociation constants (p\"K\") and standard redox potentials (ℇ°) that are measured in solution.\n\nExcept for diatomic molecules, the bond-dissociation energy differs from the \"bond energy\". While the bond-dissociation energy is the energy of a single chemical bond, the bond energy is the average of all the bond-dissociation energies of the bonds of the same type for a given molecule. For a homoleptic compound EX\"\", the E–X bond energy is (1/\"n\") multiplied by the enthalpy change of the reaction EX\"\" → E + \"n\"X. Average bond energies given in tables are the average values of the bond energies of a collection of species containing 'typical' examples of the bond in question.\n\nFor example, dissociation of HO–H bond of a water molecule (HO) requires 118.8 kcal/mol (497.1 kJ/mol). The dissociation of the remaining hydroxyl radical requires 101.8 kcal/mol (425.9 kJ/mol). The bond energy of the covalent O–H bonds in water is said to be 110.3 kcal/mol (461.5 kJ/mol), the average of these values.\n\nIn the same way, for removing successive hydrogen atoms from methane the bond-dissociation energies are 105 kcal/mol (439 kJ/mol) for \"D\"(CH–H), 110 kcal/mol (460 kJ/mol) for \"D\"(CH–H), 101 kcal/mol (423 kJ/mol) for \"D\"(CH–H) and finally 81 kcal/mol (339 kJ/mol) for \"D\"(C–H). The bond energy is, thus, 99 kcal/mol or 414 kJ/mol (the average of the bond-dissociation energies). None of the individual bond-dissociation energies equals the bond energy of 99 kcal/mol.\n\nAccording to BDE data, the strongest single bonds are Si–F bonds. The BDE for HSi–F is 152 kcal/mol, almost 50% stronger than the HC–F bond (110 kcal/mol). The BDE for FSi–F is even larger, at 166 kcal/mol. One consequence to these data are that many reactions generate silicon fluorides, such as glass etching, deprotection in organic synthesis, and volcanic emissions. The strength of the bond is attributed to the substantial electronegativity difference between silicon and fluorine, which leads to a substantial contribution from both ionic and covalent bonding to the overall strength of the bond. The C-C single bond of diacetylene (HC≡C–C≡CH) linking two sp-hybridized carbon atoms is also among the strongest, at 160 kcal/mol. The strongest bond for a neutral compound, including multiple bonds, is found in carbon monoxide at 257 kcal/mol. The protonated forms of CO, HCN and N are said to have even stronger bonds, although another study argues that the use of BDE as a measure of bond strength in these cases is misleading.\n\nOn the other end of the scale, there is no clear boundary between a very weak covalent bond and an intermolecular interaction. Lewis acid-base complexes between transition metal fragments and noble gases are among the weakest of bonds with substantial covalent character, with (CO)W:Ar having a W–Ar bond dissociation energy of less than 3.0 kcal/mol. Held together entirely by the van der Waals force, helium dimer, He, has the smallest measured bond dissociation energy of only 0.021 kcal/mol.\n\nBonds can be broken symmetrically or asymmetrically. The former is called homolysis and is the basis of the usual BDEs. Asymmetric scission of a bond is called heterolysis. For molecular hydrogen, the alternatives are:\n\nNote that in the gas phase, the enthalpy of heterolysis is larger than that of homolysis, due to the need to separate unlike charges. However, this value is lowered substantially in the presence of solvent.\n\nThe data tabulated below shows how bond strengths vary over the periodic table. \nThere is great interest, especially in organic chemistry, concerning relative strengths of bonds within a given group of compounds, and representative bond dissociation energies for common organic compounds are shown below.\n"}
{"id": "1095210", "url": "https://en.wikipedia.org/wiki?curid=1095210", "title": "Boron carbide", "text": "Boron carbide\n\nBoron carbide (chemical formula approximately BC) is an extremely hard boron–carbon ceramic, and covalent material used in tank armor, bulletproof vests, engine sabotage powders,\nas well as numerous industrial applications. With a Vickers Hardness of >30 GPa, it is one of the hardest known materials, behind cubic boron nitride and diamond.\n\nBoron carbide was discovered in 19th century as a by-product of reactions involving metal borides, but its chemical formula was unknown. It was not until the 1930s that the chemical composition was estimated as BC.\nThere remained, however, controversy as to whether or not the material had this exact 4:1 stoichiometry, as in practice the material is always slightly carbon-deficient with regard to this formula, and X-ray crystallography shows that its structure is highly complex, with a mixture of C-B-C chains and B icosahedra. These features argued against a very simple exact BC empirical formula.\nBecause of the B structural unit, the chemical formula of \"ideal\" boron carbide is often written not as BC, but as BC, and the carbon deficiency of boron carbide described in terms of a combination of the BC and BCBC units.\n\nThe ability of boron carbide to absorb neutrons without forming long-lived radionuclides makes it attractive as an absorbent for neutron radiation arising in nuclear power plants and from anti-personnel neutron bombs. Nuclear applications of boron carbide include shielding, control rod and shut down pellets. Within control rods, boron carbide is often powdered, to increase its surface area.\n\nBoron carbide has a complex crystal structure typical of icosahedron-based borides. There, B icosahedra form a rhombohedral lattice unit (space group: \"Rm\" (No. 166), lattice constants: \"a\" = 0.56 nm and \"c\" = 1.212 nm) surrounding a C-B-C chain that resides at the center of the unit cell, and both carbon atoms bridge the neighboring three icosahedra. This structure is layered: the B icosahedra and bridging carbons form a network plane that spreads parallel to the \"c\"-plane and stacks along the \"c\"-axis. The lattice has two basic structure units – the B icosahedron and the B octahedron. Because of the small size of the B octahedra, they cannot interconnect. Instead, they bond to the B icosahedra in the neighboring layer, and this decreases bonding strength in the \"c\"-plane.\n\nBecause of the B structural unit, the chemical formula of \"ideal\" boron carbide is often written not as BC, but as BC, and the carbon deficiency of boron carbide described in terms of a combination of the BC and BC units. Some studies indicate the possibility of incorporation of one or more carbon atoms into the boron icosahedra, giving rise to formulas such as (BC)CBC = BC at the carbon-heavy end of the stoichiometry, but formulas such as B(CBB) = BC at the boron-rich end. \"Boron carbide\" is thus not a single compound, but a family of compounds of different compositions. A common intermediate, which approximates a commonly found ratio of elements, is B(CBC) = BC. Quantum mechanical calculations have demonstrated that configurational disorder between boron and carbon atoms on the different positions in the crystal determines several of the materials properties - in particular, the crystal symmetry of the BC composition and the non-metallic electrical character of the BC composition.\n\nBoron carbide is known as a robust material having high hardness, high cross section for absorption of neutrons (i.e. good shielding properties against neutrons), stability to ionizing radiation and most chemicals. Its Vickers hardness (38 GPa), Elastic Modulus (460 GPa) and fracture toughness (3.5 MPa·m) approach the corresponding values for diamond (1150 GPa and 5.3 MPa·m).\n\n, boron carbide is the third hardest substance known, after diamond and cubic boron nitride, earning it the nickname \"black diamond\".\n\nBoron carbide is a semiconductor, with electronic properties dominated by hopping-type transport. The energy band gap depends on composition as well as the degree of order. The band gap is estimated at 2.09 eV, with multiple mid-bandgap states which complicate the photoluminescence spectrum. The material is typically p-type.\n\nBoron carbide was first synthesized by Henri Moissan in 1899, by reduction of boron trioxide either with carbon or magnesium in presence of carbon in an electric arc furnace. In the case of carbon, the reaction occurs at temperatures above the melting point of BC and is accompanied by liberation of large amount of carbon monoxide: \n\nIf magnesium is used, the reaction can be carried out in a graphite crucible, and the magnesium byproducts are removed by treatment with acid.\n\n\n\n"}
{"id": "6759949", "url": "https://en.wikipedia.org/wiki?curid=6759949", "title": "Bridle joint", "text": "Bridle joint\n\nA bridle joint is a woodworking joint, similar to a mortise and tenon, in that a tenon is cut on the end of one member and a mortise is cut into the other to accept it. The distinguishing feature is that the tenon and the mortise are cut to the full width of the tenon member.\n\nThe corner bridle joint (also known as a slot mortise and tenon) joins two members at their respective ends, forming a corner. This form of the joint is commonly used to house a rail in uprights, such as legs. It provides good strength in compression and is fairly resistant to stacking, although a mechanical fastener or pin is often required. The bridle joint is very popular in workbench construction.\n\nCorner bridles are often used to join frame components when the frame is to be shaped. Material can be removed from the joined members after assembly without sacrificing joint integrity.\n\nA variation of the bridle joint is the T-bridle, which joins the end of one member to the middle of another. The tee bridle joint is very strong and good for joining 2 pieces together.\n\nIn traditional timber framing the bridle joint is commonly used to join the tops of principal rafters.\n\n\nCorner bridles can be cut by the following methods:\n\n"}
{"id": "52787040", "url": "https://en.wikipedia.org/wiki?curid=52787040", "title": "CFSMC", "text": "CFSMC\n\nCFSMC, or Carbon Fiber Sheet Molding Compound (also known as CSMC or CF-SMC), is a ready to mold carbon fiber reinforced polymer composite material used in compression molding. While traditional SMC utilizes chopped glass fibers in a polymer resin, CFSMC utilizes chopped carbon fibers. The length and distribution of the carbon fibers is more regular, homogeneous, and constant than the standard glass SMC. CFSMC offers much higher stiffness and usually higher strength than standard SMC, but at a higher cost.\n\nCF-SMC are made up of carbon tow chunks, spread between two layers of uncured thermosetting resin. The carbon fibre tows are cut from prepreg UD tape. The originating tape can be made up of a certain number of fibres (filaments), thus affecting the properties of the final composite: values can vary from 3 to 50 thousand filaments, while typical tow lengths are within 10 to 50 mm. As for the resin, thermosetting resins are used: possible choices are polyester, vinyl ester or epoxy, with the former being the cheapest and the latter being the most performant. Despite not being as strong nor stiff as epoxy, vinyl ester is often used for its properties like corrosion and higher temperature resistance. The constituents are combined in sheets of prepreg material. The tows usually fall from the cutter to one of the two layers of resin, and are then covered by the second layer. The prepreg sheets of SMC are made after the viscous assemble is compacted via rolls. In this phase, any control over the orientation of the fibres is generally impossible, and the fibres can be considered to have an equiprobable orientation in all directions. \n\nOnce the prepreg sheets are made, the material can be compression moulded into the final desired shape. Compression moulding is a manufacturing technique that requires a two parts mould: the first one hosts the moulding material (charge), while the second one is mounted on a press to close the cavity while applying high pressure. Due to complex geometry, it may be necessary to cut the sheets to place them more easily in the lower mould. Then, when the upper part start closing, the material is pushed inside the cavity until complete filling. Pressure is maintained, together with elevated temperature, to allow the curing of the resin and low porosity. This stage has a heavy influence on the mechanical performances of the final product, as the flow of the viscous assemble into the cavity tends to orient the fibres along the flowing direction. By controlling the amount and direction of the flow, it is thus possible to influence the fibre orientation, having a quasi-isotropic material (low-flow moulding) or higher performances in a desired direction (high-flow moulding). \n\nThe manufacturing phase is also important to avoid, when possible, defects like weld lines. Weld lines occur when two flow fronts of material meet when filling the cavity. When this happens, the material is locally as or even more weak than the neat resin. This is generally due to multiple reasons, like absence of fibres intermingle, air traps or even low entanglement of polymeric chaining during curing.\n\nDue to their heterogeneous and anisotropic microstructure, mechanical properties of CF-SMC can vary significantly within broad ranges. Parameters having profound impact on these materials performances are mainly related to the fibres and matrix neat mechanical and geometrical properties (especially those of the fibres) and the orientation and content of the reinforcement. Modulus can vary from less than 20 GPa to 60 GPa, while strength values are within 60-500 MPa. \n\nCF-SMC can also be engineered, to some extent, to have better performances in a specific direction, in a similar fashion as continuous fibres composites. This can be achieved by carefully controlling the compression moulding stage to influence fibre orientation. When the fibres are mainly aligned with the loading direction, the material behaviour is mainly dominated by that of the fibres, thus resulting in stronger and stiffer, but also more brittle response. In the opposite case, if fibres tend to dispose perpendicular to the loading direction, the resin contributes more to the load bearing, and the overall composite will be less stiff, less strong and more ductile. Being based on hydrodynamic transport phenomena, however, the control over fibre orientation in CF-SMC is much more limited than in the continuous composites case, where orientation is often directly determined accurately by the manufacturer. In addition, while continuous fibres composites have a specific orientation, short fibre reinforced plastics can have a preferential orientation, meaning that, considering a generic system of axis, the majority of fibres can have a higher component along a direction and a lower component along the other two axis.\n\nThe discontinuous tow-based microstructure of these materials makes is even more heterogeneous than standard composites: fibre ends themselves acts as stress concentration areas for both the resin and the neighbouring tows; moreover, especially for complex shaped parts, it is impossible to prevent some local spots with badly aligned tows (e.g. perpendicular to the direction of axial stress) or with low fibre volume content, like resin pockets. Although making the material weaker and the structural design more complex, this feature makes these materials quite notch-insensitive. \n\nWhen moulded, CFSMC has a very different appearance than traditional carbon fibre fabric composites, which traditionally appear with a woven checkerboard pattern. CFSMC has the appearance of black and grey marble or burl. \n\nCF SMC combines the lightweight properties of carbon composites with a manufacturing process, as compression moulding, that allows fast manufacturing and thus is suitable for high volume industrial applications. For these reasons, the automotive industry is one of the best candidate for this technology.\n\nCar manufacturers have used standard glass SMC for over 30 years as a material for body panels in select sport cars such as the Chevrolet Corvette. Substituting glass fibres with carbon is a recent development, having been used for significant structural components of the 2003 Dodge Viper, the multifunctional spare wheel pan of Mercedes-AMG E-Class , the Mercedes-Benz SLR McLaren, the 2009 Lexus LFA, 2015 Lamborghini Huracán, the 2017 BMW 7 series and 2017 McLaren chassis. Lamborghini (together with Callaway Golf Company) patented an advanced version of CF-SMC called Forged Composite. They first introduced it in the Sesto Elemento concept car, and since then, Forged Composite has been a distinctive mark for Lamborghini cars, used both in structural and aesthetical purposes. CF-SMC use is recently spreading also to the much broader non-high performance automotive sector as for the 2017 Toyota Prius PHV.\n\nCF-SMC has also been used in the aeronautic industry by Boeing, for the 787 Dreamliner window frames, while producers suggest that the use of these materials will grow in this sector as well .\n"}
{"id": "39712209", "url": "https://en.wikipedia.org/wiki?curid=39712209", "title": "Comparison of commercial battery types", "text": "Comparison of commercial battery types\n\n Cost in USD, adjusted for inflation.\n\nUnder certain conditions, some battery chemistries are at risk of thermal runaway, leading to cell rupture or combustion. As thermal runaway is determined not only by cell chemistry but also cell size, cell design, and charge only the worst-case values are reflected here.\n\n"}
{"id": "42604608", "url": "https://en.wikipedia.org/wiki?curid=42604608", "title": "Condor Cliff Dam", "text": "Condor Cliff Dam\n\nThe Condor Cliff Dam, formerly known as Néstor Kirchner Dam and previously Condor Cliff Dam, is a concrete-face rock-fill dam being built on the Santa Cruz River about west of Puerto Santa Cruz in Santa Cruz Province, Argentina. It was renamed after the former president Néstor Kirchner, born in Santa Cruz. A consortium led by China's Gezhouba Group was awarded the contract to build the Néstor Kirchner Dam and Jorge Cepernic Dam downstream in August 2013. The consortium will also fund the construction. Both dams are expected to cost nearly US$4.8 billion. It will be built by the firm \"Electroingeniería\", led by Osvaldo Acosta and Gerardo Ferreyra. The primary purpose of the dam is hydroelectric power generation and its power station will have an installed capacity of .\n\nIn July 2015 machines arrived in Santa Cruz for the construction of the dams.\n"}
{"id": "41026", "url": "https://en.wikipedia.org/wiki?curid=41026", "title": "Dielectric", "text": "Dielectric\n\nA dielectric (or dielectric material) is an electrical insulator that can be polarized by an applied electric field. When a dielectric is placed in an electric field, electric charges do not flow through the material as they do in an electrical conductor but only slightly shift from their average equilibrium positions causing dielectric polarization. Because of dielectric polarization, positive charges are displaced in the direction of the field and negative charges shift in the opposite direction. This creates an internal electric field that reduces the overall field within the dielectric itself. If a dielectric is composed of weakly bonded molecules, those molecules not only become polarized, but also reorient so that their symmetry axes align to the field.\n\nThe study of dielectric properties concerns storage and dissipation of electric and magnetic energy in materials. Dielectrics are important for explaining various phenomena in electronics, optics, solid-state physics, and cell biophysics.\n\nAlthough the term \"insulator\" implies low electrical conduction, \"dielectric\" typically means materials with a high polarizability. The latter is expressed by a number called the relative permittivity. The term insulator is generally used to indicate electrical obstruction while the term dielectric is used to indicate the energy storing capacity of the material (by means of polarization). A common example of a dielectric is the electrically insulating material between the metallic plates of a capacitor. The polarization of the dielectric by the applied electric field increases the capacitor's surface charge for the given electric field strength.\n\nThe term \"dielectric\" was coined by William Whewell (from \"dia-\" + \"electric\") in response to a request from Michael Faraday. A \"perfect dielectric\" is a material with zero electrical conductivity (cf. perfect conductor), thus exhibiting only a displacement current; therefore it stores and returns electrical energy as if it were an ideal capacitor.\n\nThe electric susceptibility \"χ\" of a dielectric material is a measure of how easily it polarizes in response to an electric field. This, in turn, determines the electric permittivity of the material and thus influences many other phenomena in that medium, from the capacitance of capacitors to the speed of light.\n\nIt is defined as the constant of proportionality (which may be a tensor) relating an electric field E to the induced dielectric polarization density P such that\n\nwhere \"ε\" is the electric permittivity of free space.\n\nThe susceptibility of a medium is related to its relative permittivity \"ε\" by\n\nSo in the case of a vacuum,\n\nThe electric displacement D is related to the polarization density P by\n\nIn general, a material cannot polarize instantaneously in response to an applied field. The more general formulation as a function of time is\n\nThat is, the polarization is a convolution of the electric field at previous times with time-dependent susceptibility given by \"χ\"(Δ\"t\"). The upper limit of this integral can be extended to infinity as well if one defines for . An instantaneous response corresponds to Dirac delta function susceptibility .\n\nIt is more convenient in a linear system to take the Fourier transform and write this relationship as a function of frequency. Due to the convolution theorem, the integral becomes a simple product,\n\nNote the simple frequency dependence of the susceptibility, or equivalently the permittivity. The shape of the susceptibility with respect to frequency characterizes the dispersion properties of the material.\n\nMoreover, the fact that the polarization can only depend on the electric field at previous times (i.e., for ), a consequence of causality, imposes Kramers–Kronig constraints on the real and imaginary parts of the susceptibility \"χ\"(\"ω\").\n\nIn the classical approach to the dielectric model, a material is made up of atoms. Each atom consists of a cloud of negative charge (electrons) bound to and surrounding a positive point charge at its centre. In the presence of an electric field the charge cloud is distorted, as shown in the top right of the figure.\n\nThis can be reduced to a simple dipole using the superposition principle. A dipole is characterized by its dipole moment, a vector quantity shown in the figure as the blue arrow labeled \"M\". It is the relationship between the electric field and the dipole moment that gives rise to the behavior of the dielectric. (Note that the dipole moment points in the same direction as the electric field in the figure. This isn't always the case, and is a major simplification, but is true for many materials.)\n\nWhen the electric field is removed the atom returns to its original state. The time required to do so is the so-called relaxation time; an exponential decay.\n\nThis is the essence of the model in physics. The behavior of the dielectric now depends on the situation. The more complicated the situation, the richer the model must be to accurately describe the behavior. Important questions are:\n\nThe relationship between the electric field E and the dipole moment M gives rise to the behavior of the dielectric, which, for a given material, can be characterized by the function F defined by the equation:\n\nWhen both the type of electric field and the type of material have been defined, one then chooses the simplest function \"F\" that correctly predicts the phenomena of interest. Examples of phenomena that can be so modeled include:\n\n\nDipolar polarization is a polarization that is either inherent to polar molecules (orientation polarization), or can be induced in any molecule in which the asymmetric distortion of the nuclei is possible (distortion polarization). Orientation polarization results from a permanent dipole, e.g., that arising from the 104.45° angle between the asymmetric bonds between oxygen and hydrogen atoms in the water molecule, which retains polarization in the absence of an external electric field. The assembly of these dipoles forms a macroscopic polarization.\n\nWhen an external electric field is applied, the distance between charges within each permanent dipole, which is related to chemical bonding, remains constant in orientation polarization; however, the direction of polarization itself rotates. This rotation occurs on a timescale that depends on the torque and surrounding local viscosity of the molecules. Because the rotation is not instantaneous, dipolar polarizations lose the response to electric fields at the highest frequencies. A molecule rotates about 1 radian per picosecond in a fluid, thus this loss occurs at about 10 Hz (in the microwave region). The delay of the response to the change of the electric field causes friction and heat.\n\nWhen an external electric field is applied at infrared frequencies or less, the molecules are bent and stretched by the field and the molecular dipole moment changes. The molecular vibration frequency is roughly the inverse of the time it takes for the molecules to bend, and this distortion polarization disappears above the infrared.\n\nIonic polarization is polarization caused by relative displacements between positive and negative ions in ionic crystals (for example, NaCl).\n\nIf a crystal or molecule consists of atoms of more than one kind, the distribution of charges around an atom in the crystal or molecule leans to positive or negative. As a result, when lattice vibrations or molecular vibrations induce relative displacements of the atoms, the centers of positive and negative charges are also displaced. The locations of these centers are affected by the symmetry of the displacements. When the centers don't correspond, polarization arises in molecules or crystals. This polarization is called ionic polarization.\n\nIonic polarization causes the ferroelectric effect as well as dipolar polarization. The ferroelectric transition, which is caused by the lining up of the orientations of permanent dipoles along a particular direction, is called an order-disorder phase transition. The transition caused by ionic polarizations in crystals is called a displacive phase transition.\n\nIonic polarization enables the production of energy-rich compounds in cells (the proton pump in mitochondria) and, at the plasma membrane, the establishment of the resting potential, energetically unfavourable transport of ions, and cell-to-cell communication (the Na+/K+-ATPase).\n\nAll cells in animal body tissues are electrically polarized – in other words, they maintain a voltage difference across the cell's plasma membrane, known as the membrane potential. This electrical polarization results from a complex interplay between ion transporters and ion channels.\n\nIn neurons, the types of ion channels in the membrane usually vary across different parts of the cell, giving the dendrites, axon, and cell body different electrical properties. As a result, some parts of the membrane of a neuron may be excitable (capable of generating action potentials), whereas others are not.\n\nIn physics, dielectric dispersion is the dependence of the permittivity of a dielectric material on the frequency of an applied electric field. Because there is a lag between changes in polarization and changes in the electric field, the permittivity of the dielectric is a complicated function of frequency of the electric field. Dielectric dispersion is very important for the applications of dielectric materials and for the analysis of polarization systems.\n\nThis is one instance of a general phenomenon known as material dispersion: a frequency-dependent response of a medium for wave propagation.\n\nWhen the frequency becomes higher:\n\nIn the frequency region above ultraviolet, permittivity approaches the constant \"ε\" in every substance, where \"ε\" is the permittivity of the free space. Because permittivity indicates the strength of the relation between an electric field and polarization, if a polarization process loses its response, permittivity decreases.\n\nDielectric relaxation is the momentary delay (or lag) in the dielectric constant of a material. This is usually caused by the delay in molecular polarization with respect to a changing electric field in a dielectric medium (e.g., inside capacitors or between two large conducting surfaces). Dielectric relaxation in changing electric fields could be considered analogous to hysteresis in changing magnetic fields (for inductors or transformers). Relaxation in general is a delay or lag in the response of a linear system, and therefore dielectric relaxation is measured relative to the expected linear steady state (equilibrium) dielectric values. The time lag between electrical field and polarization implies an irreversible degradation of Gibbs free energy.\n\nIn physics, dielectric relaxation refers to the relaxation response of a dielectric medium to an external, oscillating electric field. This relaxation is often described in terms of permittivity as a function of frequency, which can, for ideal systems, be described by the Debye equation. On the other hand, the distortion related to ionic and electronic polarization shows behavior of the resonance or oscillator type. The character of the distortion process depends on the structure, composition, and surroundings of the sample.\n\nDebye relaxation is the dielectric relaxation response of an ideal, noninteracting population of dipoles to an alternating external electric field. It is usually expressed in the complex permittivity \"ε\" of a medium as a function of the field's frequency \"ω\":\n\nwhere \"ε\" is the permittivity at the high frequency limit, where \"ε\" is the static, low frequency permittivity, and \"τ\" is the characteristic relaxation time of the medium. Separating the real and imaginary parts of the complex dielectric permittivity yields:\n\nThe dielectric loss is also represented by:\n\nThis relaxation model was introduced by and named after the physicist Peter Debye (1913). It is characteristic for dynamic polarization with only one relaxation time.\n\n\nParaelectricity is the ability of many materials (specifically ceramics) to become polarized under an applied electric field. Unlike ferroelectricity, this can happen even if there is no permanent electric dipole that exists in the material, and removal of the fields results in the polarization in the material returning to zero. The mechanisms that cause paraelectric behaviour are the distortion of individual ions (displacement of the electron cloud from the nucleus) and polarization of molecules or combinations of ions or defects.\n\nParaelectricity can occur in crystal phases where electric dipoles are unaligned and thus have the potential to align in an external electric field and weaken it.\n\nAn example of a paraelectric material of high dielectric constant is strontium titanate.\n\nThe LiNbO crystal is ferroelectric below 1430 K, and above this temperature it transforms into a disordered paraelectric phase. Similarly, other perovskites also exhibit paraelectricity at high temperatures.\n\nParaelectricity has been explored as a possible refrigeration mechanism; polarizing a paraelectric by applying an electric field under adiabatic process conditions raises the temperature, while removing the field lowers the temperature. A heat pump that operates by polarizing the paraelectric, allowing it to return to ambient temperature (by dissipating the extra heat), bringing it into contact with the object to be cooled, and finally depolarizing it, would result in refrigeration.\n\n\"Tunable dielectrics\" are insulators whose ability to store electrical charge changes when a voltage is applied.\n\nGenerally, strontium titanate () is used for devices operating at low temperatures, while barium strontium titanate () substitutes for room temperature devices. Other potential materials include microwave dielectrics and carbon nanotube (CNT) composites.\n\nIn 2013 multi-sheet layers of strontium titanate interleaved with single layers of strontium oxide produced a dielectric capable of operating at up to 125 GHz. The material was created via molecular beam epitaxy. The two have mismatched crystal spacing that produces strain within the strontium titanate layer that makes it less stable and tunable.\n\nSystems such as have a paraelectric–ferroelectric transition just below ambient temperature, providing high tunability. Such films suffer significant losses arising from defects.\n\nCommercially manufactured capacitors typically use a solid dielectric material with high permittivity as the intervening medium between the stored positive and negative charges. This material is often referred to in technical contexts as the \"capacitor dielectric\".\n\nThe most obvious advantage to using such a dielectric material is that it prevents the conducting plates, on which the charges are stored, from coming into direct electrical contact. More significantly, however, a high permittivity allows a greater stored charge at a given voltage. This can be seen by treating the case of a linear dielectric with permittivity \"ε\" and thickness \"d\" between two conducting plates with uniform charge density \"σ\". In this case the charge density is given by\n\nand the capacitance per unit area by\n\nFrom this, it can easily be seen that a larger \"ε\" leads to greater charge stored and thus greater capacitance.\n\nDielectric materials used for capacitors are also chosen such that they are resistant to ionization. This allows the capacitor to operate at higher voltages before the insulating dielectric ionizes and begins to allow undesirable current.\n\nA \"dielectric resonator oscillator\" (DRO) is an electronic component that exhibits resonance of the polarization response for a narrow range of frequencies, generally in the microwave band. It consists of a \"puck\" of ceramic that has a large dielectric constant and a low dissipation factor. Such resonators are often used to provide a frequency reference in an oscillator circuit. An unshielded dielectric resonator can be used as a dielectric resonator antenna (DRA).\n\nFrom 2002 to 2004, the Army Research Laboratory (ARL) conducted research on thin film technology. Barium strontium titanate (BST), a ferroelectric thin film, was studied for the fabrication of radio frequency and microwave components, such as voltage-controlled oscillators, tunable filters, and phase shifters.\n\nThe research was part of an effort to provide the Army with highly-tunable, microwave-compatible materials for broadband electric-field tunable devices, which operate consistently in extreme temperatures. This work improved tunability of bulk barium strontium titanate, which is a thin film enabler for electronics components.\n\nIn a 2004 research paper, ARL researchers explored how small concentrations of acceptor dopants can dramatically modify the properties of ferroelectric materials such as BST.\n\nResearchers “doped” BST thin films with magnesium, analyzing the “structure, microstructure, surface morphology and film/substrate compositional quality” of the result. The Mg doped BST films showed “improved dielectric properties, low leakage current, and good tunability,” meriting potential for use in microwave tunable devices.\n\nDielectric materials can be solids, liquids, or gases. In addition, a high vacuum can also be a useful, nearly lossless dielectric even though its relative dielectric constant is only unity.\n\nSolid dielectrics are perhaps the most commonly used dielectrics in electrical engineering, and many solids are very good insulators. Some examples include porcelain, glass, and most plastics. Air, nitrogen and sulfur hexafluoride are the three most commonly used gaseous dielectrics.\n\n\n"}
{"id": "47635273", "url": "https://en.wikipedia.org/wiki?curid=47635273", "title": "Ekoa", "text": "Ekoa\n\nEkoa is a natural Biocomposite available in dry fabrics, pre-pregs, as well as cores and resins. Ekoa has been used in a variety of applications, including musical instruments, such as a ukulele and a guitar, as well as sports equipment, including a bicycle frame, and a lacrosse stick.\nEkoa was initially developed by Blackbird Guitars, a company that has made musical instruments out of Carbon fiber reinforced polymer, but started working on a biobased composite material that would work well for musical instruments. Blackbird worked with Entropy Resins to develop Ekoa, and released the first production musical instrument in 2013. Joe Luttwak of Blackbird and Desi Banatao of Entropy formed a separate company, Lingrove, LLC, to further develop Ekoa and expand applications. Luttwak filed for a patent for \"METHOD FOR MAKING LIGHT AND STIFF PANELS AND STRUCTURES USING NATURAL FIBER COMPOSITES\" on November 18, 2014, which was given A1 Kind Code status on May 15, 2015. Lingrove filed \"Ekoa\" as a registered trademark on November 12, 2013. The trademark was registered on February 3, 2015. The trademark is registered under two separate classes: 015 - Musical instruments, and 024 - Textiles and textile goods, not included in other classes; bed and table covers.\n\nEkoa was initially developed to combine the tone of wooden instruments with the durability of a composite instrument. Previously, composite instruments had been made with carbon fiber, glass fiber, or aluminum to achieve durability, but these materials did not have the same tonality of wood. To address this, Ekoa utilizes flax fibers and produces a tone more like wood. The first musical instrument product made of Ekoa was the Blackbird Clara concert ukulele, which has won a variety of awards in the composites industry, including at The Composites And Advanced Material Expo (CAMX), JEC Americas, and Industrial Designers Society of America's IDEA Award. Blackbird later introduced the El Capitan guitar model, also made with Ekoa.\n\nFor sports equipment, RockWest Composites has produced a bicycle frame in conjunction with Calfee, as well as a lacrosse stick with a hexagonal shape core wrapped in Ekoa twill.\n\n"}
{"id": "25942465", "url": "https://en.wikipedia.org/wiki?curid=25942465", "title": "Energy in Uruguay", "text": "Energy in Uruguay\n\nEnergy in Uruguay describes energy and electricity production, consumption and import in Uruguay. \nHistorically, energy has been a stronghold of state-owned companies, such as UTE and ANCAP.\n\nThe National Directorate of Energy () is the main governmental body in charge of energy policies.\n\nUruguay is notable for its use of renewable energies, which nowadays provide 94.5% of the country’s electricity and 55% of the country's total energy mix.\n"}
{"id": "23401977", "url": "https://en.wikipedia.org/wiki?curid=23401977", "title": "Energy in the Democratic Republic of the Congo", "text": "Energy in the Democratic Republic of the Congo\n\nThe Democratic Republic of the Congo was a net energy exporter in 2008. Most energy was consumed domestically in 2008. According to the IEA statistics the energy export was in 2008 small and less than from the Republic of Congo. 2010 population figures were 3.8 million for the RC compared to CDR 67.8 Million. In both countries journalists and media were threatened by the authorities.\n\nHuman rights in the Democratic Republic of the Congo have influenced the energy markets and economy. On 1 October 2010 the UN reported on violations of human rights and international humanitarian law committed within the DRC between March 1993 and June 2003. The report raised hopes of justice for crimes. In the International Criminal Court (ICC) are cases against using children under the age of 15 for the armed group and against ex Vice President Jean-Pierre Bemba charged with crimes against humanity.\n\nThe Democratic Republic of the Congo has reserves of petroleum, natural gas, coal, and a potential hydroelectric power generating capacity of around 100,000 MW. The Inga Dam, alone on the Congo River, has the potential capacity to generate 40,000 to 45,000 MW of electric power, sufficient to supply the electricity needs of the whole Southern Africa region. Ongoing uncertainties in the political arena, and a resulting lack of interest from investors has meant that the Inga dam's potential has been limited.\n\nIn 2001, the dam was estimated to have an installed generating capacity of 2,473 MW. It is estimated that the dam is capable of producing no more than 650–750 MW, because two-thirds of the facility's turbines do not work. There are plans to raise the Inga power station to 44,000 MW capacity by 2010. The African Development bank has agreed to supply $8 million towards it. The government has also agreed to strength the Inga-kolwezi and Inga-South Africa interconnections and to construct a 2nd power line to supply power to Kinshasa.\n\nIf harvested to its full potential, the hydroelectricity could provide power for the whole of Africa .\n\nIn 2007, the DROC had a gross production of public and self-produced electricity of 8,302 million kWh. The DROC imported 78 million kWh of electricity in 2007. The DROC is also an exporter of electric power. In 2003, electric power exports came to 1.3 TWh, with power transmitted to the Republic of Congo and its capital, Brazzaville, as well as to Zambia and South Africa. There were plans to build the Western Power Corridor (Westcor) to supply electricity from Inga III hydroelectric power plant to the Democratic Republic of the Congo, Angola, Namibia, Botswana and South Africa.\n\nThe national power company is Société nationale d'électricité (SNEL).\n\nOnly 6% of the country has access to electricity. As of 2003, 98.2% of electricity was produced by hydroelectric power.\n\nThe DROC has crude oil reserves that are second only to Angola's in southern Africa. As of 2009, the DROC’s crude oil reserves came to . In 2008, the DROC produced of oil per day and consumed . As of 2007, the DROC exported and imported .\n\nIn 2007, the DROC produced 836,000 metric tons of crude petroleum, exported 836,000 metric tons and had a reserve of 25,000,000 metric tons. The DROC had no refining capacity as of January 1, 2005, and must import refined petroleum products. In 2002, imports of refined petroleum products totaled .\n\nOil product imports consist of gasoline, jet fuel, kerosene, aviation gas, fuel oil, and liquefied petroleum gas. Oil products are exported and imported by Cohydro and Dalbit Petroleum. Dalbit Petroleum is a Kenya based energy company that supplies products to Lubumbashi and North Eastern DRC. As of 2008, the DROC had natural gas reserves of 991.1 million cu m. There was no production, consumption or importation or exportation of natural gas.\nGalaxy Moriah Oil is the government contracted supplier of oil for the DROC.\n\nyukyll;0\n\nAs of July 2005, the DROC is reported to have coal reserves of 97 million short tons. Domestic coal production and consumption in 2003 totaled 0.11 million short tons and 0.26 million shorts tons, respectively.\n\nICTs for climate change mitigation\n\nOne of the UN Millennium Development Goals is to make the benefits of new technologies - especially information and communications technologies (ICTs) – available to both industrialized nations and developing regions. In light of these goals, several projects have been founded by the International Telecommunication Union (ITU), Organisation for Economic Co-operation and Development (OECD), World Wide Fund for Nature (WWF), and other organisations in order to explore ICTs and climate change.\n\nClimate Change Legislation\n\nDRC has no national climate change policy and strategy which can present the DRC’s current and future efforts to effectively address its climate change vulnerability and adaptation. It currently relies on environment-related policies and action plans to implement climate change initiatives and activities. Nevertheless, several NGOs and donor agencies have been active in the DRC to develop an administrative structure to address the needs of environmental protection and natural resources management.\n\nThe DRC is in a very high level sun belt that makes the installation of photovoltaic systems and the use of thermal solar systems viable throughout the country. Currently there are 836 solar power systems, with a total power of 83 kW, located in Equateur (167), Katanga (159), Nord-Kivu (170), the two Kasaï provinces (170), and Bas-Congo (170). There is also the 148 Caritas network system, with a total power of 6.31 kW7. The potential for further solar development is high.\n\nThe DRC has a wide diversity of natural resources, allowing it to consider a significant growth in hydro, wind and solar energy. It has been called \"a virtual continent.\" For the first time in Africa, the Democratic Republic of Congo (DRC) has adopted an interactive atlas of renewable energy sources.\n\nThis Atlas was created by the UNDP, Netherlands Development Organization SNV, and the Congolese Ministry of Water Resources and Electricity. It has 600 interactive maps and informs policymaking on decentralizing energy and encourages further renewable energy investments.\n"}
{"id": "29563387", "url": "https://en.wikipedia.org/wiki?curid=29563387", "title": "Energy–depth relationship in a rectangular channel", "text": "Energy–depth relationship in a rectangular channel\n\nIn open channel flow, specific energy (E) is the energy length, or head, relative to the channel bottom. Specific energy is expressed in terms of kinetic energy, and potential energy, and internal energy. The Bernoulli equation, which originates from a control volume analysis, is used to describe specific energy relationships in fluid dynamics. The form of Bernoulli’s equation discussed here assumes the flow is incompressible and steady. The three energy components in Bernoulli's equation are elevation, pressure and velocity. However, since with open channel flow, the water surface is open to the atmosphere, the pressure term between two points has the same value and is therefore ignored. Thus, if the specific energy and the velocity of the flow in the channel are known, the depth of flow can be determined. This relationship can be used to calculate changes in depth upstream or downstream of changes in the channel such as steps, constrictions, or control structures. It is also the fundamental relationship used in the standard step method to calculate how the depth of a flow changes over a reach from the energy gained or lost due to the slope of the channel.\n\nWith the pressure term neglected, energy exists in two forms, potential and kinetic. Assuming all the fluid particles are moving at the same velocity, the general expression for kinetic energy applies (KE = ½mv). This general expression can be written in terms of kinetic energy per unit weight of fluid,\n\nThe kinetic energy, in feet, is represented as the velocity head,\n\nThe fluid particles also have potential energy, which is associated with the fluid elevation above an arbitrary datum. For a fluid of weight (ρg) at a height y above the established datum, the potential energy is wy. Thus, the potential energy per unit weight of fluid can be expressed as simply the height above the datum,\n\nCombining the energy terms for kinetic and potential energies along with influences due to pressure and headloss, results in the following equation:\n\nAs the fluid moves downstream, energy is lost due to friction. These losses can be due to channel bed roughness, channel constrictions, and other flow structures. Energy loss due to friction is neglected in this analysis.\n\nEquation 4 evaluates the flow at two locations: point 1 (upstream) and point 2 (downstream). As mentioned previously, the pressure at locations 1 and 2 both equal atmospheric pressure in open-channel flow, therefore the pressure terms cancel out. Headloss due to friction is also neglected when determining specific energy; therefore this term disappears as well. After these cancelations, the equation becomes,\n\nand the total specific energy at any point in the system is,\n\nTo evaluate the kinetic-energy term, the fluid velocity is needed. The volumetric discharge, Q is typically used in open channel flow calculations. For rectangular channels, the unit discharge is also used, and many alternative formulas for rectangular channels use this term instead of v or Q. In US customary units, Q is in ft/sec. and q is in ft/sec.\n\nEquation 6 can then be rewritten for rectangular channels as,\n\nFor a given discharge, the specific energy can be calculated for various flow depths and plotted on an E–y diagram. A typical E–y diagram is shown below.\n\nThree different q values are plotted on the specific energy diagram above. The unit discharges increase from left to right, meaning that q < q < q. There is a distinct asymptotic relationship as the top part of the curve approaches the E = y line and the bottom part of the curve tends toward the x-axis. Also shown are the critical energy or minimum energy, E and the corresponding critical depth value, y. The values shown are for the q discharge only, but unique critical values exist for any discharge.\n\nThe critical depth value mentioned in the E–y diagram section above is mathematically represented by the ratio of the fluid velocity to the velocity of a small amplitude gravity wave. This ratio is called the Froude number.\n\nThe critical depth has a Froude number equal to one and corresponds to the minimum energy a flow can possess for a given discharge. Not all flows are critical, so what about Froude numbers not equal to one? Froude numbers below one are considered subcritical and Froude numbers above one are considered supercritical.\n\nPhysically, subcritical flow is deep and the velocities are slow. This means subcritical flow has high potential energy and low kinetic energy. Supercritical flow on the other hand tends to be shallow and the velocities are fast. Supercritical flow has low potential energy and high kinetic energy.\n\nIf we refer back to the E–y diagram, it is seen that a line passes through the critical value on each successive discharge curve. This line corresponds to formula_13.\n\nDepth values on the E–y curve greater than the critical depth correspond to subcritical flow depths. Likewise, values less than the critical depth correspond to supercritical flow depths.\n\nFor rectangular channels, the critical depth can be calculated by taking the derivative of the energy equation and setting it equal to zero. The energy associated with the critical depth is found by placing the critical depth expression into the specific energy equation. The critical energy expression is demonstrated graphically by the line formula_14, which connects critical depth values.\n\nFor a given energy value and discharge, there generally exists two possible corresponding flow depths. In the diagram above, the alternate depths are labeled y and y and correspond to the subcritical and supercritical flow regions, respectively. This holds true for all energy values greater than critical energy. This relationship does not hold true at critical energy where only the critical depth, y, is possible and for energy values less than critical depth’s energy where there are no positive depths. The following equation can be used to solve for one alternate depth in terms of the other in rectangular channels. The values for y and y are interchangeable.\n\nIn the open-channel flow of rectangular channels, the alternate depth equation relates the upstream(y) and downstream(y) steady-state flow depths of a flow that encounters a control device, such as a sluice gate, which conserves energy for a given discharge.\n\nThe alternate depth equation can be derived in a similar fashion as the conjugate depth equation. In the open-channel flow of rectangular channels, the conjugate depth equation relates the upstream(y) and downstream(y) steady-state flow depths for a flow that encounters a pure hydraulic jump, which conserves momentum for a given discharge. The mathematical derivation of the conjugate depth equation can be a useful tool in understanding the derivation of the alternate depth equation, please refer to the above link for a more in-depth discussion of its derivation.\n\nAnother important concept that can be applied towards the derivation the alternate depth equation arises from the comparison of the dimensionless momentum function to the dimensionless specific energy function. It can be seen that the dimensionless momentum function (M) has the identical functional relationship as the dimensionless specific energy function (E) when both are properly transformed. (Henderson 1966). From this comparison it can be observed that any result that applies to the dimensionless momentum equation (M) would likewise apply to the dimensionless specific energy equation (E). From this duality concept we can determine the analog to the conjugate depth equation for the specific energy equation to provide an analytical relationship between alternate depths y and y. Below gives the mathematical derivations behind this concept:\n\nThrough the comparison of the dimensionless momentum and specific-energy functions it can be observed that our final dimensionless specific-energy equation is identical to the functional relationship as was determined for the dimensionless-momentum equation:\n\nTherefore, any result that applies to the dimensionless-momentum equation would likewise apply to the dimensionless specific-energy equation, provided the transform is used.\n\nUsing the conjugate depth equation and the duality concept between the dimensionless forms of the momentum (M) and specific energy (E) functions an analytical relationship between alternate depths can be obtained.\n\nNotice that because of the symmetry of the original conjugate depth equation, the resulting dimensionless alternate depth equation applies regardless of the Froude number at location 1. That is, y may correspond to either supercritical or subcritical flow conditions. The alternate depth relationship will yield the alternate depth to y corresponding to the opposite flow regime in either case.\n\nTo the best of the author’s knowledge, this final result for the alternate depth relationship appears in no textbook and is an original contribution by Dr. Glenn E. Moglen of Virginia Tech and appears on this website through the assistance of Paul Le Bel and the CEE 5984 Open Channel Flow course at Virginia Tech.\n\nThe concept of alternate depths can be demonstrated with a sluice gate example. Sluice gates are used to control the flow of water in open channels and under ideal conditions, where friction is ignored, they conserve energy for a given discharge.\n\nWater flows in a rectangular channel that contains a sluice gate. The upstream depth of flow, y is 5.0 ft, the sluice gate opening is 1.0 ft, and the unit discharge is, formula_51. What is the flow depth downstream of the sluice gate, y?\n\nSince energy is conserved at a sluice gate, the upstream and downstream energies are equal, or formula_52. The specific energy equation (eq. 8), alternate depth equation (eq. 16), and an E–y diagram are used to demonstrate how to solve this problem.\n\nCompare specific energies at upstream(y) and downstream(y) depths to demonstrate conservation of energy (formula_52) at a sluice gate:\n\nTherefore, formula_57 and energy is conserved.\n\n"}
{"id": "9302344", "url": "https://en.wikipedia.org/wiki?curid=9302344", "title": "European Nuclear Energy Tribunal", "text": "European Nuclear Energy Tribunal\n\nThe European Nuclear Energy Tribunal (ENET) is an international tribunal, established 1 January 1960, that operates under the auspices of the Organisation for Economic Co-operation and Development (OECD). Its member states are Austria, Belgium, Denmark, France, Germany, Ireland, Italy, Luxembourg, Netherlands, Norway, Portugal, Spain, Sweden, Switzerland, Turkey, and the United Kingdom.\n\nThe purpose of the tribunal is to hear cases concerning liability over nuclear accidents. Formerly it also had the role of hearing cases concerning the violation of the European regional nuclear safeguards system operated by the OECD but that jurisdiction was suspended in the 1970s due to its duplication of the IAEA and the Euratom systems.\n\nThe tribunal consists of seven judges appointed to five-year terms.\n\nThe judges appointed for the term of office commencing 24 May 2006 are: Dr. Peter Baumann (Austria), Ms. Mia Wouters (Belgium), Mr. Olivier Talevski (Denmark), Ms. Marie-Claire Guyader (France), Professor Armin von Bogdandy (Germany), Mr. Engelbertus Albertus [Bert] Maan (Netherlands) and Professor Vaughan Lowe (United Kingdom).\n\nThe President of the Tribunal is currently Professor Armin von Bogdandy.\n\nThe Registrar of the Tribunal is currently Ms. Julia Schwartz, Head of Legal Affairs at the NEA. The seat of the European Nuclear Energy Tribunal is in Paris, France, at the OECD offices.\n\nIn the over fifty years of its existence the tribunal has never been presented with a case.\n\n"}
{"id": "22217002", "url": "https://en.wikipedia.org/wiki?curid=22217002", "title": "Fiber pull-out", "text": "Fiber pull-out\n\nFiber pull-out is one of the failure mechanisms in fiber-reinforced composite materials. Other forms of failure include delamination, intralaminar matrix cracking, longitudinal matrix splitting, fiber/matrix debonding, and fiber fracture. The cause of fiber pull-out and delamination is weak bonding.\n\nWork for debonding, formula_1 \n\nwhere\n\nIn ceramic matrix composite material this mechanism is not a failure mechanism, but essential for its fracture toughness, which is several factors above that of conventional ceramics.\n\nThe figure is an example of how a fracture surface of this material looks like. The strong fibers form bridges over the cracks before they fail at elongations around 0.7%, and thus prevent brittle rupture of the material at 0.05%, especially under thermal shock conditions. This allows using this type of ceramics for heat shields applied for the re-entry of space vehicles, for disk brakes and slide bearing components.\n"}
{"id": "31065020", "url": "https://en.wikipedia.org/wiki?curid=31065020", "title": "Guy Ferri", "text": "Guy Ferri\n\nGuy Ferri (July 7, 1922 – 1991) was a United States diplomat and United Nations official who served as a State Department foreign service officer from 1954–1972 and as the deputy representative of the International Atomic Energy Agency to the United Nations from 1972 to 1983.\n\nBorn as Gaetano Ferri in Loreto Aprutino, Italy on July 7, 1922 to Assunta and Pasquale Ferri. The Ferri family immigrated to the United States when he was a young child and settled in Hamburg, Pennsylvania.\n\nUpon completion of high school in Hamburg, Ferri then served in the United States Army as a sergeant in North Africa and Europe during World War II and later served as a captain in the United States Air Force Reserve. Ferri completed his undergraduate studies at Georgetown University's Edmund A. Walsh School of Foreign Service and earned a master's degree in Public Administration from Harvard University. He married Teresa Bursley in 1955.\n\nDuring his career as a foreign service officer with the State Department he was stationed with his family in numerous United States embassies including Buenos Aires; Saigon, South Vietnam, and Asuncion, Paraguay, as well as a multitude of assignments in Washington D.C. He was fluent in English, Italian, Spanish, and French.\n\nFerri resided in Rye, NY during his time as deputy representative of the International Atomic Energy Agency to the United Nations and then retired with his wife to Palm Coast, Florida.\n"}
{"id": "10663469", "url": "https://en.wikipedia.org/wiki?curid=10663469", "title": "Hayward Regional Shoreline", "text": "Hayward Regional Shoreline\n\nHayward Regional Shoreline is a regional park located on the shores of the San Francisco Bay in Hayward, California. It is part of the East Bay Regional Parks system. The 1,713 acre park extends to the shores of San Lorenzo. Part of the park is former commercial salt flats purchased in 1996. A former landfill, now capped with soil and plants, is located in the park. The park includes the 250 acre tidal wetland, Cogswell Marsh, and the 364 acre Oro Loma Marsh (constructed in 1997). Located to the south of the park is the Hayward Shoreline Interpretive Center, which provides information on the Bay shore habitats. The San Francisco Bay Trail runs through the park, which connects the park with San Lorenzo Creek.\n\n"}
{"id": "635463", "url": "https://en.wikipedia.org/wiki?curid=635463", "title": "Hydrochloride", "text": "Hydrochloride\n\nIn chemistry, a hydrochloride is an acid salt resulting, or regarded as resulting, from the reaction of hydrochloric acid with an organic base (e.g. an amine). An alternative name is chlorhydrate, which comes from French. An archaic alternative name is muriate, derived from hydrochloric acid's ancient name: muriatic acid.\n\nFor example, reacting pyridine (CHN) with hydrochloric acid (HCl) yields its hydrochloride salt, pyridinium chloride. The molecular formula is either written as CHN·HCl or as CHNH Cl.\n\nConverting insoluble amines into hydrochlorides is a common way to make them water-soluble. This characteristic is particularly desirable for substances used in medications. The European Pharmacopoeia lists more than 200 hydrochlorides as active ingredients in medications. These hydrochlorides, compared to free bases, may more readily dissolve in the gastrointestinal tract and be able to be absorbed into the bloodstream more quickly. In addition, many hydrochlorides of amines have a longer shelf-life than their respective free bases.\n\n"}
{"id": "43316912", "url": "https://en.wikipedia.org/wiki?curid=43316912", "title": "International Renewable Energy Agency", "text": "International Renewable Energy Agency\n\nThe International Renewable Energy Agency (IRENA) is an intergovernmental organization to promote adoption and sustainable use of renewable energy. It was founded in 2009 and its statute entered into force on 8 July 2010. The agency is headquartered in Abu Dhabi. The Director-General of IRENA is Adnan Amin, a national of Kenya. IRENA is an official United Nations observer.\n\nThe first suggestions for an international renewable agency is based on the 1980 Brandt Report activities. NGOs and industry lobbying groups like Eurosolar, the World Council for Renewable Energy (WCRE) and the World Wind Energy Association have promoted IRENA since several decades. In 1990, the Austrian government of Franz Vranitzky suggested a reneweables agency to the UN. One of the drivers was Hermann Scheer, a German politician and lobbyist which was acting as president of EUROSOLAR and chair of WCRE.\n\nOn 15 June, at their annual event, the WWEA gave their 2010 World Wind Energy Award to the Founding member States of IRENA. They stated: \"The creation of IRENA can be seen as the most important decision ever taken on the global level in favour of renewable energy. The founding of IRENA sent out a very strong signal to the world community that renewable energy will have to play and will play a key role in the future energy supply all over the world. With the Award, WWEA would also like to indicate that WWEA is committed to work closely with IRENA and will continue to give its full support.\"\n\nSince 1981, several meetings took place to discuss the formation of IRENA. The Preparatory Conference for founding IREA was held on 10 and 11 April 2008 with 54 countries participating. Here, government representatives discussed the objectives, activities, finances, and organizational structure of IRENA. Participants expressed a need to begin a swift transition to a more secure, sustainable renewable energy economy with the assistance of an international body.\n\nThe Founding Conference of the International Renewable Energy Agency was held in Bonn, Germany, on 26 January 2009. 75 countries signed the Agency's statute. The statute entered into force on 8 July 2010, 30 days after the 25th country deposited its instrument of ratification. The Founding Conference established the Preparatory Commission for IRENA, which consists of all signatory states.\n\nDuring the first session of the Preparatory Commission in Bonn on 27 January 2009, the signatory countries adopted criteria and procedures for selecting IRENA's Interim Director-General and its interim headquarters. An Administrative Committee was created to assist the Commission in preparing its second session. The Administrative Committee prepared draft proposals for an interim work programme and budget as well as for interim staff regulations and interim financial rules. Nominations for the Interim Director-General and the interim headquarters were submitted by 30 April 2009.\n\nThe second session of the Preparatory Commission met in Sharm el-Sheikh, Egypt, on 29–30 June 2009, to elect the Interim Director General and decide the location of IRENA's interim headquarters. It was decided, that the interim headquarters will be located in Abu Dhabi, United Arab Emirates. The UAE thus became the first developing country to host a major international organisation. In addition, an innovation and technology center will be located in Bonn, and an office dedicated to liaising with the United Nations and other international institutions will be located in Vienna. Hélène Pelosse was elected as the Interim Director-General. The second session of the Preparatory Commission also adopted an interim work programme and budget as well as for interim staff regulations and interim financial rules.\n\nThe fourth session of the Preparatory Commission on 24–25 October 2010 in Abu Dhabi appointed Kenyan representative Adnan Amin, Deputy Interim Director-General, to perform functions of Interim Director-General after resignation of Hélène Pelosse. On 4 April 2011, Adnan Amin was sworn in as the first Director-General.\n\nThe agency has staged a number of events bringing together member states for interaction on ways and means of furthering renewable energy, and conducted significant research and development into viable solutions for the future. On 8 September 2014, IRENA published a notable report on its works titled \"REthinking energy\", which encouraged \"speedier adoption of renewable energy technologies,\" as \"the most feasible route to reduce carbon emissions and avoid catastrophic climate change.\" The study set out to gauge the global power sector and establish how technological advances, economic growth and climate change are transforming it. \"A convergence of social, economic and environmental forces are transforming the global energy system as we know it. But if we continue on the path we are currently on and fuel our growing economies with outmoded ways of thinking and acting, we will not be able avoid the most serious impacts of climate change,\" Director-General Amin said at a function to release the report.\n\nThe Ninth Meeting of International Renewable Energy Agency Council was held on 10–11 June 2015 at Abu Dhabi.\n\nIRENA aims to become the main driving force in promoting a transition towards the use of renewable energy on a global scale:\nActing as the global voice for renewable energies, IRENA will provide practical advice and support for both industrialised and developing countries, help them improve their regulatory frameworks and build capacity. The agency will facilitate access to all relevant information including reliable data on the potential of renewable energy, best practices, effective financial mechanisms and state-of-the-art technological expertise.\nIRENA provides advice and support to governments on renewable energy policy, capacity building, and technology transfer. IRENA will also co-ordinate with existing renewable energy organizations, such as REN21.\n\nAs of October 2018, 159 states and the European Union are members of IRENA, and a further 26 are in the process of accession.\n\nHélène Pelosse, former Interim Director General of IRENA, met with UN Secretary General Ban Ki-moon during 2009 Climate week in New York City, and together with Mr Ban explored future fields of cooperation between IRENA and various UN bodies. IRENA also seeks to cooperate with the UN and associated organisations like the United Nations University, UNESCO, the World Bank, GEF, UNIDO, UNDP, UNEP, and WTO in the areas of education and training, financing, access to energy, potential studies and trade.\n\nThe first director-general of IRENA is the current Director General Adnan Z. Amin. He was elected to the post in April 2011. He previously spent more than 25 years in the development of international environment and sustainable development policy, and worked in political, management, and interagency coordination areas of the United Nations. He was Head of the UN System Chief Executives Board for Coordination (CEB) Secretariat.\n\n"}
{"id": "15630", "url": "https://en.wikipedia.org/wiki?curid=15630", "title": "James Cook", "text": "James Cook\n\nCaptain James Cook (7 November 172814 February 1779) was a British explorer, navigator, cartographer, and captain in the Royal Navy. Cook made detailed maps of Newfoundland prior to making three voyages to the Pacific Ocean, during which he achieved the first recorded European contact with the eastern coastline of Australia and the Hawaiian Islands, and the first recorded circumnavigation of New Zealand.\n\nCook joined the British merchant navy as a teenager and joined the Royal Navy in 1755. He saw action in the Seven Years' War and subsequently surveyed and mapped much of the entrance to the Saint Lawrence River during the siege of Quebec. This helped bring Cook to the attention of the Admiralty and Royal Society. This acclaim came at a crucial moment in both Cook's career and the direction of British overseas exploration, and led to his commission in 1766 as commander of for the first of three Pacific voyages.\n\nIn three voyages, Cook sailed thousands of miles across largely uncharted areas of the globe. He mapped lands from New Zealand to Hawaii in the Pacific Ocean in greater detail and on a scale not previously achieved. As he progressed on his voyages of discovery, he surveyed and named features, and he recorded islands and coastlines on European maps for the first time. He displayed a combination of seamanship, superior surveying and cartographic skills, physical courage, and an ability to lead men in adverse conditions.\n\nCook was attacked and killed in 1779 during his third exploratory voyage in the Pacific while attempting to kidnap Kalaniʻōpuʻu, a Hawaiian chief, in order to reclaim a cutter stolen from one of his ships. He left a legacy of scientific and geographical knowledge which influenced his successors well into the 20th century, and numerous memorials worldwide have been dedicated to him.\n\nJames Cook was born on 7 November 1728 (N.S.) in the village of Marton in Yorkshire and baptised on 14 November (N.S.) in the parish church of St Cuthbert, where his name can be seen in the church register. He was the second of eight children of James Cook, a Scottish farm labourer from Ednam in Roxburghshire, and his locally born wife, Grace Pace, from Thornaby-on-Tees. In 1736, his family moved to Airey Holme farm at Great Ayton, where his father's employer, Thomas Skottowe, paid for him to attend the local school. In 1741, after five years' schooling, he began work for his father, who had been promoted to farm manager. For leisure, he would climb a nearby hill, Roseberry Topping, enjoying the opportunity for solitude. Cooks' Cottage, his parents' last home, which he is likely to have visited, is now in Melbourne, Australia, having been moved from England and reassembled, brick by brick, in 1934.\n\nIn 1745, when he was 16, Cook moved to the fishing village of Staithes, to be apprenticed as a shop boy to grocer and haberdasher William Sanderson. Historians have speculated that this is where Cook first felt the lure of the sea while gazing out of the shop window.\n\nAfter 18 months, not proving suitable for shop work, Cook travelled to the nearby port town of Whitby to be introduced to friends of Sanderson's, John and Henry Walker. The Walkers, who were Quakers, were prominent local ship-owners in the coal trade. Their house is now the Captain Cook Memorial Museum. Cook was taken on as a merchant navy apprentice in their small fleet of vessels, plying coal along the English coast. His first assignment was aboard the collier \"Freelove\", and he spent several years on this and various other coasters, sailing between the Tyne and London. As part of his apprenticeship, Cook applied himself to the study of algebra, geometry, trigonometry, navigation and astronomy—all skills he would need one day to command his own ship.\n\nHis three-year apprenticeship completed, Cook began working on trading ships in the Baltic Sea. After passing his examinations in 1752, he soon progressed through the merchant navy ranks, starting with his promotion in that year to mate aboard the collier brig \"Friendship\". In 1755, within a month of being offered command of this vessel, he volunteered for service in the Royal Navy, when Britain was re-arming for what was to become the Seven Years' War. Despite the need to start back at the bottom of the naval hierarchy, Cook realised his career would advance more quickly in military service and entered the Navy at Wapping on 17 June 1755.\n\nCook married Elizabeth Batts, the daughter of Samuel Batts, keeper of the Bell Inn in Wapping and one of his mentors, on 21 December 1762 at St Margaret's Church, Barking, Essex. The couple had six children: James (1763–1794), Nathaniel (1764–1780, lost aboard which foundered with all hands in a hurricane in the West Indies), Elizabeth (1767–1771), Joseph (1768–1768), George (1772–1772) and Hugh (1776–1793, who died of scarlet fever while a student at Christ's College, Cambridge). When not at sea, Cook lived in the East End of London. He attended St Paul's Church, Shadwell, where his son James was baptised. Cook has no direct descendants—all of his children died before having children of their own.\n\nCook's first posting was with , serving as able seaman and master's mate under Captain Joseph Hamar for his first year aboard, and Captain Hugh Palliser thereafter. In October and November 1755, he took part in \"Eagle\"'s capture of one French warship and the sinking of another, following which he was promoted to boatswain in addition to his other duties. His first temporary command was in March 1756 when he was briefly master of \"Cruizer\", a small cutter attached to \"Eagle\" while on patrol.\n\nIn June 1757 Cook formally passed his master's examinations at Trinity House, Deptford, qualifying him to navigate and handle a ship of the King's fleet. He then joined the frigate as master under Captain Robert Craig.\n\nDuring the Seven Years' War, Cook served in North America as master aboard the fourth-rate Navy vessel . With others in \"Pembroke\"s crew, he took part in the major amphibious assault that captured the Fortress of Louisbourg from the French in 1758, and in the siege of Quebec City in 1759. Throughout his service he demonstrated a talent for surveying and cartography and was responsible for mapping much of the entrance to the Saint Lawrence River during the siege, thus allowing General Wolfe to make his famous stealth attack during the 1759 Battle of the Plains of Abraham.\n\nCook's surveying ability was also put to use in mapping the jagged coast of Newfoundland in the 1760s, aboard . He surveyed the northwest stretch in 1763 and 1764, the south coast between the Burin Peninsula and Cape Ray in 1765 and 1766, and the west coast in 1767. At this time, Cook employed local pilots to point out the \"rocks and hidden dangers\" along the south and west coasts. During the 1765 season, four pilots were engaged at a daily pay of 4 shillings each: John Beck for the coast west of \"Great St Lawrence\", Morgan Snook for Fortune Bay, John Dawson for Connaigre and Hermitage Bay, and John Peck for the \"Bay of Despair\".\n\nHis five seasons in Newfoundland produced the first large-scale and accurate maps of the island's coasts and were the first scientific, large scale, hydrographic surveys to use precise triangulation to establish land outlines. They also gave Cook his mastery of practical surveying, achieved under often adverse conditions, and brought him to the attention of the Admiralty and Royal Society at a crucial moment both in his career and in the direction of British overseas discovery. Cook's map were used into the 20th century, with copies being referenced by those sailing Newfoundland's waters for 200 years.\n\nFollowing on from his exertions in Newfoundland, Cook wrote that he intended to go not only \"farther than any man has been before me, but as far as I think it is possible for a man to go\".\n\nOn 25 May 1768, the Admiralty commissioned Cook to command a scientific voyage to the Pacific Ocean. The purpose of the voyage was to observe and record the 1769 transit of Venus across the Sun which, when combined with observations from other places, would help to determine the distance of the Sun. Cook, at age 39, was promoted to lieutenant to grant him sufficient status to take the command. For its part, the Royal Society agreed that Cook would receive a one hundred guinea gratuity in addition to his Naval pay.\n\nThe expedition sailed aboard , departing England on 26 August 1768. Cook and his crew rounded Cape Horn and continued westward across the Pacific, arriving Tahiti on 13 April 1769, where the observations of the Venus Transit were made. However, the result of the observations was not as conclusive or accurate as had been hoped. Once the observations were completed, Cook opened the sealed orders which were additional instructions from the Admiralty for the second part of his voyage: to search the south Pacific for signs of the postulated rich southern continent of \"Terra Australis\".\nCook then sailed to New Zealand and mapped the complete coastline, making only some minor errors. He then voyaged west, reaching the southeastern coast of Australia on 19 April 1770, and in doing so his expedition became the first recorded Europeans to have encountered its eastern coastline.\n\nOn 23 April, he made his first recorded direct observation of indigenous Australians at Brush Island near Bawley Point, noting in his journal: \"...and were so near the Shore as to distinguish several people upon the Sea beach they appear'd to be of a very dark or black Colour but whether this was the real colour of their skins or the C[l]othes they might have on I know not.\" On 29 April, Cook and crew made their first landfall on the mainland of the continent at a place now known as the Kurnell Peninsula. Cook originally christened the area as \"Stingray Bay\", but later he crossed this out and named it \"Botany Bay\" after the unique specimens retrieved by the botanists Joseph Banks and Daniel Solander. It is here that James Cook made first contact with an aboriginal tribe known as the Gweagal.\n\nAfter his departure from Botany Bay, he continued northwards. He stopped at Bustard Bay (now known as Seventeen Seventy) on 23 May 1770. On 24 May, Cook and Banks and others went ashore. Continuing north, on 11 June a mishap occurred when HMS \"Endeavour\" ran aground on a shoal of the Great Barrier Reef, and then \"nursed into a river mouth on 18 June 1770\". The ship was badly damaged, and his voyage was delayed almost seven weeks while repairs were carried out on the beach (near the docks of modern Cooktown, Queensland, at the mouth of the Endeavour River). The voyage then continued and at about midday on 22 August 1770, they reached the northernmost tip of the coast and, without leaving the ship, Cook named it Cape York. Leaving the east coast, Cook turned west and nursed his battered ship through the dangerously shallow waters of Torres Strait. Searching for a high vantage point, Cook saw a steep hill on a nearby island from the top of which he hoped to see 'a passage into the Indian Seas'. He climbed the hill with three others, including Joseph Banks. On seeing a navigable passage, he signalled the good news down to the men on the ship, who cheered loudly.\n\nCook later wrote that he had claimed possession of the east coast when up on that hill, and named the place 'Possession Island'. However, the Admiralty's instructions did not authorized Cook to annex New Holland (Australia) and therefore it is unlikely that any possession ceremony occurred that August. Importantly, Joseph Banks, who was standing beside Cook, does not mention any such episode or announcement in his journal. Cook re-wrote his journal on his arrival in Batavia (Jakarta) when he was confronted with the news that the Frenchman, Louis Bougainville, had sailed across the Pacific the previous year.\n\nIn his revised journal entry, Cook wrote that he had claimed the entire coastline that he had just explored as British territory. He returned to England via Batavia (modern Jakarta, Indonesia), where many in his crew succumbed to malaria, and then the Cape of Good Hope, arriving at the island of Saint Helena on 12 July 1771.\n\nCook's journals were published upon his return, and he became something of a hero among the scientific community. Among the general public, however, the aristocratic botanist Joseph Banks was a greater hero. Banks even attempted to take command of Cook's second voyage but removed himself from the voyage before it began, and Johann Reinhold Forster and his son Georg Forster were taken on as scientists for the voyage. Cook's son George was born five days before he left for his second voyage.\nShortly after his return from the first voyage, Cook was promoted in August 1771 to the rank of commander. In 1772, he was commissioned to lead another scientific expedition on behalf of the Royal Society, to search for the hypothetical Terra Australis. On his first voyage, Cook had demonstrated by circumnavigating New Zealand that it was not attached to a larger landmass to the south. Although he charted almost the entire eastern coastline of Australia, showing it to be continental in size, the Terra Australis was believed to lie further south. Despite this evidence to the contrary, Alexander Dalrymple and others of the Royal Society still believed that a massive southern continent should exist.\n\nCook commanded on this voyage, while Tobias Furneaux commanded its companion ship, . Cook's expedition circumnavigated the globe at an extreme southern latitude, becoming one of the first to cross the Antarctic Circle on 17 January 1773. In the Antarctic fog, \"Resolution\" and \"Adventure\" became separated. Furneaux made his way to New Zealand, where he lost some of his men during an encounter with Māori, and eventually sailed back to Britain, while Cook continued to explore the Antarctic, reaching 71°10'S on 31 January 1774.\n\nCook almost encountered the mainland of Antarctica but turned towards Tahiti to resupply his ship. He then resumed his southward course in a second fruitless attempt to find the supposed continent. On this leg of the voyage, he brought a young Tahitian named Omai, who proved to be somewhat less knowledgeable about the Pacific than Tupaia had been on the first voyage. On his return voyage to New Zealand in 1774, Cook landed at the Friendly Islands, Easter Island, Norfolk Island, New Caledonia, and Vanuatu.\n\nBefore returning to England, Cook made a final sweep across the South Atlantic from Cape Horn and surveyed, mapped, and took possession for Britain of South Georgia, which had been explored by the English merchant Anthony de la Roché in 1675. Cook also discovered and named Clerke Rocks and the South Sandwich Islands (\"Sandwich Land\"). He then turned north to South Africa and from there continued back to England. His reports upon his return home put to rest the popular myth of Terra Australis.\n\nCook's second voyage marked a successful employment of Larcum Kendall's K1 copy of John Harrison's H4 marine chronometer, which enabled Cook to calculate his longitudinal position with much greater accuracy. Cook's log was full of praise for this time-piece which he used to make charts of the southern Pacific Ocean that were so remarkably accurate that copies of them were still in use in the mid-20th century.\n\nUpon his return, Cook was promoted to the rank of post-captain and given an honorary retirement from the Royal Navy, with a posting as an officer of the Greenwich Hospital. He reluctantly accepted, insisting that he be allowed to quit the post if an opportunity for active duty should arise. His fame extended beyond the Admiralty; he was made a Fellow of the Royal Society and awarded the Copley Gold Medal for completing his second voyage without losing a man to scurvy. Nathaniel Dance-Holland painted his portrait; he dined with James Boswell; he was described in the House of Lords as \"the first navigator in Europe\". But he could not be kept away from the sea. A third voyage was planned, and Cook volunteered to find the Northwest Passage. He travelled to the Pacific and hoped to travel east to the Atlantic, while a simultaneous voyage travelled the opposite route.\n\nOn his last voyage, Cook again commanded HMS \"Resolution\", while Captain Charles Clerke commanded . The voyage was ostensibly planned to return the Pacific Islander Omai to Tahiti, or so the public was led to believe. The trip's principal goal was to locate a Northwest Passage around the American continent. After dropping Omai at Tahiti, Cook travelled north and in 1778 became the first European to begin formal contact with the Hawaiian Islands. After his initial landfall in January 1778 at Waimea harbour, Kauai, Cook named the archipelago the \"Sandwich Islands\" after the fourth Earl of Sandwich—the acting First Lord of the Admiralty.\n\nFrom the Sandwich Islands, Cook sailed north and then northeast to explore the west coast of North America north of the Spanish settlements in Alta California. He made landfall on the Oregon coast at approximately 44°30′ north latitude, naming his landing point Cape Foulweather. Bad weather forced his ships south to about 43° north before they could begin their exploration of the coast northward. He unknowingly sailed past the Strait of Juan de Fuca and soon after entered Nootka Sound on Vancouver Island. He anchored near the First Nations village of Yuquot. Cook's two ships remained in Nootka Sound from 29 March to 26 April 1778, in what Cook called Ship Cove, now Resolution Cove, at the south end of Bligh Island. Relations between Cook's crew and the people of Yuquot were cordial but sometimes strained. In trading, the people of Yuquot demanded much more valuable items than the usual trinkets that had worked in Hawaii. Metal objects were much desired, but the lead, pewter, and tin traded at first soon fell into disrepute. The most valuable items which the British received in trade were sea otter pelts. During the stay, the Yuquot \"hosts\" essentially controlled the trade with the British vessels; the natives usually visited the British vessels at Resolution Cove instead of the British visiting the village of Yuquot at Friendly Cove.\n\nAfter leaving Nootka Sound, Cook explored and mapped the coast all the way to the Bering Strait, on the way identifying what came to be known as Cook Inlet in Alaska. In a single visit, Cook charted the majority of the North American northwest coastline on world maps for the first time, determined the extent of Alaska, and closed the gaps in Russian (from the west) and Spanish (from the south) exploratory probes of the northern limits of the Pacific.\n\nBy the second week of August 1778, Cook was through the Bering Strait, sailing into the Chukchi Sea. He headed northeast up the coast of Alaska until he was blocked by sea ice. His furthest north was 70 degrees 44 minutes. Cook then sailed west to the Siberian coast, and then southeast down the Siberian coast back to the Bering Strait. By early September 1778 he was back in the Bering Sea to begin the trip to the Sandwich (Hawaiian) Islands. He became increasingly frustrated on this voyage and perhaps began to suffer from a stomach ailment; it has been speculated that this led to irrational behaviour towards his crew, such as forcing them to eat walrus meat, which they had pronounced inedible.\n\nCook returned to Hawaii in 1779. After sailing around the archipelago for some eight weeks, he made landfall at Kealakekua Bay, on 'Hawaii Island', largest island in the Hawaiian Archipelago. Cook's arrival coincided with the \"Makahiki\", a Hawaiian harvest festival of worship for the Polynesian god Lono. Coincidentally the form of Cook's ship, HMS \"Resolution\", or more particularly the mast formation, sails and rigging, resembled certain significant artefacts that formed part of the season of worship. Similarly, Cook's clockwise route around the island of Hawaii before making landfall resembled the processions that took place in a clockwise direction around the island during the Lono festivals. It has been argued (most extensively by Marshall Sahlins) that such coincidences were the reasons for Cook's (and to a limited extent, his crew's) initial deification by some Hawaiians who treated Cook as an incarnation of Lono. Though this view was first suggested by members of Cook's expedition, the idea that any Hawaiians understood Cook to be Lono, and the evidence presented in support of it, were challenged in 1992.\n\nAfter a month's stay, Cook attempted to resume his exploration of the northern Pacific. Shortly after leaving Hawaii Island, however, the \"Resolution\"'s foremast broke, so the ships returned to Kealakekua Bay for repairs.\n\nTensions rose, and a number of quarrels broke out between the Europeans and Hawaiians at Kealakekua Bay. An unknown group of Hawaiians took one of Cook's small boats. The evening when the cutter was taken, the people had become \"insolent\" even with threats to fire upon them. Cook attempted to kidnap and ransom the King of Hawaiʻi, Kalaniʻōpuʻu.\n\nThe following day, 14 February 1779, Cook marched through the village to retrieve the king. Cook took the king (aliʻi nui) by his own hand and led him willingly away. One of Kalaniʻōpuʻu's favourite wives, Kanekapolei, and two chiefs approached the group as they were heading to boats. They pleaded with the king not to go. An old kahuna (priest), chanting rapidly while holding out a coconut, attempted to distract Cook and his men as a large crowd began to form at the shore. The king began to understand that Cook was his enemy. As Cook turned his back to help launch the boats, he was struck on the head by the villagers and then stabbed to death as he fell on his face in the surf. He was first struck on the head with a club by a chief named Kalaimanokahoʻowaha or Kanaʻina (namesake of Charles Kana'ina) and then stabbed by one of the king's attendants, Nuaa. The Hawaiians carried his body away towards the back of the town, still visible to the ship through their spyglass. Four marines, Corporal James Thomas, Private Theophilus Hinks, Private Thomas Fatchett and Private John Allen, were also killed and two others were wounded in the confrontation.\n\nThe esteem which the islanders nevertheless held for Cook caused them to retain his body. Following their practice of the time, they prepared his body with funerary rituals usually reserved for the chiefs and highest elders of the society. The body was disembowelled, baked to facilitate removal of the flesh, and the bones were carefully cleaned for preservation as religious icons in a fashion somewhat reminiscent of the treatment of European saints in the Middle Ages. Some of Cook's remains, thus preserved, were eventually returned to his crew for a formal burial at sea.\n\nClerke assumed leadership of the expedition and made a final attempt to pass through the Bering Strait. He died of tuberculosis on 22 August 1779 and John Gore, a veteran of Cook's first voyage, took command of \"Resolution\" and of the expedition. James King replaced Gore in command of \"Discovery\". The expedition returned home, reaching England in October 1780. After their arrival in England, King completed Cook's account of the voyage.\n\nDavid Samwell, who sailed with Cook on \"Resolution\", wrote of him: \"He was a modest man, and rather bashful; of an agreeable lively conversation, sensible and intelligent. In temper he was somewhat hasty, but of a disposition the most friendly, benevolent and humane. His person was above six feet high: and, though a good looking man, he was plain both in dress and appearance. His face was full of expression: his nose extremely well shaped: his eyes which were small and of a brown cast, were quick and piercing; his eyebrows prominent, which gave his countenance altogether an air of austerity.\"\n\nThe Australian Museum acquired its \"Cook Collection\" in 1894 from the Government of New South Wales. At that time the collection consisted of 115 artefacts collected on Cook's three voyages throughout the Pacific Ocean, during the period 1768–80, along with documents and memorabilia related to these voyages. Many of the ethnographic artifacts were collected at a time of first contact between Pacific Peoples and Europeans. In 1935 most of the documents and memorabilia were transferred to the Mitchell Library in the State Library of New South Wales. The provenance of the collection shows that the objects remained in the hands of Cook's widow Elizabeth Cook, and her descendants, until 1886. In this year John Mackrell, the great-nephew of Isaac Smith, Elizabeth Cook's cousin, organised the display of this collection at the request of the NSW Government at the Colonial and Indian Exhibition in London. In 1887 the London-based Agent-General for the New South Wales Government, Saul Samuel, bought John Mackrell's items and also acquired items belonging to the other relatives Reverend Canon Frederick Bennett, Mrs Thomas Langton, H.M.C. Alexander, and William Adams. The collection remained with the Colonial Secretary of NSW until 1894, when it was transferred to the Australian Museum.\n\nCook's 12 years sailing around the Pacific Ocean contributed much to European knowledge of the area. Several islands such as the Sandwich Islands (Hawaii) were encountered for the first time by Europeans, and his more accurate navigational charting of large areas of the Pacific was a major achievement.\n\nTo create accurate maps, latitude and longitude must be accurately determined. Navigators had been able to work out latitude accurately for centuries by measuring the angle of the sun or a star above the horizon with an instrument such as a backstaff or quadrant. Longitude was more difficult to measure accurately because it requires precise knowledge of the time difference between points on the surface of the earth. The Earth turns a full 360 degrees relative to the sun each day. Thus longitude corresponds to time: 15 degrees every hour, or 1 degree every 4 minutes.\n\nCook gathered accurate longitude measurements during his first voyage by his navigational skills, the help of astronomer Charles Green, and by using the newly published Nautical Almanac tables, via the lunar distance method—measuring the angular distance from the moon to either the sun during daytime or one of eight bright stars during night-time to determine the time at the Royal Observatory, Greenwich, and comparing that to his local time determined via the altitude of the sun, moon, or stars. On his second voyage, Cook used the K1 chronometer made by Larcum Kendall, which was the shape of a large pocket watch, in diameter. It was a copy of the H4 clock made by John Harrison, which proved to be the first to keep accurate time at sea when used on the ship \"Deptford's\" journey to Jamaica in 1761–62.\n\nCook succeeded in circumnavigating the world on his first voyage without losing a single man to scurvy, an unusual accomplishment at the time. He tested several preventive measures, but the most important was frequent replenishment of fresh food. It was for presenting a paper on this aspect of the voyage to the Royal Society that he was presented with the Copley Medal in 1776.\n\nEver the observer, Cook was the first European to have extensive contact with various people of the Pacific. He correctly postulated a link among all the Pacific peoples, despite their being separated by great ocean stretches (see Malayo-Polynesian languages). Cook theorised that Polynesians originated from Asia, which scientist Bryan Sykes later verified.\n\nIn New Zealand the coming of Cook is often used to signify the onset of colonisation.\n\nCook carried several scientists on his voyages; they made significant observations and discoveries. Two botanists, Joseph Banks and Swede Daniel Solander, were on the first voyage. The two collected over 3,000 plant species. Banks subsequently strongly promoted British settlement of Australia.\n\nArtists also sailed on Cook's first voyage. Sydney Parkinson was heavily involved in documenting the botanists' findings, completing 264 drawings before his death near the end of the voyage. They were of immense scientific value to British botanists. Cook's second expedition included William Hodges, who produced notable landscape paintings of Tahiti, Easter Island, and other locations.\n\nSeveral officers who served under Cook went on to distinctive accomplishments. William Bligh, Cook's sailing master, was given command of in 1787 to sail to Tahiti and return with breadfruit. Bligh is most known for the mutiny of his crew which resulted in his being set adrift in 1789. He later became governor of New South Wales, where he was the subject of another mutiny—the Rum Rebellion. George Vancouver, one of Cook's midshipmen, led a voyage of exploration to the Pacific Coast of North America from 1791 to 1794. In honour of his former commander, Vancouver's ship was named . George Dixon, who sailed under Cook on his third expedition, later commanded his own. Henry Roberts, a lieutenant under Cook, spent many years after that voyage preparing the detailed charts that went into Cook's posthumous atlas, published around 1784.\n\nCook's contributions to knowledge were internationally recognised during his lifetime. In 1779, while the American colonies were fighting Britain for their independence, Benjamin Franklin wrote to captains of colonial warships at sea, recommending that if they came into contact with Cook's vessel, they were to \"not consider her an enemy, nor suffer any plunder to be made of the effects contained in her, nor obstruct her immediate return to England by detaining her or sending her into any other part of Europe or to America; but that you treat the said Captain Cook and his people with all civility and kindness ... as common friends to mankind.\" Unknown to Franklin, Cook had met his death a month before this safe conduct \"passport\" was written.\n\nCook's voyages were involved in another unusual first. The first recorded circumnavigation of the world by an animal was by Cook's goat, who made that memorable journey twice; the first time on HMS \"Dolphin\", under Samuel Wallis, and then aboard \"Endeavour\". When they returned to England, Cook had the goat presented with a silver collar engraved with lines from Samuel Johnson: \"Perpetui, ambita bis terra, praemia lactis Haec habet altrici Capra secunda Jovis.\" (\"In fame scarce second to the nurse of Jove,/ This Goat, who twice the world had traversed round,/Deserving both her master's care and love,/Ease and perpetual pasture now has found.\") She was put to pasture on Cook's farm outside London and was reportedly admitted to the privileges of the Royal Naval Hospital at Greenwich. Cook's journal recorded the date of the goat's death: 28 March 1772.\n\nA U.S. coin, the 1928 Hawaiian Sesquicentennial half dollar carries Cook's image. Minted for the 150th anniversary of his discovery of the islands, its low mintage (10,008) has made this example of Early United States commemorative coins both scarce and expensive. The site where he was killed in Hawaii was marked in 1874 by a white obelisk set on of chained-off beach. This land, although in Hawaii, was deeded to the United Kingdom. A nearby town is named Captain Cook, Hawaii; several Hawaiian businesses also carry his name. The Apollo 15 Command/Service Module \"Endeavour\" was named after Cook's ship, , as was the . Another shuttle, \"Discovery\", was named after Cook's .\n\nThe first institution of higher education in North Queensland, Australia was named after him, with James Cook University opening in Townsville in 1970. Numerous institutions, landmarks and place names reflect the importance of Cook's contributions, including the Cook Islands, the Cook Strait, Cook Inlet, and the Cook crater on the Moon. Aoraki/Mount Cook, the highest summit in New Zealand, is named for him. Another Mount Cook is on the border between the U.S. state of Alaska and the Canadian Yukon Territory, and is designated Boundary Peak 182 as one of the official Boundary Peaks of the Hay–Herbert Treaty. A life-size statue of Cook upon a column stands in Hyde Park located in the centre of Sydney. A large aquatic monument is planned for Cook's landing place at Botany Bay, Sydney.\n\nOne of the earliest monuments to Cook in the United Kingdom is located at The Vache, erected in 1780 by Admiral Hugh Palliser, a contemporary of Cook and one-time owner of the estate. A huge obelisk was built in 1827 as a monument to Cook on Easby Moor overlooking his boyhood village of Great Ayton, along with a smaller monument at the former location of Cook's cottage. There is also a monument to Cook in the church of St Andrew the Great, St Andrew's Street, Cambridge, where his sons Hugh, a student at Christ's College, and James were buried. Cook's widow Elizabeth was also buried in the church and in her will left money for the memorial's upkeep. The 250th anniversary of Cook's birth was marked at the site of his birthplace in Marton, by the opening of the Captain Cook Birthplace Museum, located within Stewart Park (1978). A granite vase just to the south of the museum marks the approximate spot where he was born. Tributes also abound in post-industrial Middlesbrough, including a primary school, shopping square and the \"Bottle 'O Notes\", a public artwork by Claes Oldenburg, that was erected in the town's Central Gardens in 1993. Also named after Cook is the James Cook University Hospital, a major teaching hospital which opened in 2003 with a railway station serving it called James Cook opening in 2014.\nThe Royal Research Ship RRS \"James Cook\" was built in 2006 to replace the RRS \"Charles Darwin\" in the UK's Royal Research Fleet, and Stepney Historical Trust placed a plaque on Free Trade Wharf in the Highway, Shadwell to commemorate his life in the East End of London. In 2002 Cook was placed at number 12 in the BBC's poll of the 100 Greatest Britons.\n\n\n\n\n\n\n"}
{"id": "2828647", "url": "https://en.wikipedia.org/wiki?curid=2828647", "title": "Kinetite", "text": "Kinetite\n\nKinetite was an explosive material patented in 1884 by T. Petry and O. Fallenstein, It consisted of nitro-benzol thickened or gelatinised by the addition of some collodion-cotton incorporated with finely ground potassium chlorate and precipitated antimony sulphide. \nIt is an orange coloured, plastic mass, with the characteristic strong smell of nitro-benzol. It was manufactured by dissolving gun cotton in nitrobenzene.\n\nDeveloped as a safer alternative to dynamite, its manufacture and manipulation were claimed to be without danger as it required a very high temperature to ignite and, under ordinary circumstances be exploded by heat alone when unconfined. Instead, it detonated only under shock, and then only the part exposed to concussion. Largely manufactured in Germany, it was introduced to Australia in 1885 by Thomas Wilkins. In the same year it was reported to be both £5 per ton cheaper than dynamite, and more efficient for mining operations: In sinking a shaft, a given weight of dynamite enabled to be sunk in 114 shifts, and with the same weight of kinetite were sunk in 94 shifts.\n\nIt was largely unaffected by short immersion in water, when immersion was prolonged however the chlorate dissolved out, leaving a practically non-explosive residue. However, if exposed to moist and dry air alternately, the chlorate crystallised out on the surfaces rendering the explosive very sensitive.\n\nIn testing however it was found to be extremely sensitive to combined friction and percussion, and could be readily ignited by a glancing blow with wood. Found also to be chemically unstable, it was known to ignite spontaneously both in the laboratory \nand in a magazine. A report of the French commission on the use of explosives in the presence of fire-damp in mines concluded that \"The presence of chlorate of potash makes this substance too dangerous, for its use to be recommended.\"\n\nThe principle of the mixture was the enveloping of each particle of the salts in an elastic jelly composed of hydrocarbons and nitrogen. The jelly additionally supplied material to the explosion.\n\nAn 1887 analysis gave the following composition:\n"}
{"id": "48532100", "url": "https://en.wikipedia.org/wiki?curid=48532100", "title": "Laser schlieren deflectometry", "text": "Laser schlieren deflectometry\n\nLaser schlieren deflectometry (LSD) is a method for a high-speed measurement of the gas temperature in microscopic dimensions, in particular for temperature peaks under dynamic conditions at atmospheric pressure. The principle of LSD is derived from schlieren photography: a narrow laser beam is used to scan an area in a gas where changes in properties are associated with characteristic changes of refractive index. Laser schlieren deflectometry is claimed to overcome limitations of other methods regarding temporal and spatial resolution.\n\nThe theory of the method is analogous to the scattering experiment of Ernest Rutherford from 1911. However, instead of alpha particles scattered by gold atoms, here an optical ray is deflected by hot spots with unknown temperature.\nA general equation of LSD describes the dependence of the measured maximum deflection of the ray \"δ\" on the local maximum of the neutral gas temperature in the hot spot \"T\":\n\nwhere \"T\" is ambient temperature and \"δ\" is a calibration constant depending on the configuration of the experiment.\n\nLaser schlieren deflectometry has been used for investigation of the temperature dynamics, heat transfer and energy balance in a miniaturized kind of atmospheric-pressure plasma.\n\n"}
{"id": "45223395", "url": "https://en.wikipedia.org/wiki?curid=45223395", "title": "Library Oodi", "text": "Library Oodi\n\nThe Helsinki Central Library Oodi (, ), commonly referred to as Library Oodi, is an upcoming public library in Helsinki, Finland, expected to be inaugurated in late 2018. The library is under construction in the Töölönlahti district next to Helsinki Music Centre and Kiasma Museum of Contemporary Art.\n\nA bidding to build the library set out in 2012 was won by the Finnish architectural firm ALA Architects. Structural design is by Ramboll Finland, drawing on competence in Denmark and Great Britain. The library will be a three story building and will include a sauna and a ground floor movie theater.\n\nIn January 2015, the Helsinki City Council voted 75–8 to launch the building project. The estimated costs of the new library are 98 million euros, of which the state will pay 30 million in connection with the centenary of Finland's independence in 2017. The city of Helsinki has budgeted 66 million euros for the building.\n\nOn December 31, 2016, it was announced that the new library would be named \"Oodi\" (Ode). There are two official names for the library in English: \"Helsinki Central Library Oodi\" and \"Library Oodi, Helsinki.\"\n\n"}
{"id": "19185567", "url": "https://en.wikipedia.org/wiki?curid=19185567", "title": "List of glaciers of India", "text": "List of glaciers of India\n\nHimalayan region of India is home of some of the most notable glaciers in the world. This is a list of the notable glaciers in India. Most glaciers lie in the states of Sikkim, Jammu and Kashmir, Himachal Pradesh and Uttarakhand.\nFew glaciers are also found in Arunachal Pradesh.\n\nIn Arunachal Pradesh, glaciers are found in Greater Himalaya ranges which run along the Tibetan border. All peaks here rise above 4500 meters and are snow covered throughout the year.\nImportant glaciers:\n\n\n\n\n\n"}
{"id": "32343820", "url": "https://en.wikipedia.org/wiki?curid=32343820", "title": "List of solar-powered products", "text": "List of solar-powered products\n\nThe following is a list of products powered by sunlight, either directly or through electricity generated by solar panels.\n\n\nNew genius Solar Water Heater Module, first build with a black garden hose. Now, with a Solar Mat and a Solar Fountain. ( Foldable, Portable )\n12 Volt safe, Off-grid\nhttps://solarpoolheatersite.wordpress.com/\n\n"}
{"id": "12941444", "url": "https://en.wikipedia.org/wiki?curid=12941444", "title": "Local flatness", "text": "Local flatness\n\nIn topology, a branch of mathematics, local flatness is a property of a submanifold in a topological manifold of larger dimension. In the category of topological manifolds, locally flat submanifolds play a role similar to that of embedded submanifolds in the category of smooth manifolds. Local flatness and the topology of ridge networks is of importance in the study of crumpled structures with importance in materials processing and mechanical engineering.\n\nSuppose a \"d\" dimensional manifold \"N\" is embedded into an \"n\" dimensional manifold \"M\" (where \"d\" < \"n\"). If formula_1 we say \"N\" is locally flat at \"x\" if there is a neighborhood formula_2 of \"x\" such that the topological pair formula_3 is homeomorphic to the pair formula_4, with a standard inclusion of formula_5 as a subspace of formula_6. That is, there exists a homeomorphism formula_7 such that the image of formula_8 coincides with formula_5.\n\nThe above definition assumes that, if \"M\" has a boundary, \"x\" is not a boundary point of \"M\". If \"x\" is a point on the boundary of \"M\" then the definition is modified as follows. We say that \"N\" is locally flat at a boundary point \"x\" of \"M\" if there is a neighborhood formula_10 of \"x\" such that the topological pair formula_3 is homeomorphic to the pair formula_12, where formula_13 is a standard half-space and formula_5 is included as a standard subspace of its boundary. In more detail, we can set \nformula_15 and formula_16.\n\nWe call \"N\" locally flat in \"M\" if \"N\" is locally flat at every point. Similarly, a map formula_17 is called locally flat, even if it is not an embedding, if every \"x\" in \"N\" has a neighborhood \"U\" whose image formula_18 is locally flat in \"M\".\n\nLocal flatness of an embedding implies strong properties not shared by all embeddings. Brown (1962) proved that if \"d\" = \"n\" − 1, then \"N\" is collared; that is, it has a neighborhood which is homeomorphic to \"N\" × [0,1] with \"N\" itself corresponding to \"N\" × 1/2 (if \"N\" is in the interior of \"M\") or \"N\" × 0 (if \"N\" is in the boundary of \"M\").\n\n"}
{"id": "31818711", "url": "https://en.wikipedia.org/wiki?curid=31818711", "title": "Newton Stone", "text": "Newton Stone\n\nThe Newton Stone is a pillar stone, found in Aberdeenshire, Scotland. The stone contains two inscriptions, one, written in Ogham, but the second script has never been positively identified and many different decipherments or theories have been proposed since the 1860s.\n\nThe second script may have been added to the stone as recently as the late 18th or beginning of the 19th century.\n\nThe Newton Stone has been known since 1804 when the Earl of Aberdeen George Hamilton-Gordon discovered the stone by the opening up of a new road near Pitmachie Farm, Aberdeenshire, after local shepherds told him of a \"curious monument\" that sat there.\n\nThe stone was later taken and planted in the garden of Newton House, in the Parish of Culsamond about a mile north of Pitmachie Farm by the antiquarian Alexander Gordon.\n\nGordon was later indebted by the Society of Antiquaries of Scotland for writing a letter describing the original position of the Newton Stone. His letter reads:\n\nIn 1883, James Carnegie clarified that the stone was \"moved to a site [garden] behind Newton House about 1837\".\n\nThe Newton Stone contains two inscriptions. The first is an Ogham script possibly containing personal names, while the second has never been identified and became known from the early 19th century as the \"unknown script\". The Ogham script is engraved down the left-hand side of the stone and runs across part of its face. There are two rows of Ogham, a long and a short row. Across the top third of the stone, roughly central, is the unidentified script which contains 6 lines comprising 48 characters and symbols, including a swastika.\n\nThe Ogham inscription is ancient, but dates to Late Antiquity. Scholars have considered the possibility the unknown inscription was added a century or more after the Ogham.\n\nWilliam Forbes Skene dated the unknown inscription to the 9th century. It has also been proposed the \"unknown script\" is a modern forgery.\n\nJohn Pinkerton first published the engravings of the Newton Stone in his \"Inquiry into the History of Scotland\" (1814) yet made no attempt to decipher the \"unknown script\". In 1821-1822, John Stuart, Professor of Greek at Marischal College, discussed the stone in his paper entitled \"Sculpture Pillars in the Northern Part of Scotland\" addressed to the Edinburgh Society of Antiquaries. According to Stuart, the first attempt at translation was by Charles Vallancey who fancied resemblance of the characters to Latin. In 1856, Stuart published \"Sculptured Stones of Scotland\" which mentions that William Mill from Cambridge University proposed the script was Phoenician. No other theories had been proposed at that time.\n\nIt was apparently George Hamilton-Gordon's son Arthur who first took drawings of the stone to Cambridge, where Mill studied them:\n\nTherefore, a heated debate at Cambridge took place in 1862 regarding the decipherment, when Thomas Wright criticized Mill's Phoenician theory, for a more simple Latin translation: \"Here lies Constantinus, the son of\". Wright's translation was supported by the palaeographer Simonides but who substituted the Latin for Greek. Dr. Mill had died in 1853, but his paper \"On the Decipherment of the Phoenician Inscription on the Newton Stone discovered in Aberdeenshire\" was read out during the debate. His translation was:\n\nIn 1865 the antiquarian Alexander Thomson read a paper to the Society of Antiquaries of Scotland addressing five decipherment theories:\n\n\nAdditionally, George Moore proposed a Hebrew-Bactrian translation, while Thomson mentions another scholar who likened the unknown inscription to Sinaitic.\n\nThe more eccentric or fanciful decipherments such as Phoenician or Hebrew were soon rejected for Latin or Gaelic:\n\nAccording to the Scottish historian William Forbes Skene:\n\nIn 1907, William Bannerman, developed Skene's theory that the inscription contains Old Irish:\n\nLaurence Waddell however as late as 1924 offered another radical decipherment as Hitto-Phoenician. His work was strongly criticized.\n\nIn 1935, R. A. Stewart Macalister, while accepting the Ogham as ancient, considered the \"unknown script\" a modern forgery:\n\nHe also wrote:\n\nThe archaeologist C. A. Gordon however in 1956 disputed Macalister's claim:\n\nW. Douglas Simpson also rejected Macalister's assertion the unknown inscription was modern.\n\nIn 1984, Anthony Jackson a Senior Lecturer in Social Anthropology at Edinburgh University called to abandon the linguistic approach for a numerical interpretation:\n\n\n"}
{"id": "21275", "url": "https://en.wikipedia.org/wiki?curid=21275", "title": "Niobium", "text": "Niobium\n\nNiobium, formerly known as columbium, is a chemical element with symbol Nb (formerly Cb) and atomic number 41. It is a soft, grey, crystalline, ductile transition metal, often found in the minerals pyrochlore and columbite, hence the former name \"columbium\". Its name comes from Greek mythology, specifically Niobe, who was the daughter of Tantalus, the namesake of tantalum. The name reflects the great similarity between the two elements in their physical and chemical properties, making them difficult to distinguish.\n\nThe English chemist Charles Hatchett reported a new element similar to tantalum in 1801 and named it columbium. In 1809, the English chemist William Hyde Wollaston wrongly concluded that tantalum and columbium were identical. The German chemist Heinrich Rose determined in 1846 that tantalum ores contain a second element, which he named niobium. In 1864 and 1865, a series of scientific findings clarified that niobium and columbium were the same element (as distinguished from tantalum), and for a century both names were used interchangeably. Niobium was officially adopted as the name of the element in 1949, but the name columbium remains in current use in metallurgy in the United States.\n\nIt was not until the early 20th century that niobium was first used commercially. Brazil is the leading producer of niobium and ferroniobium, an alloy of 60–70% niobium with iron. Niobium is used mostly in alloys, the largest part in special steel such as that used in gas pipelines. Although these alloys contain a maximum of 0.1%, the small percentage of niobium enhances the strength of the steel. The temperature stability of niobium-containing superalloys is important for its use in jet and rocket engines.\n\nNiobium is used in various superconducting materials. These superconducting alloys, also containing titanium and tin, are widely used in the superconducting magnets of MRI scanners. Other applications of niobium include welding, nuclear industries, electronics, optics, numismatics, and jewelry. In the last two applications, the low toxicity and iridescence produced by anodization are highly desired properties.\n\nNiobium was identified by English chemist Charles Hatchett in 1801. He found a new element in a mineral sample that had been sent to England from Connecticut, United States in 1734 by John Winthrop F.R.S. (grandson of John Winthrop the Younger) and named the mineral \"columbite\" and the new element \"columbium\" after \"Columbia\", the poetical name for the United States. The \"columbium\" discovered by Hatchett was probably a mixture of the new element with tantalum.\n\nSubsequently, there was considerable confusion over the difference between columbium (niobium) and the closely related tantalum. In 1809, English chemist William Hyde Wollaston compared the oxides derived from both columbium—columbite, with a density 5.918 g/cm, and tantalum—tantalite, with a density over 8 g/cm, and concluded that the two oxides, despite the significant difference in density, were identical; thus he kept the name tantalum. This conclusion was disputed in 1846 by German chemist Heinrich Rose, who argued that there were two different elements in the tantalite sample, and named them after children of Tantalus: \"niobium\" (from Niobe) and \"pelopium\" (from Pelops). This confusion arose from the minimal observed differences between tantalum and niobium. The claimed new elements \"pelopium\", \"ilmenium\", and \"dianium\" were in fact identical to niobium or mixtures of niobium and tantalum.\n\nThe differences between tantalum and niobium were unequivocally demonstrated in 1864 by Christian Wilhelm Blomstrand and Henri Etienne Sainte-Claire Deville, as well as Louis J. Troost, who determined the formulas of some of the compounds in 1865 and finally by Swiss chemist Jean Charles Galissard de Marignac in 1866, who all proved that there were only two elements. Articles on \"ilmenium\" continued to appear until 1871.\n\nDe Marignac was the first to prepare the metal in 1864, when he reduced niobium chloride by heating it in an atmosphere of hydrogen. Although de Marignac was able to produce tantalum-free niobium on a larger scale by 1866, it was not until the early 20th century that niobium was used in incandescent lamp filaments, the first commercial application. This use quickly became obsolete through the replacement of niobium with tungsten, which has a higher melting point. That niobium improves the strength of steel was first discovered in the 1920s, and this application remains its predominant use. In 1961, the American physicist Eugene Kunzler and coworkers at Bell Labs discovered that niobium-tin continues to exhibit superconductivity in the presence of strong electric currents and magnetic fields, making it the first material to support the high currents and fields necessary for useful high-power magnets and electrical power machinery. This discovery enabled — two decades later — the production of long multi-strand cables wound into coils to create large, powerful electromagnets for rotating machinery, particle accelerators, and particle detectors.\n\n\"Columbium\" (symbol \"Cb\") was the name originally bestowed by Hatchett upon his discovery of the metal in 1801. The name reflected that the type specimen of the ore came from America (Columbia). This name remained in use in American journals—the last paper published by American Chemical Society with \"columbium\" in its title dates from 1953—while \"niobium\" was used in Europe. To end this confusion, the name \"niobium\" was chosen for element 41 at the 15th Conference of the Union of Chemistry in Amsterdam in 1949. A year later this name was officially adopted by the International Union of Pure and Applied Chemistry (IUPAC) after 100 years of controversy, despite the chronological precedence of the name \"columbium\". This was a compromise of sorts; the IUPAC accepted tungsten instead of wolfram in deference to North American usage; and \"niobium\" instead of \"columbium\" in deference to European usage. While many US chemical societies and government organizations typically use the official IUPAC name, some metallurgists and metal societies still use the original American name, \"columbium\".\n\nNiobium is a lustrous, grey, ductile, paramagnetic metal in group 5 of the periodic table (see table), with an electron configuration in the outermost shells atypical for group 5. (This can be observed in the neighborhood of ruthenium (44), rhodium (45), and palladium (46).)\n\nAlthough it is thought to have a body-centered cubic crystal structure from absolute zero to its melting point, high-resolution measurements of the thermal expansion along the three crystallographic axes reveal anisotropies which are inconsistent with a cubic structure. Therefore, further research and discovery in this area is expected.\n\nNiobium becomes a superconductor at cryogenic temperatures. At atmospheric pressure, it has the highest critical temperature of the elemental superconductors at 9.2 K. Niobium has the greatest magnetic penetration depth of any element. In addition, it is one of the three elemental Type II superconductors, along with vanadium and technetium. The superconductive properties are strongly dependent on the purity of the niobium metal.\n\nWhen very pure, it is comparatively soft and ductile, but impurities make it harder.\n\nThe metal has a low capture cross-section for thermal neutrons; thus it is used in the nuclear industries where neutron transparent structures are desired.\n\nThe metal takes on a bluish tinge when exposed to air at room temperature for extended periods. Despite a high melting point in elemental form (2,468 °C), it has a lower density than other refractory metals. Furthermore, it is corrosion-resistant, exhibits superconductivity properties, and forms dielectric oxide layers.\n\nNiobium is slightly less electropositive and more compact than its predecessor in the periodic table, zirconium, whereas it is virtually identical in size to the heavier tantalum atoms, as a result of the lanthanide contraction. As a result, niobium's chemical properties are very similar to those for tantalum, which appears directly below niobium in the periodic table. Although its corrosion resistance is not as outstanding as that of tantalum, the lower price and greater availability make niobium attractive for less demanding applications, such as vat linings in chemical plants.\n\nNiobium in the Earth's crust comprises one stable isotope, Nb. By 2003, at least 32 radioisotopes had been synthesized, ranging in atomic mass from 81 to 113. The most stable of these is Nb with a half-life of 34.7 million years. One of the least stable is Nb, with an estimated half-life of 30 milliseconds. Isotopes that are lighter than the stable Nb tend to decay by β decay, and those that are heavier tend to decay by β decay, with some exceptions. Nb, Nb, and Nb have minor β delayed proton emission decay paths, Nb decays by electron capture and positron emission, and Nb decays by both β and β decay.\n\nAt least 25 nuclear isomers have been described, ranging in atomic mass from 84 to 104. Within this range, only Nb, Nb, and Nb do not have isomers. The most stable of niobium's isomers is Nb with a half-life of 16.13 years. The least stable isomer is Nb with a half-life of 103 ns. All of niobium's isomers decay by isomeric transition or beta decay except Nb, which has a minor electron capture branch.\n\nNiobium is estimated to be the 34th most common element in the Earth’s crust, with 20 ppm. Some think that the abundance on Earth is much greater, and that the element's high density has concentrated it in the Earth’s core. The free element is not found in nature, but niobium occurs in combination with other elements in minerals. Minerals that contain niobium often also contain tantalum. Examples include columbite ((Fe,Mn)(Nb,Ta)O) and columbite–tantalite (or \"coltan\", (Fe,Mn)(Ta,Nb)O). Columbite–tantalite minerals (the most common species being columbite-(Fe) and tantalite-(Fe), where \"-(Fe)\" is the Levinson suffix informing about the prevailence of iron over other elements like manganese) are most usually found as accessory minerals in pegmatite intrusions, and in alkaline intrusive rocks. Less common are the niobates of calcium, uranium, thorium and the rare earth elements. Examples of such niobates are pyrochlore ((Na,Ca)NbO(OH,F)) (now a group name, with a relatively common example being, e.g., fluorcalciopyrochlore) and euxenite (correctly named euxenite-(Y)) ((Y,Ca,Ce,U,Th)(Nb,Ta,Ti)O). These large deposits of niobium have been found associated with carbonatites (carbonate-silicate igneous rocks) and as a constituent of pyrochlore. \n\nThe three largest currently mined deposits of pyrochlore, two in Brazil and one in Canada, were found in the 1950s, and are still the major producers of niobium mineral concentrates. The largest deposit is hosted within a carbonatite intrusion in Araxá, state of Minas Gerais, Brazil, owned by CBMM (Companhia Brasileira de Metalurgia e Mineração); the other active Brazilian deposit is located near Catalão, state of Goiás, and owned by China Molybdenum, also hosted within a carbonatite intrusion. Together, those two mines produce about 88% of the world's supply. Brazil also has a large but still unexploited deposit near São Gabriel da Cachoeira, state of Amazonas, as well as a few smaller deposits, notably in the state of Roraima.\n\nThe third largest producer of niobium is the carbonatite-hosted Niobec mine, in Saint-Honoré, near Chicoutimi, Quebec, Canada, owned by Magris Resources. It produces between 7% and 10% of the world's supply.\n\nAfter the separation from the other minerals, the mixed oxides of tantalum TaO and niobium NbO are obtained. The first step in the processing is the reaction of the oxides with hydrofluoric acid:\n\nThe first industrial scale separation, developed by de Marignac, exploits the differing solubilities of the complex niobium and tantalum fluorides, dipotassium oxypentafluoroniobate monohydrate (K<nowiki>[</nowiki>NbOF<nowiki>]</nowiki>·HO) and dipotassium heptafluorotantalate (K<nowiki>[</nowiki>TaF<nowiki>]</nowiki>) in water. Newer processes use the liquid extraction of the fluorides from aqueous solution by organic solvents like cyclohexanone. The complex niobium and tantalum fluorides are extracted separately from the organic solvent with water and either precipitated by the addition of potassium fluoride to produce a potassium fluoride complex, or precipitated with ammonia as the pentoxide:\n\nFollowed by:\n\nSeveral methods are used for the reduction to metallic niobium. The electrolysis of a molten mixture of K<nowiki>[</nowiki>NbOF<nowiki>]</nowiki> and sodium chloride is one; the other is the reduction of the fluoride with sodium. With this method, a relatively high purity niobium can be obtained. In large scale production, NbO is reduced with hydrogen or carbon. In the aluminothermic reaction, a mixture of iron oxide and niobium oxide is reacted with aluminium:\n\nSmall amounts of oxidizers like sodium nitrate are added to enhance the reaction. The result is aluminium oxide and ferroniobium, an alloy of iron and niobium used in the steel production. Ferroniobium contains between 60 and 70% niobium. Without iron oxide, the aluminothermic process is used to produce niobium. Further purification is necessary to reach the grade for superconductive alloys. Electron beam melting under vacuum is the method used by the two major distributors of niobium.\n\n, CBMM from Brazil controlled 85 percent of the world's niobium production. The United States Geological Survey estimates that the production increased from 38,700 tonnes in 2005 to 44,500 tonnes in 2006. Worldwide resources are estimated to be 4,400,000 tonnes. During the ten-year period between 1995 and 2005, the production more than doubled, starting from 17,800 tonnes in 1995. Between 2009 and 2011, production was stable at 63,000 tonnes per year, with a slight decrease in 2012 to only 50,000 tonnes per year.\n\nLesser amounts are found in Malawi's Kanyika Deposit (Kanyika mine).\n\nIn many ways, niobium is similar to tantalum and zirconium. It reacts with most nonmetals at high temperatures; with fluorine at room temperature; with chlorine and hydrogen at 200 °C; and with nitrogen at 400 °C, with products that are frequently interstitial and nonstoichiometric. The metal begins to oxidize in air at 200 °C. It resists corrosion by fused alkalis and by acids, including aqua regia, hydrochloric, sulfuric, nitric and phosphoric acids. Niobium is attacked by hydrofluoric acid and hydrofluoric/nitric acid mixtures.\n\nAlthough niobium exhibits all of the formal oxidation states from +5 to −1, the most common compounds have niobium in the +5 state. Characteristically, compounds in oxidation states less than 5+ display Nb–Nb bonding.\n\nNiobium forms oxides in the oxidation states +5 (NbO), +4 (NbO), +3 (), and the rarer oxidation state, +2 (NbO). Most common is the pentoxide, precursor to almost all niobium compounds and alloys. Niobates are generated by dissolving the pentoxide in basic hydroxide solutions or by melting it in alkali metal oxides. Examples are lithium niobate (LiNbO) and lanthanum niobate (LaNbO). In the lithium niobate is a trigonally distorted perovskite-like structure, whereas the lanthanum niobate contains lone ions. The layered niobium sulfide (NbS) is also known.\n\nMaterials can be coated with a thin film of niobium(V) oxide chemical vapor deposition or atomic layer deposition processes, produced by the thermal decomposition of niobium(V) ethoxide above 350 °C.\n\nNiobium forms halides in the oxidation states of +5 and +4 as well as diverse substoichiometric compounds. The pentahalides () feature octahedral Nb centres. Niobium pentafluoride (NbF) is a white solid with a melting point of 79.0 °C and niobium pentachloride (NbCl) is yellow (see image at left) with a melting point of 203.4 °C. Both are hydrolyzed to give oxides and oxyhalides, such as NbOCl. The pentachloride is a versatile reagent used to generate the organometallic compounds, such as niobocene dichloride (). The tetrahalides () are dark-coloured polymers with Nb-Nb bonds; for example, the black hygroscopic niobium tetrafluoride (NbF) and brown niobium tetrachloride (NbCl).\n\nAnionic halide compounds of niobium are well known, owing in part to the Lewis acidity of the pentahalides. The most important is [NbF], an intermediate in the separation of Nb and Ta from the ores. This heptafluoride tends to form the oxopentafluoride more readily than does the tantalum compound. Other halide complexes include octahedral [NbCl]:\n\nAs with other metals with low atomic numbers, a variety of reduced halide cluster ions is known, the prime example being [NbCl].\n\nOther binary compounds of niobium include niobium nitride (NbN), which becomes a superconductor at low temperatures and is used in detectors for infrared light. The main niobium carbide is NbC, an extremely hard, refractory, ceramic material, commercially used in cutting tool bits.\n\nOut of 44,500 tonnes of niobium mined in 2006, an estimated 90% was used in high-grade structural steel. The second largest application is superalloys. Niobium alloy superconductors and electronic components account for a very small share of the world production.\n\nNiobium is an effective microalloying element for steel, within which it forms niobium carbide and niobium nitride. These compounds improve the grain refining, and retard recrystallization and precipitation hardening. These effects in turn increase the toughness, strength, formability, and weldability. Within microalloyed stainless steels, the niobium content is a small (less than 0.1%) but important addition to high strength low alloy steels that are widely used structurally in modern automobiles. Niobium is sometimes used in considerably higher quantities for highly wear-resistant machine components and knives, as high as 3% in Crucible CPM S110V stainless steel.\n\nThese same niobium alloys are often used in pipeline construction.\n\nQuantities of niobium are used in nickel-, cobalt-, and iron-based superalloys in proportions as great as 6.5% for such applications as jet engine components, gas turbines, rocket subassemblies, turbo charger systems, heat resisting, and combustion equipment. Niobium precipitates a hardening γ<nowiki>\"</nowiki>-phase within the grain structure of the superalloy.\n\nOne example superalloy is Inconel 718, consisting of roughly 50% nickel, 18.6% chromium, 18.5% iron, 5% niobium, 3.1% molybdenum, 0.9% titanium, and 0.4% aluminium. These superalloys were used, for example, in advanced air frame systems for the Gemini program. Another niobium alloy was used for the nozzle of the Apollo Service Module. Because niobium is oxidized at temperatures above 400 °C, a protective coating is necessary for these applications to prevent the alloy from becoming brittle.\n\nC-103 alloy was developed in the early 1960s jointly by the Wah Chang Corporation and Boeing Co. DuPont, Union Carbide Corp., General Electric Co. and several other companies were developing Nb-base alloys simultaneously, largely driven by the Cold War and Space Race. It is composed of 89% niobium, 10% hafnium and 1% titanium and is used for liquid rocket thruster nozzles, such as the main engine of the Apollo Lunar Modules.\n\nThe nozzle of the Merlin Vacuum series of engines developed by SpaceX for the upper stage of its Falcon 9 rocket is made from a niobium alloy.\n\nThe reactivity of niobium with oxygen requires it to be worked in a vacuum or inert atmosphere, which significantly increases the cost and difficulty of production. Vacuum arc remelting (VAR) and electron beam melting (EBM), novel processes at the time, enabled the development of niobium and other reactive metals. The project that yielded C-103 began in 1959 with as many as 256 experimental niobium alloys in the \"C-series\" (possibly from columbium) that could be melted as buttons and rolled into sheet. Wah Chang had an inventory of hafnium, refined from nuclear-grade zirconium alloys, that it wanted to put to commercial use. The 103rd experimental composition of the C-series alloys, Nb-10Hf-1Ti, had the best combination of formability and high-temperature properties. Wah Chang fabricated the first 500-lb heat of C-103 in 1961, ingot to sheet, using EBM and VAR. The intended applications included turbine engines and liquid metal heat exchangers. Competing niobium alloys from that era included FS85 (Nb-10W-28Ta-1Zr) from Fansteel Metallurgical Corp., Cb129Y (Nb-10W-10Hf-0.2Y) from Wah Chang and Boeing, Cb752 (Nb-10W-2.5Zr) from Union Carbide, and Nb1Zr from Superior Tube Co.\n\nNiobium-germanium (), niobium-tin (), as well as the niobium-titanium alloys are used as a type II superconductor wire for superconducting magnets. These superconducting magnets are used in magnetic resonance imaging and nuclear magnetic resonance instruments as well as in particle accelerators. For example, the Large Hadron Collider uses 600 tons of superconducting strands, while the International Thermonuclear Experimental Reactor uses an estimated 600 tonnes of NbSn strands and 250 tonnes of NbTi strands. In 1992 alone, more than US$1 billion worth of clinical magnetic resonance imaging systems were constructed with niobium-titanium wire.\n\nThe superconducting radio frequency (SRF) cavities used in the free-electron lasers FLASH (result of the cancelled TESLA linear accelerator project) and XFEL are made from pure niobium. A cryomodule team at Fermilab used the same SRF technology from the FLASH project to develop 1.3 GHz nine-cell SRF cavities made from pure niobium. The cavities will be used in the linear particle accelerator of the International Linear Collider. The same technology will be used in LCLS-II at SLAC National Accelerator Laboratory and PIP-II at Fermilab.\n\nThe high sensitivity of superconducting niobium nitride bolometers make them an ideal detector for electromagnetic radiation in the THz frequency band. These detectors were tested at the Submillimeter Telescope, the South Pole Telescope, the Receiver Lab Telescope, and at APEX, and are now used in the HIFI instrument on board the Herschel Space Observatory.\n\nLithium niobate, which is a ferroelectric, is used extensively in mobile telephones and optical modulators, and for the manufacture of surface acoustic wave devices. It belongs to the ABO structure ferroelectrics like lithium tantalate and barium titanate. Niobium capacitors are available as alternative to tantalum capacitors, but tantalum capacitors still predominate. Niobium is added to glass to obtain a higher refractive index, making possible thinner and lighter corrective glasses.\n\nNiobium and some niobium alloys are physiologically inert and hypoallergenic. For this reason, niobium is used in prosthetics and implant devices, such as pacemakers. Niobium treated with sodium hydroxide forms a porous layer that aids osseointegration.\n\nLike titanium, tantalum, and aluminium, niobium can be heated and anodized (\"reactive metal anodization\") to produce a wide array of iridescent colours for jewelry, where its hypoallergenic property is highly desirable.\n\nNiobium is used as a precious metal in commemorative coins, often with silver or gold. For example, Austria produced a series of silver niobium euro coins starting in 2003; the colour in these coins is created by the diffraction of light by a thin anodized oxide layer. In 2012, ten coins are available showing a broad variety of colours in the centre of the coin: blue, green, brown, purple, violet, or yellow. Two more examples are the 2004 Austrian €25 150 Years Semmering Alpine Railway commemorative coin, and the 2006 Austrian €25 European Satellite Navigation commemorative coin.\nThe Austrian mint produced for Latvia a similar series of coins starting in 2004,\nwith one following in 2007.\nIn 2011, the Royal Canadian Mint started production of a $5 sterling silver and niobium coin named \"Hunter's Moon\"\nin which the niobium was selectively oxidized, thus creating unique finishes where no two coins are exactly alike.\n\nThe arc-tube seals of high pressure sodium vapor lamps are made from niobium, sometimes alloyed with 1% of zirconium; niobium has a very similar coefficient of thermal expansion, matching the sintered alumina arc tube ceramic, a translucent material which resists chemical attack or reduction by the hot liquid sodium and sodium vapour contained inside the operating lamp.\n\nNiobium is used in arc welding rods for some stabilized grades of stainless steel and in anodes for cathodic protection systems on some water tanks, which are then usually plated with platinum.\n\nNiobium is an important component of high-performance heterogeneous catalysts for the production of acrylic acid by selective oxidation of propane.\n\nNiobium is used to make the high voltage wire of the Solar Corona particles receptor module of the Parker Solar Probe \n\nNiobium has no known biological role. While niobium dust is an eye and skin irritant and a potential fire hazard, elemental niobium on a larger scale is physiologically inert (and thus hypoallergenic) and harmless. It is frequently used in jewelry and has been tested for use in some medical implants.\n\nNiobium-containing compounds are rarely encountered by most people, but some are toxic and should be treated with care. The short- and long-term exposure to niobates and niobium chloride, two chemicals that are water-soluble, have been tested in rats. Rats treated with a single injection of niobium pentachloride or niobates show a median lethal dose (LD) between 10 and 100 mg/kg. For oral administration the toxicity is lower; a study with rats yielded a LD after seven days of 940 mg/kg.\n\n"}
{"id": "11241418", "url": "https://en.wikipedia.org/wiki?curid=11241418", "title": "Orphan gene", "text": "Orphan gene\n\nOrphan genes (also called ORFans, especially in microbial literature) are genes without detectable homologues in other lineages. Orphans are a subset of taxonomically-restricted genes (TRGs), which are unique to a specific taxonomic level (e.g. plant-specific). In contrast to non-orphan TRGs, orphans are usually considered unique to a very narrow taxon, generally a species.\n\nThe classic model of evolution is based on duplication, rearrangement, and mutation of genes with the idea of common descent. Orphan genes differ in that they are lineage-specific with no known history of shared duplication and rearrangement outside of their specific species or clade. Orphan genes may arise through a variety of mechanisms, such as horizontal gene transfer, duplication and rapid divergence, and de novo origination, and may act at different rates in insects, primates, and plants. Despite their relatively recent origin, orphan genes may encode functionally important proteins.\n\nOrphan genes were first discovered when the yeast genome-sequencing project began in 1996. Orphan genes accounted for an estimated 26% of the yeast genome, but it was believed that these genes could be classified with homologues when more genomes were sequenced. At the time, gene duplication was considered the only serious model of gene evolution and there were few sequenced genomes for comparison, so a lack of detectable homologues was thought to be most likely due to a lack of sequencing data and not due to a true lack of homology. However, orphan genes continued to persist as the quantity of sequenced genomes grew, eventually leading to the conclusion that orphan genes are ubiquitous to all genomes. Estimates of the percentage of genes which are orphans varies enormously between species and between studies; 10-30% is a commonly cited figure.\n\nThe study of orphan genes emerged largely after the turn of the century. In 2003, a study of \"Caenorhabditis briggsae\" and related species compared over 2000 genes. They proposed that these genes must be evolving too quickly to be detected and are consequently sites of very rapid evolution. In 2005, Wilson examined 122 bacterial species to try to examine whether the large number of orphan genes in many species was legitimate. The study found that it was legitimate and played a role in bacterial adaptation. The definition of taxonomically-restricted genes was introduced into the literature to make orphan genes seem less \"mysterious.\"\n\nIn 2008, a yeast protein of established functionality, BSC4, was found to have evolved de novo from non-coding sequences whose homology was still detectable in sister species.\n\nIn 2009, an orphan gene was discovered to regulate an internal biological network: the orphan gene, QQS, from \"Arabidopsis thaliana\" modifies plant composition. The QQS orphan protein interacts with a conserved transcription factor, these data explain the compositional changes (increased protein) that are induced when QQS is engineered into diverse species. In 2011, a comprehensive genome-wide study of the extent and evolutionary origins of orphan genes in plants was conducted in the model plant \"Arabidopsis thaliana\" \"\n\nGenes can be tentatively classified as orphans if no orthologous proteins can be found in nearby species.\n\nOne method used to estimate nucleotide or protein sequence similarity indicative of homology (i.e. similarity due to common origin) is the Basic Local Alignment Search Tool (BLAST). BLAST allows query sequences to be rapidly searched against large sequence databases. Simulations suggest that under certain conditions BLAST is suitable for detecting distant relatives of a gene. However, genes that are short and evolve rapidly can easily be missed by BLAST.\n\nThe systematic detection of homology to annotate orphan genes is called phylostratigraphy. Phylostratigraphy generates a phylogenetic tree in which the homology is calculated between all genes of a focal species and the genes of other species. The earliest common ancestor for a gene determines the age, or phylostratum, of the gene. The term \"orphan\" is sometimes used only for the youngest phylostratum containing only a single species, but when interpreted broadly as a taxonomically-restricted gene, it can refer to all but the oldest phylostratum, with the gene orphaned within a larger clade.\n\nOrphan genes arise from multiple sources, predominantly through de novo origination, duplication and rapid divergence, and horizontal gene transfer.\n\nNovel orphan genes continually arise de novo from non-coding sequences. These novel genes may be sufficiently beneficial to be swept to fixation by selection. Or, more likely, they will fade back into the non-genic background. This latter option is supported by research in Drosophila showing that young genes are more likely go extinct.\n\nDe novo genes were once thought to be a near impossibility due to the complex and potentially fragile intricacies of creating and maintaining functional polypeptides, but research from the past 10 years or so has found multiple examples of de novo genes, some of which are associated with important biological processes, particularly testes function in animals. De novo genes were also found in fungi and plants.\n\nFor young orphan genes, it is sometimes possible to find homologous non-coding DNA sequences in sister taxa, which is generally accepted as strong evidence of de novo origin. However, the contribution of de novo origination to taxonomically-restricted genes of older origin, particularly in relation to the traditional gene duplication theory of gene evolution, remains contested.\n\nThe duplication and divergence model for orphan genes involves a new gene being created from some duplication or divergence event and undergoing a period of rapid evolution where all detectable similarity to the originally duplicated gene is lost. While this explanation is consistent with current understandings of duplication mechanisms, the number of mutations needed to lose detectable similarity is large enough as to be a rare event, and the evolutionary mechanism by which a gene duplicate could be sequestered and diverge so rapidly remains unclear.\n\nAnother explanation for how orphan genes arise is through a duplication mechanism called horizontal gene transfer, where the original duplicated gene derives from a separate, unknown lineage. This explanation for the origin of orphan genes is especially relevant in bacteria and archaea, where horizontal gene transfer is common.\n\nOrphans genes tend to be very short (~6 times shorter than mature genes), and some are weakly expressed, tissue specific and simpler in codon usage and amino acid composition. Orphan genes tend to encode more intrinsically disordered proteins, although some structure has been found in one of the best characterized orphan genes. \nOf the tens of thousands of enzymes of primary or specialized metabolism that have been characterized to date, none are orphans, or even of restricted lineage; apparently, catalysis requires hundreds of millions of years of evolution.\n\nWhile the prevalence of orphan genes has been established, the evolutionary role of orphans, and its resulting importance, is still being debated. One theory is that many orphans have no evolutionary role; genomes contain non-functional open reading frames (ORFs) that create spurious polypeptide products not maintained by selection, meaning that they are unlikely to be conserved between species and would likely be detected as orphan genes. However, a variety of other studies have shown that at least some orphans are functionally important and may help explain the emergence of novel phenotypes.\n"}
{"id": "23592123", "url": "https://en.wikipedia.org/wiki?curid=23592123", "title": "Our Friend the Atom", "text": "Our Friend the Atom\n\n\"Our Friend the Atom\" is a 1957 episode of the television series \"Disneyland\" describing the benefits of nuclear power and hosted by Heinz Haber.\n\nA book form, published in 1956, also exists.\n\nIn 1980, an updated version called \"The Atom: A Closer Look\" was released by Disney's educational media division.\n"}
{"id": "50326328", "url": "https://en.wikipedia.org/wiki?curid=50326328", "title": "Pengerang Integrated Petroleum Complex", "text": "Pengerang Integrated Petroleum Complex\n\nPengerang Integrated Petroleum Complex (PIPC) is a megaproject development in Pengerang, Kota Tinggi District, Johor, Malaysia. It spans over an area of 80 km and will house oil refineries, naphtha crackers, petrochemical plants, liquefied natural gas (LNG) terminals and a regasification plant upon completion.\n\nThe site where it stands now was chosen because of its location along major shipping route between Middle East and China. The project was launched to increase Malaysia's petrochemical output.\n\nThe Johor Petroleum Development Corporation Berhad (JPDC) was established to oversee the development of the project. JPDC was created as a subsidiary of Malaysia Petroleum Resources Corporation (MPRC)\n\nThe components of PIPC was first initiated with the development of the 5 million cubic meters capacity of Pengerang Deepwater Terminal (PDT), a joint-venture between Dialog Group, Royal Vopak of the Netherlands and Johor. It serves as a centralized storage facility for trading, refining and petrochemical industry. The USD3 billion facility includes an independent terminal for trading, a dedicated industrial terminal within PIPC, and an LNG terminal. The construction of a deepwater jetty facility enables the berthing of both ultra large crude carriers and very large crude carriers. PDT received its first shipment of oil in the first quarter of 2014.\n\nThe other component of PIPC is the Petronas Pengerang Integrated Complex (PIC), Petronas' largest downstream investment in a single location to date. The development includes the USD16 billion Refinery and Petrochemical Integrated Development Project (RAPID).\n\nThis also involves the USD11 billion associated facilities, which are air separation unit, raw water supply, cogeneration plant, regasification terminal, deepwater terminal and utilities. Upon its completion in 2019, PIC will have a refining capacity of 300,000 barrels per day with petrochemical plants yielding an estimated annual production capacity of 3.6 million tonnes of petrochemical products.\n\nIts sheltered harbour with a natural water depth of 24 meters provides berthing conditions for Ultra Large Crude Carriers (ULCCs) and Very Large Crude Carriers (VLCCs).\n\nThe complex is accessible by road using the Senai–Desaru Expressway from Johor Bahru.\n"}
{"id": "22319138", "url": "https://en.wikipedia.org/wiki?curid=22319138", "title": "Polymeric foam", "text": "Polymeric foam\n\nA polymeric foam is a foam, in liquid or solidified form, formed from polymers.\n\nExamples include:\n\n"}
{"id": "1413688", "url": "https://en.wikipedia.org/wiki?curid=1413688", "title": "Primary energy", "text": "Primary energy\n\nPrimary energy (PE) is an energy form found in nature that has not been subjected to any human engineered conversion process. It is energy contained in raw fuels, and other forms of energy received as input to a system. Primary energy can be non-renewable or renewable.\n\nWhere primary energy is used to describe fossil fuels, the embodied energy of the fuel is available as thermal energy and around 70% is typically lost in conversion to electrical or mechanical energy. There is a similar 60-80% conversion loss when solar and wind energy is converted to electricity, but today's UN conventions on energy statistics counts the electricity made from wind and solar as the primary energy itself for these sources. One consequence of this counting method is that the contribution of wind and solar energy is under reported compared to fossil energy sources, and there is hence an international debate on how to count primary energy from wind and solar. \n\nTotal primary energy supply (TPES) is the sum of production and imports subtracting exports and storage changes.\n\nThe concept of primary energy is used in energy statistics in the compilation of energy balances, as well as in the field of energetics. In energetics, a primary energy source (PES) refers to the energy forms required by the energy sector to generate the supply of energy carriers used by human society.\n\nSecondary energy is a carrier of energy, such as electricity. These are produced by conversion from a primary energy source.\n\nThe use of primary energy as a measure ignores conversion efficiency. Thus forms of energy with poor conversion efficiency, particularly the thermal sources, coal, gas and nuclear are overstated, whereas energy sources such as hydroelectricity which are converted efficiently, while a small fraction of primary energy are significantly more important than their total raw energy supply may seem to imply.\n\nPE and TPES are better defined in the context of worldwide energy supply.\n\nPrimary energy sources should not be confused with the energy system components (or conversion processes) through which they are converted into energy carriers.\n\nPrimary energy sources are transformed in energy conversion processes to more convenient forms of energy that can directly be used by society, such as electrical energy, refined fuels, or synthetic fuels such as hydrogen fuel. In the field of energetics, these forms are called energy carriers and correspond to the concept of \"secondary energy\" in energy statistics.\n\nEnergy carriers are energy forms which have been transformed from primary energy sources. Electricity is one of the most common energy carriers, being transformed from various primary energy sources such as coal, oil, natural gas, and wind. Electricity is particularly useful since it has low entropy (is highly ordered) and so can be converted into other forms of energy very efficiently. District heating is another example of secondary energy.\n\nAccording to the laws of thermodynamics, primary energy sources cannot be produced. They must be available to society to enable the production of energy carriers.\n\nConversion efficiency varies. For thermal energy, electricity and mechanical energy production is limited by Carnot's theorem, and generates a lot of waste heat. Other non-thermal conversions can be more efficient. For example, while wind turbines do not capture all of the wind's energy, they have a high conversion efficiency and generate very little waste heat since wind energy is low entropy. In principle solar photovoltaic conversions could be very efficient, but current conversion can only be done well for narrow ranges of wavelength, whereas solar thermal is also subject to Carnot efficiency limits. Hydroelectric power is also very ordered, and converted very efficiently. The amount of usable energy is the exergy of a system.\n\nSite energy is the term used in North America for the amount of end-use energy of all forms consumed at a specified location. This can be a mix of primary energy (such as natural gas burned at the site) and secondary energy (such as electricity). Site energy is measured at the campus, building, or sub-building level and is the basis for energy charges on utility bills.\n\nSource energy, in contrast, is the term used in North America for the amount of primary energy consumed in order to provide a facility’s site energy. It is always greater than the site energy, as it includes all site energy and adds to it the energy lost during transmission, delivery, and conversion. While source or primary energy provides a more complete picture of energy consumption, it cannot be measured directly and must be calculated using conversion factors from site energy measurements. For electricity, a typical value is three units of source energy for one unit of site energy. However, this can vary considerably depending on factors such as the primary energy source or fuel type, the type of power plant, and the transmission infrastructure. One full set of conversion factors is available as technical reference from Energy STAR.\n\nEither site or source energy can be an appropriate metric when comparing or analyzing energy use of different facilities. The U.S Energy Information Administration, for example, uses primary (source) energy for its energy overviews but site energy for its Commercial Building Energy Consumption Survey and Residential Building Energy Consumption Survey. The US Environmental Protection Agency's Energy STAR program recommends using source energy, and the US Department of Energy uses site energy in its definition of a zero net energy building.\n\nEnergy accidents are accidents that occur in systems that provide energy or power. These can result in fatalities, as can the normal running of many systems, for example those deaths due to pollution.\n\nGlobally, coal is responsible for 100,000 deaths per trillion kWh.\n\n\n\n"}
{"id": "34532980", "url": "https://en.wikipedia.org/wiki?curid=34532980", "title": "REScoop.eu", "text": "REScoop.eu\n\nREScoop.eu is the European federation of renewable energy cooperatives, founded in 2011.\n\nIts routine administration is handled by the Belgian cooperative Ecopower.\n\nOn December 24, 2013, the European federation of groups and cooperatives of citizens for renewable energy (REScoop.EU) was legally established under Belgian national law with a European scope. This legal act is a key issue to further develop the activities of the European federation and it constitutes the base to build a strong European renewable energy cooperative alliance.\n\n\"REScoop\" is a short term for Renewable Energy Source Cooperative.\n"}
{"id": "41392177", "url": "https://en.wikipedia.org/wiki?curid=41392177", "title": "Rui L. Reis", "text": "Rui L. Reis\n\nRui Luís Reis is a Portuguese scientist known for his research in tissue engineering, regenerative medicine, biomaterials, biomimetics, stem cells, and biodegradable polymers. Reis is Professor of tissue engineering, regenerative medicine and stem cells at the Department of Polymer Engineering, School of Engineering of the University of Minho, in Braga and Guimarães. He is the Director of the 3B's Research Group, part of the Research Institute on Biomaterials, Biodegradables and Biomimetics (I3Bs) of UMinho (www.i3bs.uminho.pt), which specializes in the areas of regenerative Medicine, tissue engineering, stem cells and biomaterials. He is also the Director of the ICVS/3B´s Associate Laboratory of UMinho. He is also the CEO of the European Institute of Excellence on Tissue Engineering and Regenerative Medicine. Rui L. Reis is also, since 2013, the Vice-Rector (Vice-President) for research and innovation of UMinho. Since 2007 he is also editor-in-chief of the \"Journal of Tissue Engineering and Regenerative Medicine\". He is since 2016 and until 2018 the Global (World) President of the Tissue Engineering and Regenerative Medicine International Society (TERMIS). He is the responsible and PI of the EU funded project for the creation of the new center of Excellence, with headquarters in AvePark in Caldas das Taipas - Guimarães, the Discoveries Center for Regenerative and Precision Medicine in a Teaming process between University of Minho, University College London, University of Porto, University of Aveiro, University of Lisbon, University NOVA Lisbon.\n\nReis was born and has always lived in Porto, being one of three children of a chemical engineering professor and a domestic. Reis spent a small part of his childhood in Metangula, Mozambique, a small town near Lake Niassa, while his father was engaged in military service during the Portuguese Colonial War. He is married with Olga Paiva and has one son, Bernardo Reis (born in 2001). He is a strong supporter of FC Porto.\n\nReis began his research into biomaterials in 1990. He studies the development of biomaterials from natural origin polymers and their possible biomedical applications, including bone replacement and fixation, drug delivery carriers, partially degradable bone cements and tissue engineering scaffolding. His team works under his leadership in the tissue engineering of many different tissues, namely: bone, cartilage, osteochondral, skin, tendon and ligaments, meniscus, intervertebral disks, as well as on peripheral and central nervous systems regeneration, diabetes treatment and cancer 3D disease models. In 2015 Reis' research team developed a new type of catheter which is made of hydrogel.\n\nIn 2014 he was made a Commander of Portugal's Order of Saint James of the Sword and in 2016, Reis was elected a foreign member of the United States National Academy of Engineering. He has won many other awards, including the Jean Leray (in 2002) and the George Winter award (in 2011) the two major awards of the European Society for Biomaterials. Rui L. Reis was also awarded the Clemson Award for Contributions to the Literature by the Society for Biomaterials (USA, in 2014) and the Contributions to the Literature Award by the European Chapter of TERMIS (TERMIS-EU in 2017). He has an Honouris Causa by the University of Granada, Spain, in 2010.\n\n"}
{"id": "297999", "url": "https://en.wikipedia.org/wiki?curid=297999", "title": "Softwood", "text": "Softwood\n\nSoftwood is wood from gymnosperm trees such as conifers. The term is opposed to hardwood, which is the wood from angiosperm trees.\n\nSoftwood is wood from gymnosperm trees such as pines and spruces. Softwoods are not necessarily softer than hardwoods. In both groups there is an enormous variation in actual wood hardness, the range of density in hardwoods completely including that of softwoods. Some hardwoods (e.g. balsa) are softer than most softwoods, while the hardest hardwoods are much harder than any softwood. The woods of longleaf pine, Douglas fir, and yew are much harder in the mechanical sense than several hardwoods.\n\nSoftwoods are generally most used by the construction industry and are also used to produce paper pulp, and card products.\n\nCertain species of softwood are more resistant to insect attack from woodworm, as certain insects prefer damp hardwood.\n\nSoftwood reproduces using cones and occasionally nuts.\n\n\nSoftwood is the source of about 80% of the world's production of timber, with traditional centres of production being the Baltic region (including Scandinavia and Russia), North America and China. Softwood is typically used in construction as structural carcassing timber, as well as finishing timber.\n\n"}
{"id": "3507365", "url": "https://en.wikipedia.org/wiki?curid=3507365", "title": "Solar panel", "text": "Solar panel\n\nPhotovoltaic solar panels absorb sunlight as a source of energy to generate electricity. A photovoltaic (PV) module is a packaged, connected assembly of typically 6x10 photovoltaic solar cells. Photovoltaic modules constitute the photovoltaic array of a photovoltaic system that generates and supplies solar electricity in commercial and residential applications. \n\nEach module is rated by its DC output power under standard test conditions (STC), and typically ranges from 100 to 365 Watts (W). The efficiency of a module determines the area of a module given the same rated output an 8% efficient 230 W module will have twice the area of a 16% efficient 230 W module. There are a few commercially available solar modules that exceed efficiency of 24% \n\nA single solar module can produce only a limited amount of power; most installations contain multiple modules. A photovoltaic system typically includes an array of photovoltaic modules, an inverter, a battery pack for storage, interconnection wiring, and optionally a solar tracking mechanism. \n\nThe most common application of solar energy collection outside agriculture is solar water heating systems. \n\nThe price of solar electrical power has continued to fall so that in many countries it has become cheaper than ordinary fossil fuel electricity from the electricity grid since 2012, a phenomenon known as grid parity.\n\nPhotovoltaic modules use light energy (photons) from the Sun to generate electricity through the photovoltaic effect. The majority of modules use wafer-based crystalline silicon cells or thin-film cells. The structural (load carrying) member of a module can either be the top layer or the back layer. Cells must also be protected from mechanical damage and moisture. Most modules are rigid, but semi-flexible ones based on thin-film cells are also available. The cells must be connected electrically in series, one to another. \n\nA PV junction box is attached to the back of the solar panel and it is its output interface.Externally, most of photovoltaic modules use MC4 connectors type to facilitate easy weatherproof connections to the rest of the system. Also, USB power interface can be used.\n\nModule electrical connections are made in series to achieve a desired output voltage or in parallel to provide a desired current capability (amperes). The conducting wires that take the current off the modules may contain silver, copper or other non-magnetic conductive transition metals. Bypass diodes may be incorporated or used externally, in case of partial module shading, to maximize the output of module sections still illuminated.\n\nSome special solar PV modules include concentrators in which light is focused by lenses or mirrors onto smaller cells. This enables the use of cells with a high cost per unit area (such as gallium arsenide) in a cost-effective way. \n\nSolar panels also use metal frames consisting of racking components, brackets, reflector shapes, and troughs to better support the panel structure.\n\nIn 1839, the ability of some materials to create an electrical charge from light exposure was first observed by Alexandre-Edmond Becquerel. This observation was not replicated again until 1873, when Willoughey Smith discovered that the charge could be caused by light hitting selenium. After this discovery, William Grylls Adams and Richard Evans Day published \"The action of light on selenium\" in 1876, describing the experiment they used to replicate Smith's results. In 1881, Charles Fritts created the first commercial solar panel, which was reported by Fritts as \"continuous, constant and of considerable force not only by exposure to sunlight but also to dim, diffused daylight.\" However, these solar panels were very inefficient, especially compared to coal-fired power plants. In 1939, Russell Ohl created the solar cell design that is used in many modern solar panels. He patented his design in 1941. In 1954, this design was first used by Bell Labs to create the first commercially viable silicon solar cell.\n\nDepending on construction, photovoltaic modules can produce electricity from a range of frequencies of light, but usually cannot cover the entire solar range (specifically, ultraviolet, infrared and low or diffused light). Hence, much of the incident sunlight energy is wasted by solar modules, and they can give far higher efficiencies if illuminated with monochromatic light. Therefore, another design concept is to split the light into six to eight different wavelength ranges that will produce a different color of light, and direct the beams onto different cells tuned to those ranges. This has been projected to be capable of raising efficiency by 50%. \n\nScientists from Spectrolab, a subsidiary of Boeing, have reported development of multi-junction solar cells with an efficiency of more than 40%, a new world record for solar photovoltaic cells. The Spectrolab scientists also predict that concentrator solar cells could achieve efficiencies of more than 45% or even 50% in the future, with theoretical efficiencies being about 58% in cells with more than three junctions.\n\nCurrently, the best achieved sunlight conversion rate (solar module efficiency) is around 21.5% in new commercial products typically lower than the efficiencies of their cells in isolation. The most efficient mass-produced solar modules have power density values of up to 175 W/m (16.22 W/ft). \n\nResearch by Imperial College, London has shown that the efficiency of a solar panel can be improved by studding the light-receiving semiconductor surface with aluminum nanocylinders similar to the ridges on Lego blocks. The scattered light then travels along a longer path in the semiconductor which means that more photons can be absorbed and converted into current. Although these nanocylinders have been used previously (aluminum was preceded by gold and silver), the light scattering occurred in the near infrared region and visible light was absorbed strongly. Aluminum was found to have absorbed the ultraviolet part of the spectrum, while the visible and near infrared parts of the spectrum were found to be scattered by the aluminum surface. This, the research argued, could bring down the cost significantly and improve the efficiency as aluminum is more abundant and less costly than gold and silver. The research also noted that the increase in current makes thinner film solar panels technically feasible without \"compromising power conversion efficiencies, thus reducing material consumption\".\n\nMicro-inverted solar panels are wired in parallel, which produces more output than normal panels which are wired in series with the output of the series determined by the lowest performing panel (this is known as the \"Christmas light effect\"). Micro-inverters work independently so each panel contributes its maximum possible output given the available sunlight.\n\nMost solar modules are currently produced from crystalline silicon (c-Si) solar cells made of multicrystalline and monocrystalline silicon. In 2013, crystalline silicon accounted for more than 90 percent of worldwide PV production, while the rest of the overall market is made up of thin-film technologies using cadmium telluride, CIGS and amorphous silicon \n\nEmerging, third generation solar technologies use advanced thin-film cells. They produce a relatively high-efficiency conversion for the low cost compared to other solar technologies. Also, high-cost, high-efficiency, and close-packed rectangular multi-junction (MJ) cells are preferably used in solar panels on spacecraft, as they offer the highest ratio of generated power per kilogram lifted into space. MJ-cells are compound semiconductors and made of gallium arsenide (GaAs) and other semiconductor materials. Another emerging PV technology using MJ-cells is concentrator photovoltaics ( CPV ).\n\nIn rigid thin-film modules, the cell and the module are manufactured in the same production line. The cell is created on a glass substrate or superstrate, and the electrical connections are created \"in situ\", a so-called \"monolithic integration\". The substrate or superstrate is laminated with an encapsulant to a front or back sheet, usually another sheet of glass. The main cell technologies in this category are CdTe, or a-Si, or a-Si+uc-Si tandem, or CIGS (or variant). Amorphous silicon has a sunlight conversion rate of 6–12%\n\nFlexible thin film cells and modules are created on the same production line by depositing the photoactive layer and other necessary layers on a flexible substrate. If the substrate is an insulator (e.g. polyester or polyimide film) then monolithic integration can be used. If it is a conductor then another technique for electrical connection must be used. The cells are assembled into modules by laminating them to a transparent colourless fluoropolymer on the front side (typically ETFE or FEP) and a polymer suitable for bonding to the final substrate on the other side.\n\nSeveral companies have begun embedding electronics into PV modules. This enables performing maximum power point tracking (MPPT) for each module individually, and the measurement of performance data for monitoring and fault detection at module level. Some of these solutions make use of power optimizers, a DC-to-DC converter technology developed to maximize the power harvest from solar photovoltaic systems. As of about 2010, such electronics can also compensate for shading effects, wherein a shadow falling across a section of a module causes the electrical output of one or more strings of cells in the module to fall to zero, but not having the output of the entire module fall to zero.\n\nModule performance is generally rated under standard test conditions (STC): irradiance of 1,000 W/m, solar spectrum of AM 1.5 and module temperature at 25°C.\n\nElectrical characteristics include nominal power (P, measured in W), open circuit voltage (V), short circuit current (I, measured in amperes), maximum power voltage (V), maximum power current (I), peak power, (watt-peak, W), and module efficiency (%).\n\nNominal voltage refers to the voltage of the battery that the module is best suited to charge; this is a leftover term from the days when solar modules were only used to charge batteries. The actual voltage output of the module changes as lighting, temperature and load conditions change, so there is never one specific voltage at which the module operates. Nominal voltage allows users, at a glance, to make sure the module is compatible with a given system.\n\nOpen circuit voltage or V is the maximum voltage that the module can produce when not connected to an electrical circuit or system. V can be measured with a voltmeter directly on an illuminated module's terminals or on its disconnected cable.\n\nThe peak power rating, W, is the maximum output under standard test conditions (not the maximum possible output). Typical modules, which could measure approximately , will be rated from as low as 75 W to as high as 350 W, depending on their efficiency. At the time of testing, the test modules are binned according to their test results, and a typical manufacturer might rate their modules in 5 W increments, and either rate them at +/- 3%, +/-5%, +3/-0% or +5/-0%.\nThe ability of solar modules to withstand damage by rain, hail, heavy snow load, and cycles of heat and cold varies by manufacturer, although most solar panels on the U.S. market are UL listed, meaning they have gone through testing to withstand hail. Many crystalline silicon module manufacturers offer a limited warranty that guarantees electrical production for 10 years at 90% of rated power output and 25 years at 80%. \n\nPotential induced degradation (also called PID) is a potential induced performance degradation in crystalline photovoltaic modules, caused by so-called stray currents. This effect may cause power loss of up to 30%.\n\nThe largest challenge for photovoltaic technology is said to be the purchase price per watt of electricity produced, new materials and manufacturing techniques continue to improve the price to power performance. The problem resides in the enormous activation energy that must be overcome for a photon to excite an electron for harvesting purposes. Advancements in photovoltaic technologies have brought about the process of \"doping\" the silicon substrate to lower the activation energy thereby making the panel more efficient in converting photons to retrievable electrons. \n\nChemicals such as Boron (p-type) are applied into the semiconductor crystal in order to create donor and acceptor energy levels substantially closer to the valence and conductor bands. In doing so, the addition of Boron impurity allows the activation energy to decrease 20 fold from 1.12 eV to 0.05 eV. Since the potential difference (E) is so low, the Boron is able to thermally ionize at room temperatures. This allows for free energy carriers in the conduction and valence bands thereby allowing greater conversion of photons to electrons.\n\nSolar panel conversion efficiency, typically in the 20% range, is reduced by dust, grime, pollen, and other particulates that accumulate on the solar panel. \"A dirty solar panel can reduce its power capabilities by up to 30% in high dust/pollen or desert areas\", says Seamus Curran, associate professor of physics at the University of Houston and director of the Institute for NanoEnergy, which specializes in the design, engineering, and assembly of nanostructures.\n\nPaying to have solar panels cleaned is often not a good investment; researchers found panels that had not been cleaned, or rained on, for 145 days during a summer drought in California, lost only 7.4% of their efficiency. Overall, for a typical residential solar system of 5 kW, washing panels halfway through the summer would translate into a mere $20 gain in electricity production until the summer drought ends—in about 2 ½ months. For larger commercial rooftop systems, the financial losses are bigger but still rarely enough to warrant the cost of washing the panels. On average, panels lost a little less than 0.05% of their overall efficiency per day.\n\nMost parts of a solar module can be recycled including up to 95% of certain semiconductor materials or the glass as well as large amounts of ferrous and non-ferrous metals. Some private companies and non-profit organizations are currently engaged in take-back and recycling operations for end-of-life modules.\n\nRecycling possibilities depend on the kind of technology used in the modules:\n\nSince 2010, there is an annual European conference bringing together manufacturers, recyclers and researchers to look at the future of PV module recycling.\n\nIn 2010, 15.9 GW of solar PV system installations were completed, with solar PV pricing survey and market research company PVinsights reporting growth of 117.8% in solar PV installation on a year-on-year basis.\n\nWith over 100% year-on-year growth in PV system installation, PV module makers dramatically increased their shipments of solar modules in 2010. They actively expanded their capacity and turned themselves into gigawatt GW players. According to PVinsights, five of the top ten PV module companies in 2010 are GW players. Suntech, First Solar, Sharp, Yingli and Trina Solar are GW producers now, and most of them doubled their shipments in 2010.\n\nThe basis of producing solar panels revolves around the use of silicon cells. These silicon cells are typically 10-20% efficient at converting sunlight into electricity, with newer production models now exceeding 22%.\nIn order for solar panels to become more efficient, researchers across the world have been trying to develop new technologies to make solar panels more effective at turning sunlight into energy.\n\nIn 2014, the world's top four solar module producers in terms of shipped capacity during the calendar year of 2014 were Yingli, Trina Solar, Sharp Solar and Canadian Solar.\n \nAverage pricing information divides in three pricing categories: those buying small quantities (modules of all sizes in the kilowatt range annually), mid-range buyers (typically up to 10 MWp annually), and large quantity buyers (self-explanatory—and with access to the lowest prices). Over the long term there is clearly a systematic reduction in the price of cells and modules. For example, in 2012 it was estimated that the quantity cost per watt was about US$0.60, which was 250 times lower than the cost in 1970 of US$150. A 2015 study shows price/kWh dropping by 10% per year since 1980, and predicts that solar could contribute 20% of total electricity consumption by 2030, whereas the International Energy Agency predicts 16% by 2050.\n\nReal world energy production costs depend a great deal on local weather conditions. In a cloudy country such as the United Kingdom, the cost per produced kWh is higher than in sunnier countries like Spain.\n\nFollowing to RMI, Balance-of-System (BoS) elements, this is, non-module cost of non-microinverter solar modules (as wiring, converters, racking systems and various components) make up about half of the total costs of installations.\n\nFor merchant solar power stations, where the electricity is being sold into the electricity transmission network, the cost of solar energy will need to match the wholesale electricity price. This point is sometimes called 'wholesale grid parity' or 'busbar parity'.\n\nSome photovoltaic systems, such as rooftop installations, can supply power directly to an electricity user. In these cases, the installation can be competitive when the output cost matches the price at which the user pays for his electricity consumption. This situation is sometimes called 'retail grid parity', 'socket parity' or 'dynamic grid parity'. Research carried out by UN-Energy in 2012 suggests areas of sunny countries with high electricity prices, such as Italy, Spain and Australia, and areas using diesel generators, have reached retail grid parity.\n\nGround-mounted photovoltaic systems are usually large, utility-scale solar power plants. Their solar modules are held in place by racks or frames that are attached to ground-based mounting supports. Ground based mounting supports include:\n\nRoof-mounted solar power systems consist of solar modules held in place by racks or frames attached to roof-based mounting supports. Roof-based mounting supports include:\n\nSolar trackers increase the amount of energy produced per module at a cost of mechanical complexity and need for maintenance. They sense the direction of the Sun and tilt or rotate the modules as needed for maximum exposure to the light. Alternatively, fixed racks hold modules stationary as the sun moves across the sky. The fixed rack sets the angle at which the module is held. Tilt angles equivalent to an installation's latitude are common. Most of these fixed racks are set on poles above ground. Panels that face West or East may provide slightly lower energy, but evens out the supply, and may provide more power during peak demand.\n\nStandards generally used in photovoltaic modules:\n\nOutdoor solar panels usually includes MC4 connectors. Automotive solar panels also can include car lighter and USB adapter. Indoor panels (including solar pv glasses, thin films and windows) can integrate microinverter (AC Solar panels).\n\nThere are many practical applications for the use of solar panels or photovoltaics. It can first be used in agriculture as a power source for irrigation. In health care solar panels can be used to refrigerate medical supplies. It can also be used for infrastructure. PV modules are used in photovoltaic systems and include a large variety of electric devices:\n\nSolar panel has been a well-known method of generating clean, emission free electricity. However, it produces only direct current electricity (DC), which is not what normal appliances use. Solar photovoltaic systems (solar PV systems) are often made of solar PV panels (modules) and inverter (changing DC to AC). Solar PV panels are mainly made of solar photovoltaic cells, which has no fundamental difference to the material for making computer chips. The process of producing solar PV cells (computer chips) is energy intensive and involves highly poisonous and environmental toxic chemicals. There are few solar PV manufacturing plants around the world producing PV modules with energy produced from PV. This measure greatly reduces the carbon footprint during the manufacturing process. Managing the chemicals used in the manufacturing process is subject to the factories' local laws and regulations. \n\nWith the increasing levels of rooftop photovoltaic systems, the energy flow becomes 2-way. When there is more local generation than consumption, electricity is exported to the grid. However, electricity network traditionally is not designed to deal with the 2- way energy transfer. Therefore, some technical issues may occur. For example in Queensland Australia, there have been more than 30% of households with rooftop PV by the end of 2017. The famous Californian 2020 duck curve appears very often for a lot of communities from 2015 onwards. An over-voltage issue may come out as the electricity flows from these PV households back to the network. There are solutions to manage the over voltage issue, such as regulating PV inverter power factor, new voltage and energy control equipment at electricity distributor level, re-conducting the electricity wires, demand side management, etc. There are often limitations and costs related to these solutions.\n\nThere is no silver bullet in electricity or energy demand and bill management, because customers (sites) have different specific situations, e.g. different comfort/convenience needs, different electricity tariffs, or different usage patterns. Electricity tariff may have a few elements, such as daily access and metering charge, energy charge (based on kWh, MWh) or peak demand charge (e.g. a price for the highest 30min energy consumption in a month). PV is a promising option for reducing energy charge when electricity price is reasonably high and continuously increasing, such as in Australia and Germany. However for sites with peak demand charge in place, PV may be less attractive if peak demands mostly occur in the late afternoon to early evening, for example residential communities. Overall, energy investment is largely an economical decision and it is better to make investment decisions based on systematical evaluation of options in operational improvement, energy efficiency, onsite generation and energy storage.\n"}
{"id": "37755119", "url": "https://en.wikipedia.org/wiki?curid=37755119", "title": "Surface and bulk erosion", "text": "Surface and bulk erosion\n\nSurface and bulk erosion are two different forms of erosion that describe how a degrading polymer erodes. In surface erosion, the polymer degrades from the exterior surface. The inside of the material does not degrade until all the surrounding material around it has been degraded. In bulk erosion, degradation occurs throughout the whole material equally. Both the surface and the inside of the material degrade. Surface erosion and bulk erosion are not exclusive, many materials undergo a combination of surface and bulk erosion. Therefore, surface and bulk erosion can be thought of as a spectrum instead of two separate categories.\n\nIn surface erosion, the erosion rate is directly proportional to the surface area of the material. For very thin materials, the surface area remains relatively constant when the material degrades, which allows surface erosion to be characterized as zero order release since the rate of degradation is constant. In bulk erosion, the erosion rate depends on the volume of the material. Due to degradation, the volume of the material decreases during bulk erosion causing the erosion rate to decrease over time. Therefore, bulk erosion rates are difficult to control since it is not zero order. To determine whether a polymer will undergo surface or bulk erosion, the degradation rate of the polymer in water (how fast the polymer reacts to water) and the rate of diffusion of water penetrating through the material must be considered. If the degradation process is faster than the diffusion process, surface erosion will occur since the material’s surface will quickly degrade before water has time to diffuse and penetrate through the material. If the diffusion process is faster than the degradation process bulk erosion will occur because water penetrates through the material before significant erosion occurs on the surface. A kinetics of the erosion of a polymer can be modified by changing the diffusion process or the degradation process. For example, blending a polymer with another polymer that is very reactive to water will speed up the degradation process and cause surface erosion. On the other hand, decreasing the dimensions of a material will allow water to travel to the center of the material more quickly, which speeds up the diffusion process and causes bulk erosion.\n\nBy mathematically modeling the rate of diffusion of water in the material and the rate of degradation of the material, it is possible to predict whether a certain material will undergo surface or bulk erosion by looking at the ratio between the two rates.\n\nThe rate of diffusion of water is modeled by the equation\n\nformula_1\n\nWhere <x> is the mean length of the material and D is the diffusion coefficient of water inside the polymer.\n\nThe rate of degradation is modeled by the following equation\n\nformula_2\n\nWhere M= molecular weight of polymer, N=Avogadro's number, N=degree of polymerization, p=density of polymer, k=degradation rate\n\nThe ratio between diffusion time and degradation time gives us a dimensionless parameter ε called the erosion number.\n\nformula_3\n\nIf ε≫1, surface erosion occurs. If ε≪1, bulk erosion occurs.\n\nFrom the model above, it is clear that certain changing certain parameters can determine what kind of erosion a polymer goes through by either increasing or decreasing the rate of the degradation process or the diffusion process. The table below summarizes how a parameter can be modified to favor surface erosion or bulk erosion.\nSince surface erosion is easier to control than bulk erosion, surface erosion is preferred in drug delivery where the release of the drug must be constant or be controlled by changing the dimensions of the material. A zero order release of a drug can be possible with surface erosion if a very thin material is used or if surface area is kept constant. Surface erosion is also useful for protecting water-soluble drugs until the time of desired drug release, because water will not penetrate through the polymer matrix and reach the drug until all the surrounding polymer has degraded. However, bulk erosion can be useful in situations that do not require controlled release, such as plastic degradation.\n"}
{"id": "38669534", "url": "https://en.wikipedia.org/wiki?curid=38669534", "title": "Surya Santoso", "text": "Surya Santoso\n\nSurya Santoso is an associate professor of Electrical Engineering at the Cockrell School of Engineering at the University of Texas at Austin and directs the Laboratory for Advanced Studies in Electric Power & Integration of Renewable Energy Systems (L-ASPIRES). A senior member of Institute of Electrical and Electronics Engineers (IEEE), he is actively involved in the IEEE Power and Energy Society and has hosted the IEEE Plain Talk on Power Quality in IEEE Power and Energy General Meeting since 2010.\n\nHe is known for his pioneering work in root-cause analysis of electric power quality disturbances and for his work in time-domain modeling and simulation of electromagnetic transients and in wind power system integration studies.\n\nDr. Santoso received his B.S. degree from Satya Wacana Christian University (1992) and his M.S. (1994) and Ph.D. (1996) degrees from the University of Texas at Austin, all in Electrical Engineering. After receiving his Ph.D. he became a postdoctoral fellow under the auspices of Electric Power Research Institute (EPRI).\n\nFrom 1997 to 2003, Dr. Santoso was a senior power systems and consulting engineer with Electrotek Concepts®. He was the lead developer for a number of intelligent systems algorithms used in power systems with applications in power quality and protection. These algorithms were commercialized as AnswerModule and are embedded in the advanced power quality monitoring platform, Dranetz’s Encore (formerly known as Signature Systems). These algorithms are also deployed in PQView - a multi-component software system developed by Electrotek Concepts® and EPRI® for building and analyzing databases of power, power quality, and energy measurements. In addition, he carried out a wide variety of power systems studies in generation, transmission, and distribution systems. Examples of these studies include but not limited to wind power integration and interconnection studies, wind turbine and plant modeling, subsynchronous resonance, harmonics, capacitor energizing, ferroresonance, transient recovery voltage of circuit breakers, lightning protection, and series capacitor bank protection.\n\nDr. Santoso joined the faculty at the Cockrell School of Engineering at the University of Texas at Austin in 2003. He teaches courses in the broad area of electric power systems, particularly in power quality, modeling and simulation of electrical transients and wind power systems. Since becoming professor, he has graduated seven doctoral and twenty-four master’s students.\n\nDr. Santoso has published over 40 journal and 65 conference articles and holds 5 patents. He co-authored one of the leading books in power quality studies, Electric Power Systems Quality, now in its third edition and is the sole author of a college textbook on the same subject, Fundamentals of Electric Power Quality. In all, his publications have been cited over 5,700 times.\n\nSince 2011 he has been an editor for ISRN Renewable Energy and since 2012 an editor for the Journal of Renewable Energy, both published by Hindawi Publishing.\n\n\n"}
{"id": "55979381", "url": "https://en.wikipedia.org/wiki?curid=55979381", "title": "Timber pilings", "text": "Timber pilings\n\nPiling foundations support many historic structures such as canneries, wharves, and shore buildings. The old pilings present challenging problems during restoration as they age and are destroyed by organisms and decay. Replacing the foundation entirely is possible but expensive. Regularly inspecting and maintaining timber piles may extend the life of the foundation.\n\nTimber pile construction in the aquatic and marine environment has a long history in Europe dating as far back as the bronze and stone age in Switzerland.\n\nDuring severe droughts in Switzerland in the mid-nineteenth century, lake areas that had been previously inundated with water, were exposed to reveal ancient archaeological remains of various types of timber piling support assemblies that served has foundations for both individual houses and community buildings. The design of these timber assemblies varied by the time of occupation, whether during the bronze or stone age, and also by geological conditions where the timbers rested.\nThe communities were called the Swiss Lake Dwellers and were located in various fresh water lakes around Zurich and other areas in Switzerland. During the archaeological excavations, many of the piles dissolved after being in contact with air.\n\nIn Venice, some of the early piling foundations were built on timber piles. The early Venetian constructors used building techniques that included using impermeable stone supported by wooden rafts and timber piles. The timber piles did not rot because they were set into the mud at the bottom of the lagoon which prevented oxygen and harmful microbes from reaching them.\n\nOver 2,000 years ago, wood builders were aware of marine borers and decay and protected wood using crude extracts and various chemicals. Further study on how to address marine borer activity and decay accelerated in the 18th century.\n\nIn the 18th and 19th centuries the study of wood preservation was revived due to the deterioration of the timber pile dikes that protected Holland as well as the high level of decay and marine borer activity in English Navy ships.\nThe early dikes in Holland were supported by timber piles.\nCreosote derived from coal processing, was discovered in the mid-18th century to prevent timber pile decay. The development of Creosote pressure treatment by John Bethel was also an important advancement in timber piling construction. Historic buildings supported by timber piles may either be either treated with Creosote or Chromated Copper Arsenate.\n\nDouglas-Fir timber piles are used most often in the Pacific Northwest while Southern Pine are used most commonly on the East Coast. Douglas-Fir is used most commonly used on the west coast due to its high strength, renewability and low cost.\n\nThere are three groups of marine borers in West Coast waters including gribbles, shipworms, and pholads, and each differs in the type of damage it causes.\n\nA Gribble (Limnoria) is a destructive crustacean that burrows into the wood surfaces. Unlike other marine borers, gribbles travel easily from timber to timber using the wood for food and shelter. Gribbles burrow to a shallow depth but can still reduce pile diameter by one inch per year; a gribble infested pile typically has an hour-glass shape at the tide line.\n\nShipworms (Teredolite) are wood-boring bivalves that burrow deeply into submerged wood. Although piles attacked by shipworms may appear sound on the surface, they may be completely riddled with a maze of tunnels. Shipworms can spread to new wood only when they are in the free-swimming larval stage. Once they attack and bore into the wood, they become imprisoned within it. Ancient mariners, realizing that shipworms were imprisoned in the wood of their ships, would sail far up river and remain in fresh water for a number of months to kill the shipworms. Experienced divers look for siphons that project from the wood or use sonic devices to estimate the extent of internal damage. Shipworm and gribble attacks can also be detected\nby immersing untreated wood panels and destructively sampling them at monthly intervals.\n\nPholads, rock-burrowing clams, burrow into and damage untreated wood in warmer waters near Hawaii and Mexico and along Oregon beaches boring only into the surface of the wood. Ensuring that the shell of the wood is undamaged will keep this Pholad borer at bay.\n\nWood above the waterline may be attacked by a number of insects, including termites, carpenter ants, and beetles. One beetle, the wharf borer (\"Nacedes melanura\"), can attack untreated or damaged treated hardwoods and conifers with high moisture contents by tunneling extensively and leaving behind dark brown fecal matter that further degrades the wood.\n\nWood decay describes wood in all stages of fungal attack, from the initial invasion of hyphae into the cell walls to the complete destruction of the wood. Wood-inhabiting fungi are most common on timber piles above the water surface since the lack of oxygen below water inhibits fungal growth.\n\nIncipient decay may develop in untreated pile tops within 1 year and reach the visible, advanced stage, termed rot, within 2 to 4 years and can extend 4 feet or more from the internally rotting areas of a Douglas-fir pile. A triangular blade scraper, a sharp shovel, or a dull probe are useful when inspecting piles for surface deterioration or marine borer attack because they allow the inspector to estimate the depth of deterioration. Because untreated wood can often be exposed while these tools are being used, a preservative solution or paste should be applied to exposed areas.\n\nCracks that have developed after the wood has been treated are highly susceptible to borers, insects and decay in the right conditions. Cracks need to be evaluated during an initial pile inspection to ascertain depth, location and treatment condition.\n\nApply a liquid preservative to cutoff tops of piles and timbers by flooding them\nwith hot creosote (150 to 200 °F), pentachlorophenol in diesel oil, or copper naphthenate in mineral spirits.\n\nA solid preservative, such as Fluor-Chrome-Arsenic-Phenol (FCAP), can be applied dry or as a paste where hawsers rip off caps and expose wood to moisture and decay organisms.\n\nCreosote effectively prevents attack by marine borers in coastal waters north of San Francisco and inorganic salts [Chromated Copper Arsenate (CCA) or Ammoniacal Copper Arsenate (ACA) are recommended south of San Francisco because of the likelihood of attack by the wood borer that is predominantly located in warmer waters.\n\nImpermeable barriers can protect preservative-treated wood piles under the waterline from marine borer attack by inhibiting the entry of borers into the wood and creating anaerobic conditions that kill established borers by limiting the available oxygen.\n\nA heavily damaged piling structure can be reinforced by cutting out the damaged section and replacing it with preservative-treated wood. Wrapping piles with plastic barriers can provide protection from marine borers for 25 years or more. Pile reinforcement with concrete can be sufficient by filling the void with coarse stone and mortar. Where damage is more severe, forms made of metal, wood, concrete, woven nylon, or pitch-impregnated fiber are attached to the pile as far down as 2 feet below the mudline.\n\nIn order to effectively preserve and maintain timber piles, regular inspection is required to detect deteriorating structures before replacement is necessary. Pile inspections should take place every five years.\n\nOne of the best ways to ascertain the cause of deterioration as well as what stage the deterioration is in, is to inspect a piling that has been removed from service. The loss of one piling used for inspection might save the remaining timber pilings and members from being replaced. In order to diagnose the problems, cut the timbers into short sections and longitudinally split each section in order to see how far the preservative has penetrated. Reuse of any treated timber pile supplied by an outside source is not recommended. Not knowing the applied treatment, past use, or if diesel fuels have been applied to the surface used to give the appearance of a recent retreatment could decrease the life of the pile. Some unscrupulous suppliers of used timber piles should be avoided, because some contractors have applied diesel fuel to the outside of the piles to bring the embedded creosote to the surface.\n\nDeep foundation\n\n\n"}
{"id": "497899", "url": "https://en.wikipedia.org/wiki?curid=497899", "title": "Vacuum fluorescent display", "text": "Vacuum fluorescent display\n\nA vacuum fluorescent display (VFD) is a display device used commonly on consumer electronics equipment such as video cassette recorders, car radios, and microwave ovens. \n\nA VFD operates on the principle of cathodoluminescence, roughly similar to a cathode ray tube, but operating at much lower voltages. Each tube in a VFD has a phosphor coated anode that is bombarded by electrons emitted from the cathode filament. In fact, each tube in a VFD is a triode vacuum tube because it also has a mesh control grid.\n\nUnlike liquid crystal displays, a VFD emits a very bright light with high contrast and can support display elements of various colors. Standard illumination figures for VFDs are around 640 cd/m with high-brightness VFDs operating at 4,000 cd/m, and experimental units as high as 35,000 cd/m depending on the drive voltage and its timing. The choice of color (which determines the nature of the phosphor) and display brightness significantly affect the lifetime of the tubes, which can range from as low as 1,500 hours for a vivid red VFD to 30,000 hours for the more common green ones. Cadmium was commonly used in VFDs in the past, but the current RoHS-compliant VFDs have eliminated this metal from their construction.\n\nVFDs can display seven-segment numerals, multi-segment alpha-numeric characters or can be made in a dot-matrix to display different alphanumeric characters and symbols. In practice, there is little limit to the shape of the image that can be displayed: it depends solely on the shape of phosphor on the anode(s).\n\nThe first VFD was the single indication DM160 by Philips in 1959. The first multi-segment VFD was the 1962 Japanese single-digit, seven-segment device. The displays became common on calculators and other consumer electronics devices. In the late 1980s hundreds of millions of units were made yearly.\n\nThe device consists of a hot cathode (filaments), anodes (phosphor) and grids encased in a glass envelope under a high vacuum condition. The cathode is made up of fine tungsten wires, coated by alkaline earth metal oxides, which emit electrons when heated by an electric current. These electrons are controlled and diffused by the grids, which are made up of thin metal. If electrons impinge on the phosphor-coated plates, they fluoresce, emitting light. Unlike the orange-glowing cathodes of traditional vacuum tubes, VFD cathodes are efficient emitters at much lower temperatures, and are therefore essentially invisible.\n\nThe principle of operation is identical to that of a vacuum tube triode. Electrons can only reach (and \"illuminate\") a given plate element if both the grid and the plate are at a positive potential with respect to the cathode. This allows the displays to be organized as multiplexed displays where the multiple grids and plates form a matrix, minimizing the number of signal pins required. In the example of the VCR display shown to the right, the grids are arranged so that only one digit is illuminated at a time. All of the similar plates in all of the digits (for example, all of the lower-left plates in all of the digits) are connected in parallel. One by one, the microprocessor driving the display enables a digit by placing a positive voltage on that digit's grid and then placing a positive voltage on the appropriate plates. Electrons flow through that digit's grid and strike those plates that are at a positive potential. The microprocessor cycles through illuminating the digits in this way at a rate high enough to create the illusion of all digits glowing at once via persistence of vision.\n\nThe extra indicators (in our example, \"VCR\", \"Hi-Fi\", \"STEREO\", \"SAP\", etc.) are arranged as if they were segments of an additional digit or two or extra segments of existing digits and are scanned using the same multiplexed strategy as the real digits. Some of these extra indicators may use a phosphor that emits a different color of light, for example, orange.\n\nThe light emitted by most VFDs contains many colors and can often be filtered to enhance the color saturation providing a deep green or deep blue, depending on the whims of the product's designers. Phosphors used in VFDs are different from those in cathode-ray displays since they must emit acceptable brightness with only around 50 volts of electron energy, compared to several thousand volts in a CRT.\n\nBesides brightness, VFDs have the advantages of being rugged, inexpensive, and easily configured to display a wide variety of customized messages, and unlike LCDs, VFDs are not limited by the response time of rearranging liquid crystals and are thus able to function normally in cold, even sub-zero, temperatures, making them ideal for outdoor devices in cold climates. Early on, the main disadvantage of such displays was their use of significantly more power (0.2 watts) than a simple LCD. This was considered a significant drawback for battery-operated equipment like calculators, so VFDs ended up being used mainly in equipment powered by an AC supply or heavy-duty rechargeable batteries.\nDuring the 1980s, this display began to be used in automobiles, especially where car makers were experimenting with digital displays for vehicle instruments such as speedometers and odometers. A good example of these were the high-end Subaru cars made in the early 1980s (referred to by Subaru enthusiasts as a \"digi-dash\", or digital dashboard). The brightness of VFDs makes them well suited for use in cars. Current models of Renault MPV, Scenic and Espace use VFD panels to show all functions on the dashboard including the radio and multi message panel. They are bright enough to read in full sunlight as well as dimmable for use at night. This panel uses four colors; the usual blue/green as well as deep blue, red and yellow/orange.\n\nThis technology was also used from 1979 to the mid-1980s in portable electronic game units. These games featured bright, clear displays but the size of the largest vacuum tubes that could be manufactured inexpensively kept the size of the displays quite small, often requiring the use of magnifying Fresnel lenses. While later games had sophisticated multi-color displays, early games achieved color effects using transparent filters to change the color of the (usually light blue) light emitted by the phosphors. High power consumption and high manufacturing cost contributed to the demise of the VFD as a videogame display. LCD games could be manufactured for a fraction of the price, did not require frequent changes of batteries (or AC adapters) and were much more portable. Since the late 1990s, backlit color active-matrix LCD displays have been able to cheaply reproduce arbitrary images in any color, a marked advantage over fixed-color, fixed-character VFDs. This is one of the main reasons for the decline in popularity of VFDs, although they continue to be made. Many low-cost DVD players still feature VFDs.\n\nFrom the mid-1980s onwards, VFDs were used for applications requiring smaller displays with high brightness specifications, though now the adoption of high-brightness organic light-emitting diodes (OLEDs) is pushing VFDs out of these markets.\n\nIn addition to the widely used fixed character VFD, a graphic type made of an array of individually addressable pixels is also available. These more sophisticated displays offer the flexibility of displaying arbitrary images, and may still be a useful choice for some types of consumer equipment.\n\nSeveral radio amateurs have experimented with the possibilities of using VFDs as triode amplifiers. In 2015, Korg released the Nutube, an analogue audio amplifier component based on VFD technology. The Nutube is used in applications such as guitar amplifiers from Vox and the Apex Sangaku headphone amplifier.\n\nFading is sometimes a problem with VFD displays. Light output drops over time due to falling emission and reduction of phosphor efficiency. How quickly and how far this falls depends on the construction and operation of the VFD. In some equipment, loss of VFD output can render the equipment inoperable.\n\nEmission may usually be restored by raising filament voltage. Thirty-three percent voltage boost can rectify moderate fade, and 66% boost severe fade. This can make the filaments visible in use, though the usual green-blue VFD filter helps reduce any such red or orange light from the filament.\n\nSome researchers suggest that \"zapping\" the phosphor and heater(s) with selected laser wavelengths (patent pending) can restore some lost emission by annealing the phosphor and exposing fresh heater electron emitting material in some cases.\nA related approach appears to also work on OLED displays by a different mechanism. \n\nOf the three prevalent display technologies – VFD, LCD, and LED – the VFD was the first to be developed. It was used in early handheld calculators. LED displays displaced VFDs in this use as the very small LEDs used required less power, thereby extending battery life, though early LED displays had problems achieving uniform brightness levels across all display segments. Later, LCDs displaced LEDs, offering even lower power requirements.\n\nThe first VFD was the single indication DM160 by Philips in 1959. It could easily be driven by transistors, so was aimed at computer applications as it was easier to drive than a neon and had longer life than a light bulb. This was made obsolete by LEDs. The 1962 Japanese single digit seven segment display in terms of anode was more like the Philips DM70 / DM71 Magic Eye as the DM160 has a spiral wire anode. The Japanese seven segment VFD meant that no patent royalties needed to be paid on desk calculator displays as would have been the case using Nixies or Planaplex neon digits. In the UK the Philips designs were made and marketed by Mullard (almost wholly owned by Philips even before WWII).\n\nThe Russian IV-15 VFD tube is very similar to the DM160. The DM160, DM70/DM71 and Russian IV-15 can (like a VFD panel) be used as triodes. The DM160 is thus the smallest VFD and smallest triode valve. The IV-15 is slightly different shape (see photo of DM160 and IV-15 for comparison).\n\n\n"}
